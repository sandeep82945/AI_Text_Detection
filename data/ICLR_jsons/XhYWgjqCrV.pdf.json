{
    "abstractText": "By contextualizing the kernel as global as possible, Modern ConvNets have shown great potential in computer vision tasks. However, recent progress on multi-order game-theoretic interaction within deep neural networks (DNNs) reveals the representation bottleneck of modern ConvNets, where the expressive interactions have not been effectively encoded with the increased kernel size. To tackle this challenge, we propose a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models with favorable complexity-performance trade-offs. MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively. MogaNet exhibits great scalability, impressive efficiency of parameters, and competitive performance compared to state-of-the-art ViTs and ConvNets on ImageNet and various downstream vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D&3D human pose estimation, and video prediction. Notably, MogaNet hits 80.0% and 87.8% accuracy with 5.2M and 181M parameters on ImageNet-1K, outperforming ParC-Net and ConvNeXtL, while saving 59% FLOPs and 17M parameters, respectively. The source code is available at https://github.com/Westlake-AI/MogaNet.",
    "authors": [
        {
            "affiliations": [],
            "name": "Siyuan Li"
        },
        {
            "affiliations": [],
            "name": "Zedong Wang"
        },
        {
            "affiliations": [],
            "name": "Zicheng Liu"
        },
        {
            "affiliations": [],
            "name": "Cheng Tan"
        },
        {
            "affiliations": [],
            "name": "Haitao Lin Di Wu"
        },
        {
            "affiliations": [],
            "name": "Zhiyuan Chen"
        },
        {
            "affiliations": [],
            "name": "Jiangbin Zheng"
        },
        {
            "affiliations": [],
            "name": "Stan Z. Li"
        }
    ],
    "id": "SP:1a3431c0362aceb0599a8c48bdddc7e666af80d3",
    "references": [
        {
            "authors": [
                "Marco Ancona",
                "Cengiz Oztireli",
                "Markus Gross"
            ],
            "title": "Explaining deep neural networks with a polynomial time algorithm for shapley value approximation",
            "venue": "In International Conference on Machine Learning (ICML), pp. 272\u2013281. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "Marco Ancona",
                "Cengiz Oztireli",
                "Markus Gross"
            ],
            "title": "Explaining deep neural networks with a polynomial time algorithm for shapley value approximation",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Georg Heigold",
                "Chen Sun",
                "Mario Lu\u010di\u0107",
                "Cordelia Schmid"
            ],
            "title": "Vivit: A video vision transformer",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Brock",
                "Soham De",
                "Samuel L. Smith"
            ],
            "title": "Characterizing signal propagation to close the performance gap in unnormalized resnets",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Brock",
                "Soham De",
                "Samuel L. Smith",
                "Karen Simonyan"
            ],
            "title": "High-performance large-scale image recognition without normalization",
            "venue": "ArXiv, abs/2102.06171,",
            "year": 2021
        },
        {
            "authors": [
                "Tom B Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos"
            ],
            "title": "Cascade r-cnn: High-quality object detection and instance segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Yue Cao",
                "Jiarui Xu",
                "Stephen Lin",
                "Fangyun Wei",
                "Han Hu"
            ],
            "title": "Gcnet: Non-local networks meet squeeze-excitation networks and beyond",
            "venue": "In International Conference on Computer Vision Workshop (ICCVW),",
            "year": 2019
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Hanting Chen",
                "Yunhe Wang",
                "Tianyu Guo",
                "Chang Xu",
                "Yiping Deng",
                "Zhenhua Liu",
                "Siwei Ma",
                "Chunjing Xu",
                "Chao Xu",
                "Wen Gao"
            ],
            "title": "Pre-trained image processing transformer",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Kai Chen",
                "Jiaqi Wang",
                "Jiangmiao Pang",
                "Yuhang Cao",
                "Yu Xiong",
                "Xiaoxiao Li",
                "Shuyang Sun",
                "Wansen Feng",
                "Ziwei Liu",
                "Jiarui Xu",
                "Zheng Zhang",
                "Dazhi Cheng",
                "Chenchen Zhu",
                "Tianheng Cheng",
                "Qijie Zhao",
                "Buyu Li",
                "Xin Lu",
                "Rui Zhu",
                "Yue Wu",
                "Jifeng Dai",
                "Jingdong Wang",
                "Jianping Shi",
                "Wanli Ouyang",
                "Chen Change Loy",
                "Dahua Lin"
            ],
            "title": "MMDetection: Open mmlab detection toolbox and benchmark",
            "venue": "https://github.com/open-mmlab/mmdetection, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Mengzhao Chen",
                "Mingbao Lin",
                "Ke Li",
                "Yunhang Shen",
                "Yongjian Wu",
                "Fei Chao",
                "Rongrong Ji"
            ],
            "title": "Cf-vit: A general coarse-to-fine method for vision transformer",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2023
        },
        {
            "authors": [
                "Yinpeng Chen",
                "Xiyang Dai",
                "Dongdong Chen",
                "Mengchen Liu",
                "Xiaoyi Dong",
                "Lu Yuan",
                "Zicheng Liu"
            ],
            "title": "Mobile-former: Bridging mobilenet and transformer",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Xu Cheng",
                "Chuntung Chu",
                "Yi Zheng",
                "Jie Ren",
                "Quanshi Zhang"
            ],
            "title": "A game-theoretic taxonomy of visual concepts in dnns",
            "venue": "arXiv preprint arXiv:2106.10938,",
            "year": 2021
        },
        {
            "authors": [
                "Vasileios Choutas",
                "Georgios Pavlakos",
                "Timo Bolkart",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "Monocular expressive body regression through body-driven attention",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Xiangxiang Chu",
                "Zhi Tian",
                "Yuqing Wang",
                "Bo Zhang",
                "Haibing Ren",
                "Xiaolin Wei",
                "Huaxia Xia",
                "Chunhua Shen"
            ],
            "title": "Twins: Revisiting the design of spatial attention in vision transformers",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2020
        },
        {
            "authors": [
                "Ekin Dogus Cubuk",
                "Barret Zoph",
                "Dandelion Man\u00e9",
                "Vijay Vasudevan",
                "Quoc V. Le"
            ],
            "title": "Autoaugment: Learning augmentation strategies from data",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Jifeng Dai",
                "Haozhi Qi",
                "Yuwen Xiong",
                "Yi Li",
                "Guodong Zhang",
                "Han Hu",
                "Yichen Wei"
            ],
            "title": "Deformable convolutional networks",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Zihang Dai",
                "Hanxiao Liu",
                "Quoc V Le",
                "Mingxing Tan"
            ],
            "title": "Coatnet: Marrying convolution and attention for all data sizes",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "St\u00e9phane d\u2019Ascoli",
                "Hugo Touvron",
                "Matthew Leavitt",
                "Ari Morcos",
                "Giulio Biroli",
                "Levent Sagun. Convit"
            ],
            "title": "Improving vision transformers with soft convolutional inductive biases",
            "venue": "arXiv preprint arXiv:2103.10697,",
            "year": 2021
        },
        {
            "authors": [
                "Huiqi Deng",
                "Qihan Ren",
                "Xu Chen",
                "Hao Zhang",
                "Jie Ren",
                "Quanshi Zhang"
            ],
            "title": "Discovering and explaining the representation bottleneck of dnns",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Honghao Chen",
                "Xiangyu Zhang",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "title": "Repmlpnet: Hierarchical vision mlp with re-parameterized locality",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohan Ding",
                "X. Zhang",
                "Yi Zhou",
                "Jungong Han",
                "Guiguang Ding",
                "Jian Sun"
            ],
            "title": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
            "venue": "Neural Networks,",
            "year": 2018
        },
        {
            "authors": [
                "Haoqi Fan",
                "Bo Xiong",
                "Karttikeya Mangalam",
                "Yanghao Li",
                "Zhicheng Yan",
                "Jitendra Malik",
                "Christoph Feichtenhofer"
            ],
            "title": "Multiscale vision transformers",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Zhen-Hua Feng",
                "Patrik Huber",
                "Josef Kittler",
                "Peter Hancock",
                "Xiao-Jun Wu",
                "Qijun Zhao",
                "Paul Koppen",
                "Matthias R\u00e4tsch"
            ],
            "title": "Evaluation of dense 3d reconstruction from 2d face images in the wild",
            "venue": "In 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG",
            "year": 2018
        },
        {
            "authors": [
                "Zhangyang Gao",
                "Cheng Tan",
                "Lirong Wu",
                "Stan Z. Li"
            ],
            "title": "Simvp: Simpler yet better video prediction",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Robert Geirhos",
                "Patricia Rubisch",
                "Claudio Michaelis",
                "Matthias Bethge",
                "Felix Wichmann",
                "Wieland Brendel"
            ],
            "title": "Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Robert Geirhos",
                "Kantharaju Narayanappa",
                "Benjamin Mitzkus",
                "Tizian Thieringer",
                "Matthias Bethge",
                "Felix A Wichmann",
                "Wieland Brendel"
            ],
            "title": "Partial success in closing the gap between human and machine vision",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Graham",
                "Alaaeldin El-Nouby",
                "Hugo Touvron",
                "Pierre Stock",
                "Armand Joulin",
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze"
            ],
            "title": "Levit: a vision transformer in convnet\u2019s clothing for faster inference",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre H Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Daniel Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Jianyuan Guo",
                "Kai Han",
                "Han Wu",
                "Chang Xu",
                "Yehui Tang",
                "Chunjing Xu",
                "Yunhe Wang"
            ],
            "title": "Cmt: Convolutional neural networks meet vision transformers",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Meng-Hao Guo",
                "Cheng-Ze Lu",
                "Zheng-Ning Liu",
                "Ming-Ming Cheng",
                "Shi-Min Hu"
            ],
            "title": "Visual attention network",
            "venue": "Computational Visual Media (CVMJ),",
            "year": 2023
        },
        {
            "authors": [
                "Kai Han",
                "An Xiao",
                "Enhua Wu",
                "Jianyuan Guo",
                "Chunjing Xu",
                "Yunhe Wang"
            ],
            "title": "Transformer in transformer",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Qi Han",
                "Zejia Fan",
                "Qi Dai",
                "Lei Sun",
                "Ming-Ming Cheng",
                "Jiaying Liu",
                "Jingdong Wang"
            ],
            "title": "Demystifying local vision transformer: Sparse connectivity, weight sharing, and dynamic",
            "venue": "weight. arXiv:2106.04263,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Katherine Hermann",
                "Ting Chen",
                "Simon Kornblith"
            ],
            "title": "The origins and prevalence of texture bias in convolutional neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Elad Hoffer",
                "Itay Hubara",
                "Daniel Soudry"
            ],
            "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Elad Hoffer",
                "Tal Ben-Nun",
                "Itay Hubara",
                "Niv Giladi",
                "Torsten Hoefler",
                "Daniel Soudry"
            ],
            "title": "Augment your batch: Improving generalization through instance repetition",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Qibin Hou",
                "Cheng Lu",
                "Mingg-Ming Cheng",
                "Jiashi Feng"
            ],
            "title": "Conv2former: A simple transformerstyle convnet for visual recognition",
            "venue": "ArXiv, abs/2211.11943,",
            "year": 1943
        },
        {
            "authors": [
                "Andrew Howard",
                "Mark Sandler",
                "Grace Chu",
                "Liang-Chieh Chen",
                "Bo Chen",
                "Mingxing Tan",
                "Weijun Wang",
                "Yukun Zhu",
                "Ruoming Pang",
                "Vijay Vasudevan"
            ],
            "title": "Searching for mobilenetv3",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Andrew G. Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision",
            "venue": "applications. ArXiv,",
            "year": 2017
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Weizhe Hua",
                "Zihang Dai",
                "Hanxiao Liu",
                "Quoc V. Le"
            ],
            "title": "Transformer quality in linear time",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q. Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2015
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2015
        },
        {
            "authors": [
                "Yifan Jiang",
                "Shiyu Chang",
                "Zhangyang Wang"
            ],
            "title": "Transgan: Two pure transformers can make one strong gan, and that can scale up",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Zihang Jiang",
                "Qibin Hou",
                "Li Yuan",
                "Daquan Zhou",
                "Yujun Shi",
                "Xiaojie Jin",
                "Anran Wang",
                "Jiashi Feng"
            ],
            "title": "All tokens matter: Token labeling for training better vision transformers",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2014
        },
        {
            "authors": [
                "Alexandre Kirchmeyer",
                "Jia Deng"
            ],
            "title": "Convolutional networks with oriented 1d kernels",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Ross B. Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Panoptic feature pyramid networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Xiangtao Kong",
                "Xina Liu",
                "Jinjin Gu",
                "Y. Qiao",
                "Chao Dong"
            ],
            "title": "Reflash dropout in image superresolution",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2012
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Siyuan Li",
                "Zicheng Liu",
                "Zedong Wang",
                "Di Wu",
                "Zihan Liu",
                "Stan Z. Li"
            ],
            "title": "Boosting discriminative visual representation learning with scenario-agnostic",
            "venue": "mixup. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Siyuan Li",
                "Zedong Wang",
                "Zicheng Liu",
                "Di Wu",
                "Stan Z. Li"
            ],
            "title": "Openmixup: Open mixup toolbox and benchmark for visual representation learning",
            "venue": "https://github.com/Westlake-AI/ openmixup,",
            "year": 2022
        },
        {
            "authors": [
                "Siyuan Li",
                "Di Wu",
                "Fang Wu",
                "Zelin Zang",
                "Stan.Z.Li"
            ],
            "title": "Architecture-agnostic masked image modeling - from vit back to cnn",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Siyuan Li",
                "Luyuan Zhang",
                "Zedong Wang",
                "Di Wu",
                "Lirong Wu",
                "Zicheng Liu",
                "Jun Xia",
                "Cheng Tan",
                "Yang Liu",
                "Baigui Sun",
                "Stan Z. Li"
            ],
            "title": "Masked modeling for self-supervised representation learning on vision and beyond",
            "venue": "ArXiv, abs/2401.00897,",
            "year": 2023
        },
        {
            "authors": [
                "Yanyu Li",
                "Geng Yuan",
                "Yang Wen",
                "Eric Hu",
                "Georgios Evangelidis",
                "S. Tulyakov",
                "Yanzhi Wang",
                "Jian Ren"
            ],
            "title": "Efficientformer: Vision transformers at mobilenet speed",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Zicheng Li",
                "Siyuan Liu",
                "Zelin Zang",
                "Di Wu",
                "Zhiyuan Chen",
                "Stan Z. Li"
            ],
            "title": "Genurl: A general framework for unsupervised representation learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2014
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross B. Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge J. Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "S. Liu",
                "Tianlong Chen",
                "Xiaohan Chen",
                "Xuxi Chen",
                "Qiao Xiao",
                "Boqian Wu",
                "Mykola Pechenizkiy",
                "Decebal Constantin Mocanu",
                "Zhangyang Wang"
            ],
            "title": "More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Jia Ning",
                "Yue Cao",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Han Hu"
            ],
            "title": "Video swin transformer",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie. A convnet for the"
            ],
            "title": "In Conference on Computer Vision and Pattern Recognition (CVPR), pp",
            "venue": "11976\u201311986, 2022b. 1, 2, 3, 5, 6, 7, 20, 21, 25, 26, 34",
            "year": 2020
        },
        {
            "authors": [
                "Zicheng Liu",
                "Siyuan Li",
                "Ge Wang",
                "Cheng Tan",
                "Lirong Wu",
                "Stan Z. Li"
            ],
            "title": "Decoupled mixup for data-efficient",
            "venue": "learning. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Zicheng Liu",
                "Siyuan Li",
                "Di Wu",
                "Zhiyuan Chen",
                "Lirong Wu",
                "Jianzhu Guo",
                "Stan Z. Li"
            ],
            "title": "Automix: Unveiling the power of mixup for stronger classifiers",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Wenjie Luo",
                "Yujia Li",
                "Raquel Urtasun",
                "Richard S. Zemel"
            ],
            "title": "Understanding the effective receptive field in deep convolutional neural networks",
            "venue": "ArXiv, abs/1701.04128,",
            "year": 2016
        },
        {
            "authors": [
                "Ningning Ma",
                "Xiangyu Zhang",
                "Hai-Tao Zheng",
                "Jian Sun"
            ],
            "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Sachin Mehta",
                "Mohammad Rastegari"
            ],
            "title": "Mobilevit: light-weight, general-purpose, and mobilefriendly vision transformer",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Muzammal Naseer",
                "Kanchana Ranasinghe",
                "Salman H Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang"
            ],
            "title": "Intriguing properties of vision transformers",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Zizheng Pan",
                "Jianfei Cai",
                "Bohan Zhuang"
            ],
            "title": "Fast vision transformers with hilo attention",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS), 2022a. 5,",
            "year": 2022
        },
        {
            "authors": [
                "Zizheng Pan",
                "Bohan Zhuang",
                "Haoyu He",
                "Jing Liu",
                "Jianfei Cai"
            ],
            "title": "Less is more: Pay less attention in vision transformers",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Namuk Park",
                "Songkuk Kim"
            ],
            "title": "How do vision transformers work",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Niki Parmar",
                "Ashish Vaswani",
                "Jakob Uszkoreit",
                "Lukasz Kaiser",
                "Noam Shazeer",
                "Alexander Ku",
                "Dustin Tran"
            ],
            "title": "Image transformer",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Francesco Pinto",
                "Philip HS Torr",
                "Puneet K Dokania"
            ],
            "title": "An impartial take to the cnn vs transformer robustness contest",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Boris Polyak",
                "Anatoli B. Juditsky"
            ],
            "title": "Acceleration of stochastic approximation by averaging",
            "venue": "Siam Journal on Control and Optimization,",
            "year": 1992
        },
        {
            "authors": [
                "Ilija Radosavovic",
                "Raj Prateek Kosaraju",
                "Ross B. Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Designing network design spaces",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Maithra Raghu",
                "Thomas Unterthiner",
                "Simon Kornblith",
                "Chiyuan Zhang",
                "Alexey Dosovitskiy"
            ],
            "title": "Do vision transformers see like convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Yansong Tang",
                "Jie Zhou",
                "Ser Nam Lim",
                "Jiwen Lu"
            ],
            "title": "Hornet: Efficient high-order spatial interactions with recursive gated convolutions",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross B. Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),",
            "year": 2015
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew G. Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Noam M. Shazeer"
            ],
            "title": "Glu variants improve transformer",
            "venue": "ArXiv, abs/2002.05202,",
            "year": 2020
        },
        {
            "authors": [
                "Chenyang Si",
                "Weihao Yu",
                "Pan Zhou",
                "Yichen Zhou",
                "Xinchao Wang",
                "Shuicheng Yan"
            ],
            "title": "Inception transformer",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Elman Mansimov",
                "Ruslan Salakhutdinov"
            ],
            "title": "Unsupervised learning of video representations using LSTMs",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2015
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Cheng Tan",
                "Siyuan Li",
                "Zhangyang Gao",
                "Wenfei Guan",
                "Zedong Wang",
                "Zicheng Liu",
                "Lirong Wu",
                "Stan Z Li"
            ],
            "title": "Openstl: A comprehensive benchmark of spatio-temporal predictive learning",
            "venue": "In Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2023
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc V. Le"
            ],
            "title": "Efficientnetv2: Smaller models and faster training",
            "venue": "In International conference on machine learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Ilya O. Tolstikhin",
                "Neil Houlsby",
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Jessica Yung",
                "Daniel Keysers",
                "Jakob Uszkoreit",
                "Mario Lucic",
                "Alexey Dosovitskiy"
            ],
            "title": "Mlp-mixer: An all-mlp architecture for vision",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herve Jegou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International Conference on Machine Learning (ICML), pp. 10347\u201310357,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Alaaeldin El-Nouby",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Gabriel Synnaeve",
                "Jakob Verbeek",
                "Herv\u2019e J\u2019egou"
            ],
            "title": "Augmenting convolutional networks with attentionbased aggregation",
            "venue": "arXiv preprint arXiv:2112.13692,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Herv\u2019e J\u2019egou"
            ],
            "title": "Deit iii: Revenge of the vit",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Anne M Treisman",
                "Garry Gelade"
            ],
            "title": "A feature-integration theory of attention",
            "venue": "Cognitive psychology,",
            "year": 1980
        },
        {
            "authors": [
                "Asher Trockman",
                "J. Zico Kolter"
            ],
            "title": "Patches are all you need",
            "venue": "ArXiv, abs/2201.09792,",
            "year": 2022
        },
        {
            "authors": [
                "Shikhar Tuli",
                "Ishita Dasgupta",
                "Erin Grant",
                "Thomas L. Griffiths"
            ],
            "title": "Are convolutional neural networks or transformers more like human vision? ArXiv",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Guangting Wang",
                "Yucheng Zhao",
                "Chuanxin Tang",
                "Chong Luo",
                "Wenjun Zeng"
            ],
            "title": "When shift operation meets vision transformer: An extremely simple alternative to attention mechanism",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Jiayun Wang",
                "Yubei Chen",
                "Rudrasis Chakraborty",
                "Stella X. Yu"
            ],
            "title": "Orthogonal convolutional neural networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Peihao Wang",
                "Wenqing Zheng",
                "Tianlong Chen",
                "Zhangyang Wang"
            ],
            "title": "Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Qiang Wang",
                "Bei Li",
                "Tong Xiao",
                "Jingbo Zhu",
                "Changliang Li",
                "Derek F. Wong",
                "Lidia S. Chao"
            ],
            "title": "Learning deep transformer models for machine translation",
            "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2019
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z. Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma"
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pvtv2: Improved baselines with pyramid vision transformer",
            "venue": "Computational Visual Media (CVMJ),",
            "year": 2022
        },
        {
            "authors": [
                "Xiaolong Wang",
                "Ross Girshick",
                "Abhinav Gupta",
                "Kaiming He"
            ],
            "title": "Non-local neural networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Sanghyun Woo",
                "Jongchan Park",
                "Joon-Young Lee",
                "In-So Kweon"
            ],
            "title": "Cbam: Convolutional block attention module",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Sanghyun Woo",
                "Shoubhik Debnath",
                "Ronghang Hu",
                "Xinlei Chen",
                "Zhuang Liu",
                "In-So Kweon",
                "Saining Xie"
            ],
            "title": "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Fang Wu",
                "Siyuan Li",
                "Lirong Wu",
                "Stan Z. Li",
                "Dragomir R. Radev",
                "Qian Zhang"
            ],
            "title": "Discovering the representation bottleneck of graph neural networks from multi-order interactions",
            "venue": "ArXiv, abs/2205.07266,",
            "year": 2022
        },
        {
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ],
            "title": "Cvt: Introducing convolutions to vision transformers",
            "venue": "International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Haixu Wu",
                "Jialong Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Flowformer: Linearizing transformers with conservation flows",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Kan Wu",
                "Houwen Peng",
                "Minghao Chen",
                "Jianlong Fu",
                "Hongyang Chao"
            ],
            "title": "Rethinking and improving relative position encoding for vision transformer",
            "venue": "In International Conference on Computer Vision (ICCV), pp. 10033\u201310041,",
            "year": 2021
        },
        {
            "authors": [
                "Kan Wu",
                "Jinnian Zhang",
                "Houwen Peng",
                "Mengchen Liu",
                "Bin Xiao",
                "Jianlong Fu",
                "Lu Yuan"
            ],
            "title": "Tinyvit: Fast pretraining distillation for small vision transformers",
            "venue": "In European conference on computer vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Yuxin Wu",
                "Justin Johnson"
            ],
            "title": "Rethinking \u201dbatch",
            "venue": "in batchnorm. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Bin Xiao",
                "Haiping Wu",
                "Yichen Wei"
            ],
            "title": "Simple baselines for human pose estimation and tracking",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In European Conference on Computer Vision (ECCV). Springer,",
            "year": 2018
        },
        {
            "authors": [
                "Tete Xiao",
                "Mannat Singh",
                "Eric Mintun",
                "Trevor Darrell",
                "Piotr Doll\u00e1r",
                "Ross B. Girshick"
            ],
            "title": "Early convolutions help transformers see better",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Xiyang Dai",
                "Jianfeng Gao"
            ],
            "title": "Focal modulation networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Hongxu Yin",
                "Arash Vahdat",
                "Jose M. Alvarez",
                "Arun Mallya",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "title": "A-vit: Adaptive tokens for efficient vision transformer",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Yang You",
                "Jing Li",
                "Sashank Reddi",
                "Jonathan Hseu",
                "Sanjiv Kumar",
                "Srinadh Bhojanapalli",
                "Xiaodan Song",
                "James Demmel",
                "Kurt Keutzer",
                "Cho-Jui Hsieh"
            ],
            "title": "Large batch optimization for deep learning: Training BERT in 76 minutes",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Weihao Yu",
                "Mi Luo",
                "Pan Zhou",
                "Chenyang Si",
                "Yichen Zhou",
                "Xinchao Wang",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Metaformer is actually what you need for vision",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Zihang Jiang",
                "Francis EH Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Francis E.H. Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "In Proceedings of the British Machine Vision Conference (BMVC),",
            "year": 2016
        },
        {
            "authors": [
                "Zelin Zang",
                "Siyuan Li",
                "Di Wu",
                "Ge Wang",
                "Lei Shang",
                "Baigui Sun",
                "Hao Li",
                "Stan Z. Li"
            ],
            "title": "Dlme: Deep local-flatness manifold embedding",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Hang Zhang",
                "Chongruo Wu",
                "Zhongyue Zhang",
                "Yi Zhu",
                "Haibin Lin",
                "Zhi Zhang",
                "Yue Sun",
                "Tong He",
                "Jonas Mueller",
                "R Manmatha"
            ],
            "title": "Resnest: Split-attention networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Hao Zhang",
                "Sen Li",
                "Yinchao Ma",
                "Mingjie Li",
                "Yichen Xie",
                "Quanshi Zhang"
            ],
            "title": "Interpreting and boosting dropout from a game-theoretic view",
            "venue": "arXiv preprint arXiv:2009.11729,",
            "year": 2020
        },
        {
            "authors": [
                "Haokui Zhang",
                "Wenze Hu",
                "Xiaoyu Wang"
            ],
            "title": "Edgeformer: Improving light-weight convnets by learning from vision transformers",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Zhun Zhong",
                "Liang Zheng",
                "Guoliang Kang",
                "Shaozi Li",
                "Yi Yang"
            ],
            "title": "Random erasing data augmentation",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2020
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Semantic understanding of scenes through the ade20k dataset",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2018
        },
        {
            "authors": [
                "Daquan Zhou",
                "Zhiding Yu",
                "Enze Xie",
                "Chaowei Xiao",
                "Anima Anandkumar",
                "Jiashi Feng",
                "Jos\u00e9 Manuel \u00c1lvarez"
            ],
            "title": "Understanding the robustness in vision transformers",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Jinghao Zhou",
                "Chen Wei",
                "Huiyu Wang",
                "Wei Shen",
                "Cihang Xie",
                "Alan Yuille",
                "Tao Kong"
            ],
            "title": "ibot: Image bert pre-training with online tokenizer",
            "venue": "arXiv preprint arXiv:2111.07832,",
            "year": 2021
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Christian Zimmermann",
                "Duygu Ceylan",
                "Jimei Yang",
                "Bryan Russell",
                "Max Argus",
                "Thomas Brox"
            ],
            "title": "Freihand: A dataset for markerless capture of hand pose and shape from single rgb images",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Loshchilov",
                "Hutter"
            ],
            "title": "2019) optimizer using a batch size of 1024, a basic learning rate",
            "year": 2019
        },
        {
            "authors": [
                "Rastegari",
                "Chen"
            ],
            "title": "2022; Zhang et al., 2022b), including employing the weight decay",
            "year": 2022
        },
        {
            "authors": [
                "initialize backbones"
            ],
            "title": "The shorter side of training images is resized to 800 pixels, and the longer side is resized to not more than 1333 pixels. We calculate the FLOPs of compared models at 800\u00d7 1280 resolutions. Experiments of COCO detection are implemented on MMDetection (Chen et al., 2019) codebase and run on 8 NVIDIA A100",
            "venue": "GPUs. View detailed results in Appendix D.2",
            "year": 2019
        },
        {
            "authors": [
                "UperNet (Xiao"
            ],
            "title": "2018b) following Swin (Liu et al., 2021), which employs AdamW optimizer using a basic learning rate of 6\u00d7 10\u22125, a weight decay of 0.01, a poly scheduler with a linear warmup of 1,500 iterations. We use ImageNet-1K and ImageNet-21K pre-trained weights to initialize the backbones accordingly. The training images are resized to 5122",
            "year": 2021
        },
        {
            "authors": [
                "Ding"
            ],
            "title": "2022b), while canonical ConvNets (using convolutions with small kernel sizes) exhibit strong bias on local texture (Geirhos et al., 2019",
            "venue": "Hermann et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Deng"
            ],
            "title": "2022) attain visual patterns and clues and conduct middle-complexity interactions to recognize objects, while a self-attention or convolution operation can only encode global or local features to conduct high or low-complexity interactions. As the existing design of DNNs only stacks regionality perception or context aggregation operations in a cascaded way, it is inevitable to encounter the representation bottleneck",
            "venue": "vision. Human brains (Treisman & Gelade,",
            "year": 2022
        },
        {
            "authors": [
                "C MogaNet-L"
            ],
            "title": "Object detection with RetinaNet (1\u00d7 training schedule) on COCO val2017. The FLOPs are measured at resolution",
            "year": 2017
        },
        {
            "authors": [
                "2017 Xie et al",
                "2018 Hu et al",
                "Zhang"
            ],
            "title": "2022a) have become the most widely adopted ConvNet architectures in numerous CV applications. For practical usage, efficient models (Ma et al., 2018",
            "venue": "Howard et al.,",
            "year": 2017
        },
        {
            "authors": [
                "Date Type Param"
            ],
            "title": "100-epoch 300-epoch (M) Train Test Acc",
            "venue": "Train Test Acc",
            "year": 2016
        },
        {
            "authors": [
                "Zhu"
            ],
            "title": "2021), while other methods adopt settings in their original paper. size patches as visual tokens to capture long-range feature interactions among these tokens by selfattention. By introducing regional inductive bias, ViT and its variants have been extended to various vision tasks Carion et al",
            "venue": "Jiang et al. (2021a); Arnab et al",
            "year": 2021
        },
        {
            "authors": [
                "Lin et al",
                "Wu"
            ],
            "title": "2022c), pure ViTs can achieve competitive performance as ConvNets in CV tasks. In the literature of Yu et al. (2022), the MetaFormer architecture substantially influenced the design of vision backbones, and all Transformer-like models (Touvron et",
            "year": 2022
        },
        {
            "authors": [
                "Mehta",
                "Rastegari",
                "Li et al",
                "2022c",
                "Chen"
            ],
            "title": "2023) with more efficient self-attentions variants (Wang et al., 2021a). Meanwhile, the incorporation of self-attention and convolution as a hybrid backbone has been vigorously studied (Guo et al., 2022",
            "venue": "Wu et al., 2021a; Dai et al.,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nBy relaxing local inductive bias, Vision Transformers (ViTs) (Dosovitskiy et al., 2021; Liu et al., 2021) have rapidly challenged the long dominance of Convolutional Neural Networks (ConvNets) (Ren et al., 2015; He et al., 2016; Kirillov et al., 2019) for visual recognition. It is commonly conjectured that such superiority of ViT stems from its self-attention operation (Bahdanau et al., 2015; Vaswani et al., 2017), which facilitates the global-range feature interaction. From a practical standpoint, however, the quadratic complexity within self-attention prohibitively restricts its computational efficiency (Wang et al., 2021a; Hua et al., 2022) and applications to high-resolution fine-grained scenarios (Zhu et al., 2021; Jiang et al., 2021a; Liu et al., 2022a). Additionally, the dearth of local bias induces the detriment of neighborhood correlations (Pinto et al., 2022).\nTo resolve this problem, endeavors have been made by reintroducing locality priors (Wu et al., 2021a; Dai et al., 2021; Han et al., 2021a; Li et al., 2022a; Chen et al., 2022) and pyramid-like hier\u2217First two authors contribute equally. \u2020Corrsponding author (stan.zq.li@westlake.edu.cn).\narchical layouts (Liu et al., 2021; Fan et al., 2021; Wang et al., 2021b) to ViTs, albeit at the expense of model generalizability and expressivity. Meanwhile, further explorations toward ViTs (Tolstikhin et al., 2021; Raghu et al., 2021; Yu et al., 2022) have triggered the resurgence of modern ConvNets (Liu et al., 2022b; Ding et al., 2022b). With advanced training setup and ViT-style framework design, ConvNets can readily deliver competitive performance w.r.t. well-tuned ViTs across a wide range of vision benchmarks (Wightman et al., 2021; Pinto et al., 2022). Essentially, most of the modern ConvNets aim to perform feature extraction in a local-global blended fashion by contextualizing the convolutional kernel or the perception module as global as possible.\nDespite their superior performance, recent progress on multi-order game-theoretic interaction within DNNs (Ancona et al., 2019b; Zhang et al., 2020; Cheng et al., 2021) unravels that the representation capacity of modern ConvNets has not been exploited well. Holistically, low-order interactions tend to model relatively simple and common local visual concepts, which are of poor expressivity and are incapable of capturing high-level semantic patterns. In comparison, the high-order ones represent the complex concepts of absolute global scope yet are vulnerable to attacks and with poor generalizability. Deng et al. (2022) first shows that modern networks are implicitly prone to encoding extremely low- or high-order interactions rather than the empirically proved more discriminative middle ones. Attempts have been made to tackle this issue from the perspective of loss function (Deng et al., 2022) and modeling contextual relations (Wu et al., 2022a; Li et al., 2023a). This unveils the serious challenge but also the great potential for modern ConvNet architecture design.\nTo this end, we present a new ConvNet architecture named Multi-order gated aggregation Network (MogaNet) to achieve adaptive context extraction and further pursue more discriminative and efficient visual representation learning first under the guidance of interaction within modern ConvNets. In MogaNet, we encapsulate both locality perception and gated context aggregation into a compact spatial aggregation block, where features encoded by the inherent overlooked interactions are forced to congregated and contextualized efficiently in parallel. From the channel perspective, as existing methods are prone to huge channel-wise information redundancy (Raghu et al., 2021; Hua et al., 2022), we design a conceptually simple yet effective channel aggregation block to adaptively force the network to encode expressive interactions that would have originally been ignored. Intuitively, it performs channel-wise reallocation to the input, which outperforms prevalent counterparts (e.g., SE (Hu et al., 2018), RepMLP (Ding et al., 2022a)) with more favorable computational overhead.\nExtensive experiments demonstrate the consistent efficiency of model parameters and competitive performance of MogaNet at different model scales on various vision tasks, including image classification, object detection, semantic segmentation, instance segmentation, pose estimation, etc. As shown in Fig. 1, MogaNet achieves 83.4% and 87.8% top-1 accuracy with 25M and 181M parameters, which exhibits favorable computational overhead compared with existing lightweight models. MogaNet-T attains 80.0% accuracy on ImageNet-1K, outperforming the state-of-the-art ParC-NetS (Zhang et al., 2022b) by 1.0% with 2.04G lower FLOPs. MogaNet also shows great performance gain on various downstream tasks, e.g., surpassing Swin-L (Liu et al., 2021) by 2.3% APb on COCO detection with fewer parameters and computational budget. It is surprising that the parameter efficiency of MogaNet exceeds our expectations. This is probably owing to the network encodes more discriminative middle-order interactions, which maximizes the usage of model parameters."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 VISION TRANSFORMERS",
            "text": "Since the success of Transformer (Vaswani et al., 2017) in natural language processing (Devlin et al., 2018), ViT has been proposed (Dosovitskiy et al., 2021) and attained impressive results on ImageNet (Deng et al., 2009). Yet, compared to ConvNets, ViTs are over-parameterized and rely on large-scale pre-training (Bao et al., 2022; He et al., 2022; Li et al., 2023a). Targeting this problem, one branch of researchers presents lightweight ViTs (Xiao et al., 2021; Mehta & Rastegari, 2022; Li et al., 2022c; Chen et al., 2023) with efficient attentions (Wang et al., 2021a). Meanwhile, the incorporation of self-attention and convolution as a hybrid backbone has been studied (Guo et al., 2022; Wu et al., 2021a; Dai et al., 2021; d\u2019Ascoli et al., 2021; Li et al., 2022a; Pan et al., 2022b; Si et al., 2022) for imparting locality priors to ViTs. By introducing local inductive bias (Zhu et al., 2021; Chen et al., 2021; Jiang et al., 2021a; Arnab et al., 2021), advanced training strategies (Touvron et al., 2021a; Yuan et al., 2021a; Touvron et al., 2022) or extra knowledge (Jiang et al., 2021b;\nLin et al., 2022; Wu et al., 2022c), ViTs can achieve superior performance and have been extended to various vision areas. MetaFormer (Yu et al., 2022) considerably influenced the roadmap of deep architecture design, where all ViTs (Trockman & Kolter, 2022; Wang et al., 2022a) can be classified by the token-mixing strategy, such as relative position encoding (Wu et al., 2021b), local window shifting (Liu et al., 2021) and MLP layer (Tolstikhin et al., 2021), etc."
        },
        {
            "heading": "2.2 POST-VIT MODERN CONVNETS",
            "text": "Taking the merits of ViT-style framework design (Yu et al., 2022), modern ConvNets (Liu et al., 2022b; 2023; Rao et al., 2022; Yang et al., 2022) show superior performance with large kernel depth-wise convolutions (Han et al., 2021b) for global perception (view Appendix E for detail backgrounds). It primarily comprises three components: (i) embedding stem, (ii) spatial mixing block, and (iii) channel mixing block. Embedding stem downsamples the input to reduce redundancies and computational overload. We assume the input feature X is in the shape RC\u00d7H\u00d7W , we have:\nZ = Stem(X), (1) where Z is downsampled features, e.g.,. Then, the feature flows to a stack of residual blocks. In each stage, the network modules can be decoupled into two separate functional components, SMixer(\u00b7) and CMixer(\u00b7) for spatial-wise and channel-wise information propagation,\nY = X + SMixer ( Norm(X) ) , (2)\nZ = Y + CMixer ( Norm(Y ) ) , (3)\nwhere Norm(\u00b7) denotes a normalization layer, e.g., BatchNorm (Ioffe & Szegedy, 2015a) (BN). SMixer(\u00b7) can be various spatial operations (e.g., self-attention, convolution), while CMixer(\u00b7) is usually achieved by channel MLP with inverted bottleneck (Sandler et al., 2018) and expand ratio r. Notably, we abstract context aggregation in modern ConvNets as a series of operations that can adaptively aggregate contextual information while suppressing trivial redundancies in spatial mixing block SMixer(\u00b7) between two embedded features:\nO = S ( F\u03c6(X),G\u03c8(X) ) , (4)\nwhere F\u03c6(\u00b7) and G\u03c8(\u00b7) are the aggregation and context branches with parameters \u03c6 and \u03c8. Context aggregation models the importance of each position on X by the aggregation branch F\u03c6(X) and reweights the embedded feature from the context branch G\u03c8(X) by operation S(\u00b7, \u00b7)."
        },
        {
            "heading": "3 MULTI-ORDER GAME-THEORETIC INTERACTION FOR DEEP ARCHITECTURE DESIGN",
            "text": "Representation Bottleneck of DNNs Recent studies toward the generalizability (Geirhos et al., 2019; Ancona et al., 2019a; Tuli et al., 2021; Geirhos et al., 2021) and robustness (Naseer et al., 2021; Zhou et al., 2022; Park & Kim, 2022) of DNNs delivers a new perspective to improve deep architectures. Apart from them, the investigation of multi-order game-theoretic interaction unveils the representation bottleneck of DNNs. Methodologically, multi-order interactions between two input variables represent marginal contribution brought by collaborations among these two and other involved contextual variables, where the order indicates the number of contextual variables within the collaboration. Formally, it can be explained by m-th order game-theoretic interaction I(m)(i, j) and m-order interaction strength J (m), as defined in (Zhang et al., 2020; Deng et al., 2022). Considering the image with n patches in total, I(m)(i, j) measures the average interaction complexity between the patch pair i, j over all contexts consisting of m patches, where 0 \u2264 m \u2264 n\u2212 2 and the\norder m reflects the scale of the context involved in the game-theoretic interactions between pixels i and j. Normalized by the average of interaction strength, the relative interaction strength J (m) with m \u2208 (0, 1) measures the complexity of interactions encoded in DNNs. Notably, low-order interactions tend to encode common or widely-shared local texture, and the high-order ones are inclined to forcibly memorize the pattern of rare outliers (Deng et al., 2022; Cheng et al., 2021). As shown in Fig. 3, existing DNNs are implicitly prone to excessively low- or high-order interactions while suppressing the most expressive and versatile middle-order ones (Deng et al., 2022; Cheng et al., 2021). Refer to Appendix B.1 for definitions and more details.\nMulti-order Interaction for Architecture Design. Existing deep architecture design is usually derived from intuitive insights, lacking hierarchical theoretic guidance. Multi-order interaction can serve as a reference that fits well with the already gained insights on computer vision and further guides the ongoing quest. For instance, the extremely high-order interactions encoded in ViTs (e.g., DeiT in Fig. 3) may stem from its adaptive global-range self-attention mechanism. Its superior robustness can be attributed to its excessive loworder interactions, representing common and widely shared local patterns. However, the absence of locality priors still leaves ViTs lacking middle-order interactions, which cannot be replaced by the low-order ones. As for modern ConvNets (e.g., SLaK in Fig. 3), despite the 51 \u00d7 51 kernel size, it still fails to encode enough expressive interactions (view more results in Appendix B.1). Likewise, we argue that such a dilemma may be attributed to the inappropriate com-\nposition of convolutional locality priors and global context injections (Treisman & Gelade, 1980; Tuli et al., 2021; Li et al., 2023a). A naive combination of self-attention or convolutions can be intrinsically prone to the strong bias of global shape (Geirhos et al., 2021; Ding et al., 2022b) or local texture (Hermann et al., 2020), infusing extreme-order interaction preference to models. In MogaNet, we aim to provide an architecture that can adaptively force the network to encode expressive interactions that would have otherwise been ignored inherently."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "4.1 OVERVIEW OF MOGANET",
            "text": "Built upon modern ConvNets, we design a four-stage MogaNet architecture as illustrated in Fig. 2. For stage i, the input image or feature is first fed into an embedding stem to regulate the resolutions and embed intoCi dimensions. Assuming the input image inH\u00d7W resolutions, features of the four stages are in H4 \u00d7 W 4 , H 8 \u00d7 W 8 , H 16 \u00d7 W 16 , and H 32 \u00d7 W 32 resolutions respectively. Then, the embedded\nfeature flows intoNi Moga Blocks, consisting of spatial and channel aggregation blocks (in Sec. 4.2 and 4.3), for further context aggregation. After the final output, GAP and a linear layer are added for classification tasks. As for dense prediction tasks (He et al., 2017; Xiao et al., 2018b), the output from four stages can be used through neck modules (Lin et al., 2017a; Kirillov et al., 2019)."
        },
        {
            "heading": "4.2 MULTI-ORDER SPATIAL GATED AGGREGATION",
            "text": "As discussed in Sec. 3, DNNs with the incompatible composition of locality perception and context aggregation can be implicitly prone to extreme-order game-theoretic interaction strengths while suppressing the more robust and expressive middle-order ones (Li et al., 2022a; Pinto et al., 2022; Deng et al., 2022). As shown in Fig. 5, the primary obstacle pertains to how to force the network to encode the originally ignored expressive interactions and informative features. We first suppose that the essential adaptive nature of attention in ViTs has not been well leveraged and grafted into ConvNets. Thus, we propose spatial aggregation (SA) block as an instantiation of SMixer(\u00b7) to learn representations of multi-order interactions in a unified design, as shown in Fig. 4a, consisting of two cascaded components. We instantiate Eq. (2) as:\nZ = X + Moga ( FD ( Norm(X) )) , (5)\nwhere FD(\u00b7) indicates a feature decomposition module (FD) and Moga(\u00b7) denotes a multi-order gated aggregation module comprising the gating F\u03c6(\u00b7) and context branch G\u03c8(\u00b7). Context Extraction. As a pure ConvNet structure, we extract multi-order features with both static and adaptive locality perceptions. There are two complementary counterparts, fine-grained local texture (low-order) and complex global shape (middle-order), which are instantiated by Conv1\u00d71(\u00b7) and GAP(\u00b7) respectively. To force the network against its implicitly inclined interaction strengths, we design FD(\u00b7) to adaptively exclude the trivial (overlooked) interactions, defined as:\nY = Conv1\u00d71(X), (6) Z = GELU ( Y + \u03b3s ( Y \u2212GAP(Y ) )) , (7)\nwhere \u03b3s \u2208 RC\u00d71 denotes a scaling factor initialized as zeros. By re-weighting the complementary interaction component Y \u2212 GAP(Y ), FD(\u00b7) also increases spatial feature diversities (Park & Kim, 2022; Wang et al., 2022b). Then, we ensemble depth-wise convolutions (DWConv) to encode multi-order features in the context branch of Moga(\u00b7). Unlike previous works that simply combine DWConv with self-attentions to model local and global interactions (Zhang et al., 2022b; Pan et al., 2022a; Si et al., 2022; Rao et al., 2022) , we employ three different DWConv layers with dilation ratios d \u2208 {1, 2, 3} in parallel to capture low, middle, and high-order interactions: given the input feature X \u2208 RC\u00d7HW , DW5\u00d75,d=1 is first applied for low-order features; then, the output is factorized into Xl \u2208 RCl\u00d7HW , Xm \u2208 RCm\u00d7HW , and Xh \u2208 RCh\u00d7HW along the channel dimension, where Cl + Cm + Ch = C; afterward, Xm and Xh are assigned to DW5\u00d75,d=2 and DW7\u00d77,d=3, respectively, while Xl serves as identical mapping; finally, the output of Xl, Xm, and Xh are concatenated to form multi-order contexts, YC = Concat(Yl,1:Cl , Ym, Yh). Notice that the proposed FD(\u00b7) and multi-order DWConv layers only require a little extra computational overhead and parameters in comparison to DW7\u00d77 used in ConvNeXt (Liu et al., 2022b), e.g., +multi-order and +FD(\u00b7) increase 0.04M parameters and 0.01G FLOPS over DW7\u00d77 as shown in Table 1. Gated Aggregation. To adaptively aggregate the extracted feature from the context branch, we employ SiLU (Elfwing et al., 2018) activation in the gating branch, i.e., x \u00b7 Sigmoid(x), which has been well-acknowledged as an advanced version of Sigmoid activation. As illustrated in Appendix C.1, we empirically show that SiLU in MogaNet exhibits both the gating effects as Sigmoid and the stable training property. Taking the output from FD(\u00b7) as the input, we instantiate Eq. (4):\nZ = SiLU ( Conv1\u00d71(X) )\ufe38 \ufe37\ufe37 \ufe38 F\u03c6 SiLU ( Conv1\u00d71(YC) )\ufe38 \ufe37\ufe37 \ufe38 G\u03c8 , (8)\nWith the proposed SA blocks, MogaNet captures more middle-order interactions, as validated in Fig. 3. The SA block produces discriminative multi-order representations with similar parameters and FLOPs as DW7\u00d77 in ConvNeXt, which is well beyond the reach of existing methods without the cost-consuming self-attentions.\n4.3 MULTI-ORDER CHANNEL REALLOCATION\nPrevalent architectures, as illustrated in Sec. 2, perform channel-mixing CMixer(\u00b7) mainly by two linear projections, e.g., 2-layer channelwise MLP (Dosovitskiy et al., 2021; Liu et al., 2021; Tolstikhin et al., 2021) with a expand ratio r or the MLP with a 3 \u00d7 3 DWConv in between (Wang et al., 2022c; Pan et al., 2022b;a). Due to the information redundancy cross channels (Woo et al., 2018; Cao et al., 2019; Tan & Le, 2019; Wang et al., 2020), vanilla MLP requires a number of parameters (r default to 4 or 8) to achieve expected performance, showing low computational efficiency as plotted in Fig. 4c. To address this issue, most current methods directly insert a channel enhancement module, e.g., SE module (Hu et al., 2018), into MLP. Unlike these designs requiring additional MLP bottleneck, motivated by FD(\u00b7), we introduce a lightweight channel aggregation module CA(\u00b7) to adaptive reallocate channel-wise features in\nhigh-dimensional hidden spaces and further extend it to a channel aggregation (CA) block. As shown in Fig. 4b, we rewrite Eq. (3) for our CA block as:\nY = GELU ( DW3\u00d73 ( Conv1\u00d71(Norm(X)) )) ,\nZ = Conv1\u00d71 ( CA(Y ) ) +X.\n(9)\nConcretely, CA(\u00b7) is implemented by a channel-reducing projection Wr : RC\u00d7HW \u2192 R1\u00d7HW and GELU to gather and reallocate channel-wise information:\nCA(X) = X + \u03b3c ( X \u2212GELU(XWr) ) , (10)\nwhere \u03b3c is the channel-wise scaling factor initialized as zeros. It reallocates the channel-wise feature with the complementary interactions (X \u2212 GELU(XWr)). As shown in Fig. 7, CA(\u00b7) enhances originally overlooked game-theoretic interactions. Fig. 4c and Fig. 6 verify the effectiveness of CA(\u00b7) compared with vanilla MLP and MLP with SE module in channel-wise effiency and representation ability. Despite some improvements to the baseline, the MLP w/ SE module still requires large MLP ratios (e.g., r = 6) to achieve expected performance while bringing extra parameters and overhead. Yet, our CA(\u00b7) with r = 4 brings 0.6% gain over the baseline at a small extra cost (0.04M extra parameters & 0.01G FLOPs) while achieving the same performance as the baseline with r = 8."
        },
        {
            "heading": "4.4 IMPLEMENTATION DETAILS",
            "text": "Following the network design style of ConvNets (Liu et al., 2022b), we scale up MogaNet for six model sizes (X-Tiny, Tiny, Small, Base, Large, and X-Large) via stacking the different number of spatial and channel aggregation blocks at each stage, which has similar numbers of parameters as RegNet (Radosavovic et al., 2020) variants. Network configurations and hyper-parameters are detailed in Table A1. FLOPs and throughputs are analyzed in Appendix C.3. We set the channels of the multi-order DWConv layers to Cl : Cm : Ch = 1:3:4 (see Appendix C.2). Similar to (Touvron et al., 2021c; Li et al., 2022a;c), the first embedding stem in MogaNet is designed as two stacked 3\u00d73 convolution layers with the stride of 2 while adopting the single-layer version for embedding stems in other three stages. We select GELU (Hendrycks & Gimpel, 2016) as the common activation function and only use SiLU in the Moga module as Eq. (8)."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "To impartially evaluate and compare MogaNet with the leading network architectures, we conduct extensive experiments across various popular vision tasks, including image classification, object detection, instance and semantic segmentation, 2D and 3D pose estimation, and video prediction. The experiments are implemented with PyTorch and run on NVIDIA A100 GPUs."
        },
        {
            "heading": "5.1 IMAGENET CLASSIFICATION",
            "text": "Settings. For classification experiments on ImageNet (Deng et al., 2009), we train our MogaNet following the standard procedure (Touvron et al., 2021a; Liu et al., 2021) on ImageNet-1K (IN-1K) for a fair comparison, training 300 epochs with AdamW (Loshchilov & Hutter, 2019) optimizer, a basic learning rate of 1\u00d7 10\u22123, and a cosine scheduler (Loshchilov & Hutter, 2016). To explore the large model capacities, we pre-trained MogaNet-XL on ImageNet-21K (IN-21K) for 90 epochs and then fine-tuned 30 epochs on IN-1K following (Liu et al., 2022b). Appendix A.2 and D.1 provide implementation details and more results. We compare three classical architectures: Pure ConvNets (C), Transformers (T), and Hybrid model (H) with both self-attention and convolution operations.\nResults. With regard to the lightweight models, Table 2 shows that MogaNet-XT/T significantly outperforms existing lightweight architectures with a more efficient usage of parameters and FLOPs. MogaNet-T achieves 79.0% top-1 accuracy, which improves models with \u223c5M parameters by at least 1.1 at 2242 resolutions. Using 2562 resolutions, MogaNet-T outperforms the current SOTA ParC-Net-S by 1.0 while achieving 80.0% top-1 accuracy with the refined settings. Even with only 3M parameters, MogaNet-XT still surpasses models with around 4M parameters, e.g., +4.6 over T2T-ViT-7. Particularly, MogaNet-T\u00a7 achieves 80.0% top-1 accuracy using 2562 resolutions and the refined training settings (detailed in Appendix C.5). As for scaling up models in Table 3, MogaNet shows superior or comparable performances to SOTA architectures with similar parameters and computational costs. For example, MogaNet-S achieves 83.4% top-1 accuracy, outperforming Swin-T and ConvNeXt-T with a clear margin of 2.1 and 1.2. MogaNet-B/L also improves recently proposed ConvNets with fewer parameters, e.g., +0.3/0.4 and +0.5/0.7 points over HorNet-S/B and SLaK-S/B. When pre-trained on IN-21K, MogaNet-XL is boosted to 87.8% top-1 accuracy with 181M parameters, saving 169M compared to ConvNeXt-XL. Noticeably, MogaNet-XL can achieve 85.1% at 2242 resolutions without pre-training and improves ConvNeXt-L by 0.8, indicating MogaNets are easier to converge than existing models (also verified in Appendix D.1)."
        },
        {
            "heading": "5.2 DENSE PREDICTION TASKS",
            "text": "Object detection and segmentation on COCO. We evaluate MogaNet for object detection and instance segmentation tasks on COCO (Lin et al., 2014) with RetinaNet (Lin et al., 2017b), MaskRCNN (He et al., 2017), and Cascade Mask R-CNN (Cai & Vasconcelos, 2019) as detectors. Following the training and evaluation settings in (Liu et al., 2021; 2022b), we fine-tune the models by the AdamW optimizer for 1\u00d7 and 3\u00d7 training schedule on COCO train2017 and evaluate on COCO val2017, implemented on MMDetection (Chen et al., 2019) codebase. The box mAP (APb) and mask mAP (APm) are adopted as metrics. Refer Appendix A.3 and D.2 for detailed settings and full results. Table 4 shows that detectors with MogaNet variants significantly outperform previous backbones. It is worth noticing that Mask R-CNN with MogaNet-T achieves 42.6 APb, outperforming Swin-T by 0.4 with 48% and 27% fewer parameters and FLOPs. Using advanced training setting and IN-21K pre-trained weights, Cascade Mask R-CNN with MogaNet-XL achieves 56.2 APb, +1.4 and +2.3 over ConvNeXt-L and RepLKNet-31L.\nSemantic segmentation on ADE20K. We also evaluate MogaNet for semantic segmentation tasks on ADE20K (Zhou et al., 2018) with Semantic FPN (Kirillov et al., 2019) and UperNet (Xiao et al., 2018b) following (Liu et al., 2021; Yu et al., 2022), implemented on MMSegmentation (Contributors, 2020b) codebase. The performance is measured by single-scale mIoU. Initialized by IN-1K or IN-21K pre-trained weights, Semantic FPN and UperNet are fine-tuned for 80K and 160K iterations by the AdamW optimizer. See Appendix A.4 and D.3 for detailed settings and full results. In Table 5, Semantic FPN with MogaNet-S consistently outperforms Swin-T and Uniformer-S by 6.2 and 1.1 points; UperNet with MogaNet-S/B/L improves ConvNeXt-T/S/B by 2.5/1.4/1.8 points. Using higher resolutions and IN-21K pre-training, MogaNet-XL achieves 54.0 SS mIoU, surpassing ConvNeXt-L and RepLKNet-31L by 0.3 and 1.6.\nArchitecture Date Type Image Param. FLOPs Top-1 Size (M) (G) Acc (%) ResNet-18 CVPR\u20192016 C 2242 11.7 1.80 71.5 ShuffleNetV2 2\u00d7 ECCV\u20192018 C 2242 5.5 0.60 75.4 EfficientNet-B0 ICML\u20192019 C 2242 5.3 0.39 77.1 RegNetY-800MF CVPR\u20192020 C 2242 6.3 0.80 76.3 DeiT-T\u2020 ICML\u20192021 T 2242 5.7 1.08 74.1 PVT-T ICCV\u20192021 T 2242 13.2 1.60 75.1 T2T-ViT-7 ICCV\u20192021 T 2242 4.3 1.20 71.7 ViT-C NIPS\u20192021 T 2242 4.6 1.10 75.3 SReT-TDistill ECCV\u20192022 T 2242 4.8 1.10 77.6 PiT-Ti ICCV\u20192021 H 2242 4.9 0.70 74.6 LeViT-S ICCV\u20192021 H 2242 7.8 0.31 76.6 CoaT-Lite-T ICCV\u20192021 H 2242 5.7 1.60 77.5 Swin-1G ICCV\u20192021 H 2242 7.3 1.00 77.3 MobileViT-S ICLR\u20192022 H 2562 5.6 4.02 78.4 MobileFormer-294M CVPR\u20192022 H 2242 11.4 0.59 77.9 ConvNext-XT CVPR\u20192022 C 2242 7.4 0.60 77.5 VAN-B0 CVMJ\u20192023 C 2242 4.1 0.88 75.4 ParC-Net-S ECCV\u20192022 C 2562 5.0 3.48 78.6 MogaNet-XT Ours C 2562 3.0 1.04 77.2 MogaNet-T Ours C 2242 5.2 1.10 79.0 MogaNet-T\u00a7 Ours C 2562 5.2 1.44 80.0\nTable 2: IN-1K classification with lightweight models. \u00a7 denotes the refined training scheme.\nArchitecture Date Type Image Param. FLOPs Top-1\nSize (M) (G) Acc (%) Deit-S ICML\u20192021 T 2242 22 4.6 79.8 Swin-T ICCV\u20192021 T 2242 28 4.5 81.3 CSWin-T CVPR\u20192022 T 2242 23 4.3 82.8 LITV2-S NIPS\u20192022 T 2242 28 3.7 82.0 CoaT-S ICCV\u20192021 H 2242 22 12.6 82.1 CoAtNet-0 NIPS\u20192021 H 2242 25 4.2 82.7 UniFormer-S ICLR\u20192022 H 2242 22 3.6 82.9 RegNetY-4GF\u2020 CVPR\u20192020 C 2242 21 4.0 81.5 ConvNeXt-T CVPR\u20192022 C 2242 29 4.5 82.1 SLaK-T ICLR\u20192023 C 2242 30 5.0 82.5 HorNet-T7\u00d77 NIPS\u20192022 C 2242 22 4.0 82.8 MogaNet-S Ours C 2242 25 5.0 83.4 Swin-S ICCV\u20192021 T 2242 50 8.7 83.0 Focal-S NIPS\u20192021 T 2242 51 9.1 83.6 CSWin-S CVPR\u20192022 T 2242 35 6.9 83.6 LITV2-M NIPS\u20192022 T 2242 49 7.5 83.3 CoaT-M ICCV\u20192021 H 2242 45 9.8 83.6 CoAtNet-1 NIPS\u20192021 H 2242 42 8.4 83.3 UniFormer-B ICLR\u20192022 H 2242 50 8.3 83.9 FAN-B-Hybrid ICML\u20192022 H 2242 50 11.3 83.9 EfficientNet-B6 ICML\u20192019 C 5282 43 19.0 84.0 RegNetY-8GF\u2020 CVPR\u20192020 C 2242 39 8.1 82.2 ConvNeXt-S CVPR\u20192022 C 2242 50 8.7 83.1 FocalNet-S (LRF) NIPS\u20192022 C 2242 50 8.7 83.5 HorNet-S7\u00d77 NIPS\u20192022 C 2242 50 8.8 84.0 SLaK-S ICLR\u20192023 C 2242 55 9.8 83.8 MogaNet-B Ours C 2242 44 9.9 84.3 DeiT-B ICML\u20192021 T 2242 86 17.5 81.8 Swin-B ICCV\u20192021 T 2242 89 15.4 83.5 Focal-B NIPS\u20192021 T 2242 90 16.4 84.0 CSWin-B CVPR\u20192022 T 2242 78 15.0 84.2 DeiT III-B ECCV\u20192022 T 2242 87 18.0 83.8 BoTNet-T7 CVPR\u20192021 H 2562 79 19.3 84.2 CoAtNet-2 NIPS\u20192021 H 2242 75 15.7 84.1 FAN-B-Hybrid ICML\u20192022 H 2242 77 16.9 84.3 RegNetY-16GF CVPR\u20192020 C 2242 84 16.0 82.9 ConvNeXt-B CVPR\u20192022 C 2242 89 15.4 83.8 RepLKNet-31B CVPR\u20192022 C 2242 79 15.3 83.5 FocalNet-B (LRF) NIPS\u20192022 C 2242 89 15.4 83.9 HorNet-B7\u00d77 NIPS\u20192022 C 2242 87 15.6 84.3 SLaK-B ICLR\u20192023 C 2242 95 17.1 84.0 MogaNet-L Ours C 2242 83 15.9 84.7 Swin-L\u2021 ICCV\u20192021 T 3842 197 104 87.3 DeiT III-L\u2021 ECCV\u20192022 T 3842 304 191 87.7 CoAtNet-3\u2021 NIPS\u20192021 H 3842 168 107 87.6 RepLKNet-31L\u2021 CVPR\u20192022 C 3842 172 96 86.6 ConvNeXt-L CVPR\u20192022 C 2242 198 34.4 84.3 ConvNeXt-L\u2021 CVPR\u20192022 C 3842 198 101 87.5 ConvNeXt-XL\u2021 CVPR\u20192022 C 3842 350 179 87.8 HorNet-L\u2021 NIPS\u20192022 C 3842 202 102 87.7 MogaNet-XL Ours C 2242 181 34.5 85.1 MogaNet-XL\u2021 Ours C 3842 181 102 87.8\nTable 3: IN-1K classification performance with scaling-up models. \u2021 denotes the model is pretrained on IN-21K and fine-tuned on IN-1K.\nArchitecture Data Method Param. FLOPs APb APm (M) (G) (%) (%) ResNet-101 CVPR\u20192016 RetinaNet 57 315 38.5 - PVT-S ICCV\u20192021 RetinaNet 34 226 40.4 - CMT-S CVPR\u20192022 RetinaNet 45 231 44.3 - MogaNet-S Ours RetinaNet 35 253 45.8 - RegNet-1.6G CVPR\u20192020 Mask R-CNN 29 204 38.9 35.7 PVT-T ICCV\u20192021 Mask R-CNN 33 208 36.7 35.1 MogaNet-T Ours Mask R-CNN 25 192 42.6 39.1 Swin-T ICCV\u20192021 Mask R-CNN 48 264 42.2 39.1 Uniformer-S ICLR\u20192022 Mask R-CNN 41 269 45.6 41.6 ConvNeXt-T CVPR\u20192022 Mask R-CNN 48 262 44.2 40.1 PVTV2-B2 CVMJ\u20192022 Mask R-CNN 45 309 45.3 41.2 LITV2-S NIPS\u20192022 Mask R-CNN 47 261 44.9 40.8 FocalNet-T NIPS\u20192022 Mask R-CNN 49 267 45.9 41.3 MogaNet-S Ours Mask R-CNN 45 272 46.7 42.2 Swin-S ICCV\u20192021 Mask R-CNN 69 354 44.8 40.9 Focal-S NIPS\u20192021 Mask R-CNN 71 401 47.4 42.8 ConvNeXt-S CVPR\u20192022 Mask R-CNN 70 348 45.4 41.8 HorNet-B7\u00d77 NIPS\u20192022 Mask R-CNN 68 322 47.4 42.3 MogaNet-B Ours Mask R-CNN 63 373 47.9 43.2 Swin-L\u2021 ICCV\u20192021 Cascade Mask 253 1382 53.9 46.7 ConvNeXt-L\u2021 CVPR\u20192022 Cascade Mask 255 1354 54.8 47.6 RepLKNet-31L\u2021 CVPR\u20192022 Cascade Mask 229 1321 53.9 46.5 HorNet-L\u2021 NIPS\u20192022 Cascade Mask 259 1399 56.0 48.6 MogaNet-XL\u2021 Ours Cascade Mask 238 1355 56.2 48.8\nTable 4: COCO object detection and instance segmentation with RetinaNet (1\u00d7), Mask RCNN (1\u00d7), and Cascade Mask R-CNN (multiscale 3\u00d7). \u2021 indicates IN-21K pre-trained models. The FLOPs are measured at 800\u00d7 1280.\nMethod Architecture Date Crop Param. FLOPs mIoUss size (M) (G) (%) PVT-S ICCV\u20192021 5122 28 161 39.8 Semantic Twins-S NIPS\u20192021 5122 28 162 44.3\nFPN Swin-T ICCV\u20192021 5122 32 182 41.5 (80K) Uniformer-S ICLR\u20192022 5122 25 247 46.6\nLITV2-S NIPS\u20192022 5122 31 179 44.3 VAN-B2 CVMJ\u20192023 5122 30 164 46.7 MogaNet-S Ours 5122 29 189 47.7 DeiT-S ICML\u20192021 5122 52 1099 44.0 Swin-T ICCV\u20192021 5122 60 945 46.1 ConvNeXt-T CVPR\u20192022 5122 60 939 46.7 UniFormer-S ICLR\u20192022 5122 52 1008 47.6 HorNet-T7\u00d77 NIPS\u20192022 5122 52 926 48.1 MogaNet-S Ours 5122 55 946 49.2 Swin-S ICCV\u20192021 5122 81 1038 48.1 ConvNeXt-S CVPR\u20192022 5122 82 1027 48.7\nUperNet SLaK-S ICLR\u20192023 5122 91 1028 49.4 (160K) MogaNet-B Ours 5122 74 1050 50.1\nSwin-B ICCV\u20192021 5122 121 1188 49.7 ConvNeXt-B CVPR\u20192022 5122 122 1170 49.1 RepLKNet-31B CVPR\u20192022 5122 112 1170 49.9 SLaK-B ICLR\u20192023 5122 135 1185 50.2 MogaNet-L Ours 5122 113 1176 50.9 Swin-L\u2021 ICCV\u20192021 6402 234 2468 52.1 ConvNeXt-L\u2021 CVPR\u20192022 6402 245 2458 53.7 RepLKNet-31L\u2021 CVPR\u20192022 6402 207 2404 52.4 MogaNet-XL\u2021 Ours 6402 214 2451 54.0\nTable 5: ADE20K semantic segmentation with semantic FPN (80K) and UperNet (160K). \u2021 indicates using IN-21K pre-trained models. The FLOPs are measured at 512\u00d72048 or 640\u00d72560.\nArchitecture Date Crop Param. FLOPs AP AP50 AP75 AR\nsize (M) (G) (%) (%) (%) (%) RSN-18 ECCV\u20192020 256\u00d7 192 9.1 2.3 70.4 88.7 77.9 77.1 MogaNet-T Ours 256\u00d7 192 8.1 2.2 73.2 90.1 81.0 78.8 HRNet-W32 CVPR\u20192019 256\u00d7 192 28.5 7.1 74.4 90.5 81.9 78.9 Swin-T ICCV\u20192021 256\u00d7 192 32.8 6.1 72.4 90.1 80.6 78.2 PVTV2-B2 CVML\u20192022 256\u00d7 192 29.1 4.3 73.7 90.5 81.2 79.1 Uniformer-S ICLR\u20192022 256\u00d7 192 25.2 4.7 74.0 90.3 82.2 79.5 ConvNeXt-T CVPR\u20192022 256\u00d7 192 33.1 5.5 73.2 90.0 80.9 78.8 MogaNet-S Ours 256\u00d7 192 29.0 6.0 74.9 90.7 82.8 80.1 Uniformer-S ICLR\u20192022 384\u00d7 288 25.2 11.1 75.9 90.6 83.4 81.4 ConvNeXt-T CVPR\u20192022 384\u00d7 288 33.1 33.1 75.3 90.4 82.1 80.5 MogaNet-S Ours 384\u00d7 288 29.0 13.5 76.4 91.0 83.3 81.4 HRNet-W48 CVPR\u20192019 384\u00d7 288 63.6 32.9 76.3 90.8 82.0 81.2 Swin-L ICCV\u20192021 384\u00d7 288 203.4 86.9 76.3 91.2 83.0 814 Uniformer-B ICLR\u20192022 384\u00d7 288 53.5 14.8 76.7 90.8 84.0 81.4 MogaNet-B Ours 384\u00d7 288 47.4 24.4 77.3 91.4 84.0 82.2\nTable 6: COCO 2D human pose estimation with Top-Down SimpleBaseline. The FLOPs are measured at 256\u00d7 192 or 384\u00d7 288.\n2D and 3D Human Pose Estimation. We evaluate MogaNet on 2D and 3D human pose estimation tasks. As for 2D key points estimation on COCO, we conduct evaluations with SimpleBaseline (Xiao et al., 2018a) following (Wang et al., 2021b; Li et al., 2022a), which fine-tunes the model for 210 epoch by Adam optimizer (Kingma & Ba, 2014). Table 6 shows that MogaNet variants yield at least 0.9 AP improvements for 256\u00d7192 input, e.g., +2.5 and +1.2 over Swin-T and PVTV2-B2 by MogaNet-S. Using 384\u00d7 288 input, MogaNet-B outperforms Swin-L and Uniformer-B by 1.0 and 0.6 AP with fewer parameters. As for 3D face/hand surface reconstruction tasks on Stirling/ESRC 3D (Feng et al., 2018) and FreiHAND (Zimmermann et al., 2019) datasets, we benchmark backbones with ExPose (Choutas et al., 2020), which fine-tunes the model for 100 epoch by Adam optimizer. 3DRMSE and Mean Per-Joint Position Error (PA-MPJPE) are the metrics. In Table 7, MogaNet-S shows the lowest errors compared to Transformers and ConvNets. We provide detailed implementations and results for 2D and 3D pose estimation tasks in Appendix D.4 and D.5.\nVideo Prediction. We further objectively evaluate MogaNet for unsupervised video prediction tasks with SimVP (Gao et al., 2022) on MMNIST (Srivastava et al., 2015), where the model predicts the successive 10 frames with the given 10 frames as the input. We train the model for 200 epochs from scratch by the Adam optimizer and are evaluated by MSE and Structural Similarity Index (SSIM). Table 7 shows that SimVP with MogaNet blocks improves the baseline by 6.58 MSE and outperforms ConvNeXt and Hor-\nNet by 1.37 and 4.07 MSE. Appendix A.7 and D.6 show more experiment settings and results."
        },
        {
            "heading": "5.3 ABLATION AND ANALYSIS",
            "text": "We first ablate the spatial aggregation module and the channel aggregation module CA(\u00b7) in Table 1 and Fig. 7 (left). Spatial modules include FD(\u00b7) and Moga(\u00b7), containing the gating branch and the context branch with multi-order DWConv layers Multi-DW(\u00b7). We found that all proposed modules yield improvements with favorable costs. Appendix C provides more ablation studies. Furthermore, Fig. 7 (right) empirically shows design modules can learn more middle-order interactions, and Fig. 8 visualizes class activation maps by Grad-CAM (Selvaraju et al., 2017) compared to existing models."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper introduces a new modern ConvNet architecture, named MogaNet, through the lens of multi-order game-theoretic interaction. Built upon the modern ConvNet framework, we present a compact Moga Block and channel aggregation module to force the network to emphasize the expressive but inherently overlooked interactions across spatial and channel perspectives. Extensive experiments verify the consistent superiority of MogaNet in terms of both performance and efficiency compared to popular ConvNets, ViTs, and hybrid architectures on various vision benchmarks."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "This work was supported by the National Key R&D Program of China (No. 2022ZD0115100), the National Natural Science Foundation of China Project (No. U21A20427), and Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University. This work was done when Zedong Wang and Zhiyuan Chen interned at Westlake University. We thank the AI Station of Westlake University for the support of GPUs. We also thank Mengzhao Chen, Zhangyang Gao, Jianzhu Guo, Fang Wu, and all anonymous reviewers for polishing the writing of the manuscript."
        },
        {
            "heading": "A.2 EXPERIMENTAL SETTINGS FOR IMAGENET",
            "text": "We conduct image classification experiments on ImageNet (Deng et al., 2009) datasets. All experiments are implemented on OpenMixup (Li et al., 2022b) and timm (Wightman et al., 2021) codebases running on 8 NVIDIA A100 GPUs. View more results in Appendix D.1.\nImageNet-1K. We perform regular ImageNet-1K training mostly following the training settings of DeiT (Touvron et al., 2021a) and RSB A2 (Wightman et al., 2021) in Table A2, which are widely\nConfiguration DeiT RSB MogaNet A2 XT T S B L XL Input resolution 2242 2242 2242 Epochs 300 300 300 Batch size 1024 2048 1024 Optimizer AdamW LAMB AdamW AdamW (\u03b21, \u03b22) 0.9, 0.999 - 0.9, 0.999 Learning rate 0.001 0.005 0.001 Learning rate decay Cosine Cosine Cosine Weight decay 0.05 0.02 0.03 0.04 0.05 0.05 0.05 0.05 Warmup epochs 5 5 5 Label smoothing 0.1 0.1 0.1 Stochastic Depth 3 3 0.05 0.1 0.1 0.2 0.3 0.4 Rand Augment 9/0.5 7/0.5 7/0.5 7/0.5 9/0.5 9/0.5 9/0.5 9/0.5 Repeated Augment 3 3 7 Mixup \u03b1 0.8 0.1 0.1 0.1 0.8 0.8 0.8 0.8 CutMix \u03b1 1.0 1.0 1.0 Erasing prob. 0.25 7 0.25 ColorJitter 7 7 7 7 0.4 0.4 0.4 0.4 Gradient Clipping 3 7 7 EMA decay 3 7 7 7 3 3 3 3 Test crop ratio 0.875 0.95 0.90\nTable A2: Hyper-parameters for ImageNet-1K training of DeiT, RSB A2, and MogaNet. We use a similar setting as RSB for XL and T versions of MogaNet and DeiT for the other versions.\nConfiguration IN-21K PT IN-1K FT S B L XL S B L XL Input resolution 2242 3842 Epochs 90 30 Batch size 1024 512 Optimizer AdamW AdamW AdamW (\u03b21, \u03b22) 0.9, 0.999 0.9, 0.999 Learning rate 1\u00d7 10\u22123 5\u00d7 10\u22125 Learning rate decay Cosine Cosine Weight decay 0.05 0.05 Warmup epochs 5 0 Label smoothing 0.2 0.1 0.1 0.2 0.2 Stochastic Depth 0 0.1 0.1 0.1 0.4 0.6 0.7 0.8 Rand Augment 9/0.5 9/0.5 Repeated Augment 7 7 Mixup \u03b1 0.8 7 CutMix \u03b1 1.0 7 Erasing prob. 0.25 0.25 ColorJitter 0.4 0.4 Gradient Clipping 7 7 EMA decay 7 3 Test crop ratio 0.90 1.0\nTable A3: Detailed training recipe for ImageNet-21K pre-training (IN-21K PT) and ImageNet-1K fine-tuning (IN-1K FT) in high resolutions for MogaNet.\nadopted for Transformer and ConvNet architectures. For all models, the default input image resolution is 2242 for training from scratch. We adopt 2562 resolutions for lightweight experiments according to MobileViT (Mehta & Rastegari, 2022). Taking training settings for the model with 25M or more parameters as the default, we train all MogaNet models for 300 epochs by AdamW (Loshchilov & Hutter, 2019) optimizer using a batch size of 1024, a basic learning rate of 1\u00d7 10\u22123, a weight decay of 0.05, and a Cosine learning rate scheduler (Loshchilov & Hutter, 2016) with 5 epochs of linear warmup (Devlin et al., 2018). As for augmentation and regularization techniques, we adopt most of the data augmentation and regularization strategies applied in DeiT training settings, including Random Resized Crop (RRC) and Horizontal flip (Szegedy et al., 2015), RandAugment (Cubuk et al., 2020), Mixup (Zhang et al., 2018), CutMix (Yun et al., 2019), random erasing (Zhong et al., 2020), ColorJitter (He et al., 2016), stochastic depth (Huang et al., 2016), and label smoothing (Szegedy et al., 2016). Similar to ConvNeXt (Liu et al., 2022b), we do not apply Repeated augmentation (Hoffer et al., 2020) and gradient clipping, which are designed for Transformers but do not enhance the performances of ConvNets while using Exponential Moving Average (EMA) (Polyak & Juditsky, 1992) with the decay rate of 0.9999 by default. We also remove additional augmentation strategies (Cubuk et al., 2019; Liu et al., 2022d; Li et al., 2021; Liu et al., 2022c), e.g., PCA lighting (Krizhevsky et al., 2012) and AutoAugment (Cubuk et al., 2019). Since lightweight architectures (3\u223c10M parameters) tend to get under-fitted with strong augmentations and regularization, we adjust the training configurations for MogaNet-XT/T following (Mehta & Rastegari, 2022; Chen et al., 2022; Zhang et al., 2022b), including employing the weight decay of 0.03 and 0.04, Mixup with \u03b1 of 0.1, and RandAugment of 7/0.5 for MogaNet-XT/T. Since EMA is proposed to stabilize the training process of large models, we also remove it for MogaNet-XT/T as a fair comparison. An increasing degree of stochastic depth path augmentation is employed for larger models. In evaluation, the top-1 accuracy using a single crop with a test crop ratio of 0.9 is reported as (Yuan et al., 2021b; Yu et al., 2022; Guo et al., 2023).\nImageNet-21K. Following ConvNeXt, we further provide the training recipe for ImageNet21K (Deng et al., 2009) pre-training and ImageNet-1K fine-tuning with high resolutions in Table A3. EMA is removed in pre-training, while CutMix and Mixup are removed for fine-tuning."
        },
        {
            "heading": "A.3 OBJECT DETECTION AND SEGMENTATION ON COCO",
            "text": "Following Swin (Liu et al., 2021) and PoolFormer (Yu et al., 2022), we evaluate objection detection and instance segmentation tasks on COCO (Lin et al., 2014) benchmark, which include 118K training images (train2017) and 5K validation images (val2017). We adopt RetinaNet (Lin et al., 2017b), Mask R-CNN (He et al., 2017), and Cascade Mask R-CNN (Cai & Vasconcelos, 2019) as the standard detectors and use ImageNet-1K pre-trained weights as the initialization of the backbones. As\nfor RetinaNet and Mask R-CNN, we employ AdamW (Loshchilov & Hutter, 2019) optimizer for training 1\u00d7 scheduler (12 epochs) with a basic learning rate of 1 \u00d7 10\u22124 and a batch size of 16. As for Cascade Mask R-CNN, the 3\u00d7 training scheduler and multi-scale training resolutions (MS) are adopted. The pre-trained weights on ImageNet-1K and ImageNet-21K are used accordingly to initialize backbones. The shorter side of training images is resized to 800 pixels, and the longer side is resized to not more than 1333 pixels. We calculate the FLOPs of compared models at 800\u00d7 1280 resolutions. Experiments of COCO detection are implemented on MMDetection (Chen et al., 2019) codebase and run on 8 NVIDIA A100 GPUs. View detailed results in Appendix D.2."
        },
        {
            "heading": "A.4 SEMANTIC SEGMENTATION ON ADE20K",
            "text": "We evaluate semantic segmentation on ADE20K (Zhou et al., 2018) benchmark, which contains 20K training images and 2K validation images, covering 150 fine-grained semantic categories. We first adopt Semantic FPN (Kirillov et al., 2019) following PoolFormer (Yu et al., 2022) and Uniformer (Li et al., 2022a), which train models for 80K iterations by AdamW (Loshchilov & Hutter, 2019) optimizer with a basic learning rate of 2 \u00d7 10\u22124, a batch size of 16, and a poly learning rate scheduler. Then, we utilize UperNet (Xiao et al., 2018b) following Swin (Liu et al., 2021), which employs AdamW optimizer using a basic learning rate of 6\u00d7 10\u22125, a weight decay of 0.01, a poly scheduler with a linear warmup of 1,500 iterations. We use ImageNet-1K and ImageNet-21K pre-trained weights to initialize the backbones accordingly. The training images are resized to 5122 resolutions, and the shorter side of testing images is resized to 512 pixels. We calculate the FLOPs of models at 800 \u00d7 2048 resolutions. Experiments of ADE20K segmentation are implemented on MMSegmentation (Contributors, 2020b) codebase and run on 8 NVIDIA A100 GPUs. View full comparison results in Appendix D.3.\nA.5 2D HUMAN POSE ESTIMATION ON COCO\nWe evaluate 2D human keypoints estimation tasks on COCO (Lin et al., 2014) benchmark based on Top-Down SimpleBaseline (Xiao et al., 2018a) (adding a Top-Down estimation head after the backbone) following PVT (Wang et al., 2021b) and UniFormer (Li et al., 2022a). We fine-tune all models for 210 epochs with Adam optimizer (Kingma & Ba, 2014) using a basic learning rate selected in {1\u00d710\u22123, 5\u00d710\u22124}, a multi-step learning rate scheduler decay at 170 and 200 epochs. ImageNet1K pre-trained weights are used as the initialization of the backbones. The training and testing images are resized to 256\u00d7192 or 384\u00d7288 resolutions, and the FLOPs of models are calculated at both resolutions. COCO pose estimation experiments are implemented on MMPose (Contributors, 2020a) codebase and run on 8 NVIDIA A100 GPUs. View full experiment results in Appendix D.4.\nA.6 3D HUMAN POSE ESTIMATION\nWe evaluate MogaNet and popular architectures with 3D human pose estimation tasks with a single monocular image based on ExPose (Choutas et al., 2020). We first benchmark widely-used ConvNets with the 3D face mesh surface estimation task based on ExPose. All models are trained for 100 epochs on Flickr-Faces-HQ Dataset (FFHQ) (Karras et al., 2019) and tested on Stirling/ESRC 3D dataset (Feng et al., 2018), which consists of facial RGB images with ground-truth 3D face scans. 3D Root Mean Square Error (3DRMSE) measures errors between the predicted and groundtruth face scans. Following ExPose, the Adam optimizer is employed with a batch size of 256, a basic learning rate selected in {2\u00d710\u22124, 1\u00d710\u22124}, a multi-step learning rate scheduler decay at 60 and 100 epochs. ImageNet-1K pre-trained weights are adopted as the backbone initialization. The training and testing images are resized to 256\u00d7 256 resolutions. Then, we evaluate ConvNets with the hand 3D pose estimation tasks. FreiHAND dataset (Zimmermann et al., 2019), which contains multi-view RGB hand images, 3D MANO hand pose, and shape annotations, is adopted for training and testing. Mean Per-Joint Position Error (PA-MPJPE) is used to evaluate 3D skeletons. Notice that a \u201cPA\u201d prefix denotes that the metric measures error after solving rotation, scaling, and translation transforms using Procrustes Alignment. Refer to ExPose for more implementation details. All models use the same training settings as the 3D face task, and the training and testing resolutions are 224\u00d7 224. Experiments of 3D pose estimation are implemented on MMHuman3D (Contributors, 2021) codebase and run on 4 NVIDIA A100 GPUs. View full results in Appendix D.5.\nA.7 VIDEO PREDICTION ON MOVING MNIST\nWe evaluate various Metaformer architectures (Yu et al., 2022) and MogaNet with video prediction tasks on Moving MNIST (MMNIST) (Lin et al., 2014) based on SimVP (Gao et al., 2022). Notice that the hidden translator of SimVP is a 2D network module to learn spatio-temporal representation, which any 2D architecture can replace. Therefore, we can benchmark various architectures based on the SimVP framework. In MMNIST (Srivastava et al., 2015), each video is randomly generated with 20 frames containing two digits in 64 \u00d7 64 resolutions, and the model takes 10 frames as the input to predict the next 10 frames. Video predictions are evaluated by Mean Square Error (MSE), Mean Absolute Error (MAE), and Structural Similarity Index (SSIM). All models are trained on MMNIST from scratch for 200 or 2000 epochs with Adam optimizer, a batch size of 16, a OneCycle learning rate scheduler, an initial learning rate selected in {1\u00d710\u22122, 5\u00d710\u22123, 1\u00d710\u22123, 5\u00d710\u22124}. Experiments of video prediction are implemented on OpenSTL1 codebase (Tan et al., 2023) and run on a single NVIDIA Tesla V100 GPU. View full benchmark results in Appendix D.6."
        },
        {
            "heading": "B EMPIRICAL EXPERIMENT RESULTS",
            "text": ""
        },
        {
            "heading": "B.1 REPRESENTATION BOTTLENECK OF DNNS FROM THE VIEW OF MULTI-ORDER INTERACTION",
            "text": "Multi-order game-theoretic interaction. In Sec. 3, we interpret the learned representation of DNNs through the lens of multi-order game-theoretic interaction (Zhang et al., 2020; Deng et al., 2022), which disentangles inter-variable communication effects in a DNN into diverse gametheoretic components of different interaction orders. The order here denotes the scale of context involved in the whole computation process of game-theoretic interaction.\nFor computer vision, the m-th order interaction I(m)(i, j) measures the average game-theoretic interaction effects between image patches i and j on allm image patch contexts. Take face recognition as an example, we can consider patches i and j as two eyes on this face. Besides, we regard other m visible image patches included on the face. The interaction effect and contribution between the eye\u2019s patches i and j toward the task depend on such m visible patches as the context, which is measured as the aforementioned I(m)(i, j). If I(m)(i, j) > 0 , patches i and j show a positive effect under m context. Accordingly, if I(m)(i, j) < 0, we consideri and j have a negative effect under m context. More importantly, interactions of low-order mainly reflect widely-shared local texture and common visual concepts. The middle-order interactions are primarily responsible for encoding discriminative high-level representations. However, the high-order ones are inclined to let DNNs memorize the pattern of rare outliers and large-scale shape with intensive global interactions, which can presumably over-fit our deep models (Deng et al., 2022; Cheng et al., 2021). Consequently, the occurrence of excessively low- or high-order game-theoretic interaction in a deep architecture may therefore be undesirable.\nFormally, given an input image x with a set of n patches N = {1, . . . , n} (e.g., an image with n pixels in total), the multi-order interaction I(m)(i, j) can be calculated as:\nI(m)(i, j) = ES\u2286N\\{i,j},|S|=m[\u2206f(i, j, S)], (11)\nwhere \u2206f(i, j, S) = f(S \u222a {i, j}) \u2212 f(S \u222a {i}) \u2212 f(S \u222a {j}) + f(S). f(S) indicates the score of output with patches in N \\ S kept unchanged but replaced with the baseline value (Ancona et al., 2019a), For example, a low-order interaction (e.g., m = 0.05n) means the relatively simple collaboration between variables i, j under a small range of context, while a high-order interaction (e.g., m = 0.95n) corresponds to the complex collaboration under a large range of context. Then, we can measure the overall interaction complexity of deep neural networks (DNNs) by the relative interaction strength J (m) of the encoded m-th order interaction:\nJ (m) = Ex\u2208\u2126Ei,j |I(m)(i, j|x)|\nEm\u2032Ex\u2208\u2126Ei,j |I(m \u2032 )(i, j|x)|\n, (12)\nwhere \u2126 is the set of all samples and 0 \u2264 m \u2265 n \u2212 2. Note that J (m) is the average interaction strength over all possible patch pairs of the input samples and indicates the distribution (area under\n1https://github.com/chengtan9907/OpenSTL\n0 0.1 0.3 0.5 0.7 0.9 1.0 Order / n\n0\n1\n2\n3\nIn ter\nac tio\nn St\nre ng\nth\nInteraction Strength of Order ConvNeXt (Tiny), 29M ResNet50, 25M SLaK (Tiny), 30M MogaNet (Small), 25M\n(a)\n0 0.1 0.3 0.5 0.7 0.9 1.0 Order / n\n0\n1\n2\n3\nIn ter\nac tio\nn St\nre ng\nth\nInteraction Strength of Order FocalNet (Tiny), 29M VAN (B2), 27M MogaNet (Small), 25M\n(b)\n0 0.1 0.3 0.5 0.7 0.9 1.0 Order / n\n0\n1\n2\n3\n4\nIn ter\nac tio\nn St\nre ng\nth\nInteraction Strength of Order DeiT (Small), 22M Swin (Tiny), 28M MogaNet (Small), 25M\n(c) Figure A2: Distributions of the interaction strength J (m) for (a) ConvNets with different convolution kernel sizes, (b) ConvNets with gating aggregations, and (c) Transformers on ImageNet1K with 2242 resolutions. Middle-order strengths mean the middle-complex interaction, where a medium number of patches (e.g., 0.2\u223c0.8n) participate.\ncurve sums up to one) of the order of interactions of DNNs. In Fig. A2, we calculate the interaction strength J (m) with Eq. 12 for the models trained on ImageNet-1K using the official implementation2 provided by (Deng et al., 2022). Specially, we use the image of 224 \u00d7 224 resolution as the input and calculate J (m) on 14 \u00d7 14 grids, i.e., n = 14 \u00d7 14. And we set the model output as f(xS) = log P (y\u0302=y|xS)1\u2212P (y\u0302=y|xS) given the masked sample xS , where y denotes the ground-truth label and P (y\u0302 = y|xS) denotes the probability of classifying the masked sample xS to the true category. Fig. A2a and Fig. A2b compare existing ConvNets with large kernels or gating designs and demonstrate that MogaNet can model middle-order interactions better to learn more informative representations.\nRelationship of explaining works of ViTs. Since the thriving of ViTs in a wide range of computer vision tasks, recent studies mainly investigate the why ViTs work from two directions: (a) Evaluation of robustness against noises finds that self-attentions (Naseer et al., 2021; Park & Kim, 2022; Zhou et al., 2021; Li et al., 2023a) or gating mechanisms (Zhou et al., 2022) in ViTs are more robust than classical convolutional operations (Simonyan & Zisserman, 2014; Szegedy et al., 2016). For example, ViTs can still recognize the target object with large occlusion ratios (e.g., only 10\u223c20% visible patches) or corruption noises. This phenomenon might stem from the inherent redundancy of images and the competition property of self-attention mechanisms (Wang et al., 2021a; Wu et al., 2022b). Several recently proposed works (Yin et al., 2022; Graham et al., 2021) show that ViTs can work with some essential tokens (e.g., 5\u223c50%) that are selected according to the complexity of input images by dynamic sampling strategies, which also utilize the feature selection properties of selfattentions. From the perspective of multi-order interactions, convolutions with local inductive bias (using small kernel sizes) prefer low-order interactions, while self-attentions without any inductive bias tend to learn low-order and high-order interactions. (b) Evaluation of out-of-distribution samples reveals that both self-attention mechanisms and depth-wise convolution (DWConv) with large kernel designs share similar shape-bias tendency as human vision (Tuli et al., 2021; Geirhos et al., 2021; Ding et al., 2022b), while canonical ConvNets (using convolutions with small kernel sizes) exhibit strong bias on local texture (Geirhos et al., 2019; Hermann et al., 2020). Current works (Ding et al., 2022b) attribute shape or texture-bias tendency to the receptive field of self-attention or convolution operations, i.e., an operation with the larger receptive field or more long-range dependency is more likely to be shape-bias. However, there are still gaps between shape-bias operations and human vision. Human brains (Treisman & Gelade, 1980; Deng et al., 2022) attain visual patterns and clues and conduct middle-complexity interactions to recognize objects, while a self-attention or convolution operation can only encode global or local features to conduct high or low-complexity interactions. As the existing design of DNNs only stacks regionality perception or context aggregation operations in a cascaded way, it is inevitable to encounter the representation bottleneck.\nB.2 VISUALIZATION OF CAM\nWe further visualize more examples of Grad-CAM (Selvaraju et al., 2017) activation maps of MogaNet-S in comparison to Transformers, including DeiT-S (Touvron et al., 2021a), T2T-ViT-\n2https://github.com/Nebularaid2000/bottleneck\nS (Yuan et al., 2021b), Twins-S (Chu et al., 2021), and Swin (Liu et al., 2021), and ConvNets, including ResNet-50 (He et al., 2016) and ConvNeXt-T (Liu et al., 2022b), on ImageNet-1K in Fig. A4. Due to the self-attention mechanism, the pure Transformers architectures (DeiT-S and T2T-ViT-S) show more refined activation maps than ConvNets, but they also activate some irrelevant parts. Combined with the design of local windows, local attention architectures (Twins-S and Swin-T) can locate the full semantic objects. Results of previous ConvNets can roughly localize the semantic target but might contain some background regions. The activation parts of our proposed MogaNet-S are more similar to local attention architectures than previous ConvNets, which are more gathered on the semantic objects."
        },
        {
            "heading": "C MORE ABLATION AND ANALYSIS RESULTS",
            "text": "In addition to Sec. 5.3, we further conduct more ablation and analysis of our proposed MogaNet on ImageNet-1K. We adopt the same experimental settings as Sec. 1."
        },
        {
            "heading": "C.1 ABLATION OF ACTIVATION FUNCTIONS",
            "text": "We conduct the ablation of activation functions used in the proposed multi-order gated aggregation module on ImageNet-1K. Table A4 shows that using SiLU (Elfwing et al., 2018) activation for both branches achieves the best performance. Similar results were also found in Transformers, e.g., GLU variants with SiLU or GELU (Hendrycks & Gimpel, 2016) yield better performances than using Sigmoid or Tanh activation functions (Shazeer, 2020; Hua et al., 2022). We assume that SiLU is the most suitable activation because it owns both the property of Sigmoid (gating effects) and GELU (training friendly), which is defined as x \u00b7 Sigmoid(x).\nTop-1 Context branch Acc (%) None GELU SiLU None 76.3 76.7 76.7 Gating Sigmoid 76.8 77.0 76.9 branch GELU 76.7 76.8 77.0\nSiLU 76.9 77.1 77.2\nTable A4: Ablation of various activation functions for the gating and context branches in the proposed Moga(\u00b7) module, which SiLU achieves the best performance in two branches.\nModules Top-1 Params. FLOPs Acc (%) (M) (G) Baseline (+Gating branch) 77.2 5.09 1.070 DW7\u00d77 77.4 5.14 1.094 DW5\u00d75,d=1 + DW7\u00d77,d=3 77.5 5.15 1.112 DW5\u00d75,d=1 + DW5\u00d75,d=2 + DW7\u00d77,d=3 77.5 5.17 1.185 +Multi-order, Cl : Cm : Ch = 1 : 0 : 3 77.5 5.17 1.099 +Multi-order, Cl : Cm : Ch = 0 : 1 : 1 77.6 5.17 1.103 +Multi-order, Cl : Cm : Ch = 1 : 6 : 9 77.7 5.17 1.104 +Multi-order, Cl : Cm : Ch = 1 : 3 : 4 77.8 5.17 1.102\nTable A5: Ablation of multi-order DWConv layers in the proposed Moga(\u00b7). The baseline adopts the MogaNet framework using the non-linear projection, DW5\u00d75, and the SiLU gating branch as SMixer(\u00b7) and using the vanilla MLP as CMixer(\u00b7)."
        },
        {
            "heading": "C.2 ABLATION OF MULTI-ORDER DWCONV LAYERS",
            "text": "In addition to Sec. 4.2 and Sec. 5.3, we also analyze the multi-order depth-wise convolution (DWConv) layers as the static regionality perception in the multi-order aggregation module Moga(\u00b7) on ImageNet-1K. As shown in Table A5, we analyze the channel configuration of three parallel dilated DWConv layers: DW5\u00d75,d=1, DW5\u00d75,d=2, and DW7\u00d77,d=3 with the channels of Cl, Cm, Ch. we first compare the performance of serial DWConv layers (e.g., DW5\u00d75,d=1+DW7\u00d77,d=3) and parallel DWConv layers. We find that the parallel design can achieve the same performance with fewer computational overloads because the DWConv kernel is equally applied to all channels. When we adopt three DWConv layers, the proposed parallel design reduces Cl + Ch and Cl + Cm times computations of DW5\u00d75,d=2 and DW5\u00d75,d=2 in comparison to the serial stack of these DWConv layers. Then, we empirically explore the optimal configuration of the three channels. We find that Cl : Cm : Ch = 1: 3: 4 yields the best performance, which well balances the small, medium, and large DWConv kernels to learn low, middle, and high-order contextual representations. We calculate and discuss the FLOPs of the proposed three DWConv layers in the next subsection to verify the efficiency. Similar conclusions are also found in relevant designs (Pan et al., 2022a; Si et al., 2022; Rao et al., 2022), where global context aggregations take the majority (e.g., 12 \u223c 3 4 channels or context components). We also verify the parallel design with the optimal configuration based\non MogaNet-S/B. Therefore, we can conclude that our proposed multi-order DWConv layers can efficiently learn multi-order contextual information for the context branch of Moga(\u00b7)."
        },
        {
            "heading": "C.3 FLOPS AND THROUGHPUTS OF MOGANET",
            "text": "FLOPs of Multi-order Gated Aggregation Module We divide the computation of the proposed multi-order gated aggregation module into two parts of convolution operations and calculate the FLOPs for each part.\n\u2022 Conv1\u00d71. The FLOPs of 1\u00d71 convolution operation \u03c6gate , \u03c6context and \u03c6out can be derived as:\nFLOPs(\u03c6gate) = 2HWC 2,\nFLOPs(\u03c6context) = 2HWC 2,\nFLOPs(\u03c6out) = 2HWC 2.\n(13)\n\u2022 Depth-wise convolution. We consider the depth-wise convolution (DW) with dilation ratio d. The DWConv is performed for the input X , where X \u2208 RHW\u00d7Cin . Therefore, the FLOPs for all DW in Moga module are:\nFLOPs(DW5\u00d75,d=1) = 2HWCinK 2 5\u00d75,\nFLOPs(DW5\u00d75,d=2) = 3\n4 HWCinK\n2 5\u00d75,\nFLOPs(DW7\u00d77,d=3) = HWCinK 2 7\u00d77.\n(14)\nOverall, the total FLOPs of our Moga module can be derived as follows:\nFLOPs(Moga) = 2HWCin\n[ 11\n8 K25\u00d75 +\n1 2 K27\u00d77 + 3Cin ] = HWCin [ 471\n4 + 6Cin\n] .\n(15)\n0 250 500 750 1000 1250 1500 Throughput (Images/s)\n76.0\n78.0\n80.0\n82.0\n84.0\n86.0\nTo p\n1 Ac\ncu ra\ncy (%\n)\nAccuracy vs. Throughput DeiT (ViT) Swin ConvNeXt MogaNet (Ours)\nFigure A3: Accuracy-throughput diagram of models on ImageNet-1K measured on an NVIDIA V100 GPU.\nThroughput of MogaNet We further analyze throughputs of MogaNet variants on ImageNet-1K. As shown in Fig. A3, MogaNet has similar throughputs as Swin Transformer while producing better performances than Swin and ConvNet. Since we add channel splitting and GAP operations in MogaNet, the throughput of ConvNeXt exceeds MogaNet to some extent."
        },
        {
            "heading": "C.4 ABLATION",
            "text": ""
        },
        {
            "heading": "OF NORMALIZATION LAYERS",
            "text": "For most ConvNets, BatchNorm (Ioffe & Szegedy, 2015b) (BN) is considered an essential component to improve the convergence speed and prevent overfitting. However, BN might cause some instability (Wu & Johnson, 2021) or harm the final performance of models (Brock et al., 2021a;b). Some recently proposed ConvNets (Liu et al., 2022b; Guo et al., 2023) replace BN by LayerNorm (Ba et al., 2016) (LN), which has been widely used in Transformers (Dosovitskiy et al., 2021) and Metaformer architectures (Yu et al., 2022), achieving relatively good performances in various scenarios. Here, we conduct an ablation of normalization (Norm) layers in MogaNet on ImageNet-1K, as shown in Table A6. As discussed in ConvNeXt (Liu et al., 2022b), the Norm layers used in each block (within) and after each stage (after) have different effects. Thus we study them separately. Table A6 shows that using BN in both places yields better performance than using LN (after) and BN (within), except MogaNet-T with 2242 resolutions, while using LN in both places performs the worst. Consequently,\nTu rtl e Bl ue J ay\nDeiT-S T2T-ViT-S Twins-S Swin-T ResNet-50 ConvNeXt-T MogaNet-SInput\nPa nd a La dy bu g\nFigure A4: Visualization of Grad-CAM activation maps of the models trained on ImageNet-1K.\nwe use BN as the default Norm layers in our proposed MogaNet for two reasons: (i) With pure convolution operators, the rule of combining convolution operations with BN within each stage is still useful for modern ConvNets. (ii) Although using LN after each stage might help stabilize the training process of Transformers and hybrid models and might sometimes bring good performance for ConvNets, adopting BN after each stage in pure convolution models still yields better performance. Moreover, we replace BN with precise BN (Wu & Johnson, 2021) (pBN), which is an optimal alternative normalization strategy to BN. We find slight performance improvements (around 0.1%), especially when MogaNet-S/B adopts the EMA strategy (by default), indicating that we can further improve MogaNet with advanced BN. As discussed in ConvNeXt, EMA might severely hurt the performances of models with BN. This phenomenon might be caused by the unstable and inaccurate BN statistics estimated by EMA in the vanilla BN with large models, which will deteriorate when using another EMA of model parameters. We solve this dilemma by exponentially increasing the EMA decay from 0.9 to 0.9999 during training as momentum-based contrastive learning methods (Caron et al., 2021; Bao et al., 2022), e.g., BYOL (Grill et al., 2020). It can also be tackled by advanced BN variants (Hoffer et al., 2017; Wu & Johnson, 2021).\nNorm (after) Input LN LN BN pBN Norm (within) size LN BN BN pBN MogaNet-T 2242 78.4 79.1 79.0 79.1 MogaNet-T 2562 78.8 79.4 79.6 79.6 MogaNet-S 2242 82.5 83.2 83.3 83.3 MogaNet-S (EMA) 2242 82.7 83.2 83.3 83.4 MogaNet-B 2242 83.4 83.9 84.1 84.2 MogaNet-B (EMA) 2242 83.7 83.8 84.3 84.4\nTable A6: Ablation of normalization layers in MogaNet."
        },
        {
            "heading": "C.5 REFINED TRAINING SETTINGS FOR LIGHTWEIGHT MODELS",
            "text": "To explore the full power of lightweight models of our MogaNet, we refined the basic training settings for MogaNet-XT/T according to RSB A2 (Wightman et al., 2021) and DeiT-III (Touvron et al., 2022). Compared to the default setting as provided in Table A2, we only adjust the learning rate and the augmentation strategies for faster convergence while keeping other settings unchanged. As shown in Table A7, MogaNet-XT/T gain +0.4\u223c0.6% when use the large learning rate of 2\u00d710\u22123 and 3-Augment (Touvron et al., 2022) without complex designs. Based on the advanced setting, MogaNet with 2242 input resolutions yields significant performance improvements against previous methods, e.g., MogaNet-T gains +3.5% over DeiT-T (Touvron et al., 2021a) and +1.2% over ParcNet-S (Zhang et al., 2022b). Especially, MogaNet-T with 2562 resolutions achieves top-1 accuracy of 80.0%, outperforming DeiT-S of 79.8% reported in the original paper, while MogaNet-XT with\nArchitecture Input Learning Warmup Rand 3-Augment EMA Top-1 size rate epochs Augment Acc (%) DeiT-T 2242 1\u00d7 10\u22123 5 9/0.5 7 3 72.2 DeiT-T 2242 2\u00d7 10\u22123 20 7 3 7 75.9 ParC-Net-S 2562 1\u00d7 10\u22123 5 9/0.5 7 3 78.6 ParC-Net-S 2562 2\u00d7 10\u22123 20 7 3 7 78.8 MogaNet-XT 2242 1\u00d7 10\u22123 5 7/0.5 7 7 76.5 MogaNet-XT 2242 2\u00d7 10\u22123 20 7 3 7 77.1 MogaNet-XT 2562 1\u00d7 10\u22123 5 7/0.5 7 7 77.2 MogaNet-XT 2562 2\u00d7 10\u22123 20 7 3 7 77.6 MogaNet-T 2242 1\u00d7 10\u22123 5 7/0.5 7 7 79.0 MogaNet-T 2242 2\u00d7 10\u22123 20 7 3 7 79.4 MogaNet-T 2562 1\u00d7 10\u22123 5 7/0.5 7 7 79.6 MogaNet-T 2562 2\u00d7 10\u22123 20 7 3 7 80.0\nTable A7: Advanced training recipes for Lightweight models of MogaNet on ImageNet-1K.\nArchitecture Type #P. FLOPs RetinaNet 1\u00d7 (M) (G) AP AP50 AP75 APS APM APL RegNet-800M C 17 168 35.6 54.7 37.7 19.7 390 47.8 PVTV2-B0 T 13 160 37.1 57.2 39.2 23.4 40.4 49.2 MogaNet-XT C 12 167 39.7 60.0 42.4 23.8 43.6 51.7 ResNet-18 C 21 189 31.8 49.6 33.6 16.3 34.3 43.2 RegNet-1.6G C 20 185 37.4 56.8 39.8 22.4 41.1 49.2 RegNet-3.2G C 26 218 39.0 58.4 41.9 22.6 43.5 50.8 PVT-T T 23 183 36.7 56.9 38.9 22.6 38.8 50.0 PoolFormer-S12 T 22 207 36.2 56.2 38.2 20.8 39.1 48.0 PVTV2-B1 T 24 187 41.1 61.4 43.8 26.0 44.6 54.6 MogaNet-T C 14 173 41.4 61.5 44.4 25.1 45.7 53.6 ResNet-50 C 37 239 36.3 55.3 38.6 19.3 40.0 48.8 Swin-T T 38 245 41.8 62.6 44.7 25.2 45.8 54.7 PVT-S T 34 226 40.4 61.3 43.0 25.0 42.9 55.7 Twins-SVT-S T 34 209 42.3 63.4 45.2 26.0 45.5 56.5 Focal-T T 39 265 43.7 - - - - - PoolFormer-S36 T 41 272 39.5 60.5 41.8 22.5 42.9 52.4 PVTV2-B2 T 35 281 44.6 65.7 47.6 28.6 48.5 59.2 CMT-S H 45 231 44.3 65.5 47.5 27.1 48.3 59.1 MogaNet-S C 35 253 45.8 66.6 49.0 29.1 50.1 59.8 ResNet-101 C 57 315 38.5 57.8 41.2 21.4 42.6 51.1 PVT-M T 54 258 41.9 63.1 44.3 25.0 44.9 57.6 Focal-S T 62 367 45.6 - - - - - PVTV2-B3 T 55 263 46.0 67.0 49.5 28.2 50.0 61.3 PVTV2-B4 T 73 315 46.3 67.0 49.6 29.0 50.1 62.7 MogaNet-B C 54 355 47.7 68.9 51.0 30.5 52.2 61.7 ResNeXt-101-64 C 95 473 41.0 60.9 44.0 23.9 45.2 54.0 PVTV2-B5 T 92 335 46.1 66.6 49.5 27.8 50.2 62.0 MogaNet-L C 92 477 48.7 69.5 52.6 31.5 53.4 62.7\nTable A8: Object detection with RetinaNet (1\u00d7 training schedule) on COCO val2017. The FLOPs are measured at resolution 800\u00d7 1280.\n2242 resolutions outperforms DeiT-T under the refined training scheme by 1.2% with only 3M parameters."
        },
        {
            "heading": "D MORE COMPARISON EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "D.1 FAST TRAINING ON IMAGENET-1K",
            "text": "In addition to Sec. 5.1, we further provide comparison results for 100 and 300 epochs training on ImageNet-1K. As for 100-epoch training, we adopt the original RSB A3 (Wightman et al., 2021) setting for all methods, which adopts LAMB (You et al., 2020) optimizer and a small training resolution of 1602. We search the basic learning in {0.006, 0.008} for all architectures and adopt the gradient clipping for Transformer-based networks. As for 300-epoch training, we report results of\nArchitecture Type #P. FLOPs Mask R-CNN 1\u00d7 (M) (G) APb APb50 AP b 75 AP m APm50 AP m 75 RegNet-800M C 27 187 37.5 57.9 41.1 34.3 56.0 36.8 MogaNet-XT C 23 185 40.7 62.3 44.4 37.6 59.6 40.2 ResNet-18 C 31 207 34.0 54.0 36.7 31.2 51.0 32.7 RegNet-1.6G C 29 204 38.9 60.5 43.1 35.7 57.4 38.9 PVT-T T 33 208 36.7 59.2 39.3 35.1 56.7 37.3 PoolFormer-S12 T 32 207 37.3 59.0 40.1 34.6 55.8 36.9 MogaNet-T C 25 192 42.6 64.0 46.4 39.1 61.3 42.0 ResNet-50 C 44 260 38.0 58.6 41.4 34.4 55.1 36.7 RegNet-6.4G C 45 307 41.1 62.3 45.2 37.1 59.2 39.6 PVT-S T 44 245 40.4 62.9 43.8 37.8 60.1 40.3 Swin-T T 48 264 42.2 64.6 46.2 39.1 61.6 42.0 MViT-T T 46 326 45.9 68.7 50.5 42.1 66.0 45.4 PoolFormer-S36 T 32 207 41.0 63.1 44.8 37.7 60.1 40.0 Focal-T T 49 291 44.8 67.7 49.2 41.0 64.7 44.2 PVTV2-B2 T 45 309 45.3 67.1 49.6 41.2 64.2 44.4 LITV2-S T 47 261 44.9 67.0 49.5 40.8 63.8 44.2 CMT-S H 45 249 44.6 66.8 48.9 40.7 63.9 43.4 Conformer-S/16 H 58 341 43.6 65.6 47.7 39.7 62.6 42.5 Uniformer-S H 41 269 45.6 68.1 49.7 41.6 64.8 45.0 ConvNeXt-T C 48 262 44.2 66.6 48.3 40.1 63.3 42.8 FocalNet-T (SRF) C 49 267 45.9 68.3 50.1 41.3 65.0 44.3 FocalNet-T (LRF) C 49 268 46.1 68.2 50.6 41.5 65.1 44.5 MogaNet-S C 45 272 46.7 68.0 51.3 42.2 65.4 45.5 ResNet-101 C 63 336 40.4 61.1 44.2 36.4 57.7 38.8 RegNet-12G C 64 423 42.2 63.7 46.1 38.0 60.5 40.5 PVT-M T 64 302 42.0 64.4 45.6 39.0 61.6 42.1 Swin-S T 69 354 44.8 66.6 48.9 40.9 63.4 44.2 Focal-S T 71 401 47.4 69.8 51.9 42.8 66.6 46.1 PVTV2-B3 T 65 397 47.0 68.1 51.7 42.5 65.7 45.7 LITV2-M T 68 315 46.5 68.0 50.9 42.0 65.1 45.0 UniFormer-B H 69 399 47.4 69.7 52.1 43.1 66.0 46.5 ConvNeXt-S C 70 348 45.4 67.9 50.0 41.8 65.2 45.1 MogaNet-B C 63 373 47.9 70.0 52.7 43.2 67.0 46.6 Swin-B T 107 496 46.9 69.6 51.2 42.3 65.9 45.6 PVTV2-B5 T 102 557 47.4 68.6 51.9 42.5 65.7 46.0 ConvNeXt-B C 108 486 47.0 69.4 51.7 42.7 66.3 46.0 FocalNet-B (SRF) C 109 496 48.8 70.7 53.5 43.3 67.5 46.5 MogaNet-L C 102 495 49.4 70.7 54.1 44.1 68.1 47.6\nTable A9: Object detection and instance segmentation with Mask R-CNN (1\u00d7 training schedule) on COCO val2017. The FLOPs are measured at resolution 800\u00d7 1280.\nRSB A2 (Wightman et al., 2021) for classical CNN or the original setting for Transformers or modern ConvNets. In Table A15, when compared with models of similar parameter size, our proposed MogaNet-XT/T/S/B achieves the best performance in both 100 and 300 epochs training. Results of 100-epoch training show that MogaNet has a faster convergence speed than previous architectures of various types. For example, MogaNet-T outperforms EfficientNet-B0 and DeiT-T by 2.4% and 8.7%, MogaNet-S outperforms Swin-T by 3.4%, and MogaNet-B outperforms Swin-S by 2.0%. Notice that ConvNeXt variants have a great convergence speed, e.g., ConvNeXt-S achieves 81.7% surpassing Swin-S by 1.5 and recently proposed ConvNet HorNet-S7\u00d77 by 0.5 with similar parameters. But our proposed MogaNet convergences faster than ConvNet, e.g., MogaNet-S outperforms ConvNeXt-T by 2.3% with similar parameters while MogaNet-B/L reaching competitive performances as ConvNeXt-B/L with only 44\u223c50% parameters."
        },
        {
            "heading": "D.2 DETECTION AND SEGMENTATION RESULTS ON COCO",
            "text": "In addition to Sec. 5.2, we provide full results of object detection and instance segmentation tasks with RetinaNet, Mask R-CNN, and Cascade Mask R-CNN on COCO. As shown in Table A8 and Table A9, RetinaNet or Mask R-CNN with MogaNet variants outperforms existing models when training 1\u00d7 schedule. For example, RetinaNet with MogaNet-T/S/B/L achieve 45.8/47.7/48.7 APb, outperforming PVT-T/S/M and PVTV2-B1/B2/B3/B5 by 4.7/4.6/5.8 and 0.3/1.2/1.7/2.6 APb; Nask R-CNN with MogaNet-S/B/L achieve 46.7/47.9/49.4 APb, exceeding Swin-T/S/B and ConvNeXt-\nArchitecture Type #P. FLOPs Cascade Mask R-CNN +MS 3\u00d7 (M) (G) APbb APb50 AP b 75 AP m APm50 AP m 75 ResNet-50 C 77 739 46.3 64.3 50.5 40.1 61.7 43.4 Swin-T T 86 745 50.4 69.2 54.7 43.7 66.6 47.3 Focal-T T 87 770 51.5 70.6 55.9 - - - ConvNeXt-T C 86 741 50.4 69.1 54.8 43.7 66.5 47.3 FocalNet-T (SRF) C 86 746 51.5 70.1 55.8 44.6 67.7 48.4 MogaNet-S C 78 750 51.6 70.8 56.3 45.1 68.7 48.8 ResNet-101-32 C 96 819 48.1 66.5 52.4 41.6 63.9 45.2 Swin-S T 107 838 51.9 70.7 56.3 45.0 68.2 48.8 ConvNeXt-S C 108 827 51.9 70.8 56.5 45.0 68.4 49.1 MogaNet-B C 101 851 52.6 72.0 57.3 46.0 69.6 49.7 Swin-B T 145 982 51.9 70.5 56.4 45.0 68.1 48.9 ConvNeXt-B C 146 964 52.7 71.3 57.2 45.6 68.9 49.5 MogaNet-L C 140 974 53.3 71.8 57.8 46.1 69.2 49.8 Swin-L\u2021 T 253 1382 53.9 72.4 58.8 46.7 70.1 50.8 ConvNeXt-L\u2021 C 255 1354 54.8 73.8 59.8 47.6 71.3 51.7 ConvNeXt-XL\u2021 C 407 1898 55.2 74.2 59.9 47.7 71.6 52.2 RepLKNet-31L\u2021 C 229 1321 53.9 72.5 58.6 46.5 70.0 50.6 HorNet-L\u2021 C 259 1399 56.0 - - 48.6 - - MogaNet-XL\u2021 C 238 1355 56.2 75.0 61.2 48.8 72.6 53.3\nTable A10: Object detection and instance segmentation with Cascade Mask R-CNN (3\u00d7 training schedule) with multi-scaling training (MS) on COCO val2017. \u2021 denotes the model is pre-trained on ImageNet-21K. The FLOPs are measured at resolution 800\u00d7 1280.\nT/S/B by 4.5/3.1/2.5 and 2.5/2.5/2.4 with similar parameters and computational overloads. Noticeably, MogaNet-XT/T can achieve better detection results with fewer parameters and lower FLOPs than lightweight architectures, while MogaNet-T even surpasses some Transformers like Swin-S and PVT-S. For example, Mask R-CNN with MogaNet-T improves Swin-T by 0.4 APb and outperforms PVT-S by 1.3 APm using only around 2/3 parameters. As shown in Table A10, Cascade Mask R-CNN with MogaNet variants still achieves the state-of-the-art detection and segmentation results when training 3\u00d7 schedule with multi-scaling (MS) and advanced augmentations. For example, MogaNet-L/XL yield 53.3/56.2 APb and 46.1/48.8 APm, which improves Swin-B/L and ConvNeXt-B/L by 1.4/2.3 and 0.6/1.4 APb with similar parameters and FLOPS."
        },
        {
            "heading": "D.3 SEMENTIC SEGMENTATION RESULTS ON ADE20K",
            "text": "In addition to Sec. 5.2, we provide comprehensive comparison results of semantic segmentation based on UperNet on ADE20K. As shown in Table A11, UperNet with MogaNet produces stateof-the-art performances in a wide range of parameter scales compared to famous Transformer, hybrid, and convolution models. As for the lightweight models, MogaNet-XT/T significantly improves ResNet-18/50 with fewer parameters and FLOPs budgets. As for medium-scaling models, MogaNet-S/B achieves 49.2/50.1 mIoUss, which outperforms the recently proposed ConvNets, e.g., +1.1 over HorNet-T using similar parameters and +0.7 over SLaK-S using 17M fewer parameters. As for large models, MogaNet-L/XL surpass Swin-B/L and ConvNeXt-B/L by 1.2/1.9 and 1.8/0.3 mIoUss while using fewer parameters.\nD.4 2D HUMAN POSE ESTIMATION RESULTS ON COCO\nIn addition to Sec. 5.2, we provide comprehensive experiment results of 2D human key points estimation based on Top-Down SimpleBaseline on COCO. As shown in Table A13, MogaNet variants achieve competitive or state-of-the-art performances compared to popular architectures with two types of resolutions. As for lightweight models, MogaNet-XT/T significantly improves the performances of existing models while using similar parameters and FLOPs. Meanwhile, MogaNet-S/B also produces 74.9/75.3 and 76.4/77.3 AP using 256\u00d7192 and 384\u00d7288 resolutions, outperforming Swin-B/L by 2.0/1.0 and 1.5/1.0 AP with nearly half of the parameters and computation budgets.\nD.5 3D HUMAN POSE ESTIMATION RESULTS\nIn addition to Sec. 5.2, we evaluate popular ConvNets and MogaNet for 3D human pose estimation tasks based on ExPose (Choutas et al., 2020). As shown in Table A12, MogaNet achieves lower regression errors with efficient usage of parameters and computational overheads. Compared to lightweight architectures, MogaNet-T achieves 6.82 MPJPE and 2.36 3DRMSE on hand and face reconstruction tasks, improving ResNet-18 and MobileNetV2 1\u00d7 by 1.29/0.04 and 1.51/0.28. Compared to models around 25\u223c50M parameters, MogaNet-S surpasses ResNet-101 and ConvNeXt-T, achieving competitive results as ConvNeXt-S with relatively smaller parameters and FLOPs (e.g., 27M/6.5G vs 52M/11.4G on FFHP). Notice that some backbones with more parameters produce worse results than their lightweight variants on the face estimation tasks (e.g., ResNet-50 and SwinS), while MogaNet-S still yields the better performance of 2.24 3DRMSE.\nD.6 VIDEO PREDICTION RESULTS ON MOVING MNIST\nIn addition to Sec. 5.2, We verify video prediction performances of various architectures by replacing the hidden translator in SimVP with the architecture blocks. All models use the same number of network blocks and have similar parameters and FLOPs. As shown in Table A14, Compared to Transformer-based and Metaformer-based architectures, pure ConvNets usually achieve lower prediction errors. When training 200 epochs, it is worth noticing that using MogaNet blocks in SimVP significantly improves the SimVP baseline by 6.58/13.86 MSE/MAE and outperforms ConvNeXt and HorNet by 1.37 and 4.07 MSE. MogaNet also holds the best performances in the extended 2000-epoch training setting."
        },
        {
            "heading": "E EXTENSIVE RELATED WORK",
            "text": "Convolutional Neural Networks ConvNets (LeCun et al., 1998; Krizhevsky et al., 2012; He et al., 2016) have dominated a wide range of computer vision (CV) tasks for decades. VGG (Simonyan & Zisserman, 2014) proposes a modular network design strategy, stacking the same type of blocks repeatedly, which simplifies both the design workflow and transfer learning for downstream tasks. ResNet (He et al., 2016) introduces identity skip connections and bottleneck modules that alleviate training difficulties (e.g., vanishing gradient). With the desired properties, ResNet and its variants (Zagoruyko & Komodakis, 2016; Xie et al., 2017; Hu et al., 2018; Zhang et al., 2022a) have become the most widely adopted ConvNet architectures in numerous CV applications. For practical usage, efficient models (Ma et al., 2018; Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019; Tan & Le, 2019; Radosavovic et al., 2020) are designed for a complexity-accuracy trade-off and hardware devices. Since the limited reception fields, spatial and temporal convolutions struggle to capture global dependency (Luo et al., 2016). Various spatial-wise or channel-wise attention strategies (Dai et al., 2017; Hu et al., 2018; Wang et al., 2018; Woo et al., 2018; Cao et al., 2019) are introduced. Recently, taking the merits of Transformer-like macro design (Dosovitskiy et al., 2021), modern ConvNets (Trockman & Kolter, 2022; Ding et al., 2022b; Liu et al., 2023; Rao et al., 2022; Kirchmeyer & Deng, 2023) show thrilling performance with large depth-wise convolutions (Han et al., 2021b) for global contextual features. Among them, VAN (Guo et al., 2023), FocalNet (Yang et al., 2022), HorNet (Rao et al., 2022), and Conv2Former (Hou et al., 2022) exploit multi-scale convolutional kernels with gating operations. However, these methods fail to ensure the networks learn the inherently overlooked features (Deng et al., 2022) and achieve ideal contextual aggregation. Unlike the previous works, we first design three groups of multi-order depth-wise convolutions in parallel followed by a double-branch activated gating operation, and then propose a channel aggregation module to enforce the network to learn informative features of various interaction scales.\nVision Transformers Transformer (Vaswani et al., 2017) with self-attention mechanism has become the mainstream choice in natural language processing (NLP) community (Devlin et al., 2018; Brown et al., 2020). Considering that global information is also essential for CV tasks, Vision Transformer (ViT) (Dosovitskiy et al., 2021) is proposed and has achieved promising results on ImageNet (Deng et al., 2009). In particular, ViT splits raw images into non-overlapping fixed-\nsize patches as visual tokens to capture long-range feature interactions among these tokens by selfattention. By introducing regional inductive bias, ViT and its variants have been extended to various vision tasks Carion et al. (2020); Zhu et al. (2021); Chen et al. (2021); Parmar et al. (2018); Jiang et al. (2021a); Arnab et al. (2021). Equipped with advanced training strategies (Touvron et al., 2021a; 2022) or extra knowledge (Jiang et al., 2021b; Lin et al., 2022; Wu et al., 2022c), pure ViTs can achieve competitive performance as ConvNets in CV tasks. In the literature of Yu et al. (2022), the MetaFormer architecture substantially influenced the design of vision backbones, and all Transformer-like models (Touvron et al., 2021a; Trockman & Kolter, 2022; Wang et al., 2022a) are\nclassified by how they treat the token-mixing approaches, such as relative position encoding (Wu et al., 2021b), local window shifting (Liu et al., 2021) and MLP layer (Tolstikhin et al., 2021), etc. Beyond the aspect of macro design, Touvron et al. (2021b); Yuan et al. (2021a) introduced knowledge distillation and progressive tokenization to boost training data efficiency. Compared to ConvNets banking on the inherent inductive biases (e.g., locality and translation equivariance), the pure ViTs are more over-parameterized and rely on large-scale pre-training (Dosovitskiy et al., 2021; Li et al., 2023b) by contrastive learning (He et al., 2020; Zang et al., 2022; Li et al., 2023c) or masked image modeling (Bao et al., 2022; He et al., 2022; Li et al., 2023a; Woo et al., 2023) to a great extent. Targeting this problem, one branch of researchers proposes lightweight ViTs (Xiao et al., 2021; Mehta & Rastegari, 2022; Li et al., 2022c; Chen et al., 2023) with more efficient self-attentions variants (Wang et al., 2021a). Meanwhile, the incorporation of self-attention and convolution as a hybrid backbone has been vigorously studied (Guo et al., 2022; Wu et al., 2021a; Dai et al., 2021; d\u2019Ascoli et al., 2021; Li et al., 2022a; Pan et al., 2022b; Si et al., 2022) for imparting regional priors to ViTs."
        }
    ],
    "title": "MOGANET: MULTI-ORDER GATED AGGREGATION NETWORK",
    "year": 2024
}