{
    "abstractText": "Convolutional neural network (CNN)-based and Transformer-based methods have recently made significant strides in time series forecasting, which excel at modeling local temporal variations or capturing long-term dependencies. However, real-world time series usually contain intricate temporal patterns, thus making it challenging for existing methods that mainly focus on temporal variations modeling from the 1D time series directly. Based on the intrinsic periodicity of time series, we propose a novel Periodicity Decoupling Framework (PDF) to capture 2D temporal variations of decoupled series for long-term series forecasting. Our PDF mainly consists of three components: multi-periodic decoupling block (MDB), dual variations modeling block (DVMB), and variations aggregation block (VAB). Unlike the previous methods that model 1D temporal variations, our PDF mainly models 2D temporal variations, decoupled from 1D time series by MDB. After that, DVMB attempts to further capture short-term and long-term variations, followed by VAB to make final predictions. Extensive experimental results across seven real-world long-term time series datasets demonstrate the superiority of our method over other state-of-the-art methods, in terms of both forecasting performance and computational efficiency. Code is available at https://github.com/Hank0626/PDF.",
    "authors": [
        {
            "affiliations": [],
            "name": "SERIES FORECASTING"
        },
        {
            "affiliations": [],
            "name": "Tao Dai"
        },
        {
            "affiliations": [],
            "name": "Beiliang Wu"
        },
        {
            "affiliations": [],
            "name": "Peiyuan Liu"
        },
        {
            "affiliations": [],
            "name": "Naiqi Li"
        },
        {
            "affiliations": [],
            "name": "Jigang Bao"
        },
        {
            "affiliations": [],
            "name": "Yong Jiang"
        },
        {
            "affiliations": [],
            "name": "Shu-Tao Xia"
        }
    ],
    "id": "SP:fd43a41fa0068c5afd9bfa2f3c813290f9c17604",
    "references": [
        {
            "authors": [
                "Rafal A Angryk",
                "Petrus C Martens",
                "Berkay Aydin",
                "Dustin Kempton",
                "Sushant S Mahajan",
                "Sunitha Basodi",
                "Azim Ahmadzadeh",
                "Xumin Cai",
                "Soukaina Filali Boubrahimi",
                "Shah Muhammad Hamdi"
            ],
            "title": "Multivariate time series dataset for space weather data analytics",
            "venue": "Scientific data,",
            "year": 2020
        },
        {
            "authors": [
                "Shaojie Bai",
                "J Zico Kolter",
                "Vladlen Koltun"
            ],
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "venue": "arXiv preprint arXiv:1803.01271,",
            "year": 2018
        },
        {
            "authors": [
                "Xuelin Cao",
                "Bo Yang",
                "Chongwen Huang",
                "George C Alexandropoulos",
                "Chau Yuen",
                "Zhu Han",
                "H Vincent Poor",
                "Lajos Hanzo"
            ],
            "title": "Massive access of static and mobile users via reconfigurable intelligent surfaces: Protocol design and performance analysis",
            "venue": "IEEE Journal on Selected Areas in Communications,",
            "year": 2022
        },
        {
            "authors": [
                "Chris Chatfield"
            ],
            "title": "The analysis of time series: an introduction",
            "year": 1981
        },
        {
            "authors": [
                "Chao Chen",
                "Karl Petty",
                "Alexander Skabardonis",
                "Pravin Varaiya",
                "Zhanfeng Jia"
            ],
            "title": "Freeway performance measurement system: mining loop detector data",
            "venue": "Transportation Research Record,",
            "year": 2001
        },
        {
            "authors": [
                "Abhimanyu Das",
                "Weihao Kong",
                "Andrew Leach",
                "Rajat Sen",
                "Rose Yu"
            ],
            "title": "Long-term forecasting with tide: Time-series dense encoder",
            "venue": "arXiv preprint arXiv:2304.08424,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Haoqi Fan",
                "Bo Xiong",
                "Karttikeya Mangalam",
                "Yanghao Li",
                "Zhicheng Yan",
                "Jitendra Malik",
                "Christoph Feichtenhofer"
            ],
            "title": "Multiscale vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Rob J Hyndman",
                "George Athanasopoulos"
            ],
            "title": "Forecasting: principles and practice",
            "venue": "OTexts,",
            "year": 2018
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "G\u00fcnter Klambauer",
                "Thomas Unterthiner",
                "Andreas Mayr",
                "Sepp Hochreiter"
            ],
            "title": "Self-normalizing neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Shiyang Li",
                "Xiaoyong Jin",
                "Yao Xuan",
                "Xiyou Zhou",
                "Wenhu Chen",
                "Yu-Xiang Wang",
                "Xifeng Yan"
            ],
            "title": "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Bryan Lim",
                "Stefan Zohren"
            ],
            "title": "Time-series forecasting with deep learning: a survey",
            "venue": "Philosophical Transactions of the Royal Society A,",
            "year": 2020
        },
        {
            "authors": [
                "Shengsheng Lin",
                "Weiwei Lin",
                "Wentai Wu",
                "Songbo Wang",
                "Yongxiang Wang"
            ],
            "title": "PETformer: Long-term time series forecasting via placeholder-enhanced transformer",
            "venue": "arXiv preprint arXiv:2308.04791,",
            "year": 2023
        },
        {
            "authors": [
                "Minhao Liu",
                "Ailing Zeng",
                "Muxi Chen",
                "Zhijian Xu",
                "Qiuxia Lai",
                "Lingna Ma",
                "Qiang Xu"
            ],
            "title": "SCInet: Time series modeling and forecasting with sample convolution and interaction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shizhan Liu",
                "Hang Yu",
                "Cong Liao",
                "Jianguo Li",
                "Weiyao Lin",
                "Alex X Liu",
                "Schahram Dustdar"
            ],
            "title": "Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Jia Ning",
                "Yue Cao",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Han Hu"
            ],
            "title": "Video swin transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Patton"
            ],
            "title": "Copula methods for forecasting multivariate time series",
            "venue": "Handbook of economic forecasting,",
            "year": 2013
        },
        {
            "authors": [
                "Michael S Ryoo",
                "AJ Piergiovanni",
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Anelia Angelova"
            ],
            "title": "Tokenlearner: What can 8 learned tokens do for images and videos",
            "venue": "arXiv preprint arXiv:2106.11297,",
            "year": 2021
        },
        {
            "authors": [
                "Chenyang Si",
                "Weihao Yu",
                "Pan Zhou",
                "Yichen Zhou",
                "Xinchao Wang",
                "Shuicheng Yan"
            ],
            "title": "Inception transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Huiqiang Wang",
                "Jian Peng",
                "Feihu Huang",
                "Jince Wang",
                "Junhui Chen",
                "Yifei Xiao"
            ],
            "title": "MICN: Multiscale local and global context modeling for long-term series forecasting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Gerald Woo",
                "Chenghao Liu",
                "Doyen Sahoo",
                "Akshat Kumar",
                "Steven Hoi"
            ],
            "title": "ETSformer: Exponential smoothing transformers for time-series forecasting",
            "venue": "arXiv preprint arXiv:2202.01381,",
            "year": 2022
        },
        {
            "authors": [
                "Sanghyun Woo",
                "Shoubhik Debnath",
                "Ronghang Hu",
                "Xinlei Chen",
                "Zhuang Liu",
                "In So Kweon",
                "Saining Xie"
            ],
            "title": "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Haixu Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Haixu Wu",
                "Tengge Hu",
                "Yong Liu",
                "Hang Zhou",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "TimesNet: Temporal 2d-variation modeling for general time series analysis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoling Xia",
                "Cui Xu",
                "Bing Nan"
            ],
            "title": "Inception-v3 for flower classification",
            "venue": "In International Conference on Image, Vision and Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Ailing Zeng",
                "Muxi Chen",
                "Lei Zhang",
                "Qiang Xu"
            ],
            "title": "Are transformers effective for time series forecasting",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Chen Zhang",
                "Yinghao Xu",
                "Yujun Shen"
            ],
            "title": "Compconv: A compact convolution module for efficient feature learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Zhang",
                "Rui Wu",
                "Sergiu M Dascalu",
                "Frederick C Harris Jr."
            ],
            "title": "Multi-scale transformer pyramid networks for multivariate time series forecasting",
            "venue": "arXiv preprint arXiv:2308.11946,",
            "year": 2023
        },
        {
            "authors": [
                "Yunhao Zhang",
                "Junchi Yan"
            ],
            "title": "Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yi Zheng",
                "Qi Liu",
                "Enhong Chen",
                "Yong Ge",
                "J Leon Zhao"
            ],
            "title": "Time series classification using multi-channels deep convolutional neural networks",
            "venue": "In International Conference on Web-age Information Management,",
            "year": 2014
        },
        {
            "authors": [
                "Haoyi Zhou",
                "Shanghang Zhang",
                "Jieqi Peng",
                "Shuai Zhang",
                "Jianxin Li",
                "Hui Xiong",
                "Wancai Zhang"
            ],
            "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Zhou",
                "Ziqing Ma",
                "Qingsong Wen",
                "Xue Wang",
                "Liang Sun",
                "Rong Jin"
            ],
            "title": "FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Time series forecasting plays an essential role in multiple applications, including weather prediction (Angryk et al., 2020), energy management (Zhou et al., 2021), financial investment (Patton, 2013), and traffic flow estimation (Chen et al., 2001). Recently, with the rapid development of deep learning, plenty of deep learning (DL)-based methods have been developed for time series forecasting (Lim & Zohren, 2021), which can be roughly divided into CNN-based (Wang et al., 2022; Liu et al., 2022a) and Transformer-based methods (Li et al., 2019; Zhou et al., 2021).\nExisting DL-based methods mainly focus on 1D temporal variation modeling directly, which plays a crucial role in time series forecasting. Among them, CNN-based methods (Bai et al., 2018; Wang et al., 2022; Wu et al., 2023) have shown the powerful ability to capture short-term variations. For example, TCN (Bai et al., 2018) incorporates the local information of time series along the temporal dimensions by utilizing convolution operations, and exhibits superior performance in short-term and medium-term predictions. However, this type of method usually fails to work well for long-term time series, due to the limited representation of long-term dependencies. By contrast, Transformerbased methods (Li et al., 2019; Zhou et al., 2021; Wu et al., 2021) excel at capturing long-term dependencies due to the use of self-attention mechanism. For example, Autoformer (Wu et al., 2021) attempts to exploit the series-wise temporal dependencies with auto-correlation mechanism. PatchTST (Nie et al., 2023) proposes a novel patching strategy to retain local semantic information within each patch. Although the Transformer-based methods have shown more competitive perfor-\n\u2217Correspondence to: Peiyuan Liu and Naiqi Li\nmance than CNN-based methods, they often suffer from heavy computational costs, especially for long-term time series input, which thus limits their real applications.\nIt is worth considering that the modeling of 1D temporal variations can be a complex task due to the intricate patterns involved. These variations can come in various types, including short-term fluctuations, falling, and rising, which can often overlap with each other (see Figure 1a). Despite the challenges, researchers have made significant progress in this area, and the most effective way to model temporal variation remains an open question. However, it is important to note that real-world time series often exhibit multi-periodicity, such as daily and weekly variations for traffic forecasting, which has been confirmed in recent work (Wu et al., 2023). Furthermore, long-term time series can be simplified or decoupled based on a predetermined period. For example, as shown in Figure 1a, the original time series can be decoupled into short-term series and long-term series, which contain short-term changes and long-term dependencies, respectively. By taking these observations into account, we can utilize period information to decouple long-term time series.\nMotivated by the above observations, we propose a novel Periodicity Decoupling Framework (PDF) for long-term series forecasting by capturing the intricate periodic information inside the time series. Based on the periodicity of the time series, the original 1D time series can be further decoupled into simpler short and long-term series, which respectively represent the local changes and global correlations of the 1D time series. Due to the diversity of short-term variations (e.g., fluctuation, rising, and falling), we employ \u201cfrequency slicing\u201d, corresponding to different periods, to divide the look-back window into several sub-sequences. For long-term variations, we utilize \u201cperiod patching\u201d to extract changes within corresponding time segments across all periods (see Figure 1a). The \u201cperiod patching\u201d ensures each patch contains rich long-term semantic information.\nTechnically, we propose a novel Periodicity Decoupling Framework (PDF) for long-term time series forecasting. As illustrated in Fig. 2, our PDF contains three main components: multi-periodic decoupling block (MDB), dual variations modeling block (DVMB), and variations aggregation block (VAB). Unlike the previous methods that focus on 1D temporal variations modeling, our PDF models 2D temporal variations. Specifically, the multi-periodic decoupling block first decouples the 1D time series into different short- and long-term 1D series based on the period of input series in the frequency domain, followed by further reshaping into 2D tensors with rich short- and long-term variations. After that, the dual variations modeling block attempts to capture short-term and longterm variations from the decoupled 2D tensors, followed by a variations aggregation block to make final predictions. Extension experiments on our PDF confirm its state-of-the-art performance across various long-term time series datasets, in terms of both forecasting performance and computational efficiency. Notably, as seen in Figure 1b, our PDF handles the long-term series (with a look-back window length of 960) better while not sacrificing computational cost (with only 24 patches) than other Tranformer-based methods.\nOur main contributions are summarized as follows:\n\u2022 We propose a novel Periodicity Decoupling Framework (PDF) for long-term series forecasting, which fully captures 2D temporal short-term and long-term variations from the decoupled series in a parallel architecture.\n\u2022 We propose multi-periodic decoupling block to capture various periods of the input series in the frequency domain. Based on the periodicity of the time series, the 1D time series can be decoupled into simpler short- and long-term series formulated with 2D tensors. To fully capture the short- and long-term variations, we propose dual variations modeling block (DVMB) with short- and long-term variations extractor, which is able to preserve the highfrequency information of short-term changes while exploiting long-term dependencies.\n\u2022 Extensive experiments demonstrate the effectiveness of our PDF over other state-of-theart methods across various long-term time series datasets, in terms of both forecasting performance and computational efficiency."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Traditional time series forecasting methods such as ARIMA (Anderson & Kendall, 1976) and HoltWinter (Hyndman & Athanasopoulos, 2018) offer robust theoretical frameworks but suffer from limitations in handling data with intricate temporal dynamics. Recent years have witnessed milestone achievements of deep learning-based approaches in time series forecasting, which mainly include CNN-based (Wu et al., 2023), and Transformer-based methods (Lim & Zohren, 2021).\nConvolutional neural network (CNN) has gained widespread popularity due to its ability to capture localized features (Xia et al., 2017; Zhang et al., 2021; Woo et al., 2023). Many CNN-based time series forecasting methods employ Temporal Convolutional Networks (TCN) to extract local temporal dynamics (Bai et al., 2018; Liu et al., 2022a; Wang et al., 2022), where MICN (Wang et al., 2022) and TimesNet (Wu et al., 2023) are related to our method. Typically, MICN attempts to combine local features and global correlations to capture the overall view of time series with convolution kernels. TimesNet focuses on modeling 2D temporal variations in 2D spaces from the extraction of \u201cintra-period\u201d and \u201cinter-period\u201d variations. However, these methods rely heavily on convolution kernels to model series variations, resulting in limited representations of long-term dependencies. Instead, our method can capture both short- and long-term variations simultaneously with dual variations modeling block.\nAnother type of Transformer-based method has shown more competitive performance in long-term time series forecasting. With the self-attention mechanism, Transformer and its variant are capable of capturing long-term dependencies and extracting global information (Dosovitskiy et al., 2021; Fan et al., 2021; Ryoo et al., 2021; Liu et al., 2022b). However, their scalability and efficiency are constrained by the quadratic complexity of the attention mechanism. To mitigate this, various techniques are proposed to reduce the complexity of the Transformer. For example, LogTrans (Li et al., 2019) utilizes convolution self-attention to reduce the space complexity. Informer (Zhou et al., 2021) applies distilling strategies to exploit the most crucial keys. Pyraformer (Liu et al., 2021) proposes a pyramid attention design with inter-scale and intra-scale connections. More recent work PatchTST (Nie et al., 2023) employs patch-based strategies to enhance the locality while improving long-term forecasting accuracy. However, existing Transformer-based methods still focus on 1D temporal variation modeling and suffer from heavy computational burden for long-term time series. Instead, we propose a more efficient Periodicity Decoupling Framework (PDF) for long-term series forecasting by fully capturing 2D temporal short-term and long-term variations in a parallel architecture."
        },
        {
            "heading": "3 PERIODICITY DECOUPLING FRAMEWORK",
            "text": ""
        },
        {
            "heading": "3.1 THE OVERALL ARCHITECTURE",
            "text": "In time series forecasting, given a historical input series XI = [x1, x2, . . . , xt]T \u2208 Rt\u00d7d, it aims to predict future output series XO = [xt+1, xt+2, . . . , xt+T ]T \u2208 RT\u00d7d, where t, T is the number of time steps in the past and future, respectively, where d > 1 is the number of dimensions. The overall architecture of our method is shown in Figure 2. In our PDF, due to the complex temporal\npatterns, it is the first step to decouple the 1D time series for better variation modeling. To this end, we design a Multi-periodic Decoupling Block to learn the periods of input series in the frequency domain and convert the 1D time series into short- and long-term series, followed by reshaping into 2D tensors. Then, the obtained short-term and long-term 2D tensors are fed into serveral Dual Variations Modeling Blocks (DVMB) to model short- and long-term variations in a parallel way. Finally, we use a Variations Aggregation Block to merge the outputs from all DVMBs to yield the final prediction XO. More details about our PDF are shown in the following sections."
        },
        {
            "heading": "3.2 MULTI-PERIODIC DECOUPLING BLOCK",
            "text": "The Multi-periodic Decoupling block uses Periodicity Extractor and Period-based Reshaper to transform 1D time series into 2D spaces. Then it utilizes Temporal Variations Decoupler to decouple the long-term and short-term information through \u201cperiod patching\u201d and \u201cfrequency slicing\u201d.\nPeriodicity Extractor. Previous work (Wu et al., 2023) emphasizes that the original 1D structure of time series inadequately represents only adjacent time point variations, and a 2D structure can effectively capture variations both within and between periods. Therefore, for a given 1D input XI \u2208 Rt\u00d7d of dimension d, we employ the Fast Fourier Transform (FFT) (Chatfield, 1981) to analyze the time series in the frequency domain as follows:\nA = Avg(Amp(FFT(XI))) (1)\nHere, FFT and Amp denote the FFT and amplitude extraction, respectively. The channel-wise average operation Avg over d channels yields A \u2208 Rt, representing the amplitudes of t frequencies. Specifically, the j-th value Aj represents the intensity of the periodic basis function for frequency f . We use the univariate XI \u2208 Rt instead of XI to denote the input time series in the following calculation, because the subsequent transformations and predictions are made in a channel-independent manner (Zheng et al., 2014; Nie et al., 2023).\nDifferent from Wu et al. (2023), we select frequencies not only focus on high amplitude but also incorporate those with significant values and amplitude. We assert that frequencies with high amplitude better represent the primary components, while those with larger values facilitate a more discernible distinction between long-term and short-term relationships. We summarize the k frequencies selection by:\nFu = arg top-m f\u2217\u2208{1,\u00b7\u00b7\u00b7 ,[ t2 ]} (A), Fk1 = arg top-k1 f\u2217\u2208{1,\u00b7\u00b7\u00b7 ,[ t2 ]} (A), {f1, \u00b7 \u00b7 \u00b7 , fk} = Fk1 \u222a top-k2(Fu \\ Fk1) (2)\nwhere Fu and Fk1 represents the u and k1 frequencies with highest amplitudes from A, respectively. We ensure that u is greater than or equal to k1. Due to the conjugate symmetry in the frequency domain, f\u2217 only focuses on the former [ t2 ] frequencies. The final set of k frequencies is composed of Fk1 and the top-k2 frequencies with the greatest values from Fu \\ Fk1 . Period-based Reshaper. Based on the selected frequencies {f1, \u00b7 \u00b7 \u00b7 , fk} and corresponding period lengths {p1, \u00b7 \u00b7 \u00b7 , pk} (pi = \u2308 tfi \u2309), we reshape the 1D input series XI \u2208 R t into k 2D tensors by: Xi2D = Reshapefi,pi(Padding(XI)), i \u2208 {1, \u00b7 \u00b7 \u00b7 , k} (3) Here, Padding(\u00b7) is employed to extend the length of XI to pi \u00d7 fi by filling zeros for Reshapefi,pi(\u00b7), where fi and pi denote the number of rows and columns of the 2D tensor, respectively. For the obtained 2D tensor Xi2D \u2208 Rfi\u00d7pi , each row represents the short-term variations and each column represents long-term variations. We then employ Temporal Variations Decoupler to decouple the long-term and short-term information through \u201cperiod patching\u201d and \u201cfrequency slicing\u201d.\nPeriod Patching: Denote the patch length as p and the stride length as s, we divide Xi2D \u2208 Rfi\u00d7pi along dimension pi and aggregate along dimension fi to form a patch. Specifically, Xi2D is patched into multiple patches xi,jg \u2208 RN\u00d7P , where N = \u230a (pi\u2212p) s \u230b + 1 is the number of patches and each patch contains P = fi \u00d7 p time steps. xi,jg denotes the j-th patch. This patching strategy condenses complete long-term variations between all periods.\nCompared with former patching strategies (Nie et al., 2023; Zhang & Yan, 2023), our patches capture a broader scope and richer semantic information, enhancing the capacity of the Transformer for modeling long-term variations. Meanwhile, because the number of patches decreases from t/s to max(pi)/s, the computational cost is significantly reduced.\nFrequency Slicing: Along with fi dimensions, we split Xi2D into several 1D short-term slices x i,r l \u2208 Rpi , where r \u2208 [1, fi] denotes the r-th row of Xi2D. Each local slice represents the short-term variations within every period."
        },
        {
            "heading": "3.3 DUAL VARIATIONS MODELING BLOCK",
            "text": "As illustrated in Figure 4, the Dual Variations Modeling Block is composed of two parts: long-term variations extractor and short-term variations extractor. It adopts a dual-branch parallel architecture to model the long-term and short-term variations in the time series. This parallel structure not only better preserves the high-frequency information of short-term changes but also enhances computational efficiency (Wang et al., 2022; Si et al., 2022). The details of each component will be given as follows.\nLong-term Variations Extractor: Given the patches xi,jg \u2208 RN\u00d7P with long-term information, we initially project them into the latent space via a linear projection: xi,jg = Linear(x i,j g ) \u2208 RN\u00d7D, where D is the dimension of latent space. Subsequently, xi,jg will go through several Transformer encoder layers. The specific process of each layer can be described as follows:\nx\u0302i,jg = BatchNorm(x i,j g +MSA(x i,j g , x i,j g , x i,j g ))\nx\u0302i,jg = BatchNorm(x\u0302 i,j g +MLP(x\u0302 i,j g ))\n(4)\nHere, BatchNorm(\u00b7) denotes batch normalization (Ioffe & Szegedy, 2015). MLP(\u00b7) is a multilayered linear feedforward neural network. Multi-head self-attention MSA(\u00b7) mechanism enhances the representation capacity by employing multiple independent self-attention heads. Each head captures different types of long-term dependencies among different patches. All these heads are combined to obtain more comprehensive dependencies by:\nXig = Linear(Flatten(x\u0302 i,j g )) \u2208 Rt (5)\nShort-term Variations Extractor: This module contains a sequence of convolution blocks, each consisting of a Conv1d layer and a non-linear activation function. These blocks are sequentially structured to gradually expand the receptive field, accommodating periods of various lengths. For each local slice xi,rl , the process of each block is:\nx\u0302i,rl = SELU(Conv1d(x i,r l )) (6)\nwhere SELU denotes scaled exponential linear units (Klambauer et al., 2017). To get the final prediction of the convolution part, we use the concatenate and truncate operations:\nXil = Truncate(Concat(x\u0302 i,r l )) (7)\nThe final output of the Dual Variations Modeling Block is the summation of Xig and X i l :\nX\u0302i = Xig +X i l (8)"
        },
        {
            "heading": "3.4 VARIATIONS AGGREGATION BLOCK",
            "text": "The Variations Aggregation Block consolidates the results from k DVMBs. Specifically, we concatenate these k results and then map them through a parameter-shared linear layer to produce univariate prediction XO \u2208 RT :\nXO = Linear(Concat(X\u0302 i)) (9)\nThe final multivariate prediction XO \u2208 RT\u00d7d is obtained by stacking d univariate prediction XO."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Datasets We conduct extensive experiments on seven popular real-world datasets (Zhou et al., 2021), including Electricity Transformer Temperature (ETT) with its four sub-datasets (ETTh1, ETTh2, ETTm1, ETTm2), Weather, Electricity, and Traffic. We adopt the same train/val/test splits ratio 0.6:0.2:0.2 as Zhou et al. (2021) for the ETT datasets and split the remaining three by the ratio of 0.7:0.1:0.2 following Wu et al. (2021).\nBaselines We select representative state-of-the-art methods from the recent LTSF landscape as baselines, including the following categories: 1) Transformer-based models: PatchTST (Nie et al., 2023) and FEDformer (Zhou et al., 2022); 2) CNN-based models: TimesNet (Wu et al., 2023) and MICN (Wang et al., 2022); 3) Linear-based models: TiDE (Das et al., 2023) and DLinear (Zeng et al., 2023). Considering varying look-back window size leads to different performances, we pick up their best performance as baselines, and corresponding results are reported from the original papers.\nSetups Following Zhou et al. (2021), we normalize the train/val/test sets to zero-mean using the mean and standard deviation from the training set. The Mean Square Error (MSE) and Mean Absolute Error (MAE) are selected as evaluation metrics, consistent with previous methods. All of the models adopt the same prediction length T = {96, 192, 336, 720}. For the look-back window t, we conduct experiments on PDF using t = 336 and t = 720 while TiDE, PatchTST, and DLinear employ t = 720, 512, 336, and all other models use t = 96."
        },
        {
            "heading": "4.1 MAIN RESULTS",
            "text": "We present multivariate long-term forecasting results in Table 1. Regarding the Count value, PDF(720) and PDF(336) achieve the best and second-best results, outperforming all other methods across different categories. Quantitatively, compared with Transformer-based models, PDF(720) yields an overall 14.59% reduction in MSE and 10.77% reduction in MAE. Compared with CNNbased models, PDF(720) yields an overall 24.61% reduction in MSE and 19.91% reduction in MAE.\nCompared with Linear-based models, PDF(720) yields an overall 7.05% reduction in MSE and 5.51% reduction in MAE. These results affirm that PDF can effectively utilize a long historical look-back window. Furthermore, PDF(720) consistently outperforms all baselines, except for TiDE which exhibits a lower MSE on the traffic dataset. However, this superior performance of TiDE on the traffic dataset is largely attributed to the prior knowledge of static covariates (Das et al., 2023)."
        },
        {
            "heading": "4.2 EFFECTIVENESS OF PERIOD PATCHING",
            "text": "Analysis of patch information. Recent works (Nie et al., 2023; Lin et al., 2023; Zhang et al., 2023) point out that enhancing the semantic information within patches can lead to improved predictions. To assess the performance of patches emphasizing more semantics information versus long-term information, we conduct the following comparative experiments: 1) PatchTST(336): Following the original PatchTST experimental setup, we set each patch length p = 16 and stride s = 8, yielding a total of 42 patches; 2) PatchTST(336)\u2217: We set p = 64, s = 14 and obtain 24 patches. Compared with PatchTST(336), each patch is longer and encompasses more semantics information. 3) PDF(336): We employ single-period patching with a period length p1 = 24 and choose p = s = 1. Given that f1 = 336/p1 = 14, each patch has a length of p \u00d7 f1 = 14. This resulted in 24 patches, each rich in long-term information.\nThe experimental results in Table 2 show that compared with PatchTST(336) and PatchTST(336)\u2217, PDF(336) demonstrates noticeable performance improvements on most datasets. These findings emphasize the importance of long-term information contained within the patches. It is noteworthy that both PatchTST(336)* and PDF(336) have the same number of patches. Even though each patch in PatchTST(336)* is longer, theoretically suggesting potential for better prediction results, its performance does not improve and is even worse than PatchTST(336) in some cases. This further indicates that merely extending the semantics information within a patch is not sufficient for enhancing prediction. The key is to ensure each patch captures more long-term information and our period patching method can effectively address this concern.\nAnalysis of efficiency. To further validate the computational efficiency of our period patching approach, we conduct experiments comparing the Multiply-Accumulate Operations (MACs) (Cao et al., 2022) of our PDF with two other patch-based methods across various look-back windows\nt \u2208 {336, 512, 720, 960} and prediction length T \u2208 {96, 192, 336, 720}. The results are summarized in Table 3. Overall, the MACs for PDF reduced by 34.64% compared to PatchTST and 74.38% compared to Crossformer. For a fixed look-back window t, the increase in MACs for PDF corresponding to the growth in prediction length T typically resides in the magnitude of millions, whereas for PatchTST and Crossformer, it is in the magnitude of gillions. The same observation is noted when keeping the prediction length constant and increasing the size of the look-back window. In extreme cases, specifically for ETTh1 with t = 960 and T = 720, PDF demonstrated superior lightweight performance, with reductions in MACs of 54.12% and 99.71% compared to PatchTST and Crossformer, respectively."
        },
        {
            "heading": "4.3 ABLATION STUDIES",
            "text": "Convolution Module. To investigate the impact of convolution in short-term variations modeling, we conduct a study comparing the following three cases: 1) Parallel Convolution; 2) Sequential Convolution; 3) Without Convolution. We perform these comparisons in four datasets. The results in Table 4 show that parallel convolution consistently outperforms its sequential counterpart, an advantage possibly stemming from the training challenges posed by deeper networks in serial architectures. Interestingly, models without convolution yield better results than those using sequential convolution, highlighting the drawbacks of overly deep serial networks. Furthermore, when compared to the model without convolution, the parallel approach achieves notable performance improvements on datasets with weaker periodicity, demonstrating its effectiveness in preserving shortterm information without increasing network depth. The observed degradation in performance for datasets with strong periodicity, such as Traffic, underscores the necessity of placing emphasis on the long-term variations across periods.\nVariations Aggregation Method. We explore two methods for aggregating the outputs of multiple DVMBs within the variations aggregation block: 1) Concat: Concatenate the outputs of all DVMBs\nand map them through linear projection; 2) Mean: Compute the average outputs of all DVMBs. The experimental results of these two aggregation strategies are presented in Table 5, which shows that the Concat operation generally has better performance than the Mean operation."
        },
        {
            "heading": "4.4 COMPUTATIONAL COMPLEXITY ANALYSIS",
            "text": "Table 6 compares the theoretical complexity per layer across different Transformer-based models. The complexity of the encoder layer in the original Transformer is O(t2). Subsequent works manage to reduce the complexity of the encoder layer to O(t log t) or even O(t). While the patch-based approaches retain quadratic complexity, the introduction of the patch length p makes O(( tp )\n2) favorable over O(t) when t is not excessively large. Notably, expect for PDF, all existing Transformerbased methods have the complexities of an encoder layer tied to the length of the look-back window t. The computational complexity of PDF is only related to the maximum decoupled periodic length pi. This ensures that even when the t is extremely large, computational costs remain low. For example, if we select the Electricity dataset with t = 105 and choose its most representative periodic pi = 24 with the patch length p = 24, our computational complexity will be significantly lower than all other methods."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "This paper introduces an efficient Periodicity Decoupling Framework (PDF) for long-term series forecasting. The PDF captures both short- and long-term temporal variations in 2D spaces. The approach involves breaking down complex 1D time series using a multi-periodic decoupling block (MDB) based on periodicity. Additionally, a dual variations modeling block (DVMB) is proposed to learn short- and long-term variations from the decoupled 2D series in parallel. Compared to previous methods that only model 1D temporal variations, our PDF performs better by effectively extracting both short- and long-term variations. Experiments on real-world datasets demonstrate the superior forecasting performance and computational efficiency over other state-of-the-art methods."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported in part by the National Key Research and Development Program of China, under Grant 2022YFF1202104, National Natural Science Foundation of China, under Grant (62302309, 62171248), Shenzhen Science and Technology Program (Grant No. JCYJ20220818101014030, JCYJ20220818101012025, WDZC20231128114058001), Open Fund of National Engineering Laboratory for Big Data System Computing Technology (Grant No. SZUBDSC-OF2024-23), and Swift Fund Fintech Funding 2023."
        }
    ],
    "year": 2024
}