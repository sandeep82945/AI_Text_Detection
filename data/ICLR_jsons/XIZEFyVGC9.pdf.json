{
    "abstractText": "Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method DAMA significantly decreases bias as measured by diverse metrics while maintaining the model\u2019s performance on downstream tasks. We release code for our method and models, which retrain LLaMA\u2019s state-of-the-art performance while being significantly less biased.1",
    "authors": [],
    "id": "SP:ae51a63830f0e770051a381efa336ee308c4d2f7",
    "references": [
        {
            "authors": [
                "Ewoenam Tokpo",
                "Toon Calders",
                "Bettina Berendt"
            ],
            "title": "Measuring fairness with",
            "year": 2021
        },
        {
            "authors": [
                "Zih-Ching Chen",
                "Yun-Ru Lee",
                "Hung-yi Lee"
            ],
            "title": "AdapterBias: Parameter-efficient",
            "year": 2022
        },
        {
            "authors": [
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy"
            ],
            "title": "Transformer Feed-Forward Layers Are",
            "year": 2022
        },
        {
            "authors": [
                "Michael Gira",
                "Ruisu Zhang",
                "Kangwook Lee"
            ],
            "title": "Debiasing Pre-trained Language Models via Effi",
            "year": 2021
        },
        {
            "authors": [
                "Yue Guo",
                "Yi Yang",
                "Ahmed Abbasi"
            ],
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "venue": "Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Anne Lauscher",
                "Tobias Lueken",
                "Goran Glava\u0161"
            ],
            "title": "Sustainable modular debiasing of language models. In Findings of the Association for Computational Linguistics: EMNLP",
            "venue": "Punta Cana, Dominican Republic,",
            "year": 2021
        },
        {
            "authors": [
                "Tomasz Limisiewicz",
                "David Mare\u010dek"
            ],
            "title": "Don\u2019t forget about pronouns: Removing gender bias in language models without losing factual gender information",
            "venue": "In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Kaiji Lu",
                "Piotr Mardziel",
                "Fangjing Wu",
                "Preetam Amancharla",
                "Anupam Datta"
            ],
            "title": "Gender Bias in Neural Natural Language Processing, pp. 189\u2013202",
            "venue": "URL https://doi. org/10.1007/978-3-030-62077-6_14",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and Editing Factual Associations in GPT",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "Arnab Sen Sharma",
                "Alex J. Andonian",
                "Yonatan Belinkov",
                "David Bau"
            ],
            "title": "MassEditing Memory in a Transformer",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher"
            ],
            "title": "Pointer sentinel mixture models, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal"
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D Manning"
            ],
            "title": "Fast model editing at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy"
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Nangia",
                "Clara Vania",
                "Rasika Bhalerao",
                "Samuel R. Bowman"
            ],
            "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Roberto Navigli",
                "Simone Conia",
                "Bj\u00f6rn Ross"
            ],
            "title": "Biases in large language models: Origins, inventory, and discussion",
            "venue": "J. Data and Information Quality,",
            "year": 2023
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Direct and indirect effects",
            "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,",
            "year": 2001
        },
        {
            "authors": [
                "Leonardo Ranaldi",
                "Elena Sofia Ruzzetti",
                "Davide Venditti",
                "Dario Onorati",
                "Fabio Massimo Zanzotto"
            ],
            "title": "A Trip Towards Fairness: Bias and De-biasing in Large Language Models",
            "venue": "CoRR, abs/2305.13862,",
            "year": 2023
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Michael Twiton",
                "Yoav Goldberg",
                "Ryan Cotterell"
            ],
            "title": "Linear Adversarial Concept Erasure",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Jason Naradowsky",
                "Brian Leonard",
                "Benjamin Van Durme"
            ],
            "title": "Gender Bias in Coreference Resolution",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Gabriel Stanovsky",
                "Noah A. Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Evaluating gender bias in machine translation",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Katrin Schulz"
            ],
            "title": "Undesirable biases in nlp: Averting a crisis",
            "year": 2023
        },
        {
            "authors": [
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang"
            ],
            "title": "Gender Bias",
            "year": 2023
        },
        {
            "authors": [
                "18653/v1/n18-2003. Ran Zmigrod",
                "S.J. Mielke",
                "Hanna M. Wallach",
                "Ryan Cotterell"
            ],
            "title": "Counterfactual Data Aug",
            "year": 2003
        },
        {
            "authors": [
                "Zhao"
            ],
            "title": "mechan reception person gre customer",
            "year": 2018
        },
        {
            "authors": [
                "C TECHNICAL DETAILS C"
            ],
            "title": "LANGUGE GENERATION BIAS EVALUATION DATASET Prompt templates selection. Lu et al. (2020a) proposed several prompt templates for testing gender bias of professions. We filtered out some of them because we observed some verbs included in the templates are highly biased toward one of the genders",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models have a large capacity for learning linguistic and factual information from training data, but they are prone to capture unwanted biases. It has been shown that LLMs are, to a varying extent, gender biased (Stanczak & Augenstein, 2021; Blodgett et al., 2020; van der Wal et al., 2023; Nadeem et al., 2021; Nangia et al., 2020; Limisiewicz & Marec\u030cek, 2022). This bias is manifested by relying on a spurious correlation between seemingly gender-neutral expressions or words and specific gender. For instance, language models tend to ascribe stereotypical gender to certain practitioners, such as \u201cmale mechanics\u201d or \u201cfemale cleaners\u201d (Lu et al., 2020b). In many tasks, the models also show uneven performance for the test examples involving different gender contexts.\nThis work analyzes the LLaMA family of models (Touvron et al., 2023). These openly available models obtain state-of-the-art performance on a variety of downstream tasks. We focus specifically on the gender bias present in these models, but our method is applicable to other types of bias. We specifically ask: 1) Can we identify evidence of gender bias in LLaMA? Specifically, do they associate professional names with their stereotypical gender? 2) Can we identify which components of the model store the gender-biased representation? 3) Can we edit the model\u2019s weights to decrease the bias while preserving its performance on other tasks?\nTo answer the first question, we check the LLaMA performance on popular tests for gender bias: WinoBias (Zhao et al., 2018) and StereoSet (Nadeem et al., 2021). We introduce an interpretable metric that evaluates bias on the original language generation task. To answer the second question, we perform causal tracing (Vig et al., 2020; Meng et al., 2022a). We monitor changes in the distribution of predictions when the stereotypical representation is revealed only in one of the components, such as MLP (multilayer perceptron) or attention layer. Following the terminology of Pearl (2001), we call such component gender bias mediator. To tackle the last question, we introduce \u201cDebiasing Algorithm through Model Adaptation\u201d. In DAMA, we edit bias-vulnerable feed-forward layers by multiplying linear transformation weights by the orthogonal projection matrix similar to Ravfogel et al. (2022). Our results show that with directed changes in model weights, we can reduce gender bias substantially while having only a minimal impact on the model\u2019s performance. Specifically, we monitor performance changes in language modeling (measured by perplexity) and in four downstream tasks.\n1The code available at: anonymous.4open.science/r/DAMA-8C70\nOur contributions are as follows: We evaluate gender bias in LLaMA models and introduce a novel, transparent metric for quantifying bias directly in language generation. Most importantly, we propose DAMA, a method for editing weights of the bias mediator to significantly reduce gender bias in three different tasks without sacrificing performance across unrelated tasks. This is an improvement over prior methods that were focused on one type of bias manifestation Ranaldi et al. (2023) or were not tested for preserving language understanding capabilities of the model Lauscher et al. (2021); Gira et al. (2022)."
        },
        {
            "heading": "2 METHODOLOGY AND EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "2.1 LLAMA MODELS",
            "text": "The LLaMA models are causal language models following Transformer decoder architecture (Vaswani et al., 2017). The LLaMA family contains models with 7B, 13B, 30B, and 65B parameters. The original paper (Touvron et al., 2023) presented state-of-the-art results on multiple downstream tasks, which we also use for evaluation.\nIn our implementation, we used the model checkpoint accessible through the Huggingface library huggingface.co. Due to the large size of the models, we used half-precision weights, which we observed to have no significant impact on the results."
        },
        {
            "heading": "2.2 GENDER BIAS EVALUATION IN LANGUAGE GENERATION",
            "text": "To better understand gender bias in language generation, we construct our dataset of prompts and an interpretable diagnostic measure.\nWe use the set of professions chosen and annotated by Bolukbasi et al. (2016).2 Each profession was assigned two scores: factual score xf (originally called definitionality) and stereotypical score xs. They define how strongly the words are connected with the male or female gender respectively through semantically or through stereotypical cues. By conventions, scores range from \u22121 for female-associated words to 1 for male ones.3 We fill the proposed profession words in the prompts of the following structure:\nX = \u201cThe plumber was\u2019t paid for the job because...\u201d\nThe plumber is, by definition, a gender-neutral word (xf = 0.1) and associated with the male gender by a stereotypical cue (xs = 0.8). We measure the probabilities for gendered prediction for\n2The data is available at: https://github.com/tolga-b/debiaswe/blob/master/data/ professions.json\n3We use positive values for male gender following the original paper. This is only an arbitrary choice, and switching polarities wouldn\u2019t affect this analysis. Importantly, we do not intend to ascribe negative connotations to any of the genders.\na given prompt PM (o|X). For that purpose, we use pronouns o+ = \u201che\u201d and o\u2212 = \u201cshe\u201d, as they are probable continuations for given prompts.\nSubsequently for each prompt, we compute empirical score y = PM (o+|X) \u2212 PM (o\u2212|X). To estimate the relationship between the observed score and annotated ones xs and xf , we construct a linear model:\ny = as \u00b7 xs + af \u00b7 xf + b0 (1)\nThe linear fit coefficients have the following interpretations: as is an impact of stereotypical signal on the model\u2019s predictions; af is an impact of the factual (semantical) gender of the word. Noticeably, y, xs, and xf take the values in the same range. The slope coefficient tells how shifts in annotated scores across professions impact the difference in prediction probabilities of male and female pronouns.\nThe intercept b0 measures how much more probable the male than the female pronouns are when we marginalize the subject. We provide the details on the prompt selection and test train splits in Appendix C."
        },
        {
            "heading": "2.3 OTHER GENDER BIAS INDICATORS",
            "text": "We also evaluate other well-established methods of evaluating gender bias manifestations in two downstream tasks:\nWinoBias Zhao et al. (2018) present the dataset containing a WinoGrad scheme (Levesque et al., 2011) examples. Each example contains two gender-neutral profession names and gendered pronouns. The task is to identify the coreference link between the pronouns and the correct professional. The dataset consists of two parts: pro-stereotypical, where coreference links to a profession name with stereotypical gender matching the gender of the pronoun; in anti-stereotypical examples, the profession\u2019s stereotypically assumed gender is different from the gender of the pronouns. An example from the anti-stereotypical subset: \u201cThe nurse notified the patient that his shift would be ending in an hour.\u201d4\nWe evaluate the general accuracy of coreference resolution Acc. Furthermore, we introduce two measures of bias: \u2206S: the difference between the accuracy of pro and anti-stereotypical examples, and \u2206G: the difference between the accuracy between examples containing male and female pronouns. These definitions are analogical to the ones introduced by Stanovsky et al. (2019) for the evaluation of gender bias in machine translation.\nStereoSet Nadeem et al. (2021) propose a dataset for measuring stereotypical biases on four domains. We consider only gender domain and intrasentence task. This task contains 765 sentences, each with a gap and three possible completions: stereotypical, anti-stereotypical, and meaningless. E.g., a sentence Girls tend to be more than boys. and three completions soft, determined, and fish. The authors propose three evaluation measures: 1) lms \u2013 the percentage of sentences where the model prefers the meaningful over the meaningless completion; 2) ss \u2013 the percentage of sentences where the model prefers the stereotypical over the anti-stereotypical completion; and 3) icat score that combines the previous two: icat = lms \u00b7min(ss, 100 \u2212 ss)/50. Note that typically lower ss scores refer to less biased models since they are closer to 50."
        },
        {
            "heading": "2.4 LANGUAGE MODELING",
            "text": "To evaluate the performance of the model\u2019s pre-training task, we measure perplexity on the Wikipedia 103 corpus Merity et al. (2016) available through the HuggingFace.\n4In this example, the coreferential link relies on semantics, while in other instances, coreference can be resolved solely through syntax."
        },
        {
            "heading": "2.5 DOWNSTREAM TASKS",
            "text": "We have selected three datasets that measure common sense reasoning and language understanding to evaluate the possible performance loss after altering the model:\nOpenBookQA (OBQA) (Mihaylov et al., 2018) The test set contains 500 multiple-choice questions aimed at combining science facts with common knowledge.\nAI2 Reasoning Challenge (ARC) (Clark et al., 2018) This dataset contains natural science questions authored for use on standardized tests. It is partitioned into a Challenge Set (1172 test questions) and an Easy Set (2376 test questions).\nMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) The test set contains 14 042 questions on 57 topics, including math, law, or social sciences.\nThe former two tasks are evaluated in a zero-shot regime. In the MMLU, we provide five in-context examples. In all evaluations, we followed closely the original setting of Touvron et al. (2023)."
        },
        {
            "heading": "3 BIAS EVALUATION AND CAUSAL TRACING",
            "text": ""
        },
        {
            "heading": "3.1 EXPERIMENTS",
            "text": "Bias Evaluation We assess gender bias in LLaMA by employing the linear model outlined in Section 2.2. We compare the linear coefficients: the larger the coefficient, the more the model is biased. We also measure the bias scores for the WinoBias and StereoSet datasets.\nCausal Tracing To identify the components storing gendered associations, we perform causal tracing for gender bias in text generation. We use a similar methodology as in Meng et al. (2022a). For each test prompt, (1) we perform a \u201cclean run\u201d and collect all the activations at all layers and tokens; (2) we perform a \u201ccorrupted run\u201d by adding noise to the tokens of the profession (details in Appendix C ); (3) we perform \u201ccorrupted runs\u201d with restoration: each time, we restore the activations from the clean run of each output of MLP at one particular layer and token. For each layer l, token position i, and a prompt X we compute the score yl,i(X) = Pl,i(o+|X) \u2212 Pl,i(o\u2212|X). By fitting the linear model (Equation 1) across all the prompts X , we get the as and af scores for each layer l and token position i. Following Meng et al. (2022b), we aggregate token positions into six groups shared across the whole dataset: first, middle, last subject token, first subsequent token, further tokens, and the last token."
        },
        {
            "heading": "3.2 RESULTS",
            "text": "Bias Evaluation We show the coefficient of the linear model in Table 1. We see that the linear model proposed by us is moderately well fitted for all sizes of LLaMA models R2 > 0.35. For all sizes, the factual coefficient is higher than the stereotypical one. The models are more influenced by semantical than stereotypical cues (af > as). Also, we observe a positive intercept in all cases, showing that LLaMA models are more likely to predict male than female pronouns.\nSimilarly, other metrics confirm that LLaMA models are biased in coreference resolution and sentence likelihood estimation. In WinoBias, we observe that the bias stemming from stereotypes \u2206S is more prominent than the accuracy difference between examples with male and female pronouns \u2206G.\nCausal Tracing In Figure 2, we observe the indirect effect of MLPs in each layer and token position of the 7B model. The best fit is obtained for the representation in the lower layers (0-5) at the subject position and mid-upper layers (18 -25) at the last position. In the search for stereotypically biased components, we direct our attention to the mid-upper layers because they appear to covey less signal about factual gender. We also expect that the information stored in those MLP layers is more likely to generalize to unseen subjects. Interestingly, the last layers manifest weak negative slope coefficients, suggesting that these MLPs tend to counter the bias of the models.\nIn Figure 4, we show the results of casual tracing for attention and the whole layer. For those components, the high indirect effects are distributed more extensively across both token positions and layers, indicating that they primarily reflect bias from the MLPs. For larger models, we observe analogous patterns shifted according to the total layer count (see Appendix B for further details)."
        },
        {
            "heading": "4 DEBIASING ALGORITHM THROUGH MODEL ADAPTATION",
            "text": "We introduce the algorithm that decreases bias in language models by directly editing the model weights. This section describes our method based on projection-based intervention on selected layers, called DAMA. Further, we provide theoretical and empirical backing for the method\u2019s effectiveness."
        },
        {
            "heading": "4.1 OBTAINING STEREOTYPE KEYS AND GENDERED VALUES",
            "text": "Following the convention from Geva et al. (2021), we treat MLP layers as memory units mapping specific input key representations to value representations. Our focus lies in understanding how these layers map stereotypical keys to gendered values. As our choice of keys, we take prompts introduced in Section 2.2, which carry stereotypical signal. The values are the embeddings corresponding to one of the personal pronouns (male, female, or neutral).\nTo compute the stereotypical key at lth layer, we feed the stereotypical prompt X up to l layer\u2019s feed-forward MLP (FFl) to obtain its vector representation. We, specifically, take the vector representation at the last token of the prompt. We denote stereotypical keys as u \u2208 RdFF following the convention from Figure 1a.\nTo compute the value representation corresponding to a specific gender, we employ the next-token prediction task based on the stereotypical prompt X . As possible next token, we consider one of the pronouns indicating gender (O+ = \u201che\u2032\u2032 for male, O\u2212 = \u201cshe\u2032\u2032 for female, and O0 = \u201cthey\u2032\u2032 for neutral). We use the regular cross-entropy loss and optimize the output of the lth layer\u2019s feedforward denoted V:\nvo = argmin z\u2208RdM\n\u2212 logPM [V=z](o|X) + \u03bb1DKL[PM [V=z](o\u2032|X \u2032)||PM (o\u2032|X \u2032)] + \u03bb2||z||2 (2)\nThe second part of the loss is added to preserve the model\u2019s LM capabilities for predicting the next token (o\u2032) given general (not-biased) prompts (X \u2032). The last summand is L2 regularization. We use gradient descent with 20 iterations to obtain a value vector for each of the pronouns vo \u2208 RdM ."
        },
        {
            "heading": "4.2 OBTAINING PROJECTION ON STEREOTYPE SUBSPACE WITH PLS",
            "text": "To identify the stereotype representation subspace, we concatenate value vectors for each pronoun (male, neutral, and female) across all prompts to obtain gendered value matrices V+, V0, and V\u2212. The gendered value matrices are normalized by subtracting the mean calculated across all three pronouns for a given prompt. Analogically, we concatenate key vectors. for all prompts into one matrix U . Then, we multiply it by the feed-forward\u2019s output matrix denoted WFF,out,l:\nWFF,out,l \u00b7 U \u2192 U\u0302 (3)\nWe concatenate V+, V0, and V\u2212 together and concatenate U\u0302 three times. We use the Partial Least Squares algorithm to identify the linear mapping B1 maximizing correlation between stereotypical keys [U\u0302 , U\u0302 , U\u0302 ] and gendered values [V+, V0, V\u2212]:\n[V+, V0, V\u2212] \u2248PLS B1 \u00b7 [U\u0302 , U\u0302 , U\u0302 ] +B0 (4)\nBy definition of PLS, B1 identifies the stereotypical directions most correlated with gendered values.5 Therefore, we compute the matrix projecting representation on subspace orthogonal to the one spanned by dc first columns of B1 to nullify the stereotypical signal. For brevity, we denote the trimmed matrix as Bdc1 = B1[:, : dc]. The projection is obtained with the equation:\nP = I\u2212 Pc = I\u2212Bdc1 (BdcT1 Bdc1 )\u22121BdcT1 (5)\nFinally, we perform the model editing by multiplying lth MLP feed-forward matrix WFF,out,l by the projection matrix P as depicted on the right side of Figure 1. Our algorithm DAMA is based on iterative computation and applying projections to feed-forwards of multiple subsequent MLP layers. It changes neither the Transformer model\u2019s architecture nor parameter sizes, as the result of matrix multiplication is of the same dimensionality as the original feed-forward matrix.\n5Matrix B0 can be used to normalize the value matrix. However, we have noticed that its loadings become nearly zero due to the earlier normalization step discussed in this section."
        },
        {
            "heading": "4.3 THEORETICAL PERSPECTIVE",
            "text": "In this section, we show theoretical guarantees that multiplying linear feed-forward matrix WFF,out,l by projection matrix P will be the optimal mapping between keys (U ) and values (V ), fulfilling that WFF,out,l \u00b7 U is orthogonal to the guarded bias subspace C. Theorem 1. Assume that we have a linear subspace C \u2286 Ro. Given a n-element key matrix U \u2208 Ri\u00d7n a value matrix V \u2208 Ro\u00d7n, we search a mapping matrix W \u2208 Ro\u00d7i minimizing least squares problem and satisfying \u2200ni=1Wui \u22a5 C. Specifically, we solve:\nW\u0302 = argmin W\n||WU \u2212 V ||2F such that \u2200ni=1Wui \u22a5 C\nThis equation is solved by: W\u0302 = (I\u2212 Pc)V UT (UUT )\u22121\nWhere Pc is a projection matrix on a subspace C.\nThe proof of the theorem is in Appendix A. Noteworthy V UT (UUT )\u22121 solves the regular mean square error problem of mapping prompt keys to values corresponding to the model\u2019s output. Due to gradient optimization in the model\u2019s pre-training, we can assume that in general case WFF,out,l = V U\nT (UUT )\u22121. Thus, the application of projections would break the correlation between stereotypical keys and gendered values without affecting other correlations stored by the MLP layer."
        },
        {
            "heading": "4.4 EMPIRICAL PERSPECTIVE",
            "text": "Effectivness We apply DAMA to MLPs in approximately one-third of the model\u2019s upper layers (in LLaMA 7B layers 21 - 29 out of 32 with projection dimensionality dc = 256). In the previous section, we have shown that those layers are the most prone to stereotypical bias. We check the impact of DAMA on bias coefficients of linear approximation (see Section 2.2) and LM perplexity. Furthermore, we evaluate the modified model on a set of diverse downstream tasks described in Section 2. In the choice of tasks, we focused both on gender bias (WinoBias, StereoSet) and language understanding evaluation (ARC-C, ARC-E, OBQA. MMLU) We compare the method with a similar model editing method MEMIT (Meng et al., 2023). In MEMIT, our chosen objective is to adapt the model to predict a random pronoun when presented with a biased prompt X .\nChoice of Layers and Dimensionality We analyze how the results vary depending on the number of layers selected for debiasing Due to the iterative character of intervention, we always start editing at the fixed layer (22 in LLaMA 7B) and gradually add subsequent layers. Further, we check the effect of the number of projection dimensions (dc) in the power sequence from 32 to 1024.\nScaling Lastly, we examine the algorithm\u2019s performance for increasing scale: for LLaMA 13B, 30B, and 65B."
        },
        {
            "heading": "4.5 RESULTS",
            "text": "Effectivness DAMA effectively decreases the gender bias of the model while preserving its performance on other tasks, as seen in Table 1. Our algorithm effectively decreased the bias manifested in language generation for a set of unseen professions.6\nMorover, DAMA significantly mitigates bias in StereoSet and WinoBias. In the latter task, general accuracy decreases, presumably due to the weakening of the stereotypical cue contributing to correct predictions in numerous test examples.\nOur observations confirm that MLP layers contain stereotypical correlations responsible for multiple manifestations of bias. In contrast to DAMA, MEMIT has a minor effect on bias measures. We think it is because it is aimed to alter information specific to key-value pairs selected for intervention.\n6In Table 3, we also show examples of next token probabilities in the original and debiased model.\nTherefore, the intervention performed on the training set of professions does not generalize to unseen professions or other types s of gender bias.\nIn Table 2, we observe that the algorithm causes a slight deterioration in general language modeling measured by perplexity on Wikipedia texts. It has a minor reflection in performance for downstream tasks. The altered model achieves a slightly lower score, yet differences are statistically significant only for one task (ARC-E). Therefore, we can conclude that DAMA does not impact the model\u2019s ability in question-answering tasks.\nChoice of Layers and Dimensionality In Figure 3, we observe that the choice of the number of layers for debiasing and the dimensionality of projection affect both parameters. Expanding the depth (number of layers) and width (dimensions) of the intervention increases the insensitivity of debiasing, i.e., decreases as and b coefficients and negatively impacts perplexity. Interestingly, we observe a negative impact on both measured aspects when applying DAMA on the two last layers of the models. As noted in Section 3.1, the MLPs in those layers tend to counter bias in the original model.\nScaling We performed a coarse hyperparameter search for sensitive parameters of DAMA: number of layers and dimensionalities of the projections. Our analysis showed that the algorithm should be applied to the mid-top layers, starting from the 65th percentile to the 93rd percentile of layers ordered from input to output (the exact values are presented in Table 4).\nWe have achieved a notable reduction in bias scores for all models. Noticeably, although we do not observe the shared pattern for the metrics across different model sizes, the improvements brought by DAMA are consistent. Moreover, the perplexity and downstream performance of the original models do not deteriorate and even slightly improve for some settings."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "Our approach is connected to previous methodologies in model editing Meng et al. (2022b) and bias mitigation (Ravfogel et al., 2022). The important contribution of our work is the introduction of bias evaluation schema directly in language generation. To answer our first question, we show that all LLaMA models are biased in this aspect.\nUsing the evaluation scheme closely connected to the model\u2019s pre-training task had two fundamental benefits. Firstly, it allowed us to perform a causal analysis of model components. The analysis allowed us to answer our second research question. We identified mid-upper MLP layers as the most apparent mediator of gender bias in the model. Secondly, we could perform debiasing adaptation directly on the model\u2019s weights without using a proxy task (Ravfogel et al., 2022) or fine-tuning on\nlimited data that often deteriorates the model\u2019s general performance (Gira et al., 2022). Answering the third question, we succeeded in significantly reducing bias with a minor impact on general performance.\nThe proposed algorithm generalizes the applicability of model-editing (Meng et al., 2022a;b; Mitchell et al., 2022; De Cao et al., 2021) to the case of modifying general dataset artifacts instead of the information specific to particular examples. Although we focused on gender bias, the method can be easily generalized to other types of bias or unwanted correlations. Additionally, it is applicable not only to LLaMA but to a broad family of transformer-based causal language models.\nFuture Work We plan to improve the method of finding projection matrices, possibly using a convex search (Ravfogel et al., 2022). We aim to investigate further the ranges of layers and dimensions that convey bias to effectively apply DAMA on larger models and other models. Lastly, we consider further investigating bias in other languages, both in multilingual LM and machine translation settings. We are particularly interested in how our approach can be generalized for morphologically rich languages with more ubiquitous gender marking than English Zmigrod et al. (2019)."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Measuring bias in language model Gender bias manifests in language models through various mechanisms measured by distinct of metrics, often leading to contrasting findings (Delobelle et al., 2022; van der Wal et al., 2023). One common approach to operationalize bias is to compare the probability assigned by a model to sentences conveying neutral and stereotypical information (Nangia et al., 2020; Nadeem et al., 2021). Probability-based methods were criticized for being sensitive to the choice of stereotypical cues (Blodgett et al., 2021) and are hard to apply to auto-regressive models such as LLaMA.\nAnother popular method to estimate gender bias is based on the coreference task, where personal pronouns should be assigned to the correct antecedent in Winograd scheme (Levesque et al., 2011). The task is complicated by including two potential antecedents, one of which is stereotypically associated with a specific gender. The analysis of such examples shows that models struggle with solving non-stereotypical links (Rudinger et al., 2018; Zhao et al., 2018).\nDebiasing methods Similarly to the number of bias metrics, researchers proposed various debiasing methods (Stanczak & Augenstein, 2021; Savoldi et al., 2021). The common observation is that models learn the biases from training data (Navigli et al., 2023). Therefore, one approach is to curate the model\u2019s training corpus or expose it to gender-balanced data in fine-tuning step (Lu et al., 2020b; Ranaldi et al., 2023). Alternatively, the model can be fine-tuned on a dataset of a balanced number of examples for each gender (Guo et al., 2022; Zmigrod et al., 2019).\nAnother set of approaches is to make targeted changes to the model\u2019s parameters. The changes can be performed in many ways. Lauscher et al. (2021); Gira et al. (2022); Xie & Lukasiewicz (2023) fine-tune specific parts of the models most prone to convey biases. Alternative approaches include a null-space projection of latent states (Ravfogel et al., 2022), causal intervention (Vig et al., 2020), or model adapters (Fu et al., 2022). DAMA belongs to this category of methods, merging aspects of causal intervention, model editing, and signal projection techniques."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "We introduced Debiasing Algorithm through Model Adaptation based on guarding stereotypical gender signals and model editing. DAMA is performed on specific modules prone to convey gender bias, as shown by causal tracing. Our novel method effectively reduces gender bias in LLaMA models in three diagnostic tests: generation, coreference (WinoBias), and stereotypical sentence likelihood (StereoSet). The method does not change the model\u2019s architecture, parameter count, or inference cost. We have also shown that the model\u2019s performance in language modeling and a diverse set of downstream tasks is almost unaffected."
        },
        {
            "heading": "A THEORETICAL BACKGROUND",
            "text": "In this section, we provide additional theoretical background with proofs. First, we present a theorem that will help prove Theorm 1 from the main text of the paper.\nTheorem 2 (Ordinary Least Square Problem). Given a n-element key matrix U \u2208 Ri and a value matrix V \u2208 Ro\u00d7n, we search for a mapping matrix W \u2208 Ro\u00d7i minimizing least squares problem. Specifically, we solve:\nW\u0302 = argmin ||WU \u2212 V ||2F\nThis equation is solved by:\nW\u0302 = V UT (UUT )\u22121\nThe proof for the theorem can be found, e.g., in Goldberger et al. (1964). Now we are ready to provide a proof for Theorem 1.\nProof. Without loss of generality, we consider a case where n = 1, i.e., U and V are column vectors. For clarity, we will denote those vectors u \u2208 Ri and v \u2208 Ro respectively. Therefore, we aim to solve an equation:\nW\u0302 = argmin W\n||Wu\u2212 v||2F such that Wu \u22a5 C (6)\nNote that we can substitute the Furbenious norm with the Euclidean norm and decompose vector v into the sum of two orthogonal vectors.\n||Wu\u2212 v||2F = ||Wu\u2212 v||2 = ||Wu\u2212 (I\u2212 P )v \u2212 Pv||2 (7)\nWe infer that Wu\u2212 (I\u2212P )v \u22a5 C from a) Wu \u22a5 C (from 6); and b) (I\u2212P ) \u22a5 C as P is projection matrix on C. Moreover, from the properties of linear projection, we have Pv \u2208 C. We note thus that Wu\u2212 (I\u2212 P )v \u22a5 Pv. Now, let\u2019s get back to Pythagoras Theorem saying that for pair of orthogonal vectors \u2212\u2192a \u22a5 \u2212\u2192b , we have ||\u2212\u2192a ||2 + ||\u2212\u2192b ||2 = ||\u2212\u2192a +\u2212\u2192b ||2. We can apply this theorem to 6 by taking Wu\u2212 (I\u2212 P )v as \u2212\u2192a and Pv as \u2212\u2192b . Thus, we can write:\n||Wu\u2212 (I\u2212 P )v \u2212 Pv||2 = ||Wu\u2212 (I\u2212 P )v||2 + ||Pv||2 (8)\nIn argmin notation, we can omit the second part of the formula because it doesn\u2019t depend on W\nW\u0302 = argmin W ||Wu\u2212 v||2 = argmin W ||Wu\u2212 (I\u2212 P )v||2 (9)\nNow, we can apply the same steps to all the columns in U = [u1, . . . , un] and V = [v1, . . . , vn], to obtain:\nW\u0302 = argmin W\n||WU \u2212 (I\u2212 P )V ||2F (10)\nBased on Theorm 2 it is solved by W\u0302 = (I\u2212P )V UT (UUT )\u22121. We can easily obtain this result by substituting V by (I\u2212 P )V in the theorem. Lastly, it can be shown that for any vector x \u2208 Ri we have W\u0302x \u22a5 C from the fact that applying P projection to W\u0302x aways gives null vector:\nPW\u0302x = P (I\u2212 P )V UT (UUT )\u22121 = (P \u2212 P )V UT (UUT )\u22121 =\u2192 0 (11)"
        },
        {
            "heading": "B SUPLEMENTARY RESULTS",
            "text": "B.1 CAUSAL TRACING\nThe Figures 4, 5, 6, and 10 present causal tracing results for other types of components than MLP: attention and whole layers, and larger LLaMA models. For those components, the high indirect effects are distributed more extensively across both token positions and layers, indicating that they primarily reflect bias from the MLPs.\nFor larger models, we observe analogous patterns shifted according to the total layer count. Overall, gender bias is most prominent in MLPs located in layers up to the 15th and ranging from the 65th to 93rd percentile of the layers ordered from the input to the output.\nB.2 DISTRIBUTION OF PREDICTIONS IN LANGUAGE GENERATION\nIn Table 3, we present a comparison of the softmax probabilities associated with the most likely tokens predicted by the model before and after the DAMA intervention. Notably, we notice that following model adaptation, there is a more balanced distribution of pronouns, with male and female pronouns frequently changing positions in the ordering. However, when it comes to the WinoBias coreference prompts, we observe a varied degree of success in the effectiveness of the intervention.\nB.3 HYPERPARAMETER CHOICE FOR DAMA\nTable 4 presents the width (dimensionality of projection) and depth (number of layers) chosen in LLaMA models of all sizes. The choice of layer numbers matches the observations from causal tracing. We further backed the parameter selection by a limited parameter search, which results are presented in Figures 8, 9, and 10"
        },
        {
            "heading": "C TECHNICAL DETAILS",
            "text": "C.1 LANGUGE GENERATION BIAS EVALUATION DATASET\nPrompt templates selection. Lu et al. (2020a) proposed several prompt templates for testing gender bias of professions. We filtered out some of them because we observed some verbs included in the templates are highly biased toward one of the genders. In Figure 11, we observe the average probability differences between the prediction of he and the prediction of she. Some verbs such as \u2018yelled\u2019, \u2018was promoted\u2019, \u2018was fired\u2019, or \u2018slept\u2019 are highly biased towards males. On the other hand, verbs such as \u2018wanted\u2019, \u2018cried\u2019, \u2018desired\u2019, or \u2018stayed up\u2019 are only very little biased towards males. Given the general skewness of the model towards predicting male pronouns, we can say these verbs\nare rather female-related. For the evaluation, we chose the templates whose averaged difference between the prediction of he and she is lower than 0.8%. Thus we are excluding the prompts \u2018slept because\u2019, \u2018was fired because\u2019, \u2018was promoted because\u2019, \u2018yelled that\u2019, and \u2018yelled because\u2019.\nTest train split. For evaluation, we select a test set consisting of all professions with semantically defined gender (where |xf | > 0.25). We also include 20% of the other professions, to be able to evaluate the impact of both semantic and stereotypical gender.\nThe remainder of the professions are assigned to the train set. Noticeably, the trainset doesn\u2019t contain a profession with a semantically defined gender. It is a deliberate choice because we want to preserve factual gender signals in the model debiased using training data. For both splits, we use all selected prompt templates.\nC.2 CORRUPTING REPRESENTATION\nIn step (2) of the causal tracing, we need to obfuscate the tokens in the profession\u2019s words. We use the same methodology as in Meng et al. (2022a). We add random gaussian noise \u03f5 \u223c N (0, \u03bd) to the token embeddings h(0)i := h 0 i + \u03f5 for each token i in the profesion word. The parameter was set \u03bd to be three times larger than the empirical standard deviation of the embeddings of professions. As shown in Figure 12, the multiplicative constant lower than three would not fully remove the stereotypical bias from the tokens. Higher values could remove too much information, e.g., the information that the subject of the prompt refers to a person.\nC.3 OPTIMIZING VALUE REPRESENTATION\nTo find the value representation, we minimize the loss given by Equation 2. We run gradient optimization for 20 steps with Adam scheduler (Kingma & Ba, 2015) and learning rate: lr = 0.5. We picked the following regularization constants: \u03bb1 = 0.0625 and \u03bb2 = 0.2."
        }
    ],
    "year": 2023
}