{
    "abstractText": "We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm only requires an equilibrium-solving oracle and an oracle that solves regularized supervised learning, and thus avoids solving constrained optimization problems within data-dependent constraints (Jin et al., 2020a; Wang et al., 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al., 2023). Moreover, the model-free version of our algorithms is the first provably efficient model-free algorithm for learning Nash equilibrium of general-sum MGs.",
    "authors": [
        {
            "affiliations": [],
            "name": "AN OPTI"
        }
    ],
    "id": "SP:a197c813b316eed9ad122c6ac931955555178418",
    "references": [
        {
            "authors": [
                "Yasin Abbasi-Yadkori",
                "D\u00e1vid P\u00e1l",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Improved algorithms for linear stochastic bandits",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Tong Zhang"
            ],
            "title": "Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Daniel Hsu",
                "Satyen Kale",
                "John Langford",
                "Lihong Li",
                "Robert Schapire"
            ],
            "title": "Taming the monster: A fast and simple algorithm for contextual bandits",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Ioannis Anagnostides",
                "Gabriele Farina",
                "Christian Kroer",
                "Andrea Celli",
                "Tuomas Sandholm"
            ],
            "title": "Faster no-regret learning dynamics for extensive-form correlated and coarse correlated equilibria",
            "venue": "arXiv preprint arXiv:2202.05446,",
            "year": 2022
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "Provable self-play algorithms for competitive reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin",
                "Tiancheng Yu"
            ],
            "title": "Near-optimal reinforcement learning with self-play",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin",
                "Huan Wang",
                "Caiming Xiong"
            ],
            "title": "Sample-efficient learning of stackelberg equilibria in general-sum games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Noam Brown",
                "Tuomas Sandholm"
            ],
            "title": "Superhuman ai for heads-up no-limit poker",
            "venue": "Libratus beats top professionals. Science,",
            "year": 2018
        },
        {
            "authors": [
                "Semih Cayci",
                "Niao He",
                "Rayadurgam Srikant"
            ],
            "title": "Linear convergence of entropy-regularized natural policy gradient with linear function approximation",
            "venue": "arXiv preprint arXiv:2106.04096,",
            "year": 2021
        },
        {
            "authors": [
                "Fan Chen",
                "Yu Bai",
                "Song Mei"
            ],
            "title": "Partially observable rl with b-stability: Unified structural condition and sharp sample-efficient algorithms",
            "venue": "arXiv preprint arXiv:2209.14990,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Chen",
                "Song Mei",
                "Yu Bai"
            ],
            "title": "Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning",
            "venue": "arXiv preprint arXiv:2209.11745,",
            "year": 2022
        },
        {
            "authors": [
                "Zixiang Chen",
                "Chris Junchi Li",
                "Angela Yuan",
                "Quanquan Gu",
                "Michael I Jordan"
            ],
            "title": "A general framework for sample-efficient function approximation in reinforcement learning",
            "venue": "arXiv preprint arXiv:2209.15634,",
            "year": 2022
        },
        {
            "authors": [
                "Zixiang Chen",
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Almost optimal algorithms for two-player zerosum linear mixture markov games",
            "venue": "In International Conference on Algorithmic Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Qiwen Cui",
                "Kaiqing Zhang",
                "Simon S Du"
            ],
            "title": "Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation",
            "venue": "arXiv preprint arXiv:2302.03673,",
            "year": 2023
        },
        {
            "authors": [
                "Varsha Dani",
                "Thomas P Hayes",
                "Sham M Kakade"
            ],
            "title": "Stochastic linear optimization under bandit feedback",
            "year": 2008
        },
        {
            "authors": [
                "Christoph Dann",
                "Mehryar Mohri",
                "Tong Zhang",
                "Julian Zimmert"
            ],
            "title": "A provably efficient model-free posterior sampling method for episodic reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Noah Golowich",
                "Kaiqing Zhang"
            ],
            "title": "The complexity of markov equilibrium in stochastic games",
            "venue": "arXiv preprint arXiv:2204.03991,",
            "year": 2022
        },
        {
            "authors": [
                "Simon Du",
                "Sham Kakade",
                "Jason Lee",
                "Shachar Lovett",
                "Gaurav Mahajan",
                "Wen Sun",
                "Ruosong Wang"
            ],
            "title": "Bilinear classes: A structural framework for provable generalization in rl",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Dylan J Foster",
                "Sham M Kakade",
                "Jian Qian",
                "Alexander Rakhlin"
            ],
            "title": "The statistical complexity of interactive decision making",
            "venue": "arXiv preprint arXiv:2112.13487,",
            "year": 2021
        },
        {
            "authors": [
                "Dylan J Foster",
                "Dean P Foster",
                "Noah Golowich",
                "Alexander Rakhlin"
            ],
            "title": "On the complexity of multi-agent decision making: From learning in games to partial monitoring",
            "venue": "arXiv preprint arXiv:2305.00684,",
            "year": 2023
        },
        {
            "authors": [
                "Sergiu Hart",
                "Andreu Mas-Colell"
            ],
            "title": "A simple adaptive procedure leading to correlated",
            "venue": "equilibrium. Econometrica,",
            "year": 2000
        },
        {
            "authors": [
                "Baihe Huang",
                "Jason D Lee",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "Towards general function approximation in zero-sum markov games",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Nan Jiang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "John Langford",
                "Robert E Schapire"
            ],
            "title": "Contextual decision processes with low bellman rank are pac-learnable",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Chi Jin",
                "Sham Kakade",
                "Akshay Krishnamurthy",
                "Qinghua Liu"
            ],
            "title": "Sample-efficient reinforcement learning of undercomplete pomdps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Jin",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Michael I Jordan"
            ],
            "title": "Provably efficient reinforcement learning with linear function approximation",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Sobhan"
            ],
            "title": "Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Yuanhao Wang",
                "Tiancheng Yu"
            ],
            "title": "V-learning\u2013a simple, efficient, decentralized algorithm for multiagent rl",
            "venue": "arXiv preprint arXiv:2110.14555,",
            "year": 2021
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Tiancheng Yu"
            ],
            "title": "The power of exploiter: Provable multi-agent rl in large state spaces",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Marc Lanctot",
                "Vinicius Zambaldi",
                "Audrunas Gruslys",
                "Angeliki Lazaridou",
                "Karl Tuyls",
                "Julien P\u00e9rolat",
                "David Silver",
                "Thore Graepel"
            ],
            "title": "A unified game-theoretic approach to multiagent reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Michael L Littman"
            ],
            "title": "Markov games as a framework for multi-agent reinforcement learning",
            "venue": "In Machine learning proceedings",
            "year": 1994
        },
        {
            "authors": [
                "Qinghua Liu",
                "Tiancheng Yu",
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "A sharp analysis of model-based reinforcement learning with self-play",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Qinghua Liu",
                "Alan Chung",
                "Csaba Szepesv\u00e1ri",
                "Chi Jin"
            ],
            "title": "When is partially observable reinforcement learning not scary",
            "venue": "arXiv preprint arXiv:2204.08967,",
            "year": 2022
        },
        {
            "authors": [
                "Qinghua Liu",
                "Csaba Szepesv\u00e1ri",
                "Chi Jin"
            ],
            "title": "Sample-efficient reinforcement learning of partially observable markov games",
            "venue": "arXiv preprint arXiv:2206.01315,",
            "year": 2022
        },
        {
            "authors": [
                "Zhihan Liu",
                "Miao Lu",
                "Wei Xiong",
                "Han Zhong",
                "Hao Hu",
                "Shenao Zhang",
                "Sirui Zheng",
                "Zhuoran Yang",
                "Zhaoran Wang"
            ],
            "title": "One objective to rule them all: A maximization objective fusing estimation and planning for exploration",
            "venue": "arXiv preprint arXiv:2305.18258,",
            "year": 2023
        },
        {
            "authors": [
                "Luke Marris",
                "Paul Muller",
                "Marc Lanctot",
                "Karl Tuyls",
                "Thore Graepel"
            ],
            "title": "Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Chengzhuo Ni",
                "Yuda Song",
                "Xuezhou Zhang",
                "Chi Jin",
                "Mengdi Wang"
            ],
            "title": "Representation learning for general-sum low-rank markov games",
            "venue": "arXiv preprint arXiv:2210.16976,",
            "year": 2022
        },
        {
            "authors": [
                "Martin L Puterman"
            ],
            "title": "Markov decision processes: discrete stochastic dynamic programming",
            "year": 2014
        },
        {
            "authors": [
                "Daniel Russo",
                "Benjamin Van Roy"
            ],
            "title": "Eluder dimension and the sample complexity of optimistic exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. nature,",
            "year": 2016
        },
        {
            "authors": [
                "Ziang Song",
                "Song Mei",
                "Yu Bai"
            ],
            "title": "When can we learn general-sum markov games with a large number of players sample-efficiently",
            "venue": "arXiv preprint arXiv:2110.04184,",
            "year": 2021
        },
        {
            "authors": [
                "Wen Sun",
                "Nan Jiang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "John Langford"
            ],
            "title": "Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches",
            "venue": "In Conference on learning theory,",
            "year": 2019
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Yi Tian",
                "Yuanhao Wang",
                "Tiancheng Yu",
                "Suvrit Sra"
            ],
            "title": "Online learning in unknown markov games",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Igor Babuschkin",
                "Wojciech M Czarnecki",
                "Micha\u00ebl Mathieu",
                "Andrew Dudzik",
                "Junyoung Chung",
                "David H Choi",
                "Richard Powell",
                "Timo Ewalds",
                "Petko Georgiev"
            ],
            "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Ruosong Wang",
                "Ruslan Salakhutdinov",
                "Lin F Yang"
            ],
            "title": "Provably efficient reinforcement learning with general value function approximation",
            "venue": "arXiv preprint arXiv:2005.10804,",
            "year": 2020
        },
        {
            "authors": [
                "Ruosong Wang",
                "Russ R Salakhutdinov",
                "Lin Yang"
            ],
            "title": "Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yining Wang",
                "Ruosong Wang",
                "Simon S Du",
                "Akshay Krishnamurthy"
            ],
            "title": "Optimism in reinforcement learning with generalized linear function approximation",
            "year": 1912
        },
        {
            "authors": [
                "Yuanhao Wang",
                "Qinghua Liu",
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation",
            "venue": "arXiv preprint arXiv:2302.06606,",
            "year": 2023
        },
        {
            "authors": [
                "Chen-Yu Wei",
                "Yi-Te Hong",
                "Chi-Jen Lu"
            ],
            "title": "Online reinforcement learning in stochastic games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Michael P Wellman"
            ],
            "title": "Methods for empirical game-theoretic analysis",
            "venue": "In AAAI,",
            "year": 2006
        },
        {
            "authors": [
                "Qiaomin Xie",
                "Yudong Chen",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "Learning zero-sum simultaneousmove markov games using function approximation and correlated equilibrium",
            "venue": "In Conference on learning theory,",
            "year": 2020
        },
        {
            "authors": [
                "Tengyang Xie",
                "Ching-An Cheng",
                "Nan Jiang",
                "Paul Mineiro",
                "Alekh Agarwal"
            ],
            "title": "Bellman-consistent pessimism for offline reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Xiong",
                "Han Zhong",
                "Chengshuai Shi",
                "Cong Shen",
                "Tong Zhang"
            ],
            "title": "A self-play posterior sampling algorithm for zero-sum markov games",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Yuan",
                "Simon S Du",
                "Robert M Gower",
                "Alessandro Lazaric",
                "Lin Xiao"
            ],
            "title": "Linear convergence of natural policy gradient methods with log-linear policies",
            "venue": "arXiv preprint arXiv:2210.01400,",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Zanette",
                "Alessandro Lazaric",
                "Mykel Kochenderfer",
                "Emma Brunskill"
            ],
            "title": "Learning near optimal policies with low inherent bellman error",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Zanette",
                "Martin J Wainwright",
                "Emma Brunskill"
            ],
            "title": "Provable benefits of actor-critic methods for offline reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhao Zhan",
                "Jason D Lee",
                "Zhuoran Yang"
            ],
            "title": "Decentralized optimistic hyperpolicy mirror descent: Provably no-regret learning in markov games",
            "venue": "arXiv preprint arXiv:2206.01588,",
            "year": 2022
        },
        {
            "authors": [
                "Wenhao Zhan",
                "Masatoshi Uehara",
                "Wen Sun",
                "Jason D Lee"
            ],
            "title": "Pac reinforcement learning for predictive state representations",
            "venue": "arXiv preprint arXiv:2207.05738,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Sham Kakade",
                "Tamer Basar",
                "Lin Yang"
            ],
            "title": "Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yulai Zhao",
                "Yuandong Tian",
                "Jason D Lee",
                "Simon S Du"
            ],
            "title": "Provably efficient policy optimization for two-player zero-sum markov games",
            "venue": "arXiv preprint arXiv:2102.08903,",
            "year": 2021
        },
        {
            "authors": [
                "Han Zhong",
                "Wei Xiong",
                "Sirui Zheng",
                "Liwei Wang",
                "Zhaoran Wang",
                "Zhuoran Yang",
                "Tong Zhang"
            ],
            "title": "A posterior sampling framework for interactive decision making",
            "year": 1962
        },
        {
            "authors": [
                "2020 Zhang et al",
                "2020 Xie et al",
                "2020 Bai et al",
                "Bai",
                "2020 Jin"
            ],
            "title": "works design the sample-efficient algorithm for two-agent zero-sum games (Wei et",
            "year": 2021
        },
        {
            "authors": [
                "Jin"
            ],
            "title": "2022b) further consider the two-player zero-sum MGs under general function approximation, and provide algorithms with a sublinear regret. Another line of research focuses on general-sum MGs with multiple players (Jin et al., 2020a; Liu et al., 2021",
            "venue": "Jin et al., 2021b; Song et al.,",
            "year": 2021
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2022b) consider learning all three equilibrium notions \u2014 NE, CCE, and CE \u2014 and their regret or sample complexity results are exponential in the number of agents. To break this exponential curse, some existing works propose decentralized algorithms for learning CCE or CE rather than NE (Jin et al., 2021b",
            "venue": "Daskalakis et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023) that build upon previous works for function approximation in the single-agent setting (Jiang et al., 2017",
            "venue": "Jin et al., 2020b; Wang et al., 2020b; Dann et al.,",
            "year": 2021
        },
        {
            "authors": [],
            "title": "2022) consider the multi-agent decoupling coefficient in the two-player zero-sum MGs, and provide the posterior sampling algorithm. However, unlike a zero-sum MG, a general-sum MG can have various equilibrium concepts, each of which aligns with a specific set of policies",
            "year": 2022
        },
        {
            "authors": [
                "setting. Chen"
            ],
            "title": "2023) generalize the complexity measure DecisionEstimation Coefficient (DEC), and learn the equilibria in model-based general-sum MGs. Ni et al. (2022) provide both a model-based algorithm and a model-free algorithm for the low-rank MGs",
            "venue": "Some previous works (Zhan et al., 2022a; Wang et al.,",
            "year": 2023
        },
        {
            "authors": [],
            "title": "log\u03a5F,\u03b4) regret for function classes with a finite multi-agent BE dimension d. It remains to see that that function classes with low multi-agent BE dimensions contain a wide range of RL problems. To this end, we prove that if the eluder dimension (Russo & Van Roy, 2013) of the function class F",
            "year": 2013
        },
        {
            "authors": [],
            "title": "regret when R = 1 (Zanette et al., 2020; Chen et al., 2022c), our result matches their results in terms of d,H and K with an extra factor |\u03a0| in the logarithmic term. Note that when the pure policy set of i-th agent",
            "year": 2022
        },
        {
            "authors": [
                "R\u03c0}. Zanette"
            ],
            "title": "2021) prove that the logarithm of cardinality of the induced covering",
            "year": 2021
        },
        {
            "authors": [],
            "title": "information-theoretic lower bound (Chen et al., 2022d) for zero-sum linear MGs. Thus, our regret matches the lower bound in terms of d, has a higher order in H compared to Xie et al. (2020) and an extra factor log |\u03a0|. Again, we can adopt the class of log-linear policies with a policy cover, which leads to log |\u03a0|",
            "year": 2020
        },
        {
            "authors": [],
            "title": "2019) provide a complexity measure \u2014 witness rank \u2014 to characterize the exploration hardness of the model-based RL problems. In the following, we extend the notion of the witness rank to MARL",
            "year": 2019
        },
        {
            "authors": [
                "Xie"
            ],
            "title": "The last inequality uses the Jensen\u2019s inequality Now we can use a similar technique",
            "year": 2021
        },
        {
            "authors": [
                "l(f"
            ],
            "title": "The last inequality uses the fact that dMADC \u2265 1, H \u2265 1. Hence, we complete the proof. F.4 PROOF OF THEOREM D.12 Proof. In this subsection, we give a detailed proof of Theorem D.12",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm only requires an equilibrium-solving oracle and an oracle that solves regularized supervised learning, and thus avoids solving constrained optimization problems within data-dependent constraints (Jin et al., 2020a; Wang et al., 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al., 2023). Moreover, the model-free version of our algorithms is the first provably efficient model-free algorithm for learning Nash equilibrium of general-sum MGs."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Multi-agent reinforcement learning (MARL) has achieved remarkable empirical successes in solving complicated games involving sequential and strategic decision-making across multiple agents (Vinyals et al., 2019; Brown & Sandholm, 2018; Silver et al., 2016). These achievements have catalyzed many research efforts focusing on developing efficient MARL algorithms in a theoretically principled manner. Specifically, a multi-agent system is typically modeled as a general-sum Markov Game (MG) (Littman, 1994), with the primary aim of efficiently discerning a certain equilibrium notion among multiple agents from data collected via online interactions. Some popular equilibrium notions include Nash equilibrium (NE), correlated equilibrium (CE), and coarse correlated equilibrium (CCE).\nHowever, multi-agent general-sum Markov Games (MGs) bring forth various challenges. In particular, empirical application suffers from the large state space. Such a challenge necessitates the use of the function approximation as an effective way to extract the essential features of RL problems and avoid dealing directly with the large state space. Yet, adopting function approximation in a general-sum MG brings about additional complexities not found in single-agent RL or a zero-sum MG. Many prevailing studies on single-agent RL or two-agent zero-sum MGs with the function approximation leverage the special relationships between the optimal policy and the optimal value function (Jin et al., 2021a; Du et al., 2021; Zhong et al., 2022; Jin et al., 2022; Huang et al., 2021; Xiong et al., 2022). In particular, in single-agent RL, the optimal policy is the greedy policy with respect to the optimal value function. Whereas in a two-agent zero-sum MG, the Nash equilibrium is obtained by solving a minimax estimation problem based on the optimal value function. Contrastingly, in a general-sum MG, individual agents possess distinct value functions, and thus there exists no unified optimal value function that characterizes the equilibrium behavior. Moreover, unlike a zero-sum MG, a general-sum MG can admit diverse equilibrium notions, where each corresponds to a set of policies. Consequently, methodologies developed for single-agent RL or zero-sum MGs cannot be directly extended to general-sum MGs.\nRecently, several works propose sample-efficient RL algorithms for general-sum MGs. In particular, Chen et al. (2022b); Foster et al. (2023) propose model-based algorithms for learning NE/CCE/CE based on multi-agent extensions of the Estimation-to-Decision algorithm (Foster et al., 2021), and they establish regret upper bounds in terms of complexity metrics that extend Decision-Estimation Coefficient (Foster et al., 2021) to MGs. In addition, Wang et al. (2023) study model-free RL for general-sum MGs with the general function approximation. They focus on developing a decentralized and no-regret algorithm that finds a CCE. Thus, it seems unclear how to design a provably sample-efficient MARL algorithm for NE/CCE/CE for general-sum MGs in a model-free manner. Furthermore, motivated by the recent development in single-agent RL (Jin et al., 2021a; Du et al., 2021; Zhong et al., 2022; Foster et al., 2021; Liu et al., 2023), we aim to develop a unified algorithmic framework for MARL that covers both model-free and model-based approaches. Thus, we aim to address the following questions:\nCan we design a unified algorithmic framework for general-sum MGs such that (i) it is provably sample-efficient in learning NE/CCE/CE in the context of the function approximation and (ii) it\ncovers both model-free and model-based MARL approaches?\nIn this paper, we provide an affirmative answer to the above questions. Specifically, we propose a unified algorithmic framework named Multi-Agent Maximize-to-EXplore (MAMEX) for generalsum MGs with the general function approximation. MAMEX extends the framework of Maximizeto-Explore (Liu et al., 2023) to general-sum MGs by employing it together with an equilibrium solver for general-sum normal-form games defined over the policy space.\nMaximize-to-Explore (MEX) is a class of RL algorithms for single-agent MDP and two-agent zerosum MGs where each new policy is updated by solving an optimization problem involving a hypothesis f , which can be regarded as the action-value function in the model-free version and the transition model in the model-based version. The optimization objective of MEX contains two terms \u2014 (a) the optimal value with respect to the hypothesis f and (b) a loss function computed from data that quantifies how far f is from being the true hypothesis. Here, the term (a) reflects the planning part of online RL and leverages the fact that the optimal policy is uniquely characterized by the given hypothesis. On the other hand, the term (b), which can be the mean-squared Bellman error or log-likelihood function, reflects the estimation part of online RL. By optimizing the sum of (a) and (b) over the space of hypotheses without any data-dependent constraints, MEX balances exploitation with exploration in the context of the function approximation.\nHowever, the first term in MEX\u2019s optimization objective leverages the fact that the optimal policy can be uniquely constructed from the optimal value function or the true model, using a greedy step or dynamic programming. Such a nice property cannot be extended to general-sum MGs, where the relationship between the equilibrium policies and value function is more complicated, and each agent has its own value function. As a result, it is impractical to construct a single-objective optimization problem in the style of MEX over the hypothesis space for general-sum MGs.\nInstead of optimizing over the spaces of hypotheses, MAMEX optimizes over the policy space. Specifically, in each iteration, MAMEX updates the joint policy of all agents by solving for a desired equilibrium (NE/CCE/CE) of a normal-form game, where the pure strategies are a class of joint policies of the n agents, e.g., the class of deterministic joint policies. Besides, for each pure strategy of this normal form game, the corresponding payoff function is obtained by solving a regularized optimization problem over the hypothesis space a\u0300 la MEX. Thus, policy updates in MAMEX involve the following two steps:\n(i) For each pure strategy \u03c0, construct the payoff function V i(\u03c0) for each agent i by solving an unconstrained and regularized optimization problem;\n(ii) Compute the NE/CCE/CE of the normal-form game over the space of pure strategies with payoff functions {Vi(\u03c0)}ni=1, where n is the number of agents.\nThe implementation of MAMEX only requires an oracle for solving a single-objective and unconstrained optimization problem and an oracle for solving NE/CCE/CE of a normal-form game. Compared to existing works that either solve constrained optimization subproblems within datadependent constraints (Wang et al., 2023), or complex multi-objective or minimax optimization subproblems (Foster et al., 2023; Chen et al., 2022b), MAMEX is more amenable to practical im-\nplementations. Furthermore, step (i) of MAMEX resembles MEX, which enables both model-free and model-based instantiations.\nWe prove that MAMEX is provably sample-efficient in a rich class of general-sum MGs. To this end, we introduce a novel complexity measure named Multi-Agent Decoupling Coefficient (MADC) to capture the exploration-exploitation tradeoff in MARL. Compared to the decoupling coefficient and its variants (Dann et al., 2021; Agarwal & Zhang, 2022; Zhong et al., 2022) proposed for the singleagent setting, MADC characterize the hardness of exploration in MGs in terms of the discrepancy between the out-of-sample prediction error and the in-sample training error incurred by minimizing a discrepancy function \u2113 on the historical data. MADC is defined based on the intuition that if a hypothesis attains a small training error on a well-explored dataset, it would also incur a small prediction error. When the MADC of an MG instance is small, achieving a small training error ensures a small prediction error, and thus exploration is relatively easy. We prove that MAMEX achieves a sublinear regret for learning NE/CCE/CE in classes with small MADCs, which includes multi-agent counterparts of models with low Bellman eluder dimensions (Jin et al., 2021a; 2022; Huang et al., 2021), Bilinear Classes (Du et al., 2021), and models with low witness ranks (Sun et al., 2019; Huang et al., 2021). When specialized to specific members within these classes, MAMEX yields comparable regret upper bounds to existing works.\nOur Contributions. In summary, our contributions are two-fold.\n\u2022 First, we provide a unified algorithmic framework named Multi-Agent Maximize-to-EXplore (MAMEX) for both model-free and model-based MARL, which is sample-efficient in finding the NE/CCE/CE in general-sum MGs with small MADCs. Moreover, MAMEX leverages an equilibrium-solving oracle for normal-form games defined over a class of joint policies for policy updates, and a single-objective optimization procedure that solves for the payoff functions of these normal-form games. To our best knowledge, the model-free version of MAMEX is the first model-free algorithm for general-sum MGs that learns all three equilibria NE, CCE, and CE with sample efficiency.\n\u2022 Second, we introduce a complexity measure, Multi-Agent Decoupling Coefficient (MADC), to quantify the hardness of exploration in a general-sum MG in the context of the function approximation. The class of MGs with low MADCs includes a rich class of MG instances, such as multi-agent counterparts of models with low Bellman eluder dimensions (Jin et al., 2021a; 2022; Huang et al., 2021), Bilinear Classes (Du et al., 2021), and models with low witness ranks (Sun et al., 2019; Huang et al., 2021). When specialized to specific MG instances in these classes, we achieve comparable regret upper bounds to existing works.\nRelated Works. Our paper is closely related to the prior research on Markov Games and MARL with the function approximation. A comprehensive summary of the related literature is in \u00a7A."
        },
        {
            "heading": "2 MODELS AND PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 MARKOV GAMES",
            "text": "For clarity, certain mathematical notations are provided in Appendix \u00a7B.\nGeneral-Sum Markov Games In this work, we consider general-sum Markov Games (MGs) in the episodic setting, which is denoted by a tuple (S, H,A, {r(i)h }i\u2208[n],h\u2208[H], {Ph}h\u2208[H], \u03c1), where n is the number of agents, H is the length of one episode, S is the state set, and A = \u2297ni=1Ai is the joint action set. Here, Ai is the action set of the agent i. Moreover, r(i)h : S \u00d7 A 7\u2192 R is the known reward function1 of the agent i at step h, Ph : S \u00d7 A \u2192 \u2206(S) is the transition kernel at the h-th step, and \u03c1 \u2208 \u2206(S) is the distribution of the initial state s1. We assume the n agents observe the same state at each step and each agent i chooses an action within its own action set Ai simultaneously. In each episode, starting from s1 \u223c p0, for each h \u2208 [H], the agents choose their joint action ah \u2208 A in state sh, where ah = (a(1)h , . . . , a (n) h ). Then, each agent i receives its own\n1Our results can be extended to the unknown stochastic reward case (Agarwal & Zhang, 2022; Zhong et al., 2022). Note that learning the transition kernel is more difficult than learning the reward.\nreward r(i)h (sh, ah), and the game move to the next state sh+1 \u223c Ph(sh+1 | sh, ah). Moreover, we assume \u2211H h=1 r (i) h (sh, ah) \u2208 [0, R] for any possible state-action sequences for some 1 \u2264 R \u2264 H .\nIn MGs, the agents\u2019 policy can be stochastic and correlated. To capture such a property, we introduce the notion of pure policy and joint policy as follows. For each agent i, its local (Markov) policy maps a state s to a distribution over the local action space Ai. We let \u03a0puri \u2286 {\u03c0 : S 7\u2192 \u2206(Ai)} denote a subset of the agent i\u2019s local policies, which is called the set of Markov pure policies. We assume the agent i\u2019s policy is a random variable taking values in \u03a0puri . Specifically, let \u03c9 \u2208 \u2126 be the random seed. The random policy \u03c0(i) = {\u03c0(i)h }h\u2208[H] for the agent i contains H mappings \u03c0 (i) h : \u2126 7\u2192 \u03a0 pur i such that \u03c0(i)h (\u03c9) \u2208 \u03a0 pur i is a pure policy. To execute \u03c0\n(i), the agent i first samples a random seed \u03c9 \u2208 \u2126, and then follows the policy \u03c0(i)h (\u03c9) for all h \u2208 [H]. The joint policy \u03c0 of the n agents is a set of policies {\u03c0(i)}ni=1 that all agents share the same random seed \u03c9. In other words, {\u03c0(i)h (\u03c9)}i\u2208[n] \u2208 \u2297ni=1\u03a0 pur i are random policies of the n agents whose randomness is correlated by the random seed \u03c9. Equivalently, we can regard \u03c0 as a random variable over \u2297ni=1\u03a0 pur i . We let \u03c0h(a | s) denote the probability of taking action a in the state s at step h. Furthermore, a special class of the joint policy is the product policy, where each agent executes their own policies independently. In other words, we have \u03c9 = (\u03c91, . . . , \u03c9n), where \u03c91, . . . , \u03c9n are independent, and each \u03c0(i) depends on \u03c9i only. As a result, we have \u03c0h(a | s) = \u220fn i=1 \u03c0 (i) h (a\n(i) | s) for any product policy \u03c0.\nFurthermore, using the notion of pure policy and joint policy, we can equivalently view the MG as a normal form game over \u03a0pur = \u2297ni=1\u03a0 pur i . That is, each pure policy can be viewed as a pure strategy of the normal form game, and each joint policy can be viewed as a mixed strategy. Such a view is without loss of generality, because we can choose \u03a0puri to be the set of all possible deterministic policies of the agent i. Meanwhile, using a general \u03a0puri , we can also incorporate parametric policies as the pure policies, e.g., log-linear policies (Xie et al., 2021; Yuan et al., 2022; Cayci et al., 2021).\nThe value function V (i),\u03c0h is the expected cumulative rewards received by the agent i from step h to step H , when all the agents follow a joint policy \u03c0, which is defined as\nV (i),\u03c0 h (s) = E\u03c0 [ H\u2211 h\u2032=h r (i) h\u2032 (sh\u2032 , ah\u2032) \u2223\u2223\u2223 sh = s]. We let V (i),\u03c0(\u03c1) = Es\u223c\u03c1[V (i),\u03c01 (s)] denote the agent i\u2019s expected cumulative rewards within the whole episode. Besides, the corresponding Q-function (action-value function) can be written as\nQ (i),\u03c0 h (s, a) = E\u03c0 [ H\u2211 h\u2032=h r (i) h\u2032 (sh\u2032 , ah\u2032) \u2223\u2223\u2223 sh = s, ah = a]. (2.1) For a joint policy \u03c0 and any agent i, we let \u03c0(\u2212i) denote the joint policy excluding the agent i. Given \u03c0(\u2212i), the best response of the agent i is defined as \u03c0(i),\u2020 = argmax\u03bd\u2208\u2206(\u03a0puri ) V\n(i),\u03bd\u00d7\u03c0(\u2212i)(\u03c1), which is random policy of the agent i that maximizes its expected rewards when other agents follow \u03c0(\u2212i). Besides, we denote \u00b5(i),\u03c0 = (\u03c0(i),\u2020, \u03c0(\u2212i)).\nOnline Learning and Solution Concepts We focus on three common equilibrium notions in the game theory: Nash Equilibrium (NE), Coarse Correlated Equilibrium (CCE) and Correlated Equilibrium (CE).\nFirst, a NE of a game is a product policy that no individual player can improve its expected cumulative rewards by unilaterally deviating its local policy. Definition 2.1 (\u03b5-Nash Equilibrium). A product policy \u03c0 is an \u03b5-Nash Equilibrium if V (i),\u00b5 (i),\u03c0\n(\u03c1) \u2264 V (i),\u03c0(\u03c1) + \u03b5 for all i \u2208 [n], where \u00b5(i),\u03c0 = (\u03c0(i),\u2020, \u03c0(\u2212i)) and \u03c0(i),\u2020 is the best response policy with respect to \u03c0(\u2212i).\nIn other words, a product policy \u03c0 is an \u03b5-Nash Equilibrium if and only if\nmax i\u2208[n]\n{ max\n\u03bd\u2208\u2206(\u03a0puri ) V (i),\u03bd\u00d7\u03c0 (\u2212i) (\u03c1)\u2212 V (i),\u03c0(\u03c1)\n} \u2264 \u03b5.\nIn this work, we design algorithms for the online and self-play setting. That is, we control the joint policy all agents, interact with the environment over K episodes, and aim to learn the desired equilibrium notion from bandit feedbacks. To this end, let \u03c0k denote the joint policy that the agents execute in the k-th episode, k \u2208 [K]. We define the Nash-regret as the cumulative suboptimality across all agents with respect to NE. Definition 2.2 (Nash-Regret). For all k \u2208 [K], let \u03c0k denote the product policy deployed in the k-th episode, then the Nash-regret is defined as\nRegNE(K) = K\u2211 k=1 n\u2211 i=1 ( V (i),\u00b5 (i),\u03c0k (\u03c1)\u2212 V (i),\u03c0 k (\u03c1) ) .\nBy replacing the concept of NE to CCE and CE, we can define CCE-regret and CE-regret in a similar way. The detailed definitions are provided in \u00a7C.\nWe note that the definitions of NE, CCE, and CE align with those defined on the normal form game defined on the space of pure policies. That is, each agent i\u2019s \u201cpure strategy\u201d is a pure policy \u03c0(i) \u2208 \u03a0puri , and the \u201cpayoff\u201d of the agent i when the \u201cmixed strategy\u201d is \u03c0 is given by V (i),\u03c0(\u03c1)."
        },
        {
            "heading": "2.2 FUNCTION APPROXIMATION",
            "text": "To handle the large state space in MARL, we assume the access to a hypothesis class F , which captures theQ function in the model-free setting and the transition kernel in the model-based setting.\nModel-Based Function Approximation In the model-based setting, the hypothesis class F contains the model (transition kernel) of MGs. Specifically, we let Pf = {P1,f \u00b7 \u00b7 \u00b7 ,PH,f} denote the transition kernel parameterized by f \u2208 F . When the model parameters are f and the joint policy is \u03c0, we denote the value function and Q-function of the agent i at the h-th step as V (i),\u03c0h,f (s) and Q (i),\u03c0 h,f (s, a) respectively. We have the Bellman equation Q(i),\u03c0h,f (s, a) = r (i) h (s, a) + Es\u2032\u223cPh,f (\u00b7|s,a)[V (i),\u03c0 h+1,f (s \u2032)].\nModel-Free Function Approximation In the model-free setting, we let F = \u2297ni=1F (i) = \u2297ni=1(\u2297Hh=1F (i) h ) be a class of Q-functions of the n agents, where F (i) h = {f (i) h : S \u00d7A 7\u2192 R} is a class ofQ-functions of the agent i at the h-th step. For any f \u2208 F , we denoteQ(i)h,f (s, a) = f (i) h (s, a) for all i \u2208 [n] and h \u2208 [H]. Meanwhile, for any joint policy \u03c0 and any f \u2208 F , we define\nV (i),\u03c0 h,f (s) = Ea\u223c\u03c0(s)[f (i) h (s, a)] = \u27e8f (i) h (s, \u00b7), \u03c0h(\u00b7 | s)\u27e9A.\nFor any joint policy \u03c0, agent i, and step h, we define the Bellman operator T (i),\u03c0h by letting\n(T (i),\u03c0h (fh+1))(s, a) = r (i) h (s, a) + Es\u2032\u223cPh(s\u2032|s,a)\u27e8fh+1(s \u2032, \u00b7), \u03c0h+1(\u00b7 | s\u2032)\u27e9A, \u2200f \u2208 F (i). (2.2)\nNote that the Bellman operator depends on the index i of the agent because the reward functions of the agents are different. Such a definition is an extension of the Bellman evaluation operator in the single-agent setting (Puterman, 2014) to the multi-agent MGs. By definition, {Q(i),\u03c0h } defined in (2.1) is the fixed point of T (i),\u03c0h , i.e., Q (i),\u03c0 h = T (i),\u03c0 h (Q (i),\u03c0 h+1 ) for all h \u2208 [H].\nFor both the model-based and the model-free settings, we impose the realizability assumption, which requires that the hypothesis space F is sufficiently expressive such that it contains the true transition model or the true Q-functions. Besides, for the model-free setting, we also require that the hypothesis classes be closed with respect to the Bellman operator. Assumption 2.3 (Realizability and Completeness). For the model-based setting, we assume the true transition model f\u2217 lies in the hypothesis class F . Besides, for the model-free setting, for any pure policy \u03c0 and any i \u2208 [n], we assume that Q(i),\u03c0 \u2208 F (i) and T (i),\u03c0h F (i) h+1 \u2286 F (i) h for all h \u2208 [H].\nCovering Number and Bracketing Number. When a function class F is infinite, the \u03b4-covering number NF (\u03b4) and the \u03b4-bracketing number BF (\u03b4) serve as surrogates of the cardinality of F . We put the definitions in \u00a7C.2.\nMulti-Agent Decoupling Coefficient Now we introduce a key complexity measure \u2014 multi-agent decoupling coefficient (MADC) \u2014 which captures the hardness of exploration in MARL. Such a notion is an extension of the decoupling coefficient (Dann et al., 2021) to general-sum MGs.\nDefinition 2.4 (Multi-Agent Decoupling Coefficient). The Multi-Agent Decoupling Coefficient of a MG is defined as the smallest constant dMADC \u2265 1 such that for any i \u2208 [n], \u00b5 > 0, {fk}k\u2208[K] \u2286 F (i), and {\u03c0k}k\u2208[K] \u2286 \u03a0pur the following inequality holds:\nK\u2211 k=1 (V (i),\u03c0k fk (\u03c1)\u2212 V (i),\u03c0 k\n(\u03c1))\ufe38 \ufe37\ufe37 \ufe38 prediction error\n\u2264 1 \u00b5 K\u2211 k=1 k\u22121\u2211 s=1\n\u2113(i),s(fk, \u03c0k)\ufe38 \ufe37\ufe37 \ufe38 training error +\u00b5 \u00b7 dMADC + 6dMADCH\ufe38 \ufe37\ufe37 \ufe38 gap , (2.3)\nwhere we define V (i),\u03c0 k\nfk (\u03c1) = Es1\u223c\u03c1[V\n(i),\u03c0k\n1,fk (s1)], and \u2113(i),s(fk, \u03c0k) is a discrepancy function that\nmeasures the inconsistency between fk and \u03c0k, on the historical data. The specific definitions of {\u2113(i),s}i\u2208[n],s\u2208[K\u22121] under the model-free and model-based settings are given in (2.4) and (2.5).\nModel-Free RL In the model-free setting, for {\u03c0k}k\u2208[K] \u2286 \u03a0pur in (2.3), the discrepancy function \u2113(i),s(f, \u03c0) for \u03c0 \u2208 \u03a0pur is defined as\n\u2113(i),s(f, \u03c0) = H\u2211 h=1 E(sh,ah)\u223c\u03c0sh((fh \u2212 T (i),\u03c0 h (fh+1))(sh, ah)) 2, \u2200f \u2208 F (i),\u2200s \u2208 [K]. (2.4)\nThat is, \u2113(i),s(f, \u03c0) measures agent i\u2019s mean-squared Bellman error for evaluating \u03c0, when the trajectory is sampled by letting all agents follow policy \u03b6s.\nModel-Based RL We choose the discrepancy function \u2113(i),s in Assumption 2.5 as\n\u2113(i),s(fk, \u03c0k) = H\u2211 h=1 E(sh,ah)\u223c\u03c0shD 2 H ( Ph,fk(\u00b7 | sh, ah)\u2225Ph,f\u2217(\u00b7 | sh, ah) ) , (2.5)\nwhere DH denotes the Hellinger distance and E(sh,ah)\u223c\u03c0sh means that the expectation is taken with respect to the randomness of the trajectory induced by \u03c0s on the true model f\u2217. Intuitively, it represents the expected in-sample distance of model fk and true model f\u2217.\nNote that the discrepancy between fk, \u03c0k in (2.3) is summed over s \u2208 [k \u2212 1]. Thus, in both the model-free and model-based settings, the training error can be viewed as the in-sample error of fk on the historical data collected before the k-th episode. Thus, for an MG with a finite MADC, the prediction error is small whenever the training error is small. Specifically, when the training error is O(K\u03b1) for some \u03b1 \u2208 (0, 2), then by choosing a proper \u00b5, we know that the prediction error grows as O( \u221a K\u03b1 \u00b7 dMADC) = o(K). In other words, as K increases, the average prediction error decays to zero. In single-agent RL, when we adopt an optimistic algorithm, the prediction error serves as an upper bound of the regret (Dann et al., 2021; Zhong et al., 2022; Jin et al., 2021a). Therefore, by quantifying how the prediction error is related to the training error, the MADC can be used to characterize the hardness of exploration in MARL.\nCompared to the decoupling coefficient and its variants for the single-agent MDP or the two-player zero-sum MG Dann et al. (2021); Agarwal & Zhang (2022); Zhong et al. (2022); Xiong et al. (2022), MADC selects the policy \u03c0k in a different way. In the single-agent setting, the policy \u03c0k is always selected as the greedy policy of fk, hence V \u03c0 k\n1,fk(\u03c1) is equivalent to the optimal value function. In the zero-sum MG, the policy pair \u03c0k is always selected as the Nash policy and the best response (Xiong et al., 2022). On the contrary, in our definition, the policy \u03c0k is not necessarily the greedy policy of fk. In fact, {\u03c0k}k\u2208[K] can be any pure policy sequence that is unrelated to {fk}k\u2208[K]. Assumption 2.5 (Finite MADC). We assume that the MADC of the general-sum MG of interest is finite, denoted by dMADC. As we will show in Section D, the class of MGs with low MADCs include a rich class of MG instances, including multi-agent counterparts of models with low Bellman eluder dimensions (Jin et al., 2021a; 2022; Huang et al., 2021), bilinear classes (Du et al., 2021), and models with low witness ranks (Sun et al., 2019; Huang et al., 2021)."
        },
        {
            "heading": "3 ALGORITHM AND RESULTS",
            "text": "In this section, we first introduce a unified algorithmic framework called Multi-Agent Maximize-toEXplore (MAMEX). Then, we present the regret and sample complexity upper bounds of MAMEX, showing that both the model-free and model-based versions of MAMEX are sample-efficient for learning NE/CCE/CE under the general function approximation."
        },
        {
            "heading": "3.1 ALGORITHM",
            "text": "Algorithm 1 Multi-Agent Maximize-to-EXplore (MAMEX) 1: Input: Hypothesis class F , parameter \u03b7 > 0, and an equilibrium solving oracle EQ. 2: for k = 1, 2, \u00b7 \u00b7 \u00b7 ,K do 3: Compute V k i (\u03c0) defined in (3.1) for all \u03c0 \u2208 \u03a0pur and all i \u2208 [n].\n4: Compute the NE/CCE/CE of the normal-form game defined on \u03a0pur with payoff functions {V ki (\u03c0)}ni=1: \u03c0k \u2190 EQ(V k 1 , V k 2 , \u00b7 \u00b7 \u00b7 , V k n). 5: Sample a pure joint policy \u03b6k \u223c \u03c0k, and collect a trajectory {skh, akh}h\u2208[H] following \u03b6k. 6: Update {L(i),k}ni=1 according to (3.2) (model-free) or (3.3) (model-based). 7: end for\nIn this subsection, we provide the MAMEX algorithm for multi-agent RL under the general function approximation, which extends the MEX algorithm (Liu et al., 2023) to general-sum MGs. Recall that the definitions of NE/CCE/CE of general-sum MGs coincide with those defined in the normalform game with pure strategies being the pure policies in \u03a0pur. Thus, when we know the payoffs {V (i),\u03c0(\u03c1)}i\u2208[n] for all \u03c0 \u2208 \u03a0pur, we can directly compute the desired NE/CCE/CE given an equilibrium solving oracle for the normal-form game. However, each V (i),\u03c0(\u03c1) is unknown and has to be estimated from data via online learning. Thus, in a nutshell, MAMEX is an iterative algorithm that consists of the following two steps:\n(a) Policy evaluation: For each k \u2208 [K], construct an estimator V ki (\u03c0) of V (i),\u03c0(\u03c1) for each pure policy \u03c0 \u2208 \u03a0pur and the agent i \u2208 [n] in each episode based on the historical data collected in the previous k\u22121 episodes. Here, the policy evaluation subproblem can be solved in both the model-free and model-based fashion.\n(b) Equilibrium finding: Compute an equilibrium (NE/CCE/CE) for the normal-form game over the space of pure policies with the estimated payoff functions {V ki (\u03c0)}ni=1. The joint policy returned by the equilibrium finding step is then executed in the next episode to generate a new trajectory.\nBy the algorithmic design, to strike a balance between exploration and exploitation, it is crucial to construct {V ki (\u03c0)}ni=1 in such a way that promotes exploration. To this end, we solve a regularized optimization problem over the hypothesis class F (i) to obtain V ki (\u03c0), where the objective function balances exploration with exploitation. We introduce the details of MAMEX as follows.\nPolicy Evaluation. For each k \u2208 [K], before the k-th episode, we have collected k \u2212 1 trajectories \u03c41:k\u22121 = \u222ak\u22121t=1 {st1, at1, rt1, \u00b7 \u00b7 \u00b7 , stH , atH , rtH}. For any i \u2208 [n], \u03c0 \u2208 \u03a0pur and f \u2208 F (i)2, we can define a data-dependent discrepancy function L(i),k\u22121(f, \u03c0, \u03c41:k\u22121). Such a function measures the in-sample error of the hypothesis f with respect a policy \u03c0, evaluated on the historical data \u03c41:k\u22121. The specific form of such a function differs under the model-free and model-based settings. In particular, as we will show in (3.2) and (3.3) below, under the model-free setting, L(i),k\u22121(f, \u03c0, \u03c41:k\u22121) is constructed based on the mean-squared Bellman error with respect to the Bellman operator T (i),\u03c0h in (2.2), while under the model-based setting, L(i),k\u22121(f, \u03c0, \u03c41:k\u22121) is constructed based on the negative log-likelihood loss. Then, for each \u03c0 \u2208 \u03a0pur and i \u2208 [n], we define V ki (\u03c0) as\nV k\ni (\u03c0) = sup f\u2208F(i)\n{ V\u0302 (i),\u03c0,k(f) := V\n(i),\u03c0 f (\u03c1)\ufe38 \ufe37\ufe37 \ufe38\n(a)\n\u2212\u03b7 \u00b7 L(i),k\u22121(f, \u03c0, \u03c41:k\u22121)\ufe38 \ufe37\ufe37 \ufe38 (b)\n} . (3.1)\n2For ease of notation, under the model-based setting, we denote F (i) = F for all agent i \u2208 [n].\nEquilibrium Finding. Afterwards, the algorithm utilizes the equilibrium oracle EQ (Line 4 of Algorithm 1) to compute an equilibrium (NE/CCE/CE) for the normal-form game over \u03a0pur with payoff functions {V ki (\u03c0)}ni=1. The solution to the equilibrium oracle is a mixed strategy \u03c0k, i.e., a probability distribution over \u03a0pur.\nFinally, we sample a random pure policy \u03b6k from \u03c0k and execute \u03b6k in the k-th episode to generate a new trajectory. See Algorithm 1 for the details of MAMEX. Here, we implicitly assume that \u03a0pur is finite for ease of presentation. For example, \u03a0pur is the set of all deterministic policies. When \u03a0pur is infinite, we can replace \u03a0pur by a 1/K-cover of \u03a0pur with respect to the distance d(i)(\u03c0(i), \u03c0\u0303(i)) = maxs\u2208S \u2225\u03c0(i)(\u00b7 | s)\u2212 \u03c0\u0303(i)(\u00b7 | s)\u22251.\nFurthermore, the objective V\u0302 (i),\u03c0,k(f) in (3.1) is constructed by a sum of (a) the value function V\n(i),\u03c0 f (\u03c1) of \u03c0 under the hypothesis f and (b) a regularized term \u2212\u03b7 \u00b7 L(i),k\u22121(f, \u03c0, \u03c41:k\u22121), and\nthe payoff function V k\ni (\u03c0) is obtained by solving a maximization problem over F (i). The two terms (a) and (b) represent the \u201dexploration\u201d and \u201dexploitation\u201d objectives, respectively, and the parameter \u03b7 > 0 controls the trade-off between them. To see this, consider the case where we only have the term (b) in the objective function. In the model-based setting, (3.1) reduces to the maximum likelihood estimation (MLE) of the model f given the historical data \u03c41:k\u22121. Then \u03c0k returned by Line 4 is the equilibrium policy computed from the MLE model. Thus, without term (a) in V\u0302 (i),\u03c0,k(f), the algorithm only performs exploitation. In addition to fitting the model, the term (a) also encourages the algorithm to find a model with a large value function under the given policy \u03c0, which promotes exploration. Under the model-free setting, only having term (b) reduces to least-squares policy evaluation (LSPE) (Sutton & Barto, 2018), and thus term (b) also performs exploitation only.\nComparison with Single-Agent MEX (Liu et al., 2023). When reduced to the single-agent MDP, MAMEX can be further simplified to the single-agent MEX algorithm (Liu et al., 2023). In particular, when n = 1, equilibrium finding is reduced to maximizing the function defined in (3.1) over single-agent policies, i.e., max\u03c0 maxf\u2208F V\u0302 \u03c0,k(f). By exchanging the order of the two maximizations, we obtain an optimization problem over the hypothesis class F , which recovers the single-agent MEX (Liu et al., 2023). In contrast, in general-sum MGs, the equilibrium policy can no longer be obtained by a single-objective optimization problem. Hence, it is unviable to directly extend MEX to optimize over hypothesis space in MARL. Instead, MAMEX solves an optimization over F in the style of MEX for each pure policy \u03c0 \u2208 \u03a0pur, and then computes the NE/CCE/CE of the normal-form game over the space of pure policies.\nComparison with Existing MARL Algorithms with Function Approximation Previous RL algorithms for MGs with the general function approximation usually require solving minimax optimization (Chen et al., 2022b; Zhan et al., 2022a; Foster et al., 2023) or constrained optimization subproblems within data-dependent constraints (Wang et al., 2023). In comparison, the optimization subproblems of MEX are single-objective and do not have data-dependent constraints, and thus seem easier to implement. For example, in practice, the inner problem can be solved by a regularized version of TD learning (Liu et al., 2023), and the outer equilibrium finding can be realized by any fast method to calculate equilibrium (Hart & Mas-Colell, 2000; Anagnostides et al., 2022).\nIn the following, we instantiate the empirical discrepancy function L(i),k\u22121 for both the model-free setting and the model-based setting.\nModel-Free Algorithm Under the model-free setting, we define the empirical discrepancy function L as follows. For any h \u2208 [H] and k \u2208 [K], let \u03bekh = {skh, akh, skh+1}. For any i \u2208 [n], \u03c0 \u2208 \u03a0pur and f \u2208 F (i), we define\nL(i),k\u22121(f, \u03c0, \u03c41:k\u22121) = H\u2211 h=1 k\u22121\u2211 j=1 [( l (i) h (\u03be j h, f, f, \u03c0) )2 \u2212 inf f \u2032h\u2208F (i) h ( l (i) h (\u03be j h, f \u2032, f, \u03c0) )2] , (3.2)\nwhere l(i)h (\u03be j h, f, g, \u03c0) = (fh(s j h, a j h) \u2212 r (i) h (s j h, a j h) \u2212 \u27e8gh+1(s j h+1, \u00b7), \u03c0h+1(\u00b7 | s j h+1)\u27e9A)2 is the mean-squared Bellman error involving fh and gh+1.\nIn Lemma E.1, we can show that L(i),k\u22121(f, \u03c0, \u03c41:k\u22121) is an upper bound of \u2211k\u22121\ns=1 \u2113 (i),s(f, \u03c0),\nwhere \u2113(i),s is defined in (2.4). Thus, the function L(i),k\u22121 can be used to control the training error in the definition of MADC.\nModel-Based Algorithm For the model-based setting, we define L(i),k\u22121 as the negative loglikelihood:\nL(i),k\u22121(f, \u03c0, \u03c41:k\u22121) = H\u2211 h=1 k\u22121\u2211 j=1 \u2212 logPh,f (sjh+1 | s j h, a j h). (3.3)\nAs we will show in Lemma E.3, the function L(i),k\u22121 can be used to control the training error in (2.3), where \u2113(i),s is defined in (2.5)."
        },
        {
            "heading": "3.2 THEORETICAL RESULTS",
            "text": "In this subsection, we present our main theoretical results and show that MAMEX (Algorithm 1) is sample-efficient for learning NE/CCE/CE in the context of general function approximation.\nTheorem 3.1. Let the discrepancy function \u2113(i),s in (2.3) be defined in (2.4) and (2.5) for model-free and model-based settings, respectively. Suppose Assumptions 2.3 and 2.5 hold. By setting K \u2265 16 and \u03b7 = 4/ \u221a K \u2264 1, with probability at least 1 \u2212 \u03b4, the regret of Algorithm 1 after K episodes is upper bounded by\nRegNE,CCE,CE(K) \u2264 O\u0303 ( nH \u221a K\u03a5F,\u03b4 + ndMADC \u221a K + ndMADCH ) ,\nwhere O\u0303(\u00b7) hides absolute constants and polylogarithmic terms in H and K, and \u03a5F,\u03b4 is a term that quantifies the complexity of the hypothesis class F . In particular, we have \u03a5F,\u03b4 = R2 log(maxi\u2208[n] NF(i)(1/K) \u00b7 |\u03a0pur|/\u03b4) in the model-free setting and \u03a5F,\u03b4 = log (BF (1/K)/\u03b4) in the model-based setting.\nTheorem 3.1 shows that our MAMEX achieves a sublinear \u221a K-regret for learning NE/CCE/CE, where the multiplicative factor depends polynomially on the number of agents n and horizon H . Thus, MAMEX is sample-efficient in the context of the general function approximation. Moreover, the regret depends on the complexity of the hypothesis class via two quantifies \u2013 the MADC dMADC, which captures the inherent challenge of exploring the dynamics of the MG, and the quantity \u03a5F,\u03b4 , which characterizes the complexity of estimating the true hypothesis f\u2217 based on data. To be more specific, in the model-free setting, since we need to evaluate each pure policy, \u03a5F,\u03b4 contains log |\u03a0pur| due to uniform concentration. When reduced to the tabular setting, we can choose \u03a0pur to be the set of deterministic policies, and both \u03a5F,\u03b4 and dMADC are polynomials of |S| and |A|. Furthermore, when specialized to tractable special cases with function approximation and some special pure policy class such as log-linear policy class Cayci et al. (2021), we show in \u00a7D that Theorem D.8 yields regret upper bounds comparable to existing works. Moreover, using the standard online-to-batch techniques, we can transform the regret bound into a sample complexity result. We defer the details to \u00a7E.3."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "In this paper, we study multi-player general-sum MGs under the general function approximation. We propose a unified algorithmic framework MAMEX for both model-free and model-based RL problems with the general function approximation. Compared with previous works that either solve constrained optimization subproblems within data-dependent sub-level sets (Wang et al., 2023), or complex multi-objective minimax optimization subproblems (Chen et al., 2022b; Foster et al., 2023), the implementation of MAMEX requires only an oracle for solving a single-objective unconstrained optimization problem with an equilibrium oracle of a normal-form game, thus being more amenable to empirical implementation. Moreover, we introduce a complexity measure MADC to capture the exploration-exploitation tradeoff for general-sum MGs. We prove that MAMEX is provably sampleefficient in learning NE/CCE/CE on RL problems with small MADCs, which covers a rich class of MG models. When specialized to the special examples with small MADCs, the regret of MAMEX is comparable to existing algorithms that are designed for specific MG subclasses."
        },
        {
            "heading": "CONTENTS",
            "text": ""
        },
        {
            "heading": "1 Introduction 1",
            "text": ""
        },
        {
            "heading": "2 Models and Preliminaries 3",
            "text": "2.1 Markov Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 Function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"
        },
        {
            "heading": "3 Algorithm and Results 7",
            "text": "3.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.2 Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"
        },
        {
            "heading": "4 Conclusion 9",
            "text": ""
        },
        {
            "heading": "A Related Work 16",
            "text": ""
        },
        {
            "heading": "B Notation 17",
            "text": ""
        },
        {
            "heading": "C Additional Definitions 17",
            "text": "C.1 NE/CCE/CE-Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.2 Covering Number and Bracketing Number . . . . . . . . . . . . . . . . . . . . . . 18"
        },
        {
            "heading": "D Relationships between MADC and Tractable RL Problems 18",
            "text": "D.1 Model-Free MARL Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nD.2 Model-Based RL Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
        },
        {
            "heading": "E Proof of Main Results 23",
            "text": "E.1 Proof of Model-Free Version of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . 23\nE.2 Proof of Model-Based Version of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . 27\nE.3 Sample Complexity Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
        },
        {
            "heading": "F Proof of Theorems and Lemmas 30",
            "text": "F.1 Proof of Theorem D.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nF.2 Proof of Theorem D.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nF.3 Proof of Theorem D.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nF.4 Proof of Theorem D.12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nF.5 Proof of Theorem D.14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nF.6 Proof of Lemma E.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nF.7 Proof of Lemma E.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nF.8 Proof of Lemma E.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nF.9 Proof of Corollary E.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\nG Technical Tools 43\nAppendix"
        },
        {
            "heading": "A RELATED WORK",
            "text": "Markov Games Markov Game (MG) (Littman, 1994) is a popular model of multi-agent reinforcement learning, which generalizes the Markov decision process to multiple agents. A series of recent works design the sample-efficient algorithm for two-agent zero-sum games (Wei et al., 2017; Zhang et al., 2020; Xie et al., 2020; Bai et al., 2020; Bai & Jin, 2020; Bai et al., 2021; Zhao et al., 2021; Huang et al., 2021; Jin et al., 2022; Chen et al., 2022b;d). For instance, Bai & Jin (2020) provide a sample-efficient algorithm in an episodic MG based on optimistic value iteration. Xie et al. (2020); Chen et al. (2022d) mainly focus on zero-sum MGs with a linear structure. Huang et al. (2021); Jin et al. (2022); Chen et al. (2022b) further consider the two-player zero-sum MGs under general function approximation, and provide algorithms with a sublinear regret. Another line of research focuses on general-sum MGs with multiple players (Jin et al., 2020a; Liu et al., 2021; Tian et al., 2021; Jin et al., 2021b; Song et al., 2021; Liu et al., 2022b; Daskalakis et al., 2022; Zhan et al., 2022a; Cui et al., 2023; Wang et al., 2023). Some of previous works (Liu et al., 2021; Tian et al., 2021; Liu et al., 2022b) consider learning all three equilibrium notions \u2014 NE, CCE, and CE \u2014 and their regret or sample complexity results are exponential in the number of agents. To break this exponential curse, some existing works propose decentralized algorithms for learning CCE or CE rather than NE (Jin et al., 2021b; Daskalakis et al., 2022; Zhan et al., 2022a; Cui et al., 2023; Wang et al., 2023).\nMoreover, there are also some works to study how to learn the equilibrium from a practical perspective. The EGTA (Wellman, 2006) uses a graph to represent the deviation for all profiles and identify the Nash equilibrium and whether a profile is relatively stable. Note if we want to evaluate all profiles and construct the whole graph, we need many samples to estimate the payoff functions for each profile, and then identify all the deviations and construct the graph. The author also mentions that one may apply machine learning techniques to fit a payoff function over the entire profile space given the available data. In fact, we use the function approximation technique to derive an estimate of all the profiles without estimating all the profiles. Thus, our approach can be considered as a learning approach to evaluate all the profiles.\nSome previous works (Lanctot et al., 2017; Marris et al., 2021) propose a practical algorithm PSRO to compute the equilibrium in Markov Games, which also needs an equilibrium-solving oracle to learn the equilibrium. To be more specific, the PSRO learns the equilibrium in the following way: At first, every player chooses a uniform policy as their strategy. The algorithm then calculates the equilibrium by a meta-solver and trains an oracle that outputs the best response \u03c0i of the equilibrium for player i. After that, the algorithm adds \u03c0i into the strategy space of the player i. Last, the algorithm simulates all the new joint policy and construct a new normal-form game for the next iteration. However, in each iteration, it should simulate all the new joint policies and estimate the return. Consequently, the sample complexity increases exponentially as the iteration rounds increase. Different from PSRO, MAMEX utilizes the function approximation technique to the value function. The precise characterization of the structure of the value function can help us to evaluate the policy without actually simulating the environment with more samples. To be more specific, at each round, instead of simulating the environment and getting a Monte-Carlo return of each joint policy, MAMEX only needs to solve a regularized optimization problem over the function space of the value function. The solution of the optimization problem is used to be a payoff for the normal-form game. Since solving this optimization subproblem does not need to additional samples, MAMEX bypasses the requirement for exponential samples to simulate the environment and estimate the value for each joint policy \u03c0. This characteristic enhances its sample efficiency in comparison to PSRO.\nMARL with Function Approximation There are many papers working on multi-player generalsum MGs with the function approximation (Zhan et al., 2022a; Ni et al., 2022; Chen et al., 2022b; Wang et al., 2023; Cui et al., 2023; Foster et al., 2023) that build upon previous works for function approximation in the single-agent setting (Jiang et al., 2017; Sun et al., 2019; Jin et al., 2020b; Wang et al., 2020b; Dann et al., 2021; Du et al., 2021; Jin et al., 2021a; Foster et al., 2021; Chen et al., 2022c; Agarwal & Zhang, 2022; Zhong et al., 2022; Liu et al., 2023). In recent years, Xiong\net al. (2022) consider the multi-agent decoupling coefficient in the two-player zero-sum MGs, and provide the posterior sampling algorithm. However, unlike a zero-sum MG, a general-sum MG can have various equilibrium concepts, each of which aligns with a specific set of policies. Hence, their definition of the multi-agent decoupling coefficient cannot be extended to the general-sum setting. Chen et al. (2022b) and Foster et al. (2023) generalize the complexity measure DecisionEstimation Coefficient (DEC), and learn the equilibria in model-based general-sum MGs. Ni et al. (2022) provide both a model-based algorithm and a model-free algorithm for the low-rank MGs. Some previous works (Zhan et al., 2022a; Wang et al., 2023; Cui et al., 2023) provide model-free algorithms that learn CCE and CE with polynomial sample complexity. Compared to their works, this paper provides a unified algorithmic framework for both model-free and model-based MARL problems, which learns NE/CCE/CE efficiently under general function approximation and provides comparable regret to existing works. In particular, our work provides the first model-free algorithm for learning NE/CCE/CE of general-sum MGs in the context of the general function approximation."
        },
        {
            "heading": "B NOTATION",
            "text": "For n sets F1, \u00b7 \u00b7 \u00b7 ,Fn, we let \u2297ni=1Fi denote F1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Fn. For a set A, we denote \u2206(A) as a set of probability distributions over A. For a vector x \u2208 Rn, we denote \u2225x\u22251 = \u2211n i=1 |xi|,\n\u2225x\u22252 = \u221a\u2211n i=1 x 2 i and \u2225x\u2225\u221e = maxni=1 |xi|. For a function f : X 7\u2192 Y , we denote \u2225f\u2225\u221e = supx\u2208X |f(x)| as the infinity norm. For two functions f, g : A 7\u2192 R, we denote \u27e8f, g\u27e9A = Ea\u2208A[f(x)g(x)] as the inner product with respect to the set A. For a Hilbert space V and f, g \u2208 V , we denote \u27e8f, g\u27e9V as the inner product defined in the Hilbert space V, and \u2225f\u2225V is the norm defined in Hilbert space V. For two distributions over P,Q \u2208 \u2206(X ), the Hellinger distance is defined asD2H(P\u2225Q) = 12Ex\u223cP [( \u221a dP (x)/dQ(x)\u22121)2]. For a vector x \u2208 Rd, the softmax\nmapping is denoted by Softmax(x) \u2208 Rd with ( Softmax(x) ) i = exi/ \u2211 i\u2208[d] e xi ."
        },
        {
            "heading": "C ADDITIONAL DEFINITIONS",
            "text": ""
        },
        {
            "heading": "C.1 NE/CCE/CE-REGRET",
            "text": "In the following, we provide the definitions of Coarse Correlated Equilibrium (CCE), Correlated Equilibrium (CE), and the corresponding regret CCE-regret and CE-regret. A Coarse Correlated Equilibrium is a joint policy \u03c0 such that no agent can achieve higher rewards by only changing its local policy. Compared with a NE, a CCE allows different agents to be correlated, while NE only considers product policies. Definition C.1 (\u03b5-Coarse Correlated Equilibrium). A joint policy \u03c0 is a \u03b5-Coarse Correlated Equilibrium if V (i),\u00b5 (i),\u03c0 (\u03c1) \u2264 V (i),\u03c0(\u03c1) + \u03b5 for all i \u2208 [n].\nHere, the definition of \u03b5-CCE is similar to that of an \u03b5-NE. But here \u03c0 is a joint policy, i.e., the randomness of the local policies of the n agents can be coupled together. As a result, CCE is a more general equilibrium notion than NE. Similarly, we can define the CCE-regret, which represents the cumulative suboptimality across all agents with respect to CCE. Definition C.2 (CCE-Regret). For all k \u2208 [K], let \u03c0k denote the joint policy that is deployed in the k-th episode, then the CCE-regret is defined as\nRegCCE(K) = K\u2211 k=1 n\u2211 i=1 ( V (i),\u00b5 (i),\u03c0k (\u03c1)\u2212 V (i),\u03c0 k (\u03c1) ) .\nLast, the Correlated Equilibrium has been extensively studied in previous works for MARL (Jin et al., 2020a; Chen et al., 2022b; Cui et al., 2023; Wang et al., 2023). To introduce the concept of CE, we need first to introduce the strategy modification. A strategy modification for the i-th agent is a mapping \u03d5i : \u03a0 pur i \u2192 \u03a0 pur i . Given any random policy \u03c0, the best strategy modification for i-th agent is defined as argmax\u03d5i E\u03c5\u223c\u03c0[V \u03d5i(\u03c5\n(i))\u00d7\u03c5(\u2212i)(\u03c1)]. A CE is a joint policy \u03c0 such that no agent can achieve higher rewards by only changing its local policy through strategic modification. Definition C.3 (\u03b5-Correlated Equilibrium). A joint policy \u03c0 is a \u03b5-Correlated Equilibrium if max\u03d5i E\u03c5\u223c\u03c0[V \u03d5i(\u03c5 (i))\u00d7\u03c5(\u2212i)(\u03c1)] \u2264 V \u03c0(\u03c1) + \u03b5 for any agent i \u2208 [n].\nWe can similarly define CE-regret as the sum of suboptimality terms with respect to CE. Definition C.4 (CE-Regret). For any k \u2208 [K], let \u03c0k denote the joint policy that is deployed in the k-th episode, the CE-regret is defined as\nRegCE(K) = K\u2211 k=1 n\u2211 i=1 ( max \u03d5i E\u03c5\u223c\u03c0k ( V (i),\u03d5i(\u03c5 (i))\u00d7\u03c5(\u2212i)(\u03c1) ) \u2212 V (i),\u03c0 k (\u03c1) ) .\nCompared to the NE/CCE regret, the strategy modification of one agent in CE can be correlated to the policies of other agents. Instead, the best response is independent of the other agents."
        },
        {
            "heading": "C.2 COVERING NUMBER AND BRACKETING NUMBER",
            "text": "When a function class F is infinite, the \u03b4-covering number and the \u03b4-bracketing number serve as surrogates of the cardinality of F . Intuitively, the \u03b4-covering number is the minimum number of balls of radius \u03b4 required to cover a set.\nDefinition C.5 (\u03b4-Covering Number). The \u03b4-covering number of a function class F with respect to distance metric d, denoted as NF (\u03b4, d), is the minimum integer q satisfying the following property: there exists a subset F \u2032 \u2286 F with |F \u2032| = q such that for any f1 \u2208 F we can find f2 \u2208 F \u2032 with d(f1, f2) \u2264 \u03b4. To simplify the notation, we write NF (\u03b4, \u2225 \u00b7 \u2225\u221e) as NF (\u03b4).\nDefinition C.6 (\u03b4-Bracketing Number). A \u03b4-bracket of size N is a bracket {gi1, gi2}Ni=1, where gi1 and gi2 are functions mapping any policy \u03c0 and trajectory \u03c4 to R, such that for all i \u2208 [N ], \u03c0 \u2208 \u03a0 we have \u2225gi1(\u03c0, \u00b7) \u2212 gi2(\u03c0, \u00b7)\u2225 \u2264 \u03b4. Also, for any f \u2208 F , there must exist an i \u2208 [N ] such that gi1(\u03c0, \u03c4H) \u2264 P\u03c0f (\u03c4H) \u2264 gi2(\u03c0, \u03c4H) for all possible \u03c4H and \u03c0. The \u03b4-bracketing number of F , denoted by BF (\u03b4), is the minimum size of a \u03b4-bracket."
        },
        {
            "heading": "D RELATIONSHIPS BETWEEN MADC AND TRACTABLE RL PROBLEMS",
            "text": "In this section, we show that the class of MGs with finite MADCs contains a rich class of models. Thus, when applied to these concrete MARL models, Theorem 3.1 shows that MAMEX learns NE/CCE/CE with provable sample efficiency.\nIn the sequel, we instantiate the discrepancy function \u2113(i),s for both model-free and model-based MARL, and introduce some concrete general-sum MG models that satisfy Assumption 2.5."
        },
        {
            "heading": "D.1 MODEL-FREE MARL PROBLEMS",
            "text": "Now we provide function classes with small MADCs including multi-agent counterparts of models with low Bellman eluder dimensions (Jin et al., 2021a; Huang et al., 2021) and Bilinear Classes (Du et al., 2021). Then, we introduce some concrete examples in these members and show that the regret upper bound of MAMEX in Theorem 3.1, when specialized to these special cases, are comparable to existing works.\nMulti-Agent Bellman Eluder Dimension Recently, Jin et al. (2021a) introduce a model-free complexity measure called Bellman Eluder dimension (BE dimension)and show that function classes with low BE dimensions contain a wide range of RL problems such as linear MDP (Jin et al., 2020b), kernel MDP (Jin et al., 2021a) and function classes with low eluder dimension (Wang et al., 2020a). In this subsection, we extend the notion of BE dimension to MARL. First, we introduce the definition of \u03b5-independence between distributions and the concept of distribution eluder dimension. Definition D.1 (\u03b5-Independent Distributions). Let G be a function class on X , and \u03c5, \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5n are probability distributions over X . We called \u03c5 is \u03b5-independent of {\u00b51 \u00b7 \u00b7 \u00b7\u00b5n} with respect to G if there exists a function g \u2208 G such that \u221a\u2211n i=1(E\u00b5i [g])2 \u2264 \u03b5 and |E\u03c5[g]| > \u03b5.\nBy this definition, if \u03bd is \u03b5-dependent of {\u00b51, \u00b7 \u00b7 \u00b7 , \u00b5n}, then whenever we have \u221a\u2211n\ni=1(E\u00b5i [g])2 \u2264 \u03b5 for some g \u2208 G, we also have |E\u03c5[g]| \u2264 \u03b5. Definition D.2 (Distribution Eluder Dimension). Let G be a function class on X and D be a family of probability measures over X . The distributional eluder dimension dimDE(G,D, \u03b5) is the length of the longest sequence \u03c11, \u00b7 \u00b7 \u00b7 , \u03c1n \u2286 D such that there exists \u03b5\u2032 \u2265 \u03b5 where \u03c1i is \u03b5\u2032-independent of {\u03c11, \u00b7 \u00b7 \u00b7 , \u03c1i\u22121} for all i \u2208 [n].\nIn other words, distributional eluder dimension dimDE(G,D, \u03b5) is the length of the longest sequences of distributions in D such that each element is \u03b5\u2032-independent of its predicessors with respect to G, from some \u03b5\u2032 \u2265 \u03f5. Such a notion generalizes the standard eluder dimension Russo & Van Roy (2013) to the distributional setting. When we set D to be the set of Dirac measures {\u03b4x(\u00b7)}x\u2208X , the distributional eluder dimension dimDE(G \u2212G,D, \u03b5) reduces to the standard eluder dimension introduced in Russo & Van Roy (2013). Here, G \u2212 G = {g1 \u2212 g2 : g1, g2 \u2208 G}. For any agent i and any pure policy \u03c0 \u2208 \u03a0pur, we denote the function class of the Bellman residual as F (i),\u03c0h = {fh \u2212 T (i),\u03c0fh+1 | f \u2208 F (i)}. Now we introduce the definition of the multi-agent BE dimension with respect to a class of distributions. Definition D.3 (Multi-Agent Bellman Eluder Dimension). Let D = {Dh}h\u2208[H] be a set of H classes of distributions over S\u00d7A, one for each step of an episode. The multi-agent Bellman eluder (BE) dimension with respect to D is defined as\ndimMABE(F ,D, \u03b5) = max h\u2208[H] max i\u2208[n]\n{ dimDE ( \u22c3 \u03c0\u2208\u03a0pur F (i),\u03c0h ,Dh, \u03b5 )} . (D.1)\nIn other words, the multi-agent BE dimension is defined as the maximum of the distribution eluder dimensions with respect toDh, based on the agent-specific Bellman residue classes \u22c3 \u03c0\u2208\u03a0pur F (i),\u03c0 h . Compared with the BE dimension for single-agent RL (Jin et al., 2021a), the multi-agent version takes the maximum over the agent index i \u2208 [n], and the function class involves the union of the function class F (i),\u03c0h for all \u03c0 \u2208 \u03a0pur. In comparison, leveraging the facts that the optimal policy is the greedy policy of the optimal value function, and that the optimal value function is the fixed point of the Bellman optimality operator, it suffices to only consider residues of the Bellman optimality operator in the definition of single-agent BE dimension. In contrast, for general-sum MGs, finding the desired equilibrium policies is not a single-objective policy optimization problem, and the notion of the Bellman optimality operator is not well-defined. As a result, to extend the concept of Bellman eluder dimension to general-sum MGs, in the function class, we take into account F (i),\u03c0h for all \u03c0 \u2208 \u03a0pur, which correspond to evaluating the performance of all the pure policies. Besides, in (D.1), we also take the maximum over all agents i \u2208 [n] and all steps h \u2208 [H], which aligns with the definition of single-agent BE dimension.\nFurthermore, in the definition of multi-agent BE dimension, we need to specify a set of distributions D = {Dh}h\u2208[H] over S\u00d7A. We consider two classes. First, letD\u2206 = {D\u2206,h}h\u2208[H] denote a class of probability measures over S \u00d7 A with D\u2206,h = {\u03b4(s,a)(\u00b7) | (s, a) \u2208 S \u00d7 A}, which contains all the Dirac measures that put mass one to a state-action pair at step h. Second, given the set of pure policies \u03a0pur, we let D\u03a0 = {D\u03a0,h}h\u2208[H] denote a class of probability measures induced \u03a0pur as follows. For any \u03c0 \u2208 \u03a0pur, when all the agents follow \u03c0 on the true MG model, they generate a Markov chain {sh, ah}h\u2208[H] whose joint distribution is determined by \u03c0, denoted by P\u03c0 . Then, for any h \u2208 [H], we define D\u03a0,h = {\u03c1 \u2208 \u2206(S \u00d7 A) | \u03c1(\u00b7) = P\u03c0((sh, ah) = \u00b7), \u03c0 \u2208 \u03a0pur}, i.e., D\u03a0,h denotes the collection of all marginal distributions of (sh, ah) induced by pure policies.\nIn the following, to simplify the notation, we denote dimMABE(F , \u03b5) = min { dimMABE(F ,D\u2206, \u03b5), dimMABE(F ,D\u03a0, \u03b5) } . (D.2)\nThe following theorem shows that, when F satisfies realizability and completeness (Assumption 2.3), for a general-sum MG with a finite multi-agent BE dimension given by (D.2), its multi-agent decoupling coefficient (Definition 2.4) is also bounded. In other words, Assumption 2.5 holds any general-sum MG model with a low multi-agent BE dimension. As a result, the class of MGs with finite multi-agent BE dimensions is a subclass of MGs with finite multi-agent decoupling coefficients. Theorem D.4 (Low Multi-Agent BE Dimension \u2286 Low MADC). Let K any integer and let F be a hypothesis class under the model-free setting, i.e., a class of Q-functions. Assume that F satisfy the\nrealizability and completeness condition specified in Assumption 2.3. Suppose that F has a finite multi-agent BE dimension d = dimMABE(F , 1/K), then with the discrepancy function \u2113(i),s given in (2.4), the multi-agent decoupling coefficient of F satisfies dMADC = O(dH logK), where O(\u00b7) omits absolute constants.\nProof. See \u00a7F.1 for a detailed proof.\nCombining Theorem 3.1 and Theorem D.4, we obtain that MAMEX achieves a sublinear O\u0303(ndH \u221a K+ndH2+nHR2 \u221a K log\u03a5F,\u03b4) regret for function classes with a finite multi-agent BE dimension d. It remains to see that that function classes with low multi-agent BE dimensions contain a wide range of RL problems. To this end, we prove that if the eluder dimension (Russo & Van Roy, 2013) of the function class F (i)h is small for all h \u2208 [H] and i \u2208 [n], then F = \u2297ni=1(\u2297Hh=1F (i) h ) has a low multi-agent BE dimension. Function classes with finite eluder dimension contains linear, generalized linear, and kernel functions (Russo & Van Roy, 2013), and thus contains a wide rage of MG models. On these MG problems, the model-free version of MAMEX achieve sample efficiency provably.\nTheorem D.5. Suppose F satisfies Assumption 2.3. For any i \u2208 [n] and h \u2208 [H], let dimE(F (i)h , \u03b5) denote the eluder dimension of F (i)h , which is a special case of the distributional eluder dimension introduced in Definition D.2. That is, dimE(F (i)h , \u03b5) is equal to dimDE(F (i) h \u2212F (i) h , D\u2206, \u03b5), where F (i)h \u2212 F (i) h = {g : g = f1 \u2212 f2, f1, f2 \u2208 F (i) h } and D\u2206 contains the class of dirac measures on S \u00d7A. Then, the multi-agent BE dimension defined in (D.2) satisfy\ndimMABE(F , \u03b5) \u2264 max h\u2208[H] max i\u2208[n] dimE(F (i)h , \u03b5).\nProof. See \u00a7F.2 for a detailed proof.\nMulti-Agent Bilinear Classes Bilinear Classes (Du et al., 2021) consists of MDP models where the Bellman error admits a bilienar structure. On these models, Du et al. (2021) propose online RL algorithms that are provably sample-efficient. Thus, Bilinear Classes is a family of tractable MDP models with general function approximation. In the sequel, we extend Bilinear Classes to generalsum MGs and show that such an extension covers some notable special cases studied in the existing works. Then, we prove that multi-agent Bilinear Classes have a small MADC, thus satisfying the Assumption 2.5. Therefore, when applied to these problems, MAMEX provably achieves sample efficiency. Definition D.6 (Multi-Agent Bilinear Classes). Let V be a Hilbert space and let \u27e8\u00b7, \u00b7\u27e9V and \u2225 \u00b7 \u2225V denote the inner product and norm on V . Given a multi-agent general-sum MG with a hypothesis class F satisfying Assumption 2.3, it belongs to multi-agent Bilinear Classes if there exist H functions {W (i)h : F (i) \u00d7 \u03a0pur 7\u2192 V}Hh=1 for each agent i \u2208 [n] and {Xh : \u03a0pur 7\u2192 V}Hh=1 such that the Bellman error of each agent i can be factorized using W (i)h and Xh. That is, for each i \u2208 [n], f \u2208 F (i), h \u2208 [H], \u03c0, \u03c0\u2032 \u2208 \u03a0pur, we have\u2223\u2223\u2223E(sh,ah)\u223c\u03c0\u2032[fh(sh, ah)\u2212 r(i)h (sh, ah)\u2212 Es\u2032\u223cPh(s\u2032|sh,ah)\u27e8fh+1(s\u2032, \u00b7), \u03c0h+1(\u00b7 | s\u2032)\u27e9A]\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2329W (i)h (f, \u03c0)\u2212W (i)h (f (i),\u00b5(i),\u03c0 , \u00b5(i),\u03c0), Xh(\u03c0\u2032)\u232aV \u2223\u2223\u2223, (D.3)\nwhere \u00b5(i),\u03c0 = (\u03c0(i),\u2020, \u03c0(\u2212i)) is the best response for i-th agent given that the other agents all follow \u03c0. Here, the function f (i),\u00b5 (i),\u03c0 is the fixed point of T (i),\u00b5(i),\u03c0 , i.e.,\nf (i),\u00b5(i),\u03c0 h = T (i),\u00b5(i),\u03c0f\u00b5 (i),\u03c0 h+1 . (D.4)\nMoreover, we require that {W (i)h , Xh}h\u2208[H] satisfy a regularity condition\nsup \u03c0\u2208\u03a0pur,h\u2208[H] \u2225Xh(\u03c0)\u2225V \u2264 1, sup i\u2208[n],f\u2208F(i),\u03c0\u2208\u03a0pur,h\u2208[H] \u2225W (i)h (f, \u03c0)\u2225V \u2264 BW , (D.5)\nwhere BW is a constant.\nIn this definition, for any \u03c0 \u2208 \u03a0pur and f \u2208 F (i),\nfh(sh, ah)\u2212 r(i)h (sh, ah)\u2212 Es\u2032\u223cPh(s\u2032|sh,ah)\u27e8fh+1(s \u2032, \u00b7), \u03c0h+1(\u00b7 | s\u2032)\u27e9A\nis the Bellman error of f at (sh, ah) for evaluating policy \u03c0 on behalf of agent i. On the left-hand side of (D.3), we evaluate such a Bellman error with respect to the distribution induced by another policy \u03c0\u2032. Equation (D.3) shows that this error can be factorized into the inner product between W (i)h and X\n(i) h , where both W (i) h only involves (f, \u03c0) while X (i) h only involves \u03c0 \u2032. Thus, multi-agent Bilinear Classes specifies a family of Markov games whose Bellman error satisfies a factorization property. Furthermore, recall that the best response \u03c0(i),\u2020 = max\u03bd\u2208\u2206(\u03a0puri ) V \u03bd,\u03c0(\u2212i) is attained at some pure policy, thus we have \u00b5(i),\u03c0 \u2208 \u03a0pur. Under Assumption 2.3, the fixed point f (i),\u00b5(i),\u03c0 in (D.4) is guaranteed to exist and belongs to F .\nWe define Xh = {Xh(\u03c0) : f \u2208 F , \u03c0 \u2208 \u03a0pur} and X = \u22c3H\nh=1 Xh. The complexity of the multiagent bilinear class essentially is determined by the complexity of the Hilbert space V . To allow V be infinite-dimensional, we introduce the notion of information gain, which characterizes the intrinsic complexity of V in terms of exploration. Definition D.7 (Information Gain). Suppose V is a Hilbert space and X \u2286 V . For \u03b5 > 0 and integer K > 0, the information gain \u03b3K(\u03b5,X ) is defined by\n\u03b3K(\u03b5,X ) = max x1,\u00b7\u00b7\u00b7 ,xK\u2208X log det\n( I + 1\n\u03b5 K\u2211 k=1 xkx \u22a4 k\n) .\nThe following theorem shows that multi-agent Bilinear Classes with small information gain have low MADCs.\nTheorem D.8 (Multi-Agent Bilinear Classes \u2286 Low MADC). For a general-sum MG in the multiagent bilinear class with a hypothesis class F , let \u03b3K(\u03b5,X ) = \u2211H h=1 \u03b3K(\u03b5,Xh) be the information gain. Then, Assumption 2.5 holds with the discrepancy function \u2113(i),s given in (2.4). In particular, we have\ndMADC \u2264 max { 1, 8R2 \u00b7 \u03b3K(1/(KB2W ),X ) } ,\nwhere BW is given in (D.5) and R \u2208 (0, H] is an upper bound on \u2211H h rh.\nProof. See \u00a7F.3 for a detailed proof.\nNow we introduce some concrete members of multi-agent Bilinear Classes, which are general-sum MGs with linear function approximation. In single-agent RL, linear Bellman complete MDPs (Wang et al., 2019) assume that the MDP model satisfies the Bellman completeness condition with respect to linear Q-functions. We can extend such a model to general-sum MGs.\nExample D.9 (Linear Bellman Complete MGs). We say a Markov Game is a linear Bellman complete MG of dimension d, if for any step h \u2208 [H] there exists a known feature \u03d5h : S \u00d7 A 7\u2192 Rd with \u2225\u03d5h(s, a)\u2225 \u2264 1 for all (s, a) \u2208 S \u00d7 A such that Assumption 2.3 holds for linear functions of \u03d5h. In other words, the Markov game satisfies Assumption 2.3 with F (i)h \u2286 {\u03d5\u22a4h \u03b8 | \u03b8 \u2208 Rd, \u2225\u03b8\u22252 \u2264\u221a d\u03b8}for all i \u2208 [n] and h \u2208 [H], where d\u03b8 > 0 is a parameter.\nIt is easy to see that Linear Bellman complete MGs belong to multi-agent Bilinear Classes by choosing\nXh(\u03c0) = E\u03c0[\u03d5(sh, ah)] \u2208 Rd, W (i)h (f, \u03c0) = \u03b8f,h \u2212 w (i) f,h,\nwhere \u03b8f,h satisfies that f(sh, ah) = \u03b8\u22a4f,h\u03d5h(sh, ah), and w (i) f,h satisfies that 3\n(w (i) f,h) \u22a4\u03d5h(sh, ah) = r (i) h (sh, ah) + Es\u2032\u223cPh(\u00b7|sh,ah)\u27e8fh+1(s \u2032, \u00b7), \u03c0h+1(\u00b7 | s\u2032)\u27e9A\n= T (i),\u03c0(fh+1) \u2208 F (i)h . 3If there are multiple \u03b8 satisfying the requirement, we can break the tie arbitrarily.\nThen, we have Xh \u2286 V = {\u03d5 \u2208 Rd : \u2225\u03d5\u22252 \u2264 1} for all h \u2208 [H] and BW = 2 \u221a d. It can be shown that the logarithm of 1/K-covering number of F is log(NF (1/K)) = O\u0303(d), and the information gain can bounded by\n\u03b3K(1/B 2 WK,X ) = H\u2211 h=1 \u03b3K(1/B 2 WK,Xh) \u2264 H\u2211 h=1 \u03b3K(1/4dK,Xh) = O\u0303(Hd),\nwhere O\u0303 omits absolute constants and logarithmic factors (Du et al., 2021; Wang et al., 2020b). Thus, by Theorem 3.1, MAMEX achieves a O\u0303(ndHR2 \u221a K + nHR2 \u221a K log |\u03a0pur| + ndH2) regret. For the single-agent setting, comparing to the state-of-the-art O\u0303(dH \u221a K) regret when R = 1 (Zanette et al., 2020; Chen et al., 2022c), our result matches their results in terms of d,H and K with an extra factor |\u03a0pur| in the logarithmic term. Note that when the pure policy set of i-th agent is selected as some particular policy classes such as log-linear policy\n\u03a0puri = {\u03c0\u03d1 : \u03c0\u03d1(\u00b7 | s) = Softmax(\u03d1 \u22a4\u03c8(s, \u00b7)), \u2225\u03d1\u22252 \u2264 1, \u2225\u03c8(\u00b7, \u00b7)\u2225 \u2264 1, \u03d1 \u2208 Rd\u03c0},\nwe can select a cover by\n\u0398\u0302 = {\u03d1\u0302 : \u03d1\u0302i = \u230a\u03d1i/\u03b5\u230b \u00d7 \u03b5, \u2225\u03d1\u22252 \u2264 1, \u03d1 \u2208 Rd\u03c0}. Zanette et al. (2021) prove that the logarithm of cardinality of the induced covering {\u03c0\u03d1 : \u03d1 \u2208 \u0398\u0302} is bounded by O\u0303(nd\u03c0), and then MAMEX provides a O\u0303((nd+ n2d\u03c0)HR2 \u221a K + ndH2) regret.\nIn particular, as one of the examples of Linear Bellman Complete MGs, Xie et al. (2020) consider a similar linear structure for two-player zero-sum games. Example D.10 (Zero-Sum Linear MGs (Xie et al., 2020)). In a zero-sum linear MG, for each (s, a, b) \u2208 S \u00d7 A \u00d7 B and h \u2208 [H], we have reward rh(s, a, b) \u2208 [0, 1], and there is a known feature map \u03d5 : S \u00d7A\u00d7 B \u2192 Rd, H known vectors \u03b8h \u2208 Rd and a vector of d unknown measures \u00b5h = {\u00b5h,d\u2032}d\u2032\u2208[d] on S such that \u2225\u03d5(\u00b7, \u00b7, \u00b7)\u22252 \u2264 1, \u2225\u03b8h\u22252 \u2264 \u221a d, \u2225\u00b5h(S)\u22252 \u2264 \u221a d and\nrh(s, a, b) = \u03d5(s, a, b) \u22a4\u03b8h, Ph(\u00b7 | s, a, b) = \u03d5(s, a, b)\u22a4\u00b5h(\u00b7).\nZero-sum linear MGs is a special case of linear Bellman complete MG with two players and d\u03b8 = 2H \u221a d, and our algorithm provides a O\u0303(dH3 \u221a K+H3 \u221a K log(|\u03a0pur|)) regret by choosingR = H and the fact that logN (i)F (1/K) = O\u0303(d). The previous work provides a O\u0303(d3/2H2 \u221a K) sublinear\nregret (Xie et al., 2020) and a \u2126(dH3/2 \u221a K) information-theoretic lower bound (Chen et al., 2022d) for zero-sum linear MGs. Thus, our regret matches the lower bound in terms of d, has a higher order in H compared to Xie et al. (2020) and an extra factor log |\u03a0pur|. Again, we can adopt the class of log-linear policies with a policy cover, which leads to log |\u03a0pur| = O\u0303(d\u03c0). Thus, MAMEX yields a O\u0303((d+ d\u03c0)H3 \u221a K) regret."
        },
        {
            "heading": "D.2 MODEL-BASED RL PROBLEMS",
            "text": "Sun et al. (2019) provide a complexity measure \u2014 witness rank \u2014 to characterize the exploration hardness of the model-based RL problems. In the following, we extend the notion of the witness rank to MARL. Example D.11 (Multi-Agent Witness Rank). Let V = {Vh : S \u00d7 A \u00d7 S 7\u2192 [0, 1]}h\u2208[H] denote a class of discriminators and let F be a hypothesis class such that the true model, denoted by f\u2217, belongs to F . We say a multi-agent witness rank of a general-sum MG is at most d, if for any model f \u2208 F and any policy \u03c0 \u2208 \u03a0pur there exist mappings {Xh : \u03a0pur \u2192 Rd}Hh=1 and {Wh : F \u2192 Rd}Hh=1 max v\u2208Vh E(sh,ah)\u223c\u03c0[(Es\u2032\u223cPh,f (\u00b7|sh,ah) \u2212 Es\u2032\u223cPh,f\u2217 (\u00b7|sh,ah))v(sh, ah, s \u2032)] \u2265 \u27e8W (i)h (f), Xh(\u03c0)\u27e9,\n(D.6)\n\u03bawit \u00b7 E(sh,ah)\u223c\u03c0[(Es\u2032\u223cPh,f (\u00b7|sh,ah) \u2212 Es\u2032\u223cPh,f\u2217 (\u00b7|sh,ah))V (i),\u03c0 h+1,f (s \u2032)] \u2264 \u27e8W (i)h (f), Xh(\u03c0)\u27e9 (D.7)\nfor all h \u2208 [H], where \u03bawit is a parameter. Here, V (i),\u03c0h+1,f is the value function of \u03c0 associated with agent i under model f . Moreover, these mappings satisfy the following regularity condition:\nsup h\u2208[H],\u03c0\u2208\u03a0pur \u2225Xh(\u03c0)\u2225 \u2264 1, sup h\u2208[H],f\u2208F,i\u2208[n] \u2225W (i)h (f)\u2225 \u2264 BW .\nCompared with the single-agent witness rank (Sun et al., 2019), the policy \u03c0 in the mapping Xh(\u03c0) and the expectation E(sh,ah)\u223c\u03c0 in (D.6) and (D.7) can be an arbitrary pure policy instead of the optimal policy \u03c0f of the model f . This stricter assumption is essential for general-sum MGs because we are interested in various equilibrium notions and each equilibrium can be non-unique. The following theorem shows that model classes with small multi-agent witness ranks have small MADCs.\nTheorem D.12 (Multi-Agent Witness Rank \u2286 Low MADC). Let F be a class of general-sum MGs whose multi-agent witness rank is no more than d. Then, for any f\u2217 \u2208 F , we have dMADC = O\u0303(Hd/\u03ba2wit), where dMADC is the multi-agent decoupling coefficient of f\u2217.\nProof. See \u00a7F.4 for detailed proof.\nThis theorem shows that the multi-agent decoupling coefficient is upper bounded by the multi-agent witness rank, which shows that the class of MG models with a finite multi-agent decoupling coefficient contains models with a finite multi-agent witness rank. Hence, many concrete MG models such as the multi-agent version of factor MDP and linear kernel MDP all have finite multi-agent decoupling coefficients. Therefore, applying Theorem 3.1 to models with a finite Multi-Agent witness rank, the model-based version of MAMEX achieves a O\u0303(nHd \u221a K/\u03ba2wit + nH \u221a K) regret with witness rank d. Note that for the model-based RL problems, our regret does not have the term log(|\u03a0pur|), because the discrepancy function \u2113(i),s in 2.5 is independent with \u03c0k. When applying our results to the single-agent setting, Theorem D.12 provides a similar regret result as in previous works (Sun et al., 2019; Zhong et al., 2022).\nAnother example of model-based RL problems is the linear mixture MG (Chen et al., 2022d), which assumes that the transition kernel P(s\u2032 | s, a) is a linear combination of d feature mappings {\u03d5i(s\u2032, s, a)}i\u2208[d], i.e. P(s\u2032 | s, a) = \u2211d i=1 \u03b8i\u03d5i(s \u2032, s, a), where a is a joint action.\nExample D.13 (Multi-Agent Linear Mixture MGs). We call one general-sum MG is a linear mixture MG with dimension d, if there exist h vectors {\u03b8h \u2208 Rd}h\u2208[H] and a known feature \u03d5(s\u2032 | s, a) \u2208 Rd such that \u2225\u03b8h\u22252 \u2264 \u221a d and Ph(s\u2032 | s, a) = \u27e8\u03b8h, \u03d5(s\u2032 | s, a)\u27e9 for any state-action pair (s\u2032, s, a) \u2208 S \u00d7 S \u00d7A.\nThe following theorem shows that a linear mixture general-sum MG has a finite multi-agent decoupling coefficient. Thus, MAMEX can be readily applied to these models with sample efficiency.\nTheorem D.14 (Multi-Agent Linear Mixture MGs \u2282 Low MADC). For a linear mixture MG with dimension d, we have dMADC = O\u0303(dHR4), where R is an upper bound on \u2211H h=1 rh.\nProof. See \u00a7F.5 for a detailed proof.\nChen et al. (2022d) provides a minimax-optimal O\u0303(dH \u221a K) regret for two-player zero-sum MGs for rh \u2208 [0, 1]. Now choose Fh = {\u03b8h \u2208 Rd}. Combining with Theorem D.14 and Theorem 3.1, and the fact that log(BF (1/K)) = O\u0303(Hd) (Liu et al., 2022a), MAMEX achieves a O\u0303(ndH5 \u221a K + ndH4) regret, where we set R = H . Compared with their regret upper bound, when applying our result to two-player zero-sum MGs by choosing n = 2, the leading term of our regret O\u0303(dH5 \u221a K) matches the minimax-optimal result in terms of d and K but with an extra multiplicative factor H2."
        },
        {
            "heading": "E PROOF OF MAIN RESULTS",
            "text": ""
        },
        {
            "heading": "E.1 PROOF OF MODEL-FREE VERSION OF THEOREM 3.1",
            "text": "Proof. We first consider learning Nash equilibrium and coarse correlated equilibrium.\nNE/CCE First, by Assumption 2.3, for any pure joint policy \u03c5, there exists a function f (i),\u03c5 \u2208 F (i) satisfies that it has no Bellman error with Bellman operator T (i),\u03c5 for any pure joint policy \u03c5, i.e.\nT (i),\u03c5h f (i),\u03c5 h+1 = f (i),\u03c5 h . (E.1)\nHence, {f (i),\u03c5h }h\u2208[H] is the Q-function of the agent i when all agents follow the policy \u03c5. Thus, we have\nV (i),\u03c5\nf(i),\u03c5 (\u03c1) = Es1\u223c\u03c1,a\u223c\u03c5(s1)[f (i),\u03c5 1 (s, a)] = Es1\u223c\u03c1,a\u223c\u03c5(s1)[Q (i),\u03c5 1 (s, a)] = V (i),\u03c5(\u03c1). (E.2)\nAlso, denote f\u0302 (i),\u03c5 = arg supf\u2208F(i) V\u0302 \u03c5 i (f) as the optimal function with respect to the regularized value V\u0302 (i),\u03c0(f) for the pure joint policy \u03c0 and agent i. Now we have\nE\u03c5\u223c\u03c0k [ V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121)\n] = E\u03c5\u223c\u03c0k [ sup\nf\u2208F(i) V\u0302 (i),\u03c5(f) ] \u2265 max\n\u03c5(i)\u2208\u03a0pur E\u03c5\u223c\u03c5(i)\u00d7\u03c0(\u2212i),k\n[ sup\nf\u2208F(i) V\u0302 (i),\u03c5(f)\n] . (E.3)\nThe inequality holds because of the property of Nash Equilibrium or Coarse Correlated Equilibrium. Then, since the best response \u03c0(i),k,\u2020 is a pure policy, we have\nmax \u03c5(i)\u2208\u03a0pur\nE\u03c5\u223c\u03c5(i)\u00d7\u03c0(\u2212i),k [\nsup f\u2208F(i)\nV\u0302 (i),\u03c5(f) ]\n\u2265 E\u03c5\u223c\u03c0(i),k,\u2020\u00d7\u03c0(\u2212i),k [\nsup f\u2208F(i)\nV\u0302 (i),\u03c5(f) ] = E\u03c5\u223c\u03c0(i),k,\u2020\u00d7\u03c0(\u2212i),k [ V\u0302 (i),\u03c5(f (i),\u03c5) ] \u2265 E \u03c5\u223c\u00b5(i),\u03c0k [ V (i),\u03c5 f(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f (i),\u03c5, \u03c5, \u03c41:k\u22121) ] , (E.4)\nwhere \u03c5 \u2208 \u03a0puri , \u00b5(i),\u03c0 k = (\u03c0(i),k,\u2020, \u03c0(\u2212i),k) and \u03c0(i),k,\u2020 is the best response given the action of other agents \u03c0(\u2212i),k. Thus, combining (E.3) and (E.4), we can derive\nE \u03c5\u223c\u00b5(i),\u03c0k\n[ V (i),\u03c5 f(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ]\n\u2264 \u03b7E \u03c5\u223c\u00b5(i),\u03c0k\n[ L(i),k\u22121(f (i),\u03c5, \u03c5, \u03c41:k\u22121) ] \u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] . (E.5)\nNow we provide the concentration lemma, which shows that the empirical discrepancy function L(i),k(f, \u03c0, \u03c41:k) is an estimate of the true discrepancy function \u2211k\u22121 s=0 \u2113 (i),s(f, \u03c0). Lemma E.1 (Concentration Lemma). For any k \u2208 [K] pure joint policy \u03c0, and {\u03b6s}k\u22121s=1 \u2208 \u03a0 that be executed in Algorithm 1 in the first k \u2212 1 episodes, with probability at least 1\u2212 \u03b4,\nL(i),k\u22121(f, \u03c0, \u03c41:k\u22121)\u2212 1 4 ( k\u22121\u2211 s=0 \u2113(i),s(f, \u03c0) ) \u2265 \u2212\u03b5conc,\nwhere \u03b5conc = max{O(HR2 log(HKmaxi\u2208[n] NF(i)(1/K)|\u03a0pur|/\u03b4)), H} and\n\u2113(i),s(f, \u03c0) = H\u2211 h=1 E(sh,ah)\u223c\u03b6sh [ ((fh \u2212 T (i),\u03c0h fh+1)(sh, ah)) 2 ] .\nProof. See \u00a7F.6 for a detailed proof.\nIn other words, if we define the event as\nE1 = { L(i),k(f, \u03c0, \u03c41:k)\u2212 1\n4 ( k\u22121\u2211 s=0 \u2113(i),s(f, \u03c0) ) \u2265 \u03b5conc,\u2200f \u2208 F (i), \u03c0 \u2208 \u03a0pur, k \u2208 [K] } ,\nwe have Pr{E1} \u2265 1\u2212 \u03b4. Note that the \u03b5conc contains log(|\u03a0pur|/\u03b4) in the logarithmic term, which arises from our policy-search style algorithm. Lemma E.2 (Optimal Concentration Lemma). For all index i \u2208 [n], all \u03c0 \u2208 \u03a0pur and function f (i),\u03c0 \u2208 F (i) such that T (i),\u03c0f (i),\u03c0 = f (i),\u03c0 , with probability at least 1\u2212 \u03b4, we have\nL(i),k(f (i),\u03c0, \u03c0, \u03c41:k) \u2264 \u03b5conc.\nProof. See \u00a7F.7 for a detailed proof.\nIn other words, if we define the event as\nE2 = {\u2200 i \u2208 [n], \u03c0 \u2208 \u03a0pur, L(i),k(f (i),\u03c0, \u03c0, \u03c41:k) \u2264 \u03b5conc}, we have Pr{E2} \u2265 1 \u2212 \u03b4. Lemma E.2 shows that the empirical discrepancy function L(i),k(f, \u03c0, \u03c41:k) is small if the function f and the policy \u03c0 are consistent, i.e. f = f (i),\u03c0 . Now by (E.5) and Lemma E.2, for any i \u2208 [n], under the event E2,\nE \u03c5\u223c\u00b5(i),\u03c0k\n[ V (i),\u03c5(\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] = E \u03c5\u223c\u00b5(i),\u03c0k [ V (i),\u03c5 f(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1)\n] = E \u03c5\u223c\u00b5(i),\u03c0k [ V (i),\u03c5 f(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ]\n\ufe38 \ufe37\ufe37 \ufe38 (a)\n+E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] .\nBy (E.5) and Lemma E.2, under event E2, (a) can be bounded by\n(a) \u2264 \u03b7E \u03c5\u223c\u00b5(i),\u03c0k\n[ L(i),k\u22121(f (i),\u03c5, \u03c5, \u03c41:k\u22121) ] \u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] (E.6)\n\u2264 \u03b7\u03b5conc \u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] . (E.7)\nNow by Assumption 2.5, on the events E1 and E2 we have\nReg(K) = K\u2211 k=1 n\u2211 i=1 ( V (i),\u00b5 (i),\u03c0k (\u03c1)\u2212 V (i),\u03c0 k (\u03c1) )\n= K\u2211 k=1 n\u2211 i=1 ( E \u03c5\u223c\u00b5(i),\u03c0k [ V (i),\u03c5(\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ]) \u2264\nK\u2211 k=1 n\u2211 i=1 ( \u03b7\u03b5conc \u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ]) . (E.8)\nNow since f\u0302 (i),\u03c5 = argmaxf\u2208F(i) [ V (i),\u03c5 f (\u03c1)\u2212 \u03b7L(i),k\u22121(f, \u03c5, \u03c41:k\u22121) ] is the optimal function with respect to the regularized value, under the event E2 we have\nV (i),\u03c5 f\u0302(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) \u2265 V (i),\u03c5 f(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f (i),\u03c5, \u03c5, \u03c41:k\u22121),\nthen we have \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) \u2265 0 and by \u03b7 \u2264 1,\n\u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) \u2264 V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1)\u2212 V (i),\u03c5 f(i),\u03c5 (\u03c1) + \u03b7L(i),k\u22121(f (i),\u03c5, \u03c5, \u03c41:k\u22121)\n\u2264 R+ \u03b7\u03b5conc \u2264 2\u03b5conc, where the last inequality follows the Lemma E.2. If we define\nL (i),k\u22121 2\u03b5conc (f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) = L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) \u00b7 I{\u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) \u2264 2\u03b5conc} and the event as\nE3 = { \u2200 i \u2208 [n], \u03c5 \u2208 \u03a0pur, L(i),k\u221212\u03b5conc (f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) = L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) } ,\nwe will have E3 \u2286 E2. Since the policy \u03b6k that algorithm executes is sampled from \u03c0k, then the sequence {Yk}Kk=1 that is defined by\nYk = E\u03c5\u223c\u03c0k [ V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)\u2212 V (i),\u03c5(\u03c1)\u2212 \u03b7L(i),k\u221212\u03b5conc (f\u0302\n(i),\u03c5, \u03c5, \u03c41:k\u22121) ]\n\u2212 ( V (i),\u03b6k\nf\u0302(i),\u03b6k (\u03c1)\u2212 V (i),\u03c5(\u03c1)\u2212 \u03b7L(i),k\u221212\u03b5conc (f\u0302\n(i),\u03b6k , \u03b6k, \u03c41:k\u22121) )\nis a martingale difference sequence. Now by Azuma-Hoeffding\u2019s inequality and Yk \u2264 R+2\u03b5conc \u2264 3\u03b5conc, with probability at least 1\u2212 \u03b4 we have\u2223\u2223\u2223\u2223\u2223 K\u2211 k=1 [ E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1)\u2212 V (i),\u03c5(\u03c1)\u2212 \u03b7L(i),k\u221212\u03b5conc (f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ]\n\u2212 ( V (i),\u03b6k\nf\u0302(i),\u03b6k (\u03c1)\u2212 V (i),\u03c5(\u03c1)\u2212 \u03b7L(i),k\u221212\u03b5conc (f\u0302 (i),\u03b6k , \u03b6k, \u03c41:k\u22121) )] \u2223\u2223\u2223\u2223\u2223 \u2264 O(\u03b5conc\u221aK). (E.9)\nDefine the event E4 as the (E.9) holds. Now by choosing \u03b74 = 1 \u00b5 = 1\u221a K and taking the union bound over the event E1, E2, E3 and E4, with probability at least 1\u2212 4\u03b4, we can get Reg(K)\n\u2264 n\u2211\ni=1 K\u2211 k=1 ( \u03b7\u03b5conc \u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ]) =\nn\u2211 i=1 K\u2211 k=1 ( \u03b7\u03b5conc \u2212 \u03b7E\u03c5\u223c\u03c0k [ L (i),k\u22121 2\u03b5conc (f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ]) \u2264\nn\u2211 i=1 K\u2211 k=1 ( \u03b7\u03b5conc \u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03b6 k , \u03b6k, \u03c41:k\u22121) + V (i),\u03b6k f\u0302(i),\u03b6 k (\u03c1)\u2212 V (i),\u03b6k (\u03c1) )\n\ufe38 \ufe37\ufe37 \ufe38 (b)\n+O\u0303(n\u03b5conc \u221a K).\n(E.10)\nThe first inequality holds because of Eq (E.8). The equality in the second line holds under Lemma E.2 (event E3 \u2286 E2). The second inequality is derived from Azuma-Hoeffding\u2019s inequality (event E4). Now using Lemma E.1 and MADC assumption, we can get (b) \u2264 \u2212 n\u2211\ni=1 K\u2211 k=1\n( \u03b7\n4 ( k\u22121\u2211 s=0 \u2113(i),s(f, \u03b6k) )) + n\u2211 i=1 K\u2211 k=1 ( V (i),\u03b6k f\u0302(i),\u03b6k (\u03c1)\u2212 V (i),\u03b6 k (\u03c1) ) + 4n \u221a K \u00b7 \u03b7\u03b5conc\n\u2264 n\u00b5 \u00b7 dMADC + 6dMADCH + 4n \u221a K\u03b5conc.\nThe second inequality uses Assumption 2.5. Now the regret can be bounded by\nReg(K) \u2264 n \u221a K \u00b7 dMADC + 6dMADCH + 4n \u221a K\u03b5conc +O(n\u03b5conc \u221a K)\n= O(n\u03b5conc \u221a K + ndMADCH + ndMADC \u221a K).\nHence, we complete the proof by noting that \u03b5conc = O\u0303(HR2 log\u03a5F,\u03b4).\nCE By changing the best response to the strategy modification, we can derive a proof for Correlated Equilibrium (CE). We simplify the notation of strategy modification as \u03d5i(\u03c5(i)) \u00d7 \u03c5(\u2212i) as \u03d5i(\u03c5). Now we have\nE\u03c5\u223c\u03c0k [ V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] = E\u03c5\u223c\u03c0k [ sup\nf\u2208F(i) V\u0302 (i),\u03c5(f) ] = max\n\u03d5i E\u03c5\u223c\u03c0k\n[ sup\nf\u2208F(i) V\u0302 (i),\u03d5i(\u03c5\n(i))\u00d7\u03c5(\u2212i)(f) ] . (E.11)\nThe second equality holds because of the property of Correlated Equilibrium. Now we have\nmax \u03d5i\nE\u03c5\u223c\u03c0k [\nsup f\u2208F(i)\nV\u0302 (i),\u03d5i(\u03c5 (i))\u00d7\u03c5(\u2212i)(f) ] \u2265 max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V (i),\u03d5i(\u03c5)\nf(i),\u03d5i(\u03c5) (\u03c1)\u2212 \u03b7L(i),k\u22121(f (i),\u03d5i(\u03c5), \u03d5i(\u03c5), \u03c41:k\u22121) ] \u2265 max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V (i),\u03d5i(\u03c5)\nf(i),\u03d5i(\u03c5) (\u03c1)\u2212 \u03b7\u03b5conc)\n] . (E.12)\nThe first equality holds by f (i),\u03d5i(\u03c5) \u2208 F (i) in (E.1), and the last inequality is derived from Lemma E.2 and \u03d5i(\u03c5) is a pure joint policy. Then, by combining (E.11) and (E.12), we can get\nmax \u03d5i\nE\u03c5\u223c\u03c0k [ V (i),\u03d5i(\u03c5) f(i),\u03d5i(\u03c5) (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ]\n\u2264 \u03b7\u03b5conc \u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) ] .\nHence, we can upper bound the regret of the agent i at k-th episode as\nmax \u03d5i\nE\u03c5\u223c\u03c0k [ V (i),\u03d5i(\u03c5 (i))\u00d7\u03c5(\u2212i)(\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] = max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V (i),\u03d5i(\u03c5) f(i),\u03d5i(\u03c5) (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] = max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V (i),\u03d5i(\u03c5) f(i),\u03d5i(\u03c5) (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] \u2264 \u03b7\u03b5conc \u2212 \u03b7E\u03c5\u223c\u03c0kL(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] .\nThe rest of the proof is the same as in NE/CCE after (E.7)."
        },
        {
            "heading": "E.2 PROOF OF MODEL-BASED VERSION OF THEOREM 3.1",
            "text": "Proof. We first consider NE/CCE.\nNE/CCE Denote f\u0302 (i),\u03c0 = arg supf\u2208F V\u0302 \u03c0i (f) as the optimal model with respect to the regularized value V\u0302 (i),\u03c0(f). Since for model-based RL problems, the empirical discrepancy function L(f, \u03c0, \u03c4) and \u2113(i),s(f, \u03c0) is independent with policy \u03c0, we simplify it as L(f, \u03c4) and \u2113(i),s(f). Then, from the definition of regularized value function V\u0302 (i),\u03c0(f), we have\nE\u03c5\u223c\u03c0k [ V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) ] = E\u03c5\u223c\u03c0k [ sup f\u2208F V\u0302 (i),\u03c5(f) ] \u2265 max \u03c5(i)\u2208\u03a0pur E\u03c5\u223c\u03c5(i)\u00d7\u03c0(\u2212i),k [ sup f\u2208F V\u0302 (i),\u03c5(f) ] . (E.13)\nThe inequality holds by the fact that \u03c0k is the NE/CCE of the regularized value function V\u0302 (i),\u03c0(f). Now since the best response \u03c0(i),k,\u2020 is a pure policy, we have\nmax \u03c5(i)\u2208\u03a0pur E\u03c5\u223c\u03c5(i)\u00d7\u03c0(\u2212i),k [ sup f\u2208F V\u0302 (i),\u03c5(f) ]\n\u2265 E\u03c5\u223c\u03c0(i),k,\u2020\u00d7\u03c0(\u2212i),k [ sup f\u2208F V\u0302 (i),\u03c5(f) ]\n\u2265 E \u03c5\u223c\u00b5(i),\u03c0k\n[ V\n(i),\u03c5 f\u2217 (\u03c1)\u2212 \u03b7L\n(i),k\u22121(f\u2217, \u03c41:k\u22121) ] . (E.14)\nThus, by combining E.13 and E.14, we have\nE \u03c5\u223c\u00b5(i),\u03c0k\n[ V\n(i),\u03c5 f\u2217 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ]\n\u2264 \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) ] . (E.15)\nNow we provide our concentration lemma for model-based RL problems. Lemma E.3 (Concentration Lemma for Model-Based RL Problems). With probability at least 1\u2212\u03b4, for any k \u2208 [K], f \u2208 F , for the executed policy {\u03b6s}k\u22121s=1 in Algorithm 1, we have\nL(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 L(i),k\u22121(f, \u03c41:k\u22121) \u2264 \u2212 k\u22121\u2211 s=1 \u2113(i),s(f) + \u03baconc, (E.16)\nwhere \u03baconc = max{2H log HBF (1/K)\u03b4 , H}, where BF (1/K) is the 1/K-bracketing number of the model class F . We also define the event E5 as the situation when (E.16) holds.\nProof. See \u00a7F.8 for detailed proof.\nBy Lemma E.3, for any i \u2208 [n],\nE \u03c5\u223c\u00b5(i),\u03c0k\n[ V (i),\u03c5(\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] = E\n\u03c5\u223c\u00b5(i),\u03c0k\n[ V\n(i),\u03c5 f\u2217 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ]\n\ufe38 \ufe37\ufe37 \ufe38 (a)\n+E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] . (E.17)\nNow substitute into equation (E.15), (a) \u2264 \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7E\u03c5\u223c\u03c0k [ L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) ] = E\u03c5\u223c\u03c0k [ \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) ] . (E.18)\nHence, combining with (E.17) and (E.18), we can get\nE \u03c5\u223c\u00b5(i),\u03c0k\n[ V (i),\u03c5(\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] \u2264 (a) + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1)\n] \u2264 E\u03c5\u223c\u03c0k [ \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) + V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)\u2212 V (i),\u03c5(\u03c1)\n] .\n(E.19)\nBy summing over k \u2208 [K] and I \u2208 [n], the regret can be obtained by\nReg(K)\n\u2264 n\u2211\ni=1 K\u2211 k=1 E\u03c5\u223c\u03c0k [ \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) + V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1)\u2212 V (i),\u03c5(\u03c1) ] .\n(E.20)\nNow we want to use Azuma-Hoeffding\u2019s inequality to transform \u03c5 \u223c \u03c0k to executed policy \u03b6k. To achieve this goal, note that by Lemma E.3, under event E5, we have\nL(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) \u2264 \u03baconc. (E.21)\nMoreover, since f\u0302 (i),\u03c5 achieves the maximum value of the regularized value function V\u0302 (i),\u03c0(f) = V\n(i),\u03c5 f (\u03c1)\u2212 L(i),k\u22121(f\u2217, \u03c41:k\u22121), we have\nL(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) \u2265 E \u03c5\u223c\u00b5(i),\u03c0k\n[ V\n(i),\u03c5 f\u2217 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f(i),\u03c5 (\u03c1) ]\n\u2265 \u2212R \u2265 \u2212\u03baconc.\nThus, if we define L(i),\u03c5\u03b5 = ( L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) ) \u00b7 I { |L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121)| \u2264 \u03b5 } ,\nwe can have |L(i),\u03c5\u03baconc | \u2264 \u03baconc is bounded under event E5. Then, with probability at least 1 \u2212 \u03b4, L(i),\u03c5\u03baconc = L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121). Then, we can apply Azuma-Hoeffding\u2019s inequality to transform the expectation to the executed policy \u03b6k.\u2223\u2223\u2223\u2223\u2223 K\u2211 k=1 L(i),\u03b6 k \u03baconc \u2212 K\u2211 k=1 E\u03c5\u223c\u03c0k [ L(i),\u03c5\u03baconc ]\u2223\u2223\u2223\u2223\u2223 = O(\u03baconc \u00b7 logK). (E.22)\nNow by taking the union bound of Azuma-Hoeffding\u2019s inequality and event E5, with probability at least 1\u2212 2\u03b4,\nReg(K) \u2264 n\u2211\ni=1 K\u2211 k=1 E\u03c5\u223c\u03c0k [ \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) + V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1)\u2212 V (i),\u03c5(\u03c1) ] =\nn\u2211 i=1 K\u2211 k=1 E\u03c5\u223c\u03c0k [ \u03b7L\u03c5\u03baconc + V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1)\u2212 V (i),\u03c5(\u03c1) ] \u2264\nn\u2211 i=1 K\u2211 k=1 ( \u03b7L\u03b6 k \u03baconc + V (i),\u03b6k f\u0302(i),\u03b6 k (\u03c1)\u2212 V (i),\u03b6k (\u03c1) )\n\ufe38 \ufe37\ufe37 \ufe38 (b)\n+O\u0303(n\u03baconc),\nwhere the first inequality holds by (E.20), the equality holds under event E5, and the last inequality holds by (E.22). Then, by Lemma E.3, under event E5, we have\n(b) = n\u2211\ni=1 K\u2211 k=1 ( \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03b6 k , \u03c41:k\u22121) + V (i),\u03c5 f\u0302(i),\u03b6k (\u03c1)\u2212 V (i),\u03b6 k (\u03c1) )\n\u2264 n\u2211\ni=1 K\u2211 k=1\n( \u2212\u03b7\nk\u22121\u2211 s=1 \u2113(i),s(f\u0302 (i),\u03b6 k ) + \u03b7\u03baconc + V (i),\u03c5 f\u0302(i),\u03b6k (\u03c1)\u2212 V (i),\u03b6 k (\u03c1)\n) .\nThen, by Assumption 2.5, (b) can be further upper bounded by\n(b) \u2264 n\u2211\ni=1 K\u2211 k=1\n( \u2212\u03b7\nk\u22121\u2211 s=1 \u2113(i),s(f\u0302 (i),\u03b6 k ) + \u03b7\u03baconc + V (i),\u03c5 f\u0302(i),\u03b6k (\u03c1)\u2212 V (i),\u03b6 k (\u03c1) ) \u2264 n\u03b7K\u03baconc + n\n\u03b7 dMADC + 6ndMADCH\n= O\u0303(n\u03baconc \u221a K + ndMADC \u221a K + ndMADCH).\nThe first inequality holds by Lemma E.3. The last equality holds by \u03b7 = 4/ \u221a K. Finally, the regret can be bounded by\nReg(K) \u2264 (b) + O\u0303(n\u03baconc) = O\u0303(n\u03baconc \u221a K + ndMADC \u221a K + ndMADCH).\nThus, we complete the proof by noting that \u03baconc = O(H)\nCorrelated Equilibrium Similar to model-free problems, we only need to replace the best response with strategy modification.\nE\u03c5\u223c\u03c0k [ V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) ] = E\u03c5\u223c\u03c0k [ sup f\u2208F V\u0302 (i),\u03c5(f) ]\n= max \u03d5i E\u03c5\u223c\u03c0k [ sup f\u2208F V\u0302 (i),\u03d5i(\u03c5)(f) ] .\nThe last equality uses the property that \u03c0k is a CE with respect to the payoff function supf\u2208F V\u0302 (i),\u03c5(f). Then, since f\u2217 \u2208 F , we can further derive\nE\u03c5\u223c\u03c0k [ V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)\u2212 \u03b7L(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121) ] \u2265 max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V\u0302 (i),\u03d5i(\u03c5)(f\u2217) ] = max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V\n(i),\u03d5i(\u03c5) f\u2217 (\u03c1)\u2212 \u03b7L\n(i),k\u22121(f\u2217, \u03c41:k\u22121) ] . (E.23)\nThe second equality holds by the property of CE. Thus, we have\nmax \u03d5i\nE\u03c5\u223c\u03c0k [V (i),\u03d5i(\u03c5) f\u2217 (\u03c1)]\u2212 E\u03c5\u223c\u03c0k [V (i),\u03c5\nf\u0302(i),\u03c5 (\u03c1)]\n\u2264 \u03b7L(i),k\u22121(f\u2217, \u03c41:k\u22121)\u2212 \u03b7E\u03c5\u223c\u03c0kL(i),k\u22121(f\u0302 (i),\u03c5, \u03c41:k\u22121). (E.24)\nHence, combining with (E.23) and (E.24), we can upper bound the regret of the agent i at k-th episode as\nmax \u03d5i\nE\u03c5\u223c\u03c0k [ V (i),\u03d5i(\u03c5 (i))\u00d7\u03c5(\u2212i)(\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] = max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V (i),\u03d5i(\u03c5) f(i),\u03d5i(\u03c5) (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] = max\n\u03d5i E\u03c5\u223c\u03c0k\n[ V (i),\u03d5i(\u03c5) f(i),\u03d5i(\u03c5) (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] \u2264 \u03b7\u03b5conc \u2212 \u03b7E\u03c5\u223c\u03c0kL(i),k\u22121(f\u0302 (i),\u03c5, \u03c5, \u03c41:k\u22121) + E\u03c5\u223c\u03c0k [ V (i),\u03c5 f\u0302(i),\u03c5 (\u03c1) ] \u2212 E\u03c5\u223c\u03c0k [ V (i),\u03c5(\u03c1) ] .\nThe rest of the proof is the same as NE/CCE after (E.15)."
        },
        {
            "heading": "E.3 SAMPLE COMPLEXITY RESULTS",
            "text": "we can utilize the standard online-to-batch techniques to transform the regret result in Theorem 3.1 into the sample complexity result. Corollary E.4. Under the same setting as in Theorem 3.1, with probability at least 1\u2212\u03b4, whenK \u2265 O\u0303 (( n2H2 + n2d2MADC\u03a5 2 F,\u03b4 ) \u00b7 \u03b5\u22122 ) , if we output the mixture policy \u03c0out = Unif({\u03c0k}k\u2208[K]), the output policy \u03c0out is a \u03b5-{NE, CCE, CE}.\nProof. See \u00a7F.9 for the proof.\nCorollary E.4 shows that MAMEX is sample-efficient for learning all three equilibria of general-sum MGs under general function approximation."
        },
        {
            "heading": "F PROOF OF THEOREMS AND LEMMAS",
            "text": ""
        },
        {
            "heading": "F.1 PROOF OF THEOREM D.4",
            "text": "Proof. The proof follows Proposition 3 in Dann et al. (2021). First, we provide the following lemma in Dann et al. (2021). Lemma F.1. For any positive real number sequence x1, \u00b7 \u00b7 \u00b7 , xn, we have\u2211n\ni=1 xi\u221a\u2211n i=1 ix 2 i\n\u2264 \u221a 1 + log n.\nNow denote E\u03b5 = dimMABE(F , \u03b5). We fix i \u2208 [n], and ignore both h and i for simplicity. Also denote e\u0302s,kh = E\u03c0s [\u03d5t] and e s,k h = e\u0302 s,k h \u00b7 I{e\u0302 s,k h > \u03b5}, where \u03d5t = (I \u2212 T (i),\u03c0k h )fh \u2208 F (i) h . We initializeK bucketsB0h, \u00b7 \u00b7 \u00b7 , B K\u22121 h , and we want to add element e k,k h for k \u2208 [K] into these buckets one by one. The rule for adding elements is as follows: If ek,kh = 0, we do not add it to any buckets. Otherwise we go through all buckets from B0h to B K\u22121 h , and add e k,k h to B\ni h whenever\u2211\ns\u2264t\u22121,s\u2208Bih\n(es,kh ) 2 < (ek,kh ) 2.\nNow assume we add ek,kh into the bucket b k h. Then, for all 1 \u2264 i \u2264 bkh \u2212 1, we have (e k,k h ) 2 \u2264\u2211 s\u2264k\u22121,s\u2208Bih (es,kh ) 2. Thus,\nK\u2211 k=1 k\u2211 s=1 (es,kh ) 2 \u2265 K\u2211 k=1 \u2211 0\u2264i\u2264bkh\u22121 \u2211 s\u2264t\u22121,s\u2208Bih (es,kh ) 2 \u2265 K\u2211 k=1 bkh(e k,k h ) 2. (F.1)\nNow note that by the definition of \u03b5-independent sequence, for the measures in Bih {\u03c0k1 , \u00b7 \u00b7 \u00b7 , \u03c0kj}, \u03c0k is a \u03b5\u2032-independent from all predecessors \u03c0k1 , \u00b7 \u00b7 \u00b7 , \u03c0kj\u22121 such that \u03b5\u2032 > \u03b5. (We can choose\n\u03b5\u2032 = ek,kh \u2212 c for enough small c such that \u221a\u2211 s\u2264t\u22121,s\u2208Bih (es,kh )\n2 \u2264 \u03b5\u2032 and \u03b5\u2032 > \u03b5 by ek,kh > \u03b5.) Thus, from the definition of BE dimension, the size of each bucket cannot exceed E\u03b5. Now by Jensen\u2019s inequality, we can get\nK\u2211 k=1 bkh(e k,k h ) 2 = K\u22121\u2211 i=1 i \u00b7 \u2211 s\u2208Bih (es,sh ) 2 \u2265 K\u22121\u2211 i=1 i|Bih| \u2211 s\u2208Bih es,sh |Bih| 2 \u2265 K\u22121\u2211 i=1 iE\u03b5 \u2211 s\u2208Bih es,sh E\u03b5 2 , (F.2)\nwhere the last inequality uses the fact that |Bih| \u2264 E\u03b5. Let xi = \u2211 s\u2208Bih (es,sh ). By Lemma F.1, we have\nK\u22121\u2211 i=1 iE\u03b5 \u2211 s\u2208Bih es,sh E\u03b5 2 = 1 E\u03b5 K\u22121\u2211 i=1 i \u00b7 \u2211 s\u2208Bih es,sh 2 \u2265 1 E\u03b5(1 + logK)  \u2211 s\u2208[K]\\B0h es,sh 2 . (F.3)\nHence, combining (F.1), (F.2) and (F.3), we can get\n\u2211 s\u2208[K]\\B0h es,sh \u2264\n( E\u03b5(1 + logK)\nK\u2211 k=1 bkh(e k,k h ) 2\n)1/2\n\u2264 ( E\u03b5(1 + logK)\nK\u2211 k=1 k\u2211 s=1 (es,kh ) 2\n)1/2 .\nNow by the definition es,kh = e\u0302 s,k h \u00b7 I{e\u0302 s,k h > \u03b5} and the fact that |B0h| \u2264 E\u03b5, we can have\nH\u2211 h=1 K\u2211 k=1 e\u0302k,kh \u2264 HK\u03b5+ H\u2211 h=1 K\u2211 k=1 ek,kh\n\u2264 HK\u03b5+min{HE\u03b5, HK}+ H\u2211\nh=1 \u2211 s\u2208[K]\\B0h es,sh .\nThen, by (F.3), we can further bounded it by\nH\u2211 h=1 K\u2211 k=1 e\u0302k,kh \u2264 HK\u03b5+min{HE\u03b5, HK}+ H\u2211 h=1\n( E\u03b5(1 + logK)\nK\u2211 k=1 k\u22121\u2211 s=1 (es,kh ) 2\n)1/2\n\u2264 HK\u03b5+min{HE\u03b5, HK}+ ( E\u03b5H(1 + logK)\nH\u2211 h=1 K\u2211 k=1 k\u22121\u2211 s=1 (es,kh ) 2\n)1/2 . (F.4)\nThe last inequality uses the Jensen\u2019s inequality Now we can use a similar technique in Xie et al. (2021). Define (r\u2032h) (i)(s, a) = fkh (s, a)\u2212 Es\u2032\u223cPh(\u00b7|s,a)\u27e8fkh+1(s\u2032, \u00b7), \u03c0kh+1(\u00b7 | s\u2032)\u27e9. Then, we have\nEs1\u223c\u03c1 [ fk1 (s1, \u03c0 k 1 (s1)) ] = E\u03c0k\n[ H\u2211\nh=1\n( fkh (sh, \u03c0 k h(sh))\u2212 fkh+1(sh+1, \u03c0kh+1(sh+1))\n)]\n= E\u03c0k\n[ H\u2211\nh=1\n( fkh (sh, \u03c0 k h(sh))\u2212 Es\u2032\u223cPh(\u00b7|s,a)\u27e8f k h+1(s \u2032, \u00b7), \u03c0kh+1(\u00b7 | s\u2032)\u27e9 )]\n= E\u03c0k\n[ H\u2211\nh=1\n(r\u2032h) (i)(s, a)\n] .\nHence, we can rewrite the regret of the k-th episode as\nEs1\u223c\u03c1 [ (fk1 (s1, \u03c0 k 1 (s1))\u2212 V (i),\u03c0 k (s1)) ] = E\u03c0k\n[ H\u2211\nh=1\n((r\u2032h) (i)(s, a)\u2212 r(i)h (s, a))\n] . (F.5)\nThe last inequality uses the fact that Then, substitute into the definition of (r\u2032h) (i), we can get Es1\u223c\u03c1 [ (fk1 (s1, \u03c0 k 1 (s1))\u2212 V (i),\u03c0 k (s1)) ]\n= E\u03c0k\n[ H\u2211\nh=1\n(fkh (s, a)\u2212 Es\u2032\u223cPh(\u00b7|s,a)\u27e8f k h+1(s \u2032, \u00b7), \u03c0kh+1(\u00b7 | s\u2032)\u27e9 \u2212 r (i) h (s, a))\n]\n= E\u03c0k\n[ H\u2211\nh=1\n(I \u2212 T (i),\u03c0 k\nh )(f k h )\n] =\nH\u2211 h=1 e\u0302k,kh ,\nwhere the first equality holds by the definition of Mk, the second equality holds by decomposing the value function to the expected cumulative sum of the reward function, and the last equality is derived by the definition of e\u0302k,kh . Now we can get\nReg(K) \u2264 K\u2211\nk=1 H\u2211 h=1 e\u0302k,kh\n\u2264 HK\u03b5+min{HE\u03b5, HK}+ ( (E\u03b5H \u00b7 2 logK)\nH\u2211 h=1 K\u2211 k=1 k\u22121\u2211 s=1 (es,kh ) 2\n)1/2 . (F.6)\nThe last inequality holds by (F.4). Now by the definition es,kh \u2264 e\u0302 s,k h = E\u03c0s [(I \u2212 T\n(i),\u03c0k\nh )(fh)] and the basic inequality \u221a ab \u2264 \u00b5a+ b/\u00b5 for \u00b5 > 0, we can derive\nReg(K)\n\u2264 HK\u03b5+min{HE\u03b5, HK}+ \u00b5 \u00b7 (E\u03b5H \u00b7 2 logK) + 1\n\u00b5 H\u2211 h=1 K\u2211 k=1 k\u22121\u2211 s=1 ( E\u03c0s [ (I \u2212 T (i),\u03c0 k h )(fh) ])2\n\u2264 HK\u03b5+min{HE\u03b5, HK}+ \u00b5 \u00b7 (E\u03b5H \u00b7 2 logK) + 1\n\u00b5 H\u2211 h=1 K\u2211 k=1 k\u22121\u2211 s=1 E\u03c0s [( I \u2212 T (i),\u03c0 k h )(fh) )2] .\nThe last inequality holds by (E[X])2 \u2264 E[X2]. Thus, by choosing \u03b5 = 1/K, we can derive\nReg(K) \u2264 H +HE1/K + \u00b5(E1/KH \u00b7 2 logK) + 1\n\u00b5 H\u2211 h=1 K\u2211 k=1 k\u22121\u2211 s=1 E\u03c0s [( I \u2212 T (i),\u03c0 k h )(fh) )2]\n\u2264 6dMADCH + \u00b5dMADC + 1\n\u00b5 H\u2211 h=1 K\u2211 k=1 k\u22121\u2211 s=1 E\u03c0s [( I \u2212 T (i),\u03c0 k h )(fh) )2] ,\nwhere dMADC = max{2E1/KH logK, 1} = O(2dimMABE(F , 1/K)H logK). The last inequality uses the fact that logK \u2265 1 and H \u2265 1."
        },
        {
            "heading": "F.2 PROOF OF THEOREM D.5",
            "text": "Proof. For any policy \u03c0 and i \u2208 [n], assume \u03b4z1 , \u00b7 \u00b7 \u00b7 , \u03b4zm is an \u03b5-independent sequence with respect to \u22c3 \u03c0\u2208\u03a0pur F (i),\u03c0 h = \u22c3 \u03c0\u2208\u03a0pur(I \u2212 T (i),\u03c0)F , where \u03b4z1 , \u03b4z2 , \u00b7 \u00b7 \u00b7 , \u03b4zm \u2208 D\u2206, i.e. \u03b4zi is a Dirichlet probability measure over S \u00d7 A that \u03b4zi = \u03b4(s,a)(\u00b7). Then, for each j \u2208 [m], there exist function f j \u2208 F (i) and policy \u03c0j \u2208 \u03a0 such that |(I \u2212 T (i),\u03c0j )fj(zj)| > \u03b5 and\u221a\u2211j\u22121\np=1 |(f p h \u2212 T (i),\u03c0 pfph+1)(zp)|2 \u2264 \u03b5. Define g j h = T (i),\u03c0\nj\nf jh+1, by Assumption 2.3, we have\ngjh \u2208 F (i) h \u2286 Fh. Thus, |(f j h \u2212 g j h)(zj)| > \u03b5 and \u221a\u2211j p=1 |(f p h \u2212 g p h)(zp)|2 < \u03b5. Thus, by the definition of eluder dimension, we have m \u2264 dimE(Fh, \u03b5). Hence, for all i and policy \u03c0,\ndimE(Fh, \u03b5) \u2265 m \u2265 max h\u2208[H] max i\u2208[n] dimDE ( \u22c3 \u03c0\u2208\u03a0pur F (i),\u03c0h ,Dh,\u2206, \u03b5 ) ,\nwhich concludes the proof."
        },
        {
            "heading": "F.3 PROOF OF THEOREM D.8",
            "text": "Proof. First, by the elliptical potential lemma introduced in Lemma G.3, if we define \u039bk,h = \u03b5I +\u2211k\u22121 s=1 xk,hx T k,h, for any {xk,h}Kk=1 \u2208 Xh we have\nK\u2211 k=1 H\u2211 h=1 min { 1, \u2225xk,h\u22252\u039b\u22121k,h } \u2264 H\u2211 h=1 2 log det ( I + 1 \u03b5 K\u2211 k=1 xk,hx T k,h ) = 2\u03b3K(\u03b5,X ). (F.7)\nNow denote \u03a3k,h = \u03b5I + \u2211k\u22121 s=1 Xh(\u03c0 s)Xh(\u03c0 s)T . Similar to Section F.1, define (r\u2032h) (i)(s, a) = fkh (s, a)\u2212 Es\u2032\u223cPh(\u00b7|s,a)\u27e8fkh+1(s\u2032, \u00b7), \u03c0kh+1(\u00b7 | s\u2032)\u27e9 \u2208 [\u22121, 1], then we can have\nEs1\u223c\u03c1 [ fk1 (s1, \u03c0 k 1 (s1))\u2212 V (i),\u03c0 k (s1) ] = E\u03c0k\n[ H\u2211\nh=1\n((r\u2032h) (i)(s, a)\u2212 r(i)h (s, a))\n] .\nThen, we can substitute the definition of (r\u2032h) (i) and derive Es1\u223c\u03c1 [ fk1 (s1, \u03c0 k 1 (s1))\u2212 V (i),\u03c0 k (s1) ]\n= E\u03c0k\n[ H\u2211\nh=1\n(fkh (s, a)\u2212 Es\u2032\u223cPh(\u00b7|s,a)\u27e8f k h+1(s \u2032, \u00b7), \u03c0kh+1(\u00b7 | s\u2032)\u27e9 \u2212 r (i) h (s, a))\n]\n= H\u2211 h=1 min {\u2223\u2223\u2223\u2223\u27e8W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k), Xh(\u03c0k)\u27e9V \u2223\u2223\u2223\u2223 , 2R} . Then, by min{x, 2R} \u2264 2Rmin{x, 1}, we have\nReg(K) = K\u2211\nk=1\nEs1\u223c\u03c1 [ (fk1 (s1, \u03c0 k 1 (s1))\u2212 V (i),\u03c0 k (s1)) ]\n\u2264 2R K\u2211\nk=1 H\u2211 h=1 min {\u2223\u2223\u2223\u2223\u27e8W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k ), Xh(\u03c0k)\u27e9V \u2223\u2223\u2223\u2223 , 1}\n= 2R K\u2211 k=1 H\u2211 h=1 min {\u2223\u2223\u2223\u2223\u27e8W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k ), Xh(\u03c0k)\u27e9V \u2223\u2223\u2223\u2223 , 1}\n\u00b7 ( I {\u2225\u2225\u2225Xh(\u03c0k)\u2225\u2225\u2225 \u03a3\u22121\nk,h\n\u2264 1 } + I {\u2225\u2225\u2225Xh(\u03c0k)\u2225\u2225\u2225 \u03a3\u22121\nk,h\n> 1 }) . (F.8)\nThe last inequality is because 1 = I{E}+ I{\u00acE} for any event. Now we decompose the (F.8) into two terms A+B, where\nA = 2R K\u2211 k=1 H\u2211 h=1 min {\u2223\u2223\u2223\u2223\u27e8W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k ), Xh(\u03c0k)\u27e9V \u2223\u2223\u2223\u2223 , 1} \u00b7 I {\u2225\u2225\u2225Xh(\u03c0k)\u2225\u2225\u2225 \u03a3\u22121 k,h \u2264 1 } ,\n(F.9)\nB = 2R K\u2211\nk=1 H\u2211 h=1 min {\u2223\u2223\u2223\u2223\u27e8W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k ), Xh(\u03c0k)\u27e9V \u2223\u2223\u2223\u2223 , 1} \u00b7 I {\u2225\u2225\u2225Xh(\u03c0k)\u2225\u2225\u2225 \u03a3\u22121 k,h > 1 } .\n(F.10)\nNow we bound A and B respectively. For A, we can use Cauchy\u2019s inequality and get\nA \u2264 2R K\u2211\nk=1 H\u2211 h=1 \u2225\u2225\u2225\u2225W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k)\u2225\u2225\u2225\u2225 \u03a3k,h \u00b7 \u2225\u2225Xh(\u03c0k)\u2225\u2225\u03a3\u22121k,h \u00b7 I{\u2225\u2225Xh(\u03c0k)\u2225\u2225\u03a3\u22121k,h \u2264 1}\n\u2264 2R K\u2211\nk=1 H\u2211 h=1 \u2225\u2225\u2225\u2225W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k)\u2225\u2225\u2225\u2225 \u03a3k,h \u00b7min {\u2225\u2225Xh(\u03c0k)\u2225\u2225\u03a3\u22121k,h , 1} .\n(F.11)\nThe first inequality holds by Cauchy\u2019s inequality that |\u27e8X,Y \u27e9| \u2264 \u2225X\u2225\u03a3\u2225Y \u2225\u03a3\u22121 .\nNow by the definition \u03a3k,h = \u03b5I + \u2211k\u22121 s=1 Xh(\u03c0 s)Xh(\u03c0\ns)T , we expand the term \u2225W (i)h (fk, \u03c0k)\u2212 W\n(i) h (f\n\u00b5(i),\u03c0 k\n, \u00b5(i),\u03c0 k )\u2225\u03a3k,h as\u2225\u2225\u2225\u2225W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k )\u2225\u2225\u2225\u2225 \u03a3k,h\n= [ \u03b5 \u00b7 \u2225\u2225\u2225W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k )\u2225\u2225\u22252\n2 + k\u22121\u2211 s=1 \u2223\u2223\u2223\u27e8W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k ), Xh(\u03c0s)\u27e9\u2223\u2223\u22232 ]1/2\n\u2264 2 \u221a \u03b5BW + [ k\u22121\u2211 s=1 \u2223\u2223\u2223\u27e8W (i)h (fk, \u03c0k)\u2212W (i)h (f\u00b5(i),\u03c0k , \u00b5(i),\u03c0k ), Xh(\u03c0s)\u27e9\u2223\u2223\u22232 ]1/2 .\nThe last inequality holds by \u221a a+ b \u2264 \u221a a+ \u221a b. Then, we can get\nK\u2211 k=1 H\u2211 h=1 \u2225W (i)h (f k, \u03c0k)\u2212W (i)h (f \u00b5(i),\u03c0 k , \u00b5(i),\u03c0 k )\u2225\u03a3k,h min { \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h , 1 }\n\u2264 K\u2211\nk=1 H\u2211 h=1 2\u221a\u03b5BW + [k\u22121\u2211 s=1 |\u27e8W (i)h (f k, \u03c0k)\u2212W (i)h (f \u00b5(i),\u03c0 k , \u00b5(i),\u03c0 k ), Xh(\u03c0 s)\u27e9|2 ]1/2 \u00b7min { \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h , 1 } \u2264 A1 +A2. (F.12)\nwhere A1 and A2 are defined as follows:\nA1 =\n( K\u2211\nk=1 H\u2211 h=1 4\u03b5B2W\n)1/2 \u00b7 ( K\u2211\nk=1 H\u2211 h=1 min { \u2225Xh(\u03c0k)\u22252\u03a3\u22121 k,h , 1 })1/2\nA2 =\n( K\u2211\nk=1 H\u2211 h=1 k\u22121\u2211 s=1 |\u27e8W (i)h (f k, \u03c0k)\u2212W (i)h (f \u00b5(i),\u03c0 k , \u00b5(i),\u03c0 k ), Xh(\u03c0 s)\u27e9|2\n)1/2\n\u00b7\n( K\u2211\nk=1 H\u2211 h=1 min{\u2225Xh(\u03c0k)\u22252\u03a3\u22121 k,h , 1}\n)1/2 ,\nNow we bound A1 and A2 respectively. First, for A1, using (F.7), we have A1 \u2264 \u221a 4\u03b5KHB2W \u00b7 2\u03b3K(\u03b5,X ).\nThen, for A2, we have\nA2 =\n( K\u2211\nk=1 H\u2211 h=1 k\u22121\u2211 s=1 |\u27e8W (i)h (f k, \u03c0k)\u2212W (i)h (f \u00b5(i),\u03c0 k , \u00b5(i),\u03c0 k ), Xh(\u03c0 s)\u27e9|2\n)1/2\u221a 2\u03b3K(\u03b5,X )\n=\n( K\u2211\nk=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k)\n)1/2\u221a 2\u03b3K(\u03b5,X ).\nThe equality holds by the definition of \u2113(i),s and the definition of multi-agent bilinear class (D.3). Then, since \u221a ab \u2264 a\u00b5+ b/\u00b5 for any \u00b5 > 0, we can further derive\nA2 \u2264 2R\u00b5 \u00b7 2\u03b3K(\u03b5,X ) + 1\n2R\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k).\nNow by adding A1 and A2 and combining with (F.11) and (F.12), we can finally get A \u2264 2R(A1 +A2) \u2264 \u221a 4\u03b5KHB2W \u00b7 8R\u03b3K(\u03b5,X ) + \u00b5 \u00b7 8R 2\u03b3K(\u03b5,X ) + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k)\n\u2264 32RB2W \u03b5HK + \u03b3K(\u03b5,X ) + \u00b5 \u00b7 8R2\u03b3K(\u03b5,X ) + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k).\nNow we have complete the bound of A. For B, by (F.7), since I{x > 1} \u2264 min{1, x2}, we know that\nK\u2211 k=1 H\u2211 h=1 I { \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h > 1 } \u2264 K\u2211 k=1 H\u2211 h=1 min { 1, \u2225xk,h\u22252\u03a3\u22121k,h } \u2264 2\u03b3K(\u03b5,X ). (F.13)\nThus, by the definition of B in (F.10), we can derive\nB \u2264 2 K\u2211\nk=1 H\u2211 h=1 I { \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h > 1 } \u2264 2\u03b3K(\u03b5,X ).\nNow note that Reg(K) \u2264 A + B, then by choosing \u03b5 = 1/32RKB2W and dMADC = max{1, 8R2\u03b3K(\u03b5,X )} we can derive\nReg(K) \u2264 A+B\n\u2264 32RB2W \u03b5HK + 3\u03b3K(\u03b5,X ) + \u00b5 \u00b7 8R2\u03b3K(\u03b5,X ) + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k)\n= H + 3\u03b3K(\u03b5,X ) + \u00b5 \u00b7 8R2\u03b3K(\u03b5,X ) + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k)\n\u2264 6dMADCH + \u00b5dMADC + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k).\nThe last inequality uses the fact that dMADC \u2265 1, H \u2265 1. Hence, we complete the proof."
        },
        {
            "heading": "F.4 PROOF OF THEOREM D.12",
            "text": "Proof. In this subsection, we give a detailed proof of Theorem D.12. First, similar to the performance difference lemma in Jiang et al. (2017), we have\nEs1\u223c\u03c1 [ V (i),\u03c0k 1,f (s1)\u2212 V (i),\u03c0k 1 (s1) ]\n= E\u03c0k [ Q (i),\u03c0k 1,f (s1, a1) ] \u2212 E\u03c0k\n[ H\u2211\nh=1\nr (i) h (sh, ah)\n]\n= E\u03c0k\n[ H\u2211\nh=1\n( Q (i),\u03c0k\nh,f (sh, ah)\u2212 r (i) h (sh, ah)\u2212Q\n(i),\u03c0k h+1,f (sh+1, ah+1) )] . (F.14)\nThe last equality holds by splitting the term. Now, since E\u03c0k [ Q (i),\u03c0k h+1,f (sh+1, ah+1) ] = E\u03c0k [ V (i),\u03c0k h+1,f (sh+1) ] = E\u03c0k [ Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah) [ V (i),\u03c0k h+1,f (sh+1) ]] ,\n(F.15)\nwe can rewrite (F.14) as Es1\u223c\u03c1 [ V (i),\u03c0k 1,f (s1)\u2212 V (i),\u03c0k 1 (s1) ]\n= E\u03c0k\n[ H\u2211\nh=1\n( Q (i),\u03c0k\nh,f (sh, ah)\u2212 r (i) h (sh, ah)\u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah)V\n(i),\u03c0k h+1,f (sh+1) )]\n= K\u2211 k=1 E\u03c0k\n[ H\u2211\nh=1\n(Esh+1\u223cPh,fk (\u00b7|sh,ah) \u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah)) [ V (i),\u03c0k h+1,fk (sh+1) ]] . (F.16)\nThen, combining (F.16) and the definition of multi-agent witness rank (D.7), we can derive K\u2211\nk=1\nEs1\u223c\u03c1 [ V (i),\u03c0k 1,fk (s1)\u2212 V (i),\u03c0 k 1 (s1) ]\n\u2264 K\u2211\nk=1 H\u2211 h=1 min { R, 1 \u03bawit |\u27e8Wh(fk), Xh(\u03c0k)\u27e9| }\n\u2264 K\u2211\nk=1 H\u2211 h=1 min { R, 1 \u03bawit |\u27e8Wh(fk), Xh(\u03c0k)\u27e9| }( I { \u2225Xh(\u03c0k)\u2225\u03a3\u22121 k,h \u2264 1 } + I { \u2225Xh(\u03c0k)\u2225\u03a3\u22121 k,h \u2265 1 }) .\n(F.17) Now note that\nK\u2211 k=1 min { 1, \u2225Xh(\u03c0k)\u22252\u03a3\u22121k,h } \u2264 2d log ( \u03b5+K \u03b5 ) \u225c D(\u03b5).\nand I{x > 1} \u2264 min{1, x2}, we can derive K\u2211\nk=1 H\u2211 h=1 I{\u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h > 1} \u2264 D(\u03b5)H. (F.18)\nThen, combining (F.17) and (F.18), we can get K\u2211\nk=1\nEs1\u223c\u03c1 [ V (i),\u03c0k 1,fk (s1)\u2212 V (i),\u03c0 k 1 (s1) ]\n\u2264 R K\u2211\nk=1 H\u2211 h=1 min { 1, 1 \u03bawit |\u27e8Wh(fk), Xh(\u03c0k)\u27e9| } \u00b7 ( I{\u2225Xh(\u03c0k)\u2225\u03a3\u22121 k,h \u2264 1}+ I{\u2225Xh(\u03c0k)\u2225\u03a3\u22121 k,h > 1} ) \u2264 R\nK\u2211 k=1 H\u2211 h=1 min { 1, 1 \u03bawit |\u27e8Wh(fk), Xh(\u03c0k)\u27e9| } \u00b7 ( I{\u2225Xh(\u03c0k)\u2225\u03a3\u22121 k,h \u2264 1} ) +D(\u03b5)HR\n\u2264 R K\u2211\nk=1 H\u2211 h=1 1 \u03bawit \u2225Wh(fk)\u2225\u03a3k,h min { 1, \u2225Xh(\u03c0k)\u22252\u03a3\u22121 k,h } \ufe38 \ufe37\ufe37 \ufe38\n(A)\n+D(\u03b5)HR. (F.19)\nThe last inequality uses the Cauchy\u2019s inequality \u27e8X,Y \u27e9 \u2264 \u2225X\u2225A\u2225Y \u2225A\u22121 and the fact that x \u00b7I{x \u2264 1} \u2264 min{1, x2}. Further, by the definition of \u03a3k,h, we decompose the first term as\n(A) \u2264 1 \u03bawit K\u2211 k=1 H\u2211 h=1 [ \u03b5 \u00b7 \u2225Wh(fk)\u222522 + k\u2211 s=1 |\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 ]1/2 min { 1, \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h }\n\u2264 1 \u03bawit K\u2211 k=1 H\u2211 h=1 \u221a\u03b5BW + [ k\u2211 s=1 |\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 ]1/2min{1, \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h} .\nThe second inequality is derived by the inequality \u2225Wh(fk)\u2225 \u2264 BW and \u221a a+ b \u2264 \u221a a+ \u221a b. Now sum over k \u2208 [K] and h \u2208 [H], we can get\n(A) \u2264 K\u2211\nk=1 H\u2211 h=1 1 \u03bawit \u221a\u03b5BW + [ k\u2211 s=1 |\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 ]1/2min{1, \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h}\n\u2264 1 \u03bawit K\u2211 k=1 H\u2211 h=1 \u221a \u03b5BW min { 1, \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h } \ufe38 \ufe37\ufe37 \ufe38\n(X)\n+ 1\n\u03bawit K\u2211 k=1 H\u2211 h=1\n[ k\u2211\ns=1\n|\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 ]1/2 min { 1, \u2225Xh(\u03c0k)\u2225\u03a3\u22121k,h } \ufe38 \ufe37\ufe37 \ufe38\n(Y )\n. (F.20)\nFirst, we try to give an upper bound for (X). By Cauchy\u2019s inequality and (F.18), we can derive\n(X) \u2264 1 \u03bawit\n( K\u2211\nk=1 H\u2211 h=1 \u03b5B2W )1/2( K\u2211 k=1 H\u2211 h=1 min { 1, \u2225Xh(\u03c0k)\u22252\u03a3\u22121k,h })1/2\n\u2264 1 \u03bawit\n\u221a HK\u03b5B2W \u00b7 D(\u03b5)H \u2264\nHK\u03b5B2W \u03ba2wit +D(\u03b5)H. (F.21)\nOn the other hand, for (Y), we can bound it using Cauchy\u2019s inequality that \u2211\na,b \u221a ab \u2264\u221a ( \u2211 a a) \u00b7 ( \u2211 b b),\n(Y ) \u2264 1 \u03bawit\n(( K\u2211\nk=1 H\u2211 h=1 k\u2211 s=1\n|\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 )(\nK\u2211 k=1 H\u2211 h=1 min { 1, \u2225Xh(\u03c0k)\u22252\u03a3\u22121k,h\n}))1/2\n\u2264 1 \u03bawit \u221a\u221a\u221a\u221aD(\u03b5)H ( K\u2211 k=1 H\u2211 h=1 k\u2211 s=1 |\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 ) .\nThe last inequality holds by the definition of D(\u03b5) in F.18. Now by the definition of multi-agent witness rank D.6, we note that\n|\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 \u2264 ( max v\u2208Vh E(sh,ah)\u223c\u03c0[(Esh+1\u223cPh,fk (\u00b7|sh,ah) \u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah))v(sh, ah, sh+1)] )2\n\u2264 max v\u2208Vh\nE(sh,ah)\u223c\u03c0s [( (Esh+1\u223cPh,fk (\u00b7|sh,ah) \u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah))v(sh, ah, sh+1) )2]\n\u2264 E(sh,ah)\u223c\u03c0s [ max v\u2208Vh ( (Esh+1\u223cPh,fk (\u00b7|sh,ah) \u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah))v(sh, ah, sh+1) )2] The last two inequalities use Jensen\u2019s inequality. Hence, by the definition of total variation distance, we can get\n|\u27e8Wh(fk), Xh(\u03c0s)\u27e9|2 \u2264 TV ( Ph,fk(\u00b7 | sh, ah),Ph,f\u2217(\u00b7 | sh, ah) )2 (F.22)\n\u2264 2D2H ( Ph,fk(\u00b7 | sh, ah),Ph,f\u2217(\u00b7 | sh, ah) ) , (F.23)\nwhere the TV(\u00b7, \u00b7) denotes the total variation distance and DH denotes the Hellinger divergence. The inequality (F.22) holds by the fact that v(sh, ah, sh+1) \u2208 [0, 1], and the (F.23) holds by the relationship between TV distance and Hellinger distance. Then, we can substitute the inequality (F.23) and get\n(Y ) \u2264 1 \u03bawit \u221a\u221a\u221a\u221aD(\u03b5)H ( K\u2211 k=1 H\u2211 h=1 k\u2211 s=1 E(sh,ah)\u223c\u03c0s2D2H ( Ph,fk(\u00b7 | sh, ah),Ph,f\u2217(\u00b7 | sh, ah) ))\n\u2264 \u00b5R \u00b7 2D(\u03b5)H \u03ba2wit + 1 \u00b5R\n( K\u2211\nk=1 H\u2211 h=1 k\u2211 s=1 E(sh,ah)\u223c\u03c0sD 2 H ( Ph,fk(\u00b7 | sh, ah),Ph,f\u2217(\u00b7 | sh, ah) )) (F.24)\nHence, combining (F.19), (F.20), (F.21) and (F.24), we can get Reg(K) = K\u2211\nk=1\nEs1\u223c\u03c1 [ V (i),\u03c0k 1,fk (s1)\u2212 V (i),\u03c0 k 1 (s1) ]\n\u2264 R \u00b7A+D(\u03b5)HR \u2264 R(X + Y ) +D(\u03b5)HR \u2264 HKR\u03b5B2W /\u03ba2wit +D(\u03b5)HR\n+ \u00b5R2 \u00b7 2D(\u03b5)H \u03ba2wit + 1 \u00b5\n( K\u2211\nk=1 H\u2211 h=1 k\u2211 s=1 E(sh,ah)\u223c\u03c0sD 2 H ( Ph,fk(\u00b7 | sh, ah),Ph,f\u2217(\u00b7 | sh, ah)\n)) .\n(F.25)\nNow by the definition of \u2113(i),s of the model-based problem in (2.5), choosing \u03b5 = \u03ba2wit/HKB 2 W and dMADC = 2R2D(\u03b5)H\n\u03ba2wit , we can get\nReg(K) \u2264 6dMADCH + \u00b5 \u00b7 dMADC + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k)\ncomplete the proof by D(\u03ba2wit/HKB2W ) = O\u0303(d)."
        },
        {
            "heading": "F.5 PROOF OF THEOREM D.14",
            "text": "Proof. First, we fix an index i \u2208 [n]. Similar to Section F.4, we can get K\u2211\nk=1\nEs1\u223c\u03c1 [ V (i),\u03c0k 1,fk (s1)\u2212 V (i),\u03c0 k 1 (s1) ]\n= K\u2211 k=1 E\u03c0k\n[ H\u2211\nh=1\n(Q (i),\u03c0k h,fk (sh, ah)\u2212 r(i)h (sh, ah)\u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah)V (i),\u03c0k h+1,fk (sh+1, ah+1))\n]\n= K\u2211 k=1 E\u03c0k\n[ H\u2211\nh=1\n(Esh+1\u223cPh,fk (\u00b7|sh,ah) \u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah))[V (i),\u03c0k h+1,fk (sh+1)]\n]\n= K\u2211 k=1 H\u2211 h=1 (\u03b8h,fk \u2212 \u03b8\u2217h)TE\u03c0k [\u222b S \u03d5h(s \u2032 | s, a)V (i),\u03c0 k h+1,fk (s\u2032)ds ] ,\nwhere the last equality is because of the property of the linear mixture MG.\nNow we denote\nWh(f) = R(\u03b8h,f \u2212 \u03b8\u2217h) (F.26)\nXh(f, \u03c0) = E\u03c0\n[\u222b S \u03d5h(s\n\u2032 | s, a)V (i),\u03c0h+1,f (s\u2032)ds R\n] . (F.27)\nThen, we have \u2225Wh(f)\u2225 \u2264 2 \u221a d, \u2225Xh(f, \u03c0)\u2225 \u2264 1 and\nK\u2211 k=1 Es1\u223c\u03c1 [ V (i),\u03c0k 1,fk (s1)\u2212 V (i),\u03c0 k 1 (s1) ] \u2264 K\u2211 k=1 H\u2211 h=1 min{\u27e8Wh(fk), Xh(fk, \u03c0k)\u27e9, R}.\nNow similar to Section F.4, if we replaceXh(\u03c0k) toXh(fk, \u03c0k), from (F.21) and (F.24) withBW = 2 \u221a dR we can get\nK\u2211 k=1 Es1\u223c\u03c1 [ V (i),\u03c0k 1,fk (s1)\u2212 V (i),\u03c0 k 1 (s1) ]\n\u2264 HKR\u03b54dR2 +D(\u03b5)HR+ \u00b5R4 \u00b7 2D(\u03b5)H + 1 \u00b5R2\n( K\u2211\nk=1 H\u2211 h=1 k\u2211 s=1\n\u27e8Wh(fk), Xh(fs, \u03c0s)\u27e92 ) ,\n(F.28) where D(\u03b5) = 2d log ( \u03b5+K \u03b5 ) . Moreover, by (F.26) and (F.27), note that\n\u27e8Wh(fk), Xh(fs, \u03c0s)\u27e9 = (\u03b8h,fk \u2212 \u03b8\u2217h)TE\u03c0s [\u222b\nS \u03d5h(s\n\u2032 | s, a)V (i),\u03c0 s h+1,fs(s \u2032)ds ] = E\u03c0s [ (Esh+1\u223cPh,fk (\u00b7|sh,ah) \u2212 Esh+1\u223cPh,f\u2217 (\u00b7|sh,ah)[V (i),\u03c0s h+1,fs(sh+1)] ]\n\u2264 E\u03c0s [ 2\u2225V (i),\u03c0 s h+1,fs(\u00b7)\u2225\u221e \u00b7 dTV(Ph,fk(\u00b7 | sh, ah)\u2225Ph,f\u2217(\u00b7 | sh, ah)) ]\n\u2264 E\u03c0s [ 2 \u221a 2RDH(Ph,fk(\u00b7 | sh, ah)\u2225Ph,f\u2217(\u00b7 | sh, ah)) ] .\nHence, from (F.28) and Jensen\u2019s inequality that (E[X])2 \u2264 E[X2], we can have Reg(K) \u2264 K\u2211\nk=1\nEs1\u223c\u03c1 [ V (i),\u03c0k 1,fk (s1)\u2212 V (i),\u03c0 k 1 (s1) ]\n\u2264 HKR\u03b54dR2 +D(\u03b5)HR+ \u00b5R4 \u00b7 2D(\u03b5)H\n+ 1\n\u00b5R2\n( K\u2211\nk=1 H\u2211 h=1 k\u2211 s=1 E(sh,ah)\u223c\u03c0s [ 8R2D2H ( Ph,fk(\u00b7 | sh, ah),Ph,f\u2217(\u00b7 | sh, ah)\n)]) .\nBy the definition of discrepancy function \u2113(i),s in (2.5), and choosing \u03b5 = 1/HKd, dMADC = HR4D(1/HKd) = O\u0303(HdR4), we can derive\nReg(K) \u2264 4R3 +D(1/HKd)HR+ \u00b5dMADC + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k)\n\u2264 6dMADCH + \u00b5dMADC + 1\n\u00b5 K\u2211 k=1 k\u22121\u2211 s=1 \u2113(i),s(fk, \u03c0k).\nHence, we complete the proof."
        },
        {
            "heading": "F.6 PROOF OF LEMMA E.1",
            "text": "Proof. The proof is modified from Zhong et al. (2022). Define Wj,h be the filtration induced by {sk1 , ak1 , r (i),k 1 , \u00b7 \u00b7 \u00b7 , skH , akH , r (i),k H } j\u22121 k=1. First, for h \u2208 [H], i \u2208 [n], f \u2208 F (i) and \u03c0 \u2208 \u03a0, we define the random variable\nY (i) j (h, f, \u03b6 k) = ( fh(s j h, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8fh+1(s j h+1\u00b7), \u03b6 k h+1(\u00b7 | s j h+1)\u27e9 )2 \u2212 ( T (i),\u03b6 k\nh (f)(s j h, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8fh+1(s j h+1\u00b7), \u03b6 k h+1(\u00b7 | s j h+1)\u27e9\n)2 .\nBy taking conditional expectation of Yj with respect to a j h, s j h, we can get\nE[Y (i)j (h, f, \u03b6 k) | Wj,h] = Esh,ah\u223c\u03b6j [(fh \u2212 T\n(i),\u03b6k\nh (f))(sh, ah)] 2\nand\nE[(Y (i)j (h, f, \u03b6 k))2 | Wj,h] \u2264 2R2E[Y (i)j (h, f, \u03b6 k) | Wj,h],\nwhere 3R \u2265 |fh(sjh, a j h) \u2212 r (i) h (s j h, a j h) \u2212 \u27e8fh+1(s j h+1\u00b7), \u03b6kh+1(\u00b7 | s j h+1)\u27e9)| is the constant upper bound. Denote Zj = Y (i) j (h, f, \u03b6 k) \u2212 Esh+1 [Y (i) j (h, f, \u03b6\nk) | Wj,h] with |Zj | \u2264 4R2. By the Freedman inequality, for any 0 < \u03b7 < 14R , with probability at least 1\u2212 \u03b4,\nk\u2211 j=1 Zj = O \u03b7 k\u2211 j=1 Var[Y (i)j (h, f, \u03b6 k) | Wj,h] + log(1/\u03b4) \u03b7  \u2264 O\n\u03b7 k\u2211 j=1 E[(Y (i)j (h, f, \u03b6 k))2 | Wj,h] + log(1/\u03b4) \u03b7  \u2264 O\n\u03b7 k\u2211 j=1 2R2E[Y (i)j (h, f, \u03b6 k) | Wj,h] + log(1/\u03b4) \u03b7  . By choosing \u03b7 = min { 1 4R , \u221a log(1/\u03b4)\n\u221a 2R \u221a\u2211k j=1 E[Y (i) j (h,f,\u03b6 k)|Wj,h]\n} , we will have\nk\u2211 j=1 Zj = O\nR \u221a\u221a\u221a\u221a k\u2211\nj=1\nE[Y (i)j (h, f, \u03b6k) | Wj,h] log(1/\u03b4) +R 2 log(1/\u03b4)\n .\nSimilarly, if we apply the Freedman\u2019s inequality with \u2212 \u2211k\nj=1 Zj , with probability at least 1\u2212 2\u03b4,\u2223\u2223\u2223\u2223\u2223 k\u2211\nj=1\nZj \u2223\u2223\u2223\u2223\u2223 = O R \u221a\u221a\u221a\u221a k\u2211 j=1 E[Y (i)j (h, f, \u03b6k) | Wj,h] log(1/\u03b4) +R 2 log(1/\u03b4)  . Denote the \u03c1-covering set of F (i) as CF(i)(\u03c1), then for any f \u2208 F (i), \u03b6 \u2208 \u03a0 pur i , there exists a pair f\u0303 \u2208 CF(i)(\u03c1) such that\u2223\u2223\u2223(fh(sh, ah)\u2212r(i)h (sh, ah)\u2212 \u27e8fh+1(sh+1, \u00b7), \u03b6h+1(\u00b7 | sh+1)\u27e9) \u2212 ( f\u0303h(sh, ah)\u2212 r(i)h (sh, ah)\u2212 \u27e8f\u0303h+1(sh+1, \u00b7), \u03b6h+1(\u00b7 | sh+1)\u27e9\n)\u2223\u2223\u2223 \u2264 3\u03c1 for all (sh, ah, sh+1) \u2208 S \u00d7A\u00d7 S. Now by taking a union bound over CF(i)(\u03c1), we have that with probability at least 1\u2212 \u03b4, for all f\u0303 \u2208 CF(i)(\u03c1),\u2223\u2223\u2223\u2223\u2223 k\u2211 j=1 Y\u0303 (i) j (h, f\u0303 , \u03b6)\u2212 k\u2211 j=1 E[Y\u0303 (i)j (h, f\u0303 , \u03b6) | Wj,h]\n\u2223\u2223\u2223\u2223\u2223 = O R \u221a\u221a\u221a\u221a k\u2211\nj=1\nE[Y\u0303 (i)j (h, f\u0303 , \u03b6) | Wj,h]\u03b9+R 2\u03b9  , (F.29) where \u03b9 = 2 log(HK|CF(i)(\u03c1)|/\u03b4) \u2264 2 log(HKNF(i)(\u03c1)).\nNow note that for all f \u2208 F (i), \u03b6 \u2208 \u03a0puri , we have H\u2211\nh=1 k\u22121\u2211 j=0 Y (i) j (h, f, \u03b6)\n= H\u2211 h=1 k\u22121\u2211 j=0 (fh(s j h, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8fh+1(s j h+1, \u00b7), \u03b6h+1(\u00b7 | s j h+1)\u27e9) 2\n\u2212 (T (i),\u03b6h (f)(s j h, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8fh+1(s j h+1, \u00b7), \u03b6h+1(\u00b7 | s j h+1)\u27e9) 2\n\u2264 H\u2211\nh=1 k\u22121\u2211 j=0 (fh(s j h, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8fh+1(s j h+1, \u00b7), \u03b6h+1(\u00b7 | s j h+1)\u27e9) 2\n\u2212 inf f \u2032h\u2208F (i) h\n(T (i),\u03b6 k h (f \u2032)(sjh, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8fh+1(s j h+1, \u00b7), \u03b6h+1(\u00b7 | s j h+1)\u27e9) 2\n= L(i),k\u22121(f, \u03b6, \u03c41:k\u22121).\nThen, by (F.29) we can get H\u2211\nh=1 k\u22121\u2211 j=0 E[Y\u0303 (i)j (h, f\u0303 , \u03b6) | Wj,h] \u2264 4L (i),k\u22121(f\u0303 , \u03b6, \u03c41:k\u22121) +O(HR2\u03b9).\nNow similar to (Jin et al., 2021a), by the definition of \u03c1-covering number, for any k \u2208 [K], f \u2208 F (i) and \u03b6 \u2208 \u03a0puri ,\nH\u2211 h=1 k\u22121\u2211 j=0 E[Y (i)j (h, f, \u03b6) | Wj,h] \u2264 4L (i),k\u22121(f, \u03b6, \u03c41:k\u22121) +O(HR2\u03b9+HRk\u03c1).\nNow since sjh, a j h \u223c \u03b6j , we can have k\u22121\u2211 j=0 \u2113j,(i)(f, \u03b6k) = k\u22121\u2211 j=0 E[Y (i)j (h, f, \u03b6 k) | Wj,h]\n\u2264 4L(i),k\u22121(f, \u03b6k, \u03c41:k\u22121) +O(HR2\u03b9+HRk\u03c1). We complete the proof by choosing \u03c1 = 1/K and choose \u03b5conc = O(HR2\u03b9+HRk\u03c1) = O(HR2\u03b9)."
        },
        {
            "heading": "F.7 PROOF OF LEMMA E.2",
            "text": "Proof. First, for any f \u2208 F (i) and \u03c0 \u2208 \u03a0pur we define the random variable\nQ (i) j (h, f, \u03c0) = (fh(s j h, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8f \u2217 h+1(s j h+1, \u00b7), \u03c0h+1(\u00b7 | s j h+1)\u27e9) 2\n\u2212 (f\u2217h(s j h, a j h)\u2212 r (i) h (s j h, a j h)\u2212 \u27e8f \u2217 h+1(s j h+1, \u00b7), \u03c0h+1(\u00b7 | s j h+1)\u27e9) 2.\nThen, by similar derivations in Lemma E.1, we can get\nE[Q(i)j (h, f, \u03c0) | Wj,h] = Esh,ah\u223c\u03b6j [(fh \u2212 T (i),\u03c0(f\u2217))(sh, ah)] 2 \u2265 0,\nE[(Q(i)j (h, f, \u03c0)) 2 | Wj,h] \u2264 2R2E[Q(i)j (h, f, \u03c0) | Wj,h].\nThen, by Freedman\u2019s inequality, with probability at least 1\u2212 \u03b4, for all elements in f\u0303 \u2208 CF(i)(\u03c1), we have \u2223\u2223\u2223\u2223\u2223 k\u22121\u2211 j=0 Q\u0303 (i) j (h, f\u0303 , \u03c0)\u2212 k\u22121\u2211 j=0 E[Q\u0303(i)j (h, f\u0303 , \u03c0) | Wj,h] \u2223\u2223\u2223\u2223\u2223 = O R \u221a\u221a\u221a\u221ak\u22121\u2211\nj=0\nEsh+1 [Q\u0303 (i) j (h, f\u0303 , \u03c0) | Wj,h]\u03b9+R 2\u03b9  , then we can have\nk\u22121\u2211 j=0 Q\u0303 (i) j (h, f\u0303 , \u03c0) \u2265 \u2212O(R 2\u03b9).\nThus, by the definition of CF(i)(\u03c1), for all f \u2208 F (i) and \u03c0 \u2208 \u03a0 pur i , we have\n\u2212 k\u22121\u2211 j=0 Q (i) j (h, f, \u03c0) \u2264 O(R 2\u03b9+Rk\u03c1).\nThus,\nL(i),k(f, \u03c0) = H\u2211 h=1 \u2212 inf f\u2208F(i) k\u22121\u2211 j=0 Q (i) j (h, f, \u03c0)  \u2264 O(HR2\u03b9+HRk\u03c1) = O(HR2\u03b9). Thus, we complete the proof."
        },
        {
            "heading": "F.8 PROOF OF LEMMA E.3",
            "text": "Proof. For simplicity, we first assume F is a finite class. Given a model f \u2208 F and h \u2208 [H], we define Xjh,f = log Ph,f\u2217 (sjh+1|s j h,a j h)\nPh,f (sjh+1|s j h,a j h)\n. Thus,\nL(i),k(f\u2217, \u03c41:k)\u2212 L(i),k(f, \u03c41:k) = \u2212 H\u2211\nh=1 k\u2211 j=1 Xjh,f . (F.30)\nNow we define the filtration Gj as\nGj = \u03c3({s1h, a1h, \u00b7 \u00b7 \u00b7 , s j h, a j h}).\nThen, by Lemma G.1 for all f \u2208 F , with probability at least 1\u2212 \u03b4, we have\n\u2212 k\u2211\nj=1\nXj h,f\u0304\n\u2264 k\u2211\nj=1\nlogE [ exp { \u22121 2 Xj h,f\u0304 } \u2223\u2223\u2223\u2223\u2223 Gj\u22121 ] + log(H|F|/\u03b4).\nNow we decompose the first term at the right side as\nE [ exp { \u22121 2 Xj h,f\u0304 } \u2223\u2223\u2223\u2223\u2223 Gj\u22121 ]\n= E\n \u221a\u221a\u221a\u221a logPh,f (sjh+1 | sjh, ajh)\nPh,f\u2217(sjh+1 | s j h, a j h)\n\u2223\u2223\u2223\u2223\u2223 Gj\u22121 \n= E(sjh,ajh)\u223c\u03c0jEsh+1\u223cPh,f\u2217 (\u00b7|sjh,ajh)\n \u221a\u221a\u221a\u221a Ph,f (sjh+1 | sjh, ajh)\nPh,f\u2217(sjh+1 | s j h, a j h)\n\u2223\u2223\u2223\u2223\u2223 Gj\u22121 \n= E(sjh,ajh)\u223c\u03c0j [\u222b \u221a Ph,f (sjh+1 | s j h, a j h)Ph,f\u2217(s j h+1 | s j h, a j h)dsjh+1 ] = 1\u2212 1\n2 E(sjh,ajh)\u223c\u03c0j [D 2 H(Ph,f (s j h+1 | s j h, a j h)\u2225Ph,f\u2217(s j h+1 | s j h, a j h))].\nNow by the inequality log x \u2264 x\u2212 1, we have\n\u2212 k\u2211\nj=1\nXjh,f \u2264 k\u2211\nj=1\n( 1\u2212 1\n2 E(sjh,ajh)\u223c\u03c0j [D 2 H(Ph,f (s j h+1 | s j h, a j h)\u2225Ph,f\u2217(s j h+1 | s j h, a j h))] ) \u2212 1 + log(H|F|/\u03b4)\n\u2264 \u2212 k\u2211\nj=1\n1 2 E(sjh,ajh)\u223c\u03c0j\n[ D2H(Ph,f (s j h+1 | s j h, a j h)\u2225Ph,f\u2217(s j h+1 | s j h, a j h)) ]\n+ log(H|F|/\u03b4).\nSum over h \u2208 [H] with (F.30), we can complete the proof by\n\u2212 H\u2211\nh=1 k\u2211 j=1 Xjh,f \u2264 \u2212 k\u2211 j=1 \u2113(i),j(f) + \u03baconc,\nwhere \u03baconc = H log(H|F|/\u03b4). For infinite model classes F , we can use 1/K-bracketing number BF (1/K) to replace the cardinality |F| (Liu et al., 2022a; Zhong et al., 2022; Zhan et al., 2022b)."
        },
        {
            "heading": "F.9 PROOF OF COROLLARY E.4",
            "text": "Proof. We provide the proof for NE. The proof for CCE/CE are the same by replacing the NEregret to the CCE/CE-regret. By taking the minimum of the index of agents rather than adding them together, we can modify the proof of Theorem 3.1 and derive, with probability at least 1\u2212 \u03b4,\n1\nK\n( K\u2211\nk=1\nmax i\u2208[n]\n( V (i),\u00b5 (i),\u03c0k (\u03c1)\u2212 V (i),\u03c0 k (\u03c1) )) \u2264 O\u0303 ( H\u03a5F,\u03b4\u221a\nK + dMADC\u221a K + dMADCH K\n) .\nHence, by choosing K = O\u0303 ( (H2\u03a52F,\u03b4 + d 2 MADC) \u00b7 \u03b5\u22122 + dMADCH \u00b7 \u03b5\u22121 ) with \u03b5 < 1, we have\nmax i\u2208[n]\n( V (i),\u00b5 (i),\u03c0out (\u03c1)\u2212 V (i),\u03c0out(\u03c1) ) = 1\nK\n( K\u2211\nk=1\nmax i\u2208[n]\n( V (i),\u00b5 (i),\u03c0k (\u03c1)\u2212 V (i),\u03c0 k (\u03c1) ))\n\u2264 \u03b5,\nwhere the second inequality holds from \u03c0out = Unif({\u03c0k}k\u2208[K]). Hence, \u03c0out is a \u03b5-NE."
        },
        {
            "heading": "G TECHNICAL TOOLS",
            "text": "We provide the following lemma to complete the proof of model-based RL problems. The detailed proof can be found in (Foster et al., 2021). Lemma G.1. For any real-valued random variable sequence {Xk}k\u2208[K] adapted to a filtration {Gk}k\u2208[K], with probability at least 1\u2212 \u03b4, for any k \u2208 [K], we can have\n\u2212 k\u2211\ns=1\nXk \u2264 k\u2211\ns=1\nlogE[exp(\u2212Xs) | Fs\u22121] + log(1/\u03b4).\nIn the next lemma, we introduce the Freedman\u2019s inequality, which has been commonly used in previous RL algorithms. (Jin et al., 2021b; Chen et al., 2022c; Zhong et al., 2022) Lemma G.2 (Freedman\u2019s Inequality (Agarwal et al., 2014)). Let {Zk}k\u2208[K] be a martingale difference sequence that adapted to filtration {Fk}k\u2208[K]. If |Zk| \u2264 R for all k \u2208 [K], then for \u03b7 \u2208 (0, 1R ), with probability at least 1\u2212 \u03b4, we can have\nK\u2211 k=1 Xk = O\n( \u03b7\nK\u2211 k=1 E[X2k | Fk\u22121] + log(1/\u03b4) \u03b7\n) .\nThe next elliptical potential lemma is first introduced in the linear bandit literature (Dani et al., 2008; Abbasi-Yadkori et al., 2011) and then applied to the RL problems with Bilinear Classes (Du et al., 2021) and the general function approximation (Chen et al., 2022a; Zhong et al., 2022). Lemma G.3 (Elliptical Potential Lemma). Let {xk}Kk=1 be a sequence of real-valued vector, i.e. xk \u2208 Rd for any k \u2208 [K]. Then, if we define \u039bi = \u03b5I + \u2211K k=1 xkx T k , we can get that\nK\u2211 k=1 min { 1, \u2225xi\u22252\u039b\u22121i } \u2264 2 log ( det(\u039bK+1) det(\u039b1) ) \u2264 2 log det ( I + 1 \u03b5 K\u2211 k=1 xkx T k ) .\nProof. The proof is provided in Lemma 11 of (Abbasi-Yadkori et al., 2011)."
        }
    ],
    "year": 2023
}