{
    "abstractText": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and visiononly input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. Further details can be seen on our project website: https://sites.google.com/view/safedreamer.",
    "authors": [],
    "id": "SP:601984f7cd843b792b0e8b66469364c9d77d8db1",
    "references": [
        {
            "authors": [
                "Joshua Achiam",
                "David Held",
                "Aviv Tamar",
                "Pieter Abbeel"
            ],
            "title": "Constrained policy optimization",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Rishabh Agarwal",
                "Max Schwarzer",
                "Pablo Samuel Castro",
                "Aaron C Courville",
                "Marc Bellemare"
            ],
            "title": "Deep reinforcement learning at the edge of the statistical precipice",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Eitan Altman"
            ],
            "title": "Constrained Markov decision processes: stochastic modeling",
            "year": 1999
        },
        {
            "authors": [
                "Yarden As",
                "Ilnura Usmanova",
                "Sebastian Curi",
                "Andreas Krause"
            ],
            "title": "Constrained policy optimization via bayesian world models",
            "venue": "arXiv preprint arXiv:2201.09802,",
            "year": 2022
        },
        {
            "authors": [
                "Marc G Bellemare",
                "Will Dabney",
                "R\u00e9mi Munos"
            ],
            "title": "A distributional perspective on reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Yuanpei Chen",
                "Tianhao Wu",
                "Shengjie Wang",
                "Xidong Feng",
                "Jiechuan Jiang",
                "Zongqing Lu",
                "Stephen McAleer",
                "Hao Dong",
                "Song-Chun Zhu",
                "Yaodong Yang"
            ],
            "title": "Towards human-level bimanual dexterous manipulation with reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078,",
            "year": 2014
        },
        {
            "authors": [
                "Juntao Dai",
                "Jiaming Ji",
                "Long Yang",
                "Qian Zheng",
                "Gang Pan"
            ],
            "title": "Augmented proximal policy optimization for safe reinforcement learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Shuo Feng",
                "Haowei Sun",
                "Xintao Yan",
                "Haojie Zhu",
                "Zhengxia Zou",
                "Shengyin Shen",
                "Henry X Liu"
            ],
            "title": "Dense reinforcement learning for safety validation of autonomous vehicles",
            "year": 2023
        },
        {
            "authors": [
                "Farama Foundation"
            ],
            "title": "A standard api for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly gym)",
            "venue": "https://github.com/ Farama-Foundation/Gymnasium,",
            "year": 2022
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Jimmy Ba",
                "Mohammad Norouzi"
            ],
            "title": "Dream to control: Learning behaviors by latent imagination",
            "venue": "arXiv preprint arXiv:1912.01603,",
            "year": 2019
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Ian Fischer",
                "Ruben Villegas",
                "David Ha",
                "Honglak Lee",
                "James Davidson"
            ],
            "title": "Learning latent dynamics for planning from pixels",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Mohammad Norouzi",
                "Jimmy Ba"
            ],
            "title": "Mastering atari with discrete world models",
            "venue": "arXiv preprint arXiv:2010.02193,",
            "year": 2020
        },
        {
            "authors": [
                "Danijar Hafner",
                "Jurgis Pasukonis",
                "Jimmy Ba",
                "Timothy Lillicrap"
            ],
            "title": "Mastering diverse domains through world models",
            "venue": "arXiv preprint arXiv:2301.04104,",
            "year": 2023
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Xiaolong Wang",
                "Hao Su"
            ],
            "title": "Temporal difference learning for model predictive control",
            "venue": "arXiv preprint arXiv:2203.04955,",
            "year": 2022
        },
        {
            "authors": [
                "Tairan He",
                "Weiye Zhao",
                "Changliu Liu"
            ],
            "title": "Autocost: Evolving intrinsic cost for zero-violation reinforcement learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Yannick Hogewind",
                "Thiago D Simao",
                "Tal Kachman",
                "Nils Jansen"
            ],
            "title": "Safe reinforcement learning from pixels using a stochastic latent representation",
            "venue": "arXiv preprint arXiv:2210.01801,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish K Jayant",
                "Shalabh Bhatnagar"
            ],
            "title": "Model-based safe deep reinforcement learning via a constrained proximal policy optimization algorithm",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Ji",
                "Borong Zhang",
                "Jiayi Zhou",
                "Xuehai Pan",
                "Weidong Huang",
                "Ruiyang Sun",
                "Yiran Geng",
                "Yifan Zhong",
                "Juntao Dai",
                "Yaodong Yang"
            ],
            "title": "Safety-gymnasium: A unified safe reinforcement learning benchmark, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Jiaming Ji",
                "Jiayi Zhou",
                "Borong Zhang",
                "Juntao Dai",
                "Xuehai Pan",
                "Ruiyang Sun",
                "Weidong Huang",
                "Yiran Geng",
                "Mickel Liu",
                "Yaodong Yang"
            ],
            "title": "Omnisafe: An infrastructure for accelerating safe reinforcement learning research, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Torsten Koller",
                "Felix Berkenkamp",
                "Matteo Turchetta",
                "Andreas Krause"
            ],
            "title": "Learning-based model predictive control for safe exploration",
            "venue": "IEEE Conference on Decision and Control (CDC),",
            "year": 2018
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "A path towards autonomous machine intelligence version 0.9",
            "venue": "Open Review,",
            "year": 2022
        },
        {
            "authors": [
                "Alex X Lee",
                "Anusha Nagabandi",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Quanyi Li",
                "Zhenghao Peng",
                "Lan Feng",
                "Qihang Zhang",
                "Zhenghai Xue",
                "Bolei Zhou"
            ],
            "title": "Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zuxin Liu",
                "Hongyi Zhou",
                "Baiming Chen",
                "Sicheng Zhong",
                "Martial Hebert",
                "Ding Zhao"
            ],
            "title": "Constrained model-based reinforcement learning with robust cross-entropy method",
            "venue": "arXiv preprint arXiv:2010.07968,",
            "year": 2020
        },
        {
            "authors": [
                "Haitong Ma",
                "Changliu Liu",
                "Shengbo Eben Li",
                "Sifa Zheng",
                "Jianyu Chen"
            ],
            "title": "Joint synthesis of safety certificate and safe control policy using constrained reinforcement learning",
            "venue": "In Learning for Dynamics and Control Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Yecheng Jason Ma",
                "Andrew Shen",
                "Osbert Bastani",
                "Jayaraman Dinesh"
            ],
            "title": "Conservative and adaptive penalty for model-based safe reinforcement learning",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas M Moerland",
                "Joost Broekens",
                "Aske Plaat",
                "Catholijn M Jonker"
            ],
            "title": "Model-based reinforcement learning: A survey",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jorge Nocedal",
                "Stephen J. Wright"
            ],
            "title": "Numerical Optimization",
            "year": 2006
        },
        {
            "authors": [
                "Athanasios S Polydoros",
                "Lazaros Nalpantidis"
            ],
            "title": "Survey of model-based reinforcement learning: Applications on robotics",
            "venue": "Journal of Intelligent & Robotic Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Ray",
                "Joshua Achiam",
                "Dario Amodei"
            ],
            "title": "Benchmarking safe exploration in deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1910.01708,",
            "year": 1910
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Harshit Sikchi",
                "Wenxuan Zhou",
                "David Held"
            ],
            "title": "Learning off-policy with online planning",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "J Ci Simo",
                "TA1143885 Laursen"
            ],
            "title": "An augmented lagrangian treatment of contact problems involving friction",
            "venue": "Computers & Structures,",
            "year": 1992
        },
        {
            "authors": [
                "Aivar Sootla",
                "Alexander I Cowen-Rivers",
                "Taher Jafferjee",
                "Ziyan Wang",
                "David H Mguni",
                "Jun Wang",
                "Haitham Ammar"
            ],
            "title": "Saut\u00e9 rl: Almost surely safe reinforcement learning using state augmentation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Aivar Sootla",
                "Alexander I Cowen-Rivers",
                "Taher Jafferjee",
                "Ziyan Wang",
                "David H Mguni",
                "Jun Wang",
                "Haitham Ammar"
            ],
            "title": "Saut\u00e9 rl: Almost surely safe reinforcement learning using state augmentation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Stooke",
                "Joshua Achiam",
                "Pieter Abbeel"
            ],
            "title": "Responsive safety in reinforcement learning by pid lagrangian methods",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Geraud Nangue Tasse",
                "Tamlin Love",
                "Mark Nemecek",
                "Steven James",
                "Benjamin Rosman"
            ],
            "title": "Rosarl: Reward-only safe reinforcement learning",
            "venue": "arXiv preprint arXiv:2306.00035,",
            "year": 2023
        },
        {
            "authors": [
                "Garrett Thomas",
                "Yuping Luo",
                "Tengyu Ma"
            ],
            "title": "Safe reinforcement learning by imagining the near future",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ international conference on intelligent robots and systems,",
            "year": 2012
        },
        {
            "authors": [
                "Kim P. Wabersich",
                "Melanie N. Zeilinger"
            ],
            "title": "A predictive safety filter for learning-based control of constrained nonlinear dynamical systems, 2021",
            "year": 2021
        },
        {
            "authors": [
                "J Beau W Webber"
            ],
            "title": "A bi-symmetric log transformation for wide-range data",
            "venue": "Measurement Science and Technology,",
            "year": 2012
        },
        {
            "authors": [
                "Min Wen",
                "Ufuk Topcu"
            ],
            "title": "Constrained cross-entropy method for safe reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Long Yang",
                "Jiaming Ji",
                "Juntao Dai",
                "Linrui Zhang",
                "Binbin Zhou",
                "Pengfei Li",
                "Yaodong Yang",
                "Gang Pan"
            ],
            "title": "Constrained update projection approach to safe policy optimization",
            "venue": "arXiv preprint arXiv:2209.07089,",
            "year": 2022
        },
        {
            "authors": [
                "Ray"
            ],
            "title": "2023a). This discrepancy highlights a challenge inherent in model-free algorithms, especially in situations with tight budget",
            "year": 2023
        },
        {
            "authors": [
                "Brockman"
            ],
            "title": "2016), involves a 2D image-based racing simulation with tracks that vary in each episode. The observation comprises a 64\u00d764\u00d73 pixel top-down view of the car. The action space is continuous and three-dimensional, encompassing steering, acceleration, and braking actions",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and visiononly input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. Further details can be seen on our project website: https://sites.google.com/view/safedreamer."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "A challenge in the real-world deployment of RL agents is to prevent unsafe situations (Feng et al., 2023). SafeRL proposes a practical solution by defining a constrained Markov decision process (CMDP) (Altman, 1999) and integrating an additional cost function to quantify potential hazardous behaviors. In this process, agents aim to maximize rewards while maintaining costs below predefined constraint thresholds. Several remarkable algorithms have been developed on this foundation (Achiam et al., 2017; Yang et al., 2022; Hogewind et al., 2022).\nHowever, with the cost threshold nearing zero, existing Lagrangian-based methods often fail to meet constraints or meet them but cannot complete the tasks. Conversely, by employing an internal dynamics model, an agent can effectively plan action trajectories that not only secure high rewards but also converge towards a nearly zero cost (Wen & Topcu, 2018; Hansen et al., 2022). This underscores the significance of dynamics models and planning in such contexts. However, obtaining a ground-truth simulator in complex scenarios like vision-only autonomous driving is impractical (Chen et al., 2022; Li et al., 2022). Additionally, the high expense of long-horizon planning forces optimization over a finite horizon, yielding temporally locally optimal and unsafe solutions. To fill such gaps, we aim to answer the following question:\nHow can we develop a safety-aware world model to balance long-term rewards and costs of agent?\nAutonomous Intelligence Architecture (LeCun, 2022) introduces a world model and utilizes costs reflecting the discomfort level of the agent. Subsequently, Hogewind et al. (2022); As et al. (2022); Jayant & Bhatnagar (2022) offer solutions to ensure safe and reward balance via the world model. However, these methods fail to realize nearly zero-cost performance across several environments due to inaccuracies in modeling the agent\u2019s safety in current or future states. In this work, we introduce SafeDreamer (see Figure 2a), which integrates safety planning and the Lagrangian method within a world model to balance errors between cost models and critics. A detailed comparison with various algorithms can be found in Table 1. In summary, our contributions are:\n\u2022 We present the online safety-reward planning algorithm (OSRP) (as shown in Figure 2b) and substantiate the feasibility of using online planning within the world model to satisfy constraints in vision-only tasks. In particular, we employ the Constrained Cross-Entropy Method (Wen & Topcu, 2018) in the planning process.\n\u2022 We integrate Lagrangian methods with the safety-reward online and background planning within the world model to balance long-term reward and cost. This gives rise to two algorithms, OSRP-Lag and BSRP-Lag, as depicted in Figure 2c and 2d.\n\u2022 SafeDreamer handles both low-dimensional and visual input tasks, achieving nearly zero-cost performance in the Safety-Gymnasium benchmark. We further illustrate that SafeDreamer outperforms competing model-based and model-free methods within this benchmark."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Safe Reinforcement Learning SafeRL aims to manage optimization objectives under constraints (Altman, 1999; Achiam et al., 2017; Yang et al., 2022). Achiam et al. (2017) proposes the generalpurpose policy search algorithm, ensuring adherence to near-constraints iteratively. Nevertheless, second-order methods (Achiam et al., 2017; Yang et al., 2020) impose considerable computational costs. Zhang et al. (2020); Yang et al. (2022) circumvent these challenges and deliver superior results by mitigating approximation errors inherent in Taylor approximations and inverting high-dimensional Fisher information matrices. Sootla et al. (2022b) introduces Saute\u0301 MDPs, which eliminate safety constraints by incorporating them into the state-space and reshaping the objective. This approach satisfies the Bellman equation and advances us toward solving tasks with constraints that are almost surely met. Tasse et al. (2023) investigated optimizing policies based on the expected upper bound (Minmax penalty) of rewards in defined unsafe states. However, these SafeRL methods struggle to achieve zero violation, even when the cost limit is set to zero, due to the inherent uncertainty in the policy and the randomness of the environment itself. He et al. (2023) proposed AutoCost that automatically identifies well-defined cost functions crucial for achieving zero violation performance. From the perspective of energy functions, Ma et al. (2022a) simultaneously synthesizes energyfunction-based safety certificates and learns safe control policies, without relying on prior knowledge of control laws or perfect safety certificates. Despite these advances, our experiments reveal that the convergence of safe model-free algorithms still requires substantial interaction with the environment, and model-based methods can effectively boost sample efficiency.\nSafe Model-based RL Model-based RL approaches (Polydoros & Nalpantidis, 2017) serve as promising alternatives for modeling environment dynamics, categorizing into online planning\u2014utilizing the world model for action selection (Hafner et al., 2019b; Camacho et al., 2007), and background planning\u2014leveraging the world model for policy updates (Hafner et al., 2019a). Notably, methods like MPC (Camacho et al., 2007) and CCEM (Wen & Topcu, 2018) exemplify online planning by ensuring action safety (Koller et al., 2018; Wabersich & Zeilinger, 2021; Liu et al., 2020), though this can lead to short-sighted decisions due to the limited scope of planning and absence of critics. Recent progress seeks to embed terminal value functions into online model planning, fostering consideration of long-term rewards (Sikchi et al., 2022; Hansen et al., 2022; Moerland et al., 2023), but not addressing long-term safety. On the other hand, background planning methods like those in Jayant & Bhatnagar (2022); Thomas et al. (2021) employ ensemble Gaussian models and safety value functions to update policy with PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018), respectively. However, challenges remain in adapting to vision input tasks. In this regard, LAMBDA (As et al., 2022) represents a key progression in visual SafeRL by extending the DreamerV1 (Hafner et al., 2019a) with principles from the Lagrangian method (Boyd & Vandenberghe, 2004). However, the constraints intrinsic to DreamerV1 limit its efficacy due to disregarding necessary adaptive modifications to reconcile variances in signal magnitudes and ubiquitous instabilities within all model elements (Hafner et al., 2023). This original framework\u2019s misalignment with online planning engenders suboptimal results and a deficiency in low-dimensional input, thereby greatly reducing the benefits of the world model. Meanwhile, Safe SLAC (Hogewind et al., 2022) integrates the Lagrangian mechanism into SLAC (Lee et al., 2020), achieving comparable performance to LAMBDA on vision-only tasks. Yet, it does not maximize the potential of the world model to augment safety, overlooking imaginary rollouts that could potentially enhance online or background planning."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Constrained Markov Decision Process (CMDP) SafeRL is often formulated as a CMDPM = (S,A,P, R, C, \u00b5, \u03b3) (Altman, 1999). The state and action spaces are S andA, respectively. Transition probability P(s\u2032|s, a) refers to transitioning from s to s\u2032 under action a. R(s\u2032|s, a) stands for the\nreward acquired upon transitioning from s to s\u2032 through action a. The cost function set C = {(Ci, bi)}mi=1 encompasses cost functions Ci : S \u00d7 A \u2192 R and cost thresholds bi, i = 1, \u00b7 \u00b7 \u00b7 ,m. The initial state distribution and the discount factor are denoted by \u00b5(\u00b7) : S \u2192 [0, 1] and \u03b3 \u2208 (0, 1), respectively. A stationary parameterized policy \u03c0\u03b8 represents the action-taking probability \u03c0\u03b8(a|s) in state s. All stationary policies are represented as \u03a0\u03b8 = {\u03c0\u03b8 : \u03b8 \u2208 Rp}, where \u03b8 is the learnable network parameter. We define the infinite-horizon reward function and cost function as JR(\u03c0\u03b8) and JCi (\u03c0\u03b8), respectively, as follows:\nJR(\u03c0\u03b8) = Es0\u223c\u00b5,at\u223c\u03c0\u03b8 [ \u221e\u2211 t=0 \u03b3tR(st+1|st, at) ] , JCi (\u03c0\u03b8) = Es0\u223c\u00b5,at\u223c\u03c0\u03b8 [ \u221e\u2211 t=0 \u03b3tCi(st+1|st, at) ]\nThe goal of CMDP is to achieve the optimal policy:\n\u03c0\u22c6 = argmax \u03c0\u03b8\u2208\u03a0C\nJR(\u03c0\u03b8) (1)\nwhere \u03a0C = \u03a0\u03b8 \u2229 {\u2229mi=1JCi (\u03c0\u03b8) \u2264 bi} denotes the feasible set of policies.\nSafe Model-based RL Problem We formulate a Safe Model-based RL problem as follows: max \u03c0\u03b8\u2208\u03a0\u03b8 JR\u03d5 (\u03c0\u03b8) s.t. J C \u03d5 (\u03c0\u03b8) \u2264 b (2)\nJR\u03d5 (\u03c0\u03b8) = E [ \u221e\u2211 t=0 \u03b3tR(st+1|st, at)|s0 \u223c \u00b5, st+1 \u223c P\u03d5(\u00b7|st, at), at \u223c \u03c0\u03b8 ]\n(3)\nJC\u03d5 (\u03c0\u03b8) = E [ \u221e\u2211 t=0 \u03b3tC(st+1|st, at)|s0 \u223c \u00b5, st+1 \u223c P\u03d5(\u00b7|st, at), at \u223c \u03c0\u03b8 ]\n(4)\nIn the above, P\u03d5(\u00b7|st, at) is a \u03d5-parameterized world model, we assume the initial state s0 is sampled from the true initial state distribution \u00b5 and st+1 \u223c P\u03d5(\u00b7|st, at),\u2200t > 0. We would use the world model P\u03d5 to roll out imaginary trajectories and then estimate their reward and cost returns required for policy optimization algorithms. Without loss of generality, we will restrict our discussion to the case of one constraint with a cost function C and a cost threshold b."
        },
        {
            "heading": "4 METHODS",
            "text": "In this section, we introduce SafeDreamer, a framework for integrating safety-reward planning of the world model with the Lagrangian methods to balance rewards and costs. The world model is trained through a replay buffer of past experiences as the agent interacts with the environment. Meanwhile, we elucidate the notation for our safe model-based agent, assuming access to the learned world model, which generates latent rollouts for online or background planning, depicted in Figure 1. The design and training objectives of these models are described in Section 5."
        },
        {
            "heading": "4.1 MODEL COMPONENTS",
            "text": "SafeDreamer includes the world model and actor-critic models. At each time step, the world model receives an observation ot and an action at. The observation is condensed into a discrete representation zt. Then, zt, along with the action, are used by the sequence model to predict the next representation z\u0302t+1. We represent the model state st = {ht, zt} by concatenating ht and zt, where ht is a recurrent state. Decoders employ st to predict observations, rewards, and costs. Meanwhile, st serves as the input of the actor-critic models to predict reward value vrt , cost value vct , and action at. Our model components are as follows:\nObservation encoder: zt \u223c E\u03d5 (zt | ht, ot) Observation decoder: o\u0302t \u223c O\u03d5 (o\u0302t | st) Reward decoder: r\u0302t \u223c R\u03d5 (r\u0302t | st) Cost decoder: c\u0302t \u223c C\u03d5 (c\u0302t | st) Sequence model: ht, z\u0302t = S\u03d5 (ht\u22121, zt\u22121, at\u22121) Actor: at \u223c \u03c0\u03b8(at | st) Reward critic: v\u0302rt \u223c V\u03c8r (v\u0302rt | st) Cost critic: v\u0302ct \u223c V\u03c8c(v\u0302ct | st)\n4.2 ONLINE SAFETY-REWARD PLANNING (OSRP) VIA WORLD MODEL\nAlgorithm 1: Online Safety-Reward Planning. 1 Input: current model state st, planning horizon H , 2 num sample/policy/safe trajectories N\u03c0N , N\u03c0\u03b8 , Ns, 3 Lagrangian multiplier \u03bbp, cost limit b, \u00b50, \u03c30 forN 4 for j \u2190 1 to J do 5 Init. empty safe/candidate/elite actions set\nAs, Ac, Ae 6 Sample N\u03c0N +N\u03c0\u03b8 traj. {si, ai, si+1} t+H i=t\nusingN (\u00b5j\u22121, (\u03c3j\u22121)2I), \u03c0\u03b8 within S\u03d5 with st as the initial state\n7 for all N\u03c0N +N\u03c0\u03b8 trajectories do 8 Init. trajectory cost JC,H\u03d5 (\u03c0)\u2190 0 9 for t\u2190 H to 1 do\n10 Compute R\u03bb(st), C\u03bb(st) via Equation (6) 11 JC,H\u03d5 \u2190 \u03b3J C,H \u03d5 + C\u03d5(st) 12 end 13 JR\u03d5 , J C \u03d5 \u2190 R\u03bb(s1), C\u03bb(s1) 14 JC \u2032\n\u03d5 \u2190 (JC,H\u03d5 /H)L 15 if JC \u2032 \u03d5 < b then 16 As \u2190 As \u222a {at:t+H} 17 end 18 end 19 Select sorting key \u2126, candidate action set Ac 20 Select the top-k action trajectories with highest \u2126 values among Ac as elite actions Ae 21 \u00b5j , \u03c3j = MEAN(Ae), STD (Ae) 22 end 23 Return: a \u223c N (\u00b5J , (\u03c3J)2I)\nSafeDreamer (OSRP) We introduce online safety-reward planning (OSRP) through the world model, depicted in Figure 2b. The online planning procedure is conducted at every decision time t, generating state-action trajectories from the current state st within the world model. Each trajectory is evaluated by learned reward and cost models, along with their critics, and the optimal safe action trajectory is selected for execution in the environment. Specifically, we adopt the Constrained Cross-Entropy Method (CCEM) (Wen & Topcu, 2018) for planning. To commence this process, we initialize (\u00b50, \u03c30)t:t+H , with \u00b50, \u03c30 \u2208 R|A|, which represent a sequence of action mean and standard deviation spanning over the length of planning horizon H . Following this, we independently sample N\u03c0N state trajectories using the current action distribution N (\u00b5j\u22121, \u03c3j\u22121) at iteration j \u2212 1 within the world model. Additionally, we sample N\u03c0\u03b8 state trajectories using a rewarddriven actor, similar to Hansen et al. (2022); Sikchi et al. (2022), accelerating the convergence of planning. Afterward, we estimate the reward return JR\u03d5 and cost return J C \u03d5 of each trajectory, as detailed in lines 9 - 14 of Algorithm 1. The estimation of JR\u03d5 is obtained using the TD(\u03bb) value of reward, denoted as R\u03bb, which balances the bias and variance of the crit-\nics through bootstrapping and Monte Carlo value estimation (Hafner et al., 2023). The total cost over H steps, denoted as JC,H\u03d5 , is computed using the cost model C\u03d5: J C,H \u03d5 = \u2211t+H t \u03b3\ntC\u03d5(st). We use TD(\u03bb) value of cost C\u03bb to estimate JC\u03d5 and define J C\u2032 \u03d5 = (J C,H \u03d5 /H)L as an alternative estimation for avoiding errors of the cost critic, where L signifies the episode length. Concurrently, we set the\ncriterion for evaluating whether a trajectory satisfies the cost threshold b as JC \u2032\n\u03d5 < b. We use |As| to denote the number of trajectories that meet the cost threshold and represent the desired number of safe trajectories as Ns. Upon this setup, one of the following two conditions will hold:\n1. If |As| < Ns: We employ \u2212JC \u2032 \u03d5 as the sorting key \u2126. The complete set of sampled action\ntrajectories {at:t+H}N\u03c0N +N\u03c0\u03b8i=1 serves as the candidate action set Ac. 2. If |As| \u2265 Ns: We adopt JR\u03d5 as the sorting key \u2126 for SafeDreamer (OSRP), and JR\u03d5 \u2212 \u03bbpJC\u03d5 for\nSafeDreamer (OSRP-Lag), respectively, in Algorithm 1, with further discussion in Section 4.3. The safe action trajectory set As is selected as the candidate actions set Ac.\nWe select the top-k action trajectories, those with the highest \u2126 values, from the candidate action set Ac to be the elite actions Ae. Subsequently, we obtain new parameters uj and \u03c3j at iteration j:\n\u00b5j = 1k \u2211k i=1A i e, \u03c3 j = \u221a 1 k \u2211k i=1(A i e \u2212 uj)2. The planning process is concluded after reaching a predetermined number of iterations J . An action trajectory is sampled from the final distribution N (\u00b5J , (\u03c3J)2I). Subsequently, the first action from this trajectory is executed in the real environment.\n4.3 LAGRANGIAN METHOD WITH THE WORLD MODEL PLANNING\nThe Lagrangian method stands as a general solution for SafeRL, and the commonly used ones are Augmented Lagrangian (Dai et al., 2023) and PID Lagrangian (Stooke et al., 2020). However, it has been observed that employing Lagrangian approaches for model-free SafeRL results in suboptimal performance under low-cost threshold settings, attributable to inaccuracies in the critic\u2019s estimation. This work integrates the Lagrangian method with online and background planning phases to balance errors between cost models and critics. By adopting the relaxation technique of the Lagrangian method (Nocedal & Wright, 2006), the Equation (2) is transformed into an unconstrained safe model-based optimization problem, where \u03bbp is the Lagrangian multiplier:\nmax \u03c0\u03b8\u2208\u03a0\u03b8 min \u03bbp\u22650\nJR\u03d5 (\u03c0\u03b8)\u2212 \u03bbp ( JC\u03d5 (\u03c0\u03b8)\u2212 b ) (5)\nSafeDreamer (OSRP-Lag) We introduced the PID Lagrangian method (Stooke et al., 2020) into our online safety-reward planning scheme, yielding SafeDreamer (OSRP-Lag), as shown in Figure 2c. In the online planning process, the sorting key \u2126 is determined by JR\u03d5 \u2212 \u03bbpJC\u03d5 when |As| \u2265 Ns, where JC\u03d5 is approximated using the TD(\u03bb) value of cost, denoted as C \u03bb, computed with V\u03c8c and C\u03d5:\nC\u03bb(st) =\n{ C\u03d5(st) + \u03b3 ( (1\u2212 \u03bb)V\u03c8c (st+1) + \u03bbC\u03bb(st+1) ) if t < H\nV\u03c8c (st) if t = H (6)\nThe intuition of OSRP-Lag is to optimize a conservative policy under comparatively safe conditions, considering long-term risks. The process for updating the Lagrangian multiplier remains consistent with Stooke et al. (2020) and depends on the episode cost encountered during the interaction between agent and environment. Refer to Algorithm 3 for additional details.\nSafeDreamer (BSRP-Lag) We use the Lagrangian method in background safety-reward planning (BSRP) for the safe actor training, which is referred to as SafeDreamer (BSRP-Lag), as shown in Figure 2d. During the actor training, we produce imagined latent rollouts of length T = 15 within the world model in the background. We begin with observations from the replay buffer, sampling actions from the actor, and observations from the world model. The world model also predict rewards and costs, from which we compute TD(\u03bb) value R\u03bb(st) and C\u03bb(st). These values are then utilized in stochastic backpropagation (Hafner et al., 2023) to update the safe actor, a process we denominate as background safety-reward planning. The training loss in this process guides the safe actor to maximize expected reward return and entropy while minimizing cost return, utilizing the Augmented Lagrangian method (Simo & Laursen, 1992; As et al., 2022):\nL(\u03b8) = \u2212 T\u2211 t=1 sg ( R\u03bb(st) ) + \u03b7H [\u03c0\u03b8 (at | st)]\u2212\u03a8 ( sg(C\u03bb(st)), \u03bb k p, \u00b5 k )\n(7)\nHere, sg(\u00b7) represents the stop-gradient operator of the world model and critics, \u00b5k = max(\u00b5k\u22121(\u03bd+ 1.0), 1.0) represents a non-decreasing term that corresponds to the current gradient step k, \u03bd > 0. Define \u2206 = ( C\u03bb(st)\u2212 b ) . The update rules for the penalty term in the training loss and the Lagrangian multiplier are as follows:\n\u03a8 ( sg(C\u03bb(st)), \u03bb k p, \u00b5 k ) , \u03bbk+1p = \u03bbkp\u2206+ \u00b5k 2 \u2206 2, \u03bbkp + \u00b5 k\u2206 if \u03bbkp + \u00b5 k\u2206 \u2265 0\n\u2212 (\u03bb k p)\n2\n2\u00b5k , 0 otherwise\n(8)"
        },
        {
            "heading": "5 PRACTICAL IMPLEMENTATION",
            "text": "Leveraging the framework above, we develop SafeDreamer, a safe model-based RL algorithm that extends the architecture of DreamerV3 (Hafner et al., 2023).\nWorld Model Implementation The world model, trained via a variational auto-encoding (Kingma & Welling, 2013), transforms observations ot into latent representations zt. These representations are used in reconstructing observations, rewards, and costs, enabling the evaluation of reward and safety of action trajectories during planning. The representations zt are regularized towards a prior by a regularization loss, ensuring the representations are predictable. We utilize the predicted distribution over z\u0302t from the sequence model as this prior. Meanwhile, we utilize representation zt as a posterior, and the future prediction loss trains the sequence model to leverage historical information from times before t to construct a prior approximating the posterior at time t. The observation, reward, and cost decoders are trained by optimizing the log-likelihood:\nL(\u03d5) = T\u2211 t=1 \u03b1q KL [zt \u2225 sg(z\u0302t)]\ufe38 \ufe37\ufe37 \ufe38 regularization loss +\u03b1pKL [sg(zt) \u2225 z\u0302t]\ufe38 \ufe37\ufe37 \ufe38 future prediction loss\n\u2212 \u03b2o lnO\u03d5 (ot | st)\ufe38 \ufe37\ufe37 \ufe38 observation loss \u2212\u03b2r lnR\u03d5 (rt | st)\ufe38 \ufe37\ufe37 \ufe38 reward loss \u2212\u03b2c lnC\u03d5 (ct | st)\ufe38 \ufe37\ufe37 \ufe38 cost loss\n(9)\nHere, sg(\u00b7) is the stop-gradient operator. The world model are based on the Recurrent State Space Model (RSSM) (Hafner et al., 2019b), utilizing GRU (Cho et al., 2014) for sequence modeling.\nThe SafeDreamer algorithm Algorithm 2 illustrates how the introduced components interrelate to build a safe model-based agent. We sample a batch of B sequences of length L from a replay buffer to train the world model during each update. In SafeDreamer (OSRP and OSRP-Lag), the reward-driven actor is trained by minimizing Equation (10), aiming to maximize both the reward and entropy. The reward-driven actor serves to guide the planning process, which avoids excessively conservative policy. In OSRP-Lag, the cost return of each episode is utilized to update the Lagrangian multiplier \u03bbp via Algorithm 3. Specifically, SafeDreamer (BSRP-Lag) update the safe actor and\nLagrangian multiplier \u03bbp via Equation (7) and Equation (8), respectively. Subsequently, the safe actor is employed to generate actions during exploration, which speeds up decision-making and avoids the requirement for online planning. See Appendix A for the design of critics and additional details."
        },
        {
            "heading": "6 EXPERIMENTAL RESULTS",
            "text": "This section addresses the following issues: (1). How does SafeDreamer compare to existing safe model-free RL algorithms? (2). How does SafeDreamer compare to safe model-based RL algorithms? (3). Does SafeDreamer excel in visual and low-dimensional input environments?"
        },
        {
            "heading": "6.1 TASKS SPECIFICATION",
            "text": "We use different robotic agents in Safety-Gymnasium (Ji et al., 2023a), which includes all tasks in Safety-Gym (Ray et al., 2019). The goal in both environments is to navigate robots to predetermined locations while avoiding collisions with hazards and vases. We evaluate algorithms using five fundamental environments (refer to Figure 28). The agent receives 64\u00d764 egocentric images or low-dimensional sensor inputs. Performance is assessed using metrics from Ray et al. (2019): \u2022 Average undiscounted reward return over E episodes: J\u0302(\u03c0) = 1E \u2211E i=1 \u2211Tep t=0 rt\n\u2022 Average undiscounted cost return over E episodes: J\u0302c(\u03c0) = 1E \u2211E i=1 \u2211Tep t=0 ct \u2022 Average cost throughout the entire training phase, namely the cost rate: Given a total of T interaction steps, we define the cost rate \u03c1c(\u03c0) = \u2211T t=0 ct T .\nWe calculate J\u0302(\u03c0) and J\u0302c(\u03c0) by averaging episode costs and rewards over E = 10 episodes, each of length Tep = 1000, without network updates. Unlike other metrics, \u03c1c(\u03c0) is calculated using costs incurred during training, not evaluation. Details are in Appendix D."
        },
        {
            "heading": "6.2 COMPARISONS",
            "text": "We compare SafeDreamer with other SafeRL algorithms. For further hyperparameter configurations and experiments, refer to Appendix B and C. The baselines include: (1). PPO-Lag, TRPO-Lag\n(Ray et al., 2019) (Model-free): Lagrangian versions of PPO and TRPO. (2). CPO (Achiam et al., 2017) (Model-free): A policy search algorithm with near-constraint satisfaction guarantees. (3). Saute\u0301 PPO, TRPO (Sootla et al., 2022a) (Model-free): Eliminates safety constraints into the statespace and reshape the objective function. (4). Safe SLAC (Hogewind et al., 2022) (Model-based): Combines SLAC (Lee et al., 2020) with the Lagrangian methods. (5). LAMBDA (As et al., 2022) (Model-based): Implemented in DreamerV1, combines Bayesian and Lagrangian methods. (6). MPC (Wen & Topcu, 2018; Liu et al., 2020) (Model-based): Employs CCEM for MPC with a ground-truth simulator, termed MPC:sim. (7). MBPPO-Lag (Jayant & Bhatnagar, 2022) (Model-based): Trains the policy through background planning using ensemble Gaussian models. (8). DreamerV3 (Hafner et al., 2023) (Model-based): Integrates practical techniques and excels across domains."
        },
        {
            "heading": "6.3 RESULTS",
            "text": "The results of experiments are shown in Figure 4 and Figure 5, and each experiment is conducted across 3 runs. SafeDreamer surpasses existing model-free methods. Meanwhile, SafeDreamer attains rewards similar to model-based RL methods like LAMBDA, with a substantial 90.5% reduction in costs. Additionally, SafeDreamer (OSRP-Lag) exhibits 33.3% and 30.4% reductions in cost rate compared to OSRP and BSRP-Lag, respectively, indicating that incorporating the Lagrangian method in online planning enhances safety during the training process.\nSwift Convergence to Nearly Zero-cost As shown in Figure 15, SafeDreamer surpasses model-free algorithms regarding both rewards and costs. Although model-free algorithms can decrease costs, they struggle to achieve higher rewards. This challenge stems from their reliance on learning a policy purely through trial-and-error, devoid of world model assistance, which hampers optimal solution discovery with limited data samples. In Safety-Gymnasium tasks, the agent begins in a safe region at episode reset without encountering obstacles. A feasible solution apparent to humans is to keep the agent stationary, preserving its position. However, even with this simplistic policy, realizing zero-cost with model-free algorithms either demands substantial updates or remains elusive in some tasks.\nDual Objective Realization: Balancing Enhanced Reward with Minimized Cost As depicted in Figure 3, SafeDreamer uniquely attains minimal costs while achieving higher rewards in the five visual-only safety tasks. In contrast, model-based algorithms such as LAMBDA and Safe SLAC attain a cost limit beyond which further reductions are untenable due to the inaccuracies of the world model. On the other hand, in environments with denser or more dynamic obstacles, such as SafetyPointGoal2 and SafetyPointButton1, MPC struggles to ensure safety due to the absence of a cost critic within a limited online planning horizon. Integrating a world model with critics enables agents to effectively utilize information on current and historical states to ensure their safety.\nFrom the beginning of training, our algorithms demonstrate safety behavior, ensuring extensive safe exploration. Specifically, in the SafetyPointGoal1 and SafetyPointPush1 environments, SafeDreamer matches the performance of DreamerV3 in reward while preserving nearly zero-cost.\nMastering Diverse Tasks in Visual and Low-dimensional Tasks We conducted evaluations within two low-dimensional sensor input environments, namely SafetyPointGoal1 (vector) and SafetyRacecarGoal1 (vector) (refer to Figure 5). The reward of MBPPO-Lag ceases to increase when the cost begins to decrease, similar to the phenomenon observed in PPO-Lag. Distinctly, our algorithm optimizes rewards while concurrently achieving a substantial cost reduction. Our results highlight our contribution in revealing the potential of planning within a word model to achieve exceptional results within the Safety-Gymnasium benchmark.\nPerformance Discussion under Different Tasks In the PointGoal1, OSRP exhibited higher costs compared to OSRP-Lag, potentially due to inaccuracies in the cost model. The more conservative policy outcome of OSRP-Lag implies that integrating a cost critic via the Lagrangian method can effectively reduce the errors associated with estimating cost return solely on the basis of the cost model. In the RacecarGoal1, the larger size of the Racecar contributes to a higher likelihood of contact with obstacles. As a result, the OSRP-Lag algorithm exhibits a more conservative approach in this scenario compared to the other two algorithms. In the PointButton1 scenario, which requires the agent to navigate around dynamically moving obstacles, OSRP\noutperformed BSRP in terms of rewards. This may be attributed to its use of the world model for online planning, enabling it to predict dynamic environmental changes in real time. In contrast, BSRP, which relies solely on its actor network for action decisions and lacks online planning, is less adept at anticipating real-time dynamic changes in obstacles. In the PointPush1, the optimal policy involves the agent learning to wedge its head in the middle gap of the box to push it quickly. We noticed that BSRP could converge to this optimal policy, whereas OSRP and OSRP-Lag had difficulty discovering it during online planning. This may be due to the limited length and number of planning trajectories, making it hard to search for dexterous strategies within a finite time. BSRP\u2019s significant advantage in this environment suggests it might be better suited for static environments that require more precise operations. In the PointGoal2, with more obstacles in the environment, OSRP-Lag, which introduces a cost critic to estimate cost return, tends to be more conservative than OSRP. This leads to a slower increase in reward for OSRP-Lag. Although OSRP\u2019s reward increases as quickly as BSRP\u2019s, its cost is higher compared to the other two algorithms, indicating that relying solely on the cost model for planning can result in errors."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this work, we tackle the issue of zero-cost performance within SafeRL, with the objective of finding an optimal policy while satisfying safety constraints. We introduce SafeDreamer, a safe model-based RL algorithm that utilizes safety-reward planning of world models and the Lagrangian methods to balance long-term rewards and costs. SafeDreamer surpasses previous methods by achieving higher rewards and lower costs, demonstrating superior performance in tasks with lowdimensional and visual inputs. To our knowledge, SafeDreamer is the first algorithm to achieve nearly zero-cost in final performance, utilizing vision-only input in the Safety-Gymnasium benchmark. However, SafeDreamer trains each task independently, incurring substantial costs with each individual task. Future research should leverage offline data from multiple tasks to pre-train the world model, examining its ability to facilitate the safe exploration of new tasks. Considering the various constraints that robots must adhere to in the real world, utilizing a world model for effective environmental modeling and deploying it in robots to accomplish constraint-based tasks is a potential direction.\nReproducibility Statement: All SafeDreamer agents are trained on one Nvidia 3090Ti GPU each and all experiments are conducted utilizing the Safety-Gymnasium benchmark1. The source code and other results are available on the project website: https://sites.google.com/view/ safedreamer. We implemented the baseline algorithms based on the following code repository: LAMBDA2, Safe SLAC3, MBPPO-Lag4, and Dreamerv35, respectively. We further included additional baseline algorithms: CPO, TRPO-Lag, PPO-Lag, Saute\u0301 PPO and Saute\u0301 TRPO, sourced from the Omnisafe6. For additional hyperparameter configurations and experiments, please refer to Appendix B and Appendix C."
        },
        {
            "heading": "A DETAILS OF SAFEDREAMER TRAINING PROCESS",
            "text": ""
        },
        {
            "heading": "A.1 THE PROCESS OF SAFEDREAMER",
            "text": "Algorithm 2: SafeDreamer. Input: batch length T , batch size B, episode length L, initial Lagrangian multiplier \u03bbp\n1 Initialize the world model parameters \u03d5, actor-critic parameters \u03b8, V\u03c8r , V\u03c8c ; 2 Initialize dataset D using a random policy; 3 while not converged do 4 Sample B trajectories {ot, at, ot+1, rt+1, ct+1}t:t+T \u223c D ; 5 Update the world model via minimize Equation (9) ; 6 Condense ot:t+T into st:t+T via the world model; 7 Generate latent rollouts using the actor within the world model with st:t+T as the initial state; 8 Update the reward and cost critics via minimize Equation (11); 9 Update the actor and Lagrangian multiplier \u03bbp;\n10 JC \u2190 0; 11 for t = 1 to L do 12 Condense ot into latent state via st \u223c S\u03d5(ot, st\u22121, at\u22121); 13 Sample at from the safe actor or Safety-Reward Planner; 14 Execute action at, observe ot+1, rt+1, ct+1 returned from the environment; 15 Update dataset D \u2190 D \u222a {ot, at, ot+1, rt+1, ct+1}; 16 JC \u2190 JC + ct+1; 17 end 18 end"
        },
        {
            "heading": "A.2 THE PROCESS OF PID LAGRANGIAN",
            "text": "Algorithm 3: PID Lagrangian (Stooke et al., 2020) Input: Proportional coefficient Kp, integral coefficient Ki, differential coefficient Kd\n1 Initialize previous integral item: I0 \u2190 0; 2 Initialize previous episode cost: J0C \u2190 0; 3 while iteration k continues do 4 Receive cost JkC ; 5 P \u2190 JkC \u2212 d; 6 D \u2190 max(0, JkC \u2212 Jk\u22121C ); 7 Ik \u2190 max(0, Ik\u22121 +D); 8 \u03bbp \u2190 max(0,KpP +KiIk +KdD); 9 Return Lagrangian multiplier \u03bbp;\n10 end\nPID Lagrangian with Planning The safety planner might fail to ensure zero-cost when the required hazard detection horizon surpasses the planning horizon, in accordance with the constraint (JC,H\u03d5 /H)L < b during planning. To overcome this limitation, we introduced the PID Lagrangian method (Stooke et al., 2020) into our planning scheme for enhancing the safety of exploration, yielding SafeDreamer (OSRP-Lag).\nCost Threshold Settings Our empirical studies show that for all SafeDreamer variations, a cost threshold of 2.0 leads to consistent convergence towards nearly zero-cost. Notably, when the cost threshold is set at 1.0 or below, SafeDreamer(OSRP) and SafeDreamer(OSRP-Lag) both struggle to converge within the allotted time. In contrast, SafeDreamer(LAG) manages to operate with a zero-cost threshold on some tasks but requires extended periods to achieve convergence. As such, in our current experimental setup, the cost threshold is set to 2.0.\nReward-driven Actor A reward-driven actor is trained to guide the planning process, enhancing reward convergence. The actor loss aims to balance the maximization of expected reward and entropy. The gradient of the reward term is estimated using stochastic backpropagation (Hafner et al., 2023), whereas that of the entropy term is calculated analytically:\nL(\u03b8) = \u2212 T\u2211 t=1 sg ( R\u03bbt (st) ) + \u03b7H [\u03c0\u03b8 (at | st)] (10)\nReward and Cost Critics The sparsity and heterogeneous distribution of costs within the environment complicate the direct regression of these values, thereby presenting substantial challenges for the learning efficiency of the cost critic. Leveraging the methodology from (Hafner et al., 2023), we employ a trio of techniques for critic training: discrete regression, twohot encoded targets (Bellemare et al., 2017; Schrittwieser et al., 2020), and symlog smoothing (Hafner et al., 2023; Webber, 2012):\nLreward critic(\u03c8r) = \u2212 T\u2211 t=1 sg ( twohot ( symlog ( R\u03bbt )))T ln p\u03c8r (\u00b7 | st) (11)\nwhere the critic network output the Twohot softmax probability p\u03c8 (bi | st) and\ntwohot(x)i =  |bk+1 \u2212 x| / |bk+1 \u2212 bk| if i = n|bk \u2212 x| / |bk+1 \u2212 bk| if i = n+ 10 else , n = |B|\u2211 j=1 1(bj<x), B = [\u221220 . . . 20]\nWe define the bi-symmetric logarithm function (Webber, 2012) as symlog(x) = sign(x) ln(|x|+ 1) and its inverse function, as symexp(x) = sign(x)(exp(|x|)\u2212 1). The cost critic loss, represented as Lcost critic(\u03c8c), is calculated similarly, using TD(\u03bb) value of cost C\u03bbt . Additionally, extending the principle of Onehot encoding to continuous values, Twohot encoding allows us to reconstruct target values after encoding:v\u03c8 (st) = symexp ( p\u03c8 (\u00b7 | st)T B ) . For further details, please refer to\n(Hafner et al., 2023)."
        },
        {
            "heading": "B HYPERPARAMETERS",
            "text": "The SafeDreamer experiments were executed utilizing Python3 and Jax 0.3.25, facilitated by CUDA 11.7, on an Ubuntu 20.04.2 LTS system (GNU/Linux 5.8.0-59-generic x86 64) equipped with 40 Intel(R) Xeon(R) Silver 4210R CPU cores (operating at 240GHz), 251GB of RAM, and an array of 8 GeForce RTX 3090Ti GPUs.\nMPC:sim. We apply CCEM to MPC using a ground-truth simulator, denoted as MPC:sim, omitting the use of a value function, thus establishing a non-parametric baseline. We restrict the number of sample trajectories to 150, with a planning horizon of 15 and 6 iterations, due to the computational demands of simulator-based planning. MPC exhibits rapid convergence with the ground truth simulator and performs well in scenarios with fewer obstacles, but exhibits suboptimal performance in denser and dynamically changing obstacle environments."
        },
        {
            "heading": "C EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 EXPERIMENTS ON METADRIVE",
            "text": "MetaDrive (Li et al., 2022) stands as a comprehensive, effective simulation environment for autonomous vehicle research. It features designed environments optimized for developing safe policies. The observation in MetaDrive combines vector input and first-person view input. The cost function in this setting is defined as follows:\nC(s, a) = { 1, if collides or out of the road 0, otherwise\nWe maintained a consistent environment by conducting both training and testing on the identical roadway. To augment the complexity, vehicle positions were randomized at the beginning of each episode reset. As illustrated in Table 7, SafeDreamer successfully achieved a 100% rate of convergence after engaging in 4M steps of environmental interaction. Notably, in comparison to PPOReward Shaping, SafeDreamer demonstrated enhanced efficiency in cost reduction. We observed that SafeDreamer has adopted a safe policy. Specifically, it pauses to monitor traffic flow when encountering dense vehicle presence ahead, resuming forward progression only when the vehicular density diminishes."
        },
        {
            "heading": "C.2 EXPERIMENTS ON CAR-RACING",
            "text": "We utilize the Car-Racing environment from OpenAI Gym\u2019s gymnasium.envs.box2d interface, as established by Brockman et al. (2016). Our adaptation involves augmenting this environment with a cost function analogous to the one described in Ma et al. (2022b). A cost of 1 per step is assigned for any instance of wheel skid due to excessive force:\nC(s, a) = { 1, if any wheel of the car skids 0, otherwise\nAs depicted in Figure 8 and Table 8. SafeDreamer demonstrates rapid convergence, achieving near zero-cost within 0.2M steps. Compared to DreamerV3, SafeDreamer significantly reduces vehicle slippage, thereby enhancing driving robustness.\nTable 9: Results on FormulaOne.\nAverage Reward Return Average Cost Return\nDreamerV3 32.2 \u00b1 1.22 56.36 \u00b1 2.32 Ours (BSRP-Lag) 25.3 \u00b1 0.51 0.43 \u00b1 0.08"
        },
        {
            "heading": "C.3 FORMULAONE",
            "text": "(a) Environment map (b) First-person perspective\nFigure 10: The FormulaOne benchmark.\nWe utilized the visual environment FormulaOne from Ji et al. (2023a), where the agent receives 64x64x3 image input, as illustrated in Figure 10. The environment\u2019s complexity and visual aspects significantly increase the demands on the algorithm. We employed Level 1 of FormulaOne, which requires the agent to maximize its reach to the goal while avoiding barriers and racetrack fences. Each episode begins with the agent randomly placed at one of seven checkpoints.\nAs illustrated in Figure 9 and Table 9, SafeDreamer attains nearly zero-cost after 1.5M training steps and demonstrates efficient obstacle avoidance in task completion. This indicates that SafeDreamer can achieve convergence and accomplish nearly zero-cost even in complex visual environments."
        },
        {
            "heading": "C.4 EXPERIMENTS ON SAFETY-GYMNASIUM",
            "text": "C.4.1 VIDEO PREDICTION\nUsing the past 25 frames as context, our world model predicts the next 45 steps in Safety-Gymnasium based solely on the given action sequence, without intermediate image access."
        },
        {
            "heading": "C.4.2 DREAMERV3 VS SAFEDREAMER",
            "text": ""
        },
        {
            "heading": "C.4.3 EVALUATION OF SAFEDREAMER.",
            "text": ""
        },
        {
            "heading": "C.5 ABLATION STUDIES",
            "text": ""
        },
        {
            "heading": "D ENVIRONMENTS",
            "text": "Safety-Gymnasium is an environment library specifically designed for SafeRL. This library builds on the foundational Gymnasium API (Brockman et al., 2016; Foundation, 2022), utilizing the high-performance MuJoCo engine (Todorov et al., 2012). We conducted experiments in five different environments, namely, SafetyPointGoal1, SafetyPointGoal2, SafetyPointPush1, SafetyPointButton1, and SafetyRacecarGoal1, as illustrated in Figure 28. Following Ji et al. (2023a), we adjusted the arrangement of the cameras to provide more information to planning-based algorithms in processing visual inputs compared to Ray et al. (2019). Following Liu et al. (2020); Jayant & Bhatnagar (2022); Sikchi et al. (2022), we modified the state representation to better facilitate model learning."
        },
        {
            "heading": "D.1 AGENT SPECIFICATION",
            "text": "Point. The Point robot, functioning in a 2D plane, is controlled by two distinct actuators: one governing rotation and another controlling linear movement. This separation of controls significantly eases its navigation. A small square, positioned in the robot\u2019s front, assists in visually determining its orientation and crucially supports Point in effectively manipulating boxes encountered during tasks.\nRacecar. The robot exhibits realistic car dynamics, operating in three dimensions, and controlled by a velocity and a position servo. The former adjusts the rear wheel speed to the target, and the latter fine-tunes the front wheel steering angle. The dynamics model is informed by the widely recognized MIT Racecar project. To achieve the designated goal, it must appropriately coordinate the steering angle and speed, mirroring human car operation."
        },
        {
            "heading": "D.2 TASK REPRESENTATION",
            "text": "Tasks within Safety-Gymnasium are distinct and are confined to a single environment each, as shown in Figure 30.\nGoal: The task requires a robot to navigate towards multiple target positions. Upon each successful arrival, the robot\u2019s goal position is randomly reset, retaining the global configuration. Attaining a target location, signified by entering the goal circle, provides a sparse reward. Additionally, a dense reward encourages the robot\u2019s progression through proximity to the target.\nPush: The task requires a robot to manipulate a box towards several target locations. Like the goal task, a new random target location is generated after each successful completion. The sparse reward is granted when the box enters the designated goal circle. The dense reward comprises two parts: one for narrowing the agent-box distance, and another for advancing the box towards the final target.\nButton: The task requires the activation of numerous target buttons distributed across the environment. The agent navigates and interacts with the currently highlighted button, the goal button. Upon pressing the correct button, a new goal button is highlighted, while maintaining the overall environment. The sparse reward is issued upon successfully activating the current goal button, with the dense reward component encouraging progression toward the highlighted target button."
        },
        {
            "heading": "D.3 CONSTRAINT SPECIFICATION",
            "text": "Pillars: These are used to symbolize substantial cylindrical obstacles within the environment, typically incurring costs upon contact.\nHazards: These are integrated to depict risky areas within the environment that induce costs when an agent navigates into them.\nVases: Exclusively incorporated for Goal tasks, vases denote static and delicate objects within the environment. Contact or displacement of these objects yields costs for the agent.\nGremlins: Specifically employed for Button tasks, gremlins signify dynamic objects within the environment that can engage with the agent. Contact with these objects yields costs for the agent."
        },
        {
            "heading": "D.4 EVALUATION METRICS",
            "text": "In our experiments, we employed a specific definition of finite horizon undiscounted return and cumulative cost. Furthermore, we unified all safety requirements into a single constraint (Ray et al., 2019). The safety assessment of the algorithm was conducted based on three key metrics: average episodic return, average episodic cost, and the convergence speed of the average episodic cost. These metrics served as the fundamental basis for ranking the agents, and their utilization as comparison criteria has garnered widespread recognition within the SafeRL community (Achiam et al., 2017; Zhang et al., 2020; As et al., 2022).\n\u2022 Any agent that fails to satisfy the safety constraints is considered inferior to agents that meet these requirements or limitations. In other words, meeting the constraint is a prerequisite for considering an agent superior.\n\u2022 When comparing two agents, A and B, assuming both agents satisfy the safety constraints and have undergone the same number of interactions with the environment, agent A is deemed superior to agent B if it consistently outperforms agent B in terms of return. Simply put, if agent A consistently achieves higher rewards over time, it is considered superior to agent B.\n\u2022 In scenarios where both agents satisfy the safety constraint and report similar rewards, their relative superiority is determined by comparing the convergence speed of the average episodic cost. This metric signifies the rate at which the policy can transition from an initially unsafe policy to a feasible set. The importance of this metric in safe RL research cannot be overstated."
        }
    ],
    "title": "SAFEDREAMER: SAFE REINFORCEMENT LEARNING",
    "year": 2023
}