{
    "abstractText": "In deep metric learning, the triplet loss has emerged as a popular method to learn many computer vision and natural language processing tasks such as facial recognition, object detection, and visual-semantic embeddings. One issue that plagues the triplet loss is network collapse, an undesirable phenomenon where the network projects the embeddings of all data onto a single point. Researchers predominately solve this problem by using triplet mining strategies. While hard negative mining is the most effective of these strategies, existing formulations lack strong theoretical justification for their empirical success. In this paper, we utilize the mathematical theory of isometric approximation to show an equivalence between the triplet loss sampled by hard negative mining and an optimization problem that minimizes a Hausdorff-like distance between the neural network and its ideal counterpart function. This provides the theoretical justifications for hard negative mining\u2019s empirical efficacy. Experiments performed on the Market-1501 and Stanford Online Products datasets with various network architectures corroborate our theoretical findings, indicating that network collapse tends to happen when the batch size is too large or embedding dimension is too small. In addition, our novel application of the isometric approximation theorem provides the groundwork for future forms of hard negative mining that avoid network collapse.",
    "authors": [
        {
            "affiliations": [],
            "name": "ISOMETRIC APPROXIMATION THEOREM"
        },
        {
            "affiliations": [],
            "name": "Albert Xu"
        },
        {
            "affiliations": [],
            "name": "Jhih-Yi Hsieh"
        },
        {
            "affiliations": [],
            "name": "Bhaskar Vundurthy"
        },
        {
            "affiliations": [],
            "name": "Nithya Kemp"
        },
        {
            "affiliations": [],
            "name": "Eliana Cohen"
        },
        {
            "affiliations": [],
            "name": "Lu Li"
        },
        {
            "affiliations": [],
            "name": "Howie Choset"
        }
    ],
    "id": "SP:3edf2e907348cd15e35ceef54e4e3775d725e91c",
    "references": [
        {
            "authors": [
                "Pekka Alestalo",
                "DA Trotsenko",
                "Jussi"
            ],
            "title": "V\u00e4is\u00e4l\u00e4. Isometric approximation",
            "venue": "Israel Journal of Mathematics,",
            "year": 2001
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Kevin Swersky",
                "Mohammad Norouzi",
                "Geoffrey E Hinton"
            ],
            "title": "Big self-supervised models are strong semi-supervised learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Fartash Faghri",
                "David J Fleet",
                "Jamie Ryan Kiros",
                "Sanja Fidler"
            ],
            "title": "Vse++: Improving visualsemantic embeddings with hard negatives",
            "venue": "arXiv preprint arXiv:1707.05612,",
            "year": 2017
        },
        {
            "authors": [
                "R Hadsell",
                "S Chopra",
                "Y LeCun"
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),",
            "year": 2006
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Hermans",
                "Lucas Beyer",
                "Bastian Leibe"
            ],
            "title": "In defense of the triplet loss for person reidentification",
            "venue": "arXiv preprint arXiv:1703.07737,",
            "year": 2017
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Armand Joulin",
                "Li Fei-Fei"
            ],
            "title": "Deep fragment embeddings for bidirectional image sentence mapping",
            "venue": "CoRR, abs/1406.5679,",
            "year": 2014
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In Proceedings of the IEEE international conference on computer vision workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Elad Levi",
                "Tete Xiao",
                "Xiaolong Wang",
                "Trevor Darrell"
            ],
            "title": "Rethinking preventing class-collapsing in metric learning with margin-based losses",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Weiyang Liu",
                "Yandong Wen",
                "Zhiding Yu",
                "Ming Li",
                "Bhiksha Raj",
                "Le Song"
            ],
            "title": "Sphereface: Deep hypersphere embedding for face recognition",
            "venue": "CoRR, abs/1704.08063,",
            "year": 2017
        },
        {
            "authors": [
                "Lin Ma",
                "Zhengdong Lu",
                "Hang Li"
            ],
            "title": "Learning to answer questions from image using convolutional neural network",
            "venue": "CoRR, abs/1506.00333,",
            "year": 2015
        },
        {
            "authors": [
                "Hyun Oh Song",
                "Yu Xiang",
                "Stefanie Jegelka",
                "Silvio Savarese"
            ],
            "title": "Deep metric learning via lifted structured feature embedding",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Jihoon Tack",
                "Sangwoo Mo",
                "Jongheon Jeong",
                "Jinwoo Shin"
            ],
            "title": "CSI: novelty detection via contrastive learning on distributionally shifted instances",
            "venue": "CoRR, abs/2007.08176,",
            "year": 2020
        },
        {
            "authors": [
                "Jussi Vaisala"
            ],
            "title": "Isometric approximation property in euclidean spaces. Israel",
            "venue": "Journal of Mathematics,",
            "year": 2002
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "CoRR, abs/1411.4555,",
            "year": 2014
        },
        {
            "authors": [
                "Chao-Yuan Wu",
                "R Manmatha",
                "Alexander J Smola",
                "Philipp Krahenbuhl"
            ],
            "title": "Sampling matters in deep embedding learning",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Hong Xuan",
                "Abby Stylianou",
                "Xiaotong Liu",
                "Robert Pless"
            ],
            "title": "Hard negative examples are hard, but useful",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Liang Zheng",
                "Liyue Shen",
                "Lu Tian",
                "Shengjin Wang",
                "Jingdong Wang",
                "Qi Tian"
            ],
            "title": "Scalable person re-identification: A benchmark",
            "venue": "In Computer Vision, IEEE International Conference on,",
            "year": 2015
        },
        {
            "authors": [
                "Wenzhao Zheng",
                "Yuanhui Huang",
                "Borui Zhang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Dynamic metric learning with cross-level concept distillation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Wenzhao Zheng",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Deep metric learning with adaptively composite dynamic constraints",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Mo Zhou",
                "Zhenxing Niu",
                "Le Wang",
                "Zhanning Gao",
                "Qilin Zhang",
                "Gang Hua"
            ],
            "title": "Ladder loss for coherent visual-semantic embedding",
            "venue": "CoRR, abs/1911.07528,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Research in deep metric learning investigates techniques for training deep neural networks to learn similarities and dissimilarities between data samples. This is typically achieved by learning a distance metric via feature embeddings in Rn. Deep metric learning is commonly applied to face recognition Schroff et al. (2015); Liu et al. (2017); Hermans et al. (2017) and other computer vision tasks Tack et al. (2020); Chen et al. (2020a) where there is an abundance of label values.\nContrastive loss Hadsell et al. (2006) and triplet loss Schroff et al. (2015) are two prominent examples of deep metric learning, each with variants to address specific applications. For instance, SimCLR Chen et al. (2020a;b) is a recent contrastive loss variant designed to perform unsupervised deep metric learning with state-of-the-art performance on ImageNet Russakovsky et al. (2015). Ladder Loss Zhou et al. (2019), a generalized variant of the triplet loss, handles coherent visualsemantic embedding and has important applications in multiple visual and language understanding tasks Karpathy et al. (2014); Ma et al. (2015); Vinyals et al. (2014). Cross-level concept distillation Zheng et al. (2022) achieves state of the art performance on hierarchical image classification and dynamic metric learning. Given the success of metric learning in a wide range of applications, we see value in investigating its underlying theories. In particular, we focus on the triplet loss and present a theoretical framework which explains observed but previously unexplained behaviors of the triplet loss.\nA triplet selection strategy is fundamental to any triplet loss-based deep metric learning Wu et al. (2017). This paper deals with hard negative mining, a triplet selection strategy that outperforms other mining strategies in a number of applications, for instance, person re-identification Hermans et al. (2017). In some scenarios, hard negative mining is known to suffer from network collapse,\na phenomenon where the network projects all data points onto a single point. Schroff et al. (2015) observe this effect in their experiments with a person re-identification dataset. On the other hand, Hermans et al. (2017) show that hard negative mining does not suffer from collapsed solutions for a similar dataset. These seemingly contradictory results showcase the need for a theoretical framework to explain the nature of hard negative mining and the root cause for any collapsed solutions.\nThere has been some prior literature investigating the phenomenon of network collapse. Xuan et al. (2020) show that hard negative mining leads to collapsed solutions by analyzing the gradients of a simplified neural network model. Levi et al. (2021) prove that, under a label randomization assumption, the globally optimal solution to the triplet loss necessarily exhibits network collapse. However, neither analysis offers sufficient explanation for why hard negative mining can work in practice Hermans et al. (2017); Faghri et al. (2017) and how one may reproduce such desirable behavior.\nIn this work, we explain why network collapse happens by using the theory of isometric approximation Vaisala (2002a) to draw an equivalence between the triplet loss with hard negative mining and a Hausdorff-like distance metric (Sec 3.2.1). On the Hausdorff-like metric, we observe that collapsed solutions are more likely when the batch size is large or when the embedding dimension is small. Our experiments with the person re-identification dataset (Market-1501 Zheng et al. (2015)) reconcile the findings of Schroff et al. (2015), where a batch size in the order of thousands led to network collapse, and Hermans et al. (2017), where a batch size of N = 72 showed no network collapse. We further support our predictions via experiments spanning three additional datasets (SOP Oh Song et al. (2016), CARS Krause et al. (2013), and CUB200 Wah et al. (2011)) and three different network architectures (ResNet-18, ResNet-50 He et al. (2016), GoogLeNet Szegedy et al. (2015), and a 2-layer convnet).\nThe paper is organized as follows. We begin with the definition of triplet loss with hard negative mining and then present the isometric approximation theorems in Section 2. In Section 3, we define the Hausdorff-like distance and outline a proof of its equivalence to the triplet loss with hard negative mining. This leads to a measure for network collapse followed by an illustration on how the network collapse is related to the batch size and embedding dimension. Section 4 demonstrates the validity of our theory with experiments on four datasets spanning three network architectures. Section 5 concludes the paper with possible future applications for our theory."
        },
        {
            "heading": "2 BACKGROUND AND DEFINITIONS",
            "text": "Let X be the data manifold and let Y be the classes with |Y| = c being the number of classes. Let h : X \u2192 Y be the true hypothesis function, or true labels of the data. Then the dataset consists of pairs {(xk, yk)}Nk=1 with xk \u2208 X , yk \u2208 Y and yk = h(xk). We define the learned neural network as a function f\u03b8 : X \u2192 Rn which maps similar points in the data manifold X to similar points in Rn.\nAs our paper focuses on metric learning, we define the similarity between embeddings to be the Euclidean distance\nd\u03b8(x1, x2) = ||f\u03b8(x1)\u2212 f\u03b8(x2)|| (1)\nwhere x1, x2 \u2208 X ."
        },
        {
            "heading": "2.1 TRIPLET LOSS AND HARD NEGATIVE MINING",
            "text": "In this section, we discuss the triplet loss that considers triplets of data composed of the anchor (x \u2208 X ), positive (x+), and negative (x\u2212) samples, described in (2a) and (2b). The similarity relation (2a) requires that the anchor and positive samples must be of the same class, while the dissimilarity relation (2b) requires the anchor and negative must be of different classes.\nx+ \u2208 {x\u2032 \u2208 X |h(x) = h(x\u2032)} (2a) x\u2212 \u2208 {x\u2032 \u2208 X |h(x) \u0338= h(x\u2032)} (2b)\nRestating the objective of supervised metric learning, the embedding of the anchor sample must be closer to the positive than the negative for every triplet. An example of a satisfactory triplet is shown\nin Figure 1. Formally, we express this relation via (3), where \u03b1 is the margin term.\nd\u03b8(x, x +) + \u03b1 \u2264 d\u03b8(x, x\u2212) \u2200 x, x+, x\u2212 \u2208 X (3)\nThis leads to the definition of the triplet loss in (4). LTriplet = [ d\u03b8(x, x +)\u2212 d\u03b8(x, x\u2212) + \u03b1 ] +\n(4)\nThe function [ \u00b7 ]+ = max(\u00b7, 0) zeroes negative values in order to ignore all the triplets that already satisfy the desired relation. Definition 2.1. \u03b1-Triplet-Separated. We refer to m non-empty subsets X1, \u00b7 \u00b7 \u00b7 , Xm \u2282 Rn as \u03b1-Triplet-Separated if for every Xi and Xj with i \u0338= j we have\n||x\u2212 y||+ \u03b1 \u2264 ||x\u2212 z|| \u2200x, y \u2208 Xi,\u2200z \u2208 Xj (5)\nThis property can be extended to a function f\u03b8 : X \u2192 Rn by checking whether the embedding subsets Xif\u03b8 are \u03b1-Triplet-Separated.\nXif\u03b8 = {f\u03b8(x)|x \u2208 X , h(x) = i} (6)\nIt is worth noting that LTriplet(f\u03b8) = 0 if and only if f\u03b8 is \u03b1-Triplet-Separated. An example of two Triplet-Separated sets is shown in Figure 1.\nAs mentioned in Section 1, the triplet loss relies heavily on its triplet mining strategy to achieve its performance for two popularly accepted reasons: First, enumerating all O(N3) triplets of data every iteration would be too computationally intensive to be tractable. Second, improper sampling of triplets risks network collapse Xuan et al. (2020). Our work substantiates the use of hard negative mining, a successful triplet mining strategy, by characterizing conditions that lead to network collapse."
        },
        {
            "heading": "2.2 ISOMETRIC APPROXIMATION",
            "text": "We will present a novel application of the isometric approximation theorem Vaisala (2002a) in Euclidean subsets in order to mathematically justify hard negative mining. The isometric approximation theorem primarily defines the behavior of near-isometries, or functions that are close to isometries, as given by Definition 2.2.\nDefinition 2.2. \u03b5-nearisometry. Let X and Y be real normed spaces. A function f : A \u2192 Y where A \u2282 X is called an \u03b5-nearisometry (\u03b5 > 0) if\u2223\u2223\u2223\u2223||f(x)\u2212 f(y)|| \u2212 ||x\u2212 y||\u2223\u2223\u2223\u2223 \u2264 \u03b5, \u2200 x, y \u2208 A (7) In other words, an \u03b5-nearisometry is a function that preserves the distance metric within \u03b5. The isometric approximation theorem seeks to determine how close f is to an isometry, say U : X \u2192 Y , as given by (8). Note qA(\u03b5) is a function of \u03b5 that is fixed for a given A and is thus independent of f . Consequently, inequality (8) holds for all \u03b5 > 0 and all \u03b5-nearisometries f .\n||f(x)\u2212 U(x)|| \u2264 qA(\u03b5) \u2200 x \u2208 A (8)\nNow consider the case where X and Y are n-dimensional Euclidean metric spaces, making A \u2282 Rn. Then the following theorems and definitions Vaisala (2002a;b); Alestalo et al. (2001) prove that qA(\u03b5) is linear in \u03b5 given a thickness condition on the set A. Definition 2.3. Thickness. For each unit vector e \u2208 Sn\u22121, define the projection \u03c0e : Rn \u2192 R by the dot product \u03c0e(x) = x \u00b7 e. The thickness of a bounded set A is the number\n\u03b8(A) = inf e\u2208Sn\u22121 diam(\u03c0eA) (9a)\nwhere diam(X) = sup r1,r2\u2208X ||r1 \u2212 r2|| (9b)\nTheorem 2.4 (From Theorem 3.3 Alestalo et al. (2001)). Suppose that 0 < q \u2264 1 and A \u2282 Rn is a compact set with \u03b8(A) \u2265 q diam(A). Let f : A \u2192 Rn be an \u03b5-nearisometry. Then there is an isometry U : Rn \u2192 Rn such that\n||f(x)\u2212 U(x)|| \u2264 cn\u03b5/q \u2200x \u2208 A (10)\nwith cn depending only on dimension.\nAs this property depends entirely on the set A, we call Theorem 2.4 the c-Isometric Approximation Property (c-IAP) on set A with c = cn diam(A)/\u03b8(A)."
        },
        {
            "heading": "3 THEORETICAL CONTRIBUTIONS",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW AND PROBLEM SETUP",
            "text": "From the background and definitions, the goal of the triplet loss is to learn a function f\u03b8 such that the induced distance metric d\u03b8 satisfies the property in (3). In this paper, we aim to justify the use of hard negative mining with the triplet loss for this task and offer theoretical explanations for why it sometimes leads to collapsed solutions. Furthermore, we wish to be able to predict when network collapse happens.\nIn this section, we first prove an equivalence between a Hausdorff-like distance (13) and the triplet loss with hard negative mining. Then, with the diameter of the network\u2019s embedding set as an indicator for network collapse (diam(Xf\u03b8 ) = 0), we use the previous equivalence to show that network collapse happens when the batch size is large or when the embedding dimension is small.\n3.2 EQUIVALENCE BETWEEN dHAUS AND TRIPLET LOSS\nWe begin with the definition of the Hausdorff-like distance dhaus, draw an equivalence to the isometric error diso, which is equivalent to the triplet loss within a constant factor. Then, we illustrate all three distance functions and their equivalence with a toy example.\n3.2.1 HAUSDORFF-LIKE DISTANCE dHAUS\nReiterating the training objective from the problem setup, we aim to learn a function f\u03b8 that is \u03b1Triplet-Separated (Definition 2.1). We restate this problem as a distance minimization problem, and prove that it is equivalent to hard negative mining with the triplet loss.\nFirst we construct set of all functions f : X \u2192 Rn that are \u03b1-Triplet-Separated and denote it with F\u03b1TS . We next construct the Hausdorff-like distance metric (denoted by dhaus) between these functions that compares the embedding subsets via the Hausdorff distance metric dH .\nXif = {f(x)|x \u2208 X , h(x) = i} (11) dhaus(f1, f2) = max\ni\u2208Y dH(X\ni f1 , X i f2) (12)\nOne way to solve metric learning is to find the closest f\u03b8 to any function in F\u03b1TS as indicated by (13).\ndhaus(f\u03b8,F\u03b1TS) = inf f\u2217haus\u2208F\u03b1TS dhaus(f\u03b8, f \u2217 haus) (13)\nIn this paper, we claim that the triplet loss with hard negative mining is equivalent to minimizing dhaus(f\u03b8,FTS) within a constant factor (see Corollary 3.4).\n3.2.2 ISOMETRIC APPROXIMATION APPLIED TO dHAUS\nIn this section, we present Theorem 3.2 to show that minimizing the Hausdorff-like distance is equivalent to minimizing a difference in distance metrics, referred to as the isometric error (Definition 3.1).\nDefinition 3.1. isometric error. For two functions f, g : X \u2192 Rn, we define the isometric error diso to be the maximum difference between their distance metrics.\ndiso(f, g) = sup x1,x2\u2208X \u2223\u2223\u2223\u2223||f(x1)\u2212 f(x2)|| \u2212 ||g(x1)\u2212 g(x2)||\u2223\u2223\u2223\u2223 (14) Similar to (13), we extend the definition of isometric error to diso(f\u03b8,F\u03b1TS) as follows:\ndiso(f\u03b8,F\u03b1TS) = inf f\u2217iso\u2208F\u03b1TS diso(f\u03b8, f \u2217 iso) (15)\nTheorem 3.2. dhaus(f\u03b8,F\u03b1TS) and diso(f\u03b8,F\u03b1TS) upper bound each other within a linear factor for all f\u03b8 with some minimum thickness \u03b8\u2217.\nWe present the proof for Theorem 3.2 in Appendix A. Theorem 3.2 shows that dhaus and diso are exchangeable as minimization objectives because they upper bound each other within linear factors. And as diso is a difference of two distance functions, we can derive the triplet loss."
        },
        {
            "heading": "3.2.3 RECOVERING THE TRIPLET LOSS",
            "text": "In this section, we present Theorem 3.3 to show that the isometric error (Definition 3.1) is equivalent to the triplet loss sampled by hard negative mining.\nTheorem 3.3. The triplet loss sampled by hard negative mining and the isometric error diso upper bound each other within a linear factor.\nWe present the proof for Theorem 3.3 in Appendix B, proving that diso is exchangable with the triplet loss sampled by hard negative mining. Thus from Theorems 3.2 and 3.3, we have Corollary 3.4.\nCorollary 3.4. The optimal solution to the triplet loss sampled by hard negative mining is equivalent to the optimal solution to dhaus(f\u03b8,F\u03b1TS) within a constant factor.\nProof. The proof follows from Theorems 3.2 and 3.3, where we show that dhaus, diso, and triplet loss sampled by hard negative mining upper and lower bound each other by constant factors. Consequently, the optimal solution to triplet loss sampled by hard negative mining, and to dhaus(f\u03b8,F\u03b1TS), are equivalent within a constant factor."
        },
        {
            "heading": "3.2.4 ILLUSTRATIVE EXAMPLES FOR TRIPLET LOSS EQUIVALENCE",
            "text": "In this section, we illustrate the key ideas of the previous section\u2019s theorems by using a toy example with margin \u03b1 = 0, N = 5 points, and embedding dimension d = 2. As we will illustrate the equivalence between the triplet loss with the Hausdorff-like distance and isometric error, we can visualize the embedding points without any underlying data or neural network. See Figure 2 for the toy example setup.\nAlso shown in Figure 2 is a visualization of dhaus(f\u03b8,FTS). The numerical value of dhaus(f\u03b8,FTS) is determined by the maximum length of the arrows, which is marked in the figure with black outlines. Here, we compute the ideal f\u2217haus, see (13), by optimizing the embedding points.\nFigure 3 illustrates diso(f\u03b8,FTS), which measures the difference in distance metric. Note that the f\u2217iso that minimizes diso(f\u03b8, f \u2217 iso) is not necessarily the same as f \u2217 haus. Revisiting the second part of the proof for Theorem 3.2 (Also see Appendix A), dhaus(f\u03b8,FTS) is lower bounded by 0.5diso(f\u03b8,FTS) and upper bounded by cdiso(f\u03b8,FTS). For this specific toy example, we calculate the constant factor error to be c = 0.53. This essentially illustrates Theorem 3.2.\nLastly, we show the triplet loss sampled by hard negative mining on the right of Figure 3. The equivalence proved by Theorem 3.3 is shown by comparing the two figues in Figure 3, as the triplet selected by hard negative mining corresponds with the same three points with the largest discrepancies in distance metric. Through Figures 2 and 3, we have a visualization of the statement and proof of Corollary 3.4."
        },
        {
            "heading": "3.3 THEORETICAL INSIGHTS ON NETWORK COLLAPSE",
            "text": "We can use the embedding set diameter diam(Xf\u03b8 ) to indicate for network collapse, as the diameter becomes near zero when the network is collapsed. Furthermore, if we assume that training always ends with a Triplet-Separated network fTS (collapsed or not), we have the following triangle inequality (16). The change in diameter cannot exceed twice the maximum displacement of each embedding point as measured by the Hausdorff-like distance (13).\n|diam(XfTS )\u2212 diam(Xfinit)| \u2264 2dhaus(finit,FTS) (16)\nRe-arranging (16) and substituting in the constant-factor equivalence proven by Corollary 3.4, we find an inequality lower-bounding the diameter of the triplet-separated network.\ndiam(XfTS ) \u2265 diam(Xfinit)\u2212 cLtriplet(finit) (17)\nFrom (17), we hypothesize two factors that influence network collapse are batch size and embedding dimension. As the batch size grows large, the triplet loss necessarily grows while the embedding diameter remains constant. Therefore we expect that large batch size leads to network collapse. On the other hand, increasing embedding dimension should increase the initial embedding diameter more than the triplet loss. As a result, low embedding dimension should lead to network collapse.\nTo illustrate our hypothesis that large batch size leads to network collapse, we show another toy example with N = 20 points with dimension d = 2 in Figure 4. The arrows illustrate the f\u2217 that minimizes dhaus(f\u03b8,FTS) for this example, which is a collapsed function. We hypothesize that when f\u2217 is collapsed, the function f\u03b8 learned by using the triplet loss would also be collapsed."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "As mentioned in Section 1, current literature observes that hard negative mining results in network collapse inconsistently. Our theory proposes the testable hypothesis that network collapse happens when the batch size is too large, or when the embedding dimension is not large enough. We test\nour claims by first conducting an experiment on the Market1501 dataset Zheng et al. (2015) with a Resnet-18 He et al. (2016) architecture. This closely follows the work of Hermans et al. (2017), where the specific emphasis was on proving the efficacy of hard negative mining. To this end, we use a PK-style sampling method where P people are sampled per batch and K images are selected per person for a total batch size of N = P \u00d7K, on the Market1501 dataset. Our first experiment (see Figure 5 (a)), illustrates the effect of batch size on network collapse. Here, we use a fixed embedding dimension of d = 128, train until step 40,000, and repeat each trial 3 times. The batch size P and K are varied on a grid P \u2208 {2, 4, 8, 18} and K \u2208 {2, 4, 8, 16, 32} for a total of 20 combinations. Using (17) with constant value c = 2, we observe that the lower bound, shown in dashed blue, appears to hold across the tested batch size values. Further, as the batch size increases, we see that the embedding diameter decreases until there is a network collapse. This confirms our hypothesis that large batch size leads to network collapse.\nWe then conduct an additional experiment (see Figure 5 (b)) to study the relation between the embedding dimension and network collapse. Here, we first fix P = 8 and K = 4 for each batch and vary the embedding dimension d from 4 to 1024. The network architecture and number of training steps are the same as the previous experiment (Figure 5 (a)). Once again, using (17) with c = 2, we observe that the theoretical diameter lower bound (dashed blue line) appears to hold across the tested embedding dimension values. Furthermore, as the embedding dimension decreases, we see that the embedding diameter also decreases to a point where the network collapses, thus confirming our second hypothesis.\nIt is worth noting that the experiments described above utilize a margin parameter \u03b1 with a value of 0. While it is conjectured that the parameter \u03b1 can be tuned to prevent network collapse, we show with an additional set of experiments that \u03b1 does not play as significant a role in network collapse as the batch size or embedding dimension. Specifically, the experiments are conducted on the same Market-1501 dataset and Resnet-18 architecture but with \u03b1 = 0.01 and \u03b1 = 0.05. Larger \u03b1 were also tested, but discarded due to significant overfit. These results are presented in Appendix C.\nWe further solidify our claims by conducting three more experiments on the Market1501 dataset Zheng et al. (2015) with a ResNet-50, a 2-layer convolutional network and GoogLeNet Szegedy et al. (2015) (see Appendix E for results). Branching out to other datasets, we also conducted three additional experiments on Stanford Online Products (SOP) Oh Song et al. (2016), Stanford Cars (CARS) Krause et al. (2013), and Caltech-UCSD Birds (CUB200) Wah et al. (2011) using a Resnet-18 architecture (see Appendix D for results). The same network collapse behavior has been\nobserved in each of these experiments, demonstrating the adverse effects of large batch size and low embedding dimension irrespective of dataset or network architecture.\nIn summary, we observe that the hard negative mining performs well with lower batch size, as reported by Hermans et al. (2017), and exhibits network collapse when the batch size is increased to a large value, which agrees with the triplet loss collapse reported by Schroff et al. (2015). This resolves the apparent contradiction from prior work and offers a unified explanation for network collapse in the context of hard negative mining."
        },
        {
            "heading": "5 DISCUSSION AND CONCLUSION",
            "text": "In this paper, we apply the isometric approximation theorem to prove that the triplet loss sampled by hard negative mining is equivalent to minimizing a Hausdorff-like distance. This mathematical foundation helps us present novel insights into hard negative mining by establishing a relationship between network collapse and the batch size or the embedding dimension. Our work presents mathematical proofs to support this relation and discusses extensive experiments to corroborate the same.\nWhile we note that network collapse negatively affects the network\u2019s performance on downstream tasks, a lack of network collapse does not necessarily guarantee good performance. While simply choosing batch size to be small and embedding dimension to be large would be effective for avoiding network collapse, it is not necessarily good for optimizing network performance. Future work could involve looking at the effects of small batch size or large embedding dimension and recommending hyperparameter choices for batch size and embedding dimension.\nFurthermore, it is worth noting that the isometric approximation theorem is independent of the triplet loss. Consequently, the theorem can be applied to any system utilizing the Euclidean metric, for instance, the pairwise contrastive loss Hadsell et al. (2006) (L = d(x, x+)\u2212 [\u03b1\u2212d(x, x\u2212)]+) or the margin loss Wu et al. (2017) (L = [d(x, x+)+\u03b1\u2212 \u03b2]+ + [\u2212d(x, x\u2212)+\u03b1+ \u03b2]+). On the other hand, the unified metric learning formulation defined by Zheng et al. (2023) opens new avenues to explore alternative sampling methods and their respective functions in place of our Hausdorff-like metric dhaus. Through this and future work, we intend to leverage mathematical tools from functional analysis to explain fundamental principles in modern machine learning and artificial intelligence research."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "Thanks to Dr. Jeff Schneider for invaluable advice, discussion, and feedback. We would also like to thank the anonymouse reviewers for their comments and suggestions.\nA portion of this research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-20-2-0175. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
        },
        {
            "heading": "A PROOF OF THEOREM 3.2",
            "text": "We now present Lemma A.1 that extends the results of Theorem 2.4 to apply to diso and use this result to prove Theorem 3.2. Lemma A.1. If diso(f, g) = \u03b5 and \u03b8(f(X )) \u2265 q, then there is a function U isometric to f such that:\n||g(x)\u2212 U(x)|| \u2264 cn\u03b5/q \u2200x \u2208 X (18)\nProof. If f is invertible, then gf\u22121 is a function Rn \u2192 Rn. gf\u22121 is an \u03b5-nearisometry because diso(f, g) = \u03b5. Then if \u03b8(f(X )) \u2265 q, the conditions for Theorem 2.4 are satisfied, so there exists an isometry U1 : Rn \u2192 Rn\n||gf\u22121(x)\u2212 U1(x)|| \u2264 cn\u03b5/q \u2200x \u2208 f(X ) (19)\nThen ||g(x)\u2212 U1(f(x))|| \u2264 cn\u03b5/q \u2200x \u2208 X (20)\nTherefore if f is invertible, (18) holds with U = U1f .\nIf f is not invertible, then there exists x1 \u0338= x2 \u2208 X such that f(x1) = f(x2). We divide the elements of X into subsets X \u2020 and X \u2032 such that f is invertible on X \u2020, f(X \u2020) = f(X ), and diso is unchanged on X \u2020. Consequently, (20) holds on X \u2020. Moving our attention to X \u2032, for all x\u2032 \u2208 X \u2032 there exists x\u2020 \u2208 X \u2020 such that f(x\u2032) = f(x\u2020). Then because diso is unchanged on X \u2020, ||f(x\u2032) \u2212 g(x\u2032)|| \u2264 ||f(x\u2020) \u2212 g(x\u2020)|| \u2264 cn\u03b5/q. Therefore (18) holds for f and g on X .\nProof. [Theorem 3.2] We first prove that diso upper bounds dhaus. To this end, fix the minimizing function f\u2217iso in the following expression:\ndiso(f\u03b8,F\u03b1TS) = inf f\u2217iso\u2208F\u03b1TS diso(f\u03b8, f \u2217 iso) (21)\nFrom Lemma A.1 we have that: sup x\u2208X ||f\u03b8(x)\u2212 f\u2217iso(x)|| \u2264 c diso(f\u03b8, f\u2217iso) (22)\nwith c = cn/\u03b8\u2217. From the definition of Hausdorff-like distance (12) we have (23), and from (13) we have (24):\ndhaus(f\u03b8, f \u2217 iso) \u2264 sup x\u2208X ||f\u03b8(x)\u2212 f\u2217iso(x)|| (23)\ndhaus(f\u03b8,F\u03b1TS) \u2264 dhaus(f\u03b8, f\u2217iso) (24)\n(25) follows from (22-24), proving that diso upper bounds dhaus within a constant factor of c. dhaus(f\u03b8,F\u03b1TS) \u2264 c diso(f\u03b8,F\u03b1TS) (25)\nFor the converse claim that dhaus upper bounds diso, we once again fix the f\u2217haus that minimizes the following expression:\ndhaus(f\u03b8,F\u03b1TS) = sup x\u2208X ||f\u03b8(x)\u2212 f\u2217haus(x)|| (26)\nNext, for the four points f\u03b8(x1), f\u03b8(x2), f\u2217haus(x1), and f \u2217 haus(x2), apply the triangle inequality via (27) to get (28).\n||f\u03b8(x1)\u2212 f\u03b8(x2)|| \u2264 ||f\u03b8(x1)\u2212 f\u2217haus(x1)||+||f\u2217haus(x1)\u2212 f\u2217haus(x2)||+ ||f\u2217haus(x2)\u2212 f\u03b8(x2)||  (27) \u2264\n(||f\u2217haus(x1)\u2212 f\u2217haus(x2)||+ 2 sup x\u2208X ||f\u03b8(x)\u2212 f\u2217haus(x)|| ) (28)\nIt is worth noting that (28) holds for all x1, x2 \u2208 X . Furthermore, we can swap f\u03b8 and f\u2217haus in (28) and use (26) to get (29) and thus (30).\ndiso(f\u03b8, f \u2217 haus) =\nsup x1,x2\u2208X \u2223\u2223\u2223\u2223||f\u03b8(x1)\u2212 f\u03b8(x2)|| \u2212 ||f\u2217haus(x1)\u2212 f\u2217haus(x2)||\u2223\u2223\u2223\u2223 \u2264 2dhaus(f\u03b8,F\u03b1TS)\n(29)\ndiso(f\u03b8,F\u03b1TS) \u2264 diso(f\u03b8, f\u2217haus) \u2264 2dhaus(f\u03b8,F\u03b1TS) (30)\n(30) proves that dhaus upper bounds diso within a constant factor of 2.\nThus we prove that dhaus and diso upper bound each other within constant factors."
        },
        {
            "heading": "B PROOF OF THEOREM 3.3",
            "text": "Proof. [Theorem 3.3] Here, we present a detailed proof for the theorem using equations (31-42)\nFrom the definition of diso in (31), we introduce the anchor, positive, and negative triplet (x, x+, x\u2212) in (32) by re-labelling x1 \u2192 x. Recognizing that x2 must either have the same or different label from x1, we re-label x2 \u2192 x+ or x2 \u2192 x\u2212, and pick the max of these distances for any given triplet.\ndiso(f\u03b8,F\u03b1TS) = inf f\u2217\u2208F\u03b1TS sup x1,x2\u2208X \u2223\u2223\u2223\u2223||f\u03b8(x1)\u2212 f\u03b8(x2)|| \u2212 ||f\u2217(x1)\u2212 f\u2217(x2)||\u2223\u2223\u2223\u2223 (31) = inf\nf\u2217\u2208F\u03b1TS sup x,x+,x\u2212 max {\u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x+)||\u2223\u2223\u2223\u2223, \u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)||\u2223\u2223\u2223\u2223} (32)\nInequality (33) follows from max(a, b) \u2264 a+ b for positive a, b.\n\u2264 inf f\u2217\u2208F\u03b1TS sup x,x+,x\u2212 \u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x+)||\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)||\u2223\u2223\u2223\u2223 (33)\nNow fix the f\u2217 that minimizes (33). We next prove via contradiction that the first term (34a) is positive and the second term (34b) is negative.\n||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x+)|| (34a) ||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)|| (34b)\nThere are four cases we must consider here, as we treat the zero case as either positive or negative. Case 1: (34a) is positive, (34b) is positive. Denoting this as ++, our four cases are (1 : ++), (2 : \u2212\u2212), (3 : \u2212+), (4 : +\u2212). Now we prove by contradiction that case 4 is the only valid one. Case 1(++): Consider the function f\u2020(x) = (1 + \u03b4)f\u2217(x) where \u03b4 > 0 is a small constant. Then diso(f\u03b8, f \u2020) < diso(f\u03b8, f \u2217), contradicting the statement that f\u2217 minimizes diso.\nCase 2(\u2212\u2212): Consider the function f\u2020(x) = (1 \u2212 \u03b4)f\u2217(x) where \u03b4 > 0 is a small constant. Then diso(f\u03b8, f \u2020) < diso(f\u03b8, f \u2217), contradicting the statement that f\u2217 minimizes diso.\nCase 3(\u2212+): We can algebraically rearrange (33) to get:\n||f\u2217(x)\u2212 f\u2217(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)|| \u2212 ||f\u03b8(x)\u2212 f\u03b8(x+)||+ ||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2265 0 (35) ||f\u2217(x)\u2212 f\u2217(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)|| \u2264 0 (36)\n\u2212 ( ||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| ) \u2265 0 (37)\n(36) comes from the definition of f\u2217 as a Triplet-Separated function; then (37) comes from combining (35) and (36). However, this means that the triplet that maximizes the expression has negative triplet loss, therefore there must be some other f\u22172 with a smaller value. This contradicts the statement that f\u2217 minimizes diso.\nWith Cases 1, 2, and 3 eliminated, we only have Case 4 and all the zero cases (00, +0, \u22120, 0+, 0\u2212). We note that the cases \u22120 and 0+ can be dis-proven using the same logic as Case 3. This leaves the four following valid cases (00, 0\u2212, +0, +\u2212), where we can connect back to (33) and write:\nsup x,x+,x\u2212 \u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x+)||\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)||\u2223\u2223\u2223\u2223 (38)\n= sup x,x+,x\u2212\n||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2212 ( ||f\u2217(x)\u2212 f\u2217(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)|| ) (39)\nNote that (39) resembles the triplet loss. The triplet loss for f\u2217 cannot dominate the maximum triplet loss for f\u03b8, otherwise it would contradict the statement that f\u2217 minimizes the isometric error, giving us:\n\u2212 ( ||f\u2217(x)\u2212 f\u2217(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)|| ) \u2264 sup\nx,x+,x\u2212 ||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u03b8(x)\u2212 f\u03b8(x\u2212)||\n(40) (41)\nUsing (40), we have the following relation with respect to (39).\n\u2264 2 sup x,x+,x\u2212 ||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u03b8(x)\u2212 f\u03b8(x\u2212)||+ \u03b1 (42)\nNote that (42) is identical to twice the expression for the triplet loss sampled by hard negative mining. Therefore the triplet loss sampled by hard negative mining upper bounds the isometric error by a constant factor of 2.\nAdditionally, we can prove that the triplet loss sampled by hard negative mining upper bounds the isometric error. Starting from the definition of isometric error in (43), inequality (44) follows from max(a, b) \u2265 (a+ b)/2.\ndiso(f\u03b8,F\u03b1TS) = inf f\u2217\u2208F\u03b1TS sup x1,x2\u2208X \u2223\u2223\u2223\u2223||f\u03b8(x1)\u2212 f\u03b8(x2)|| \u2212 ||f\u2217(x1)\u2212 f\u2217(x2)||\u2223\u2223\u2223\u2223 (43) \u2265 1\n2 inf f\u2217\u2208F\u03b1TS sup\nx,x+,x\u2212 \u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x+)||\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)||\u2223\u2223\u2223\u2223 (44)\nOnce again fixing f\u2217, we have equality (45) by the same logic as the previous part. Inequality (46) follows from the fact that ||f\u2217(x) \u2212 f\u2217(x+)|| \u2212 ||f\u2217(x) \u2212 f\u2217(x\u2212)|| \u2264 \u2212\u03b1 by the definition of f\u2217 as \u03b1 Triplet-Separated.\n= 1\n2 sup x,x+,x\u2212 ||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u03b8(x)\u2212 f\u03b8(x\u2212)|| \u2212\n( ||f\u2217(x)\u2212 f\u2217(x+)|| \u2212 ||f\u2217(x)\u2212 f\u2217(x\u2212)|| ) (45)\n\u2265 1 2 sup x,x+,x\u2212 ||f\u03b8(x)\u2212 f\u03b8(x+)|| \u2212 ||f\u03b8(x)\u2212 f\u03b8(x\u2212)||+ \u03b1 (46)\nTherefore isometric error upper bounds the triplet loss sampled by hard negative mining by a constant factor of 2.\nC ADDITIONAL EXPERIMENTS FOR NON-ZERO \u03b1\nIn the experiments shown in the main paper, we fixed \u03b1 to be zero. To prove that using a nonzero \u03b1 does not affect the results, we repeated the Market1501 experiments with \u03b1 = 0.01 and 0.05, observing that our results remain consistent. See Figure 6 for the embedding diameter vs batch size plots, which follow the same trends as the \u03b1 = 0 experiment shown in Figure 5. Similarly, the embedding diameter vs dimension plots in Figure 7 also follow similar trends to those shown by the \u03b1 = 0 experiments in Figure 5.\nIn addition, we also conducted experiments with a larger \u03b1 = 0.1, but decided to stop there as the trained networks began to exhibit significant overfit. In particular, the validation loss values began to increase while the training loss flatlined at the L = 0.1 line. As a result, the final embedding diameters on the validation data did not show any meaningful pattern, so we choose not to show those plots in the appendix."
        },
        {
            "heading": "D ADDITIONAL EXPERIMENTS FOR CUB200, SOP, AND CARS DATASETS",
            "text": "In the main paper, our experiments were conducted only on the Market1501 dataset. To show that our results generalize beyond just one dataset, we have replicated the same experiments on 3 additional datasets: Stanford Online Products (SOP) Oh Song et al. (2016), Caltech Birds (CUB200) Wah et al. (2011), and Stanford Cars (CARS) Krause et al. (2013). We train a Resnet18 network from scratch for each experiment with 3 repetitions for each hyperparameter combination, and leave the margin parameter \u03b1 = 0. One difference from the main experiments is that these three datasets\ndo not lend themselves to the PK-style sampling that Market1501 uses, so we use a fixed batch size without the same-class guarantees that exist for PK-style sampling. Results are shown in Figures 8 and 9.\nOn the SOP dataset (Figure 8), we observe that networks collapse for large batch size and small embedding dimension, exactly the same as the Market-1501 dataset shown in the main paper. For the CUB and CARS datasets (Figure 9), we also observe that networks collapse for large batch size. However, when experimenting with the embedding dimension, we observed that the CUB and CARS datasets are extremely sensitive to the batch size; a lower batch size would never collapse the network, and a higher batch size would always collapse the network. To mitigate this sensitivity, we attempted to explore larger models with higher embedding dimensions than 1024, but were constrained by our hardware and GPU specifications."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENTS WITH DIFFERENT BASE NETWORK",
            "text": "As our theory only assumes that an ideal \u03b1-Triplet Separated network exists in the function space parameterized by the neural net\u2019s architecture, it stands to reason that our theory should be somewhat architecture-agnostic. In the main paper, we used a Resnet-18 base, and in this appendix we train a randomly initialized ResNet-50 architecture, GoogLeNet architecture Szegedy et al. (2015) as well as a simple 2-layer convolutional network. The results for ResNet-50 are shown in Figure 10. We do observe that the networks collapse for large batch size and small embedding dimension, just as they do using the ResNet-18 backbone.\nThe results for GoogLeNet are shown in Figure 11. We do observe that the networks collapse for large batch size and small embedding dimension, just as they do using the ResNet-18 backbone. We also observe a few instances of network collapse for a high embedding dimension. However, we noted memory warnings during training due to the large size of the network, and we attribute these anomalies to our hardware constraints.\nHowever, we also observed a couple instances of collapse for high embedding dimension, where our theory hypothesized that collapse would not occur. We note that while training the high embedding dimension networks, our code was raising memory warnings because the network was too large, so we suspect that these results may not be entirely accurate.\nOn the other hand, the results for a 2-layer convolutional network are shown in Figure 12. Like with GoogLeNet and ResNet, we observe that large batch size and small embedding dimension both lead to network collapse."
        }
    ],
    "year": 2024
}