{
    "abstractText": "Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomialexpressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme to learn high-degree equivariant polynomials whose coefficients are controlled by attention scores. Polynormer has been evaluated on 13 homophilic and heterophilic datasets, including large graphs with millions of nodes. Our extensive experiment results show that Polynormer outperforms state-of-the-art GNN and GT baselines on most datasets, even without the use of nonlinear activation functions. Source code of Polynormer is freely available at: github.com/cornell-zhang/Polynormer.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenhui Deng"
        },
        {
            "affiliations": [],
            "name": "Zichao Yue"
        },
        {
            "affiliations": [],
            "name": "Zhiru Zhang"
        }
    ],
    "id": "SP:8a555b94841b7a5ec5fc25ea52e59329a89f0090",
    "references": [
        {
            "authors": [
                "Uri Alon",
                "Eran Yahav"
            ],
            "title": "On the bottleneck of graph neural networks and its practical implications",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Waiss Azizian",
                "Marc Lelarge"
            ],
            "title": "Expressive power of invariant and equivariant graph neural networks",
            "venue": "arXiv preprint arXiv:2006.15646,",
            "year": 2020
        },
        {
            "authors": [
                "Deyu Bo",
                "Chuan Shi",
                "Lele Wang",
                "Renjie Liao"
            ],
            "title": "Specformer: Spectral graph neural networks meet transformers",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Aleksandar Bojchevski",
                "Johannes Gasteiger",
                "Bryan Perozzi",
                "Amol Kapoor",
                "Martin Blais",
                "Benedek R\u00f3zemberczki",
                "Michal Lukasik",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Scaling graph neural networks with approximate pagerank",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Dexiong Chen",
                "Leslie O\u2019Bray",
                "Karsten Borgwardt"
            ],
            "title": "Structure-aware transformer for graph representation learning",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jinsong Chen",
                "Kaiyuan Gao",
                "Gaichao Li",
                "Kun He"
            ],
            "title": "Nagphormer: A tokenized graph transformer for node classification in large graphs",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ming Chen",
                "Zhewei Wei",
                "Zengfeng Huang",
                "Bolin Ding",
                "Yaliang Li"
            ],
            "title": "Simple and deep graph convolutional networks",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Zhengdao Chen",
                "Soledad Villar",
                "Lei Chen",
                "Joan Bruna"
            ],
            "title": "On the equivalence between graph isomorphism testing and function approximation with gnns",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Eli Chien",
                "Jianhao Peng",
                "Pan Li",
                "Olgica Milenkovic"
            ],
            "title": "Adaptive universal generalized pagerank graph neural network",
            "venue": "arXiv preprint arXiv:2006.07988,",
            "year": 2020
        },
        {
            "authors": [
                "Krzysztof Choromanski",
                "Han Lin",
                "Haoxian Chen",
                "Tianyi Zhang",
                "Arijit Sehanobish",
                "Valerii Likhosherstov",
                "Jack Parker-Holder",
                "Tam\u00e1s Sarl\u00f3s",
                "Adrian Weller",
                "Thomas Weingarten"
            ],
            "title": "From blocktoeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Grigorios G Chrysos",
                "Stylianos Moschoglou",
                "Giorgos Bouritsas",
                "Yannis Panagakis",
                "Jiankang Deng",
                "Stefanos Zafeiriou"
            ],
            "title": "P-nets: Deep polynomial neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Grigorios G Chrysos",
                "Markos Georgopoulos",
                "Jiankang Deng",
                "Jean Kossaifi",
                "Yannis Panagakis",
                "Anima Anandkumar"
            ],
            "title": "Augmenting deep classifiers with polynomial neural networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Di Giovanni",
                "T Konstantin Rusch",
                "Michael M Bronstein",
                "Andreea Deac",
                "Marc Lackenby",
                "Siddhartha Mishra",
                "Petar"
            ],
            "title": "Veli\u010dkovi\u0107. How does over-squashing affect the power of gnns",
            "venue": "arXiv preprint arXiv:2306.03589,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Thomas Laurent",
                "Yoshua Bengio",
                "Xavier Bresson"
            ],
            "title": "Graph neural networks with learnable structural and positional representations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan Eric Lenssen"
            ],
            "title": "Fast graph representation learning with pytorch geometric",
            "venue": "arXiv preprint arXiv:1903.02428,",
            "year": 2019
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Aleksandar Bojchevski",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Predict then propagate: Graph neural networks meet personalized pagerank",
            "venue": "arXiv preprint arXiv:1810.05997,",
            "year": 2018
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chenqing Hua",
                "Guillaume Rabusseau",
                "Jian Tang"
            ],
            "title": "High-order pooling for graph neural networks with tensor decomposition",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Grigorevich Ivakhnenko"
            ],
            "title": "Polynomial theory of complex systems",
            "venue": "IEEE transactions on Systems, Man, and Cybernetics, pp",
            "year": 1971
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907,",
            "year": 2016
        },
        {
            "authors": [
                "Kezhi Kong",
                "Jiuhai Chen",
                "John Kirchenbauer",
                "Renkun Ni",
                "C. Bayan Bruss",
                "Tom Goldstein"
            ],
            "title": "GOAT: A global transformer on large-scale graphs",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Devin Kreuzer",
                "Dominique Beaini",
                "William L. Hamilton",
                "Vincent L\u00e9tourneau",
                "Prudencio Tossou"
            ],
            "title": "Rethinking graph transformers with spectral attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Chien-Kuo Li"
            ],
            "title": "A sigma-pi-sigma neural network (spsnn)",
            "venue": "Neural Processing Letters,",
            "year": 2003
        },
        {
            "authors": [
                "Xiang Li",
                "Renyu Zhu",
                "Yao Cheng",
                "Caihua Shan",
                "Siqiang Luo",
                "Dongsheng Li",
                "Weining Qian"
            ],
            "title": "Finding global homophily in graph neural networks when meeting heterophily",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Derek Lim",
                "Felix Hohne",
                "Xiuyu Li",
                "Sijia Linda Huang",
                "Vaishnavi Gupta",
                "Omkar Bhalerao",
                "Ser Nam Lim"
            ],
            "title": "Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Liheng Ma",
                "Chen Lin",
                "Derek Lim",
                "Adriana Romero-Soriano",
                "Puneet Kumar Dokania",
                "Mark Coates",
                "Philip H.S. Torr",
                "Ser Nam Lim"
            ],
            "title": "Graph inductive biases in transformers without message",
            "venue": "passing. ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Haggai Maron",
                "Heli Ben-Hamu",
                "Nadav Shamir",
                "Yaron Lipman"
            ],
            "title": "Invariant and equivariant graph networks",
            "venue": "arXiv preprint arXiv:1812.09902,",
            "year": 2018
        },
        {
            "authors": [
                "Haggai Maron",
                "Heli Ben-Hamu",
                "Hadar Serviansky",
                "Yaron Lipman"
            ],
            "title": "Provably powerful graph networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sunil Kumar Maurya",
                "Xin Liu",
                "Tsuyoshi Murata"
            ],
            "title": "Simplifying approach to node classification in graph neural networks",
            "venue": "Journal of Computational Science,",
            "year": 2022
        },
        {
            "authors": [
                "P\u00e9ter Mernyei",
                "C\u0103t\u0103lina Cangea"
            ],
            "title": "Wiki-cs: A wikipedia-based benchmark for graph neural networks",
            "venue": "arXiv preprint arXiv:2007.02901,",
            "year": 2020
        },
        {
            "authors": [
                "Kenta Oono",
                "Taiji Suzuki"
            ],
            "title": "Graph neural networks exponentially lose expressive power for node classification",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Mangpo Phothilimthana",
                "Sami Abu-El-Haija",
                "Kaidi Cao",
                "Bahare Fatemi",
                "Michael Burrows",
                "Charith Mendis",
                "Bryan Perozzi"
            ],
            "title": "Tpugraphs: A performance prediction dataset on large tensor computational graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "Oleg Platonov",
                "Denis Kuznedelev",
                "Michael Diskin",
                "Artem Babenko",
                "Liudmila Prokhorenkova"
            ],
            "title": "A critical look at the evaluation of GNNs under heterophily: Are we really making progress",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Omri Puny",
                "Derek Lim",
                "Bobak Toussi Kiani",
                "Haggai Maron",
                "Yaron Lipman"
            ],
            "title": "Equivariant polynomials for graph neural networks",
            "venue": "ArXiv, abs/2302.11556,",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Qin",
                "Weixuan Sun",
                "Hui Deng",
                "Dongxu Li",
                "Yunshen Wei",
                "Baohong Lv",
                "Junjie Yan",
                "Lingpeng Kong",
                "Yiran Zhong"
            ],
            "title": "cosformer: Rethinking softmax in attention",
            "venue": "arXiv preprint arXiv:2202.08791,",
            "year": 2022
        },
        {
            "authors": [
                "Ladislav Rampasek",
                "Mikhail Galkin",
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Guy Wolf",
                "Dominique Beaini"
            ],
            "title": "Recipe for a general, powerful, scalable graph transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Emanuele Rossi",
                "Bertrand Charpentier",
                "Francesco Di Giovanni",
                "Fabrizio Frasca",
                "Stephan G\u00fcnnemann",
                "Michael Bronstein"
            ],
            "title": "Edge directionality improves learning on heterophilic graphs",
            "venue": "arXiv preprint arXiv:2305.10498,",
            "year": 2023
        },
        {
            "authors": [
                "T Konstantin Rusch",
                "Benjamin P Chamberlain",
                "Michael W Mahoney",
                "Michael M Bronstein",
                "Siddhartha Mishra"
            ],
            "title": "Gradient gating for deep multi-rate learning on graphs",
            "venue": "arXiv preprint arXiv:2210.00513,",
            "year": 2022
        },
        {
            "authors": [
                "Ryoma Sato"
            ],
            "title": "A survey on the expressive power of graph neural networks",
            "venue": "arXiv preprint arXiv:2003.04078,",
            "year": 2020
        },
        {
            "authors": [
                "Yoan Shin",
                "Joydeep Ghosh"
            ],
            "title": "The pi-sigma network: An efficient higher-order neural network for pattern classification and function approximation",
            "venue": "In IJCNN-91-Seattle international joint conference on neural networks,",
            "year": 1991
        },
        {
            "authors": [
                "Yunchong Song",
                "Chenghu Zhou",
                "Xinbing Wang",
                "Zhouhan Lin"
            ],
            "title": "Ordered gnn: Ordering message passing to deal with heterophily and over-smoothing",
            "venue": "arXiv preprint arXiv:2302.01524,",
            "year": 2023
        },
        {
            "authors": [
                "Marshall H Stone"
            ],
            "title": "The generalized weierstrass approximation theorem",
            "venue": "Mathematics Magazine,",
            "year": 1948
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam M. Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Christodoulos Voutriaridis",
                "Yiannis S Boutalis",
                "Basil G Mertzios. Ridge polynomial networks in pattern recognition. In Proceedings EC-VIP-MC"
            ],
            "title": "4th EURASIP Conference focused on Video/Image Processing and Multimedia Communications (IEEE Cat",
            "venue": "No. 03EX667), volume 2, pp. 519\u2013524. IEEE, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "Minjie Wang",
                "Da Zheng",
                "Zihao Ye",
                "Quan Gan",
                "Mufei Li",
                "Xiang Song",
                "Jinjing Zhou",
                "Chao Ma",
                "Lingfan Yu",
                "Yu Gai"
            ],
            "title": "Deep graph library: A graph-centric, highly-performant package for graph neural networks",
            "venue": "arXiv preprint arXiv:1909.01315,",
            "year": 2019
        },
        {
            "authors": [
                "Ruoxi Wang",
                "Rakesh Shivanna",
                "Derek Cheng",
                "Sagar Jain",
                "Dong Lin",
                "Lichan Hong",
                "Ed Chi"
            ],
            "title": "Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems",
            "venue": "In Proceedings of the web conference",
            "year": 2021
        },
        {
            "authors": [
                "Qitian Wu",
                "Wentao Zhao",
                "Zenan Li",
                "David Wipf",
                "Junchi Yan"
            ],
            "title": "Nodeformer: A scalable graph structure learning transformer for node classification",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Qitian Wu",
                "Chenxiao Yang",
                "Wentao Zhao",
                "Yixuan He",
                "David Wipf",
                "Junchi Yan"
            ],
            "title": "DIFFormer: Scalable (graph) transformers induced by energy constrained diffusion",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "arXiv preprint arXiv:1810.00826,",
            "year": 2018
        },
        {
            "authors": [
                "Yujun Yan",
                "Milad Hashemi",
                "Kevin Swersky",
                "Yaoqing Yang",
                "Danai Koutra"
            ],
            "title": "Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2022
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chulhee Yun",
                "Srinadh Bhojanapalli",
                "Ankit Singh Rawat",
                "Sashank J Reddi",
                "Sanjiv Kumar"
            ],
            "title": "Are transformers universal approximators of sequence-to-sequence functions",
            "year": 1912
        },
        {
            "authors": [
                "Zaixi Zhang",
                "Qi Liu",
                "Qingyong Hu",
                "Chee-Kong Lee"
            ],
            "title": "Hierarchical graph transformer with adaptive node sampling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Haiteng Zhao",
                "Shuming Ma",
                "Dongdong Zhang",
                "Zhi-Hong Deng",
                "Furu Wei"
            ],
            "title": "Are more layers beneficial to graph transformers",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jiong Zhu",
                "Yujun Yan",
                "Lingxiao Zhao",
                "Mark Heimann",
                "Leman Akoglu",
                "Danai Koutra"
            ],
            "title": "Beyond homophily in graph neural networks: Current limitations and effective designs",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Dwivedi"
            ],
            "title": "1-WL test as well as 1-WL GNNs Sato (2020)",
            "venue": "Rampasek et al",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "As conventional graph neural networks (GNNs) are built upon the message passing scheme by exchanging information between adjacent nodes, they are known to suffer from over-smoothing and over-squashing issues (Oono & Suzuki, 2020; Alon & Yahav, 2021; Di Giovanni et al., 2023), resulting in their limited expressive power to (approximately) represent complex functions (Xu et al., 2018; Oono & Suzuki, 2020). Inspired by the advancements of Transformer-based models in language and vision domains (Vaswani et al., 2017; Dosovitskiy et al., 2021), graph transformers (GTs) have become increasingly popular in recent years, which allow nodes to attend to all other nodes in a graph and inherently overcome the aforementioned limitations of GNNs. In particular, Kreuzer et al. (2021) have theoretically shown that GTs with unbounded layers are universal equivariant function approximators on graphs. However, it is still unclear how to unlock the expressivity potential of GTs in practice since the number of GT layers is typically restricted to a small constant.\nIn literature, several prior studies have attempted to enhance GT expressivity by properly involving inductive bias through positional encoding (PE) and structural encoding (SE). Specifically, Ying et al. (2021); Chen et al. (2022a); Zhao et al. (2023); Ma et al. (2023) integrate several SE methods with GT to incorporate critical structural information such as node centrality, shortest path distance, and graph substructures. Moreover, Kreuzer et al. (2021); Dwivedi et al. (2022); Rampasek et al. (2022); Bo et al. (2023) introduce various PE approaches based upon Laplacian eigenpairs. Nonetheless, these methods generally involve nontrivial overheads to compute PE/SE, and mostly adopt the self-attention module in the vanilla Transformer model that has quadratic complexity with respect to the number of nodes in a graph, prohibiting their applications in large-scale node classification tasks. To address the scalability challenge, many linear GTs have been recently proposed. Concretely, Choromanski et al. (2021); Zhang et al. (2022); Shirzad et al. (2023); Kong et al. (2023) aim to sparsify the self-attention matrix via leveraging node sampling or expander graphs, while Wu et al. (2022; 2023) focus on kernel-based approximations on the self-attention matrix. Unfor-\ntunately, both prior work (Platonov et al., 2023) and our empirical results indicate that those linear GT models still underperform state-of-the-art GNN counterparts on several popular datasets, which poses a serious concern regarding the practical advantages of linear GTs over GNNs.\nIn this work, we provide an orthogonal way to ease the tension between expressivity and scalability of GTs. Specifically, we propose Polynormer, a linear GT model that is polynomial-expressive: an L-layer Polynormer can expressively represent a polynomial of degree 2L, which maps input node features to output node representations and is equivariant to node permutations. Note that the polynomial expressivity is well motivated by the Weierstrass theorem which guarantees any smooth function can be approximated by a polynomial (Stone, 1948). To this end, we first introduce a base attention model that explicitly learns a polynomial function whose coefficients are determined by the attention scores among nodes. By imposing the permutation equivariance constraint on polynomial coefficients based on graph topology and node features separately, we derive local and global attention models respectively from the base model. Subsequently, Polynormer adopts a linear localto-global attention paradigm for learning node representations, which is a common practice for efficient transformers in language and vision domains yet less explored on graphs. To demonstrate the efficacy of Polynormer, we conduct extensive experiments by comparing Polynormer against 22 competitive GNNs and GTs on 13 node classification datasets that include homophilic and heterophilic graphs with up to millions of nodes. We believe this is possibly one of the most extensive comparisons in literature. Our main technical contributions are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to propose a polynomial-expressive graph transformer, which is achieved by introducing a novel attention model that explicitly learns a high-degree polynomial function with its coefficients controlled by attention scores.\n\u2022 By integrating graph topology and node features into polynomial coefficients separately, we derive local and global equivariant attention modules. As a result, Polynormer harnesses the local-to-global attention mechanism to learn polynomials that are equivariant to node permutations.\n\u2022 Owing to the high polynomial expressivity, Polynormer without any activation function is able to surpass state-of-the-art GNN and GT baselines on multiple datasets. When further combined with ReLU activation, Polynormer improves accuracy over those baselines by a margin of up to 4.06% across 11 out of 13 node classification datasets, including both homophilic and heterophilic graphs.\n\u2022 Through circumventing the computation of dense attention matrices and the expensive PE/SE methods used by prior arts, our local-to-global attention scheme has linear complexity in regard to the graph size. This renders Polynormer scalable to large graphs with millions of nodes."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "There is an active body of research on GTs and polynomial networks, from which we draw inspiration to build a polynomial-expressive GT with linear complexity. In the following, we present preliminaries for both areas and provide an overview of their related work.\nGraph transformers exploit the Transformer architecture (Vaswani et al., 2017) on graphs. Specifically, given an n-node graph G and its node feature matrix X \u2208 Rn\u00d7d, where d represents the node feature dimension, a GT layer first projects X into query, key, and value matrices, i.e., Q = XWQ,K = XWK ,V = XWV , where WQ,WK ,WV \u2208 Rd\u00d7d are three trainable weight matrices. Subsequently, the output X \u2032 with self-attention is calculated as:\nS = QKT\u221a\nd ,X \u2032 = softmax(S)V (1)\nwhere S \u2208 Rn\u00d7n is the self-attention matrix. For simplicity of illustration, we assume query, key, and value have the same dimension and only consider the single-head self-attention without bias terms. The extension to the multi-head attention is standard and straightforward.\nAs Equation 1 completely ignores the graph topology, various PE/SE methods have been proposed to incorporate the critical graph structural information into GTs. Concretely, Kreuzer et al. (2021); Dwivedi et al. (2022); Rampasek et al. (2022); Bo et al. (2023) use the top-k Laplacian eigenpairs as node PEs, while requiring nontrivial computational costs to learn the sign ambiguity of Laplacian eigenvectors. Similarly, SE methods also suffer from high complexity for computing the distance of all node pairs or sampling graph substructures (Ying et al., 2021; Chen et al., 2022a; Zhao et al.,\n2023; Ma et al., 2023). Apart from the expensive PE/SE computation, most of these approaches follow Equation 1 to compute the dense attention matrix S, resulting their quadratic complexity with regard to the number of nodes. While there are scalable GTs recently proposed by linearizing the attention matrix and not involving PE/SE, they lack a thorough analysis of their expressivity in practice and may perform worse than state-of-the-art GNNs (Choromanski et al., 2021; Zhang et al., 2022; Shirzad et al., 2023; Kong et al., 2023; Wu et al., 2022; 2023). In this work, we provide a novel way to balance the expressivity and scalability of GTs, via introducing a linear GT model that can expressively represent a high-degree polynomial.\nPolynomial networks aim to learn a function approximator where each element of the output is expressed as a polynomial of the input features. Formally, we denote the mode-m vector product of a tensor T \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IM with a vector u \u2208 RIm by T\u00d7mu. Given the input feature x \u2208 Rn and the output y \u2208 Ro, polynomial networks learn a polynomial P : Rn \u2192 Ro with degree R \u2208 N:\nyj = P(x)j = bj +w[1]j T x+ xTW [2] j x+ W [3] j \u00d71 x\u00d72 x\u00d73 x+ \u00b7 \u00b7 \u00b7+ W [R] j R\u220f r=1 \u00d7rx (2)\nwhere bj \u2208 R and {W[r]j \u2208 R n\u00d7 n\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 n\ufe38 \ufe37\ufe37 \ufe38\nr times }Rr=1 are learnable parameters for the j-th element of output y. Note that Equation 2 can be naturally extended to a more general case where both the input and output of the polynomial P are matrices or higher-order tensors. Moreover, suppose x and y have the same dimension (e.g., x,y \u2208 Rn), we say a polynomial P is permutation equivariant if for any permutation \u03b1 of the indices {1, 2, \u00b7 \u00b7 \u00b7 , n}, the following property holds:\nP(\u03b1 \u00b7 x) = \u03b1 \u00b7 P(x) = \u03b1 \u00b7 y (3) Note that when x \u2208 Rn represents node features for an n-node graph (i.e., each node feature has a scalar value), the aforementioned equivariance property essentially indicates the polynomial P is equivariant to node permutations. It is worth mentioning that the polynomial networks are fundamentally distinct from the polynomial graph filtering methods widely explored in spectral-based GNNs, which purely focus on the polynomials of graph shift operators rather than node features.\nIn the literature, the notion of learnable polynomial functions can be traced back to the group method of data handling (GMDH), which learns partial descriptors that capture quadratic correlations between specific pairs of input features (Ivakhnenko, 1971). Later, the pi-sigma network (Shin & Ghosh, 1991) and its extensions (Voutriaridis et al., 2003; Li, 2003) have been proposed to capture higher-order feature interactions, while they struggle with scaling to high-dimensional input features. Recently, Chrysos et al. (2020) introduce P-nets that utilize a special kind of skip connections to efficiently implement the polynomial expansion with high-dimensional features. In addition, Chrysos et al. (2022) express as polynomials a collection of popular neural networks, such as AlexNet (Krizhevsky et al., 2012), ResNet (He et al., 2016), and SENet (Hu et al., 2018), and improve their polynomial expressivity accordingly. While these approaches have demonstrated promising results on image and audio classification tasks, they are not directly applicable to graphs.\nTo learn polynomials on graph-structured data, Maron et al. (2018) first introduce the basis of constant and linear functions on graphs that are equivariant to node permutations. Subsequently, a series of expressive graph models have been developed by leveraging polynomial functions (Maron et al., 2019; Chen et al., 2019; Azizian & Lelarge, 2020; Hua et al., 2022). More recently, Puny et al. (2023) demonstrate that the polynomial expressivity is a finer-grained measure than the traditional Weisfeiler-Lehman (WL) hierarchy for assessing the expressive power of graph learning models. Besides, they devise a graph polynomial model that achieves strictly better than 3-WL expressive power with quadratic complexity. However, these prior studies either only consider polynomials on local structures or cannot scale to large graphs due to their high complexity. In contrast, our work learns a high-degree polynomial on node features while integrating graph topology into polynomial coefficients. As a result, the learned polynomial function captures both local and global structural information with linear complexity, rendering it applicable to large-scale graphs."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this work, we follow the common setting that there are a graph G = (V, E) and its node feature matrix X \u2208 Rn\u00d7d, where n and d denote the number of nodes and node feature dimension, respectively. Our goal is to design a polynomial-expressive GT model F that produces node representations Y = F(G,X) = PG(X), where PG is a high-degree polynomial on X whose learnable\ncoefficients encode the information of G. To this end, we first introduce a base attention model in Section 3.1 that explicitly learns high-degree polynomials. To enable the learned polynomial equivariant to node permutations, we extend the base model to equivariant local and global (linear) attention models in Section 3.2, via incorporating graph topology and node features respectively. Finally, Section 3.3 presents the proposed Polynormer architecture that employs a local-to-global attention scheme based on the equivariant attention models. As a result, Polynormer preserves high polynomial expressivity while simultaneously benefiting from linear complexity."
        },
        {
            "heading": "3.1 A POLYNOMIAL-EXPRESSIVE BASE MODEL WITH ATTENTION",
            "text": "As any polynomial can be expressed as a linear combination of monomials, a natural way to build a polynomial-expressive model is to explicitly capture the monomial terms. To this end, we follow conventional polynomial networks (Chrysos et al., 2020; 2022; Wang et al., 2021) to provide Definition 3.1 of the polynomial expressivity.\nDefinition 3.1. Given an n-node graph with node features X \u2208 Rn\u00d7d, a model P : Rn\u00d7d \u2192 Rn\u00d7d is r-polynomial-expressive if for any node i and degree-(r-1) monomial Mr\u22121 formed by rows in X , P can be parameterized such that P(X)i = Xi \u2299Mr\u22121, where \u2299 denotes the Hadamard product.\nPolynomial expressivity of prior graph models. For typical message-passing GNN models, we can unify their convolution layer as: P(X)i = \u2211 j ci,jXj , where ci,j is the edge weight between nodes i and j. As each output is a linear combination of input node features, these models are at most 1-polynomial-expressive. Hence, they mainly rely on the activation function to capture nonlinearity, instead of explicitly learning high-degree polynomials. In regard to GTs and high-order GNNs (e.g., gating-based GNNs), they only capture a subset of all possible monomials with a certain degree, resulting in their limited polynomial expressivity, as discussed in Appendix C\nA motivating example. Before constructing a polynomial-expressive model, let us first examine a simplified scenario where node features x \u2208 Rn, i.e., each node feature is a scalar. In this context, we consider a model P that outputs y = P(x) = (Wx)\u2299 (x+ b), where W \u2208 Rn\u00d7n and b \u2208 Rn are weight matrices. Figure 1 shows that this model P is able to represent degree-2 polynomials, which consist of all possible monomials of degree up to 2 (except the constant term). Besides, as W controls the coefficients of quadratic monomials (i.e., x1x2, x1x3, and x2x3), we can interpret W as a general attention matrix, where Wi,j represents the importance of node j to node i.\nMotivated by the above example, we proceed to establish a base model using the following definition; we then theoretically analyze its polynomial expressivity.\nDefinition 3.2. Given the input node features X(0) \u2208 Rn\u00d7d and trainable weight matrices W \u2208 Rn\u00d7n,B \u2208 Rn\u00d7d, a model P is defined as the base model if its l-th layer is computed as:\nX(l) = (W (l)X(l\u22121))\u2299 (X(l\u22121) +B(l)) (4) Theorem 3.3. An L-layer base model P is 2L-polynomial-expressive. The complete proof for Theorem 3.3 is available in Appendix A. Theorem 3.3 shows that the polynomial expressivity of the base model increases exponentially with the number of layers. Additionally, Appendices A and B reveal that W and B control the learned polynomial coefficients, which can be viewed as attention scores for measuring the importance of different node feature interactions.\nLimitations of the base model. As the matrices W and B fail to exploit any graph inductive bias, the base model learns polynomials that are not equivariant to node permutations. Besides, the model has quadratic complexity due to the n2 size of W and thus cannot scale to large graphs. To address both limitations, we derive two equivariant models with linear attention in Section 3.2."
        },
        {
            "heading": "3.2 EQUIVARIANT ATTENTION MODELS WITH POLYNOMIAL EXPRESSIVITY",
            "text": "For clarity, we omit the layer index (l) in the ensuing discussion unless it is explicitly referenced. Instead of learning the non-equivariant matrix B \u2208 Rn\u00d7d in Equation 4, we replace it with learnable weights \u03b2 \u2208 Rd sharing across nodes, i.e., B = 1\u03b2T , where 1 \u2208 Rn denotes the all-ones vector. In addition, we apply linear projections on X to allow interactions among feature channels. This leads to Equation 5, where V = XWV and H = XWH with trainable matrices WV ,WH \u2208 Rd\u00d7d.\nX = (WV )\u2299 (H + 1\u03b2T ) (5)\nSubsequently, it is straightforward to show that we only need to achieve permutation equivariance on the term WV in Equation 5 to build an equivariant model. To this end, we introduce the following two equivariant attention models, by leveraging graph topology and node features respectively.\nEquivariant local attention. We incorporate graph topology information by setting W = A, where A is a (general) sparse attention matrix such that the nonzero elements in A represent attention scores of adjacent nodes. While A can be implemented by adopting any sparse attention methods previously proposed, we choose the GAT attention scheme (Velic\u030ckovic\u0301 et al., 2017) due to its efficient implementation. We leave the exploration of other sparse attention approaches to future work. Consequently, the term AV holds the equivariance property, i.e., (PAP T )(PV ) = P (AV ) for any node permutation matrix P . It is noteworthy that replacing W with A essentially imposes a sparsity constraint on the learned polynomial of the base model, such that polynomial coefficients are nonzero if and only if the corresponding monomial terms are formed by features of nearby nodes.\nEquivariant global attention. By setting W = softmax(S), where S denotes the global selfattention matrix defined in Equation 1, the term WV in Equation 5 becomes softmax(S)V that is permutation equivariant (Yun et al., 2019). According to our discussion in Appendix A, an L-layer model based on the updated Equation 5 learns an equivariant polynomial of degree 2L with the coefficients determined by attention scores in S. More importantly, the learned polynomial contains all monomial basis elements that capture global and high-order feature interactions. Nonetheless, the dense attention matrix S still has quadratic complexity in regard to the number of nodes, rendering the approach not scalable to large graphs. To tackle this issue, we linearize the global attention by introducing a simple kernel trick in Equation 6, where Q = XWQ,K = XWK with weight matrices WQ,WK , and \u03c3 denotes the sigmoid function to guarantee the attention scores are positive. The denominator term in Equation 6 ensures that the sum of attention scores is normalized to 1 per node. In this way, we preserve two key properties of the softmax function: non-negativity and normalization. As we can first compute \u03c3(KT )V whose output is then multiplied by \u03c3(Q), Equation 6 avoids computing the dense attention matrix, resulting in the linear global attention. In Appendix D, we further demonstrate the advantages of our approach over prior kernel-based linear attention methods in terms of hyperparameter tuning and training stability on large graphs.\nWV = \u03c3(Q)\u03c3(KT ) \u03c3(Q) \u2211\ni \u03c3(K T i,:)\nV = \u03c3(Q)(\u03c3(KT )V ) \u03c3(Q) \u2211\ni \u03c3(K T i,:)\n(6)\nComplexity analysis. Given a graph with n nodes and m edges, suppose the hidden dimension is d \u226a n, then the local attention model has the complexity of O(md + nd2). Since we exploit the kernel trick to linearize the computation of global attention in Equation 6, the complexity of global attention model is reduced from O(n2d) to O(nd2). Hence, both proposed equivariant attention models have linear complexity with respect to the number of nodes/edges.\nDiscussion. It is noteworthy that the introduced local and global attention models are intended to enable the base model to learn high-degree equivariant polynomials, which is fundamentally distinct from the purpose of prior attention models in literature. This allows our proposed Polynormer model to outperform those attention-based baselines, even without nonlinear activation functions."
        },
        {
            "heading": "3.3 THE POLYNORMER ARCHITECTURE",
            "text": "Guided by the aforementioned equivariant models with linear attention, Figure 2(b) shows the Polynormer architecture that adopts a local-to-global attention scheme based on the following modules:\nLocal attention module. We employ the local attention approach via replacing W in Equation 5 with the sparse attention matrix A as discussed in Section 3.2, which explicitly learns an equivariant\npolynomial with coefficients controlled by A and \u03b2. Since the attention scores in A are normalized in the range of (0, 1), we apply a sigmoid function \u03c3 on \u03b2 to scale it into the same range as A, which prevents \u03b2 from dominating the polynomial coefficients. This results in the local attention layer of Polynormer shown in Equation 7. By stacking L1 layers, the local attention module outputs Xlocal = \u2211L1 l=1 X\n(l), which captures equivariant polynomials with different degrees based on local attention. Xlocal then serves as the input for the following global attention module.\nX = AV \u2299 (H + \u03c3(1\u03b2T )) (7) Global attention module. By substituting Equation 6 into Equation 5, we obtain the global attention layer shown in Equation 8. After stacking L2 global attention layers, the global attention module takes as input Xlocal and produces the final node representations for downstream tasks.\nX = \u03c3(Q)(\u03c3(KT )V ) \u03c3(Q) \u2211\ni \u03c3(K T i,:)\n\u2299 (H + \u03c3(1\u03b2T )) (8)\nNote that the Polynormer architecture described above does not apply any activation function on the outputs per layer. Inspired by prior polynomial networks on images (Chrysos et al., 2022), we can optionally integrate ReLU activation into each layer, which potentially further improves the model performance by involving additional nonlinearity. We denote this version by Polynormer-r.\nDiscussion. Compared to the popular architecture of prior GTs shown in Figure 2(a), Polynormer builds upon our proposed base model, thus preserving high polynomial expressivity. Besides, our local-to-global attention scheme outperforms the prior local-and-global attention scheme, as empirically demonstrated in Section 4.3. Another important advantage of Polynormer is its ease of implementation. By avoiding the enumeration of equivariant polynomials, Polynormer offers a concise way to learn high-degree equivariant polynomials, which can be easily implemented using popular graph learning frameworks such as PyG (Fey & Lenssen, 2019) and DGL (Wang et al., 2019). We further provide an analysis of Polynormer expressivity under the WL hierarchy in Appendix K."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We have conducted an extensive evaluation of Polynormer against state-of-the-art (SOTA) GNN and GT models on both homophilic and heterophilic graphs, where nearby nodes tend to have the same or different labels. Besides, we demonstrate the scalability of Polynormer on large-scale graphs that contain millions of nodes. In addition, we perform additional ablation analysis to understand the effectiveness of the local-to-global attention scheme adopted by Polynormer. Finally, we visualize Polynormer attention scores to showcase its ability of capturing critical global structures.\nExperimental Setup. Appendix E shows the details of all 13 datasets used in our experiments, which consist of 7 homophilic and 6 heterophilic graphs. Notably, the heterophilic datasets are from Platonov et al. (2023); Lim et al. (2021), which have addressed serious issues (e.g., train-test data leakage) of conventional popular datasets such as chameleon and squirrel. We compare Polynormer against 10 competitive GNNs. Note that prior graph polynomial models except tGNN (Hua et al., 2022) discussed in Section 2 run out of GPU memory even on the smallest dataset in our\nexperiments. Besides, we also evaluate 6 GTs that have shown promising results on the node classification task. We report the performance results of baselines from their original papers or official leaderboards whenever possible, as those results are obtained by well-tuned models. For baselines whose results are not publicly available on given datasets, we tune their hyperparameters to achieve the highest possible accuracy. Detailed hyperparameter settings of baselines and Polynormer are available in Appendix H. Our hardware information is provided in Appendix F."
        },
        {
            "heading": "4.1 PERFORMANCE ON HOMOPHILIC AND HETEROPHILIC GRAPHS",
            "text": "Performance on homophilic graphs. We first compare Polynormer with 16 popular baselines that include SOTA GNNs or GTs on 5 common homophilic datasets. As shown in Table 1, Polynormer is able to outperform all baselines on 3 out of 5 datasets, which clearly demonstrate the efficacy of the proposed polynomial-expressive model. Moreover, incorporating the ReLU function can lead to further improvements in the accuracy of Polynormer. This is because the nonlinearity imposed by ReLU introduces additional higher-order monomials, which in turn enhance the quality of node representations. It is noteworthy that the accuracy gain of Polynormer over SOTA baselines is 1.65% on Computer, which is a nontrivial improvement given that those baselines have been finely tuned on these well-established homophilic datasets.\nPerformance on heterophilic graphs. Table 2 reports the average results over 10 runs on heterophilic graphs. Notably, the GT baselines underperform SOTA GNNs on all 5 datasets, which raises concerns about whether those prior GT models properly utilize the expressivity brought by the self-attention module. Moreover, there is no baseline model that consistently ranks among the top 3 models across 5 datasets. In contrast, by integrating the attention mechanism into the polynomial-expressive model, Polynormer surpasses all baselines on 4 out of 5 datasets. Furthermore, Polynormer-r consistently outperforms baselines across all datasets with the accuracy improvement by a margin up to 3.55% (i.e., 97.46% vs. 93.91% on minesweeper)."
        },
        {
            "heading": "4.2 PERFORMANCE ON LARGE GRAPHS",
            "text": "We conduct experiments on large graphs by comparing Polynormer against GTs as well as some representative GNN models. Since the graphs in ogbn-products and pokec are too large to be loaded into the GPU memory for full-batch training, we leverage the random partitioning method adopted by prior GT models Wu et al. (2022; 2023) to perform mini-batch training. As shown in Table 3, Polynormer-r outperforms all baselines on these large graphs with the accuracy gain of up to 4.06%. Besides, we can observe that the accuracy of Polynormer drops 1.64% when removing ReLU activation on ogbn-arxiv. Thus, we believe the nonlinearity associated with ReLU is more critical on\nogbn-arxiv, which is known to be a challenging dataset (Shirzad et al., 2023). In addition, Appendix J provides the training time and memory usage of Polynormer and GT baselines. Moreover, we further evaluate Polynormer on an industrial-level graph benchmark named TpuGraphs (Phothilimthana et al., 2024), whose results are provided in Appendix L."
        },
        {
            "heading": "4.3 ABLATION ANALYSIS ON POLYNORMER ATTENTION SCHEMES",
            "text": "Figure 3 shows the comparison of the SOTA baseline (red bar), Polynormer without global attention (orange bar), a variant of Polynormer where the local and global attention modules are applied simultaneously to update node features per layer (blue bar), and Polynormer (green bar). We provide the detailed architecture of the Polynormer variant in Appendix G. Besides, we omit the results of Polynormer without local attention, as it completely ignores the graph structural information and thus performs poorly on the graph datasets.\nBy comparing the orange and green bars in Figure 3, we can observe that the local attention model achieves comparable results to Polynormer on homophilic graphs (Computer and Photo), while it lags behind Polynormer on heterophilic graphs (roman-empire and minesweeper). This observation\nhighlights the efficacy of the global attention module in Polynormer, which captures global information that proves more beneficial for heterophilic graphs (Li et al., 2022), as further illustrated in Section 4.4. Moreover, Figure 3 also demonstrates that the local-to-global attention adopted by Polynormer performs better than its counterpart, i.e., local-and-global attention, which has been widely used by prior GT models (Rampasek et al., 2022; Wu et al., 2022; 2023; Kong et al., 2023). We attribute it to that the local-and-global attention may introduce the risk of mixing local and non-local interactions (Di Giovanni et al., 2023), while the local-to-global attention inherently avoids this issue. This is further confirmed by the results on Computer, where the local-and-global attention model performs even worse than the local attention alone model."
        },
        {
            "heading": "4.4 VISUALIZATION",
            "text": "For the sake of clear visualization, we randomly sample 100 nodes from minesweeper, based on which three heatmaps are drawn in Figure 4. Notably, the coordinates in each heatmap correspond to node indices in the graph. We first draw Figure 4(a) by assigning every heatmap value to 1 if the corresponding two nodes have the same label and their shortest path distance is up to 5, and 0 otherwise. This essentially highlights locally important neighbors per node. Similarly, we further visualize globally important nodes by removing the local constraint, resulting in Figure 4(b). Figure 4(c) visualizes the attention matrix in the last global attention layer of Polynormer. Note that we linearly scale the attention scores such that the maximum value is 1 for the visualization purpose. By comparing Figures 4(a) and 4(b), we observe numerous globally important nodes that can be exploited for accurate predictions on target nodes. Figure 4(c) clearly showcases that the attention scores of Polynormer effectively differentiate those globally important nodes from unimportant ones. As a result, these attention scores enable Polynormer to focus on crucial monomial terms that consist of a sequence of globally important nodes, allowing it to learn critical global structural information."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "This work introduces Polynormer that adopts a linear local-to-global attention scheme with high polynomial expressivity. Our experimental results indicate that Polynormer achieves SOTA results on a wide range of graph datasets, even without the use of nonlinear activation functions."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported in part by NSF Awards #2118709 and #2212371, a Qualcomm Innovation Fellowship, and ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA."
        },
        {
            "heading": "A PROOF FOR THEOREM 3.3",
            "text": "Proof. For the convenience of our proof, we denote the initial feature matrix by X \u2208 Rn\u00d7d, the number of layers by L, the number of nodes by n, and [n] = {1, 2, ..., n}. Besides, we set B(l) = 0,\u2200l \u2208 [L]. The role of B(l) is analyzed in Appendix B. By defining I to be an ordered set that consists of node indices (e.g., I = (i1, i2, i3, i4)), we define the following function cL on coefficients:\ncL(I;W ) = W (L) i1,i2\ufe38 \ufe37\ufe37 \ufe38\n20 times\nW (L\u22121) i2,i3 W (L\u22121) i3,i4\ufe38 \ufe37\ufe37 \ufe38\n21 times\n\u00b7 \u00b7 \u00b7W (1)i2L\u22121 ,i2L\u22121+1 \u00b7 \u00b7 \u00b7W (1) i2L\u22121,i2L\ufe38 \ufe37\ufe37 \ufe38\n2L\u22121 times\n(9)\nwhere W (j)ia,ib represents the (ia, ib)-th block in W at the j-th layer. Based on Equation 9, we further define the following monomial fL of degree 2L, where each Xi denotes a d-dimensional feature vector of node i:\nfL(I;W ,X) = cL(I;W ) Xi1 \u2299Xi2 \u00b7 \u00b7 \u00b7 \u2299Xi2L (10)\nNext, we prove by induction that the L-layer base model produces the following node representation X\n(L) i for any node i \u2208 [n]:\nX (L) i = \u2211 I\u2208Si\n2L\nfL(I;W ,X) (11)\nwhere Si2L is a set that represents all the combinations of choosing 2 L elements from [n] (with replacement), with the first element fixed to be i, i.e., Si2L := {y \u2208 [n] 2L | y1 = i}.\nBase case. When L = 1, we have:\nX (1) i = ( n\u2211 j=1 W (1) i,j Xj)\u2299Xi\n= n\u2211 j=1 W (1) i,j Xi \u2299Xj\n= \u2211\nI\u2208Si 21\nW (1) i1,i2 Xi1 \u2299Xi2\n= \u2211\nI\u2208Si 21\nf1(I;W ,X) (12)\nInduction step. When L = l, suppose we have X(l)i = \u2211\nI\u2208Si 2l\nfl(I;W ,X). Then, for L = l + 1,\nwe have:\nX (l+1) i = ( n\u2211 j=1 W (l+1) i,j X (l) j )\u2299X (l) i\n= ( n\u2211 j=1 W (l+1) i,j \u2211 J\u2208Sj\n2l\nfl(J ;W ,X))\u2299 ( \u2211 I\u2208Si\n2l\nfl(I;W ,X))\n= ( n\u2211 j=1 W (l+1) i,j \u2211 J\u2208Sj\n2l\ncl(J ;W ) Xj1 \u2299Xj2 \u00b7 \u00b7 \u00b7 \u2299Xj2l )\n\u2299 ( \u2211 I\u2208Si\n2l\ncl(I;W ) Xi1 \u2299Xi2 \u00b7 \u00b7 \u00b7 \u2299Xi2l )\n= \u2211 I\u2208Si\n2l\nn\u2211 j=1 \u2211 J\u2208Sj\n2l\ncl(I;W ) W (l+1) i,j cl(J ;W ) Xi1 \u2299Xi2 \u00b7 \u00b7 \u00b7 \u2299Xi2l\n\u2299Xj1 \u2299Xj2 \u00b7 \u00b7 \u00b7 \u2299Xj2l = \u2211 I\u2208Si\n2l+1\ncl+1(I;W ) Xi1 \u2299Xi2 \u00b7 \u00b7 \u00b7 \u2299Xi2l \u2299Xi2l+1 \u00b7 \u00b7 \u00b7 \u2299Xi2l+1\n= \u2211\nI\u2208Si 2l+1\nfl+1(I;W ,X) (13)\nThe first 3 equations and the last equation above are straightforward. For the fourth equation, we essentially expand the product in LHS to the sum in RHS. For the second last euqation, we merge the weight matrix W (l+1) into the coefficient function cl(J ;W ), and then replace the index set J = {j1, j2, ..., j2l} with I \u2032 = {i2l+1, i2l+2, ..., i2l+1}, which is combined with the set {i1, i2, ..., i2l} to obtain the new index set I = {i1, i2, ..., i2l , i2l+1, ..., i2l+1} \u2208 Si2l+1 . This complements our proof for Equation 11.\nAs for any node i, the first index of I \u2208 Si2L is always i, we can rewrite Equation 11 as:\nX (L) i = \u2211 I\u2208Si\n2L\nfL(I;W ,X)\n= \u2211\nI\u2208Si 2L\ncL(I;W ) Xi1 \u2299Xi2 \u00b7 \u00b7 \u00b7 \u2299Xi2L\n= Xi1 \u2299 \u2211\nI\u2208Si 2L\ncL(I;W ) Xi2 \u00b7 \u00b7 \u00b7 \u2299Xi2L\n= Xi \u2299 \u2211\nI\u2032\u2208S2L\u22121\nW (L) i,i\u20321 W (L\u22121) i\u20321,i \u2032 2 \u00b7 \u00b7 \u00b7W (1)i\u2032 2L\u22122 ,i\u2032 2L\u22121 Xi\u20321 \u00b7 \u00b7 \u00b7 \u2299Xi\u20322L\u22121 (14)\nwhere S2L\u22121 is a set that represents all the combinations of choosing 2L\u22121 elements from [n] (with replacement), i.e., S2L\u22121 := {y \u2208 [n]2 L\u22121}.\nAs a result, for any degree-(2L-1) monomial M2 L\u22121 formed by rows in X , we can denote it by Xj1\u2299Xj2 \u00b7 \u00b7 \u00b7\u2299Xj2L\u22121 , which corresponds to a specific oredered index set I \u2032 \u2208 S2L\u22121 in Equation 14. Therefore, we can always parameterize weight matrices W (l), l \u2208 [L] such that only the elements with indices I \u2032 that determine the given monomial are 1 in Equation 14, and all other elements in W (l) are set to 0. Consequently, Equation 14 with parameterized W (l) becomes Xi \u2299M2\nL\u22121, which complements our proof for Theorem 3.3.\nDiscussion. Notably, Equation 9 reveals that the weight matrix W controls the coefficients of the monomial terms of degree 2L in the learned polynomial. Thus, if we replace W with any attention matrix S (e.g., the global self-attention matrix described in Section 3.2), then the attention scores in S naturally control all the monomial coefficients, which essentially capture the importance of different (global) node feature interactions with order 2L. In Appendix B, we are going to provide the analysis to demonstrate that the matrix B in Equation 4 controls the coefficients of lower-degree monomials in practice."
        },
        {
            "heading": "B ANALYSIS ON B IN EQUATION 4",
            "text": "By ignoring the weight matrix B in Equation 4, we have demonstrated in Appendix A that the base model learns a polynomial that encompasses all possible monomials of degree 2L. However, our proof in Appendix A also reveals that the learned polynomial is unable to represent any monomials with degrees smaller than 2L. This raises concerns since these lower-degree monomial terms are often crucial for the model predictions (Hua et al., 2022). Fortunately, we are going to show that incorporating B into Equation 4 enables the L-layer base model to capture all monomial terms with degrees up to 2L (except the constant term).\nSpecifically, we can consider each monomial term of the learned polynomial as generated in the following way: for each layer l of the base model, we select two monomials, denoted as M (l)1 and\nM (l) 2 , from the terms (W (l)X(l\u22121)) and (X(l\u22121)+B(l)) in Equation 4, respectively. Consequently, the generated monomial after layer l is obtained by the Hadamard product, i.e., M (l) = M (l)1 \u2299M (l) 2 . We continue this process recursively until we obtain M (L) at the L-th layer.\nRegarding the selection of a monomial M (l)2 from (X (l\u22121) +B(l)), two scenarios arise: (i) If M (l)2 is a monomial term from the polynomial X(l\u22121), it increases the degree of the generated monomial M (l) by up to the degree of X(l\u22121) (i.e., 2l\u22121, as analyzed in Appendix A). (ii) If M (l)2 = B\n(l), it does not increase the degree.\nIf we never choose M (l)2 = B (l) for all l \u2208 [L], then the generated monomial at layer L has a degree of 2L, as proven in Appendix A. Hence, each time we select M (l)2 = B (l) at layer l, we reduce the degree of a monomial at the L-th layer (whose original degree is 2L) by 2l\u22121. In other words, the more we opt for M (l)2 = B\n(l), the smaller the degree of the monomial at the L-th layer, and vice versa. In the extreme scenario where we choose M (l)2 = B\n(l) for all l \u2208 [L], we obtain a monomial of degree 2L \u2212 2L\u22121 \u2212 \u00b7 \u00b7 \u00b7 \u2212 21 \u2212 20 = 1. Consequently, the weight matrices B(l), l \u2208 [L], enable the base model to capture all possible monomials with degrees ranging from 1 up to 2L.\nDiscussion. The above analysis demonstrates the importance of B in Equation 4, i.e., controlling the coefficients of lower-degree monomials, which is another key distinction of our approach to previous gating-based GNNs that ignore B in their gating units (Yan et al., 2022; Rusch et al., 2022; Song et al., 2023)."
        },
        {
            "heading": "C POLYNOMIAL EXPRESSIVITY OF PRIOR GRAPH MODELS",
            "text": "Polynomial expressivity of GTs. For the sake of simplicity, we omit the softmax operator in GTs and assume each node feature contains a scalar value, i.e., x \u2208 Rn for an n-node graph. Besides, we denote the weights for query, key, and value by wq , wk, and wv respectively. Consequently, for any node i, a GT layer produces x\u2032i = wqwkwv \u2211 j xixjxj = wqwkwvxi \u2211 j x 2 j , which consists of degree-3 monomials. However, there are still many degree-3 monomials that GTs fail to capture. For instance, any monomial in the set {x2ixj | i \u0338= j} cannot be captured by x\u2032i, resulting in the limited polynomial expressivity of prior GT models.\nPolynomial expressivity of high-order GNNs. We initially focus on a closely related model known as tGNN, which is recently proposed by Hua et al. (2022). In tGNN, the polynomial for each target node i is computed as [x1, 1]W \u2299 \u00b7 \u00b7 \u00b7 \u2299 [xk, 1]W , where [xi, 1] represents the concatenation of the feature vector of node i with a constant 1. As asserted by Hua et al. (2022), tGNN learns a multilinear polynomial in which no variables appear with a power of 2 or higher. Consequently, tGNN cannot represent any monomials where a specific variable has a power of at least 2, limiting its polynomial expressivity.\nFurthermore, there exist multiple GNN models based on gating mechanisms that implicitly capture high-order interactions among node features (e.g., GGCN (Yan et al., 2022), G2-GNN (Rusch et al., 2022), and OrderedGNN (Song et al., 2023)). However, these models primarily consider local structures and, therefore, are unable to learn high-order monomials formed by features of distant nodes. Additionally, few gating-based GNNs considers leveraging the weight matrix B in Equation 4 to learn lower-degree monomial terms, as discussed in Appendix B.\nConclusion. Due to the limitations of polynomial expressivity analyzed above, previous GTs and high-order GNNs still necessitate nonlinear activation functions to attain reasonable accuracy. In contrast, our method achieves higher accuracy than these models even without activation functions, thanks to its high polynomial expressivity."
        },
        {
            "heading": "D COMPARISON TO KERNEL-BASED LINEAR ATTENTION MODELS",
            "text": "D.1 HYPERPARAMETER TUNING\nIn literature, several linear GTs employ kernel-based methods to approximate the dense selfattention matrix within the vanilla Transformer architecture. Concretely, both GraphGPS (with\nPerformer-based attention) (Rampasek et al., 2022) and NodeFormer (Wu et al., 2022) utilize a kernel function based on positive random features (PRFs). However, PRFs introduce a critical hyperparameter, denoted as m, which determines the dimension of the transformed features. In addition to this, NodeFormer introduces another crucial hyperparameter, \u03c4 , controlling the temperature in Gumbel-Softmax, which is integrated into the kernel function. Furthermore, the DiFFormer model (Wu et al., 2023) offers a choice between two types of kernel functions in practice.\nConsequently, these kernel-based GTs require extensive tuning of these critical hyperparameters, which can be time-consuming, particularly when dealing with large graphs. In contrast, our proposed linear global attention, as defined in Equation 6, eliminates the need for any hyperparameters in model tuning.\nD.2 TRAINING STABILITY\nTo the best of our knowledge, the work most similar to Equation 6 is cosFormer (Qin et al., 2022), which primarily focuses on text data rather than graphs. The key distinction between cosFormer and our proposed linear global attention lies in their choice of kernel function. CosFormer utilizes the ReLU function instead of the Sigmoid function \u03c3 in Equation 6. While ReLU performs well on text data containing up to 16K tokens per sequence, we observe that it leads to training instability when applied to large graphs with millions of nodes.\nSpecifically, if we substitute ReLU into Equation 6, the denominator term \u2211\ni \u03c3(K T i,:) becomes\u2211\ni ReLU(K T i,:). This modification results in a training loss of NaN during our experiments on both the ogbn-products and pokec datasets. The issue arises because ReLU only sets negative values in Ki,: to zero while preserving positive values. As a consequence, the term \u2211 i ReLU(K T i,:) accumulates potentially large positive values across millions of nodes. This leads to the denominator in Equation 6 exceeding the representational capacity of a 32-bit floating-point format and results in a NaN loss during model training.\nIn contrast, our approach employs the Sigmoid function, which maps all elements in Ki,: to the range (0, 1). As a result, \u2211 i \u03c3(K T i,:) does not produce excessively large values, avoiding the issue of NaN loss."
        },
        {
            "heading": "E DATASET DETAILS",
            "text": "Table 4 shows the statistics of all 13 datasets used in our experiments. The homophily score per dataset is computed based on the metric proposed by Lim et al. (2021) (higher score means more homophilic).\nTrain/Valid/Test splits. For Computer, Photo, CS, and Physics datasets, we adhere to the widely accepted practice of randomly dividing nodes into training (60%), validation (20%), and test (20%) sets (Chen et al., 2022b; Shirzad et al., 2023). As for the remaining datasets in our experiments, we\nuse the official splits provided in their respective papers (Mernyei & Cangea, 2020; Hu et al., 2020; Lim et al., 2021; Platonov et al., 2023)."
        },
        {
            "heading": "F HARDWARE INFORMATION",
            "text": "We conduct all experiments on a Linux machine equipped with an Intel Xeon Gold 5218 CPU (featuring 8 cores @ 2.30 GHz) and 4 RTX A6000 GPUs (each with 48 GB of memory). It is worth noting that Polynormer only requires 1 GPU for training, while the remaining GPUs are used to run baseline experiments in parallel."
        },
        {
            "heading": "G A VARIANT OF POLYNORMER",
            "text": "In the following, we provide the detailed architecture of the variant of Polynormer mentioned in Section 4.3, which is built upon the local-and-global attention scheme, i.e., the local and global attention layers of Polynormer are employed in parallel to update node features.\nX = (AV + \u03c3(Q)(\u03c3(KT )V ) \u03c3(Q) \u2211\ni \u03c3(K T i,:)\n)\u2299 (H + \u03c3(1\u03b2T )) (15)\nwhere matrices Q, K, V , H are obtained by linearly projecting the node feature matrix X from the previous layer. It is worth pointing out that this type of attention has been widely used by prior GT models (Rampasek et al., 2022; Wu et al., 2022; 2023; Kong et al., 2023). However, our ablation study in Section 4.3 indicates that the local-and-global attention performs worse than the local-to-global attention adopted by Polynormer."
        },
        {
            "heading": "H HYPERPARAMETERS SETTINGS",
            "text": "H.1 BASELINE MODELS\nFor the homophilic datasets listed in Table 1, we present the results of GCN (Kipf & Welling, 2016), GAT (Velic\u030ckovic\u0301 et al., 2017), APPNP (Gasteiger et al., 2018), GPRGNN (Chien et al., 2020), PPRGo (Bojchevski et al., 2020), NAGphormer (Chen et al., 2022b), and Exphormer (Shirzad et al., 2023), as reported in Chen et al. (2022b); Shirzad et al. (2023).\nFor the heterophilic datasets in Table 2, we provide the results of GCN, GraphSAGE (Hamilton et al., 2017), GAT-sep, H2GCN (Zhu et al., 2020), GPRGNN, FSGNN (Maurya et al., 2022), and GloGNN (Li et al., 2022), as reported in Platonov et al. (2023).\nIn the case of large-scale datasets listed in Table 3, we include the results of GCN, GAT, GPRGNN, LINKX (Lim et al., 2021), and GOAT (Kong et al., 2023), as reported in Hu et al. (2020); Lim et al. (2021); Zhang et al. (2022); Kong et al. (2023).\nFor baseline models without publicly available results on given datasets, we obtain their highest achievable accuracy through tuning critical hyperparameters as follows:\nGCNII (Chen et al., 2020). We set the hidden dimension to 512, the learning rate to 0.001, and the number of epochs to 2000. We perform hyperparameter tuning on the number of layers from {5, 10}, the dropout rate from {0.3, 0.5, 0.7}, \u03b1 from {0.3, 0.5, 0.7}, and \u03b8 from {0.5, 1.0}. GGCN (Yan et al., 2022). We set the hidden dimension to 512, the learning rate to 0.001, and the number of epochs to 2000. We perform hyperparameter tuning on the number of layers from {5, 10}, the dropout rate from {0.3, 0.5, 0.7}, the decay rate \u03b7 from {0.5, 1.0, 1.5}, and the exponent from {2, 3}. OrderedGNN (Song et al., 2023). We set the hidden dimension to 512, the learning rate to 0.001, and the number of epochs to 2000. We perform hyperparameter tuning on the number of layers from {5, 10}, the dropout rate from {0.3, 0.5, 0.7}, and the chunk size from {4, 16, 64}. tGCN (Hua et al., 2022). We set the hidden dimension to 512, the learning rate to 0.001, and the number of epochs to 2000. We perform hyperparameter tuning on the number of layers from {2, 3}, the dropout rate from {0.3, 0.5, 0.7}, and the rank from {256, 512}.\nG2-GNN (Rusch et al., 2022). We set the hidden dimension to 512, the learning rate to 0.001, and the number of epochs to 2000. We perform hyperparameter tuning on the number of layers from {5, 10}, the dropout rate from {0.3, 0.5, 0.7}, and the exponent p from {2, 3, 4}. DIR-GNN (Rossi et al., 2023). We set the hidden dimension to 512, the learning rate to 0.001, the number of epochs to 2000, and \u03b1 to 0.5. Besides, we choose GATConv with the type of jumping knowledge as \u201cmax\u201d. We perform hyperparameter tuning on the number of layers from {3, 5}, the dropout rate from {0.3, 0.5, 0.7}. GraphGPS (Rampasek et al., 2022). We choose GAT as the MPNN layer type and Performer as the global attention layer type. We set the number of layers to 2, the number of heads to 8, the hidden dimension to 64, and the number of epochs to 2000. We perform hyperparameter tuning on the learning rate from {1e\u22124, 5e\u22124, 1e\u22123}, and the dropout rate from {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}. NAGphormer (Chen et al., 2022b). We set the hidden dimension to 512, the learning rate to 0.001, the batch size to 2000, and the number of epochs to 500. We perform hyperparameter tuning on the number of layers from {1, 2, 3}, the number of heads from {1, 8}, the number of hops from {3, 7, 10}, and the dropout rate from {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}. Exphormer (Shirzad et al., 2023). We choose GAT as the local model and Exphormer as the global model. We set the number of epochs to 2000 and the number of heads to 8. We perform hyperparameter tuning on the learning rate from {1e\u2212 4, 1e\u2212 3}, the number of layers from {2, 4}, the hidden dimension form {64, 80, 96}, and the dropout rate from {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}. NodeFormer (Wu et al., 2022). We set the number of epochs to 2000. Additionally, we perform hyperparameter tuning on the learning rate from {1e \u2212 4, 1e \u2212 3, 1e \u2212 2}, the number of layers from {1, 2, 3}, the hidden dimension from {32, 64, 128}, the number of heads from {1, 4}, M from {30, 50}, K from {5, 10}, rb order from {1, 2}, the dropout rate from {0.0, 0.3}, and the temperature \u03c4 from {0.10, 0.15, 0.20, 0.25, 0.30, 0.40, 0.50}. DIFFormer (Wu et al., 2023). We use the \u201csimple\u201d kernel. Moreover, we perform hyperparameter tuning on the learning rate from {1e\u2212 4, 1e\u2212 3, 1e\u2212 2}, the number of epochs from {500, 2000}, the number of layers from {2, 3}, the hidden dimension form {64, 128}, the number of heads from {1, 8}, \u03b1 from {0.1, 0.2, 0.3}, and the dropout rate from {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}. GOAT (Kong et al., 2023). We set the \u201cconv type\u201d to \u201cfull\u201d, the number of layers to 1 (fixed by GOAT), the number of epochs to 200, the number of centroids to 4096, the hidden dimension to 256, the dropout of feed forward layers to 0.5, and the batch size to 1024. We perform hyperparameter tuning on the learning rate from {1e \u2212 4, 1e \u2212 3, 1e \u2212 2}, the global dimension from {128, 256}, and the attention dropout rate from {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.\nH.2 POLYNORMER\nLike the baseline models, we set the hidden dimension to 512 and the learning rate to 0.001. Additionally, we introduce a warm-up stage dedicated to training the local module. This step ensures that the node representations generated by the local module capture meaningful graph structural information before being passed to the global module. Moreover, we leverage 8 heads for our attention modules. For the ogbn-products and pokec datasets, we use minibatch training with a batch size of 250, 000 and 600, 000 respectively, while for the other datasets, we employ full batch training. In Table 5, we provide the critical hyperparameters of Polynormer used with each dataset. We refer readers to our source code for the detailed model implementation and hyperparameter settings."
        },
        {
            "heading": "I MODEL IMPLEMENTATION",
            "text": "# N: the number of nodes # M: the number of edges # D: the node feature dimension # L1: the number of local attention layers # L2: the number of global attention layers\n# x: input node feature matrix with shape [N, D] # edge_index: input graph structure with shape [2, M] # local_convs: local attention layers (e.g., GATConv from PyG) # local_betas: trainable weights with shape [L1, D] # global_convs: global attention layers (implemented in Code 2) # global_betas: trainable weights with shape [L2, D]\n# equivariant local attention module x_local = 0 for i, local_conv in enumerate(local_convs):\nh = h_lins[i](x) beta = F.sigmoid(local_betas[i]).unsqueeze(0) x = local_conv(x, edge_index) * (h + beta) x_local += x\n# equivariant global attention module x = x_local for i, global_conv in enumerate(global_convs):\ng = g_lins[i](x) beta = F.sigmoid(global_betas[i]).unsqueeze(0) x = global_conv(x) * (g + beta)\n# output linear layer out = pred_lin(x)\n# negative log-likelihood loss calculation y_pred = F.log_softmax(out, dim=1) loss = criterion(y_pred[train_idx], y_true[train_idx])\nCode 1: PyTorch-style Pseudocode for Polynormer\n# N: the number of nodes # M: the number of edges # D: the node feature dimension # H: the number of attention heads\n# x: node feature matrix reshaped to [N, D]\n# linear projections to get query, key, value matrices q = q_lins[i](x) # reshaped to [N, D/H, H] k = k_lins[i](x) # reshaped to [N, D/H, H] v = v_lins[i](x) # reshaped to [N, D/H, H]\n# map q and k to (0, 1) for kernel approximation q = F.sigmoid(q)\nk = F.sigmoid(k)\n# numerator kv = torch.einsum(\u2019nmh, ndh -> mdh\u2019, k, v) num = torch.einsum(\u2019nmh, mdh -> ndh\u2019, q, kv) # [N, D/H, H]\n# denominator k_sum = torch.einsum(\u2019ndh -> dh\u2019, k) den = torch.einsum(\u2019ndh, dh -> nh\u2019, q, k_sum).unsqueeze(1) # [N, 1, H]\n# linear global attention based on kernel trick x = num/den x = x.reshape(N, D) x = LayerNorm(x)\nCode 2: PyTorch-style Pseudocode for Linear Global Attention Layer"
        },
        {
            "heading": "J SCALABILITY ANALYSIS OF POLYNORMER",
            "text": "J.1 PROFILING RESULTS ON SYNTHETIC GRAPHS\nTo validate the linear complexity of Polynormer with respect to the number of nodes/edges in a graph, we generate synthetic Erdos-Renyi (ER) graphs with node count ranging from 1, 000 to 200, 000. We control the edge probability in the ER model to achieve an average node degree of around 5 and set the node feature dimension to 100. Figure 5 (left) clearly shows that the memory usage of Polynormer linearly increases with graph size, confirming its linear space complexity. As for the time complexity of Polynormer, we observe in Figure 5 (right) that the GPU training time remains nearly constant when increasing the graph size. We attribute it to that Polynormer is essentially a compute-bound model, and the GPU device (RTX A6000) used in our experiments can efficiently handle parallel computations on graphs with different sizes. This observation is further supported by the CPU training time results, which linearly increase with graph size, as the CPU is less powerful than the GPU for parallel computation.\nJ.2 PROFILING RESULTS ON LARGE REALISTIC GRAPHS\nTable 6 shows the profiling results of GAT, linear GTs, and Polynormer in terms of training time per epoch and memory usage on large realistic graphs. Note that we perform full batch training for all models whenever possible, since it avoids the nontrivial overhead associated with graph sampling in mini-batch training.\nThe results demonstrate that Polynormer consistently ranks among the top 3 fastest models, with relatively low memory usage. We attribute this efficiency advantage to the implementation of Polynormer that only involves common and highly-optimized built-in APIs from modern graph learning frameworks (e.g., PyG). In contrast, the GT baselines incorporate less common or expensive compute kernels (e.g., complicated kernel functions, Gumbel-Softmax, and nearest neighbor search),\nmaking them more challenging for graph learning frameworks to accelerate. Thus, we believe Polynormer is reasonably efficient and scalable in practice."
        },
        {
            "heading": "K WL EXPRESSIVITY OF POLYNORMER",
            "text": "While we have shown that Polynormer is polynomial-expressive, it is also interesting to see its expressivity under the Weisfeiler-Lehman (WL) hierarchy. To this end, we are going to firstly show that Polynormer is at least as expressive as 1-WL GNNs, and then introduce a simple way that renders Polynormer strictly more expressive than 1-WL GNNs.\nFor the sake of illustration, let us revisit Equation 4 in the following:\nX = (WX)\u2299 (X +B) (16)\nwhere W \u2208 Rn\u00d7n, B \u2208 Rn\u00d7d are trainable weight matrices, and X \u2208 Rn\u00d7d represents node features. Suppose all nodes have identical features (followed by the original formulation of WL algorithm), if we replace B = 1\u03b2T , the term X + 1\u03b2T is essentially a constant feature vector shared across nodes. As a result, the term (WX) \u2299 (X + 1\u03b2T ) is reduced to WX . By properly designing the weight matrix W (e.g. the adjacency matrix with self-loop), the term WX can be reduced to 1-WL GNNs. Hence, Polynormer based on Equation 16 is at least as expressive as 1-WL GNNs.\nTo make Polynormer strictly more expressive than 1-WL GNNs, let us focus on how to properly design the weight matrix B. Previously, we set B = 1\u03b2T . Note that the constant vector 1 \u2208 Rn here is essentially the eigenvector v1 that corresponds to the smallest (normalized) graph Laplacian eigenvalue. This motivates us to design B = v2\u03b2T , where v2 denotes the eigenvector corresponding to the second smallest (i.e., first non-trivial) eigenvalue of the normalized Laplacian. Consequently, Polynormer can be built upon the following Equation:\nX = (WX)\u2299 (X + v2\u03b2T ) (17)\nNotably, the vector v2 essentially encodes the node positional information from graph spectrum into polynomial coefficients B, which makes each output node feature unique in Equation 17 and thus allows Polynormer to distinguish non-isomorphic graphs that 1-WL GNNs fail to distinguish, as empirically confirmed in Figure 6. It is noteworthy that this approach is fundamentally different from prior PE methods based on Laplacian eigenvectors. Specifically, prior PE methods primarily focus on concatenating node positional encodings with node features. In contrast, we incorporate the positional information into the polynomial coefficients learned by Polynormer.\nTo distinguish the aforementioned two options of designing the weight matrix B, we denote the Polynormer of using the constant vector v1 and the second Laplacian vector v2 by Polynormer-v1 and Polynormer-v2, respectively. In the following, we empirically show that Polynormer-v2 is more powerful than 1-WL GNNs on distinguishing non-isomorphic graphs.\nExperimental setup. Without loss of generality, we focus on the local attention layer of Polynormer, and set \u03b2 as well as initial node features to be 1. As the attention scores between identical features\nare not meaningful, we replace the sparse attention matrix introduced in Section 3.2 with the random walk matrix A\u0302 = AD\u22121, where A and D denote the adjacency and degree matrices, respectively.\nExperimental results. As shown in Figure 6, Polynormer-v2 is able to distinguish non-isomorphic graphs such as circular skip link (CSL) graphs, which are known to be indistinguishable by the 1-WL test as well as 1-WL GNNs Sato (2020); Rampasek et al. (2022); Dwivedi et al. (2022).\nWhile Polynormer-v2 is strictly more powerful than the 1-WL algorithm, our empirically results in Table 7 indicate that it does not clearly outperform Polynormer-v1 on realistic node classification datasets. One possible reason is that each node possesses unique features in realistic datasets, which diminishes the importance of incorporating additional node positional information into Polynormer. Hence, we implement Polynormer based on Polynormer-v1 in practice to avoid computing the Laplacian eigenvector, and leave the improvement of Polynormer-v2 to our future work."
        },
        {
            "heading": "L CASE STUDY: AI MODEL RUNTIME PREDICTION ON TPU",
            "text": "Apart from comparing Polynormer against conventional GRL models on mainstream graph datasets, we further evaluate it on TpuGraphs (Phothilimthana et al., 2024), a large-scale runtime prediction dataset on tensor computation graphs.\nDataset details. Unlike prior datasets for program runtime prediction that are relatively small (up to 100 nodes), TpuGraphs is a runtime prediction dataset on full tensor programs, represented as\ncomputation graphs. Each graph within the dataset represents the primary computation of an ML program, typically encompassing one or more training steps or a single inference step. These graphs are obtained from open-source ML programs and include well-known models such as ResNet, EfficientNet, Mask R-CNN, and various Transformer models designed for diverse tasks like vision, natural language processing, speech, audio, recommender systems, and generative AI. Each data sample in the dataset comprises a computational graph, a compilation configuration, and the corresponding execution time when the graph is compiled with the specified configuration on a TPU v3, an accelerator tailored for ML workloads. The compilation configuration governs how the XLA (Accelerated Linear Algebra) compiler transforms the graph through specific optimization passes. The TpuGraphs dataset is composed of two main categories based on the compiler optimization level: (1) TpuGraphs-Layout (layout optimization) and (2) TpuGraphs-Tile (tiling optimization). Layout configurations dictate how tensors are organized in physical memory, determining the dimension order for each input and output of an operation node. Notably, TpuGraphs-Layout has 4 collections based on ML model type and compiler configuration as follows:\n\u2022 ML model type: \u2013 NLP: computation graphs of BERT models, \u2013 XLA: computation graphs of ML models from various domains such as vision, NLP, speech, audio, and recommendation. \u2022 Compiler configuration:\n\u2013 Default: configurations picked by XLA compiler\u2019s heuristic. \u2013 Random: randomly picked configurations.\nOn the other hand, tile configurations control the tile size of each fused subgraph. The TpuGraphsLayout comprises 31 million pairs of graphs and configurations, with an average of over 7, 700 nodes per graph. In contrast, the TpuGraphs-Tile includes 13 million pairs of kernels and configurations, averaging 40 nodes per kernel subgraph.\nL.1 RESULTS ON TPUGRAPHS-LAYOUT COLLECTIONS.\nAs shown in Table 8, we compare Polynormer against the GraphSAGE baseline provided by Google Hamilton et al. (2017). To ensure a fair comparison, we only change the model architecture while keeping all other configurations the same (i.e., no additional feature engineering). Experimental results show that Polynormer/Polynormer-local outperforms GraphSAGE on all collections. In particular, Polynormer achieves 10.3% accuracy improvement over GraphSAGE and 6.3% accuracy improvement over Polynormer-local on the XLA-Random collection, which is the most challenging collection since XLA consists of diverse graph structures from different domains and Random contains various compiler configurations. This showcases that the global attention of Polynormer effectively captures critical global structures in different graphs, owing to its high polynomial expressivity.\nL.2 RESULTS ON TPUGRAPHS (LAYOUT + TILE).\nWe further evaluate Polynormer on the whole TpuGraphs dataset that contains Layout and Tile optimizations. Table 9 shows that the local attention module alone in Polynormer is able to outperform the baseline model by 19.6%, indicating the efficacy of our polynomial-expressive architecture for\ncapturing critical local structures. Moreover, the accuracy of Polynormer gets further improved when adding the global attention module, leading to a 31.8% accuracy improvement over the baseline model. This confirms the effectiveness of the proposed local-to-global attention scheme."
        }
    ],
    "title": "POLYNORMER: POLYNOMIAL-EXPRESSIVE GRAPH TRANSFORMER IN LINEAR TIME",
    "year": 2024
}