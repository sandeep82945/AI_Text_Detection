{
    "abstractText": "We resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of \u00d5(|S||A|tmix\u03b5)* and a lower bound of \u03a9(|S||A|tmix\u03b5). In these expressions, |S| and |A| denote the cardinalities of the state and action spaces respectively, tmix serves as a uniform upper limit for the total variation mixing times, and \u03b5 signifies the error tolerance. Therefore, a notable gap of tmix still remains to be bridged. Our primary contribution is the development of an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5). This marks the first algorithm and analysis to reach the literature\u2019s lower bound. Our new algorithm draws inspiration from ideas in Li et al. (2020), Jin & Sidford (2021), and Wang et al. (2023). Additionally, we conduct numerical experiments to validate our theoretical findings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shengbo Wang"
        },
        {
            "affiliations": [],
            "name": "Jose Blanchet"
        },
        {
            "affiliations": [],
            "name": "Peter Glynn"
        }
    ],
    "id": "SP:7f37f1ecd85de3e1cef5ed883c955da802402041",
    "references": [
        {
            "authors": [
                "Alekh Agarwal",
                "Sham Kakade",
                "Lin F. Yang"
            ],
            "title": "Model-based reinforcement learning with a generative model is minimax optimal",
            "venue": "Proceedings of Thirty Third Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad Gheshlaghi Azar",
                "R\u00e9mi Munos",
                "Hilbert J. Kappen"
            ],
            "title": "Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model",
            "venue": "Mach. Learn.,",
            "year": 2013
        },
        {
            "authors": [
                "Maury Bramson"
            ],
            "title": "Stability of Queueing Networks: \u00c9cole d\u2019\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVI",
            "year": 2006
        },
        {
            "authors": [
                "Y. Deng",
                "F. Bao",
                "Y. Kong",
                "Z. Ren",
                "Q. Dai"
            ],
            "title": "Deep direct reinforcement learning for financial signal representation and trading",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Arie Hordijk",
                "Henk Tijms"
            ],
            "title": "A modified form of the iterative method of dynamic programming",
            "venue": "The Annals of Statistics,",
            "year": 1975
        },
        {
            "authors": [
                "Yujia Jin",
                "Aaron Sidford"
            ],
            "title": "Efficiently solving MDPs with stochastic mirror descent, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Yujia Jin",
                "Aaron Sidford"
            ],
            "title": "Towards tight bounds on the sample complexity of average-reward MDPs, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Koulik Khamaru",
                "Eric Xia",
                "Martin J. Wainwright",
                "Michael I. Jordan"
            ],
            "title": "Instance-optimality in optimal value estimation: Adaptivity via variance-reduced q-learning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jens Kober",
                "J Andrew Bagnell",
                "Jan Peters"
            ],
            "title": "Reinforcement learning in robotics: A survey",
            "venue": "The International Journal of Robotics Research,",
            "year": 2013
        },
        {
            "authors": [
                "Gen Li",
                "Yuting Wei",
                "Yuejie Chi",
                "Yuantao Gu",
                "Yuxin Chen"
            ],
            "title": "Breaking the sample size barrier in model-based reinforcement learning with a generative model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gen Li",
                "Changxiao Cai",
                "Yuxin Chen",
                "Yuantao Gu",
                "Yuting Wei",
                "Yuejie Chi"
            ],
            "title": "Is q-learning minimax optimal? A tight sample complexity analysis",
            "venue": "arXiv preprint arXiv:2102.06548,",
            "year": 2021
        },
        {
            "authors": [
                "Gen Li",
                "Laixi Shi",
                "Yuxin Chen",
                "Yuejie Chi",
                "Yuting Wei"
            ],
            "title": "Settling the sample complexity of model-based offline reinforcement learning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Sean Meyn",
                "Richard L. Tweedie"
            ],
            "title": "Markov Chains and Stochastic Stability",
            "venue": "doi: 10.1017/CBO9780511626630",
            "year": 2009
        },
        {
            "authors": [
                "M.L Puterman"
            ],
            "title": "Average Reward and Related Criteria, chapter 8, pp. 331\u2013440",
            "venue": "ISBN 9780470316887. doi: https://doi.org/10.1002/9780470316887.ch8",
            "year": 1994
        },
        {
            "authors": [
                "Fereshteh Sadeghi",
                "Sergey Levine"
            ],
            "title": "Cad2rl: Real single-image flight without a single real image",
            "venue": "arXiv preprint arXiv:1611.04201,",
            "year": 2016
        },
        {
            "authors": [
                "Aaron Sidford",
                "Mengdi Wang",
                "Xian Wu",
                "Lin Yang",
                "Yinyu Ye"
            ],
            "title": "Near-optimal time and sample complexities for solving Markov decision processes with a generative model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Martin J. Wainwright"
            ],
            "title": "Variance-reduced q-learning is minimax optimal, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Jinghan Wang",
                "Mengdi Wang",
                "Lin F. Yang"
            ],
            "title": "Near sample-optimal reduction-based policy learning for average reward MDP, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Mengdi Wang"
            ],
            "title": "Primal-dual \u03c0 learning: Sample complexity and sublinear run time for ergodic Markov decision problems, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Zihan Zhang",
                "Qiaomin Xie"
            ],
            "title": "Sharper model-free reinforcement learning for average-reward Markov decision processes, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Li"
            ],
            "title": "The following proposition in Li et al. (2020) proves the former requirement",
            "year": 2020
        },
        {
            "authors": [
                "B \u03b5"
            ],
            "title": "REDUCTION BOUND AND OPTIMAL SAMPLE COMPLEXITY FOR AMDP This this section, we prove Theorem 2 given the DMDP optimal sample complexity result in Theorem 1. To achieve this, we need the following lemma that allows us to compare the long run average value and the discounted value of the MDP",
            "venue": "From Lemma",
            "year": 2021
        },
        {
            "authors": [
                "Agarwal"
            ],
            "title": "2020) to decouple the statistical dependence of p\u0302s\u2032,a\u2032 and \u03c0\u03020 at a particular state action pair (s\u2032, a\u2032) \u2208 S \u00d7 A. To simplify notation, we use z = (s\u2032, a\u2032) \u2208 S \u00d7 A to denote a pair of state and action. Define the kernel K\u0302",
            "year": 2020
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2020) indicates that it is sufficient to set \u03be = (1 \u2212 \u03b3)\u03b7/4 where we recall that \u03b7 is the optimality gap parameter of the event \u03a9\u03b7 in (A.3). Lemma 2 (Li et al",
            "venue": "|N\u03be|",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "This paper offers a theoretical contribution to the area of reinforcement learning (RL) by providing the first provably optimal sample complexity guarantee for a tabular RL environment in which a controller wishes to maximize the long run average reward governed by a Markov decision process (MDP).\nThe landscape of RL has been illuminated by remarkable empirical achievements across a diverse spectrum of applications (Kober et al., 2013; Sadeghi & Levine, 2016; Deng et al., 2017). As a consequence, a great deal of research effort has been channeled into RL theory and its applications within operations research and the management sciences. In many real-world scenarios, influenced by engineering and managerial considerations, the MDP model naturally unfolds within an environment where viable policies must be stable (Bramson, 2008). In these settings, the controlled Markov chain induced by a reasonable policy will typically converge in distribution to a unique steady state, regardless of the initial condition. This phenomenon is known as mixing. Within such modeling environments, the long run average reward emerges as a well-defined and pertinent performance measure to maximize. Its relevance is particularly pronounced in scenarios where there is an absence of an inherent time horizon or discount factor. In situations where a system exhibits fast mixing, a finite-time observation of the state process can offer a good statistical representation of its long-term behavior. As a result, it becomes reasonable to anticipate that, for such systems, a lower-complexity algorithm for policy learning is attainable.\nRecognized as a significant and challenging open problem in RL theory, the optimal sample complexity for average reward MDPs (AMDPs) under a generative model, i.e., a sampler capable of generating new states for the controlled Markov chain conditioned on any state and action, has been\n*The O\u0303, \u2126\u0303, \u0398\u0303 hide log factors.\nextensively investigated in the literature (refer to Table 1). In this paper, our focus is on learning an optimal policy for a uniformly ergodic AMDP (Meyn & Tweedie, 2009; Wang et al., 2023), and we attain the first optimal sample complexity bound within this context. Specifically, assuming finite state and action spaces, uniform ergodicity implies that the transition matrix P\u03c0 induced by any stationary Markov deterministic policy \u03c0 has the property that Pm\u03c0 converges to a matrix with identical rows in \u21131 distance as m\u2192 \u221e at a geometric rate. The time constant of this geometric convergence is known as the mixing time. The largest mixing time over all \u03c0 is an important parameter denoted by tmix (see equation (2.2)). In this setting, we establish the following main result: Theorem 0 (Informal). Assuming that the AMDP is uniformly ergodic, the sample complexity of learning a policy that achieves a long run average reward within \u03f5 \u2208 (0, 1] of the optimal value with high probability is\n\u0398\u0303\n( |S||A|tmix\n\u03f52\n) ,\nwhere |S|, |A| are the cardinality of the state and action spaces, respectively.\nA rigorous version of Theorem 0 is presented in Theorem 2. We highlight that this is the first optimal result in the domain of sample complexity of AMDPs, as it achieves the lower bound in Jin & Sidford (2021) for uniformly ergodic AMDPs."
        },
        {
            "heading": "1.1 LITERATURE REVIEW",
            "text": "In this section, we review pertinent literature to motivate our methodology and draw comparisons with our contributions.\nSample Complexity of Average Reward RL: The relevant literature is summarized in Table 1. It\u2019s important to note that the works mentioned in this table revolve around the concept of mixing, albeit from distinct perspectives. On one side, Wang (2017); Jin & Sidford (2020; 2021), and the current paper assumes a uniformly ergodic AMDP. Conversely, Wang et al. (2022) and Zhang & Xie (2023) operate under a milder assumption, considering weakly communicating AMDPs as defined in Puterman (1994). Under this assumption, only the optimal average reward is guaranteed to remain independent of the initial state and is attained by a stationary Markov deterministic policy. In particular, certain policies in this setting might not lead to a mixing Markov chain, potentially rendering the policy\u2019s average reward state dependent. Consequently, the uniform mixing time upper bound tmix is infinite within such instances. In this context, a pertinent complexity metric parameter denoted as H is the span semi-norm (see (A.1)) upper bound of transient value functions H = max\u03c0\u0304 |u\u03c0\u0304|span, where the max is taken over all optimal policies and u\u03c0\u0304 is defined in (2.5)\u2021\u2021. As demonstrated in\n\u2021H is the maximum span of optimal transient value functions. See the literature review for a discussion. \u2020Let \u03b7\u03c0 be the stationary distribution of P\u03c0 , then \u03c4 := max\u03c0\u2208\u03a0 maxs\u2208S \u03b7\u03c0(s)/mins \u03b7\u03c0(s). *SMD stands for stochastic mirror descent while DMDP stands for discounted MDP.\n\u2021\u2021For periodic chains, the limit in (2.5) is understood as the Cesaro limit, see Puterman (1994).\nWang et al. (2022), it holds that H \u2264 8tmix when the reward is bounded in [0, 1]. It\u2019s important to note that H depends on the specific reward function, whereas tmix does not. Therefore, the reverse inequality cannot hold, even within the realm of uniformly ergodic MDPs. This is evident when we consider a reward function that is identically 0. However, it\u2019s worth mentioning that the family of worst-case uniformly ergodic MDP instances presented in Wang et al. (2023) exhibitsH = \u0398(tmix). See Section 5 for some more discussion.\nAdditionally, we highlight that two existing papers in the literature, namely Jin & Sidford (2021) and Wang et al. (2022), employ the reduction to a discounted MDP (DMDP) method to facilitate policy learning for AMDPs. This paper follows the same approach. In Section 1.2, we will offer an in-depth discussion of this methodology and a comparison with these two papers.\nSample Complexity of Discounted Tabular RL: In recent years, there has been substantial interest in understanding the worst-case sample complexity theory of tabular DMDPs. This research has given rise to two primary approaches: model-based and model-free. Model-based strategies involve constructing an empirical MDP model from data and employing dynamic programming (see Azar et al. (2013); Sidford et al. (2018); Agarwal et al. (2020); Li et al. (2022)), yielding optimal upper and lower bounds of \u0398\u0303(|S||A|(1\u2212\u03b3)\u22123\u03f5\u22122), where \u03b3 is the discount factor. Meanwhile, the model-free route maintains lower-dimensional statistics of the transition data, as exemplified by the iconic Qlearning (Watkins & Dayan, 1992) and its extensions. Li et al. (2021) demonstrates that the classic Q-learning algorithm has a worst-case sample complexity of \u0398\u0303(|S||A|(1 \u2212 \u03b3)\u22124\u03f5\u22122). However, Wainwright (2019) introduced a variance-reduced Q-learning variant that reaches the same lower bound as model-based methods.\nWorst-case analysis provides uniform guarantees for convergence rates across all \u03b3-discounted MDP instances. However, the worst-case examples that achieve the lower bound must have a transition kernel or reward function that depends on \u03b3 (Wang et al., 2023). In contrast, Khamaru et al. (2021) focuses on instance-dependent settings, where the transition kernel and reward function are fixed, and proves matching sample complexity upper and lower bounds. Wang et al. (2023) concentrates on scenarios in which the class of MDPs can encompass arbitrary reward functions, while the transition kernels are assumed to obey various mixing time upper bounds. A particular setting is when this mixing time upper bound is uniform across all policies. This specific setting aligns with the central objective of this paper: Derive optimal sample complexity theory for uniformly ergodic AMDPs."
        },
        {
            "heading": "1.2 ALGORITHM METHODOLOGY",
            "text": "Our approach to algorithmic design draws inspiration from Jin & Sidford (2021), wherein the optimal policy of a uniformly ergodic AMDP is approximated by that learned from a discounted MDP (DMDP) with a discount factor 1 \u2212 \u03b3 = \u0398(\u03f5/tmix). This idea of approximating the AMDP by a DMDP been considered and implemented since 1970s, see Hordijk & Tijms (1975). It is known as the reduction method in the AMDP sample complexity literature. As shown in Table 1, however,\n\u00a7The lower bound in Wang et al. (2023) assumes |S||A| = O(1).\nboth Jin & Sidford (2021) and Wang et al. (2022) employed this strategy and yet obtained an \u03f5\u22123 dependency, deviating from the canonical \u03f5\u22122 rate obtained in this paper.\nTo understand this deviation and motivate our methodology, we provide a brief discussion of the sample complexity theory for uniformly ergodic DMDPs. Prior to Wang et al. (2023), the DMDP algorithm, known as perturbed model-based planning, and the analysis in Li et al. (2020) achieved a sample complexity dependence on 1 \u2212 \u03b3 of O\u0303((1 \u2212 \u03b3)\u22123). Though optimal for the worst-case DMDP, this dependence on 1 \u2212 \u03b3 is sub-optimal for policy learning of uniformly ergodic DMDPs. On the other hand, the state-of-the-art algorithm and associated analysis in Wang et al. (2023) yield an optimal \u0398\u0303(tmix(1\u2212\u03b3)\u22122) dependence, as displayed in Table 2. The sub-optimal \u03f5\u22123 dependency observed in both Jin & Sidford (2021) and Wang et al. (2022) can be attributed to their utilization of the O\u0303((1\u2212 \u03b3)\u22123) result from Li et al. (2020), without taking into account the complexity reduction from the associated mixing assumptions.\nBuilding upon the aforementioned DMDP results, our strategy is rooted in recognizing that the optimal sample complexity of uniformly ergodic DMDPs is \u0398\u0303(|S||A|tmix(1\u2212\u03b3)\u22122\u03f5\u22122). As shown in Table 2, the algorithm presented in Wang et al. (2023) is capable of achieving this complexity. However, as indicated in the third column of Table 2, it necessitates a minimum sample size of \u2126\u0303(|S||A|(1\u2212\u03b3)\u22123). This quantity can be interpreted as the initial setup cost of the algorithm which is indifferent to the specification of \u03f5. Unfortunately, \u2126\u0303(|S||A|(1 \u2212 \u03b3)\u22123) proves to be excessively large for our objective. For a more comprehensive discussion on this issue, see Section 3.1.\nTo overcome this challenge, in this paper, we successfully establish an optimal sample complexity upper bound for the algorithm proposed in Li et al. (2020) in the setting of uniformly ergodic DMDPs. We achieve this while simultaneously maintaining a minimum sample size requirement of \u2126\u0303(|S||A|(1\u2212 \u03b3)\u22121). This optimal sample complexity bound for uniformly ergodic DMDPs, which builds upon and enhances the findings in Wang et al. (2023), is of independent theoretical significance. The formal statement is presented in Theorem 1, Section 3.1. This accomplishment is made possible by applying analytical techniques presented in Wang et al. (2023). In conjunction with the reduction methodology outlined in Jin & Sidford (2021), these developments collectively result in the AMDP Algorithm 2, which attains the optimal sample complexity as outlined in Theorem 0."
        },
        {
            "heading": "2 MARKOV DECISION PROCESSES: DEFINITIONS",
            "text": "We consider the setting of MDPs with finite state and action space S and A. Let P(S) denote the set of probability measures on S. An element p \u2208 P(S) can be seen as a row vector in R|S|. Let M(r, P, \u03b3) denote a discounted MDP (DMDP), where r : S \u00d7 A \u2192 [0, 1] is the reward function, P : S\u00d7A\u2192 P(S) is the transition kernel, and \u03b3 \u2208 (0, 1) is the discount factor. Note that P can be identified with a s, a indexed collection of measures {ps,a \u2208 P(S) : (s, a) \u2208 S \u00d7A}. We denote an average reward MDP (AMDP) with the same reward function and transition kernel by M\u0304(r, P ). Let H = (|S| \u00d7 |A|)Z\u22650 and H the product \u03c3-field form the underlying measureable space. Define the stochastic process {(Xt, At), t \u2265 0} by the point evaluation Xt(h) = st, At(h) = at for all t \u2265 0 for any h = (s0, a0, s1, a1, . . . ) \u2208 H. At each time and current state Xt, if action At is chosen, the decision maker will receive a reward r(Xt, At). Then, the law of the subsequent state satisfies L(Xt+1|X0, A0, . . . , Xt, At) = pXt,At(\u00b7) w.p.1. It is well known that to achieve optimal decision making in the context of infinite horizon AMDPs or DMDPs (to be introduced), it suffices to consider the policy class \u03a0 consisting of stationary, Markov, and deterministic policies; i.e. any \u03c0 \u2208 \u03a0 can be seen as a function \u03c0 : S \u2192 A. For \u03c0 \u2208 \u03a0 and initial distribution \u00b5 \u2208 P(S), there is a unique probability measure Q\u03c0\u00b5 on the product space s.t. the chain {(Xt, At), t \u2265 0} has finite dimensional distributions Q\u03c0\u00b5(X0 = s0, A0 = a0, . . . , At = at) = \u00b5(s0)ps0,\u03c0(s0)(s1) . . . pst\u22121,\u03c0(st\u22121)(st)1 {\u03c0(si) = ai,\u2200i} ,\nwhere 1 is the indicator function. Note that this also implies that {Xt, t \u2265 0} is a Markov chain under Q\u03c0\u00b5 with transition matrix P\u03c0 defined by\nP\u03c0(s, s \u2032) = ps,\u03c0(s)(s \u2032).\nAlso, we define r\u03c0 by r\u03c0(s) = r(s, \u03c0(s))."
        },
        {
            "heading": "2.1 DISCOUNTED MDPS",
            "text": "For \u03c0 \u2208 \u03a0, let E\u03c0\u00b5 denote the expectation under under Q\u03c0\u00b5. For \u00b5 with full support S, the discounted value function v\u03c0(s) of a DMDP is defined via\nv\u03c0(s) := E\u03c0\u00b5 [ \u221e\u2211 t=0 \u03b3tr(Xt, At) \u2223\u2223\u2223\u2223\u2223X0 = s ] .\nIt can be seen as a vector v\u03c0 \u2208 R|S|, and can be computed using the formula v\u03c0 = (I \u2212 \u03b3P\u03c0)\u22121r\u03c0. The optimal discounted value function is defined as\nv\u2217(s) := max \u03c0\u2208\u03a0 v\u03c0(s), s \u2208 S.\nFor probability measure p on S, let p[v] denote the sum \u2211\ns\u2208S p(s)v(s). It is well known that v \u2217 is\nthe unique solution of the following Bellman equation:\nv\u2217(s) = max a\u2208A (r(s, a) + \u03b3ps,a[v \u2217]) . (2.1)\nMoreover, \u03c0\u2217(s) \u2208 argmaxa\u2208A (r(s, a) + \u03b3ps,a[v\u2217]) is optimal and hence v\u2217 = (I\u2212\u03b3P\u03c0\u2217)\u22121r\u03c0\u2217 ."
        },
        {
            "heading": "2.2 AVERAGE REWARD MDP",
            "text": "As explained in the introduction, in this paper, we assume that the MDP of interest is uniformly ergodic. That is, for all \u03c0 \u2208 \u03a0, P\u03c0 is uniformly ergodic. By Wang et al. (2023), this is equivalent to the setting in (Wang, 2017; Jin & Sidford, 2021) where the authors assume\ntmix := max \u03c0\u2208\u03a0 inf\n{ m \u2265 1 : max\ns\u2208S \u2225Pm\u03c0 (s, \u00b7)\u2212 \u03b7\u03c0(\u00b7)\u22251 \u2264\n1\n2\n} <\u221e. (2.2)\nHere \u03b7\u03c0(\u00b7) is the stationary distribution of P\u03c0 and \u2225\u00b7\u22251 is \u21131 distance between two probability vectors. Further, this is also shown in Wang et al. (2023) to be equivalent to the following assumption: Assumption 1 (Uniformly Ergodic MDP). For any \u03c0 \u2208 \u03a0, there exists m \u2265 1, q \u2264 1 and probability measure \u03c8 \u2208 P(S) such that for all s \u2208 S, Pm\u03c0 (s, \u00b7) \u2265 q\u03c8(\u00b7).\nHere, the notation Pm\u03c0 denotes themth power of the matrix P\u03c0 . Assumption 1 is commonly referred to, in the general state-space Markov chain literature, as P\u03c0 satisfying the Doeblin condition (Meyn & Tweedie, 2009). In the context of Assumption 1, we define the minorization time as follows. Definition 1 (Minorization Time). Define the minorization time for a uniformly ergodic kernel P\u03c0 as\ntminorize(P\u03c0) := inf\n{ m/q : inf\ns\u2208S Pm\u03c0 (s, \u00b7) \u2265 q\u03c8(\u00b7) for some \u03c8 \u2208 P(S)\n} .\nDefine the minorization time for the uniformly ergodic MDP as\ntminorize := max \u03c0\u2208\u03a0 tminorize(P\u03c0).\nSince \u03a0 is finite, the above max is always achieved, and hence tminorize < \u221e. Moreover, Theorem 1 of Wang et al. (2023) shows that tminorize and tmix are equivalent up-to constant factors:\ntminorize \u2264 22tmix \u2264 22 log(16)tminorize. (2.3) Therefore, the subsequent complexity dependence written in terms of tminorize can be equivalently expressed using tmix.\nUnder Assumption 1, for any initial distribution X0 \u223c \u00b5, the long run average reward of any policy \u03c0 \u2208 \u03a0 is defined as\n\u03b1\u03c0 := lim T\u2192\u221e\n1 T E\u03c0\u00b5 [ T\u22121\u2211 t=0 r(Xt, At) ] where the limit always exists and doesn\u2019t depend on \u00b5. The long run average reward \u03b1\u03c0 can be characterized via any solution pair (u, \u03b1), u : S \u2192 R and \u03b1 \u2208 R to the Poisson\u2019s equation,\nr\u03c0 \u2212 \u03b1 = (I \u2212 P\u03c0)u. (2.4)\nUnder Assumption 1, a solution pair (u, \u03b1) always exists and is unique up to a shift in u; i.e. {(u+ ce, \u03b1) : c \u2208 R}, where e(s) = 1,\u2200s \u2208 S, are all the solution pairs to (2.4). In particular, for any \u03c0 \u2208 \u03a0, define the transient value function a.k.a. the bias as\nu\u03c0(s) := lim T\u2192\u221e E\u03c0s [ T\u22121\u2211 t=0 (r\u03c0(Xt)\u2212 \u03b1\u03c0) ] (2.5)\nwhere the limit always exists. Then, (u\u03c0, \u03b1\u03c0) is the unique up to a shift solition to (2.4).\nSimilar to DMDPs, define the optimal long run average reward \u03b1\u0304 as\n\u03b1\u0304 := max \u03c0\u2208\u03a0\n\u03b1\u03c0.\nThen, for any \u03c0\u0304 that achieve the above maximum, (u\u03c0\u0304, \u03b1\u0304) solves r\u03c0\u0304 \u2212 \u03b1\u0304 = (I \u2212 P\u03c0\u0304)u\u03c0\u0304."
        },
        {
            "heading": "3 OPTIMAL SAMPLE COMPLEXITIES UNDER A GENERATIVE MODEL",
            "text": "In this section, we aim to develop an algorithm for AMDPs that achieves the sample complexity as presented in Theorem 0. The randomness used by the algorithm arises from an underlying probability space (\u2126,F , P ). We proceed under the assumption that we have access to a generative model, or sampler, which allows us to independently draw samples of the subsequent state from the transition probability {ps,a(s\u2032) : s\u2032 \u2208 S} given any state action pair (s, a). As explained in Section 1.2, our methodology closely aligns with the approach introduced in Jin & Sidford (2021). We leverage the policy acquired from a suitably defined DMDP as an approximation to the optimal policy for the AMDP. However, prior to this work, the state-of-the-art DMDP algorithms, along with the accompanying worst-case sample complexity analysis, fall short in achieving the optimal sample complexity articulated in Theorem 0 (Jin & Sidford, 2021; Wang et al., 2022). This limitation emanates from the fact that the sample complexity of uniformly ergodic DMDPs has a dependence (1 \u2212 \u03b3)\u22122, as demonstrated in the preceding research Wang et al. (2023). This is a notable improvement over the (1\u2212\u03b3)\u22123 dependence associated with the worst-case-optimal DMDP theory (Li et al., 2020), a foundation upon which Jin & Sidford (2021) and Wang et al. (2022) were constructed. Consequently, to attain the desired complexity result, our initial step involves establishing enhanced sample complexity assurances for uniformly ergodic DMDPs."
        },
        {
            "heading": "3.1 A SAMPLE EFFICIENT ALGORITHM FOR UNIFORMLY ERGODIC DMDPS",
            "text": "We recognize that the optimal sample complexity of uniformly ergodic DMDPs should be \u0398\u0303(|S||A|tminorize(1 \u2212 \u03b3)\u22122\u03f5\u22122); c.f. (Wang et al., 2023). Regrettably, as mentioned earlier, the algorithm introduced in Wang et al. (2023), while indeed capable of achieving this sample complexity, falls short in terms of the minimum sample size as it requires n = \u2126\u0303(|S||A|(1 \u2212 \u03b3)\u22123) for its execution, a complexity that is too large for our purposes.\nTo elaborate on this issue, we note that the algorithm introduced in Wang et al. (2023) constitutes a variant of the variance-reduced Q-learning (Wainwright, 2019). This family of algorithms necessitates a minimum sample size of n = \u2126\u0303(|S||A|(1 \u2212 \u03b3)\u22123), as per existing knowledge. Remarkably, model-based algorithms can achieve significantly smaller minimal sample sizes: e.g. n = \u2126\u0303(|S||A|(1\u2212\u03b3)\u22122) in Agarwal et al. (2020), and the full-coverage n = \u2126\u0303(|S||A|(1\u2212\u03b3)\u22121) in Li et al. (2020). See Table 2. This comparison motivates our adoption of the algorithm introduced in Li et al. (2020) and to draw upon the techniques utilized for analyzing mixing MDPs from Wang et al. (2023). The synergistic combination of these ideas, as elucidated in Theorem 1, results in an algorithm with the optimal sample complexity of \u0398\u0303(|S||A|tminorize(1 \u2212 \u03b3)\u22122\u03f5\u22122) and minimum sample size n = \u2126\u0303(|S||A|(1\u2212 \u03b3)\u22121) at the same time. In this section, our analysis suppose Assumption 1, tminorize \u2264 (1\u2212 \u03b3)\u22121, and \u03b3 \u2265 12 ."
        },
        {
            "heading": "3.1.1 THE DMDP ALGORITHM AND ITS SAMPLE COMPLEXITY",
            "text": "We introduce the perturbed model-based planning algorithm for DMDPs in Li et al. (2020). Let \u03b6 > 0 be a design parameter that we will specify later. Consider a random perturbation of the\nreward function R(s, a) := r(s, a) + Z(s, a), (s, a) \u2208 S \u00d7A\nwhere the random element Z : S \u00d7A\u2192 [0, \u03b6] Z(s, a) \u223c Unif(0, \u03b6) (3.1)\ni.i.d. \u2200(s, a) \u2208 S \u00d7 A. The reason to consider a perturbed reward with amplitude parameter \u03b6 is to ensure that the optimality gap of the optimal policy, compared with any other suboptimal policy, is sufficiently large. To accomplish this, it is not necessary to assume uniform distributions of Z(s, a). However, we opt for this choice for the sake of convenience. This reward perturbation technique is well motivated in Li et al. (2020). So, we refer the readers to this paper for a detailed exposition. Then, the perturbed model-based planning algorithm therein is formulated in Algorithm 1.\nAlgorithm 1 Perturbed Model-based Planning (Li et al., 2020): PMBP(\u03b3, \u03b6, n)\nInput: Discount factor \u03b3 \u2208 (0, 1). Perturbation amplitude \u03b6 > 0. Sample size n \u2265 1. Sample Z as in (3.1) and compute R = r + Z. Sample i.i.d. S(1)s,a, S (2) s,a, . . . , S (n) s,a , for each (s, a) \u2208 S \u00d7 A. Then, compute the empirical kernel P\u0302 := {p\u0302s,a(s\u2032) : (s, a) \u2208 S \u00d7A, s\u2032 \u2208 S} where\np\u0302s,a(s \u2032) :=\n1\nn n\u2211 i=1 1 { S(i)s,a = s \u2032 } , (s, a) \u2208 S \u00d7A.\nCompute the solution v\u03020 to the empirical version of the Bellman equation (2.1); i.e. \u2200s \u2208 S, v\u03020(s) = maxa\u2208A (R(s, a) + \u03b3p\u0302s,a[v\u03020]). Then, extract the greedy policy\n\u03c0\u03020(s) \u2208 arg max a\u2208A (R(s, a) + \u03b3p\u0302s,a[v\u03020]) , s \u2208 S.\nreturn \u03c0\u03020.\nBy (2.1), \u03c0\u03020 returned by Algorithm 1 optimal for the DMDP instance M(R, P\u0302 , \u03b3). Also, computing v\u03020 therein involves solving a fixed point equation in R|S|. This can be done (with machine precision) by performing value or policy iteration using no additional samples.\nIn summary, Li et al. (2020) proposed to use \u03c0\u03020, the optimal policy of the reward-perturbed empirical DMDP M(R, P\u0302 , \u03b3), as the estimator to the optimal policy \u03c0\u2217 of the DMDP M(r, P, \u03b3). They proved that this procedure achieves a sample complexity upper bound of \u0398\u0303(|S||A|(1 \u2212 \u03b3)\u22123\u03f5\u22122) over all DMDPs (not necessarily uniformly ergodic ones) with a minimum sample size requirement n = \u2126\u0303(|S||A|(1 \u2212 \u03b3)\u22121). We are able to improve the analysis and achieve an accelerated result of independent interest in the context of uniformly ergodic DMDPs. Before introducing our results, let us define two parameters. Recall \u03b6 from (3.1). For prescribed error probability \u03b4 \u2208 (0, 1), define\n\u03b2\u03b4(\u03b7) = 2 log\n( 24|S||A| log2((1\u2212 \u03b3)\u22121)\n(1\u2212 \u03b3)2\u03b7\u03b4\n) and \u03b7\u2217\u03b4 =\n\u03b6\u03b4(1\u2212 \u03b3) 9|S||A|2 .\nThe reason for defining \u03b2\u03b4(\u00b7) and \u03b7\u2217\u03b4 will become clear when we introduce the intermediate results in Proposition A.1 and A.2 in the Appendix. Now, we are ready to state improved error and sample complexity bounds for Algorithm 1 that achieve minmax optimality for uniformly ergodic DMDPs: Theorem 1. Suppose Assumption 1 is in force. Then, for any \u03b3 \u2208 [1/2, 1), \u03b6 > 0, and n \u2265 64\u03b2\u03b4(\u03b7 \u2217 \u03b4 )(1\u2212 \u03b3)\u22121, the policy \u03c0\u03020 returned by Algorithm 1 PMBP(\u03b3, \u03b6, n) satisfies\n0 \u2264 v\u2217 \u2212 v\u03c0\u03020 \u2264 2\u03b6 1\u2212 \u03b3 + 486\n\u221a \u03b2\u03b4(\u03b7\u2217\u03b4 )tminorize\n(1\u2212 \u03b3)2n (3.2)\nw.p. at least 1\u2212 \u03b4. Consequently, choose \u03b6 = (1\u2212\u03b3)\u03f5/4, the sample complexity to achieve an error 0 < \u03f5 \u2264 \u221a tminorize/(1\u2212 \u03b3) w.p. at least 1\u2212 \u03b4 is\nO\u0303 ( |S||A|tminorize (1\u2212 \u03b3)2\u03f52 ) .\nThe proof of Theorem 1 is deferred to Appendix A. Remark. Compare to the worst case result in, e.g., Azar et al. (2013) and Li et al. (2020), Theorem 1 replaces a power of (1\u2212 \u03b3)\u22121 by tminorize in (3.2). The implied sample complexity upper bound matches the lower bound in Wang et al. (2023) up to log factors. So, O\u0303 can be replaced by \u0398\u0303. Also, this is achieved with a minimum sample size n = 64\u03b2\u03b4(\u03b7\u2217\u03b4 )(1 \u2212 \u03b3)\u22121. This and the mixing time equivalence (2.3) confirms the sample complexity claim in Table 2."
        },
        {
            "heading": "3.2 AN OPTIMAL SAMPLE COMPLEXITY UPPER BOUND FOR AMDPS",
            "text": "Using the sample complexity upper bound in Theorem 1, we then adapt the reduction procedure considered in Jin & Sidford (2021). This leads to an algorithm that learns an \u03f5-optimal policy for any AMDP satisfying Assumption 1 with the optimal sample complexity. Concretely, we run the reduction and perturbed model-based planning Algorithm 2.\nAlgorithm 2 Reduction and Perturbed Model-based Planning Input: Error tolerance \u03f5 \u2208 (0, 1]. Assign\n\u03b3 = 1\u2212 \u03f5 19tminorize , \u03b6 = 1 4 (1\u2212 \u03b3)tminorize, and n =\nc\u03b2\u03b4(\u03b7 \u2217 \u03b4 )\n(1\u2212 \u03b3)2tminorize\nwhere c = 4 \u00b7 4862. Run Algorithm 1 with parameter specification PMBP(\u03b3, \u03b6, n) and obtain output \u03c0\u03020. return \u03c0\u03020.\nThis algorithm has the following optimal sample complexity guarantee: Theorem 2. Suppose Assumption 1 is in force. The policy \u03c0\u03020 output by Algorithm 2 satisfies 0 \u2264 \u03b1\u0304\u2212 \u03b1\u03c0\u03020 \u2264 \u03f5 w.p. at least 1\u2212 \u03b4. Moreover, the total number of samples used is\nO\u0303\n( |S||A|tminorize\n\u03f52\n) .\nThe proof of Theorem 2 is deferred to Appendix B. Remark. This achieves the minmax lower bound in Jin & Sidford (2021) up to log factors. So, O\u0303 can be replaced by \u0398\u0303. This and the equivalence relationship (2.3) between tminorize and tmix is a formal statement of Theorem 0."
        },
        {
            "heading": "4 NUMERICAL EXPERIMENTS",
            "text": "In this section, we conduct two numerical experiments to verify our algorithm\u2019s optimal sample complexity dependence on \u03f5 and tminorize. The family of reward functions and transition kernels used for both experiments belongs to the family of hard instances constructed in Wang et al. (2023). By the same reduction argument, this family of AMDPs has sample complexity \u2126(tminorize\u03f5\u22122).\nFirst, we verify the \u03f5 dependence of our algorithm. To achieve this, we study the error of estimating the true average reward \u03b1\u0304 = 0.5 as the sample size increases. The experiment runs 300 replications of Algorithm 2 under different sample sizes and constructs the red data points and regression line in Figure 1a. We also implement the algorithm proposed in Jin & Sidford (2021), conduct the same experiment, and produce the data in blue. Our algorithm outperforms the prior work. Moreover, in log-log scale, the red regression line has a slope that is close to \u22121/2, indicating a n\u22121/2 convergence rate. This verifies the optimal O\u0303(\u03f5\u22122) dependence of our Algorithm 2. On the other hand, the blue line has a slope near \u22121/3, indicating a suboptimal O\u0303(\u03f5\u22123) for the algorithm in Jin & Sidford (2021). This significant improvement is due to our optimal implementation and analysis of the DMDP algorithm in Li et al. (2020), reducing sample complexity dependence on (1\u2212 \u03b3)\u22121. Next, we verify our algorithm\u2019s sample complexity dependence on tminorize. Fix a \u03f5 > 0 and recall that 1 \u2212 \u03b3 = \u0398(\u03f5/tminorize) = \u0398(tminorize\u22121). So, the optimal sample size for Algorithm\n(a) Convergence rate comparison with Jin & Sidford (2021). A \u22120.5 slope verifies the O\u0303(\u03f5\u22122) dependence. (b) Verification of tminorize dependence. A 0 slope indicates the O\u0303(tminorize) dependence.\nFigure 1: Numerical experiments using the hard MDP instance in Wang et al. (2023).\n2 is n = \u0398\u0303((1 \u2212 \u03b3)\u22122tminorize\u22121) = \u0398\u0303 (tminorize) which is linear in tminorize. Thus, applying Algorithm 2 to the hard instance with minorization time tminorize using sample size n = Ctminorize for some large constant C, our sample complexity upper bound suggests that the estimation error of \u03b1\u0304 should be O\u0303(1). Consequently, plotting the average error against the number of samples used or, equivalently, tminorize on a log-log scale while varying tminorize should yield a line with a near 0 (or possibly a negative) slope, according to our theory. The experiments in Figure 1b use C = 4500 for the purple line and C = 18000 for the blue line. With tminorize varying within the range [10, 1000], both regression lines exhibit a near 0 slope, indicating a O\u0303(tminorize) dependence.\nTherefore, through the numerical experiments presented in Figure 1, we separately verify the optimality of our algorithm\u2019s sample complexity dependence on both \u03f5 and tminorize."
        },
        {
            "heading": "5 CONCLUDING REMARKS",
            "text": "We now discuss certain limitations intrinsic to our proposed methodology as well as potential avenues for future research. Firstly, the use of the DMDP approximation approach necessitates a priori knowledge of an upper bound on the uniform mixing time for the transition kernel P in order to initiate the algorithm. In practical applications, such an upper bound can be challenging to obtain, or in certain instances, result in excessively pessimistic estimates. One could circumvent this by using an additional logarithmic factor of samples, thus allowing for the algorithm to operate with geometrically larger values of tmix, and terminate after a suitable number of iterations. Nevertheless, this approach still hinges upon the knowledge of the mixing time to achieve optimal termination.\nSecondly, we assume a strong form of MDP mixing known as uniform ergodicity. While theoretically optimal, this notion of mixing yields conservative upper bounds on sample complexity in situations wherein suboptimal policies induce Markov chains with especially large mixing times. It is our contention, supported by the findings presented in references such as Wang et al. (2022) and Wang et al. (2023), that the sample complexity should be contingent solely upon the properties of the optimal policies. Moreover, the complexity measure parameter H that is used in Wang et al. (2022); Zhang & Xie (2023) has several advantages over tmix that lie beyond the ability to generalize to weakly communicating MDPs. In particular, for periodic chains and special situations where the optimal average reward remains state-independent despite the presence of multiple recurrent classes of the transition kernel induced by an optimal policy, the parameter H is well-defined but tmix is not (Puterman, 1994). Consequently, we are now dedicating research effort to the development of an algorithm and analysis capable of achieving a sample complexity of O\u0303(|S||A|H\u03f5\u22122). Lastly, as the assumption of uniform ergodicity extends beyond finite state space MDPs, we aspire to venture into the realm of general state-space MDPs. Our objective is to extrapolate the principles underpinning our methodology to obtain sample complexity results for general state space MDPs."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The material in this paper is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF 1915967, 2118199, 2229012, 2312204."
        },
        {
            "heading": "A STATISTICAL PROPERTIES OF THE ESTIMATORS OF UNIFORMLY ERGODIC",
            "text": "DMDPS\nIn this section, our objective is to establish concentration properties pertaining to the estimators of the value function and optimal policies. Before discussing these statistical properties, we introduce some notations and auxiliary quantities that facilitate our analysis.\nUnder Assumption 1, it is useful to consider the span semi-norm (Puterman, 1994). For vector v \u2208 V = Rd, let 1 be the vector with all entries equal to one and define\n|v|span := infc\u2208R \u2225v \u2212 c1\u2225\u221e\n= max 1\u2264i\u2264d vi \u2212 min 1\u2264i\u2264d\nvi. (A.1)\nNote that the span semi-norm satisfies the triangle inequality |v + w|span \u2264 |v|span + |w|span.\nThe analysis in this section will make use of the following standard deviation parameters: define \u03c3(v)(s, a) := \u221a ps,a[v2]\u2212 ps,a[v]2, and \u03c3\u03c0(v)(s) := \u03c3(v)(s, \u03c0(s)). (A.2)\nLet \u03c00 be an optimal policy associated with MDPs M(\u03b3, P,R). Recall that \u03c0\u03020 is optimal for M(\u03b3, P\u0302 , R). Define for any \u03c0 \u2208 \u03a0, v\u03c00 := (I \u2212 \u03b3P\u03c0)\u22121R\u03c0 and v\u0302\u03c00 := (I \u2212 \u03b3P\u0302\u03c0)\u22121R\u03c0 . Also, let q\u03020 = (I \u2212 \u03b3P\u0302 \u03c0\u03020)\u22121R\u03c0\u03020 . Consider the following event\n\u2126\u03b7 := { inf s\u2208S ( v\u0302\u03c0\u030200 (s)\u2212 max b \u0338=\u03c0\u03020(s) q\u03020(s, b) ) \u2265 \u03b7 } . (A.3)\nThis is a set of good events on which the optimality gap of MDP M(\u03b3, P\u0302 , R) is larger than \u03b7. Recall the definition of the reward perturbation Z in (3.1) and perturbed reward R. For any \u03c0 \u2208 \u03a0, let R\u03c0(s) = R(s, \u03c0(s)) and Z\u03c0(s) = Z(s, \u03c0(s)) for all s \u2208 S. To achieve the desired sample efficiency in terms of the minimum sample size, Li et al. (2020) recursively defines the auxiliary values:\nh\u03c00 = R\u03c0; v \u03c0 0 = (I \u2212 \u03b3P\u03c0)\u22121h\u03c00 ; v\u0302\u03c00 = (I \u2212 \u03b3P\u0302\u03c0)\u22121h\u03c00\nh\u03c0l = \u03c3\u03c0(v \u03c0 l\u22121); v \u03c0 l = (I \u2212 \u03b3P\u03c0)\u22121h\u03c0l ; v\u0302\u03c0l = (I \u2212 \u03b3P\u0302\u03c0)\u22121h\u03c0l\n(A.4)\nfor all l \u2265 1. Using these sequences and the techniques in Wang et al. (2023), we are able to show the following concentration bound: Proposition A.1. Assume Assumption 1 and for some \u03b7 \u2264 1, P (\u2126\u03b7) \u2265 1 \u2212 \u03b4/3. For n \u2265 64\u03b2\u03b4(\u03b7)(1\u2212 \u03b3)\u22121, then w.p. at least 1\u2212 \u03b4 under P\u2225\u2225\u2225v\u0302\u03c0\u030200 \u2212 v\u03c0\u030200 \u2225\u2225\u2225\u221e \u2264 243 \u221a \u03b2\u03b4(\u03b7)tminorize (1\u2212 \u03b3)2n , and v\u03c000 \u2212 v \u03c0\u03020 0 \u2264 486 \u221a \u03b2\u03b4(\u03b7)tminorize (1\u2212 \u03b3)2n\nwhere\n\u03b2\u03b4(\u03b7) = 2 log\n( 24|S||A| log2((1\u2212 \u03b3)\u22121)\n(1\u2212 \u03b3)2\u03b7\u03b4\n) .\nThe proof of Proposition A.1 is provided in section C.2. We see that the conclusion of A.1 will resemble the statement of Theorem 1 if P (\u2126\u03b7) \u2265 1 \u2212 \u03b4/3 as well as v\u03c000 \u2248 v\u2217 and v \u03c0\u03020 0 \u2248 v\u03c0\u03020 are sufficiently close. The following proposition in Li et al. (2020) proves the former requirement. Proposition A.2 (Lemma 6, Li et al. (2020)). Let\n\u03b7\u2217\u03b4 = \u03b6\u03b4(1\u2212 \u03b3) 9|S||A|2 ,\nthen P (\u2126\u03b7\u2217\u03b4 ) \u2265 1\u2212 \u03b4/3.\nA.1 PROOF OF THEOREM 1\nNow, we are ready to present of proof of Theorem 1 given Propositions A.1 and A.2.\nProof. Observe that for any \u03c0 \u2208 \u03a0, by the construction in (A.4),\nv\u03c00 = (I \u2212 \u03b3P\u03c0)\u22121R\u03c0 = v\u03c0 + (I \u2212 \u03b3P\u03c0)\u22121Z\u03c0.\nTherefore,\n\u2225v\u03c00 \u2212 v\u03c0\u2225\u221e \u2264 \u03b6 \u2225\u2225(I \u2212 \u03b3P\u03c0)\u22121\u2225\u2225\u221e,\u221e \u2264 \u03b61\u2212 \u03b3 .\nConsider\nv\u2217 \u2212 v\u03c0\u03020 = (v\u2217 \u2212 v\u03c0 \u2217 0 ) + (v \u03c0\u2217 0 \u2212 v \u03c00 0 ) + (v \u03c00 0 \u2212 v \u03c0\u03020 0 ) + (v \u03c0\u03020 0 \u2212 v\u03c0\u03020)\n\u2264 2\u03b6 1\u2212 \u03b3 + v\u03c000 \u2212 v \u03c0\u03020 0\nwhere the optimality of \u03c00 implies that v\u03c0 \u2217 0 \u2212 v \u03c00 0 \u2265 0. By Proposition A.1 and A.2, w.p. at least 1\u2212 \u03b4\nv\u03c000 \u2212 v \u03c0\u03020 0 \u2264 486\n\u221a \u03b2\u03b4(\u03b7\u2217\u03b4 )tminorize\n(1\u2212 \u03b3)2n .\nprovided that n \u2265 64\u03b2\u03b4(\u03b7\u2217\u03b4 )(1\u2212 \u03b3)\u22121. To arrive at the sample complexity bound, we first note that \u03b2\u03b4(\u03b7\u2217\u03b4 ) has log dependence on \u03b6. Choose \u03b6 = (1\u2212 \u03b3)\u03f5/4 and for c = 4 \u00b7 4862,\nn = ctminorize\u03b2\u03b4(\u03b7\n\u2217 \u03b4 )\n(1\u2212 \u03b3)2\u03f52 = O\u0303 ( tminorize (1\u2212 \u03b3)2\u03f52 ) .\nThen for \u03f5 \u2264 \u221a tminorize(1\u2212 \u03b3)\u22121, n \u2265 64\u03b2\u03b4(\u03b7\u2217\u03b4 )(1\u2212 \u03b3)\u22121 and w.p. at least 1\u2212 \u03b4\nv\u2217 \u2212 v\u03c0\u03020 \u2264 \u03f5 2 + 486\u03f5\u221a c = \u03f5."
        },
        {
            "heading": "B REDUCTION BOUND AND OPTIMAL SAMPLE COMPLEXITY FOR AMDP",
            "text": "This this section, we prove Theorem 2 given the DMDP optimal sample complexity result in Theorem 1. To achieve this, we need the following lemma that allows us to compare the long run average value and the discounted value of the MDP. From Lemma 3 Jin & Sidford (2021) and Theorem 1 of Wang et al. (2023) (c.f. equation (2.3)), we have that\nLemma 1. Under Assumption 1, for all \u03c0 \u2208 \u03a0,\n\u2225(1\u2212 \u03b3)v\u03c0 \u2212 \u03b1\u03c0\u2225\u221e \u2264 9(1\u2212 \u03b3)tminorize.\nB.1 PROOF OF THEOREM 2\nGiven Lemma 1 and Theorem 1, we present the proof of Theorem 2.\nProof of Theorem 2. First, note that (1\u2212 \u03b3)\u22121 \u2265 tminorize \u2265 1. By Theorem 1 and the choice of \u03b6 and n, the policy \u03c0\u03020 satisfies v\u2217 \u2212 v\u03c0\u03020 \u2264 tminorize w.p. at least 1\u2212 \u03b4.\nLet \u03c0\u0304 denote an optimal policy of the average reward problem. Then, w.p. at least 1\u2212 \u03b4,\n\u03b1\u0304\u2212 \u03b1\u03c0\u03020 = [\u03b1\u0304\u2212 (1\u2212 \u03b3)v\u03c0\u0304] + (1\u2212 \u03b3)[v\u03c0\u0304 \u2212 v\u2217] + (1\u2212 \u03b3)[v\u2217 \u2212 v\u03c0\u03020 ] + [(1\u2212 \u03b3)v\u03c0\u03020 \u2212 \u03b1\u03c0\u03020 ] (i)\n\u2264 [\u03b1\u0304\u2212 (1\u2212 \u03b3)v\u03c0\u0304] + (1\u2212 \u03b3)[v\u2217 \u2212 v\u03c0\u03020 ] + [(1\u2212 \u03b3)v\u03c0\u03020 \u2212 \u03b1\u03c0\u03020 ] (ii) \u2264 9(1\u2212 \u03b3)tminorize + (1\u2212 \u03b3)tminorize + 9(1\u2212 \u03b3)tminorize (iii)\n\u2264 \u03f5\nwhere (i) uses the optimality of \u03c0\u2217, (ii) follows from Lemma 1 and the tminorize-optimality of \u03c0\u03020, (iii) is due to the choice 1\u2212 \u03b3 = \u03f519tminorize . The total number of samples used is\n|S||A|n = O\u0303 (\n|S||A| (1\u2212 \u03b3)2tminorize\n) = O\u0303 ( |S||A|tminorize\n\u03f52\n) .\nThis implies the statement of Theorem 2."
        },
        {
            "heading": "C PROOFS OF KEY PROPOSITIONS",
            "text": "C.1 DECOUPLING THE DEPENDENCE OF P\u0302 AND \u03c0\u03020\nIn this section, we introduce the method proposed by Agarwal et al. (2020) to decouple the statistical dependence of p\u0302s\u2032,a\u2032 and \u03c0\u03020 at a particular state action pair (s\u2032, a\u2032) \u2208 S \u00d7 A. To simplify notation, we use z = (s\u2032, a\u2032) \u2208 S \u00d7 A to denote a pair of state and action. Define the kernel K\u0302(z) :={ \u03ba\u0302 (z) s,a \u2208 P(S) : (s, a) \u2208 S \u00d7A } s.t. \u03ba\u0302(z)s\u2032,a\u2032 = \u03b4s\u2032 and \u03ba\u0302 (z) s,a = p\u0302s,a for all (s, a) \u0338= z. Therefore,\nunder K\u0302(z) and at state action pair z = (s\u2032, a\u2032), the MDP will transition to s\u2032 w.p.1, while other transitions are done according to P\u0302 .\nNow, for fixed \u03c1 \u2208 R, define a modified reward\nh(z,\u03c1)(s, a) = \u03c11 {z = (s, a)}+R1 {z \u0338= (s, a)} , (s, a) \u2208 S \u00d7A.\nLet g\u0302(z,\u03c1) be the unique solution of the Bellman equation\ng(s) = max a\u2208A\n( h(z,\u03c1)(s, a) + \u03b3\u03ba\u0302(z)s,a[g] ) .\nLet \u03c7\u0302(z,\u03c1) be any optimal policy associated with g\u0302(z,\u03c1). Now notice that since \u03ba\u0302(z)s\u2032,a\u2032 = \u03b4s\u2032 is nonrandom. Thus, g\u0302(z,\u03c1) and \u03c7\u0302(z,\u03c1) are independent of p\u0302z . By the definition of the auxiliary value functions in expression (A.4) above, v(z,\u03c1)l := v \u03c7\u0302(z,\u03c1)\nl is also independent of p\u0302z . Therefore, if we define Gz := \u03c3(p\u0302s,a : (s, a) \u0338= z,R), then v(z,\u03c1)l is measureable w.r.t. Gz .\nTherefore, we can decouple the dependence of P\u0302s,a and \u03c0\u03020 by replacing \u03c0\u03020 with \u03c7\u0302(z, \u03c1) for some \u03c1 so that \u03c0\u03020 = \u03c7\u0302(z, \u03c1) with high probability. To achieve this, we first prescribe a finite set (a \u03be-net of the interval [\u2212(1 \u2212 \u03b3)\u22121, (1 \u2212 \u03b3)\u22121] that will be denoted by N\u03be) of possible values for \u03c1, and prove that for sufficiently small \u03be, such a \u03c1 can be picked from this set. Note that the motivation for constructing N\u03b6 and seeking a \u03c1 within it stems from the finite nature of this set, which enables us to apply the union bound technique.\nConcretely, define an \u03be-net on [\u2212(1\u2212 \u03b3)\u22121, (1\u2212 \u03b3)\u22121] by\nN\u03be := {\u2212k\u03be\u03be,\u2212(k\u03be \u2212 1)\u03be, . . . , 0, . . . , (k\u03be \u2212 1)\u03be, k\u03be\u03be} where k\u03be = \u230a (1\u2212 \u03b3)\u22121\u03be\u22121 \u230b . Note that, |N\u03be| \u2264 2(1 \u2212 \u03b3)\u22121\u03be\u22121. Then the following lemma in Li et al. (2020) indicates that it is sufficient to set \u03be = (1 \u2212 \u03b3)\u03b7/4 where we recall that \u03b7 is the optimality gap parameter of the event \u2126\u03b7 in (A.3).\nLemma 2 (Li et al. (2020), Lemma 4). For each z \u2208 S \u00d7 A, there exists \u03c1z \u2208 N(1\u2212\u03b3)\u03b7/4 s.t. \u03c7\u0302(z, \u03c1z)(\u03c9) = \u03c0\u03020(\u03c9) for all \u03c9 \u2208 \u2126\u03b7 .\nWith this decoupling technique and policies \u03c7\u0302(z, \u03c1z), we can approximate v\u03c0\u03020l by v (z,\u03c1) l with \u03c1z \u2208 N(1\u2212\u03b3)\u03b7/4. In particular, the following concentration inequality for v (z,\u03c1) l can be translated to that of v\u03c0\u03020l , leading to our proof of Proposition A.1.\nLemma 3 (Bernstein\u2019s Inequality). For each z \u2208 S\u00d7A, consider any finite set U (z) \u2282 R, then w.p. at least 1\u2212 \u03b4 under P , we have that for all 0 \u2264 l \u2264 l\u2217, z \u2208 S \u00d7A, \u03c1 \u2208 U (z)\u2223\u2223\u2223(p\u0302z \u2212 pz) [v(z,\u03c1)l ]\u2223\u2223\u2223 \u2264 \u221a \u03b2\nn \u03c3(v\n(z,\u03c1) l )(z) +\n\u03b2\nn \u2223\u2223\u2223v(z,\u03c1)l \u2223\u2223\u2223 span\nwhere\n\u03b2 := 2 log\n( 2|S||A|l\u2217|U |\n\u03b4 ) and |U | := supz\u2208S\u00d7A |U (z)|.\nThis lemma is proved in Appendix D.1. We note that, of course, N(1\u2212\u03b3)\u03b7/4 will be used in place of U (z).\nC.2 PROOF OF PROPOSITION A.1\nBefore we prove Proposition A.1, we introduce the following lemma that converts Bernstein-type inequalities for {vl} as in Lemma 3 to concentration bound of v\u03020. Lemma 4. Fix any b > 0 and possibly data dependent policy \u03c0\u0303(\u03c9) \u2208 \u03a0. LetB \u2282 \u2126 be s.t. \u2200\u03c9 \u2208 B, the sequence { v\u03c0\u0303l }\ndefined in (A.4) satisfies\u2223\u2223\u2223(P\u0302\u03c0\u0303 \u2212 P\u03c0\u0303)v\u03c0\u0303l \u2223\u2223\u2223 \u2264\u221a bn\u03c3\u03c0\u0303(v\u03c0\u0303l ) + bn \u2223\u2223v\u03c0\u0303l \u2223\u2223span for all l \u2264 l\u2217 = \u230a 1 2 log2((1\u2212 \u03b3) \u22121) \u230b , where the absolute value is taken entry-wise. Then\n\u2225\u2225v\u0302\u03c0\u03030 \u2212 v\u03c0\u03030 \u2225\u2225\u221e \u2264 243 \u221a btminorize (1\u2212 \u03b3)2n\non B, provided that n \u2265 64b(1\u2212 \u03b3)\u22121.\nThe proof of this lemma is provided in Appendix D.2. Note the appearance of tminorize in the bound. This is a consequence of the analysis technique in Wang et al. (2023). With the above auxiliary definitions and results, we prove Proposition A.1\nProof of Proposition A.1. We proceed with proving the first bound. By Lemma 2 and 3 with \u03b4 replaced by \u03b4/3 and U (z) by N(1\u2212\u03b3)\u03b7/4, there exists B \u2282 \u2126 s.t. P (B) \u2265 1\u2212 \u03b4/3 and on B \u2229 \u2126\u03b7\u2223\u2223\u2223(p\u0302z \u2212 pz) [v\u03c0\u03020l ]\u2223\u2223\u2223 = \u2223\u2223\u2223(p\u0302z \u2212 pz) [v(z,\u03c1z)l ]\u2223\u2223\u2223\n\u2264 \u221a \u03b2\u03b4(\u03b7)\nn \u03c3(v\n(z,\u03c1z) l )(z) +\n\u03b2\u03b4(\u03b7)\nn \u2223\u2223\u2223v(z,\u03c1z)l \u2223\u2223\u2223 span\n\u2264 \u221a \u03b2\u03b4(\u03b7)\nn \u03c3(v\u03c0\u03020l )(z) +\n\u03b2\u03b4(\u03b7)\nn \u2223\u2223\u2223v\u03c0\u03020l \u2223\u2223\u2223 span .\nfor all 0 \u2264 l \u2264 l\u2217 = \u230a 1 2 log2((1\u2212 \u03b3) \u22121) \u230b , z \u2208 S \u00d7A, \u03c1 \u2208 U (z). In particular, on B \u2229 \u2126\u03b7\u2223\u2223\u2223(P\u0302\u03c0\u03020 \u2212 P\u03c0\u03020)v\u03c0\u03020l \u2223\u2223\u2223 \u2264 \u221a \u03b2\u03b4(\u03b7)\nn \u03c3\u03c0\u03020(v\n\u03c0\u03020 l ) +\n\u03b2\u03b4(\u03b7)\nn \u2223\u2223\u2223v\u03c0\u03020l \u2223\u2223\u2223 span\nall 0 \u2264 l \u2264 l\u2217. Therefore, we conclude that by Lemma 4\u2225\u2225\u2225v\u0302\u03c0\u030200 \u2212 v\u03c0\u030200 \u2225\u2225\u2225\u221e \u2264 243 \u221a \u03b2\u03b4(\u03b7)tminorize (1\u2212 \u03b3)2n\non B \u2229 \u2126\u03b7 . Sinece P (\u2126\u03b7) \u2265 1\u2212 \u03b4/3, apply union bound, one obtains the first claimed. To prove the second claim, we note that\n0 \u2264 v\u03c000 \u2212 v \u03c0\u03020 0\n= (v\u03c000 \u2212 v\u0302 \u03c00 0 ) + (v\u0302 \u03c00 0 \u2212 v\u0302 \u03c0\u03020 0 ) + (v\u0302 \u03c0\u03020 0 \u2212 v \u03c0\u03020 0 )\n\u2264 \u2225v\u03c000 \u2212 v\u0302 \u03c00 0 \u2225\u221e + \u2225\u2225\u2225v\u0302\u03c0\u030200 \u2212 v\u03c0\u030200 \u2225\u2225\u2225\u221e where the last inequality follows from v\u0302\u03c000 \u2212 v\u0302 \u03c0\u03020 0 \u2264 0. Hence, it remains to bound\n\u2225\u2225v\u2217 \u2212 v\u0302\u03c0\u2217\u2225\u2225\u221e. It is easy to see that the same proof of Lemma 3 implies that\u2223\u2223\u2223(P\u0302\u03c00 \u2212 P\u03c00)v\u03c00l \u2223\u2223\u2223 \u2264 \u221a \u03b2\u03b4(\u03b7)\nn \u03c3\u03c00(v\n\u03c00 l ) +\n\u03b2\u03b4(\u03b7)\nn |v\u03c00l |span\nfor all 0 \u2264 l \u2264 l\u2217 w.p. at least 1\u2212 \u03b4/3. Therefore, by Lemma 4,\n\u2225v\u03c000 \u2212 v\u0302 \u03c00 0 \u2225\u221e \u2264 243 \u221a \u03b2\u03b4(\u03b7)tminorize (1\u2212 \u03b3)2n .\nw.p. at least 1\u2212 \u03b4/3. Again, an application of the union bound completes the proof."
        },
        {
            "heading": "D PROOFS OF AUXILIARY LEMMAS",
            "text": "D.1 PROOF OF LEMMA 3\nProof. Recall that Gz := \u03c3(p\u0302s,a : (s, a) \u0338= z,R). For each z \u2208 S \u00d7 A, define the probability measure Pz(\u00b7) := P (\u00b7|Gz) and expectation Ez[\u00b7] := E[\u00b7|Gz]. Fix any 0 \u2264 l \u2264 l\u2217 and \u03c1 \u2208 U (z). Since (p\u0302z \u2212 pz)[1] = 0, \u2223\u2223\u2223(p\u0302z \u2212 pz) [v(z,\u03c1)l ]\u2223\u2223\u2223 \u2264 2 \u2223\u2223\u2223v(z,\u03c1)l \u2223\u2223\u2223\nspan .\nAs p\u0302z is independent of Gz , and v(z,\u03c1)l is measureable w.r.t. Gz ,\nEz [ (p\u0302z \u2212 pz) [ v (z,\u03c1) l ]] = 0.\nTherefore, by Bernstein\u2019s inequality, for any t \u2265 0\nPz (\u2223\u2223\u2223(p\u0302z \u2212 pz) [v(z,\u03c1)l ]\u2223\u2223\u2223 > t) \u2264 2 exp \u2212 n2t2\n2n\u03c32(v (z,p) l )(z) + 4 3 \u2223\u2223\u2223v(z,\u03c1)l \u2223\u2223\u2223 span nt  . Choose t s.t. the r.h.s. is less than \u03b4/(|S||A|l\u2217|U |), we find that it is sufficient for\nt \u2265 \u221a\u221a\u221a\u221a2 log ( 2|S||A|l\u2217|U |\u03b4 ) n \u03c3(v (z,p) l )(z) + 4 3 log ( 2|S||A|l\u2217|U | \u03b4 ) \u2223\u2223\u2223v(z,\u03c1)l \u2223\u2223\u2223 span n .\nClearly\nt0 :=\n\u221a \u03b2\nn \u03c3(v\n(z,\u03c1) l ) +\n\u03b2\nn \u2223\u2223\u2223v(z,\u03c1)l \u2223\u2223\u2223 span\nsatisfies the above inequality. Therefore,\nP ( max\nl\u2264l\u2217,z\u2208S\u00d7A max \u03c1\u2208U(z) \u2223\u2223\u2223(p\u0302z \u2212 pz) [v(z,\u03c1)l ]\u2223\u2223\u2223 > t0) \u2264\n\u2211 l\u2264l\u2217,z\u2208S\u00d7A \u2211 \u03c1\u2208U(z) EPz (\u2223\u2223\u2223(p\u0302z \u2212 pz) [v(z,\u03c1)l ]\u2223\u2223\u2223 > t0) \u2264\n\u2211 l\u2264l\u2217,z\u2208S\u00d7A \u2211 \u03c1\u2208U(z)\n\u03b4\n|S||A|l\u2217|U |\n\u2264 \u03b4 This directly implies the statement of the lemma.\nD.2 PROOF OF LEMMA 4\nProof. First, observe that by the definition in (A.4),\nv\u0302\u03c0\u0303l \u2212 v\u03c0\u0303l = (I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121h\u03c0\u0303l \u2212 (I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121(I \u2212 \u03b3P\u0302\u03c0\u0303)v\u03c0\u0303l = (I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121(I \u2212 \u03b3P\u03c0\u0303)v\u03c0\u0303l \u2212 (I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121(I \u2212 \u03b3P\u0302\u03c0\u0303)v\u03c0\u0303l = \u03b3(I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121(P\u0302\u03c0\u0303 \u2212 P\u03c0\u0303)v\u03c0\u0303l\nLet us define \u2206l := \u2225\u2225v\u0302\u03c0\u0303l \u2212 v\u03c0\u0303l \u2225\u2225\u221e. By the assumption of Lemma 4, for all 0 \u2264 l \u2264 l\u2217, on the event B in the lemma,\n\u2206l (i) \u2264 \u2225\u2225\u2225\u03b3(I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121 \u2223\u2223\u2223(P\u0302\u03c0\u0303 \u2212 P\u03c0\u0303)v\u03c0\u0303l \u2223\u2223\u2223\u2225\u2225\u2225\u221e\n(ii) \u2264 \u03b3 \u221a b\nn \u2225\u2225\u2225(I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121\u03c3\u03c0\u0303(v\u03c0\u0303l )\u2225\u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u0303l \u2223\u2223span = \u03b3 \u221a b\nn \u2225\u2225v\u0302\u03c0\u0303l+1\u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u0303l \u2223\u2223span \u2264 \u03b3 \u221a b\nn \u2206l+1 + \u03b3\n\u221a b\nn \u2225\u2225v\u03c0\u0303l+1\u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u0303l \u2223\u2223span\n(D.1)\nwhere (i) and (ii) follow from (I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121 being non-negative so that (I \u2212 \u03b3P\u0302\u03c0)\u22121h \u2264 (I \u2212 \u03b3P\u0302\u03c0)\n\u22121|h| \u2264 (I \u2212 \u03b3P\u0302\u03c0)\u22121g for all function h : S \u2192 R and g \u2265 |h|. We can think of (D.1) as a recursive bound for \u22060. To analyze this recursive bound, we first consider the following. By Lemma 11 of Li et al. (2020), we have that for l \u2265 0\n\u2225\u2225v\u03c0\u0303l+1\u2225\u2225\u221e = \u2225\u2225(I \u2212 \u03b3P\u03c0\u0303)\u22121\u03c3\u03c0\u0303(v\u03c0\u0303l )\u2225\u2225\u221e \u2264 4 \u03b3 \u221a 1\u2212 \u03b3\n\u2225\u2225v\u03c0\u0303l \u2225\u2225\u221e \u2264 . . .\n\u2264 (\n4\n\u03b3 \u221a 1\u2212 \u03b3\n)l \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e (D.2)\nTherefore, expanding the recursion (D.1)\n\u22060 \u2264 \u03b3 \u221a b\nn \u22061 + \u03b3\n\u221a b\nn \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u03030 \u2223\u2223span \u2264 . . .\n\u2264 ( \u03b3 \u221a b\nn\n)l\u2217 \u2206l\u2217 +\nl\u2217\u2211 k=1\n( \u03b3 \u221a b\nn )k \u2225\u2225v\u03c0\u0303k\u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n l\u2217\u22121\u2211 k=0 ( \u03b3 \u221a b n )k \u2223\u2223v\u03c0\u0303k \u2223\u2223span Since v\u03c0\u0303k \u2265 0 for all k \u2265 0,\n\u2223\u2223v\u03c0\u0303k \u2223\u2223span \u2264 \u2225\u2225v\u03c0\u0303k\u2225\u2225\u221e. We have that \u22060 \u2264 ( \u03b3 \u221a b\nn\n)l\u2217 \u2206l\u2217 + ( 1 +\n\u03b3b\n(1\u2212 \u03b3)n ) l\u2217\u2211 k=1 ( \u03b3 \u221a b n )k \u2225\u2225v\u03c0\u0303k\u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u03030 \u2223\u2223span =: E1 + E2 + E3 (D.3)\nWe first consider E1. By the identities (D.1) and (D.2) \u2206l\u2217 \u2264 \u03b3 \u221a b\nn \u2225\u2225\u2225(I \u2212 \u03b3P\u0302\u03c0\u0303)\u22121\u03c3\u03c0\u0303(v\u03c0\u0303l )\u2225\u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u0303l\u2217 \u2223\u2223span \u2264 \u03b3\n1\u2212 \u03b3\n\u221a b\nn \u2225\u2225\u03c3\u03c0\u0303(v\u03c0\u0303l )\u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u0303l\u2217 \u2223\u2223span \u2264 ( \u03b3\n(1\u2212 \u03b3)\n\u221a b\nn +\n\u03b3b\n(1\u2212 \u03b3)n )\u2225\u2225v\u03c0\u0303l\u2217\u2225\u2225\u221e \u2264 ( \u03b3\n(1\u2212 \u03b3)\n\u221a b\nn +\n\u03b3b\n(1\u2212 \u03b3)n\n)( 4\n\u03b3 \u221a 1\u2212 \u03b3 )l\u2217\u22121 \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e \u2264 \u03b3 2\n4\n(\u221a b\n(1\u2212 \u03b3)n +\nb\n(1\u2212 \u03b3)n\n)( 4\n\u03b3 \u221a 1\u2212 \u03b3 )l\u2217 \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e (i)\n\u2264 \u03b3 2\n2\n\u221a b\n(1\u2212 \u03b3)n\n( 4\n\u03b3 \u221a 1\u2212 \u03b3 )l\u2217 \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e where (i) follows from n \u2265 b/(1\u2212 \u03b3). Therefore,\nE1 \u2264 \u03b32\n2 \u221a 1\u2212 \u03b3\n( 16b\n(1\u2212 \u03b3)n\n)(l\u2217+1)/2\u221a b\nn \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e (i) \u2264 1\u221a 1\u2212 \u03b3 2\u2212(l \u2217+2) \u221a b n\n\u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e (ii)\n\u2264 1\u221a 1\u2212 \u03b3\n2log2(1\u2212\u03b3)/2 \u221a b\nn \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e \u2264 \u221a b\nn \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e where (i) follows from the assumption that n \u2265 64b(1\u2212\u03b3)\u22121 and (ii) is due to l\u2217+2 \u2265 12 log2((1\u2212 \u03b3)\u22121).\nNext, we bound E2. By (D.2)\nE2 \u2264 \u03b3 \u221a 1\u2212 \u03b3 2 l\u2217+1\u2211 k=1\n( \u03b3 \u221a b\nn\n)k ( 4\n\u03b3 \u221a 1\u2212 \u03b3 )k \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e \u2264 2\u03b3 \u221a b\nn \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e \u221e\u2211 k=0\n(\u221a 16b\n(1\u2212 \u03b3)n\n)k\n\u2264 2 \u221a b\nn \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e . Also, note that v\u03c0\u03030 = (I \u2212 \u03b3P\u03c0\u0303)\u22121r\u03c0\u0303 = v\u03c0\u0303 and v\u03c0\u03031 = (I \u2212 \u03b3P\u03c0\u0303)\u22121\u03c3\u03c0\u0303(v\u03c0\u0303). By Proposition 6.1 and Corollary 6.2.1 of Wang et al. (2023)\u2223\u2223v\u03c0\u03030 \u2223\u2223span \u2264 3tminorize and \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e \u2264 80\u221atminorize1\u2212 \u03b3 . Thus we conclude that\n\u22060 \u2264 3 \u221a b\nn \u2225\u2225v\u03c0\u03031 \u2225\u2225\u221e + \u03b3b(1\u2212 \u03b3)n \u2223\u2223v\u03c0\u03030 \u2223\u2223span \u2264 1\n1\u2212 \u03b3\n( 240 \u221a btminorize\nn + 3btminorize n\n)\n\u2264 243 \u221a btminorize (1\u2212 \u03b3)2n\nwhere the last inequality follows from tminorize \u2264 (1\u2212\u03b3)\u22121 and so btminorize/n \u2264 1 by assumption on n."
        }
    ],
    "title": "AVERAGE REWARD MARKOV DECISION PROCESSES",
    "year": 2024
}