{
    "abstractText": "Energy-based models (EBMs) are known in the Machine Learning community for decades. Since the seminal works devoted to EBMs dating back to the noughties, there have been a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN-based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present a novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. From the theoretical perspective, we prove generalization bounds for our technique. In practice, we validate its applicability in toy 2D and image domains. To showcase the scalability, we empower our method with a pre-trained StyleGAN and apply it to high-res AFHQ 512 \u00d7 512 unpaired I2I translation. For simplicity, we choose simple shortand long-run EBMs as a backbone of our Energy-guided Entropic OT approach, leaving the application of more sophisticated EBMs for future research. Our code is available at: https: //github.com/PetrMokrov/Energy-guided-Entropic-OT Figure 1: AFHQ 512\u00d7 512 Cat\u2192Dog unpaired translation by our Energy-guided EOT solver applied in the latent space of StyleGAN2-ADA. Our approach does not need data2latent encoding. Left: source samples; right: translated samples.",
    "authors": [
        {
            "affiliations": [],
            "name": "Petr Mokrov"
        },
        {
            "affiliations": [],
            "name": "Alexander Korotin"
        },
        {
            "affiliations": [],
            "name": "Alexander Kolesov"
        },
        {
            "affiliations": [],
            "name": "Nikita Gushchin"
        },
        {
            "affiliations": [],
            "name": "Evgeny Burnaev"
        }
    ],
    "id": "SP:3fcf201bf470cb254067d479afa7561654cac133",
    "references": [
        {
            "authors": [
                "David Alvarez-Melis",
                "Nicolo Fusi"
            ],
            "title": "Geometric dataset distances via optimal transport",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "David Alvarez-Melis",
                "Yair Schiff",
                "Youssef Mroueh"
            ],
            "title": "Optimizing functionals on the space of probabilities with input convex neural networks",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Arip Asadulaev",
                "Alexander Korotin",
                "Vage Egiazarian",
                "Petr Mokrov",
                "Evgeny Burnaev"
            ],
            "title": "Neural optimal transport with general cost functionals",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Julio Backhoff-Veraguas",
                "Mathias Beiglb\u00f6ck",
                "Gudmun Pammer"
            ],
            "title": "Existence, duality, and cyclical monotonicity for weak transport costs",
            "venue": "Calculus of Variations and Partial Differential Equations,",
            "year": 2019
        },
        {
            "authors": [
                "Valentin De Bortoli",
                "James Thornton",
                "Jeremy Heng",
                "Arnaud Doucet"
            ],
            "title": "Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tianrong Chen",
                "Guan-Horng Liu",
                "Evangelos Theodorou"
            ],
            "title": "Likelihood training of schr\u00f6dinger bridge using forward-backward SDEs theory",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jooyoung Choi",
                "Sungwon Kim",
                "Yonghyun Jeong",
                "Youngjune Gwon",
                "Sungroh Yoon"
            ],
            "title": "Ilvr: Conditioning method for denoising diffusion probabilistic models",
            "venue": "ieee. In CVF international conference on computer vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Yunjey Choi",
                "Youngjung Uh",
                "Jaejun Yoo",
                "Jung-Woo Ha"
            ],
            "title": "Stargan v2: Diverse image synthesis for multiple domains",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Christian Clason",
                "Dirk A Lorenz",
                "Hinrich Mahler",
                "Benedikt Wirth"
            ],
            "title": "Entropic regularization of continuous optimal transport problems",
            "venue": "Journal of Mathematical Analysis and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Cuturi"
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Max Daniels",
                "Tyler Maunu",
                "Paul Hand"
            ],
            "title": "Score-based generative neural networks for large-scale optimal transport",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Eustasio del Barrio",
                "Alberto Gonz\u00e1lez Sanz",
                "Jean-Michel Loubes",
                "Jonathan Niles-Weed"
            ],
            "title": "An improved central limit theorem and fast convergence rates for entropic transportation costs",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2023
        },
        {
            "authors": [
                "Ishan Deshpande",
                "Ziyu Zhang",
                "Alexander G Schwing"
            ],
            "title": "Generative modeling using the sliced wasserstein distance",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yilun Du",
                "Igor Mordatch"
            ],
            "title": "Implicit generation and modeling with energy based models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yilun Du",
                "Shuang Li",
                "B. Joshua Tenenbaum",
                "Igor Mordatch"
            ],
            "title": "Improved contrastive divergence training of energy based models",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Jiaojiao Fan",
                "Amirhossein Taghvaei",
                "Yongxin Chen Chen"
            ],
            "title": "Scalable computations of wasserstein barycenter via input convex neural networks",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaojiao Fan",
                "Shu Liu",
                "Shaojun Ma",
                "Hao-Min Zhou",
                "Yongxin Chen"
            ],
            "title": "Neural monge map estimation and its applications",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Chris Finlay",
                "Augusto Gerolin",
                "Adam M Oberman",
                "Aram-Alexandre Pooladian"
            ],
            "title": "Learning normalizing flows from entropy-kantorovich potentials",
            "venue": "arXiv preprint arXiv:2006.06033,",
            "year": 2020
        },
        {
            "authors": [
                "Ruiqi Gao",
                "Erik Nijkamp",
                "Diederik P Kingma",
                "Zhen Xu",
                "Andrew M Dai",
                "Ying Nian Wu"
            ],
            "title": "Flow contrastive estimation of energy-based models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Ruiqi Gao",
                "Yang Song",
                "Ben Poole",
                "Ying Nian Wu",
                "Diederik P Kingma"
            ],
            "title": "Learning energy-based models by diffusion recovery likelihood",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Milena Gazdieva",
                "Litu Rout",
                "Alexander Korotin",
                "Andrey Kravchenko",
                "Alexander Filippov",
                "Evgeny Burnaev"
            ],
            "title": "An optimal transport perspective on unpaired image super-resolution",
            "venue": "arXiv preprint arXiv:2202.01116,",
            "year": 2022
        },
        {
            "authors": [
                "Aude Genevay"
            ],
            "title": "Entropy-regularized optimal transport for machine learning",
            "venue": "PhD thesis, Paris Sciences et Lettres (ComUE),",
            "year": 2019
        },
        {
            "authors": [
                "Aude Genevay",
                "Marco Cuturi",
                "Gabriel Peyr\u00e9",
                "Francis Bach"
            ],
            "title": "Stochastic optimization for largescale optimal transport",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Aude Genevay",
                "L\u00e9naic Chizat",
                "Francis Bach",
                "Marco Cuturi",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Sample complexity of sinkhorn divergences",
            "venue": "In The 22nd international conference on artificial intelligence and statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Mark Girolami",
                "Ben Calderhead"
            ],
            "title": "Riemann manifold langevin and hamiltonian monte carlo methods",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2011
        },
        {
            "authors": [
                "Nathael Gozlan",
                "Cyril Roberto",
                "Paul-Marie Samson",
                "Prasad Tetali"
            ],
            "title": "Kantorovich duality for general transport costs and applications",
            "venue": "Journal of Functional Analysis,",
            "year": 2017
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville"
            ],
            "title": "Improved training of wasserstein gans",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Nikita Gushchin",
                "Alexander Kolesov",
                "Alexander Korotin",
                "Dmitry Vetrov",
                "Evgeny Burnaev"
            ],
            "title": "Entropic neural optimal transport via diffusion processes",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Tian Han",
                "Erik Nijkamp",
                "Xiaolin Fang",
                "Mitch Hill",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Divergence triangle for joint training of generator model, energy-based model, and inferential model",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey E Hinton"
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Neural computation,",
            "year": 2002
        },
        {
            "authors": [
                "Matthew D Hoffman",
                "Andrew Gelman"
            ],
            "title": "The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2014
        },
        {
            "authors": [
                "Xun Huang",
                "Ming-Yu Liu",
                "Serge Belongie",
                "Jan Kautz"
            ],
            "title": "Multimodal unsupervised image-to-image translation",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Hicham Janati",
                "Boris Muzellec",
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Entropic optimal transport between unbalanced gaussian measures has a closed form",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jason M Klusowski",
                "Andrew R Barron"
            ],
            "title": "Approximation by combinations of relu and squared relu ridge functions with l1 and l controls",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Alexander Korotin",
                "Vage Egiazarian",
                "Arip Asadulaev",
                "Alexander Safin",
                "Evgeny Burnaev"
            ],
            "title": "Wasserstein-2 generative networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Korotin",
                "Lingxiao Li",
                "Justin Solomon",
                "Evgeny Burnaev"
            ],
            "title": "Continuous wasserstein-2 barycenter estimation without minimax optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Korotin",
                "Vage Egiazarian",
                "Lingxiao Li",
                "Evgeny Burnaev"
            ],
            "title": "Wasserstein iterative networks for barycenter estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Korotin",
                "Alexander Kolesov",
                "Evgeny Burnaev"
            ],
            "title": "Kantorovich strikes back! wasserstein GANs are not optimal transport",
            "venue": "In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Korotin",
                "Daniil Selikhanovych",
                "Evgeny Burnaev"
            ],
            "title": "Kernel neural optimal transport",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Korotin",
                "Daniil Selikhanovych",
                "Evgeny Burnaev"
            ],
            "title": "Neural optimal transport",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Rithesh Kumar",
                "Sherjil Ozair",
                "Anirudh Goyal",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Maximum entropy generators for energy-based models",
            "venue": "arXiv preprint arXiv:1901.08508,",
            "year": 2019
        },
        {
            "authors": [
                "John Lawson",
                "George Tucker",
                "Bo Dai",
                "Rajesh Ranganath"
            ],
            "title": "Energy-inspired models: Learning with sampler-induced distributions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yann LeCun",
                "Sumit Chopra",
                "Raia Hadsell",
                "M Ranzato",
                "Fujie Huang"
            ],
            "title": "A tutorial on energy-based learning",
            "venue": "Predicting structured data,",
            "year": 2006
        },
        {
            "authors": [
                "Xingchao Liu",
                "Chengyue Gong",
                "qiang liu"
            ],
            "title": "Flow straight and fast: Learning to generate and transfer data with rectified flow",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Guansong Lu",
                "Zhiming Zhou",
                "Jian Shen",
                "Cheng Chen",
                "Weinan Zhang",
                "Yong Yu"
            ],
            "title": "Large-scale optimal transport via adversarial training with cycle-consistency",
            "venue": "arXiv preprint arXiv:2003.06635,",
            "year": 2020
        },
        {
            "authors": [
                "Giulia Luise",
                "Saverio Salzo",
                "Massimiliano Pontil",
                "Carlo Ciliberto"
            ],
            "title": "Sinkhorn barycenters with free support via frank-wolfe algorithm",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Shaojun Ma",
                "Shu Liu",
                "Hongyuan Zha",
                "Haomin Zhou"
            ],
            "title": "Learning stochastic behaviour from aggregate data",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ashok Makkuva",
                "Amirhossein Taghvaei",
                "Sewoong Oh",
                "Jason Lee"
            ],
            "title": "Optimal transport mapping via input convex neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Simone Di Marino",
                "Augusto Gerolin"
            ],
            "title": "An optimal transport approach for the schr\u00f6dinger bridge problem and convergence of sinkhorn algorithm",
            "venue": "Journal of Scientific Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Gonzalo Mena",
                "Jonathan Niles-Weed"
            ],
            "title": "Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "SDEdit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Petr Mokrov",
                "Alexander Korotin",
                "Lingxiao Li",
                "Aude Genevay",
                "Justin M Solomon",
                "Evgeny Burnaev"
            ],
            "title": "Large-scale wasserstein gradient flows",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tuan Nguyen",
                "Trung Le",
                "He Zhao",
                "Quan Hung Tran",
                "Truyen Nguyen",
                "Dinh Phung"
            ],
            "title": "Most: Multi-source domain adaptation via optimal transport for student-teacher learning",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Mitch Hill",
                "Tian Han",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "On the anatomy of mcmc-based maximum likelihood learning of energy-based models",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Ruiqi Gao",
                "Pavel Sountsov",
                "Srinivas Vasudevan",
                "Bo Pang",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "MCMC should mix: Learning energy-based model with neural transport latent space MCMC",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Tomohiro Nishiyama"
            ],
            "title": "Convex optimization on functionals of probability densities",
            "venue": "arXiv preprint arXiv:2002.06488,",
            "year": 2020
        },
        {
            "authors": [
                "Marcel Nutz"
            ],
            "title": "Introduction to entropic optimal transport",
            "venue": "Lecture notes, Columbia University,",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yixuan Qiu",
                "Lingsong Zhang",
                "Xiao Wang"
            ],
            "title": "Unbiased contrastive divergence algorithm for training energy-based latent variable models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Rigollet",
                "Austin J Stromme"
            ],
            "title": "On the sample complexity of entropic optimal transport",
            "venue": "arXiv preprint arXiv:2206.13472,",
            "year": 2022
        },
        {
            "authors": [
                "Gareth O Roberts",
                "Richard L Tweedie"
            ],
            "title": "Exponential convergence of langevin distributions and their discrete approximations",
            "year": 1996
        },
        {
            "authors": [
                "Litu Rout",
                "Alexander Korotin",
                "Evgeny Burnaev"
            ],
            "title": "Generative modeling with optimal transport maps",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Giorgi Rukhaia"
            ],
            "title": "A FreeForm Optics Application of Entropic Optimal Transport",
            "venue": "PhD thesis, Universite\u0301 Paris sciences et lettres,",
            "year": 2021
        },
        {
            "authors": [
                "Ruslan Salakhutdinov",
                "Andriy Mnih",
                "Geoffrey Hinton"
            ],
            "title": "Restricted boltzmann machines for collaborative filtering",
            "venue": "In Proceedings of the 24th international conference on Machine learning,",
            "year": 2007
        },
        {
            "authors": [
                "Sergey Samsonov",
                "Evgeny Lagutin",
                "Marylou Gabri\u00e9",
                "Alain Durmus",
                "Alexey Naumov",
                "Eric Moulines"
            ],
            "title": "Local-global MCMC kernels: the best of both worlds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Filippo Santambrogio"
            ],
            "title": "Optimal transport for applied mathematicians",
            "venue": "Birka\u0308user, NY,",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey Schiebinger",
                "Jian Shu",
                "Marcin Tabaka",
                "Brian Cleary",
                "Vidya Subramanian",
                "Aryeh Solomon",
                "Joshua Gould",
                "Siyan Liu",
                "Stacie Lin",
                "Peter Berube"
            ],
            "title": "Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming",
            "year": 2019
        },
        {
            "authors": [
                "Vivien Seguy",
                "Bharath Bhushan Damodaran",
                "Remi Flamary",
                "Nicolas Courty",
                "Antoine Rolet",
                "Mathieu Blondel"
            ],
            "title": "Large scale optimal transport and mapping estimation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Shai Shalev-Shwartz",
                "Shai Ben-David"
            ],
            "title": "Understanding machine learning: From theory to algorithms",
            "venue": "Cambridge university press,",
            "year": 2014
        },
        {
            "authors": [
                "Jian Shen",
                "Yanru Qu",
                "Weinan Zhang",
                "Yong Yu"
            ],
            "title": "Wasserstein distance guided representation learning for domain adaptation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Yuyang Shi",
                "Valentin De Bortoli",
                "Andrew Campbell",
                "Arnaud Doucet"
            ],
            "title": "Diffusion schr\u00f6dinger bridge matching",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Justin Solomon",
                "Fernando De Goes",
                "Gabriel Peyr\u00e9",
                "Marco Cuturi",
                "Adrian Butscher",
                "Andy Nguyen",
                "Tao Du",
                "Leonidas Guibas"
            ],
            "title": "Convolutional wasserstein distances: Efficient optimal transportation on geometric domains",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Diederik P Kingma"
            ],
            "title": "How to train your energy-based models",
            "venue": "arXiv preprint arXiv:2101.03288,",
            "year": 2021
        },
        {
            "authors": [
                "Sreejith Sreekumar",
                "Zhengxin Zhang",
                "Ziv Goldfeld"
            ],
            "title": "Non-asymptotic performance guarantees for neural estimation of f-divergences",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Amirhossein Taghvaei",
                "Amin Jalali"
            ],
            "title": "2-wasserstein approximation via restricted convex potentials with application to improved training for gans",
            "year": 1902
        },
        {
            "authors": [
                "Tijmen Tieleman"
            ],
            "title": "Training restricted boltzmann machines using approximations to the likelihood gradient",
            "venue": "In Proceedings of the 25th international conference on Machine learning,",
            "year": 2008
        },
        {
            "authors": [
                "Piyush Tiwary",
                "Kinjawl Bhattacharyya",
                "Prathosh AP"
            ],
            "title": "Boundary preserving twin energy-basedmodels for image to image translation",
            "year": 2022
        },
        {
            "authors": [
                "Francisco Vargas",
                "Pierre Thodoroff",
                "Austen Lamacraft",
                "Neil Lawrence"
            ],
            "title": "Solving schr\u00f6dinger bridges via maximum likelihood",
            "year": 2021
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: old and new, volume 338",
            "year": 2009
        },
        {
            "authors": [
                "Rui Wang",
                "Ruiyi Zhang",
                "Ricardo Henao"
            ],
            "title": "Wasserstein uncertainty estimation for adversarial domain matching",
            "venue": "Frontiers in big Data,",
            "year": 2022
        },
        {
            "authors": [
                "Yisen Wang",
                "Bo Dai",
                "Lingkai Kong",
                "Sarah Monazam Erfani",
                "James Bailey",
                "Hongyuan Zha"
            ],
            "title": "Learning deep hidden nonlinear dynamics from aggregate data",
            "venue": "arXiv preprint arXiv:1807.08237,",
            "year": 2018
        },
        {
            "authors": [
                "Jianwen Xie",
                "Yang Lu",
                "Song-Chun Zhu",
                "Yingnian Wu"
            ],
            "title": "A theory of generative convnet",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Jianwen Xie",
                "Yang Lu",
                "Ruiqi Gao",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Cooperative training of descriptor and generator networks",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jianwen Xie",
                "Zilong Zheng",
                "Xiaolin Fang",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Learning cycleconsistent cooperative networks via alternating mcmc teaching for unsupervised cross-domain translation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yujia Xie",
                "Minshuo Chen",
                "Haoming Jiang",
                "Tuo Zhao",
                "Hongyuan Zha"
            ],
            "title": "On scalable and efficient computation of large scale optimal transport",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Xuwang Yin",
                "Shiying Li",
                "Gustavo K Rohde"
            ],
            "title": "Learning energy-based models with adversarial training",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Min Zhao",
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Zhao",
                "Changyou Chen"
            ],
            "title": "Unpaired image-to-image translation via latent energy transport",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Zhao",
                "Jianwen Xie",
                "Ping Li"
            ],
            "title": "Learning energy-based generative models via coarse-to-fine expanding and sampling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "2019 Xie et al",
                "2022 Rout et al",
                "2022 Gazdieva et al",
                "2022b Korotin et al",
                "Taghvaei",
                "2019 Jalali",
                "Liu"
            ],
            "title": "2023). However, the majority of these methods model OT as a deterministic map which for each input point x assigns a single data point y rather than distribution \u03c0(y|x). Only a limited number of approaches are capable of solving OT problems",
            "year": 2023
        },
        {
            "authors": [],
            "title": "FEOT = {fEOT : f \u2208 F} = {\u2212\u03b5 logZ(f",
            "year": 2014
        },
        {
            "authors": [
                "Dogs) StyleGAN2-ADA Karras"
            ],
            "title": "2020) is taken from the official PyTorch implementation",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Energy-based models (EBMs) are known in the Machine Learning community for decades. Since the seminal works devoted to EBMs dating back to the noughties, there have been a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN-based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present a novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. From the theoretical perspective, we prove generalization bounds for our technique. In practice, we validate its applicability in toy 2D and image domains. To showcase the scalability, we empower our method with a pre-trained StyleGAN and apply it to high-res AFHQ 512 \u00d7 512 unpaired I2I translation. For simplicity, we choose simple short- and long-run EBMs as a backbone of our Energy-guided Entropic OT approach, leaving the application of more sophisticated EBMs for future research. Our code is available at: https: //github.com/PetrMokrov/Energy-guided-Entropic-OT\nLeft: source samples; right: translated samples."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The computational Optimal Transport (OT) field is an emergent and fruitful area in the Machine Learning research which finds its applications in generative modelling (Arjovsky et al., 2017; Gulrajani et al., 2017; Deshpande et al., 2018), domain adaptation (Nguyen et al., 2021; Shen et al., 2018; Wang et al., 2022), unpaired image-to-image translation (Xie et al., 2019; Hu et al.), datasets manipulation (Alvarez-Melis & Fusi, 2020), population dynamics (Ma et al., 2021; Wang et al., 2018), gradient flows modelling (Alvarez-Melis et al., 2022; Mokrov et al., 2021), barycenter estimation\n(Korotin et al., 2022a; Fan et al., 2021). The majority of the applications listed above utilize OT as a loss function, e.g., have WGAN-like objectives which compare the generated (fake) and true data distributions. However, for some practical use cases, e.g., unpaired image-to-image translation (Korotin et al., 2023b), it is worth modelling the OT maps or plans by themselves.\nThe existing approaches which recover OT plans are based on various theoretically-advised techniques. Some of them (Makkuva et al., 2020; Korotin et al., 2021a) utilize the specific form of the cost function, e.g., squared Euclidean distance. The others (Xie et al., 2019; Lu et al., 2020) modify GAN objectives with additional OT regularizer, which results in biased OT solvers (Gazdieva et al., 2022, Thm. 1). The works (Fan et al., 2023; Korotin et al., 2023b; Rout et al., 2022) take advantage of dual OT problem formulation. They are capable of tackling unbiased large-scale continuous OT with general cost functions but may yield fake solutions (Korotin et al., 2023a). To overcome this issue, (Korotin et al., 2023a) propose to use strictly convex regularizers which guarantee the uniqueness of the recovered OT plans. And one popular choice which has been extensively studied both in discrete (Cuturi, 2013) and continuous (Genevay et al., 2016; Clason et al., 2021) settings is the Entropy. The well-studied methodological choices for modelling Entropy-regularized OT (EOT) include (a) stochastic dual maximization approach which prescribes alternating optimization of dual potentials (Seguy et al., 2018; Daniels et al., 2021) and (b) dynamic setup having connection to Schr\u00f6dinger bridge problem (Bortoli et al., 2021; Gushchin et al., 2023; Chen et al., 2022). In contrast to the methods presented in the literature, we come up with an approach for solving EOT built upon EBMs.\nContributions. We propose a novel energy-based view on the EOT problem. 1. We take advantage of weak dual formulation for the EOT problem and distinguish the EBM-related\nnature of dual potential which originates due to this formulation (\u00a73.1). 2. We propose theoretically-grounded yet easy-to-implement modifications to the standard EBMs\ntraining procedure which makes them capable of recovering the EOT plans (\u00a73.2). 3. We establish generalization bounds for the EOT plans learned via our proposed method (\u00a73.3). 4. We showcase our algorithm\u2019s performance on low- and moderate-dimensional toy setups and\nlarge-scale 512\u00d7512 images transfer tasks solved with help of a pre-trained StyleGAN (\u00a75). Notations. Throughout the paper,X andY are compact subsets of the Euclidean space, i.e.,X \u2282 RDx and Y \u2282 RDy . The continuous functions on X are denoted as C(X ). In turn, P(X ) are the sets of Borel probability distributions on X . Given distributions P \u2208 P(X ) and Q \u2208 P(Y), \u03a0(P,Q) designates the set of couplings between the distributions P and Q, i.e., probability distributions on product space X \u00d7 Y with the first and second marginals given by P and Q, respectively. We use \u03a0(P) to denote the set of probability distributions on X \u00d7 Y with the first marginal given by P. The absolutely continuous probability distributions on X are Pac(X ). For P \u2208 Pac(X ) we use dP(x)dx and dQ(y) dy to denote the corresponding probability density functions. Given distributions \u00b5 and \u03c1 defined on a set Z , \u00b5\u226a \u03c1 means that \u00b5 is absolutely continuous with respect to \u03c1."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 OPTIMAL TRANSPORT",
            "text": "The generic theory behind OT could be found in (Villani et al., 2009; Santambrogio, 2015). For the specific details on EOT, see (Genevay et al., 2016; Genevay, 2019).\nLet P \u2208 P(X ) and Q \u2208 P(Y). The primal OT problem due to Kantorovich (Villani et al., 2009) is:\nOTc(P,Q) def = inf\n\u03c0\u2208\u03a0(P,Q) \u222b X\u00d7Y c(x, y)d\u03c0(x, y). (1)\nIn the equation above, c : X \u00d7 Y \u2192 R is a continuous cost function which reflects a practitioner\u2019s knowledge of how data from the source and target distribution should be aligned. Typically, the cost function c(x, y) is chosen to be Euclidean norm \u2225x\u2212 y\u22252 yielding the 1-Wasserstain distance (W1) or halved squared Euclidean norm 12\u2225x\u2212 y\u2225 2 2 yielding the square of 2-Wasserstein distance (W22). The distributions \u03c0\u2217 \u2208 \u03a0(P,Q) which minimize objective (1) are called the Optimal Transport plans. Problem (1) may have several OT plans (Peyr\u00e9 et al., 2019, Remark 2.3) and in order to impose the uniqueness and obtain a more tractable optimization problem, a common trick is to regularize (1) with strictly convex (w.r.t. distribution \u03c0) functionalsR : P(X \u00d7 Y)\u2192 R.\nEntropy-regularized Optimal Transport. In our work, we utilize the popular Entropic regularization (Cuturi, 2013) which has found its applications in various works (Solomon et al., 2015; Schiebinger et al., 2019; Rukhaia, 2021). This is mainly because of amenable sample complexity (Genevay, 2019, \u00a73) and tractable dual representation of the Entropy-regularized OT problem which can be leveraged by, e.g., Sinkhorn\u2019s algorithm (Cuturi, 2013; Vargas et al., 2021). Besides, the EOT objective is known to be strictly convex (Genevay et al., 2016) thanks to the strict convexity of Entropy H and KL divergence (Santambrogio, 2015; Nutz, 2021; Nishiyama, 2020) appearing in EOT formulations. Let \u03b5 > 0. The EOT problem can be formulated in the following ways: EOT(1)c,\u03b5(P,Q) EOT(2)c,\u03b5(P,Q) EOTc,\u03b5(P,Q) def = min \u03c0\u2208\u03a0(P,Q) \u222b X\u00d7Y c(x, y)d\u03c0(x, y) +  +\u03b5KL (\u03c0\u2225P\u00d7Q) , (2) \u2212\u03b5H(\u03c0), (3) \u2212\u03b5 \u222b XH(\u03c0(\u00b7|x))dP(x). (4)\nThese formulations are equivalent when P and Q are absolutely continuous w.r.t. the corresponding standard Lebesgue measures since KL (\u03c0\u2225P\u00d7Q) = \u2212 \u222b X H(\u03c0(\u00b7|x))dP(x) +H(Q) = \u2212H(\u03c0) + H(Q) +H(P). In other words, the equations (2), (3) and (4) are the same up to additive constants.\nIn the remaining paper, we will primarily work with the EOT formulation (4), and, henceforth, we will additionally assume P \u2208 Pac(X ), Q \u2208 Pac(Y) when necessary. Let \u03c0\u2217 \u2208 \u03a0(P,Q) be the solution of EOT problem. The measure disintegration theorem yields: d\u03c0\u2217(x, y) = d\u03c0\u2217(y|x)d\u03c0\u2217(x) = d\u03c0\u2217(y|x)dP(x). Distributions \u03c0\u2217(\u00b7|x) will play an important role in our analysis. In fact, they constitute the only ingredient needed to (stochastically) transform a source point x \u2208 X to target samples y1, y2, \u00b7 \u00b7 \u00b7 \u2208 Y w.r.t. EOT plan. We say that distributions {\u03c0\u2217(\u00b7|x)}x\u2208X are the optimal conditional plans. EOT problem as a weak OT (WOT) problem. EOT problem (4) can be understood as the so-called weak OT problem (Gozlan et al., 2017; Backhoff-Veraguas et al., 2019). Given a weak transport cost C : X \u00d7 P(Y) \u2192 R which penalizes the displacement of a point x \u2208 X into a distribution \u03c0(\u00b7|x) \u2208 P(Y), the weak OT problem is given by\nWOTC(P,Q) def = inf\n\u03c0\u2208\u03a0(P,Q) \u222b X C(x, \u03c0(\u00b7|x)) d\u03c0(x)\ufe38 \ufe37\ufe37 \ufe38\n=dP(x)\n. (5)\nEOT formulation (4) is a particular case of weak OT problem (5) for weak transport cost: CEOT(x, \u03c0(\u00b7|x)) = \u222b Y c(x, y)d\u03c0(y|x)\u2212 \u03b5H(\u03c0(\u00b7|x)). (6) Note that if weak cost C is strictly convex and lower semicontinuous, as it is the case for CEOT, the solution for (5) exists and unique (Backhoff-Veraguas et al., 2019).\nWeak OT dual formulation of the EOT problem. Similar to the case of classical Kantorovich OT (1), the weak OT problem permits the dual representation. Let f \u2208 C(Y). Following (BackhoffVeraguas et al., 2019, Eq. (1.3)) one introduces weak C-transform fC : X \u2192 R by\nfC(x) def = inf\n\u00b5\u2208P(Y)\n{ C(x, \u00b5)\u2212 \u222b Y f(y)d\u00b5(y) } . (7)\nFor our particular case of EOT-advised weak OT cost (6), equation (7) reads as\nfCEOT(x) = min \u00b5\u2208P(Y) {\u222b Y c(x, y)d\u00b5(y)\u2212 \u03b5H(\u00b5)\u2212 \u222b Y f(y)d\u00b5(y) } def = min \u00b5\u2208P(Y) Gx,f (\u00b5). (8)\nNote that the existence and uniqueness of the minimizer for (8) follows from Weierstrass theorem (Santambrogio, 2015, Box 1.1.) along with lower semicontinuity and strict convexity of Gx,f in \u00b5. The dual weak functional FwC : C(Y)\u2192 R for primal WOT problem (5) is\nFwC (f) def = \u222b X fC(x)dP(x) + \u222b Y f(y)dQ(y).\nThanks to the compactness of X and Y , there is the strong duality (Gozlan et al., 2017, Thm. 9.5):\nEOTc,\u03b5(P,Q) = sup f\u2208C(Y) {\u222b X min \u00b5x\u2208P(Y) Gx,f (\u00b5x)dP(x) + \u222b Y f(y)dQ(y) } = sup f\u2208C(Y) FwCEOT(f). (9)\nWe say that (9) is the weak dual objective. It will play an important role in our further analysis."
        },
        {
            "heading": "2.2 ENERGY-BASED MODELS",
            "text": "The EBMs are a fundamental class of deep Generative Modelling techniques (LeCun et al., 2006; Salakhutdinov et al., 2007) which parameterize distributions of interest \u00b5 \u2208 P(Y) by means of the Gibbs-Boltzmann distribution density:\nd\u00b5(y)\ndy =\n1 Z exp (\u2212E(y)) . (10)\nIn the equation above E : Y \u2192 R is the Energy function (negative unnormalized log-likelihood), and Z = \u222b Y exp(\u2212E(y))dy is the normalization constant, known as the partition function.\nLet \u00b5 \u2208 P(Y) be a true data distribution which is accessible by samples and \u00b5\u03b8(y), \u03b8 \u2208 \u0398 be a parametric family of distributions approximated using, e.g., a deep Neural Network E\u03b8, which imitates the Energy function in (10). In EBMs framework, one tries to bring the parametric distribution \u00b5\u03b8 to the reference one \u00b5 by optimizing the KL divergence between them. The minimization of KL (\u00b5\u2225\u00b5\u03b8) is done via gradient descent by utilizing the well-known gradient (Xie et al., 2016):\n\u2202\n\u2202\u03b8 KL (\u00b5\u2225\u00b5\u03b8) = \u222b Y \u2202 \u2202\u03b8 E\u03b8(y)d\u00b5(y)\u2212 \u222b Y [ \u2202 \u2202\u03b8 E\u03b8(y) ] d\u00b5\u03b8(y). (11)\nThe expectations on the right-hand side of (11) are estimated by Monte-Carlo, which requires samples from \u00b5 and \u00b5\u03b8. While the former are given, the latter are usually obtained via Unadjusted Langevin Algorithm (ULA) (Roberts & Tweedie, 1996). It iterates the discretized Langevin dynamics\nYl+1 = Yl \u2212 \u03b7\n2\n\u2202\n\u2202y E\u03b8(Yl) +\n\u221a \u03b7\u03bel , \u03bel \u223c N (0, 1), (12)\nstarting from a simple prior distribution Y0 \u223c \u00b50, for L steps, with a small discretization step \u03b7 > 0. In practice, there have been developed a lot of methods, which improve or circumvent the procedure above by informative initialization (Hinton, 2002; Du & Mordatch, 2019), more sophisticated MCMC approaches (Lawson et al., 2019; Qiu et al., 2020; Nijkamp et al., 2022), regularizations (Du et al., 2021; Kumar et al., 2019), explicit auxiliary generators (Xie et al., 2018; Yin et al., 2022; Han et al., 2019; Gao et al., 2020). The application of these EBM improvements for the EOT problem is a fruitful avenue for future work. For a more in-depth discussion of the methods for training EBMs, see a recent survey (Song & Kingma, 2021)."
        },
        {
            "heading": "3 TAKING UP EOT PROBLEM WITH EBMS",
            "text": "In this section, we connect EBMs and the EOT problem and exhibit our proposed methodology. At first, we present some theoretical results which characterize weak dual objective (9) and its optimizers (\u00a73.1). Secondly, we develop the optimization procedure (\u00a73.2) and corresponding algorithm capable of implicitly recovering EOT plans. Thirdly, we establish generalization bounds for our proposed method (\u00a73.3). All proofs are situated in Appendix B."
        },
        {
            "heading": "3.1 ENERGY-GUIDED REFORMULATION OF WEAK DUAL EOT",
            "text": "We start our analysis by taking a close look at objective (9). The following proposition characterizes the inner min\u00b5x optimization problem arising in (9). Theorem 1 (Optimizer of weak CEOT-transform). Let f \u2208 C(Y) and x \u2208 X . Then inner weak dual objective min\u00b5\u2208P(Y) Gx,f (\u00b5) (8) permits the unique minimizer \u00b5fx which is given by\nd\u00b5fx(y)\ndy\ndef =\n1\nZ(f, x) exp\n( f(y)\u2212 c(x, y)\n\u03b5\n) , (13)\nwhere Z(f, x) def = \u222b Y exp ( f(y)\u2212c(x,y) \u03b5 ) dy.\nBy substituting minimizer (13) to (8), we obtain the close form for the weak CEOT-transform: fCEOT(x) = Gx,f (\u00b5fx) = \u2212\u03b5 logZ(f, x) = \u2212\u03b5 log (\u222b\nY exp\n( f(y)\u2212 c(x, y)\n\u03b5\n) dy ) . (14)\nThe equation (14) resembles (c, \u03b5)-transform (Genevay, 2019, Eq. 4.15) appearing in standard semi-dual EOT formulation (Genevay, 2019, \u00a74.3). For completeness, we shortly introduce\nthe dual EOT and semi-dual EOT problems in Appendix A, relegating readers to (Genevay, 2019) for a more thorough introduction. In short, it is the particular form of weak dual EOT objective, which differs from semi-dual EOT objective, and allows us to utilize EBMs, as we show in \u00a73.2. Thanks to (14), objective (9) permits the reformulation:\nEOTc,\u03b5(P,Q) = sup f\u2208C(Y) FwCEOT(f) = sup f\u2208C(Y)\n{ \u2212 \u03b5 \u222b X logZ(f, x)dP(x) + \u222b Y f(y)dQ(y) } . (15)\nFor a given f \u2208 C(Y), consider the distribution d\u03c0f (x, y) def= d\u00b5fx(y)dP(x). We prove, that the optimization of weak dual objective (15) brings \u03c0f closer to the optimal plan \u03c0\u2217. Theorem 2 (Bound on the quality of the plan recovered from the dual variable). For brevity, define the optimal value of (9) by Fw,\u2217CEOT def = EOTc,\u03b5(P,Q). For every f \u2208 C(Y) it holds that\nFw,\u2217CEOT \u2212 F w CEOT(f) = \u03b5 \u222b X KL ( \u03c0\u2217(\u00b7|x)\u2225\u00b5fx ) dP(x) = \u03b5KL ( \u03c0\u2217\u2225\u03c0f ) . (16)\nFrom our Theorem 2 it follows that given an approximate maximizer f of dual objective (15), one immediately obtains a distribution \u03c0f which is close to the optimal plan \u03c0\u2217. Actually, \u03c0f is formed by conditional distributions \u00b5fx (Theorem 1), whose energy functions are given by f (adjusted with transport cost c). Below we show that f in (15) can be optimized analogously to EBMs as well."
        },
        {
            "heading": "3.2 OPTIMIZATION PROCEDURE",
            "text": "Following the standard machine learning practices, we parameterize functions f \u2208 C(Y) as neural networks f\u03b8 with parameters \u03b8 \u2208 \u0398 and derive the loss function corresponding to (15) by:\nL(\u03b8) def = \u2212\u03b5 \u222b X logZ(f\u03b8, x)dP(x) + \u222b Y f\u03b8(y)dQ(y). (17)\nThe conventional way to optimize loss functions such as (17) is the stochastic gradient ascent. In the following result, we derive the gradient of L(\u03b8) w.r.t. \u03b8. Theorem 3 (Gradient of the weak dual loss L(\u03b8)). It holds true that:\n\u2202\n\u2202\u03b8 L(\u03b8) = \u2212 \u222b X \u222b Y [ \u2202 \u2202\u03b8 f\u03b8(y) ] d\u00b5f\u03b8x (y)dP(x) + \u222b Y \u2202 \u2202\u03b8 f\u03b8(y)dQ(y). (18)\nFormula (18) resembles the gradient of Energy-based loss, formula (11). This allows us to look at EOT problem (4) from the perspectives of EBMs. In order to emphasize the novelty of our approach, and, simultaneously, establish the deep connection between the optimization of weak dual objective in form (15) and EBMs, below we characterize the similarities and differences between standard EBMs optimization procedure and our proposed EOT-encouraged gradient ascent following \u2202L(\u03b8)/\u2202\u03b8 .\nDifferences. In contrast to the case of EBMs, potential f\u03b8, optimized by means of loss function L, does not represent an energy function by itself. However, the tandem of cost function c and f\u03b8 helps to recover the Energy functions of conditional distributions \u00b5f\u03b8x :\nE \u00b5 f\u03b8 x (y) = c(x, y)\u2212 f\u03b8(y) \u03b5 .\nTherefore, one can sample from distributions \u00b5f\u03b8x following ULA (12) or using more advanced MCMC approaches (Girolami & Calderhead, 2011; Hoffman et al., 2014; Samsonov et al., 2022). In practice, when estimating (18), we need samples (x1, y1), (x2, y2), . . . (xN , yN ) from distribution d\u03c0f\u03b8 (x, y)\ndef = d\u00b5f\u03b8x (y)dP(x). They could be derived through the simple two-stage procedure:\n1. Sample x1, . . . xN \u223c P , i.e., derive random batch from the source dataset. 2. Sample y1|x1 \u223c \u00b5f\u03b8x1 , . . . , yN |xN \u223c \u00b5 f\u03b8 xN , e.g., performing Langevin steps (12).\nSimilarities. Besides a slightly more complicated two-stage procedure for sampling from generative distribution \u03c0f\u03b8 , the gradient ascent optimization with (18) is similar to the gradient descent with\n(11). This allows a practitioner to adopt the existing practically efficient architectures of EBMs, e.g., (Du & Mordatch, 2019; Du et al., 2021; Gao et al., 2021; Zhao et al., 2021), in order to solve EOT.\nAlgorithm. We summarize our findings and detail our optimization procedure in the Algorithm 1. The procedure is basic, i.e., for the sake of simplicity, we specifically remove all technical tricks which are typically used when optimizing EBMs (persistent replay buffers (Tieleman, 2008), temperature adjusting, etc.). Particular implementation details are given in the experiments section (\u00a75).\nWe want to underline that our theoretical and practical setup allows performing theoretically-grounded truly conditional data generation by means of EBMs, which unlocks the data-to-data translation applications for the EBM community. Existing approaches leveraging such applications with Energyinspired methodology lack theoretical interpretability, see discussions in \u00a74.1.\nAlgorithm 1: Entropic Optimal Transport via Energy-Based Modelling Input :Source and target distributions P and Q, accessible by samples;\nEntropy regularization coefficient \u03b5 > 0, cost function c(x, y) : RDx \u00d7 RDy \u2192 R; number of Langevin steps K > 0, Langevin discretization step size \u03b7 > 0; basic noise std \u03c30 > 0; potential network f\u03b8 : RDy \u2192 R, batch size N > 0.\nOutput : trained potential network f\u03b8\u2217 recovering optimal conditional EOT plans for i = 1, 2, . . . do\nDerive batches {xn}Nn=1 = X \u223c P, {yn}Nn=1 = Y \u223c Q of sizes N; Sample basic noise Y (0) \u223c N (0, \u03c30) of size N; for k = 1, 2, . . . ,K do\nSample Z(k) = {z(k)n }Nn=1, where z (k) n \u223c N (0, 1); Obtain Y (k) = {y(k)n }Nn=1 with Langevin step: y (k) n \u2190 y(k\u22121)n + \u03b72\u03b5 \u00b7 stop_grad ( \u2202 \u2202y [f\u03b8(y)\u2212 c(xn, y)] \u2223\u2223 y=y (k\u22121) n ) + \u221a \u03b7z (k) n\nL\u0302\u2190 \u2212 1N [ \u2211 y (K) n \u2208Y (K) f\u03b8 ( y (K) n )] + 1N [ \u2211 yn\u2208Y f\u03b8 (yn) ] ;\nPerform a gradient step over \u03b8 by using \u2202L\u0302\u2202\u03b8 ;"
        },
        {
            "heading": "3.3 GENERALIZATION BOUNDS FOR LEARNED ENTROPIC PLANS",
            "text": "Below, we sort out the question of how far a recovered plan is from the true optimal plan \u03c0\u2217.\nIn practice, the source and target distributions are given by empirical samples XN = {xn}Nm=1 \u223c P and YM = {ym}Mm=1 \u223c Q, i.e., finite datasets. Besides, the available potentials f come from restricted functional class F \u2282 C(Y), e.g., f are neural networks. Therefore, in practice, we actually optimize the following empirical counterpart of the weak dual objective (15)\nmax f\u2208F\nF\u0302wCEOT(f) def = max\nf\u2208F\n{ \u2212 \u03b5 1\nN N\u2211 n=1 logZ(f, xn) + 1 M M\u2211 m=1 f(ym) } .\nand recover f\u0302 def= argmaxf\u2208F F\u0302 w CEOT (f). A question arises: how close is \u03c0f\u0302 to the OT plan \u03c0\u2217?\nOur Theorem 4 below characterizes the error between \u03c0f\u0302 and \u03c0\u2217 arising due to approximation (F is restricted), and estimation (finite samples of P,Q are available) errors. To bound the estimation error, we employ the well-known Rademacher complexity (Shalev-Shwartz & Ben-David, 2014, M26). Theorem 4 (Finite sample learning guarantees). Denote the functional class of weak CEOT-transforms corresponding to F by FCEOT = {\u2212\u03b5 logZ(f, \u00b7) : f \u2208 F}. Let RN (F ,Q) and RM (FCEOT ,P) denote the Rademacher complexities of functional classes F and FCEOT w.r.t. Q and P for sample sizes N and M , respectively. Then the following upper bound on the error between \u03c0\u2217 and \u03c0f\u0302 holds:\nE [ KL ( \u03c0\u2217\u2225\u03c0f\u0302 ) ] \u2264 Estimation error\ufe37 \ufe38\ufe38 \ufe37 \u03b5\u22121 [ 4RN (FCEOT ,P) + 4RM (F ,Q) ] + Approximation error\ufe37 \ufe38\ufe38 \ufe37 \u03b5\u22121 [ Fw,\u2217CEOT \u2212maxf\u2208F F w CEOT(f) ] . (19)\nwhere the expectation is taken over random realizations of datasets XN \u223c P, YM \u223c Q of sizes N,M .\nWe note that there exist many statistical bounds for EOT (Genevay, 2019; Genevay et al., 2019; Rigollet & Stromme, 2022; Mena & Niles-Weed, 2019; Luise et al., 2019; del Barrio et al., 2023), yet they are mostly about sample complexity of EOT, i.e., estimation of the OT cost value, or accuracy of the estimated barycentric projection x 7\u2192 \u222b Y y d\u03c0\n\u2217(y|x) in the non-parametric setup. In contrast to these works, our result is about the estimation of the entire OT plan \u03c0\u2217 in the parametric setup. Our Theorem 4 could be improved by deriving explicit numerical bounds. This can be done by analyzing particular NNs architectures, similar to (Klusowski & Barron, 2018; Sreekumar et al., 2021). We leave the corresponding analysis to follow-up research."
        },
        {
            "heading": "4 RELATED WORKS",
            "text": "In this section, we look over existing works which are the most relevant to our proposed method. We divide our survey into two main parts. Firstly, we discuss the EBM approaches which tackle similar practical problem setups. Secondly, we perform an overview of solvers dealing with Entropyregularized OT. The discussion of general-purpose OT solvers is available in Appendix A.2."
        },
        {
            "heading": "4.1 ENERGY-BASED MODELS FOR UNPAIRED DATA-TO-DATA TRANSLATION",
            "text": "Given a source and target domains X and Y , accessible by samples, the problem of unpaired datato-data translation (Zhu et al., 2017) is to transform a point x \u2208 X from the source domain to corresponding points yx1 , y x 2 , \u00b7 \u00b7 \u00b7 \u2282 Y from the target domain while \u201cpreserving\u201d some notion of x\u2019s content. In order to solve this problem, (Zhao & Chen, 2021; Zhao et al., 2021) propose to utilize a pretrained EBM of the target distribution Q, initialized by source samples x \u223c P. In spite of plausibly-looking practical results, the theoretical properties of this approach remain unclear. Furthermore, being passed through MCMC, the obtained samples may lose the conditioning on the source samples. In contrast, our proposed approach is free from the aforementioned problems and can be tuned to reach the desired tradeoff between the conditioning power and data variability. The authors of (Xie et al., 2021) propose to cooperatively train CycleGAN and EBMs to solve unpaired I2I problems. However, in their framework, EBMs just help to stabilize the training of I2I maps and can not be considered as primal problem solvers."
        },
        {
            "heading": "4.2 ENTROPY-REGULARIZED OT",
            "text": "To the best of our knowledge, all continuous EOT solvers are based either on KL-guided formulation (2) (Genevay et al., 2016; Seguy et al., 2018; Daniels et al., 2021) or unconditional entropic one (3) with its connection to the Schr\u00f6dinger bridge problem (Finlay et al., 2020; Bortoli et al., 2021; Gushchin et al., 2023; Chen et al., 2022; Shi et al., 2023). Our approach seems to be the first which takes advantage of conditional entropic formulation (4). Methods (Genevay et al., 2016; Seguy et al., 2018; Daniels et al., 2021) exploit dual form of (2), see (Genevay, 2019, Eq. 4.2), which is an unconstrained optimization problem w.r.t. a couple of dual potentials (u, v). However, (Genevay et al., 2016; Seguy et al., 2018) do not provide a direct way for sampling from optimal conditional plans \u03c0\u2217(y|x), since it requires the knowledge of target distribution Q. In order to leverage this issue, (Daniels et al., 2021) proposes to employ a separate score-based model approximating Q. At the inference stage (Daniels et al., 2021) utilizes MCMC sampling, which makes this work to be the closest to ours. The detailed comparison is given below:\n1. The authors of (Daniels et al., 2021) optimize dual potentials (u, v) following the dual form of (2). This procedure is unstable for small \u03b5 as it requires the exponentiation of large numbers which are of order \u03b5\u22121. At the same time, a \u201csmall \u03b5\u201d regime is practically important for some downstream applications when one needs a close-to-deterministic plan between X and Y domains. On the contrary, our Energy-based approach does not require exponent computation and can be adapted for a small \u03b5 by proper adjusting of ULA (12) parameters (step size, number of steps, etc.).\n2. In (Daniels et al., 2021), it is mandatory to have three models, including a third-party score-based model. Our algorithm results in a single potential f\u03b8 capturing all the information about the OT conditional plans and only optionally may be combined with an extra generative model (M5.3).\nThe alternative EOT solvers (Finlay et al., 2020; Bortoli et al., 2021; Gushchin et al., 2023; Chen et al., 2022; Vargas et al., 2021; Shi et al., 2023) are based on the connection between primal EOT (3) and the Schr\u00f6dinger bridge problem. The majority of these works model the EOT plan as a\ntime-dependent stochastic process with learnable drift and diffusion terms, starting from P at the initial time and approaching Q at the final time. This requires resource-consuming techniques to solve stochastic differential equations. Moreover, the aforementioned methods work primarily with the quadratic cost and hardly could be accommodated for a more general case."
        },
        {
            "heading": "5 EXPERIMENTAL ILLUSTRATIONS",
            "text": "In what follows, we demonstrate the performance of our method on toy 2D scenario, Gaussian-toGaussian and high-dimensional AFHQ Cat/Wild\u2192Dog image transformation problems solved using the latent space of a pretrained StyleGAN2-ADA (Karras et al., 2020). In the first two experiments the cost function is chosen to be squared halved l2 norm: c(x, y) = 12\u2225x\u2212 y\u2225 2 2, while in the latter case, it is more tricky and involves StyleGAN generator. An additional experiment with Colored MNIST images translation setup is considered in Appendix C.1.\nOur code is written in PyTorch and publicly available at https://github.com/ PetrMokrov/Energy-guided-Entropic-OT. The actual neural network architectures as well as practical training setups are disclosed in the corresponding subsections of Appendix D."
        },
        {
            "heading": "5.1 TOY 2D",
            "text": "We apply our method for 2D Gaussian\u2192Swissroll case and demonstrate the qualitative results on Figure 2 for Entropy regularization coefficients \u03b5 = 0.1, 0.001. Figure 2b shows that our method succeeds in transforming source distribution P to target distribution Q for both Entropy regularization coefficients. In order to ensure that our approach learns optimal conditional plans \u03c0\u2217(y|x) well, and correctly solves EOT problem, we provide Figures 2c and 2d. On these images, we pick several points x \u2208 X and demonstrate samples from the conditional plans \u03c0( \u00b7 |x), obtained either by our method (\u03c0(\u00b7|x) = \u00b5f\u03b8x ) or by discrete EOT solver (Flamary et al., 2021). In contrast to our approach, the samples generated by the discrete EOT solver come solely from the training dataset. Yet these samples could be considered as a fine approximation of ground truth in 2D."
        },
        {
            "heading": "5.2 GAUSSIAN-TO-GAUSSIAN",
            "text": "Here we validate our method in Gaussian-to-Gaussian transformation tasks in various dimensions (Dx = Dy = 2, 16, 64, 128), for which the exact optimal EOT plans are analytically known (Janati et al., 2020). We choose \u03b5 = 0.1, 1, 10 and compare the performance of our approach with those, described in \u00a74.2. We report the BW22 -UVP metric, see Appendix D.2 for the explanation, between the learned \u03c0\u0302 and optimal \u03c0\u2217 plans in Table 1. As we can see, our method manages to recover the optimal plan rather well compared to baselines. Technical peculiarities are disclosed in Appendix D."
        },
        {
            "heading": "5.3 HIGH-DIMENSIONAL UNPAIRED IMAGE-TO-IMAGE TRANSLATION",
            "text": "Left: source; right: translated.\nIn this subsection, we deal with the large-scale unpaired I2I setup. As with many other works in EBMs, e.g., (Zhao & Chen, 2021; Tiwary et al., 2022), we consider learning in the latent space. We pick a pre-trained StyleGANV2-Ada (Karras et al., 2020) generator G for Dogs AFHQ 512\u00d7 512 dataset and consider Cat\u2192Dog (and Wild\u2192Dog) unpaired translation. As P, we use the dataset of images of cats (or wild); as Q, we useN (0, I512), i.e., the latent distribution of the StyleGAN. We use our method with \u03f5 = 1 to\nlearn the EOT between P and Q with cost 12\u2225x\u2212G(z)\u2225\n2, i.e., \u21132 between the input image and the image generated from the latent code z. Note that our method trains only one MLP network f\u03b8 acting on the latent space, which is then used for inference (combined with G). Moreover, our approach does not need a generative model of the source distribution P, and does not need encoder (data2latent) networks. The qualitative results are provided in Figures 1 and 3. Our method allows us to translate the images from one domain to the other while maintaining the similarity with the input image. For more examples and qualitative comparisons, see Appendix C.2. For the quantitative analysis, we compare our approach with popular unpaired I2I models ILVR (Choi et al., 2021), SDEdit (Meng et al., 2022), EGSDE (Zhao et al., 2022), CycleGAN (Zhu et al., 2017), MUNIT (Huang et al., 2018) and StarGANv2 (Choi et al., 2020), the obtained FID metrics are reported in Table 2. As we can see, our approach achieves comparable-to-SOTA quality."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "Our work paves a principled connection between EBMs and EOT. The latter is an emergent problem in generative modelling, with potential applications like unpaired data-to-data translation (Korotin et al., 2023b). Our proposed EBM-based learning method for EOT is theoretically grounded and we provide proof-of-concept experiments. We believe that our work will inspire future studies that will further empower EOT with recent EBMs capable of efficiently sorting out truly large-scale setups (Du et al., 2021; Gao et al., 2021; Zhao et al., 2021).\nThe limitations of our method roughly match those of basic EBMs. Namely, our method requires using MCMC methods for training and inference. This may be time-consuming. For the extended discussion of limitations, see Appendix F.\nThe broader impact of our work is the same as that of any generative modelling research. Generative models may be used for rendering, image editing, design, computer graphics, etc. and simplify the existing digital content creation pipelines. At the same time, it should be taken into account that the rapid development of generative models may also unexpectedly affect some jobs in the industry.\n1FID scores of the baselines are taken from (Zhao et al., 2022). In order to estimate FID, we downscale the generated images to 256\u00d7 256 for a fair comparison with the alternative methods."
        },
        {
            "heading": "7 ACKNOWLEDGEMENTS",
            "text": "The work was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021)."
        },
        {
            "heading": "A EXTENDED BACKGROUND AND RELATED WORKS",
            "text": ""
        },
        {
            "heading": "A.1 DUAL/SEMI-DUAL EOT PROBLEMS AND THEIR RELATION TO WEAK DUAL EOT PROBLEM",
            "text": "Dual formulation of the EOT problem. Primal EOT problem (2) has the dual reformulation (Genevay et al., 2016). Let u \u2208 C(X ) and v \u2208 C(Y). Define the dual functional by\nF\u03b5(u, v) def = \u222b X u(x)dP(x)+ \u222b Y v(y)dQ(y)\u2212 \u03b5 \u222b X\u00d7Y exp ( u(x) + v(y)\u2212 c(x, y) \u03b5 ) d [P\u00d7Q](x, y).\n(20)\nThen the strong duality holds (Genevay et al., 2016, Proposition 2.1), i.e.,\nEOT(1)c,\u03b5(P,Q) = sup u\u2208C(X ),v\u2208C(Y) F\u03b5(u, v). (21)\nThe sup here may not be attained in C(X ), C(Y), yet it is common to relax the formulation and consider u \u2208 L\u221e(P) and v \u2208 L\u221e(Q) instead (Marino & Gerolin, 2020). In this case, the sup becomes max (Genevay, 2019, Theorem 7). The potentials u\u2217, v\u2217 which constitute a solution of the relaxed (21) are called the (Entropic) Kantorovich potentials. The optimal transport plan \u03c0\u2217 which solves the primal problem (2) could be recovered from a pair of Kantorovich potentials (u\u2217, v\u2217) as\nd\u03c0\u2217(x, y) = exp\n( u\u2217(x) + v\u2217(y)\u2212 c(x, y)\n\u03b5\n) dP(x)dQ(y). (22)\nFrom the practical viewpoint, the dual objective (21) is an unconstrained maximization problem which can be solved by conventional optimization procedures. The existing methods based on (21) as well as their limitations and drawbacks are discussed in the related works section, \u00a74.2.\nSemi-dual formulation of the EOT problem. Objective (21) is a convex optimization problem. By fixing a function v \u2208 C(Y) and applying the first-order optimality conditions for the marginal optimization problem supu\u2208C(X ) F\u03b5(u, v), one can recover the solution of\nvc,\u03b5 def = argmax\nu\u2208C(X ) F\u03b5(u, v) (23)\nin the closed-form: vc,\u03b5(x) = \u2212\u03b5 log (\u222b\nY exp\n( v(y)\u2212 c(x, y)\n\u03b5\n) dQ(y) ) . (24)\nFunction vc,\u03b5 is called (c, \u03b5)-transform (Genevay, 2019). By substituting the argument u \u2208 C(X ) of the max with vc,\u03b5 in equation (21) and performing several simplifications, one can recover the objective\nEOT(1)c,\u03b5(P,Q) = sup v\u2208C(Y) \u222b X vc,\u03b5(x)dP(x) + \u222b Y v(y)dQ(y). (25)\nThis one is called the semi-dual formulation of EOT problem (Genevay, 2019, \u00a74.3). It is not so popular as the classical dual problem (21) since the estimation of vc,\u03b5 is non-trivial, yet it has a direct relation to formulation (9), which forms the basis of our proposed method. In order to comprehend this relation, below we compare (c, \u03b5)-transform and min\u00b5 Gx,f (\u00b5), given by (14). Correspondence between (semi-) dual EOT and weak dual EOT. As we already pointed out in the main part of the manuscript, equation (14) which then appears in weak dual objective (15) looks similar to (c, \u03b5)-transform (24). The difference is the integration measure, which is Q in the case of (24) and the standard Lebesgue one in our case (14).\nFrom the theoretical point of view, such dissimilarity is not significant. That is why semi-dual (25) and weak dual (9) optimization problems are expected to share such properties as convergence, existence of optimizers and so on. In particular, there is a relation between potentials u, v that appear in (semi-) dual EOT problems (21, 25), and our optimized potential f from (9).\nLet Q \u2208 Pac(Y), and EQ : Y \u2192 R be the energy function of Q, i.e., dQ(y)dy \u221d exp (\u2212EQ(y)). Consider the parameterization of potentials v by means of f as follows:\nv(y)\u2190 f(y) + \u03b5EQ(y). (26)\nThen, vc,\u03b5(x) = \u2212\u03b5 log (\u222b\nY exp\n( v(y)\u2212 c(x, y)\n\u03b5\n) dQ(y) ) = \u2212\u03b5 log\n(\u222b Y exp ( f(y)\u2212 c(x, y) \u03b5 ) exp (EQ(y)) dQ(y) dy dy ) = \u2212\u03b5 log\n(\u222b Y exp ( f(y)\u2212 c(x, y) \u03b5 ) exp (EQ(y)) exp (\u2212EQ(y)) dy ) + Const(Q)\nsee (14) = \u2212\u03b5 logZ(f, x) + Const(Q). (27)\nBy substituting (26, 27) in (25) we obtain that semi-dual EOT objective (25) recovers our weak dual objective (15) up to reparameterization (26):\nEOT(1)c,\u03b5(P,Q) = sup v\u2208C(Y) {\u222b X vc,\u03b5(x)dP(x) + \u222b Y v(y)dQ(y) } = Const1(Q)\ufe38 \ufe37\ufe37 \ufe38\n=\u03b5H(Q)\n+ sup f\u2208C(Y)\u2212\u03b5EQ {\u222b X [ \u2212 \u03b5 logZ(f, x) ] dP(x) + \u222b Y f(y)dQ(y) } .\nStrictly speaking, the optimization class C(Y)\u2212\u03b5EQ = {f\u2212\u03b5EQ, f \u2208 C(Y)} in the equation above is different from C(Y) that appears in (15). However, under mild assumptions on EQ the corresponding optimization problems are similar. One important consequence of the observed equivalence is the existence of optimal potential f\u2217 (not necessary to be continuous) which solves weak dual objective (9). It can be expressed through optimal Kantorovich potential v\u2217 by f\u2217 = v\u2217 \u2212 \u03b5EQ. From the practical point of view, the difference between (14) and (24) is much more influential. Actually, it is the particular form of internal weak dual problem solution (14) that allows us to utilize EBMs, see \u00a73.2."
        },
        {
            "heading": "A.2 DISCRETE AND CONTINUOUS OT SOLVERS REVIEW",
            "text": "Discrete OT. The discrete OT (DOT) is the specific domain in OT research area, which deals with distributions supported on finite discrete sets. There have been developed various methods for solving DOT problems (Peyr\u00e9 et al., 2019), the most efficient of which deals with discrete EOT (Cuturi, 2013). In spite of good theoretically-grounded convergence guarantees, it is hard to adopt the DOT solvers for out-of-distribution sampling and mapping, which limits their applicability in some real-world scenarios.\nContinuous OT. In the continuous setting, the source and target distributions become accessible only by samples from (limited) budgets. In this case, OT plans are typically parameterized with neural networks and optimized with the help of SGD-like methods by deriving random batches from the datasets. The approaches dealing with such practical setup are called continuous OT solvers.\nThere exists a lot of continuous OT solvers (Makkuva et al., 2020; Korotin et al., 2021a; Fan et al., 2023; Xie et al., 2019; Rout et al., 2022; Gazdieva et al., 2022; Korotin et al., 2022b; Taghvaei & Jalali, 2019; Liu et al., 2023). However, the majority of these methods model OT as a deterministic map which for each input point x assigns a single data point y rather than distribution \u03c0(y|x). Only a limited number of approaches are capable of solving OT problems which require stochastic mapping, and, therefore, potentially applicable for our EOT case. Here we exclude methods designed specifically for EOT and cover them in the further narrative.\nThe recent line of works (Korotin et al., 2023b;a; Asadulaev et al., 2024) considers dual formulation of weak OT problem (Gozlan et al., 2017) and comes up with maxmin objective for various weak (Korotin et al., 2023b;a) and even general (Asadulaev et al., 2024) cost functionals. However, their proposed methodology requires the estimation of weak cost by samples, which complicates its application for EOT. An alternative concept (Xie et al., 2019) works with primal OT formulation (1) and lifts boundary source and target distribution constraints by WGAN losses. It also utilizes sample estimation of corresponding functionals and can not be directly adapted for EOT setup."
        },
        {
            "heading": "B PROOFS",
            "text": ""
        },
        {
            "heading": "B.1 PROOF OF THEOREM 1",
            "text": "Proof. In what follows, we analyze the optimizers of the objective min\u00b5\u2208P(Y) Gx,f (\u00b5) introduced in (8). Let \u00b5 \u2208 P(Y). We have:\nGx,f (\u00b5) = \u222b Y c(x, y)d\u00b5(y) + \u03b5 \u222b Y log d\u00b5(y) dy d\u00b5(y)\u2212 \u222b Y f(y)d\u00b5(y)\n= \u03b5 \u222b Y ( c(x, y)\u2212 f(y) \u03b5 + log d\u00b5(y) dy ) d\u00b5(y)\n= \u03b5 \u222b Y ( \u2212 logZ(f, x)\ufe38 \ufe37\ufe37 \ufe38\ndoes not depend on y\n+ logZ(f, x)\u2212 f(y)\u2212 c(x, y) \u03b5\ufe38 \ufe37\ufe37 \ufe38\n=\u2212 log d\u00b5 f x(y) dy\n+ log d\u00b5(y)\ndy\n) d\u00b5(y)\n= \u2212\u03b5 logZ(f, x) + \u03b5 \u222b Y ( \u2212 log d\u00b5 f x(y) dy + log d\u00b5(y) dy ) d\u00b5(y)\n= \u2212\u03b5 logZ(f, x) + \u03b5KL ( \u00b5\u2225\u00b5fx ) . (28)\nThe last equality holds true thanks to the fact that \u00b5fx \u2208 Pac(Y) and \u2200y \u2208 Y : d\u00b5fx(y)\ndy > 0. This leads to the conclusion that the absolute continuity of \u00b5 (\u00b5\u226a \u03bb, where \u03bb is the Lebesgue measure on Y) is equivalent to the absolute continuity of \u00b5 w.r.t. \u00b5fx (\u00b5\u226a \u00b5fx). In particular, if \u00b5 /\u2208 Pac(Y), then the last equality in the derivations above reads as +\u221e = +\u221e. From (28) we conclude that \u00b5fx = argmin\n\u00b5\u2208P(Y) Gx,f (\u00b5)."
        },
        {
            "heading": "B.2 PROOF OF THEOREM 2",
            "text": "Proof. Recall that we denote the optimal value of weak dual objective (9) by Fw,\u2217CEOT . It equals to EOTc,\u03b5(P,Q) given by formula (4), thanks to the strong duality, i.e.,\nFw,\u2217CEOT = \u222b X\u00d7Y c(x, y)d\u03c0\u2217(x, y)\u2212 \u03b5 \u222b X H(\u03c0\u2217(y|x))dP(x). (29)\nFor a potential f \u2208 C(Y), our Theorem 1 yields:\nFwCEOT(f)= \u222b X Gx,f (\u00b5fx)dP(x)+ \u222b Y f(y)dQ(y)Eq. (14)= \u2212\u03b5 \u222b X logZ(f, x)dP(x)+ \u222b Y f(y)dQ(y). (30)\nIn what follows, we combine (29) and (30):\nFw,\u2217CEOT \u2212 F w CEOT(f) =\u222b\nX\u00d7Y c(x, y)d\u03c0\u2217(x, y)\u2212\u03b5 \u222b X H(\u03c0\u2217(y|x))dP(x)+\u03b5 \u222b X logZ(f, x) dP(x)\ufe38 \ufe37\ufe37 \ufe38\n=d\u03c0\u2217(x)\n\u2212 \u222b Y f(y) dQ(y)\ufe38 \ufe37\ufe37 \ufe38\n=d\u03c0\u2217(y)\n=\n\u222b X\u00d7Y [ c(x, y)\u2212 f(y) ] d\u03c0\u2217(x, y)\u2212 \u03b5 \u222b X H(\u03c0\u2217(y|x))dP(x) + \u03b5 \u222b X logZ(f, x)d\u03c0\u2217(x) =\n\u2212\u03b5 \u222b X\u00d7Y f(y)\u2212 c(x, y) \u03b5\ufe38 \ufe37\ufe37 \ufe38\n=log exp( f(y)\u2212c(x,y)\u03b5 )\nd\u03c0\u2217(x, y) + \u03b5 \u222b X\u00d7Y\nlog Z(f, x)\ufe38 \ufe37\ufe37 \ufe38 = \u222b Y exp( f(y)\u2212c(x,y) \u03b5 )dy\nd\u03c0\u2217(x, y)\u2212 \u03b5 \u222b X H(\u03c0\u2217(y|x))dP(x) =\n\u2212\u03b5 {\u222b X\u00d7Y log [ 1 Z(f, x) exp ( f(y)\u2212 c(x, y) \u03b5 ) \ufe38 \ufe37\ufe37 \ufe38\n= d\u00b5 f x(y) dy\n] d\u03c0\u2217(x, y) } \u2212 \u03b5 \u222b X H(\u03c0\u2217(y|x))dP(x) =\n\u2212\u03b5 \u222b X\u00d7Y log ( d\u00b5fx(y) dy ) d\u03c0\u2217(x, y)\u2212 \u03b5 \u222b X H(\u03c0\u2217(y|x))dP(x) =\n\u2212\u03b5 \u222b X \u222b Y log ( d\u00b5fx(y) dy ) d\u03c0\u2217(y|x) d\u03c0\u2217(x)\ufe38 \ufe37\ufe37 \ufe38\n=dP(x)\n+\u03b5 \u222b X \u222b Y log ( d\u03c0\u2217(y|x) dy ) d\u03c0\u2217(y|x)dP(x) =\n\u03b5 \u222b X {\u222b Y [ log ( d\u03c0\u2217(y|x) dy ) \u2212 log ( d\u00b5fx(y) dy )] d\u03c0\u2217(y|x) } dP(x) =\n\u03b5 \u222b X KL ( \u03c0\u2217(\u00b7|x)\u2225\u00b5fx ) dP(x) .\nIn the last transition of the derivations above, we use the equality\u222b Y [ log ( d\u03c0\u2217(y|x) dy ) \u2212 log ( d\u00b5fx(y) dy )] d\u03c0\u2217(y|x) = KL ( \u03c0\u2217(\u00b7|x)\u2225\u00b5fx ) .\nIt holds true due to the same reasons, as explained in the proof of Theorem 1, after formula (28). We are left to show that \u222b X KL ( \u03c0\u2217(\u00b7|x)\u2225\u00b5fx ) dP(x) = KL ( \u03c0\u2217\u2225\u03c0f ) . Since d\u03c0\u2217(x, y) = d\u03c0\u2217(y|x)dP(x) and d\u03c0f (x, y) = d\u00b5fx(y)dP(x), we derive:\u222b X KL ( \u03c0\u2217(\u00b7|x)\u2225\u00b5fx ) dP(x) = \u222b X \u222b Y log ( d\u03c0\u2217(y|x) d\u00b5fx(y) ) d\u03c0\u2217(y|x)dP(x) =\n\u222b X \u222b Y log ( d\u03c0\u2217(y|x)dP(x) d\u00b5fx(y)dP(x) ) d\u03c0\u2217(y|x)dP(x) =\u222b\nX\u00d7Y log\n( d\u03c0\u2217(x, y)\nd\u03c0f (x, y)\n) d\u03c0\u2217(x, y) = KL ( \u03c0\u2217\u2225\u03c0f ) ,\nwhich completes the proof."
        },
        {
            "heading": "B.3 PROOF OF THEOREM 3",
            "text": "Proof. The direct derivations for (18) read as follows:\n\u2202\n\u2202\u03b8 L(\u03b8) = \u2212\u03b5 \u222b X \u2202 \u2202\u03b8 logZ(f\u03b8, x)dP(x) + \u222b Y \u2202 \u2202\u03b8 f\u03b8(y)dQ(y) =\n\u2212\u03b5 \u222b X\n1\nZ(f\u03b8, x)\n{ \u2202\n\u2202\u03b8 \u222b Y exp ( f\u03b8(y)\u2212 c(x, y) \u03b5 ) dy } dP(x) + \u222b Y \u2202 \u2202\u03b8 f\u03b8(y)dQ(y) =\n\u2212\u03b5 \u222b X { 1 Z(f\u03b8, x) \u222b Y [ \u2202 \u2202\u03b8f\u03b8(y) \u03b5 ] exp ( f\u03b8(y)\u2212 c(x, y) \u03b5 ) dy } dP(x) + \u222b Y \u2202 \u2202\u03b8 f\u03b8(y)dQ(y) =\n\u2212 \u222b X {\u222b Y [ \u2202 \u2202\u03b8 f\u03b8(y) ] 1 Z(f\u03b8, x) exp ( f\u03b8(y)\u2212 c(x, y) \u03b5 ) dy\ufe38 \ufe37\ufe37 \ufe38\n=d\u00b5 f\u03b8 x (y)\n} dP(x) + \u222b Y \u2202 \u2202\u03b8 f\u03b8(y)dQ(y) =\n\u2212 \u222b X \u222b Y [ \u2202 \u2202\u03b8 f\u03b8(y) ] d\u00b5f\u03b8x (y)dP(x) + \u222b Y \u2202 \u2202\u03b8 f\u03b8(y)dQ(y) ,\nwhich finishes the proof."
        },
        {
            "heading": "B.4 PROOF OF THEOREM 4",
            "text": "To begin with, for the ease of reading and comprehension, we recall the statistical learning setup from \u00a73.3. In short, XN and YM are empirical samples from P and Q, respectively, F \u2282 C(Y) is a function class in which we are looking for a potential f\u0302 , which optimizes the empirical weak dual objective\nF\u0302wCEOT(f) = \u2212\u03b5 1\nN N\u2211 n=1 logZ(f, xn) + 1 M M\u2211 m=1 f(ym),\ni.e., f\u0302 = argmaxf\u2208F F\u0302 w CEOT (f). Additionally, we introduce fF def= argmaxf\u2208F F w CEOT\n(f). It optimizes weak dual objective (9) in the function class under consideration. Following statistical generalization practices, our analysis utilizes Rademacher complexity. We recall the definition below. Definition 1 (Rademacher complexityRN (F , \u00b5)). Let F be a function class and \u00b5 be a distribution. The Rademacher complexity of F with respect to \u00b5 of sample size N is\nRN (F , \u00b5) def =\n1 N E { sup f\u2208F N\u2211 n=1 f(xn)\u03c3n } ,\nwhere {xn}Nn=1 \u223c \u00b5 are mutually independent, {\u03c3n}Nn=1 are mutually independent Rademacher random variables, i.e., Prob ( {\u03c3n = 1} ) = Prob ( {\u03c3n = \u22121} ) = 0.5, and the expectation is taken with respect to both {xn}Nn=1 and {\u03c3n}Nn=1.\nNow we are ready to verify our Theorem 4.\nProof. Our Theorem 2 yields that \u03b5KL ( \u03c0\u2217\u2225\u03c0f\u0302 ) = Fw,\u2217CEOT \u2212 F w CEOT(f\u0302).\nBelow we upper-bound the right-hand side of the equation above.\nFw,\u2217CEOT \u2212 F w CEOT(f\u0302) =\nFw,\u2217CEOT \u2212 F w CEOT(f F ) + FwCEOT(f F )\u2212 F\u0302wCEOT(f\u0302) + F\u0302 w CEOT(f\u0302)\u2212 F w CEOT(f\u0302) \u2264\u2223\u2223Fw,\u2217CEOT \u2212 FwCEOT(fF )\u2223\u2223 + (31)\u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223 + (32)\u2223\u2223F\u0302wCEOT(f\u0302)\u2212 FwCEOT(f\u0302)\u2223\u2223 . (33)\nAnalysis of (31). Equation (31) relates to approximation error and depends on the richness of class F . The detailed investigation of how could the approximation error be treated in the context of our Energy-guided EOT setup and, more generally, EBMs is an interesting avenue for future work.\nAnalysis of (32). Similar to (Taghvaei & Jalali, 2019, Theorem 3.4), we estimate (32) using the Rademacher complexity bounds. First, we need the following technical Lemma.\nLemma 1. For each particular samples XN , YM , there exists f\u0303 \u2208 F , such that:\u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223 \u2264 \u2223\u2223FwCEOT(f\u0303)\u2212 F\u0302wCEOT(f\u0303)\u2223\u2223 Proof. Let\u2019s consider\n\u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223. There are two possibilities. 1. FwCEOT(f\nF ) \u2265 F\u0302wCEOT(f\u0302). Since \u2200f \u2208 F : F\u0302wCEOT(f\u0302) \u2265 F\u0302 w CEOT (f), and, in particular, F\u0302wCEOT(f\u0302) \u2265 F\u0302 w CEOT\n(fF ), it holds:\u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223 = FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302) \u2264 FwCEOT(f F )\u2212 F\u0302wCEOT(f F ) \u2264\n\u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(fF )\u2223\u2223, i.e., we set f\u0303 \u2190 fF .\n2. F\u0302wCEOT(f\u0302) > F w CEOT (fF ). Similar to the previous case, \u2200f \u2208 F : FwCEOT(f F ) \u2265 FwCEOT(f) \u21d2 F w CEOT\n(fF ) \u2265 FwCEOT(f\u0302). Analogous derivations read as:\u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223 = F\u0302wCEOT(f\u0302)\u2212 FwCEOT(fF ) \u2264\nF\u0302wCEOT(f\u0302)\u2212 F w CEOT(f\u0302) \u2264 \u2223\u2223FwCEOT(f\u0302)\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223, i.e., we set f\u0303 \u2190 f\u0302 and finish the proof of the Lemma.\nNow we continue the proof of our Theorem. For particular samples XN and YM , consider f\u0303 from Lemma 1. We derive: \u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223 \u2264 \u2223\u2223FwCEOT(f\u0303)\u2212 F\u0302wCEOT(f\u0303)\u2223\u2223 =\u2223\u2223\u2223\u2223\u222b\nX\n[ \u2212\u03b5 logZ(f\u0303 , x) ] dP(x)+ \u222b Y f\u0303(y)dQ(y)\u2212 { N\u2211 n=1 \u2212\u03b5 logZ(f\u0303 , xn) N + M\u2211 m=1 f\u0303(ym) M }\u2223\u2223\u2223\u2223 =\u2223\u2223\u2223\u2223{\u222b X [ \u2212\u03b5 logZ(f\u0303 , x) ] dP(x)\u2212 N\u2211 n=1 \u2212\u03b5 logZ(f\u0303 , xn) N } + {\u222b Y f\u0303(y)dQ(y)\u2212 M\u2211 m=1 f\u0303(ym) M\n}\u2223\u2223\u2223\u2223 \u2264\u2223\u2223\u2223\u2223\u222b X [ \u2212\u03b5 logZ(f\u0303 , x) ] dP(x)\u2212 N\u2211 n=1 \u2212\u03b5 logZ(f\u0303 , xn) N \u2223\u2223\u2223\u2223+\u2223\u2223\u2223\u2223\u222b Y f\u0303(y)dQ(y)\u2212 M\u2211 m=1 f\u0303(ym) M\n\u2223\u2223\u2223\u2223 \u2264 sup f\u2208F \u2223\u2223\u2223\u2223\u222b X fCEOT(x)dP(x)\u2212 N\u2211 n=1 fCEOT(xn) N \u2223\u2223\u2223\u2223+ sup f\u2208F \u2223\u2223\u2223\u2223\u222b Y f(y)dQ(y)\u2212 M\u2211 m=1 f(ym) M\n\u2223\u2223\u2223\u2223 = sup\nh\u2208FCEOT \u2223\u2223\u2223\u2223\u222b X h(x)dP(x)\u2212 N\u2211 n=1 h(xn) N \u2223\u2223\u2223\u2223+ sup f\u2208F \u2223\u2223\u2223\u2223\u222b Y f(y)dQ(y)\u2212 M\u2211 m=1 f(ym) M \u2223\u2223\u2223\u2223 . Recall that FCEOT = {fCEOT : f \u2208 F} = {\u2212\u03b5 logZ(f, \u00b7) : f \u2208 F}. Thanks to well-known Rademacher bound (Shalev-Shwartz & Ben-David, 2014, Lemma 26.2), it holds:\nE { sup\nh\u2208FCEOT \u2223\u2223\u2223\u2223\u222b X h(x)dP(x)\u2212 N\u2211 n=1 h(xn) N \u2223\u2223\u2223\u2223 } \u2264 2RN (FCEOT ,P),\nE { sup f\u2208F \u2223\u2223\u2223\u2223\u222b Y f(y)dQ(y)\u2212 M\u2211 m=1 f(ym) M \u2223\u2223\u2223\u2223 } \u2264 2RM (F ,Q),\nwhere the expectations are taken with respect to samples XN and YM . Combining the results above, we conclude:\nE \u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223 \u2264 2RN (FCEOT ,P) + 2RM (F ,Q). (34)\nAnalysis of (33). Similar to the previous case, we obtain the inequality:\u2223\u2223F\u0302wCEOT(f\u0302)\u2212 FwCEOT(f\u0302)\u2223\u2223 \u2264 sup\nh\u2208FCEOT \u2223\u2223\u2223\u2223\u222b X h(x)dP(x)\u2212 N\u2211 n=1 h(xn) N \u2223\u2223\u2223\u2223+ sup f\u2208F \u2223\u2223\u2223\u2223\u222b Y f(y)dQ(y)\u2212 M\u2211 m=1 f(ym) M \u2223\u2223\u2223\u2223 . Therefore,\nE \u2223\u2223F\u0302wCEOT(f\u0302)\u2212 FwCEOT(f\u0302)\u2223\u2223 \u2264 2RN (FCEOT ,P) + 2RM (F ,Q). (35)\nBy gathering equations (31, 34, 35), we prove the theorem: E [ KL ( \u03c0\u2217\u2225\u03c0f\u0302 ) ] = \u03b5\u22121E { Fw,\u2217CEOT \u2212 F w CEOT(f\u0302) } \u2264\n\u03b5\u22121 \u2223\u2223Fw,\u2217CEOT \u2212 FwCEOT(fF )\u2223\u2223+ \u03b5\u22121E\u2223\u2223FwCEOT(fF )\u2212 F\u0302wCEOT(f\u0302)\u2223\u2223+ \u03b5\u22121E\u2223\u2223F\u0302wCEOT(f\u0302)\u2212 FwCEOT(f\u0302)\u2223\u2223 \u2264\n\u03b5\u22121 \u2223\u2223Fw,\u2217CEOT \u2212 FwCEOT(fF )\u2223\u2223+ \u03b5\u22121[4RN (FCEOT ,P) + 4RM (F ,Q)] ."
        },
        {
            "heading": "C EXTENDED EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 COLORED MNIST",
            "text": "In this subsection, we consider Colored MNIST (Gushchin et al., 2023, M5.3). Following (Gushchin et al., 2023), we set source and target distributions P and Q to be colored handwritten images of digits \u201c2\u201d and \u201c3\u201d accordingly. The entropic regularization coefficients are in range \u03b5 = 0.01, 0.1, 1.\n\u03f5 0.01 0.1 1 LPIPS (VGG) 0.043 0.063 0.11\nTable 3: LPIPS variance of Our method for ColoredMNIST \u201c2\u201d\u2192\u201c3\u201d transfer.\nThe qualitative results of learning our model directly in the data space are presented in Figure 4. As we can see, our learned EOT plans successfully preserve color and geometry of the transformed images. Generated images (Figures 4b, 4c, 4d) are slightly noised since we add noise to target images when training for\nstability. For quantitative analysis and comparison with competitive methods, we borrow the results on the ColoredMNIST transfer problem for (Bortoli et al., 2021), (Daniels et al., 2021), (Gushchin et al., 2023) from (Gushchin et al., 2023). Additionally, we run the code for the recent (Shi et al., 2023) on our own. The methods generally work for different \u03b5 due to their principles, and we choose \u03b5 = 1 as an admissible entropic regularization power for all methods except (Daniels et al., 2021) which struggles for small \u03b5, see the discussion in \u00a7 4.2. For it, we choose \u03b5 = 25. The obtained FID metrics are reported in Table 4. For the qualitative performance of baselines, see (Gushchin et al., 2023, Fig. 2). Besides, we provide Table 3 with LPIPS metric to show that the diversity of our method increases with \u03b5.\nWe honestly state that the FID of our approach is not good. One reason is that the default Langevin dynamic produces slightly noisy samples. FID is known to terribly react to any noise. Secondly, we emphasize that we adapt the simplest long-run EBMs with persistent replay buffer (Nijkamp et al., 2020) for the experiment, see Appendix D.4 for the details. We leave the applications of modern EBMs which can generate sharp data (Du & Mordatch, 2019; Du et al., 2021) for future research."
        },
        {
            "heading": "C.2 EXTENDED HIGH-DIMENSIONAL UNPAIRED IMAGE-TO-IMAGE TRANSLATION",
            "text": "In this section, we provide additional quantitative results and comparisons for our considered highdimensional I2I setup. In Table 5, we show uncurated samples from our approach learned on 512 \u00d7 512 AFHQ Cat\u2192Dog and Wild\u2192Dog image transfer problems. To compare our visual results with alternatives, we demonstrate the pictures generated by (Zhao et al., 2022) and (Daniels et al., 2021) solvers, see Figure 6. The former demonstrates SOTA results, see Table 2, but has no relation to OT. The latter is the closest approach to ours. For (Daniels et al., 2021), we trained their algorithm in the same setup as we used, with the latent space of the StylaGAN and transport cost c(x, y) = 12\u2225x\u2212G(z)\u2225 2 2, see \u00a7 5.3. We found that their method works only for \u03b5 = 10000 yielding unconditional generation. It is in concordance with our findings about the approach, see discussion in \u00a7 4.2, and the insights from the original work, see (Daniels et al., 2021, \u00a75.1)."
        },
        {
            "heading": "D EXPERIMENTAL DETAILS",
            "text": "General Details. For the first two experiments, we take the advantage of replay buffer B constructed as described in (Du & Mordatch, 2019). When training, the ULA algorithm is initialized by samples from B with probability p = 0.95 and from Gaussian noise with probability 1\u2212 p = 0.05. For the last two image-data experiments, we also use a similar replay buffer but with p = 1, i.e., we do not update B with short-run samples."
        },
        {
            "heading": "D.1 TOY 2D DETAILS",
            "text": "We parameterize the potential f\u03b8 as MLP with two hidden layers and LeakyReLU(negative_slope= 0.2) as the activation function. Each hidden layer has 256 neurons. The hyperparameters of Algorithm 1 are as follows: K = 100, \u03c30 = 1, N = 1024, see the meaning of each particular variable in the algorithm listing. The Langevin discretization steps are \u03b7 = 0.05 for \u03b5 = 0.1 and \u03b7 = 0.005 for \u03b5 = 0.001. The reported numbers are chosen for reasons of the training stability.\nComputation complexity. The experiment was conducted on a single GTX 1080 Ti and took approximately two hours for each entropy regularization parameter value."
        },
        {
            "heading": "D.2 GAUSSIAN-TO-GAUSSIAN DETAILS",
            "text": "For the source and target distributions, we choose P = N (0,\u03a3X) and Q = N (0,\u03a3Y ) with \u03a3X and \u03a3Y chosen at random. For the reproducibility, these parameters are provided in the code.\nThe details of baseline methods (Table 1) are given in Appendix E. For each particular parameter set \u03b5,D, our trained potential f\u03b8 is given by MLP with three hidden layers, 512 neurons each, and ReLU activation. The same architectural setup is chosen for \u230aGushchin et. al.\u2309 (Gushchin et al., 2023) for fair comparison. The hyperparameters of Algorithm 1 are the same for each \u03b5,D: K = 100, \u03c30 = 1, N = 1024, \u03b7 = 0.1. At the inference stage, we run ULA steps for Ktest = 700 iterations. The only parameter which we choose to be dependent on \u03b5,D is the learning rate. It affects the stability of the training. The particular learning rates which are used in our experiments are given in Table 5. More specific training peculiarities could be found in our code.\nBures-Wasserstein UVP metric. The BW22 -UVP metric (Korotin et al., 2021b, Eq. 18) is the Wasserstein-2 distance between distributions \u03c01 and \u03c02 that are coarsened to Gaussians and normalised by the variance of distribution \u03c02:\nBW22-UVP(\u03c01, \u03c02) def = 100% 1 2Var(\u03c02)\nW22 ( N (\u00b5\u03c01 ,\u03a3\u03c01),N (\u00b5\u03c02 ,\u03a3\u03c02) ) .\nIn our experiment, \u03c02 is the optimal plan \u03c0\u2217 which is known to be Gaussian, and \u03c01 is the learned plan \u03c0\u0302, whose mean and covariance are estimated by samples.\nComputation complexity. Each experiment with particular \u03b5,D takes approximately 12 hours on a single GTX 1080 Ti."
        },
        {
            "heading": "D.3 HIGH-DIMENSIONAL UNPAIRED IMAGE-TO-IMAGE TRANSLATION DETAILS",
            "text": "General details. In this experiment, we learn EOT between a source distribution of images P \u2208 P(R3\u00d7512\u00d7512) and Q = N (0, I512) \u2208 P(R512), which is the latent distribution of the pretrained StyleGAN G. We use non-euclidean cost c(x, y) = 12\u2225x\u2212G(z)\u2225 2 2. Below we describe the primary idea behind this choice. Consider the pushforward distribution Qambi def= G\u266fQ. In our case, it is the parametric distribution of AFHQ Dogs. Thanks to our specific cost function, the learned optimal conditional plans \u03c0f\u0302 (\u00b7|x) between P and Q help to approximate (given \u03b5 is sufficiently small) the standard Euclidean Optimal Transport between P and Qambi, which is the motivating problem of several OT researches (Makkuva et al., 2020; Korotin et al., 2021a). The corresponding (stochastic) mapping is given by pushforward distributions G\u266f\u03c0f\u0302 (\u00b7|x). In practice, we sample from \u03c0f\u0302 (\u00b7|x) using our cost-guided MCMC and then pass the obtained samples through G. Note that our setup seems to be the first theoretically-advised attempt to leverage W22 OT between 512\u00d7 512 images. Technical details. The AFHQ dataset is taken from the StarGAN v2 (Choi et al., 2020) github:\nhttps://github.com/clovaai/stargan-v2.\nThe dataset includes three groups of high-quality 512\u00d7 512 images: Dogs, Cats and Wilds (wildlife animals). The latter two groups are used as the source distributions P. The pretrained (on AFHQ Dogs) StyleGAN2-ADA Karras et al. (2020) is taken from the official PyTorch implementation:\nhttps://github.com/NVlabs/stylegan2-ada-pytorch.\nAs a potential f\u03b8 which operates in the 512-dimensional latent space of the StyleGAN model, we choose fully-connected MLP with ReLU activations and three hidden layers with 1024, 512 and 256 neurons, accordingly. The training hyperparameters are: K = 100, \u221a \u03b7 = 0.008, \u03c30 = 1.0, N = 128. For both Cat\u2192Dog and Wild\u2192Dog experiments, we train our model for 11 epochs with a learning\nrate 10\u22124 which starts monotonically decreasing to 10\u22125 after the fifth epoch. At inference, we initialize the sampling procedure (in the latent space) with standard Normal noise. Then we repeat Langevin steps for K inittest = 1000 iterations with \u221a \u03b7 = 0.008. After that, additional K refinetest = 1000\nsteps are performed with \u221a \u03b7 = 0.0005. The obtained latent codes are then passed through the StyleGAN generator, yielding the images from the AFHQ Dog dataset.\nComputation complexity. The training takes approximately a day on 4 A100 GPUs. The inference (as described above) takes about 20 minutes per batch on a single A100 GPU."
        },
        {
            "heading": "D.4 COLORED MNIST DETAILS",
            "text": "For generating Colored MNIST dataset, we make use of the code, provided by the authors of (Gushchin et al., 2023). The dataset consists of colored images of digit \u201c2\u201d (\u2248 7K) and colored images of digit \u201c3\u201d (\u2248 7K). The images are scaled to resolution 32\u00d7 32. To solve the problem in view, we adapt the base EBM code from (Nijkamp et al., 2020):\nhttps://github.com/point0bar1/ebm-anatomy.\nWe do nothing but embed the cost function gradient\u2019s computation when performing Langevin steps, leaving all the remaining technical details unchanged. In particular, we utilize simple CNNs with LeakyReLU(negative_slope= 0.05) activations as our learned potential f\u03b8. We pick batch size N = 256 and initialize the persistent replay buffer at random using Uniform[\u22121, 1]3\u00d732\u00d732 distribution. We use Adam optimizer with learning rate gradually decreasing from 3 \u00b7 10\u22125 to 10\u22125. The reported images 4 correspond to approximately 7000 training iterations.\nFor each entropic coefficient \u03b5 = 0.01, 0.1, 1, we run 6 experiments with the parameters given in Table 6. For training stability, we add Gaussian noise N (0, 9 \u00b7 \u03b7) to target samples when computing loss estimate L\u0302 in Algorithm 1.\nAt the inference stage, we initialize the MCMC chains with source data samples. The ULA steps are repeated for Ktest = 2000 iterations with the same Langevin discretization step size \u03b7 as the one used at the training stage. For each \u03b5, the reported images 4 are picked for those parameters set K, \u03b7, which we found to be the best in terms of qualitative performance.\nFor LPIPS calculation, we use the official code:\nhttps://github.com/richzhang/PerceptualSimilarity,\nwhere we pick VGG backbone for calculating lpips features. To calculate the resulting metric, we sample 18 target images from \u03c0f\u0302 (\u00b7|x) for each test source image x. For every pair of these 18 images, we compute LPIPS and report the average value (along the generated target images and source ones).\nComputation complexity. It takes approximately one day on one V100 GPU to complete an image data experiment for each set of parameters."
        },
        {
            "heading": "E DETAILS OF THE BASELINE METHODS",
            "text": "In this section, we discuss details of the baseline methods with which we compare our method on the Gaussian-to-Gaussian transformation problem.\n\u230aDaniels et.al.\u2309 (Daniels et al., 2021). We use the code from the authors\u2019 repository\nhttps://github.com/mdnls/scones-synthetic\nfor their evaluation in the Gaussian case. We employ their configuration blob/main/config.py.\n\u230aSeguy et.al.\u2309 (Seguy et al., 2018). We use the part of the code of SCONES corresponding to learning dual OT potentials blob/main/cpat.py and the barycentric projection blob/main/bproj.py in the Gaussian case with configuration blob/main/config.py.\n\u230aChen et.al.\u2309 (Joint) (Chen et al., 2022). We utilize the official code from\nhttps://github.com/ghliu/SB-FBSDE\nwith their configuration blob/main/configs/default_checkerboard_config.py for the checkerboard-to-noise toy experiment, changing the number of steps of dynamics from 100 to 200 steps. Since their hyper-parameters are developed for their 2-dimensional experiments, we increase the number of iterations for dimensions 16, 64 and 128 to 15 000.\n\u230aChen et.al.\u2309 (Alt) (Chen et al., 2022). We also take the code from the same repository as above. We base our configuration on the authors\u2019 one (blob/main/configs/default_moon_to_spiral_config.py) for the moon-tospiral experiment. As earlier, we increase the number of steps of dynamics up to 200. Also, we change the number of training epochs for dimensions 16, 64 and 128 to 2,4 and 8 correspondingly.\n\u230aDe Bortoli et.al.\u2309 (Bortoli et al., 2021). We utilize the official code from\nhttps://github.com/JTT94/diffusion_schrodinger_bridge\nwith their configuration blob/main/conf/dataset/2d.yaml for toy problems. We increase the amount of steps of dynamics to 200 and the number of steps of IPF procedure for dimensions 16, 64 and 128 to 30, 40 and 60, respectively.\n\u230aVargas et.al.\u2309 (Vargas et al., 2021). We use the official code from\nhttps://github.com/franciscovargas/GP_Sinkhorn\nwith hyper-parameters from blob/main/notebooks/2D Toy Data/2d_examples.ipynb. We set the number of steps to 200. As earlier, we increase the number of steps of IPF procedure for dimensions 16, 64 and 128 to 1000, 3500 and 5000, respectively.\n\u230aVargas et.al.\u2309 (Vargas et al., 2021). We tested the official code from\nhttps://github.com/franciscovargas/GP_Sinkhorn\nInstead of Gaussian processes, we used a neural network as for \u230aENOT\u2309. We use N = 200 discretization steps as for other SB solvers, 5000 IPF iterations, and 512 samples from distributions P0 and P1 in each of them. We use the Adam optimizer with lr = 10\u22124 for optimization.\n\u230aGushchin et.al.\u2309 (Gushchin et al., 2023) We use the code provided by the authors. In our experiments, we use exactly the same hyperparameters for this setup as the authors (Gushchin et al., 2023, Appendix B), except the number of discretization steps N , which we set to 200 as well as for other Schr\u00f6dinger Bridge based methods."
        },
        {
            "heading": "F EXTENDED DISCUSSION OF LIMITATIONS",
            "text": "In general, the main limitation of our approach is the usage of MCMC. This procedure is timeconsuming and requires adjusting several hyperparameters. Moreover, in practice, it may not always converge to the desired distribution which introduces additional biases. The other details of launching our proposed algorithm arise due to its connection to EBM\u2019s learning procedure. It is known that EBMs for generative modelling could be trained by two different optimization regimes: short-run (non-convergent) training and long-run (convergent) training (Nijkamp et al., 2020). In the first regime, the learned potential does not necessarily represent the energy function of the learned distribution. Because of this, the short-run mode may not always be adapted for Energyguided EOT, since it seems crucial for f\u03b8 to represent the true component of the conditional Energy potentials E\n\u00b5 f\u03b8 x (y) = c(x,y)\u2212f\u03b8(y)\u03b5 . In particular, for our Colored MNIST experiment, we found\nthe short-run regime to be unstable and utilize exclusively long-run mode. At the same time, for\nmoderate-dimensional Toy 2D and Gaussian-to-Gaussian experiments as well as for latent-space high-dimensional I2I setup, non-convergent training was successful."
        }
    ],
    "title": "ENERGY-GUIDED ENTROPIC NEURAL OPTIMAL TRANSPORT",
    "year": 2024
}