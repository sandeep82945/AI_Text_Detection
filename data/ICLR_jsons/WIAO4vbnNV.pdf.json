{
    "abstractText": "Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.",
    "authors": [
        {
            "affiliations": [],
            "name": "ING WITH"
        },
        {
            "affiliations": [],
            "name": "DIFFERENTIABLE MOTION ESTIMATORS"
        }
    ],
    "id": "SP:b92e24c088a0a2478bac00c08907794cb19410b3",
    "references": [
        {
            "authors": [
                "Arpit Bansal",
                "Hong-Min Chu",
                "Avi Schwarzschild",
                "Soumyadip Sengupta",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Universal guidance for diffusion models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A. Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Tsai-Shien Chen",
                "Chieh Hubert Lin",
                "Hung-Yu Tseng",
                "Tsung-Yi Lin",
                "Ming-Hsuan Yang"
            ],
            "title": "Motionconditioned diffusion model for controllable video synthesis",
            "venue": "arXiv preprint arXiv:2304.14404,",
            "year": 2023
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alex Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "In Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "P. Fischer",
                "E. Ilg",
                "P. H\u00e4usser",
                "C. Haz\u0131rba\u015f",
                "V. Golkov",
                "P. v.d. Smagt",
                "D. Cremers",
                "T. Brox"
            ],
            "title": "Flownet: Learning optical flow with convolutional networks",
            "venue": "In IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Dave Epstein",
                "Allan Jabri",
                "Ben Poole",
                "Alexei A Efros",
                "Aleksander Holynski"
            ],
            "title": "Diffusion selfguidance for controllable image generation",
            "venue": "Neural Information Processing Systems (NeurIPS),",
            "year": 2023
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "arXiv preprint arXiv:2208.01618,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2012
        },
        {
            "authors": [
                "Zeqi Gu",
                "Abe Davis"
            ],
            "title": "Filtered-guided diffusion: Fast filter guidance for black-box diffusion models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Amir Hertz",
                "Ron Mokady",
                "Jay Tenenbaum",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Prompt-to-prompt image editing with cross attention control",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "arXiv preprint arxiv:2006.11239,",
            "year": 2020
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of StyleGAN",
            "venue": "In Proc. CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Li",
                "Xue Xu",
                "Xinyan Xiao",
                "Jiachen Liu",
                "Hu Yang",
                "Guohao Li",
                "Zhanpeng Wang",
                "Zhifan Feng",
                "Qiaoqiao She",
                "Yajuan Lyu",
                "Hua Wu"
            ],
            "title": "Upainting: Unified text-to-image diffusion generation with cross-modal guidance, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "N. Mayer",
                "E. Ilg",
                "P. H\u00e4usser",
                "P. Fischer",
                "D. Cremers",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation",
            "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "SDEdit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Mou",
                "Xintao Wang",
                "Jiechong Song",
                "Ying Shan",
                "Jian Zhang"
            ],
            "title": "Dragondiffusion: Enabling drag-style manipulation on diffusion models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Xingang Pan",
                "Ayush Tewari",
                "Thomas Leimk\u00fchler",
                "Lingjie Liu",
                "Abhimitra Meka",
                "Christian Theobalt"
            ],
            "title": "Drag your gan: Interactive point-based manipulation on the generative image manifold",
            "venue": "arXiv preprint arXiv:2305.10973,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Roich",
                "Ron Mokady",
                "Amit H Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Pivotal tuning for latent-based editing of real images",
            "venue": "ACM Trans. Graph.,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S. Sara Mahdavi",
                "Rapha Gontijo Lopes",
                "Tim Salimans",
                "Jonathan Ho",
                "David J Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yujun Shi",
                "Chuhui Xue",
                "Jiachun Pan",
                "Wenqing Zhang",
                "Vincent YF Tan",
                "Song Bai"
            ],
            "title": "Dragdiffusion: Harnessing diffusion models for interactive point-based image editing",
            "venue": "arXiv preprint arXiv:2306.14435,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "URL https://arxiv.org/abs/2010.02502",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Zachary Teed",
                "Jia Deng"
            ],
            "title": "Raft: Recurrent all-pairs field transforms for optical flow",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Narek Tumanyan",
                "Michal Geyer",
                "Shai Bagon",
                "Tali Dekel"
            ],
            "title": "Plug-and-play diffusion features for text-driven image-to-image translation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Bram Wallace",
                "Akash Gokul",
                "Stefano Ermon",
                "Nikhil Naik"
            ],
            "title": "End-to-end diffusion latent optimization improves classifier guidance, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yinhuai Wang",
                "Jiwen Yu",
                "Jian Zhang"
            ],
            "title": "Zero-shot image restoration using denoising diffusion null-space model",
            "venue": "The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Haofei Xu",
                "Jing Zhang",
                "Jianfei Cai",
                "Hamid Rezatofighi",
                "Dacheng Tao"
            ],
            "title": "Gmflow: Learning optical flow via global matching",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models, 2023",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent advances in diffusion models have provided users with the ability to manipulate images in a variety of ways, such as by conditioning on text (Hertz et al., 2022; Tumanyan et al., 2023), instructions (Brooks et al., 2023), or other images (Gal et al., 2022). Yet existing methods often struggle to make many seemingly simple changes to image structure, like moving an object to a specific position or changing its shape.\nAn emerging line of work has begun to address these problems by incorporating motion prompts, such as user-provided displacement vectors that indicate where a given point on an object should move (Chen et al., 2023; Pan et al., 2023; Shi et al., 2023; Mou et al., 2023; Epstein et al., 2023). However, these methods have significant limitations. First, they are largely limited to sparse motion inputs, often only allowing constraints to be placed on one point at a time. This makes it difficult to precisely capture complex motions, especially those that require moving object parts in nonrigid ways. Second, existing methods often require text inputs and place strong restrictions on the underlying network architecture, such as by being restricted to editing objects that receive cross-attention from a text token, by requiring per-image finetuning, or by relying on features from specific layers of specific diffusion architectures.\nTo address these issues, we propose motion guidance, a simple, zero-shot method that takes advantage of recent advances in motion estimation. This method allows users to edit images by specifying a dense, and possibly complex, flow field indicating where each pixel should move in the edited image (Fig. 1 ). We guide the diffusion sampling process using a loss function incorporating an off-the-shelf optical flow estimator, similarly to classifier guidance Dhariwal & Nichol (2021). As part of each diffusion sampling step, we match the generated image with the input source image, and measure the extent to which it deviates from the user-provided flow field. We then augment the noise estimate with gradients through our loss, in the process backpropagating through the optical flow estimator. Simultaneously, we encourage the generated image to be visually similar to the source image by encouraging corresponding pixels to be photoconsistent, allowing our model to trade off between the fidelity of motion and visual appearance.\nIn contrast to other approaches, our method does not require any training, and does not explicitly constrain the diffusion network architecture. We show that our method works for both real and\ngenerated images, and that it supports a wide range of complex target motions, such as flow fields that have been extracted from a video. We make the following contributions:\n\u2022 We propose motion guidance, a zero-shot technique that allows users to specify a motion field to edit an image. \u2022 We show that off-the-shelf optical flow networks provide a useful guidance signal for diffusion. \u2022 Through qualitative and quantitative experiments, we show that our model can handle a wide\nrange of complex motion fields, including compositions of translations, rotations, homographies, stretching, deformations, and even flow fields extracted from a video."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Diffusion Models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) are powerful generative models that learn to reverse a forward process where data is iteratively converted to noise. The reverse process is parameterized by a neural network that estimates the noise, /\u03b8(xt, t, y), given the noisy datapoint xt, the timestep t, and optionally some conditioning y, for example an embedding of a text prompt. This noise estimate is then used to gradually remove the noise from the data point under various update rules, such as DDPM or DDIM (Song et al., 2020).\nGuidance. Diffusion models admit a technique called guidance, in which the denoising process is perturbed toward a desired outcome. This perturbation may come from the diffusion model itself as in classifier-free guidance (Ho & Salimans, 2022; Nichol et al., 2021), a classifier as in ImageNet classifier guidance Dhariwal & Nichol (2021), or in general the gradients of an energy function L(x). Many forms of guidance function, L(x), have been proposed including CLIP embedding distance (Wallace et al., 2023; Nichol et al., 2021), bilateral filters (Gu & Davis, 2023), and internal\nrepresentations of diffusion models themselves (Epstein et al., 2023). Bansal et al. (2023) propose a one-step approximation to achieve guidance on off-the-shelf models such as segmentation, detection, facial recognition, and style networks. Our method proposes using off-the-shelf optical flow models as guidance to achieve motion-based image editing.\nImage Editing. Diffusion models, due to their strong image priors, are an attractive starting point for many image editing techniques. Some methods, such as SDEdit (Meng et al., 2022), propose to run the forward diffusion process partially, and then denoise the partially corrupted image. Other methods modify the denoising step as in Lugmayr et al. (2022) and Wang et al. (2023). Still others finetune a diffusion model for image manipulation tasks (Brooks et al., 2023; Zhang & Agrawala, 2023). Another class of diffusion-based image manipulation techniques uses the features and attention inside the denoising models themselves to edit images. These techniques work due to the observation that features can encode attributes, identity, style, and structure. Methods in this vein include Prompt-to-Prompt (Hertz et al., 2022), Plug-and-Play (Tumanyan et al., 2023), and SelfGuidance (Epstein et al., 2023).\nThe majority of these techniques enable impressive editing of image style, but are unable to modify the image structure. A more recent line of work seeks to achieve editing of structure. These methods are closest to ours. DragGAN (Pan et al., 2023) proposes an optimization scheme over StyleGAN features that allows users to \u201cdrag\u201d points in an image to new locations. However, these results are constrained to narrowly trained GANs. In order to achieve much wider applicability concurrent work has adapted the DragGAN optimization scheme to diffusion models (Shi et al., 2023; Mou et al., 2023), however these approaches either require fine-tuning with LoRA (Hu et al., 2021) for each image or have not been shown to work on complex, densely defined deformations. In very recent work, Epstein et al. (2023) perform motion editing with classifier guidance, using the activations and textual-visual cross-attention of the diffusion model to specify object position and shape. This limits the complexity of deformations and constrains manipulable objects to those that have a corresponding textual token in the prompt. In contrast, our guidance comes from an off-the-shelf optical flow network and thus does not suffer from these problems, nor does it rely on a particular architecture for the diffusion model. Overall our method seeks to build upon past and concurrent work, by enabling dense, complex, pixel-level motion manipulation for a wide range of images in a zero-shot manner."
        },
        {
            "heading": "3 METHOD",
            "text": "Our goal is to allow a user to edit a source image x\u2217 2 Rw\u00d7h\u00d73 by specifying a target flow field f 2 Rw\u00d7h\u00d72 that indicates how each pixel should move. To do this, we develop techniques that guide a diffusion model\u2019s denoising process using an off-the-shelf optical flow network."
        },
        {
            "heading": "3.1 GUIDANCE IN DIFFUSION MODELS",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2021) generate images by repeatedly denoising a sample of random noise. They are commonly represented (Ho et al., 2020) as functions /\u03b8(xt; t, y) that predict the noise in a noisy data point xt at step t of the diffusion process, using an optional conditioning signal y. Classifier guidance (Dhariwal & Nichol, 2021) seeks to generate samples that minimize a function L(x), such as an object classification loss, by augmenting the noise estimate with the gradients of L:\n/\u0303\u03b8(xt; t, y) = /\u03b8(xt; t, y) + trxtL(xt), (1)\nwhere /\u0303\u03b8 is the new noise function and t is a weighting schedule. A key benefit of this approach is that it does not require retraining, and it does not explicitly place restrictions on L, except for differentiability. Recent work has shown that this approach can be straightforwardly extended to use functions L that accept only clean (denoised) images as input (Bansal et al., 2023)."
        },
        {
            "heading": "3.2 MOTION GUIDANCE",
            "text": "We will use guidance to manipulate the positions and shapes of objects in images. We design a guidance function L(x) (Eq. 1 ) that measures how well a generated image, x, captures the desired motion. We encourage the optical flow between x\u2217 and x to be f . Given a (differentiable) off-theshelf optical flow estimator F (\u00b7, \u00b7), we will minimize the loss\nLflow(x) = kF (x \u2217,x) fk1. (2)\nWhile we found that performing guidance on this loss could produce the desired motions, we observed that the edited objects would often change color or texture. This is because motion estimation models are invariant to many color variations1, thus there is little to constrain the appearance of the object to match that of the source image. To address this, we add a loss that encourages corresponding pixels to have similar colors\nLcolor(x) = kx \u2217 warp(x, F (x\u2217,x))k1, (3)\nwhere warp(x, f) indicates a backward warp of an image x using the displacement field f . The full loss we use for guidance is then\nL(x) = flowLflow(x) + colormcolorLcolor(x), (4)\nwhere flow and color are hyperparameters that trade off between the model\u2019s adherence to the target motion and the visual fidelity to objects in the source image, and mcolor is a mask to handle occlusions, explained below."
        },
        {
            "heading": "3.3 IMPLEMENTING MOTION GUIDANCE",
            "text": "We find a number of techniques useful for producing high quality edits with motion guidance.\nHandling Occlusions. When objects move, they occlude pixels in the background. Since these background pixels have no correspondence in the generated image, the color loss (Eq. 3 ) is counterproductive. We adopt the convention that the target flow specifies the motions of the foreground, i.e., that when an object moves, it will occlude the pixels that it overlaps with. To implement this, we create an occlusion mask from the target flow and mask out the color loss in the occluded regions. Please see Appendix A1 for details.\nEdit Mask. For a given target flow, often many pixels will not move at all. In these cases, we can reuse content from the source image by automatically constructing an edit mask m from the target flow, indicating which pixels require editing. This mask consists of all locations that any pixel is moving to or from. To apply the edit mask during diffusion sampling, we assume access to a sequence of noisy versions of the source image x\u2217 for each timestep, which we denote x\u2217\nt . For\nreal images this sequence can be constructed by injecting the appropriate amount of noise into x\u2217 at each timestep t, or alternatively it can be obtained through DDIM inversion (Song et al., 2020). For images sampled from the diffusion model, we can simply cache the x\u2217\nt from the reverse process.\nThen, similarly to Lugmayr et al. (2022), during our denoising process at each timestep t we replace pixels outside of the edit mask with the cached content: xt m xt + (1 m) x \u2217 t .\n1This is quite useful for flow estimation on videos, where lighting may change rapidly frame to frame.\nHandling Noisy Images. In standard classifier guidance, noisy images xt are passed into the guidance function. In our case, this results in a distribution mismatch with the off-the-shelf optical flow model, which has only been trained on clean images. To address this we adopt the technique of Bansal et al. (2023) and compute the guidance function on a one step approximation of the clean x0 given by:\nx\u03020(xt) = xt p 1 \u00b5t/\u03b8(xt, t)p\n\u00b5t\n, (5)\nresulting in the gradient rxtL(x\u03020(xt)), with a more in-domain input to the guidance function.\nRecursive Denoising. Another problem we encounter is that the optical flow model, due to its complexity and size, can be quite hard to optimize through guidance. Previous work (Lugmayr et al., 2022; Wang et al., 2023; Bansal et al., 2023) has shown that repeating each denoising step for a total of K steps, where K is some hyperparameter, can result in much better convergence. We adopt this recursive denoising technique to stabilize our method and empirically find that this resolves many optimization instabilities, in part due to an iterative refinement effect (Lugmayr et al., 2022) and also in part due to the additional steps that the guidance process takes, effectively resulting in more optimization steps over the guidance energy.\nGuidance Clipping. We also find that clipping guidance gradients prevents instabilities during denoising. Concurrent work by Gu & Davis (2023) introduces an adaptive clipping strategy, but we find that simply clipping gradients by their norm to a pre-set threshold, cg , works well in our case."
        },
        {
            "heading": "4 RESULTS",
            "text": "We evaluate our method\u2019s ability to manipulate image structure, both qualitatively and quantitatively, on both real and generated images. Additional results can be found in the appendix, which we highly encourage the reader to examine."
        },
        {
            "heading": "4.1 IMPLEMENTATION DETAILS",
            "text": "We use RAFT (Teed & Deng, 2020) as our flow model. To create target flow fields, we compose a set of elementary flows and use a segmentation model (Kirillov et al., 2023) for masking. Target flow construction and hyperparameters are discussed in detail in Appendix A1 . For our experiments we use Stable Diffusion (Rombach et al., 2021). Rather than performing diffusion directly on pixels, Stable Diffusion performs diffusion in a latent space, with an encoder and decoder to convert between pixel and latent space. To accommodate this, we precompose the decoder with the motion guidance function, L(D(\u00b7)), so that the guidance function can accept latent codes. Additionally, we downsample our edit mask to 64\u00e5 64, the spatial size of the Stable Diffusion latent space."
        },
        {
            "heading": "4.2 QUALITATIVE RESULTS",
            "text": "Main qualitative results can be found in Figures 1 , 2 , 3 , and 4 . We point out several strengths of our method:\nDisocclusions. Because our method is built on top of a powerful diffusion model, it is able to reason about disoccluded regions. For example, in Figure 1c our model moves the lion to the left and is able to fill in the previously occluded face.\nDiverse Flows. Our method successfully manipulates images using translations (Figures 1c , 1h , 3a ), rotations (Figures 1b , 4a ), stretches (Figures 1a , 1f , 1g ), and scaling (Figures 2e , 2f , 4b ). It also handles complex deformations (Figures 1e , 3c , 4c ), homographies (Figures 1d , 3b ), and multiple objects (Figures 4e , 1e ). Moreover, our method is able to handle relatively large movements, as compared to typical optical flows (Figures 2b , 2c , 2d , 3a ). We are able to achieve successes on diverse flows due to the generality of both the optical flow and diffusion model we use.\nDiverse Images. We show that our approach works on a wide range of input images with diverse objects and backgrounds, including non-photorealistic images such as paintings or sketches (Figures 1f , 4b , 5a , 5b , 5c ), despite the fact that the optical flow network was not trained on these styles. This is because optical flow is a relatively low level signal and doesn\u2019t require understanding of high\nlevel semantics to estimate. Additionally, we show that our method works for both synthetic images as well as real images (Figures 1g , 1h , 3c , 4e , 7b , 7c , 7d ).\nSoft Optimization. In contrast to forward warping, our method optimizes a soft constraint: satisfy the flow while also producing a plausible image. Because of this, our method works even for coarsely defined target flows as shown in Figures 1c , 1h , 5b . This is particularly useful when we do motion transfer in Section 4.6 and Figure 7 , where the extracted flow is only roughly aligned with the source image. Another benefit of soft optimization is that our method works even when target flows are ambiguous, such as when flows specify that two objects should overlap as in Figure 1e .\nText Conditioning. Because our method is independent of classifier-free guidance we can use it with or without text conditioning. This is useful for editing real images, for which no caption exists. In all figures, samples conditioned on text will have the prompt written in italics below the example."
        },
        {
            "heading": "4.3 ABLATIONS",
            "text": "In Figure 3 we qualitatively ablate out key components of our guidance function.\nNo Recursive Denoising. Without recursive denoising our method converges much less frequently, often resulting in samples with heavy artifacts as in Figure 3c . Even in cases where the guidance mostly succeeds, artifacts can still occur as in Figure 3a .\nNo Color Loss. Without the color loss, our method generally moves objects to the correct locations, but objects tend to change color as in Figure 3a . These samples still achieve a low flow loss because of the color invariance of the optical flow network, highlighting the need for the color loss. In some cases we see catastrophic failure, as in Figure 3c .\nNo Flow Loss. Removing the flow loss also removes any knowledge the method has of the target flow. To address this we also modify the color loss (Eq. 3 ), replacing the computed flow with the target flow. Without the flow loss our method is able to move objects to the correct location but often hallucinates things in disoccluded areas, such as the half apple in Figure 3a or the waterfall in Figure 3c . This is because the color loss does not produce any gradient signal in disoccluded areas due to the warping operation. In the disoccluded areas the diffusion model effectively acts freely with no guidance at all, resulting in these hallucinations.\nNo Occlusion Masking. Without occlusion masking the color loss is incorrect in occluded areas. This is because the target flow in occluded areas is invalid, so the warping operation mismatches pixels. As a result we tend to see \u201cghosting\u201d of moved objects, where objects blend into the background or take on the color of the disoccluded background, as can be seen in Figure 3a or Figure 3c . In more extreme cases, the object can fail to move into the occluding region at all, as in Figure 3b ."
        },
        {
            "heading": "4.4 BASELINES",
            "text": "We present comparisons between our method and baselines in Figure 4 . For a more extensive set of comparisons please see Figure A5 in the appendix.\nInstructPix2Pix. InstructPix2Pix (Brooks et al., 2023) distills Prompt-to-Prompt (Hertz et al., 2022) into a diffusion model that is conditioned on textual editing instructions. Because this model is only text-conditioned, it is not possible to give it access to the target flow, but we try to faithfully summarize the flow in an instruction. As can be seen, despite our best efforts, InstructPix2Pix is never able to move objects significantly although it does occasionally alter the image in rather unpredictable ways. This failure can be attributed to two factors. Firstly, text is not an ideal method for describing motion. For example it is quite hard to encapsulate the motion in Figure 4e in text without being excessively verbose. And secondly, feature-copying methods such as Prompt-toPrompt often fail to make significant structural edits to images.\nForward Warp with SDEdit. We also use baselines that explicitly use the target flow. Specifically, we forward warp the latent code by the target flow and use SDEdit (Meng et al., 2022), conditioned on the text prompt (if given) at various noise levels to \u201cclean\u201d the warped image. While SDEdit succeeds to a degree in Figure 4b and Figure 4d , the results are of lower quality and the failure cases contain severe errors.\nForward Warp with RePaint. In addition to cleaning the forward warped images using SDEdit, we try the diffusion based inpainting method RePaint (Lugmayr et al., 2022), conditioned on the text prompt (if given), to fill in disoccluded areas in the forward warp. The inpainted areas are generally realistic looking, but often not exactly correct. For example in Figure 4d , an additional teapot is generated in the disoccluded area.\nDragGAN. Recent work has proposed an iterative two-step optimization procedure over GAN features to \u201cdrag\u201d user specified handle points to target points. While the method, DragGAN (Pan et al., 2023), is impressive it has only been demonstrated on StyleGANs (Karras et al., 2020) trained on narrow domains, such as dogs, lions, or human faces, limiting the scope of such a method. To\ndemonstrate this, we show out-of-domain cases in which DragGAN fails in Figure 5 . Following DragGAN, to invert an image to a StyleGAN latent code we use PTI (Roich et al., 2021). We then subsample the target flow to obtain handle and target points that are required by DragGAN. DragGAN GPU memory usage scales linearly with the number of handle points, so densely sampling the flow is not feasible, even on an NVIDIA A40 GPU with 48 GB of memory. DragGAN also supports an edit mask, so we provide it with the same automatically generated edit mask that our method uses. We find that while the GAN inversion is successful even on out-of-domain cases, DragGAN motion edits contain many artifacts."
        },
        {
            "heading": "4.5 QUANTITATIVE RESULTS",
            "text": "We are interested in not just optimizing for a specific target flow, but also generating images that are faithful to the source image and that are coherent. Therefore we evaluate our method on two metrics which are designed to reflect the trade-offs we are interested in. One metric is the Flow Loss, Lflow from Equation 2 , which should measure how accurately a generated image adheres to the target flow. Because our guidance function actually uses Lflow with RAFT, we present results using both RAFT and GMFlow (Xu et al., 2022) in case we are overfitting. For our second metric we compute the CLIP Similarity as the cosine distance between the CLIP visual embeddings of the source and generated images. We treat this as a measure of faithfulness to the source image as well as a measure of general image quality.\nWe evaluate on two different datasets. The first dataset is composed of examples with handcrafted target flows, a subset of which can be seen in Figures 1 , 2 , 3 , 4 , and 7 . This dataset has the advantage of containing interesting motions that are of practical interest. In addition, we can write highly specific instructions for the InstructPix2Pix baseline for a fair comparison. However, this dataset is curated to an extent. We ameliorate this by performing an additional evaluation on an automatically generated dataset based on KITTI (Geiger et al., 2012), which contains egocentric driving videos with labeled bounding boxes on cars. To build our dataset we construct target flows consisting of random translations on cars. For more details on the datasets please see Appendix A2 .\nWe show results in Figure 6 . As can be seen, our method offers an attractive trade-off between satisfying the target flow and keeping faithful to the source image. The RePaint baseline, which is essentially forward warping and then inpainting, achieves a low flow loss due to the explicit warp operation but suffers from artifacts due to aliasing and inpainting and thereby results in a low CLIP similarity. The SDEdit baseline, which also uses forward warping, can achieve a low flow loss if the noise level is low, but also falls short on the visual similarity metric."
        },
        {
            "heading": "4.6 MOTION TRANSFER",
            "text": "Sometimes it is hard to specify a flow by hand. In these cases we find that our method can successfully \u201ctransfer\u201d motion from a video to an image. We give examples in Figure 7 , where we extract the flow from a video of the earth rotating and use it as our target flow. We find that even if the extracted flow does not overlap perfectly with the target image the desired motion can be achieved. We attribute this to the fact that our guidance is a soft optimization and the diffusion model overall ensures high-level semantic consistency."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "Limitations. While our method produces high quality motion-conditioned edits, it is also susceptible to various weaknesses. In particular, we inherit the deficiencies of diffusion models and guidance based methods, such as slow sampling speed. In addition, we also inherit the limitations of our optical flow method, and find that certain target flows are not possible. We give a comprehensive discussion of our limitations in Appendix A4 ."
        },
        {
            "heading": "5.1 CONCLUSION",
            "text": "Our results suggest that we can manipulate the positions and shapes of objects in images by guiding the diffusion process using an off-the-shelf optical flow estimator. Our proposed method is simple, does not require text or rely on a particular diffusion architecture, and does not require any training. We see our work opening two broad areas of research. The first direction is in further integrating motion estimation models into image manipulation models. Second, our work opens the possibility of repurposing other low-level computer vision models for image generation tasks through diffusion guidance."
        }
    ],
    "title": "MOTION GUIDANCE: DIFFUSION-BASED IMAGE EDIT-",
    "year": 2023
}