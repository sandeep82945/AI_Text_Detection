{
    "abstractText": "Finetuning language models on domain-specific corpus is a common approach to enhance their domain knowledge and capability. While improving performance on domain tasks, it often brings a side-effect of forgetting of the model\u2019s general abilities. In this study, we analyze the effects of finetuning on language models by dissecting its impacts on the modeling of topic, style, and factual knowledge in text. Our method uses instruction-following LLMs such as ChatGPT to autogenerate controlled-variable text examples which we use to probe the model. Our findings reveal that finetuning results in significant shifts in the language model\u2019s topic and style priors, while actual knowledge learning only contributes to a small fraction of the total probability change. Analysis shows that the adaptation of topic and style priors behave akin to learning simple features: they are learned rapidly and require little model capacity. They are also learned independently and primarily at the beginning of a text sequence. In contrast, factual knowledge is learned stably but slowly and requires significant model capacity. The findings offer insights and understanding into the finer dynamics of learning and forgetting in language models, and potentially inform future research on improving domain adaptation and addressing the challenges of continual language learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiao Zhang"
        },
        {
            "affiliations": [],
            "name": "Ji Wu"
        }
    ],
    "id": "SP:5994738e667594557272bee4346ea6f26f764fc1",
    "references": [
        {
            "authors": [],
            "title": "Influence of human behavior and the principle of least effort on library",
            "year": 2020
        },
        {
            "authors": [
                "Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instructionfinetuned language models",
            "venue": "CoRR, abs/2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord"
            ],
            "title": "Think you have solved question answering? try arc, the AI2 reasoning challenge",
            "venue": "CoRR, abs/1803.05457,",
            "year": 2018
        },
        {
            "authors": [
                "Terry Eagleton"
            ],
            "title": "Literary theory: An introduction",
            "year": 2011
        },
        {
            "authors": [
                "Robert M French"
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in cognitive sciences,",
            "year": 1999
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Litu Ou",
                "Ashish Sabharwal",
                "Tushar Khot"
            ],
            "title": "Specializing smaller language models towards multi-step reasoning",
            "venue": "In International Conference on Machine Learning, ICML 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Zhenxin Fu",
                "Xiaoye Tan",
                "Nanyun Peng",
                "Dongyan Zhao",
                "Rui Yan"
            ],
            "title": "Style transfer in text: Exploration and evaluation",
            "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Leo Gao",
                "Jonathan Tow",
                "Stella Biderman",
                "Sid Black",
                "Anthony DiPofi",
                "Charles Foster",
                "Laurence Golding",
                "Jeffrey Hsu",
                "Kyle McDonell",
                "Niklas Muennighoff",
                "Jason Phang",
                "Laria Reynolds",
                "Eric Tang",
                "Anish Thite",
                "Ben Wang",
                "Kevin Wang",
                "Andy Zou"
            ],
            "title": "A framework for few-shot language model evaluation",
            "venue": "URL https://doi.org/10.5281/zenodo",
            "year": 2021
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard S. Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A. Wichmann"
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nat. Mach. Intell.,",
            "year": 2020
        },
        {
            "authors": [
                "Kshitij Gupta",
                "Benjamin Th\u00e9rien",
                "Adam Ibrahim",
                "Mats Leon Richter",
                "Quentin Gregory Anthony",
                "Eugene Belilovsky",
                "Irina Rish",
                "Timoth\u00e9e Lesort"
            ],
            "title": "Continual pre-training of large language models: How to re-warm your model? In Workshop on Efficient Systems for Foundation Models",
            "venue": "URL https://openreview.net/forum?id=pg7PUJe0Tl",
            "year": 2023
        },
        {
            "authors": [
                "M.A.K. Halliday"
            ],
            "title": "An Introduction to Functional Grammar. Arnold, London, 2 edition",
            "year": 1994
        },
        {
            "authors": [
                "Peter Henderson",
                "Mark S. Krass",
                "Lucia Zheng",
                "Neel Guha",
                "Christopher D. Manning",
                "Dan Jurafsky",
                "Daniel E. Ho"
            ],
            "title": "Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "NIPS Deep Learning and Representation Learning Workshop,",
            "year": 2015
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun"
            ],
            "title": "Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, pp. 14852\u201314882",
            "venue": "Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Hofmann"
            ],
            "title": "Probabilistic latent semantic analysis",
            "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "Or Honovich",
                "Thomas Scialom",
                "Omer Levy",
                "Timo Schick"
            ],
            "title": "Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "venue": "ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-Kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alex Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL",
            "venue": "Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Sohee Yang",
                "Joongbo Shin",
                "Janghoon Han",
                "Gyeonghun Kim",
                "Stanley Jungkyu Choi",
                "Minjoon Seo"
            ],
            "title": "Towards continual knowledge learning of language models",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Yejin Bang",
                "Andrea Madotto",
                "Pascale Fung"
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv.,",
            "year": 2023
        },
        {
            "authors": [
                "Xisen Jin",
                "Dejiao Zhang",
                "Henghui Zhu",
                "Wei Xiao",
                "Shang-Wen Li",
                "Xiaokai Wei",
                "Andrew O. Arnold",
                "Xiang Ren"
            ],
            "title": "Lifelong pretraining: Continually adapting language models to emerging corpora",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Zixuan Ke",
                "Yijia Shao",
                "Haowei Lin",
                "Tatsuya Konishi",
                "Gyuhak Kim",
                "Bing Liu"
            ],
            "title": "Continual pretraining of language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Zachary Kenton",
                "Tom Everitt",
                "Laura Weidinger",
                "Iason Gabriel",
                "Vladimir Mikulik",
                "Geoffrey Irving"
            ],
            "title": "Alignment of language agents",
            "venue": "CoRR, abs/2103.14659,",
            "year": 2021
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Aitor Lewkowycz",
                "Anders Andreassen",
                "David Dohan",
                "Ethan Dyer",
                "Henryk Michalewski",
                "Vinay V. Ramasesh",
                "Ambrose Slone",
                "Cem Anil",
                "Imanol Schlag",
                "Theo GutmanSolo",
                "Yuhuai Wu",
                "Behnam Neyshabur",
                "Guy Gur-Ari",
                "Vedant Misra"
            ],
            "title": "Solving quantitative reasoning problems with language models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Jiwei Li",
                "Minh-Thang Luong",
                "Dan Jurafsky"
            ],
            "title": "A hierarchical neural autoencoder for paragraphs and documents",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Jack Hessel",
                "Youngjae Yu",
                "Xiang Ren",
                "Kai-Wei Chang",
                "Yejin Choi"
            ],
            "title": "Symbolic chain-of-thought distillation: Small models can also \u201dthink\u201d step-by-step",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Kevin J. Liang",
                "Weituo Hao",
                "Dinghan Shen",
                "Yufan Zhou",
                "Weizhu Chen",
                "Changyou Chen",
                "Lawrence Carin"
            ],
            "title": "Mixkd: Towards efficient distillation of large-scale language models",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Lin",
                "Shujie Liu",
                "Muyun Yang",
                "Mu Li",
                "Ming Zhou",
                "Sheng Li"
            ],
            "title": "Hierarchical recurrent neural network for document modeling",
            "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans"
            ],
            "title": "Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, pp. 3214\u20133252",
            "venue": "Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Renqian Luo",
                "Liai Sun",
                "Yingce Xia",
                "Tao Qin",
                "Sheng Zhang",
                "Hoifung Poon",
                "Tie-Yan Liu"
            ],
            "title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
            "venue": "Briefings in Bioinformatics, 23(6),",
            "year": 2022
        },
        {
            "authors": [
                "Piotr Nawrot",
                "Szymon Tworkowski",
                "Michal Tyrolski",
                "Lukasz Kaiser",
                "Yuhuai Wu",
                "Christian Szegedy",
                "Henryk Michalewski"
            ],
            "title": "Hierarchical transformers are more efficient language models. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 1559\u20131571",
            "venue": "Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Jianmo Ni",
                "Jiacheng Li",
                "Julian J. McAuley"
            ],
            "title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L. Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray",
                "John Schulman",
                "Jacob Hilton",
                "Fraser Kelton",
                "Luke Miller",
                "Maddie Simens",
                "Amanda Askell",
                "Peter Welinder",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "Technical report,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Nasim Rahaman",
                "Aristide Baratin",
                "Devansh Arpit",
                "Felix Draxler",
                "Min Lin",
                "Fred Hamprecht",
                "Yoshua Bengio",
                "Aaron Courville"
            ],
            "title": "On the spectral bias of neural networks",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Petr Sgall",
                "Eva Hajicov\u00e1",
                "Jarmila Panevov\u00e1"
            ],
            "title": "The meaning of the sentence in its semantic and pragmatic aspects",
            "venue": "Springer Science & Business Media,",
            "year": 1986
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Tao Lei",
                "Regina Barzilay",
                "Tommi S. Jaakkola"
            ],
            "title": "Style transfer from non-parallel text by cross-alignment",
            "venue": "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Alan Karthikesalingam",
                "Vivek Natarajan"
            ],
            "title": "Towards expert-level medical question answering with large language models",
            "venue": "CoRR, abs/2305.09617,",
            "year": 2023
        },
        {
            "authors": [
                "Antonio Torralba",
                "Alexei A. Efros"
            ],
            "title": "Unbiased look at dataset bias",
            "venue": "In The 24th IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2011
        },
        {
            "authors": [
                "Xinyi Wang",
                "Wanrong Zhu",
                "Michael Saxon",
                "Mark Steyvers",
                "William Yang Wang"
            ],
            "title": "Large language models are implicitly topic models: Explaining and finding good demonstrations for incontext learning",
            "venue": "In Workshop on Efficient Systems for Foundation Models @ ICML2023,",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "venue": "ACL",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Chaoyi Wu",
                "Weixiong Lin",
                "Xiaoman Zhang",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "title": "Pmc-llama: Towards building open-source language models for medicine, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Zhi-Qin John Xu",
                "Yaoyu Zhang",
                "Yanyang Xiao"
            ],
            "title": "Training behavior of deep neural network in frequency domain",
            "venue": "In Neural Information Processing - 26th International Conference, ICONIP 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Jure Leskovec",
                "Percy Liang"
            ],
            "title": "Linkbert: Pretraining language models with document links. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, pp. 8003\u20138016",
            "venue": "Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Volume 1: Long Papers, pp. 4791\u20134800",
            "venue": "Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "LM D ADDITIONAL"
            ],
            "title": "EVALUATION Language model evaluation: general abilities We evaluate LLaMA 2 7B finetuned in our analysis on Hellaswag (Zellers et al., 2019), ARC (Challenge Set) (Clark et al., 2018) and MMLU (Hendrycks et al., 2021), using the Language Model Evaluation Harness framework",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) pre-trained on general corpus show impressive common-sense knowledge, reasoning ability, and zero-shot performance on a variety of tasks (OpenAI, 2023; Touvron et al., 2023; Chung et al., 2022). Finetuning LLMs on domain corpus further enhances their domain knowledge and ability, substantially improving performance on domain tasks (Lewkowycz et al., 2022; Chen et al., 2021; Singhal et al., 2023). However, it is also observed that finetuning language models can lead to forgetting of previously learned information (Jang et al., 2022; Chen et al., 2020), which is often mitigated in practice by mixing general corpus with domain data in finetuning (Rozie\u0300re et al., 2023; Ouyang et al., 2022).\nTo better understand the effect of finetuning on a language model (specifically, we study \u201cdomain finetuning\u201dof general models on a domain corpus), we perform a dissection analysis on how language models model different factors of text. We analysis the topic (the overall theme, e.g., \u201clanguage model finetuning\u201d), the style (the structure, tone and diction, e.g., academic writing in ICLR paper format), and the factual knowledge (detailed factual information, e.g., methods, citation, and results in this paper) as three main components of text. As language models represent probability distributions of text, this dissection allows us to observe how they assign probabilities to text of different content during finetuning. This gives a finer and alternative perspective compared to existing analysis based mainly on downstream task performance.\nTo understand the behavior of language models, a common approach is probing language models with specifically designed examples (Srivastava et al., 2022; Lin et al., 2022). We create samples of text with specific combinations of content and style and use them to query the language model\u2019s likelihood. We study open LLMs such as LLaMA (Touvron et al., 2023) domain finetuned with the conventional causal language modeling recipe used in pre-training. Following a recent trend of automatic data generation with LLMs (Honovich et al., 2023; Ho et al., 2023), we use ChatGPT to\nsystematically generate high-quality text samples, enabling controlled-variable probing of language models with minimal human effort in data curation.\nOur investigation reveals that while domain finetuning enhances domain knowledge, it also induces strong topic and style biases in the language model towards the training data, making the model much less likely to generate text with other topics and styles. More interestingly, we found many characteristics that differentiate the learning dynamics of simple topic and style biases vs. factual knowledge. The following two findings summarizes the main contributions of our analysis:\n\u2022 Domain finetuning leads to a significant change in the topic and style priors of the language model, biasing them towards the training data. Effect caused by such bias dominates the learning and forgetting observed in finetuning. The learning of factuals knowledge only contributes to a small part of the change in modeling probabilities, which offers a possible explanation of the difficulty in preserving general abilities while assimilating knowledge in domain finetuning.\n\u2022 Topic and style biases are learned like simple features, while factual knowledge are learned like complex features in finetuning. Biases are learned rapidly with a strength growing with the learning rate, and they require little model capacity to learn. Even considerable dataset debiasing only partially mitigates them. The biases are also predominantly acquired at the beginning of the text sequence, independent of other biases. In contrast, factual knowledge is learned stably, relatively unaffected by token position, learning rate, or data mixture. The learning of factual knowledge also requires significant model capacity available for finetuning.\nOur finding suggests that domain finetuning of language models has potential for improvement in the light of a better understanding of the learning dynamics. They could also help us identify the sources of catastrophic forgetting (French, 1999) in language models in order to facilitate effective lifelong learning of general purpose LLMs. Our data 1 and code 2 are made publicly available."
        },
        {
            "heading": "2 METHOD",
            "text": ""
        },
        {
            "heading": "2.1 ESTIMATING CONTENT AND STYLE PROBABILITIES IN A LANGUAGE MODEL",
            "text": "Our method involves estimating content and style probabilities under a language model by querying it with specific text examples. With a generative model p of text, we can roughly decompose the probability of a document x into its generating factors. In this study, we assume that x is mainly determined by three factors: topic (the main topic of text), style (the writing style), and factual (the factual knowledge included in the text):\np(x) = p(topic, style, factual) = p(topic)p(factual|topic)p(style|topic,factual) = p(topic)p(factual|topic)p(style|topic) (1)\nNote that the decomposition is only approximate and may not reflect the true generating process of text. The factors, their granularity, and the order of dependence are chosen for convenience of the analysis of the particular factor we are interested in. To simplify the analysis, we make a reasonable assumption that the factual and style are independent given the topic.\nSuppose we want to estimate the probability of different styles under model p: consider two documents xA and xB sharing an identical topic and factual content but written in styles A and B, respectively. The likelihood ratio between these documents under p becomes the likelihood ratio of the two styles (conditioned on the content).\np(xA) p(xB) = p(styleA|topic) p(styleB |topic)\n1https://huggingface.co/datasets/xiaozeroone/pubmed_derived* 2https://github.com/xiaozeroone/lm_finetune_dissect*\nNow that we want to estimate the likelihood of style A vs. style B, we can use a dataset of document pairs {(xiA, xiB)}Ni=1, where xiA and xiB only differ in style. All documents also have the same topic. The likelihood ratio can be estimated by averaging over the dataset to smooth out its possible dependency on specific documents:\nlog p(styleA|topic) p(styleB |topic) \u2248 1 N N\u2211 i=1 log p(xiA) p(xiB) (2)\n= 1\nN N\u2211 i=1 log p(xiA)\u2212 1 N N\u2211 i=1 log p(xiB) (3)\nwhich can be easily calculated for causal language models as the difference between the average cross-entropy loss on the two set of examples {xiA}Ni=1 and {xiB}Ni=1. The likelihood ratio between various topics can be similarly estimated by changing the order of decomposition in Eq. 1. We do not get the raw probability, e.g., p(sports), but we can use the likelihood ratio, e.g., p(sports)/p(politics), to learn about the topic probabilities. Though language models do not explicitly learn a topic distribution like LDA topic models (Blei et al., 2003), they could model an implicit topic variable through approximate Bayesian inference (Wang et al., 2023a).\nFor the factual factor, we are interested in the likelihood ratio of factual vs. counterfactuals, e.g., p(\u201cthe sky is blue\u201d)/p(\u201cthe sky is red\u201d), because such ratio represents the modeling of knowledge in the model. Calculating the ratio would require pairs of documents that use factural and counterfactuals with the same topic and style."
        },
        {
            "heading": "2.2 MANIPULATING CONTENT AND STYLE IN TEXT WITH INSTRUCTION-FOLLOWING LLMS",
            "text": "Documents that differ in only one factor, e.g., style, might not be easy to find in existing corpus. We leverage the language understanding and instruction following capabilities of instruction-finetuned LLMs to rewrite existing documents, letting it identify and manipulate the content and the style of text. We found that with appropriate prompts, LLMs such as ChatGPT can generate high-quality rewrites of a passage, altering the style while preserving the content and vice versa. For example, we can explicitly ask ChatGPT to change the topic, the factual, or the style of a passage, while keeping other elements unchanged (Figure 1):\nThe results from rewriting show that ChatGPT effectively satisfies these strict rewriting requirements. For instance, it can produce new content compatible with the original text\u2019s structure and\nstyle. We found that GPT-3.5 is capable enough for this task, although GPT-4 (OpenAI, 2023) produces more successful rewrites for harder cases, and its performance is less sensitive to the prompt.\nEffective rewriting consists of generating good quality text and adhering to the instruction. We use language modeling perplexity as a measure of quality and naturalness and found that the ChatGPT generated rewrites typically have a low increase in perplexity which indicates their quality. Adherence is measured by human judging whether the rewrite successfully complies with the instruction. We found that ChatGPT has a high success rate of around 95% (see Appendix A.3 for evaluation)."
        },
        {
            "heading": "3 RESULTS",
            "text": ""
        },
        {
            "heading": "3.1 ANALYSIS SETUP",
            "text": "Data. We utilize two corpus in our analysis: PubMed1, a collection of biomedical papers abstracts, and C4 (Raffel et al., 2020), a large corpus of web text. PubMed is commonly used in finetuning language models for the biomedical domain (Yasunaga et al., 2022; Luo et al., 2022; Wu et al., 2023). We use it as a representative of domain corpus and use it to finetune LLMs. We use C4 as a representative of general-domain corpus and use it for evaluation.\nFor probing the content and style probabilities as in Section 2.1, we use ChatGPT to rewrite documents from PubMed and C4 as described in Section 2.2. To make the analysis tractable, we sample two random subsets of 1000 documents from PubMed and C4, and then rewrite the documents with ChatGPT to generate documents with their topic, style, and factual changed. The generated derived datasets are listed in Table 1. Instructions used for generating each dataset and examples from the derived datasets are listed in Appendix A.1 and A.2.\nIn the following analysis, we calculate log-likelihood ratios by subtracting the negative causal language modeling loss l between a derived and an original dataset as in Equation 3. For example, to measure the likelihood of biomedical topic vs. nonbiomedical topic, we calculate\nlog p(biomedical)\np(nonbiomedical) = l(PubMed)\u2212 l(PubMed-nonbiomedical)\nWhile we focus on the PubMed corpus in most parts of our analysis, we also apply the same protocol to two more domain corpus, Pile of Law Henderson et al. (2022) in the legal domain and Amazon reviews Ni et al. (2019) in the customer review domain, for comparative analysis. Results of limited experiments on those two domains are deferred to Appendix D.\nFinetuning setup. We finetune three language models, GPT-2 XL (Radford et al., 2019), LLaMA 2 7B and LLaMA 2 13B (Touvron et al., 2023), on the PubMed abstracts using conventional causal\n1https://pubmed.ncbi.nlm.nih.gov. We use the annual baseline data of 2023.\nlanguage modeling loss. We finetune models on subsets of different sizes, up to 1 million abstracts. We use both full-finetuning and low-rank finetuning (Hu et al., 2022). We use AdamW optimizer (Loshchilov & Hutter, 2019) with a learning rate of 3e-6 for full-finetuning LLaMA and 1e-4 for full-finetuning GPT-2 XL and low-rank finetuning of LLaMA, all with 10% warm-up and linear learning rate decay. Learning rates are selected for each model using a grid search on a validation set. The batch size is set to 64. Other details of finetuning can be found in Appendix B."
        },
        {
            "heading": "3.2 THE CHANGING TOPIC AND STYLE PRIORS DURING LM FINETUNING",
            "text": "Domain finetuning leads to significant change in topic and style probabilities. Figure 2 shows the change of likelihood ratios between different topics, styles and factual during finetuning. The likelihood of the dominant topic (biomedical) and style (academic) in the PubMed corpus increases significantly during finetuning with respect to other topics and styles. This implies an increase in the prior probability of the training topic and style and a decrease of other topics and styles in the finetuned model. Comparing the likelihood ratio of styles academic/casual with academic/rap, it is clear that the probabilities of styles that are more different from the training style (rap) have greater reduction than styles that are closer to the training style (casual).\nAll the likelihood ratios change monotonically, with most showing an approximate loglinear relationship with the amount of finetuning data. The topic and style prior probabilities are continually biasing towards the finetuning data. The factual/counterfactual likelihood ratio changes at a slower rate, reflecting the learning of new factual knowledge from the domain data and the forgetting of factual knowledge in the pre-training data. We show that the factual ratio correlates well with downstream question answering performance in Appendix D.\nLearned topic and style biases are independent. Figure 3 shows the likelihood ratios of topics conditioned on different styles and of styles conditioned on different topics. Notably,\nthe likelihood ratios of topics are changing similarly under different styles and vice versa, except for very small training sizes. This suggests that the learned topic and style biases in finetuning are generally independent of each other.\nThis independence would allow us to drop the conditioning in the likelihood ratios in Equation 2 and let us study the change of topic and style probabilities separately."
        },
        {
            "heading": "3.3 ABLATING LEARNING AND FORGETTING IN LM FINETUNING",
            "text": "Evaluation on derived datasets allows us to ablate the effect of learning and forgetting in language model finetuning by introducing or removing one factor at a time. Here, we use learning to refer to the loss reduction on the domain corpus and forgetting to refer to the loss increase on a general corpus (which roughly represents the data distribution in pre-training) in language model finetuning.\nFigure 4 shows the ablation results, with learning measured on PubMed and forgetting measured on C4. The adaptation to biomedical topic and academic style contributes to the main part of the loss reduction on PubMed and loss increase on C4. This shows that the change of topic and style prior probabilities is the main cause of the observed learning and forgetting in language model finetuning.\nHowever, the goal of finetuning on domain corpus is usually acquiring domain knowledge rather than adapting to the topic and style of the domain text. We can see that the learning and forgetting of factuals is steadily increasing with the amount of training data, although it only contributes to a small portion of the total loss change. This shows that adaptation to domain topic and style is a significant and probably unavoidable side effect of domain finetuning. This overly strong adaptation is one possible reason for the catastrophic forgetting observed in the finetuning of language models."
        },
        {
            "heading": "3.4 CHARACTERISTICS OF BIAS LEARNING AND KNOWLEDGE LEARNING",
            "text": "We next delve into the distinct characteristics of bias learning and factual knowledge learning during language model finetuning.\nTopic and style biases are most significant on the first few tokens and are learned quickly. To look at more details on how each factor affects the probability of a document, we compute likelihood ratios separately for tokens at various positions within the text. Figure 5 shows that the likelihood ratios are clearly changing in different ways for the first few tokens and later tokens. The topic and style biases are much more significant at the beginning of the document (position \u226410) and are quickly learned with 1-10k documents. This implies that in unconditional generation, the finetuned language model will be much more likely to generate text with the topic and style of the finetuning data by preferring those topics and styles early in generation.\nFor later tokens (position \u2265100), the topic and style biases are much weaker in comparison but are growing steadily with increased training data. The biases in later tokens seem quite consistent regardless of position till the end of text, thus contributing significant change to the whole document\u2019s probability. This part likely affects the conditional generation of language models by slightly biasing the generated text towards the topic and style of the finetuning data in each generation step.\nCompared to topic and style biases, the learning of factuals appears more uniform across positions. This is because factual information can appear at any position, and there can be independent appearances of multiple factuals within one document. The difference in learning speed is likely that topic and syle are simple features that are easier to learn, and that the model may alrady seen these features during pre-training on general corpora, unlike factual knowledge which are more domain-specific.\nTopic and style biases require minimal capacity to learn, knowledge learning requires much more. To examine the different natures of bias and knowledge learning, we finetune LLaMA 2 7B with variable numbers of trainable parameters to simulate different capacities available during finetuning. In full-finetuning, all parameters are tunable. In low-rank finetuning, only a small number of parameters are tunable, controlled by the rank r. For example, for r = 8, 2, and 1, the number of tunable parameters is 0.3%, 0.07%, and 0.04% of the total model parameters.\nFigure 6 compares the change of likelihood ratios with full and low-rank finetuning at different r. Interestingly, topic and style biases are learned comparably to full finetuning with just 0.02% of tunable parameters. On the other hand, factual learning is significantly hindered by low-rank finetuning at large training sizes. This suggests that the changes of topic and style probabilities are simple biases that only require adjustments in a low-dimensional subspace of the model\u2019s representations, whereas factual knowledge learning may encapsulate encoding a large number of complex patterns which requires much more model capacity.\n(Side note: the learning of factuals can cause a decrease of l(PubMed) therefore is also affecting the topic ratio p(biomedical)/p(nonbiomedical) on PubMed. When capacity is limited, the topic ratio and factual ratio simultaneously reduce on Pubmed in Figure 6.)\nTopic and style biases magnify with learning rate, knowledge learning does not. We also examine the effect of learning rate on the learning of different factors. Figure 7 shows that the learned topic and style biases increase with the learning rate and are non-saturating, while the learning of factual remains consistent and does not increase with the learning rate.\nThis shows that a large learning rate magnifies learned bias, which is also correlated with the forgetting of general abilities (evaluated in Appendix D). A smaller learning rate might suffice for knowledge learning and offers a better tradeoff between learning new knowledge and preserving existing knowledge and abilities in domain finetuning.\nMixing unbiased data reduces learned bias, but only to a limited degree. Mixing general corpus with domain corpus is a common strategy to avoid forgetting and over-adaptation. We examine the effect of data mixture using likelihood ratios in Figure 8, mixing Wikipedia text1 into the PubMed corpus. The results indicate that mixing a small portion of general text reduces the learned biases, but the reduction is limited and only increases modestly with the proportion of general text. This shows that while it is possible to reduce the learned biases without affecting knowledge learning by mixing a small portion of general corpus, eliminating or considerably attenuating the biases may require a high general-to-domain data ratio, making it very uneconomic in terms of training cost."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Dataset bias and shortcut learning. Datasets used in machine learning often inevitably contain biases in the data distribution (Torralba & Efros, 2011). These superficial correlations in the data can be learned as a shortcut to achieve good performance on the training set (Geirhos et al., 2020). The issue is more pronounced in neural networks due to the tendency to learn simple features first, a phenomenon known as spectral bias (Rahaman et al., 2019; Xu et al., 2019). By adapting to biases, language models can achieve loss reduction without learning much underlying knowledge. Such \u201csurface learning\u201d (Geirhos et al., 2020) is analogous to the \u201cprinciple of least effort\u201d in linguistics where language speakers generally try to minimize effort in communication (Chang, 2016).\nContinual learning in language models. Finetuning pre-trained language models can improve model\u2019s performance on a new domain (Chen et al., 2021; Lewkowycz et al., 2022) or a series of domains via continual pre-training (Gupta et al., 2023; Jin et al., 2022; Ke et al., 2023). Jang et al. (2022) specifically study knowledge learning in continual pre-training. Forgetting is frequently observed in continual pre-training and all the above work implement techniques to alleviate forgetting. Rehearsal (Chaudhry et al., 2019), regularization (Kirkpatrick et al., 2017), parameter isolation (Rusu et al., 2016), or a combination of multiple methods are often used. Mixing general corpus into the finetuning data also serves as a particular form of rehearsal.\nData generation with LLMs. LLMs such as GPT-3 (Brown et al., 2020) have been used to label examples for a variety of tasks (Liang et al., 2021; Hsieh et al., 2023). The generated labels can be used to train smaller specialized models as a form of knowledge distillation (Hinton et al., 2015). LLMs have also been used to generate rationales and reasoning steps, enabling the transfer of reasoning abilities (Fu et al., 2023; Ho et al., 2023; Li et al., 2023). They also generates instruction data for instruction-tuning and alignment of LMs (Wang et al., 2023b; Honovich et al., 2023).\nDecomposition analysis of text. Separating the content and form has been a traditional approach in literary theories Eagleton (2011). Content analysis includes aspects like themes, ideas, and the narrative, while form (style) analysis deals with the use of literary devices like metaphors, tones, and the organization of the text. Mutiple linguistic theories further decompose the content of text into an overall topic and specific information, for example topic-focus articulation (Sgall et al., 1986) and theme-rheme analysis (Halliday, 1994).\n120230901 dump from https://dumps.wikimedia.org, English only\nIn machine learning, the three components constitute invidividual topics of study. For example, topic modeling (Hofmann, 1999; Blei et al., 2003) studies the topic distribution of text, style transfer studies manipulation of style (Shen et al., 2017) and how to separate style from content (Fu et al., 2018). Information extraction (Brin, 1998; Banko et al., 2007) studies identifying factual information from text. Also, in document modeling, several work uses a hierachical structure to model the overall theme and specific information in text(Lin et al., 2015; Li et al., 2015; Nawrot et al., 2022), in a similar spirit as we did in this work."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "Domain corpus used in language model finetuning can often exhibit significant homogeneity in topic and style, creating a statistically simple and salient feature easily learned by the model (Rahaman et al., 2019). We show that such adaptation creates a strong bias in the language model towards the training distribution. The bias stably increases with the amount of training data and can overshadow the learning of knowledge. The presence of a strong bias potentially makes the evaluation of knowledge learning more difficult, as an overly strong bias might interfere with the general reasoning abilities of the model.\nOur observation shows that the topic and style biases are learned quickly and require little model capacity, which could mean that bias learning is hard to avoid in domain finetuning. Mixing general corpus in the finetuning data reduces the bias but adds significant training cost. This also poses a challenge for lifelong learning of language models. For example, when LLMs are used as general purpose agents, we want them to learn new knowledge from data without adapting too much to any individual data distribution.\nWhile the current study aim to uncover the learning dynamics in domain finetuning, we believe that by identifying bias learning as a major hindrance in domain finetuning and showing the distinct behaviors of bias learning and knowledge learning, we also pointed out potential directions to improve knowledge learning and forgetting mitigation. For example, based on the observation that bias learning mostly happens on the first few tokens of each sequence, we could mask out the loss from the first few tokens in the finetuning objective to expect reduced bias learning. Based on the different capacity requirement of bias and knowledge learning, in principle we could use a small low-rank adapter to learn the bias, and subtract its weights from the full finetuned model to remove the bias while keeping the learned knowledge. We leave the exploration of such methods for future work.\nLimitations. While our analysis leads to interesting findings on the learning dynamics of language models, it is limited in the following ways:\n\u2022 Separability of text-generating factors: the decomposition of generating factors is only approximate and there may not be a generally agreed way to decompose. The boundary between content and style is not always clear, for example, terminology use is part of content and is also part of style. Interdependence between content and style sometimes prevents changing one factor without changing the other. In most domain corpora, the separation is clear enough for our analysis, evidenced by the quality of rewritten documents.\n\u2022 Quality of rewriting with LLMs: several issues may limit the quality of generated rewrites. Safety alignment: requests for rewriting in the biomedical domain are sometimes rejected by LLM due to safety alignment to reduce harmful outputs (Kenton et al., 2021). Pretraining bias: LLMs may tend to generate text with certain topics or styles under a general instruction, which may create a bias in the generated data. Hallucination (Ji et al., 2023): LLMs have a certain probability of generating factually inaccurate content.\n\u2022 Data dependency: the quantitative observations would reflect certain characteristics of the corpus (for example, the style adaptation in training depends on the style distribution in the corpus). We compare with more domain corpus in Appendix D and found our qualitative observations generalizes to other domains.\n\u2022 Limited training: we only finetuned on a maximum of 1 million documents (<1B total tokens). Although we observe a consistent trend of learning of different factors with the amount of training data, it may not generalize to very large training sizes."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The work is supported by National Key R&D Program of China (2021ZD0113402). We thank the anonymous reviewers for helpful comments and feedback."
        },
        {
            "heading": "A DATA",
            "text": ""
        },
        {
            "heading": "A.1 DATASET GENERATION",
            "text": "Instructions used for each generating each derived datasets are listed below. A non-cherry picked example is also provided.\nWe found that ChatGPT (GPT-3.5-turbo) is capable enough for this task with some tuning of the prompts. We also found that GPT-4 require significantly less tuning of the prompts to produce successful rewrites, and can produce successful rewrites for examples on which ChatGPT fails. We use ChatGPT (GPT-3.5-turbo) with the following prompts for generating the derived datasets for the analysis.\nWe use nucleus sampling with p=0.9 to reduce the likelihood of generating low-quality rewrites."
        },
        {
            "heading": "PubMed-nonbiomedical",
            "text": "Prompt:\nPlease rewrite the following passage by changing its topic to an irrelevant topic in art, finance, education or software. - please keep the structure of the passage strictly unchanged - please directly give the output without any comments"
        },
        {
            "heading": "PubMed-counterfactual",
            "text": "Prompt:\nPlease change the biomedical terms in the following passage into other random biomedical terms so that the biomedical knowledge is disrupted. - please keep the main topic and the words that are not biomedical terms unchanged - please directly give the output without any comments"
        },
        {
            "heading": "PubMed-casual",
            "text": "Prompt:\nPlease rewrite the following passage using a casual style. - please keep the content (including all terminology) strictly unchanged - please directly give the output without any comments"
        },
        {
            "heading": "PubMed-rap",
            "text": "Prompt:\nPlease rewrite the following passage using the style of rap. - please keep the content (including all terminology) strictly unchanged - please directly give the output without any comments"
        },
        {
            "heading": "C4-biomedical",
            "text": "Prompt:\nPlease rewrite the following passage by replacing its main topic with a biology or medicine related topic (for instance, some disease, diagnosis, drug, or treatment). - please keep the style and the structure of the passage unchanged - please directly give the output without any comments"
        },
        {
            "heading": "C4-counterfactual",
            "text": "Prompt:\nPlease swap all the nouns in the following passage into random related nouns so that every piece of information given become random and completely different from the original. - every piece of information must be changed - please directly give the output without any comments\nC4-academic\nPrompt:\nPlease rewrite the following passage using the style of an abstract of a research paper (without title). - please keep all the content strictly unchanged - please directly give the output without any comments\nWe randomly sampled 1000 documents from PubMed and C4 (the validation split) respectively (each document have at least 500 characters), and generated derived datasets from the samples. The generated datasets are listed in Table 1."
        },
        {
            "heading": "A.2 EXAMPLES OF GENERATED TEXT",
            "text": ""
        },
        {
            "heading": "PubMed (original)",
            "text": "According to preliminary data, seroconversion after mRNA SARS-CoV-2 vaccination might be unsatisfactory in Kidney Transplant Recipients (KTRs). However, it is unknown if seronegative patients develop at least a cellular response that could offer a certain grade of protection against SARS-CoV-2. To answer this question, we prospectively studied 148 recipients of either kidney (133) or kidney-pancreas (15) grafts with assessment of IgM/IgG spike (S) antibodies and ELISpot against the nucleocapside (N) and the S protein at baseline and 2 weeks after receiving the second dose of the mRNA-1273 (Moderna) vaccine. At baseline, 31 patients (20.9%) had either IgM/IgG or ELISpot positivity and were considered to be SARS-CoV-2-pre-immunized, while 117 (79.1%) patients had no signs of either cellular or humoral response and were considered SARS-CoV-2-na\u0131\u0308ve. After vaccination, na\u0131\u0308ve patients who developed either humoral or cellular response were finally 65.0%, of which 29.9% developed either IgG or IgM and 35.0% S-ELISpot positivity. Factors associated with vaccine unresponsiveness were diabetes and treatment with antithymocytes globulins during the last year. Side effects were consistent with that of the pivotal trial and no DSAs developed after vaccination. In conclusion, mRNA1273 SARS-CoV-2 vaccine elicits either cellular or humoral response in almost two thirds of KTRs."
        },
        {
            "heading": "PubMed-nonbiomedical",
            "text": "According to preliminary data, the use of grayscale in abstract painting might be unsatisfactory in terms of conveying emotions. However, it is unknown if paintings created without grayscale can still evoke a certain level of emotional response. To answer this question, we conducted a prospective study involving 148 artists who work primarily with either acrylic (133) or oil (15) paints, assessing the use of grayscale and color saturation in their artworks. The assessment was done at baseline and 2 weeks after the artists experimented with incorporating grayscale into their paintings. At baseline, 31 artists (20.9%) had already used grayscale or experimented with it in their artworks, and were considered pre-immunized to grayscale, while 117 (79.1%) artists had no signs of using grayscale and were considered grayscale-na\u0131\u0308ve. After the experiment, grayscale-na\u0131\u0308ve artists who incorporated grayscale into their paintings accounted for 65.0% of the participants, with 29.9% using grayscale as part of their color palette and 35.0% achieving grayscale effects through techniques such as cross-hatching. Factors associated with unresponsiveness to grayscale experimentation were the artists\u2019 preference for vibrant colors and their training in traditional painting techniques. Overall, the results indicate that the use of grayscale in abstract paintings can elicit a response in almost two thirds of artists."
        },
        {
            "heading": "PubMed-counterfactual",
            "text": "According to preliminary data, seroconversion after DNA HIV-1 vaccination might be unsatisfactory in Brain Tumor Recipients (BTRs). However, it is unknown if seronegative patients develop at least a cellular response that could offer a certain grade of protection against HIV-1. To answer this question, we prospectively studied 148 recipients of either brain (133) or brain-pancreas (15) grafts with assessment of IgM/IgG spike (S) antibodies and ELISpot against the nucleocapside (N) and the S protein at baseline and 2 weeks after receiving the second dose of the DNA-7532 (GenBio) vaccine. At baseline, 31 patients (20.9%) had either IgM/IgG or ELISpot positivity and were considered to be HIV-1-pre-immunized, while 117 (79.1%) patients had no signs of either cellular or humoral response and were considered HIV-1-na\u0131\u0308ve. After vaccination, na\u0131\u0308ve patients who developed either humoral or cellular response were finally 65.0%, of which 29.9% developed either IgG or IgM and 35.0% S-ELISpot positivity. Factors associated with vaccine unresponsiveness were diabetes and treatment with antithymocytes globulins during the last year. Side effects were consistent with that of the pivotal trial and no DSAs developed after vaccination. In conclusion, DNA-7532 HIV-1 vaccine elicits either cellular or humoral response in almost two thirds of BTRs.\nPubMed-casual\nSo, here\u2019s the deal: the early data suggests that people who have undergone kidney transplants (yeah, those Kidney Transplant Recipients, or KTRs) might not be getting a good immune response after getting the mRNA SARS-CoV-2 vaccine. But here\u2019s the thing we don\u2019t know yet: even if these patients don\u2019t have antibodies, could they still have some sort of cellular response that gives them at least some level of protection against SARS-CoV-2? To find out, we did a study with 148 people who got either a kidney (133 of them) or a kidneypancreas (15 of them) transplant. We checked their IgM/IgG spike (S) antibodies and did ELISpot tests to see their response to the nucleocapside (N) and S proteins before the vaccine and two weeks after they got the second dose of the mRNA-1273 (Moderna) vaccine. At the start, we found that 31 patients (that\u2019s about 20.9% of them) already had either IgM/IgG antibodies or a positive ELISpot result, meaning they had some level of pre-existing immunity to SARS-CoV-2. The other 117 patients (about 79.1% of them) showed no signs of either a cellular or humoral response and were considered SARS-CoV-2-na\u0131\u0308ve. After getting vaccinated, we found that 65.0% of the initially na\u0131\u0308ve patients ended up developing either a humoral or a cellular response. Out of those, 29.9% had either IgG or IgM antibodies, and 35.0% had a positive S-ELISpot result. We also looked at some factors that might be linked to not responding well to the vaccine, and found that diabetes and treatment with antithymocyte globulins in the past year were associated with a lower response. The side effects were pretty much what we expected based on the main trial, and we didn\u2019t see any development of donor-specific antibodies after vaccination. So, to wrap it up, the mRNA-1273 SARS-CoV-2 vaccine seems to trigger either a cellular or a humoral response in almost two-thirds of kidney transplant recipients."
        },
        {
            "heading": "PubMed-rap",
            "text": "Yo, check it out, here\u2019s the deal, Preliminary data reveals what\u2019s real, Seroconversion after SARS-CoV-2 vax, Might not be satisfactory in KTRs, that\u2019s the facts. But hold up, can seronegative patients shine? With some cellular response to draw the line, Against SARS-CoV-2, can they protect? That\u2019s the question we\u2019re here to dissect. Prospective study, 148 recipients in sight, Kidney or kidney-pancreas, they\u2019re in the fight, Testing IgM/IgG spike antibodies, so fly, ELISpot against N and S, let\u2019s give it a try. At baseline, 31 patients were pre-immunized, With IgM/IgG or ELISpot, they were baptized, But 117 patients showed no response at all, SARS-CoV-2-na\u0131\u0308ve, they stood tall. After vaccination, the game changed, 65.0% of the na\u0131\u0308ve were rearranged, With humoral or cellular response in their veins, 29.9% with IgG/IgM, the victory remains. And 35.0% with S-ELISpot positivity, They\u2019re building defenses with ferocity, Factors linked to unresponsiveness, it\u2019s true, Diabetes and antithymocyte treatment can skew. Side effects aligned with the pivotal trial, No DSAs developed, that\u2019s a smile, In conclusion, the mRNA-1273 vaccine, Brings cellular or humoral response on the scene. KTRs, listen up, almost two-thirds rejoice, With immunity rising, you have a choice, To fight off SARS-CoV-2, stand tall and bold, Thanks to Moderna\u2019s vaccine, the story unfolds."
        },
        {
            "heading": "A.3 QUALITY ASSESSMENT OF GENERATED TEXT",
            "text": "Quality. The per-token perplexity of the original and derived datasets under LLaMA 2 13B is listed in Table 2. We can see that the perplexity of the derived datasets are higher than the original datasets but not much (except for the counterfactual rewrites which produce text contradicting common knowledge, therefore can have high perplexity under a language model). The generated\ntext are of acceptable quality and are not significantly different from the original text in terms of natural-ness judged by a language model.\nAdherence. We also evaluate the adherence of the generated text to the instruction. We randomly sampled 100 examples from each derived dataset and asked human annotators to label whether the generated text successfully comply to the instruction. \u201cGood\u201d means the generated text generally comply to the instruction, \u201cpartial\u201d means the generated text only comply to part of the instruction (for example, the text was successfully changed to the requested style but the content was also significantly changed), and \u201cbad\u201d means the generated text does not comply to the instruction. The results are listed in Table 3. We can see that the generated text are generally of high adherence to the instruction. It seems that changing style has a higher success rate than changing content, which could mean that identifying and changing style is easier for a LLMs."
        },
        {
            "heading": "B FINETUNING SETUP",
            "text": "To determine the learning rate, we perfomed grid search and train models under 100k documents with learning rates {1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4} for full-finetune and {1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3} for low-rank finetune. The final learning rate is chosen as the one that gives the lowest perplexity on the validation set. We use a batch size of 64.\nDuring finetuning, we treat each document as a separate sequence to keep the structure of the document for better analysis of per-position likelihoods. This is different from a common LM pre-training setup where all the documents are concatenated together. All documents are truncated to a maximum of 1024 tokens.\nFinetuning is performed with Huggingface\u2019s transformer library (Wolf et al., 2020), with bfloat16 mix-precision on NVIDIA A100 GPUs."
        },
        {
            "heading": "C RESULTS ON MORE DOMAIN DATA",
            "text": "To explore the effect of finetuning on other domain data, we perform similar analysis on the legal domain and the customer review domain with the setup in Section 3.1. For the legal domain, we use the Pile of Law corpus (Henderson et al., 2022). We finetune LLaMA 2 7B on up to 1M court opinions from the \u201cCourt Listener Opinions\u201d subset of the Pile of Law corpus. For the customer review domain, we use the Amazon reviews dataset (Ni et al., 2019). We finetune LLaMA 2 7B on up to 1M reviews of automotive products from the \u201cautomotive\u201d subset of the Amazon reviews dataset. We also rewrite the original documents to generate derived datasets. The datasets used are listed in Table 4.\nFigure 9 and 10 shows the change of likelihood ratios between different topics, styles and factual during finetuning on the legal and the customer review domain. Similar to the biomedical domain, the likelihood of the dominant topic and style in the training corpus increases significantly during finetuning with respect to other topics and styles. The Pile of Law data has a dominant topic of legal affairs and a dominant style of court opinion. The Amazon reviews data has a dominant topic of automotive products but the style is generally casual and more diverse than other domain text, therefore the adaptation of style prior is less significant. This shows that the presentation of dominant topic and style in the training corpus would invariably leads to strong adaptation of topic and style priors under the present language model finetuning regime.\nFor all the domains, the factual/counterfactual likelihood ratio changes at a significantly slower rate than the topic and style likelihood ratios, showing that the effect of topic and style adaptation on text modeling probabilities are much more significant than the effect of knowledge learning."
        },
        {
            "heading": "D ADDITIONAL LM EVALUATION",
            "text": "Language model evaluation: general abilities We evaluate LLaMA 2 7B finetuned in our analysis on Hellaswag (Zellers et al., 2019), ARC (Challenge Set) (Clark et al., 2018) and MMLU (Hendrycks et al., 2021), using the Language Model Evaluation Harness framework Gao et al. (2021). Zero-shot and 5-shot performance is presented in Table 5.\nLanguage model evaluation: medical knowledge To verify that medical knowledge is learned through finetuning on the PubMed corpus, we evaluate LLaMA 2 7B on clinical subsets from MMLU, following the Med-PaLM 2 paper (Singhal et al., 2023). Results (5-shot) are listed in Table 6. We further plot the factual/counterfactual likelihood ratio and the average accuracy on MMLU clinical subsets on the same graph in Figure 11. The two curves show a similar trend, indicating that the factual/counterfactual likelihood ratio is indeed an indicator of the learning of biomedical knowledge, to the degree that MMLU clinical subsets reflects knowledge learning on PubMed."
        }
    ],
    "title": "DISSECTING LEARNING AND FORGETTING IN LAN- GUAGE MODEL FINETUNING",
    "year": 2024
}