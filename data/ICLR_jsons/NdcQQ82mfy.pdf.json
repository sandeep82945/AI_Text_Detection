{
    "abstractText": "Branch-and-bound (B&B) has long been favored for tackling complex Mixed Integer Programming (MIP) problems, where the choice of branching strategy plays a pivotal role. Recently, Imitation Learning (IL)-based policies have emerged as potent alternatives to traditional rule-based approaches. However, it is nontrivial to acquire high-quality training samples, and IL often converges to suboptimal variable choices for branching, restricting the overall performance. In response to these challenges, we propose a novel hybrid online and offline reinforcement learning (RL) approach to enhance the branching policy by cost-effective training sample augmentation. In the online phase, we train an online RL agent to dynamically decide the sample generation processes, drawing from either the learning-based policy or the expert policy. The objective is to strike a balance between exploration and exploitation of the sample generation process. In the offline phase, a value function is trained to fit each decision\u2019s cumulative reward and filter the samples with high cumulative returns. This dual-purpose function not only reduces training complexity but also enhances the quality of the samples. To assess the efficacy of our data augmentation mechanism, we conduct comprehensive evaluations across a range of MIP problems. The results consistently show that it excels in making superior branching decisions compared to state-of-the-art learning-based models and the open-source solver SCIP. Notably, it even often outperforms Gurobi.",
    "authors": [
        {
            "affiliations": [],
            "name": "Changwen Zhang"
        },
        {
            "affiliations": [],
            "name": "Wenli Ouyang"
        },
        {
            "affiliations": [],
            "name": "Hao Yuan"
        },
        {
            "affiliations": [],
            "name": "Liming Gong"
        },
        {
            "affiliations": [],
            "name": "Yong Sun"
        },
        {
            "affiliations": [],
            "name": "Ziao Guo"
        },
        {
            "affiliations": [],
            "name": "Zhichen Dong"
        },
        {
            "affiliations": [],
            "name": "Junchi Yan"
        }
    ],
    "id": "SP:f8a3c58dc01ed0c7ba95d6bee9e8ade6bb82b629",
    "references": [
        {
            "authors": [
                "Tobias Achterberg",
                "Thorsten Koch",
                "Alexander Martin"
            ],
            "title": "Branching rules revisited",
            "venue": "Operations Research Letters,",
            "year": 2005
        },
        {
            "authors": [
                "Tobias Achterberg",
                "Thorsten Koch",
                "Alexander Martin"
            ],
            "title": "Branching rules revisited",
            "venue": "Operations Research Letters,",
            "year": 2005
        },
        {
            "authors": [
                "Alejandro Marcos Alvarez",
                "Quentin Louveaux",
                "Louis Wehenkel"
            ],
            "title": "A supervised machine learning approach to variable branching in branch-and-bound",
            "venue": "In ECML,",
            "year": 2014
        },
        {
            "authors": [
                "David Applegate",
                "Robert Bixby",
                "Va\u0161ek Chv\u00e1tal",
                "William Cook"
            ],
            "title": "Finding cuts in the tsp (a preliminary report)",
            "venue": "Technical report,",
            "year": 1995
        },
        {
            "authors": [
                "Christian Artigues",
                "Michel Gendreau",
                "Louis-Martin Rousseau",
                "Adrien Vergnaud"
            ],
            "title": "Solving an integrated employee timetabling and job-shop scheduling problem via hybrid branch-and-bound",
            "venue": "Computers & Operations Research,",
            "year": 2009
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Travis Dick",
                "Tuomas Sandholm",
                "Ellen Vitercik"
            ],
            "title": "Learning to branch",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Brucker",
                "Bernd Jurisch",
                "Bernd Sievers"
            ],
            "title": "A branch and bound algorithm for the job-shop scheduling problem",
            "venue": "Discrete applied mathematics,",
            "year": 1994
        },
        {
            "authors": [
                "Zixuan Cao",
                "Yang Xu",
                "Zhewei Huang",
                "Shuchang Zhou"
            ],
            "title": "Ml4co-kida: Knowledge inheritance in dataset aggregation",
            "venue": "arXiv preprint arXiv:2201.10328,",
            "year": 2022
        },
        {
            "authors": [
                "Zixuan Cao",
                "Yang Xu",
                "Zhewei Huang",
                "Shuchang Zhou"
            ],
            "title": "Ml4co-kida: Knowledge inheritance in dataset aggregation",
            "venue": "arXiv preprint arXiv:2201.10328,",
            "year": 2022
        },
        {
            "authors": [
                "Xinyue Chen",
                "Zijian Zhou",
                "Zheng Wang",
                "Che Wang",
                "Yanqiu Wu",
                "Keith Ross"
            ],
            "title": "Bail: Best-action imitation learning for batch deep reinforcement learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Santanu S. Dey",
                "Yatharth Dubey",
                "Marco Molinaro",
                "Prachi Shah"
            ],
            "title": "A theoretical and computational analysis of full strong-branching",
            "venue": "arXiv e-prints,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "Off-policy deep reinforcement learning without exploration",
            "year": 2018
        },
        {
            "authors": [
                "Gerald Gamrath",
                "Daniel Anderson",
                "Ksenia Bestuzheva",
                "Wei-Kun Chen",
                "Leon Eifler",
                "Maxime Gasse",
                "Patrick Gemander",
                "Ambros Gleixner",
                "Leona Gottwald",
                "Katrin Halbig"
            ],
            "title": "The scip optimization suite",
            "year": 2020
        },
        {
            "authors": [
                "Gerald Gamrath",
                "Timo Berthold",
                "Domenico Salvagnin"
            ],
            "title": "An exploratory computational analysis of dual degeneracy in mixed-integer programming",
            "venue": "EURO Journal on Computational Optimization,",
            "year": 2020
        },
        {
            "authors": [
                "Tanmay Gangwani",
                "Qiang Liu",
                "Jian Peng"
            ],
            "title": "Learning self-imitating diverse policies",
            "venue": "arXiv preprint arXiv:1805.10309,",
            "year": 2018
        },
        {
            "authors": [
                "Maxime Gasse",
                "Didier Ch\u00e9telat",
                "Nicola Ferroni",
                "Laurent Charlin",
                "Andrea Lodi"
            ],
            "title": "Exact combinatorial optimization with graph convolutional neural networks",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Shengcheng Shao",
                "Yuanming Zhu",
                "Dong Zhang",
                "Tao Quan",
                "Zixuan Cao",
                "Yang Xu",
                "Zhewei Huang",
                "Shuchang Zhou",
                "Chen Binbin",
                "He Minggui",
                "Hao Hao",
                "Zhang Zhiyu",
                "An Zhiwu",
                "Mao Kun"
            ],
            "title": "The machine learning for combinatorial optimization competition (ml4co): Results and insights",
            "venue": "Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track,",
            "year": 2021
        },
        {
            "authors": [
                "Wasu Glankwamdee",
                "Jeff Linderoth"
            ],
            "title": "Lookahead branching for mixed integer programming",
            "venue": "In International Conference on Supercomputing,",
            "year": 2011
        },
        {
            "authors": [
                "Yijie Guo",
                "Jongwook Choi",
                "Marcin Moczulski",
                "Samy Bengio",
                "Mohammad Norouzi",
                "Honglak Lee"
            ],
            "title": "Self-imitation learning via trajectory-conditioned policy for hard-exploration",
            "year": 2019
        },
        {
            "authors": [
                "Prateek Gupta",
                "Maxime Gasse",
                "Elias Khalil",
                "Pawan Mudigonda",
                "Andrea Lodi",
                "Yoshua Bengio"
            ],
            "title": "Hybrid models for learning to branch",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "A Hanif Halim",
                "IJAoCMiE Ismail"
            ],
            "title": "Combinatorial optimization: comparison of heuristic algorithms in travelling salesman problem",
            "venue": "Archives of Computational Methods in Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "Gregor Hendel"
            ],
            "title": "Enhancing mip branching decisions by using the sample variance of pseudo costs",
            "venue": "In International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research,",
            "year": 2015
        },
        {
            "authors": [
                "A. Hussein",
                "M.M. Gaber",
                "E. Elyan",
                "C. Jayne"
            ],
            "title": "Imitation learning: A survey of learning methods",
            "venue": "ACM Computing Surveys,",
            "year": 2017
        },
        {
            "authors": [
                "Yang Li",
                "Xinyan Chen",
                "Wenxuan Guo",
                "Xijun Li",
                "Wanqian Luo",
                "Junhua Huang",
                "Hui-Ling Zhen",
                "Mingxuan Yuan",
                "Junchi Yan"
            ],
            "title": "Hardsatgen: Understanding the difficulty of hard sat formula generation and a strong structure-hardness-aware baseline",
            "venue": "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
            "year": 2023
        },
        {
            "authors": [
                "Chang Liu",
                "Zhichen Dong",
                "Haobo Ma",
                "Weilin Luo",
                "Bowen Pang",
                "Xijun Li",
                "Jia Zeng",
                "Junchi Yan"
            ],
            "title": "L2p-mip: Learning to presolve for mixed integer programming",
            "venue": "In ICLR,",
            "year": 2024
        },
        {
            "authors": [
                "ZA Lomnicki"
            ],
            "title": "A \u201cbranch-and-bound\u201d algorithm for the exact solution of the three-machine scheduling problem",
            "venue": "Journal of the operational research society,",
            "year": 1965
        },
        {
            "authors": [
                "Junhyuk Oh",
                "Yijie Guo",
                "Satinder Singh",
                "Honglak Lee"
            ],
            "title": "Self-imitation learning",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Christopher WF Parsonson",
                "Alexandre Laterre",
                "Thomas D Barrett"
            ],
            "title": "Reinforcement learning for branch-and-bound optimisation using retrospective trajectories",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Jagat Patel",
                "John W Chinneck"
            ],
            "title": "Active-constraint variable ordering for faster feasibility of mixed integer linear programs",
            "venue": "Mathematical Programming,",
            "year": 2007
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Aviral Kumar",
                "Grace Zhang",
                "Sergey Levine"
            ],
            "title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning",
            "year": 1910
        },
        {
            "authors": [
                "Antoine Prouvost",
                "Justin Dumouchelle",
                "Lara Scavuzzo",
                "Maxime Gasse",
                "Didier Ch\u00e9telat",
                "Andrea Lodi"
            ],
            "title": "Ecole: A gym-like library for machine learning in combinatorial optimization solvers",
            "year": 2011
        },
        {
            "authors": [
                "Qingyu Qu",
                "Xijun Li",
                "Yunfan Zhou"
            ],
            "title": "Yordle: An efficient imitation learning for branch and bound",
            "venue": "arXiv preprint arXiv:2202.01896,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Qu",
                "Xijun Li",
                "Yunfan Zhou",
                "Jia Zeng",
                "Mingxuan Yuan",
                "Jie Wang",
                "Jinhu Lv",
                "Kexin Liu",
                "Kun Mao"
            ],
            "title": "An improved reinforcement learning algorithm for learning to branch, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "St\u00e9phane Ross",
                "Geoffrey J. Gordon",
                "J. Andrew Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In AISTATS,",
            "year": 1984
        },
        {
            "authors": [
                "Lara Scavuzzo",
                "Feng Chen",
                "Didier Ch\u00e9telat",
                "Maxime Gasse",
                "Andrea Lodi",
                "Neil Yorke-Smith",
                "Karen Aardal"
            ],
            "title": "Learning to branch with tree mdps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In IEEE/RSJ international conference on intelligent robots and systems,",
            "year": 2012
        },
        {
            "authors": [
                "Jie Wang",
                "Zijie Geng",
                "Xijun Li",
                "Jianye Hao",
                "Yongdong Zhang",
                "Feng Wu"
            ],
            "title": "G2milp: Learning to generate mixed-integer linear programming instances for milp solvers",
            "venue": "NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Qing Wang",
                "Jiechao Xiong",
                "Lei Han",
                "Han Liu",
                "Tong Zhang"
            ],
            "title": "Exponentially weighted imitation learning for batched historical data",
            "venue": "NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Jiayi Zhang",
                "Chang Liu",
                "Xijun Li",
                "Hui-Ling Zhen",
                "Mingxuan Yuan",
                "Yawen Li",
                "Junchi Yan"
            ],
            "title": "A survey for solving mixed integer programming via machine learning",
            "year": 2023
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Kaixiang Lin",
                "Bo Dai",
                "Jiayu Zhou"
            ],
            "title": "Self-adaptive imitation learning: Learning tasks with delayed rewards from sub-optimal demonstrations",
            "venue": "In AAAI,",
            "year": 2014
        },
        {
            "authors": [],
            "title": "For all candidates j \u2208 F , calculate a score value sj \u2208 R 5: return an index j \u2208 F with sj = maxk\u2208F (sk) There are many rule-based expert strategies, such as strong branching (Applegate et al., 1995), active constraint (Patel",
            "year": 2007
        },
        {
            "authors": [
                "Scavuzzo"
            ],
            "title": "2019) and set SCIP\u2019s parameter the same with Scavuzzo et al. (2022), Note that we use the model published by Parsonson et al. (2023) and Scavuzzo et al. (2022), their model is trained in the 500 rows, 1000 columns, and 400 rows, 750 columns set covering problem",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "As a general formulation and long-standing challenge, Mixed Integer Programming (MIP) (Zhang et al., 2023) spreads wide applications ranging from manufacturing (Artigues et al., 2009) to route planning (Halim & Ismail, 2019). Many exact algorithms (Lomnicki, 1965) have been proposed, with common adoption of branch and bound (Brucker et al., 1994). It recursively divides the solution space into a search tree and calculates the relaxation and boundary to prune the subtrees proved not to contain the optimal solution. At each iteration, two important decisions need to be made, including node selection and variable selection, which determine the next node to evaluate and select the variables to partition the search space. In this paper, we will focus on variable selection.\nChallenges: Variable selection decisions often rely on heuristics from domain experts (Achterberg et al., 2005b), like strong branching, active constraint (AC) (Patel & Chinneck, 2007), pseudo cost branching (PC) (Hendel, 2015), reliability pseudo cost branching (RB). However, the expert rulebased policies can often only find the locally best variable to branch on (Achterberg et al., 2005a). For example, for the most famous expert strong branching, it targets to select the variables that deliver the best one-step progress in the dual bound improvements, which has been proved not always the \u2018golden standard\u2019 (Dey et al., 2021; Qu et al., 2022a). So, Glankwamdee & Linderoth (2011) proposed\n\u2217Correspondence author. The work was partly supported by NSFC (92370201, 62222607).\nthe lookahead branching, which considered more dual bound information from deeper levels to deal with the potential local search limitations of strong branching. While in general, these effective approaches were usually computationally expensive. In this respect, an increasing number of studies are resorting to a learning-based branching strategy (Balcan et al., 2018), in which imitation learning (IL) on strong branching (Hussein et al., 2017) has shown superiority. However, strong branching cannot produce completely reliable labels due to the dual degeneracy (Cao et al., 2022b; Gamrath et al., 2020b) in the LP solution, which may cause the product score of strong branching to combine the improvements of the two child nodes close to zero. In addition, IL-based branching approaches usually require a large amount of training data and GPU resources for training.\nTo tackle the aforementioned challenges, we propose an innovative hybrid approach that combines online and offline RL1 techniques to enhance the training samples for branching policy learning. It involves two distinct phases. In the online phase, we leverage an online RL agent, acting as a dynamic collector. Its primary role is to guide decisions regarding the choice of sample generation processes. These processes draw from two potential sources: the learning-based policy and the expert policy. Contrasting with solely relying on expert knowledge, the samples could be generated by the learning-based policy with the hope of bolstering exploration in unfamiliar scenarios, thereby enhancing the performance of IL. The intuition is that learning-based policy (Gasse et al., 2019) can deliver a smaller B&B tree than strong branching in some instances, as shown in the Appendix A.6. Besides, Oh et al. (2018) has proved that past good experiences can add exploration for imitation learning and may find a potentially better policy than only imitating the expert policy. The offline phase involves training an offline agent. This offline agent is tasked with fitting the cumulative reward function for each decision made. It operates as a filter, sifting through the generated samples to identify those with superior cumulative returns. These high-quality samples are then earmarked for training the branching policy. By doing so, we not only expect to improve the performance of the branching policy but also minimize the overall training cost. The highlights of this paper are:\n1) We propose an iterative collection and filtering framework that leverages a combination of online and offline RL techniques to enhance the training samples. The online RL agent serves as a collector responsible for determining the sample generation process, and choosing between the learning-based policy and the expert policy. The offline RL agent functions as a filter, identifying samples with high cumulative returns. This framework enhances the efficacy (inference performance) and efficiency (training complexity) of the learning-based branching methods. 2) Extensive experiments on MIP show that it consistently outperforms the default heuristic policy adopted in open-source solver SCIP. Compared with learning-based algorithms, including the best-performing methods in the Machine Learning for Combinatorial Optimization (ML4CO) 2021 competition2, our method can further enhance the branching policy, illustrating its effectiveness as a plugin orthogonal to peer methods.\nDifference to existing works: Table 1 compares our approach with existing works in detail. The most critical difference is the hybrid sample collection and filtering scheme with online and offline RL agents. Peer works mainly depend on either the expert-based samples (Gasse et al., 2019; Cao et al., 2022a), or the pure self-generated samples with RL manner (Parsonson et al., 2023; Scavuzzo et al., 2022). To combine the best of the two worlds, Qu et al. (2022b) tries to mix the generated samples that lead to high performance with expert demonstration data within the training, purely by a predefined hyperparameter G0. In this paper, we propose to train an online RL to adaptively and dynamically mix the best of two worlds, which is more reasonable and adaptive. In addition, we also proposed a new offline RL agent to further filter the mixed generated samples, minimizing the overall training costs and enhancing performance to some extent."
        },
        {
            "heading": "2 PRELIMINARIES AND RELATED WORK",
            "text": "Preliminaries: Mixed integer programming (MIP) can be defined with three elements: optimization objectives, decision variables, and constraints, which can be given by:\nargmin x\n{ c\u22a4x|Ax \u2264 b, l \u2264 x \u2264 u,x \u2208 Zq \u00d7 Rm\u2212q} (1)\nwhere x is the decision variables of total size m. n and m denote the number of constraints and decision variables, respectively. q denotes the number of integer variables, and the remaining m\u2212 q\n1Offline RL is a filtration mechanism similar to (Chen et al., 2020). 2https://www.ecole.ai/2021/ml4co-competition\nvariables are continuous. The objective is to minimize c\u22a4x with the constraints Ax \u2264 b, where A \u2208 Rn\u00d7m represents the constraint coefficient, and b \u2208 Rn denotes the right-hand-side vector. The x satisfying all the constraints and minimizing the objective is the optimal solution.\nThe flow of the B&B can be depicted as follows. Firstly, the raw MIP can be defined as the root node of a search tree. It recursively selects a node from the search tree by the node selection rule and then selects a variable to decompose the selected node. When branching on variable xi, the optimal solution x\u2217i is first computed by imposing a linear programming relaxation, and the relaxed objective value can be defined as the dual bound. If x\u2217i does not meet the integrity constraint, the variable xi needs to be branched, decomposing the MIP from the current node into two sub-problems by adding two new constraints xi \u2265 \u2308x\u2217i \u2309 and xi \u2264 \u230ax\u2217i \u230b. The iterations continue until convergence or time up. Branching policy for branch and bound: Expert rule-based branching policies are widely adopted in MIP solvers, including strong branching (Applegate et al., 1995), active constraint (Patel & Chinneck, 2007) etc, which are generally difficult to design, and can usually find suboptimal variables to branch on. Furthermore, some of them can be extremely time-consuming, like strong branching.\nLearning-based policies were recently devised to replace the above expert rule-based policies, such as imitation learning on some best-performing expert rule-based policies, like strong branching (Applegate et al., 1995; Gasse et al., 2019; Alvarez et al., 2014; Gupta et al., 2020) and active constraint (Patel & Chinneck, 2007). Gasse et al. (2019) first proposed to utilize bipartite graph convolutional neural network (GCNN) to approximate strong branching decisions by IL. To make the expert samples closer to real-world applications and further improve the performance, Cao et al. (2022b) proposed a Dagger-like method to enhance the sample collection phase. These methods deliver better performance and efficiency. In addition, RL-based methods are also recently studied, which may produce even better branching policies compared with expert rule-based policies. Scavuzzo et al. (2022) proposed a new tree Markov Decision Process, a more suitable framework for learning to branch. Parsonson et al. (2023) proposed the retro-branching approach, by learning from deconstructing trajectories within the sub-trees to enhance the branching policy.\nIn general, purely exploiting what expert policies know limits the performance of imitation learning in some cases. While RL, free from expert rules, may be challenging in training, especially for some large-scale problems. Note the conducted extensive experiments on the RL-based methods in Appendix A.5, revealing the limitation of RL-based approaches. In this paper, we propose a hybrid approach that embeds RL into imitation learning to enhance the training samples, balancing between exploiting what experts know and exploring potential high-reward unknowns.\nTraining set augmentation with RL. Recently, RL-based data augmentation approaches have been widely studied. Wang et al. (2018) developed a monotonic advantage re-weighted IL strategy to enhance policy learning. Peng et al. (2019) proposed an advantage-weighted regression-based scheme for offline RL. Chen et al. (2020) proposed the Best-Action Imitation Learning (BAIL) approach, utilizing the V function to select actions for imitation learning that promised to be high-performing. With similar insights, Qu et al. (2022a) tends to utilize BAIL to improve the quality of expert samples for imitation learning. In general, these RL-based methods mainly rely on Monte Carlo return with a discount factor. However, in the branching scenario, the dual bound may remain unchanged for hundreds or even thousands of iterations, which means that the discount factor will interfere with the accurate evaluation of the samples. In this respect, we deal with the above issue by setting the discount factor as 1 to calculate the actual Monte Carlo return.\nImitation learning and self-imitation learning: Imitation learning (IL) has been widely adopted in machine learning for combinatorial optimization, including solving MIP (Zhang et al., 2023). As a special case for IL, the main idea of self-IL (Oh et al., 2018; Gangwani et al., 2018; Guo et al., 2019;\nZhu et al., 2014) is to learn the agent\u2019s past experiences to indirectly drive exploration. It is believed that the policy can be learned iteratively from the agent without any feedback from an external expert. It shows that past good experiences are helpful on hard exploration tasks and achieved good results in the Mujoco (Todorov et al., 2012). But in the branching scenario, each branching may have hundreds of thousands of candidate variables, which means there large state-actions space and it is a well-known difficulty for RL (Ecoffet et al., 2021), so we will use the online RL agent to adaptively choose samples generation processes, either from learning-based policy or the expert policy, with the aim of leveraging the strengths of both approaches."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Fig. 1 depicts our approach with notations throughout this paper as listed in Table 2."
        },
        {
            "heading": "3.1 APPROACH OVERVIEW",
            "text": "Our approach, named Hybrid Reinforcement Learning for Training Set Augmentation (HRL-Aug), is a hybrid online and offline approach to enhance the training samples towards branching policy learning. The innovation lies in the introduction of the data augmentation scheme, as meticulously designed for the realm of imitation learning. It comprises three components: an online Reinforcement Learning (RL) agent, an offline RL agent, and an imitation learning-based policy, as shown in Fig 1. The hybrid online and offline data augmentation scheme generates high-quality samples for the IL-based GCNN network iteratively until the stop condition is met.\nThe Online RL Agent is utilized to select better samples from the expert policy or learning-based policy at each variable selection decision. It plays a pivotal role by facilitating profound exploration of uncharted scenarios. By striking a balance between exploration and exploitation, it effectively steer clear of local optimal solutions for branching variables.\nTable 2: Description of the notations in the paper.\nsft state of offline reinforcement learning at node t sot state of online reinforcement learning at node t aot action of online reinforcement learning at node t Sst scores for branching variables given by expert at node t Spt scores for branching variables given by learned policies at node t Ot bipartite graph features at node t Rt cumulative reward at node t rt reward at node t \u03c0i policy trained by imitation learning at the i-th iteration At candidate branching variable set at node t z hyper-parameter controlling the ratio of selected samples f\u03b8off offline agent parameterized by \u03b8off f\u03b8on online agent parameterized by \u03b8on \u03b3 discount factor for Monte Carlo Return calculation\nThe Offline RL Agent evaluates the cumulative rewards of generated samples from the online phase, and filtering the highquality samples for subsequent policy training. As will be seen from our ablation study, it improves training efficiency.\nThe Imitation learning-based policy is trained with the generated samples from the hybrid online and offline agent. It can be regarded as a crucial validation of our data augmentation framework. It emerges as a pivotal component capable of supplanting expert-based policies within solvers, akin to SCIP as\nexemplified in this study. GCNN is selected as the backbone. We take comprehensive tests on the generalization capabilities of our framework to other approaches, and the details are given in Sec. 4.4."
        },
        {
            "heading": "3.2 ONLINE RL PHASE",
            "text": "Imitation learning-based models have shown their superior performance over expert rule-based policies (Gasse et al., 2019). However, its performance largely depends on high-quality expert samples, which are difficult to collect. Recently, self-imitation learning has proved that using past good experiences to explore new behaviors may find a potentially better policy (Oh et al., 2018). However, the optimization may be completely aimless at the early training stage when starting from scratch, which may hurt the performance and greatly increase the complexity.\nAn intuitive idea is to combine the best of the two worlds: expert rules and past good policies, which requires an approach to decide the sample generation processes, denoted as the online phase in this paper. However, it is difficult to evaluate the branching decisions, making it a challenging task to choose samples from expert rules or learning-based policies. Moreover, in the branching scenario, samples are collected by iterative variable selection decisions, and the current decision largely depends on the former actions and states, making it a sequential decision problem. In this respect, we tend to define the online phase as a Markov Decision Process (MDP) and model the sample generation process with RL.\nFirstly, we define the online RL phase as a sample selector and collector at each branching node. In this respect, the action aot was defined as the sample generation decision from expert rules or past learned policies at each node t, which is a discrete action space, only with two actions. The state sot is formulated as s o t = (S s t , S p t , Ot), where Sst denotes the expert score for variables, and the Spt is the score from learned policies. Ot\ndenotes the bipartite graph feature, and the details of features are listed in Table 11 in the Appendix. Secondly, it is essential to drive effective evaluation for samples. We utilize the dual bound changes as the reward rt, and Fig. 2 depicts an example on an MIP instance. The dual bound may remain unchanged for hundreds of steps. This reveals that it may require thousands of or even longer steps to accurately evaluate the cumulative reward. However, with the discount factor \u03b3 < 1, the reward may have little contribution to the cumulative reward after thousands of branches, which can cause an inaccurate evaluation of the action. Hence we set \u03b3 = 1, and the cumulative reward Rt at node t is:\nRt = t\u22121\u2211 v=t rv (2)\nwhere t\u22121 is the final expanded node in an episode. With the above definition, we can train the online RL agent with the advantage actor-critic method (ActorCritic). The agent dynamically decides sample generation processes, either from the expert rule or the GCNN policy \u03c0i\u22121. In the training phase, GCNN policy \u03c0i is iteratively optimized and the online RL agent relies on this evolving GCNN to construct the input states. This introduces a potential challenge, as the previously trained online agent may become unreliable due to the evolving policy. To address this issue, we utilize the up-to-date \u03c0i\u22121 to train the online agent. With the online update mechanism, we can optimize the online sample collection agent and obtain past good experiences, to achieve better exploration."
        },
        {
            "heading": "3.3 OFFLINE RL PHASE",
            "text": "With the online sample collection agent, we can easily obtain hybrid samples from both the expert rule and the GCNN policy. However, with the iterative sample generation process, the training samples constantly accumulate and require huge computing resources for imitation learning. In this respect, batch deep reinforcement learning (BDRL) (Fujimoto et al., 2018; Chen et al., 2020) was proposed to strive for both simplicity and performance by selecting high-quality samples with offline methods. In this paper, we leverage similar ideas and develop an offline RL agent to approximate the cumulative reward and reduce the data scale by filtering samples, thus reducing the training complexity.\nFor branching, the action aft is defined as the variable selection from the candidate branching variable set. The states for the offline learning phase at node t can be defined as sft = (Ot, At), where At\ndenotes the candidate branching variable set, Ot is the bipartite graph feature. The discount factor is still set as \u03b3 = 1, with the cumulative reward defined as Eq. 2, to evaluate the quality of the samples.\nWith the above definition, let offline agent denoted by a neural network characterized by \u03b8off = (w, b). \u03b8off take the {(Ot, At, Rt), t = 1, 2, ...,m} collected from the online RL phase as input and output a real number to fit the cumulative reward, thereby obviating the need for exploration. Then the offline agent is regarded as a \u03bb-regularized upper envelope for the following constrained optimization problem Eq. 3. This concept arises due to the inequality f\u03b8off (Ot, At) \u2265 Rt, and this prompts us to define f\u03b8off (Ot, At) as the \"upper envelope\", a construct geared towards aligning the envelope as closely as feasible with Rt. Then the offline RL agent is used to select high-quality samples generated in the online phase. With the approximate cumulative reward generated from the offline agent, we can select the high-quality samples by Eq. 4, where z is a hyper-parameter, controlling the ratio of the selected data samples, f\u03b8off denotes the offline agent.\nmin \u03b8off m\u2211 t=1 [f\u03b8off (Ot, At) \u2212 Rt]2 + \u03bb||\u03c9||2 s.t. f\u03b8off (Ot, At) \u2265 Rt, t = 1, 2, . . . ,m (3)\nRt \u2265 zf\u03b8off (Ot, At) (4) Note that the offline RL agent undergoes training solely during the first iteration, as empirical evidence suggests that its performance is already satisfactory at that point."
        },
        {
            "heading": "3.4 HYBRID TRAINING PIPELINE",
            "text": "The hybrid online and offline RL agents are combined as a hybrid pipeline to further enhance the branching policy via training set augmentation, whose details are given in Algorithm 1. It operates iteratively to collect training samples, with each iteration involving the incremental training of the foundational GCNN model. For efficiency, the frequency of the online RL training will be controlled by the hyperparameter Freq (see Line 6), and the offline RL agent is exclusively trained during the first iteration. In particular, the online agent will employ the most current GCNN policy for its training every time when the conditions in Line 6 are met. In the training phase for the online agent f\u03b8on , we will collect the bipartite graph features Ot, along with the scores (S s t , S p t ) from the expert rule and the GCNN policy \u03c0i\u22121 as the states in Line 7. Afterward, the samples generated from the online phase (see Line 9-13) are used to train the offline agent f\u03b8off at the first iteration in Line 15, which is capable of selecting samples with high cumulative rewards and will be applied for all the successive iterations. Finally, the generated samples are further aggregated to train the GCNN policy \u03c0i, used for subsequent training. The pipeline persists until it reaches the iteration limit N ."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We compare competing learning-based approaches and SCIP\u2019s default branching rule. We run all the experiments on an Intel(R) Xeon(R) Silver 4210 2.20GHz CPU and an NVIDIA A100 GPU."
        },
        {
            "heading": "4.1 EXPERIMENTAL PROTOCOLS",
            "text": "Dataset. We perform evaluation on popular binary integer programming problems: set covering, combinatorial auctions, maximum independent set, and capacitated facility location. Instance generation completely refers to (Gasse et al., 2019), and their variable and constraint sizes are listed in Table 3. For each problem, we generate 10,000 instances for training, 4,000 for validation, and 40 for testing. The test instances are all generated via different random seeds. The training and validation are performed on the Easy instances (see Table 3), and we also generated 40 Medium and Hard testing instances for each problem to further test the generalization ability. Besides, we conduct experiments on more difficult MIP problems from the ML4CO 2021 competition (ML4CO datasets) (Gasse et al., 2022), including Balanced Item Placement (1,083 columns, 195 rows), Workload Apportionment (61,000 columns, 64,314 rows), and Anonymous, which is from the MIRPLIB library (PAP, 2014) and inspired by real-world applications of large-scale problems.\nHyperparameters. All models are trained with Ecole (Prouvost et al., 2020). To verify the effect of different branching strategies, we disable SCIP\u2019s heuristic and default restart policy. In the training phase, we collect 10,000 training and 4,000 validation samples at each iteration, and the training iterations N = 20, 50 for binary integer programming problem and ML4CO datasets respectively, the frequency of training the online RL agent Freq = 5. At each iteration, the embedding size for the offline agent is 64, and the hidden layer size for both online and offline RL is set to 128. We set\nthe discount factor to 1, and use Adam optimizer with learning rate 3\u00d7 10\u22123 to train both RL agents. We use SCIP 7.0.3 as the backend solver, with a time limit of 1 hour for binary integer programming problems which are the same as (Gasse et al., 2019), and 900 seconds for ML4CO datasets.\nAlgorithm 1 Hybrid RL for the training set augmentation to enhance branching for B&B-based MIP. Input: Training instances; Number of samples generated each iteration C; The total number of algorithm iterations N ; The frequency of training the online RL agent Freq; Initial branching policy \u03c00; Collect all samples generated in each iteration D Collect the samples generated in the i-th iteration Di Output: Branch policy based on Hybrid RL \u03c0N ; 1: Let i = 1; 2: Let D=\u2205; 3: while i \u2264 N do 4: Let Di = \u2205 5: Let t = 1; 6: if i== 1 or i % Freq == 0 then 7: f\u03b8on = ActorCritic(\u03c0i\u22121, expert) 8: end if 9: while t \u2264 C do 10: Ot, At, Rt, Sst , S p t = f\u03b8on(\u03c0i\u22121, expert); 11: Di = Di \u222a {Ot, At, Rt, Sst , Spt }; 12: t = t+ 1; 13: end while 14: if i== 1 then 15: f\u03b8off = BDRL(Di); 16: end if 17: Remove samples from Di as Eq. 4; 18: D = D \u222aDi; 19: \u03c0i = GCNN(D); 20: i = i+ 1; 21: end while Evaluation. As discussed above, for each binary integer programming problem, evaluation is performed on different difficulty levels (Easy, Medium, Hard), each with 40 instances using five different seeds, which amounts to a total of 200 solving attempts per method to report aggregated results over the same instance. We use similar metrics as those in (Gasse et al., 2019), including the 1-shifted geometric mean of the solving time to measure the solving efficiency (Time), the final node counts of the solved instances among all baselines (Nodes), and the number of times each branching policy delivers the fastest solving time, over the number of instances solved to optimal (Wins). Note that we also report the average per-instance standard deviation. As it is too large to solve in a reasonable time, we evaluate methods with similar metrics as in ML4CO 2021 competition: average DualGap and dualbound within the solving time limit. DualGap for each instance is: DualGap(t) = |d(t)\u2217 \u2212 p\u2217| max(|d(t)\u2217| , |p\u2217|) (5)\nwhere d(t)\u2217 is the best dual bound at time t and p\u2217 is the best known solution value for the instance. We also compare the cumulative reward within the fixed time bound by Eq. 6, where Tc\u22a4x\u2217 is an instance-specific constant w.r.t. the optimal objective c\u22a4x\u2217.\nTc\u22a4x\u2217 \u2212 \u222b \u22a4 0 d(t)\u2217dt (6)"
        },
        {
            "heading": "4.2 COMPARATIVE EXPERIMENT",
            "text": "Baselines. We compare our method with four baselines: 1) SCIP (v7.0.3) (Gamrath et al., 2020a): State-of-the-art open-source solver with hybrid expert branching rules, named reliable pseudo cost branching. 2) GUROBI (v9.5.0): State-of-the-art commercial solver with hybrid expert branching rules which is known often much more effective than open-source solver. 3) GCNN (Gasse et al., 2019): An IL-based model with graph convolutional networks for scoring. 4) ML4CO-KIDA (Cao et al., 2022a): a Dagger-like (Ross et al., 1984) method based on IL, which is the best-performing method in the ML4CO 2021 competition.\nTraining. For iterative-based methods, including our proposed hybrid framework and ML4CO-KIDA, we generate 10,000 and 4,000 samples for training and validation at each iteration, respectively. As for the IL-based GCNN, we generate 100,000 samples for training and 40,000 samples for validation. Note that on the ML4CO datasets, we will use the model published by ML4CO-KIDA\u2019s author on balanced item placement and anonymous problems, and reproduce ML4CO-KIDA using the source code for the Workload Apportionment problem. The accuracy on testing instances for different problems is listed in Table 4, where (acc@n) denotes the accuracy of the selected branching variable by branching policy rules for sample generation, ranking top n among the predictions.\nComparative analysis. Table 5 depicts the overall performance on four binary integer programming problems. For the easy instances, learning-based methods significantly outperform the expert rulebased policies, showing the effectiveness of the IL framework. Our method shows consistently higher\nsolving efficiency with quite close performance for easy problems. Interestingly, the results reveal that there is no direct connection between the final node counts and the solving time.\nTable 6 gathers the results for ML4CO datasets. ML4CO-KIDA significantly outperforms GCNN, which reveals that iterative-based methods may deal with some challenges faced by pure IL models. Among all the competing baselines, our hybrid framework consistently dominates the others, illustrating the effectiveness of online RL which can drive much deeper exploration from the iteratively updated policies. Notably, the ablation study shows our proposed method can even outperform Gurobi on item placement problems, purely assisted by the open-source solver SCIP.\nGeneralization analysis. We further test our method on some difficult instances (Medium and Hard) to evaluate the generalization ability, and the overall results are gathered in Table 5. As can be seen, the default SCIP branching strategies significantly underperform the learning-based policies on solving time, revealing the limitations of the expert rules, especially on large-scale instances. Similarly, our proposed method consistently outperforms the competing baselines in terms of solving efficiency, showing a better generalization ability to larger instances."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "We present an ablation study to verify the effectiveness of the online RL agent and offline RL agent. Specifically, we evaluate our method and ML4CO-KIDA, with different iteration steps.\nIn Table 7 and Table 8, the training time is categorized into sample generation time and model training time. For the training sample generation time, our approach closely aligns with that of ML4CO-KIDA\non both the combinatorial auction and anonymous problem. However, with our newly incorporated offline RL agent, the training sample size of our proposed HRL-Aug reduced by around 30% at each iteration compared with ML4CO-KIDA. In general, the overall model training time was reduced by 28%, and 38% on combinatorial auction and anonymous problems, respectively, thus illustrating the efficacy and essential role of the offline RL agent.\nTable 9 ablates the effect of online/offline RL for item placement. HRL-Aug-on means disabling the offline agent, and thus the training samples are not filtered by their qualities. HRL-Aug-off means disabling the online agent, and thus the expert experience is only considered. The results suggest that the online part is more important, and both parts have positive effects."
        },
        {
            "heading": "4.4 GENERALIZABILITY OVER VARIABLE SELECTION POLICY EMBODIMENT",
            "text": "To assess our methodology\u2019s network-wise generalization, we position it within the framework of a combinatorial auction problem. The validation process was rigorously executed on a set of easy-level problems, comprising 40 instances. Each instance underwent testing under five seeds. To verify the adaptability and generalization potential of our approach across diverse network architectures, we replaced GCNN with alternative models such as the Multilayer Perceptron (MLP, input features=17, hidden size=64, output features=1), Convolutional Neural Networks (CNN, input channels=17, output channels=64, kernel size=3), and Recurrent Neural Network (RNN, input size=17, hidden size=64, num layers=1, sequence length=500) as variable selection policies for both our method and ML4CO-KIDA. A comprehensive depiction of performance metrics has been thoughtfully compiled and presented in Table 10. The outcomes are unequivocal \u2013 our approach consistently outperforms both ML4CO-KIDA and SCIP. This observation substantiates the inherent model-agnostic nature of our methodology, emphasizing its versatility and effectiveness across various scenarios."
        },
        {
            "heading": "5 CONCLUSION AND OUTLOOK",
            "text": "We have proposed a hybrid online and offline RL approach to enhance the branching policy via efficient training set augmentation. Hybrid agents perform in an iterative manner, with an online RL agent deciding sample generation, either from the expert rule or learning-based policy, and an offline RL agent further filtering the generated samples with high cumulative returns. Experiments on different MIP problems show its effectiveness, even achieving superior performance over the leading commercial solver in some cases. HRL-Aug also shows superior generalization ability. Future work may combine learning to presolve (Liu et al., 2024) and instance generation (Wang et al., 2023; Li et al., 2023) with learning-based solvers in a synergetic manner."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 THE FEATURE OF GCNN\nThe bipartite graph features used throughout this paper are listed in Table 11.\nBranching is the core of the branch-and-bound algorithm, and designing effective strategies was critical to mixed Integer programming (MIP) solving right from the beginning.\nThe overall pipeline for branching is listed as follows:\nAlgorithm 2 Generic branching (variable selection) 1: Input: Current subproblem Q with an optimal linear programming (LP) solution x\u030c /\u2208 XMIP 2: Output: An index j \u2208 I of an integer variable xj with the fractional LP value x\u030c /\u2208 Z 3: Let F = {j \u2208 I|x\u030c /\u2208 Z} be the set of branching candidates. 4: For all candidates j \u2208 F , calculate a score value sj \u2208 R 5: return an index j \u2208 F with sj = maxk\u2208F (sk)\nThere are many rule-based expert strategies, such as strong branching (Applegate et al., 1995), active constraint (Patel & Chinneck, 2007), and so on. For example, the widely utilized strong branching will select the fractional variable with the best dual bound improvement to branch on, listed as Eq. 7.\nsj = max{c\u030cQ\u2212j \u2212 c\u030cQ, c\u030cQ+j \u2212 c\u030cQ} (7)\nwhere sj denotes the score for each fractional candidate j, Q\u2212j represents the left subproblem by adding the trivial inequality xj \u2264 \u230ax\u030cj\u230b, and Q+j represents the right subproblem by adding the trivial inequality xj \u2265 \u2308x\u030cj\u2309. c\u030c denotes the objective value of the LP relaxations. For the GCNN policy, at each variable selection iteration, it will get the input features listed in Table 11, and predict sj for each fractional candidate variable. Then it will select an index j \u2208 F with sj = maxk\u2208F (sk) as the branching variable.\nA.3 DIFFERENCE IN METHODOLOGY AND PERFORMANCE WITH ML4CO-KIDA (CAO ET AL., 2022A)\nKIDA is a Dagger-like method based on IL. Though interacting with the solver with 95% probability of using model \u03c0i\u22121 and 5% probability of using Strong Branching in the solving process, it only collected training samples from Strong Branching. While in our paper, there are the following major differences, which are also our contributions:\n1) In the solving process, the variable selection from \u03c0i\u22121 or strong branching was not decided by pre-defined parameters (95% vs 5% in KIDA). We proposed an online RL agent to dynamically determine the selection and may generate more reasonable samples. You can see Section 3.2 for details.\n2) Different from KIDA, samples from strong branching and \u03c0i\u22121 will both be collected by the decision from the online agent.\n3) We devised an extra offline learning phase to filter higher cumulative reward samples, which can significantly reduce the training complexity and may improve imitation learning performance to some extent.\nBesides, the data augmentation mechanism in the proposed methodology can be segmented into collectors and filters. The KIDA has no filtering phase. Key distinctions exist in the collection phase:\n1) Selection Mechanism: In the case of KIDA, its selector operates under the influence of a Bernoulli distribution, wherein a hyperparameter is determined based on an in-depth grasp of domain expertise. Nonetheless, this approach grapples with manageability issues, particularly when the decision problem grows in complexity. Our approach takes a different path, circumventing this challenge. We leverage an online reinforcement learning agent that imbibes knowledge from data iteratively, thereby gradually converging toward an optimal distribution. This distribution is notably more intricate than the Bernoulli model, especially tailored to effectively tackle intricate decision problems.\n2) Collection Mechanism: In the realm of KIDA, its collector solely acquires samples from the expert policy of strong branching. This policy, albeit optimal at the immediate juncture, might not necessarily hold the same effectiveness over an extended trajectory. Our approach, on the other hand, goes beyond this confined paradigm. We amass samples not just from the expert policy, but also from the model policy. This judicious balance between exploitation and exploration forms the bedrock of our imitation learning, propelling us to converge toward the zenith of the optimal distribution. This distribution, then, serves as the compass guiding our branching decisions, distinctly setting us apart from KIDA.\nA.4 FURTHER DISCUSSION ON THE NOVELTY OF OUR DATA AUGMENTATION FRAMEWORK FOR IMITATION LEARNING\nOur primary innovation lies in the introduction of the data augmentation Framework, which is meticulously designed for the realm of imitation learning. This framework comprises three pivotal components: an online Reinforcement Learning (RL) agent, an offline RL agent, and an imitation learning-based policy. Refer to Figure 1 for a comprehensive depiction of this framework\u2019s architecture and functionality.\n1) The Online RL Agent: The online RL agent plays a pivotal role by facilitating the profound exploration of uncharted scenarios. By striking a balance between exploration and exploitation, it effectively steers clear of local optimal solutions for branching variables. Consequently, the presence of the online agent substantially enhances the overall effectiveness of imitation learning.\n2) The Offline RL Agent: As evidenced by Table 7 and Table 8, the primary advantage of the offline RL agent lies in its ability to enhance the efficiency of imitation learning. Remarkably, it accomplishes this by reducing the model training time by approximately 30%, so the model training time was reduced by 28%, and 38% in combinatorial auction and anonymous problem respectively. Thereby illustrating the efficacy and efficiency of the offline RL agent.\n3) The Imitation learning-based policy: A crucial validation of our proposed Data Augmentation Framework (HRL-Aug) unfolds within the context of branching scenarios. At the core of this validation lies the deployment of an imitation learning-based policy. This policy, meticulously learned\nthrough the imitation learning process, emerges as a pivotal component capable of supplanting expert-based policies within solvers, akin to SCIP as exemplified in this study.\nOur main contribution is the novel data augmentation framework for imitation learning. We have indeed undertaken comprehensive tests on the generalization capabilities of our proposed framework. Specifically, we have replaced the learning-based Graph Convolutional Network (GCN) with alternative architectures, such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs) as shown in section 4.4. Our approach consistently outperforms both ML4CO-KIDA and SCIP. This observation substantiates the inherent model-agnostic nature of our methodology, emphasizing its versatility and effectiveness across various scenarios.\nA.5 COMPARE WITH THE RL FOR BRANCHING METHOD\nRecently RL-based methods have been proposed to enhance the branching policies, which do not rely on the expert rule, and might find a better policy for branching. To further validate the effectiveness of our method, we conducted an experiment in the set covering to compare with two RL-based methods, including Scavuzzo et al. (2022) and Parsonson et al. (2023). We use similar metrics as those in Gasse et al. (2019) and set SCIP\u2019s parameter the same with Scavuzzo et al. (2022), Note that we use the model published by Parsonson et al. (2023) and Scavuzzo et al. (2022), their model is trained in the 500 rows, 1000 columns, and 400 rows, 750 columns set covering problem respectively. Then, evaluation is performed for Easy(500 rows, 1000 columns) and Medium (1000 rows, 1000 columns) on 40 generated instances using five different seeds, which amounts to a total of 200 solving attempts per method. As shown in Table 12, the results show that our method is obviously better than the RL-based method both in solving time and nodes.\nA.6 COMPARE THE NUMBER OF NODES SOLVED TO OPTIMAL BETWEEN GCNN (GASSE ET AL., 2019) AND STRONG BRANCHING.\nStrong Branching is the most efficient branching strategy in terms of the number of nodes in the B&B tree. To verify whether the learned GCNN (Gasse et al., 2019) can achieve a smaller B&B tree than strong branching or not, we perform experiments on the combinatorial auction problem with 200 easy difficulty instances, as shown in Table 13.\nSpecifically, we compare the number of nodes each branching policy solved to optimal\u2013Wins (Nodes). The Win (Nodes) shows that the GCNN (Gasse et al., 2019) can perform better than Strong Branching in some instances. It empirically verifies that the quality of variable selection in strong branching can be further improved."
        }
    ],
    "year": 2024
}