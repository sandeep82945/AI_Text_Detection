{
    "abstractText": "We introduce Transformer-VQ, a decoder-only transformer computing softmaxbased dense self-attention in linear time. Transformer-VQ\u2019s efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: https://github.com/transformer-vq/transformer_vq k1 k2 k3 k4 k5 7\u2192 VQ \u2248 k\u03021 k\u03022 k\u03023 k\u03024 k\u03025 Figure 1: Schematic of the VQ-Attention approximation. The colorful and blank boxes depict the keys and attention weights, respectively. The keys on the right have been vector-quantized. Since the green keys k2, k5 map to the same code, they have the same attention weights in this attention head.",
    "authors": [
        {
            "affiliations": [],
            "name": "VECTOR QUANTIZATION"
        },
        {
            "affiliations": [],
            "name": "Lucas D. Lingle"
        }
    ],
    "id": "SP:cd72f17bdc164f92fd029242bd5f2acf0704b3be",
    "references": [
        {
            "authors": [
                "Joshua Ainslie",
                "Santiago Ontanon",
                "Chris Alberti",
                "Vaclav Cvicek",
                "Zachary Fisher",
                "Philip Pham",
                "Anirudh Ravula",
                "Sumit Sanghai",
                "Qifan Wang",
                "Li Yang"
            ],
            "title": "ETC: Encoding long and structured inputs in transformers",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "CoRR, abs/2004.05150,",
            "year": 2020
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations of Python+NumPy programs, 2018",
            "venue": "URL http://github.com/google/jax",
            "year": 2018
        },
        {
            "authors": [
                "Aydar Bulatov",
                "Yuri Kuratov",
                "Mikhail Burtsev"
            ],
            "title": "Recurrent memory transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "CoRR, abs/1904.10509,",
            "year": 2019
        },
        {
            "authors": [
                "Krzysztof Marcin Choromanski",
                "Valerii Likhosherstov",
                "David Dohan",
                "Xingyou Song",
                "Andreea Gane",
                "Tam\u00e1s Sarl\u00f3s",
                "Peter Hawkins",
                "Jared Quincy Davis",
                "Afroz Mohiuddin",
                "Lukasz Kaiser",
                "David Benjamin Belanger",
                "Lucy J Colwell",
                "Adrian Weller"
            ],
            "title": "Rethinking attention with performers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah"
            ],
            "title": "Fiedel. Palm: Scaling language modeling with pathways, 2022",
            "venue": "URL https://arxiv.org/abs/2204.02311",
            "year": 2022
        },
        {
            "authors": [
                "Patryk Chrabaszcz",
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "A downsampled variant of imagenet as an alternative to the CIFAR",
            "venue": "datasets. CoRR,",
            "year": 2017
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime Carbonell",
                "Quoc Le",
                "Ruslan Salakhutdinov"
            ],
            "title": "Transformer-XL: Attentive language models beyond a fixed-length context",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Tri Dao",
                "Daniel Y. Fu",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022",
            "venue": "URL https://arxiv.org/abs/",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Gritsenko",
                "Vighnesh Birodkar",
                "Cristina Vasconcelos",
                "Yi Tay",
                "Thomas Mensink",
                "Alexander Kolesnikov",
                "Filip Paveti\u0107",
                "Dustin Tran",
                "Thomas Kipf",
                "Mario Lu\u010di\u0107",
                "Xiaohua Zhai",
                "Daniel Keysers",
                "Jeremiah Harmsen",
                "Neil Houlsby"
            ],
            "title": "Scaling vision transformers to 22 billion parameters, 2023",
            "venue": "URL http://arxiv.org/abs/2302.05442",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Tim Dettmers",
                "Luke Zettlemoyer"
            ],
            "title": "The case for 4-bit precision: k-bit inference scaling laws, 2023",
            "venue": "URL http://arxiv.org/abs/2212.09720",
            "year": 2023
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "GPT3.int8(): 8-bit matrix multiplication for transformers at scale",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Heewoo Jun",
                "Christine McLeavey Paine",
                "Jong Wook Kim",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Jukebox: A generative model for music, 2020",
            "venue": "URL https://arxiv.org/abs/",
            "year": 2005
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
            "venue": "CoRR, abs/1702.03118,",
            "year": 2017
        },
        {
            "authors": [
                "Erich Elsen",
                "Augustus Odena",
                "Maxwell Nye",
                "Sa\u011fnak Ta\u015f\u0131rlar",
                "Tri Dao",
                "Curtis Hawthorne",
                "Deepak Moparthi",
                "Arushi Somani"
            ],
            "title": "Releasing persimmon-8b, 2023",
            "venue": "URL https://www.adept. ai/blog/persimmon-8b",
            "year": 2023
        },
        {
            "authors": [
                "Angela Fan",
                "Edouard Grave",
                "Armand Joulin"
            ],
            "title": "Reducing transformer depth on demand with structured dropout",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Angela Fan",
                "Thibaut Lavril",
                "Edouard Grave",
                "Armand Joulin",
                "Sainbayar Sukhbaatar"
            ],
            "title": "Addressing some limitations of transformers with feedback memory, 2020b. URL https://arxiv.org/ abs/2002.09402",
            "year": 2002
        },
        {
            "authors": [
                "Elias Frantar",
                "Saleh Ashkboos",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "title": "OPTQ: Accurate quantization for generative pre-trained transformers",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Matej Grcic",
                "Ivan Grubisic",
                "Sinisa Segvic"
            ],
            "title": "Densely connected normalizing flows",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Albert Gu",
                "Karan Goel",
                "Christopher Re"
            ],
            "title": "Efficiently modeling long sequences with structured state spaces",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ruiqi Guo",
                "Quan Geng",
                "David Simcha",
                "Felix Chern",
                "Sanjiv Kumar",
                "Xiang Wu"
            ],
            "title": "New loss functions for fast maximum inner product search",
            "venue": "URL http: //arxiv.org/abs/1908.10396",
            "year": 1908
        },
        {
            "authors": [
                "Ramin Hasani",
                "Mathias Lechner",
                "Tsun-Hsuan Wang",
                "Makram Chahine",
                "Alexander Amini",
                "Daniela Rus"
            ],
            "title": "Liquid structural state-space models, 2022",
            "venue": "URL https://arxiv.org/abs/",
            "year": 2022
        },
        {
            "authors": [
                "Louay Hazami",
                "Rayhane Mama",
                "Ragavan Thurairatnam"
            ],
            "title": "Efficient-VDVAE: Less is more, 2022",
            "venue": "URL http://arxiv.org/abs/2203.13751",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Heek",
                "Anselm Levskaya",
                "Avital Oliver",
                "Marvin Ritter",
                "Bertrand Rondepierre",
                "Andreas Steiner",
                "Marc van Zee"
            ],
            "title": "Flax: A neural network library and ecosystem for JAX, 2023",
            "venue": "URL http://github.com/google/flax",
            "year": 2023
        },
        {
            "authors": [
                "Tom Henighan",
                "Jared Kaplan",
                "Mor Katz",
                "Mark Chen",
                "Christopher Hesse",
                "Jacob Jackson",
                "Heewoo Jun",
                "Tom B. Brown",
                "Prafulla Dhariwal",
                "Scott Gray",
                "Chris Hallacy",
                "Benjamin Mann",
                "Alec Radford",
                "Aditya Ramesh",
                "Nick Ryder",
                "Daniel M. Ziegler",
                "John Schulman",
                "Dario Amodei",
                "Sam McCandlish"
            ],
            "title": "Scaling laws for autoregressive generative modeling",
            "venue": "URL https://arxiv.org/abs/2010.14701",
            "year": 2010
        },
        {
            "authors": [
                "Alex Henry",
                "Prudhvi Raj Dachapally",
                "Shubham Shantaram Pawar",
                "Yuxuan Chen"
            ],
            "title": "Query-key normalization for transformers",
            "venue": "pp. 4246\u20134253,",
            "year": 2020
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark",
                "Tom Hennigan",
                "Eric Noland",
                "Katie Millican",
                "George van den Driessche",
                "Bogdan Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre"
            ],
            "title": "Training compute-optimal large language models, 2022",
            "venue": "URL https://arxiv.org/abs/",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Weizhe Hua",
                "Zihang Dai",
                "Hanxiao Liu",
                "Quoc Le"
            ],
            "title": "Transformer quality in linear time",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "DeLesley Hutchins",
                "Imanol Schlag",
                "Yuhuai Wu",
                "Ethan Dyer",
                "Behnam Neyshabur"
            ],
            "title": "Block-recurrent transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Felix Gimeno",
                "Andy Brock",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Joao Carreira"
            ],
            "title": "Perceiver: General perception with iterative attention",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jed Souter",
                "Dan Steinberg",
                "Andy Swing",
                "Mercedes Tan",
                "Gregory Thorson",
                "Bo Tian",
                "Horia Toma",
                "Erick Tuttle",
                "Vijay Vasudevan",
                "Richard Walter",
                "Walter Wang",
                "Eric Wilcox",
                "Doe Hyun Yoon"
            ],
            "title": "In-datacenter performance analysis of a tensor processing unit",
            "venue": "SIGARCH Comput. Archit. News,",
            "year": 2017
        },
        {
            "authors": [
                "Lukasz Kaiser",
                "Samy Bengio",
                "Aurko Roy",
                "Ashish Vaswani",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Noam Shazeer"
            ],
            "title": "Fast decoding in sequence models using discrete latent variables",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "venue": "CoRR, abs/2001.08361,",
            "year": 2020
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Apoorv Vyas",
                "Nikolaos Pappas",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Transformers are RNNs: Fast autoregressive transformers with linear attention",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Kitaev",
                "Lukasz Kaiser",
                "Anselm Levskaya"
            ],
            "title": "Reformer: The efficient transformer",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson"
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2018
        },
        {
            "authors": [
                "Doyup Lee",
                "Chiheon Kim",
                "Saehoon Kim",
                "Minsu Cho",
                "Wook-Shin Han"
            ],
            "title": "Autoregressive image generation using residual quantization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "James Lee-Thorp",
                "Joshua Ainslie",
                "Ilya Eckstein",
                "Santiago Ontanon"
            ],
            "title": "FNet: Mixing tokens with Fourier transforms",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Lei"
            ],
            "title": "When attention meets fast recurrence: Training language models with reduced compute",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Yaron Lipman",
                "Ricky T.Q. Chen",
                "Heli Ben-Hamu",
                "Maximilian Nickel",
                "Matthew Le"
            ],
            "title": "Flow matching for generative modeling",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Peter J. Liu",
                "Mohammad Saleh",
                "Etienne Pot",
                "Ben Goodrich",
                "Ryan Sepassi",
                "Lukasz Kaiser",
                "Noam Shazeer"
            ],
            "title": "Generating wikipedia by summarizing long sequences",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Zichang Liu",
                "Aditya Desai",
                "Fangshuo Liao",
                "Weitao Wang",
                "Victor Xie",
                "Zhaozhuo Xu",
                "Anastasios Kyrillidis",
                "Anshumali Shrivastava"
            ],
            "title": "Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time, 2023",
            "venue": "URL http://arxiv.org/ abs/2305.17118",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Shahar Lutati",
                "Itamar Zimerman",
                "Lior Wolf"
            ],
            "title": "Focus your attention (with adaptive IIR filters), 2023",
            "venue": "URL http://arxiv.org/abs/2305.14952",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Xiang Kong",
                "Sinong Wang",
                "Chunting Zhou",
                "Jonathan May",
                "Hao Ma",
                "Luke Zettlemoyer"
            ],
            "title": "LUNA: Linear unified nested attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Chunting Zhou",
                "Xiang Kong",
                "Junxian He",
                "Liangke Gui",
                "Graham Neubig",
                "Jonathan May",
                "Luke Zettlemoyer"
            ],
            "title": "Mega: Moving average equipped gated attention",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Matt Mahoney"
            ],
            "title": "Large text compression benchmark, 2011. URL: http://mattmahoney.net/ dc/text.html",
            "year": 2011
        },
        {
            "authors": [
                "Chengzhi Mao",
                "Lu Jiang",
                "Mostafa Dehghani",
                "Carl Vondrick",
                "Rahul Sukthankar",
                "Irfan Essa"
            ],
            "title": "Discrete representations strengthen vision transformer robustness",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Harsh Mehta",
                "Ankit Gupta",
                "Ashok Cutkosky",
                "Behnam Neyshabur"
            ],
            "title": "Long range language modeling via gated state spaces, 2022",
            "venue": "URL http://arxiv.org/abs/2206.13947",
            "year": 2022
        },
        {
            "authors": [
                "Piotr Nawrot",
                "Szymon Tworkowski",
                "Michal Tyrolski",
                "Lukasz Kaiser",
                "Yuhuai Wu",
                "Christian Szegedy",
                "Henryk Michalewski"
            ],
            "title": "Hierarchical transformers are more efficient language models",
            "venue": "CoRR, abs/2110.13711,",
            "year": 2021
        },
        {
            "authors": [
                "Piotr Nawrot",
                "Jan Chorowski",
                "Adrian \u0141a\u0144cucki",
                "Edoardo M. Ponti"
            ],
            "title": "Efficient transformers with dynamic token pooling, 2023",
            "venue": "URL http://arxiv.org/abs/2211.09761",
            "year": 2023
        },
        {
            "authors": [
                "Emilio Parisotto",
                "H. Francis Song",
                "Jack W. Rae",
                "Razvan Pascanu",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Siddhant M. Jayakumar",
                "Max Jaderberg",
                "Raphael Lopez Kaufman",
                "Aidan Clark",
                "Seb Noury",
                "Matthew M. Botvinick",
                "Nicolas Heess",
                "Raia Hadsell"
            ],
            "title": "Stabilizing transformers for reinforcement learning",
            "venue": "URL http://arxiv.org/abs/1910.06764",
            "year": 1910
        },
        {
            "authors": [
                "Hao Peng",
                "Nikolaos Pappas",
                "Dani Yogatama",
                "Roy Schwartz",
                "Noah Smith",
                "Lingpeng Kong"
            ],
            "title": "Random feature attention",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Poli",
                "Stefano Massaroli",
                "Eric Nguyen",
                "Daniel Y. Fu",
                "Tri Dao",
                "Stephen Baccus",
                "Yoshua Bengio",
                "Stefano Ermon",
                "Christopher R\u00e9"
            ],
            "title": "Hyena hierarchy: Towards larger convolutional language models, 2023",
            "venue": "URL http://arxiv.org/abs/2302.10866",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Qin",
                "Xiaodong Han",
                "Weixuan Sun",
                "Dongxu Li",
                "Lingpeng Kong",
                "Nick Barnes",
                "Yiran Zhong"
            ],
            "title": "The devil in linear transformer",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Qin",
                "Weixuan Sun",
                "Hui Deng",
                "Dongxu Li",
                "Yunshen Wei",
                "Baohong Lv",
                "Junjie Yan",
                "Lingpeng Kong",
                "Yiran Zhong"
            ],
            "title": "CosFormer: Rethinking softmax in attention",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jiezhong Qiu",
                "Hao Ma",
                "Omer Levy",
                "Wen-tau Yih",
                "Sinong Wang",
                "Jie Tang"
            ],
            "title": "Blockwise selfattention for long document understanding",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Markus N. Rabe",
                "Charles Staats"
            ],
            "title": "Self-attention does not need o(n2) memory",
            "venue": "CoRR, abs/2112.05682,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners, 2019",
            "venue": "https: //cdn.openai.com/better-language-models/language_models_are_ unsupervised_multitask_learners.pdf Last visited on 2023/09/07",
            "year": 2023
        },
        {
            "authors": [
                "Jack W. Rae",
                "Anna Potapenko",
                "Siddhant M. Jayakumar",
                "Chloe Hillier",
                "Timothy P. Lillicrap"
            ],
            "title": "Compressive transformers for long-range sequence modelling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Irving"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "venue": "CoRR, abs/2112.11446,",
            "year": 2021
        },
        {
            "authors": [
                "Prajit Ramachandran",
                "Barret Zoph",
                "Quoc V. Le"
            ],
            "title": "Searching for activation",
            "venue": "functions. CoRR,",
            "year": 2017
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "CoRR, abs/2102.12092,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Razavi",
                "Aaron van den Oord",
                "Oriol Vinyals"
            ],
            "title": "Generating diverse high-fidelity images with vq-vae-2",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Hongyu Ren",
                "Hanjun Dai",
                "Zihang Dai",
                "Mengjiao Yang",
                "Jure Leskovec",
                "Dale Schuurmans",
                "Bo Dai"
            ],
            "title": "Combiner: Full attention transformer with sparse computation cost",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Roller",
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason E Weston"
            ],
            "title": "Hash layers for large sparse models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aurko Roy",
                "Mohammad Saffar",
                "Ashish Vaswani",
                "David Grangier"
            ],
            "title": "Efficient content-based sparse attention with routing transformers",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Aurko Roy",
                "Rohan Anil",
                "Guangda Lai",
                "Benjamin Lee",
                "Jeffrey Zhao",
                "Shuyuan Zhang",
                "Shibo Wang",
                "Ye Zhang",
                "Shen Wu",
                "Rigel Swavely",
                "Yu Tao",
                "Phuong Dao",
                "Christopher Fifty",
                "Zhifeng Chen",
                "Yonghui Wu"
            ],
            "title": "N-Grammer: Augmenting transformers with latent n-grams, 2022",
            "venue": "URL https://arxiv.org/abs/2207.06366",
            "year": 2022
        },
        {
            "authors": [
                "Noam Shazeer"
            ],
            "title": "Fast transformer decoding: One write-head is all you need",
            "venue": "CoRR, abs/1911.02150,",
            "year": 2019
        },
        {
            "authors": [
                "Noam Shazeer"
            ],
            "title": "GLU variants improve transformer, 2020",
            "venue": "URL https://arxiv.org/abs/",
            "year": 2002
        },
        {
            "authors": [
                "Noam Shazeer",
                "Mitchell Stern"
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory",
            "venue": "cost. CoRR,",
            "year": 2018
        },
        {
            "authors": [
                "Jimmy T.H. Smith",
                "Andrew Warrington",
                "Scott W. Linderman"
            ],
            "title": "Simplified state space layers for sequence modeling, 2022",
            "venue": "URL https://arxiv.org/abs/2208.04933",
            "year": 2022
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Adaptive attention span in transformers",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Da Ju",
                "Spencer Poff",
                "Stephen Roller",
                "Arthur Szlam",
                "Jason Weston",
                "Angela Fan"
            ],
            "title": "Not all memories are created equal: Learning to forget by expiring",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Simeng Sun",
                "Kalpesh Krishna",
                "Andrew Mattarella-Micke",
                "Mohit Iyyer"
            ],
            "title": "Do long-range language models actually use long-range context",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Tay",
                "Dara Bahri",
                "Liu Yang",
                "Donald Metzler",
                "Da-Cheng Juan"
            ],
            "title": "Sparse sinkhorn attention",
            "venue": "CoRR, abs/2002.11296,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Donald Metzler"
            ],
            "title": "Efficient transformers: A survey",
            "venue": "CoRR, abs/2009.06732,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Tay",
                "Dara Bahri",
                "Donald Metzler",
                "Da-Cheng Juan",
                "Zhe Zhao",
                "Che Zheng"
            ],
            "title": "Synthesizer: Rethinking self-attention for transformer models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Oriol Vinyals",
                "Koray Kavukcuoglu"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Nal Kalchbrenner",
                "Koray Kavukcuoglu"
            ],
            "title": "Pixel recurrent neural networks",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Apoorv Vyas",
                "Angelos Katharopoulos",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Fast transformers with clustered attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ningning Wang",
                "Guobing Gan",
                "Peng Zhang",
                "Shuai Zhang",
                "Junqiu Wei",
                "Qun Liu",
                "Xin Jiang"
            ],
            "title": "ClusterFormer: Neural clustering attention for efficient and effective transformer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Shuohang Wang",
                "Luowei Zhou",
                "Zhe Gan",
                "Yen-Chun Chen",
                "Yuwei Fang",
                "Siqi Sun",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "Cluster-former: Clustering-based sparse transformer for question answering",
            "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP",
            "year": 2021
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z. Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma"
            ],
            "title": "Linformer: self-attention with linear complexity",
            "venue": "CoRR, abs/2006.04768,",
            "year": 2020
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Markus Norman Rabe",
                "DeLesley Hutchins",
                "Christian Szegedy"
            ],
            "title": "Memorizing transformers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yunyang Xiong",
                "Zhanpeng Zeng",
                "Rudrasis Chakraborty",
                "Mingxing Tan",
                "Glenn Fung",
                "Yin Li",
                "Vikas Singh"
            ],
            "title": "Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Zihao Ye",
                "Qipeng Guo",
                "Quan Gan",
                "Xipeng Qiu",
                "Zheng Zhang"
            ],
            "title": "BP-Transformer: Modelling long-range context via binary partitioning",
            "venue": "CoRR, abs/1911.04070,",
            "year": 2019
        },
        {
            "authors": [
                "Lili Yu",
                "D\u00e1niel Simig",
                "Colin Flaherty",
                "Armen Aghajanyan",
                "Luke Zettlemoyer",
                "Mike Lewis"
            ],
            "title": "Megabyte: Predicting million-byte sequences with multiscale transformers, 2023",
            "venue": "URL http: //arxiv.org/abs/2305.07185",
            "year": 2023
        },
        {
            "authors": [
                "Biao Zhang",
                "Rico Sennrich"
            ],
            "title": "Root mean square layer normalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zhenyu Zhang",
                "Ying Sheng",
                "Tianyi Zhou",
                "Tianlong Chen",
                "Lianmin Zheng",
                "Ruisi Cai",
                "Zhao Song",
                "Yuandong Tian",
                "Christopher R\u00e9",
                "Clark Barrett",
                "Zhangyang Wang",
                "Beidi Chen"
            ],
            "title": "H2o: Heavyhitter oracle for efficient generative inference of large language models, 2023",
            "venue": "URL https: //arxiv.org/abs/2306.14048",
            "year": 2023
        },
        {
            "authors": [
                "Shangchen Zhou",
                "Kelvin C.K. Chan",
                "Chongyi Li",
                "Chen Change Loy"
            ],
            "title": "Towards robust blind face restoration with codebook lookup transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhu",
                "Wei Ping",
                "Chaowei Xiao",
                "Mohammad Shoeybi",
                "Tom Goldstein",
                "Anima Anandkumar",
                "Bryan Catanzaro"
            ],
            "title": "Long-short transformer: Efficient transformers for language and vision",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenhai Zhu",
                "Radu Soricut"
            ],
            "title": "H-transformer-1D: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
            "venue": "pp. 3801\u20133815,",
            "year": 2021
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "2022), such that all the operations for a transformer layer are performed one input block at a time. To ground all comparisons, we benchmark the throughput against a transformer using unquantized quadratic-time attention, the same attention head type (SHGA, MQA, or MHA), and an identical non-codebook parameter count. Table 6: Training throughput comparison (tokens/sec) on Google Cloud VM with 8 TPU",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformer (Vaswani et al., 2017) language models would ideally scale to long sequences, since their predictive abilities often improve as context length increases (Dai et al., 2019; Kaplan et al., 2020). Unfortunately, the standard transformer uses a self-attention mechanism with a quadratic time complexity with respect to sequence length. This limits the practicality of applying transformers to very long sequences, since increasing the sequence length by a factor of 10n increases the attention computations by a factor of 100n. Transformer variants that overcome this efficiency bottleneck have the potential to facilitate new long-context applications and enable new breakthroughs.\nUp to this point, a variety of efficient transformers (Tay et al., 2020b) have been proposed to scale to long sequences. Techniques include sparsity (Child et al., 2019; Ye et al., 2019; Beltagy et al., 2020; Kitaev et al., 2020; Qiu et al., 2020; Roy et al., 2021; Tay et al., 2020a; Sukhbaatar et al., 2021; Wu et al., 2022; Liu et al., 2023; Zhang et al., 2023), compression (Liu et al., 2018; Rae et al., 2020; Ainslie et al., 2020; Zhu et al., 2021; Ren et al., 2021; Nawrot et al., 2021; 2023), low-rank approximations (Wang et al., 2020; Vyas et al., 2020; Katharopoulos et al., 2020; Xiong et al., 2021; Tay et al., 2021; Choromanski et al., 2021), and cross-attention operations (Dai et al., 2019; Ma et al., 2021; Hutchins et al., 2022; Hawthorne et al., 2022). Other efficient sequence models have also been proposed (Gu et al., 2022; Lee-Thorp et al., 2022; Mehta et al., 2022; Smith et al., 2022; Hasani et al., 2022; Poli et al., 2023; Peng et al., 2023).\nIn this paper, we present Transformer-VQ, a transformer decoder with dense self-attention computible in linear time with respect to sequence length. This is made possible through a combination of vectorquantized keys, localized positional biases, and compressive cache that can be attended to efficiently, while yielding the same results as an uncompressed variable-length cache. Transformer-VQ is also simple to implement sampling for."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 NOTATION",
            "text": "The real numbers are denoted by R and the extended real numbers R \u222a {\u2212\u221e,\u221e} by R\u0304. Zero-based indices are used for all tensors. When indexing a matrix M along the first axis, we use Mi to denote a column vector and Mi,: to denote a row vector. The functions LN(\u00b7), Softmax(\u00b7), Concat(\u00b7) denote LayerNorm (Ba et al., 2016), softmax, and concatenation, each applied row-wise. The symbols ,,\u221d, , exp(\u00b7), \u03b4a,b,SG(\u00b7) denote equality by definition, proportionality, element-wise product, element-wise exponentiation, Kronecker delta function, and the stop-gradient operator. We slightly abuse notation to write inner products of vectors u,v as u>v, and outer products as uv>.\nWe assume familiarity with transformers (Vaswani et al., 2017), and use the notation Dm to denote the model width, Dk to denote the query/key vector width, and Dv to denote the value vector width."
        },
        {
            "heading": "2.2 VECTOR QUANTIZATION",
            "text": "Vector quantization (VQ) is a technique used extensively throughout this work. In this subsection we briefly review vector quantization, motivate its use in self-attention, and discuss the backpropagationcompatible VQ scheme introduced by van den Oord et al. (2017)."
        },
        {
            "heading": "2.3 VECTOR QUANTIZERS AND CODEBOOKS",
            "text": "Definition 2.1. A vector quantizer is a function VQ(\u00b7; C) with domain RD and codomain RD. For an input x, its output x\u0302 is given by\nz , arg min s ||x\u2212Cs||2 (1)\nx\u0302 , Cz (2)\nwhere C \u2208 RS\u00d7D is known as the codebook. The row indices {0, . . . , S \u2212 1} of C are called shortcodes, and the rows themselves are called codewords. Theorem 2.2 (Based on Guo et al. (2019)). Let q \u2208 RD be a random variable with Eq[qq>] = \u03c32ID for some \u03c3 > 0, and let k \u2208 RD be a random variable independent of q. Let \u03d5 : RD \u2192 RD be a deterministic function. Then\nEq,k||q>k\u2212 q>\u03d5(k)||2 \u221d Ek||k\u2212 \u03d5(k)||2. (3) Corollary 2.3. Let the conditions of Theorem 2.2 hold. Given the constraint that \u03d5(RD) = {Cs}S\u22121s=0 , the choice \u03d5(\u00b7) = VQ(\u00b7; C) minimizes Eq,k||q>k\u2212 q>\u03d5(k)||2. Corollary 2.4. Let the conditions of Theorem 2.2 hold. With k\u0302 = VQ(k; C) we have\narg min C Eq,k||q>k\u2212 q>k\u0302||2 = arg min C Ek||k\u2212 k\u0302||2. (4)\nRemark 2.5. Fnding the global minimizer C\u2217 = arg minC Ek||k\u2212 k\u0302||2 is expensive, so in practice we approximate it using the method from van den Oord et al. (2017); Razavi et al. (2019)."
        },
        {
            "heading": "2.4 VECTOR-QUANTIZED REPRESENTATION LEARNING",
            "text": "Definition 2.6 (Based on van den Oord et al. (2017)). A vector-quantizer with straight-through estimator is a function STVQ(\u00b7; C) with domain RD and codomain RD. For an input x, its output x\u0302 is given by\nz , arg min s ||x\u2212Cs||2 (5)\nx\u0302 , x + SG(Cz \u2212 x). (6) Remark 2.7. For any x \u2208 RD, STVQ(x; C) evaluates to VQ(x; C). However, for purposes of backpropagation, the Jacobian of the quantizer w.r.t. its input will now be an identity matrix everywhere, instead of a zero matrix almost everywhere. Intuitively, when using STVQ, gradients w.r.t. the quantizer outputs are copied \u2018straight through\u2019 to the inputs. Remark 2.8. We overload the notation STVQ(\u00b7; C) to operate row-wise on matrix-valued inputs."
        },
        {
            "heading": "3 TRANSFORMER-VQ",
            "text": "We now propose Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time. Proofs for all theoretical results are given in Appendix A."
        },
        {
            "heading": "3.1 QUADRATIC-TIME FORMULATION",
            "text": "Definition 3.1. Vector-Quantized Self-Attention is a function VQAttn(\u00b7; C,W{Q,K,V,G,O}) with domain RT\u00d7Dm and codomain RT\u00d7Dm . For an input X, its output Y is defined via\nX\u0303 , LN(X) \u2208 RT\u00d7Dm (7) Q , \u03c4\u22120.5LN(X\u0303WQ) \u2208 RT\u00d7Dk (8) K , \u03c4\u22120.5LN(X\u0303WK) \u2208 RT\u00d7Dk (9) V , \u03c6v(X\u0303WV ) \u2208 RT\u00d7Dv (10) G , \u03c6g(X\u0303WG) \u2208 RT\u00d7Dv (11) K\u0302 , STVQ(K; C) \u2208 RT\u00d7Dk (12) W , \u03c6w(QK\u0302\n> + B) \u2208 RT\u00d7T (13) O , (WV) G \u2208 RT\u00d7Dv (14) Y , X + OWO \u2208 RT\u00d7Dm (15)\nwhere each W\u2022 denotes a trainable projection, B denotes positional biases and/or mask, \u03c4 is a fixed constant, and the \u03c6v, \u03c6g, \u03c6w are element-wise or row-wise nonlinearities. The query/key LayerNorms use unit gain and zero bias, and STVQ(\u00b7; C) denotes row-wise application of vector-quantization with a straight-through gradient estimator (van den Oord et al., 2017). Remark 3.2. Our attention mechanism is applied to a gated attention unit (GAU) design inspired by Hua et al. (2022). GAU is a single-headed gated attention mechanism and generally uses a small key width Dk = 128, and a large value width Dv = 2Dm, with two GAUs replacing a single transformer layer. This yields a similar parameter count and compute requirement as the usual transformer layer. Remark 3.3. Prior work has also applied LayerNorm or similar to the queries and keys in attention (Henry et al., 2020; Roy et al., 2021; Zhu et al., 2021; Wu et al., 2022; Hutchins et al., 2022; Dehghani et al., 2023; Elsen et al., 2023), generally finding it to improve numerical stability and convergence."
        },
        {
            "heading": "3.2 WARMUP: LINEAR-TIME ENCODER ATTENTION",
            "text": "To simplify the theorems for decoder-only attention and build intuition, we first discuss a setting where there is no causal mask. Theorem 3.4. Suppose Bi,j = 0 for all i, j, and \u03c6w is an element-wise nonlinearity. Then the attention weights in Definition 3.1 can be factored:\nW , \u03c6w(QK\u0302 > + B) (16)\n= \u03c6w(QK\u0302 >) (17)\n= \u03c6w(QC >)\u2206 (18)\nwhere \u03c6w(QC>) \u2208 RT\u00d7S , \u2206 \u2208 RS\u00d7T and \u2206s,t , \u03b4s,zt . Here, \u03b4\u00b7,\u00b7 denotes the Kronecker delta function and zt is the VQ shortcode for timestep t.\nTheorem 3.5. Suppose Bi,j = 0 for all i, j, and \u03c6w is the row-wise softmax nonlinearity. Then the attention weights in Definition 3.1 can be factored:\nW , \u03c6w(QK\u0302 > + B) (19)\n= \u03c6w(QK\u0302 >) (20)\n= Diag(exp(QC>)\u22061)\u22121 exp(QC>)\u2206 (21)\nwhere 1 \u2208 RT , Diag(exp(QC>)\u22061)\u22121 exp(QC>) \u2208 RT\u00d7S , \u2206 \u2208 RS\u00d7T and \u2206s,t , \u03b4s,zt . Here, \u03b4\u00b7,\u00b7 denotes the Kronecker delta function and zt is the VQ shortcode for timestep t."
        },
        {
            "heading": "3.3 LINEAR-TIME DECODER ATTENTION",
            "text": "Theorem 3.6. Let L be a divisor of T . Suppose Bi,j = \u2212\u221e for j > i (causal masking), and Bi,j = 0 for j < i \u2212 L (no bias outside a sliding window). Define \u2206 \u2208 RS\u00d7T with \u2206s,t , \u03b4s,zt . Let \u03c6w be an element-wise nonlinearity with \u03c6w(\u2212\u221e) = 0. For a tensor M, let M(...,n,...) denote the slice M...,nL:(n+1)L,..., where unsliced dimensions will be denoted by \u2018:\u2019. Then the product WV in Definition 3.1 can be computed using the following block-level recurrence:\nU(n) , { U(n\u2212 1) + \u2206(:,n)V(n,:) if n \u2265 0 0 otherwise\n(22)\n(WV)(n,:) = \u03c6w(Q(n,:)C >)U(n\u2212 2) (23)\n+ \u03c6w(Q (n,:)K\u0302>(n\u22121,:) + B(n,n\u22121))V(n\u22121,:) (24)\n+ \u03c6w(Q (n,:)K\u0302>(n,:) + B(n,n))V(n,:) (25)\nwhere any tensor slice M(...,n,...) is defined as a zero tensor of width L in the sliced dimension if any block slice index n is less than zero (zero-padding).\nTheorem 3.7. Let the assumptions of Theorem 3.6 hold, but suppose \u03c6w is now the row-wise softmax nonlinearity. Let 1 \u2208 RT . Let A , exp(QK\u0302> + B). Then the product WV in Definition 3.1 can be computed using the following block-level recurrence:\nU(n) , { U(n\u2212 1) + \u2206(:,n)V(n,:) if n \u2265 0 0 otherwise\n(26)\nL(n) , { L(n\u2212 1) + \u2206(:,n)1(n) if n \u2265 0 0 otherwise\n(27)\n(AV)(n,:) = exp(Q(n,:)C >)U(n\u2212 2) (28)\n+ exp(Q(n,:)K\u0302 > (n\u22121,:) + B(n,n\u22121))V(n\u22121,:) (29)\n+ exp(Q(n,:)K\u0302 > (n,:) + B(n,n))V(n,:) (30)\n(A1)(n) = exp(Q(n,:)C >)L(n\u2212 2) (31)\n+ exp(Q(n,:)K\u0302 > (n\u22121,:) + B(n,n\u22121))1(n\u22121) (32)\n+ exp(Q(n,:)K\u0302 > (n,:) + B(n,n))1(n) (33)\n(WV)(n,:) = Diag((A1)(n)) \u22121(AV)(n,:). (34)\nRemark 3.8. Theorem 3.7 provides an algorithm to compute VQ-Attention from the queries, keys, values, gates, and codebook in O(L(S + 2L)(Dk + Dv)) time per query block, and therefore O(T (S + 2L)(Dk +Dv)) time per sequence. Remark 3.9. For numerical stability, we use an equivalent implementation of Theorem 3.7 that stores the running mean of the value vectors assigned to a given shortcode, instead of the sum as done by U(n\u2212 2). The result is made equivalent by moving the logarithm of the counts L(n\u2212 2) inside the exponentials exp(Q(n,:)C>) appearing in (AV)(n,:) and (A1)(n). See pseudocode in Appendix E.\nRemark 3.10. The general strategy of computing un-normalized softmax and its denominator is also used by many prior methods, including Memory-Efficient Attention (Rabe & Staats, 2021), FlashAttention (Dao et al., 2022), and RWKV (Peng et al., 2023); however, the first two techniques do not run in linear time, and the last one couples a recurrent state size to the model width, which is contrary to the principle of transformers."
        },
        {
            "heading": "3.4 LEARNING ALGORITHM",
            "text": ""
        },
        {
            "heading": "3.4.1 TRAINING LOSS",
            "text": "Let \u03b8 denote the set of non-codebook parameters of a transformer with N VQ-Attention layers, and let C = {C(`)}N\u22121`=0 denote the set of the layers\u2019 codebooks. For autoregressive modeling of a sequence X = {xt}Tt=0, we define the Transformer-VQ training loss as L(X;\u03b8, C) = LCE(X;\u03b8, C) + \u03b2LVQ(X;\u03b8, C) (35) where \u03b2 > 0 is a hyperparameter known as the commit loss coefficient, and\nLCE(X;\u03b8, C) , 1\nT T\u22121\u2211 t=0 \u2212 ln p(xt+1|x\u2264t,\u03b8, C) (36)\nLVQ(X;\u03b8, C) , 1\nT T\u22121\u2211 t=0 N\u22121\u2211 `=0 ||K(`)t \u2212 SG(C(`)zt )|| 2 2. (37)\nThus, the training loss is the average next-token cross-entropy loss, plus the average token\u2019s commitment losses (van den Oord et al., 2017), summed over layer codebooks. The non-codebook parameters \u03b8 receive a gradient from both loss terms. Following van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means."
        },
        {
            "heading": "3.4.2 TRAINING UPDATES",
            "text": "Instead of updating on the full sequence loss given above, we generally update every W/L query blocks, where W T , which resembles a strategy used in prior works (Dai et al., 2019; Wu et al., 2022; Hutchins et al., 2022).\nEach update is obtained by backpropagating through a window of W timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every W/L query blocks.\nWhen W/L = 1, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When W/L > 1, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window."
        },
        {
            "heading": "4 RELATED WORK",
            "text": ""
        },
        {
            "heading": "4.1 HIERARCHICAL ATTENTION",
            "text": "Combiner (Ren et al., 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu & Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al., 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al., 2021) and MegaByte (Yu et al., 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers.\nTransformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every L tokens, there are no sporadic \u2018feature consolidation\u2019 operations required during sampling."
        },
        {
            "heading": "4.2 KERNELIZABLE ATTENTION",
            "text": "Kernelizable attention (Katharopoulos et al., 2020; Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks.\nClustering attention (Vyas et al., 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd\u2019s iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation."
        },
        {
            "heading": "4.3 COMPRESSIVE ATTENTION",
            "text": "Compressive Transformers (Rae et al., 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022; Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al. (2021); Hutchins et al. (2022). More generally, Transformer-VQ differs from compressive/recurrent transformers in that it has an equivalence to quadratic-time attention over vector-quantized keys. In other words, if the keys are already vector-quantized, the Transformer-VQ cache losslessly reduces the cost to linear time.\nPerceivers (Jaegle et al., 2021; Hawthorne et al., 2022) use cross-attention to attend to long sequences, and compute self-attention over only a narrow stack of \u2018latents\u2019. Transformer-VQ differs from Perceivers in that it computes dense self-attention in linear time, instead of just cross-attention. Thus, while Perceivers\u2019 long-range layers incur a quadratic time complexity during sampling, TransformerVQ generates sequences in linear time."
        },
        {
            "heading": "4.4 GATED SEQUENCE MODELS",
            "text": "Gated attention was introduced in FLASH (Hua et al., 2022) as a fusion of attention sublayers (Vaswani et al., 2017) and GLU-based MLP sublayers (Shazeer, 2020). Various gating mechanisms have previously been used to stabilize training of transformers (Parisotto et al., 2019) and other sequence models including S4 (Gu et al., 2022), GSS (Mehta et al., 2022), MEGA (Ma et al., 2023) and RWKV (Peng et al., 2023). Transformer-VQ uses the original gating formulation from Hua et al. (2022), and develops a new attention mechanism."
        },
        {
            "heading": "4.5 VQ, K-MEANS, AND BEYOND",
            "text": "Ideas relating to k-means, vector quantization, and/or codebooks have also been applied in transformers for sparse attention (Roy et al., 2021; Wang et al., 2021; 2022), feature learning (Mao et al., 2022; Roy et al., 2022), sparsely-activated MLPs (Lample et al., 2019), and expert selection (Roller et al., 2021). These works generally feature codebooks or similar within a transformer architecture. Several works also have proposed models that feature a codebook somewhere outside a transformer, e.g., when transformers are priors for VQ-VAEs (Kaiser et al., 2018; Dhariwal et al., 2020; Ramesh et al., 2021; Lee et al., 2022; Zhou et al., 2022). Transformer-VQ uses one codebook within each layer and, in contrast to all of the aforementioned works, computes dense self-attention in linear time.\nTransformer-VQ is not directly related to methods which quantize the weights of a transformer e.g., Dettmers et al. (2022); Dettmers & Zettlemoyer (2023); Frantar et al. (2023). Such methods are typically applied after training to reduce the memory overhead of the model weights, while still computing in higher precision. As such, they do not affect the bitwidth of the queries, keys, or values, nor the complexity of self-attention. However, if applying our method to large models, these approaches may be complementary during inference."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Transformer-VQ is implemented in Jax (Bradbury et al., 2018) and Flax (Heek et al., 2023). For training, we use TPU v3 pod slices (Jouppi et al., 2017). Hyperparameters follow Appendix C unless specifically mentioned. Generated samples for all models are provided in Appendix D."
        },
        {
            "heading": "5.1 PRELIMINARY STUDIES",
            "text": ""
        },
        {
            "heading": "5.1.1 CODEBOOK SIZE ABLATIONS",
            "text": "Larger codebook sizes may allow more flexible attention patterns and could improve the fidelity of the gradients, both of which are likely to benefit model quality at the expense of additional wall time. To investigate, we ablate the codebook size S using the Enwik8 dataset (described in \u00a7 5.2.1), and report the lowest validation bits-per-byte (BPB, lower is better) obtained by each model in Table 1.\nTable 1: Codebook size ablations.\nSetting Val. BPB Latency (Rel.)\nS = 256 1.010 0.927 S = 512 1.005 1.0 S = 1024 1.000 1.109\nTable 2: Compressive cache ablation.\nCompressive cache Val. BPB Latency (Rel.)\nNo 1.026 0.836 Yes 1.010 0.927\nTable 1 confirms the intuition that larger codebooks improve the prediction quality (lower BPB) in return for additional wall time per training step. In particular, for this dataset and model size, increasing the codebook size by a factor of two appears to decrease the validation BPB by about a factor of 0.995. This result may suggest that the validation loss follows a power-law scaling (Kaplan et al., 2020) w.r.t. codebook size, though more experiments are needed to verify this phenomenon, and it is subject to the caveat that the validation loss must eventually level off (Henighan et al., 2020; Hoffmann et al., 2022), as the model cannot be expected to obtain zero loss at infinite codebook size."
        },
        {
            "heading": "5.1.2 COMPRESSIVE CACHE ABLATION",
            "text": "Since our model has several architectural differences from most prior works, the benefit of the compressive cache must be shown directly. To investigate, we train a model with the compressive cache omitted, using codebook size S = 256. We report the validation BPB for Enwik8 in Table 2.\nAs shown in Table 2, removing the compressive cache reduces the wall time per step by a factor of about 1.1 at the evaluated model size, but leads to a significant drop in quality (higher bits-per-byte). This confirms the importance of our compressive cache mechanism."
        },
        {
            "heading": "5.1.3 LATENCY AND THROUGHPUT",
            "text": "We now measure the training latency (seconds per step) and compute the training throughput (tokens per second). The latter is computed as tokens per batch divided by latency, and allows a direct efficiency comparison across different sequence lengths. We benchmark on a TPU v3 with 8 cores, using a global batch size of 8 sequences. For these experiments, we scale the sequence length T by multiples of 4\u00d7, and backpropagate through the entire sequence length. We compare an unquantized quadratic-time full attention baseline (\u2018Full\u2019) to our proposed linear-time VQ-Attention (\u2018VQ\u2019) using Theorem 3.7. Since this theorem does not require access to the output gates, VQ-Attention can be extended to multi-head attention variants as well. For each attention type, we therefore benchmark three head types: multi-head attention (\u2018MHA\u2019; Vaswani et al. (2017)), multi-query attention (\u2018MQA\u2019; Shazeer (2019)), and single-head gated attention (\u2018SHGA\u2019 aka GAU; Hua et al. (2022)). For VQ-attention, we use codebook size S = 512 and block length L = 512, which is the same as our later experiments. All models use roughly 190M parameters total.\nAs shown in Table 6, our model has a 3x lower latency/3x higher throughput than the quadratic attention baseline at T = 8192 when using SHGA for both. Moreover, Transformer-VQ is over 6x faster than the quadratic-time baselines when using MQA/MHA for both models. As the sequence\nlength increases to T = 32768, Transformer-VQ is over 12x faster than the quadratic time baseline when both use SHGA. For MQA/MHA, the quadratic-time attention gives an out-of-memory error at T = 32768, while Transformer-VQ maintains comparable or better throughput than with 4x shorter sequences. Table 7 even shows that Transformer-VQ can scale to sequences of length T = 131072 without a substantial decrease in throughput and without running out of memory."
        },
        {
            "heading": "5.2 QUANTITATIVE RESULTS",
            "text": "To assess the ability of Transformer-VQ to learn long-range dependencies, we now conduct a series of large-scale experiments, benchmarking on several long-range autoregressive modeling tasks. For fair comparison, we only benchmark against models (a) trained without using any extra data or augmentation, and (b) evaluated with fixed parameters. In all cases, we use codebook size S = 512.\n5.2.1 ENWIK8\nEnwik8 is a byte-level language modeling dataset consisting of 100 million bytes of unprocessed Englishlanguage Wikipedia articles (Mahoney, 2011), with long-term dependencies that may span tens of thousands of bytes. Per convention, it is split into train, validation, and test sets of 90 million, 5 million, and 5 million bytes, respectively (Child et al., 2019; Rae et al., 2020).\nFor this dataset, we trained a Transformer-VQ with 190M parameters, smaller than the model by Dai et al. (2019). We report test bits-per-byte (BPB) in Table 3.\nTransformer-VQ obtains a BPB of 0.99, notably matching the result of the large Transformer-XL model from Dai et al. (2019), while using 33% fewer parameters and a 75% shorter cache that covers a longer context.\nFor this dataset, we found overfitting was a significant issue, and due to the compressive cache mechanism, using i.i.d. attention dropout was not possible. Sweeping over the residual dropout rate, weight decay coefficient, and layerdrop (Fan et al., 2020a) rate, we found a setting yielding good generalization. Nonetheless Transformer-VQ does fall short of state-of-the-art here, with several works using complex recurrence or forgetting mechanisms and obtaining better Enwik8 results."
        },
        {
            "heading": "5.2.2 PG-19",
            "text": "PG-19 is an open-vocabulary language modeling dataset consisting of 11 gigabytes of text from over 28,000 freely-available Project Gutenberg books published prior to 1919 (Rae et al., 2020). The average number of words per book is nearly 70,000, enabling learning long-term dependencies, especially in novels (Sun et al., 2021; Hutchins et al., 2022).\nFor this dataset, we trained a Transformer-VQ with 1.3B parameters, similar to the largest model by Hutchins et al. (2022). Since PG-19 is an open-vocabulary dataset, we first learned a SentencePiece vocabulary (Kudo & Richardson, 2018) of size 32,000 using the BPE method. Following the calculations of Rae et al. (2020), we report the test set word-level perplexity (WLP) in Table 4.\nTransformer-VQ obtains a WLP of 26.6, very close to the state-of-the-art by Block-Recurrent Transformers (Hutchins et al., 2022). Interestingly, since our Transformer-VQ design is equivalent to using dense self-attention with vector-quantized keys, our strong result shows that models using selfattention only (no recurrence) can also be highly competitive on PG-19. This affirms the efficacy of standalone self-attention as a method for sequence processing at scale. Furthermore, compared to the Block-Recurrent Transformer, our model can be implemented via intra-block sums and cross-block reductions, a strategy also used by FLASH (Hua et al., 2022) and shown to be faster in Appendix B. Lastly, we avoid the instabilities of FLASH (Qin et al., 2022a; Ma et al., 2023) thanks to softmax normalization and our cache normalization (\u00a7 3.9)."
        },
        {
            "heading": "5.2.3 IMAGENET64",
            "text": "ImageNet64 is an image dataset consisting of over 1.2 million images downsampled to 64x64 resolution (Chrabaszcz et al., 2017; Deng et al., 2009). Flattening the images yields an autoregressive density estimation task on sequences of over 12,000 bytes each. Note since the official test set is not public for this dataset, we report results on the official validation set. For validation purposes we used a held-out set of about 80,000 examples from the training split.\nFor this dataset, we trained Transformer-VQ models with 190M and 1.2B parameters, similar to the Enwik8 and PG-19 models, respectively. We report the bits-per-byte on the official validation set in Table 5. Several of the earlier baselines used an earlier variant of downsampled ImageNet prepared by van den Oord et al. (2016) with a different downsampling algorithm. Since that variant has been unavailable through official channels for about a year, we used the newer variant following Lipman et al. (2023). We emphasize that our results using the newer variant cannot be directly compared with baselines using the earlier variant; however, due to several reporting ambiguities, Table 5 does not symbolically distinguish variants used.\nTransformer-VQ with 190M parameters is roughly the same size as the Efficient VDVAE (Hazami et al., 2022), but obtains a better result of 3.22 BPB, setting a new state-of-the-art for small models. Transformer-VQ with 1.2B parameters obtains a 3.16 BPB, setting a new absolute state-of-the-art on this dataset, and generates high-fidelity samples on par with Perceiver AR while using 33% fewer steps, omitting its image-specific architectural adjustments, and generating samples in linear time."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Transformer-VQ is a transformer decoder computing softmax-based self-attention in linear time. Its efficient attention is enabled by vector-quantized keys, which allow our cache to be attended to in compressed form, while yielding the same result as uncompressed attention over the same keys. Large-scale experiments show Transformer-VQ is an efficient and flexible autoregressive model, with state-of-the-art results or near on PG-19 and ImageNet64. Future work directions include formal scaling laws, larger models, and porting to lower-level frameworks like Pallas, Triton, or CUDA.\nREPRODUCIBILITY STATEMENT\nTo facilitate reproducibility, our attention mechanism is described mathematically in Section 3, and pseudocode is provided in Appendix E. In addition, our hyperparameters and other implementation details are given in Appendix C, and our implementation is open-sourced at the link in the abstract."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We are grateful to the anonymous reviewers for their helpful feedback. In addition, we would like to express our gratitude to the Python community, especially the Jax ecosystem contributors, for the effective libraries used in this project. This project was generously supported with Cloud TPUs from Google\u2019s TPU Research Cloud (TRC)."
        },
        {
            "heading": "A THEOREMS",
            "text": "A.1 PROOF OF THEOREM 2.2\nProof. This proof is based on Guo et al. (2019). Invoking the fact that q,k, \u03d5(k) \u2208 RD, the assumed independence between q and k, the law of iterated expectations, and the isotropy assumption on q, i.e., Eq[qq>] = \u03c32ID for \u03c32 > 0, we have\nEq,k[q>k\u2212 q>\u03d5(k)]2 (38) = Eq,k[q>(k\u2212 \u03d5(k))]2 (39) = Eq,k[q>(k\u2212 \u03d5(k))]>[q>(k\u2212 \u03d5(k))] (40) = Eq,k(k\u2212 \u03d5(k))>qq>(k\u2212 \u03d5(k)) (41) = Ek(k\u2212 \u03d5(k))>Eq[qq>](k\u2212 \u03d5(k)) (42) \u221d Ek(k\u2212 \u03d5(k))>ID(k\u2212 \u03d5(k)) (43) = Ek||k\u2212 \u03d5(k)||2. (44)\nA.2 PROOF OF COROLLARY 2.3\nProof. By definition, VQ(k; C) , arg minc\u2208{Cs}S\u22121s=0 ||k\u2212 c|| 2. In other words, \u03d5(k) = VQ(k; C) minimizes ||k\u2212 \u03d5(k)||2 under the constraint that the outputs of \u03d5 are limited to the rows of C, i.e., \u03d5(RD) = {Cs}S\u22121s=0 . Since this choice is a pointwise minimizer under the given constraint, it is also a minimizer of the expectation Ek||k\u2212 \u03d5(k)||2 under the same constraint. Under the assumptions of Theorem 2.2, the aforementioned expectation is equal to Eq,k||q>k \u2212 q>\u03d5(k)||2 up to a positive proportionality constant \u03c32. As a result, VQ(k; C) is also a minimizer of the expectation Eq,k||q>k\u2212 q>\u03d5(k)||2 under the same constraint on the output of \u03d5.\nA.3 PROOF OF THEOREM 3.4\nProof. When \u03c6w is an element-wise nonlinearity, \u03c6(c) is well-defined, where c is any scalar. Then using definitions alone, we have\n[\u03c6w(QC >)\u2206]i,j = \u03c6w(QC >)i,:\u2206:,j (45)\n= S\u22121\u2211 s=0 \u03c6w(QC >)i,s\u2206s,j (46)\n= S\u22121\u2211 s=0 \u03c6w(Qi,:C > s,:)\u03b4s,zj (47) = \u03c6w(Qi,:C > zj ,:) (48)\n= \u03c6w(Qi,:K\u0302 > j,:) (49)\n= [\u03c6w(QK\u0302 >)]i,j (50)\nA.4 PROOF OF THEOREM 3.5\nProof. By Theorem 3.4 with \u03c6w(\u00b7) = exp(\u00b7), we have exp(QK\u0302>) = exp(QC>)\u2206. Invoking the definition of row-wise softmax and applying substitution, we have\nSoftmax(QK\u0302>) = Diag(exp(QK\u0302>)1)\u22121 exp(QK\u0302>) (51)\n= Diag(exp(QC>)\u22061)\u22121 exp(QC>)\u2206. (52)\nA.5 PROOF OF THEOREM 3.6\nProof. For n = 0, 1 the result follows by inspection.\nFor n \u2265 2, by the definition of U(n\u2212 2) we have\nU(n\u2212 2) = (n\u22121)L\u22121\u2211\nj=0\n\u2206:,jVj,: = \u2206(:,0:n\u22121)V(0:n\u22121,:). (53)\nNote that in our notation, the superscripts\u2019 block index range is non-inclusive on the ending value, so \u2206(:,0:n\u22121)V(0:n\u22121,:) is equal to the sum of the matrix products for the matrix blocks from 0 to n\u2212 2. Thus, by substitution, we have\n\u03c6w(Q(n,:)C >)U(n\u2212 2) = \u03c6w(Q(n,:)C>)\u2206(:,0:n\u22121)V(0:n\u22121,:) (54)\nWe invoke the same argument as in the proof of Theorem 3.4 to conclude \u03c6w(Q(n,:)C>)\u2206(:,0:n\u22121) = W(n,0:n\u22121). Substituting this expression into the right-hand side above gives\n\u03c6w(Q(n,:)C >)U(n\u2212 2) = W(n,0:n\u22121)V(0:n\u22121,:). (55)\nSubstituting this expression into the formula for (WV)(n,:) claimed in the theorem statement, and invoking the same argument as in the proof of Theorem 3.4 on the middle term, we see the claimed formula has the form W(n,0:n\u22121)V(0:n\u22121,:) + W(n,n\u22121)V(n\u22121,:) + W(n,n)V(n,:). The diagonal block W(n,n) of W is causally masked, so the sum of the three terms indeed equals (WV)(n,:).\nA.6 PROOF OF THEOREM 3.7\nProof. Recall that we defined A , exp(QK\u0302> + B). The proposed expression for (AV)(n,:) follows from Theorem 3.6 with \u03c6w(\u00b7) = exp(\u00b7). The proposed expression for (A1)(n) follows by a substitution argument using (AV)(n,:). Normalizing (AV)(n,:) by (A1)(n) and iterating over n thus yields all blocks of the product WV when the nonlinearity \u03c6w is row-wise softmax."
        },
        {
            "heading": "B THROUGHPUT",
            "text": "We present throughput results for three methods to compute the cache variables: serial scan, matmul, and associative scan. The first two are generalizations of the cross-block reduction methods from FLASH (Hua et al., 2022), which were a simple cumulative sum and matrix multiplication by a lower-triangular matrix of ones, respectively. We found our proposed generalizations were necessary for stable training, an issue where FLASH has known weaknesses (Qin et al., 2022a; Ma et al., 2023). Pseudocode for each of our stable reduction methods is given in Appendix E.\nIn addition to the three cross-block reduction methods to compute the cache variables from parallelcomputed per-block summaries, we also benchmark an input scanning implementation of VQAttention inspired by Wu et al. (2022); Hutchins et al. (2022), such that all the operations for a transformer layer are performed one input block at a time. To ground all comparisons, we benchmark the throughput against a transformer using unquantized quadratic-time attention, the same attention head type (SHGA, MQA, or MHA), and an identical non-codebook parameter count."
        },
        {
            "heading": "C TRAINING DETAILS",
            "text": "C.1 HYPERPARAMETERS\nPer-dataset hyperparameters are provided below.\nNote that the 190M parameter ImageNet64 result was added after the other experiments had concluded. To avoid biasing its result, we use the exact same architectural hyperparameters as the Enwik8 model, and the exact same regularization as the larger ImageNet64 model. The smaller ImageNet model was trained in a newer version of our codebase optimized for higher throughput and faster compile times, rather than training on long sequences in constant space via input scans and truncated backprop through time. The attention in the optimized codebase was unit-tested to match the original.\nC.2 IMPLEMENTATION\nWeights and token embeddings were initialized following Chowdhery et al. (2022). For the small model, the classifier layer omits LayerNorm and is independently parameterized. For the large model, the classifier layer uses LayerNorm and its projection is tied with the token embedding table, then scaled down by a large constant. For image datasets, we add absolute sinusoidal position embeddings, scaled by a trainable scalar, to the token embeddings (Hua et al., 2022; Vaswani et al., 2017). We used a maximum angular wavelength of 105 for all sinusoidal embeddings.\nWe used the pre-norm placement of LayerNorm (Radford et al., 2019), and always used the RMS LayerNorm variant (Zhang & Sennrich, 2019). For the activations, we used \u03c6w = Softmax and \u03c6v = \u03c6g = SiLU, the self-gated activation (Elfwing et al., 2017; Ramachandran et al., 2017). Several models use LayerDrop for regularization (Fan et al., 2020a), and following the Transformer-XL codebase (Dai et al., 2019) models apply dropout to the flipped sinusoidal embeddings used for (local) relative positional biases.\nWe used float32 parameters, with bfloat16 precision for most computations (Rae et al., 2021). For the AdamW optimizer (Loshchilov & Hutter, 2019), we used gradient clip 0.1, max learning rate \u03b1 = 0.0004 and hyperparameters \u03b21 = 0.9, \u03b22 = 0.98, = 10\u22129. For the Adafactor optimizer (Shazeer & Stern, 2018), we used relative stepsizes, update clip 1.0, max learning rate \u03b1 = 0.01, and hyperparameters \u03b2\u03021 = 0.0, \u03b2\u03022,t = 1 \u2212 t\u22120.8. We used weight decay with a constant schedule throughout training and omit decay on any one-dimensional parameter tensors (Radford et al., 2019). The codebook commit coefficient was always \u03b2 = 0.0001 and codebook EMA rate was always \u03b3 = 0.99. Learning rates were linearly warmed up for 10,000 steps, then decayed by a 10x factor using a cosine schedule."
        },
        {
            "heading": "D GENERATED SAMPLES",
            "text": "D.1 QUALITATIVE ANALYSIS\nD.1.1 PG-19\nNo effort has been made to explain elementary methods of photography, for the reason that such explanation has been found in the publications of every leading technical journal. The endeavor has been to present what is\nWe generated 128 sequences using nucleus sampling (Holtzman et al., 2020). In Figure 4, we observe a sample except in which our PG-19 model synthesizes high-quality text, and maintains a consistent tone, topic, and train of thought. These observations were found to hold for the vast majority of the samples we generated.\nD.1.2 IMAGENET64\nFigures 3 and 5 show a subset of samples with the same indices from two batches with different nucleus settings. We see that our large ImageNet64 model synthesizes sequences of over 12,000 bytes and is capable of depicting relatively high-fidelity ocean water, shorelines, leaves, insects, trees, animals, people, mountains, and architecture.\nD.2 EXTENSIVE SAMPLES\nSamples for Enwik8, PG-19, and ImageNet64 can be viewed at the anonymized URLs in Table 11.\nD.3 IMAGENET64 - FULL BATCH"
        },
        {
            "heading": "E PSEUDOCODE",
            "text": "import flax.linen as nn import jax import jax.numpy as jnp import chex\nclass VQAttn(nn.Module): n_code: int d_k: int d_v: int\n@nn.compact def __call__(self, x):\n\"\"\"Input shape: [batch size, num blocks, block length, model width].\"\"\" B, R, C, D = x.shape S, K, V = self.n_code, self.d_k, self.d_v x_tilde = RMSLayerNorm(axis=\u22121)(x) q = RMSLayerNorm(axis=\u22121)(nn.Dense(self.d_k)(x_tilde)) k = RMSLayerNorm(axis=\u22121)(nn.Dense(self.d_k)(x_tilde)) v = jax.nn.silu(nn.Dense(self.d_v)(x_tilde)) g = jax.nn.silu(nn.Dense(self.d_v)(x_tilde)) quantizer = VectorQuantizer(codebook_size=self.n_code, width=self.d_k) k_hat, z, l_commit, l_codebook = quantizer(k) # quantized keys, shortcodes, etc c = quantizer.get_codebook()\nlocal_biases = XLBiasProducer(width=self.d_k, length=2\u2217C)(q) chex.assert_shape(local_biases, [B, R, C, 2\u2217C]) local_biases_prev, local_biases_present = jnp.split(local_biases, 2, axis=\u22121) scores_present = jnp.einsum(\"brik,brjk\u2212>brij\", q, k_hat) scores_present += local_biases_present scores_present \u2212= 1e30 \u2217 (1 \u2212 jnp.tril(jnp.ones_like(scores_present)))\nk_hat_prev = jnp.pad(k_hat[:, :\u22121], ((0, 0), (1, 0), (0, 0), (0, 0))) v_prev = jnp.pad(v[:, :\u22121], ((0, 0), (1, 0), (0, 0), (0, 0))) scores_prev = jnp.einsum(\"brik,brjk\u2212>brij\", q, k_hat_prev) scores_prev += local_biases_prev scores_prev = jnp.pad(\nscores_prev[:, 1:], ((0, 0), (1, 0), (0, 0), (0, 0)), constant_values=\u22121e30,\n)\nscores_cache = jnp.einsum(\"brik,sk\u2212>bris\", q, c) cache_u_div_l_by_block, cache_l_by_block = get_cache_vars(z, v, S) chex.assert_shape(cache_u_div_l_by_block, [B, R, S, V]) chex.assert_shape(cache_l_by_block, [B, R, S]) count_biases = jnp.where(\njnp.greater(cache_l_by_block, jnp.zeros_like(cache_l_by_block)), jnp.log(jnp.clip(cache_l_by_block, a_min=1.0)), jnp.full_like(cache_l_by_block, fill_values=\u22121e30),\n) scores_cache += jnp.expand_dims(count_biases, axis=\u22122)\nscores_present_max = jnp.max(scores_present, axis=\u22121) scores_prev_max = jnp.max(scores_present, axis=\u22121) scores_cache_max = jnp.max(scores_cache, axis=\u22121) scores_max = jnp.maximum(\njnp.maximum(scores_present_max, scores_prev_max), scores_cache_max,\n) scores_max = jax.lax.stop_gradient(scores_max) scores_present \u2212= scores_max[..., None] scores_prev \u2212= scores_max[..., None] scores_cache \u2212= scores_max[..., None]\na_present = jnp.exp(scores_present) a_prev = jnp.exp(scores_prev) a_cache = jnp.exp(scores_cache) d = jnp.sum(a_present, axis=\u22121) d += jnp.sum(a_prev, axis=\u22121) d += jnp.sum(a_cache, axis=\u22121) w_present = a_present / d[..., None] w_prev = a_prev / d[..., None] w_cache = a_cache / d[..., None] wv = jnp.einsum(\"brij,brjv\u2212>briv\", w_present, v) wv += jnp.einsum(\"brij,brjv\u2212>briv\", w_prev, v_prev) wv += jnp.einsum(\"bris,brsv\u2212>briv\", w_cache, cache_u_div_l_by_block) o = wv \u2217 g residual = nn.Dense(D)(o) return x + residual, l_commit, l_codebook\nCode 1: Jax/Flax pseudocode for VQ-Attention.\ndef get_cache_vars(z, v, n_code): # throughout this function, we often use clipping of elementwise denominators at 1 to avoid nans. # in the places where we do this, it does not alter the actual cache variable estimates # since the corresponding entries in the numerator will be zero when the clip is applied.\ndelta = jax.nn.one_hot(z, num_classes=n_code, dtype=v.dtype, axis=\u22121) delta1_by_block = jnp.einsum(\"bris\u2212>brs\", delta) deltav_by_block = jnp.einsum(\"bris,briv\u2212>brsv\", delta, v) deltav_by_block_normalized = jnp.divide(\ndeltav_by_block, jnp.clip(delta1_by_block[..., None], a_min=1.0),\n)\ndef scan_func(carry, in_dict): # computes running average of the value vectors for each shortcode (\"upper div lower\"), # and running count (\"lower\"). lower = carry[\"lower\"] lower_block = in_dict[\"delta1_by_block\"] lower_new = lower + lower_block f1 = jnp.divide(lower, jnp.clip(lower_new, a_min=1.0)) f2 = jnp.divide(lower_block, jnp.clip(lower_new, a_min=1.0)) upper_div_lower_new = jnp.add(\nf1[..., None] \u2217 carry[\"upper_div_lower\"], f2[..., None] \u2217 in_dict[\"deltav_by_block_normalized\"],\n) carry_new = dict(\nupper_div_lower=upper_div_lower_new, lower=lower_new,\n) return carry_new, carry_new # state to carry, output to save\n# before we scan, we have to transpose since jax only supports scans along axis 0. # this is still fast, possibly because jnp.transpose might be choosing to return a view deltav_by_block_normalized = jnp.transpose(deltav_by_block_normalized, (1, 0, 2, 3)) delta1_by_block = jnp.transpose(delta1_by_block, (1, 0, 2)) _, cache_vars = jax.lax.scan(\nf=scan_func, init=dict(\nupper_div_lower=jnp.zeros(dtype=self.dtype, shape=deltav_by_block_normalized.shape[1:]), lower=jnp.zeros(dtype=self.dtype, shape=delta1_by_block.shape[1:]),\n), xs=dict(\ndeltav_by_block_normalized=deltav_by_block_normalized, delta1_by_block=delta1_by_block,\n), unroll=1,\n)\ncache_var_upper_div_lower = jnp.pad( jnp.transpose(cache_vars[\"upper_div_lower\"][:\u22122], (1, 0, 2, 3)), ((0, 0), (2, 0), (0, 0), (0, 0)), ) cache_var_lower = jnp.pad(\njnp.transpose(cache_vars[\"lower\"][:\u22122], (1, 0, 2)), ((0, 0), (2, 0), (0, 0)),\n) return cache_var_upper_div_lower, cache_var_lower\nCode 2: Jax/Flax pseudocode to get cache variables for all blocks; serial scan version.\ndef get_cache_vars(z, v, n_code): # throughout this function, we often use clipping of elementwise denominators at 1 to avoid nans. # in the places where we do this, it does not alter the actual cache variable estimates # since the corresponding entries in the numerator will be zero when the clip is applied.\ndelta = jax.nn.one_hot(z, num_classes=n_code, dtype=v.dtype, axis=\u22121) delta1_by_block = jnp.einsum(\"bris\u2212>brs\", delta) deltav_by_block = jnp.einsum(\"bris,briv\u2212>brsv\", delta, v) deltav_by_block_normalized = jnp.divide(\ndeltav_by_block, jnp.clip(delta1_by_block[..., None], a_min=1.0),\n)\ndelta1_by_block_tiled = jnp.einsum( \"brs,bgs\u2212>bsrg\", jnp.ones_like(delta1_by_block), delta1_by_block, ) delta1_by_block_tiled = jnp.tril(delta1_by_block_tiled) delta1_fracs_by_block = jnp.divide(\ndelta1_by_block_tiled, jnp.clip(jnp.einsum(\"bsrg\u2212>bsr\", delta1_by_block_tiled)[..., None], a_min=1.0),\n) deltav_by_block_cumulative_normalized = jnp.einsum(\n\"bsrg,bgsv\u2212>brsv\", delta1_fracs_by_block, deltav_by_block_normalized ) delta1_by_block_cumulative = jnp.cumsum(delta1_by_block, axis=1)\ncache_var_upper_div_lower = jnp.pad( deltav_by_block_cumulative_normalized[:, :\u22122], ((0, 0), (2, 0), (0, 0), (0, 0)), ) cache_var_lower = jnp.pad(\ndelta1_by_block_cumulative[:, :\u22122], ((0, 0), (2, 0), (0, 0)) ) return cache_var_upper_div_lower, cache_var_lower\nCode 3: Jax/Flax pseudocode to get cache variables for all blocks; matmul version\ndef get_cache_vars(z, v, n_code): # throughout this function, we often use clipping of elementwise denominators at 1 to avoid nans. # in the places where we do this, it does not alter the actual cache variable estimates # since the corresponding entries in the numerator will be zero when the clip is applied.\ndelta = jax.nn.one_hot(z, num_classes=n_code, dtype=v.dtype, axis=\u22121) delta1_by_block = jnp.einsum(\"bris\u2212>brs\", delta) deltav_by_block = jnp.einsum(\"bris,briv\u2212>brsv\", delta, v) deltav_by_block_normalized = jnp.divide(\ndeltav_by_block, jnp.clip(delta1_by_block[..., None], a_min=1.0),\n)\ndef merge_func(a, b): a_upper_div_lower = a[0] b_upper_div_lower = b[0] a_lower = a[1] b_lower = b[1] lower_new = a_lower + b_lower term1 = jnp.multiply(\njnp.divide(a_lower, jnp.clip(lower_new, a_min=1.0))[..., None], a_upper_div_lower,\n) term2 = jnp.multiply(\njnp.divide(b_lower, jnp.clip(lower_new, a_min=1.0))[..., None], b_upper_div_lower,\n) upper_div_lower_new = term1 + term2 return upper_div_lower_new, lower_new\nassoc_scan_output = jax.lax.associative_scan( fn=merge_func, elems=(deltav_by_block_normalized, delta1_by_block), reverse=False, axis=1, ) deltav_by_block_normalized_cumulative = assoc_scan_output[0] delta1_by_block_cumulative = assoc_scan_output[1] cache_var_upper_div_lower = jnp.pad(\ndeltav_by_block_normalized_cumulative[:, :\u22122], ((0, 0), (2, 0), (0, 0), (0, 0)),\n) cache_var_lower = jnp.pad(\ndelta1_by_block_cumulative[:, :\u22122], ((0, 0), (2, 0), (0, 0)) ) return cache_var_upper_div_lower, cache_var_lower\nCode 4: Jax/Flax pseudocode to get cache variables for all blocks; associative scan version"
        }
    ],
    "title": "TRANSFORMER-VQ: LINEAR-TIME TRANSFORMERS",
    "year": 2024
}