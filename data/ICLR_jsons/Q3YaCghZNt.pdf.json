{
    "abstractText": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
    "authors": [],
    "id": "SP:eb2ea7ea06c6e0e329c87e81bad027e60267d004",
    "references": [
        {
            "authors": [
                "Dirk Beyer"
            ],
            "title": "Competition on software verification and witness validation: Sv-comp 2023",
            "year": 2023
        },
        {
            "authors": [
                "Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yiannis Charalambous",
                "Norbert Tihanyi",
                "Ridhi Jain",
                "Youcheng Sun",
                "Mohamed Amine Ferrag",
                "Lucas C. Cordeiro"
            ],
            "title": "A new era in software security: Towards self-healing software via large language models and formal verification, 2023",
            "year": 2023
        },
        {
            "authors": [
                "nan Saeta",
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel. Palm"
            ],
            "title": "Scaling language modeling with pathways, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Emily First",
                "Markus N. Rabe",
                "Talia Ringer",
                "Yuriy Brun"
            ],
            "title": "Baldur: Whole-proof generation and repair with large language models",
            "venue": "CoRR, abs/2303.04910,",
            "year": 2023
        },
        {
            "authors": [
                "Mikhail R Gadelha",
                "Felipe R Monteiro",
                "Jeremy Morse",
                "Lucas C Cordeiro",
                "Bernd Fischer",
                "Denis A Nicole"
            ],
            "title": "Esbmc 5.0: an industrial-strength c model checker",
            "venue": "In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Garg",
                "Daniel Neider",
                "P. Madhusudan",
                "Dan Roth"
            ],
            "title": "Learning invariants using decision trees and implication counterexamples",
            "venue": "In Rastislav Bod\u0131\u0301k and Rupak Majumdar (eds.), Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,",
            "year": 2016
        },
        {
            "authors": [
                "Matthias Heizmann",
                "J\u00fcrgen Christ",
                "Daniel Dietsch",
                "Evren Ermis",
                "Jochen Hoenicke",
                "Markus Lindenmann",
                "Alexander Nutz",
                "Christian Schilling",
                "Andreas Podelski"
            ],
            "title": "Ultimate automizer with smtinterpol: (competition contribution)",
            "venue": "In International Conference on Tools and Algorithms for the Construction and Analysis of Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Kroening",
                "Michael Tautschnig"
            ],
            "title": "Cbmc\u2013c bounded model checker: (competition contribution)",
            "year": 2014
        },
        {
            "authors": [
                "Aitor Lewkowycz",
                "Anders Andreassen",
                "David Dohan",
                "Ethan Dyer",
                "Henryk Michalewski",
                "Vinay V. Ramasesh",
                "Ambrose Slone",
                "Cem Anil",
                "Imanol Schlag",
                "Theo GutmanSolo",
                "Yuhuai Wu",
                "Behnam Neyshabur",
                "Guy Gur-Ari",
                "Vedant Misra"
            ],
            "title": "Solving quantitative reasoning problems with language models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Kexin Pei",
                "David Bieber",
                "Kensen Shi",
                "Charles Sutton",
                "Pengcheng Yin"
            ],
            "title": "Can large language models reason about program invariants",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Rahul Sharma",
                "Saurabh Gupta",
                "Bharath Hariharan",
                "Alex Aiken",
                "Percy Liang",
                "Aditya V. Nori"
            ],
            "title": "A data driven approach for algebraic loop invariants",
            "venue": "Programming Languages and Systems - 22nd European Symposium on Programming,",
            "year": 2013
        },
        {
            "authors": [
                "Xujie Si",
                "Aaditya Naik",
                "Hanjun Dai",
                "Mayur Naik",
                "Le Song"
            ],
            "title": "Code2inv: A deep learning framework for program verification",
            "venue": "Computer Aided Verification - 32nd International Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Martin White",
                "Michele Tufano",
                "Matias Martinez",
                "Martin Monperrus",
                "Denys Poshyvanyk"
            ],
            "title": "Sorting and transforming program repair ingredients via deep learning code similarities",
            "venue": "IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE,",
            "year": 2019
        },
        {
            "authors": [
                "an LLM",
                "Minerva (Lewkowycz"
            ],
            "title": "2022), to generate theorem proofs that are checked by Isabelle theorem prover. They also proposed a proof repair procedure. In contrast, our interactive decision procedure relies on an automated reasoner to generate proofs and only uses LLMs generated program properties. There are a number of learning approaches to automatically generate invariants",
            "venue": "Baldur (First et al.,",
            "year": 2023
        },
        {
            "authors": [
                "2016 Garg et al",
                "2013). Sharma Sharma et al"
            ],
            "title": "proposed a data-driven iterative approach to derive algebraic equation invariants from data generated through concrete executions of the program. Garg et al. (2016) proposed a decision tree-based approach to learn invariants from examples; however, the space of possible invariants is limited to a logical combination of binary linear constraints. The most related work to our framework is Code2Inv (Si et al., 2020)",
            "year": 2013
        },
        {
            "authors": [
                "Pei"
            ],
            "title": "2023) investigates the potential of invariant generation for Java programming language. While this framework does not incorporate automated reasoning components, it shows the potential of LLMs to uncover program properties. Table 2 presents a summary of learning-based approaches",
            "venue": "F PROMPTING THE GPT",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "AI-powered language models are being routinely used to help developers. Examples include program synthesis from natural language descriptions by GPT-4 (OpenAI, 2023) or Github Copilot (Chen et al., 2021; GitHub, 2021), and repairing code (White et al., 2019), among others. These models have shown impressive results in generating correct code in many programming languages.\nAn important research question is whether modern AI models are capable of understanding the logic behind the programs they analyze. Recently, several approaches have been proposed to combine the strengths of formal verification and Large Language Models (LLMs) that demonstrate such capabilities. For example, Pei et al. (2023) made an important step in this direction by investigating whether LLMs can generate program properties, namely, program invariants, which remains a crucial and challenging task for automated program verification (Clarke et al., 2018). The authors demonstrated that LLMs are effective in generating program invariants on a set of synthetic Java programs. Another example is the recent work by Charalambous et al. (2023), who demonstrated that LLM models can be used to repair vulnerabilities in code, given examples of incorrect behavior. They provided compelling evidence of the complementary strengths of LLMs, which serve as a generator for code repair snippets, and formal techniques, which are used to check the correctness of the generated code. While previous work shows promise in program analysis tasks, they do not provide a formalization of the interaction between LLMs and formal verifiers; require manual efforts, or are limited to the invariant generation process as a stand-alone procedure.\nIn this work, we propose a novel LLM-powered framework, LEMUR, for automated program verification tasks. Our key insight is to combine LLMs\u2019 ability to perform abstract high-level reasoning and automated reasoners\u2019 ability to perform precise low-level reasoning. Specifically, LLMs are employed to propose program invariants in the form of sub-goals, which are then checked by automated reasoners. This transforms the program verification tasks into a series of deductive steps suggested by LLMs and subsequently validated by automated reasoners. Our main contributions are: \u2022 a novel framework to combine LLMs and automated reasoners for program verification; \u2022 a presentation of LEMUR as a proof system and a proof of its soundness, which to the best of our\nknowledge, is the first formalization of such a hybrid approach; \u2022 an instantiation of the LEMUR calculus that gives a sound and terminating algorithm; \u2022 an implementation of the proposed framework (using OpenAI\u2019s GPT models) and several opti-\nmizations to enhance its practical efficiency; \u2022 an experimental evaluation of LEMUR on two sets of benchmarks that demonstrates its efficiency\ncompared with both existing AI-powered and conventional verification tools. We highlight that LEMUR is the first fully automated framework combining LLMs and reasoners."
        },
        {
            "heading": "2 DEFINITIONS",
            "text": "Given a program P : Prog, a reachability property, or simply property, is a tuple p = \u27e8\u03d5, l\u27e9, where \u03d5 : Pred is a Boolean predicate of the program state and l : N is a program line. The negation of p, denoted \u00acp, is defined as \u27e8\u00ac\u03d5, l\u27e9. Next we introduce several useful definitions and their properties. Definition 2.1. A property p = \u27e8\u03d5, l\u27e9 is an invariant on P , denoted Inv(P, p), iff p holds (i.e., \u03d5 always evaluates to true at line l) for all possible executions of the program P . Example 2.1. Consider a simple program P on Figure 2 (the first frame, top row). P instantiates an unsigned 32-bit integer variable x to 0 and increases its value by 4 on each loop iteration. A property p = \u27e8\u03d5, l\u27e9 is specified on the 4th line, so \u03d5 = (x != 30) and l = 4 for this property. It is easy to see that p is an invariant as x is not divisible by 3, for example. \u25a0\nNext, we introduce a notion of assumption on a program P . An assumption q = \u27e8\u03d5, l\u27e9 is a property that is assumed in a program by altering a program behavior. Definition 2.2. An assumption q = \u27e8\u03d5, l\u27e9 is a property that modifies the program as follows\n1. if \u03d5 holds at line l then the program P continues execution without changes; 2. if \u03d5 does not hold at line l then P terminates at l.\nWe use P \u2032 = Asm(P, q) to denote a modification of P with the assumption q. An assumption can itself be an invariant. We now introduce a special notion of an implication. Definition 2.3. Let P be a program, and p, q be properties on P . We say that q implies p with respect to P , denoted q \u2212\u2192\nP p, iff p is an invariant in Asm(P, q).\nExample 2.2. Consider the program P on Figure 2 and an assumption q = \u27e8\u03d5 = (x%4==1), l = 3\u27e9 that modifies the original program P , giving P \u2032 = Asm(P, q). The first frame in the bottom row of Figure 2 depicts P \u2032. To see a difference between P and P \u2032, we observe that the loop is executed only once in P \u2032: x=0 when it enters the loop so (x%4)!=1, the \u03d5 is violated, and P \u2032 terminates. If we consider an alternative assumption q\u2032 = \u27e8\u03d5 = (x%2==0), l = 3\u27e9 The second frame at the top depicts P \u2032 for q\u2032. We can see that its predicate \u03d5 holds for all executions. Hence, q\u2032 is an invariant for the original program P . Finally, we can see q\u2032 \u2212\u2192\nP p, where p is from Example 2.1. \u25a0\nThe following propositions follow from the definitions above. Proposition 2.1. Let P be a program, and p, q be properties on P:\n\u2022 The property p is an invariant on P if q is an invariant on P and q implies p with respect to P . More formally, (Inv(P, q) \u2227 q \u2212\u2192\nP p) \u21d2 Inv(P, p).\n\u2022 The property p is not an invariant on P if the property p is not an invariant on P \u2032 = Asm(P, q). More formally, \u00acInv(P \u2032, p) \u21d2 \u00acInv(P, p) \u00acInv(P, q) \u21d2 \u00acInv(P \u2032, p).\nProposition 2.2. For any property p on a program P , p \u2212\u2192 P p.\nProposition 2.3. For any properties p, q, r on a program P , p \u2212\u2192 P q and q \u2212\u2192 P r, then p \u2212\u2192 P r.\nNote that it is possible that neither a property p nor its negation \u00acp holds on a program. Example 2.3. Consider again our example from Example 2.1 and two properties at line 3: p = \u27e8\u03d5 = (x%8==4), l = 3\u27e9 and p\u2032 = \u27e8\u03d5\u2032 = (x%8!=4), l = 3\u27e9. Neither p nor p\u2032 is an invariant in P . On the first loop iteration, we have that x=0 before line 3 so \u03d5\u2032 holds and \u03d5 does not at line 3. On the second loop iteration, we have that x=4 before line 3 so \u03d5 holds and \u03d5\u2032 does not. \u25a0\nDefinition 2.4. A property p = \u27e8\u03d5, l\u27e9 is stable for P , denoted S(P, p), if, for each execution of the program, either \u03d5 always evaluates to true at line l or \u03d5 always evaluates to false at line l.\nExample 2.4. Consider an example to illustrate the definition of stability.\nLine 1: uint32_t x=rand(); Line 2: assert(x==1);\nThe property in Line 2 is stable, as it always evaluates to true or false within a single execution.\u25a0\nAn invariant must be stable, but a property that is not an invariant might still be stable. For example, any property on a program without loops is stable. If a p is stable, then \u00acp is also stable. Lemma 2.1. Consider a program P , two properties p, q on P , and a program P \u2032 = Asm(P, q). The property p is an invariant on P , if 1) q is stable for P; 2) q implies p with respect to P; and 3) \u00acq implies p with respect to P . More formally: S(P, q) \u2227 (q \u2212\u2192\nP p) \u2227 (\u00acq \u2212\u2192 P p) \u21d2 Inv(P, p).\nProof. In App. B.\nAssume we have a verifier V : Prog\u00d7P(Prop) \u00d7 Prop 7\u2192 {TRUE, FALSE,UNKNOWN}, which takes as inputs a program P , a set of assumptions A and a property p, and checks whether A implies p. More precisely, given set of assumption A = {q1, . . . , qn} we construct a new program P \u2032 = Asm(Asm((. . . ,Asm(P, q1)), qn\u22121), qn) and the verifier checks if p is an invariant in this program. Hence, a statement that A implies p on P means that p is an invariant in P \u2032. V is sound, meaning if V returns TRUE, then A implies p, and if V returns FALSE, then A does not imply p. Note that A can be empty, in which case the verifier essentially checks whether p is an invariant in general. When the verifier V returns TRUE, we say p is proven; and when V returns FALSE, we say the property is falsified. V is incomplete, meaning that V can return UNKNOWN. In practice, V is instantiated as automated program verifiers such as CBMC (Kroening & Tautschnig, 2014), ESBMC (Gadelha et al., 2018), and UAUTOMIZER (Heizmann et al., 2013). We provide an overview of the main techniques that these tools employ in Section A and note here that a crucial challenge shared across existing verifiers is the automatic decomposition of a verification task into smaller, more manageable sub-tasks. This decomposition requires high-level reasoning that is difficult to automate using conventional formal methods, but plausible to be performed by LLMs, with their documented code-understanding capability. However, we must ensure soundness when LLMs are used to automatically perform this high-level reasoning in program verification tasks."
        },
        {
            "heading": "3 LEMUR: INTEGRATING LLMS IN PROGRAM VERIFICATION",
            "text": "We present a proof system LEMUR that combines LLMs and automated reasoners for proving a property on a program. The calculus operates over a configuration, which is either one of the distinguished symbols {SUCCESS, FAIL} or a tuple \u27e8P,A,M\u27e9, where P is a program, A is either \u2205 or a singleton representing the assumption, and M is a list of properties referred to as proof goals. M itself is referred to as a trail. The last element of M represents the current property to prove. We use the notation : : to denote a concatenation of two lists. In particular, M = M\u2032 : : pmeans that M is a concatenation of a trail M\u2032 and a property p, where p is the last element of M. The rules describe the conditions under which a certain configuration can transform into another configuration. In this calculus, verifying whether Inv(P, p) holds, boils down to finding a sequence of valid rule applications from the starting configuration \u27e8P,\u2205, [p]\u27e9 to either SUCCESS or FAIL. Our calculus performs oracle calls to LLMs to propose new properties and revise them. The oracle Opropose proposes new properties for a given program and the current proof goal as inputs. Namely, Opropose : Prog\u00d7Prop 7\u2192 P(Prop). An important insight here is that LLMs are capable of generating new properties that are likely to 1) be invariants, and 2) imply the proof goal given a prompt. We will discuss strategies to generate prompts in Section 4. Importantly, properties generated by an LLM are treated as assumptions until we can prove that they are invariants of the original program. The oracle Orepair revises previously proposed properties. e.g. if we determine that a property q previously produced by Opropose does not hold or does not imply the current proof goal. In this case, we request an LLM to repair q. We have Orepair : Prog\u00d7Prop\u00d7Prop\u00d7{FALSE,UNKNOWN} 7\u2192 P(Prop), whose inputs comprise a program, two properties, and a solver return value. The first property is our current proof goal, and the second property q is usually an assumption previously proposed by oracles. The output of Orepair is a new set of properties. In practice, we implement it with a prompt to an LLM to either correct or strengthen q (see Section 4). Finally, the calculus performs an external call to a verifier V to check whether a property holds. The proof rules of LEMUR are shown in Fig. 1. Each rule defines a set of preconditions that specify the configurations where it can be applied. Note again that our preconditions permit invocations of LLMs and/or verifiers. The rules within the calculus can be partitioned into four groups.\nThe first group contains rules that are responsible for generating new proof goals given specific configurations. These rules are Propose, Repair 1, and Repair 2. The Propose rule states that if the verifier is unable to prove or disprove the current proof goal p, we could invoke the oracle Opropose to obtain a property q, and update A to be {q}. It is also possible to modify the proposed property produced by Opropose. The Repair 1 rule can be applied when the current assumption q is not sufficient for the verifier to prove the current proof goal p. In this case, we could use the oracle Orepair to propose ways to strengthen q and choose one of them, q\u2019, as the new assumption. On the other hand, the Repair 2 rule can be applied when q is already in the trail but is falsified by the verifier V . In this case, we could use Orepair to repair q and update A accordingly. The second group specifies how LEMUR makes progress in the proof. The Decide rule specifies that the condition under which the assumption q can be made the new proof goal (i.e., being appended to M)\u2014when the verifier V is able to prove that q implies the current proof goal. The third group defines how LEMUR can recover from faulty assumptions. In particular, the Backtrack rule allows us to revert to the previous proof goal (the second to the last property in the trail M) and pick a different assumption suggested by Opropose, if there are at least two elements in the trail and the verifier cannot prove the current proof goal. Note that Backtrack might not be the only applicable rule in this case. For example, Repair 1 is also applicable. In practice, we need a strategy to decide between multiple applicable rules. This discussion is deferred to Sec. 4.\nThe final group specifies three termination conditions that can be either SUCCESS or FAIL. The Success 1 rule states that whenever the assumption is empty and the verifier is able to prove the current proof goal (i.e., the last property p in the trail M), we can transition into the SUCCESS state. If the verifier can directly prove the original property, then the rule can be directly applied to the starting configuration to reach SUCCESS. Otherwise, p would come from the oracles and is different from the original property. Success 2 states that if the last two elements of the trail M are p, and q, the current proof goal q := \u27e8\u03d5, l\u27e9 is stable (as defined in Sec. 2), and the verifier is also able to also prove p under the assumption of \u27e8\u00ac\u03d5, l\u27e9, then p is an invariant and we can transition to SUCCESS. The Success 2 rule constitutes a way to utilize an incorrect sub-goal q proposed by the LLM-based oracles to decompose the verification task: we separately reason about the cases when q holds and when it does not hold. Finally, if the verifier V proves that the original property is not an invariant, whether under an assumption or not, then we transition to the FAIL state using Fail.\nNote that the program P remains unchanged throughout the transitions. We keep it as part of the state for two reasons. First, P is an input to the verifiers and the oracles. Second, in the future, it\nmight be possible to augment the proof system to update P , by, for example, rewriting the program using LLMs in an invariant-preserving manner.\nWe state the following soundness properties about LEMUR. The proof is presented in App. C.1. Theorem 3.1 (Soundness). Given a property p and a program P , if SUCCESS is reached by a sequence of valid rule applications starting from \u27e8P,\u2205, [p0]\u27e9, then p0 is an invariant on P . Theorem 3.2 (Soundness 2). Given a property p and a program P , if FAIL is reached by a sequence of valid rule applications starting from \u27e8P,\u2205, [p0]\u27e9, then p0 is not an invariant on P .\nExample 3.1. To provide more intuition about the proof system and to motivate the design choices when instantiating LEMUR, we consider again our running example. Figure 2 illustrates how LEMUR can be used to verify properties in practice. In Figure 2 each frame represents a state of the program. Transitions between states are depicted by arrows, with each arrow marked with the rule applied to execute this transition. In this example, our goal is to prove the property x!=30 in a while loop that keeps adding 4 to an unsigned 32-bit integer variable x. We note that this particular verification task is adapted from a similar one in the SV-COMP competition.1 While seemingly trivial, during the competition, 19 out of the 24 participating tools (including the overall winner of the competition UAUTOMIZER) were not able to solve this benchmark.\nGiven such a verification problem, we create an initial configuration \u27e8P,\u2205, [p])\u27e9 where P is the given problem and p = \u27e8x!=30, 3\u27e9.2 Suppose the verifier V is unable to solve this problem and returns UNKNOWN. In this case, we need to generate a new proof goal, so the only rule we could apply is Propose. To do so, we invoke the LLM-based oracle Opropose to obtain a set of new properties that are potentially themselves invariants and might help prove the property. An example prompt is given on the left bottom part. This is not the exact prompt that we found the most effective in practice and we defer the discussion of prompting strategies to Sec. 4. Suppose the oracle returns two potential predicates at the beginning of the while loop: x%2==0 and x%4==1 at line 3. The Propose allows us to make one of them the current assumption.\nCase (x%2==0): The top row illustrates what happens when we transition into \u27e8P, {q = \u27e8x%2==0, 2\u27e9}, [p]\u27e9. While q is indeed an invariant, it does not help to prove the assertion and V would return UNKNOWN. This satisfies the condition to apply the Repair 1 rule, which would invoke the oracle Orepair to strengthen q. Suppose in this case, the oracle suggests the predicate q\u2032 = x%4==0, which clearly implies the original property x!=30. Suppose then V(P, {q\u2032}, p) returns TRUE. We could apply the Decide rule and transition to \u27e8P,\u2205, [p, q\u2032]\u27e9, making q\u2019 the current proof goal. Proving q\u2019 is arguably easier because x%4==0 is inductive (i.e., if it holds in one iteration and then it will hold in the next iteration), making conventional automated reasoning techniques such as k-induction applicable. Suppose V(P,\u2205, q\u2032) = TRUE, we could apply Success 1 and transition into the SUCCESS state, thus completing the proof.\nWe discuss the case (x%4==1) in the full version of the example (Example C.1 in Appendix C.1). \u25a0 1https://sv-comp.sosy-lab.org/2023/results/results-verified/META_ ReachSafety.table.html#/table?filter=id_any(value(jain_5-2)) 23 is the line number (in the snippet) where the predicate is asserted.\nAlgorithm 1 The LEMUR procedure 1: Input: A program P , a property p. 2: Output: SUCCESS only if Inv(P, p); FAIL only if \u00acInv(P, p); and UNKNOWN if inconclusive. 3: Parameters: Verifier V , oracles Opropose and Orepair (which satisfy Condition 1), number of proposals k 4: function lemur check(P, p) 5: d 7\u2192 V(P,\u2205, p) 6: if d = FALSE then return FAIL \u25b7 Fail 7: else if d = TRUE then return SUCCESS \u25b7 Success 1 8: else 9: i, Q 7\u2192 0,Opropose(P, p) 10: while i < k \u2227 |Q| > 0 do 11: i 7\u2192 i+ 1 12: q 7\u2192 pop(Q) 13: e 7\u2192 V(P, {q}, p) \u25b7 Propose/Backtrack 14: if e = FALSE then return FAIL \u25b7 Fail 15: else if e = TRUE then 16: f 7\u2192 lemur check(P, q) \u25b7 Decide 17: if f = SUCCESS then return SUCCESS \u25b7 Success 1 18: else if S(P, q) \u2227 (V(P, {\u00acq}, p) = TRUE) then return SUCCESS \u25b7 Success 2 19: else if f = FAIL then Q 7\u2192 join(Q,Orepair(P, p, q, FALSE)) \u25b7 Repair 2 20: else continue 21: else Q 7\u2192 join(Q,Orepair(P, p, q, UNKNOWN)) \u25b7 Repair 1 22: return UNKNOWN"
        },
        {
            "heading": "4 INSTANTIATING THE LEMUR CALCULUS",
            "text": "In this section, we present strategies to instantiate LEMUR as an automated decision procedure. While we showed that LEMUR calculus is a sound procedure, there are no guarantees that it terminates. Here, we will discuss two sources of non-termination in this calculus.\nThe first one corresponds to unbounded suggestions of new sub-goals to prove the current proof goal. Concretely, when trying to prove a particular proof goal p, we could get stuck if V(P, {q}, p) = UNKNOWN or V(P,\u2205, q) = FALSE for each proposed assumption q. This could be due to limitations in either the LLM or the verifier. One way to avoid this type of non-termination is by putting an upper bound on the number of proposed assumptions to prove each proof goal. That is, for any proof goal p, we require that V(P, {q}, p) is invoked for at most k different q\u2019s. The second source of non-termination corresponds to an unbounded depth of the trail M. Concretely, it is possible to construct an infinite sequence of Propose and Decide where 1) the verifier returns UNKNOWN on the current proof goal; 2) the oracle proposes an assumption that is not invariant but implies the current proof goal; 3) the verifier proves the implication; 4) the assumption becomes the new proof goal; and 5) repeat. This case can be avoided by adding a side condition to the rules that the property proposed by the oracles (q = \u27e8\u03c8, l\u2032\u27e9) must be at a smaller program line than the current proof goal (p = \u27e8\u03d5, l\u27e9), that is,\n\u27e8\u03c8, l\u2032\u27e9 \u2208 O\u2217(P, \u27e8\u03d5, l\u27e9, . . .) \u21d2 l\u2032 < l (Condition 1)\nBased on the strategy described above, a terminating (by Thm. 4.1 at the end of this section) and sound (by Thm. 3.1) algorithm for checking whether a property p is an invariant on a program P is presented in Alg. 1. Alg. 1 is a recursive procedure lemur check. It takes a program P and a property p as inputs. If lemur check returns SUCCESS, then the property is an invariant. If lemur check returns FAIL, then the property is not an invariant. The function can also return UNKNOWN, if the analysis is inconclusive. At the high level, Alg. 1 searches for a potential subgoal q that implies the current goal p (lines 9\u201321). If such q is identified in line 13, we recurse to prove q (line 16). The while loop starting at line 10 ensures that at most k attempts can be utilized to generate a new subgoal for p. See a full description of Alg 1 in Appendix D. The comments in Alg. 1 show which rule is applied at the lines. The algorithm is sound as it only applies the rules of the calculus. We prove that the algorithm terminates in Appendix D.\nTheorem 4.1 (Termination). Given a program P , and a property p on the program, Alg. 1 terminates with either SUCCESS, FAIL, or UNKNOWN."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We have presented the LEMUR calculus and described a sound and terminating algorithm based on LEMUR. In this section, we investigate the following question:\n\u2022 Can we develop a practical automated verification procedure based on Alg 1? [Yes] \u2022 Is LEMUR competitive with existing end-to-end learning-based verification approaches? [Yes] \u2022 Can LEMUR already prove hard benchmarks that are beyond the reach of state-of-the-art conven-\ntional program verifiers? [In several cases]"
        },
        {
            "heading": "5.1 BUILDING AN LLM-BASED PROGRAM VERIFIER",
            "text": "We report the practical considerations when building a prototype of Alg. 1. There are two types of external calls that Alg. 1 depends on. The first type is calls to V . We use off-the-shelf verifiers in our framework that are extensively tested by the community (described in later paragraphs), so we have some expectations about their performance. However, the main source of uncertainty in building LEMUR comes from interaction with the second type of calls, calls to LLM oracles, as we treat them as black boxes. In our framework, the oracles Opropose and Orepair automatically prompt a GPTfamily model through the OpenAI API and parse its outputs. We use GPT-family by default. We found that while GPT has great potential in generating sensible loop invariants, it still has practical limitations. We report several tactics that we found useful in practice.\n\u2022 Formatting the output: We initially investigated whether the popular chain-of-thought (CoT) reasoning (Wei et al., 2022) can be useful to discover new properties given P and p. We found that GPT\u2019s outputs were verbose and often contained irrelevant/incorrect statements, even in cases where useful invariants were contained in the outputs. This behavior increases the cost and makes it difficult to extract invariants from the output. To address these issues, we used incontext learning to format the output of GPT. For example, adding Your output should be \"assert(...);// Line number\" to the prompt is sufficient for GPT to consistently generate outputs of exactly this format, without providing verbose explanations.\n\u2022 Inserting markers to the program: We found that the current versions of GPT are not good at counting program lines. In many cases, the predicate generated by GPT is \u201ccorrect\u201d but the line number is off by a small margin. This is highly undesirable as an invariant at a wrong position is of no use to the verifier. To mitigate this challenge, we inserted placeholder lines of the form \"// Line A\", \"// Line B\" to the program and prompted GPT to generate invariants of the form assert(...);// Line name (for those specific locations). As a simple practical heuristics, we insert placeholders to right before the loop and the beginning of the loop.\n\u2022 Ordering the proposal: The output of an oracle call is non-deterministic for a given prompt, depending on the hyper-parameters of the call. Moreover, the oracles produce a set of properties and we need good heuristics to choose the order of trying those properties. A heuristic we found practically useful is to prompt GPT multiple times and order the properties by the frequency they are proposed (breaking ties by preferring shorter expressions). Moreover, instead of relying on string matching, we treat two proposals the same if their abstract syntax trees are equivalent.\nThe exact prompts are described in Appendix F. We consider two state-of-the-art C program formal analyzers for V , ESBMC (Gadelha et al., 2018) and UAUTOMIZER (Heizmann et al., 2013). The former is based on K-induction and the latter is based on predicate-abstraction. In particular, ESBMC and UAUTOMIZER are the top two performing non-portfolio solvers in the reachability track of the SV-COMP (Beyer, 2023). And UAUTOMIZER is the overall winner of the competition. By default, we impose a 30-second time limit for each invocation of the verifier. That is, if the verifier does not terminate within 30 seconds, then the verifier returns UNKNOWN. The total cost incurred by using the OpenAI API services for the experiments (including testing phases) is $1000. We will release the source code and the benchmarks for the community to make further improvements."
        },
        {
            "heading": "5.2 LOOP INVARIANT GENERATION BENCHMARKS",
            "text": "A prominent approach in learning-based end-to-end program verification is Code2Inv, which uses reinforcement learning to train an invariant synthesizer to propose loop invariants. In this section,\nwe study how LEMUR compares with this approach. The Code2Inv (Si et al., 2020) benchmark set contains 133 benchmarks, each containing a C program and a property expressed as an assert statement in the program. Each program contains a single loop and each loop can have nested ifthen-else blocks (without nested loops). Programs in the benchmark may also have uninterpreted functions (emulating external function calls) in branches or loop termination conditions. The assertion to check is always after the loop. As reported in the original Code2Inv paper, these problems can be efficiently solved using state-of-the-art invariant synthesis solvers and the goal was to evaluate the ability of Code2Inv to generate a real invariant that implies the property at the beginning of the loop. To have a fair comparison, we prompt the oracles to generate invariants in the same location as Code2Inv in Alg. 1.\nWe use the k-induction-based verifier, ESBMC, to check the implication (line 13 in Alg. 1) which aligns with the verification procedure used in Code2Inv. We report the number of solved instances as well as the number of failed suggestions (either itself cannot be verified or ESBMC times out on the implication check). As a point of comparison, we report those statistics from the original Code2Inv approach, which combines graph and recurrent neural networks to model the program graph and learn from counterexamples. Code2Inv was given a one-hour timeout. In addition, we also report ESBMC\u2019s performance on this set of benchmarks. The result is shown in Table 1a (the Time column shows the average time in seconds to solve a benchmark).\nWith a 10-minute timeout, ESBMC alone can solve 68 problems. On the other hand, LEMUR(GPT4) can solve 107 problems within the same time limit. Surprisingly, this approach solves more instances than Code2Inv, which is specifically designed for invariant synthesis tasks. Moreover, LEMUR(GPT4) is able to find the correct loop invariant with on average 4.7 attempts, while it takes Code2Inv on average > 20 attempts to do so. For problems unsolved by ESBMC but solved by LEMUR(GPT4), a histogram of the values of Log2 of the number of proposals is shown in Fig. 3. While in most cases, Alg. 1 can propose the correct proposals within 4 attempts, there are still benchmarks that take LEMUR(GPT4) many rounds of proposal and repair to find the desired loop invariant, e.g. one of the benchmarks took 177 proposals. In addition, we experimented with the GPT-3.5 turbo LLM model, denoted LEMUR(GPT3), as shown\nin Table 1a. Note that LEMUR(GPT3) solved four fewer benchmarks and required more time and additional calls to a GPT-3.5 turbo oracle."
        },
        {
            "heading": "5.3 SOLVING HARD SV-COMP BENCHMARKS",
            "text": "Next, we study LEMUR\u2019s ability to solve hard benchmarks from the Software-Verification Competition 2023 (Beyer, 2023). Due to budget limitations, we focus on benchmarks with less than 150 tokens (after removing comments, unnecessary headers, and clang-formatting). We select 50 benchmarks that ESBMC and UAUTOMIZER are unable to solve within 20 minutes and evaluate LEMUR on them with the same timeout. The property is expected to hold in all benchmarks. These benchmarks are considerably more challenging than the Code2Inv programs. While the latter has one loop and follows a strict format, the SV-COMP benchmark presents a more diverse set of benchmarks, with multiple loops present in many programs.\nThe results are shown in Table 1b. Impressively, with the guidance of the proof goals suggested by the LLM, LEMUR(GPT4) is able to solve 26 of the 50 SV-COMP benchmarks. While neither ESBMC nor UAUTOMIZER can solve a single benchmark alone. Upon closer examination, 6 of the solved instances contain 2 loops, 3 contain 3 loops, and 3 contain 4 or more loops. To our knowledge, this is the first time a learning-based verification approach 1) can handle programs with more than one loop; and 2) boosts the performance of state-of-the-art conventional C program verifiers.\nThe average number of proposals before solving a problem is higher compared to the Code2Inv benchmarks (9.1 vs. 4.7). Fig. 4 sheds more light on the behavior of LEMUR(GPT4) (each bar on the figure corresponds to a set of values). In particular, 16 of the 26 solved instances require more than 6 proposals in total.\nWe found that the LLM oracles can produce surprisingly insightful loop invariants that are difficult for conventional formal methods to synthesize. While predicate-abstraction-based techniques typically generate predicates that involve only the operators and values in the program and follow a particular template, LLM is not constrained by these limitations. For example, for the program in Fig. 2, GPT-4 can consistently generate x%4==0 as the loop invariant although the modulo operator is not present in the program. Appendix. F.1\nshows an example where LLM understands the range of unsigned char as the key to proving the given property and suggests variable bounds as the assumption. There are also several cases where the LLM generates disjunctive invariants that precisely characterize the behavior of the loops.\nFinally, we compared the performance of LEMUR(GPT4) and LEMUR(GPT3) on SV-COMP benchmarks. We can observe that LEMUR(GPT4) significantly outperforms LEMUR(GPT3) across all metrics. For example, LEMUR(GPT4) solved 26 problems, while LEMUR(GPT3) only solved 15."
        },
        {
            "heading": "6 DISCUSSION OF LIMITATIONS AND EXTENSIONS",
            "text": "In this work, we proposed a novel framework, LEMUR, that combines automated reasoning and LLMs. To the best of our knowledge, LEMUR is the first framework that provides a theoretical foundation for such integration, i.e., a formal calculus, and practical algorithmic instantiation of the calculus. LEMUR primarily focuses on imperative languages, and extending it to functional languages is the subject of future research. We also implemented LEMUR as a fully automated framework and demonstrated its efficiency on standard benchmark sets. We conclude by discussing the current limitations of LEMUR, which point to future research directions to extend the framework.\nAs we mentioned above, the practical performance of LEMUR depends on two types of external calls: the verifiers and the LLMs. Any improvements in these tools should translate into LEMUR improvements. Currently, modern verifiers are capable of handling relatively small programs (see SV-COMP\u201923 (Beyer, 2023)). Interestingly, even when provided with a strong invariant, they sometimes cannot solve the verification problem. One research direction that we envision is to customize LEMUR to a particular backend verifier to obtain better performance and solve larger programs.\nWhile our experience with LLMs was largely positive (see Section 5.1 for a discussion on the limitations we have successfully overcome), there are more interesting challenges to tackle. First, LLMs can take a limited number of tokens as inputs, and many practical programs exceed that limit. Second, it is sometimes challenging for LLMs to generate more complex logical formula such as nested if-then-else properties. We believe that to overcome this limitation we need to 1) develop a prompting language for invariant generation with LLMs, and 2) fine-tune LLMs for invariant generation tasks using this language. Third, reasoning about programs with multiple loops remain challenging for the LLMs. We believe fine-tuning will likely help to address this challenge. Fourth, we observed that the performance of LEMUR may vary depending on the LLM oracle. For example, our experiments demonstrate that GPT-4 is superior to GPT-3.5 turbo on tested benchmarks. Finally, due to the limitations of LLMs and automated reasoners, our hybrid framework is not yet able to offer a significant leap in automatically verifying complex properties on real-world C libraries. However, a modular approach, where large parts of the program are abstracted and summarized in the form of pre- and post-conditions, can benefit from frameworks like LEMUR."
        },
        {
            "heading": "A BACKGROUND",
            "text": "Automated reasoning tools. We overview several popular techniques that are used by modern program verification solvers, like CBMC (Kroening & Tautschnig, 2014), ESBMC (Gadelha et al., 2018), and UAUTOMIZER (Heizmann et al., 2013).\nThe Bounded Model Checking (BMC) approach is an iterative technique that verifies the program for each unwind bound up to a maximum value, m, e.g. it can perform m loop unwinding. It either finds a counterexample within m steps or returns UNKNOWN. This approach usually employs SMT solvers to find counterexamples very efficiently at each step. However, for non-trivial systems unrolling can be expensive and memory-consuming. Moreover, vanilla BMC can only check finite reachability, e.g. it cannot prove loop invariants, for example.\nAnother popular technique is the k-induction method, which allows BMC-style methods to prove properties like loop invariants. This approach is also iterative. First, we check whether a property holds for k steps from a valid state. If it does not, we find a counterexample. Otherwise, we check an inductive hypothesis that if the property holds for k steps from a state then the property holds for the k + 1th step. If it does, the property holds for any number of steps. If not, k-induction either increases k or returns UNKNOWN. As in the case of BMC, unrolling can be computationally expensive. Moreover, k-induction is not complete; there are properties that are not k-inductive for any k.\nThe last approach we consider is abstract interpretation verification methods. Such methods create abstract representations of program states and variables. These abstract representations are simplified versions of the actual program states, focusing on relevant aspects while ignoring irrelevant details. The choice of abstraction depends on the specific properties to verify. Moreover, if a property holds for an abstract program, then it holds for the original program. The reverse is not true. Hence, if a property does not hold for an abstract program, we need to refine the abstract representation to exclude the counterexample. The main challenge here is how to come up with a good abstraction and how to design a refinement procedure.\nLarge Language Models. Large Language Models belong to a class of artificial intelligence models used in natural language processing tasks. These models are designed to process and generate both human language and structured inputs, such as code in various programming languages. Examples of large language models include Generative Pre-trained Transformer models like GPT3 (Brown et al., 2020) or GPT-4 (OpenAI, 2023), Bidirectional Encoder Representations from Transformers, BERT (Devlin et al., 2019), and others. LLMs are getting increasingly popular as an AIassistance for code generation tasks, like PaLM (Chowdhery et al., 2022), GitHub Copilot (Chen et al., 2021; GitHub, 2021), etc.\nLLMs are usually trained in two steps. The main phase is training, where these models are exposed to very large corpora of data, usually collected over the internet. The architecture of LLMs is based on transformers and has a very large number of parameters. Therefore, it can capture relations between different parts of the input text to produce coherent outputs. For some applications, we need to perform fine-tuning to expose the model to application-specific inputs. During inference, when a user provides inputs and a prompt that contains instructions to an LLM, it generates the output with respect to these instructions."
        },
        {
            "heading": "B DEFINITIONS",
            "text": "Lemma 2.1. Consider a program P , two properties p, q on P , and a program P \u2032 = Asm(P, q). The property p is an invariant on P , if 1) q is stable for P; 2) q implies p with respect to P; and 3) \u00acq implies p with respect to P . More formally: S(P, q) \u2227 (q \u2212\u2192\nP p) \u2227 (\u00acq \u2212\u2192 P p) \u21d2 Inv(P, p).\nProof. Let q = \u27e8\u03d5, l\u27e9. By the definition of stability, for any execution of P , either \u03d5 always evaluates to true at line l or \u00ac\u03d5 always evaluates to true at line l. In either case, the property p holds by the definition of implication. Therefore, p holds for all executions of P , i.e., Inv(P, p)."
        },
        {
            "heading": "C LEMUR: INTEGRATING LLMS IN PROGRAM VERIFICATION",
            "text": "C.1 SOUNDNESS OF LEMUR\nLemma C.1. For any configuration \u27e8P,A,M\u27e9 created by a sequence of valid rule applications starting from an initial configuration \u27e8P,\u2205, [p0]\u27e9, M is not empty.\nProof. This can be proven by induction on the length of the sequence. In the base case, M is [p0]. In the inductive case, the length of M does not reduce except in the Backtrack rule which requires M to have at least 2 elements in the pre-condition. Thus, M is not empty.\nLemma C.2. Let \u27e8P,A,M\u27e9 be a configuration created by a sequence of valid rule applications starting from an initial configuration \u27e8P,\u2205, [p0]\u27e9, and let p be the last element of M. We have p \u2212\u2192\nP p0.\nProof. We prove a stronger property, that for each element p in M, p \u2212\u2192 P p0. We induct on the length of the sequence. In the base case, p0 \u2212\u2192 P p0 by proposition 2.2. In the inductive case, we proceed by cases. Success 1, Success 2, Fail cannot be applied. In the post conditions of Propose, Backtrack, Repair 1, and Repair 2, M either shrinks or remains the same. Therefore, the inductive hypothesis can be directly applied. If Decide rule is to be applied. In the pre-condition, the trail is M : : p, the current assumption is {q} and q \u2212\u2192\nP p. In the post condition, M becomes M : : p : : q.\nBy the inductive hypothesis, p \u2212\u2192 P p0. Furthermore, by Proposition 2.3, q \u2212\u2192 P p0.\nLemma C.3. Let \u27e8P,A,M : : p : : p\u2032\u27e9 be a configuration created by a sequence of valid rule applications starting from an initial configuration \u27e8P,\u2205, [p0]\u27e9, we have p\u2032 \u2212\u2192\nP p.\nProof. This can be proven by induction on the length of the sequence.\nTheorem 3.1. Given a property p and a program P , if SUCCESS is reached by a sequence of valid rule applications starting from \u27e8P,\u2205, [p0]\u27e9, then p0 is an invariant on P .\nProof. We can transition into SUCCESS by either the Success 1 rule or the Success 2 rule. In the pre-condition of Success 1, the trail is of the form M : : p, and the verifier V proves that Inv (P , p). By Lemma C.2, p \u2212\u2192\nP p0. Further by Proposition 2.1, we have Inv(P, p0). On the other hand,\nin the pre-condition of Success 2, the trail is of the form M : : p : : p\u2032. By Lemma C.3, p\u2032 \u2212\u2192 P p. In addition, p\u2032 is stable and \u00acp\u2032 \u2212\u2192 P\np. Therefore, by Lemma 2.1, p is an invariant of P . Since we also know from Lemma C.2 that p \u2212\u2192\nP p0, it follows from Proposition 2.1 that p0 is an invariant of\nP .\nTheorem 3.2. Given a property p and a program P , if FAIL is reached by a sequence of valid rule applications starting from \u27e8P,\u2205, [p0]\u27e9, then p0 is not an invariant on P .\nProof. We transition into the FAIL state only when the verifier V(P,A, p0) = FALSE. Even if A is not empty, p0 is still not an invariant by Prop. 2.1.\nExample C.1. To provide more intuition about the proof system and to motivate the design choices when instantiating LEMUR, we consider again our running example. Figure 2 illustrates how LEMUR can be used to verify properties in practice. In Figure 2 each frame represents a state of the program. Transitions between states are depicted by arrows, with each arrow marked with the rule applied to execute this transition. In this example, our goal is to prove the property x!=30 in a while loop that keeps adding 4 to an unsigned 32-bit integer variable x. We note that this particular verification task is adapted from a similar one in the SV-COMP competition.3 While seemingly\n3https://sv-comp.sosy-lab.org/2023/results/results-verified/META_ ReachSafety.table.html#/table?filter=id_any(value(jain_5-2))\ntrivial, during the competition, 19 out of the 24 participating tools (including the overall winner of the competition UAUTOMIZER) were not able to solve this benchmark.\nGiven such a verification problem, we create an initial configuration \u27e8P,\u2205, [p])\u27e9 where P is the given problem and p = \u27e8x!=30, 3\u27e9.4 Suppose the verifier V is unable to solve this problem and returns UNKNOWN. In this case, we need to generate a new proof goal, so the only rule we could apply is Propose. To do so, we invoke the LLM-based oracle Opropose to obtain a set of new properties that are potentially themselves invariants and might help prove the property. An example prompt is given on the left bottom part. This is not the exact prompt that we found the most effective in practice and we defer the discussion of prompting strategies to Sec. 4. Suppose the oracle returns two potential predicates at the beginning of the while loop: x%2==0 and x%4==1 at line 3. The Propose allows us to make one of them the current assumption.\nCase (x%2==0): The top row illustrates what happens when we transition into \u27e8P, {q = \u27e8x%2==0, 2\u27e9}, [p]\u27e9. While q is indeed an invariant, it does not help to prove the assertion and V would return UNKNOWN. This satisfies the condition to apply the Repair 1 rule, which would invoke the oracle Orepair to strengthen q. Suppose in this case, the oracle suggests the predicate q\u2032 = x%4==0, which clearly implies the original property x!=30. Suppose then V(P, {q\u2032}, p) returns TRUE. We could apply the Decide rule and transition to \u27e8P,\u2205, [p, q\u2032]\u27e9, making q\u2019 the current proof goal. Proving q\u2019 is arguably easier because x%4==0 is inductive (i.e., if it holds in one iteration and then it will hold in the next iteration), making conventional automated reasoning techniques such as k-induction applicable. Suppose V(P,\u2205, q\u2032) = TRUE, we could apply Success 1 and transition into the SUCCESS state, thus completing the proof.\nCase (x%4==1): The bottom row illustrates a different chain of rule applications when we picked the property r = \u27e8x%4==1, 2\u27e9 from the first proposal. While r does not hold, it does imply x!=30. Suppose this implication is proven by the verifier. We could apply Decide and transition to \u27e8P,\u2205, [p, r]\u27e9. Since r is not an invariant, V(P,\u2205, r) would be either UNKNOWN or FALSE. Either way, we could apply Backtrack and try a new assumption proposed by Opropose. In practice, we could either invoke the stochastic Opropose again or pick an un-attempted property (e.g., \u27e8x%2==0, 2\u27e9 proposed previously). In the illustration, we invoke Opropose again and obtain the \u201ccorrect\u201d predicate x%4==0, which would allow us to prove the property in two more steps. \u25a0\nD INSTANTIATING THE LEMUR CALCULUS\nHere, we describe Alg. 1. First, the algorithm checks whether the current p can be verified by V or if a counterexample exists (line 5). If so, it returns either SUCCESS or FAIL to the upper level of recursion or terminates if lemur check is at the top level. If V cannot prove p, i.e. it returns UNKNOWN, lemur check enters a new phase of subgoal generation, where LLM oracles are used to propose new or repair existing properties (lines 9\u201321). In this phase, we start by calling Opropose to generate a new subgoal (line 9). The while loop at line 10 ensures that at most k attempts can be unitized to generate a new subgoal for p. In line 13, we call V to check whether q implies p. If V returns FALSE, we know that p is not an invariant and return FAIL (line 14). If V returns UNKNOWN, then we need to repair q; for example, we might strengthen q and try again to prove implication. Otherwise, if q does imply p, we recurse to prove q (line 16). The last logical block of lemur check in lines 17\u201320 addresses the output of the recursive call. If we have successfully proved that q is an invariant, we return SUCCESS. Otherwise, if q is stable (see Definition 2.4), we can check whether \u00acq implies p (line 18). If so, by Lemma 2.1, we can conclude that p is an invariant and also return SUCCESS. If we prove that q is FALSE, we can repair q by informing an LLM oracle that the property does not hold (line 19). Finally, if f is UNKNOWN then we continue to the next iteration of the while loop and consider the next proposed sub-goal.\nSecond, we present a proof of Theorem 4.1. Theorem 4.1. Given a program P , and a property p on the program, Alg. 1 terminates with either SUCCESS, FAIL, or UNKNOWN.\nProof. Suppose p = \u27e8\u03d5, l\u27e9. We prove with a decreasing argument on l. When l = 0, the algorithm terminates without entering the while loop, because Opropose satisfies Condition 1 and\n43 is the line number (in the snippet) where the predicate is asserted.\nOpropose(P, p) = \u2205. In the recursive case, the while loop is executed for at most k iterations. In each iteration, we show that for the second input to lemur check (Line 16), q = \u27e8\u03c8, l\u2032\u27e9, we have l\u2032 < l. This is true because q is generated either by Opropose(P, p) or Orepair(P, p, . . .), both satisfying Condition 1."
        },
        {
            "heading": "E RELATED WORK",
            "text": "There has been a lot of interest in using LLMs to augment formal reasoning. Charalambous et al. (2023) proposed a novel framework, ESBMC-AI, that integrated LLMs reasoning and formal verification. They also applied their framework to the analysis of C programs focusing on memory safety properties. The main idea is to use LLMs as a code repair generator that can fix faulty code using a carefully designed prompt, a program, and a counterexample provided by a bounded model checker. However, ESBMC-AI assumes that program rewriting done by an LLM is valid, i.e. syntactically and semantically correct. The latter is challenging to prove in an automatic manner as it requires program equivalence checking. Our framework does not use LLMs to modify code and treat the outputs of the LLM as suggestions until we prove that they are correct. Another example of an automated framework is Baldur (First et al., 2023), which uses an LLM, Minerva (Lewkowycz et al., 2022), to generate theorem proofs that are checked by Isabelle theorem prover. They also proposed a proof repair procedure. In contrast, our interactive decision procedure relies on an automated reasoner to generate proofs and only uses LLMs generated program properties.\nThere are a number of learning approaches to automatically generate invariants (Si et al., 2020; Garg et al., 2016; Sharma et al., 2013). Sharma et al. (2013) proposed a data-driven iterative approach to derive algebraic equation invariants from data generated through concrete executions of the program. Garg et al. (2016) proposed a decision tree-based approach to learn invariants from examples; however, the space of possible invariants is limited to a logical combination of binary linear constraints. The most related work to our framework is Code2Inv (Si et al., 2020), which proposed learning program invariants using machine learning techniques and employed automatic reasoning to verify the programs. The main principle of partitioning responsibilities between automated reasoners and LLMs is similar to our framework. However, we provide a formalization for such interactive procedures with formal calculus and a strategy to use it in practice. Our procedure is more general as it allows the generation of sequences of logically related properties, and we demonstrate that it is more efficient in practice. Finally, recent work by Pei et al. (2023) investigates the potential of invariant generation for Java programming language. While this framework does not incorporate automated reasoning components, it shows the potential of LLMs to uncover program properties. Table 2 presents a summary of learning-based approaches."
        },
        {
            "heading": "F PROMPTING THE GPT",
            "text": "In this section, we describe how we automatically constructed the prompts in Opropose and Orepair, and show examples of the prompts and the GPT outputs. We provided in the supplementary materials the execution traces of LEMUR on solved benchmarks used in our experiments.\nF.1 PROPOSING NEW PROPERTIES\nGiven a program P and a property represented as a C assert statement in P , we inserted the placeholder lines \u201c// Line A\u201d, \u201c// Line B\u201d... to dedicated program lines as described in 5.1. Our prompt has the following structure:\n---------\n[P1] Print [P2] as valid C assertions at line[P3] [P4] that help prove the assertion. Use \u2019&&\u2019 or \u2019||\u2019 if necessary. Don\u2019t explain. Your answer should be \u2019assert(...); // line [P5]\u2019 ---------\nP1 is the program with the placeholder lines and the C assertion; P2 is either facts or loop invariants depending on whether all the placeholder lines are in loops; P3 is either empty or s depending on the number of placeholder lines; P4 are the placeholder line names (e.g., A, B); and P5 is either A, if there is only one placeholder line, or name, if there are multiple.\nBelow is an example extracted from the execution of our prototype on an SV-COMP benchmark:\nint main() { unsigned char n = (unsigned char) rand(); if (n == 0) { return 0; } unsigned char v = 0; unsigned int s = 0; unsigned int i = 0; while (i < n) { // Line A v = (unsigned char) rand(); s += v; ++i; } assert(s >= v); return 1; } Print loop invariants as valid C assertions at line A that help prove the assertion. Use \u2019&&\u2019 or \u2019||\u2019 if necessary. Don\u2019t explain. Your answer should be \u2019assert(...); // line A\u2019\nBelow are GPT-4\u2019s responses on multiple attempts as well as the final set of extracted properties our prototype proceeds to prove.\nGPT output 1 with penalty 1.5: assert(i <= n); // line A assert(s >= i); // line A GPT output 2 with penalty 1.5: assert(i <= n); // line A assert(s >= 0 && v >= 0); // line A GPT output 3 with penalty 1.5: assert(i <= n); // line A assert(s >= 0 && s <= i*UCHAR_MAX); // line A GPT output 4 with penalty 1.5: assert(i <= n); // line A assert(s >= 0 && s <= i*UCHAR_MAX); // line A GPT output 1 with penalty 2: assert(i <= n); // line A assert(s >= 0 && s <= UCHAR_MAX * i); // line A GPT output 2 with penalty 2: assert(i <= n); // line A assert(s >= i*v && s <= (i*255)); // line A GPT output 3 with penalty 2: assert(i <= n); // line A assert(s >= 0 && s <= i*UCHAR_MAX); // line A GPT output 4 with penalty 2: assert(i <= n); // line A\nassert(s >= 0 && s <= UCHAR_MAX * i); // line A\nFound 6 potential sub-goals Goal 1: i <= n after line 8 Goal 2: s >= 0 && s <= i*0xff after line 8 Goal 3: s >= 0 && s <= 0xff * i after line 8 Goal 4: s >= i after line 8 Goal 5: s >= 0 && v >= 0 after line 8 Goal 6: s >= i*v && s <= (i*255) after line 8\nF.2 REPAIRING A PREVIOUS PROPOSAL\nThe oracle Orepair uses a similar prompting template:\n--------- [P1] Print [P2] as valid C assertions at line A that help prove the assertion. Your previous answer [P3] is [P4]. Use \u2019&&\u2019 or \u2019||\u2019 if necessary. Don\u2019t explain. Your answer should be \u2019assert(...);\u2019 ---------\nP1 is the program with the placeholder line (corresponding to the assumption p = \u27e8\u03d5, l\u27e9 to repair) and the C assertion; P2 is either facts or loop invariants depending on whether p is in a loop; P3 is p; P4 is either incorrect or too weak.\nBelow is an example extracted from the execution of our prototype on another SV-COMP benchmark:\nint main() { unsigned int n = (unsigned int) rand(); unsigned int x = n, y = 0, z; while (x > 0) { x--; y++; } z = y; // Line A while (z > 0) { x++; z--; } assert(y + z == n); return 1; } Print facts as valid C assertions at line A that help prove the assertion. Your previous answer \u2019x + z == n\u2019 is too weak. Use \u2019&&\u2019 or \u2019||\u2019 if necessary. Don\u2019t explain. Your answer should simply be \u2019assert(...);\u2019\nThe GPT outputs on different prompting attempts with different penalties and the extracted properties:\nGPT output 1 with penalty 1.5: assert(x + y == n); // Line A assert(x + z == y); // Line B GPT output 2 with penalty 1.5: assert(x + y == n); // Line A assert(x + z == n); // Line B GPT output 1 with penalty 2:\nassert(x + y == n); // Line A assert(x + z == n); // Line B GPT output 2 with penalty 2: assert(x + y == n); // Line A assert(z + x == n); // Line B\nFound 4 potential adapted sub-goals Goal 1: x + y == n after line 7 Goal 2: x + z == n after line 7 Goal 3: x + z == y after line 7 Goal 4: z + x == n after line 7"
        }
    ],
    "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
    "year": 2023
}