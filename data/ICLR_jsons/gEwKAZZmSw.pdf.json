{
    "abstractText": "Sampling-based algorithms, which eliminate \u201cunimportant\u201d computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process. The implementation is available at https://github.com/thu-ml/VCAS.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziteng Wang"
        },
        {
            "affiliations": [],
            "name": "Jianfei Chen"
        },
        {
            "affiliations": [],
            "name": "Jun Zhu"
        }
    ],
    "id": "SP:eca9ae6e058407f70ec908d01b073cdfd08437b4",
    "references": [
        {
            "authors": [
                "Guillaume Alain",
                "Alex Lamb",
                "Chinnadhurai Sankar",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Variance reduction in sgd by distributed importance sampling",
            "venue": "arXiv preprint arXiv:1511.06481,",
            "year": 2015
        },
        {
            "authors": [
                "L\u00e9on Bottou",
                "Frank E Curtis",
                "Jorge Nocedal"
            ],
            "title": "Optimization methods for large-scale machine learning",
            "venue": "SIAM Review,",
            "year": 2018
        },
        {
            "authors": [
                "Haw-Shiuan Chang",
                "Erik Learned-Miller",
                "Andrew McCallum"
            ],
            "title": "Active bias: Training more accurate neural networks by emphasizing high variance samples",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jianfei Chen",
                "Lianmin Zheng",
                "Zhewei Yao",
                "Dequan Wang",
                "Ion Stoica",
                "Michael Mahoney",
                "Joseph Gonzalez"
            ],
            "title": "Actnn: Reducing training memory footprint via 2-bit activation compressed training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Lai Chung"
            ],
            "title": "A course in probability theory",
            "venue": "Academic press,",
            "year": 2001
        },
        {
            "authors": [
                "Cody Coleman",
                "Christopher Yeh",
                "Stephen Mussmann",
                "Baharan Mirzasoleiman",
                "Peter Bailis",
                "Percy Liang",
                "Jure Leskovec",
                "Matei Zaharia"
            ],
            "title": "Selection via proxy: Efficient data selection for deep learning",
            "year": 1906
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Anurag Arnab",
                "Lucas Beyer",
                "Ashish Vaswani",
                "Yi Tay"
            ],
            "title": "The efficiency misnomer",
            "venue": "arXiv preprint arXiv:2110.12894,",
            "year": 2021
        },
        {
            "authors": [
                "Chaosheng Dong",
                "Xiaojie Jin",
                "Weihao Gao",
                "Yijia Wang",
                "Hongyi Zhang",
                "Xiang Wu",
                "Jianchao Yang",
                "Xiaobing Liu"
            ],
            "title": "One backward from ten forward, subsampling for large-scale deep learning",
            "venue": "arXiv preprint arXiv:2104.13114,",
            "year": 2021
        },
        {
            "authors": [
                "Petros Drineas",
                "Michael W Mahoney"
            ],
            "title": "Lectures on randomized numerical linear algebra",
            "venue": "The Mathematics of Data,",
            "year": 2018
        },
        {
            "authors": [
                "Yang Fan",
                "Fei Tian",
                "Tao Qin",
                "Jiang Bian",
                "Tie-Yan Liu"
            ],
            "title": "Learning what data to learn",
            "venue": "arXiv preprint arXiv:1702.08635,",
            "year": 2017
        },
        {
            "authors": [
                "Mohsen Fayyaz",
                "Ehsan Aghazadeh",
                "Ali Modarressi",
                "Mohammad Taher Pilehvar",
                "Yadollah Yaghoobzadeh",
                "Samira Ebrahimi Kahou"
            ],
            "title": "Bert on a data diet: Finding important examples by gradient-based pruning",
            "venue": "arXiv preprint arXiv:2211.05610,",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Cramming: Training a language model on a single gpu in one day",
            "venue": "arXiv preprint arXiv:2212.14034,",
            "year": 2022
        },
        {
            "authors": [
                "Torsten Hoefler",
                "Dan Alistarh",
                "Tal Ben-Nun",
                "Nikoli Dryden",
                "Alexandra Peste"
            ],
            "title": "Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Le Hou",
                "Richard Yuanzhe Pang",
                "Tianyi Zhou",
                "Yuexin Wu",
                "Xinying Song",
                "Xiaodan Song",
                "Denny Zhou"
            ],
            "title": "Token dropping for efficient bert pretraining",
            "venue": "arXiv preprint arXiv:2203.13240,",
            "year": 2022
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Angela H Jiang",
                "Daniel L-K Wong",
                "Giulio Zhou",
                "David G Andersen",
                "Jeffrey Dean",
                "Gregory R Ganger",
                "Gauri Joshi",
                "Michael Kaminksy",
                "Michael Kozuch",
                "Zachary C Lipton"
            ],
            "title": "Accelerating deep learning by focusing on the biggest losers",
            "venue": "arXiv preprint arXiv:1910.00762,",
            "year": 2019
        },
        {
            "authors": [
                "Tyler B Johnson",
                "Carlos Guestrin"
            ],
            "title": "Training deep models faster with robust, approximate importance sampling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jean Kaddour",
                "Oscar Key",
                "Piotr Nawrot",
                "Pasquale Minervini",
                "Matt J Kusner"
            ],
            "title": "No train no gain: Revisiting efficient training algorithms for transformer-based language models",
            "venue": "arXiv preprint arXiv:2307.06440,",
            "year": 2023
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Not all samples are created equal: Deep learning with importance sampling",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Conglong Li",
                "Zhewei Yao",
                "Xiaoxia Wu",
                "Minjia Zhang",
                "Yuxiong He"
            ],
            "title": "Deepspeed data efficiency: Improving deep learning model quality and training efficiency via efficient data sampling and routing",
            "venue": "arXiv preprint arXiv:2212.03597,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoxuan Liu",
                "Lianmin Zheng",
                "Dequan Wang",
                "Yukuo Cen",
                "Weize Chen",
                "Xu Han",
                "Jianfei Chen",
                "Zhiyuan Liu",
                "Jie Tang",
                "Joey Gonzalez"
            ],
            "title": "Gact: Activation compressed training for generic network architectures",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Online batch selection for faster training of neural networks",
            "venue": "arXiv preprint arXiv:1511.06343,",
            "year": 2015
        },
        {
            "authors": [
                "Paulius Micikevicius",
                "Sharan Narang",
                "Jonah Alben",
                "Gregory Diamos",
                "Erich Elsen",
                "David Garcia",
                "Boris Ginsburg",
                "Michael Houston",
                "Oleksii Kuchaiev",
                "Ganesh Venkatesh",
                "Hao Wu"
            ],
            "title": "Mixed precision training, 2018",
            "year": 2018
        },
        {
            "authors": [
                "S\u00f6ren Mindermann",
                "Jan M Brauner",
                "Muhammed T Razzak",
                "Mrinank Sharma",
                "Andreas Kirsch",
                "Winnie Xu",
                "Benedikt H\u00f6ltgen",
                "Aidan N Gomez",
                "Adrien Morisot",
                "Sebastian Farquhar"
            ],
            "title": "Prioritized training on points that are learnable, worth learning, and not yet learnt",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Deanna Needell",
                "Rachel Ward",
                "Nati Srebro"
            ],
            "title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Mahdi Nikdan",
                "Tommaso Pegolotti",
                "Eugenia Iofinova",
                "Eldar Kurtic",
                "Dan Alistarh"
            ],
            "title": "Sparseprop: Efficient sparse backpropagation for faster training of neural networks",
            "venue": "arXiv preprint arXiv:2302.04852,",
            "year": 2023
        },
        {
            "authors": [
                "Xu Ouyang",
                "Shahina Mohd Azam Ansari",
                "Felix Xiaozhu Lin",
                "Yangfeng Ji"
            ],
            "title": "Efficient model finetuning for text classification via data filtering",
            "venue": "arXiv preprint arXiv:2207.14386,",
            "year": 2022
        },
        {
            "authors": [
                "Mansheej Paul",
                "Surya Ganguli",
                "Gintare Karolina Dziugaite"
            ],
            "title": "Deep learning on a data diet: Finding important examples early in training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Md Aamir Raihan",
                "Tor Aamodt"
            ],
            "title": "Sparse weight activation training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "David E Rumelhart",
                "Geoffrey E Hinton",
                "Ronald J Williams"
            ],
            "title": "Learning representations by backpropagating",
            "venue": "errors. nature,",
            "year": 1986
        },
        {
            "authors": [
                "Vatsal Shah",
                "Xiaoxia Wu",
                "Sujay Sanghavi"
            ],
            "title": "Choosing the sample with lowest loss makes sgd robust",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Zhewei Yao",
                "Xiaoxia Wu",
                "Conglong Li",
                "Connor Holmes",
                "Minjia Zhang",
                "Cheng Li",
                "Yuxiong He"
            ],
            "title": "Random-ltd: Random and layerwise token dropping brings efficient training for large-scale transformers",
            "venue": "arXiv preprint arXiv:2211.11586,",
            "year": 2022
        },
        {
            "authors": [
                "Minghe Zhang",
                "Chaosheng Dong",
                "Jinmiao Fu",
                "Tianchen Zhou",
                "Jia Liang",
                "Jia Liu",
                "Bo Liu",
                "Michinari Momma",
                "Bryan Wang",
                "Yan Gao"
            ],
            "title": "Adaselection: Accelerating deep learning training through data subsampling",
            "venue": "arXiv preprint arXiv:2306.10728,",
            "year": 2023
        },
        {
            "authors": [
                "Minjia Zhang",
                "Yuxiong He"
            ],
            "title": "Accelerating training of transformer-based language models with progressive layer dropping",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Peilin Zhao",
                "Tong Zhang"
            ],
            "title": "Stochastic optimization with importance sampling for regularized loss minimization",
            "venue": "In international conference on machine learning,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nTraining neural networks can be computationally intensive. Contemporary networks typically employ stochastic gradient methods (Bottou et al., 2018) for training, which iteratively process batches of data to compute stochastic gradients through forward propagation (FP) and back propagation (BP) techniques (Rumelhart et al., 1986). FP+BP are costly, as they need to process every datum in the batch and every connection in the network, resulting in a multiplicative time complexity of batch size and model size. Such a time complexity becomes increasingly problematic in the era of big data and big models.\nData samples are not equally important. Some might be easy for the network to learn, while others might be extremely hard. Training can be accelerated by utilizing this disparity, focusing the available computational resources on more pivotal samples. At a high level, this can be achieved by further sampling the batch with higher keep probability of more important samples. The computational overhead is consequently diminished, in proportion to the quantity of retained samples. Various methods are proposed to assess the importance of samples, including\nmeta-learning methods (Fan et al., 2017; Coleman et al., 2019; Mindermann et al., 2022), loss-based methods (Loshchilov & Hutter, 2015; Chang et al., 2017; Jiang et al., 2019; Ouyang et al., 2022),\n1Corresponding author.\nand gradient norm based methods (Needell et al., 2014; Zhao & Zhang, 2015; Alain et al., 2015; Johnson & Guestrin, 2018; Katharopoulos & Fleuret, 2018).\nWhile such methods seem promising, one core concern of sampling-based methods is their robustness. Misjudging the importance can hamper convergence, potentially leading to degraded accuracy and even longer training time than uniform sampling. Moreover, the optimal sample ratio is influenced by data distribution, which differs between tasks and is challenging to determine in advance. In general, there is a \u201cno-free-lunch\u201d phenomenon (Kaddour et al., 2023), where aggressive sampling often comes at the cost of reduced robustness.\nIn this work, we propose a robust variance-controlled adaptive sampling (VCAS) algorithm for deep learning under the stochastic optimization framework. VCAS computes a cost-effective approximated stochastic gradient (ASG) by partially conducting backpropagation for specific data and tokens. This ASG is unbiased, and we have developed an adaptive sampling method to meticulously control the variance of the ASG, aligning it with the original stochastic gradient\u2019s variance. Consequently, convergence remains largely unaffected, with our method mirroring the progression of exact algorithms, as delineated in Fig. 1.\nUnlike previous methods, VCAS construct the ASG in a fine-grained manner. Rather than dropping samples one-time in a whole, VCAS gradually drops more samples when backpropagating from topmost to bottommost network layers, as the gradient getting sparser. Furthermore, VCAS also more aggressively drops data in finer granularity of tokens rather than samples when computing the weight gradients. VCAS can achieve smaller variance under a given computational budget compared to coarse grained sampling on the data dimension.\nWe evaluate VCAS on multiple finetuning and pre-training tasks of language models and vision transformers. VCAS can preserve the original training loss trajectory and the validation accuracy on all tasks, while adaptively determining the computational saving depending on the difficulty of the task. VCAS can reduce the computational cost of backpropagation by up to 73.87%, and reduce the overall training computation by up to 49.58%."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Methods focusing on the difference of data, known as online batch selection (Loshchilov & Hutter, 2015), can be mainly categorized into three classes: meta learning methods, loss based methods and gradient norm based methods. In this section we will discuss these three ways separately and briefly introduce other orthogonal efficient training methods.\nMeta Learning Methods. Some works formulate data sampling into an optimization problem and train a separate meta predictor to solve it. Fan et al. (2017) use deep reinforcement learning to train an agent for data selection. Coleman et al. (2019) and Mindermann et al. (2022) train a separate cheaper model with similar architecture for guidance. However, training a meta predictor will introduce further overhead and it\u2019s a non-trivial learning task with more uncertainty introduced for weak theoretical guarantee.\nLoss Based Methods. Loss is a natural indicator of the importance of different data. Loshchilov & Hutter (2015) maintains a history of losses and develops a sophisticated distribution based on the value or rank of loss. Jiang et al. (2019) and Ouyang et al. (2022) simplify it with sampling distribution proportion to the percentile of loss in the history. Chang et al. (2017) broadens the history to every datum and proposes to sample by the variance of prediction probability directly linked with previous losses. Dong et al. (2021) provides another method of minimizing the L2 norm between the sampled loss and the exact counterpart. Shah et al. (2020) samples the smallest loss for robustness to outliers. Zhang et al. (2023) ensembles several loss methods with a preset sample ratio and varies the weights assigned to these methods adaptively. Simple and effective as they may be, the loss based methods are heuristic and always need a hyperparameter of sample ratio to tune for different tasks, violating the goal of efficient training.\nGradient Norm Based Methods. Previous works have proved that the optimal data sampling distribution for SGD is proportional to the gradient norm(Needell et al., 2014; Zhao & Zhang, 2015). But calculating the gradient norm is prohibitive since it needs a full process of backpropagation. To solve this problem, Alain et al. (2015) applies distributed training with many workers calculating\nthis importance score in parallel. Johnson & Guestrin (2018) uses a second-order approximation of gradient norm with history maintained. Closely related to our work, Katharopoulos & Fleuret (2018) develops a pure online algorithm by constructing an upper bound of gradient norm to sample with much cheaper computation. These methods are usually more expensive but have relatively strong theoretical guarantees. So we follow this way in our activation sampling.\nOrthogonal Efficient Training Methods. Data pruning (Paul et al., 2021; Fayyaz et al., 2022) focuses on filtering less informative data before the whole training. Architecture pruning like layer dropping (Huang et al., 2016; Zhang & He, 2020) and token dropping (Hou et al., 2022; Yao et al., 2022; Li et al., 2022) modifies the architecture to make models faster to train with modest affect to performance. Mixed precision training and quantization (Micikevicius et al., 2018; Chen et al., 2021; Liu et al., 2022) change the training procedure to use low-precision in calculation for acceleration. Sparsity(Hoefler et al., 2021) focuses on pruning near-zero values in weights, activations, or gradients to achieve a low FLOPs(Raihan & Aamodt, 2020) and low memory footprint(Nikdan et al., 2023), yet is usually hard to bring a wall-clock time reduction like us due to the lack of hardware support(NVIDIA, 2021). All these works are orthogonal to our work since we focus on the computation approximation of a certain model architecture on a certain dataset with a certain training procedure to bring real training acceleration."
        },
        {
            "heading": "3 VARIANCE-CONTROLLED SAMPLING AS STOCHASTIC OPTIMIZATION",
            "text": "In this section, we present a high-level overview of our sampling algorithm as stochastic optimization. Consider the learning problem of a model f(X; \u03b8) parameterized by \u03b8 on a dataset D = {(Xi, yi)}|D|i=1 with a loss function \u2113(\u00b7, \u00b7). Define the learning objective as\nL(\u03b8) = EB [\u2113(f(X; \u03b8), y)] , (1)\nwhere the expectation is taken over all possible batches B = (X, y) from D. The model parameters can be learned by stochastic optimization algorithms (Bottou et al., 2018) with a stochastic gradient (SG) g(\u03b8;B) := \u2207\u03b8\u2113(f(X; \u03b8), y), which is an unbiased approximation of\u2207\u03b8L(\u03b8). However, computing the stochastic gradient can be still too expensive, since it requires the full forward and back propagation, which iterate over all model parameters and all data in the batch. We build a cheap stochastic approximation g(\u03b8;B, \u03f5) of the SG, which we refer as approximated stochastic gradient (ASG). ASG only computes the backpropagation partially, and is therefore cheaper than the SG. The randomness in the computing procedure of ASG is captured by \u03f5. We ensure that ASG is unbiased: E\u03f5[g(\u03b8;B, \u03f5)] = g(\u03b8;B).\nWith an unbiased SG, stochastic optimization algorithms are guaranteed to converge to a stationary point of Eq. (1), while the converge speed depends on the variance (cf. Bottou et al. (2018)). Therefore, if the variance of the ASG can be controlled to the similar variance level of SG, substituting the SG with ASG should have little impact to the convergence behavior. In fact, by the law of total variance (Chung, 2001), the variance of ASG can be decoupled as\nVar [g(\u03b8;B, \u03f5)] = Var [g(\u03b8;B)] + EB [Var\u03f5 [g(\u03b8;B, \u03f5)]] , where the first term is the intrinsic variance of SG caused by subsampling batches from the dataset, and the second term is the additional variance incurred by ASG. In the subsequent sections, we will discuss our constructions of the ASG, which incurs negligible additional variance compared to SG."
        },
        {
            "heading": "4 FINE-GRAINED SAMPLING",
            "text": "Here we present variance-controlled adaptive sampling (VCAS), a specific construction of the ASG. We compute ASG by approximating the backpropagation in a fine-grained manner, and speed up matrix multiplications with importance sampling on the data dimension.\nAssume a batch X of shape N \u00d7 T \u00d7 K, where N is the batch size, T is the number of tokens of each datum, and K is the dimensionality. For an L-layer network , the model f(X; \u03b8) can be described by the following forward propagation procedure: Z(0) = X,Z(l) = f (l) ( Z(l\u22121); \u03b8(l) ) , f(X; \u03b8) = Z(L), where Z(l) and \u03b8(l) are the activation and parameters of the\nl-th layer, and \u03b8 = (\u03b8(l))Ll=1. The SG can be computed by back-propagation in the following form: \u2207Z(l\u22121) = h(l) ( \u2207Z(l) ;Z(l\u22121), \u03b8(l) ) ,\u2207\u03b8(l) = g(l) ( \u2207Z(l) ;Z(l\u22121), \u03b8(l) ) , where \u2207Z(l) and \u2207\u03b8(l) denote the activation / weight gradient, h(l) and g(l) denote the function that calculates input / weight gradient of layer l with the output gradient, layer input and weight. The SG g(\u03b8;B) = (\u2207\u03b8(l))Ll=1. As illustrated by Fig. 3, the activation gradients \u2207Z(l) are sparse: the gradient (\u2207Z(l))i is close to zero for most sample i, except for a few important samples. Such sparsity becomes more prominent as backpropagating to lower layers and as the training progresses. To speed up computation, we add samplers in the backpropagation graph:\n\u2207\u0302Z(l) = SampleA\u03f5,\u03c1l (\u2207Z(l)) , \u2207Z(l\u22121) = h (l) ( \u2207\u0302Z(l) ;Z(l\u22121), \u03b8(l) ) ,\n\u2207\u0303Z(l) = SampleW\u03bel,\u03bdl ( \u2207\u0302Z(l) , Z(l\u22121) ) , \u2207\u03b8(l) = g(l) ( \u2207\u0303Z(l) ;Z(l\u22121), \u03b8(l) ) . (2)\nThe sampler SampleA\u03f5,\u03c1l (\u00b7) randomly filter out unimportant data from the activation gradient, the keep ratio is \u03c1l, with the randomness captured by \u03f5. The sampler is applied for each layer, so the activation gradient becomes increasingly sparse when backpropagating from the L-th layer to the first layer. The sampler SampleW\u03bel,\u03bdl (\u00b7) filters (data, token) pairs specifically for weight gradient calculation, with a keep ratio \u03bdl and the randomness \u03bel. With these samplers, we only need to compute backpropagation for the retained data / token, so the computational cost is reduced. The sampling procedure is illustrated in Fig. 2, which constructs an unbiased ASG g(\u03b8;B, \u03f5, \u03be, \u03c1, \u03bd) = (\u2207\u03b8(l))Ll=1, with\u2207\u03b8(l) defined as Eq. (2), and \u03be = (\u03bel)Ll=1, \u03c1 = (\u03c1)Ll=1, \u03bd = (\u03bdl)Ll=1."
        },
        {
            "heading": "4.1 ACTIVATION GRADIENT",
            "text": "We apply unbiased low-variance approximation to the activation gradient to speed up subsequent computation. For an activation gradient tensor G of shape N \u00d7 T \u00d7K, we sample\nG\u0302 = SampleA\u03f5,\u03c1 (G) = G \u25e6 (m(\u03f5, \u03c1)\u2297 1\u2297 1),\nwhere \u25e6 is element-wise product, and \u2297 is tensor outer product. The mask m \u2208 RN is a random Bernoulli vector: m(\u03f5, \u03c1)i = Bern(pi; \u03f5)/pi , where \u2211N i=1 pi = N\u03c1, and Bern(p; \u03f5) denotes a Bernoulli random number generator with probability p and randomness \u03f5. Since E[m(\u03f5, \u03c1)i] = 1,\u2200i, the approximation is unbiased: E[G\u0302] = G. The sampler zeros out the gradient for all the data whose m(\u03f5, \u03c1)i = 0. The amount of retained data is N\u03c1 in expectation. With the sampler, we only need to compute backpropagation for retained data, so the cost is \u03c1 times lower.\nThe variance of the approximation is Var [ G\u0302 ] = \u2211N i=1 1\u2212pi pi \u2225Gi\u22252F , where we define the variance\nof a random tensor element-wise as Var [ G\u0302 ] = \u2211 ijk Var [ G\u0302ijk ] , and Gi denotes the i-th matrix of\nG in the N dimension. We compute the keep probability (pi) to minimize the variance, deriving a distribution proportional to the gradient norm of each datum: pi \u221d \u2225Gi\u2225F . Minimizing the variance of the activation gradient not necessarily minimize the variance of ASG, which is the gradient of parameters. Nevertheless, this is a useful heuristic which empirically achieves low variance as is revealed by Katharopoulos & Fleuret (2018), and the ASG variance will be carefully controlled by our adaptive algorithm, as we shall see soon in Sec. 5."
        },
        {
            "heading": "4.2 WEIGHT GRADIENT",
            "text": "We can accelerate the computation of weight gradient for linear layers by sampling in both data and token dimensions. Consider the approximate back propagation of a linear layer Z(l) = Z(l\u22121)\u03b8(l) \u22a4 :\n\u2207\u0302Z(l) = SampleA\u03f5,\u03c1l (\u2207Z(l)) , \u2207\u0303Z(l) = SampleW\u03bel,\u03bdl ( \u2207\u0302Z(l) , Z(l\u22121) ) , \u2207\u03b8(l) = \u2207\u0303\u22a4Z(l)Z (l\u22121)\nin matrix form, where we reshape the activation/gradients to NT\u00d7K, and \u2207\u0302Z(l) is already a sampled matrix with only NT\u03c1l non-zero rows in expectation. However, \u2207\u0302Z(l) is only sampled in the data dimension. In fact, even (\u2207\u0302Z(l))i is retained for some datum i, it might still have some rows (i.e., tokens) which are close to zero. We can further sample\n\u2207\u0303Z(l) = SampleW\u03bel,\u03bdl ( \u2207\u0302Z(l) , Z(l\u22121) ) = \u2207\u0302Z(l) \u25e6 (m(\u03be, \u03bd)\u22a41),\nwhere the mask m \u2208 RNL is a random Bernoulli vector, and 1 is an all-one vector: m(\u03be, \u03bd)i = Bern(qi; \u03f5)/qi, where \u2211NT i=1 qi = NT\u03c1l\u03bdl. The variance is\nVar [ \u2207\u0303\u03b8(l) ] = NT\u2211 i=1 1\u2212 qi qi \u2225\u2225\u2225\u2207\u0302Z(l)i\u2225\u2225\u22252 2 \u2225\u2225\u2225Z(l\u22121)i \u2225\u2225\u22252 2 . (3)\nThe minimal variance solution is qi \u221d \u2225\u2225\u2225\u2207\u0302Z(l)i\u2225\u2225\u2225\n2 \u2225\u2225\u2225Z(l\u22121)i \u2225\u2225\u2225 2 . This sampling method is also known\nas leverage score sampling in randomized numerical linear algebra (Drineas & Mahoney, 2018)."
        },
        {
            "heading": "5 ADAPTING SAMPLE RATIOS",
            "text": "The question remained is how to set the sample ratios (\u03c1l)Ll=1 and (\u03bdl) L l=1. There is a tradeoff: lowering the sample ratio reduces the computational cost, but increases the variance. As discussed in Sec. 3, this ratio should be set to ensure that the additional variance of ASG is marginal compared to the original variance of SG. Adapting the sample ratio is nontrivial since the gradient sparsity pattern vary across layers and vary over time during training. In this section, we present an adaptation algorithm to control the variance during the entire training trajectory.\nFirst, we introduce a single hyperparameter s to control the sample ratios (\u03c1l)Ll=1 for all layers. Intuitively, when the gradient norm (\u2225Gi\u2225F ) N i=1 becomes sparser, we can more aggressively utilize\nsmaller keep ratio \u03c1l to maximize speedup. Therefore, we compute \u03c1l based on the sparsity pl of the gradient norm sequence:\npl(s) = min{n/N | n\u2211\ni=1 \u2225Gi\u2225F \u2265 s N\u2211 i=1 \u2225Gi\u2225F }, \u03c1l(s) = max j\u2264l pj(s) (4)\nwhere s \u2208 [0, 1] is a hyperparameter on how much gradient norm is preserved. It\u2019s shown in Fig. 3 that gradient norm grows sparser with layer, yielding a descending trend of pl for l from L to 1. Thus it\u2019s reasonable to construct a monotone increasing sequence of {\u03c1l}Ll=1 based on {pl}Ll=1. By law of total variance, we can decompose the variance of ASG as\nVar [g(\u03b8;B, \u03f5, \u03be, \u03c1, \u03bd)] = Var [g(\u03b8;B)] + EB[Var\u03f5 [g(\u03b8;B, \u03f5, \u03c1(s))]] + EB,\u03f5[Var\u03be [g(\u03b8;B, \u03f5, \u03be, \u03c1, \u03bd]],\nwhere we write g(\u03b8;B, \u03f5, \u03c1) := E\u03be[g(\u03b8;B, \u03f5, \u03be, \u03c1, \u03bd)] to be the ASG without the sampler for weight gradient computation. The three variance terms are the SG variance, the variance introduced by approximately computing activation gradient, and the variance introduced by approximately computing weight gradient, respectively. Our algorithm adaptively tunes s and \u03bd during train to control the last two variance terms to be fractional comparing to the first variance term.\nControlling EB[Var\u03f5 [g(\u03b8;B, \u03f5, \u03c1(s))]]: We adopt a zeroth order method to adapt the hyperparameter s to keep EB[Var\u03f5 [g(\u03b8;B, \u03f5, \u03c1(s))]] = \u03c4actVar [g(\u03b8;B)], where \u03c4act \u226a 1 is a small constant. That is, the additional variance raised by approximately computing activation gradient is only \u03c4act times the SG variance itself. Since larger s increases the keep ratio and decreases the variance, we adopt the update:\ns\u2190 s+ \u03b1 sign (EB[Var\u03f5 [g(\u03b8;B, \u03f5, \u03c1(s))]]\u2212 \u03c4actVar [g(\u03b8;B)]) , (5)\nwhere sign(x) = +1 when x \u2265 0 and sign(x) = \u22121 when x < 0, and \u03b1 is a step size. We approximate the expectation and variance with empirical ones with M Monte Carlo repetitions. Therefore, each update requires O(M2) FP+BPs, and we run the update every F SGD iterations, where F \u226bM2.\nControlling EB,\u03f5[Var\u03be [g(\u03b8;B, \u03f5, \u03be, \u03c1, \u03bd]]: As the variance sums up for each parameter \u03b8(l), we can further decompose the variance as\nEB,\u03f5[Var\u03be [g(\u03b8;B, \u03f5, \u03be, \u03c1, \u03bd]] = L\u2211\nl=1\nEB,\u03f5 [ Var\u03be [ g(l)(\u03b8;B, \u03f5, \u03bel, \u03c1, \u03bdl ]] , (6)\nwhere g(l) is the gradient of the l-th layer (i.e., \u2207\u03b8(l) ). We control the variance of each layer separately to keep EB,\u03f5 [ Var\u03be [ g(l)(\u03b8;B, \u03f5, \u03bel, \u03c1, \u03bdl) ]] = \u03c4wVar [ g(l)(\u03b8;B) ] . Again, this is achieved by a zeroth-order algorithm:\n\u03bdl \u2190 \u03bdl\u03b2sign(EB,\u03f5[Var\u03be[g (l)(\u03b8;B,\u03f5,\u03bel,\u03c1,\u03bdl)]]\u2212\u03c4wVar[g(l)(\u03b8;B)]), (7)\nwhere Var\u03be [ g(l) ] can be computed analytically by Eq. 3, and \u03b2 is a multiplier.\nNow we are fully prepared to present the whole picture of VCAS in Alg. 1. Please refer to Appendix. D for more details about the algorithm."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "6.1 TRAINING FLOPS REDUCTION",
            "text": "We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. We compare our algorithm with the exact training and two previous works in BP sampling: a loss based method SB(selective backprop) in Johnson & Guestrin (2018) and a gradient norm based method UB(upper bound) in Katharopoulos & Fleuret (2018). We choose these two methods since they are entirely online and need little modification to the original training pipeline like us. The results are shown in Tab. 1. All results are the average of 3 different seeds except for BERT-base pretraining and ViT finetuning on ImageNet-1k which we use 1.\nAlgorithm 1 Variance controlled adaptive sampling(VCAS) for backpropagation Require: update frequency F , Monte-Carlo repetition number M , variance tolerant ratio for activation \u03c4act,\nfor weight \u03c4w, s step size \u03b1, weight ratio multiplier \u03b2\ns\u2190 1, activation sample ratio schedule {\u03c1l}Ll=1 \u2190 1, weight sample ratios {\u03bdl}Ll=1 \u2190 1 t\u2190 0 while not converge do\nif t mod F = 0 then for i in 1, . . . ,M do\n(Xi, yi)\u2190 batch selected randomly SGD gradient Gs,i \u2190 exact backward using (Xi, yi) for j in 1, . . . ,M do\nactivation gradient Gact,i,j \u2190 backward using (Xi, yi) with SampleA only calculate weight variance Vw,i,j analytically with Eq. 3 and Eq. 6\nend for end for SGD variance Vs \u2190 1M\u22121 \u2211M i=1 \u2225\u2225\u2225Gs,i \u2212 1M \u2211Mi=1 Gs,i\u2225\u2225\u22252 F\nactivation variance Vact \u2190 1M \u2211M i=1 ( 1 M \u2211M j=1 \u2225Gact,i,j \u2212Gs,i\u2225 2 F ) weight variance Vw \u2190 1M \u2211M i=1 ( 1 M \u2211M j=1 Vw,i,j\n) update s with Vact and Vs according to Eq. 5 update {\u03c1l}Ll=1 with new s according to Eq. 4 update {\u03bdl}Ll=1 with Vw and Vs according to Eq. 7\nend if backward with SampleA and SampleW t\u2190 t+ 1\nend while\nNote that to avoid falling into the pitfall of unfair comparison with baseline which is not tuned under efficient settings as is pointed out by Dehghani et al. (2021) and Kaddour et al. (2023), for all these experiments we use the same conservative setting of \u03c4act = \u03c4w = 0.025, \u03b1 = 0.01, \u03b2 = 0.95,M = 2. We preset all these values heuristically without any tuning or prior knowledge. The only hyperpamater we modified among different tasks is the variance calculation frequency F , which can be defined easily according to the total training steps.\nIn fact, all the hyperparameters introduced by VCAS have explicit meanings and are insensitive. We show experimentally that though extra tuning may achieve a slightly better result, overall VCAS is robust to these hyperparameters with reasonable values. Please refer to Appendix. A for details about ablation studies on these insensitive hyperparameters.\nFor SB and UB, we both adopt a sample ratio of 1/3, since it\u2019s the recommended setting in the original papers and it can achieve a FLOPs reduction of 1 \u2212 (1 + 2 \u2217 1/3)/3 = 44.44% which is close to the results we get in most tasks. An exception is BERT-base pretraining task where we find the FLOPs reduction achievable is low so we manually set the sample ratio of SB and UB to get the same FLOPs reduction as VCAS, so that they can still give a decent result. Nevertheless we are indeed favoring these methods by helping them to define a reasonable sample ratio, which can not be done themselves.\nFrom the table we can see that overall VCAS is better than SB and UB with the least impact on final train loss and final evaluation accuracy. With FLOPs reduction of up to 49.58%, VCAS can still achieve nearly the same results with the exact counterpart."
        },
        {
            "heading": "6.2 WALL-CLOCK TIME REDUCTION",
            "text": "We record the wall-clock time of BERT-large finetuning on MNLI and ViT-large finetuning on ImageNet-1k with NVIDIA 3090Ti, the results are depicted in Tab. 2 and Tab. 3.\nFrom these tables, we can find that VCAS can translate FLOPs reduction into wall-clock time reduction as effectively as simpler online batch sampling methods like UB and SB that drop part of\ndata one-time in a whole, while enjoying mirrored performance with the exact training under theoretical guarantee.\nThe success of VCAS comes in two ways. One is the fine-grained sampling strategy that samples activation and weight jointly, which enables us to achieve much lower FLOPs given the variance budget. The other is the variance controlled framework combined with the self-adaptation algorithm, with which we are able to learn the proper sample ratios of different training phases. In the following two subsections, we will experimentally show the effectiveness of these two folds."
        },
        {
            "heading": "6.3 EFFECTIVENESS OF FINE-GRAINED SAMPLING",
            "text": "We compare VCAS that samples activation and weight jointly with strategies that solely sampling activation or weight. Specifically, we keep an equal extra variance for BERT-base finetuning on MNLI. We set \u03c4act = \u03c4w = 0.025 for VCAS, \u03c4act = 0.05 for activation sampling only and \u03c4w = 0.05 for weight sampling only. We find that under the preliminary that \u03c4act, \u03c4w \u226a 1, the results\n0 10000 20000 30000 40000 steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nFL OP\ns r ed\nuc tio\nn ra\ntio 0.4156\n0.2219\n0.3406 VCAS activation only weight only\nFigure 4: FLOPs reduction ratio of VCAS vs. sampling activation or weight solely with equal variance. 0 5000 10000 15000 20000 25000 30000 35000 steps\n10 2\n10 1\n100\n101\n102\n103\nva ria\nnc e\nSGD_var SGD_var * threshold\nVCAS_act_var VCAS_weight_var\nUB_var SB_var\nFigure 5: Gradient variance of different methods.\nof these sampling strategies show no significant difference due to controlled variance. While as is shown in Fig. 4, VCAS can achieve a much greater FLOPs reduction with the same total variance introduced. It\u2019s reasonable since we can utilize more sparsity in both data and token dimensions with a fine-grained sampling strategy of VCAS."
        },
        {
            "heading": "6.4 EFFECTIVENESS OF VARIANCE CONTROL AND SELF-ADAPTATION",
            "text": "In Fig. 5 we plot the variance of different methods during training process of BERT-base finetuning on MNLI. We can find that VCAS is able to control the extra sampling variance introduced to our preset threshold, while for other variance-unaware algorithms like UB and SB, the extra variance is out of control with a similar FLOPs reduction.\nWith carefully controlled variance, a similar convergence with exact training is guaranteed as we mentioned in the introduction. As is depicted in Fig. 1 and Fig. 6 for BERT-base finetuning on MNLI, VCAS shares nearly the same convergence trajectory with the exact training with reduced FLOPs, while UB converges slightly slower due to uncontrolled variance, and SB converges in an entirely different trajectory with variance introduced far larger than exact."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "We propose VCAS, a robust sampling method for back propagation with controlled variance and self-adaptive sample ratios. VCAS computes an approximate stochastic gradient by applying finegrained sampling to gradually remove samples and tokens during backpropagation. VCAS enjoys similar variance, convergence trajectory, and final accuracy with exact back propagation, while reduces the training cost by up to 49.58%."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "The authors would like to thank Bingrui Li and Weiyu Huang for their valuable discussions and help on algorithm design and implementation details. This work was supported by the National Key Research and Development Program of China (No. 2021ZD0110502), NSFC Projects (Nos. 62376131, 62061136001, 62106123, 62076147, U19A2081, 61972224), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z is also supported by the XPlorer Prize."
        },
        {
            "heading": "A ABLATION ON HYPERPARAMETERS",
            "text": "There are a few hyperparameters in our self-adaptation algorithm, but all of them have explicit meaning. In this section we show that though extra tuning of these hyperparameters may achieve a slightly better result, overall VCAS is robust to these hyperparameters with reasonable values. We conduct ablation experiments on two tasks: BERT-base finetuning on SST-2 and MNLI. All the results are averaged over 3 different seeds.\nA.1 ACTIVATION AND WEIGHT VARIANCE THRESHOLDS \u03c4act, \u03c4w\nThe main hyperparameters in VCAS is the variance thresholds of activation \u03c4act and weight \u03c4w. For these two thresholds, how to split total variance among them is a big problem with optimal solution differing across models and tasks. So without prior knowledge introduced, we compromise by keeping \u03c4act = \u03c4w = \u03c4 \u226a 1. We further conduct an ablation on \u03c4 from 0.01 to 0.5 as is shown in Tab. 4 for SST-2 and Tab. 5 for MNLI. From the results we can find that a satisfactory outcome is assured regardless of the specific value of \u03c4 provided that \u03c4 \u226a 1, which proves the robustness of VCAS.\nA.2 MONTE-CARLO REPETITIONS M\nTo calculate variances, VCAS introduces an overhead of extra iterations quadratic with Monte-Carlo repetitions M .\nObviously bigger M will bring more precise empirical variance, yet the cost is prohibitive.\nWe experiment on different M from 2 to 10 and find no significant difference in the empirical variance as is shown in Fig. 7 for SST-2 and Fig. 8 for MNLI. Therefore, we adopted the setting of M = 2, with which we only need to perform 6 extra iterations that is negligible if the variance calculation frequency is large enough like 100 in SST-2 and 500 in MNLI.\nA.3 VARIANCE CALCULATION FREQUENCY F\nSimilar to M , the variance calculation frequency F is also a trade-off between better empirical approximation and less overhead introduced. We experimented on F = 50, 100, 200, 500, 1000 in Tab. 6 for SST-2 and Tab. 7 for MNLI. We can see that although as F grows larger the overhead of VCAS is gradually relieved, with a too large F , like F = 1000 in SST-2 that leads to only 6 times of self-adaptation update, the sample ratio schedule is not fully explored and the final FLOPs reduction is even smaller. Therefore, for all these tasks we set F to be at least 1/50 of total training steps and no more than 500 due to slight marginal gains.\nA.4 s UPDATE STEP \u03b1 AND WEIGHT RATIO MULTIPLIER \u03b2\nA simple grid search is conducted for \u03b1 \u2208 {0.005, 0.01, 0.02} and \u03b2 \u2208 {0.95, 0.9, 0.8} in Fig. 9 for SST-2 and Fig. 10 for MNLI. From the figures, we can find that we are able to trade convergence for efficiency with a more aggressive setting of larger \u03b1 and smaller \u03b2, yet all results here are decent\nwith a final accuracy drop of no more than 0.3% for both tasks. Thus, VCAS is robust to different \u03b1 and \u03b2."
        },
        {
            "heading": "84.12 84.21 84.19",
            "text": ""
        },
        {
            "heading": "84.23 84.29 84.17",
            "text": "From all the ablation results above, we can see that VCAS is robust to all these hyperparameters with reasonable values, proving the insensitiveness.\nB INSIGHTS ON UPDATE OF s, {\u03c1l} AND {\u03bdl}\nIn this section, we will show how the gradient norm preserving ratio s as well as all the sample ratios {\u03c1l} and {\u03bdl} update across the training. We record the update process of BERT-base finetuning on MNLI with different variance tolerance thresholds \u03c4 as in Appendix. A.1. All results are averaged on three different seeds.\nFig. 11a depicts the update of s. For non-decreasing {\u03c1l}, we plot the update of the first and the last values \u03c11, \u03c1L in Fig. 11b, with other values lying between. For {\u03bdl}, we show the update of the first three ones \u03bd1, \u03bd2, \u03bd3 in Fig. 11c and observe similar behavior of other weights.\nIt is seen in Fig. 11 that during training of BERT-base on MNLI, the gradient norm preserving ratio s first decreases and then shows a slight downward trend. The activation sample ratios {\u03c1l} gradually decrease with an abrupt change between epochs due to the rapid decline of train loss caused by the lowered learning rate in the linear learning rate scheduler. The weight sample ratios {\u03bdl} first decrease and then fluctuate to match the change of activation sample ratios."
        },
        {
            "heading": "C PERFORMANCE ON CNN",
            "text": "In Sec. 6, we mainly experiment with Transformer-based models and Adam optimizers. But the variance controlled adaptation depicted in Sec. 5 holds universally for any DNNs with SGD-based optimizers, since it just provides an approximated stochastic gradient with controlled variance to estimate the full gradient. In this section, we employ VCAS on other architectures and other optimizers to prove its versatility.\nFor CNN, it is noted that the weight sampler SampleW in Sec. 4 designed for linear layers is not usable for convolution layers. Thus we employ VCAS with a degraded version of activation sampling only.\nWe experiment with WideResNet-18 with widen factor w = 4 pretraining on ImageNet. We use eight NVIDIA 3090Ti to parallel the training with Distributed Data Parallel(DDP). We employ SGDM optimizer with momentum m = 0.9. The results are in Tab. 8.\nFrom the table we can see VCAS is also capable of accelerating the training of CNN. Besides, the parallel setting also proves the parallelizability of VCAS. The relatively low but still decent time reduction can be explained with Amdahl\u2019s Law since VCAS only accelerate the calculation part and is not able to accelerate other parts like communication cost during parallel training."
        },
        {
            "heading": "D DETAILS ABOUT ALGORITHM. 1",
            "text": "It should be noted that some parts of Alg. 1 are simplified for clarity and we list the implementation details below:\nIn the algorithm table, we put the calculation of empirical variances out of the two Monte-Carlo loops for simplicity. Yet practically we can calculate Vact and Vw inside the loops and average the variance scalars outside. Therefore, we only need to store three tensors additionally regardless of M : SGD gradient Gs,i to calculate Vact, and its running mean and running square mean to calculate Vs. By sampling only part of parameters to keep gradients, like 1% in our experiments, the memory overhead can be neglected.\nBesides, since weight sample ratios {\u03bdl} are updated parameter-wise according to Eq. 7, the empirical weight variances and SGD variances are also stored parameter-wise when implemented.\nUpdate of activation sample ratios {\u03c1l} requires finding out gradient sparsity {pl} with the new s according to Eq. 4. In implementation, this is achieved by calculating possible new {\u03c1l} with both s + \u03b1 and s \u2212 \u03b1 inside the Monte-Carlo loops and averaging them outside. Then just choose the proper one with new s."
        },
        {
            "heading": "E PROOF",
            "text": "E.1 PROOF TO UNBIASEDNESS OF VCAS\nLet\u2019s first consider a L-layer MLP. (Note: for simplicity we mildly abuse the term \u201dlayer\u201d here, representing a single operation like matrix multiplication and ReLU)\nFor the last layer L, the output gradient \u2207Z(L) is calculated from the loss directly, the same as the Exact BP. Since activation sampler \u2207\u0302Z(L) = SampleA\u03f5,\u03c1L (\u2207Z(L)) is unbiased, we have:\nE [ \u2207\u0302Z(L) ] = \u2207Z(L)\nWhen back propagation proceeds, we may encounter two types of layers: linear and non-linear. For the linear layer, we have:\n\u2207Z(L\u22121) = \u2207\u0302Z(L)\u03b8(L)\nThus unbiasedness is preserved with the output gradient of the (L\u2212 1)-th layer: E [\u2207Z(L\u22121) ] = E [ \u2207\u0302Z(L) ] \u03b8(L) = \u2207Z(L)\u03b8(L) = Exact BP result\nWhile for the non-linear layer like ReLU, we have:\n\u2207Z(L\u22121) = \u2207\u0302Z(L) \u2299 JZ(L)\nwhere \u2299 is the Hadamard product and JZ(L) is the Jacobbi matrix determined by Z(L) which is saved in forward pass and is exact. Thus again we derive the the output gradient of the (L \u2212 1)-th layer being unbiased:\nE [\u2207Z(L\u22121) ] = E [ \u2207\u0302Z(L) ] \u2299 JZ(L) = \u2207Z(L) \u2299 JZ(L) = Exact BP result\nThus by induction, VCAS assures all activation gradients \u2207\u0302Z(l) , l = 1 . . . L being unbiased. Then for weight gradients, since weight sampler \u2207\u0303Z(l) = SampleW\u03bel,\u03bdl ( \u2207\u0302Z(l) , Z(l\u22121) ) is unbiased, we have:\nE [ \u2207\u0303Z(l) ] = E [ \u2207\u0302Z(l) ] = \u2207Z(l)\nFinally, we derive all weight gradients being unbiased: E [\u2207\u03b8(l) ] = E [ \u2207\u0303Z(l) ]\u22a4 Z(l\u22121) = \u2207\u22a4Z(l)Z (l\u22121) = Exact BP result\nFor more complicated neural networks like CNN and Transformer, since operations like convolutions and layernorm are all linear transforms, by similar reasoning the unbiasedness still holds."
        },
        {
            "heading": "F EXPERIMENT DETAILS",
            "text": "F.1 BERT-BASE PRETRAINING\nFor BERT-base pretraining we use a crammed BERT in Geiping & Goldstein (2022) with the recipe same as the original settings of 1 day training on a single NVIDIA 2080Ti. The full results are as follows in Tab. 9\nFrom the table we can find that although VCAS achieves a relatively high train loss, the downstream task performance is still competent with exact training. While SB and UB both perform worse on CoLA, which is a vulnerable task, reflecting that they have changed the original convergence trajectory of SGD.\nF.2 RECIPE OF OTHER TASKS\nFor BERT finetuning, we use AdamW optimizer with lr = 2e\u22125 and wd = 0.01. The learning rate scheduler is a linear one with warmup ratio = 0.1. We set epoch numbers N = 3 and a batch size of batch size = 32.\nFor ViT finetuning, we use Adam optimizer with lr = 2e\u22125. A linear lr scheduler with no warmup employed. We run N = 5 epochs with batch size batch size = 32"
        },
        {
            "heading": "G LIMITATIONS",
            "text": "VCAS is designed for adaptively learning the proper sample ratios of large model training on large datasets. It is not suitable for small models with low gradient variances resulting in increased numerical errors, or small datasets with few training steps that is insufficient for the update process in VCAS.\nThe weight sampler SampleW in VCAS is specially designed for linear layers and is not usable for other operations like convolution. But the activation sampler SampleA can be applied to all mainstream architectures with deep layers. So for CNN or RNN, we need to employ a degraded version of VCAS with activation sampling only, as shown in Appendix. C.\nVCAS focuses on mirroring the exact training with theoretical guarantee and is lack of exploration of other possible convergence trajectories that may bring a better result. Thus it is not recommended when the original training recipe is under-optimized."
        }
    ],
    "title": "VARIANCE-CONTROLLED ADAPTIVE SAMPLING",
    "year": 2024
}