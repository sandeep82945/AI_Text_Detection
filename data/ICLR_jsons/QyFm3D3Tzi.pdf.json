{
    "abstractText": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuan Yuan"
        },
        {
            "affiliations": [],
            "name": "Chenyang Shao"
        },
        {
            "affiliations": [],
            "name": "Jingtao Ding"
        },
        {
            "affiliations": [],
            "name": "Depeng Jin"
        },
        {
            "affiliations": [],
            "name": "Yong Li"
        }
    ],
    "id": "SP:08e889390a9c0f103fd658420502475c7884ec80",
    "references": [
        {
            "authors": [
                "Yuval Alaluf",
                "Omer Tov",
                "Ron Mokady",
                "Rinon Gal",
                "Amit Bermano"
            ],
            "title": "Hyperstyle: Stylegan inversion with hypernetworks for real image editing",
            "venue": "In Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Lei Bai",
                "Lina Yao",
                "Can Li",
                "Xianzhi Wang",
                "Can Wang"
            ],
            "title": "Adaptive graph convolutional recurrent network for traffic forecasting",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ivana Bala\u017eevi\u0107",
                "Carl Allen",
                "Timothy M Hospedales"
            ],
            "title": "Tucker: Tensor factorization for knowledge graph completion",
            "venue": "arXiv preprint arXiv:1901.09590,",
            "year": 2019
        },
        {
            "authors": [
                "Fan Bao",
                "Shen Nie",
                "Kaiwen Xue",
                "Chongxuan Li",
                "Shi Pu",
                "Yaole Wang",
                "Gang Yue",
                "Yue Cao",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "One transformer fits all distributions in multi-modal diffusion at scale",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jingtao Ding",
                "Guanghui Yu",
                "Yong Li",
                "Depeng Jin",
                "Hui Gao"
            ],
            "title": "Learning from hometown and current city: Cross-city poi recommendation via interest drift and transfer learning",
            "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Yuntao Du",
                "Jindong Wang",
                "Wenjie Feng",
                "Sinno Pan",
                "Tao Qin",
                "Renjun Xu",
                "Chongjun Wang"
            ],
            "title": "Adarnn: Adaptive learning and forecasting of time series",
            "venue": "In Proceedings of the 30th ACM international conference on information & knowledge management,",
            "year": 2021
        },
        {
            "authors": [
                "Ziya Erko\u00e7",
                "Fangchang Ma",
                "Qi Shan",
                "Matthias Nie\u00dfner",
                "Angela Dai"
            ],
            "title": "Hyperdiffusion: Generating implicit neural fields with weight-space diffusion",
            "venue": "arXiv preprint arXiv:2303.17015,",
            "year": 2023
        },
        {
            "authors": [
                "Ziquan Fang",
                "Dongen Wu",
                "Lu Pan"
            ],
            "title": "When transfer learning meets cross-city urban flow prediction: spatio-temporal adaptation matters",
            "year": 2022
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Shansan Gong",
                "Mukai Li",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "Lingpeng Kong"
            ],
            "title": "Diffuseq: Sequence to sequence text generation with diffusion models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Bin Guo",
                "Jing Li",
                "Vincent W Zheng",
                "Zhu Wang",
                "Zhiwen Yu"
            ],
            "title": "Citytransfer: Transferring interand intra-city knowledge for chain store site recommendation based on multi-source urban data",
            "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "David Ha",
                "Andrew M Dai",
                "Quoc V Le"
            ],
            "title": "hypernetworks,\u201d in 5th international conference on learning representations, iclr 2017",
            "venue": "In Conference Track Proceedings, OpenReview. net,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tianfu He",
                "Jie Bao",
                "Ruiyuan Li",
                "Sijie Ruan",
                "Yanhua Li",
                "Li Song",
                "Hui He",
                "Yu Zheng"
            ],
            "title": "What is the human mobility in a new city: Transfer mobility knowledge across cities",
            "venue": "In Proceedings of The Web Conference",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.02303,",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Jin",
                "Kai Chen",
                "Qiang Yang"
            ],
            "title": "Selective cross-city transfer learning for traffic prediction via source city region re-weighting",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy S Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "Diffusionlm improves controllable text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yaguang Li",
                "Rose Yu",
                "Cyrus Shahabi",
                "Yan Liu"
            ],
            "title": "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Zhonghang Li",
                "Lianghao Xia",
                "Yong Xu",
                "Chao Huang"
            ],
            "title": "Gpt-st: Generative pre-training of spatiotemporal graph neural networks",
            "venue": "arXiv preprint arXiv:2311.04245,",
            "year": 2023
        },
        {
            "authors": [
                "Yan Liu",
                "Bin Guo",
                "Daqing Zhang",
                "Djamal Zeghlache",
                "Jingmin Chen",
                "Ke Hu",
                "Sizhe Zhang",
                "Dan Zhou",
                "Zhiwen Yu"
            ],
            "title": "Knowledge transfer with weighted adversarial network for cold-start store site recommendation",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),",
            "year": 2021
        },
        {
            "authors": [
                "Zhanyu Liu",
                "Guanjie Zheng",
                "Yanwei Yu"
            ],
            "title": "Cross-city few-shot traffic forecasting via traffic pattern bank",
            "venue": "arXiv preprint arXiv:2308.09727,",
            "year": 2023
        },
        {
            "authors": [
                "Zhaoyang Liu",
                "Yanyan Shen",
                "Yanmin Zhu"
            ],
            "title": "Inferring dockless shared bike distribution in new cities",
            "venue": "In Proceedings of the eleventh ACM international conference on web search and data mining,",
            "year": 2018
        },
        {
            "authors": [
                "Bin Lu",
                "Xiaoying Gan",
                "Weinan Zhang",
                "Huaxiu Yao",
                "Luoyi Fu",
                "Xinbing Wang"
            ],
            "title": "Spatio-temporal graph few-shot learning with cross-city knowledge transfer",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengxiong Luo",
                "Dayou Chen",
                "Yingya Zhang",
                "Yan Huang",
                "Liang Wang",
                "Yujun Shen",
                "Deli Zhao",
                "Jingren Zhou",
                "Tieniu Tan"
            ],
            "title": "Videofusion: Decomposed diffusion models for high-quality video generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob Mcgrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Zheyi Pan",
                "Yuxuan Liang",
                "Junbo Zhang",
                "Xiuwen Yi",
                "Yong Yu",
                "Yu Zheng"
            ],
            "title": "Hyperst-net: Hypernetworks for spatio-temporal forecasting",
            "venue": "arXiv preprint arXiv:1809.10889,",
            "year": 2018
        },
        {
            "authors": [
                "Zheyi Pan",
                "Yuxuan Liang",
                "Weifeng Wang",
                "Yong Yu",
                "Yu Zheng",
                "Junbo Zhang"
            ],
            "title": "Urban traffic prediction from spatio-temporal data using deep meta learning",
            "venue": "In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2019
        },
        {
            "authors": [
                "Zheyi Pan",
                "Wentao Zhang",
                "Yuxuan Liang",
                "Weinan Zhang",
                "Yong Yu",
                "Junbo Zhang",
                "Yu Zheng"
            ],
            "title": "Spatio-temporal meta learning for urban traffic prediction",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Yanbo Pang",
                "Kota Tsubouchi",
                "Takahiro Yabe",
                "Yoshihide Sekimoto"
            ],
            "title": "Intercity simulation of human mobility at rare events via reinforcement learning",
            "venue": "In Proceedings of the 28th International Conference on Advances in Geographic Information Systems,",
            "year": 2020
        },
        {
            "authors": [
                "William Peebles",
                "Ilija Radosavovic",
                "Tim Brooks",
                "Alexei A Efros",
                "Jitendra Malik"
            ],
            "title": "Learning to learn with generative models of neural network checkpoints",
            "venue": "arXiv preprint arXiv:2209.12892,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Konstantin Sch\u00fcrholt",
                "Dimche Kostadinov",
                "Damian Borth"
            ],
            "title": "Self-supervised representation learning on neural network weights for model characteristic prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Konstantin Sch\u00fcrholt",
                "Boris Knyazev",
                "Xavier Gir\u00f3-i Nieto",
                "Damian Borth"
            ],
            "title": "Hyperrepresentations as generative models: Sampling unseen neural network weights",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zezhi Shao",
                "Zhao Zhang",
                "Fei Wang",
                "Wei Wei",
                "Yongjun Xu"
            ],
            "title": "Spatial-temporal identity: A simple yet effective baseline for multivariate time series forecasting",
            "venue": "In Proceedings of the 31st ACM International Conference on Information & Knowledge Management,",
            "year": 2022
        },
        {
            "authors": [
                "Zezhi Shao",
                "Zhao Zhang",
                "Fei Wang",
                "Yongjun Xu"
            ],
            "title": "Pre-training enhanced spatial-temporal graph neural network for multivariate time series forecasting",
            "venue": "In KDD,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Yihong Tang",
                "Ao Qu",
                "Andy HF Chow",
                "William HK Lam",
                "SC Wong",
                "Wei Ma"
            ],
            "title": "Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities",
            "venue": "In Proceedings of the 31st ACM International Conference on Information & Knowledge Management,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Cl\u00e9ment Vignac",
                "Igor Krawczuk",
                "Antoine Siraudin",
                "Bohan Wang",
                "Volkan Cevher",
                "Pascal Frossard"
            ],
            "title": "Digress: Discrete denoising diffusion for graph generation",
            "venue": "In Proceedings of the 11th International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Binwu Wang",
                "Yudong Zhang",
                "Xu Wang",
                "Pengkun Wang",
                "Zhengyang Zhou",
                "Lei Bai",
                "Yang Wang"
            ],
            "title": "Pattern expansion and consolidation on evolving graphs for continual traffic prediction",
            "venue": "In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Leye Wang",
                "Xu Geng",
                "Xiaojuan Ma",
                "Feng Liu",
                "Qiang Yang"
            ],
            "title": "Cross-city transfer learning for deep spatio-temporal prediction",
            "venue": "In Proceedings of the 28th International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Xu Wang",
                "Lianliang Chen",
                "Hongbo Zhang",
                "Pengkun Wang",
                "Zhengyang Zhou",
                "Yang Wang"
            ],
            "title": "A multi-graph fusion based spatiotemporal dynamic learning framework",
            "venue": "In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Xu Wang",
                "Pengfei Gu",
                "Pengkun Wang",
                "Binwu Wang",
                "Zhengyang Zhou",
                "Lei Bai",
                "Yang Wang"
            ],
            "title": "Graph-free learning in graph-structured data: A more efficient and accurate spatiotemporal learning perspective",
            "venue": "arXiv preprint arXiv:2301.11742,",
            "year": 2023
        },
        {
            "authors": [
                "Ying Wei",
                "Yu Zheng",
                "Qiang Yang"
            ],
            "title": "Transfer knowledge between cities",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Haomin Wen",
                "Youfang Lin",
                "Yutong Xia",
                "Huaiyu Wan",
                "Qingsong Wen",
                "Roger Zimmermann",
                "Yuxuan Liang"
            ],
            "title": "Diffstg: Probabilistic spatio-temporal graph forecasting with denoising diffusion models",
            "venue": "In Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Zonghan Wu",
                "Shirui Pan",
                "Guodong Long",
                "Jing Jiang",
                "Chengqi Zhang"
            ],
            "title": "Graph wavenet for deep spatial-temporal graph modeling",
            "venue": "arXiv preprint arXiv:1906.00121,",
            "year": 2019
        },
        {
            "authors": [
                "Yutong Xia",
                "Yuxuan Liang",
                "Haomin Wen",
                "Xu Liu",
                "Kun Wang",
                "Zhengyang Zhou",
                "Roger Zimmermann"
            ],
            "title": "Deciphering spatio-temporal graph forecasting: A causal lens and treatment",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "Huaxiu Yao",
                "Yiding Liu",
                "Ying Wei",
                "Xianfeng Tang",
                "Zhenhui Li"
            ],
            "title": "Learning from multiple cities: A meta-learning approach for spatial-temporal prediction",
            "venue": "In WWW,",
            "year": 2019
        },
        {
            "authors": [
                "Bing Yu",
                "Haoteng Yin",
                "Zhanxing Zhu"
            ],
            "title": "Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting",
            "venue": "arXiv preprint arXiv:1709.04875,",
            "year": 2017
        },
        {
            "authors": [
                "Yuan Yuan",
                "Jingtao Ding",
                "Chenyang Shao",
                "Depeng Jin",
                "Yong Li"
            ],
            "title": "Spatio-temporal diffusion point processes",
            "venue": "In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyang Zhou",
                "Qihe Huang",
                "Kuo Yang",
                "Kun Wang",
                "Xu Wang",
                "Yudong Zhang"
            ],
            "title": "Maintaining the status quo: Capturing invariant relations for ood spatiotemporal learning. 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyang Zhou",
                "Jiahao Shi",
                "Hongbo Zhang",
                "Qiongyu Chen",
                "Xu Wang",
                "Hongyang Chen",
                "Yang Wang"
            ],
            "title": "Crest: A credible spatiotemporal learning framework for uncertainty-aware traffic forecasting",
            "venue": "In The 17th ACM International Conference on Web Search and Data Mining,",
            "year": 2024
        },
        {
            "authors": [
                "Zhilun Zhou",
                "Jingtao Ding",
                "Yu Liu",
                "Depeng Jin",
                "Yong Li"
            ],
            "title": "Towards generative modeling of urban flow through knowledge-enhanced denoising diffusion",
            "venue": "arXiv preprint arXiv:2309.10547,",
            "year": 2023
        },
        {
            "authors": [
                "Fuzhen Zhuang",
                "Zhiyuan Qi",
                "Keyu Duan",
                "Dongbo Xi",
                "Yongchun Zhu",
                "Hengshu Zhu",
                "Hui Xiong",
                "Qing He"
            ],
            "title": "A comprehensive survey on transfer learning",
            "venue": "Proceedings of the IEEE,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Spatio-temporal prediction is a fundamental problem in various smart city applications (Xia et al., 2024; Zhou et al., 2024; Wang et al., 2023a;c;b). Many deep learning models are proposed to solve this problem, whose successes however rely on large-scale spatio-temporal data. Due to imbalanced development levels and different data collection policies, urban spatio-temporal data, such as traffic and crowd flow data, are usually limited in many cities and regions. Under these circumstances, the model\u2019s transferability under data-scarce scenarios is of pressing importance.\nTo address this issue, various transfer learning approaches have emerged for spatio-temporal modeling. Their primary goal is to leverage knowledge and insights gained from one or multiple source cities and apply them effectively to a target city. These approaches can be broadly classified into two main categories. (1) Coarse-grained methods consider each city as a unified entity and transfer the learned knowledge at the city level (Pang et al., 2020; He et al., 2020; Ding et al., 2019; Tang et al., 2022). (2) Fine-grained methods dissect cities into smaller regions to enable a more refined exploration of region-level knowledge (Wang et al., 2019; Guo et al., 2018; Yao et al., 2019; Liu et al., 2021; Jin et al., 2022; Lu et al., 2022), which have shown better performance due to the inherent disparities between source and target cities. However, existing fine-grained methods largely rely on elaborated matching designs, such as utilizing auxiliary data for similarity calculation (Wang et al., 2019) or incorporating multi-task learning to obtain implicit representations (Lu et al., 2022). How to enable a more general knowledge transfer to automated retrieving similar characteristics across source and target cities still remains unsolved.\n\u2217Equal contribution. \u2020Corresponding author.\nRecently, pre-trained models have yielded significant breakthroughs in the fields of Natural Language Processing (NLP) (Brown et al., 2020; Vaswani et al., 2017). Prompting techniques are also introduced to reduce the gap between fine-tuning and pre-training (Brown et al., 2020). At its core, the adoption of pre-trained models embodies the fundamental principles of transfer learning, allowing the model to acquire a broad understanding of various patterns and subsequently adapt to address specific tasks. Nowadays, what\u2019s particularly noteworthy is that advanced pre-trained models no longer require laborious fine-tuning, but leverage effective prompting techniques for fast adaptation (Brown et al., 2020; Rombach et al., 2022). Such capability serves as a crucial remedy for the current limitations in spatio\u2014temporal few-shot learning, which offers the potential to enhance fine-grained transfer by general matching techniques.\nHowever, despite these remarkable advancements of pre-trained models, there remains a notable gap in developing pre-trained models tailored for spatio-temporal scenarios. This disparity can be attributed to several challenges. Firstly, NLP benefits from a shared vocabulary that can be applied across various scenarios or tasks, while urban areas across different cities are geographically disjoint, lacking common elements that enable straightforward knowledge transfer. Secondly, substantial divergence often exists in data distributions between source and target cities, leading to the potential noise or even counterproductive information in the knowledge acquired from source cities (Jin et al., 2022). Additionally, pattern divergence exists even within one city due to regions with different functions. These divergences pose challenges to the effective transfer of knowledge to a target city. In other words, the knowledge obtained from the data often contains noise and bias, making it hard to train a universal model that perfectly fits patterns with high variations in different cities. Achieving effective transfer in this context requires addressing a more complex setting of generalization across cities. The key challenge, therefore, lies in determining what transferable knowledge, analogous to the shared semantic structure in NLP, can be established for urban settings.\nIn this work, we present a generative pre-training framework for spatio-temporal few-shot learning. Instead of fitting the spatio-temporal data with a unified model, we propose a novel pre-training strategy that captures universal patterns from optimized neural network parameters. In simpler terms, we recast spatio-temporal few-shot learning as pre-training a generative hypernetwork. This hypernetwork is designed to adaptively generate unique parameters for spatio-temporal prediction models guided by prompts.\nEssentially, our pre-training approach empowers the capability to adaptively generate distinctive neural networks in response to diverse data distributions, which addresses the challenges arising from data gaps across cities or regions. To elaborate, we begin with a set of neural networks optimized for spatio-temporal predictions. Then we design a Transformer-based diffusion model to generate network parameters from Gaussian noise conditioned on the prompt. We also design a class of conditioning strategies for the prompt to guide the denoising process. Consequently, when presented with a target prompt encoding spatio-temporal characteristics of the target scenario, the diffusion model generates corresponding neural networks for accurate predictions. Our framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models. We summarize our contributions as follows:\n\u2022 We propose to leverage pre-training paradigm to achieve effective fine-grained spatio-temporal knowledge transfer across different cities, which stands as a pioneering practice in handling urban data-scarce scenarios with pretrained models.\n\u2022 We propose a novel Generative Pre-training framework based on Diffusion models, called GPD. It leverages a Transformer-based diffusion model and city-specific prompts to generate neural networks, opening new possibilities for improving spatio-temporal modeling.\n\u2022 Extensive experiments on multiple real-world scenarios demonstrate that GPD achieves superior performance towards data-scarce scenarios with an average improvement of 7.87% over the best baseline on four datasets."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Spatio-Temporal Few-Shot Learning. Addressing data scarcity is a pervasive challenge in machine learning-driven urban computing applications (Wei et al., 2016; He et al., 2020; Zhou et al., 2023a). This issue is particularly pronounced in cities lacking advanced digital infrastructure or in\nthe initial stages of deploying sensors, as they struggle to accumulate adequate data. To solve these issues, spatio-temporal few-shot learning (Zhuang et al., 2020) has emerged as a promising solution. Existing solutions can be broadly categorized into coarse-grained and fine-grained methods. Coarse-grained methods (Fang et al., 2022; Liu et al., 2018; Pang et al., 2020; He et al., 2020; Ding et al., 2019; Tang et al., 2022; Yao et al., 2019) treat each city as a whole and apply transfer learning techniques, such as adversarial learning (Fang et al., 2022) and meta-learning (Yao et al., 2019), to leverage knowledge across urban contexts. Differently, fine-grained methods (Wang et al., 2019; Guo et al., 2018; Liu et al., 2021; Jin et al., 2022) divide cities into smaller regions and identify similar region pairs for knowledge transfer. They use various techniques, such as Similarity-based matching (Wang et al., 2019) or re-weighting (Jin et al., 2022), to facilitate the transfer of insights between these regions. Fine-grained methods have demonstrated better performance due to the inherent disparities between source and target cities (Jin et al., 2022). (Lu et al., 2022) proposed to learn meta-knowledge to facilitate node-level knowledge transfer. Instead, we propose a generative pre-training framework made up of a diffusion-based hypernetwork and adaptive conditioning strategy. Our framework not only exhibits powerful parameter generation capabilities but also offers remarkable flexibility by allowing the use of various forms of prompts.\nDiffusion Models. Diffusion probabilistic models (Ho et al., 2020; Song et al., 2020; Nichol & Dhariwal, 2021) have emerged as a powerful alternative to generation tasks, which not only outperform Generative Adversarial Networks (Goodfellow et al., 2020) on image generation tasks with higher fidelity (Dhariwal & Nichol, 2021; Rombach et al., 2022), but also enable effective crossmodal conditioning (Bao et al., 2023; Nichol et al., 2022). In addition to image generation, diffusion models have also been utilized for other tasks, such as video generation (Ho et al., 2022; Luo et al., 2023), text generation (Li et al., 2022; Gong et al., 2022), implicit neural fields generation (Erkoc\u0327 et al., 2023), network learning (Peebles et al., 2022), graph generation (Vignac et al., 2023), and spatio-temporal prediction (Wen et al., 2023; Yuan et al., 2023). In contrast, our approach leverages diffusion models to generate neural network parameters of spatio-temporal prediction models.\nHypernetworks. Hypernetworks represent a class of generative models tasked with producing parameters for other neural networks (Ha et al., 2017). These hypernetworks serve two primary purposes: (1) the generation of task-specific parameters (Ha et al., 2017; Alaluf et al., 2022), where hypernetworks are usually jointly trained with specific task objectives, and (2) the exploration of neural network characteristics (Schu\u0308rholt et al., 2022; 2021), which focus on learning representations of neural network weights to gain insights of network properties. In our work, we align with the first purpose, focusing on the generation of parameters tailored to the target city for spatiotemporal prediction. There are also hypernetworks for spatio-temporal prediction (Pan et al., 2019; Bai et al., 2020; Pan et al., 2020; 2018; Li et al., 2023), which use non-shared parameters for different nodes. Our key differences lie in three aspects. Firstly, our framework ensures compatibility with state-of-the-art models, while those works propose specific model designs. Secondly, different from the pre-training solution (Li et al., 2023) with a mask autoencoder, we focus on the pre-training of model parameters. Thirdly, our approach is a spatio-temporal few-shot learning framework, capable of acquiring knowledge from multiple cities and effectively transferring it to few-shot scenarios. In contrast, those works have certain limitations in data-scarce urban environments."
        },
        {
            "heading": "3 PROPOSED METHOD",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARY",
            "text": "We introduce some preliminaries related to our research problem. Definition 1 (Spatio-Temporal Graph). A spatio-temporal graph (STG) as GST = (V, E ,A,X ), where V is the node set, E is the edge set, A is the adjacency matrix, and X denotes the node feature. The node features can encompass various aspects such as crowd flows and traffic speed.\nFor example, for the crowd flow dataset, nodes correspond to segmented regions within the city, with edges indicating their geographic adjacency. For the traffic speed dataset, nodes represent the sensors deployed along the road network, and edges signify the connections between these sensors. The construction of the spatio-temporal graph is adaptable to the specific requirements of each task. Problem 1 (Spatio-Temporal Prediction). For a spatio-temporal graph GST , suppose we have L historical signals [Xt\u2212L+1, Xt\u2212L+2, . . . , Xt] for each node and predict the future n steps\n[Xt+1, . . . , Xt+n]. The prediction task is formulated as learning a \u03b8-parameterized model F given a spatio-temporal graph GST : [Xt+1, . . . , Xt+n] = F\u03b8(Xt\u2212L+1, Xt\u2212L+2, . . . , Xt;GST ). Problem 2 (Spatio-Temporal Few-Shot Learning). Spatio-temporal few-shot learning is formulated as learning knowledgeK from source cities Csource1:P = {Csource1 , . . . , CsourceP }, and then transfer the learned knowledge K to the target city with few-shot structured data to facilitate spatio-temporal predictions.\nProblem 3. We formulate the spatio-temporal few-shot learning as pre-training a diffusion model to conditionally generates neural network parameters, and then utilizing it to generate parameters for the target city\u2019s prediction model. Suppose we have a collective of learned prediction models F = {F\u03b81 , F\u03b82 , ..., F\u03b8N } from a set of data-rich source cities Csource1:P = {Csource1 , . . . , CsourceP }, we aim to pre-train a diffusion model to generate F with source-city prompts. The optimized diffusion model serves as the learned knowledge K, which can be transferred to the target city."
        },
        {
            "heading": "3.2 OVERALL FRAMEWORK",
            "text": "Figure 1 provides an illustrative overview of our proposed framework, which contains three phases. The left part shows the preparation of model parameters, from which we can obtain a collection of optimized neural network parameters. The middle part illustrates the pre-training phase within source cities, where the diffusion model G\u03b3 is trained to generate meaningful parameters from Gaussian noise given source prompts. The right part demonstrates the how we transfer the learned diffusion model to facilitate spatio-temporal predictions in the target city. In the following subsections, we elaborate on the details of these phases."
        },
        {
            "heading": "3.3 SPATIO-TEMPORAL PREDICTION MODEL",
            "text": "Our framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models. Here we utilize well-established STG models, specifically STGCN (Yu et al., 2017) and GWN (Wu et al., 2019), and MLP-based model STID (Shao et al., 2022a) , as prediction models1. In this work, we train the prediction model (denoted as F\u03b8i ) separately for each region within a given city."
        },
        {
            "heading": "3.4 GENERATIVE PRE-TRAINING ON PARAMETER SPACE",
            "text": "Our approach is a conditional generative framework designed to directly learn from model parameters of source cities. This pre-training process enables the generation of new model parameters for target cities. Our training paradigm encompasses a three-phase approach as follows.\n1Appendix A.4 provides more details of the three models.\nPreparation of Neural Networks. As the initial step, we optimize spatio-temporal prediction models for each region within source cities and save their optimized network parameters. It\u2019s important to note that, while we use the same network architecture for different regions, each set of model parameters is uniquely optimized for its respective region. In other words, there is no parameter sharing, and each model is meticulously trained to achieve peak performance specific to its region. This optimization process can be formulated as follows:\n\u03b8i = argmin \u03b8i \u2211 t \u2225\u2225f\u03b8i(Xt\u2212L+1i , Xt\u2212L+2i , . . . , Xti ;GST )\u2212 [Xt+1, . . . , Xt+n]\u2225\u22252 , T (\u03b8) = {\u03b81, \u03b82, . . . , \u03b8M},M =\n\u2211 s=1,...,N ns,\nwhere i denotes the ith region within a source city, and \u03b8i represents parameters of its dedicated prediction model. N denotes the total number of source cities, and ns indicates the number of regions in source city s. T (\u03b8) encompasses the optimized parameters of prediction models. To optimize these parameters, we employ the standard Adam optimizer. These meticulously optimized parameters serve as the ground truth for the subsequent generative pre-training process. Algorithm 1 in Appendix A.5 illustrates the training process. Then we transform the parameters of each model into a vector-based format.\nGenerative Pre-Training of the Diffusion Model. Using the dataset comprising pre-trained parameters of prediction models and region prompts, we employ a generative model denoted as G\u03b3 to learn the process of generating model parameters in a single pass. Specifically, G\u03b3 predicts the distribution of parameters pG(\u03b8i|pi), where pi corresponds to the prompt of region i. We adopt diffusion models (Ho et al., 2020) as our generative model due to its efficacy in various generation tasks (Ho et al., 2022; Li et al., 2022; Vignac et al., 2023). Moreover, it has shown superior performance on multi-modal conditional generation (Bao et al., 2023; Nichol et al., 2022; Saharia et al., 2022).\nWe train the diffusion model to sample parameters by gradually denoising the vector from the Gaussian noise. This process is intuitively reasonable as it intriguingly mirrors the optimization journey from random initialization which is a well-established practice in existing optimizers like Adam. Our model takes two parts as the input: a noise-corrupted parameter vector \u03b8k and a region prompt p, with k representing the step in the forward diffusion process. The training objective is as follows:\nL = E\u03b80,\u03f5\u223cN (0,1),k[\u2225\u03f5\u2212 \u03f5\u03b3(\u03b8k, p, k)\u22252], (1)\nwhere \u03f5 denotes the noise to obtain \u03b8k from \u03b80. Algorithm 2 illustrates the pre-training procedure.\nSampling. After pre-training, we can generate parameters \u03b8i by querying G\u03b3 using a region prompt p for the target cities. When the region prompt in the target city closely resembles a region in multiple source cities, we can seamlessly approximate the model parameters specific to the target domain. In essence, we harness the power of the prompt to facilitate efficient knowledge transfer and precise parameter matching, which leverages the inherent similarities between regions across cities. The generation is an iterative sampling process from step k = K to k = 0, which denoises the Gaussian noise into meaningful parameters. The generation process is formulated as follows:\n\u03b8k\u22121 = 1 \u221a \u03b1k (\u03b8k \u2212 \u03b2k\u221a 1\u2212 \u03b1k \u03f5\u03b3(\u03b8 k, p, k)) + \u221a \u03c3\u03b3z, (2)\nwhere z \u223c N (0, I) for k > 1 and z = 0 for k = 1. Algorithm 3 illustrates the procedure."
        },
        {
            "heading": "3.5 ARCHITECTURE",
            "text": "The generative model is a transformer-based diffusion architecture that learns the relationships across different layers of the prediction model via effective self-attention. It has been shown that the Transformer can effectively capture relationships of each token in long sequences. We find it to be a good choice to learn inter-layer and intra-layer interactions.\nParameter Tokenizers. Spatio-temporal prediction models always exhibit complicated architectures consisting of various neural network layers, such as temporal layers, spatial layers, and spatiotemporal interaction layers. Usually, these layers are characterized by different shapes. These heterogeneous layer structures pose a challenge if we want to leverage the Transformer model to capture intricate relationships between them. To provide a clearer conceptual understanding, Table 4 and Table 5 in Appendix A.4 illustrate the layer structures of an spatio-temporal model. It is evident that different layers possess parameter tensors of diverse shapes. Therefore, it is necessary to decompose these parameters into vector-based tokens of uniform dimensions while preserving the inherent connectivity relationships within the original prediction model.\nWe achieve this by determining the greatest common divisor (denoted as g) of the parameters amount across all layers, then we perform layer segmentation by reshaping each layer into several tokens as: nm = Nm/g, where nm is the number of tokens for the layer m and Nm is the number of parameters of the layer m. After that, the tokens are connected in a sequential manner, ensuring that layers that are adjacent in the network structure also maintain adjacency within the resulting sequence. This systematic approach facilitates the effective utilization of Transformer-based architectures in handling spatio-temporal prediction models with heterogeneous layer structures.\nRegion Prompt. The selection of region prompts offers flexibility, as long as they can capture the distinctive characteristics of a specific region. Various static features, such as population, region area, function, and distribution of points of interest (POI), can be leveraged for this purpose. In this work, we utilize region prompts from two aspects: spatial and temporal domains. For the spatial prompt, we utilize node embeddings extracted from a pre-trained knowledge graph. It only utilizes relations like region adjacency and functional similarity, which are easily obtained in all cities. Simultaneously, we employ self-supervised learning techniques (Shao et al., 2022b) on the very limited time series data (three days) to derive the embedding for the temporal prompt. Appendix A.2 provides more details of the prompt design.\nDenoising Network. Figure 2 demonstrates the architecture of the denoising network, which adopts a prompt-based transformer diffusion model. After the layer segmentation, the parameters are restructured into a token sequence. In the denoising process, the transformer layers operate on the noised token sequence. In addition to the noised sequence, our transformer diffusion model also takes in the timestep k and region prompt p. We anticipate that the model is capable of generating outputs conditioned on the region prompt. To this end, we explore several conditioning approaches, which introduce small but important modifications to the standard transformer layer design. The conditioning strategies are shown in Figure 2(b) and 2(c). We also explore more conditioning strategies, such as \u201cPost-adaptive conditioning\u201d and \u201cAdaptive norm conditioning\u201d (see Appendix A.3).\n\u2022 Pre-conditioning. \u201cPre\u201d denotes that the prompt is integrated into the token sequence before being fed into self-attention layers. In this conditioning strategy, we simply add the spatial prompt and temporal prompt into a spatio-temporal prompt. Then, we uniformly add it to each token. This design allows us to leverage standard transformer layers without requiring any modifications.\n\u2022 Pre-conditioning with inductive bias. When adding the vector embeddings of k and p to the token embeddings, we introduce inductive bias: the timestep vector is added uniformly to each token, while the spatial prompt and temporal prompt are incorporated into spatial-related and temporal-related parameters, respectively. Inductive bias can be introduced flexibly based on the used spatio-temporal model architectures.\n\u2022 Pre-adaptive conditioning. In this variant, the operation on the timestep embedding remains the same. However, concerning the prompt, we treat the embeddings of p as a two-element sequence. An attention layer is introduced to realize the \u201cadaptive\u201d mechanism, dynamically determining to what extent the prompt should be added to specific token embeddings. This approach aims to empower the model to learn how to adaptively utilize the prompts, enhancing its conditioning capabilities."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets. We conduct experiments on two types of spatio-temporal prediction tasks: crowd flow prediction and traffic speed prediction. As for crowd flow prediction, we conducted experiments on three real-world datasets, including New York City, Washington, D.C., and Baltimore. Each dataset contains hourly urban flow of all regions, and we use half of the day (12 hours) as a training sample. As for traffic speed prediction, we conduct experiments on four real-world datasets, including MetaLA, PEMS-BAy, Didi-Chengdu, and Didi-Shenzhen. Their time intervals are 5min, 5min, 10min, and 10min. We use 12 points as a training sample. For both tasks, we categorized the datasets into source cities and one target city. For example, if one specific city is set as the target dataset, we assume access to only a limited quantity of data, such as three-day data (existing models usually require several months of data to train the model). The diffusion model is trained using the other source cities with rich data. This same division and training strategy is consistently applied when targeting different cities. Appendix B.1 provides more details of the used datasets.\nBaselines and Metrics. To evaluate the performance of our proposed framework, we compare it against classic models and state-of-the-art urban transfer approaches, including History Average (HA), ARIMA, RegionTrans (Wang et al., 2019), DASTNet (Tang et al., 2022), AdaRNN (Du et al., 2021), MAML (Finn et al., 2017), TPB (Liu et al., 2023), and ST-GFSL (Lu et al., 2022). We use two widely-used regression metrics: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Appendix B.2 and B.3 provide more details of baselines and implementation."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "Performance Comparison. We compare the performance of our proposed GPD with baselines on two tasks: crowd flow prediction and traffic speed prediction. In both tasks, our goal is to transfer knowledge from the source cities to the target city. Due to space limits, Table 1 only reports the comparison results for the mean error of prediction over the subsequent 6 steps for some datasets. Comprehensive evaluations, including step-wise performance on all datasets, can be found in Appendix C. Based on these results, we have these noteworthy observations:\n\u2022 Consistent Superiority. GPD consistently achieves the best performance compared with baseline approaches. When compared against the best-performing baselines (indicated by underlining in Table 1), GPD achieves an average error reduction of 4.31%, 17.1%, 2.1% and 8.17% in terms of MAE for Washington D.C., Baltimore, LA, and Chengdu. These improvements indicate that GPD enables effective knowledge transfer regarding parameter generation.\n\u2022 Competitive Long-Term Prediction. GPD exhibits its competitiveness particularly in long-term prediction scenarios, as shown in Tables in Appendix C. For example, when considering Baltimore as the target city (Appendix Table 11), compared with the state-of-the-art baseline STGFSL, GPD shows an improvement in MAE performance of up to 22.1% at the 6th step, compared to a 5.9% improvement at the 1st step. This notable trend can be attributed to the inherent design of our framework, which facilitates the transfer of long-term temporal knowledge to the target city.\nPerformance across Cities. To investigate the influence of different source cities on our experimental outcomes, we conducted a series of experiments using various source cities for the target\ncity. For instance, when considering Washington D.C. as the target city, we analyzed three distinct source city scenarios: (1) using New York City (N) as the source city, (2) utilizing Baltimore (B) as the source city, and (3) incorporating both NYC and Baltimore (N+B) as source cities. The results summarized in Table 2 indicate that incorporating data from multiple source cities offers substantial benefits for both short-term and long-term predictions across all three target cities. This suggests that our framework effectively learned useful and transferable knowledge from these source cities. In one-step predictions, the inclusion of two source cities, as opposed to just one, results in performance improvements: 8.1% and 3.2% for Washington, 1.5% and 11.4% for Baltimore, and 17.5% and 20.3% for NYC. When extending our analysis to six-step predictions, the advantages of utilizing two source cities over one are even more pronounced, with performance improvements of 16.4% and 12.0% for Washington, 14.2% and 18.1% for Baltimore, and 18.9% and 30.1% for NYC. These results emphasize the significance of leveraging data from multiple source cities for the pre-training."
        },
        {
            "heading": "4.3 FLEXIBILITY OF GPD",
            "text": "Our framework is designed to be model-agnostic, providing support for cutting-edge spatio-temporal models. To validate the versatility of our framework, we also integrated another two advanced spatio-temporal prediction models, Graph WaveNet (GWN) (Wu et al., 2019) and Spatial-Temporal Identity (STID) (Shao et al., 2022a), into our framework. Specifically, STGCN and GWN are graphbased architectures, while STID does not leverage graph neural networks.\nFigure 7 in Appendix presents a comprehensive performance comparison across the three spatiotemporal models\u2014STGCN, GWN, and STID\u2014across four datasets encompassing both crowd flow prediction and traffic speed prediction. The results reveal that STGCN and GWN exhibit nearly comparable performance, with GWN slightly outperforming. STID, given its relatively simpler design\nand potential limitations in modeling complex spatio-temporal relationships, lags slightly behind STGCN and GWN. Therefore, we implement GWN as the base model for baselines to examine the effectiveness of our framework. Appendix C.1 provides the detailed comparison results. This comparison underscores our framework\u2019s adaptability to integrate existing spatio-temporal models and its inherent potential to enhance prediction performance, particularly as more powerful models become available."
        },
        {
            "heading": "4.4 IN-DEPTH STUDY OF GPD",
            "text": "Conditioning Strategy. We investigate how the conditioning strategies affect the model\u2019s performance. Figure 3 presents the comparison results. As we can observe, inducing inductive bias to the conditioning strategy consistently leads to lower prediction errors compared to other conditioning approaches across various datasets. The second-best performing strategy is the \u201cAdaptive Sum of Prompts\u201d, which employs an attention mechanism to flexibly aggregate prompts related to different aspects for each token. This approach exhibits promise in addressing diverse spatio-temporal models in a general and flexible manner. Conversely, the conditioning strategy without self-attention operation demonstrates the poorest performance. This outcome indicates the importance of incorporating the prompt within the Transformer\u2019s self-attention layers for the generative pre-training process.\nPrompts. We conduct experiments to investigate the influence of prompt selection on the final performance. Figure 4 illustrates the comparison results of three prompt methods: the spatio-temporal prompt (ours), the spatial prompt (w/o temporal), and the temporal prompt (w/o spatial). As we can observe, eliminating either the spatial or temporal prompt leads to a reduction in the model\u2019s performance, while employing both the spatio-temporal prompt yields the most favorable performance. These findings underscore the significance of harnessing the characteristics of the target city from both spatial and temporal perspectives in the context of spatiotemporal prediction."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "GPD introduces a pioneering generative pre-training framework for spatio-temporal few-shot learning, addressing the challenges posed by data scarcity and heterogeneity in smart city applications. By conducting pre-training in the parameter space, GPD addresses the inherent disparities present in the data space across different cities and enables a new but effective knowledge transfer. Its model-agnostic nature ensures compatibility with existing urban computing models, making it a valuable tool for researchers and practitioners in the field. Our framework represents a significant advancement in urban transfer learning, which has the potential to revolutionize smart city applications in data-scarce environments and contribute to more sustainable and efficient urban development. As for future work, researchers can explore more sophisticated methods for prompt selection, such as leveraging large language models to capture the unique characteristics of cities."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported in part by the National Natural Science Foundation of China under U23B2030, U22B2057 and U20B2060, and the National Key Research and Development Program of China under grant 2020YFA0711403. This work is also supported by a grant from the Guoqiang Institute, Tsinghua University under 2021GQG1005."
        },
        {
            "heading": "A METHODOLOGY DETAILS",
            "text": ""
        },
        {
            "heading": "A.1 DIFFERENT KNOWLEDGE FOR TRANSFER",
            "text": "Figure 5 compares the difference between our framework and other urban transfer learning methods, which focus on parameter-space knowledge and data-space knowledge, respectively."
        },
        {
            "heading": "A.2 REGION PROMPTS",
            "text": "We design region prompts to leverage auxiliary data that captures city characteristics. This prompting technique facilitates to utilize external information more flexibly. In our experiments, we adopt a granular approach by crafting distinct prompts for each region within the city. These prompts are meticulously designed to encapsulate the unique characteristics of each region. To facilitate accurate spatio-temporal predictions, we create two specific types of prompts: a spatial prompt and a temporal prompt. The spatial prompt is tailored to provide an in-depth representation of the spatial attributes, encompassing geographical features, environmental conditions, and interconnections with neighboring regions. Complementing the spatial prompt, we also introduce a temporal prompt. This prompt captures the temporal dynamics of each region.\nSpatial Prompt. The spatial prompt is obtained by pre-training an urban knowledge graph (UKG) (Zhou et al., 2023b), which is meticulously designed to encapsulate the extensive environmental information within a city. Specifically, we represent urban regions as distinct entities within the UKG framework. We use relations \u201cBorderBy\u201d and \u201cNearBy\u201d to capture the spatial adjacency among regions. By leveraging this adjacency representation, we aim to capture the influence that proximate regions exert on one another. Furthermore, our UKG incorporates an understanding of the functional similarity between these urban regions. This insight is quantified by computing the cosine similarity of the distribution of Points of Interest (POI) categories between region pairs. We establish a \u201cSimilarFunc\u201d relation to establish connections between regions that exhibit functional similarity, which emphasizes the critical role played by shared functions in shaping the urban landscape.\nTo extract the spatial prompts from the constructed Urban Knowledge Graph (UKG), we employ a state-of-the-art KG embedding model, TuckER (Balaz\u030cevic\u0301 et al., 2019), to learn an embedding representation for each region. TuckER evaluates the plausibility of triplets as follows:\n\u03d5(h, r, t) =W \u00d71 eh \u00d72 er \u00d73 et, (3)\nwhereW \u2208 Rd3KG is a learnable tensor,\u00d7n denotes the tensor product along the nth dimension, and eh, er, et \u2208 RdKG are the embeddings of head entity h, tail entity t and relation r respectively. The primary objective of the KG embedding model is to maximize the scoring function for triplets that exist in the UKG, thereby preserving the knowledge contained within the UKG.\nIn summary, our UKG leverages the \u2019BorderBy\u2019 and \u2019NearBy\u2019 relationships to articulate spatial connections and the \u2019SimilarFunc\u2019 relationship to underscore functional parallels between urban regions. The benefits of UKG are twofold. First, it integrates various relationships within the city, allowing the learned embeddings of regions to provide descriptive information about their respective urban environments. Secondly, in contrast to time series data collected by sensors or GPS devices, the features utilized in the Urban Knowledge Graph (UKG) are readily available in all urban areas. This accessibility makes the UKG scalable and adaptable to cities, even those with limited development levels. The details of relations in UKG are shown in Table 3.\nTemporal Prompt. The temporal prompt is derived through a strategic application of an unsupervised pre-training model designed for time series, as introduced by Shao et al (Shao et al., 2022b). This approach shares similarities with the concept of a Masked AutoEncoder (MAE) (He et al., 2022) for sequence data. Initially, the time series data for each region is subjected to a masking procedure, where random patches within the time series are concealed. Subsequently, an encoderdecoder model is trained on this modified data to reconstruct the original time series. This training process is centered around the objective of reconstructing the complete time series based solely on the partially observable series patches. Given that time series data often exhibits lower information density, a relatively high masking ratio (75%) is employed. This higher masking ratio is crucial for creating a self-supervised learning challenge that encourages the model to capture meaningful temporal patterns from incomplete observations. Upon successful completion of the self-supervised learning phase for time series data, the output of the encoder yields the temporal embeddings. In essence, this method capitalizes on self-supervised learning to extract valuable temporal features from time series data, which can subsequently be used as temporal prompts."
        },
        {
            "heading": "A.3 CONDITIONING STRATEGIES",
            "text": "Pre-conditioning. \u201cPre\u201d denotes that the prompt is integrated into the token sequence before being fed into self-attention layers. In this method, we simply add the region prompt p to the token embeddings within the input sequence.\nPre-conditioning with inductive bias. In this variant, we adopt a different approach to add the region prompt p to the token embeddings within the input sequence. The spatial prompt is incorporated into spatial-related parameters uniformly and the temporal prompt into temporal-related parameters uniformly as follows:\n[xs,1,xs,1, \u00b7 \u00b7 \u00b7 ,xs,m] = [xs,1,xs,1, \u00b7 \u00b7 \u00b7 ,xs,m] + [ps,ps, \u00b7 \u00b7 \u00b7 ,ps\ufe38 \ufe37\ufe37 \ufe38 m ]\n[xt,1,xt,1, \u00b7 \u00b7 \u00b7 ,xt,n] = [xt,1,xt,1, \u00b7 \u00b7 \u00b7 ,xt,n] + [pt,pt, \u00b7 \u00b7 \u00b7 ,pt\ufe38 \ufe37\ufe37 \ufe38 n ], (4)\nwhere m and n represent the number of tokens for spatial parameters and temporal parameters, ps denotes the spatial prompt, and pt denotes the temporal prompt.\nPre-adaptive conditioning. In this variant, we introduce an attention mechanism, which determines to what extent the prompt should be added to specific token embeddings. We denote the prompt as p \u2208 R2\u00d7E , where E is the embedding size of spatial and temporal prompts. This approach aims to empower the model to learn how to adaptively utilize the prompts, enhancing its conditioning capabilities. The utilization of the prompt can be formulated as follows:\nuj = tanh(Wwpj + bw), j \u2208 {0, 1} (5)\n\u03b1i,j = exp(uTj , xi)\u2211 k exp(u T k , xi)\n(6)\nPi = \u2211 j \u03b1i,jpj , j \u2208 {0, 1} (7)\nwhere p0 and p1 represent the spatial prompt and temporal prompt, respectively, Pi denotes the aggregated prompt from two aspects for the ith token.\nPost-adaptive Conditioning. Figure 6 (a) illustrates this conditioning strategy. The aggregated prompt based on the attention mechanism is added after the multi-head self-attention in each transformer layer. Specifically, the query used for spatio-temporal attentive aggregation is the output of the multi-head self-attention layer.\nAdaptive norm conditioning. Figure 6 (b) illustrates this conditioning strategy. The aggregated prompt based on the attention mechanism is used for re-scaling the output in each layer norm."
        },
        {
            "heading": "A.4 NETWORK LAYERS OF SPATIO-TEMPORAL PREDICTION MODELS",
            "text": "We provide details of parameter tokenizers introduced in Section 3.5. In our experiments, we implement our framework on three spatio-temporal prediction models, STGCN (Yu et al., 2017), GWN (Wu et al., 2019), and STID (Shao et al., 2022a). We present how to transform their network layers into a vector-based token sequence. According to Table 4 and Table 5, the transformation from parameter layers to a token sequence can be formulated as follows:\ngcd = GCD(numel(s1), numel(s2), . . . , numel(sm))\nLi = ci \u2217 numel(si)/gcd L = \u2211 i Li, (8)\nwhere GCD denotes the calculation of the Greatest Common Divisor, m denotes the number of different layer types, si denotes the shape, numel denotes the total number of elements in the tensor\nci denotes the count of this type of layer. In this way, we obtain a token sequence with length as L and embedding size as gcd.\nSTGCN (Yu et al., 2017). Spatio-temporal graph convolution network. Different from regular convolutional and recurrent units, this model build convolutional structures on graphs. We use a 3-layer STGCN block, and utilize a 1-layer MLP as the output predictor. Table 4 shows the detailed network layers of a STGCN.\nGWN (Wu et al., 2019). Graph WaveNet. This model developed a novel adaptive dependency matrix and learned it through node embeddings to capture the spatial dependency. It also combines with a stacked dilated causal convolution component. We use a 2-layer 4-block GWN model. Table 4 shows the detailed network layers of a GWN.\nSTID (Shao et al., 2022a). Spatio-Temporal Identity. This model is a simple Multi-Layer Perceptrons (MLPs)-based approach. By identifying the indistinguishability of samples in both spatial and temporal dimensions, it is simple yet effective for spatio-temporal prediction."
        },
        {
            "heading": "A.5 ALGORITHMS",
            "text": "We present the training algorithm for spatio-temporal graph prediction in Algorithm 1. Besides, we present the training algorithm and sampling algorithm for the diffusion model in Algorithm 2 and Algorithm 3, respectively."
        },
        {
            "heading": "B EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 DATASETS",
            "text": "In this section, we introduce the details of the used real-world datasets.\nAlgorithm 1 Model Parameter Preparation 1: Input: Dataset D = {D1, D2, . . . , DMs}, neural networks F = {f\u03b81 , f\u03b82 , . . . , f\u03b8Ms } of the\nspatio-temporal prediction model, loss function L, parameter storage S. 2: Output: Parameter storage S. 3: Initialize: Learnable parameters \u03b8m for fm, parameter storage S = {}. 4: for m \u2208 {1, 2, . . . ,Ms} do 5: for epoch \u2208 {1, 2, . . . , Niter} do 6: Sample a mini-batch of inputs and labels from the dataset Dm {x, y} \u223c Dm 7: Compute the predictions y\u0302 \u2190 f\u03b8m(x) 8: Compute the loss L \u2190 L(y\u0302, y) 9: Update the model\u2019s parameters \u03b8m \u2190 update(L; \u03b8m)\n10: end for 11: Save the optimized model S \u2190 S \u222a {\u03b8m} 12: end for\nAlgorithm 2 Generative Pre-training of the Diffusion Model 1: Input: Parameter storage S, diffusion model G. 2: Initialize: Learnable parameters \u03b3 for G 3: for epoch \u2208 {1, 2, . . . , Niter} do 4: Sample a parameter sample \u03b80 \u223c q(\u03b80) and \u03f5 \u223c N (0, I) 5: Sample diffusion step k \u223c Uniform(1, . . . ,K) 6: Take gradient descent step on\n\u2207\u03b3\u2225\u03f5\u2212 \u03f5\u03b3( \u221a \u03b1k\u03b80 + \u221a 1\u2212 \u03b1k\u03f5, p, k)\u22252\n7: end for\nAlgorithm 3 Parameter Sampling 1: Input: Gaussian noise \u03b8K \u223c N (0, I), prompts P = {p1, p2, . . . ,M} 2: Output: Parameters \u03b8t = {\u03b81,0, \u03b82,0, . . . , \u03b8Mt,0}. 3: Initialize: Learnable parameters \u03b3 for G, \u03b8s = {}. 4: for m \u2208 {1, 2, . . . ,Mt} do 5: for k = K \u2192 1 do 6: if K > 1 then 7: z \u223c N (0, I) 8: else 9: z = 0 10: end if 11:\n\u03b8m,k\u22121 = 1 \u221a \u03b1k (\u03b8m,k \u2212 \u03b2k\u221a 1\u2212 \u03b1k\n\u03f5\u03b3(\u03b8m,k, p, k)) + \u221a \u03b2kz\n12: end for 13: \u03b8s = \u03b8s \u222a \u03b8m,0 14: end for"
        },
        {
            "heading": "Datasets METR-LA PEMS-BAY Didi-Chengdu Didi-Shenzhen",
            "text": ""
        },
        {
            "heading": "B.1.1 CROWD FLOW PREDICTION.",
            "text": "\u2022 NYC Dataset. In this dataset, we define regions as census tracts and aggregate taxi trips from NYC Open Data to derive hourly inflow and outflow information. The division of train and test regions is based on community districts. Specifically, the Manhattan borough comprises 12 community districts, and we designate 9 of them as train regions, reserving the remaining 3 for testing. Region-specific features encompass the count of Points of Interest (POIs), area, and population.\n\u2022 Washington, D.C. Dataset. Regions in this dataset are defined as census tracts, and inflow data is calculated based on Point of Interest (POI)-level hourly visits. The partitioning of train and test regions is done by counties. Specifically, regions within the District of Columbia are selected as train regions, and regions in Arlington County are designated as test regions. This dataset comprises rich region features, including demographics and socioeconomic indicators.\n\u2022 Baltimore Dataset. Similar to the D.C. dataset, regions, and inflow data in the Baltimore dataset are obtained in the same manner. Train regions consist of regions in Baltimore City, while test regions encompass Baltimore County. This dataset includes the same set of features as the D.C. dataset."
        },
        {
            "heading": "B.1.2 TRAFFIC SPEED PREDICTION.",
            "text": "We conducted our performance evaluation using four real-world traffic speed datasets, following the data preprocessing procedures established in prior literature (Li et al., 2018; Lu et al., 2022). To construct the spatio-temporal graph, we treated each traffic sensor or road segment as an individual vertex within the graph. We then computed pairwise road network distances between these sensors. Finally, we construct the adjacency matrix of the nodes using road network distances and a thresholded Gaussian kernel.\n\u2022 METR-LA (Li et al., 2018; Lu et al., 2022). The traffic data in our study were obtained from observation sensors situated along the highways of Los Angeles County. We utilized a total of 207 sensors, and the dataset covered a span of four months, ranging from March 1, 2012, to June 30, 2012. To facilitate our analysis, the sensor readings were aggregated into 5-minute intervals.\n\u2022 PEMS-BAY (Li et al., 2018; Lu et al., 2022). The PEMS-BAY dataset comprises traffic data collected over a period of six months, from January 1st, 2017, to June 30th, 2017, within the Bay Area. The dataset is composed of records from 325 traffic sensors strategically positioned throughout the region.\n\u2022 Didi-Chengdu (Lu et al., 2022). We utilized the Traffic Index dataset for Chengdu, China, which was generously provided by the Didi Chuxing GAIA Initiative. Our dataset selection encompassed the period from January to April 2018 and covered 524 roads situated within the central urban area of Chengdu. The data was collected at 10-minute intervals to facilitate our analysis.\n\u2022 Didi-Shenzhen (Lu et al., 2022). We utilized the Traffic Index dataset for Shenzhen, China, which was generously provided by the Didi Chuxing GAIA Initiative. Our dataset selection included data from January to April 2018 and encompassed 627 roads located in the downtown area of Shenzhen. The data collection was conducted at 10-minute intervals to facilitate our analysis."
        },
        {
            "heading": "B.2 BASELINES",
            "text": "\u2022 HA. Historical average approach models time series as a seasonal process and leverages the average of previous seasons for predictions. In this method, we utilize a limited set of target city data to compute the daily average value for each node. This historical average then serves as the baseline for predicting future values.\n\u2022 ARIMA. Auto-regressive Integrated Moving Average model is a widely recognized method for comprehending and forecasting future values within a time series.\n\u2022 RegionTrans (Wang et al., 2019). RegionTrans assesses the similarity between source and target nodes, employing it as a means to regulate the fine-tuning of the target. We use STGCN and GWN as its base model.\n\u2022 DASTNet (Tang et al., 2022). Domain Adversarial Spatial-Temporal Network, which undergoes pre-training on data from multiple source networks and then proceeds to fine-tune using the data specific to the target network\u2019s traffic.\n\u2022 AdaRNN (Du et al., 2021). This cutting-edge transfer learning framework is designed for nonstationary time series data. The primary objective of this model is to mitigate the distribution disparity within time series data, enabling the training of an adaptive model based on recurrent neural networks (RNNs).\n\u2022 MAML (Finn et al., 2017). Model-Agnostic Meta Learning is an advanced meta-learning technique designed to train a model\u2019s parameters in a way that a minimal number of gradient updates result in rapid learning on a novel task. MAML achieves this by acquiring an improved initialization model through the utilization of multiple tasks to guide the learning process of the target task.\n\u2022 TPB (Liu et al., 2023). Traffic Pattern Bank-based approach. TPB employs a pre-trained traffic patch encoder to transform raw traffic data from cities with rich data into a high-dimensional space. In this space, a traffic pattern bank is established through clustering. Subsequently, the traffic data originating from cities with limited data availability can access and interact with the traffic pattern bank to establish explicit relationships between them.\n\u2022 ST-GFSL (Lu et al., 2022). ST-GFSL generates node-specific parameters based on node-level meta-knowledge drawn from the graph-based traffic data. This approach ensures that parameters are tailored to individual nodes, promoting parameter similarity among nodes that exhibit similarity in the traffic data.\nB.3 IMPLEMENTATION DETAILS\nIn the experiments, we set the number of diffusion steps N=500. The learning rate is set to 8e-5 and the number of training epochs ranges from 3000 to 12000. The dimensions of KG embedding and time embedding are both 128. Regarding the spatio-temporal prediction, we use 12 historical time steps to predict 6 future time steps. Our framework can be effectively trained within 3 hours and all experiments were completed on one NVIDIA GeForce RTX 4090."
        },
        {
            "heading": "C ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 FLEXIBILITY OF GPD",
            "text": "To demonstrate the flexibility of our framework, we compare the performance with GWN as the base model. Figure 8 to Figure 10 illustrate the comparison results. As we can observe, our frame-\nwork still achieves the best performance on all datasets. The advantage of our framework is more significant for long-step predictions."
        },
        {
            "heading": "C.2 TIME CONSUMPTION ANALYSIS",
            "text": "Table 9 present a detailed comparison of the computational cost of our proposed model against baselines. Our model demonstrates efficient training, completing within 3 hours on a single GPU (RTX-2080Ti), which we believe is an acceptable time consumption. It is important to note that the slightly longer time consumption in our model is attributed to the step-by-step denoising process implemented in DDPM approach (Ho et al., 2020).\nWe choose DDPM because it is a classic, simple but effective diffusion framework. Notably, our framework is designed to be compatible with more efficient diffusion models, such as DDIM (Song et al., 2020). Employing these models has the potential to significantly reduce the current computational cost. We want to underscore that our framework has the ability to achieve remarkably better performance with a relatively lower overhead."
        },
        {
            "heading": "C.3 FINE-GRAINED PERFORMANCE ANALYSIS",
            "text": "We conducted fine-grained evaluations to discern the types of regions in target cities that benefit more from knowledge transfer in our pretraining framework. For this analysis, we selected Baltimore as the target city, while Washington D.C. and NYC served as source cities. To initiate our investigation, we analyzed the performance differences across various regions within Baltimore. The distribution of performance across these regions is visualized in Figure 11. This observation of varied performance across different regions prompted us to explore the factors contributing to this performance variance, aiming to gain insights into more effective knowledge transfer strategies.\nIn our analysis, we employed clustering on the time series data from the source cities. Figure 12 provides a visual representation of the time series data through Principal Component Analysis (PCA). Simultaneously, we conducted an in-depth analysis focusing on regions within the target city. Specifically, we selected two regions with relatively superior performance (MAE = 2.53 and 2.33) and two regions with comparatively lower performance (MAE = 30.2, 31.8). As illustrated in Figure 12, regions with better performance (depicted in red as the \u201dgood case\u201d) seamlessly align with one of the clusters identified in the source cities. In contrast, regions with lower performance (depicted in blue as the \u201dbad case\u201d) are situated farther from the clustering centers, struggling to align with any specific clusters. This insightful observation aligns with the intuitive notion that regions sharing similar patterns are more receptive to knowledge transfer within our pretraining framework. Consequently, leveraging more diverse cities as source cities for pretraining the generative framework promises to improve overall performance. This insight is consistent with the principles observed\nin Large Language Models, which often exhibit enhanced performance when trained on a highly diverse dataset (Gao et al., 2020)."
        },
        {
            "heading": "C.4 SYNTHETIC DATA EXPERIMENTS",
            "text": "The effectiveness of our approach is grounded in two key hypotheses: Assumption 1. Regions with distinct spatio-temporal patterns correspond to different optimized model parameters. This relationship can be mathematically expressed as follows:\nP \u2217i \u0338= P \u2217j if D(Xi, Xj) > \u03f5, (9) where P \u2217i represents the optimized model parameters for region i, Xi denotes the spatio-temporal pattern of the region i, D signifies the dissimilarity metric between the patterns of two regions. Assumption 2. There exists a mapping relation f\u03b8 between the conditions from each region Ci and the optimized model parameters P \u2217i .\nPi = f\u03b8(Ci) where \u03b8 = argmin \u03b8 \u2211 i Distance(P \u2217i , f\u03b8(Xi)) (10)\nwhere Pi is the generated model parameters.\nTo empirically validate these hypotheses, we conducted experiments on two synthetic datasets, allowing us to manipulate pattern similarities between nodes. This design enables an investigation into the framework\u2019s ability to generate effective parameters. The experimental setup involves two cities: one designated as the source city, and the other as the target city, each comprising 200 regions. We first construct a graph by randomly adding edges between regions. Then, for each region, we generated a time series following specific patterns while incorporating random noise. This approach ensured that regions have distinct spatio-temporal patterns.\nIn Figure 13, we present the parameter maps of well-trained models for three regions in the source city. Notably, regions A (Figure 13(b)) and B (Figure 13(d)) exhibit highly similar time series patterns, whereas region C (Figure 13(f)) displays distinctly different patterns. In the meantime, the\nsymmetrical positions of nodes A and B in the graph (depicted in Figure 15) suggest that regions A and B have very similar spatial patterns. Therefore, we can assume that regions A and B have very similar spatio-temporal patterns. Consequently, the parameter maps for regions A and B showcase similar distributions, diverging from the parameter map of region C. This observation underscores the necessity of employing non-shared parameters for regions to effectively adapt to diverse spatiotemporal patterns.\nMoving on to the parameter generation results for the target city, we selected two regions labeled as M and N, characterized by similar time series patterns and spatial connection locations (see Figure 16). Figure 14 illustrates the comparison results. Specifically, Figures 14(a) and 14(c) depict the ground truth parameter maps for these two regions, revealing similar distribution patterns. Figures 14(b) and 14(d) illustrate the generated parameter maps for the same regions. Notably, the generated parameter maps closely align with their corresponding ground-truth distributions. This comprehensive analysis of parameter generation, presented in Figures 13, Figures 14, Figure 15, and Figure 16, collectively validates the capability of our framework in effectively generating parameters that capture diverse spatio-temporal patterns across different regions."
        },
        {
            "heading": "C.5 PREDICTION RESULTS OF THE REMAINING CITIES",
            "text": "Table 10 to Table 14 show the prediction results of other datasets."
        }
    ],
    "title": "SIVE NEURAL NETWORK GENERATION",
    "year": 2024
}