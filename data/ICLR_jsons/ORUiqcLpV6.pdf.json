{
    "abstractText": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
    "authors": [],
    "id": "SP:510d715192192321e6c369039c0194a89698b42b",
    "references": [
        {
            "authors": [
                "Ahmed Abdelreheem",
                "Kyle Olszewski",
                "Hsin-Ying Lee",
                "Peter Wonka",
                "Panos Achlioptas"
            ],
            "title": "Scanents3d: Exploiting phrase-to-3d-object correspondences for improved visio-linguistic models in 3d scenes",
            "venue": "arXiv preprint arXiv:2212.06250,",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed Abdelreheem",
                "Ujjwal Upadhyay",
                "Ivan Skorokhodov",
                "Rawan Al Yahya",
                "Jun Chen",
                "Mohamed Elhoseiny"
            ],
            "title": "3dreftransformer: fine-grained object identification in real-world scenes using natural language",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3941\u20133950,",
            "year": 2022
        },
        {
            "authors": [
                "Panos Achlioptas",
                "Ahmed Abdelreheem",
                "Fei Xia",
                "Mohamed Elhoseiny",
                "Leonidas Guibas"
            ],
            "title": "Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Eslam Bakr",
                "Yasmeen Alsaedy",
                "Mohamed Elhoseiny"
            ],
            "title": "Look around and refer: 2d synthetic semantics knowledge distillation for 3d visual grounding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sutskever",
                "Dario Amodei"
            ],
            "title": "Language Models are Few-Shot Learners, July 2020",
            "venue": "URL http://arxiv.org/abs/2005.14165. arXiv:2005.14165 [cs]",
            "year": 2005
        },
        {
            "authors": [
                "Dave Zhenyu Chen",
                "Angel X Chang",
                "Matthias Nie\u00dfner"
            ],
            "title": "Scanrefer: 3d object localization in rgb-d scans using natural language",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Lang Chen",
                "Matthew A Lambon Ralph",
                "Timothy T Rogers"
            ],
            "title": "A unified model of human semantic knowledge and its disorders",
            "venue": "Nature human behaviour,",
            "year": 2017
        },
        {
            "authors": [
                "Shizhe Chen",
                "Pierre-Louis Guhur",
                "Makarand Tapaswi",
                "Cordelia Schmid",
                "Ivan Laptev"
            ],
            "title": "Language conditioned spatial relation reasoning for 3d object grounding",
            "venue": "arXiv preprint arXiv:2211.09646,",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Crowston"
            ],
            "title": "Amazon mechanical turk: A research tool for organizations and information systems scholars",
            "venue": "Shaping the Future of ICT Research. Methods and Approaches,",
            "year": 2012
        },
        {
            "authors": [
                "Yaodong Cui",
                "Ren Chen",
                "Wenbo Chu",
                "Long Chen",
                "Daxin Tian",
                "Ying Li",
                "Dongpu Cao"
            ],
            "title": "Deep learning for image and point cloud fusion in autonomous driving: A review",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X. Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas Funkhouser",
                "Matthias Nie\u00dfner"
            ],
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes, 2017a",
            "year": 2017
        },
        {
            "authors": [
                "Bo Dai",
                "Yuqi Zhang",
                "Dahua Lin"
            ],
            "title": "Detecting visual relationships with deep relational networks, 2017b",
            "year": 2017
        },
        {
            "authors": [
                "Prithiviraj Damodaran"
            ],
            "title": "Parrot: Paraphrase generation for nlu., 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jiajun Deng",
                "Zhengyuan Yang",
                "Tianlang Chen",
                "Wengang Zhou",
                "Houqiang Li"
            ],
            "title": "Transvg: End-toend visual grounding with transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Mingtao Feng",
                "Zhen Li",
                "Qi Li",
                "Liang Zhang",
                "XiangDong Zhang",
                "Guangming Zhu",
                "Hui Zhang",
                "Yaonan Wang",
                "Ajmal Mian"
            ],
            "title": "Free-form description guided 3d visual graph network for object grounding in point cloud",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Weixi Feng",
                "Xuehai He",
                "Tsu-Jui Fu",
                "Varun Jampani",
                "Arjun Akula",
                "Pradyumna Narayana",
                "Sugato Basu",
                "Xin Eric Wang",
                "William Yang Wang"
            ],
            "title": "Training-free structured diffusion guidance for compositional text-to-image synthesis, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yalda Ghasemi",
                "Heejin Jeong",
                "Sung Ho Choi",
                "Kyeong-Beom Park",
                "Jae Yeol Lee"
            ],
            "title": "Deep learningbased object detection in augmented reality: A systematic review",
            "venue": "Computers in Industry,",
            "year": 2022
        },
        {
            "authors": [
                "Dailan He",
                "Yusheng Zhao",
                "Junyu Luo",
                "Tianrui Hui",
                "Shaofei Huang",
                "Aixi Zhang",
                "Si Liu"
            ],
            "title": "Transrefer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Joy Hsu",
                "Jiayuan Mao",
                "Jiajun Wu"
            ],
            "title": "Ns3d: Neuro-symbolic grounding of 3d objects and relations",
            "venue": "arXiv preprint arXiv:2303.13483,",
            "year": 2023
        },
        {
            "authors": [
                "Pin-Hao Huang",
                "Han-Hung Lee",
                "Hwann-Tzong Chen",
                "Tyng-Luh Liu"
            ],
            "title": "Text-guided graph neural networks for referring 3d instance segmentation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Shijia Huang",
                "Yilun Chen",
                "Jiaya Jia",
                "Liwei Wang"
            ],
            "title": "Multi-view transformer for 3d visual grounding",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Hutsebaut-Buysse",
                "Kevin Mets",
                "Steven Latr\u00e9"
            ],
            "title": "Hierarchical reinforcement learning: A survey and open research challenges",
            "venue": "Machine Learning and Knowledge Extraction,",
            "year": 2022
        },
        {
            "authors": [
                "Ayush Jain",
                "Nikolaos Gkanatsios",
                "Ishita Mediratta",
                "Katerina Fragkiadaki"
            ],
            "title": "Looking outside the box to ground language in 3d scenes",
            "venue": "arXiv preprint arXiv:2112.08879,",
            "year": 2021
        },
        {
            "authors": [
                "Ayush Jain",
                "Nikolaos Gkanatsios",
                "Ishita Mediratta",
                "Katerina Fragkiadaki"
            ],
            "title": "Bottom up top down detection transformers for language grounding in images and point clouds",
            "venue": "In Computer Vision\u2013 ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Jang",
                "Sudheendra Vijayanarasimhan",
                "Peter Pastor",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "End-to-end learning of semantic grasping",
            "venue": "arXiv preprint arXiv:1707.01932,",
            "year": 2017
        },
        {
            "authors": [
                "Zhiwei Jia",
                "Fangchen Liu",
                "Vineet Thumuluri",
                "Linghao Chen",
                "Zhiao Huang",
                "Hao Su"
            ],
            "title": "Chain-ofthought predictive control",
            "venue": "arXiv preprint arXiv:2304.00776,",
            "year": 2023
        },
        {
            "authors": [
                "Yong Jiang",
                "Wenjuan Han",
                "Kewei Tu"
            ],
            "title": "Unsupervised neural dependency parsing",
            "venue": "pp. 763\u2013771,",
            "year": 2016
        },
        {
            "authors": [
                "Justin Johnson",
                "Agrim Gupta",
                "Li Fei-Fei"
            ],
            "title": "Image generation from scene graphs, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Mannat Singh",
                "Yann LeCun",
                "Ishan Misra",
                "Gabriel Synnaeve",
                "Nicolas Carion"
            ],
            "title": "MDETR - modulated detection for end-to-end multi-modal understanding",
            "venue": "CoRR, abs/2104.12763,",
            "year": 2021
        },
        {
            "authors": [
                "Haresh Karnan",
                "Garrett Warnell",
                "Xuesu Xiao",
                "Peter Stone"
            ],
            "title": "Voila: Visual-observation-only imitation learning for autonomous navigation",
            "venue": "In 2022 International Conference on Robotics and Automation (ICRA),",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Yikang Li",
                "Wanli Ouyang",
                "Xiaogang Wang",
                "Xiao\u2019ou Tang"
            ],
            "title": "Vip-cnn: Visual phrase guided convolutional neural network, 2017a",
            "year": 2017
        },
        {
            "authors": [
                "Yikang Li",
                "Wanli Ouyang",
                "Bolei Zhou",
                "Kun Wang",
                "Xiaogang Wang"
            ],
            "title": "Scene graph generation from objects, phrases and region captions, 2017b",
            "year": 2017
        },
        {
            "authors": [
                "Luyang Liu",
                "Hongyu Li",
                "Marco Gruteser"
            ],
            "title": "Edge assisted real-time object detection for mobile augmented reality",
            "venue": "In The 25th annual international conference on mobile computing and networking,",
            "year": 2019
        },
        {
            "authors": [
                "Chao Lou",
                "Wenjuan Han",
                "Yuhuan Lin",
                "Zilong Zheng"
            ],
            "title": "Unsupervised vision-language parsing: Seamlessly bridging visual scene graphs with language structures via dependency relationships, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Junyu Luo",
                "Jiahui Fu",
                "Xianghao Kong",
                "Chen Gao",
                "Haibing Ren",
                "Hao Shen",
                "Huaxia Xia",
                "Si Liu"
            ],
            "title": "3d-sps: Single-stage 3d visual grounding via referred point progressive selection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Qing Lyu",
                "Shreya Havaldar",
                "Adam Stein",
                "Li Zhang",
                "Delip Rao",
                "Eric Wong",
                "Marianna Apidianaki",
                "Chris Callison-Burch"
            ],
            "title": "Faithful chain-of-thought reasoning",
            "venue": "arXiv preprint arXiv:2301.13379,",
            "year": 2023
        },
        {
            "authors": [
                "Aman Madaan",
                "Amir Yazdanbakhsh"
            ],
            "title": "Text and patterns: For effective chain of thought, it takes two to tango",
            "venue": "arXiv preprint arXiv:2209.07686,",
            "year": 2022
        },
        {
            "authors": [
                "Jennifer C McVay",
                "Michael J Kane"
            ],
            "title": "Conducting the train of thought: working memory capacity, goal neglect, and mind wandering in an executive-control task",
            "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,",
            "year": 2009
        },
        {
            "authors": [
                "Khanh Nguyen",
                "Debadeepta Dey",
                "Chris Brockett",
                "Bill Dolan"
            ],
            "title": "Vision-based navigation with language-based assistance via imitation learning with indirect intervention",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Katherine Osborne-Crowley"
            ],
            "title": "Social cognition in the real world: reconnecting the study of social cognition with social reality",
            "venue": "Review of General Psychology,",
            "year": 2020
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "Kyeong-Beom Park",
                "Minseok Kim",
                "Sung Ho Choi",
                "Jae Yeol Lee"
            ],
            "title": "Deep learning-based smart task assistance in wearable augmented reality",
            "venue": "Robotics and Computer-Integrated Manufacturing,",
            "year": 2020
        },
        {
            "authors": [
                "Xavier Puig",
                "Kevin Ra",
                "Marko Boben",
                "Jiaman Li",
                "Tingwu Wang",
                "Sanja Fidler",
                "Antonio Torralba"
            ],
            "title": "Virtualhome: Simulating household activities via programs",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Rui Qian",
                "Xin Lai",
                "Xirong Li"
            ],
            "title": "3d object detection for autonomous driving: a survey",
            "venue": "Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "URL http://arxiv.org/abs/1910.10683",
            "year": 1910
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084,",
            "year": 2019
        },
        {
            "authors": [
                "Junha Roh",
                "Karthik Desingh",
                "Ali Farhadi",
                "Dieter Fox"
            ],
            "title": "Languagerefer: Spatial-language model for 3d visual grounding",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salzmann",
                "Boris Ivanovic",
                "Punarjay Chakravarty",
                "Marco Pavone"
            ],
            "title": "Trajectron++: Dynamicallyfeasible trajectory forecasting with heterogeneous data",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Schuster",
                "Ranjay Krishna",
                "Angel Chang",
                "Li Fei-Fei",
                "Christopher D. Manning"
            ],
            "title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval",
            "venue": "In Proceedings of the Fourth Workshop on Vision and Language,",
            "year": 2015
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Wang",
                "Yongbin Sun",
                "Ziwei Liu",
                "Sanjay E. Sarma",
                "Michael M. Bronstein",
                "Justin M. Solomon"
            ],
            "title": "Dynamic graph cnn for learning on point clouds, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Erik Wijmans",
                "Samyak Datta",
                "Oleksandr Maksymets",
                "Abhishek Das",
                "Georgia Gkioxari",
                "Stefan Lee",
                "Irfan Essa",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Embodied question answering in photorealistic environments with point cloud perception",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Wu",
                "Jiayuan Mao",
                "Yufeng Zhang",
                "Yuning Jiang",
                "Lei Li",
                "Weiwei Sun",
                "Wei-Ying Ma"
            ],
            "title": "Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Yanmin Wu",
                "Xinhua Cheng",
                "Renrui Zhang",
                "Zesen Cheng",
                "Jian Zhang"
            ],
            "title": "Eda: Explicit text-decoupling and dense alignment for 3d visual and language learning",
            "venue": "arXiv preprint arXiv:2209.14941,",
            "year": 2022
        },
        {
            "authors": [
                "Danfei Xu",
                "Yuke Zhu",
                "Christopher B. Choy",
                "Li Fei-Fei"
            ],
            "title": "Scene graph generation by iterative message",
            "year": 2017
        },
        {
            "authors": [
                "Jianwei Yang",
                "Jiasen Lu",
                "Stefan Lee",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "Graph r-cnn for scene graph generation, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Mengjiao Sherry Yang",
                "Dale Schuurmans",
                "Pieter Abbeel",
                "Ofir Nachum"
            ],
            "title": "Chain of thought imitation with procedure cloning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Songyang Zhang",
                "Liwei Wang",
                "Jiebo Luo"
            ],
            "title": "Sat: 2d semantics assisted training for 3d visual grounding",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhihao Yuan",
                "Xu Yan",
                "Yinghong Liao",
                "Ruimao Zhang",
                "Sheng Wang",
                "Zhen Li",
                "Shuguang Cui"
            ],
            "title": "Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhihao Yuan",
                "Xu Yan",
                "Zhuo Li",
                "Xuhao Li",
                "Yao Guo",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "title": "Toward explainable and fine-grained 3d grounding through referring textual phrases",
            "venue": "arXiv preprint arXiv:2207.01821,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola"
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2302.00923,",
            "year": 2023
        },
        {
            "authors": [
                "Lichen Zhao",
                "Daigang Cai",
                "Lu Sheng",
                "Dong Xu"
            ],
            "title": "3dvg-transformer: Relation modeling for visual grounding on point clouds",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong",
                "Yifan Du",
                "Chen Yang",
                "Yushuo Chen",
                "Zhipeng Chen",
                "Jinhao Jiang",
                "Ruiyang Ren",
                "Yifan Li",
                "Xinyu Tang",
                "Zikang Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen"
            ],
            "title": "A Survey of Large Language Models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Zhao"
            ],
            "title": "2021) achieved remarkable performance by applying self-attention and cross-attention on input features. While methods such as BEAUTY-DETR (Jain et al., 2022) and (Luo et al., 2022) adopt a single-stage approach, most works follow the two-stage approach with pre-detected object proposals. LAR (Bakr et al., 2022), and SAT (Yang et al., 2021) use a multi-modal approach by introducing 2D images of the scenes to aid object grounding",
            "venue": "(He et al.,",
            "year": 2021
        },
        {
            "authors": [
                "2022 Lou et al",
                "2017 Xu et al",
                "2019 Wu et al",
                "2016 Jiang et al",
                "2018 Yang et al",
                "Schuster"
            ],
            "title": "Scene graphs have been used in scene understanding to enhance image captioning and predicting pairwise relations between detected objects (Li et al., 2017b; Dai et al., 2017b; Li et al., 2017a). Other works have leveraged graphs to enhance text-to-image generation models by learning the relations between the objects in the sentence (Feng",
            "year": 2015
        },
        {
            "authors": [
                "Johnson"
            ],
            "title": "Existing off-the-shelf tools (Schuster et al., 2015) can parse the language description grammatically into a scene graph with a dependency tree (Wu et al., 2022)",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nThe 3D visual grounding task involves identifying and localizing objects in a 3D scene based on a natural language description or query. This task is crucial for many applications, such as robotics (Nguyen et al., 2019; Karnan et al., 2022; Wijmans et al., 2019), virtual reality (Puig et al., 2018; Ghasemi et al., 2022; Park et al., 2020; Osborne-Crowley, 2020; Liu et al., 2019), and autonomous driving (Qian et al., 2022; Cui et al., 2021; Jang et al., 2017; Deng et al., 2021). The goal is to enable machines to understand natural language and interpret it in the context of a 3D environment. Although 3D visual grounding has significantly advanced, current solutions cannot imitate the human perception system nor be interpretable. To address this gap, we propose a Chain-of-Thoughts 3D visual grounding framework, termed CoT3DRef. One of the biggest challenges in machine learning is understanding how the model arrives at its decisions. Thus, the concept of Chain-of-Thoughts (CoT)\ncomes in. Although CoT is widely applied in Natural Language Processing (NLP) applications (Wei et al., 2022; Chowdhery et al., 2022; Lyu et al., 2023; Wang et al., 2022; Zhang et al., 2023; Madaan & Yazdanbakhsh, 2022), it is less explored in vision applications. Understanding the CoT is crucial for several reasons. Firstly, it helps explain how the model arrived at its decision, which is essential for transparency and interpretability. Secondly, it helps identify potential biases or errors in the model, which can be addressed to improve its accuracy and reliability. Third, it is a critical step toward intelligent systems that mimic human perception. Similar to machine learning models, our perception\nsystem can be thought of as a Chain-of-Thoughts (McVay & Kane, 2009; Chen et al., 2017) - a series of intermediate steps that enable us to arrive at our final perception of the world.\nIn this paper, we mainly answer the following question: Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system? To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2seq) task. The input sequence combines 3D objects from the input scene and an input utterance describing a specific object. On the output side, in contrast to the existing 3D visual grounding architectures, we predict the target object and a chain of anchors in a causal manner. This chain of anchors is based on the logical sequence of steps a human will follow to reach the target. For instance, in Figure 1, to reach the chair target, we first have to localize the white and red boxes, then the bookshelf. By imitating the human learning process, we can devise a transparent and interpretable 3D framework that details the model\u2019s steps until localizing the target. To show that our proposed framework can be easily integrated into any existing architecture, we incorporated it into four different baselines: LAR (Bakr et al., 2022), SAT (Yang et al., 2021), MVT (Huang et al., 2022), and ViL (Chen et al., 2022). CoT3DRef achieves state-of-the-art results on Sr3D, Nr3D (Achlioptas et al., 2020), and ScanRefer (Chen et al., 2020) without requiring additional manual annotations by devising an efficient pseudo-label generator to provide inexpensive guidance to improve learning efficiency. Whereas it boosts the performance by 3.6%, 4%, 5%, 0.5% on Nr3D and 10%, 11%, 9%, 1% on Sr3D, respectively. Proper design of such an approach is pivotal in attaining a significant performance gain while circumventing the need for intensive human annotations. A pertinent example can be drawn from the labeling procedure employed in PhraseRefer (Yuan et al., 2022), which demanded a cumulative workforce commitment of 3664 hours, roughly equivalent to an extensive five-month timespan. Moreover, using additional manual annotations on the Nr3D dataset led to a noteworthy enhancement in referring accuracy, boosting the performance by 9% compared to the baselines: LAR, SAT, and MVT, respectively. Consequently, using ScanRefer, our approach surpasses SAT and MVT by 6.5% and 6.8%, respectively. In addition, as depicted in Figure 2, CoT3DRef shows a remarkable capability of learning from limited data, where training on only 10% of the data is enough to beat all the baselines, which are trained on the entire data. Our contributions are summarized as follows:\n\u2022 We propose a 3D data-efficient Chain-of-Thoughts based framework, CoT3DRef, that generates an interpretable chain of predictions till localizing the target. \u2022 We devise an efficient pseudo-label generator to provide inexpensive guidance to improve learning efficiency. \u2022 Our proposed framework achieves state-of-the-art performance on Nr3D, Sr3D, and ScanRefer benchmarks without requiring manually annotated data. \u2022 Using 10% of the data, our framework surpasses the existing state-of-the-art methods."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "3D visual grounding. Significant progress has been made in 3D visual grounding thanks to advancements in deep learning and computer vision, as well as the availability of grounded datasets\n(Achlioptas et al., 2020; Chen et al., 2020; Abdelreheem et al., 2022a; Yuan et al., 2022). One approach involves using graph-based models (Achlioptas et al., 2020; Feng et al., 2021; Yuan et al., 2021; Huang et al., 2021) to represent the scene as nodes and edges, while attention mechanisms help to focus on relevant parts of the scene. Another approach (Roh et al., 2022) is to convert the visual input into language tokens using a classification head. These tokens can then be combined with the input utterance and fed into a transformer architecture to learn the relationships between input sequence elements. Moreover, recent work (Bakr et al., 2022; Yang et al., 2021) explores distilling knowledge from 2D to 3D in a multi-view setup. However, none of the existing works models the explicit reasoning process behind the prediction of the target object.\nChain of thoughts. The chain-of-thought concept has been used in many different machine learning applications, including natural language processing (Wei et al., 2022; Chowdhery et al., 2022; Lyu et al., 2023; Wang et al., 2022; Zhang et al., 2023; Madaan & Yazdanbakhsh, 2022), and robotics (Jia et al., 2023; Yang et al., 2022). In the context of 3D Visual grounding, developing a chain-of-thought approach provides a natural way to explicitly model the grounding reasoning process, which to the best of our knowledge has not been explored. An extended version is discussed in the Appendix A.9."
        },
        {
            "heading": "3 COT3DREF",
            "text": "In this section, we propose a simple yet effective approach to decompose the referring task into multiple interpretable steps by modeling the problem as Seq2Seq, as shown in Figure 3. First, we briefly cover the general 3D visual grounding models\u2019 skeleton and its main components (Sec. 3.1). Then, we present our method in detail (Sec. 3.2). Finally, we detail our loss function (Sec. 3.4)."
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "We built our framework in a generic way where it could be integrated into any state-of-the-art 3D visual grounding model. An arbitrary 3D visual grounding model mainly consists of three essential blocks; a Visual Encoder, a Language Encoder, and a Multi-Modal Fusion module.\nVisual Encoder. An arbitrary 3D scene S \u2208 RN\u00d76 is represented by N points with spatial and color information, i.e., XYZ, and RGB, respectively. Using one of the off-the-shelf 3D object detectors or manual annotations, we have access to the object proposals P = {Pk}Lk=1, where Pk \u2208 RN\n\u2032\u00d76, N \u2032 represents the number of the object\u2019s points and L is the number of proposals in the scene. Then, the visual encoder encodes the proposals into lower-resolution feature maps V = {Vk}Lk=1, where Vk \u2208 R1\u00d7d and d is the number of hidden dimensions. Language Encoder. Simultaneously, given an input utterance that describes a particular object in a scene, called target, a pre-trained BERT model (Devlin et al., 2018) encodes the input sentence into T = {Tj}Wj=1, where W is the maximum sentence length. Then, a language classification head is utilized to predict only the referred object. We notice that limiting the language encoder to only predict the target object restricts its capability of learning representative features.\nMulti-Modal Fusion. After encoding the object proposals and the utterance, a multi-modal fusion block is exploited to refine the visual features V based on the language embeddings T generating fused\nfeatures F = {Fk}Lk=1, where Fk \u2208 R1\u00d7d. Both graphs (Achlioptas et al., 2020) and transformers (Huang et al., 2022; Bakr et al., 2022; Yang et al., 2021) are explored to capture the correlation between the two different modalities. However, our CoT framework can be integrated easily into any existing 3D visual grounding architecture, regardless of how the multi-modal features F are obtained."
        },
        {
            "heading": "3.2 CHAIN-OF-THOUGHTS",
            "text": "We decompose the referring task into multiple interpretable steps, whereas to reach the final target the model must first predict the anchors one by one, in a logical order called Chain-of-Thoughts. To this end, we first have to predict the anchors from the input utterance, then sort the anchors in a logical order using our Pathway module. Then, we replace the naive referring decoder with our Chain-of-Thoughts decoder.\nWe formulate the referring task as a Seq2Seq problem by localizing the anchors as an intermediate step. Instead of anticipating the target directly, we first predict the chain of anchors sequentially, then utilize them to predict the target.\nPathway generation. First, we extend the language head, i.e., Target Anchors Head, to extract both the target and the anchors OT = {OTi }Mi=1, where M is the maximum number of objects in the sentence, depicted in the lower red part of Figure 3. We add a \u201cno_obj\u201d class to pad the output to the maximum length M . However, the predicted anchors are unsorted or sorted based on the occurrence order in the sentence, which can not be fit in our Chain-of-Thoughts framework. Accordingly, we introduce a \u201cPathway Head\u201d which takes the encoded sentence T and the predicted objects of the utterance OT to produce logically ordered objects OP = ( OPi )M i=1\n. One possible solution is to exploit an MLP head to predict the logical order for each object. However, for better performance, we use a single transformer encoder layer to capture the correlation between different objects.\nSequence-to-Sequence. Similar to the language stream, we first employ a parallel referring head to localize the referred object alongside the anchors, ignoring their logical order. The parallel referring head only takes the multi-modal features F as input and localizes the target and the anchors RF = {RFi }Mi=1. We experiment both with localizing the target and the anchors in parallel, and localizing them one by one in a sequential logical order using previously localized objects as prior information. In other words, we formulate the 3D referring task as a Seq2Seq task, where the input is two sequences; 1) a set of object proposals P , 2) a sequence of words (utterance). The output is a sequence of locations for the anchors and the target. Specifically, we add positioning awareness to the plain localized objects RF using the predicted ordered objects OP , which act as a positional encoding. These positions indicate a logical order that mimics human perception (McVay & Kane, 2009; Chen et al., 2017). Furthermore, we employ a single transformer decoder layer, depicted in Figure 3, to localize the objects in sequence w.r.t the predicted logical order OP . In this decoder layer, the queries are RF + OP and the values and keys are F . Accordingly, the attention maps denoted as A follow Eq. 1.\nA = \u03c3( (R F +OP)FT\u221a\nd ), (1)\nwhere \u03c3 is the Softmax function and d is the embedding dimensions. We use a masked self-attention layer to enforce the Chain-of-Thoughts, where while predicting the next object\u2019s location, we only attend to the previously located objects. This could be interpreted as a one-direction CoT. We also experiment with another variant where no masking is applied so that we can attend to any object in the chain, as shown in Appendix A.6."
        },
        {
            "heading": "3.3 PSEUDO LABELS",
            "text": "During the training phase, our proposed framework requires more information than the standard available GT in existing datasets (Achlioptas et al., 2020; Chen et al., 2020). These datasets only annotate the referred object and the target. However, our framework requires anchor annotations. Three types of extra annotations are needed: 1) Given an input utterance, we need to identify the mentioned objects other than the target; the anchors. 2) Once we extract the anchors from the utterance, we need to know their logical order to create a chain of thoughts. 3) Finally, we need the localization information for each anchor, i.e., to assign a bounding box to every anchor.\nTo make our framework self-contained and scalable, we do not require any manual effort. Instead, we collect pseudo-labels automatically without any human intervention.\nAnchors parser. We extract the textual information from the utterance using rule-based heuristics and a scene graph parser (Schuster et al., 2015; Wu et al., 2022). First, we extract the whole mentioned objects and their relations from the utterance using the scene graph parser. Then, we match the objects to their closest matching class from the ScanNet labels (Dai et al., 2017a) using SBERT (Reimers & Gurevych, 2019). Due to the free-from nature of Nr3D, the anchors mentioned in the GT descriptions sometimes do not precisely match the ScanNet class labels. For instance, the GT description is \u201cThe plant at the far right-hand side of the bookcase tucked in the furthest corner of the desk.\u201d However, there is no \u201cbookcase\u201d class in ScanNet. Therefore, we need to match it to the nearest ScanNet class labels, which in this case will be \u201cbookshelf.\u201d\nAnchors pathway. We utilize GPT-3.5 (Ouyang et al., 2022) to extract the logical order of objects given an input utterance, using in-context learning (Brown et al., 2020). The full prompt used for pathway extraction is provided in Appendix A.12.\nAnchors localization. The anchors localization module employs object proposals P , extracted relations R and utterance objects O\u0303T to establish associations between anchors and object bounding boxes within an input scene. Our method involves iterating over all anchors extracted from the utterance and searching for candidate objects in P that share the same class. When the anchor class is represented singularly in the scene, we return the matched object. However, in scenarios where disambiguation is required due to multiple objects belonging to the anchor class, we leverage the parsed spatial relations and the localized objects within the scene to identify the intended anchor accurately , termed FIND in Algorithm 1. However, it is not guaranteed that the FIND function will be able to localize the remaining unlocalized anchors accurately. Thus, in this case, as shown in the last step in Algorithm 1, we randomly sample an object of the same class. We summarize our localization method in Algorithm 1.\nAlgorithm 1 Localizing objects mentioned in an input utterance Input:\n\u2022 O\u0303T = {oi}Ki=1: unlocalized objects mentioned in the utterance, extracted using the syntactic parser. \u2022 P : object proposals for the scene P = (B, C), with: B = {bi}Li=1: bounding boxes bi \u2208 R\n6 and C = {ci}Li=1 classes of the L object proposals for the scene. \u2022 RELATE : O\u0303T \u2192 R\u00d7 O\u0303T : map objects mentioned in the input utterance to a \u27e8spatial relation, object\u27e9 pair. \u2022 FIND : P \u00d7R\u00d7 Pm \u2192 P : map a localized object, a relation and a set of m candidate objects to an output localized object.\nOutput: \u2022A: localized anchors used in the utterance, whereA \u2286 P .\n1: function LOCALIZE(o,P,A) \u25b7 Localizing a single utterance 2: K \u2190 {Pj | o \u221d cj , cj \u2208 C} \u25b7 Find a set of candidates with the same class as o 3: if |K| = 1 then \u25b7 A single object has the anchor class 4: returnK 5: for (rm, ok) \u2208 RELATE(o) do 6: if ok \u2208 A then: return FIND(Pk, rm,K) \u25b7 The related object is already localized 7: p\u2190 LOCALIZE(ok,P,A) \u25b7 Otherwise, attempt to localize the related object 8: if p \u0338= \u2205 then: return {p} \u222a FIND(p, rm,K) 9: return {p}, p \u223c {pi | pi = (bi, ci), o \u221d ci, pi \u2208 P} \u25b7 If all else fails: randomly sample an object of the same class\n10: A \u2190 \u2205 11: for oi \u2208 O\u0303T do: A \u2190 A\u222a LOCALIZE(oi,P,A)"
        },
        {
            "heading": "3.4 LOSSES",
            "text": "Mainly, three losses exist in most existing 3D visual grounding architectures. Two of them are considered auxiliary losses, i.e., 3D classification loss LVcls and language classification loss LTcls. Hence, the third one is the primary loss, i.e., referring loss Lref . First, we extend the language classification loss LTcls by recognizing the referred object class and the anchors based on the input utterance. Similarly, the referring loss is extended to localize both the target and the anchors, termed as parallel referring loss LPref , as we localize the target and anchors in one step. Furthermore, we add another referring loss after the transformer decoder, termed as CoT referring loss LCOTref . Finally, an auxiliary distractor binary classification loss Ldist is introduced to distinguish between the target and the distractors, i.e., objects with the same class as the target. Grouping all these losses, we optimize\nMethod AnchorsRef. Acc. Target Ref. Acc. Baseline N/A 55.1\n+CoT +Zeroing Anchor Loss 4.5 55.0 +CoT +Pseudo Labels 72.3 60.4 +CoT +Human Labels 73.6 64.4\nthe whole model in an end-to-end manner with the following loss function:\nL = (\u03bbV \u00b7 LVcls) + (\u03bbT \u00b7 LTcls) + \u03bbref \u00b7 (LPref + LCOTref ) + \u03bbdist \u00b7 Ldist, (2)\nwhere \u03bb is the corresponding loss weight for each term."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "Datasets. To probe the effectiveness of our proposed framework, CoT3DRef, we conduct evaluations on three 3D visual-grounding benchmarks, namely Nr3D, Sr3D (Achlioptas et al., 2020) and ScanRefer (Chen et al., 2020). Nr3D contains 41.5K natural, free-form utterances gathered from humans through a referring game, while Sr3D consists of 83.5K synthetic utterances. Consequently, ScanRefer provides 51.5K utterances of 11K objects for 800 3D indoor scenes.\nNetwork Configuration. We model the Pathway module using only one transformer encoder layer, and the CoT decoder using a single transformer decoder layer. The number of heads used are 7 and 16 for the Pathway module and CoT decoder, respectively. The number of proposals L and the maximum sentence length W are 52 and 24, respectively. L and W define the sizes of the input sequences to our CoT decoder. The maximum number M of objects in the sentence, the output sequence length for our CoT decoder, is 8 and 3 for Nr3D and Sr3D, respectively. Following previous works (Abdelreheem et al., 2022b; Achlioptas et al., 2020; He et al., 2021; Roh et al., 2022; Jain et al., 2021; Yang et al., 2021; Qi et al., 2017), we randomly sample 1024 points for each proposal, set the hidden dimensions d to 768, and train the model for 100 epochs from scratch using the weight initialization strategy described in (He et al., 2015). The initial learning rate is set to 10\u22124 and decreases by 0.65 every ten epochs. The Adam optimizer (Kingma & Ba, 2014) and a mini-batch size of 24 per GPU are used for training all the models. We set the losses weights as follows: \u03bbV = 5, \u03bbT = 0.5, Lref = 5, and \u03bbdist = 1. We used the PyTorch framework and a single NVIDIA A6000 GPU for training."
        },
        {
            "heading": "4.1 ABLATION STUDIES",
            "text": "We conducted several ablation studies to validate each module in our framework, termed CoT3DRef.\nCoT vs. Parallel. To assess our CoT3DRef framework, we have to disentangle the CoT from the pseudo label generation module. In other words, it could be thought that the achieved gain caused by just accessing additional supervision signal, i.e., anchors\u2019 annotations. To this end, we implement a parallel approach, that has access to the anchors\u2019 labels, however, localizes the targets and anchors in one shot without any interaction between them. In contrast, our CoT3DRef framework leverages the causality between the anchors and the target through our chain-of-thoughts decoder. As shown in Table 1, on the challenging setup, where we assume access for only 10% of the training data while testing on the entire testing dataset, the parallel variant boosts the performance by 4% and 6.5% over the vanilla MVT using Nr3D and Sr3D, respectively (row b). On the other hand, our CoT3DRef framework surpasses the vanilla MVT by 10% and 16.4% using Nr3D and Sr3D, respectively (row c). Consequently, using the entire data, our CoT surpasses both the parallel and the vanilla MVT approaches by 3% and 5% on Nr3D, and by 1% and 6.7% on Sr3D, respectively.\nDistractor Loss. We add an auxiliary distractor binary classification loss Ldist to disambiguate the target and the distractors, as discussed in Sec. 3.4. As shown in Table 1, rows d and h, incorporating it boosts the performance by 0.5-1%.\nAnchors Quality Effect. To establish the affirmative role of anchors in refining target localization accuracy without inducing detrimental effects, we conducted a worst-case simulation. In this simulation, all anchors were falsely detected. In other words, we deliberately zero the localization loss associated with anchors during training, causing the decoder to randomly and inaccurately predict anchor locations. Remarkably, the target accuracy remained unaltered, as shown in Table 3, reflecting the robustness of the approach. The accuracy experienced a decrement of approximately 5%, dropping from 60.4% to 55%. Encouragingly, even in this scenario, the accuracy mirrored the baseline performance, steadfast at 55.1%. This substantiates that while anchor detection may exhibit inaccuracies, the broader framework\u2019s efficacy in target localization remains largely unaffected. In contrast, we replaced the pseudo labels with manual annotations (Abdelreheem et al., 2022a). This substitution serves as an upper-bound reference point for evaluation. As shown in Table 3, the exchange of noisy pseudo labels with precise manual annotations led to a noteworthy 4% enhancement in referring accuracy for Nr3D, elevating it from 60.4% to 64.4%.\nNumber of Transformer Blocks. As shown in Table 2, we have explored using 1, 2, and 4 transformer blocks in our CoT referring decoder. However, we didn\u2019t notice a significant gain in performance; therefore, we preferred to use a single transformer block."
        },
        {
            "heading": "4.2 COMPARISON TO STATE-OF-THE-ART",
            "text": "We verify the effectiveness of our proposed framework, termed CoT3DRef on three well-known 3D visual grounding benchmarks, i.e., Nr3D, Sr3D (Achlioptas et al., 2020) and ScanRefer (Chen et al., 2020). By effectively localizing a chain of anchors before the final target, we achieve state-of-the-art\nresults without requiring any additional manual annotations. As shown in Table 4, when we integrate our module into four baselines; LAR (Bakr et al., 2022), SAT (Yang et al., 2021), MVT (Huang et al., 2022) and ViL (Chen et al., 2022), it boosts the accuracy by 3.6%, 4%, 5%, 0.5% on Nr3D and 10%, 11%, 9%, 1% on Sr3D, respectively. The disparity between the gain achieved on Nr3D and Sr3D is due to our pseudo label module that hinders achieving more gain on Nr3D. Exchanging of our noisy pseudo labels with precise manual annotations led to a noteworthy enhancement in referring accuracy, where our module boosts the performance by 9% compared to the baselines; LAR, SAT and MVT, respectively. This outcome underscores our model\u2019s ability to yield enhanced performance not only for simpler descriptions (Sr3D) but also in the context of more intricate, free-form descriptions (Nr3D). We have a limited gain on only ViL. A detailed analysis that justifies this behaviour is mentioned in the Appendix A.4.\nNr3D+Sr3D. In addition, we jointly train on Nr3D and Sr3D. Whereas, we augment the Nr3D training data with Sr3D, while testing on the same original Nr3D test-set. Consistently with the previous results mentioned in Table 1 and 4, we surpass all the existing work by 3.5%. As shown in Table 5, we achieve 62.5% grounding accuracy, while MVT only achieves 58.5% accuracy.\nScanRefer. To further show the effectiveness of our proposed method, we have conducted several experiments on the ScanRefer dataset across different data percentages on MVT and SAT baselines. As shown in Table 6, we outperform both MVT and SAT by a significant margin across the entire data percentages, i.e., 10%, 40%, 70%, and 100%. More specifically, integrating our CoT framework into MVT has boosted the performance by 12.2%, 8.6%, 8%, and 6.8%, respectively. In addition to MVT, we have integrated our CoT framework into the SAT baseline, where the same performance gain has been achieved, probing our method\u2019s effectiveness across a comprehensive range of baseline models, datasets, and different available data percentages.\nData Efficiency. To further validate the effectiveness of our framework, we assess it on a more challenging setup, where we assume access to only limited data. Four percentage of data is tested, i.e., 10%, 40%, 70%, and 100%. As shown in Figure 2, on the Sr3D dataset, using only 10% of the data, we match the same performance of MVT and SAT that are trained on 100% of the data. This result highlights the data efficiency of our method. Furthermore, when trained on 10% of the data on Nr3D with noisy pseudo labels (Sec. 3.3), we still surpass all the baselines with considerable margins.\nQualitative results. As shown in Figure 4, the first three examples show that our model successfully localizes the referred objects by leveraging the mentioned anchors, such as \u201cthe table with 5 chairs around\u201d. However, in the ambiguous description shown in the fourth example: \u201c2nd stool from the left\u201d, the model incorrectly predicts the stool, as it is view-dependent. In other words, if you look at the stools from the other side, our predicted box will be correct. Additionally, the last example shows a challenging scenario where a negation in the description is not properly captured by our model."
        },
        {
            "heading": "5 DISCUSSION AND LIMITATIONS",
            "text": "Comparison with PhraseRefer (Yuan et al., 2022) and ScanEnts (Abdelreheem et al., 2022a). We acknowledge the great work proposed by PhraseRefer (Yuan et al., 2022) and ScanEnts (Abdelreheem et al., 2022a), which paved the way for the importance of paying attention not just to the target object but also to the anchors. Thus, underlying approaches have some similarities in terms of demonstrating the importance of including anchors in the pipeline. However, there are a lot of significant differences: 1) Design a CoT framework for 3D visual grounding that explicitly models causal reasoning while interpreting the instruction inspired by the human perception system. 2) Show that we can design an efficient framework that achieves state-of-the-art results on three challenging benchmarks, i.e., Nr3D, Sr3D (Achlioptas et al., 2020), and ScanRefer (Chen et al., 2020), without requiring human labels. Proper design of such an approach is pivotal in attaining a good performance while circumventing the need for intensive human annotations. A pertinent example can be drawn from the labeling procedure employed in PhraseRefer (Yuan et al., 2022), which demanded a cumulative workforce commitment of 3664 hours, roughly equivalent to an extensive five-month timespan.\nPseudo labels accuracy. The accuracy of the pseudo-labels plays a vital role in the overall performance. To evaluate its performance, we manually collect ground-truth labels for 1) the predicted orderings of the anchors in the utterance and 2) the final localization of the anchors as predicted by the geometry module, based on 10% of the Nr3D. To evaluate orderings predicted by in-context learning, we use the normalized Levenshtein edit distance between two sequences, where a length of 1 means that every object in the sequence is incorrect. We achieve an average distance of 0.18 between predicted and ground-truth orderings. We consider anchor-wise localization accuracy to evaluate the geometry module\u2019s accuracy, where the anchors considered for each sequence are those mentioned in the input utterance. We achieve a 77 % accuracy for this task compared to human annotators. Overall, a significant accuracy gap is measured between automatically collected pseudo-labels and ground-truth data, contributing to the performance loss observed on the Nr3D dataset.\nPseudo module limitations. Despite designing a data efficient framework that could be integrated into any existing 3D visual grounding architecture, and achieve SOTA results without requiring any manual annotation efforts, our pseudo label module hinders achieving more gain on Nr3D. Accordingly, we encourage the future efforts to try to enhance the pseudo module performance. In addition, the anchor localization block in our pseudo module is tailored on ScanNet dataset (Dai et al., 2017a), and will thus need some adaptations to be usable on other 3D scene datasets.\nPathway module limitations. Our Pathway module is responsible of generating a chain of logical order for the extracted objects from the input utterance. However, it does not handle the multi-path scenario, where multiple paths are valid. For instance, given this utterance \u201cIt is the chair besides the desk, which has a book and a lamp above it\u201d, we have two possible starting points, i.e., locating the lamp first or the book. Thus, for simplicity, we start by the last mentioned object in the utterance, in the multiple paths scenario. Nevertheless, one possible solution to handle this limitation implicitly through building a graph that reasons the different possibilities (Salzmann et al., 2020)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose CoT3DRef: a novel and interpretable framework for 3D visual grounding. By formulating the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, our approach predicts a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. Our framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. Furthermore, our proposed chain-ofthoughts module can easily be integrated into other architectures. Through extensive experiments, we demonstrate consistent performance gains over previous state-of-the-art methods operating on Referit3D. Importantly, our approach does not rely on any additional manual annotations. Instead, we leverage automatic rule-based methods, syntactic parsing, and in-context learning to collect pseudo-labels for the anchor objects, thereby eliminating the laborious and time-consuming process of manually annotating anchors. Overall, our work advances 3D visual grounding by making a step towards bridging the gap between machine perception and human-like understanding of 3D scenes."
        },
        {
            "heading": "A APPENDIX",
            "text": "Our Appendix contains the following sections:\n\u2022 Identification of failure cases as a benefit of the interpretability. \u2022 Broader Impact and Ethical Considerations. \u2022 GPT-generated path influence on the overall grounding accuracy. \u2022 Justification for CoT performance when integration into Vil3DRef. \u2022 Do we use additional supervision? \u2022 Causal vs. Non-Causal CoT. \u2022 Nr3D Ambiguity. \u2022 Related Work. \u2022 Explore data-augmentation effect in the limited data scenario (10%). \u2022 Pseudo labels human-evaluation. \u2022 In-context prompt used for object order extraction. \u2022 Examples of generated paraphrases for an utterance.\nA.1 IDENTIFICATION OF FAILURE CASES AS A BENEFIT OF THE INTERPRETABILITY\nTo demonstrate our interpretability abilities, we visualize attention maps as shown in Figure 5. In Figure 5, the input description is \u201cthe chair to the far left hand side of the taller desk with the computer monitor on it.\u201d Accordingly, there are two anchors mentioned in the description, i.e., desk and monitor, and the target is the chair. The correct chair should be number two, however, the model predicts number four. By visualizing the attention maps, on the left part of Figure 5, we can identify the main cause of the wrong prediction, whereas, the first anchor localize a wrong desk (desk #3) instead of desk #2. Therefore, the rest of the chain, i.e., the monitor and the chair are localized wrongly. This example also shows the ambiguity in Nr3D, where it is hard to say which desk is the taller, desk #2 or desk #3."
        },
        {
            "heading": "A.2 BROADER IMPACT AND ETHICAL CONSIDERATIONS",
            "text": "3D visual grounding holds profound implications across diverse domains and applications, spanning from integrating outdoor systems, e.g., autonomous driving and indoor navigation systems.\nOutdoor navigation systems: 3D visual grounding empowers the agents to execute a broad spectrum of tasks, facilitating assistive systems catering to the needs of elderly individuals and those\nwith disabilities\u2014such as the visually impaired. For instance, in the context of visually impaired individuals, the technology\u2019s ability to ground objects within their 3D environment from language descriptions is pivotal in navigation tasks such as determining the nearest exit or chair.\nAutonomous driving: In the realm of autonomous driving, a 3D visual grounding system takes on a critical role in translating navigational instructions into actionable responses within the dynamic visual landscape. This technology enables seamless communication between the language input, such as navigation commands, and the 3D world the autonomous vehicle operates in. For instance, when given a directive like \"merge into the right lane at the upcoming intersection where the three pedestrians are walking,\" the 3D visual grounding system interprets this command by analyzing the spatial layout, recognizing lanes and pedestrians, and identifying the appropriate merging point. By bridging the gap between language instructions and the complex visual cues inherent in driving scenarios, this system enhances the vehicle\u2019s ability to accurately interpret and execute instructions, contributing to safer and more efficient autonomous driving experiences.\nData-efficiency importance for autonomous driving: Introducing a data-efficient solution for the 3D visual grounding task holds immense importance in the context of autonomous driving, particularly considering the challenges associated with manually labeling vast amounts of 3D point cloud data from lidar sensors. The conventional approach of manually annotating such data is laborintensive and economically burdensome. By proposing a data-efficient solution, our system addresses a critical bottleneck in developing autonomous driving technology. This allows the autonomous vehicle to learn and generalize from minimal labeled data, significantly reducing the dependency on large, expensive datasets. The capacity to make accurate 3D visual groundings with limited labeled information not only streamlines the training process but also makes deploying autonomous vehicles more scalable and cost-effective.\nFederated learning for indoor navigation systems: Aligned with this imperative, the inherent data-efficient nature of our approach positions it as an ideal candidate for federated learning schemes, which is perfectly aligned with the indoor setup when privacy is required, e.g., the robot is operating in a personalized environment such as the client home. This becomes particularly relevant when a robot is introduced into a novel indoor environment and necessitates rapid learning from minimal interactions with its new user without sending the private data of the new environment to the server; thus, learning from limited data is essential."
        },
        {
            "heading": "A.3 GPT-GENERATED PATH INFLUENCE ON THE OVERALL GROUNDING ACCURACY",
            "text": "The GPT-generated path is only used as a teacher label for training a simple MLP that predicts the path during the inference time. Therefore, GPT is not involved in the system during inference, as we train a simple MLP layer, i.e., the Pathway-Head in Figure 3, to predict the logical order. For instance, when the system is deployed on a robot, it will be efficient as our simple MLP Pathway-Head is utilized to generate a reasonable order based on the input description.\nAccordingly, GPT could be seen as a design choice to obviate the need for human annotations during training.\nIn Table 7, the Pseudo and GT labels are for the anchors\u2019 localization, not the path. For the path, we always use our MLP-predicted path. However, the accuracy will not be affected if we replace it with the GPT-generated path. As shown in Table 7, we achieve 64.4% and 64.5% using our predicted and GPT-generated paths, respectively. This is an intuitive result, as when we assess the performance of the Pathway Head separately, we get 98% accuracy w.r.t the GPT-generated path. Therefore, replacing the Pathway Head generated path by the GPT ones is expected not to impact the final performance."
        },
        {
            "heading": "A.4 JUSTIFICATION FOR COT PERFORMANCE WHEN INTEGRATION INTO VIL3DREF",
            "text": "For ViL, our observations, in Figure 2, reveal a substantial performance enhancement in the demanding data-efficient scenario, accompanied by marginal improvements when employing the entire dataset. These modest gains are likely due to nearing the peak attainable accuracy of the dataset, influenced by the limitations posed by annotation errors.\nTo substantiate our hypothesis, we have conducted three experiments:"
        },
        {
            "heading": "A.5 DO WE USE ADDITIONAL SUPERVISION?",
            "text": "Our main thesis is to show that the design of our CoT framework can be accomplished with notable efficiency, obviating the need for human intervention. Furthermore, we emphasize its demonstrable capacity for substantial enhancement of existing methodologies, particularly within the context of data efficiency, as shown in Figure 2. To this end, we have devised an efficient pseudo-label generator to provide inexpensive guidance to improve the learning efficiency of most existing methods, as we demonstrated in our experiments in Table 4 and Figure 2. These pseudo labels are not required at test time. Proper design of such an approach is pivotal in attaining a significant performance gain while circumventing the need for intensive human annotations. A pertinent example can be drawn from the labeling procedure employed in PhraseRefer (Yuan et al., 2022), which demanded a cumulative workforce commitment of 3664 hours, roughly equivalent to an extensive five-month timespan. Hence, our pseudo-label generation module is delineated into three core components: the anchor parser, the anchor localization module, and the pathway prediction module. Notably, it is essential to highlight that solely the pathway prediction component leverages a pre-trained model to generate a ground truth but is not involved during training or at inference. More specifically, the anchor parser and localization modules do not contain any learnable parameters nor pre-trained models, as they use rule-based heuristics where only two pieces of information are needed: 1- objects\u2019 class labels. 2 - objects\u2019 position. These two sources of supervision are utilized by all referring architectures, where object class labels are used as ground truth for the classification head, and object positions are incorporated in the model\u2019s pipeline either by concatenation with the objects\u2019 point clouds or input to the multi-modal transformer as a positional encoding. Accordingly, both\nmodules do not require any additional supervision. Solely, the anchor pathway prediction module requires a pre-trained model to generate the ground truth path, but it is not involved during training or at inference. The trajectory of reasoning (referred to as the Chain-of-Thoughts) produced by the pathway prediction module is indispensable within our framework. The omission of the anchor chain would inevitably result in the degradation of our framework, resembling the parallel scenario wherein anchors and the target object are independently detected. Conversely, our framework adopts a sequential, auto-regressive approach, wherein each anchor is predicted consecutively. To show the effectiveness of our proposed method, we compare it against the parallel approach, as shown in Table 1. In summary, solely the pathway prediction component leverages a pre-trained model to generate a ground truth but is not involved during training or at inference. Additionally, with zero human efforts, we achieve a significant performance gain on four distinct baselines, as shown in Figure 2."
        },
        {
            "heading": "A.6 CAUSAL VS. NON-CAUSAL COT",
            "text": "In contrast to the conventional teacher-forcing technique, our approach employs a parallel referring head to predict the initial, unordered locations of anchors, depicted in green within Figure 3. This implementation facilitates an investigation into the implications of causal masking, by removing the causality constraints. Essentially, this entails permitting the decoder to encompass a broader scope by attending to all anchors concurrently, irrespective of their order. However, our CoT decoder still adhere to the correct sequential order for prediction of both anchors and the target object autoregressively. Figure 6, illustrates the modifications made to the architecture to accommodate the random masking approach. Empirical results, as shown in Table 9, indicate a marginal enhancement in accuracy on the Nr3D dataset\u2014almost 0.5%\u2014upon relaxation of the causality constraints. However, no discernible improvements materialize on the Sr3D dataset."
        },
        {
            "heading": "A.7 NR3D AMBIGUITY",
            "text": "The challenges posed by Nr3D are not solely attributed to its free-form nature; rather, it is compounded by its susceptibility to ambiguity. A visual representation of this issue is provided in Figure 7 showcasing instances of ambiguity where even human annotators encounter difficulties in accurately\nlocalizing objects due to viewpoint-dependent considerations. In such cases, correct predictions hinge on the speaker\u2019s viewpoint, an aspect that often remains undisclosed.\nTo fortify this standpoint, we conducted an in-depth analysis of the consistency between two recently released manually annotated datasets (Yuan et al., 2022; Abdelreheem et al., 2022a). Surprisingly, the matching accuracy between these annotations stands at a mere 80%, signifying the extent of inherent data ambiguity and the ensuing challenges. In essence, divergent annotations from distinct human annotators due to ambiguity impedes the potential for further enhancements."
        },
        {
            "heading": "A.8 ANCHOR LOCALIZATION ACCURACY",
            "text": "For each scene, we have the objects\u2019 proposals P = (B, C), where B = {bi}Li=1 bounding boxes bi \u2208 R6, C = ciLi=1 is the class label and L is the number of objects in the scene. Given the extracted objects\u2019 names, O\u0303T = {oi}Ki=1, using our syntactic parser, the anchors localization module automatically assign each object\u2019s name to the corresponding box, A\u0303 = (B\u0303, O\u0303T ), where B\u0303 \u2286 B and A\u0303 \u2286 P . To assess the assignation quality, we ask the annotators to assign a box for each extracted object\u2019s name O\u0303T from the input utterance, creating the GT set A = (B\u0304, O\u0303T ), where B\u0304 \u2286 B and A \u2286 P . Then, we measure the precision between the two sets, i.e., A\u0303 and A."
        },
        {
            "heading": "A.9 RELATED WORK",
            "text": "Our novel architecture draws success from several areas, including 3D visual grounding, chain-ofthought reasoning, and scene graph parsers.\n3D visual grounding. Existing work can be grouped into one- and two-stage approaches. Singlestage methods depend on detecting objects directly by fusing text features with point-cloud level visual representations and enable grounding multiple objects from a single sentence (Kamath et al., 2021; Luo et al., 2022). Two-stage methods dissociate the object detection task from selecting a target among detected objects (Achlioptas et al., 2020; Chen et al., 2020), by detecting objects during the first stage and then classifying the resulting objects. To benchmark the 3D visual grounding task, ReferIt3D (Achlioptas et al., 2020) and ScanRefer (Chen et al., 2020) were introduced by collecting textual annotations from the ScanNet dataset (Dai et al., 2017a). Early work depended on graph-based approaches (Velic\u030ckovic\u0301 et al., 2018; Wang et al., 2019) to infer spatial relations. The object graph is constructed by connecting each object with its top nearest neighbors (Abdelreheem et al., 2022b; Feng et al., 2021; Huang et al., 2021; Yuan et al., 2021) based on Euclidean distances. Taking advantage of the Transformer\u2019s attention mechanism, which is naturally suitable for multi-modal features fusion, (He et al., 2021; Zhao et al., 2021) achieved remarkable performance by applying self-attention and cross-attention on input features. While methods such as BEAUTY-DETR (Jain et al., 2022) and (Luo et al., 2022) adopt a single-stage approach, most works follow the two-stage approach with pre-detected object proposals. LAR (Bakr et al., 2022), and SAT (Yang et al., 2021) use a multi-modal approach by introducing 2D images of the scenes to aid object grounding. Conversely, MVT (Huang et al., 2022) focuses on 3D point clouds by extracting multiple views of the objects and aggregating them to improve view robustness to make the transformer view-invariant. LanguageRefer (Roh et al., 2022) converts the visual grounding task into a language-based object prediction problem instead of the cross-modal approach. Similarly, NS3D (Hsu et al., 2023) focuses on language features by translating language into hierarchical programs. EDA (Wu et al., 2022) explicitly separates textual attributes and performs dense alignment between language and point cloud objects. ViL3DRel (Chen et al., 2022) includes a spatial self-attention layer that considers the relative distances and orientations between objects in 3D point clouds.\nChain of thoughts. In machine learning, a chain of thought refers to the series of intermediate steps that a model takes to arrive at its final decision or prediction. The chain-of-thought concept has been used in many different machine learning applications, including natural language processing (Wei et al., 2022; Chowdhery et al., 2022; Lyu et al., 2023; Wang et al., 2022; Zhang et al., 2023; Madaan & Yazdanbakhsh, 2022), and robotics (Jia et al., 2023; Yang et al., 2022). In natural language processing, chain-of-thought prompting adds intermediate reasoning steps to the output of LLMs, which leads to increased reasoning capabilities (Zhao et al., 2023; Wei et al., 2022). In robotics, understanding the chain of thought can help to identify which sensory inputs were used to arrive at a particular action or decision. Moreover, (Jia et al., 2023) divides the primary goal into subgoals to tackle the complex\nsequential decision-making problems using the Hierarchical RL (HRL) (Hutsebaut-Buysse et al., 2022).\nScene graph parser. Extracting scene graphs is a fundamental task in NLP and multimodal vision. This problem depends on extracting directed graphs, where nodes are objects and edges represent relationships between objects (Lou et al., 2022; Xu et al., 2017; Wu et al., 2019; Jiang et al., 2016; Yang et al., 2018; Schuster et al., 2015). Scene graphs have been used in scene understanding to enhance image captioning and predicting pairwise relations between detected objects (Li et al., 2017b; Dai et al., 2017b; Li et al., 2017a). Other works have leveraged graphs to enhance text-to-image generation models by learning the relations between the objects in the sentence (Feng et al., 2023; Johnson et al., 2018). Existing off-the-shelf tools (Schuster et al., 2015) can parse the language description grammatically into a scene graph with a dependency tree (Wu et al., 2022)."
        },
        {
            "heading": "A.10 EXPLORE DATA-AUGMENTATION EFFECT IN THE LIMITED DATA SCENARIO (10%)",
            "text": "Visual-based Augmentation. Two visual augmentations are explored at the scene and point cloud levels. At the scene level, we randomly shuffle every object presented in the scene with another object of the same class from one of the ScanNet scenes. At the point cloud level, we apply a small random isotropic Gaussian noise, rotate the points along each axis, and randomly flip the x and y axes. In addition, we apply a small random RGB translation over color features. These visual augmentations are used to enhance the robustness and generalization capabilities of our architecture.\nLanguage-based Augmentation. We utilize a T5 (Raffel et al., 2019) transformer model fine-tuned on paraphrase datasets to augment the input utterances. To ensure the quality of the generated paraphrases, we use the Parrot library (Damodaran, 2021) to filter them based on a fluency and adequacy score. This aims to enhance the naturalness and diversity of the generated utterances. We showcase an example of an input utterance alongside its corresponding generated paraphrases in Table 11.\nReferring-based Augmentation. The maximum number of objects in the sentence M is 8 and 3 for Nr3D and Sr3D, respectively. However, in 52% and 98.5% of Nr3D and Sr3D, respectively, two objects only are mentioned in the utterance; the target and one anchor. This motivates us to apply a simple referring-based augmentation by swapping the target and the anchor while keeping the same semantic information. For instance, if the relation between two objects is \u201cnear to\u201d, it should be the same after swapping. However, if it was \u201con the right of\u201d, it should be swapped to be \u201con the left of\u201d.\nResults. We have explored several augmentation techniques to enhance our architecture\u2019s robustness and generalization capabilities. However, we report the results on only 10% of the data as we notice no gain is achieved when leverage more training data, e.g., 40%. As shown in Table 10, the language and referring augmentations (rows b and f) increase the performance slightly. However, the two visual-based augmentations (rows c, d, and e) do not give any performance gain."
        },
        {
            "heading": "A.11 PSEUDO LABELS HUMAN-EVALUATION",
            "text": "The accuracy of the pseudo-labels plays a vital role in the overall performance. To evaluate pseudolabels, we manually collect ground-truth labels for:\n1. The predicted orderings of the anchors in the utterance. Figures 8 and 9 depict our UserInterface (UI), designed on Amazon-Mechanical-Turk (AMT) (Crowston, 2012) for objects\u2019 order and relations collecting. We detailed the instructions on the left, and the task is mentioned on the right. The annotators are asked to extract the mentioned objects in the input sentence in chronological order and their spatial relationship, then assign each object a logical order to reach the end goal, the target. We demonstrated the instructions on the left for each sample for better performance. To evaluate orderings predicted by in-contextlearning, we use the normalized Levenshtein edit distance between two sequences, where a distance of 1 means that every object in the sequence is incorrect. We achieve an average distance of 0.18 between predicted and ground-truth orderings. 2. The final localization of the anchors as predicted by the geometry module, as demonstrated in Figures 10, 11, 12, and 13. To evaluate the geometry module\u2019s accuracy, we consider anchor-wise localization accuracy. Therefore, we design a web-based interface to collect ground-truth data for 10% of Nr3D dataset. At the entry screen, Depicted in Figure 10, we list the available scenes and the corresponding number of sentences for each scene in parentheses. More specifically, for each scene, we demonstrate the number of annotated sentences divided by the total number of sentences in that scene. To facilitate the annotators\u2019 task, we mark the annotated sentences in green and the remaining in yellow. Afterwards, when the annotator clicks on a particular scene, the 3D interactive window will be displayed at the top, and corresponding sentences will be displayed below it, as shown in Figure 11. We mark the anchors\u2019 words in a clickable yellow box. Once a box is clicked, we show the corresponding boxes with the same class label as the clicked word. Finally, the annotator\u2019s task is to pick the correct box out of the colored boxes. For instance in Figure 12, we have a unique box for the word window; therefore, only one box is shown. In contrast, Figure 13 shows more challenging sample, where we have nine boxes for the word chair. The selected box by the annotator is marked in green, while the other distractors are marked in red.\nA.12 IN-CONTEXT PROMPT USED FOR OBJECT ORDER EXTRACTION\nWe provide below the abridged prompt we use to extract the logical order of objects to follow to find the target object, following a given input utterance. We find empirically that 1) providing a large number of examples, 2) mentioning reasoning explanations, 3) asking to first provide the list of mentioned objects, and 4) repeating instructions at the end of the prompt increases the quality of the predicted object orderings produced by GPT3.5.\nGiven a set of instructions to locate a target object in a room, your task is to extract the order of objects mentioned in each instruction. 1. First list all the unordered mentioned objects in the sentence using the format: \"L: [object_1, object_2, object_3]\". 2. Then order the objects using the format \"R: [object order]\" and mark the target object with \"t\". Only include the names of the objects. Additionally, please exclude any additional information or descriptors such as color or size adjectives. The target is the subject of the sentence.\nQ: \"The pillow on the couch that is on the right hand side of the room, when you are facing the TV, and is closest to the TV\" L: [TV, couch, pillow] R: [1: TV, 2: couch, t: pillow] (explanation: we first need to find the TV to orient ourselves, then find the couch on the right hand-side of the room, and then the pillow on the couch. Do not mention the explanations in the future.) Q: \"The pillow on the bed that is on the far end of the room and is at the rear and right hand side of the bed\" L: [pillow, bed] R: [1: bed, t: pillow] Q: \"Standing at the end of the bed looking towards the pillows, choose the pillow that is in the front, smaller and more to the right.\" L: [bed, pillow] R: [1: bed, t: pillow] Q: \"There are two groups of kitchen cabinets; three along the wall with the stove, and two on the opposite wall. Looking at the wall opposite the stove, the one with the refrigerator, please select the kitchen cabinet closest to the refrigerator.\" L: [kitchen cabinet, wall, stove, refrigerator] [...]\nDo not mention explanations. Only include objects in the list. Do not add any text beyond the reply. The target is the subject of the sentence. Mention all relevant objects in the reply. Don\u2019t use color or size adjectives. Please exclude any color adjectives in your response. Avoid using any descriptive words related to color in your response. Query:"
        },
        {
            "heading": "A.13 EXAMPLE OF GENERATED PARAPHRASES",
            "text": "In Table 11 below, we provide an example of an input utterance sampled from Nr3D and its associated generated paraphrases."
        }
    ],
    "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
    "year": 2023
}