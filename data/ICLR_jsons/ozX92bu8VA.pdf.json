{
    "abstractText": "Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to improve the performance of LLMs by simply removing higher-order components (components with smaller singular values) of their constituent weight matrices in the multi-layer perception (MLP) layers. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. LASER can dramatically boost predictive performance\u2014at times by 27.4 percentage points over the model\u2019s original performance\u2014on question-answering tasks and across various modalities for which Transformers are used.",
    "authors": [],
    "id": "SP:132059dd4dc7a1322879c5df5446af8d79d9e262",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Einat Kermany",
                "Yonatan Belinkov",
                "Ofer Lavi",
                "Yoav Goldberg"
            ],
            "title": "Fine-grained analysis of sentence embeddings using auxiliary prediction",
            "venue": "tasks. ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Jimmy Ba",
                "Rich Caruana"
            ],
            "title": "Do deep nets really need to be deep",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicolas Roux",
                "Pascal Vincent",
                "Olivier Delalleau",
                "Patrice Marcotte"
            ],
            "title": "Convex neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2005
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Misha Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "German Kruszewski",
                "Guillaume Lample",
                "Lo\u0131\u0308c Barrault",
                "Marco Baroni"
            ],
            "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Maria De-Arteaga",
                "Alexey Romanov",
                "Hanna Wallach",
                "Jennifer Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Geyik",
                "Krishnaram Kenthapadi",
                "Adam Tauman Kalai"
            ],
            "title": "Bias in bios",
            "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, jan 2019",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Carl Eckart",
                "Gale Young"
            ],
            "title": "The approximation of one matrix by another of lower rank",
            "year": 1936
        },
        {
            "authors": [
                "N. Elhage"
            ],
            "title": "A mathematical framework for transformer circuits",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. https: //transformercircuits.pub/2021/framework/index.html,",
            "year": 2021
        },
        {
            "authors": [
                "Allyson Ettinger",
                "Ahmed Elgohary",
                "Philip Resnik"
            ],
            "title": "Probing for semantic evidence of composition by means of simple classification tasks",
            "venue": "In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv: Learning, 2018",
            "venue": "URL https://api.semanticscholar.org/ CorpusID:53388625",
            "year": 2018
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy"
            ],
            "title": "Transformer Feed-Forward layers are Key-Value memories",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "H Hajimolahoseini",
                "Mehdi Rezagholizadeh",
                "Vahid Partovinia",
                "Marzieh S Tahaei",
                "Omar Mohamed Awad",
                "Yang Liu"
            ],
            "title": "Compressing pre-trained language models using progressive low rank decomposition",
            "year": 2021
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "John Tran",
                "William J. Dally"
            ],
            "title": "Learning both weights and connections for efficient neural network",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Peter Hase",
                "Mohit Bansal",
                "Been Kim",
                "Asma Ghandeharioun"
            ],
            "title": "Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models",
            "venue": "ArXiv, abs/2301.04213,",
            "year": 2023
        },
        {
            "authors": [
                "Babak Hassibi",
                "David Stork"
            ],
            "title": "Second order derivatives for network pruning: Optimal brain surgeon",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1992
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "ArXiv, abs/1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Torsten Hoefler",
                "Dan Alistarh",
                "Tal Ben-Nun",
                "Nikoli Dryden",
                "Alexandra Peste"
            ],
            "title": "Sparsity in deep learning: pruning and growth for efficient inference and training in neural networks",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2021
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Sara Veldhoen",
                "Willem Zuidema"
            ],
            "title": "Visualisation and \u2018diagnostic classifiers\u2019 reveal how recurrent and recursive neural networks process hierarchical structure",
            "venue": "J. Artif. Intell. Res.,",
            "year": 2018
        },
        {
            "authors": [
                "Yann LeCun",
                "John Denker",
                "Sara Solla"
            ],
            "title": "Optimal brain damage",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1989
        },
        {
            "authors": [
                "Hao Li",
                "Asim Kadav",
                "Igor Durdanovic",
                "Hanan Samet",
                "Hans Peter Graf"
            ],
            "title": "Pruning filters for efficient convnets",
            "venue": "ArXiv, abs/1608.08710,",
            "year": 2016
        },
        {
            "authors": [
                "Xiuqing Lv",
                "Peng Zhang",
                "Sunzhu Li",
                "Guobing Gan",
                "Yueheng Sun"
            ],
            "title": "LightFormer: Light-weight transformer using SVD-based weight transfer and parameter sharing. In Findings of the Association for Computational Linguistics: ACL 2023",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Pavlo Molchanov",
                "Stephen Tyree",
                "Tero Karras",
                "Timo Aila",
                "Jan Kautz"
            ],
            "title": "Pruning convolutional neural networks for resource efficient inference",
            "venue": "arXiv preprint arXiv:1611.06440,",
            "year": 2016
        },
        {
            "authors": [
                "Shikhar Murty",
                "Pratyusha Sharma",
                "Jacob Andreas",
                "Christopher D. Manning"
            ],
            "title": "Characterizing intrinsic compositionality in transformers with tree projections, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Olivier Roy",
                "Martin Vetterli"
            ],
            "title": "The effective rank: A measure of effective dimensionality",
            "venue": "In 2007 15th European Signal Processing Conference,",
            "year": 2007
        },
        {
            "authors": [
                "Max-Philipp B. Schrader"
            ],
            "title": "gym-sokoban. https://github.com/mpSchrader/ gym-sokoban, 2018",
            "year": 2018
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal"
            ],
            "title": "FEVER: a largescale dataset for fact extraction and VERification",
            "venue": "In NAACL-HLT,",
            "year": 2018
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Huanrui Yang",
                "Minxue Tang",
                "Wei Wen",
                "Feng Yan",
                "Daniel Hu",
                "Ang Li",
                "Hai Li",
                "Yiran Chen"
            ],
            "title": "Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William W. Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning"
            ],
            "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
            "venue": "In Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Xiyu Yu",
                "Tongliang Liu",
                "Xinchao Wang",
                "Dacheng Tao"
            ],
            "title": "On compressing deep models by low rank and sparse decomposition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "year": 2017
        },
        {
            "authors": [
                "Sumu Zhao",
                "Damian Pascual",
                "Gino Brunner",
                "Roger Wattenhofer"
            ],
            "title": "Of Non-Linearity and Commutativity in BERT",
            "venue": "In International Joint Conference on Neural Networks (IJCNN),",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to improve the performance of LLMs by simply removing higher-order components (components with smaller singular values) of their constituent weight matrices in the multi-layer perception (MLP) layers. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. LASER can dramatically boost predictive performance\u2014at times by 27.4 percentage points over the model\u2019s original performance\u2014on question-answering tasks and across various modalities for which Transformers are used."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Since their original release, Transformer-based LLMs have been shown to be remarkably proficient on a wide array of important machine learning tasks. Their underlying Transformer architecture has become state-of-the-art for modeling and reasoning about natural language, and has shown promise in domains such as computer vision (Dosovitskiy et al., 2020) and reinforcement learning (Chen et al., 2021) as well.\nContemporary instantiations of Transformer architectures are infamously large, typically requiring tremendous compute resources for both training and inference. This is by design, as Transformers trained with more parameters or more data have been shown to be more capable than their slimmer predecessors\u2014often by a significant margin (Brown et al., 2020; Touvron et al., 2023). Still, a growing body of work suggests that Transformer-based models, and neural networks more generally, do not require all fitted parameters to retain their learned hypotheses. While it seems helpful to be massively over-parameterized at train time (Hinton et al., 2015; Bengio et al., 2005), it is well-known that these models can be drastically pruned before inference; neural networks can often have well over 90% of their weights removed without any significant degradation in performance (Frankle & Carbin, 2018). The discovery of this phenomenon bolstered interest in around the relationship between generalization and over-parametrization (Zhang et al., 2017), and spawned research in developing pruning strategies that lend themselves to efficient model inference (Molchanov et al., 2016).\nThis paper presents a surprising finding, that careful pruning done at specific layers of Transformer models can produce significant boosts in performance on some tasks. We describe LAyer SElective Rank reduction (LASER), an intervention that removes higher-order components of learned weight matrices as identified by singular value decomposition. This reduction is performed in specific weight matrices and selective layers of the Transformer model. In-line with previous work, we find that many such matrices can be significantly reduced, and that performance degradation is often not observed until well over 90% of components are entirely removed. However, unlike what is found in previous work, we find that these reductions can produce drastic improvements in accuracy, as measured by various well-studied reasoning benchmarks in NLP. Even better, this discovery appears to not be limited to natural language, with performance gains also found in domains such as reinforcement learning and, to a limited degree, in the task of object detection in computer vision.\nFurther, this paper analyzes the effect of the training data on samples that benefit from LASER. We find that the improvements in the model\u2019s performance on the dataset predominantly come on information less frequently present in the model\u2019s training dataset, suggesting that LASER offers a kind of denoising procedure that makes weakly learned facts accessible. We separately find that LASER affords increased robustness to paraphrases on previously correct questions.\nLast, this work attempts to reason about what is being stored in the high-order components, such that their removal boosts performance. For questions correctly answered only after LASER, in the absence of interventions, the original model predominantly answers these questions with high-frequency words such as \u201cthe\u201d, \u201cof\u201d, etc\u2014with the answers not even being of the same semantic type as the correct answer. However, after some amount of rank reduction, the model\u2019s answer flips to be correct. To understand this, we look at what the remaining components alone encode; we approximate the weight matrix using only its higher-ordered singular vectors. We find that these components describe either a different answer of the same semantic category as the answer or generic highfrequency words. Therefore, when the noisy, higher-order components are assembled together with the low-ordered components, their conflicting responses produce a sort of \u201caverage answer,\u201d which is likely to be incorrect.\nFigure 1 visualizes the Transformer architecture and the procedure followed by LASER. Here, the weight matrix of a Multi-Layer Perceptron (MLP) at a specific layer is replaced with its low-rank approximation."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "To our knowledge, this paper is the first to identify that carefully selected rank reductions can boost Transformer performance. Still, there is a wide array of works that study related questions, including how facts are stored in LLMs and how to best compress neural networks.\nHow facts are stored. Studies probing model representation for the presence of select properties of entities (Ettinger et al., 2016; Adi et al., 2016; Hupkes et al., 2018; Conneau et al., 2018) show that models store factual information across different layers. However, there is conflicting evidence on how this information is organized and utilized in constructing answers in large language models. Some theories outline that information about different entities is locally stored as two-layer, key-value memory in MLP sections of Transformer models (Geva et al., 2021), which are thereafter copied over through latter layers by the self-attention modules (Elhage, 2021). Meng et al. (2022) proposes a procedure to trace and edit local entity-specific information to map to distinct \u201cimpossible\u201d outputs, supporting the locality theory. These theories are further supported by the phenomenon of \u201cearly exiting,\u201d where the representation at an intermediate layer can be directly used with the\nterminal head of the model to correctly generate an output (Zhao et al., 2021). In contrast, studies by (Hase et al., 2023) have observed that information about some of the same entities or entity relations can be modified by making edits to a variety of layers in the model architecture, and therefore, that facts are stored across layers in a fragmented fashion.\nModel compression. Neural network pruning methods (LeCun et al., 1989; Hassibi & Stork, 1992; Han et al., 2015; Li et al., 2016; Frankle & Carbin, 2018) have found that models could be significantly pruned (often to removing over 90% of parameters) with very little drop in accuracy, significantly reducing the storage requirements of the model. There have also been proposed approaches that prune these models in a structured manner, to facilitate improvements in inference time (Molchanov et al., 2016). The existence of sparse sub-networks (Frankle & Carbin, 2018; Hoefler et al., 2021) has been found to be true for convolutional neural networks and fully connected networks and Transformer models (Lv et al., 2023; Murty et al., 2022). To our knowledge, model pruning techniques have always done a unilateral reduction across all parameters, without targeting any specific layers \u2014 leading to predictive performance either staying the same or decreasing (Frankle & Carbin, 2018). In this work, however, we find that the effect of reduction on accuracy is non-uniform across different layer types; performance degradation can be found by reducing early layers, while significant performance benefits are available, often by pruning the later layers.\nLow-rank approximations of weight matrices. Most pruning methods reduce parameters in order of their absolute magnitude (Frankle & Carbin, 2018). Another approach to model approximation is to reduce the rank of its constituent weight matrices, keeping the top k components found by SVD. While matrices of neural models, including Transformer models, have been found to be wellapproximated using this approach (Lv et al., 2023; Hajimolahoseini et al., 2021; Yu et al., 2017), where markedly reduced versions of the model can preserve its behavior, research has shown that performance eventually declines as the severity of the intervention increases. Note that these reductions are typically done unilaterally, removing the same number of components in every weight matrix in the model. In contrast to these findings, we show that a targeted rank reduction, affecting only a single weight matrix, can offer significant benefits to the predictive accuracy of Transformers.\nModel distillation and low-rank training. Ba & Caruana (2014); Hinton et al. (2015) have trained smaller networks to mimic the behavior of larger networks, showing neural networks might be significantly over-parametrized and can be replaced with efficient versions of the same. To our knowledge, no report of an improvement in the model\u2019s predictions as a consequence of this procedure has been shown. (Yang et al., 2020) have enforced low-rank-ness of weight matrices for the purposes of memory efficiency, but the resulting models fail to achieve performance equivalent to their overparametrized counterparts. The result suggests that overparametrization is helpful for the identification of well-generalizing parameters by SGD (Bengio et al., 2005; Hinton et al., 2015; Zhang et al., 2017)."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "We review basic notations first and then describe the core components of our study.\nMaths Notation. We use R to denote real numbers, N to denote natural numbers, small letters such as v \u2208 Rd to denote a d-dimensional vector, and capital letters such as W \u2208 Rm\u00d7n to denote a matrix of size m\u00d7 n. We use \u2225v\u22252 to denote the Euclidean norm of a vector v and \u2225W\u22252 to denote the spectral norm of a matrixW . We use [N ] to denote the set {1, 2, \u00b7 \u00b7 \u00b7 , N}. We will use rank(W ) to denote the rank of a matrix W and \u03c3\u2193i (W ) to denote its i th largest singular value.\nTransformer Architecture. We provide a concise description of vanilla Transformer architecture that is relevant to our analysis. A Transformer architecture can be thought of as L layers of Transformer blocks. The lth block maps a sequence of T -length vector sequence (h(l\u22121)1 , \u00b7 \u00b7 \u00b7 , h (l\u22121) T ) to another T -length vector sequence (h(l)1 , \u00b7 \u00b7 \u00b7 , h (l) T ), where all vectors are d-dimensional. This transformation is accomplished using two sequential steps: a self-attention mechanism to mix information across time steps, and a feed-forward network to process information within each time step.\nWe describe a basic version of these transformations for a fixed lth layer and drop the superscript (l \u2212 1) for clarity.1\nA single-head self-attention mechanism first maps each vector hi to a query vector qi = Wqhi, a key vector ki = Wkhi and a value vector vi = Wvhi where Wq,Wk,Wv \u2208 Rd\u00d7d are layerspecific weight matrices. We then compute attention probabilities p(j | i) = exp(q \u22a4 i kj/ \u221a d)\u2211T\nl=1 exp(q \u22a4 i kl/\n\u221a d) for every i, j \u2208 [T ]. These are used to compute the attention vector zi = \u2211T\nj=1 p(j | i)vj . A k-head self-attention computes a set of k attention vectors by using different linear transformations for key, query, and value, and then concatenates these attention vectors. These k-separate linear transformations for key, query, and value can all be absorbed into their respective matrices Wq \u2208 Rd\u00d7dk,Wk \u2208 Rd\u00d7dk and Wv \u2208 Rd\u00d7dk. Finally, the self-attention mechanism outputs ui = ziWo+ hi using a projection matrix Wo \u2208 Rdk\u00d7d. The feed-forward step applies a 2-layer multi-layer perception (MLP) \u03c8 : Rd \u2192 Rd to each vector ui \u2208 Rd separately. The MLP typically has a ReLu (?) or GELU activation function (Hendrycks & Gimpel, 2016) and in some models such as Llama, the bias of linear layers is set to 0. We denote the weight matrices of the first and second linear layers of this MLP by Uin and Uout respectively. The output of this lth Transformer block is then given by h(l)i = \u03c8(ui) + ui.\nIn summary, a Transformer architecture has the following weight matrices W = {Wq,Wk,Wv,Wo, Uin, Uout} for each layer, in addition to the embedding matrix for embedding input tokens, a projection weight matrix applied after the final layer before taking softmax, and all weight matrices associated with layer normalization. In our work, we will focus primarily on the matrices in W and intervene by modifying them. Rank-r Approximation and SVD. Given a matrix W \u2208 Rm\u00d7n and r \u2208 N, a rank-r approximation problem requires finding a matrix W\u0302 that minimizes \u2225W \u2212 W\u0302\u22252 and satisfies rank ( W\u0302 ) \u2264 r.\nEckart\u2013Young\u2013Mirsky theorem provides an optimal solution of this problem using Singular Value Decomposition (SVD) (Eckart & Young, 1936). Formally, an SVD of a matrix W is given by W = U\u03a3V \u22a4 where U = [u1, u2, \u00b7 \u00b7 \u00b7 , um] \u2208 Rm\u00d7m and V = [v1, v2, \u00b7 \u00b7 \u00b7 , vn] \u2208 Rn\u00d7n and \u03a3 \u2208 Rm\u00d7n. The column vectors of U and V constitute an orthonormal basis of Rm and Rn respectively, and \u03a3 is a diagonal matrix whose diagonal entries are given by the singular values of W in descending order. One can also express the SVD of W as W = \u2211min{m,n} i=1 \u03c3 \u2193 i (W )uiv \u22a4 i .\nAccording to Eckart\u2013Young\u2013Mirsky theorem, the matrix W\u0302 = \u2211r\ni=1 \u03c3 \u2193 i (W )uiv \u22a4 i is an optimal\nsolution to the rank-r approximation problem for any given desired rank r \u2264 min{m,n}. In this work, we will use the word higher-ordered components to refer to entries in the SVD corresponding to the components with smaller singular values. These components are removed by LASER. The term lower-ordered components is used to refer to singular vectors corresponding to large singular values. These components are kept in a low-rank approximation of the matrix.\n4 LAYER SELECTIVE RANK REDUCTION (LASER) In this section, we formally describe the LASER intervention. A single-step LASER intervention is defined by three quantities (\u03c4, \u2113, \u03c1): a parameter type (\u03c4 ), layer number (\u2113), and rate reduction (\u03c1). These values together describe which matrix will be replaced by their low-rank approximations and how severe the approximations will be. The parameter type describes which matrix type we are going to intervene in. We focus on the matrices in W = {Wq,Wk,Wv,Wo, Uin, Uout} which consist of the matrices in the MLP and attention layers. The layer number describes the layer at which we intervene (the first layer is indexed from 0). E.g., the Llama-2 has 32 layers and so \u2113 \u2208 {0, 1, 2, \u00b7 \u00b7 \u00b7 31}. Finally, \u03c1 \u2208 [0, 1) describes what fraction of the maximum rank should be preserved upon doing its low-rank approximation. For example, let \u03c4 = Uin \u2208 Rd\u00d7d, then the maximum rank of this matrix is d. We replace it with a rank \u230a\u03c1 \u00b7 d\u230b-approximation. Figure 1 shows an example of LASER. In this figure, we have \u03c4 = Uin and \u2113 = L indicating that we update the weight matrix in the first layer of MLP in the Transformer block of the Lth layer. The other parameter (not shown in the Figure) controls the k in the rank-k approximation.\n1Various Transformer models often have small differences in how these transformations are implemented. Our goal is not to provide a full survey of these details but to capture essential terminology for our results.\nLo ss\nLo ss\nLayer number Layer number Layer number Reduction %: 10 25 40 50 60 75 90 92.5 95 97.5 98 98.5 99 99.5 99.75\nFigure 2: The effect of rank reduction across different layer types is not uniform. This figure shows the effect of rank reduction for GPT-J as studied on the CounterFact dataset. The dashed line is the base model loss. In the attention layers (key, query, value, out matrices), while its clear matrices could be significantly rank-reduced without damaging the learned hypothesis, there is very little performance increase. Alternatively, for the multi-layer perceptron (MLP) layers, rank reduction goes from uniformly harming to improving the model\u2019s performance (at layer 20).\nLASER throttles the flow of certain information in the network, which surprisingly can produce significant performance benefits. These interventions can also be easily composed\u2014we can apply a set of interventions {(\u03c4i, \u2113i, \u03c1i)}mi=1 in any order. The LASER approach is to simply search over interventions of this type, and to exercise the modification that offers most benefits. There are many other ways in which one can add and compose these interventions, however, we defer this to future work."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "This section studies the consequences of LASER throughout various layers of the Transformer architecture. We first perform a motivating analysis of the CounterFact question-answering dataset in conjunction with a pretrained GPT-J model, and investigate the performance of the model and its variability as we search over potential interventions. Following that, we look at the effect of LASER across different models, datasets and modalities.\nGPT-J, CounterFact and PILE. We use the GPT-J model with 27 layers and 6B parameters pretrained on the PILE dataset. The first part of the analysis focuses on GPT-J primarily because its training data is available and analyzed. We evaluate the model\u2019s behavior on the CounterFact dataset. Every datapoint in this dataset contains entries (subject, relation, answer) and three paraphrased prompts for each question. For example: (Danielle Darrieux, mother tongue, French)."
        },
        {
            "heading": "5.1 A THOROUGH ANALYSIS WITH GPT-J ON THE COUNTERFACT DATASET",
            "text": "Figure 2 shows the result of applying various amounts of rank reduction to each matrix in the Transformer architecture on the classification loss for this dataset. These plots are grouped, such that each sub-figure corresponds only to the indicated type of weight matrices. Note that each Transformer layer consists of a small, two-layer MLP. The constituent input and output matrices are shown separately. Different colors indicate different percentages of components being removed.\nThe attention plots in this figure exemplify what is already known about these models\u2014weight matrices can be drastically reduced without much degradation in model performance. The more interesting result, however, is in the MLP layers. Here, not only can matrices be rank-reduced\nwithout degrading classification performance, but large improvements are seen in later layers of the model. This trend is most stark in the input matrix of the MLP. While there are gains with LASER in the attention layers too, the benefits are smaller. In the section that follows, we demonstrate the effectiveness of LASER across a wide array of datasets and Transformer models. Because a thorough search can be computationally intensive, and consistent improvements seem concentrated to reducing the MLP layers, all results that follow this section consider a reduced search over only these layers unless stated otherwise.\nImproved accuracy and robustness to paraphrases. The CounterFact dataset is used to test the model\u2019s factual knowledge of data from Wikipedia. Since GPT-J is trained on PILE, whose contents include Wikidata, different facts in CounterFact are part of the model\u2019s training data, albeit in different quantities. As all answers are a single token in this setting, we compute top-k accuracy based on whether the correct answer is in the top-k predicted tokens. As seen in Fig. 2 and Table. 1, we find that the model\u2019s top-1 accuracy on facts in CounterFact increases from 13.3% to 24.1% when reductions are done on a single layer. It is important to note that these improvements are a result of rank-reduction alone, and do not involve any further training or fine-tuning of the pretrained GPT-J model. Furthermore, the improvements that come with rank-reduction are systematic. The set of datapoints that the model gets correct only grows with increasing amounts of reduction as opposed to a random movement of datapoints into and out of the set or correct items; if a model gets an answer right with a certain amount of rank reduction (x), the model continues to get the answer correct for larger rank reductions (y where y > x). We evaluate the model\u2019s robustness to paraphrases by computing the percentage of datapoints where the model gets all paraphrases of a given question correct. For datapoints that the model already gets correct, the model\u2019s robustness to paraphrases also improves with LASER by roughly 24.8 percentage points.\nStacking improvements from reduction across layers. We find that even further improvements in the model\u2019s performance can be made by simultaneously performing different amounts of rank reduction across the different layers. The top-10 accuracy of the base GPTJ model is 43.6%. After\ndoing the best single-step LASER it went up to 52%. A naive strategy of composing LASER by performing the maximally improving reduction in each layer further improved the top-10 accuracy to 59.8%. This is a marked, 16.2% absolute improvement in accuracy over the base model.\nEffect on language modeling and fluency. While the model\u2019s factuality improves, does the reduction affect the model\u2019s performance on other metrics? To understand this, we evaluate the model\u2019s perplexity, i.e., its original training objective, on its training data. For layers corresponding to the MLP input matrices, the perplexity of the model increases from 4.8 to 5.0, showing that the language modeling objective is indeed slightly affected. For the MLP output layers, the perplexity of GPT-J on PILE increases from 4.8 to 4.9 with LASER. It may be possible to fix this small degradation by calibrating the temperature of the model."
        },
        {
            "heading": "5.1.1 WHICH FACTS IN THE DATASET ARE RECOVERED BY RANK REDUCTION?",
            "text": "To understand this phenomenon, we look at the questions correctly answered after LASER and the effect of how often the information associated with the question appears in the training data. For every datapoint in CounterFact, we retrieve all the examples in PILE that contain a mention of both the entity and the answer. We then compute the frequency of how often information associated with each evaluation question appears in the training data. We find that the facts recovered on rank reduction are most likely to be infrequently present in the data (Figure 3)."
        },
        {
            "heading": "5.1.2 WHAT ARE HIGHER ORDER COMPONENTS STORING?",
            "text": "We saw above how retaining the lower-ordered components improves model performance on the task of open-ended question answering. We also saw that for the task of question answering the gains come on questions whose answers are supported by less frequently occurring data in the training set. While it is clear that eliminating the higher ordered components \u201cdenoises\u201d the model and helps recover \u201chidden,\u201d less-frequent information, several questions arise. First, why are the higher-ordered components noisy? Second, what are the higher-ordered components computing that their removal\nimproves model performance? This section studies the above two questions using the CounterFact dataset and GPT-J.\nTo understand what the higher-ordered components are representing, we approximate the final weight matrix using its higher-ordered components (rather than the low-order components used by LASER). We analyze how the model\u2019s behavior changes on datapoints that GPT-J originally gets incorrect but are flipped to being correct upon performing LASER.\nFirst, we note that when the original, unmodified model does not answer these questions correctly, it often responds with common words, such as \u201ca,\u201d \u201cthe,\u201d \u201cof,\u201d and other highly frequent tokens. After performing LASER, where we retain the top-k components, the model\u2019s answers to these questions flip from generic words to the correct entity. For the same datapoints, when we approximate the model by instead retaining the higher-ordered components, we find that the model predicts incorrect entities that are of the same semantic type as the correct answer. However, as we include more lower-ordered components, the model\u2019s output changes to predicting these common words tokens.\nWe hypothesize that these matrices often encode multiple conflicting responses, and that when all components are used they clash to produce a generic token. Removing the higher-order components, which anecdotally appear to often capture incorrect responses of the correct type, resolves this internal conflict and allows the model to respond accurately."
        },
        {
            "heading": "5.2 HOW GENERALLY DOES THIS HOLD?",
            "text": "Open-ended question answering. To evaluate the effect of LASER on the model\u2019s factual knowledge we evaluate the model\u2019s performance before and after LASER on four question answering datasets, including CounterFact (Meng et al., 2022), HotPotQA (Yang et al., 2018), Fever (Thorne et al., 2018), and Bias in Bios (De-Arteaga et al., 2019). While CounterFact, HotPotQA and Fever test the model\u2019s factuality, Bias in Bios more broadly tests the language model\u2019s reasoning and language understanding abilities alongside factuality. Model interventions are selected based on a validation set, and results are reported on the test set. Datasets that only provide a test set are split into a separate validation and test set. The models used for the task of question answering include, Roberta, GPT-J (6B), and LLAMA2 (7B).\nEvaluation metrics. For each of these tasks, we evaluate the model\u2019s performance on a range of metrics as described including: (i) 0-1 accuracy. We generate a sequence of N tokens using the LLM and then report 1 if the answer text is in the generated text and 0 otherwise, (ii) top-k. If the answer is in the top-k predicted tokens, (iii) acc. If the answer lies in a small set of values, we compute if the correct answer has the highest log probabilities, (iv) loss. We report the log-loss of the true data. We report log-loss for all settings. We test the generality of this result by evaluating a collection of language models on different benchmarks. As seen in Table. 1, we find that even severe reductions result in no deterioration in the model\u2019s accuracy and can lead to improvements in their performance. The amount of reduction required differs from model to model.\nModel Name Dataset\nCIFAR-10\n(aug.) (no aug.) SVHN\nViT (2 patches) 79.03 67.73 84.80 with LASER 79.32 68.26 84.94 ViT (4 patches) 79.80 65.97 87.58 with LASER 79.96 66.19 87.72\nTable 2: Effect of LASER on ViT for the task of image classification on CIFAR-10 and SVHN.\nModel Name Accuracy Return Transformer 50.67% 0.575\nwith LASER 53% 0.965\nTable 3: Effect on LASER on a 6-layer Decision Transformer agent. The base model is trained and evaluated in a challenging 10\u00d7 10 Sokoban domain."
        },
        {
            "heading": "5.3 NON-TEXT DOMAINS",
            "text": "To understand if this phenomenon may have any significance outside the task of Question Answering in the textual domain and extends to non-linguistic tasks, we evaluate the effect of rank reduction on decision-making and computer vision tasks.\nPolicy learning. For Policy learning, we evaluate the effect of LASER on a decision Transformer model trained on the game of Sokoban and evaluated on the same game. This is a challenging planning problem where the agent has to move and push several blocks to holes. The task is completed when all blocks are on top of holes. The input to the decision Transformer is the visual state of the environment at a given state, and the output is the low-level action. We find that for a decision Transformer trained on Sokoban, models solved 3% more tasks with LASER (Table 3). Details of the experiment can be found in the Supplementary.\nImage classification. For the task of image classification, we train a vision Transformer (ViT) model on the task of image classification on CIFAR-10 and SVHN. The model is trained both with and without data augmentation. The data augmentation method includes resizing, cropping, and rotating images. This was done to verify if the improvements in the model\u2019s performance with the interventions change with data augmentation. In the case of a ViT trained on CIFAR, we find a slight increase in the model\u2019s performance on the image classification task (Table 2).\nAlthough the improvements are much smaller, they are consistent despite the severity with which reductions are performed. This can be because the phenomenon is either text-specific or requires a large enough Transformer model."
        },
        {
            "heading": "6 CONCLUSION AND DISCUSSION",
            "text": "This paper describes LASER, a phenomenon where performing a low-rank approximation of specific layer types at specific layers of the transformer block can improve the performance of LLMs on the task of Question Answering. We find this to be true across five different question-answering datasets and three different Transformer models. We also observe performance gains for a decision Transformer in an embodied domain and weakly in a vision Transformer on the task of image classification. We find that improvements in the accuracy of the model are on information that is less frequent in the training data and that LASER jointly makes the model more robust to paraphrases of the questions. We further found that the higher-ordered components of some of these matrices encode either high-frequency words or alternate answers of the same semantic type as the correct answer. These noisy, higher-ordered components can overpower the stable lower-ordered components and result in the model answering questions incorrectly. In these cases, performing LASER acts as a denoising technique and reduces the internal conflicts present in potential responses.\nThe paper highlights an interesting phenomenon, where deleting information in a model helps rather than hinders the performance. It is counter-intuitive and requires further study. Learning (i) why higher-ordered components in weight matrices accumulate noisy answers in the course of training and (ii) why this is specifically true for later layers in the MLP is important to not only for our understanding of the success of LASER, but for understanding the behavior of LLMs more generally."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A DATASET DETAILS",
            "text": "CounterFact. The CounterFact dataset is derived from the PARAREL dataset ? and contains knowledge tuples of the kind tc = (s, r, oc), where s is the subject, r is the relation and o is the object. These tuples are constructed using entities listed in WikiData. The datapoints are accompanied by handwritten prompt templates for each category. The CounterFact dataset also contains suggested edits to the true facts represented in the dataset. For this study, the set of counterfactual edits are not used.\nPILE. The PILE dataset is an approximately 1TB language modeling dataset that was used to pre-train GPT-J. It contains text from 22 smaller datasets, including Wikipedia, OpenWebText2, and StackExchange, to name a few. The PILE dataset was used to study the effect of LASER on the behavior of the model on the original training data distribution. For the study on quantifying the occurrences of entities in the training data, the training data split of PILE was used. However, the measure of change in perplexity of the model after LASER was measured on the validation split of the dataset.\nHotpotQA. We use the HotPotQA dataset available on HuggingFace. An example question is \u201cWhat are the names of the current members of American heavy metal band who wrote the music for Hurt Locker The Musical?\u201d and the answer is \u201cHetfield and Ulrich, longtime lead guitarist Kirk Hammett, and bassist Robert Trujillo\u201d. We use the validation split of this dataset that contains 7100 datapoints and an equal number of datapoints from the train split.\nFever. FEVER (Fact Extraction and Verification) dataset consists of claims and a label of whether those claims are true or false. The goal is to predict the label given the claim. The dataset consists of 185,445 such claims and is constructed using data from Wikipedia. An example claim is \u201cRoman Atwood is a content creator.\u201d and the label is \u201ctrue\u201d.\nBias in Bios. Bias in Bios is a dataset that consists of descriptions of people and the task is to correctly guess the gender and occupation of the person. An example datapoint is \u201cNancy Lee graduated from Lehigh University, with honours in 1998. Nancy has years of experience in weight loss surgery, patient support, education, and diabetes\u201d, the gender label is \u201cFemale\u201d and the profession is \u201cNurse\u201d.\nIn addition to the previous datasets that were evaluated in Table 1, we also provide additional experiments with the following datasets in the Appendix.\nEpistemic Reasoning: The epistemic reasoning dataset tests the model\u2019s ability to make entailment decisions from a pair of sentences. Specifically, epistemic reasoning, assesses models\u2019 understanding of the \u2018theory of mind,\u2019 which involves the capacity to analyze the cognitive states of various agents. A datapoint consists of a text containing a premise and hypothesis and a label which is either \u2018Entailment\u2018 or \u2018Non-entailment\u2019. An example datapoint is \u201cPremise: Emma knows that James thinks that there is milk in the fridge. Hypothesis: James thinks that there is milk in the fridge.\u201d The label here is Entailment.\nTruthfulQA: TruthfulQA evaluates the extent to which models replicate human falsehoods. The benchmark contains 817 questions that span 38 categories, including health, law, finance and politics. The truthfulQA dataset emphasizes that solely increasing the scale of models may not be as effective in enhancing truthfulness as fine-tuning with training objectives that go beyond imitating text from the web. An example datapoint is \u201cWhat happens to you if you eat watermelon seeds? [ \u201dNothing happens\u201d, \u201dYou eat watermelon seeds\u201d, \u201dThe watermelon seeds pass through your digestive system\u201d, \u201dYou will not digest the watermelon seeds\u201d, \u201dThe watermelon seeds will be excreted\u201d , \u201dYou grow watermelons in your stomach\u201d, \u201dYou get sick\u201d, \u201dYou have bad dreams\u201d, \u201dYou die\u201d, \u201dYou get indigestion\u201d, \u201dYou fall unconscious\u201d, \u201dYou digest the watermelon seeds\u201d]\u201d.\nQA Wiki Data: The task evaluates a model\u2019s ability to answer open domain, cloze-style, question answering on randomly sampled Wikipedia triplets. This tests the model\u2019s world knowledge on a large collection of facts and information from a knowledge graph extracted from Wikipedia data. An example data point from this dataset is, \u201cGabon shares a border with Cameroon\u201d."
        },
        {
            "heading": "B DETAILS OF NON-TEXT DOMAINS",
            "text": "Sokoban Details. We show an image of the Sokoban task in Figure 5. The sokoban task is a warehouse-keeping transportation game that requires long-horizon reasoning and planning over multiple time steps. The task of the agent is to move all boxes to their target locations without getting locked in. We use the Gym Sokoban environment Schrader (2018). We train a 5-layer decision transformer model using 106 optimal episodes of this game. In our setting, the maximum return of the game is set to 10.\nCIFAR 10 / Vision Transformer. For the visual reasoning task, we train a vision transformer (ViT) model on data from CIFAR 10 and on the Street View House Numbers (SVHN) dataset. The ViT model used for this task contains TODO number of layers and TODO number of parameters. The model is trained to classify the images across these two datasets. Models are trained both with and without data augmentation."
        },
        {
            "heading": "C EXTENDED ANALYSIS",
            "text": ""
        },
        {
            "heading": "C.1 ADDITIONAL RESULTS",
            "text": "Table 4 shows the effect of LASER on three additional datasets. This includes Epistemic Reasoning from Big Bench Hard (BBH) (logic and reading comprehension), TruthfulQA (language model truthfulness), and QA Wiki Data from Big Bench (world knowledge). For this study, we only focus on GPT-J and LLAMA2 which are more powerful than Roberta. We use 20% of the dataset as validation and find the right LASER hyperparameters by choosing from a set that maximizes the\nvalidation accuracy. The results demonstrate notable improvements with LASER, similar to the results in Table 1 of the original paper."
        },
        {
            "heading": "C.2 ARE THE MATRICES ALREADY LOW-RANK?",
            "text": "We find that LASER approximated matrices with their low-rank approximations much beyond their effective rank as computed by (Roy & Vetterli, 2007). To study this, we computed the effective rank of the MLP matrices for which LASER helps for GPT-J model using the method described by Roy & Vetterli (2007). The plot shows that although matrices of the later layer have a lower effective rank than the earlier layers, the computed effective rank is significantly larger than the reduction % until which LASER helps."
        },
        {
            "heading": "C.3 DOES THE PERFORMANCE CONTINUE TO IMPROVE TILL THE RANK OF THE MATRIX IS ONE?",
            "text": "We see that for many of the matrices, as seen in Figure 2, in cases where reduction helps, with increasing amounts of rank-reduction, the model first monotonically improves before it starts to worsen. The point up to which it improves varies depending on the layer type and layer number. However, the monotonic improvement and worsening are observed consistently.\nWhat is the effect of removing the layer completely? We find that, removing the layer completely can be better than retaining its matrix with its full rank, however it is observed to be worse than the model with the low-rank approximation of the matrix."
        },
        {
            "heading": "C.4 ARE THE BENEFITS IN GENERALIZATION ACROSS TASKS COMING FROM LASER ON THE SAME LAYERS FOR A GIVEN MODEL?",
            "text": "We find that the maximum improvements on different tasks come from LASER on different layers of the model. Figure 7 shows that for GPT-J on different tasks, the best-performing models across tasks have reduced matrices in different layers."
        },
        {
            "heading": "C.5 MEASURING PERPLEXITY ON PILE.",
            "text": "To measure the effect of the interventions on language modelling, we compute the perplexity of the reduced model on the evaluation set of PILE. The perplexity of the fixed-length GPT-J model is\nevaluated using the sliding window strategy over the sequence of tokens with a stride of 512 tokens. While there is an improvement in the task at hand, the model\u2019s perplexity worsens slightly. We don\u2019t yet fully understand what the worsening in perplexity of the model corresponds to and leave this for future study.\nC.6 FINAL LASER SEARCH RESULTS\nTable 5 shows the final search results of LASER for models and datasets from Table 1. These values are obtained by reporting the optimal LASER parameters that maximize the validation accuracy. Similarly, Table 6 provide search results for the additional experiments in Table 4. The results show that the optimal improvements in the models typically come from later layers in the transformer model, typically from reducing the MLP Input matrix. For reference, recall that Llama2 has 32 layers, GPTJ has 28 layers, and Roberta has 12 layers. The magnitudes of reduction are also quite large, with the rank at times being reduced to 1% of the original matrix\u2019s rank."
        },
        {
            "heading": "D OTHER TYPES OF MATRIX APPROXIMATION",
            "text": "Besides approximating the weight matrices of the LLMs with their rank-k approximations we also tried to approximate the matrices by Absolute Weight Pruning (Frankle & Carbin, 2018). Here, we zero out a fraction of the weights of the matrix by their absolute magnitude. The results for GPT-J on Counterfact can be seen in Figure 9. In this case, too, we find that the accuracy of the model on the task increases with pruning later layers of the MLP. However, we leave further study of why that is for future work.\nE IMPLEMENTATION DETAILS"
        },
        {
            "heading": "E.1 CODE",
            "text": "We use PyTorch for all experiments. We use the HuggingFace implementation for all three large language models. We use Llama2 7GB weights provided by Meta. We use the SVD implementation available in PyTorch to do the experiments. The code can be found at:"
        },
        {
            "heading": "E.2 COMPUTE DETAILS",
            "text": "We ran each experiment on a cluster with V100 and A2600. Each experiment took about 1-3hrs to finish. For all setting, we search over hyperparameters listed in Table 7. For the GPTJ+CounterFact setting, depending on the experiment and plots, we run a much more fine-grained search over each hyperparameter."
        },
        {
            "heading": "F FUTURE DIRECTIONS",
            "text": "We discuss important directions for future research below.\nReduction %: 10 25 40 50 60 75 90 92.5 95 97.5 98 98.5 99 99.5 99.75\nFigure 9: Besides LASER, we also observe an improvement in model performance with Layerselective Absolute weight pruning. Understanding the extent of this and the connections between the two is an important direction for future research.\nLASER hyperparameter Search Space \u03c4 MLP weight matrices Uin and Uout \u2113 all layers in the model \u03c1 {0.9, 0.8, 0.6, 0.2, 0.1, 0.05, 0.01}\nTable 7: LASER hyperparameters\nAre benefits of LASER model dependent? Although broadly, the improvements in GPT-J are more pronounced than in LLAMA2 or Roberta, there are some domains where improvements in LLAMA2 (FEVER) and Roberta (Bias in bios) are more significant than in GPT-J as well. To study this, there are several possible causes that we believe merit investigation, including the capacity of the model, the amount of training data, and the particulars of the optimization procedure.\nEffect of structural choices in how and what information is stored in models. Examining how structural choices in model design affect the low-rank and high-rank components of the weight matrices of a transformer model is an intriguing question that could advance our understanding of this phenomenon and transformer models in general."
        }
    ],
    "title": "THE TRUTH IS IN THERE: IMPROVING REASONING",
    "year": 2023
}