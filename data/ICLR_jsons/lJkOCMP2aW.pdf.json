{
    "abstractText": "Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multiscale Transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of Pathformer. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios. The code is made available at https://github.com/decisionintelligence/pathformer.",
    "authors": [
        {
            "affiliations": [],
            "name": "Peng Chen"
        },
        {
            "affiliations": [],
            "name": "Yingying Zhang"
        },
        {
            "affiliations": [],
            "name": "Yunyao Cheng"
        },
        {
            "affiliations": [],
            "name": "Yang Shu"
        },
        {
            "affiliations": [],
            "name": "Yihang Wang"
        },
        {
            "affiliations": [],
            "name": "Qingsong Wen"
        },
        {
            "affiliations": [],
            "name": "Bin Yang"
        },
        {
            "affiliations": [],
            "name": "Chenjuan Guo"
        }
    ],
    "id": "SP:11d26bd569c926dba75cc707d00f91d97f9d1b33",
    "references": [
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "David Campos",
                "Tung Kieu",
                "Chenjuan Guo",
                "Feiteng Huang",
                "Kai Zheng",
                "Bin Yang",
                "Christian S. Jensen"
            ],
            "title": "Unsupervised time series outlier detection with diversity-driven convolutional ensembles",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2022
        },
        {
            "authors": [
                "Cristian Challu",
                "Kin G. Olivares",
                "Boris N. Oreshkin",
                "Federico Garza Ram\u0131\u0301rez",
                "Max Mergenthaler Canseco",
                "Artur Dubrawski"
            ],
            "title": "NHITS: neural hierarchical interpolation for time series forecasting",
            "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),",
            "year": 2023
        },
        {
            "authors": [
                "Cathy WS Chen",
                "Richard Gerlach",
                "Edward MH Lin",
                "WCW Lee"
            ],
            "title": "Bayesian forecasting for financial risk management, pre and post the global financial crisis",
            "venue": "Journal of Forecasting,",
            "year": 2012
        },
        {
            "authors": [
                "Weiqi Chen",
                "Wenwei Wang",
                "Bingqing Peng",
                "Qingsong Wen",
                "Tian Zhou",
                "Liang Sun"
            ],
            "title": "Learning to rotate: Quaternion transformer for complicated periodical time series forecasting",
            "venue": "In International Conference on Knowledge Discovery & Data Mining (KDD),",
            "year": 2022
        },
        {
            "authors": [
                "Yunyao Cheng",
                "Peng Chen",
                "Chenjuan Guo",
                "Kai Zhao",
                "Qingsong Wen",
                "Bin Yang",
                "Christian S. Jensen"
            ],
            "title": "Weakly guided adaptation for robust time series forecasting",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2024
        },
        {
            "authors": [
                "Junyoung Chung",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "KyungHyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "year": 2014
        },
        {
            "authors": [
                "Razvan-Gabriel Cirstea",
                "Bin Yang",
                "Chenjuan Guo"
            ],
            "title": "Graph attention recurrent neural networks for correlated time series forecasting",
            "venue": "In International Conference on Knowledge Discovery & Data Mining (KDD),",
            "year": 2019
        },
        {
            "authors": [
                "Razvan-Gabriel Cirstea",
                "Tung Kieu",
                "Chenjuan Guo",
                "Bin Yang",
                "Sinno Jialin Pan"
            ],
            "title": "EnhanceNet: Plugin neural networks for enhancing correlated time series forecasting",
            "venue": "In IEEE International Conference on Data Engineering (ICDE),",
            "year": 2021
        },
        {
            "authors": [
                "Razvan-Gabriel Cirstea",
                "Chenjuan Guo",
                "Bin Yang",
                "Tung Kieu",
                "Xuanyi Dong",
                "Shirui Pan"
            ],
            "title": "Triformer: Triangular, variable-specific attentions for long sequence multivariate time series forecasting",
            "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),",
            "year": 2022
        },
        {
            "authors": [
                "Razvan-Gabriel Cirstea",
                "Bin Yang",
                "Chenjuan Guo",
                "Tung Kieu",
                "Shirui Pan"
            ],
            "title": "Towards spatiotemporal aware traffic time series forecasting",
            "venue": "In IEEE International Conference on Data Engineering (ICDE),",
            "year": 2022
        },
        {
            "authors": [
                "James W Cooley",
                "John W Tukey"
            ],
            "title": "An algorithm for the machine calculation of complex fourier series",
            "venue": "Mathematics of computation,",
            "year": 1965
        },
        {
            "authors": [
                "Abhimanyu Das",
                "Weihao Kong",
                "Andrew Leach",
                "Rajat Sen",
                "Rose Yu"
            ],
            "title": "Long-term forecasting with tide: Time-series dense encoder",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Dean"
            ],
            "title": "Introducing pathways: A next-generation ai architecture, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Marco AR Ferreira",
                "David M Higdon",
                "Herbert KH Lee",
                "Mike West"
            ],
            "title": "Multi-scale and hidden resolution time series models",
            "year": 2006
        },
        {
            "authors": [
                "Ronghang Hu",
                "Amanpreet Singh",
                "Trevor Darrell",
                "Marcus Rohrbach"
            ],
            "title": "Iterative answer prediction with pointer-augmented multimodal transformers for textvqa",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Rob J Hyndman",
                "Yeasmin Khandakar"
            ],
            "title": "Automatic time series forecasting: the forecast package for r",
            "venue": "Journal of statistical software,",
            "year": 2008
        },
        {
            "authors": [
                "Ming Jin",
                "Huan Yee Koh",
                "Qingsong Wen",
                "Daniele Zambon",
                "Cesare Alippi",
                "Geoffrey I Webb",
                "Irwin King",
                "Shirui Pan"
            ],
            "title": "A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection. arXiv, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Ming Jin",
                "Shiyu Wang",
                "Lintao Ma",
                "Zhixuan Chu",
                "James Y Zhang",
                "Xiaoming Shi",
                "Pin-Yu Chen",
                "Yuxuan Liang",
                "Yuan-Fang Li",
                "Shirui Pan"
            ],
            "title": "Time-LLM: Time series forecasting by reprogramming large language models. arXiv, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Tung Kieu",
                "Bin Yang",
                "Chenjuan Guo",
                "Razvan-Gabriel Cirstea",
                "Yan Zhao",
                "Yale Song",
                "Christian S. Jensen"
            ],
            "title": "Anomaly detection in time series with robust variational quasi-recurrent autoencoders",
            "venue": "In IEEE International Conference on Data Engineering (ICDE),",
            "year": 2022
        },
        {
            "authors": [
                "Tung Kieu",
                "Bin Yang",
                "Chenjuan Guo",
                "Christian S. Jensen",
                "Yan Zhao",
                "Feiteng Huang",
                "Kai Zheng"
            ],
            "title": "Robust and explainable autoencoders for unsupervised time series outlier detection",
            "venue": "In IEEE International Conference on Data Engineering (ICDE),",
            "year": 2022
        },
        {
            "authors": [
                "Taesung Kim",
                "Jinhee Kim",
                "Yunwon Tae",
                "Cheonbok Park",
                "Jang-Ho Choi",
                "Jaegul Choo"
            ],
            "title": "Reversible instance normalization for accurate time-series forecasting against distribution shift",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Hao Li",
                "Jie Shao",
                "Kewen Liao",
                "Mingjian Tang"
            ],
            "title": "Do simpler statistical methods perform better in multivariate long sequence time-series forecasting",
            "venue": "In International Conference on Information & Knowledge Management (CIKM),",
            "year": 2022
        },
        {
            "authors": [
                "Shiyang Li",
                "Xiaoyong Jin",
                "Yao Xuan",
                "Xiyou Zhou",
                "Wenhu Chen",
                "Yu-Xiang Wang",
                "Xifeng Yan"
            ],
            "title": "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Yanghao Li",
                "Chao-Yuan Wu",
                "Haoqi Fan",
                "Karttikeya Mangalam",
                "Bo Xiong",
                "Jitendra Malik",
                "Christoph Feichtenhofer"
            ],
            "title": "Mvitv2: Improved multiscale vision transformers for classification and detection",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Minhao Liu",
                "Ailing Zeng",
                "Muxi Chen",
                "Zhijian Xu",
                "Qiuxia Lai",
                "Lingna Ma",
                "Qiang Xu"
            ],
            "title": "Scinet: Time series modeling and forecasting with sample convolution and interaction",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Shizhan Liu",
                "Hang Yu",
                "Cong Liao",
                "Jianguo Li",
                "Weiyao Lin",
                "Alex X. Liu",
                "Schahram Dustdar"
            ],
            "title": "Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Yong Liu",
                "Haixu Wu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Non-stationary transformers: Exploring the stationarity in time series forecasting",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Yu Ma",
                "Bin Yang",
                "Christian S. Jensen"
            ],
            "title": "Enabling time-dependent uncertain eco-weights for road networks",
            "venue": "In Proceedings of the ACM on Management of Data,",
            "year": 2014
        },
        {
            "authors": [
                "Hao Miao",
                "Yan Zhao",
                "Chenjuan Guo",
                "Bin Yang",
                "Zheng Kai",
                "Feiteng Huang",
                "Jiandong Xie",
                "Christian S. Jensen"
            ],
            "title": "A unified replay-based continuous learning framework for spatio-temporal prediction on streaming data",
            "venue": "In IEEE International Conference on Data Engineering (ICDE),",
            "year": 2024
        },
        {
            "authors": [
                "Michael Mozer"
            ],
            "title": "Induction of multiscale temporal structure",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 1991
        },
        {
            "authors": [
                "Zhicheng Pan",
                "Yihang Wang",
                "Yingying Zhang",
                "Sean Bin Yang",
                "Yunyao Cheng",
                "Peng Chen",
                "Chenjuan Guo",
                "Qingsong Wen",
                "Xiduo Tian",
                "Yunliang Dou"
            ],
            "title": "Magicscaler: Uncertainty-aware, predictive autoscaling",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2023
        },
        {
            "authors": [
                "Simon Aagaard Pedersen",
                "Bin Yang",
                "Christian S. Jensen"
            ],
            "title": "Anytime stochastic routing with hybrid learning",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2020
        },
        {
            "authors": [
                "Syama Sundar Rangapuram",
                "Matthias W. Seeger",
                "Jan Gasthaus",
                "Lorenzo Stella",
                "Yuyang Wang",
                "Tim Januschowski"
            ],
            "title": "Deep state space models for time series forecasting",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Rajat Sen",
                "Hsiang-Fu Yu",
                "Inderjit S. Dhillon"
            ],
            "title": "Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Mohammad Amin Shabani",
                "Amir H. Abdi",
                "Lili Meng",
                "Tristan Sylvain"
            ],
            "title": "Scaleformer: Iterative multi-scale refining transformers for time series forecasting",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need. Advances in neural information processing systems (NeurIPS), 2017",
            "year": 2017
        },
        {
            "authors": [
                "Huiqiang Wang",
                "Jian Peng",
                "Feihu Huang",
                "Jince Wang",
                "Junhui Chen",
                "Yifei Xiao"
            ],
            "title": "MICN: multiscale local and global context modeling for long-term series forecasting",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Junke Wang",
                "Zuxuan Wu",
                "Wenhao Ouyang",
                "Xintong Han",
                "Jingjing Chen",
                "Yu-Gang Jiang",
                "SerNam Lim"
            ],
            "title": "M2TR: multi-modal multi-scale transformers for deepfake detection",
            "venue": "In International Conference on Multimedia Retrieval (ICMR),",
            "year": 2022
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Wenxiao Wang",
                "Lu Yao",
                "Long Chen",
                "Binbin Lin",
                "Deng Cai",
                "Xiaofei He",
                "Wei Liu"
            ],
            "title": "Crossformer: A versatile vision transformer hinging on cross-scale attention",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Qingsong Wen",
                "Tian Zhou",
                "Chaoli Zhang",
                "Weiqi Chen",
                "Ziqing Ma",
                "Junchi Yan",
                "Liang Sun"
            ],
            "title": "Transformers in time series: A survey",
            "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),",
            "year": 2023
        },
        {
            "authors": [
                "Ruofeng Wen",
                "Kari Torkkola",
                "Balakrishnan Narayanaswamy",
                "Dhruv Madeka"
            ],
            "title": "A multi-horizon quantile recurrent forecaster",
            "year": 2017
        },
        {
            "authors": [
                "Haixu Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Haixu Wu",
                "Tengge Hu",
                "Yong Liu",
                "Hang Zhou",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Timesnet: Temporal 2d-variation modeling for general time series analysis",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Xinle Wu",
                "Dalin Zhang",
                "Chenjuan Guo",
                "Chaoyang He",
                "Bin Yang",
                "Christian S. Jensen"
            ],
            "title": "AutoCTS: Automated correlated time series forecasting",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2022
        },
        {
            "authors": [
                "Xinle Wu",
                "Dalin Zhang",
                "Miao Zhang",
                "Chenjuan Guo",
                "Bin Yang",
                "Christian S. Jensen"
            ],
            "title": "AutoCTS+: Joint neural architecture and hyperparameter search for correlated time series forecasting",
            "venue": "Proceedings of the ACM on Management of Data,",
            "year": 2023
        },
        {
            "authors": [
                "Zonghan Wu",
                "Shirui Pan",
                "Guodong Long",
                "Jing Jiang",
                "Xiaojun Chang",
                "Chengqi Zhang"
            ],
            "title": "Connecting the dots: Multivariate time series forecasting with graph neural networks",
            "venue": "In International Conference on Knowledge Discovery & Data Mining (KDD),",
            "year": 2020
        },
        {
            "authors": [
                "Ailing Zeng",
                "Muxi Chen",
                "Lei Zhang",
                "Qiang Xu"
            ],
            "title": "Are transformers effective for time series forecasting",
            "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),",
            "year": 2023
        },
        {
            "authors": [
                "Kai Zhao",
                "Chenjuan Guo",
                "Peng Han",
                "Miao Zhang",
                "Yunyao Cheng",
                "Bin Yang"
            ],
            "title": "Multiple time series forecasting with dynamic graph modeling",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2024
        },
        {
            "authors": [
                "Haoyi Zhou",
                "Shanghang Zhang",
                "Jieqi Peng",
                "Shuai Zhang",
                "Jianxin Li",
                "Hui Xiong",
                "Wancai Zhang"
            ],
            "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
            "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),",
            "year": 2021
        },
        {
            "authors": [
                "Tian Zhou",
                "Ziqing Ma",
                "Qingsong Wen",
                "Xue Wang",
                "Liang Sun",
                "Rong Jin"
            ],
            "title": "FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Tian Zhou",
                "Peisong Niu",
                "Xue Wang",
                "Liang Sun",
                "Rong Jin"
            ],
            "title": "One fits all: Power general time series analysis by pretrained lm",
            "year": 2023
        },
        {
            "authors": [
                "Zhaoyang Zhu",
                "Weiqi Chen",
                "Rui Xia",
                "Tian Zhou",
                "Peisong Niu",
                "Bingqing Peng",
                "Wenwei Wang",
                "Hengbo Liu",
                "Ziqing Ma",
                "Xinyue Gu"
            ],
            "title": "Energy forecasting with robust, flexible, and explainable machine learning",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Time series forecasting is an essential function for various industries, such as energy, finance, traffic, logistics, and cloud computing (Chen et al., 2012; Cirstea et al., 2022b; Ma et al., 2014; Zhu et al., 2023; Pan et al., 2023; Pedersen et al., 2020), and is also a foundational building block for other time series analytics, e.g., outlier detection Campos et al. (2022); Kieu et al. (2022b). Motivated by its widespread application in sequence modeling and impressive success in various fields such as CV and NLP (Dosovitskiy et al., 2021; Brown et al., 2020), Transformer (Vaswani et al., 2017) receives emerging attention in time series (Wen et al., 2023; Wu et al., 2021; Chen et al., 2022; Liu et al., 2022c). Despite the growing performance, recent works have started to challenge the existing designs of Transformers for time series forecasting by proposing simpler linear models with better performance (Zeng et al., 2023). While the capabilities of Transformers are still promising in time series forecasting (Nie et al., 2023), it calls for better designs and adaptations to fulfill its potential.\nReal-world time series exhibit diverse variations and fluctuations at different temporal scales. For instance, the utilization of CPU, GPU, and memory resources in cloud computing reveals unique temporal patterns spanning daily, monthly, and seasonal scales Pan et al. (2023). This calls for multi-scale modeling (Mozer, 1991; Ferreira et al., 2006) for time series forecasting, which extracts temporal features and dependencies from various scales of temporal intervals. There are two aspects to consider for multiple scales in time series: temporal resolution and temporal distance. Temporal resolution corresponds to how we view the time series in the model and determines the length of each temporal patch or unit considered for modeling. In Figure 1, the same time series can be divided\n\u2217Part of the work was done during the internship at Alibaba Group. \u2020Corresponding author\ninto small patches (blue) or large ones (yellow), leading to fine-grained or coarse-grained temporal characteristics. Temporal distance corresponds to how we explicitly model temporal dependencies and determines the distances between the time steps considered for temporal modeling. In Figure 1, the black arrows model the relations between nearby time steps, forming local details, while the colored arrows model time steps across long ranges, forming global correlations.\nTo further explore the capability of extracting correlations in Transformers for time series forecasting, in this paper, we focus on the aspect of enhancing multi-scale modeling with the Transformer architecture. Two main challenges limit the effective multi-scale modeling in Transformers. The first challenge is the incompleteness of multi-scale modeling. Viewing the data from different temporal resolutions implicitly influences the scale of the subsequent modeling process (Shabani et al., 2023). However, simply changing temporal resolutions cannot emphasize temporal dependencies in various ranges explicitly and efficiently. On the contrary, considering different temporal distances enables modeling dependencies from different ranges, such as global and local correlations (Li et al., 2019). However, the exact temporal distances of global and local intervals are influenced by the division of data, which is incomplete from a single view of temporal resolution. The second challenge is the fixed multi-scale modeling process. Although multi-scale modeling reaches a more complete understanding of time series, different series prefer different scales depending on their specific temporal characteristics and dynamics. For example, comparing the two series in Figure 1, the series above shows rapid fluctuations, which may imply more attention to fine-grained and short-term characteristics. The series below, on the contrary, may need more focus on coarsegrained and long-term modeling. The fixed multi-scale modeling for all data hinders the grasp of critical patterns of each time series, and manually tuning the optimal scales for a dataset or each time series is time-consuming or intractable. Solving these two challenges calls for adaptive multi-scale modeling, which adaptively models the current data from certain multiple scales.\nInspired by the above understanding of multi-scale modeling, we propose Multi-scale Transformers with Adaptive Pathways (Pathformer) for time series forecasting. To enable the ability of more complete multi-scale modeling, we propose a multi-scale Transformer block unifying multi-scale temporal resolution and temporal distance. Multi-scale division is proposed to divide the time series into patches of different sizes, forming views of diverse temporal resolutions. Based on each size of divided patches, dual attention encompassing inter-patch and intra-patch attention is proposed to capture temporal dependencies, with inter-patch attention capturing global correlations across patches and intra-patch attention capturing local details within individual patches. We further propose adaptive pathways to activate the multi-scale modeling capability and endow it with adaptive modeling characteristics. At each layer of the model, a multi-scale router adaptively selects specific sizes of patch division and the subsequent dual attention in the Transformer based on the input data, which controls the extraction of multi-scale characteristics. We equip the router with trend and seasonality decomposition to enhance its ability to grasp the temporal dynamics. The router works with an aggregator to adaptively combine multi-scale characteristics through weighted aggregation. The layer-by-layer routing and aggregation form the adaptive pathways of multi-scale modeling throughout the Transformer. To the best of our knowledge, this is the first study that introduces adaptive multi-scale modeling for time series forecasting. Specifically, we make the following contributions:\n\u2022 We propose a multi-scale Transformer architecture. It integrates the two perspectives of temporal resolution and temporal distance and equips the model with the capacity of a more complete multi-scale time series modeling.\n\u2022 We further propose adaptive pathways within multi-scale Transformers. The multi-scale router with temporal decomposition works together with the aggregator to adaptively extract and aggregate multi-scale characteristics based on the temporal dynamics of input data, realizing adaptive multi-scale modeling for time series.\n\u2022 We conduct extensive experiments on different real-world datasets and achieve state-ofthe-art prediction accuracy. Moreover, we perform transfer learning experiments across datasets to validate the strong generalization of the model."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Time Series Forecasting. Time series forecasting predicts future observations based on historical observations. Statistical modeling methods based on exponential smoothing and its different flavors serve as a reliable workhorse for time series forecasting (Hyndman & Khandakar, 2008; Li et al., 2022a). Among deep learning methods, GNNs model spatial dependency for correlated time series forecasting (Jin et al., 2023a; Wu et al., 2020; Zhao et al., 2024; Cheng et al., 2024; Miao et al., 2024; Cirstea et al., 2021). RNNs model the temporal dependency (Chung et al., 2014; Kieu et al., 2022a; Wen et al., 2017; Cirstea et al., 2019). DeepAR (Rangapuram et al., 2018) uses RNNs and autoregressive methods to predict future short-term series. CNN models use the temporal convolution to extract the sub-series features (Sen et al., 2019; Liu et al., 2022a; Wang et al., 2023). TimesNet (Wu et al., 2023a) transforms the original one-dimensional time series into a two-dimensional space and captures multi-period features through convolution. LLM-based methods also show effective performance in this field (Jin et al., 2023b; Zhou et al., 2023). Additionally, some methods are incorporating neural architecture search to discover optimal architectures(Wu et al., 2022; 2023b).\nTransformer models have recently received emerging attention in time series forecasting (Wen et al., 2023). Informer (Zhou et al., 2021) proposes prob-sparse self-attention to select important keys, Triformer (Cirstea et al., 2022a) employs a triangular architecture, which manages to reduce the complexity. Autoformer (Wu et al., 2021) proposes auto-correlation mechanisms to replace selfattention for modeling temporal dynamics. FEDformer (Zhou et al., 2022) utilizes fourier transformation from the perspective of frequency to model temporal dynamics. However, researchers have raised concerns about the effectiveness of Transformers for time series forecasting, as simple linear models prove to be effective or even outperform previous Transformers (Li et al., 2022a; Challu et al., 2023; Zeng et al., 2023). Meanwhile, PatchTST (Nie et al., 2023) employs patching and channel independence with Transformers to effectively enhance the performance, showing that the Transformer architecture still has its potential with proper adaptation in time series forecasting.\nMulti-scale Modeling for Time Series. Modeling multi-scale characteristics proves to be effective for correlation learning and feature extraction in the fields such as computer vision (Wang et al., 2021; Li et al., 2022b; Wang et al., 2022b) and multi-modal learning (Hu et al., 2020; Wang et al., 2022a), which is relatively less explored in time series forecasting. N-HiTS (Challu et al., 2023) employs multi-rate data sampling and hierarchical interpolation to model features of different resolutions. Pyraformer (Liu et al., 2022b) introduces a pyramid attention to extract features at different temporal resolutions. Scaleformer (Shabani et al., 2023) proposes a multi-scale framework, and the need to allocate a predictive model at different temporal resolutions results in higher model complexity. Different from these methods, which use fixed scales and cannot adaptively change the multi-scale modeling for different time series, we propose a multi-scale Transformer with adaptive pathways that adaptively model multi-scale characteristics based on diverse temporal dynamics."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "To effectively capture multi-scale characteristics, we propose multi-scale Transformers with adaptive pathways (named Pathformer). As depicted in Figure 2, the whole forecasting network is composed of Instance Norm, stacking of Adaptive Multi-Scale Blocks (AMS Blocks), and Predictor. Instance Norm (Kim et al., 2022) is a normalization technique employed to address the distribution shift between training and testing data. Predictor is a fully connected neural network, proposed due to its applicability to forecasting for long sequences (Zeng et al., 2023; Das et al., 2023).\nThe core of our design is the AMS Block for adaptive modeling of multi-scale characteristics, which consists of the multi-scale Transformer block and adaptive pathways. Inspired by the idea of patch-\ning in Transformers (Nie et al., 2023), the multi-scale Transformer block integrates multi-scale temporal resolutions and distances by introducing patch division with multiple patch sizes and dual attention on the divided patches, equipping the model with the capability to comprehensively model multi-scale characteristics. Based on various options of multi-scale modeling in the Transformer block, adaptive pathways utilize the multi-scale modeling capability and endow it with adaptive modeling characteristics. A multi-scale router selects specific sizes of patch division and the subsequent dual attention in the Transformer based on the input data, which controls the extraction of multi-scale features. The router works with an aggregator to combine these multi-scale characteristics through weighted aggregation. The layer-by-layer routing and aggregation form the adaptive pathways of multi-scale modeling throughout the Transformer blocks. In the following parts, we describe the multi-scale Transformer block and the adaptive pathways of the AMS Block in detail."
        },
        {
            "heading": "3.1 MULTI-SCALE TRANSFORMER BLOCK",
            "text": "Multi-scale Division. For the simplicity of notations, we use a univariate time series for description, and the method can be easily extended to multivariate cases by considering each variable independently. In the multi-scale Transformer block, We define a collection of M patch size values as S = {S1, . . . , SM}, with each patch size S corresponding to a patch division operation. For the input time series X \u2208 RH\u00d7d, where H denotes the length of the time series and d denotes the dimension of features, each patch division operation with the patch size S divides X into P (with P = H/S) patches as (X1,X2, . . . ,XP ), where each patch Xi \u2208 RS\u00d7d contains S time steps. Different patch sizes in the collection lead to various scales of divided patches and give various views of temporal resolutions for the input series. This multi-scale division works with the dual attention mechanism described below for multi-scale modeling.\nDual Attention. Based on the patch division of each scale, we propose dual attention to model temporal dependencies over the divided patches. To grasp temporal dependencies from different temporal distances, we utilize patch division as guidance for different temporal distances, and the dual attention mechanism consists of intra-patch attention within each divided patch and inter-patch attention across different patches, as shown in Figure 3(a).\nConsider a set of patches (X1,X2, . . . ,XP ) divided with the patch size S, intra-patch attention establishes relationships between time steps within each patch. For the i-th patch Xi \u2208 RS\u00d7d, we first embed the patch along the feature dimension d to get Xiintra \u2208 RS\u00d7dm , where dm represents the dimension of embedding. Then we perform trainable linear transformations on Xiintra to obtain the key and value in attention operations, denoted as Kiintra, V i intra \u2208 RS\u00d7dm . We employ a trainable query matrix Qiintra \u2208 R1\u00d7dm to merge the context of the patch and subsequently compute the\ncross-attention between Qiintra,K i intra, V i intra to model local details within the i-th patch:\nAttniintra = Softmax(Q i intra(K i intra) T / \u221a dm)V i intra. (1)\nAfter intra-patch attention, each patch has transitioned from its original input length of S to the length of 1. The attention results from all the patches are concatenated to produce the output of intra-attention on the divided patches as Attnintra \u2208 RP\u00d7dm , which represents the local details from nearby time steps in the time series:\nAttnintra = Concat(Attn 1 intra, . . . ,Attn P intra). (2)\nInter-patch attention establishes relationships between patches to capture global correlations. For the patch-divided time series X \u2208 RP\u00d7S\u00d7d, we first perform feature embedding along the feature dimension from d to dm and then rearrange the data to combine the two dimensions of patch quantity S and feature embedding dm, resulting in Xinter \u2208 RP\u00d7d \u2032 m , where d \u2032\nm = S \u00b7 dm. After such embedding and rearranging process, the time steps within the same patch are combined, and thus we perform self-attention over Xinter to model correlations between patches. Following the standard self-attention protocol, we obtain the query, key, and value through linear mapping on Xinter, denoted as Qinter,Kinter, Vinter \u2208 RP\u00d7d \u2032 m . Then, we compute the attention Attninter, which involves interaction between patches and represents the global correlations of the time series:\nAttninter = Softmax(Qinter(Kinter) T / \u221a d\u2032m)Vinter. (3)\nTo fuse global correlations and local details captured by dual attention, we rearrange the outputs of intra-patch attention to Attnintra \u2208 RP\u00d7S\u00d7dm , performing linear transformations on the patch size dimension from 1 to S, to combine time steps in each patch, and then add it with inter-patch attention Attninter \u2208 RP\u00d7S\u00d7dm to obtain the final output of dual attention Attn \u2208 RP\u00d7S\u00d7dm . Overall, the multi-scale division provides different views of the time series with different patch sizes, and the changing patch sizes further influence the dual attention, which models temporal dependencies from different distances guided by the patch division. These two components work together to enable multiple scales of temporal modeling in the Transformer."
        },
        {
            "heading": "3.2 ADAPTIVE PATHWAYS",
            "text": "The design of the multi-scale Transformer block equips the model with the capability of multiscale modeling. However, different series may prefer diverse scales, depending on their specific temporal characteristics and dynamics. Simply applying more scales may bring in redundant or useless signals, and manually tuning the optimal scales for a dataset or each time series is timeconsuming or intractable. An ideal model needs to figure out such critical scales based on the input data for more effective modeling and better generalization of unseen data.\nPathways and Mixture of Experts are used to achieve adaptive modeling (Dean, 2021; Shazeer et al., 2016). Based on these concepts, we propose adaptive pathways based on multi-scale Transformer to model adaptive multi-scale, depicted in Figure 2. It contains two main components: the multi-scale router and the multi-scale aggregator. The multi-scale router selects specific sizes of patch division based on the input data, which activates specific parts in the Transformer and controls the extraction of multi-scale characteristics. The router works with the multi-scale aggregator to combine these characteristics through weighted aggregation, obtaining the output of the Transformer block.\nMulti-Scale Router. The multi-scale router enables data-adaptive routing in the multi-scale Transformer, which selects the optimal sizes for patch division and thus controls the process of multi-scale modeling. Since the optimal or critical scales for each time series can be impacted by its complex inherent characteristics and dynamic patterns, like the periodicity and trend, we introduce a temporal decomposition module in the router that encompasses both seasonality and trend decomposition to extract periodicity and trend patterns, as illustrated in Figure 3(b).\nSeasonality decomposition involves transforming the time series from the temporal domain into the frequency domain to extract the periodic patterns. We utilize the Discern Fourier Transform (DFT) (Cooley & Tukey, 1965), denoted as DFT(\u00b7), to decompose the input X into Fourier basis and select the Kf basis with the largest amplitudes to keep the sparsity of frequency domain. Then, we obtain the periodic patterns Xsea through an inverse DFT, denoted as IDFT(\u00b7). The process is as follows: Xsea = IDFT({f1, . . . , fKf }, A,\u03a6), (4) where \u03a6 and A represent the phase and amplitude of each frequency from DFT(X), {f1, . . . , fKf } represents the frequencies with the top Kf amplitudes. Trend decomposition uses different kernels of average pooling for moving averages to extract trend patterns based on the remaining part after the seasonality decomposition Xrem = X\u2212 Xsea. For the results obtained from different kernels, a weighted operation is applied to obtain the representation of the trend component:\nXtrend = Softmax(L(Xrem)) \u00b7 (Avgpool(Xrem)kernel1 , . . . ,Avgpool(Xrem)kernelN ), (5) where Avgpool(\u00b7)kerneli is the pooling function with the i-th kernel, N corresponds to the number of kernels, Softmax(L(\u00b7)) controls the weights for the results from different kenerls. We add the seasonality pattern and trend pattern with the original input X, and then perform a linear mapping Linear(\u00b7) to transform and merge them along the temporal dimension to get Xtrans \u2208 Rd. Based on the results Xtrans from temporal decomposition, the router employs a routing function to generate the pathway weights, which determines the patch sizes to choose for the current data. To avoid consistently selecting a few patch sizes, causing the corresponding scales to be repeatedly updated while neglecting other potentially useful scales in the multi-scale Transformer, we introduce noise terms to add randomness in the weight generation process. The whole process of generating pathway weights is as follows:\nR(Xtrans) = Softmax(XtransWr + \u03f5 \u00b7 Softplus(XtransWnoise)), \u03f5 \u223c N (0, 1), (6) where R(\u00b7) represents the whole routing function, Wr and Wnoise \u2208 Rd\u00d7M are learnable parameters for weight generation, with d denoting the feature dimension of Xtrans and M denoting the number of patch sizes. To introduce sparsity in the routing and encourage the selection of critical scales, we perform topK selection on the pathway weights, keeping the top K pathway weights and setting the rest weights as 0, and denote the final result as R\u0304(Xtrans).\nMulti-Scale Aggregator. Each dimension of the generated pathway weights R\u0304(Xtrans) \u2208 RM correspond to a patch size in the multi-scale Transformer, with R\u0304(Xtrans)i > 0 indicating performing this size Si of patch division and the dual attention and R\u0304(Xtrans)i = 0 indicating ignoring this patch size for the current data. Let Xiout denote the output of the multi-scale Transformer with the patch size Si, due to the varying temporal dimensions produced by different patch sizes, the aggregator first perform a transformation function Ti(\u00b7) to align the temporal dimension from different scales. Then, the aggregator performs weighted aggregation for the multi-scale outputs based on the pathway weights to get the final output of this AMS block:\nXout = M\u2211 i=1 I(R\u0304(Xtrans)i > 0)R(Xtrans)iTi(Xiout). (7)\nI(R\u0304(Xtrans)i > 0) is the indicator function which outputs 1 when R\u0304(Xtrans)i > 0, and otherwise outputs 0, indicating that only the top K patch sizes and the corresponding outputs from the Transformer are considered or needed during aggregation."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 TIME SERIES FORECASTING",
            "text": "Datasets. We conduct experiments on nine real-world datasets to assess the performance of Pathformer, encompassing a range of domains, including electricity transportation, weather forecasting, and cloud computing. These datasets include ETT (ETTh1, ETTh2, ETTm1, ETTm2), Weather, Electricity, Traffic, ILI, and Cloud Cluster (Cluster-A, Cluster-B, Cluster-C).\nBaselines and Metrics. We choose some state-of-the-art models to serve as baselines, including PatchTST (Nie et al., 2023), NLinear (Zeng et al., 2023), Scaleformer (Shabani et al., 2023), TIDE (Das et al., 2023), FEDformer (Zhou et al., 2022), Pyraformer (Liu et al., 2022b), and Autoformer (Wu et al., 2021). To ensure fair comparisons, all models follow the same input length (H = 36 for the ILI dataset and H = 96 for others) and prediction length (F \u2208 {24, 49, 96, 192} for Cloud Cluster datasets, F \u2208 {24, 36, 48, 60} for ILI dataset and F \u2208 {96, 192, 336, 720} for others). We select two common metrics in time series forecasting: Mean Absolute Error (MAE) and Mean Squared Error (MSE).\nImplementation Details. Pathformer utilizes the Adam optimizer (Kingma & Ba, 2015) with a learning rate set at 10\u22123. The default loss function employed is L1 Loss, and we implement early stopping within 10 epochs during the training process. All experiments are conducted using PyTorch and executed on an NVIDIA A800 80GB GPU. Pathformer is composed of 3 Adaptive Multi-Scale Blocks (AMS Blocks). Each AMS Block contains 4 different patch sizes. These patch sizes are selected from a pool of commonly used options, namely {2, 3, 6, 12, 16, 24, 32}. Main Results. Table 1 shows the prediction results of multivariable time series forecasting, where Pathformer stands out with the best performance in 81 cases and the second-best in 5 cases out of the overall 88 cases. Compared with the second-best baseline, PatchTST, Pathformer demonstrates a significant improvement, with an impressive 8.1% reduction in MSE and a 6.4% reduction in MAE. Compared with the strong linear models NLinear, Pathformer also outperforms them comprehensively, especially on large datasets such as Electricity and Traffic. This demonstrates the potential of Transformer architecture for time series forecasting. Compared with the multi-scale models Pyraformer and Scaleformer, Pathformer exhibits good performance improvements, with a substantial 36.4% reduction in MSE and a 19.1% reduction in MAE. This illustrates that the proposed comprehensive modeling from both temporal resolution and temporal distance with adaptive pathways is more effective for multi-scale modeling."
        },
        {
            "heading": "4.2 TRANSFER LEARNING",
            "text": "Experimental Setting. To assess the transferability of Pathformer, we benchmark it against three baselines: PatchTST, FEDformer, and Autoformer, devising two distinct transfer experiments. In the context of evaluating transferability across different datasets, models initially undergo pre-training on the ETTh1 and ETTm1. Subsequently, we fine-tune them using the ETTh2 and ETTm2. For assessing transferability towards future data, models are pre-trained on the first 70% of the training data sourced from three clusters: Cluster-A, Cluster-B, and Cluster-C. This pre-training is followed by fine-tuning the remaining 30% of the training data specific to each cluster. In terms of methodology for baselines, we explore two approaches: direct prediction (zero-shot) and full-tuning. Deviating from these approaches, Pathformer integrates a part-tuning strategy. In this approach, specific parameters, like those of the router network, undergo fine-tuning, resulting in a significant reduction in computational resource demands.\nTransfer Learning Results. Table 2 presents the outcomes of our transfer learning evaluation. Across both direct prediction and full-tuning methods, Pathformer surpasses the baseline models, highlighting its enhanced generalization and transferability. One of the key strengths of Pathformer lies in its adaptive capacity to select varying scales for different temporal dynamics. This adaptability allows it to effectively capture complex temporal patterns present in diverse datasets, consequently demonstrating superior generalization and transferability. Part-tuning is a lightweight fine-tuning method that demands fewer computational resources and reduces training time on average by 52%, while still achieving prediction accuracy nearly comparable to Pathformer full-tuning. Moreover, it outperforms the full-tuning of other baseline models on the majority of datasets. This demonstrates that Pathformer can provide effective lightweight transfer learning for time series forecasting."
        },
        {
            "heading": "4.3 ABLATION STUDIES",
            "text": "To ascertain the impact of different modules within Pathformer, we perform ablation studies focusing on inter-patch attention, intra-patch attention, time series decomposition, and Pathways. The W/O Pathways configuration entails using all patch sizes from the patch size pool for every dataset, eliminating adaptive selection. Table 3 illustrates the unique impact of each module. The influence of Pathways is significant; omitting them results in a marked decrease in prediction accuracy. This emphasizes the criticality of optimizing the mix of patch sizes to extract multi-scale characteristics, thus markedly improving the model\u2019s prediction accuracy. Regarding efficiency, intra-patch attention is notably adept at discerning local patterns, contrasting with inter-patch attention which primarily captures wider global patterns. The time series decomposition module decomposes trend\nand periodic patterns to improve the ability to capture the temporal dynamics of its input, assisting in the identification of appropriate patch sizes for combination.\nVarying the Number of Adaptively Selected Patch Sizes. Pathformer adaptively selects the top K patch sizes for combination, adjusting to different time series samples. We evaluate the influence of different K values on prediction accuracy in Table 4. Our findings show that K = 2 and K = 3 yield better results than K = 1 and K = 4, highlighting the advantage of adaptively modeling critical multi-scale characteristics for improved accuracy. Additionally, distinct time series samples benefit from feature extraction using varied patch sizes, but not all patch sizes are equally effective.\nVisualization of Pathways Weights. We show three samples and depict their average Pathways weights for each patch size in Figure 4. Our observations reveal that the samples possess unique Pathways weight distributions. Both Samples 1 and 2, which demonstrate longer seasonality and similar trend patterns, show similar visualized Pathways weights. This manifests in the higher weights they attribute to the larger patch sizes. On the other hand, Sample 3, which is characterized by its shorter seasonality pattern, aligns with higher weights for the smaller patch sizes. These observations underscore Pathformer\u2019s adaptability, emphasizing its ability to discern and apply the optimal patch size combinations for the diverse seasonality and trend patterns across samples."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we propose Pathformer, a Multi-Scale Transformer with Adaptive Pathways for time series forecasting. It integrates multi-scale temporal resolutions and temporal distances by introducing patch division with multiple patch sizes and dual attention on the divided patches, enabling the comprehensive modeling of multi-scale characteristics. Furthermore, adaptive pathways dynamically select and aggregate scale-specific characteristics based on the different temporal dynamics. These innovative mechanisms collectively empower Pathformer to achieve outstanding prediction performance and demonstrate strong generalization capability on several forecasting tasks."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by National Natural Science Foundation of China (62372179) and Alibaba Innovative Research Program."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "A.1.1 DATASETS",
            "text": "The Special details about experiment datasets are as follows: ETT 1 datasets consist of 7 variables, originating from two different electric transformers. It covers the period from January 2016 to January 2018. Each electric transformer has data recorded at 15-minute and 1-hour granularities, labeled as ETTh1, ETTh2, ETTm1, and ETTm2. Weather 2 dataset comprises 21 meteorological indicators in Germany, collected every 10 minutes. Electricity 3 dataset contains the power consumption of 321 users, recorded every hour, spanning from July 2016 to July 2019. ILI 4 collects weekly data on patients with influenza-like illness from the Centers for Disease Control and Prevention of the United States spanning the years 2002 to 2021. Traffic 5 comprises hourly data sourced from the California Department of Transportation. This dataset delineates road occupancy rates measured by various sensors on the freeways of the San Francisco Bay area. Cloud cluster datasets are private business data, documenting customer resource demands at 1-minute intervals for three clusters: cluster-A, cluster-B, cluster-C, where A,B,C represent different cities, covering the period from February 2023 to April 2023. For dataset preparation, we follow the established practice from previous studies (Zhou et al., 2021; Wu et al., 2021). Detailed statistics are shown in Table 5."
        },
        {
            "heading": "A.1.2 BASELINES",
            "text": "In the realm of time series forecasting, numerous models have surfaced in recent years. We choose models with superior predictive performance from 2021 to 2023 as baselines, including the 2021 state-of-the-art (SOTA) Autoformer, the 2022 SOTA FEDformer, and the 2023 SOTA PatchTST and NLinear, among others. The specific code repositories for each of these models are as follows:\n\u2022 PatchTST: https://github.com/yuqinie98/PatchTST\n\u2022 NLinear: https://github.com/cure-lab/LTSF-Linear\n\u2022 FEDformer: https://github.com/MAZiqing/FEDformer\n\u2022 Scaleformer: https://github.com/borealisai/scaleformer\n\u2022 TiDE: https://github.com/google-research/google-research/tree/master/tide\n\u2022 Pyraformer: https://github.com/ant-research/Pyraformer\n\u2022 Autoformer: https://github.com/thuml/Autoformer"
        },
        {
            "heading": "A.2 UNIVARIATE TIME SERIES FORECASTING",
            "text": "We conducted univariate time series forecasting experiments on the ETT and Cloud cluster datasets. As shown in Table 6, Pathformer stands out with the best performance in 50 cases and as the secondbest in 5 out of 56 instances. Pathformer has outperformed the second-best baseline PatchTST, especially on the Cloud cluster datasets. Our model Pathformer demonstrates excellent predictive performance in both multivariate and univariate time series forecasting.\n1https://github.com/zhouhaoyi/ETDataset 2https://www.bgc-jena.mpg.de/wetter/ 3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 4https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html 5https://pems.dot.ca.gov/\nA.3 VARYING THE INPUT LENGTH WITH TRANSFORMER MODELS\nIn time series forecasting tasks, the size of the input length determines how much historical information the model receives. We select models with better predictive performance from the main experiments as baselines. We configure different input lengths to evaluate the effectiveness of Pathformer and visualize the prediction results for input lengths of 48,192. From Figure 5, Pathformer consistently outperforms the baselines on the ETTh1, ETTh2, Weather, and Electricity. As depicted in Table 7 and Table 8, for H = 48, 192, Pathformer stands out with the best performance in 46, 44 cases out of 48, respectively. Based on the results above, it is evident that Pathformer outperforms the baselines across different input lengths. As the input length increases, the prediction metrics of Pathformer continue to decrease, indicating that it is capable of modeling longer sequences."
        },
        {
            "heading": "A.4 MORE COMPARISONS WITH SOME BASIC BASELINES",
            "text": "To validate the effectiveness of Pathformer, we conducted extensive experiments with some recent basic baselines that exhibited good performance: DLinear, NLinear, and N-HiTS, using long input sequence length (H = 336). As depicted in Table 9, our proposed model Pathformer outperforms\nthese baselines for the input length 336. Zeng et al. (2023) point out that the previous Transformer cannot extract temporal relations well from longer input sequences, but our proposed Pathformer performs better with a longer input length, indicating that considering adaptive multi-scale modeling can be an effective way to enhance such a relation extraction ability of Transformers."
        },
        {
            "heading": "A.5 DISCUSSION",
            "text": ""
        },
        {
            "heading": "A.5.1 COMPARE WITH PATCHTST",
            "text": "PatchTST divides time series into patches, with empirical evidence proving that patching is an effective method to enhance model performance in time series forecasting. Our proposed model Pathformer extends the patching approach to incorporate multi-scale modeling. The main differences with PatchTST are as follows: (1) Partitioning with Multiple Patch Sizes: PatchTST employs a single patch size to partition time series, obtaining features with a singular resolution. In contrast, Pathformer utilizes multiple different patch sizes at each layer for partitioning. This approach captures multi-scale features from the perspective of temporal resolutions. (2) Global correlations between patches and local details in each patch: PatchTST performs attention between divided patches, overlooking the internal details in each patch. In contrast, Pathformer not only considers the correlations between patches but also the detailed information within each patch. It introduces dual attention(inter-patch attention and intra-patch attention) to integrate global correlations and local details, capturing multi-scale features from the perspective of temporal distances. (3)Adaptive Multi-scale Modeling: PatchTST employs a fixed patch size for all data, hindering the grasp of critical patterns in different time series. We propose adaptive pathways that dynamically select varying patch sizes tailored to the features of individual samples, enabling adaptive multi-scale modeling."
        },
        {
            "heading": "A.5.2 COMPARE WITH N-HITS",
            "text": "N-HiTS utilizes the modeling of multi-scale features for time series forecasting, but it differs from Pathformer in the following aspects: (1) N-HiTS models time series features of different resolutions through multi-rate data sampling and hierarchical interpolation. In contrast, Pathformer not only takes into account time series features of different resolutions but also approaches multi-scale modeling from the perspective of temporal distance. Simultaneously considering temporal resolutions and temporal distances enables a more comprehensive approach to multi-scale modeling. (2) N-HiTS employs fixed sampling rates for multi-rate data sampling, lacking the ability to adaptively perform multi-scale modeling based on differences in time series samples. In contrast, Pathformer has the capability for adaptive multi-scale modeling. (3) N-HiTS adopts a linear structure to build its model framework, whereas Pathformer enables multi-scale modeling in a Transformer architecture."
        },
        {
            "heading": "A.5.3 COMPARE WITH SCALEFORMER",
            "text": "Scaleformer also utilizes the modeling of multi-scale features for time series forecasting. It differs from Pathformer in the following aspects: (1) Scaleformer obtains multi-scale features with different temporal resolutions through downsampling. In contrast, Pathformer not only considers time series features of different resolutions but also models from the perspective of temporal distance, taking into account global correlations and local details. This provides a more comprehensive approach to multi-scale modeling through both temporal resolutions and temporal distances. (2) Scaleformer requires the allocation of a predictive model at different temporal resolutions, resulting in higher model complexity than Pathformer. (3) Scaleformer employs fixed sampling rates, while Pathformer has the capability for adaptive multi-scale modeling based on the differences in time series samples."
        },
        {
            "heading": "A.6 EXPERIMENTS ON LARGE DATASETS",
            "text": "The current time series forecasting benchmarks are relatively small, and there is a concern that the predictive performance of the model might be influenced by overfitting. To address this issue, we explore larger datasets to validate the effectiveness of the proposed model. The detailed process is as follows: We seek larger datasets from two perspectives: data volume and the number of variables. We add two datasets, the Wind Power dataset, and the PEMS07 dataset, to evaluate the performance of Pathformer on larger datasets. The Wind Power dataset comprises 7397147 timestamps, reaching a sample size in the millions, and the PEMS07 dataset includes 883 variables. As depicted in Table 10, Pathformer demonstrates superior predictive performance on these larger datasets compared with some state-of-the-art methods such as PatchTST, DLinear, and Scaleformer.\nWe visualize the prediction results of Pathformer on the Electricity dataset. As illustrated in Figure 6, for prediction lengths F = 96, 192, 336, 720, the prediction curve closely aligns with the Ground Truth curve, indicating the outstanding predictive performance of Pathformer. Meanwhile, Pathformer demonstrates effectiveness in capturing multi-period and complex trends present in diverse samples. This serves as evidence of its adaptive modeling capability for multi-scale characteristics."
        }
    ],
    "title": "PATHFORMER: MULTI-SCALE TRANSFORMERS WITH ADAPTIVE PATHWAYS FOR TIME SERIES FORECASTING",
    "year": 2024
}