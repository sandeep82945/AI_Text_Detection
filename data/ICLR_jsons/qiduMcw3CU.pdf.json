{
    "abstractText": "It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-optimal behaviours zero-shot. We demonstrate this experimentally in a tabular setting, as well as in a high-dimensional video game and continuous control environment. Finally, we also demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.",
    "authors": [
        {
            "affiliations": [],
            "name": "Geraud Nangue Tasse"
        },
        {
            "affiliations": [],
            "name": "Devon Jarvis"
        },
        {
            "affiliations": [],
            "name": "Steven James"
        },
        {
            "affiliations": [],
            "name": "Benjamin Rosman"
        }
    ],
    "id": "SP:cefe6293f8bb026776dabf1f5567e38945635d63",
    "references": [
        {
            "authors": [
                "Safa Alver",
                "Doina Precup"
            ],
            "title": "Constructing a good behavior basis for transfer using generalized policy updates",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Brandon Araki",
                "Xiao Li",
                "Kiran Vodrahalli",
                "Jonathan DeCastro",
                "Micah Fry",
                "Daniela Rus"
            ],
            "title": "The logical options framework",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jose Arjona-Medina",
                "Michael Gillhofer",
                "Michael Widrich",
                "Thomas Unterthiner",
                "Johannes Brandstetter",
                "Sepp Hochreiter"
            ],
            "title": "Rudder: Return decomposition for delayed rewards",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Andre Barreto",
                "Diana Borsa",
                "John Quan",
                "Tom Schaul",
                "David Silver",
                "Matteo Hessel",
                "Daniel Mankowitz",
                "Augustin Zidek",
                "Remi Munos"
            ],
            "title": "Transfer in deep reinforcement learning using successor features and generalised policy improvement",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Andr\u00e9 Barreto",
                "Diana Borsa",
                "Shaobo Hou",
                "Gheorghe Comanici",
                "Eser Ayg\u00fcn",
                "Philippe Hamel",
                "Daniel Toyama",
                "Shibl Mourad",
                "David Silver",
                "Doina Precup"
            ],
            "title": "The option keyboard: Combining skills in reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Andr\u00e9 Barreto",
                "Shaobo Hou",
                "Diana Borsa",
                "David Silver",
                "Doina Precup"
            ],
            "title": "Fast reinforcement learning with generalized policy updates",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Alberto Camacho",
                "Rodrigo Toro Icarte",
                "Toryn Q Klassen",
                "Richard Anthony Valenzano",
                "Sheila A McIlraith"
            ],
            "title": "Ltl and beyond: Formal languages for reward function specification in reinforcement learning",
            "venue": "In IJCAI,",
            "year": 2019
        },
        {
            "authors": [
                "Vanya Cohen",
                "Geraud Nangue Tasse",
                "Nakul Gopalan",
                "Steven James",
                "Matthew Gombolay",
                "Benjamin Rosman"
            ],
            "title": "Learning to follow language instructions with compositional policies",
            "venue": "In AAAI Fall Symposium Series,",
            "year": 2021
        },
        {
            "authors": [
                "Vanya Cohen",
                "Geraud Nangue Tasse",
                "Nakul Gopalan",
                "Steven James",
                "Ray Mooney",
                "Benjamin Rosman"
            ],
            "title": "End-to-end learning to follow language instructions with compositional policies",
            "venue": "In Workshop on Language and Robotics at CoRL",
            "year": 2022
        },
        {
            "authors": [
                "Alexandre Duret-Lutz",
                "Alexandre Lewkowicz",
                "Amaury Fauchille",
                "Thibaud Michaud",
                "Etienne Renault",
                "Laurent Xu"
            ],
            "title": "Spot 2.0\u2014a framework for ltl and \u03c9-automata manipulation",
            "venue": "In International Symposium on Automated Technology for Verification and Analysis,",
            "year": 2016
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actorcritic methods",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Vitchyr Pong",
                "Aurick Zhou",
                "Murtaza Dalal",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Composable deep reinforcement learning for robotic manipulation",
            "venue": "IEEE International Conference on Robotics and Automation,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Hunt",
                "Andre Barreto",
                "Timothy Lillicrap",
                "Nicolas Heess"
            ],
            "title": "Composing entropic policies using divergence correction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Matthias Hutsebaut-Buysse",
                "Kevin Mets",
                "Steven Latr\u00e9"
            ],
            "title": "Hierarchical reinforcement learning: A survey and open research challenges",
            "venue": "Machine Learning and Knowledge Extraction,",
            "year": 2022
        },
        {
            "authors": [
                "Rodrigo Toro Icarte",
                "Toryn Klassen",
                "Richard Valenzano",
                "Sheila McIlraith"
            ],
            "title": "Using reward machines for high-level task specification and decomposition in reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Rodrigo Toro Icarte",
                "Toryn Klassen",
                "Richard Valenzano",
                "Sheila McIlraith"
            ],
            "title": "Using reward machines for high-level task specification and decomposition in reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Rodrigo Toro Icarte",
                "Toryn Q Klassen",
                "Richard Valenzano",
                "Sheila A McIlraith"
            ],
            "title": "Reward machines: Exploiting reward function structure in reinforcement learning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2022
        },
        {
            "authors": [
                "Kishor Jothimurugan",
                "Rajeev Alur",
                "Osbert Bastani"
            ],
            "title": "A composable specification language for reinforcement learning tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Kishor Jothimurugan",
                "Suguman Bansal",
                "Osbert Bastani",
                "Rajeev Alur"
            ],
            "title": "Compositional reinforcement learning from logical specifications",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "George Konidaris",
                "Andrew Barto"
            ],
            "title": "Skill discovery in continuous reinforcement learning domains using skill chaining",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Orna Kupferman",
                "Moshe Y Vardi"
            ],
            "title": "Model checking of safety properties",
            "venue": "Formal methods in system design,",
            "year": 2001
        },
        {
            "authors": [
                "Sergey Levine",
                "Chelsea Finn",
                "Trevor Darrell",
                "Pieter Abbeel"
            ],
            "title": "End-to-end training of deep visuomotor policies",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Xiao Li",
                "Cristian-Ioan Vasile",
                "Calin Belta"
            ],
            "title": "Reinforcement learning with temporal logic rewards",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Littman",
                "Ufuk Topcu",
                "Jie Fu",
                "Charles Isbell",
                "Min Wen",
                "James MacGlashan"
            ],
            "title": "Environmentindependent task specifications via GLTL",
            "venue": "arXiv preprint arXiv:1704.04341,",
            "year": 2017
        },
        {
            "authors": [
                "Jason Xinyu Liu",
                "Ankit Shah",
                "Eric Rosen",
                "George Konidaris",
                "Stefanie Tellex"
            ],
            "title": "Skill transfer for temporally-extended task specifications",
            "venue": "arXiv preprint arXiv:2206.05096,",
            "year": 2022
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei Rusu",
                "Joel Veness",
                "Marc Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "Geraud Nangue Tasse",
                "Steven James",
                "Benjamin Rosman"
            ],
            "title": "A Boolean task algebra for reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Geraud Nangue Tasse",
                "Steven James",
                "Benjamin Rosman"
            ],
            "title": "Generalisation in lifelong reinforcement learning through logical composition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Geraud Nangue Tasse",
                "Steven James",
                "Benjamin Rosman"
            ],
            "title": "World value functions: Knowledge representation for multitask reinforcement learning",
            "venue": "In The 5th Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM),",
            "year": 2022
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Michael Chang",
                "Grace Zhang",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "MCP: Learning composable hierarchical control with multiplicative compositional policies",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Amir Pnueli"
            ],
            "title": "The temporal logic of programs",
            "venue": "In 18th Annual Symposium on Foundations of Computer Science,",
            "year": 1977
        },
        {
            "authors": [
                "Alex Ray",
                "Joshua Achiam",
                "Dario Amodei"
            ],
            "title": "Benchmarking safe exploration in deep reinforcement learning.(2019)",
            "venue": "OpenAI,",
            "year": 2019
        },
        {
            "authors": [
                "Richard Sutton",
                "Doina Precup",
                "Satinder Singh"
            ],
            "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
            "venue": "Artificial Intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "Matthew. Taylor",
                "Peter Stone"
            ],
            "title": "Transfer learning for reinforcement learning domains: a survey",
            "venue": "Journal of Machine Learning Research,",
            "year": 2009
        },
        {
            "authors": [
                "Emanuel Todorov"
            ],
            "title": "Compositionality of optimal control laws",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Pashootan Vaezipoor",
                "Andrew C Li",
                "Rodrigo A Toro Icarte",
                "Sheila"
            ],
            "title": "A Mcilraith. Ltl2action: Generalizing ltl instructions for multi-task rl",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Van Niekerk",
                "Steven James",
                "Adam Earle",
                "Benjamin Rosman"
            ],
            "title": "Composing value functions in reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "C. Watkins"
            ],
            "title": "Learning from delayed rewards",
            "venue": "PhD thesis, King\u2019s College,",
            "year": 1989
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "While reinforcement learning (RL) has achieved recent success in several applications, ranging from video games (Badia et al., 2020) to robotics (Levine et al., 2016), there are several shortcomings that hinder RL\u2019s real-world applicability. One issue is that of sample efficiency\u2014while it is possible to collect millions of data points in a simulated environment, it is simply not feasible to do so in the real world. This inefficiency is exacerbated when a single agent is required to solve multiple tasks, as we would expect of a generally intelligent agent.\nOne approach to overcoming this challenge is to reuse learned behaviours to solve new tasks (Taylor & Stone, 2009), preferably without further learning. Such an approach is often compositional\u2014 an agent first learns individual skills and then combines them to produce novel behaviours. There are several notions of compositionality in the literature, such as spatial composition (Todorov, 2009; Van Niekerk et al., 2019), where skills are combined to produce a new single behaviour to be executed to achieve sets of high-level goals (\u201dpick up an object that is both blue and a box\u201d), and temporal composition (Sutton et al., 1999; Jothimurugan et al., 2021), where sub-skills are invoked one after the other to achieve sequences of high-level goals (for example, \u201cpickup a blue object and then a box\u201d).\nSpatial composition is commonly achieved through a weighted combination of learned successor features (Barreto et al., 2018; 2019; Alver & Precup, 2022). Notably, work by Nangue Tasse et al. (2020; 2022b) has demonstrated spatial composition using Boolean operators, such as negation and conjunction, producing semantically meaningful behaviours without further learning. This ability can then be leveraged by agents to follow natural language instructions (Cohen et al., 2021; 2022).\nOne of the most common approaches to temporal composition is to learn options for achieving the sub-goals present in temporal logic tasks while learning a high-level policy over the options to actually solve the task, then reusing the learned options in new tasks (Araki et al., 2021; Icarte et al.,\n2022). However, other works like Vaezipoor et al. (2021) have proposed end-to-end neural network architectures for learning sub-skills from a training set that can generalise to similar new tasks.\nLiu et al. (2022) observe that for all these prior works, some of the sub-skills (e.g., options) learned from previous tasks can not be transferred satisfactorily to new tasks and provide a method to determine when this is the case. For example, if the agent has previously learned an option for \u201cgetting blue objects\u201d and another for \u201cgetting boxes\u201d, it can reuse them to \u201cpickup a blue object and then a box\u201d, but it cannot reuse them to \u201cpickup a blue object that is not a box, and then a box that is not blue\u201d. We can observe that this problem is because all the compositions in prior works are either strictly temporal or strictly spatial. While the example shows that temporal composition alone is insufficient, notice that spatial composition is also not enough for solving long-horizon tasks. In these instances, it is often near impossible for the agent to learn, owing to the large sequence of actions that must be executed before a learning signal is received (Arjona-Medina et al., 2019).\nHence, this work aims to address the highlighted problem by combining the approaches above to develop an agent capable of both zero-shot spatial and temporal composition. We particularly focus on temporal logic composition, such as linear temporal logic (LTL) (Pnueli, 1977), allowing agents to sequentially chain and order their skills while ensuring certain conditions are always or never met. We make the following main contributions:\n1. Skill machines: We propose skill machines (SM), which are finite state machines (FSM) that encode the solution to any task specified using any given regular language (such as regular fragments of LTL) as a series of Boolean compositions of skill primitives\u2014composable sub-skills for achieving high-level goals in the environment. An SM is defined by translating the regular language task specification into an FSM, and defining the skill to use per FSM state as a Boolean composition of pretrained skill primitives.\n2. Zero-shot and few-shot learning using skill machines: By leveraging reward machines (RM) (Icarte et al., 2018a)\u2014finite state machines that encode the reward structure of a task\u2014we show how an SM can be obtained directly from an LTL task specification, and prove that these SMs are satisficing\u2014given a task specification and regular reachability assumptions, an agent can successfully solve the task while adhering to any constraints. We further show how standard off-policy RL algorithms can be used to improve the resulting behaviours when optimality is desired. This is achieved with no new assumption in RL.\n3. Emperical and qualitative results: We demonstrate our approach in several environments, including a high-dimensional video game and a continuous control environment. Our results indicate that our method is capable of producing near-optimal to optimal behaviour for a variety of long-horizon tasks without further learning, including empirical results that far surpass all the representative state-of-the-art baselines."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We model the agent\u2019s interaction with the world as a Markov Decision Process (MDP), given by (S,A, \u03c1, R, \u03b3), where (i) S is the finite set of all states the agent can be in; (ii) A is the finite set of actions the agent can take in each state; (iii) \u03c1(s\u2032|s, a) is the dynamics of the world; (iv) R : S\u00d7A\u00d7S \u2192 R is the reward function; (v) \u03b3 \u2208 [0, 1] is a discount factor. The agent\u2019s aim is to compute a Markov policy \u03c0 from S toA that optimally solves a given task. Instead of directly learning a policy, an agent can instead learn a value function that represents the expected return of executing an action a from a state s, and then following \u03c0: Q\u03c0(s, a) = E\u03c0 [ \u2211\u221e t=0 \u03b3\ntR(st, at, st+1)]. The optimal action-value function is given by Q\u2217(s, a) = max\u03c0 Q\u03c0(s, a) for all states s and actions a, and the optimal policy follows by acting greedily with respect to Q\u2217 at each state: \u03c0\u2217(s) \u2208 argmaxa Q\u2217(s, a)."
        },
        {
            "heading": "2.1 LTL AND REWARD MACHINES",
            "text": "One difficulty with the standard MDP formulation is that the agent is often required to solve a complex long-horizon task using only a scalar reward signal as feedback from which to learn. To overcome this, a common approach is to use reward machines (RM) (Icarte et al., 2018b), which provide structured feedback to the agent in the form of a finite state machine (FSM). Given temporal logic tasks specified using regular languages, such as regular fragments of LTL (like safe, co-safe, and finite trace LTL),\nCamacho et al. (2019) shows that such task specifications can be converted to RMs with rewards of 1 for accepting transitions and 0 otherwise (Figure 1 shows an example).1 Hence, without loss of generality, we will focus our attention on tasks specified using regular fragments of LTL\u2014such as cosafe LTL (Kupferman & Vardi, 2001). These LTL specifications and RMs encode the task to be solved using a set of propositional symbols P that represent high-level environment features as follows: Definition 2.1 (LTL). An LTL expression is defined using the following recursive syntax: \u03c6 := p | \u00ac\u03c6 | \u03c61 \u2228 \u03c62 | \u03c61 \u2227 \u03c62 | X\u03c6 | G\u03c6 | \u03c61U\u03c62 | \u03c61F\u03c62, where p \u2208 P; \u00ac (not), \u2228 (or), \u2227 (and) are the usual Boolean operators; X (neXt), G (Globally or always), U (Until), F (Finally or eventually) are the LTL temporal operators; and \u03c6,\u03c61, \u03c62 are any valid LTL expression. Definition 2.2 (RM). Given a set of environment states S and actions A, a reward machine is a tuple RSA = \u27e8U , u0, \u03b4u, \u03b4r\u27e9 where (i) U is a finite set of states; (ii) u0 \u2208 U is the initial state; (iii) \u03b4u : U \u00d7 2P \u2192 U is the state-transition function; and (iv) \u03b4r : U \u00d7 2P \u2192 {0, 1} is the state-reward function.2\nTo incorporate RMs into the RL framework, the agent must be able to determine a correspondence between abstract RM propositions and states in the environment. To achieve this, the agent is equipped with a labelling function L : S \u2192 2P that assigns truth values to each state the agent visits in its environment. The agent\u2019s aim now is to learn a policy \u03c0 : S \u00d7 U \u2192 A that maximises the rewards from an RM while acting in an environment \u27e8S,A, \u03c1, \u03b3,P, L\u27e9. However, the rewards from the reward machine are not necessarily Markov with respect to the environment. Icarte et al. (2022) shows that a product MDP (Definition 2.3 below) between the environment and a reward machine guarantees that the rewards are Markov such that the policy can be learned with standard algorithms such as Q-learning. This is because the product MDP uses the cross-product to consolidate how actions in the environment result in simultaneous transitions in the environment and state machine. Thus, product MDPs take the form of standard, learnable MDPs. In the rest of this work, we will refer to these product MDPs as tasks. To ensure that the optimal policy is also the policy that maximises the probability of satisfying the temporal logic task specification, we will henceforth assume that the environment dynamics are deterministic. Definition 2.3 (Tasks). Let \u27e8S,A, \u03c1, \u03b3,P, L\u27e9 represent the environment and \u27e8U , u0, \u03b4u, \u03b4r\u27e9 be an RM representing the task rewards. Then a task is a product MDP MT = \u27e8ST ,A, \u03c1T , RT , \u03b3\u27e9 between the environment and the RM, where ST := S \u00d7 U , RT (\u27e8s, u\u27e9, a, \u27e8s\u2032, u\u2032\u27e9) := \u03b4r(u, l\u2032), \u03c1T (\u27e8s, u\u27e9, a) := \u27e8s\u2032, u\u2032\u27e9, s\u2032 \u223c \u03c1(\u00b7|s, a), u\u2032 = \u03b4u(u, l\u2032), and l\u2032 = L(s\u2032)."
        },
        {
            "heading": "2.2 LOGICAL SKILL COMPOSITION",
            "text": "Consider the multitask setting where for each task M , an agent is required to reach some terminal goal states in a goal space G \u2286 S. Nangue Tasse et al. (2020; 2022a) develop a framework for this setting that allows agents to apply the Boolean operations \u2227, \u2228 and \u00ac over the space of tasks and value functions. This is achieved by first defining a goal-oriented reward function RM (s, g, a) that extends the task rewards RM (s, a) to penalise an agent for achieving goals different from the one it wished to achieve: RM (s, g, a) := RMIN if (g \u0338= s and s is terminal) else RM (s, a); where RMIN is the lower bound of the reward function. Using RM (s, g, a), the related goal-oriented value function can be defined as Q\u03c0\u0304MM (s, g, a) = E\u03c0\u0304M [ \u2211\u221e t=0 \u03b3\ntRM (st, g, at)]. Despite the modification of the regular RL objective, an agent can always recover the regular optimal policy of the given task by maximising over goals and actions: \u03c0\u2217M (s) \u2208 argmaxa maxg Q\u2217M (s, g, a). If a new task can be represented as the logical expression of previously learned tasks, and all tasks differ only in their rewards at goal states (that is, all tasks share the same state and action space, transition dynamics, discount factor, and non-terminal rewards), Nangue Tasse et al. (2022a) prove that the optimal policy can immediately be obtained by composing the learned goal-oriented value functions using the same expression. For example, the \u2228, \u2227, and \u00ac of two goal-reaching tasks A and B can respectively be solved as follows (we omit the value functions\u2019 parameters for readability):\nQ\u2217A \u2228Q\u2217B = max{Q\u2217A,Q\u2217B}; Q\u2217A \u2227Q\u2217B = min{Q\u2217A,Q\u2217B}; \u00acQ\u2217A = (Q\u2217MAX +Q\u2217MIN )\u2212Q\u2217A; where Q\u2217MAX and Q \u2217 MIN are the goal-oriented value functions for the maximum task (R\u03c4 = RMAX for all G) and minimum task (R\u03c4 = RMIN for all G), respectively. Following Nangue Tasse et al. (2022b), we will also refer to these goal-oriented value functions as world value functions (WVFs).\n1Accepting transitions are those at which the high-level task\u2014described, for example, by LTL\u2014is satisfied. 2RMs are more general, but for clarity, we focus on the subset that is obtained from regular languages."
        },
        {
            "heading": "3 SKILL COMPOSITION FOR TEMPORAL LOGIC TASKS",
            "text": "button ( ), or a blue region ( ). The agent first learns skill primitives to reach these 3 objects (the red, green, and blue sample trajectories obtained from them respectively). Then given any task specification over these 3 objects, such as: \u201cNavigate to a button and then to a cylinder while never entering blue regions\u201d with LTL specification (F ( \u2227X(F )))\u2227 (G \u00ac ), the agent first translates the LTL task specification into an RM, then plans which spatial skill to use at each temporal node using value iteration and composes its skill primitives to obtain said spatial skills (skill machine), and finally uses them to solve the task without further learning. The RM is obtained by converting the LTL expression into an FSM using Spot (Duret-Lutz et al., 2016), then giving a reward of 1 for accepting transitions and 0 otherwise. The nodes labeled t in the RM and SM represent terminal states (sink/absorbing states where no transition leaves the state).\nTo describe our approach, we use the Safety Gym Domain (Ray et al., 2019) shown in Figure 1 as a running example. Here, the agent moves by choosing a direction and force (A = R2) and observes a real vector containing various sensory information like joint velocities and distance to the objects in its surrounding (S = R60). The LTL tasks in this environment are defined over 3 propositions: P = { , , }, where each proposition is true when the agent is \u03f5 = 1 metre near its respective location.\nNow consider an agent that has learned how to \u201cGo to the cylinder\u201d (F ), \u201cGo to a button\u201d (F ), and \u201cGo to a blue region\u201d (F ). Say the agent is now required to solve the task with LTL specification (F ( \u2227X(F )))\u2227 (G \u00ac ). Using prior LTL transfer works (Vaezipoor et al., 2021; Jothimurugan et al., 2021; Liu et al., 2022), the agent would have learned options for solving the first 3 tasks, but then would be unable to transfer those skills to immediately solve this new task. This is because the new task requires the agent to first reach a button that is not in a blue region (eventually satisfy \u2227\u00ac ) while not entering a blue region along the way (always satisfy \u00ac ). Similarly, it then must\neventually satisfy \u2227 \u00ac while never satisfying . However, all 3 options previously learned will enter a blue region if it is along the agent\u2019s path. Hence the agent will need to learn new options for achieving \u2227 \u00ac and \u2227 \u00ac where the option policies never enter along the way.\nIn general, we can see that there are 22 P\npossible Boolean expressions the agent may be required to eventually satisfy (spatial curse), and 22 P possible Boolean expressions the agent may be required to always satisfy (temporal curse). This highlights the curses of dimensionality present in temporal logic tasks. In this section, we will introduce skill primitives as the proposed solution for addressing the aforementioned curses of dimensionality. We will then introduce skill machines as a state machine that can encode the solution to any temporal logic task by leveraging skill primitives."
        },
        {
            "heading": "3.1 FROM ENVIRONMENT TO PRIMITIVES",
            "text": "We desire an agent capable of learning a sufficient set of skills that can be used to solve new tasks, specified through LTL, with little or no additional learning. To achieve this, we introduce the notion of primitives which aims to address the spatial and temporal curses of dimensionality as follows:\nSpatial curse of dimensionality: To address this, we can learn WVFs (the composable value functions described in Section 2.2) for eventually achieving each proposition, then compose them to eventually achieve the Boolean expression over the propositions. For example, we can learn WVFs for tasks F , F , and F ). However, the product MDP for LTL specified tasks have different states and dynamics (see Definition 2.3). Hence, they do not satisfy the assumptions for zero-shot logical composition (Section 2.2). To address this problem, we define task primitives. These are cross product MDPs for achieving each proposition when the agent decides to terminate, and share the same state space and dynamics. We then define skill primitives as their corresponding WVFs.\nTemporal curse of dimensionality: To address this, we introduce the concept of constraints C \u2286 {p\u0302 | p \u2208 P},3 which we use to augment the state space of task primitives. In a given environment, a constraint is a proposition that an agent may be required to always keep True or always keep False in some FSM state of a temporal logic task. Equivalently, it is a proposition that may transition the agent from some FSM state into a failure FSM state (an FSM sink state from which it can never solve the task). It is used to ensure that composed skill primitives can maintain themselves in their current FSM state while achieving their respective Boolean expressions (if they are achievable). For example, some tasks like (F ( \u2227X(F ))) \u2227 (G \u00ac ) require the agent to solve a task like F ( \u2227X(F )) while never setting to True (G \u00ac ). By setting the proposition as a constraint when learning a primitive (e.g achieving ), the agent keeps track (in its cross-product state) of whether or not it has reached a blue region in a trajectory that did not start in a blue region. That is, in an episode where the agent does not start in a blue region but later goes through a blue region and terminates at a button, the agent will achieve the goal g = { ,\u0302} \u2208 2P\u222aC . We henceforth assume the general case C = {p\u0302 | p \u2208 P} for our theory, then later consider different choices for C in our experiments. We now formally define the notions of task primitives and skill primitives such as \u201cGo to a button\u201d: Definition 3.1 (Primitives). Let \u27e8S,A, \u03c1, \u03b3,P, L\u27e9 represent the environment the agent is in, and C be the set of constraints. We define a task primitive here as an MDP Mp = \u27e8SG ,AG , \u03c1G , Rp, \u03b3\u27e9 with absorbing states G = 2P\u222aC that corresponds to achieving a proposition p \u2208 P \u222a C, where SG := (S \u00d7 2C) \u222a G; AG := A\u00d7A\u03c4 , where A\u03c4 = {0, 1} is an action that terminates the task;\n\u03c1G(\u27e8s, c\u27e9, \u27e8a, a\u03c4 \u27e9) := { l\u2032 \u222a c if a\u03c4 = 1 \u27e8s\u2032, c\u2032\u27e9 otherwise ; Rp(\u27e8s, c\u27e9, \u27e8a, a\u03c4 \u27e9) := { 1 if a\u03c4 = 1 and p \u2208 l\u2032 \u222a c 0 otherwise ,\nwhere s\u2032 \u223c \u03c1(\u00b7|s, a), l = L(s), l\u2032 = L(s\u2032), and c\u2032 = c \u222a ((l\u0302 \u2295 l\u0302\u2032) \u2229 C). A skill primitive is defined as Q\u2217p(\u27e8s, c\u27e9, g, \u27e8a, a\u03c4 \u27e9), the WVF for the task primitive Mp.\nThe above defines the state space of primitives to be the product of the environment states and the set of constraints, incorporating the set of propositions that are currently true. The action space is augmented with a terminating action following Barreto et al. (2019) and Nangue Tasse et al. (2020), which indicates that the agent wishes to achieve the goal it is currently at, and is similar to an option\u2019s termination condition (Sutton et al., 1999). The transition dynamics update the environment state s and the set of constraints violated c when any other action is taken. Here, the labeling function is used to return the set of propositions l and l\u2032 achieved in s and s\u2032 respectively. Any constraint present exclusively in l or l\u2032 is added to c, since it has not been kept always True or always False. Finally, the agent receives a reward of 1 when it terminates in a state where the proposition p is true, and 0 otherwise. Figure A7 shows examples of the resulting optimal policies when the set of constraints is empty and non-empty.\nSince all task primitivesMG := {Mp | p \u2208 P\u222aC} share the same state space, action space, dynamics, and rewards at non-terminal states, the corresponding skill primitives Q\u2217G := {Q\u2217p | p \u2208 P \u222a C} can be composed to achieve any Boolean expression over P \u222a C (Nangue Tasse et al., 2022a). We next introduce skill machines which leverages skill primitives to encode the solution to temporal logic tasks."
        },
        {
            "heading": "3.2 SKILL MACHINES",
            "text": "We now have agents capable of solving any logical composition of task primitivesMG by learning only their corresponding skill primitives Q\u2217G and using the zero-shot composition operators (Section\n3The notation p\u0302 represents when a literal (a proposition p \u2208 P or its negation \u00acp) is being used as a constraint. Similarly, we will use P\u0302 or \u03c3\u0302 respectively when the literals in a set P or Boolean expression \u03c3 are constraints.\n2.2). Given this compositional ability over skills, and reward machines that expose the reward structure of tasks, agents can solve temporally extended tasks with little or no further learning. To achieve this, we define a skill machine (SM) as a representation of logical and temporal knowledge over skills. Definition 3.2 (Skill Machine). Let \u27e8S,A, \u03c1, \u03b3,P, L\u27e9 represent the environment the agent is in, and Q\u2217G be the corresponding skill primitives with constraints C. Given a reward machine RSA = \u27e8U , u0, \u03b4u, \u03b4r\u27e9, a skill machine is a tuple Q\u2217SA = \u27e8U , u0, \u03b4u, \u03b4Q\u27e9 where \u03b4Q : U \u2192 [SG \u00d7AG \u2192 R] is the state-skill function defined by:\n\u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) := max g\u2208G Q\u2217\u03c3u(\u27e8s, c\u27e9, g, \u27e8a, 0\u27e9),\nand Q\u2217\u03c3u is the composition of skill primitives Q \u2217 G according to a Boolean expression \u03c3u \u2208 22 P\u222aC .\nFor a given state s \u2208 S in the environment, the set of constraints violated c \u2286 C, and state u in the skill machine, the skill machine computes a skill \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) that an agent can use to take an action a. The environment then transitions to the next state s\u2032 with true propositions l\u2032\u2014where \u27e8s\u2032, c\u2032\u27e9 \u2190 PG(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) and l\u2032 \u2190 L(s\u2032)\u2014and the skill machine transitions to u\u2032 \u2190 \u03b4u(u, l\u2032). This process is illustrated in Figure A8 for the skill machine shown in Figure 1. Remarkably, because the Boolean compositions of skill primitives are optimal, there always exists a choice of skill machine that is optimal with respect to the corresponding reward machine, as shown in Theorem 3.3, which demonstrates that SMs can be used to solve tasks without having to relearn action level policies: Theorem 3.3. Let \u03c0\u2217(s, u) be the optimal policy for a task MT specified by an RM RSA. Then there exists a corresponding skill machine Q\u2217SA such that \u03c0\u2217(s, u) \u2208 argmaxa\u2208A \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9)."
        },
        {
            "heading": "3.3 FROM REWARD MACHINES TO SKILL MACHINES",
            "text": "In the previous section, we introduced skill machines and showed that they can be used to represent the logical and temporal composition of skills needed to solve tasks specified by reward machines. However, we only proved their existence\u2014for a given task, how can we acquire an SM that solves it?\nZero-shot via planning over the RM: To obtain the SM that solves a given RM, we first plan over the reward machine (using value iteration, for example) to produce action-values for each transition. We then select skills for each SM state greedily by applying Boolean composition to skill primitives according to the Boolean expressions defining: (i) the transition with the highest value (propositions to eventually satisfy); and (ii) the transitions with zero value (constrains to always satisfy). This process is illustrated by Figure A9. Since the skills per SM state are selected greedily, the policy generated by this SM is recursively optimal (Hutsebaut-Buysse et al., 2022)\u2014that is, it is locally optimal (optimal for each sub-task) but may not be globally optimal (optimal for the overall task). Interestingly, we show in Theorem A.2 that this policy is also satisficing (reaches an accepting state) if we assume global reachability\u2014all FSM transitions (that is all Boolean expressions \u03c3 \u2208 22P ) are achievable from any environment state. This is a more relaxed version of the assumption \u201cany state is reachable from any other state\u201d that is required to prove optimality in most RL algorithms, since an agent cannot learn an optimal policy if there are states it can never reach. Theorem 3.4. Let RSA = \u27e8U , u0, \u03b4u, \u03b4r\u27e9 be a satisfiable RM where all the Boolean expressions \u03c3 defining its transitions are in negation normal form (NNF) (Robinson & Voronkov, 2001) and are achievable from any state s \u2208 S. Define the corresponding SM Q\u2217SA = \u27e8U , u0, \u03b4u, \u03b4Q\u27e9 with \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) 7\u2192 maxg\u2208G Q\u2217(\u03c3P\u2227\u00ac\u03c3C)(\u27e8s, c\u27e9, g, \u27e8a, 0\u27e9) where \u03c3P := argmax\u03c3 Q\n\u2217(u, \u03c3), \u03c3C := \u2228 {\u03c3\u0302 | Q\u2217(u, \u03c3) = 0}, and Q\u2217(u, \u03c3) is the optimal Q-function for RSA. Then, \u03c0(s, u) \u2208 argmaxa\u2208A \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) is satisficing.\nTheorem A.2 is critical as it provides soundness guarantees, ensuring that the policy derived from the skill machine will always satisfy the task requirements.\nFew-shot via RL in the environment: Finally, in cases where the composed skill \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) obtained from the approximate SM is not sufficiently optimal, we can use any off-policy RL algorithm to learn the task-specific skill QT (s, u, a) few-shot. This is achieved by using the maximising Qvalues max{\u03b3QT , (1\u2212 \u03b3)\u03b4Q} in the exploration policy during learning. Here, the discount factor \u03b3 determines how much of the composed policy to use. Consider Q-learning, for example: during the \u03f5-greedy exploration, we use a\u2190 argmaxA max{\u03b3QT , (1\u2212 \u03b3)\u03b4Q} to select greedy actions. This improves the initial performance of the agent where \u03b3QT < (1\u2212 \u03b3)\u03b4Q, and guarantees convergence in the limit of infinite exploration, as in vanilla Q-learning. Appendix A.2 illustrates this process."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate our approach in three domains, including a high-dimensional, continuous control task. In particular, we consider the Office Gridworld (Figure A2a), the Moving Targets domain (Figure A1) and the Safety Gym domain (Figure 1). We briefly describe the domains and training procedure here, and provide more detail and hyperparameter settings in the appendix.\nOffice Gridworld (Icarte et al., 2022): Tasks are specified over 10 propositions P = {A,B,C,D,\u273d,K,B, x,B+, x+} and 1 constraint C = {\u273d\u0302}. We learn the skill primitives Q\u2217G (visualised by Figure A3) using goal-oriented Q-learning (Nangue Tasse et al., 2020), where the agent keeps track of reached goals and uses Q-learning (Watkins, 1989) to update the WVF with respect to all previously seen goals at every time step.\nMoving Targets Domain (Nangue Tasse et al., 2020): This is a canonical object collection domain with high dimensional pixel observations (84\u00d784\u00d73 RGB images). The agent here needs to pick up objects of various shapes and colours; collected objects respawn at random empty positions similarly to previous object collection domains (Barreto et al., 2020). There are 3 object colours\u2014beige ( ), blue ( ), purple ( )\u2014and 2 object shapes\u2014squares ( ), circles ( ). The tasks here are defined over 5 propositions P = { , , , , } and 5 constraints C = P\u0302 . We learn the corresponding skill primitives with goal-oriented Q-learning, but using deep Q-learning (Mnih et al., 2015) to update the WVFs.\nSafety Gym Domain (Ray et al., 2019): A continuous state and action space (S = R60,A = R2) domain where an agent, represented by a point mass, must navigate to various regions defined by 3 propositions (P = { , , }) corresponding to its 3 LiDAR sensors for the red cylinder , the green buttons , and the blue regions . We learn the three skill primitives corresponding to each predicate (with constraints C = {\u0302}), using goal-oriented Q-learning and TD3 (Fujimoto et al., 2018)."
        },
        {
            "heading": "4.1 ZERO-SHOT AND FEW-SHOT TEMPORAL LOGICS",
            "text": "Task Description \u2014 LTL 1 Deliver coffee to the office without breaking decorations | ( F ( K \u2227X ( F x ))) \u2227 (G \u00ac\u273d)\n2 Patrol rooms A, B, C, and D without breaking any decoration \u2014 (F (A \u2227X (F (B \u2227X (F (C \u2227X (FD))))))) \u2227 (G \u00ac\u273d) 3 Deliver coffee and mail to the office without breaking any decoration \u2014 (( F ( K \u2227X ( F ( B \u2227X ( Fx ))))) \u2228 ( F ( B \u2227X ( F ( K \u2227X ( Fx )))))) \u2227(G\u00ac\u273d) 4 Deliver mail to the office until there is no mail left, then deliver coffee to office while there are people in the office, then patrol rooms A-B-C-D-A, and never break a decoration \u2014 ( F ( B \u2227X ( F ( x \u2227X ( \u00acBU ( \u00acB+ \u2227B \u2227X ( F ( K \u2227X ( \u00acxU ( \u00acx+ \u2227 x \u2227X\n(FA \u2227X (F (B \u2227X (F (C \u2227X (F (D \u2227X (FA)))))))))))))))))) \u2227 (G \u00ac\u273d)\nTable 1: Tasks in the Office Gridworld. The RMs are generated from the LTL expressions.\nWe use the Office Gridworld as a multitask domain, and evaluate how long it takes an agent to learn a policy that can solve the four tasks described in Table 1. The tasks are sampled uniformly at random for each episode. In all of our experiments, we compare the performance of SMs without further learning and SMs paired with Q-learning (QL-SM) with that of regular Q-learning (QL) and the following state-of-the-art RM-based baselines (Icarte et al., 2022): (i) Counterfactual RMs (CRM): This augments Q-learning by updating the action-value function at each state (Q(s, u, a)) not just with respect to the current RM transition, but also with respect to all possible RM transitions from the current environment state. This is representative of approaches that leverage the compositional structure of RMs to learn optimal policies efficiently. (ii) Hierarchical RMs (HRM): The agent here uses Q-learning to learn options to achieve each RM state-transition, and an option policy to select which options to use at each RM state that are grounded in the environment states. This is representative of option-based approaches that learn hierarchically-optimal policies. (iii) Reward-shaped variants (QL-RS, CRM-RS, HRM-RS): The agent here uses the values obtained from value iteration over the RMs for reward shaping, on top of the regular QL, CRM, HRM algorithms. This is representative of approaches that leverage planning over the RM to speed up learning.\nIn addition to learning all four tasks at once, we also experiment with Tasks 1, 3 and 4 in isolation. In these single-task domains, the difference between the baselines and our approach should be more pronounced, since QL, CRM and HRM now cannot leverage the shared experience across multiple tasks. Thus, the comparison between multi-task and single-task learning in this setting will evaluate the benefit of the compositionality afforded by SMs, given that the 11 skill primitives used by the SMs here are pretrained only once for 1 \u00d7 105 time steps and used for all four experiments. For fairness towards the baselines, we run each of the four experiments for 4\u00d7 105 time steps. The results of these four experiments are shown in Figure 2. Regular Q-learning struggles to learn Task 3 and completely fails to learn the hardest task (Task 4). Additionally, notice that while QL and CRM can theoretically learn the tasks optimally given infinite time, only HRM, SM, and QL-SM are able to learn hard long horizon tasks in practice (like task 4). This is because of the temporal composition of skills leveraged in HRM, SM, and QL-SM. In addition, the skill machines are being used to zero-shot generalise to the office tasks using skill primitives. Thus using the skill machines alone (SM in Figure 2) may provide sub-optimal performance compared to the task-specific agents, since the SMs have not been trained to optimality and are not specialised to the domain. Even under these conditions, we observe that SMs perform near-optimally in terms of final performance, and due to the amortised nature of learning the WVF will achieve its final rewards from the first epoch.\nFinally, it is apparent from the results shown in Figure 2 that SMs paired with Q-learning (QL-SM) achieve the best performance when the zero-shot performance is not already optimal (see Appendix A4 for the trajectories of the agent with and without few-shot learning). Additionally, SMs with Q-learning always begin with a significantly higher reward and converge on their final performance faster than all baselines. The speed of learning is due to the compositionality of the skill primitives with SMs, and the high final performance is due to the generality of the learned primitives being paired with the domain-specific Q-learner. In sum, skill machines provide fast composition of skills and achieve optimal performance compared to all benchmarks when paired with a learning algorithm."
        },
        {
            "heading": "4.2 ZERO-SHOT TRANSFER WITH FUNCTION APPROXIMATION",
            "text": "We now demonstrate our temporal logic composition approach in the Moving Targets domain where function approximation is required. Figure 3 shows the average returns of the optimal policies and SM policies for the four tasks described in Table 2 with a maximum of 50 steps per episode. Our results show that even when using function approximation with sub-optimal skill primitives, the zero-shot policies obtained from skill machines are very close to optimal on average. We also observe that for very challenging tasks like Tasks 3 and 4 (where the agent must satisfy difficult temporal constraints), the compounding effect of the sub-optimal policies sometimes leads to failures. Finally, we provide a qualitative demonstration of our method\u2019s applicability to continuous control tasks using Safety Gym, a benchmark domain used for developing safe RL methods (Ray et al., 2019). We define a set of increasingly complex tasks and visualise the resulting trajectories after composing the agent\u2019s learned primitive skills. Figure 1 illustrates the trajectory that satisfies the task requiring the agent to navigate to a blue region, then to a red cylinder, and finally to another red cylinder while avoiding blue regions. See Appendix A.5 for all task specifications and trajectory visualisations.\nTask Description \u2014 LTL 1 Pick up any object. Repeat this forever. \u2014\nF ( \u2228 ) 2 Pick up blue then purple objects, then ob-\njects that are neither blue nor purple. Repeat this forever. \u2014 F ( \u2227 X(F ( \u2227 X(F (( \u2228 ) \u2227 \u00ac( \u2228 ))))))\n3 Pick up blue objects or squares, but never blue squares. Repeat this forever. \u2014 (F ( \u2228 )) \u2227 (G \u00ac( \u2227 )) 4 Pick up non-square blue objects, then nonblue squares in that order. Repeat this forever. \u2014 F ((\u00ac \u2227 ) \u2227X(F ( \u2227 \u00ac )))\nTable 2: Tasks in the Moving Targets domain. To repeat forever, the terminal states of the RMs generated from LTL are removed, and transitions to them are looped back to the start state."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Regularisation has previously been used to achieve semantically meaningful disjunction (Todorov, 2009; Van Niekerk et al., 2019) or conjunction (Haarnoja et al., 2018; Hunt et al., 2019). Weighted composition has also been demonstrated; for example, Peng et al. (2019) learn weights to compose existing policies multiplicatively to solve new tasks. Approaches built on successor features (SF) are capable of solving tasks defined by linear preferences over features (Barreto et al., 2020)., while Alver & Precup (2022) show that an SF basis can be learned that is sufficient to span the space of tasks under consideration. By contrast, our framework allows for both spatial composition (including operators such as negation that others do not support) and temporal composition such as LTL.\nA popular way of achieving temporal composition is through the options framework (Sutton et al., 1999). Here, high-level skills are first discovered and then executed sequentially to solve a task (Konidaris & Barto, 2009). Barreto et al. (2019) leverage the SF and options framework and learn how to linearly combine skills, chaining them sequentially to solve temporal tasks. However, these approaches offer a relatively simple form of temporal composition. By contrast, we are able to solve tasks expressed through regular languages zero-shot, while providing soundness guarantees.\nApproaches to defining tasks using human-readable logic operators also exist. Li et al. (2017) and Littman et al. (2017) specify tasks using LTL, which is then used to generate a reward signal for an RL agent. Camacho et al. (2019) perform reward shaping given LTL specifications, while Jothimurugan et al. (2019) develop a formal language that encodes tasks as sequences, conjunctions and disjunctions of subtasks. This is then used to obtain a shaped reward function that can be used for learning. These approaches focus on how to improve learning given such specifications, but we show how an explicitly compositional agent can immediately solve such tasks using WVFs without further learning."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We proposed skill machines\u2014finite state machines that can be learned from reward machines\u2014that allow agents to solve extremely complex tasks involving temporal and spatial composition. We demonstrated how skills can be learned and encoded in a specific form of goal-oriented value function that, when combined with the learned skill machines, are sufficient for solving subsequent tasks without further learning. Our approach guarantees that the resulting policy adheres to the logical task specification, which provides assurances of safety and verifiability to the agent\u2019s decision making, important characteristics that are necessary if we are to ever deploy RL agents in the real world. While the resulting behaviour is provably satisficing, empirical results demonstrate that the agent\u2019s performance is near optimal; further fine-tuning can be performed should optimality be required, which greatly improves the sample efficiency. We see this approach as a step towards truly generally intelligent agents, capable of immediately solving human-specifiable tasks in the real world with no further learning."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "GNT is supported by an IBM PhD Fellowship. This research was supported, in part, by the National Research Foundation (NRF) of South Africa under grant number 117808. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NRF.\nThe authors acknowledge the Centre for High Performance Computing (CHPC), South Africa, for providing computational resources to this research project. Computations were also performed using High Performance Computing Infrastructure provided by the Mathematical Sciences Support unit at the University of the Witwatersrand."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 PROOFS OF THEORETICAL RESULTS\nTheorem A.1. Let \u03c0\u2217(s, u) be the optimal policy for a task MT specified by an RM RSA. Then there exists a corresponding skill machine Q\u2217SA such that\n\u03c0\u2217(s, u) \u2208 argmax a\u2208A \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9).\nProof. Define the skill per SM state Q\u2217u to be the Boolean composition of skill primitives that satisfy the set of propositions g \u2208 2P\u222aC , where g is the set of propositions satisfied and constraints violated when following \u03c0\u2217(s, u). Then \u03c0\u2217(s, u) \u2208 argmaxa\u2208A \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) since Q\u2217u is optimal using Nangue Tasse et al. (2022b) and optimal policies maximise the probability reaching goals (since the rewards are non-zero only at the desirable goal states, where they are 1).\nTheorem A.2. Let RSA = \u27e8U , u0, \u03b4u, \u03b4r\u27e9 be a satisfiable RM where all the Boolean expressions \u03c3 defining its transitions are in negation normal form (NNF) (Robinson & Voronkov, 2001) and are achievable from any state s \u2208 S. Define the corresponding SM Q\u2217SA = \u27e8U , u0, \u03b4u, \u03b4Q\u27e9 with \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) 7\u2192 maxg\u2208G Q\u2217(\u03c3P\u2227\u00ac\u03c3C)(\u27e8s, c\u27e9, g, \u27e8a, 0\u27e9) where \u03c3P \u2190 argmax\u03c3 Q\n\u2217(u, \u03c3), \u03c3C \u2190 \u2228 {\u03c3\u0302 | Q\u2217(u, \u03c3) = 0}, and Q\u2217(u, \u03c3) is the optimal Q-function for RSA. Then, \u03c0(s, u) \u2208 argmaxa\u2208A \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9) is satisficing.\nProof. This follows from the optimality of Boolean skill composition and the optimality of value iteration, since each transition of the RM is satisfiable from any environment state.\nA.2 FULL PSEUDO-CODES OF FRAMEWORK\nAlgorithm 1: Q-learning for skill primitives Input : S,A,P, C, \u03b3, \u03b1,RMAX = 1, RMIN = 0 Initialise : QMAX(\u27e8s, c\u27e9, g, \u27e8a, a\u03c4 \u27e9) and QMIN (\u27e8s, c\u27e9, g, \u27e8a, a\u03c4 \u27e9), goal buffer G = {\u2205}\n1 foreach episode do 2 Observe initial state s \u2208 S and true propositions l \u2208 2P , sample c \u2208 2C and g \u2208 G 3 while episode is not done do\n4 \u27e8a, a\u03c4 \u27e9 \u2190 argmax\u27e8a,a\u03c4 \u27e9 QMAX(\u27e8s, c\u27e9, g, \u27e8a, a\u03c4 \u27e9) if Bernoulli(1\u2212 \u03f5) = 1sample \u27e8a, a\u03c4 \u27e9 \u2208 A \u00d7 {0, 1} otherwise 5 Execute a and observe next state s\u2032 and true propositions l\u2032\n6 Get true constraints c\u2032 \u2190 c \u222a ((l\u0302 \u2295 l\u0302\u2032) \u2229 C) 7 if (a\u03c4 = 1) then G\u2190 G \u222a {l\u2032 \u222a c} 8 foreach Q \u2208 {QMAX ,QMIN} do 9 if (a\u03c4 \u0338= 1) then r \u2190 0\n10 if (a\u03c4 = 1 and Q = QMAX ) then r \u2190 RMAX 11 if (a\u03c4 = 1 and Q = QMIN ) then r \u2190 RMIN 12 foreach g\u2032 \u2208 G do 13 r\u0304 \u2190 RMIN if (a\u03c4 = 1 and g\u2032 \u0338= l\u2032 \u222a c) else r 14 if (s\u2032 is terminal or a\u03c4 = 1) then 15 Q(\u27e8s, c\u27e9, g\u2032, \u27e8a, a\u03c4 \u27e9)\n\u03b1\u2190\u2212 r\u0304 16 else 17 Q(\u27e8s, c\u27e9, g\u2032, \u27e8a, a\u03c4 \u27e9) \u03b1\u2190\u2212 [ r\u0304 + \u03b3max\u27e8a\u2032,a\u2032\u03c4 \u27e9 Q(\u27e8s \u2032, c\u2032\u27e9, g\u2032, \u27e8a\u2032, a\u2032\u03c4 \u27e9) ] 18 s\u2190 s\u2032 and c\u2190 c\u2032 19 if (a\u03c4 = 1) then terminate episode 20 QG \u2190 \u2205 21 foreach p \u2208 P \u222a C do 22 Qp(\u27e8s, c\u27e9, g, \u27e8a, a\u03c4 \u27e9) := QMAX(\u27e8s, c\u27e9, g, \u27e8a, a\u03c4 \u27e9) if (p \u2208 g) else QMIN (\u27e8s, c\u27e9, g, \u27e8a, a\u03c4 \u27e9) 23 QG \u2190QG \u222a {Qp} 24 return QG\nAlgorithm 2: Skill machine from reward machine Input : QG , \u27e8U , u0, \u03b4u, \u03b4r, \u27e9, \u03b3 Initialise :RM value function Q(u, \u03c3), value iteration error \u2206 = 1\n1 Let B(u) := the set of Boolean expressions defining the RM transitions \u03b4u(u, \u00b7) /* Value iteration */ 2 while \u2206 > 0 do 3 \u2206\u2190 0 4 for u \u2208 U do 5 for \u03c3 \u2208 B(u) do 6 v\u2032 \u2190 \u03b4r(u, \u03c3) + \u03b3max\u03c3\u2032 Q(\u03b4u(u, \u03c3), \u03c3\u2032) 7 \u2206 = max{\u2206, |Q(u, \u03c3)\u2212 v\u2032|} 8 Q(u, \u03c3)\u2190 v\u2032 9\n/* Skill machine\u2019s skill function */ 10 for u \u2208 U do 11 \u03c3P , \u03c3C \u2190 argmax\u03c3\u2032 Q(u, \u03c3\u2032), \u2228 {\u03c3\u0302 | Q(u, \u03c3) = 0} 12 Q\u03c3P\u2227\u00ac\u03c3C \u2190 composition of QG as per the Boolean expression \u03c3P \u2227 \u00ac\u03c3C 13 \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9)\u2190 maxg\u2208G Q\u03c3P\u2227\u00ac\u03c3C (\u27e8s, c\u27e9, g, \u27e8a, 0\u27e9) 14 return \u27e8U , u0, \u03b4u, \u03b4Q\u27e9\nAlgorithm 3: Zero-shot and Few-shot Q-learning with skill machines Input : S,A,P, C, \u27e8U , u0, \u03b4u, \u03b4Q\u27e9, \u03b3, \u03b1 Initialise : Q(s, u, a)\n1 foreach episode do 2 Observe initial state s \u2208 S and propositions l \u2208 2P , RM state u\u2190 u0 and constraints c\u2190 \u2205 3 while episode is not done do 4 if zero-shot then 5 a\u2190 argmax\na \u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9)\n6 else /* Fewshot by using \u03b4Q in the behaviour policy */\n7 a\u2190{ argmax\na (max{\u03b3Q(s, u, a), (1\u2212 \u03b3)\u03b4Q(u)(\u27e8s, c\u27e9, \u27e8a, 0\u27e9)}) if Bernoulli(1\u2212 \u03f5) = 1\nsample a \u2208 A otherwise 8 Take action a and observe the next state s\u2032 and true propositions l\u2032\n9 Get reward r \u2190 \u03b4r(u)(s, a, s\u2032), true constraints c\u2190 c \u222a ((l\u0302 \u2295 l\u0302\u2032) \u2229 C), 10 and the next RM state u\u2032 \u2190 \u03b4u(u, l\u2032) 11 if u \u0338= u\u2032 then c\u2190 \u2205 12 if s\u2032 or u\u2032 is terminal then 13 Q(s, u, a)\n\u03b1\u2190\u2212 r 14 else 15 Q(s, u, a) \u03b1\u2190\u2212 [ r + \u03b3max\na\u2032 Q(s\u2032, u\u2032, a\u2032) ] 16 s\u2190 s\u2032 and u\u2190 u\u2032\nA.3 DETAILS OF EXPERIMENTAL SETTINGS\nIn this section we elaborate further on the hyper-parameters for the various experiments in Section 4. We also describe the pretraining of WVFs for all of the experimental settings which corresponds to learning the task primitives for each domain. The same hyper-parameters are used for all algorithms in a particular experiment. This is to ensure that we evaluate the relative performance fairly and consistently. The full list of hyper-parameters for the Office World, Moving Targets and SafeAI Gym domain experiments are shown in Tables A1-A3 respectively.\nHyper-parameter Value\nTimesteps 1e5 Training exploration (\u03f5) 0.5\nPer-episode evaluation exploration (\u03f5) 0.1 Discount Factor (\u03b3) 0.9\nTable A1: Table of hyper-parameters used for Q-learning in the Office World experiments.\nTo use skill machines we require pre-trained WVFs. As mentioned above, all WVFs are trained using the same hyper-parameters as any other training. Additionally, for all experiments the WVFs are pre-trained on the base task primitives for the domain. For example, in the Office World domain, the WVFs are trained on the |P \u222a C| base task primitives corresponding to achieving each predicate, P = {A,B,C,D,\u273d,K,B, x,B+, x+} (reaching states the predicate is set to True), with constraints C = {\u273d\u0302}. All other primitives in this domain can be obtained zero-shot through value function composition. Similarly, for the moving targets domain (Figure A1), the WVFs are pre-trained on the primitives corresponding to obtaining objects by shape or colour in the environment separately, P = { , , , , }, with constraints C = P\u0302 . From here the value functions for finding objects of particular colours or any more complex primitives can be composed zero-shot. Finally, for the SafeAI Gym environment the base skill primitives correspond to going to a cylinder ( ), a button ( ), and a blue region ( ): P = { , , }, trained with constraints C = {\u0302}.\nHyper-parameter Value\nTimesteps 1e6 Neural Network architecture CNN + MLP\nCNN architecture Defaults of Mnih et al. (2015) MLP hidden layers 1024\u00d7 1024\u00d7 1024 Start exploration (\u03f5) 1 End exploration (\u03f5) 0.1\nExploration decay duration (\u03f5) 5e5 Discount Factor (\u03b3) 0.99\nOthers Defaults of Mnih et al. (2015)\nTable A2: Table of hyper-parameters used for Deep Q-learning in the Moving Targets experiments.\nHyper-parameter Value\nTimesteps 1e6 Neural Network architecture MLP\nMLP hidden layers 2024\u00d7 2024\u00d7 2024 Max episodes length 100\nTarget noise 0.2 Action noise 0.2\nDiscount Factor (\u03b3) 0.99 Others Defaults of Achiam (2018)\nTable A3: Table of hyper-parameters used for the TD3 in the SafeAI Gym experiments.\nFigure A1: Moving Targets domain\nA.4 OFFICE GRIDWORLD ADDITIONAL EXPERIMENTS AND FIGURES\nWe illustrate the environment and an example temporal logic task in it in Figure A2. In this environment, an agent (blue circle) can move to adjacent cells in any of the cardinal directions (|A| = 4) and observe its (x, y) position (|S| = 120). Cells markedK,B, and x respectively represent the coffee, mail, and office locations. Those marked \u273d indicate decorations that are broken if the agent collides with them, and A\u2013D indicate the corner rooms. The reward machines that specify tasks in this environment are defined over 10 propositions: P = {A,B,C,D,\u273d,K,B, x,B+, x+}, where the first 8 propositions are true when the agent is at their respective locations,B+ is true when the agent is atB and there is mail to be collected, and x+ is true when the agent is at x and there is someone in the office.\n(a) Office Gridworld (b) Reward Machine (c) Skill Machine Figure A2: Illustration of (a) the office gridworld where the blue circle represents the agent; (b) the reward machine for the task \u201cdeliver coffee and mail to the office without breaking any decoration\u201d, given by the LTL specification(( F ( K \u2227X ( F ( B \u2227X ( Fx ))))) \u2228 ( F ( B \u2227X ( F ( K \u2227X ( Fx )))))) \u2227 (G\u00ac\u273d); (c) the skill machine obtained from the reward machine which can then be used to achieve the task specification zero-shot\u2014the red trajectory in (a). The nodes labeled t represent terminal states.\n(a) Room A (b) Room B (c) Room C (d) Room D\n(e) Decoration \u273d (f) CoffeeK (g) MailB (h) Office x\n(i) Mails presentB+ (j) People present x+ (k) Decoration constraint \u273d\u0302\nFigure A3: The policies (arrows) and value functions (heat map) of the primitive tasks in the Office Gridworld. These are obtained by maximising over the goals of the learned WVFs.\nFigure A3 shows the skill primitives learned for each proposition in the environment, and Figure A4 shows the trajectories of our zero-shot agent (SM) and few-shot agent (QL-SM) for various tasks. Finally, we run 2 experiments to demonstrate the performance of our zero-shot and few-shot approach when the global reachability assumption does not hold.\n1. When the reachability assumption is not satisfied in some initial states: In the first experiment (Figure A5), the agent needs to solve task 1 of Table 1 ((F (K \u2227X(F x))) \u2227 (G \u00ac\u273d)), but we modify the environment such that one of the coffee locations is absorbing (a sink environment state). This breaks the global reachability assumption since the agent can no longer reach the office location after it reaches the absorbing coffee location. As a result, we observe that the zero-shot agent (SM) is even more sub-optimal than before\nbecause it cannot satisfy the task when it starts at locations that are closer to the absorbing coffee location. However, we can observe that the few-shot agent (QL-SM) is still able to learn the optimal policy, starting with the same performance as the zero-shot agent. Note that the hierarchical agent (HRM) also converges to the same performance as our zero-shot agent because it also tries to reach the nearest coffee location.\n2. When the reachability assumption is not satisfied in all initial states: In the second experiment (Figure A6), the agent needs to solve the task with LTL specification (F x)\u2227(\u00acxUK)\u2014 the environment is still modified such that one of the coffee locations is absorbing. Here, the Boolean expressionK \u2227 x is not satisfiable since there is no state where both propositions (K and x) are true. Hence, this can be seen as the worst-case scenario for our approach (without outright making the task unsatisfiable), since K \u2227 x is the Boolean expression greedily selected in the starting RM state. As a result, our zero-shot agent completely fails to solve this task. Even in this case, we can observe that the few-shot agent is still able to learn the optimal policy.\n(a) Task 1 zero-shot (SM) (b) Task 1 few-shot (QL-SM)\n(c) Task 2 zero-shot (SM) (d) Task 2 few-shot (QL-SM)\n(e) Task 3 zero-shot (SM) (f) Task 3 few-shot (QL-SM)\nFigure A4: Agent trajectories for various tasks in the Office Gridworld (Table 1) using the skill machine without further learning (left) and with further learning (right).\n(a) Reward machine (b) Value iterated RM (c) Skill machine\n0 1 2 3 4 Steps 1e5\n0\n250\n500\n750\n1000\n1250\n1500\n1750\nTo ta\nl R ew\nar d\nSM (Ours) QL-SM (Ours) CRM CRM-RS HRM HRM-RS QL QL-RS\n(d) Average Returns (e) Zero-shot (SM) (f) Few-shot (QL-SM)\nFigure A5: Results for the task with LTL specification (F (K\u2227X(F x)))\u2227 (G \u00ac\u273d) when the global reachability assumption does not hold. Here, the Office Gridworld is modified such that the position of the lower right coffee (the red cell) is made absorbing (the agent can not leave that state after reaching it). We show: (a) the reward machine for the task, (b) the value iterated reward machine (using \u03b3 = 0.9), (c) the resulting skill machine, (d) the resulting average returns compared to the baselines, (e) sample trajectories of the zero-shot agent, and (f) sample trajectory of the few-shot agent.\n(a) Reward machine (b) Value iterated RM (c) Skill machine\n0 1 2 3 4 Steps 1e5\n0\n200\n400\n600\n800\n1000\n1200\nTo ta\nl R ew\nar d\nSM (Ours) QL-SM (Ours) CRM CRM-RS HRM HRM-RS QL QL-RS\n(d) Average Returns (e) Zero-shot (SM) (f) Few-shot (QL-SM)\nFigure A6: Results for the task with LTL specification (F x)\u2227(\u00acx UK) where the global reachability assumption is not satisfied. Here, the Office Gridworld is modified such that the position of the lower right coffee (the red cell) is made absorbing. We show (a) the RM for the task, (b) the value iterated RM (using \u03b3 = 0.9), (c) the resulting SM, (d) the resulting average returns compared to the baselines, (e) sample trajectories of the zero-shot agent, and (f) sample trajectory of the few-shot agent.\nA.5 FUNCTION APPROXIMATION WITH CONTINUOUS ACTIONS AND STATES\nWe demonstrate our temporal logic composition approach in a Safety Gym domain (Ray et al., 2019) which has a continuous state space (S = R60) and continuous action space (A = R2). The agent here is a point mass that needs to navigate to various regions defined by 3 propositions (P = { , , }) corresponding to its 3 lidar sensors for the red cylinder ( ), the green buttons ( ), and the blue regions ( ). The agent, 4 buttons and 2 blue regions are randomly placed on the plane. The cylinder is randomly placed on one of the buttons. We first learn the 4 base skill primitives corresponding to each predicate (with constraints C = {\u0302}), with goal-oriented Q-learning Nangue Tasse et al. (2020) but using Twin Delayed DDPG (Fujimoto et al., 2018) to update the WVFs. Figure A10 shows the trajectories of the SM policies for the six tasks described in Table A4. Our results shows that skill primitives can be leveraged to achieve zero-shot temporal logics even in continuous domains.\n(a) M \u2227 \u00acM ,\ng = { , }, r = 1\n(b) M \u2227\u00acM \u2227\u00acM\u0302 , g = { , }, r = 1 (c) M \u2227M \u2227 \u00acM\u0302 , g = { , , }, r = 1 (d) M \u2227\u00acM \u2227\u00acM\u0302 , g = { , , \u0302}, r = 0\nFigure A7: Effect of constraints on primitives (C = {\u0302}). We show compositions of task primitives (for example M\n\u2227 \u00acM where the agent needs to achieve \u2227 \u00ac ), trajectories, goal reached (g), and reward obtained (r) when following: (a-c) Optimal policies; and (d) a non-optimal policy.\n(a) Skill machine (b) c = \u2205, l = \u2205, Q\u03c3u = Q \u2227 \u00acQ \u2227 \u00acQ\u0302 (c) c=\u2205, l={ }, Q\u03c3u = Q \u2227 \u00acQ \u2227 \u00acQ\u0302 (d) c = \u2205, l = { , },episode terminates Figure A8: Execution of a skill machine in the Safety Gym domain. (a) An example skill machine; (b) A snapshot of the environment at the initial state. In this state, no constraint has been reached (c = \u2205), no proposition is true (l = \u2205), the SM is at state u = u0, and the composed skill outputted by the SM is Q\u03c3u = Q \u2227 \u00acQ \u2227 \u00acQ\u0302 (which the agent uses to act in the environment); (c) The trajectory of the agent until it achieves \u2227 \u00ac \u2227 \u00ac \u0302. In the current environment state, no constraint has been reached (c = \u2205), the agent is at a green button (l = { }), the SM transitions to state u = u1, and the composed skill outputted by the SM is Q\u03c3u = Q\n\u2227 \u00acQ \u2227 \u00acQ\u0302 (which the agent uses to act in the environment); (d) The trajectory of the agent until the agent achieves\n\u2227 \u00ac \u2227 \u00ac\u0302. In the current environment state, no constraint has been reached (c = \u2205), the agent is at the red cylinder and a green button (l = { , }), the SM transitions to the terminal state t, and the episode terminates.\n(a) Reward machine (b) Value iterated RM (c) Skill machine\nFigure A9: The reward machine, value iterated reward machine (using \u03b3 = 0.9) and skill machine for the task with LTL specification (F ( \u2227X(F )))\u2227(G \u00ac ). The agent composes its skill primitives to achieve \u03c3P \u2227 \u00ac\u03c3C = ( \u2227 \u00ac ) \u2227 \u00ac(\u0302) at u0 and \u03c3P \u2227 \u00ac\u03c3C = ( \u2227 \u00ac ) \u2227 \u00ac(\u0302) at u1.\nTask Description \u2014 LTL\n1 Navigate to a button and then to a cylinder. \u2014 (F ( \u2227X(F ))) 2 Navigate to a button and then to a cylinder while never entering blue regions\n\u2014 (F ( \u2227X(F ))) \u2227 (G \u00ac ) 3 Navigate to a button, then to a cylinder without entering blue regions, then to a button\ninside a blue region, and finally to a cylinder again. \u2014 F ( \u2227X(F (( \u2227 \u00ac ) \u2227X(F (( \u2227 ) \u2227X(F ))))))\n4 Navigate to a button and then to a cylinder in a blue region. \u2014 (F ( \u2227X(F \u2227 ))) 5 Navigate to a cylinder, then to a button in a blue region, and finally to a cylinder again.\n\u2014 (F ( \u2227X(F (( \u2227 ) \u2227X( )))) 6 Navigate to a blue region, then to a button with a cylinder, and finally to a cylinder while\navoiding blue regions. \u2014 (F ( \u2227X(F (( \u2227 ) \u2227X((F ) \u2227 (G\u00ac ))))))\nTable A4: Tasks in the Safety Gym domains. The RMs are generated from the LTL expressions.\n(a) Task 1 (b) Task 2 (c) Task 3\n(d) Task 4 (e) Task 5 (f) Task 6\nFigure A10: Visualisations of the trajectories obtained by following the zero-shot composed policies from the skill machine for tasks in Table A4."
        }
    ],
    "title": "SKILL MACHINES: TEMPORAL LOGIC SKILL COMPOSITION IN REINFORCEMENT LEARNING",
    "year": 2024
}