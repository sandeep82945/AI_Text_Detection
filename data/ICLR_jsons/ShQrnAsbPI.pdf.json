{
    "abstractText": "Recent years have witnessed a burgeoning interest in federated learning (FL). However, the contexts in which clients engage in sequential learning remain underexplored. Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL). Existing research in FCL primarily focuses on mitigating the catastrophic forgetting issue of continual learning while collaborating with other clients. We argue that forgetting phenomena are not invariably detrimental. In this paper, we consider a more practical and challenging FCL setting characterized by potentially unrelated or even antagonistic data/tasks across different clients. In the FL scenario, statistical heterogeneity and data noise among clients may exhibit spurious correlations which result in biased feature learning. While existing CL strategies focus on the complete utilization of previous knowledge, we found that forgetting biased information was beneficial in our study. Therefore, we propose a new concept accurate forgetting (AF) and develop a novel generative-replay method AF-FCL that selectively utilizes previous knowledge in federated networks. We employ a probabilistic framework based on a normalizing flow model to quantify the credibility of previous knowledge. Comprehensive experiments affirm the superiority of our method over baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "CONTINUAL LEARNING"
        },
        {
            "affiliations": [],
            "name": "Abudukelimu Wuerkaixi"
        },
        {
            "affiliations": [],
            "name": "Sen Cui"
        },
        {
            "affiliations": [],
            "name": "Jingfeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Kunda Yan"
        },
        {
            "affiliations": [],
            "name": "Bo Han"
        },
        {
            "affiliations": [],
            "name": "Gang Niu"
        },
        {
            "affiliations": [],
            "name": "Lei Fang"
        },
        {
            "affiliations": [],
            "name": "Changshui Zhang"
        },
        {
            "affiliations": [],
            "name": "Masashi Sugiyama"
        }
    ],
    "id": "SP:8dfc7fc3b03f0a9129fc89a6a2436758273e6541",
    "references": [
        {
            "authors": [
                "Yavuz Faruk Bakman",
                "Duygu Nur Yaldiz",
                "Yahya H Ezzeldin",
                "Salman Avestimehr"
            ],
            "title": "Federated orthogonal training: Mitigating global catastrophic forgetting in continual federated learning",
            "venue": "arXiv preprint arXiv:2309.01289,",
            "year": 2023
        },
        {
            "authors": [
                "Johann Brehmer",
                "Kyle Cranmer"
            ],
            "title": "Flows for simultaneous manifold learning and density estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Giuseppe Canonaco",
                "Alex Bergamasco",
                "Alessio Mongelluzzo",
                "Manuel Roveri"
            ],
            "title": "Adaptive federated learning in presence of concept drift",
            "venue": "In 2021 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2021
        },
        {
            "authors": [
                "Fernando E Casado",
                "Dylan Lema",
                "Roberto Iglesias",
                "Carlos V Regueiro",
                "Sen\u00e9n Barro"
            ],
            "title": "Federated and continual learning for classification tasks in a society of devices",
            "year": 2006
        },
        {
            "authors": [
                "Pengfei Chen",
                "Guangyong Chen",
                "Junjie Ye",
                "Pheng-Ann Heng"
            ],
            "title": "Noise against noise: stochastic label noise helps combat inherent label noise",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Gregory Cohen",
                "Saeed Afshar",
                "Jonathan Tapson",
                "Andre Van Schaik"
            ],
            "title": "Emnist: Extending mnist to handwritten letters",
            "venue": "In 2017 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2017
        },
        {
            "authors": [
                "Jiahua Dong",
                "Lixu Wang",
                "Zhen Fang",
                "Gan Sun",
                "Shichao Xu",
                "Xiao Wang",
                "Qi Zhu"
            ],
            "title": "Federated class-incremental learning",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Arthur Douillard",
                "Matthieu Cord",
                "Charles Ollion",
                "Thomas Robert",
                "Eduardo Valle"
            ],
            "title": "Podnet: Pooled outputs distillation for small-tasks incremental learning",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Conor Durkan",
                "Artur Bekasov",
                "Iain Murray",
                "George Papamakarios"
            ],
            "title": "Neural spline flows. In Advances in Neural Information Processing Systems",
            "venue": "Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Yongxin Guo",
                "Tao Lin",
                "Xiaoying Tang"
            ],
            "title": "A new analysis framework for federated learning on time-evolving heterogeneous data, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Bo Han",
                "Gang Niu",
                "Xingrui Yu",
                "Quanming Yao",
                "Miao Xu",
                "Ivor W. Tsang",
                "Masashi Sugiyama"
            ],
            "title": "SIGUA: forgetting may make learning with noisy labels more robust",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Sean M Hendryx",
                "Dharma Raj KC",
                "Bradley Walls",
                "Clayton T Morrison"
            ],
            "title": "Federated reconnaissance: Efficient, distributed, class-incremental learning",
            "venue": "arXiv preprint arXiv:2109.00150,",
            "year": 2021
        },
        {
            "authors": [
                "Myeongho Jeon",
                "Daekyung Kim",
                "Woochul Lee",
                "Myungjoo Kang",
                "Joonseok Lee"
            ],
            "title": "A conservative approach for unbiased learning on unknown biases",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Myeongho Jeon",
                "Hyoje Lee",
                "Yedarm Seong",
                "Myungjoo Kang"
            ],
            "title": "Learning without prejudices: Continual unbiased learning via benign and malignant forgetting",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Ellango Jothimurugesan",
                "Kevin Hsieh",
                "Jianyu Wang",
                "Gauri Joshi",
                "Phillip B Gibbons"
            ],
            "title": "Federated learning under distributed concept drift",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Prafulla Dhariwal"
            ],
            "title": "Glow: Generative flow with invertible 1x1 convolutions",
            "venue": "In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Abhishek Kumar",
                "Hal Daume III"
            ],
            "title": "Learning task grouping and overlap in multi-task learning",
            "venue": "arXiv preprint arXiv:1206.6417,",
            "year": 2012
        },
        {
            "authors": [
                "Matthias De Lange",
                "Rahaf Aljundi",
                "Marc Masana",
                "Sarah Parisot",
                "Xu Jia",
                "Ales Leonardis",
                "Gregory G. Slabaugh",
                "Tinne Tuytelaars"
            ],
            "title": "A continual learning survey: Defying forgetting in classification tasks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2022
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Shengyuan Hu",
                "Ahmad Beirami",
                "Virginia Smith"
            ],
            "title": "Ditto: Fair and robust federated learning through personalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Zhizhong Li",
                "Derek Hoiem"
            ],
            "title": "Learning without forgetting",
            "venue": "In Computer Vision - ECCV 2016 - 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Chenghao Liu",
                "Xiaoyang Qu",
                "Jianzong Wang",
                "Jing Xiao"
            ],
            "title": "Fedet: A communication-efficient federated class-incremental learning framework based on enhanced transformer",
            "venue": "In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Seyed-Iman Mirzadeh",
                "Mehrdad Farajtabar",
                "Razvan Pascanu",
                "Hassan Ghasemzadeh"
            ],
            "title": "Understanding the role of training regimes in continual learning",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Seyed-Iman Mirzadeh",
                "Mehrdad Farajtabar",
                "Dilan G\u00f6r\u00fcr",
                "Razvan Pascanu",
                "Hassan Ghasemzadeh"
            ],
            "title": "Linear mode connectivity in multitask and continual learning",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Gary Sivek",
                "Ananda Theertha Suresh"
            ],
            "title": "Agnostic federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "year": 2011
        },
        {
            "authors": [
                "Augustus Odena",
                "Christopher Olah",
                "Jonathon Shlens"
            ],
            "title": "Conditional image synthesis with auxiliary classifier gans",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning, ICML 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Kunjal Panchal",
                "Sunav Choudhary",
                "Subrata Mitra",
                "Koyel Mukherjee",
                "Somdeb Sarkhel",
                "Saayan Mitra",
                "Hui Guan"
            ],
            "title": "Flash: Concept drift adaptation in federated learning",
            "year": 2023
        },
        {
            "authors": [
                "Daiqing Qi",
                "Handong Zhao",
                "Sheng Li"
            ],
            "title": "Better generative replay for continual federated learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Liangqiong Qu",
                "Yuyin Zhou",
                "Paul Pu Liang",
                "Yingda Xia",
                "Feifei Wang",
                "Ehsan Adeli",
                "Li Fei-Fei",
                "Daniel Rubin"
            ],
            "title": "Rethinking architecture design for tackling data heterogeneity in federated learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Sebastian Thrun"
            ],
            "title": "A lifelong learning perspective for mobile robot control",
            "venue": "In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 1994
        },
        {
            "authors": [
                "Anastasiia Usmanova",
                "Fran\u00e7ois Portet",
                "Philippe Lalanda",
                "Germ\u00e1n Vega"
            ],
            "title": "A distillation-based approach integrating continual learning and federated learning for pervasive services",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris S. Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Liyuan Wang",
                "Xingxing Zhang",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "A comprehensive survey of continual learning: Theory, method and application",
            "venue": "arXiv preprint arXiv:2302.00487,",
            "year": 2023
        },
        {
            "authors": [
                "Christina Winkler",
                "Daniel E. Worrall",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "Learning likelihoods with conditional normalizing flows",
            "year": 1912
        },
        {
            "authors": [
                "Chenshen Wu",
                "Luis Herranz",
                "Xialei Liu",
                "Yaxing Wang",
                "Joost van de Weijer",
                "Bogdan Raducanu"
            ],
            "title": "Memory replay gans: Learning to generate new categories without forgetting",
            "venue": "In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning",
            "venue": "algorithms. CoRR,",
            "year": 2017
        },
        {
            "authors": [
                "Qiang Yang",
                "Yang Liu",
                "Tianjian Chen",
                "Yongxin Tong"
            ],
            "title": "Federated machine learning: Concept and applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
            "year": 2019
        },
        {
            "authors": [
                "Jaehong Yoon",
                "Wonyong Jeong",
                "Giwoong Lee",
                "Eunho Yang",
                "Sung Ju Hwang"
            ],
            "title": "Federated continual learning with weighted inter-client transfer",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jaehong Yoon",
                "Wonyong Jeong",
                "Giwoong Lee",
                "Eunho Yang",
                "Sung Ju Hwang"
            ],
            "title": "Federated continual learning with weighted inter-client transfer",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jingfeng Zhang",
                "Xilie Xu",
                "Bo Han",
                "Tongliang Liu",
                "Gang Niu",
                "Lizhen Cui",
                "Masashi Sugiyama"
            ],
            "title": "Noilin: Improving adversarial training and correcting stereotype of noisy labels",
            "venue": "arXiv preprint arXiv:2105.14676,",
            "year": 2021
        },
        {
            "authors": [
                "Jingfeng Zhang",
                "Bo Song",
                "Haohan Wang",
                "Bo Han",
                "Tongliang Liu",
                "Lei Liu",
                "Masashi Sugiyama"
            ],
            "title": "Badlabel: A robust perspective on evaluating and enhancing label-noise learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2024
        },
        {
            "authors": [
                "Jianing Zhu",
                "Jingfeng Zhang",
                "Bo Han",
                "Tongliang Liu",
                "Gang Niu",
                "Hongxia Yang",
                "Mohan Kankanhalli",
                "Masashi Sugiyama"
            ],
            "title": "Understanding the interaction of adversarial training with noisy labels",
            "venue": "arXiv preprint arXiv:2102.03482,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Junyuan Hong",
                "Jiayu Zhou"
            ],
            "title": "Data-free knowledge distillation for heterogeneous federated learning",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "y Jeon"
            ],
            "title": "Relying on such biased attribute would result in poor generalizability of the algorithm. The biased features could be attained through biased training dataset and the learned mapping f relying on the biased features may not perform well in the testing dataset. For instance, if in the training image dataset all cows are standing on the grass, the machine learning model may rely on the attribute \u2019grass\u2019 for classifying images of cows",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Continual learning is a learning scenario where a model tries to learn a series of new arriving tasks and maintain performance on old tasks (Thrun, 1994; Kumar & Daume III, 2012; Li & Hoiem, 2016; Jeon et al., 2023). This approach, inspired by human lifelong learning, is central to advancing the development of artificial general intelligence. Since birth, a person would gather experience about real world by constantly learning various tasks and remembering them. Humans not only accumulate knowledge through self-directed learning but also collaboratively learn from others. However, concerns about data privacy and communication overhead arise when cooperating with others. Federated learning, which has attracted significant interests and gained various applications in industry (McMahan et al., 2017; Yang et al., 2019; Li et al., 2021), has been an alternative to addressing these concerns. This leads to the concept of federated continual learning (FCL) (Qi et al., 2023), incorporating continual learning into federated learning.\nIn FCL, the goal is that clients learn models for their private sequential tasks collaboratively without violating the data privacy of individual clients. This could encounter challenges from three fronts. One is statistical heterogeneity due to non-IID data across local clients. Such heterogeneity could severely degrade performance (Qu et al., 2022) when learning from clients collaboratively. Another is catastrophic forgetting, stemming from restricted access to data from previous tasks due to realistic factors such as storage constraints, privacy issues, etc (Wang et al., 2023). This can lead the model to lose its ability to perform previous tasks proficiently after assimilating new tasks. There are\n*These authors contributed equally to this work. \u2020Corresponding authors Code is at: https://github.com/zaocan666/AF-FCL.\na few studies seeking to address the above two problems in FCL. For example, Usmanova et al. (2021) extended the Learning without Forgetting (Li & Hoiem, 2016) method to the FCL scenario, memorizing previous tasks among all clients. The third concern is associated with the potential introduction of feature bias resulting from the federated scenario, which in turn could impact the memory within CL models. Research indicates that the memorization of noisy labels can significantly impair the model\u2019s performance (Han et al., 2020).\nExisting research developing FCL methods mainly assumed that thorough memorization of previous tasks yields overall performance benefits (Usmanova et al., 2021; Qi et al., 2023). Elaborate strategies were employed to memorize previous information (Yoon et al., 2021a; Liu et al., 2023). In practice, feature bias typically exists in the dataset, especially when there are lots of clients within the federated network. Because of such statistical heterogeneity, biased or even harmful information from particular clients may reside in the memory bank (i.e., memory buffer, generative models or model parameters) as shown in Figure 1. The federated model may inadvertently learn to identify and rely upon spurious correlations arising from diverse tasks among multiple clients. Furthermore, the model may integrate label noise (Zhang et al., 2024) introduced by a few clients. For example, in a federated learning system implemented among hospitals nationwide, these medical institutions may encounter varying disease profiles over time. Besides, hospitals located at distinct geographical areas often cater to diverse distributions as depicted in Figure 1. Therefore, strategically mitigating erroneous knowledge during the acquisition of new tasks is required.\nMotivated by the phenomenon in reality that the new arriving tasks of each client may not be correlated, we consider a more practical and challenging FCL setting in this paper: limitless task pool (LTP). From a temporal perspective, the tasks that a single client randomly selects from the LTP at various time points might be unrelated or even antagonistic, thereby presenting a significant challenge for model learning. To overcome the problem, we propose a novel generation-based method Accurate Forgetting Federated Continual Learning (AF-FCL). We argue that the forgetting phenomena are not invariably detrimental (Han et al., 2020). Conversely, accurate forgetting mitigates the negative impact of the heterogeneity on model learning.\nInstead of learning a generative adversarial network (GAN) for indiscriminate generative-replay in existing FCL methods (Qi et al., 2023), AF-FCL aims to facilitate a selective utilization of previous knowledge through correlation estimation. In order to accurately identify benign knowledge from previous tasks, we achieve correlation estimation with a learned normalizing flow (NF) model (Durkan et al., 2019; Winkler et al., 2019; Rezende & Mohamed, 2015) in feature space. Specifically, an NF model could map an arbitrarily complex data distribution to a pre-defined distribution through a sequence of bijective transformations. Such invertability enables the NF to have a lossless memory of the input knowledge and accurately estimate the probability density of observed data. While the information in the NF model could contain biased features or spurious correlation due to heterogeneous data, we suggest outlier features with respect to the current tasks are suspicious and may pose a threat\nto the learning process. More precisely, the credibility of a particular feature could be quantified with its probability density in the current tasks.\nExperimental results corroborate that AF-FCL significantly outperforms all baselines on a series of benchmark datasets. We summarize our key contributions as follows:\n\u2022 We consider a more practical and challenging FCL setting. We suggest the harm of remembering biased or irrelevant features, which could be unavoidable in the federated scenario due to statistical heterogeneity.\n\u2022 We propose the concept accurate forgetting and develop a novel generative method, AF-FCL. It adaptively mitigates erroneous information by correlation estimation with an NF model.\n\u2022 We conduct extensive experiments on a series of benchmark datasets. The results with ablation studies demonstrate the effectiveness and superiority of our proposed accurate forgetting over existing state-of-the-art methods."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Continual Learning. Continual learning has witnessed the development of diverse methodologies (Lange et al., 2022), which can be roughly divided into three families: (I) Regularization-based methods: LwF employs the knowledge distillation loss, where the previous model\u2019s output is utilized as soft labels for the current tasks when working with new data (Li & Hoiem, 2016). Stable SGD (Mirzadeh et al., 2020) demonstrated performance enhancements by calibrating pivotal hyperparameters and systematically reducing the learning rate upon the arrival of each task. (II) Parameter isolation methods: Rusu et al. (2016) suggested augmentation of the model with new branches tailored to incoming tasks. (III) Replay-based methods: Generative replay-based methods use an auxiliary generator to model the data distribution of acquired knowledge, producing synthetic data for replay in instances (Odena et al., 2017; Wu et al., 2018). While existing research predominantly focused on the efficient memorization of past knowledge, we turn our attention to a more foundational question: is prior knowledge perpetually beneficial?\nFederated Learning. Federated learning represents a distributed learning paradigm among multiple clients and a central server. Researchers have been endeavoring to address the statistical heterogeneity by developing a comprehensive global model (Wang et al., 2020). Mohri et al. (2019) aimed to achieve a fair distribution of model performance by optimizing its efficacy across any given target distribution. Zhu et al. (2021b) suggested the utilization of a generator to aggregate user information. This, in turn, guides the local training by employing the acquired knowledge as an inductive bias. In this work, we consider a more challenging learning problem associated with statistical heterogeneity in federated scenarios: how to facilitate collaboration when all clients are tackling different tasks?\nFederated Continual Learning. To date, there are a few studies in the domain of federated continual learning. Casado et al. (2020) studied the scenario of data distributions changing over time in federated learning. Federated reconnaissance presented a scenario with incrementally new classes during training and proposed to utilize prototype networks (Hendryx et al., 2021). Guo et al. (2021) proposed a regularization-based algorithm and a new theoretical framework for it. Usmanova et al. (2021) presented a distillation-based method to deal with catastrophic forgetting, using previous model and global model as teachers for the training of local models. Yoon et al. (2021b) proposed a novel parameter isolation method for the federated diagram, where the network weights are decomposed into global parameters and task-specific parameters. Dong et al. (2022) considered a federated classincremental setting and developed a distillation-based method to alleviate catastrophic forgetting from both local and global perspectives. Qi et al. (2023) customized the generative replay based method ACGAN with model consolidation and consistency enforcement. Our method considers the issue of memorizing biased feature due to statistical heterogeneity, exhibiting notable differences compared to the aforementioned methods."
        },
        {
            "heading": "3 PROBLEM DEFINITION",
            "text": ""
        },
        {
            "heading": "3.1 NOTATIONS",
            "text": "Continual Learning. In standard continual learning scenario, there are a sequence of tasks T = {T 1, T 2, . . . , T T }, where T is the number of tasks, and T t is the t-th task. Each dataset is composed\nof nt pairs of data and labels: Dt = {xtk, ytk}n t\nk=1. When learning on the t-th task, one has no direct access to previous data Dt \u2032 , t\u2032 < t. The goal of continual learning is to effectively manage the current task while preserving its performance on all previous tasks:\nmin \u03b8t\n[L(\u03b8t; T 1),L(\u03b8t; T 2), . . . ,L(\u03b8t; T t)], (1)\nwhere L is the risk objective of tasks and \u03b8t is the model parameters learned on the t-th task. Federated Learning and Statistical Heterogeneity. In federated learning scenario, there are N clients, and each client owns a private dataset. The goal of federated learning is collaboratively learning models without accessing the datasets belonging to the local clients. The data of clients consists of the input space Xi and output space Yi, where Xi and Yi are shared across all clients. There are ni samples in the i-th client denoted as { xik, y i k }ni k=1\n. Different clients may exhibit non-identical joint distributions p(x, y) of features and labels, i.e., p(xi1 , yi1) \u0338= p(xi2 , yi2), where i1 \u0338= i2."
        },
        {
            "heading": "3.2 FEDERATED CONTINUAL LEARNING",
            "text": "FCL refers to a practical learning scenario that melds the principles of federated learning and continual learning. Suppose there are N clients, and each client possesses a private series of datasets {Dtk} T\nt=1. Please note that, at a given step t, client k can only have access to Dtk as in continual learning. In existing literature, the primary focus is on a specific task reshuffling setting, wherein the task set is identical for all users, yet the arrival sequence of tasks differs (Yoon et al., 2021a). In practical scenarios, it may be observed that the task set of clients is not necessarily correlated. Thus we consider a practical setting, the limitless task pool (LTP), denoted as T . For each client, the dataset Dtk of the k-th client at step t corresponds to a particular learning task T tk \u2282 T . There is no guaranteed relation among the tasks {T 1k , T 2k , . . . , T Tk } in the k-th client at different steps. Similarly, at step t, there could be no relation among the tasks {T t1 , T t2 , . . . , T tN} across different clients. Limitless Task Pool. In the setting of LTP, tasks are selected randomly from a substantial repository of tasks, creating a situation where two clients may not share any common tasks, i.e.,\u2223\u2223\u2223{T ip }tpi=1 \u2229 {T iq }tqi=1\u2223\u2223\u2223 \u2265 0, p, q = 1, 2 . . . , N . More importantly, clients possess diverse joint distributions of data and labels p(x, y) due to statistical heterogeneity. Therefore, features learned from other clients could invariably introduce bias when applied to the current task.\nBiased Features. The bias originating from a particular client can adversely affect the performance of the model across different clients and a range of tasks. We tackle a more practical and challenging FCL problem that differs from the task reshuffling setting (Yoon et al., 2021a) from two perspectives: (I) For different steps, tasks allocated to each client are randomly drawn from an extensive task pool. (II) For different clients, tasks across various clients may be unrelated or even contradictory in each step, consequently amplifying bias during the learning process.\nOur goal is to facilitate the collaborative construction of the global model with parameters \u03b8. Under the privacy constraint inherent in federated learning and continual learning, we aim to harmoniously learn current tasks while preserving performance on previous tasks for all clients, thereby seeking to optimize performance across all tasks seen so far by all clients, i.e.,\nmin \u03b8t\n[SL1 ,SL2 , . . .SLN ],where SLi = [L(\u03b8t; T 1i ),L(\u03b8t; T 2i ), . . . ,L(\u03b8t; T ti )]. (2)"
        },
        {
            "heading": "4 VALIDATION OF ACCURATE FORGETTING",
            "text": "In this section, we present the results on a noisy dataset to intuitively demonstrate the effectiveness of our motivation and approach."
        },
        {
            "heading": "4.1 DATASET WITH LABEL NOISE",
            "text": "We argue that forgetting is not invariably detrimental within the realm of FCL and propose the concept of accurate forgetting. To validate our argument and the efficacy of our proposed method, we curate the EMNIST-noisy dataset, wherein a subset of noisy clients is simulated by introducing random labels to the data. Additionally, we acknowledge the presence of noise in practical datasets, notably in the form of label noise.\nAs a character image dataset, the EMNIST-noisy dataset comprises 8 clients, each encompassing 6 tasks, with each task containing 2 classes of character images. We randomly select several clients and\nassign random labels for their initial three tasks, as displayed in Figure 2(a). These incorrect labels have the potential to propagate adverse effects, affecting subsequent task learning across different clients through the memory bank. After learning sequentially on all tasks, we evaluate the final three tasks, which do not contain any noisy labels. This evaluation allows us to exclusively assess the impact of incorporating noisy information from previous tasks into the memory bank."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "The baselines in Figure 2(b) are representative CL and FCL methods. It is observed that: (I) the performance of the baselines demonstrates inferiority compared to the naive FedAvg method; (II) the performance of the baselines suffers a rapid deterioration with an increasing number of noisy clients.\nThese CL and FCL baseline methods are meticulously designed to effectively retain knowledge from previous tasks. However, the presence of noisy clients introduces harmful information into the model learning process. The memorization of such erroneous information proves detrimental to the overall performance. Consequently, the baselines exhibit suboptimal performance compared to FL method, which does not employ explicit memorization techniques. In contrast, our approach incorporates adaptive mechanisms to mitigate the impact of erroneous information. By effectively alleviating the adverse influence of noisy clients, our method consistently surpasses all baselines. Notably, the performance of our method maintains relative stability even with an increasing number of noisy clients in the dataset."
        },
        {
            "heading": "5 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "5.1 PRELIMINARY: NORMALIZING FLOW",
            "text": "Normalizing flow is a type of generative model. It is able to map a complex, multi-modal distribution to a simple probability distribution such as standard Gaussian distribution through a sequence of smooth and invertible transformations (Rezende & Mohamed, 2015). In particular, an NF model is a diffeomorphism g composed of a series of invertible transformations g = g1 \u25e6 g2 . . . \u25e6 gk, of which a widely applied transformation is affine coupling layer (Kingma & Dhariwal, 2018).\nLossless Memory. Through meticulous design of the invertible layers, normalizing flow accomplishes a bijective transformation, preserving the one-to-one correspondence between the elements of the input and output spaces. The bijectivity ensures a lossless memory of the original input. Consequently, this inherent property of NF is pivotal in enabling the accurate modeling of complex distributions, and stands central in generative applications.\nExact Likelihood Estimation. The invertibility enables precise estimation of the probability density of data samples within the learned dataset. Specifically, with a target dataset Z = {zi}ni=1, zi \u2208 Rd and a prior distribution pu(u), u \u2208 Rd, an NF model learns the diffeomorphism g with the parameters \u03d5 that maps dataset distribution pz to the prior: u = g(z). Under above transformation, the probability density of the given datapoint z can be computed as:\nlog pz(z) = log pu(u) + log \u2223\u2223\u2223\u2223det \u2202u\u2202z \u2223\u2223\u2223\u2223 = log pu(g(z)) + k\u22121\u2211\nl=1\nlog \u2223\u2223\u2223\u2223det \u2202gl+1\u2202gl \u2223\u2223\u2223\u2223 , (3)\nPublished as a conference paper at ICLR 2024\nNormal distribution in latent space\nNF model \ud835\udc88 \ud835\udc89\ud835\udc83,\ud835\udc84\u210e\ud835\udc4e(\ud835\udc65) \ud835\udc54\u22121 \u0d24\ud835\udc62\n\u2112\ud835\udc50\ud835\udc59\ud835\udc60 Probability of each generated feature p\ud835\udc37\ud835\udc58 \ud835\udc61(\u0d24\ud835\udc62) \ud835\udc37\ud835\udc58 \ud835\udc61 \ud835\udc89\ud835\udc82 \u210e\ud835\udc4e \u2032 (\ud835\udc65) \u2112\ud835\udc3e\ud835\udc37\u2212 2 Old feature extractor \u210e\ud835\udc4e \u2032 \ud835\udc89\ud835\udc83,\ud835\udc84 \u2112\ud835\udc50\ud835\udc52 \ud835\udc65 p\ud835\udc37\ud835\udc58 \ud835\udc61 \u0d24\ud835\udc62\ud835\udc56 \u22c5 \u2112\ud835\udc50\ud835\udc52 \u0d24\ud835\udc62\ud835\udc56 \u0d24\ud835\udc62 Normal distribution in latent space NF model \ud835\udc88 \ud835\udc89\ud835\udc83,\ud835\udc84\u210e\ud835\udc4e(\ud835\udc65) \ud835\udc54\u22121 \u0d24\ud835\udc62 \u2112\ud835\udc50\ud835\udc59\ud835\udc60 Probability of each generated feature p\ud835\udc37\ud835\udc58 \ud835\udc61(\u0d24\ud835\udc62) \ud835\udc37\ud835\udc58 \ud835\udc61 \ud835\udc89\ud835\udc82 \u210e\ud835\udc4e \u2032 (\ud835\udc65) \u2112\ud835\udc3e\ud835\udc37\u2212 2 Old feature extractor \u210e\ud835\udc4e \u2032 \ud835\udc89\ud835\udc83,\ud835\udc84 \u2112\ud835\udc50\ud835\udc52 \ud835\udc65 \u2112\ud835\udc50\ud835\udc52 g \u0d24\ud835\udc62\nwhere gl denotes input of the l-th transformation of NF model. The transformations of the NF model are deliberately crafted to facilitate efficient computation of their Jacobian determinants \u2223\u2223\u2223det \u2202gl+1\u2202gl \u2223\u2223\u2223. A conditional NF model can take label y as conditional information in likelihood estimation pz(z, y).\nThe Training of NF Models. The training objective of NF model is also derived from Eq. 3 , trained to maximize the likelihood of samples from target dataset Z, i.e.,\nLNF (g;Z) = \u2212 1\nn n\u2211 i=1 log pz(zi) = \u2212 1 n n\u2211 i=1\n( log pu(g(zi)) +\nk\u22121\u2211 l=1 log \u2223\u2223\u2223\u2223\u2223det \u2202gl+1i\u2202gli \u2223\u2223\u2223\u2223\u2223 ) . (4)\n5.2 AN OVERVIEW OF AF-FCL\nIn FCL, statistical heterogeneity among clients brings extra challenges for continuously learning a sequence of tasks. Especially in LTP setting, particular clients could possess unrelated tasks and biased dataset. When bias or spurious correlation from particular clients is memorized by the model, a decline in model performance may occur in the task sequences of all clients. Therefore, a direct deployment of continual learning methods designed to mitigate catastrophic forgetting is hard to address the heterogeneity issues in FCL.\nWe propose a novel method AF-FCL , which adaptively utilizes memorized knowledge and learns unbiased feature for all clients under the FedAvg framework (McMahan et al., 2017). The training schematic of the classifier in each client is illustrated in Figure 3. Overall, the implementation of AF-FCL consists of the following components: (I) feature generative-replay. To prevent complete forgetting, we train a global NF model in the feature space of classifier for generative replay. (II) knowledge distillation. Additionally, we employ knowledge distillation in the feature space to mitigate significant drift, thereby enhancing the stability of the training process for the NF model. (III) correlation estimation. We suggest that features exhibiting outlier characteristics with respect to the current tasks can potentially undermine the learning process. Therefore, we assess the reliability of the generated feature by its probability density within the current tasks."
        },
        {
            "heading": "5.3 ACCURATE FORGETTING FOR HETEROGENEOUS FCL",
            "text": "The above Sec.5.2 gives an overview of our method. In this section, we provide a detailed description of AF-FCL and how it is implemented.\nGenerative-replay in Feature Space. We consider the classification tasks, where we need to train a classifier with L layers: h = {h1, h2, . . . , hL}. We split the classifier into three sub-modules: ha = {h1, h2, . . . , hl}, hb = {hl+1, . . . , hL\u22121}, hc = {hL}. The ha and hb are two successive feature extractors, hc is the classifier head. To maintain the performance on previous tasks, we train a conditional normalizing flow model g in the feature space, which is the output space of ha. In this way, the normalizing flow model retains the feature of previous tasks. The NF model g is trained globally with FedAvg algorithm using client datasets and sampled data:\nL\u0303NF (g;Dtk, Gz) = \u2212 1 |Dtk| \u2211\nxi,yi\u223cDtk\nlog pz(ha(xi), yi)\u2212 1 |Gz| \u2211\nzi,yi\u223cGz\nlog pz(zi, yi), (5)\nwhere Dtk is the dataset of the t-th task in the k-th client, and pz is the likelihood calculated as in Eq. 3. Gz is the feature set sampled from NF model g\u2032 (g\u2032 is the stored NF model after training on the last task), so that the current NF model avoids forgetting previous features.\nNormalizing flows operate within a latent space that maintains dimensional parity with the target data space. Training the NF model in high-dimensional data space X could be computationally intensive. Furthermore, the inherent sparsity of raw data can hinder the NF model\u2019s capacity to obtain a representative sample of the data distribution (Brehmer & Cranmer, 2020). Therefore, we train the NF model in the compact, low-dimensional feature space as opposed to the data space, thereby reducing the complexity of generation.\nWe also leverage the feature space to extract more robust semantic information.\nKnowledge Distillation for a Consistent Feature Distribution. The NF model is trained in the feature space of classifier to maintain previous knowledge. The NF model retains knowledge from previous tasks, conveying it to the classifier via feature generation. Yet, feature extractor of the classifier undergoes continual modifications throughout the training process. If the feature space of the classifier drifts significantly, the knowledge memorized by the NF model may become obsolete.\nTherefore, the feature space of the classifier needs to retain relative consistency during the training. We propose to apply knowledge distillation in the feature space of the classifier to control the drift of feature distribution:\nLKD(h;Dtk) = 1\nntk ntk\u2211 i=1 ||ha(xi)\u2212 h\u2032a(xi)||2, (6)\nwhere h\u2032a is the stored classifier feature extractor after training on the last task.\nCorrelation Estimation for Accurate Forgetting. From the above, we train the classifier with the aid of NF model by generating features. However, utilizing previous knowledge without discrimination may lead to biased model as stated before. Thus we propose to accurately exploit the memorized knowledge with the characteristics of the NF model for correlation estimation. In particular, when training the classifier for the t-th task of client k, we firstly map the feature of local data to the latent space of normalizing flow, i.e., U\u0302 tk = {ui = g(ha(xi))} ntk i=1, xi \u2208 Dtk. As the NF models transform the features to a disentangled latent space, which is the centered isotropic multivariate Gaussian. Therefore, we approximate the true distribution U tk in each class as a multivariate Gaussian with a diagonal covariance structure. The mean vector \u00b5tk and covariance matrix \u03a3 t k of U\u0302 t k can be easily computed by\n\u00b5tk = 1\nntk ntk\u2211 i=1 ui, \u03a3 t k = 1 ntk ntk\u2211 i=1 diag(ui \u2212 \u00b5tk) \u00b7 diag(ui \u2212 \u00b5tk), ui \u2208 U\u0302 tk, (7)\nwhere diag(u) turns the vector u into a diagonal matrix.\nFor generative replay, we sample a batch of latent vectors in the NF model and project them to feature space: U\u0304g = {u\u0304i, z\u0304i = g\u22121(u\u0304i), y\u0304i}ni=1, u\u0304i \u2208 pu. Please note that we use bar superscripts to denote generated data. The generated features from NF model represent the knowledge of previous tasks among all clients. However, in FCL scenario, there may exist irrelevant or even biased feature from other clients due to statistical heterogeneity. Enhancing the memorizing of biased feature could cause subpar performance or even failing to converge. Considering that outlier features with respect to the current tasks could be unreliable, we quantify the credibility of generated feature with its relevance to local dataset. To evaluate the correlation between the generated feature and the current task, we propose to use the the probability density of the sampled latent vector u\u0304i within the current feature distribution quantified in Eq.(7), i.e.,\npDtk(u\u0304i) = 1\u221a\n(2\u03c0)d|\u03a3tk| exp\n( \u22121 2 (u\u0304i \u2212 \u00b5tk)T (\u03a3tk)\u22121(u\u0304i \u2212 \u00b5tk) ) (8)\nThe probability pDtk(u\u0304i) above quantifies the degree of correlation between the current task in the local client and the sampled features from NF model. We use the correlation probability of the generated features to re-weight the loss objective Lgce. And the final objective Lcls consisting of three\nterms is as follows:\nLxce(h;Dtk) = 1\nntk ntk\u2211 i=1 Lce(h(xi), yi),\nLgce(h; U\u0304g) = 1\nn \u2211 u\u0304i\u2208U\u0304g pDtk(u\u0304i)Lce(hb,c(g \u22121(u\u0304i)), y\u0304i),\nLcls(h;Dtk, U\u0304g) =Lxce(h;Dtk) + Lgce(h; U\u0304g) + LKD(h;Dtk)\n(9)\nwhere Lxce(h;Dtk) denotes the cross-entropy loss of raw dataset, and Lgce(h; U\u0304g) denotes the unbiased objective of generated data. With the proposed method, the classifier learns beneficial features from previous tasks and accurately forgetting biased features. Moreover, the NF model memorizes more benign features. Both the NF model and the classifier are expected to be of increasing generalizability with the advancement of training progress. The implementation of AF-FCL is in Algorithm 1."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "6.1 EXPERIMENTAL SETTINGS AND EVALUATIONS",
            "text": "Datasets and Settings. We curate three FCL datasets with different settings. We use N to denote the number of clients, T to denote the number of tasks in each client, C to denote the number of classes in each task. For the EMNIST-based dataset containing 26 classes of handwritten letter images (Cohen et al., 2017), we set the following two settings with N=8, T=6, C=2. 1) EMNISTLTP: in LTP setting, we randomly sampled classes from the entire dataset for each client. 2) EMNIST-shuffle: in conventional shuffle setting, the task sets are consistent across all clients, while arranged in different orders. 3) CIFAR100: We randomly sample 20 classes among 100 classes of CIFAR100 (Krizhevsky et al., 2009) as a task for each of the 10 clients, and there are 4 tasks for each client (N = 10, T = 4, C = 20). 4) MNIST-SVHN-F: We set 10 clients with this mixed dataset. Each client contains 6 tasks, and each task has 3 classes.\nMetrics. We use the metrics of accuracy and average forgetting for evaluation following recent works (Mirzadeh et al., 2021; Yoon et al., 2021a). Average forgetting assesses the extend of backward transfer during continual learning, quantified as the disparity between the peak accuracy and the ending accuracy of each task."
        },
        {
            "heading": "6.2 BASELINES",
            "text": "We compare our method AF-FCL with baselines from FL, CL and FCL. In FL, we consider two representative models FedAvg (McMahan et al., 2017) and FedProx (Li et al., 2020). In CL, PODNet incorporates a spatial-based distillation loss onto the feature maps of the classifier (Douillard et al., 2020). ACGAN-Replay employs a GAN-based generative replay method (Wu et al., 2018). The CL models are respectively combined with the FL models. In FCL, FLwF2T leverages the concept of knowledge distillation within the framework of federated learning (Usmanova et al., 2021). FedCIL extends the ACGAN-Replay method within the federated scenario (Qi et al., 2023). GLFC exploits a distillation-based method to alleviate the issue of catastrophic forgetting from both local and global perspectives (Dong et al., 2022)."
        },
        {
            "heading": "6.3 EXPERIMENTS ON EMNIST-BASED DATASETS",
            "text": "EMNIST-LTP. In this dataset, clients may encompass unrelated tasks, thus rendering the dataset challenging. As the results shown in Table 1, some of the CL methods integrated with FL algorithms demonstrate comparable performance to that of FCL methods in the EMNIST-LTP dataset. For instance, the average accuracy of ACGAN-Replay+FedProx is 41.3%, higher than two FCL methods FLwF2T and GLFC. This phenomenon can be attributed to challenge posed by the elevated degree of heterogeneity under the LTP setting, which is difficult for these FCL methods to deal with, consequently diminishing their inherent advantages. Nevertheless, our method outperforms all the baselines in the EMNIST-LTP dataset. We argue that statistical heterogeneity in federated networks inevitably results in biased information residing in the memory bank. Both CL methods and existing FCL methods assume that memorization is beneficial, potentially losing their advantages under LTP setting. Our method adopts accurate forgetting to mitigate the negative impact of heterogeneity and selectively encourages the forgetting of malign information. It shows the highest accuracy rate and lowest forgetting rate.\nEMNIST-shuffle. Different from the EMNIST-LTP dataset, EMNIST-shuffle represents a more tractable dataset within the conventional setting, resulting in higher overall accuracy rates as in Table 1. The FCL methods exhibit superior accuracy compared to CL methods, underscoring their strength. And our method still showcases a superior capacity than all baselines in this commonly adopted dataset setting."
        },
        {
            "heading": "6.4 EXPERIMENTS ON MORE COMPLICATED DATASETS",
            "text": "CIFAR100 comprises 100 classes of images. The composite dataset MNIST-SVHN-F comprises two distinct digit classification datasets: MNIST and SVHN, characterized by complex colors and backgrounds, along with a clothing image classification dataset. Table 2 displays the results of these two challenging datasets CIFAR100 and MNIST-SVHN-F. Different tasks exhibit reliance on varying features. For instance, shape features pertinent to digits differ significantly from those relevant to clothing classification. A naive collaboration among clients may lead to a model overly reliant on spurious correlations, overlooking the importance of task-specific features. We suggest a strategy of selective utilization and memorization of learned feature. By relying on the generated features with a higher correlation, AF-FCL significantly exceeds the performance of baselines."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this study, we navigate the challenges of continual learning in real-world federated contexts, specifically when faced with data or task streams that might be biased or noisy across clients. Current research in continual learning emphasizes the adverse consequences of \"catastrophic forgetting\". However, we advocate for a perspective that reveals the merit of selective forgetting, especially as a mechanism to mitigate the biased information induced by statistical heterogeneity in reality. Inspired by it, we present a generative framework, termed as AF-FCL, meticulously crafted to achieve targeted forgetting by re-weighting generated features based on inferred correlations. The experimental results clearly demonstrate its effectiveness."
        },
        {
            "heading": "8 ACKNOWLEDGMENTS*",
            "text": "This work is funded by the Natural Science Fundation of China(NSFC. No. 62176132) and the Guoqiang Institute of Tsinghua University, with Grant No. 2020GQG0005. MS was supported by JST CREST Grant Number JPMJCR18A2 and a grant from Apple, Inc. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc. BH was supported by the NSFC General Program No. 62376235 and CCF-Baidu Open Fund."
        },
        {
            "heading": "A DATASETS",
            "text": "We construct a series of datasets comprising multiple federated clients, with each client possessing a sequence of tasks. Suppose we use N to denote the number of clients, T to denote the number of tasks in each client, C to denote the number of classes in each task. We curate tasks by randomly selecting several classes from the datasets and sample part of the instances from these classes. Adhering to the principle of class incremental learning, there are no overlapped classes between any two tasks within a client.\nEMNIST-LTP. The EMNIST dataset is a character classification dataset with 26 classes (Cohen et al., 2017). It contains 145600 instances of 26 English letters. The data contains both upper and lower case with the same label, making it more challenging for classification. To curate a dataset under LTP setting, we randomly sampled classes from the entire dataset for each client. The EMNIST-LTP dataset consists of 8 clients, with each client encompassing 6 tasks, each task comprising 2 classes (N = 8, T = 6, C = 2).\nEMNIST-shuffle. In conventional reshuffling setting, the task sets are consistent across all clients, while arranged in different orders. Therefore, with the same structure as EMNIST-LTP, we construct EMNIST-shuffle dataset with 8 clients, 6 tasks each, and each task comprising 2 classes. While the 6 tasks of all clients are the same but in shuffled orders.\nEMNIST-noisy. In this paper, we argue that forgetting is not invariably detrimental in FCL and propose the concept of accurate forgetting. To validate our argument and effectiveness of our method, we curate the EMNIST-noisy dataset with a few malicious clients by assigning random labels to the data. Besides, there could be noise in realistic dataset, including label noise. And malicious clients with adversarial behavior should also be taken into consideration under cross-device setting in Federate Learning. Robustness of FCL methods is crucial in real-world application. The EMNISTnoisy possesses the same structure as EMNIST-LTP dataset (N = 8, T = 6, C = 2). We randomly selects several clients and assign random labels to their first three tasks. After learning sequentially on all tasks, we evaluate on the last three tasks without noisy labels. By this means, we assess the impact of incorporating noisy information into the memory bank from previous tasks.\nCIFAR100. As a challenging image classification dataset, CIFAR100 consists of low resolution images containing various objects and complex image backgrounds (Krizhevsky et al., 2009). We randomly sample 20 classes among 100 classes of CIFAR100 as a task for each of the 10 clients, and there are 4 tasks for each client (N = 10, T = 4, C = 20). For each class, we randomly sample 400 instances into the client dataset.\nMNIST-SVHN-F. The mixed dataset is constructed with MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011) and FashionMNIST (Xiao et al., 2017). Similar to MNIST, SVHN dataset serves as a benchmark for digit classification tasks, notable for its representation of real-world scenarios with complex backgrounds. We unify the labels of these two datasets. FashionMNIST dataset is designed for clothing image classification. We set 10 clients in the mixed dataset, with each client containing 6 tasks, and each task has 3 classes. (N = 10, T = 6, C = 3). In this mixed dataset, different\ntasks rely on different features. For example, shape features that are relevant to digit classification differ significantly from those that are important for classifying clothing items. If clients collaborate naively, it may result in a model that relies too heavily on spurious correlations, thus neglecting the significance of task-specific features."
        },
        {
            "heading": "B BASELINES",
            "text": "We compare our method AF-FCL with two baselines from FL, two baselines from CL and three baselines from FCL. The FL methods simply train a global model on sequential tasks, without any memorizing technique. The CL methods are respectively combined with the FL methods, training a global model while fighting catastrophic forgetting. The FCL methods focus on addressing the issues of catastrophic forgetting along with statistical heterogeneity.\nFedAvg (McMahan et al., 2017). As a representative FL method, FedAvg trains the models in each client with local dataset and averages their parameters to attain a global model.\nFedProx (Li et al., 2020). The algorithm is similiar to FedAvg. While training local models, a regularization term is employed to govern the proximity between the local parameters and the global parameters. This regularization term serves to effectively control the degree of deviation exhibited by the local models from the global model during the training process.\nPODNet (Douillard et al., 2020). As a CL method, the algorithm incorporates a spatial-based distillation loss onto the feature maps of the classifier. This loss term serves to encourage the local models to align their respective feature maps with those of the previous model, thereby maintaining the performance in previous tasks.\nACGAN-Replay. This CL algorithm employs a GAN-based generative replay method (Wu et al., 2018). The algorithm trains an ACGAN in the data space to memorize the distribution of previous tasks. While learning on new tasks, the classifier is trained on new task data along with generated data from ACGAN.\nFLwF2T. As a FCL algorithm, FLwF2T leverages the concept of knowledge distillation within the framework of federated learning (Usmanova et al., 2021). It employs both the old classifier from previous task and global classifier from server to train the local classifier.\nFedCIL. The FCL algorithm extends the ACGAN-Replay method within the federated scenario, addressing the statistical heterogeneity issue with distillation loss (Qi et al., 2023).\nGLFC. In FCL scenario, the algorithm exploits a distillation-based method to alleviate the issue of catastrophic forgetting from both local and global perspectives (Dong et al., 2022).\nC IMPLEMENTATION DETAILS"
        },
        {
            "heading": "C.1 ALGORITHM",
            "text": "The algorithm of our method is detailed in Algorighm 1."
        },
        {
            "heading": "C.2 METRICS",
            "text": "We use the metrics of accuracy and average forgetting for evaluation following recent works (Mirzadeh et al., 2021; Yoon et al., 2021a). Suppose at,ik is the test set accuracy of the i\u2212th task after learning the t\u2212th task in client k. Average Accuracy. We evaluate the performance of the model on all tasks in all clients after it finish learning all tasks. By using a weighted average, we calculated the test set accuracy for all seen tasks across all clients, with the number of samples in each task serving as the weights:\nAverage Accuracy = 1\u2211N\nk=1 \u2211T i=1 n i k N\u2211 k=1 T\u2211 i=1 aT,ik \u2217 n i k. (10)\nThis approach allows us to account for variations in task difficulty and ensure a fair evaluation across different tasks and clients.\nAlgorithm 1 Federated continual learning framework AF-FCL Input: Datasets of T tasks for N clients {D1, D2, . . . , DN}, Dk = {T 1k , T 2k , . . . , T tk }, classifier h and normalizing flow model g; 1: for task t = 1, 2, . . . , T do 2: h\u2032 \u2190 h; g\u2032 \u2190 g 3: for round r = 1, 2, . . . do 4: Server randomly selects clients C for local training and send them model parameters 5: for client Ck \u2208 C do 6: Optimize g as in Eq. 5 with client dataset Dtk and g\u2032 7: Calculate distribution parameters of client data with g as in Eq. 7 8: Generate features u\u0304i with g and perform likelihood estimation with above parameters 9: Optimize h as in Eq. 9 with client dataset, generated features, exact likelihood pDtk(u\u0304i) and h\u2032 10: end for 11: the Server aggregates the parameters of hi\u03b8 and gi\u03d5 from clients C and weighted averages the parameters by client data number 12: end for 13: end for 14: Output: the learned classification model h.\nAverage Forgetting The metric of average forgetting assesses the extend of backward transfer during continual learning, quantified as the disparity between the peak accuracy and the ending accuracy of each task. We also use a weighted average when calculating average forgetting:\nAverage Forgetting = 1\u2211N\nk=1 \u2211T\u22121 i=1 n i k N\u2211 k=1 T\u22121\u2211 i=1 max t\u2208{1,...,T\u22121} (at,ik \u2212 a T,i k ) \u2217 n i k. (11)"
        },
        {
            "heading": "C.3 OPTIMIZATION",
            "text": "The Adam optimizer is employed for training all models. For all experiments except for CIFAR100, a learning rate of 1e-4 is utilized, with a global communication round of 60, and local iteration of 100. We set learning rate as 1e-3, global communication round as 40, and local iteration as 400 for CIFAR100. Consistent with prior research (Yoon et al., 2021a; Qi et al., 2023), all clients participate in each communication round. For training, a mini-batch size of 64 is adopted. The number of generated samples in an iteration aligns with this mini-batch size. We report the mean and standard deviation of each experiment, conducted three times with different random seed."
        },
        {
            "heading": "C.4 MODEL ARCHITECTURES",
            "text": "In the case of CIFAR100, we utilize the feature extractor of a ResNet-18 (He et al., 2016) as ha and hb comprises two FC layers , both with 512 units. While for other datasets we adopt a three-layer CNN followed by a FC layer with 512 units as ha. The channel numbers of the convolutional layers are [64, 128, 256]. And hb is represented by a FC layer. The outputs of ha belong to R512. All the FC layers employed in the architectures consist of 512 units. The convolutional layers and FC layers are followed by a Leaky ReLU layer. Another FC layer serves as hc and operates as the classification head.\nThe NF models consist of four layers of random permutation layer and affine coupling layer. The random permutation layers randomly permute the input vector so that various dependency among dimensions of input vectors could be effectively modeled. The inverse function of random permutation layers is to reversely permute the vector back to the original order. The affine coupling layers firstly partition the input vector into two halves xa and xb. Then an affine transformation is applied to one part of the input, conditioned on the other part:\nya = exp(s(xa))\u2299 xb + t(xa), (12) yb = xb, (13)\nwhere s and t denote functions that create scaling and translation parameters, which we implemented with 2 blocks of residual neural network and learned from the data. The output vector y is the concatenation of ya and yb. The invertibility of affine coupling transformation is readily apparent."
        },
        {
            "heading": "D ADDITIONAL EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 ABLATION STUDIES",
            "text": "Our method consists of three major components: (I) feature generative-replay (GR). For generative replay, we train a global NF model in the feature space of classifier. By augmenting the learning process of classifier with the generated features, we prevent complete forgetting of previous tasks. (II) knowledge distillation (KD). The NF model is trained in the feature space of classifier. To maintain the stability of the training process for the NF model, a knowledge distillation loss is employed in the feature space of classifier, mitigating significant drift. (III) correlation estimation for accurate forgetting (AF). We assess the reliability of the generated feature by its probability density within the current tasks. Leveraging the NF model, we approximate the local feature distribution to evaluate the probability of a given generated feature aligning with the current distribution.\nWe conduct ablation studies on the EMNIST-LTP and EMNIST-shuffle dataset as displayed in Table 3. Our method achieves optimal performance with all the three modules. Without the GR module, the AF module also loses efficacy. Therefore, left with the KD module, the performance of our model is comparable to that of PODNet and FLwF2T which relies on knowledge distillation to retain previous knowledge. Without the AF module, our method degrades into naive generative reply based method, thus the performance is close to FedCIL and ACGAN-Replay."
        },
        {
            "heading": "D.2 CIFAR100 IN A DIFFERENT SETTING",
            "text": "We conduct experiments on CIFAR100 with a more challenging setting. We randomly sample 10 classes among 100 classes of CIFAR100 as a task for each of the 8 clients, and there are 6 tasks for each client (N = 8, T = 6, C = 10). For each class, we randomly sample 400 instances into the client dataset. Therefore, each client possesses more tasks while less samples per task.\nAs shown in Table 4, our method attains the highest accuracy among the evaluated methods. Although the CL methods and conventional FCL methods emphasize the retention of knowledge acquired from previous tasks, indiscriminate memorization of potentially erroneous knowledge can detrimentally impact the performance on previous tasks. In contrast, our proposed method adopts a adaptive approach to forgetting biased features, resulting in a notable reduction of forgetting compared to established baselines, thus preserving a higher degree of task-specific knowledge retention."
        },
        {
            "heading": "D.3 RESULTS OF EMNIST-NOISY DATASET",
            "text": "We conduct experiments in the EMNIST-noisy dataset with an increasing number of noisy clients. We display the complete comparison of accuracy and forgetting among baselines here. It is observed\nthat the performance of the methods consistently diminishes with the escalating count of noisy clients, as depicted in Table 5. The presence of noisy clients introduce harmful information into the model learning process and memorization of such information proves detrimental to the overall performance. Thus, some of the CL and FCL methods, which aim to fight forgetting, exhibit inferior performance compared to FL methods. Our approach employs adaptive mechanisms to mitigate the impact of erroneous information. By alleviating the negative influence of noisy clients, our method consistently surpasses all baselines in both accuracy and resistance to forgetting."
        },
        {
            "heading": "D.4 RESULTS OF IMAGENET-SUBSET DATASET",
            "text": "We conducted experiments on a subset of the ImageNet dataset. Each client among 10 clients contains 4 tasks, where each task consists of 40 classes among 200 classes. As shown in the table below, our method surpasses existing baselines. This empirical evidence demonstrates the efficacy of our method, particularly in handling richer semantic information on large datasets such as ImageNet."
        },
        {
            "heading": "E COMPUTATION ANALYSIS AND DEVICES",
            "text": "As a generative-replay based model, AF-FCL has a similar number of parameters with other generative-replay based methods, including the baselines FedCIL, ACGAN-Replay, etc. Due to the special design of NF models, the generation and density estimation of them are fast and efficient. Therefore, AF-FCL does not bring many extra computational and communication costs. We provide the running-time comparisons with baselines in Table 7. As shown in the table, running-time of the proposed method is less than that of the generative-replay based models mentioned above.\nDevices In the experiments, we conduct all methods on a local Linux server that has two physical CPU chips (Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz) and 32 logical kernels. All methods are implemented using Pytorch framework and all models are trained on GeForce RTX 2080 Ti GPUs."
        },
        {
            "heading": "F RELATED NOTIONS",
            "text": ""
        },
        {
            "heading": "F.1 BIASED FEATURES",
            "text": "Researchers have employed various definitions for biased features, one of which involves defining them as spurious correlations. We denote X , Y as an input and output space of machine learning algorithm. An algorithm learns a mapping from the data x \u2208 X to the prediction y\u0302 \u2208 Y: y\u0302 = f(x). We assume there are attributes \u03b31, \u03b32, ... abstracted from the data x. For example, \u03b31 represents the shape of the object in the input image x, and \u03b32 denotes the number of black pixels in the input image x. The machine learning algorithm actually relies on many attributes to conduct infering: y\u0302 = f(\u03b3i1 , \u03b3i2 , ..., \u03b3iN ). We define an attribute \u03b3 as biased feature if it does not comply with the natural meaning of the target y Jeon et al. (2022). Relying on such biased attribute would result in poor generalizability of the algorithm. The biased features could be attained through biased training dataset and the learned mapping f relying on the biased features may not perform well in the testing dataset. For instance, if in the training image dataset all cows are standing on the grass, the machine learning model may rely on the attribute \u2019grass\u2019 for classifying images of cows.\nIn Sec. 4, we instantiate biased features with label noise (Zhang et al., 2021; Chen et al., 2020). With random labels, the model probably extracts misaligned attributes. In benchmark datasets, machine learning models may also learn biased features even without label noise (Zhu et al., 2021a)."
        },
        {
            "heading": "F.2 CONCEPT DRIFTS",
            "text": "Different from the studies about Federated Continual Learning, the evaluation in the concept drift studies is conducted at each time step. Therefore, there is no memorization requirement or catastrophic forgetting problem in the concept drift studies. A novel clustering algorithms for reacting to concept drifts is proposed (Jothimurugesan et al., 2023). Adaptive-FedAVG adapted the learning rate to react to concept drift (Canonaco et al., 2021). Panchal et al. proposed to detect concept drift through the magnitude of parameter updates and designed a novel adaptive optimizer Panchal et al. (2023)."
        },
        {
            "heading": "F.3 ORTHOGONAL TRAINING",
            "text": "The incorporation of orthogonal training and our accurate forgetting method is a promising direction. Bakman et al. proposed to modify the subspace of model layers in learning new tasks such that it is orthogonal to the global principal subspace of old tasks (Bakman et al., 2023). By distinguishing the subspace inside the model for each task, catastrophic forgetting of old tasks is mitigated, and it also relieves the influence of unrelated tasks. We will continue to explore the employment of orthogonal training in our method.\nOur method explicitly quantifies the correlations of generated features through probability calculations. Moreover, we facilitate selective forgetting by assigning lower weights to erroneous old knowledge, thus enabling the classifier to discard biased features and achieve improved overall performance."
        }
    ],
    "year": 2024
}