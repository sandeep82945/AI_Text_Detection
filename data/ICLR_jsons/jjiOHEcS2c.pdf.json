{
    "abstractText": "Merging multi-exposure images is a common approach for obtaining high dynamic range (HDR) images, with the primary challenge being the avoidance of ghosting artifacts in dynamic scenes. Recent methods have proposed using deep neural networks for deghosting. However, the methods typically rely on sufficient data with HDR ground-truths, which are difficult and costly to collect. In this work, to eliminate the need for labeled data, we propose SelfHDR, a self-supervised HDR reconstruction method that only requires dynamic multi-exposure images during training. Specifically, SelfHDR learns a reconstruction network under the supervision of two complementary components, which can be constructed from multi-exposure images and focus on HDR color as well as structure, respectively. The color component is estimated from aligned multi-exposure images, while the structure one is generated through a structure-focused network that is supervised by the color component and an input reference (e.g., medium-exposure) image. During testing, the learned reconstruction network is directly deployed to predict an HDR image. Experiments on real-world images demonstrate our SelfHDR achieves superior results against the state-of-the-art self-supervised methods, and comparable performance to supervised ones. Codes are available at https: //github.com/cszhilu1998/SelfHDR.",
    "authors": [
        {
            "affiliations": [],
            "name": "DYNAMIC SCENES"
        },
        {
            "affiliations": [],
            "name": "Zhilu Zhang"
        },
        {
            "affiliations": [],
            "name": "Haoyu Wang"
        },
        {
            "affiliations": [],
            "name": "Shuai Liu"
        },
        {
            "affiliations": [],
            "name": "Xiaotao Wang"
        },
        {
            "affiliations": [],
            "name": "Lei Lei"
        },
        {
            "affiliations": [],
            "name": "Wangmeng Zuo"
        }
    ],
    "id": "SP:36cb4da2c058ec2eedc70441a8c66fef52e22f88",
    "references": [
        {
            "authors": [
                "Guanying Chen",
                "Chaofeng Chen",
                "Shi Guo",
                "Zhetong Liang",
                "Kwan-Yee K Wong",
                "Lei Zhang"
            ],
            "title": "Hdr video reconstruction: A coarse-to-fine network and a real-world benchmark dataset",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Paul E Debevec",
                "Jitendra Malik"
            ],
            "title": "Recovering high dynamic range radiance maps from photographs",
            "venue": "In ACM SIGGRAPH,",
            "year": 2008
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Ben Fei",
                "Zhaoyang Lyu",
                "Liang Pan",
                "Junzhe Zhang",
                "Weidong Yang",
                "Tianyue Luo",
                "Bo Zhang",
                "Bo Dai"
            ],
            "title": "Generative diffusion prior for unified image restoration and enhancement",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Hu",
                "Orazio Gallo",
                "Kari Pulli",
                "Xiaobai Sun"
            ],
            "title": "Hdr deghosting: How to deal with saturation",
            "venue": "In CVPR,",
            "year": 2013
        },
        {
            "authors": [
                "Xin Huang",
                "Qi Zhang",
                "Ying Feng",
                "Hongdong Li",
                "Xuan Wang",
                "Qing Wang"
            ],
            "title": "Hdr-nerf: High dynamic range neural radiance fields",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoyang Huang",
                "Xiaoyu Shi",
                "Chao Zhang",
                "Qiang Wang",
                "Ka Chun Cheung",
                "Hongwei Qin",
                "Jifeng Dai",
                "Hongsheng Li"
            ],
            "title": "Flowformer: A transformer architecture for optical flow",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Kim Jun-Seong",
                "Kim Yu-Ji",
                "Moon Ye-Bin",
                "Tae-Hyun Oh"
            ],
            "title": "Hdr-plenoxels: Self-calibrating high dynamic range radiance fields",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Nima Khademi Kalantari",
                "Ravi Ramamoorthi"
            ],
            "title": "Deep high dynamic range imaging of dynamic scenes",
            "venue": "ACM TOG,",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Chul Lee",
                "Yuelong Li",
                "Vishal Monga"
            ],
            "title": "Ghost-free high dynamic range imaging via rank minimization",
            "venue": "IEEE signal processing letters,",
            "year": 2014
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jiezhang Cao",
                "Guolei Sun",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ce Liu"
            ],
            "title": "Beyond pixels: exploring new representations and applications for motion analysis",
            "venue": "PhD thesis, Massachusetts Institute of Technology,",
            "year": 2009
        },
        {
            "authors": [
                "Shuaizheng Liu",
                "Xindong Zhang",
                "Lingchen Sun",
                "Zhetong Liang",
                "Hui Zeng",
                "Lei Zhang"
            ],
            "title": "Joint hdr denoising and fusion: A real-world mobile hdr image dataset",
            "year": 2023
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Liu",
                "Yinglong Wang",
                "Bing Zeng",
                "Shuaicheng Liu"
            ],
            "title": "Ghost-free high dynamic range imaging with context-aware transformer",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Kede Ma",
                "Hui Li",
                "Hongwei Yong",
                "Zhou Wang",
                "Deyu Meng",
                "Lei Zhang"
            ],
            "title": "Robust multi-exposure image fusion: a structural patch decomposition approach",
            "venue": "IEEE TIP,",
            "year": 2017
        },
        {
            "authors": [
                "Rafa\u0142 Mantiuk",
                "Kil Joong Kim",
                "Allan G Rempel",
                "Wolfgang Heidrich"
            ],
            "title": "Hdr-vdp-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions",
            "venue": "ACM TOG,",
            "year": 2011
        },
        {
            "authors": [
                "Tom Mertens",
                "Jan Kautz",
                "Frank Van Reeth"
            ],
            "title": "Exposure fusion: A simple and practical alternative to high dynamic range photography",
            "venue": "In Computer Graphics Forum,",
            "year": 2009
        },
        {
            "authors": [
                "B Mildenhall",
                "PP Srinivasan",
                "M Tancik",
                "JT Barron",
                "R Ramamoorthi",
                "R Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Peter Hedman",
                "Ricardo Martin-Brualla",
                "Pratul P Srinivasan",
                "Jonathan T Barron"
            ],
            "title": "Nerf in the dark: High dynamic range view synthesis from noisy raw images",
            "year": 2022
        },
        {
            "authors": [
                "Michal Nazarczuk",
                "Sibi Catley-Chandar",
                "Ales Leonardis",
                "Eduardo P\u00e9rez Pellitero"
            ],
            "title": "Self-supervised hdr imaging from motion and exposure",
            "venue": "cues. arXiv preprint arXiv:2203.12311,",
            "year": 2022
        },
        {
            "authors": [
                "Yuzhen Niu",
                "Jianbin Wu",
                "Wenxi Liu",
                "Wenzhong Guo",
                "Rynson WH Lau"
            ],
            "title": "Hdr-gan: Hdr image reconstruction from multi-exposed ldr images with large motions",
            "venue": "IEEE TIP,",
            "year": 2021
        },
        {
            "authors": [
                "Tae-Hyun Oh",
                "Joon-Young Lee",
                "Yu-Wing Tai",
                "In So Kweon"
            ],
            "title": "Robust high dynamic range imaging by rank minimization",
            "venue": "IEEE TPAMI,",
            "year": 2014
        },
        {
            "authors": [
                "K Ram Prabhakar",
                "Rajat Arora",
                "Adhitya Swaminathan",
                "Kunal Pratap Singh",
                "R Venkatesh Babu"
            ],
            "title": "A fast, scalable, and reliable deghosting method for extreme exposure",
            "year": 2019
        },
        {
            "authors": [
                "K Ram Prabhakar",
                "Gowtham Senthil",
                "Susmit Agrawal",
                "R Venkatesh Babu",
                "Rama Krishna Sai S Gorthi"
            ],
            "title": "Labeled from unlabeled: Exploiting unlabeled data for few-shot deep hdr deghosting",
            "year": 2021
        },
        {
            "authors": [
                "Pradeep Sen",
                "Nima Khademi Kalantari",
                "Maziar Yaesoubi",
                "Soheil Darabi",
                "Dan B Goldman",
                "Eli Shechtman"
            ],
            "title": "Robust patch-based hdr reconstruction of dynamic scenes",
            "venue": "ACM TOG,",
            "year": 2012
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Jou Won Song",
                "Ye-In Park",
                "Kyeongbo Kong",
                "Jaeho Kwak",
                "Suk-Ju Kang"
            ],
            "title": "Selective transhdr: Transformer-based selective hdr imaging using ghost region mask",
            "year": 2022
        },
        {
            "authors": [
                "Deqing Sun",
                "Xiaodong Yang",
                "Ming-Yu Liu",
                "Jan Kautz"
            ],
            "title": "Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume",
            "year": 2018
        },
        {
            "authors": [
                "Steven Tel",
                "Zongwei Wu",
                "Yulun Zhang",
                "Barth\u00e9l\u00e9my Heyrman",
                "C\u00e9dric Demonceaux",
                "Radu Timofte",
                "Dominique Ginhac"
            ],
            "title": "Alignment-free hdr deghosting with semantics consistent transformer",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "A TOMASZEWSKA"
            ],
            "title": "Image registration for multi-exposure high dynamic range image acquisition",
            "venue": "In Proceedings of International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG),",
            "year": 2007
        },
        {
            "authors": [
                "Okan Tarhan Tursun",
                "Ahmet O\u011fuz Aky\u00fcz",
                "Aykut Erdem",
                "Erkut Erdem"
            ],
            "title": "An objective deghosting quality metric for hdr images",
            "venue": "In Computer Graphics Forum. Wiley Online Library,",
            "year": 2016
        },
        {
            "authors": [
                "Lin Wang",
                "Kuk-Jin Yoon"
            ],
            "title": "Deep learning for hdr imaging: State-of-the-art and future trends",
            "venue": "IEEE TPAMI,",
            "year": 2021
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE TIP,",
            "year": 2004
        },
        {
            "authors": [
                "Shangzhe Wu",
                "Jiarui Xu",
                "Yu-Wing Tai",
                "Chi-Keung Tang"
            ],
            "title": "Deep high dynamic range imaging with large foreground motions",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Qingsen Yan",
                "Jinqiu Sun",
                "Haisen Li",
                "Yu Zhu",
                "Yanning Zhang"
            ],
            "title": "High dynamic range imaging by sparse representation",
            "year": 2017
        },
        {
            "authors": [
                "Qingsen Yan",
                "Dong Gong",
                "Qinfeng Shi",
                "Anton van den Hengel",
                "Chunhua Shen",
                "Ian Reid",
                "Yanning Zhang"
            ],
            "title": "Attention-guided network for ghost-free high dynamic range imaging",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Qingsen Yan",
                "Yu Zhu",
                "Yanning Zhang"
            ],
            "title": "Robust artifact-free high dynamic range imaging of dynamic scenes",
            "venue": "Multimedia Tools and Applications,",
            "year": 2019
        },
        {
            "authors": [
                "Qingsen Yan",
                "Weiye Chen",
                "Song Zhang",
                "Yu Zhu",
                "Jinqiu Sun",
                "Yanning Zhang"
            ],
            "title": "A unified hdr imaging method with pixel and patch level",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Qingsen Yan",
                "Song Zhang",
                "Weiye Chen",
                "Hao Tang",
                "Yu Zhu",
                "Jinqiu Sun",
                "Luc Van Gool",
                "Yanning Zhang"
            ],
            "title": "Smae: Few-shot learning for hdr deghosting with saturation-aware masked autoencoders",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Wei Zhang",
                "Wai-Kuen Cham"
            ],
            "title": "Gradient-directed multiexposure composition",
            "venue": "IEEE TIP,",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Merging multi-exposure images is a common approach for obtaining high dynamic range (HDR) images, with the primary challenge being the avoidance of ghosting artifacts in dynamic scenes. Recent methods have proposed using deep neural networks for deghosting. However, the methods typically rely on sufficient data with HDR ground-truths, which are difficult and costly to collect. In this work, to eliminate the need for labeled data, we propose SelfHDR, a self-supervised HDR reconstruction method that only requires dynamic multi-exposure images during training. Specifically, SelfHDR learns a reconstruction network under the supervision of two complementary components, which can be constructed from multi-exposure images and focus on HDR color as well as structure, respectively. The color component is estimated from aligned multi-exposure images, while the structure one is generated through a structure-focused network that is supervised by the color component and an input reference (e.g., medium-exposure) image. During testing, the learned reconstruction network is directly deployed to predict an HDR image. Experiments on real-world images demonstrate our SelfHDR achieves superior results against the state-of-the-art self-supervised methods, and comparable performance to supervised ones. Codes are available at https: //github.com/cszhilu1998/SelfHDR."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Scenes with wide brightness ranges are often visible to human observers, but capturing them completely with digital or smartphone cameras can be arduous due to the restricted dynamic range of sensors. For instance, during sunset, the sun and sky are substantially brighter than the surrounding landscape, leading the camera sensor to either over-expose the sky or under-expose the landscape. To obtain high dynamic range (HDR) photos in these conditions, exposure bracketing technology becomes a popular option. It captures multiple low dynamic range (LDR) images with varying exposures, which are then merged into an HDR result (Debevec & Malik, 2008; Mertens et al., 2009).\nUnfortunately, when the multi-exposure images are misaligned due to camera shake and object movement, ghosting artifacts may exist in the result. Traditional methods to remove the ghosting include rejecting misaligned areas (Zhang & Cham, 2011; Lee et al., 2014; Oh et al., 2014; Yan et al., 2017), aligning input images (TOMASZEWSKA, 2007; Hu et al., 2013; Yan et al., 2019b), and using patch-based composite (Sen et al., 2012; Hu et al., 2013; Ma et al., 2017). With the development of deep learning (He et al., 2016; Dosovitskiy et al., 2020; Liu et al., 2021), recent advances (Kalantari et al., 2017; Wu et al., 2018; Yan et al., 2019a; Liu et al., 2022; Yan et al., 2023a; Tel et al., 2023) proposed training deep neural networks (DNN) for deghosting in a data-driven supervised manner, performing more effectively than traditional ones.\nHowever, DNN-based HDR reconstruction methods usually require sufficient labeled data, each of which should include the input dynamic multi-exposure images and the corresponding HDR ground-truth (GT) image. In order to ensure position alignment between the input reference (e.g., medium-exposure) frame and GT, previous works (Kalantari et al., 2017; Chen et al., 2021; Liu et al., 2023) generally capture the dynamic inputs with the controllable object (generally a person)\n\u2217Corresponding author.\nmotion in a static background, and construct GT by merging static multiple-exposure images of the reference scene. Such a collection process is cumbersome and involves high time as well as labor costs, thus limiting the number and diversity of the datasets. To alleviate the need for labeled data, FSHDR (Prabhakar et al., 2021) explores a few-shot manner, and Nazarczuk et al. (Nazarczuk et al., 2022) introduce a fully self-supervised approach. The main idea is to construct pseudo-inputs and pseudo-targets for HDR reconstruction. Nevertheless, their performance is unsatisfactory, as the motion and illumination in synthetic LDR images exhibit gaps with real-world ones.\nIn this work, we aim to reconstruct HDR images directly with real-world multi-exposure images in a self-supervised manner, without synthesizing any pseudo-input data. This objective should be feasible, as most of the information required for HDR results can derive from input data. The property will be more intuitive when HDR color and structure are observed, respectively. Specifically, HDR color knowledge can be inferred from aligned multi-exposure images, and HDR structure can be extracted from some one of the inputs.\nWe further propose SelfHDR, a self-supervised method for HDR image reconstruction. Inspired by the above data characteristics, SelfHDR decomposes the latent HDR GT into available color and structure components, and then takes them to supervise the learning of the reconstruction network. On the one hand, the color component is estimated from multi-exposure images aligned by optical flow. On the other hand, the structure component is generated by feeding aligned inputs into a structure-focused network, which is learned under the supervision of the color component and an input reference (e.g., medium-exposure) image. Moreover, during the training phase of structurefocused and reconstruction networks, elaborate masks are embedded into loss functions to circumvent harmful information in supervision. During inference, only the reconstruction network is required to predict the HDR result from unseen multi-exposure images.\nWe evaluate the proposed self-supervised methods using four existing HDR reconstruction networks, respectively. The models are trained on Kalantari et al. dataset (Kalantari et al., 2017), and tested on multiple datasets (Kalantari et al., 2017; Sen et al., 2012; Tursun et al., 2016). The results show our SelfHDR obtains 1.58 dB PSNR gain compared to the state-of-the-art self-supervised method that uses the same reconstruction network, and achieves comparable performance to supervised ones, especially in terms of visual effects. Besides, we conduct extensive and comprehensive ablation studies, analyzing the effectiveness of different components and variants.\nTo sum up, the main contributions of this work include:\n\u2022 We propose a self-supervised HDR reconstruction method named SelfHDR, which learns an HDR reconstruction network by decomposing latent ground-truth into constructible color and structure component supervisions.\n\u2022 The color component is estimated from aligned multi-exposure images, while the structure one is generated using a structure-focused network supervised by the color component and an input reference image.\n\u2022 Experiments show that our SelfHDR outperforms the state-of-the-art self-supervised methods, and achieves comparable performance to supervised ones."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 SUPERVISED HDR IMAGING WITH MULTI-EXPOSURE IMAGES",
            "text": "The main challenge of HDR imaging with multi-exposure images is to avoid ghosting artifacts. DNN-based HDR deghosting methods have exhibited a more satisfactory ability than traditional ones. For the first time, Kalanrati et al. (Kalantari et al., 2017) collect a real-world dataset and propose a data-driven convolutional neural network (CNN) approach to merge LDR images aligned by optical flow. Wu et al. (Wu et al., 2018) utilize the multiple encoders and one decoder architecture to handle image misalignment, discarding the optical flow. Yan et al. (Yan et al., 2019a) present a spatial attention mechanism for deghosting. In addition, we recommend Wang et al.\u2019s survey (Wang & Yoon, 2021) for more CNN-based HDR reconstruction methods (Prabhakar et al., 2019; Niu et al., 2021).\nRecently, with the development of Transformer (Dosovitskiy et al., 2020; Liu et al., 2021), some works (Liu et al., 2022; Song et al., 2022; Yan et al., 2023a; Tel et al., 2023) bring in self- and\ncross- attention to alleviate the ghosting artifacts. For example, Liu et al. (Liu et al., 2022) propose HDR-Transformer, which embeds a local context extractor into SwinIR (Liang et al., 2021) basic block for jointly capturing global and local dependencies. Song et al. (Song et al., 2022) suggest selectively applying the transformer and CNN model to misaligned and aligned areas, respectively. However, both CNN- and Transformer-based methods require sufficient labeled data for training networks, while the data collection is time-consuming and laborious."
        },
        {
            "heading": "2.2 FEW-SHOT AND SELF-SUPERVISED HDR IMAGING WITH MULTI-EXPOSURE IMAGES",
            "text": "To alleviate the reliance on HDR ground-truths, few-shot and self-supervised HDR reconstruction methods have been explored. FSHDR (Prabhakar et al., 2021) combines unlabeled dynamic samples with few labeled samples to train a neural network, then leverages the model output of unlabeled samples as a pseudo-HDR to generate pseudo-LDR images. Ultimately the HDR reconstruction network is learned with synthetic pseudo-pairs. Nazarczuk et al. (Nazarczuk et al., 2022) select well-exposure LDR patches as pseudo-HDR to generate pseudo-LDR, while the static LDR patches are directly merged for HDR ground-truths. However, due to unrealistic motion and illumination in synthetic LDR images, such methods exhibit performance gaps compared to supervised ones. Recently, SAME (Yan et al., 2023b) generates saturated regions in a self-supervised manner first, and then performs deghosting via a semi-supervised framework. But it still has limited performance improvement. In this work, we take full advantage of the internal characteristics of multi-exposure images to present a self-supervised approach SelfHDR, which achieves comparable performance to supervised ones.\nFurthermore, some works incorporate emerging techniques to investigate self-supervised HDR reconstruction. For instance, GDP (Fei et al., 2023) exploits multi-exposure images to guide the denoising process of pre-trained diffusion generative models (Ho et al., 2020; Song et al., 2021), reconstructing HDR image. Mildenhall et al. (Mildenhall et al., 2022), Jun et al. (Jun-Seong et al., 2022), and Huang et al. (Huang et al., 2022a) employ NeRF (Mildenhall et al., 2020) to synthesize HDR images and the novel HDR views. However, these methods are less practical, since the specific models need to be re-optimized when facing new scenarios."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 MOTIVATION AND OVERVIEW",
            "text": "Revisit Supervised HDR Reconstruction. The combination of multi-exposure images enables HDR imaging in scenes with a wide range of brightness levels. In static scenes, the HDR image can be easily generated through a weighted sum of multi-exposure images (Debevec & Malik, 2008). However, when applying this approach in dynamic scenes, it will lead to ghosting artifacts. As a result, several recent works (Kalantari et al., 2017; Yan et al., 2019a; Liu et al., 2022; Tel et al., 2023) have suggested learning a deep neural network in a supervised manner for deghosting. Concretely, denote the LDR image taken with exposure time ti by Ii, where i = 1, 2, 3 and t1 < t2 < t3. They first map the LDR images to the linear domain, which can be written as,\nHi = I \u03b3 i /ti, (1)\nwhere \u03b3 denotes the gamma correction parameter and is generally set to 2.2. Then they concatenate Ii and Hi, feeding them to the reconstruction network R with parameters \u0398R, i.e.,\nY\u0302 = R(X1,X2,X3; \u0398R), (2)\nwhere Xi = {Ii,Hi}, Y\u0302 denotes the reconstructed HDR image. The optimized network parameters can be obtained by the following formula,\n\u0398\u2217R = argmin \u0398R L(T (Y\u0302 ), T (Y )), (3)\nwhere L represents the loss function, Y denotes the HDR GT image. T is the tone-mapping process, represented as,\nT (Y ) = log(1 + \u00b5Y ) log(1 + \u00b5) , where \u00b5 = 5, 000. (4)\nMotivation of SelfHDR. The acquisition of labeled data for HDR reconstruction is usually timeconsuming and laborious. To alleviate the requirement of HDR GT, some works (Prabhakar et al., 2021; Yan et al., 2023b; Nazarczuk et al., 2022) have explored few-shot and zero-shot HDR reconstruction by constructing pseudo-pairs. However, their performance is unsatisfactory due to the gaps between the simulated pairs and real-world ones, especially in a fully self-supervised manner.\nIn this work, we expect to get rid of the demand for synthetic data, achieving self-supervised HDR reconstruction directly with real-world dynamic multi-exposure images. The goal should be feasible, as the multi-exposure images have provided probably sufficient information for HDR reconstruction. The property will be more intuitive when the color and structure are observed, respectively. On the one hand, the color of HDR images can be estimated from aligned inputs. On the other hand, the structure information of the HDR images can be generally discovered in some one of multi-exposure images, i.e., most textures exist in the medium-exposure image, dark details are obvious in the high-exposure one, and bright scenes are clearly visible in the low-exposure one.\nWhat we need to do is to dig for the right information from the multi-exposure images for constructing the HDR image. However, it is actually difficult to explore a straightforward self-supervised method that generates HDR images directly. Considering the above properties of HDR color and structure, we treat the two components respectively for ease of self-supervised implementation. Note that it can be a focus or emphasis on color and structure relatively, not necessarily an absolute separation.\nSpecifically, when training a self-supervised HDR reconstruction network with given multi-exposure images as input, suitable supervision signals have to be prepared. Instead of looking for a complete HDR image, we construct the color and structure components of the supervision respectively (see Sec. 3.2). Then we learn the network under the guidance of both components (see Sec. 3.3)."
        },
        {
            "heading": "3.2 CONSTRUCTING COLOR AND STRUCTURE COMPONENTS",
            "text": "3.2.1 CONSTRUCTING COLOR COMPONENT\nOptical Flow Alignment\nMerge (Eqn. ( ))\nStructureFocused Network\nStructure-Expansion Loss (Eqn. ( ))\nStructure-Preserving Loss (Eqn. ( ))\nOptical Flow Alignment\nReconstruction Network\nColor Mapping Loss (Eqn. ( ))\nTraining Stage 1: Preparing for Supervisions\nStructure Mapping Loss (Eqn. ( ))\nReconstruction Network\nTesting\nFrozen\nTrainable\n\ud835\udc7f1 \ud835\udc7f2 \ud835\udc7f3\n\ud835\udc7f1 \ud835\udc7f2 \ud835\udc7f3\n\ud835\udc7f1 \ud835\udc7f2 \ud835\udc7f3\n\ud835\udc80\ud835\udc50\ud835\udc5c\ud835\udc59\ud835\udc5c\ud835\udc5f\n\ud835\udc80\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc62\n\ud835\udc80\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc62\n\ud835\udc80\nTraining Stage 2: Learning HDR with Constructed Supervisions\nStructureFocused Network\n\ud835\udc7f1 \ud835\udc7f2 \ud835\udc7f3\nThe color component should represent the HDR color as faithfully as possible, and it can be estimated by fusing the aligned multi-exposure images. Multiple frames in dynamic scenes are generally not aligned caused by camera shake or object motion. Although sometimes a few regions are aligned well, they are not enough to generate acceptable color components. In view\nof the effective capabilities of the optical flow estimation method (Liu et al., 2009), it is a natural idea to perform pre-alignment first. Concretely, taking the medium-exposure image I2 as the reference, we calculate the optical flow from I2 to I1 and I3, respectively. Thus, we can back warp H1 and H3 according to the calculated optical flow, obtaining H\u03031 and H\u03033 that are roughly aligned with H2. Then we can predict the color component Ycolor with the following formula,\nYcolor = A1H\u03031 +A2H2 +A3H\u03033\nA1 +A2 +A3 , (5)\nwhere Ai represents pixel-level fusion weight. We follow Kalantari et al. (Kalantari et al., 2017) and express Ai as, A1 = 1\u2212 \u039b1(I2), A2 = \u039b2(I2), A3 = 1\u2212 \u039b3(I2), (6) where \u039bi(I2) is shown in Fig. 1.\nWhen the images are perfectly aligned, the color components Ycolor can be regarded as an HDR image directly. However, such an ideal state is hard to reach due to object occlusion and sometimes non-robust optical flow model. Small errors during pre-alignment may cause blurring, while large ones cause ghosting in color components. Nevertheless, regardless of the ghosting areas, the rest can record the rough color value, and in which well-aligned ones can offer both good color and structure cues of HDR images. Moreover, for the areas with alignment errors, we further construct structure components to guide the reconstruction network in the next subsection.\n0 0.5 1\n1\n\ud835\udc702\n\u039b1(\ud835\udc702)\n0 0.5 1\n1\n\ud835\udc702\n\u039b2(\ud835\udc702)\n0 0.5 1\n1\n\ud835\udc702\n\u039b3(\ud835\udc702)"
        },
        {
            "heading": "3.2.2 CONSTRUCTING STRUCTURE COMPONENT",
            "text": "Although the medium-exposure image can provide most of the texture information, it is not optimal to take it as the only structure guidance for the HDR reconstruction, as the dark areas may be unclear and over-exposed ones may exist in it. Besides, it is not easy to put into practice when using the low-exposure and high-exposure images as guidance, due to the position and color differences between the HDR image and them. Fortunately, the previously constructed color component Ycolor can preserve the structure of dark and over-exposed areas to some extent. Therefore, we can combine the medium-exposure image and color component Ycolor to help construct the structure component.\nConcretely, we first learn a structure-focused network with the guidance of medium-exposure image and color component Ycolor. During training, the network takes the multi-exposure images as input, as shown in Fig. 2. On the one hand, the medium-exposure image guides the network to preserve well-exposed textures from the input reference image. It is accomplished by a structure-preserving loss Lsp, which can be written as,\nLsp(Y\u0302stru,H2) = \u2225(T (Y\u0302stru)\u2212 T (H2)) \u2217Msp\u22251, (7)\nwhere Y\u0302stru denotes the network output. Msp emphasizes the well-exposed areas, and mitigates the adverse effects of dark and over-exposed ones in the reference image H2. The function \u039b2(I2) (see Fig. 1) can do just that, so we set Msp to \u039b2(I2). On the other hand, the color component Ycolor guides the network to learn the structure from non-reference images by calculating structureexpansion loss Lse, which can be written as,\nLse(Y\u0302stru,Ycolor) = \u2225(T (Y\u0302stru)\u2212 T (Ycolor)) \u2217Mse\u22251. (8) Mse is a binary mask, distinguishing whether the pixel of Ycolor is composited from well-aligned multi-exposure ones. We design each pixel Mpse of Mse as,\nMpse = { 1 |((T (Ycolor)\u2212 T (H2)) \u2217 \u039b2(I2))p| < \u03c3se 0 |((T (Ycolor)\u2212 T (H2)) \u2217 \u039b2(I2))p| \u2265 \u03c3se , (9)\nwhere \u03c3se is a threshold and set to 5/255. In short, the parameter \u0398S of structure-focused network S is jointly optimized by structure-preserving and structure-expansion loss terms, i.e.,\n\u0398\u2217S = argmin \u0398S [Lse(Y\u0302stru,Ycolor) + \u03bbspLsp(Y\u0302stru,H2)], (10)\nwhere \u03bbsp denotes the weight coefficient of structure-preserving loss and is set to 4.\nThen, we feed aligned multi-exposure images rather than original ones into the pre-trained structurefocused network S. The final structure component Ystru can be expressed as,\nYstru = S(X\u03031,X2, X\u03033; \u0398\u2217S), (11)\nwhere X\u03031 and X\u03033 denote aligned X1 and X3 with the reference of X2. Such an operation can help structure-focused networks reduce the alignment burden, thus further enhancing the structure component. In addition, benefiting from the supervision of the color component, the structural component Ystru also has some color cues, although it mainly focuses on the HDR textures."
        },
        {
            "heading": "3.3 LEARNING HDR WITH COLOR AND STRUCTURE COMPONENTS",
            "text": "With the color and structure components as guidance, we can train an HDR reconstruction network R without other ground-truths. The optimized network parameters \u0398\u2217R can be modified from Eqn. (3) to the following formula,\n\u0398\u2217R = argmin \u0398R [Lcolor(Y\u0302 ,Ycolor) + \u03bbstruLstru(Y\u0302 ,Ystru)], (12)\nwhere Y\u0302 represents the network output. Lcolor and Lstru denote color mapping and structure mapping loss terms, respectively. \u03bbstru is the weight coefficient of Lstru and is set to 1. For color mapping term, we adopt \u21131 loss, which can be written as,\nLcolor(Y\u0302 ,Ycolor) = \u2225(T (Y\u0302 )\u2212 T (Ycolor)) \u2217Mcolor\u22251, (13)\nwhere Mcolor is similar as Mse, and is also a binary mask. It excludes areas where optical flow is estimated incorrectly when generating Ycolor. Instead of using Eqn. (9), here we can utilize Ystru to design a more accurate mask, which can be expressed as,\nMpcolor = { 1 |(T (Ycolor)\u2212 T (Ystru))p| < \u03c3color 0 |(T (Ycolor)\u2212 T (Ystru))p| \u2265 \u03c3color , (14)\nwhere p denotes a pixel, \u03c3color is a threshold and set to 10/255. For structure mapping term, we adopt VGG-based (Simonyan & Zisserman, 2015) perceptual loss, which can be written as,\nLstru(Y\u0302 ,Ystru) = \u2211 k \u2225\u03d5k(T (Y\u0302 ))\u2212 \u03d5k(T (Ystru))\u22251, (15)\nwhere \u03d5k(\u00b7) denotes the output of k-th layer in VGG (Simonyan & Zisserman, 2015) network."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 IMPLEMENTATION DETAILS",
            "text": "Framework Details. Note that this work does not focus on the design of network architectures, and we employ existing ones directly. The structure-focused and reconstruction networks use the same architecture. And we adopt CNN-based (i.e., AHDRNet (Yan et al., 2019a) and FSHDR (Prabhakar et al., 2021)) and Transformer-based (i.e., HDR-Transformer (Liu et al., 2022), and SCTNet (Tel et al., 2023)) networks for experiments, respectively. Besides, the optical flow is calculated by Liu et al. (Liu et al., 2009)\u2019s approach, as recommended in (Kalantari et al., 2017; Prabhakar et al., 2021).\nDatasets. Experiments are mainly conducted on Kalantari et al. dataset (Kalantari et al., 2017), which are extensively utilized in previous works. The dataset consists of 74 samples for training and 15 for testing. Each sample comprises three LDR images, captured at exposure values of {\u22122, 0, 2} or {\u22123, 0, 3}, alongside a corresponding HDR GT image. We use these testing images for both quantitative and qualitative evaluations. Additionally, following (Kalantari et al., 2017; Yan et al., 2019a; Liu et al., 2022), we take the Sen et al. (Sen et al., 2012) and Tursun et al. (Tursun et al., 2016) datasets (without GT) for further qualitative comparisons.\nTraining Details. The structure-focused and reconstruction networks are trained successively, and share the same settings. The training patches of size 128\u00d7128 are randomly cropped from the original images. The batch size is set to 16. Adam (Kingma & Ba, 2015) with \u03b21 = 0.9 and \u03b22 = 0.999 is\ntaken to optimize models for 150 epochs. The learning rate is initially set to 1\u00d7 10\u22124 for CNN-based networks and 2\u00d7 10\u22124 for Transformer-based ones, and reduces by half every 50 epochs. Evaluation Configurations. We use PSNR and SSIM (Wang et al., 2004) as evaluation metrics. PSNR and SSIM are both calculated on the linear and tone-mapped domains, denoted as \u2018-l\u2019 and \u2018-u\u2019, respectively. Moreover, we adopt HDR-VDP-2 (Mantiuk et al., 2011) that measures the human visual difference between results and targets. The higher HDR-VDP-2, the better results."
        },
        {
            "heading": "4.2 COMPARISON WITH STATE-OF-THE-ARTS",
            "text": "As described in Sec. 4.1, we adopt four existing HDR reconstruction networks (i.e., AHDRNet, FSHDR, HDR-Transformer, and SCTNet) for experiments, respectively. We compare them with the corresponding supervised manners and two self-supervised methods (i.e., FSHDRK=0 and Nazarczuk et al. (Nazarczuk et al., 2022)). And the results of few-shot FSHDR are also provided.\nQuantitative Results. Table 1 shows the quantitative comparison results. As loss functions are always calculated on tone-mapped images, and HDR images are typically viewed on LDR displays, we suggest taking evaluation metrics in the tone-mapped domain (i.e., PSNR-u and SSIM-u) as the primary reference. From the table, four SelfHDR versions all outperform the previous self-supervised methods. Especially, with the same reconstruction network, our SelfHDRFSHDR achieves 1.58 dB PSNR gain than FSHDRK=0. The results of SelfHDR can be further improved with the use of more advanced reconstruction networks (i.e., HDR-Transformer and SCTNet). Moreover, in comparison with the corresponding supervised methods, SelfHDR has comparable performance overall.\nQualitative Results. The visual comparisons on Kalantari et al. dataset (Kalantari et al., 2017) as well as Sen et al. (Sen et al., 2012) and Tursun et al. (Tursun et al., 2016) datasets are shown in Fig. 3 and Fig. 4, respectively. Our results have fewer artifacts than FSHDRK=0, and sometimes even outperform the corresponding supervised methods. They show the same trend as the quantitative ones. Please see Sec. E in the appendix for more results."
        },
        {
            "heading": "5 ABLATION STUDY",
            "text": "The ablation studies are all conducted using AHDRNet (Yan et al., 2019a) as the structure-focused and reconstruction network."
        },
        {
            "heading": "5.1 EFFECT OF COLOR AND STRUCTURE SUPERVISION",
            "text": "The quantitative results of color and structure components (Ycolor and Ystru) are given in Table 2. From the table, the final HDR results achieve better performance than both Ycolor and Ystru on PSNR-u, SSIM-u, and HDR-VDP-2. It indicates that the two components are complementary and\ntaking them as supervision is appropriate and effective. Furthermore, we conduct the following two experiments to further illustrate the effectiveness.\nComparision with Component Fusion. It may be a more natural idea to obtain HDR results by fusing the color and structure components. Here we implement that by calculating McolorYcolor + (1\u2212Mcolor)Ystru. We empirically re-adjust the hyperparameter \u03c3color in Eqn. (14), but find it gets the best results when Mcolor = 0. In other words, it is difficult to achieve better results by fusing two components simply. Instead, our SelfHDR provides a more flexible and efficient way.\nRefining Structure Component. Denote Y\u0302 \u2217 by the reconstruction network output when inputting multi-exposure images aligned by optical flow (Liu et al., 2009). From another point of view, Y\u0302 \u2217 can be regarded as a refined structure component with higher quality. Thus, we further take Ycolor and Y\u0302 \u2217 as new supervisions to re-train a reconstruction model, while the performance does not improve. It demonstrates that Ystru generated by structure-focused network is already sufficient.\nTable 3: Effect of loss terms (Lse and Lsp) when training structure-focused network.\nLse / Lsp Ystru PSNR-u / PSNR-l Y\u0302 PSNR-u / PSNR-l\n\u00d7 / \u2713 38.24 / 33.61 38.79 / 33.72 \u2713 / \u00d7 42.69 / 41.89 43.09 / 41.13 \u2713 / \u2713 43.38 / 41.74 43.68 / 41.09\nTable 4: Effect of different Mcolor when training reconstruction network.\nMcolor Y\u0302\nPSNR-u / PSNR-l\n\u00d7 43.55 / 41.12 Eqn. (9) 43.59 / 41.02 Eqn. (14) 43.68 / 41.09\nTable 6: Effect of pre-alignment processing when constructing Ycolor and Ystru.\nYcolor / Ystru Y\u0302\nPSNR-u / PSNR-l\n\u00d7 / \u00d7 35.50 / 34.95 \u00d7 / \u2713 41.66 / 40.90 \u2713 / \u00d7 43.41 / 40.76 \u2713 / \u2713 43.68 / 41.09"
        },
        {
            "heading": "5.2 EFFECT OF LOSS TERMS AND MASKS",
            "text": "Structure-Focused Network. The structure-focused network is trained with the supervision of color component and input reference, implementing by calculating structure-expansion loss Lse and structure-preserving loss Lsp, respectively. Here we explore the effect of different supervisions by using Lsp or Lse only. From Table 3, it can be seen that Lsp may play a weaker role, as it mainly constrains the well-exposed areas whose structure may be also fine in Ycolor. Nevertheless, combining two supervisions is more favorable than using one, thus both are indispensable.\nMoreover, we conduct ablation experiments on the designed masks (Msp and Mse) in loss terms. The results in Table 5 show that the masks are competent in avoiding harmful information from supervision. The visualizations of the masks are given in Sec. A of the appendix.\nReconstruction Network. For training the reconstruction network, the adverse effect of ghosting regions from color supervision Ycolor needs to be avoided as well. We utilize structure component to design a more accurate mask in Eqn. (14), and it does show better results than Eqn. (9) from Table 4.\nIn addition, we conduct the experiments with different hyperparameters \u03c3se (see Eqn. (9)) and \u03c3color (see Eqn. (14)) in Sec. D of the appendix."
        },
        {
            "heading": "5.3 EFFECT OF OPTICAL FLOW PRE-ALIGNMENT",
            "text": "When constructing color and structure supervisions, the inputs need to be pre-aligned by the optical flow approach. Here we remove the pre-alignment processing separately to investigate its impact on the final HDR results, which are shown in Table 6. From the table, pre-alignment during obtaining Ycolor is crucial, and pre-alignment during obtaining Ystru can further improve performance. The corresponding quantitative results of Ycolor and Ystru can be seen in Sec. B of the appendix."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "By exploiting the internal properties of multi-exposure images, we propose a self-supervised high dynamic range (HDR) reconstruction method named SelfHDR for dynamic scenes. In SelfHDR, the reconstruction network is learned under the supervision of two complementary components, which focus on the color and structure of HDR images, respectively. The color components are synthesized by merging aligned multi-exposure images. The structure components are constructed by feeding aligned inputs into the structure-focused network, which is trained with the supervision of color components and input reference images. Experiments show that SelfHDR outperforms the state-of-the-art self-supervised methods, and achieves comparable results to supervised ones. The discussion on method limitation and future work can be seen in Sec. F and Sec. G of the appendix."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is partially supported by the National Natural Science Foundation of China (NSFC) under Grant No. U19A2073."
        },
        {
            "heading": "A ANALYSIS AND VISUALIZATION OF MASKS",
            "text": "In order to avoid harmful information in supervision during training structure-focused network, we carefully design the mask Msp and Mse for calculating structure-preserving loss Lsp and structureexpansion loss Lse, respectively. The quantitative results of related ablation experiments are shown in Table 5. Here we give more analysis about the elaborate masks and visualize an example in Fig. A. Therein, the corresponding color component is shown in Fig. A (g).\nMask Msp in Structure-Preserving Loss. The structure-preserving loss aims at guiding the network to preserve textures of the input reference image, and it is calculated between model output and linear medium-exposure image H2. From Table 5, it leads to poor performance when measuring the distance between the two directly, as the structural information of dark and over-exposed regions is incomplete in medium-exposure image H2.\nThus, we suggest embedding a mask Msp into the loss, and it should emphasize the well-exposed areas and mitigate the adverse effects of dark as well as over-exposed areas. \u039b2(I2) in Fig. 1 can do just that, and we adopt it as Msp. The visualization of a \u039b2(I2) example is shown in Fig. A (i). It can be seen that the overexposed area surrounded by the blue box is successfully suppressed.\nMask Mse in Structure-Expansion Loss. The structure-expansion loss aims at guiding the network to learn textures from non-reference inputs, and it is calculated between model output and color component Ycolor. As Ycolor is obtained by fusing aligned multi-exposure images (see Fig. A (b), (e), and (f)), it is inevitable that ghosting artifacts exist in Ycolor (see the area surrounded by the red box in Fig. A (g)) when the alignment fails.\nThus, a mask Mse should be designed to circumvent the adverse effects of these ghosting areas for better guiding the network. It is not appropriate to calculate the mask based on the simple difference between Ycolor and reference image H2. Because even if the dark and over-exposed areas are aligned well, the difference between Ycolor and H2 is still large (see the area surrounded by the blue box in Fig. A (j)). As a result, we utilize \u039b2(I2) again to mitigate the adverse effects of these areas. Specifically, we multiply \u039b2(I2) to Ycolor and H2 for calculating the difference, as shown in Eqn. (9). A mask example is shown in Fig. A (k). It can be seen that the well-aligned over-exposed areas surrounded by the blue box are successfully retained, and only the misaligned area is masked.\nWith the designed masks, the generated structure component Ystru in Fig. A (l) combines the strengths of the supervisions Ycolor and H2, while discarding their weaknesses."
        },
        {
            "heading": "B EFFECT OF OPTICAL FLOW PRE-ALIGNMENT",
            "text": "When constructing color and structure supervisions, the inputs need to be pre-aligned by the optical flow approach (Liu et al., 2009). We remove the pre-alignment processing separately to investigate its impact on the final HDR results, which are shown in Table A. From the table, the pre-alignment during obtaining Ycolor is crucial, as Ycolor affects the quality of Ystru, while Ycolor and Ystru decide the final HDR result. On this basis, pre-alignment during generating Ystru can further improve performance, achieving 0.27 dB PSNR gain on the HDR result Y\u0302 .\na Low\u2212Exposure Image \ud835\udc701 b Medium\u2212Exposure Image \ud835\udc702 c High\u2212Exposure Image \ud835\udc703 d Ground-Truth\ne Aligned Low\u2212Exposure Image \ud835\udc701 f Aligned High\u2212Exposure Image \ud835\udc703 g Color Component \ud835\udc80\ud835\udc50\ud835\udc5c\ud835\udc59\ud835\udc5c\ud835\udc5f h Linear Medium\u2212Exposure Image \ud835\udc6f2\ni \u039b2 \ud835\udc702 in Fig. 1 j 1 \u2212\ud835\udc74\ud835\udc60\ud835\udc52 (w/o \u039b2 \ud835\udc702 ) k 1 \u2212\ud835\udc74\ud835\udc60\ud835\udc52 (w/ \u039b2 \ud835\udc702 ) l Structure Component \ud835\udc80\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc62\nFigure A: Visualization of masks and related images. (a)\u223c(c) show the multi-exposure images, while (d) is the corresponding ground-truth from Kalantari et al. (Kalantari et al., 2017) dataset. (e) and (f) show the aligned low-exposure and aligned high-exposure images, respectively, which are obtained by optical flow (Liu et al., 2009) alignment with the reference of medium-exposure image. (g) is the constructed color component by fusing aligned multi-exposure images. (h) is the medium-exposure image in the linear domain. (i) is the mask as a blending weight in Fig. 1. (j) and (k) denote the masks 1\u2212Mse (see Eqn. (9)) constructed without and with \u039b2(I2), respectively. (l) is the generated structure component. The red box indicates the area where optical flow alignment fails, and the blue box indicates the area with high brightness.\nTable A: Effect of pre-alignment processing when constructing supervision information (Ycolor and Ystru). The final HDR results (Y\u0302 ) are obtained by learning the model with corresponding (Ycolor and Ystru) supervisions.\nYcolor Ystru Ycolor PSNR-u / PSNR-l Ystru PSNR-u / PSNR-l Y\u0302 PSNR-u / PSNR-l\nPre-Alignment Processing\n\u00d7 \u00d7 25.69 / 31.31 34.58 / 34.35 35.50 / 34.95 \u00d7 \u2713 25.69 / 31.31 39.04 / 40.38 41.66 / 40.90 \u2713 \u00d7 34.45 / 39.01 43.07 / 40.45 43.41 / 40.76 \u2713 \u2713 34.45 / 39.01 43.38 / 41.74 43.68 / 41.09\nIn addition, we further evaluate the effect of pre-alignment on generating Ystru. Specifically, we test the structure-focused network on 74 training scenes with and without optical flow pre-alignment, respectively. The results of Ystru show the pre-alignment manner has 0.44dB PSNR-u and 1.46dB PSNR-l gains on average. Moreover, we compare the results between the two manners one by one. We find that only in 6 scenes, the results without pre-alignment are more than 0.1dB better than those with pre-alignment on PSNR-u. In the other 68 scenes, the pre-alignment manner always gives better or comparable results."
        },
        {
            "heading": "C EFFECT OF DIFFERENT ALIGNMENT METHODS",
            "text": "The quality of color components mainly relies on the alignment method. In this work, for the sake of fairness, we follow Kalantari et al. (Kalantari et al., 2017) and FSHDR (Prabhakar et al., 2021), adopting Liu et al. (Liu et al., 2009)\u2019s approach for optical flow alignment. Besides, we additionally conduct experiments with other commonly used optical flow networks, i.e., PWC-Net (Sun et al.,\nTable B: Effect of optical flow alignment methods.\nAlignment Method PSNR-u / SSIM-u PSNR-l / SSIM-l HDR-VDP-2\nPWC-Net (Sun et al., 2018) 43.45 / 0.9898 40.67 / 0.9864 64.07 FlowFormer (Huang et al., 2022b) 43.50 / 0.9900 40.60 / 0.9862 64.43\nLiu et al. (Liu et al., 2009) 43.68 / 0.9901 41.09 / 0.9873 64.57\nTable C: Effect of \u03c3se in Eqn. (9).\n\u03c3se Ystru PSNR-u / PSNR-l Y\u0302 PSNR-u / PSNR-l\n2.5/255 43.27 / 41.49 43.64 / 40.88 5/255 43.38 / 41.74 43.68 / 41.09 7.5/255 43.18 / 41.67 43.57 / 41.04\nTable D: Effect of \u03c3color in Eqn. (14).\n\u03c3color Y\u0302\nPSNR-u / PSNR-l\n5/255 43.60 / 41.08 10/255 43.68 / 41.09 15/255 43.61 / 41.10\n2018) and FlowFormer (Huang et al., 2022b). As shown in Tab. B, although Liu et al.\u2019s approach is relatively early, it is more robust for multi-exposure image alignment than recent learning-based PWC-Net and FlowFormer.\nD ABLATION STUDY ON ADJUSTING \u03c3se AND \u03c3color\nThe hyperparameters \u03c3se (see Eqn. (9)) and \u03c3color (see Eqn. (14)) are set to 5/255 and 10/255 by default for experiments, respectively. Here, we vary \u03c3se or \u03c3color to conduct experiments. Table C and D show the experimental results, respectively. The results show that the sensitivity \u03c3se and \u03c3color of our SelfHDR is acceptable."
        },
        {
            "heading": "E ADDITIONAL QUALITATIVE COMPARISONS",
            "text": "Additional visual comparisons on Kalantari et al. (Kalantari et al., 2017) dataset are shown in Fig. B and Fig. C, respectively. Our SelfHDR has fewer ghosting artifacts than zero-shot FSHDR (i.e., FSHDRK=0) (Prabhakar et al., 2021). Sometimes, SelfHDR even outperforms the corresponding supervised methods. Red arrows in the results indicate areas with ghosting artifacts in other methods."
        },
        {
            "heading": "F LIMITATION",
            "text": "First, the main limitation is the requirement for clear input images, i.e., they should be noise-free and blur-free. When noise exists in short-exposure images or blur exists in long-exposure images, SelfHDR can not remove noise and blur, as shown in Fig. D. Second, when the scene irradiance changes drastically in shooting multi-exposure images, SelfHDR may fail, as the constructed color components may be inaccurate.\nActually, most existing multi-exposure HDR reconstruction methods (including supervised and selfsupervised ones) only focus on removing ghosting artifacts caused by misalignment between inputs, having these limitations as well. Our SelfHDR has taken a step toward more realistic self-supervised HDR imaging by deghosting, while our ongoing work is to further address these limitations."
        },
        {
            "heading": "G FUTURE WORK",
            "text": "In future work, we will explore self-supervised HDR reconstruction when considering more realistic shooting conditions. In low-light environments, there may exist noise in short-exposure images and blur in long-exposure images. In order to achieve a self-supervised algorithm, we can combine HDR reconstruction with some self-supervised denoising and debluring works to process input images for removing undesirable degradations. Moreover, an adaptive method may need to be explored to select a more appropriate image as a base frame. For example, when a medium-exposure image\nFigure B: Visual comparison on Kalantari et al. dataset (Kalantari et al., 2017). Red arrows indicate areas with ghosting artifacts from other methods. \u2018HDR-Tra.\u2019 denotes HDR-Transformer.\nsuffers more severe degradations than others, the method should adaptively take short-exposure or long-exposure images as a new base frame for HDR reconstruction.\nIn addition, as a self-supervised method, it has the potential to produce better results and bring better generalization by exploiting more multi-exposure images without the target HDR images. We will explore scaling up training data in the future.\nFigure C: Visual comparison on Kalantari et al. dataset (Kalantari et al., 2017). Red arrows indicate areas with ghosting artifacts from other methods. \u2018HDR-Tra.\u2019 denotes HDR-Transformer.\nFigure D: Failure cases. Noise or blur may exist in the results."
        }
    ],
    "year": 2024
}