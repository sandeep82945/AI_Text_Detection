{
    "abstractText": "This paper introduces \u03c02vec, a method for representing black box policies as comparable feature vectors. Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for a policy. \u03c02vec represents the behaviors of policies by capturing statistics of how the behavior evolves the features from a pretrained model, using a successor feature framework. We focus on the offline setting where both policies and their representations are trained on a fixed dataset of trajectories. Finally, we employ linear regression on \u03c02vec vector representations to predict the performance of held out policies. The synergy of these techniques results in a method for efficient policy evaluation in resource constrained environments.",
    "authors": [
        {
            "affiliations": [],
            "name": "SUCCESSOR FEATURES"
        },
        {
            "affiliations": [],
            "name": "Gianluca Scarpellini"
        },
        {
            "affiliations": [],
            "name": "Ksenia Konyushkova"
        },
        {
            "affiliations": [],
            "name": "Yutian Chen"
        }
    ],
    "id": "SP:cc260e5a34adf368bdcc95a671bceb6fab532766",
    "references": [
        {
            "authors": [
                "Andr\u00e9 Barreto",
                "Will Dabney",
                "R\u00e9mi Munos",
                "Jonathan J Hunt",
                "Tom Schaul",
                "Hado P van Hasselt",
                "David Silver"
            ],
            "title": "Successor features for transfer in reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Marc G Bellemare",
                "Will Dabney",
                "R\u00e9mi Munos"
            ],
            "title": "A distributional perspective on reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Kiant\u00e9 Brantley",
                "Soroush Mehri",
                "Geoff J Gordon"
            ],
            "title": "Successor feature sets: Generalizing successor representations across policies",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Chang",
                "Kaiwen Wang",
                "Nathan Kallus",
                "Wen Sun"
            ],
            "title": "Learning bellman complete representations for offline policy evaluation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Dayan"
            ],
            "title": "Improving generalization for temporal difference learning: The successor representation",
            "venue": "Neural computation,",
            "year": 1993
        },
        {
            "authors": [
                "Carl Doersch",
                "Ankush Gupta",
                "Larisa Markeeva",
                "Adri\u00e0 Recasens",
                "Lucas Smaira",
                "Yusuf Aytar",
                "Jo\u00e3o Carreira",
                "Andrew Zisserman",
                "Yi Yang"
            ],
            "title": "Tap-vid: A benchmark for tracking any point in a video",
            "venue": "arXiv preprint arXiv:2211.03726,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yuqing Du",
                "Ksenia Konyushkova",
                "Misha Denil",
                "Akhil Raju",
                "Jessica Landon",
                "Felix Hill",
                "Nando de Freitas",
                "Serkan"
            ],
            "title": "Cabi. Vision-language models as success detectors",
            "venue": "arXiv preprint arXiv:2303.07280,",
            "year": 2023
        },
        {
            "authors": [
                "Mehrdad Farajtabar",
                "Yinlam Chow",
                "Mohammad Ghavamzadeh"
            ],
            "title": "More robust doubly robust off-policy evaluation",
            "year": 2018
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4rl: Datasets for deep data-driven reinforcement learning, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Justin Fu",
                "Mohammad Norouzi",
                "Ofir Nachum",
                "George Tucker",
                "Ziyu Wang",
                "Alexander Novikov",
                "Mengjiao Yang",
                "Michael R Zhang",
                "Yutian Chen",
                "Aviral Kumar"
            ],
            "title": "Benchmarks for deep off-policy evaluation",
            "venue": "arXiv preprint arXiv:2103.16596,",
            "year": 2021
        },
        {
            "authors": [
                "Caglar Gulcehre",
                "Ziyu Wang",
                "Alexander Novikov",
                "Thomas Paine",
                "Sergio G\u00f3mez",
                "Konrad Zolna",
                "Rishabh Agarwal",
                "Josh S Merel",
                "Daniel J Mankowitz",
                "Cosmin Paduraru"
            ],
            "title": "Rl unplugged: A suite of benchmarks for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Abhishek Gupta",
                "Vikash Kumar",
                "Corey Lynch",
                "Sergey Levine",
                "Karol Hausman"
            ],
            "title": "Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning",
            "year": 1910
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Irpan",
                "Kanishka Rao",
                "Konstantinos Bousmalis",
                "Chris Harris",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Off-policy evaluation via off-policy classification",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Max Jaderberg",
                "Volodymyr Mnih",
                "Wojciech Marian Czarnecki",
                "Tom Schaul",
                "Joel Z Leibo",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Reinforcement learning with unsupervised auxiliary tasks",
            "venue": "arXiv preprint arXiv:1611.05397,",
            "year": 2016
        },
        {
            "authors": [
                "Dmitry Kalashnikov",
                "Alex Irpan",
                "Peter Pastor",
                "Julian Ibarz",
                "Alexander Herzog",
                "Eric Jang",
                "Deirdre Quillen",
                "Ethan Holly",
                "Mrinal Kalakrishnan",
                "Vincent Vanhoucke"
            ],
            "title": "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation",
            "venue": "arXiv preprint arXiv:1806.10293,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), ICLR (Poster),",
            "year": 2015
        },
        {
            "authors": [
                "Ksenia Konyushova",
                "Yutian Chen",
                "Thomas Paine",
                "Caglar Gulcehre",
                "Cosmin Paduraru",
                "Daniel J Mankowitz",
                "Misha Denil",
                "Nando de Freitas"
            ],
            "title": "Active offline policy selection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Laskin",
                "Aravind Srinivas",
                "Pieter Abbeel"
            ],
            "title": "Curl: Contrastive unsupervised representations for reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Hoang Le",
                "Cameron Voloshin",
                "Yisong Yue"
            ],
            "title": "Batch policy learning under constraints",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Alex X Lee",
                "Coline Manon Devin",
                "Yuxiang Zhou",
                "Thomas Lampe",
                "Konstantinos Bousmalis",
                "Jost Tobias Springenberg",
                "Arunkumar Byravan",
                "Abbas Abdolmaleki",
                "Nimrod Gileadi",
                "David Khosid"
            ],
            "title": "Beyond pick-and-place: Tackling robotic stacking of diverse shapes",
            "venue": "In 5th Annual Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Lehnert",
                "Michael L Littman"
            ],
            "title": "Successor features combine elements of model-free and modelbased reinforcement learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Lihong Li",
                "Wei Chu",
                "John Langford",
                "Robert E Schapire"
            ],
            "title": "A contextual-bandit approach to personalized news article recommendation",
            "year": 2010
        },
        {
            "authors": [
                "Qiang Liu",
                "Lihong Li",
                "Ziyang Tang",
                "Dengyong Zhou"
            ],
            "title": "Breaking the curse of horizon: Infinitehorizon off-policy estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Suraj Nair",
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Chelsea Finn",
                "Abhinav Gupta"
            ],
            "title": "R3m: A universal visual representation for robot manipulation",
            "venue": "arXiv preprint arXiv:2203.12601,",
            "year": 2022
        },
        {
            "authors": [
                "Xinkun Nie",
                "Emma Brunskill",
                "Stefan Wager"
            ],
            "title": "Learning when-to-treat policies",
            "venue": "arXiv preprint arXiv:1905.09751,",
            "year": 2019
        },
        {
            "authors": [
                "Tom Le Paine",
                "Cosmin Paduraru",
                "Andrea Michi",
                "Caglar Gulcehre",
                "Konrad Zolna",
                "Alexander Novikov",
                "Ziyu Wang",
                "Nando de Freitas"
            ],
            "title": "Hyperparameter selection for offline reinforcement learning",
            "year": 2007
        },
        {
            "authors": [
                "Doina Precup"
            ],
            "title": "Eligibility traces for off-policy policy evaluation",
            "venue": "Computer Science Department Faculty Publication Series, pp",
            "year": 2000
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Scott Reed",
                "Konrad Zolna",
                "Emilio Parisotto",
                "Sergio Gomez Colmenarejo",
                "Alexander Novikov",
                "Gabriel Barth-Maron",
                "Mai Gimenez",
                "Yury Sulsky",
                "Jackie Kay",
                "Jost Tobias Springenberg"
            ],
            "title": "A generalist agent",
            "venue": "Transactions on Machine Learning Research (TMLR),",
            "year": 2022
        },
        {
            "authors": [
                "Chris Reinke",
                "Xavier Alameda-Pineda"
            ],
            "title": "Successor feature representations",
            "venue": "arXiv preprint arXiv:2110.15701,",
            "year": 2021
        },
        {
            "authors": [
                "Max Schwarzer",
                "Ankesh Anand",
                "Rishab Goel",
                "R Devon Hjelm",
                "Aaron Courville",
                "Philip Bachman"
            ],
            "title": "Data-efficient reinforcement learning with self-predictive representations",
            "venue": "arXiv preprint arXiv:2007.05929,",
            "year": 2020
        },
        {
            "authors": [
                "Mohit Sharma",
                "Claudio Fantacci",
                "Yuxiang Zhou",
                "Skanda Koppula",
                "Nicolas Heess",
                "Jon Scholz",
                "Yusuf Aytar"
            ],
            "title": "Lossless adaptation of pretrained vision models for robotic manipulation",
            "year": 2023
        },
        {
            "authors": [
                "Richard S Sutton",
                "A Rupam Mahmood",
                "Martha White"
            ],
            "title": "An emphatic approach to the problem of off-policy temporal-difference learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Georgios Theocharous",
                "Philip S Thomas",
                "Mohammad Ghavamzadeh"
            ],
            "title": "Personalized ad recommendation systems for life-time value optimization with guarantees",
            "year": 2015
        },
        {
            "authors": [
                "Tianhe Yu",
                "Deirdre Quillen",
                "Zhanpeng He",
                "Ryan Julian",
                "Karol Hausman",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "venue": "In Conference on robot learning,",
            "year": 2020
        },
        {
            "authors": [
                "Michael R Zhang",
                "Tom Le Paine",
                "Ofir Nachum",
                "Cosmin Paduraru",
                "George Tucker",
                "Ziyu Wang",
                "Mohammad Norouzi"
            ],
            "title": "Autoregressive dynamics models for offline policy evaluation and optimization",
            "venue": "arXiv preprint arXiv:2104.13877,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Robot time is an important bottleneck in applying reinforcement learning in real life robotics applications. Constraints on robot time have driven progress in sim2real, offline reinforcement learning (offline RL), and data efficient learning. However, these approaches do not address the problem of policy evaluation which is often time intensive as well. Various proxy metrics were introduced to eliminate the need for real robots in the evaluation. For example, in sim2real we measure the performance in simulation (Lee et al., 2021). In offline RL we rely on Off-policy Evaluation (OPE) methods (Gulcehre et al., 2020; Fu et al., 2021). For the purpose of deploying a policy in the real world, recent works focused on Offline Policy Selection (OPS), where the goal is to select the best performing policy relying only on offline data. While these methods are useful for determining coarse relative performance of policies, one still needs time on real robot for more reliable estimates (Levine et al., 2020).\nOur proposed \u03c02vec aims at making efficient use of the evaluation time. Efficient offline policy evaluation and selection is relevant in reinforcement learning projects, where researchers often face the challenge of validating improvements. \u03c02vec enables researchers to make more informed decisions regarding which new policy iterations to prioritize for real-world testing or to identify and discard less promising options early in the development process. In particular, we predict the values of unknown policies from a set of policies with known values in an offline setting, where a large dataset of historical trajectories from other policies and human demonstrations is provided. The last step requires policies to be represented as vectors which are comparable and thus can serve as an input to the objective function. Prior work from Konyushova et al. (2021) represents policies by the actions that they take on a set of canonical states, under the assumption that similar actions in similar states imply similar behaviour. However, this assumption is sometimes violated in practice. This work aims at finding more suitable representation by characterizing the policies based on how they change the environment.\nTo represent policies, our method \u03c02vec combines two components: successor features and foundation models. We adapt the framework of Q-learning of successor features (Barreto et al., 2017) to the\noffline setting by applying the Fitted Q evaluation (FQE) algorithm (Le et al., 2019) which is typically used for off-policy evaluation (OPE). In this work the features for individual states are provided by a general purpose pretrained visual foundation model (Bommasani et al., 2021). The resulting representations can be used as a drop in replacement for the action-based representation used by Konyushova et al. (2021).\nOur experiments show that \u03c02vec achieves solid results in different tasks and across different settings. To summarize, our main contributions are the following:\n\u2022 We propose \u03c02vec, a novel policy representation of how the policies change the environment, which combines successor features, foundation models, and offline data; \u2022 We evaluate our proposal through extensive experiments predicting return values of held out policies in 3 simulated and 2 real environments. Our approach outperforms the baseline and achieves solid results even in challenging real robotic settings and out-of-distribution scenarios; \u2022 We investigate various feature encoders, ranging from semantic to geometrical visual foundation models, to show strengths and weaknesses of various representations for the task at hand."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Representation of black-box policies. In this paper, our objective is to create vector representations for policies to predict their performance. We treat policies as black-boxes (i.e., no access to internal state, parameters, or architectures) that yield actions for a given observation. It is important to emphasize that our objective differs from representation learning for RL (Schwarzer et al., 2020; Jaderberg et al., 2016; Laskin et al., 2020), as we focus on representing policies rather than training feature encoders for downstream tasks.\nKonyushova et al. (2021) studied a setting where the goal is to identify the best policy from a set of policies with a dataset of offline experience and limited access to the environment. Each policy is represented by a vector of actions at a fixed set of states. While this representation performs well in certain applications, it may not be the most effective for predicting policy performance. For instance, consider two policies that generate random actions at each state. These policies do not exhibit meaningfully different beahviour, so for policy evaluation purposes, we expect them to be similar. However, the action policy representation categorizes these policies as different. This paper proposes a method to address this limitation by measuring trajectory-level changes in the environment.\nIn BCRL (Chang et al., 2022), a state-action feature representation is proposed for estimating policy performance. However, the representation of each policy is independent of other policies and thus cannot be employed to regress the performance of new policies given a set of evaluated policies.\nOffline Policy Evaluation. Off-policy Evaluation (OPE) aims to evaluate a policy given access to trajectories generated by another policy. It has been extensively studied across many domains (Li et al., 2010; Theocharous et al., 2015; Kalashnikov et al., 2018; Nie et al., 2019). Broad categories of OPE methods include methods that use importance sampling (Precup, 2000), binary classification (Irpan et al., 2019), stationary state distribution (Liu et al., 2018), value functions (Sutton et al., 2016;\nLe et al., 2019), and learned transition models (Zhang et al., 2021), as well as methods that combine two or more approaches (Farajtabar et al., 2018). The main focus of the OPEs approaches is on approximating the return values function for a trained policy, while \u03c02vec goes beyond classical OPE and focuses on encoding the behavior of the policy as vectors, in such a way that those vectors are comparable, to fit a performance predictor.\nFoundation Models for Robotics. Foundation models are large, self-supervised models (Bommasani et al., 2021) known for their adaptability in various tasks (Sharma et al., 2023). We compare three representative foundation models (Radford et al., 2021; Dosovitskiy et al., 2021; Doersch et al., 2022). Our proposal, \u03c02vec, is independent of the feature encoder of choice. Better or domain-specific foundation models may improve results but are not the focus of this study."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW",
            "text": "Our setting is the following. We start with a large dataset of historical trajectories D, and a policyagnostic state-feature encoder \u03d5 : S \u2192 RN . Given a policy \u03c0, our objective is to use these ingredients to create a policy embedding \u03a8\u03d5\u03c0 \u2208 RN that represents the behavior of \u03c0 (and can be used to predict its performance).\nWe aim to create this embedding offline, without running the policy \u03c0 in the environment. Although we can evaluate \u03c0 for any state in our historical dataset D, we emphasize that we do not have access to any on policy trajectories from \u03c0, which significantly complicates the process of creating an embedding that captures the behavior of \u03c0.\nOur method \u03c02vec has three steps:\n1. Choose a policy-agnostic state-feature encoder \u03d5. We discuss several options for \u03d5 below and in the experiments; however, \u03c02vec treats the policy-agnostic state-feature encoder as a black box, allowing us to leverage generic state-feature representations in our work.\n2. Train a policy-specific state-feature encoder \u03c8\u03d5\u03c0 : (S,A) \u2192 RN . In this step we combine the policy-agnostic state-feature encoder \u03d5, and the policy \u03c0, to create policy-specific statefeature encoder by training on the historical dataset D. The policy-specific state features \u03c8\u03d5\u03c0(s) capture statistics of how \u03c0 would change the environment were it to be run starting from the state s.\n3. Aggregate the policy-specific state-features to create state-agnostic policy features \u03a8\u03d5\u03c0 that represent the behavior of \u03c0 in a state-independent way.\nUsing the steps outlined above we can collect a dataset of policy-specific state-independent features paired with measured policy performance. This dataset can be used to train a model that predicts the performance of a policy from its features using supervised learning. Because we compute features for a policy using only offline data, when we receive a new policy we can compute its policy-specific state-independent features and apply the performance model to predict its performance before running it in the environment. In the following sections we expand on each step."
        },
        {
            "heading": "3.2 POLICY-AGNOSTIC STATE FEATURES",
            "text": "The role of the state-feature encoder \u03d5 is to produce an embedding that represents an individual state of the environment. In this paper we focus on state encoders \u03d5 : I \u2192 RN that consume single images I . Generically our method is agnostic to the input space of the state-feature encoder, but practically speaking it is convenient to work with image encoders because that gives us access to a wide range of pretrained generic image encoders that are available in the literature.\nWe also consider a few simple ways to construct more complex features from single image features. When each state provides multiple images we embed each image separately and sum the result to create a state embedding. We also consider creating embeddings for transitions (s, s\u2032) by computing \u2206\u03d5(s, s\u2032) \u225c \u03d5(s\u2032)\u2212 \u03d5(s). Both cases allow us to leverage features from pretrained models."
        },
        {
            "heading": "3.3 POLICY-SPECIFIC STATE FEATURES",
            "text": "The next step is to use the policy-agnostic state-feature encoder \u03d5 that provides a generic representation for individual states to train a policy-specific state-feature encoder \u03c8\u03d5\u03c0 : (S,A) \u2192 RN that represents the effect that \u03c0 would have on the environment if it were run starting from the given state.\nThe work of Dayan (1993); Barreto et al. (2017) on successor features provides a basis for our approach to policy representation. We briefly review successor features here, and comment below on how we make use of them. We refer the reader to recent literature covering successor features Lehnert & Littman (2020); Brantley et al. (2021); Reinke & Alameda-Pineda (2021).\nSuppose that the reward function for a task can be written as a linear function\nr(s, a, s\u2032) = \u27e8\u03d5(s, a, s\u2032),wtask\u27e9, (1)\nwhere \u03d5(s, a, s\u2032) \u2208 RN encodes the state-transition as a feature vector and wtask \u2208 RN are weights. Barreto et al. (2017) observe that if the reward can be factored as above, then the state-action-value function for a policy \u03c0 can be written as\nQ\u03c0(s, a) = E(s\u2032|s)\u223cD,a\u223c\u03c0(s) [ \u221e\u2211 i=t \u03b3i\u2212tr(si, ai, si+1) ] = \u27e8\u03c8\u03d5\u03c0(s, a),wtask\u27e9 , (2)\nwhere\n\u03c8\u03d5\u03c0(s, a) = E(s\u2032|s)\u223cD,a\u223c\u03c0(s) [ \u221e\u2211 i=t \u03b3i\u2212t\u03d5(si, ai, si+1) ] , (3)\n(s|s\u2032) \u223c D is a transition from the environment, and \u03b3 is the discount factor. The corresponding state-value function is V \u03c0(s) \u225c Q\u03c0(s, \u03c0(s)) = \u27e8\u03c8\u03d5\u03c0(s, \u03c0(s)),wtask\u27e9 \u225c \u27e8\u03c8\u03d5\u03c0(s),wtask\u27e9. We will use the notation \u03c8\u03d5\u03c0(s) \u225c \u03c8 \u03d5 \u03c0(s, \u03c0(s)) frequently throughout the remainder of the paper.\nThe value of \u03c8\u03d5\u03c0(s) is known as the successor features of the state s under the policy \u03c0. Successor features were originally motivated through the above derivation as a way of factoring the value function of a policy into a task-independent behavior component (the successor features) that is independent of the task, and a task-dependent reward component that is independent of behavior.\nFor our purposes we will mostly ignore the reward component (although we return to it in one of the experiments) and focus on the behavior term shown in Equation 3. This term is interesting to us for two reasons. First, we can see by inspection of the RHS that the value of \u03c8\u03d5\u03c0(s) = \u03c8 \u03d5 \u03c0(s, \u03c0(s)) represents the behavior of \u03c0 as a future discounted sum of state features along a trajectory obtained by running \u03c0 beginning from the state s. In other words, \u03c8\u03d5\u03c0 represents the behavior of \u03c0 in terms of\nthe features of the states that \u03c0 will encounter, where the state features are themselves given by the policy-agnostic state-feature encoder from the previous section. Figure 2 summarizes the relationship between successor features \u03c8 and state encoders \u03d5.\nSecond, Equation 3 satisfies the Bellman equation meaning that the function \u03c8\u03d5\u03c0(s, a) can be estimated from off-policy data in a task-agnostic way using a modified version of Q-learning, where the scalar value reward in ordinary Q-learning is replaced with the vector valued transition features \u03d5(s, a, s\u2032). We rely on Fitted Q Evaluation (FQE, Le et al. (2019)), an offline Q-learning based algorithm, and thus, we obtain a representation of policy behavior purely from data without executing the policy in the environment. Given a dataset D and a policy \u03c0, FQE estimates its state-action-value function Q\u03c0(s, a) according to the following bootstrap loss:\nL(\u03b8) = E(s,a,r,s\u2032)\u223cD,a\u2032\u223c\u03c0(s\u2032) [ \u2225\u03c8\u03c0\u03b8 (s, a)\u2212 (\u03d5(s, a, s\u2032) + \u03c8\u03c0\u03b8 (s\u2032, a\u2032))\u22252 ] . (4)\nFQE is simple to implement and it performs competitively with other OPE algorithms in a variety of settings (Fu et al., 2021) including simulated and real robotics domains (Paine et al., 2020; Konyushova et al., 2021). We use FQE with our historical dataset D to train a policy-specific state-action-feature network \u03c8\u03d5\u03c0(s, a), which we then use as the policy-specific state-feature encoder \u03c8\u03d5\u03c0(s) \u225c \u03c8 \u03d5 \u03c0(s, \u03c0(s)) by plugging in the policy action."
        },
        {
            "heading": "3.4 STATE-AGNOSTIC POLICY FEATURES",
            "text": "We obtain a single representation \u03a8\u03d5\u03c0 of a policy \u03c0 from the state-dependent successor features \u03c8 \u03d5 \u03c0(s) for that policy by averaging the successor features over a set of canonical states:\n\u03a8\u03d5\u03c0 = Es\u223cDcan [\u03c8\u03d5\u03c0(s)], (5) where Dcan is a set of states sampled from historical trajectories. We sample the canonical states set Dcan \u2282 D uniformly from from our historical dataset, as in Konyushova et al. (2021), ensuring that each canonical state comes from a different trajectory for better coverage. We average successor features over the same set Dcan for every policy. The intuition behind this representation is that \u03c8\u03d5\u03c0(s) represents the expected change that \u03c0 induces in the environment by starting in the state s; by averaging over Dcan, \u03a8\u03d5\u03c0 represents an aggregated average effect of the behavior of \u03c0."
        },
        {
            "heading": "3.5 PERFORMANCE PREDICTION",
            "text": "We aim at predicting the performance of novel, unseen policies. We begin with a dataset of historical policies for which we have measured performance \u03a0 = {. . . , (\u03c0i, Ri), . . .}. For each policy in this dataset we create an embedding using the above procedure to obtain a new dataset {. . . , (\u03a8\u03d5\u03c0i , Ri), . . .} and then train a performance model R\u0302i = f(\u03a8 \u03d5 \u03c0i) using supervised learning. Given a new policy \u03c0\u2217 we can then predict its performance before running it in the environment by computing the \u03c02vec features for the new policy using the above procedure and applying the performance model to obtain R\u0302\u2217 = f(\u03a8\u03d5\u03c0\u2217)."
        },
        {
            "heading": "4 EXPERIMENTAL SETUP",
            "text": "In this section we describe the feature encoders, domains, and evaluation procedures, followed by details about our baselines. More details about our architecture, domains, and training procedure can be found in the Appendix.\nFeature encoder. Firstly, the Random feature encoder employs a randomly-initialized ResNet50 (He et al., 2016). Random features are trivial to implement, and achieve surprisingly strong performance in many settings (Rahimi & Recht, 2007). Here they serve as a simple baseline.\nNext, we explore with CLIP (Radford et al., 2021). CLIP-network is trained to match image and text embeddings on a large-scale dataset of image caption pairs. Intuitively, by aligning image and text features, CLIP network is trained to encode high-level semantic information.\nVisual Transformers (VIT) (Dosovitskiy et al., 2021) treat images as a 1D sequence of patches and learn visual features via an attention mechanism. In our experiments the visual transformer is pre-trained on imagenet classification.\nLastly, we explore Track-any-point (TAP) (Doersch et al., 2022), a general-purpose network for point tracking in videos. The network is pre-trained to track arbitrary points over video sequences and as a result it learns to understand the low-level geometric features in a scene. We use an attention layer trained to select task-relevant features from the TAP model to reduce dimensionality.\nThis set of feature encoders spans a spectrum of properties as they are created by optimising different objectives. At one extreme CLIP features are trained to align image features with a text description, and encode the semantics of the image. At the other extreme TAP features are trained to track points in videos, and capture low level geometric and texture information. VIT features are in the middle, they need to encode both semantics and local texture to accomplish classification tasks. Depending on the environment and task at hand, better state representation is likely to result in better prediction properties of \u03c02vec. We leave the question of finding the best representation as future work.\nDomains. We present extensive experiments to support \u03c02vec\u2019s capabilities across three simulated domains\u2014Insert Gear (Sim), Metaworld, and Franka-Kitchen, and two real domains\u2014Insert Gear (Real) and RGB Stacking (Figure 3). In each domain we use a dataset of offline human demonstrations (Metaworld and Kitchen) and held out policies trajectories (RGBStacking and Insert Gear) for training policy representations. Each policy is treated as a black-box where we do not have any prior knowledge about the architecture or training parameters. We provide further details in Supplementary.\nEvaluation. We assess the quality of the policy representations by measuring the ability of the model f to predict the performance of held out policies (see Section 3.5). We adopt k-fold cross validation over the set \u03a0 and report results averaged over cross validation folds. Following previous works on offline policy evaluation (Paine et al., 2020; Fu et al., 2021), we adopt the following three complementary metrics. We report further details in the Supplementary.\n\u2022 Normalized Mean Absolute Error (NMAE) measures the accuracy of the prediction w.r.t. the ground-truth. We adopt MAE instead of MSE to be robust to outliers and we normalize the error to be in range between the return values for each environment. Lower is better. \u2022 Rank Correlation measures how the estimated values correlate with the ground-truth. Correlation focuses on how many evaluations on the robot are required to find the best policy. Higher is better. \u2022 Regret@1 measures the performance difference between the best policy and the predicted best policy, normalized w.r.t. the range of returns values for each environment. Lower is better.\nCorrelation and Regret@1 are the most relevant metric for evaluating \u03c02vec on OPS. On the other hand, NMAE refers to the accuracy of the estimated return value and is suited for OPE.\nBaselines. The problem in this paper is to represent policies in such a way that the representations can be used to predict the performance of other policies given the performance of a subset of policies. Importantly, to address this problem the representation should 1) encode the behavior of the policy, 2) in a way that is comparable with the representations of other policies, and 3) does not require online\ndata. Active Offline Policy Selection (AOPS) (Konyushova et al., 2021) stands alone as a notable work that delves into policy representation from offline data with the task of deciding which policies should be evaluated in priority to gain the most information about the system. AOPS showed that representing policies according to its algorithm leads to faster identification of the best policy. In AOPS\u2019s representation, which we call \u201cActions\u201d, policies are represented through the actions that the policies take on a fixed set of canonical states. We build Actions representation as follows. We run each policy \u03c0 on the set of states Dcan sampled from historical trajectories. Next, we concatenate the resulting set of actions {\u03c0(s)}s\u2208Dcan into a vector. To the best of our knowledge, the Actions representation is the only applicable baseline in the setting that we adopt in this paper. Nevertheless, OPE methods that estimate policy performance from a fixed offline dataset are standard methodology in offline RL literature. Although these methods do not take the full advantage of the problem setting in this paper (the performance of some of the policies is known) they can still serve for comparison. In this paper, we compared against FQE which is a recommended OPE method that strikes a good balance between performance (it is among the top methods) and complexity (it does not require a world model) (Fu et al., 2021)."
        },
        {
            "heading": "5 RESULTS",
            "text": "We report results for various feature encoders for Insert gear (sim and real) and RGBStacking. Similarly, we report averaged results for over 4 tasks and 3 point of view for Metaworld and over 5 tasks and 3 point of view for Kitchen. Along with results for each feature encoder, we report the average results of picking the best feature encoder for each task (BEST-\u03d5). Similarly, we report as BEST-CLIP and BEST-VIT the average results when adopting the best feature encoder between CLIP/VIT and \u2206CLIP/\u2206VIT. We identify the best feature encoder for a task by conducting cross-validation on previously evaluated policies and pick the best encoder in terms of regret@1.\nOur results demonstrate that (i) \u03c02vec outperforms the Actions baseline models consistently across real and simulated robotics environments and multiple tasks, showcasing the framework\u2019s effectiveness in representing policies. Furthermore, we demonstrate the applicability to real-world robotic settings, specifically in the challenging Insert Gear (Real) environment, where even underperforming policies contribute to improved policy evaluation. We show that choosing the best model as\na feature-extractor greatly improves results (ii). Finally, we adopt \u03c02vec to solve Equation 2 and estimate policies\u2019 return values in the Metaworld\u2019s assembly environment, without relying on any ground-truth data (iii). Although the successor feature assumption of linearity of rewards is violated, \u03c02vec still ranks policies competitively in the offline setting when compared to FQE. In the Appendix, we provide an intuition for choosing the best \u03d5 based on the correlation between task difficulty (iv), and we study the effect of different dataset types, such as demonstrations and trajectories from held out policies (v). We investigate \u03c02vec\u2019s generalization capabilities (vi), including out-of-distribution scenarios (vii). We also demonstrate that \u03c02vec represents random policies close in the feature space (viii), and that \u03c02vec is robust to canonical state coverage (ix) and effective with online data (x).\n(i) \u03c02vec consistently outperforms Actions. We compare \u03c02vec and Actions across all scenarios. Our method outperforms Actions representation when predicting values of unseen policies in both real robotics scenarios\u2013RGB stacking and insert-gear (real)\u2013as shown in Table 1. In the former, \u03a8VIT achieves regret@1 of 0.036 compared to Actions\u2019 0.074, with a relative improvement of 51%. In the latter, \u03a8CLIP improves over Actions by achieving regret@1 0.267 compared to Actions\u2019 0.578 and drastically outperform Actions in terms of correlation by achieving +0.618 compared to Actions\u2019 \u22120.545. \u03c02vec performs robustly on insert gear (real) despite policies\u2019 performances for this task vary greatly (see supplementary for per-task policies performances).\nWe also evaluate our approach in the simulated counterpart Insert Gear (Sim). In this environment, \u03a8CLIP and \u03a8TAP achieve regret@1 of 0.314 and 0.359 respectively, compared to Actions 0.427. We underline the dichotomy between geometrical and semantic features: \u03a8TAP performs best in terms of NMAE and Correlation, while \u03a8CLIP outperforms in Regret@1. These results highlight how various \u03d5 compare depending on setting, type of task, and policy performance.\n(ii) When evaluating across multiple settings, selecting \u03d5 leads to better results. We compare \u03c02vec with different foundation models across 12 Metaworld settings and 15 Kitchen settings. Table 2 reports the average results across all settings for Metaworld and Kitchen. In Metaworld, we notice that Actions performs on par with \u03a8CLIP, \u03a8VIT, and their respective variations \u2206CLIP and \u2206VIT, in terms of correlation and regret@1, while our approach consistently outperforms Actions in terms of NMAE. As these domains have less state variability, Actions represent policies robustly. We test CLIP/\u2206CLIP\nand VIT/\u2206VIT on previously evaluated policies for each task through cross-validation to identify the best feature encoder for the task in terms of regret@1. We report \u03a8BEST-CLIP and \u03a8BEST-VIT as the average results over the best among CLIP/VIT and \u2206CLIP/\u2206VIT. \u03a8BEST-CLIP achieves regret@1 0.194 and correlation 0.351, outperforming Actions representation. We highlight that the choice of \u03d5 is critical, since \u03a8random\u2014using a randomly-initialized ResNet50 as feature extractor\u2014underperforms. Moreover, \u03c02vec with the best \u03d5 drastically improves, achieving regret@1 of 0.153 compared to Actions 0.232. We notice similar improvements when evaluating on Kitchen\u2019s 15 settings. Table 2 compares choosing the BEST \u03d5 w.r.t. to VIT and CLIP, and against Actions. In Kitchen, \u03a8VIT outperforms \u03a8CLIP and Actions, while \u03a8BEST\u2212\u03d5 achieves the overall best results.\n(iii) \u03c02vec enables fully-offline policy selection. By directly modelling the relationship between successor features and returns, we avoid the linear reward assumption of the original successor features work. This is preferable since rewards are generally not linearly related to state features. However, this restricts our method to settings where some policies\u2019 performance is known. To evaluate performance in a fully-offline setting, we fit a linear model the task reward r\u0302 = \u27e8\u03d5(s),wtask\u27e9 given the state\u2019s feature representation \u03d5(s), as in Equation 2 from the original successor features work. Next we predict policies returns as R\u0302i = \u27e8\u03a8\u03d5\u03c0i ,wtask\u27e9. We compare our approach to FQE in Table 3 and find that while our method\u2019s return predictions are inaccurate (as evidenced by the high NMAE), it still performs well in ranking policies (higher Correlation and lower Regret@1)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We presented \u03c02vec, a framework for offline policy representation via successor features. Our method treats the policy as a black box, and creates a representation that captures statistics of how the policy changes the environment rather than its idiosyncrasies. The representations can be trained from offline data, and leverage the pretrained features of visual foundation models to represent individual states of the environment. In our experiments, we represented policies by relying on visual features from semantic (CLIP), geometric (TAP), and visual (VIT) foundation models. We showed that \u03c02vec outperforms previously used Actions based representations and generalizes to fully-offline settings. Overall, our experiments showcase the effectiveness and versatility of \u03c02vec in representing policies and its potential for various applications in reinforcement learning.\nMoving forward, we acknowledge that finding the optimal combination of these elements remains an ongoing challenge. Future work should explore diverse foundation models, offline learning algorithms for successor feature training, and dataset choices. Fine-tuning the feature encoder \u03d5 along with \u03c8\u03c0\u03d5 is interesting but pose challenges, as each feature encoder would specialize to predict features for a specific policy, resulting in policy representations that are independent and not comparable. We leave end-to-end fine-tuning as future work. Integrating \u03c02vec into AOPS framework (Konyushova et al., 2021) for enhanced offline policy selection is another intriguing avenue. Additionally, extending \u03c02vec to augment the Generalized Policy Improvement (Barreto et al., 2017) in offline settings presents exciting research opportunities."
        },
        {
            "heading": "7 APPENDIX",
            "text": ""
        },
        {
            "heading": "7.1 DOMAINS",
            "text": "The Metaworld Yu et al. (2020) and Kitchen Gupta et al. (2019) domains are widely known in the literature. They contain many tasks from multiple views, however, the variability among tasks is low. For example, the robotic arm in Metaworld is initialized within a narrow set of positions, while in Kitchen the object positions are fixed. The task in real robot RGB Stacking Lee et al. (2021); Reed et al. (2022) is to stack the red object on top of the blue object with green object as a distractor, where objects are of various geometric shapes. This task is difficult because the objects have unusual shapes and may be positioned at any point in the workspace. We also consider a challenging gear insertion task in sim and real where the objective is to pick up a gear of certain size (from an arbitrary position in the workspace) in the presence of other gears and insert it onto a specific shaft on the gear assembly base (arbitrary position in real, fixed in sim). We describe data and policies for each domain below.\nInsert Gear (Sim). We use 18 policies for Insert Gear (Sim) task in the simulated environment. We take an intermediate and the last checkpoint for each policy. We collect trajectories with T = 300 steps from a single \u03c0 and train all \u03c8\u03c0i on those trajectories. The state s consists of two images, one from a left camera and one from a right camera, and proprioception sensing. All the policies in this domain have the following architecture. Image observations are encoded using a (shared) ResNet, and proprioception is embedded using an MLP. Then, two embeddings are concatenated and further processed by an MLP, followed by an action head.\nInsert Gear (Real). The observable state consists of three points of view: a camera on the left of the basket, a camera on the right of the basket, and an egocentric camera on the gripper. The state also includes proprioception information about the arm position. The setup corresponds to the medium gear insertion task described in the work of Du et al. (2023). We conduct experiments on the Insert Gear (Real) task on a real robotic platform by evaluating 18 policies. We collect a dataset of trajectories with a hold-out policy trained on human demonstrations. Next, we train our set of policies on this dataset and we evaluate \u03c02vec. The state and he policy architecture are the same as in Insert Gear (Sim).\nRGB stacking. We use 12 policies trained with behavior cloning on a previously collected dataset of demonstrations for RGB stacking task with a real robotic arm. Each policy is trained with a variety of hyperparameters. The state s consists of images from the basket cameras, one on the left and one on the right, and a first person camera mounted on the end-effector, and proprioception sensing. For training \u03c02vec, we adopted an offline dataset of trajectories. We collected the trajectories by running a policy trained on human demonstrations. Trajectory length varies between 800 and 1000 steps. We built the evaluation dataset Dcan by sampling 5, 000 trajectories and then sampling one state from each of them. Policy architecture follows the description in Lee et al. (2021).\nMetaworld. For Metaworld, we consider 4 tasks: assembly, button press, bin picking, and drawer open. We use 3 points of views (left, right, and top), as specified in Sharma et al. (2023); Nair et al. (2022). For each task-camera pair, we adopt 12 policies as proposed by Sharma et al. (2023) for the particular task and point of view. The policies vary by the hyperparameters used during training (learning rate, seed, and feature extractor among NFnet, VIT, and ResNet). Next, we train a successor feature network \u03c8\u00b7\u03c0 for each policy \u03c0 on a cumulative dataset of demonstrations for all tasks and views. At evaluation, we build Dcan by uniformly sampling one state from each demonstration.\nFranka-Kitchen. For Kitchen, we consider 5 tasks: Knob-on, Left door open, light on, microwave open, and door open with 3 points of views: left, right, and top, as provided by previous works (Sharma et al., 2023; Nair et al., 2022). For each task and point of view, we use human demonstrations provided by Fu et al. (2020). We also adopt policies {\u03c0i} proposed by Sharma et al. (2023). Each policy solves each task using proprioception and image information from a single point of view, and the policies vary by the hyperparameters used during training (learning rate, seed, and feature extractor among NFnet, ViT, and ResNet). Additional details can be found in Sharma et al. (2023).\nTable 4 reports mean and standard deviation of the expected return values for the policies under consideration. We highlight that Metaworld and Insert gear have high standard-deviation (standard\ndeviation is 75% or more of the mean return value), as we have extremely good and extremely bad policies. On the contrary, return values for RGB Stacking and Kitchen vary less, i.e., most of the policies for these environments achieve similar performance."
        },
        {
            "heading": "7.2 ARCHITECTURE AND TRAINING DETAILS",
            "text": "The architecture of successor features network \u03c8\u03d5\u03c0(s) for a policy \u03c0 is illustrated in Figure 4. The network takes state-action pairs as input; it encodes actions and proprioception with a multi-layer perceptron, and visual observations using a ResNet50. When the environment observation consists of multiple images (e.g., multiple camera views of the same scene), we embed each image separately and average the resulting vectors. We concatenate the state and action encodings and process the resulting feature vector with a MLP. Finally, the network \u03c8\u03d5\u03c0(s) outputs a vector of dimension R\nN\u00d7B , where N is the dimension of the feature vector \u03d5(s) represented as a distribution over B bins1. \u03c8\u03d5\u03c0(s) returns N histograms, where each histogram \u03c8i approximates the distribution of the discounted sum of feature \u03d5i over policy \u03c0. For each environment, we inspect the range of values assumed by \u03d5 to find the min (lower) and max (upper) bound of the histogram. At inference time, we take the expected value of each histogram to compute the successor features vector.\nWe train \u03c8\u03d5\u03c0(s, a) using an FQE with a distributional objective (Le et al., 2019; Bellemare et al., 2017). Training the successor features network only requires offline data (separate for each domain) and does not require any online interactions. We train the network for 50, 000 steps for Metaworld and Kitchen and 100, 000 steps for RGB Stacking, Insert Gear (Sim), and Insert Gear (Real). We adopt\n1We use B = 51 bins in all experiments.\ndifferent training steps because Metaworld and Kitchen are more predictable than RGB Stacking and Insert gear and less demonstrations are provided. We use the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 3e\u22125 and a discount factor of \u03b3 = 0.99. For evaluation, we adopt 3-fold cross-validation in all experiments."
        },
        {
            "heading": "7.3 METRICS",
            "text": "We adopt three common metrics Fu et al. (2021): NMAE, correlation, regret@1.\n\u2022 Normalized Mean Absolute Error (NMAE) is defined as the difference between the value and estimated value of a policy:\nNMAE = |V \u03c0 \u2212 V\u0302 \u03c0|, (6)\nwhere V \u03c0 is the true value of the policy, and V\u0302 \u03c0 is the estimated value of the policy. \u2022 Regret@1 is the difference between the value of the best policy in the entire set, and the\nvalue of the best predicted policy. It is defined as:\nRegret@1 = max i\u22081:N V \u03c0i \u2212 max j\u2208(1:N) V \u03c0j . (7)\n\u2022 Rank Correlation (also Spearman\u2019s \u03c1) measures the correlation between the estimated rankings of the policies\u2019 value estimates and their true values. It can be written as:\nCorr = Cov(V \u03c01:N , V\u0302 \u03c0 1:N )\n\u03c3(V \u03c01:N )\u03c3(V\u0302 \u03c0 1:N )\n. (8)\nCorrelation and regret@1 are the most relevant metrics for evaluating \u03c02vec on Offline Policy Selection (OPS), where the focus is on ranking policies based on values and selecting the best policy.\nRegret@1 is commonly adopted in assessing performances for Offline Policy Selection, as it directly measures how far off the best-estimated policy is to the actual best policy.\nCorrelation is relevant for measuring how the method ranks policies by their expected return value. By relying on methods that achieve higher correlation and thus are consistent in estimating policy values, researchers and practitioners can prioritize more promising policies for online evaluation.\nOn the other hand, NMAE refers to the accuracy of the estimated return value. NMAE is especially significant when aiming at estimating the true value of a policy and is suited for Offline Policy Evaluation (OPE), where we are interested to know the values of each policy. We assess \u03c02vec\u2019s representation in both settings, showing that \u03c02vec consistently outperforms the baseline in both metrics. We improve the discussion on metrics in the Appendix of the manuscript."
        },
        {
            "heading": "7.4 ADDITIONAL EXPERIMENTS",
            "text": "(iv) Correlation between task difficulty and \u03d5. We notice that policy performance varies widely in Insert Gear (Sim) and Insert Gear (Real), as most of the policies fail to solve the task (see\nsupplementary for per-task policies performances). The gap is evident when compared to the average return value for RGB Stacking, where standard deviation is negligible. Our intuition is that in hard-to-solve scenarios the actions are often imperfect and noisy. This interpretation would explain poor performance of the Actions baseline. The comparison of \u03a8CLIP and \u03a8VIT across environments suggests a correlation between the choice of \u03d5 and policies return values. \u03a8CLIP performs better than \u03a8VIT in Insert Gear (Sim), Insert Gear (Real), and Metaworld, where we report the highest standard deviation among policies performances. \u03a8CLIP is robust when the task is hard and most of the policies fail to solve it. On the other hand, \u03a8VIT is the best option in Kitchen and RGB stacking, where the standard deviation of policies returns is low or negligible.\n(v) Studying the performance of \u03c02vec with different datasets. We investigate how modifications of the dataset for training \u03c02vec improves performance in Metaworld. Intuitively, if the training set for \u03c02vec closely resembles the set of reachable states for a policy \u03c0, solving Equation 2 leads to a more close approximation of the real successor feature of \u03c0 in (s, a). We empirically test this claim as follows. We collect 1, 000 trajectories for each task-view setting using a pre-trained policy. Next, we train successor features network \u03c8\u03d5\u03c0 for each policy \u03c0 and feature encoder \u03d5, and represent each policy as \u03a8\u03d5\u03c0. Table 5 reports results on Metaworld when training \u03c02vec with the aforementioned dataset (named trajectory in the Table). In this setting, \u03a8CLIP and \u03a8VIT outperform both their counterpart trained on demonstrations and Actions representation, reaching respectively regret@1 of 0.189 and 0.187. These results slightly improve if we assume to opt for the best dataset for each task and take the average, as reported under best dataset in Table 5. Overall, choosing the best feature encoder \u03d5 and best dataset for any given task leads to the best performing \u03a8BEST\u2212\u03d5 with correlation 0.615 and regret@1 0.135\u2013improving over Actions by 0.26 and 0.10 respectively.\n(vi) \u03c02vec generalizes better while Actions works with policies are very similar. We explore how Actions and \u03c02vec compare in the special scenario where all policies are similar. We take 4 intermediate checkpoints at the end of the training for each policy as a set of policies to represent.\nOur intuition is that intermediate checkpoints for a single policy are similar to each other in how they behave. Next, we represent each checkpoint with \u03a8CLIP and Actions. We compare cross-validating the results across all checkpoints w.r.t. training on checkpoints for 3 policies and testing on checkpoints for the holdout policy. Table 6 reports results of this comparison on Metaworld\u2019s assembly-left task. We notice that Actions representations fail to generalize to policies that greatly differ from the policies in the training set. Fitting the linear regressor with Actions achieves a negative correlation of \u22120.189 and regret@1 0.034. On the other hand, \u03a8CLIP is robust to unseen policies and outperforms Actions with positive correlation 0.190 and lower regret of 0.029.\n(vii) \u03c02vec performs in out-of-distribution scenarios. We conduct another investigation to explore \u03c02vec performances in a out-of-distribution setting. We hypothesize that \u03c02vec represents policies in meaningful ways even when those policies are deployed in settings that differ from the training set, thanks to the generalisation power of foundation models. Table 7 compares \u2206CLIP and Actions in evaluating policies trained for Metaworld\u2019s assembly-right and tested in Metaworld\u2019s assembly-left. \u03c02vec achieves reget@1 of 0.300 and NMAE of 0.227, outperforming Actions by 0.175 and 0.136 respectively. We leave further exploration of \u03c02vec in out-of-distribution settings for further works.\n(viii) \u03c02vec represents random policies close in the representation space. Intuitively, we expect that random policies do not modify the environment in a meaningful way. Therefore, their representations should be closer to each other compared to the similarity between the more meaningful trained policies. We investigate this claim as follows. We provide a set of 6 trained policies and a set of 6 random policies for Metaworld assembly-left. We compute the average and max distance among the random policies representations, normalized by the average intraset distance between trained policies representations. We compare our \u03a8CLIP and \u03a8random with Actions. Table 8 reports the results that clearly support our intuition. Both \u03a8\u2206CLIP and \u03a8Random represent random policies close to each other, as the average distance of their representation is respectively 0.03 and 0.11 and the maximum distance 0.22 and 0.17 respectively. On the other hand, if we represent policies with Actions, the representations average and maximum distances are 0.39 and 0.62, meaning that random policies are represented far apart from each other.\n(ix) Canonical state coverage. We sample canonical states uniformly from the dataset that was used for training offline RL policies. Even though there is some intuition that selecting canonical states to represent the environment better can be beneficial, even simple sampling at random worked well in our experiments. We conduct further experiments to ablate the state coverage. We adopt demonstrations from Metaworld Assembly task and adopt the initial state of each trajectory for \u03c02vec\u2019s and Actions representation. By adopting the initial state of a trajectory, \u03c02vec cannot rely on the state coverage. We report the results in Table 10. We show that \u03c02vec is robust to state coverage, showing SOTA performances even when the canonical states coverage is limited to the first state of each demonstration.\n(x) \u03c02vec from online data. We ideally want to evaluate policies without deployment on a real robot, which is often time-consuming and can lead to faults and damages. Nonetheless, we explore \u03c02vec capabilities for representing policies from online data. For each policy \u03c0, we collect a dataset of trajectories by deploying the policy on the agent. Next, we train \u03c8\u03c0 on the dataset of \u03c0\u2019s trajectories and compute its \u03c02vec\u2019s representation. Table 9 reports results when training \u03c02vec on online data on Insert Gear (sim) task. We show that \u03c02vec\u2019s performances improve with respect to the offline counterpart. This result is expected: a better dataset coverage leads to improved results, as we also showed in (v)."
        }
    ],
    "year": 2024
}