{
    "abstractText": "This paper introduces the Fair Fairness Benchmark (FFB), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is important for ethical compliance. However, there exist challenges in comparing and developing fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an opensource standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from 45,079 experiments, 14,428 GPU hours. We believe that our work will significantly facilitate the growth and development of the fairness research community. The benchmark is available at https://anonymous.4open.science/r/fair_fairness_benchmark-351D.",
    "authors": [],
    "id": "SP:7ffb5c62b01af39c5e4b4f16872daa831b3a96b2",
    "references": [
        {
            "authors": [
                "Tameem Adel",
                "Isabel Valera",
                "Zoubin Ghahramani",
                "Adrian Weller"
            ],
            "title": "One-network adversarial fairness",
            "venue": "In Proceedings of the AAAI,",
            "year": 2019
        },
        {
            "authors": [
                "Wael Alghamdi",
                "Hsiang Hsu",
                "Haewon Jeong",
                "Hao Wang",
                "Peter Michalak",
                "Shahab Asoodeh",
                "Flavio Calmon"
            ],
            "title": "Beyond adult and compas: Fair multi-class prediction via information projection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sina Baharlouei",
                "Maher Nouiehed",
                "Ahmad Beirami",
                "Meisam Razaviyayn"
            ],
            "title": "R\u00e9nyi fair inference",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Joachim Baumann",
                "Anik\u00f3 Hann\u00e1k",
                "Christoph Heitz"
            ],
            "title": "Enforcing group fairness in algorithmic decision making: Utility maximization under sufficiency",
            "venue": "In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2022
        },
        {
            "authors": [
                "Rachel K.E. Bellamy",
                "Kuntal Dey",
                "Michael Hind",
                "Samuel C. Hoffman",
                "Stephanie Houde",
                "Kalapriya Kannan",
                "Pranay Lohia",
                "Jacquelyn Martino",
                "Sameep Mehta",
                "Aleksandra Mojsilovic",
                "Seema Nagar",
                "Karthikeyan Natesan Ramamurthy",
                "John Richards",
                "Diptikalyan Saha",
                "Prasanna Sattigeri",
                "Moninder Singh",
                "Kush R. Varshney",
                "Yunfeng Zhang"
            ],
            "title": "AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias, October 2018",
            "year": 2018
        },
        {
            "authors": [
                "Alex Beutel",
                "Jilin Chen",
                "Zhe Zhao",
                "Ed H Chi"
            ],
            "title": "Data decisions and theoretical implications when adversarially learning fair representations",
            "venue": "arXiv preprint arXiv:1707.00075,",
            "year": 2017
        },
        {
            "authors": [
                "Sarah Bird",
                "Miro Dud\u00edk",
                "Richard Edgar",
                "Brandon Horn",
                "Roman Lutz",
                "Vanessa Milan",
                "Mehrnoosh Sameki",
                "Hanna Wallach",
                "Kathleen Walker"
            ],
            "title": "Fairlearn: A toolkit for assessing and improving fairness in ai",
            "venue": "Technical Report MSR-TR-2020-32,",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Buyl",
                "Tijl De Bie"
            ],
            "title": "Optimal transport of classifiers to fairness",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Flavio Calmon",
                "Dennis Wei",
                "Bhanukiran Vinzamuri",
                "Karthikeyan Natesan Ramamurthy",
                "Kush R Varshney"
            ],
            "title": "Optimized pre-processing for discrimination prevention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Simon Caton",
                "Christian Haas"
            ],
            "title": "Fairness in machine learning: A survey",
            "venue": "arXiv preprint arXiv:2010.04053,",
            "year": 2020
        },
        {
            "authors": [
                "Junyi Chai",
                "Taeuk Jang",
                "Xiaoqian Wang"
            ],
            "title": "Fairness without demographics through knowledge distillation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra Chouldechova"
            ],
            "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
            "venue": "Big data,",
            "year": 2017
        },
        {
            "authors": [
                "Ching-Yao Chuang",
                "Youssef Mroueh"
            ],
            "title": "Fair mixup: Fairness via interpolation",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Sam Corbett-Davies",
                "Emma Pierson",
                "Avi Feller",
                "Sharad Goel",
                "Aziz Huq"
            ],
            "title": "Algorithmic decision making and the cost of fairness",
            "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In NAACL-HLT",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Ba Diederik P. Kingma"
            ],
            "title": "Adam: a method for stochastic optimization",
            "venue": "CoRR, abs/1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Frances Ding",
                "Moritz Hardt",
                "John Miller",
                "Ludwig Schmidt"
            ],
            "title": "Retiring adult: New datasets for fair machine learning",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Dheeru Dua",
                "Casey Graff"
            ],
            "title": "UCI machine learning repository, 2017",
            "venue": "URL http://archive.ics. uci.edu/ml",
            "year": 2017
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Moritz Hardt",
                "Toniann Pitassi",
                "Omer Reingold",
                "Richard Zemel"
            ],
            "title": "Fairness through awareness",
            "venue": "In Conference on Innovations in Theoretical Computer Science (ITCS),",
            "year": 2012
        },
        {
            "authors": [
                "Harrison Edwards",
                "Amos Storkey"
            ],
            "title": "Censoring representations with an adversary",
            "venue": "arXiv preprint arXiv:1511.05897,",
            "year": 2015
        },
        {
            "authors": [
                "Michael Feldman",
                "Sorelle A Friedler",
                "John Moeller",
                "Carlos Scheidegger",
                "Suresh Venkatasubramanian"
            ],
            "title": "Certifying and removing disparate impact",
            "venue": "In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2015
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario March",
                "Victor Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Arthur Gretton",
                "Olivier Bousquet",
                "Alex Smola",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Measuring statistical dependence with hilbert-schmidt norms",
            "venue": "In International conference on algorithmic learning theory,",
            "year": 2005
        },
        {
            "authors": [
                "Xiaotian Han",
                "Zhimeng Jiang",
                "Hongye Jin",
                "Zirui Liu",
                "Na Zou",
                "Qifan Wang",
                "Xia Hu"
            ],
            "title": "Retiring $\\delta \\text{DP}$: New distribution-level metrics for demographic parity",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nati Srebro"
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Brian Hsu",
                "Rahul Mazumder",
                "Preetam Nandy",
                "Kinjal Basu"
            ],
            "title": "Pushing the limits of fairness impossibility: Who\u2019s the fairest of them all",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ray Jiang",
                "Aldo Pacchiano",
                "Tom Stepleton",
                "Heinrich Jiang",
                "Silvia Chiappa"
            ],
            "title": "Wasserstein fair classification",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Faisal Kamiran",
                "Toon Calders"
            ],
            "title": "Data preprocessing techniques for classification without discrimination",
            "venue": "Knowledge and information systems,",
            "year": 2012
        },
        {
            "authors": [
                "Toshihiro Kamishima",
                "Shotaro Akaho",
                "Hideki Asoh",
                "Jun Sakuma"
            ],
            "title": "Fairness-aware classifier with prejudice remover regularizer",
            "venue": "In Joint European conference on machine learning and knowledge discovery in databases. Springer,",
            "year": 2012
        },
        {
            "authors": [
                "Jon Kleinberg",
                "Sendhil Mullainathan",
                "Manish Raghavan"
            ],
            "title": "Inherent trade-offs in the fair determination of risk scores",
            "venue": "arXiv preprint arXiv:1609.05807,",
            "year": 2016
        },
        {
            "authors": [
                "Jeff Larson",
                "Surya Mattu",
                "Lauren Kirchner",
                "Julia Angwin"
            ],
            "title": "Propublica compas analysis\u2014data and analysis for \u2018machine bias",
            "venue": "https://github.com/propublica/compas-analysis,",
            "year": 2016
        },
        {
            "authors": [
                "Tai Le Quy",
                "Arjun Roy",
                "Vasileios Iosifidis",
                "Wenbin Zhang",
                "Eirini Ntoutsi"
            ],
            "title": "A survey on datasets for fairness-aware machine learning",
            "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,",
            "year": 2022
        },
        {
            "authors": [
                "Peizhao Li",
                "Hongfu Liu"
            ],
            "title": "Achieving fairness at no utility cost via data reweighing with influence",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Zhu Li",
                "Adrian Perez-Suay",
                "Gustau Camps-Valls",
                "Dino Sejdinovic"
            ],
            "title": "Kernel dependence regularizers and gaussian processes with applications to algorithmic fairness",
            "year": 1911
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Christos Louizos",
                "Kevin Swersky",
                "Yujia Li",
                "Max Welling",
                "Richard S Zemel"
            ],
            "title": "The variational fair autoencoder",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "Gilles Louppe",
                "Michael Kagan",
                "Kyle Cranmer"
            ],
            "title": "Learning to pivot with adversarial networks. NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "David Madras",
                "Elliot Creager",
                "Toniann Pitassi",
                "Richard Zemel"
            ],
            "title": "Learning adversarially fair and transferable representations",
            "venue": "International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel McNamara",
                "Cheng Soon Ong",
                "Robert C Williamson"
            ],
            "title": "Costs and benefits of fair representation learning",
            "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2019
        },
        {
            "authors": [
                "Ninareh Mehrabi",
                "Fred Morstatter",
                "Nripsuta Saxena",
                "Kristina Lerman",
                "Aram Galstyan"
            ],
            "title": "A survey on bias and fairness in machine learning",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "Anay Mehrotra",
                "Nisheeth Vishnoi"
            ],
            "title": "Fair ranking with noisy protected attributes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Dana Pessach",
                "Erez Shmueli"
            ],
            "title": "A review on fairness in machine learning",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2022
        },
        {
            "authors": [
                "Charan Reddy",
                "Deepak Sharma",
                "Soroush Mehri",
                "Adriana Romero Soriano",
                "Samira Shabanian",
                "Sina Honari"
            ],
            "title": "Benchmarking bias mitigation algorithms in representation learning through fairness metrics",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks,",
            "year": 2021
        },
        {
            "authors": [
                "Charan Reddy",
                "Deepak Sharma",
                "Soroush Mehri",
                "Adriana Romero-Soriano",
                "Samira Shabanian",
                "Sina Honari"
            ],
            "title": "Benchmarking bias mitigation algorithms in representation learning through fairness metrics",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Salvatore Ruggieri",
                "Jose M Alvarez",
                "Andrea Pugnana",
                "Franco Turini"
            ],
            "title": "Can we trust fair-ai",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Saeed Sharifi-Malvajerdi",
                "Michael Kearns",
                "Aaron Roth"
            ],
            "title": "Average individual fairness: Algorithms, generalization and experiments",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Changjian Shui",
                "Gezheng Xu",
                "Qi Chen",
                "Jiaqi Li",
                "Charles X Ling",
                "Tal Arbel",
                "Boyu Wang",
                "Christian Gagn\u00e9"
            ],
            "title": "On learning fairness and accuracy on multiple subgroups",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mingyang Wan",
                "Daochen Zha",
                "Ninghao Liu",
                "Na Zou"
            ],
            "title": "In-processing modeling techniques for machine learning fairness: A survey",
            "venue": "ACM Transactions on Knowledge Discovery from Data,",
            "year": 2023
        },
        {
            "authors": [
                "Michael Wick",
                "Jean-Baptiste Tristan"
            ],
            "title": "Unlocking fairness: a trade-off revisited",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ruicheng Xian",
                "Lang Yin",
                "Han Zhao"
            ],
            "title": "Fair and optimal classification via post-processing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Muhammad Bilal Zafar",
                "Isabel Valera",
                "Manuel Gomez Rogriguez",
                "Krishna P. Gummadi"
            ],
            "title": "Fairness constraints: Mechanisms for fair classification",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2017
        },
        {
            "authors": [
                "Mike Zajko"
            ],
            "title": "Artificial intelligence, algorithms, and social inequality: Sociological contributions to contemporary debates",
            "venue": "Sociology Compass,",
            "year": 2022
        },
        {
            "authors": [
                "Rich Zemel",
                "Yu Wu",
                "Kevin Swersky",
                "Toni Pitassi",
                "Cynthia Dwork"
            ],
            "title": "Learning fair representations",
            "venue": "In International conference on machine learning,",
            "year": 2013
        },
        {
            "authors": [
                "Brian Hu Zhang",
                "Blake Lemoine",
                "Margaret Mitchell"
            ],
            "title": "Mitigating unwanted biases with adversarial learning",
            "venue": "In AAAI/ACM Conference on AI, Ethics, and Society (AIES),",
            "year": 2018
        },
        {
            "authors": [
                "Guanhua Zhang",
                "Yihua Zhang",
                "Yang Zhang",
                "Wenqi Fan",
                "Qing Li",
                "Sijia Liu",
                "Shiyu Chang"
            ],
            "title": "Fairness reprogramming",
            "venue": "arXiv preprint arXiv:2209.10222,",
            "year": 2022
        },
        {
            "authors": [
                "Zhifei Zhang",
                "Yang Song",
                "Hairong Qi"
            ],
            "title": "Age progression/regression by conditional adversarial autoencoder",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2017
        },
        {
            "authors": [
                "Han Zhao",
                "Geoffrey J. Gordon"
            ],
            "title": "Inherent tradeoffs in learning fair representations",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [],
            "title": "A classifier satisfies equalized odds if the predicted outcome Y is independent of the sensitive attribute S conditioned on the label Y",
            "year": 2016
        },
        {
            "authors": [
                "P. Kingma"
            ],
            "title": "2014) as the optimizer with a learning rate of 0.001 for both tabular and image data. As these objectives, utility and fairness, often present trade-offs, it can be challenging to determine when to stop model training, and this issue is rarely discussed in the previous literature. In this work, we adopted a straightforward stopping strategy based on our experience. We employ a linear decay strategy for the learning",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Machine learning models trained on biased data have been found to perpetuate and even exacerbate the bias against historically underrepresented and disadvantaged demographic groups when deployed (Mehrabi et al., 2021; Pessach & Shmueli, 2022; Caton & Haas, 2020; Wan et al., 2023). As a result, concerns about fairness have gained significant attention, especially as applications of these models expand to high-stakes domains such as criminal justice, hiring process, and credit scoring. To mitigate such algorithmic bias, a variety of fairness criteria and algorithms have been proposed, which impose statistical constraints on the model to ensure equitable treatment under the respective fairness notions (Hsu et al., 2022; Chai et al., 2022; Reddy et al., 2021b). However, a fair and objective comparison between the proposed and existing algorithms to enforce fairness can be difficult due to the following reasons:\n\u2022 Hard to compare the performance of two objectives: utility1 and fairness. Often, there is a trade-off between these two objectives (McNamara et al., 2019; Zhao & Gordon, 2022; Xian et al., 2023). Besides, the instability of the fairness performance of those methods during the training process can complicate the pursuit of an optimal balance (Xian et al., 2023).\n\u2022 Inconsistent experimental settings (Madras et al., 2018; Louizos et al., 2016): Fairness methods can also be hindered by variations in dataset preprocessing and the use of different backbones. These discrepancies can lead to inconsistent (Table 1) and unfair comparison, further complicating the comparison of methods.\n\u2022 Stable and customizable implementation of commonly used fairness methods are not accessible. The fairness methods are often implemented in different programming languages and frameworks, complicating the reproduction and the comparative analysis of various fairness approaches.\n\u2022 Current fairness packages (Bellamy et al., 2018; Bird et al., 2020) and tools often suffer from a lack of extensibility. This can make it difficult for researchers and practitioners to build upon existing methods and develop new ones.\n1We use utility to represent the performance of the downstream tasks, which is commonly used in fairness community (Dwork et al., 2012; Li & Liu, 2022; Baumann et al., 2022).\nThis work aims to facilitate the growth and development of the fairness research community by addressing existing challenges, promoting more accessible and reproducible methods for fairness implementation, and establishing efficient benchmarking techniques. To achieve these goals, we develop a standardized benchmark for evaluating and comparing in-process group fairness methods, which we make open-source. We also conduct comprehensive experiments and analysis of fairness methods. The major contributions of our benchmark are summarized as follows:\n\u2022 Extensible, Minimalistic, and Research-oriented Open-Source Code: We offer open-source implementation for all preprocessing, methods, metrics, and training results, thus facilitating other researchers to utilize and build upon this work. The Fair Fairness Benchmark is publicly available, making it easy for researchers and practitioners to use and contribute to.\n\u2022 Unified Fairness Method Benchmarking Pipelines: Our benchmark includes a unified fairness method development and evaluation pipeline, with three key components: First, we provide a thorough statistical and experimental analysis of widely-used fairness datasets and identity some widely-used datasets unsuitable for studying fairness issues; second, we standardize preprocessing for these datasets, ensuring consistent, comparable evaluations; lastly, we present a range of bias mitigation algorithms and comprehensive evaluation metrics for group fairness.\n\u2022 Comprehensive Benchmarking and Detailed Obsevations: We conduct comprehensive experiments on 14 datasets (each with two sensitive attributes), 6 utility metrics and 9 fairness metrics. We run a total of 45, 079 experiments. Our experiments yield the following key insights: \u2780 Not all widely used fairness datasets stably exhibit fairness issues. (Important for evaluation). \u2781 The HSIC achieves the best utility-fairness trade-off among the tested methods. (Not a popular baseline before) \u2782 Utility-fairness trade-offs are generally controllable by the hyperparameter. (Despite adversarial debiasing method) \u2783 Stopping training while learning rate is lower enough is effective. (Practical strategy).\nScope of this Work. We focus on the problem of in-processing group fairness, which is defined as discrimination against demographic groups. We consider the fairness methods in the context of binary classification and binary sensitive attributes.\nNotation. The dataset is denoted as {(xi, si, yi)Ni=1}, where N is the number of samples. For the sample i in the dataset, xi \u2208 Rd is non-sensitive attributes, si \u2208 {0, 1} represents the binary sensitive attribute, and yi \u2208 {0, 1} is the label of the downstream task. We use y\u0302 \u2208 {0, 1} to denote the predicted label of the downstream task, which is obtained by thresholding the output of a machine learning model f(x) : Rd \u2192 [0, 1] with trainable parameter \u03b8. Accordingly, we use X , Y , S and Y\u0302 to denote the random variables."
        },
        {
            "heading": "2 WHY IS THIS BENCHMARK NEEDED?",
            "text": "Ensuring fairness in algorithmic predictions is crucial in the field of machine learning. However, fairly comparing the current fairness method is challenging due to inconsistencies in data pre-processing and a lack of flexibility in existing fairness packages. This section aims to analyze the critical issue of current fairness methods and discuss the urgent need for a benchmark to address these challenges.\nCurrent fairness packages lack flexibility for researchers. AIF360 (Bellamy et al., 2018) and FairLearn (Bird et al., 2020) are two well-known fairness packages that have successfully mitigated bias in machine learning algorithms. As popular open-source Python packages, they provide practitioners with toolkits for detecting and mitigating bias in their models and evaluating model fairness. AIF360 is a comprehensive fairness toolkit offering a variety of algorithms and metrics for addressing bias in machine learning models. FairLearn also provides multiple algorithms and fairness metrics for assessing model performance. While both packages are highly recognized within the fair machine learning community, they may not give researchers the desired flexibility for research purposes. We provide a more in-depth comparison between AIF360, FairLearn, and FFB in Appendix A.\nInconsistent experimental setting leads to unfair comparison. Prior research has often experienced inconsistencies in data pre-processing and train test split, which has led to divergent performance results that hinder comparison and reproducibility. Minor variations in data preparation and dataset split can significantly impact the performance of machine learning algorithms. Due to these issues, the reported accuracy of tabular data (Adult,German) varies in different papers (Madras et al., 2018; Zemel et al., 2013; Edwards & Storkey, 2015; Feldman et al., 2015; Louizos et al., 2016) shown in\nTable 1. To tackle these issues, we propose a standardized and consistent data pre-processing and split approach, including data normalization, outlier removal, and the implementation of a uniform train-test split ratio.\nSufficient and in-depth analysis of fairness methods is urgently needed. The current fairness method lacks a comprehensive comparison, such as the training curves and stability, the influence of the utility-fairness trade-off parameters, and how to select the best utilityfairness trade-offs during the training process. Our benchmark addresses these shortcomings by offering more in-depth and multifaceted analysis. To present a more thorough understanding of fairness methods, we investigate the training stability, model performance under various fairness constraints, and the selection of best-performing models.\n3 FFB: FAIR FAIRNESS BENCHMARK\nTo overcome the above limitations of the previous methods, we introduce the Fair Fairness Benchmark (FFB), a extensible, minimalistic, and research-oriented fairness benchmark package. Compare to other fairness packages, FFB codebase has the following main characteristics:\n\u2022 Extensible: We provide the source code for fairness methods implementation, allowing researchers to modify, extend, and tailor these methods to suit their specific requirements and implement new ideas upon our code. This fosters a more customizable approach to developing fairness methods.\n\u2022 Minimalistic: We focus on delivering core fairness methods and allowing users to understand the fundamental techniques comprehensively without being overwhelmed by unnecessary complexity. This approach ensures that users can easily implement and integrate our methods into their existing workflows while maintaining a solid grasp of the underlying concepts.\n\u2022 Research-oriented: We include benchmark datasets and evaluation metrics that facilitate assessing fairness methods. This simplifies the research process, allowing researchers to quickly compare and analyze the effectiveness of different methods in various scenarios."
        },
        {
            "heading": "3.1 GROUP FAIRNESS METRICS",
            "text": "To provide a comprehensive comparison for bias mitigating methods, we consider multiple fairness metrics, including demographic parity, p-rule, equality of opportunity, equalized odds, the area between CDF curves, etc. Table 2 presents the fairness metrics used in Section 4 and Section 5."
        },
        {
            "heading": "3.2 BENCHMARKING DATASETS",
            "text": "To provide a comprehensive comparison of fairness methods, we adopted multiple commonly-used fairness datasets (Le Quy et al., 2022) for our experiments, including tabular and image datasets. Table 3 summarizes the datasets used. they are publicly available and can be downloaded from the corresponding websites. We also present the number of features and the ratio of the target label and sensitive attributes. For example, the target label ratio of KDDCensus is 1 : 14.76, which is an extremely unbalanced dataset."
        },
        {
            "heading": "3.3 DATA PREPROCESSING",
            "text": "To ensure a fair comparison and maintain the reproducibility of the fairness approach, we adhere to a conventional data preprocessing strategy. We apply standard normalization for numerical features while employing one-hot encoding to process the categorical features. We also split the data into training and test sets with random seeds. We use the training set to train the model and the test set to evaluate the model\u2019s performance. We use the same data preprocessing strategy for all the datasets."
        },
        {
            "heading": "3.4 BENCHMARKING FAIRNESS METHODS",
            "text": "In this section, we introduce the benchmarking methodology employed in our experiments. The benchmarking methods can be classified into three categories: surrogate loss, independence constraints, and adversarial debiasing techniques. In this paper, we focus on in-processing methods for fairness primarily due to the following: 1) They intervene directly in the learning algorithm to ensure fairness, which provides a more nuanced and effective approach to mitigating bias; 2) The emergence of more in-processing techniques designed in deep neural networks calls for systematic comparison; 3) In-processing methods are susceptible to information leakage since they do not require sensitive attributes during inference. In particular, we consider the following three types of in-processing methods. Gap Regularization (Chuang & Mroueh, 2020) simplifies optimization by offering a smooth approximation to real loss functions, which are often non-convex or difficult to optimize directly. This approach includes DiffDP, DiffEodd, and DiffEopp. Independence introduces fairness constraint into the optimization process to minimize the impact of protected attributes on model predictions while maintaining performance. This approach includes PRemover (Kamishima et al., 2012) and HSIC (Li et al., 2019). Adversarial Learning2 minimizes the utility loss while preventing the adversary from accurately predicting the protected attributes. This approach includes\n2For adversarial learning methods, we use gradient reversal (Ganin et al., 2016) for better training stability.\nAdvDebias (Zhang et al., 2018; Louppe et al., 2017; Beutel et al., 2017; Edwards & Storkey, 2015; Adel et al., 2019) and LAFTR (Madras et al., 2018). The fairness methods are present as follows:\n\u2022 ERM is a standard machine learning method that minimizes the empirical risk of the training data. It is a common baseline for fairness methods.\n\u2022 DiffDP, DiffEopp, DiffEodd are the gap regularization methods for demographic parity, equalized opportunity, and equalized odds (Chuang & Mroueh, 2020). As these fairness definitions cannot be optimized directly, gap regularization is alternative loss and differentiable and can be optimized using gradient descent.\n\u2022 PRemover (Kamishima et al., 2012) (PrejudiceRemover) minimizes the mutual information between the prediction accuracy and the sensitive attributes.\n\u2022 HSIC (Gretton et al., 2005; Baharlouei et al., 2020; Li et al., 2019) minimizes the Hilbert-Schmidt Independence Criterion between the prediction accuracy and the sensitive attributes.\n\u2022 AdvDebias (Zhang et al., 2018; Louppe et al., 2017; Beutel et al., 2017; Edwards & Storkey, 2015; Adel et al., 2019) (adversarial debiasing) maximize a classifier for prediction ability and simultaneously minimizes an adversary to predict sensitive attributes from the predictions.\n\u2022 LAFTR (Madras et al., 2018) is a fair representation learning method that aims to learn an intermediate representation that minimizes the classification loss, reconstruction error, and the adversary\u2019s ability to predict the sensitive attributes from the representation."
        },
        {
            "heading": "4 BIAS EXAMINATION FOR WIDELY USED FAIRNESS BENCHMARK DATASETS",
            "text": "The presence of bias in the current widely used dataset is not well examined and investigated as it should be, even though such bias can significantly influence the assessment of fairness methods. As such, our work endeavors to delve deeper into this issue by exploring the inherent bias in the widely used dataset. We aim to assess the suitability of these datasets for fairness evaluation critically.\n\u2780 Not all widely used fairness datasets stably exhibit fairness issues. We systematically identify datasets that not only have demonstrated bias but are also prevalently used in fairness research. We found that in some cases, the bias in these datasets is either not consistently present or its manifestation varies significantly. This finding indicates that relying on these datasets for fairness analysis might not always provide stable or reliable results, suggesting the need for more rigorous dataset selection or bias evaluation methodologies in future fairness studies. The biased datasets are marked with \" while unbiased ones are with %. TheS\" indicates that the bias exists but with a large standard deviation."
        },
        {
            "heading": "5 BENCHMARKING CURRENT FAIRNESS METHODS",
            "text": "This section presents comprehensive experiments to benchmark the performance of existing inprocessing group fairness methods."
        },
        {
            "heading": "5.1 HOW THE BIAS MITIGATING METHODS PERFORM ON UTILITY-FAIRNESS TRADE-OFFS?",
            "text": "In this section, we present the results of experiments conducted to assess the performance of existing in-processing fairness methods in terms of the utility-fairness trade-offs. We analyze how well these methods balance optimizing utility and ensuring fairness in decision-making. To accurately reflect the performance of the different methods, we aggregate the performance across different datasets in\none figure. To do so, we normalize the utility (acc,auc) and fairness (abcc, dp) performance based on the performance of the ERM. From the results, we make the following major observations:\n\u2781 The utility-fairness performance of the current fairness method exhibits trade-offs. We first present the utility-fairness trade-offs of the existing in-processing fairness methods in Figure 1. We conduct experiments using various in-processing fairness methods and analyze the ability to adjust the trade-offs to cater to specific needs while maintaining a balance between accuracy and fairness.\n\u2782 The HSIC method achieves the best utility-fairness trade-off overall. The HSIC method consistently excels in balancing utility and fairness, outperforming other approaches across our tests. This method, depicted in green in Figure 1, shows particular effectiveness when applied to tabular data. It exhibits a significant ability to improve fairness measures without compromising the precision of utility, maintaining high accuracy in predictions. This quality affirms the robustness of the HSIC method in preserving utility-fairness equilibrium under various conditions. However, when this method is applied to image data, it exhibits a relative performance decline, showing lower fairness and utility scores.\n\u2783 Adversarial debiasing methods exhibit instability. As illustrated in Figure 1, the utility-fairness points representing the AdvDebias method are scattered randomly across the figures, failing to depict a consistent trade-off pattern. This randomness suggests an inherent instability in adversarial debiasing methods. This inconsistency is further demonstrated in subsequent experiments, where the training curves reveal that these methods are difficult to control effectively."
        },
        {
            "heading": "5.2 CAN THE UTILITY-FAIRNESS TRADE-OFFS BE CONTROLLED?",
            "text": "Hereby we investigate the extent to which the utility-fairness trade-offs can be controlled. We conduct experiments using various in-processing fairness methods and analyze the ability of trade-offs.\n\u2784 The utility-fairness trade-offs are generally controllable. The intensity of color in Figure 2 corresponds to the size of the control parameters. With the exception of adversarial debiasing, we find that the performance of most methods can be modulated effectively through the adjustment of hyperparameters. Specifically, larger control hyperparameters tend to yield lower dp and abcc values, indicating enhanced fairness. This implies that the trade-offs between utility and fairness can be actively managed in most cases, providing a crucial degree of flexibility in fairness-oriented data processing tasks. In comparison, the adversarial debiasing method (AdvDebias) is hard to control."
        },
        {
            "heading": "5.3 HOW DO UTILITY AND FAIRNESS PERFORMANCE CHANGE DURING TRAINING PROCESS?",
            "text": "In this section, we thoroughly examine the training curves of existing inprocessing fairness methods, which are not sufficiently explored in previous studies. We conduct a series of experiments to track the evolution of utility and fairness performance throughout the training process and evaluate the impact of these dynamics on the final results. We presented the training curves in Figures 3 and 4 for tabular data and image data, respectively.\n\u2785 The training curves of utility are stable, while those of fairness are unstable. Results on both tabular and image data show that the standard deviation for fairness metrics is significantly larger than that for utility metrics. Among the fairness methods, LAFTR shows the most stable fairness performance. Even though the value\nof fairness metrics is small, the large standard deviation still suggests unstable fairness performance. The results indicate a future direction focused on enhancing fairness training stability.\n\u2786 Stopping training while the learning rate is lower enough proves effective. In our work, we halt the model training when the learning rate diminishes to a value less than 1e\u22125, which is decayed by multiplying by 0.1 every 50 training steps. The utilization of learning rate decay to halt training results in stable fairness metrics, thereby affirming its efficacy and reasonableness."
        },
        {
            "heading": "5.4 EXPERIMENT ON TEXT DATA",
            "text": "In this section, we conducted experiments on text data, specifically on comment toxicity classification using the Jigsaw toxic comment dataset (Jigsaw, 2018). The task is to predict whether a comment is toxic or not. Part of this dataset is labeled with identity attributes, such as gender and race. In our study, we regard race and gender and race as sensitive attributes. We follow the setting in (Chuang & Mroueh, 2020) to transform each comment into a vector using BERT (Devlin et al., 2019). Subsequently, we employ an MLP to predict based on the encoded vector. We present the result on the utility-fairness trade-offs in Figure 5, training process in Figure 6, and controllability of trade-offs in Appendix H.\nFrom these results, we observe a similar trend of utility-fairness trade-offs in text data as in tabular and image data in most cases, but there are some slight differences. We made the following observations: 1) Utility and fairness typically exhibit trade-offs. 2) The training stability has an opposite trend compared to tabular/image data. The training curves for utility are unstable, while those for fairness are relatively unstable for most bias mitigation methods."
        },
        {
            "heading": "6 DISCUSSIONS",
            "text": "This paper introduces Fair Fairness Benchmark (FFB) to benchmark the in-processing group fairness models, offering extensible, minimalistic, and research-oriented open-source code, as well as comprehensive experiments.\nSocial Impact. Our benchmark, with its extensible, minimalistic, and research-oriented opensource code, is designed to facilitate researchers and practitioners to explore and implement fairness methods. Standardized dataset preprocessing and reference baseline implementation will help reduce inconsistencies and make fairness more accessible, especially for beginners in the field. Ultimately, our work aims to stimulate future research on fairness and foster the development of fairness models.\nLimitations. When the ground truth is biased, relying solely on traditional accuracy can be problematic, as mentioned in (Wick et al., 2019; Zajko, 2022; Ruggieri et al., 2023). We employ multiple metrics (accuracy, average precision, AUC) for evaluating \"utility\" instead of just relying on accuracy, aiming to alleviate this problem to some extent.\nFuture work. Our plan for subsequent phases of this work involves extending the scope of the FFB to include a wider range of in-processing group fairness methods. Moreover, we intend to incorporate additional definitions of fairness into our evaluations. Exploring methods to measure unbiased accuracy is a promising direction for future research in fairness.\nREPRODUCIBILITY STATEMENT\nTo ensure the reproducibility of our experiments and benefit the research community, we provide the sample source code in the anonymous link at https://anonymous.4open.science/r/fair_ fairness_benchmark-351D. We will also open-source all the training logs on https://wandb.ai. However, we are currently unable to do so in order to maintain anonymity in the submission."
        },
        {
            "heading": "A RELATED WORK",
            "text": "Algorithmic Fairness Fairness in machine learning has garnered considerable attention in recent years. The goal of fairness in machine learning is to ensure that the machine learning models are fair and unbiased towards an individual or group. Thus, fairness in machine learning can be divided into t categories: group fairness (Dwork et al., 2012; Hardt et al., 2016; Corbett-Davies et al., 2017; Zafar et al., 2017; Madras et al., 2018) and individual fairness (Dwork et al., 2012; Sharifi-Malvajerdi et al., 2019). Group fairness aims to ensure that the machine learning models are fair to different groups of people, while individual fairness aims to \u201csimilar individuals should be treated similarly.\u201d To mitigate fairness and bias problems in machine learning models, bias mitigation methods can be divided into three categories: pre-processing (Kamiran & Calders, 2012; Calmon et al., 2017), in-processing (Kamishima et al., 2012; Zhang et al., 2018; Madras et al., 2018; Zhang et al., 2022; Buyl & De Bie, 2022; Alghamdi et al., 2022; Shui et al., 2022; Mehrotra & Vishnoi, 2022), and post-processing (Hardt et al., 2016; Jiang et al., 2020). Given that the group fairness metrics are widely adopted in real-world applications and the emergence of more in-processing techniques designed in deep neural network models, we focus on benchmarking in-processing methods for group fairness for neural network models for tabular and image data.\nFairness Packages and Benchmarks There are many fairness packages in the literature. Among them, AIF360 (Bellamy et al., 2018) and FairLearn (Bird et al., 2020) are the two most widely used Python packages that provide a set of metrics and algorithms to measure and mitigate bias in machine learning models. They provide a set of metrics to measure the bias of machine learning models, including disparate impact, statistical parity difference, and equal opportunity difference, and a set of algorithms to mitigate the bias of machine learning models. However, both AIF360 and FairLearn implement the bias mitigation algorithms using Scikit-learn (Pedregosa et al., 2011) API design (e.g., the use of fit() function) with complicated class inheritance, making the understanding and direct modification of implementation difficult. In comparison, our benchmark decouples the implementation of different bias mitigation algorithms using Pytorch-style (Paszke et al., 2019) training scripts and provides a unified fairness evaluation interface for a comprehensive list of group fairness metrics. One recently proposed benchmark (Reddy et al., 2021a) also aims to benchmark bias mitigation algorithms. However, their benchmark only includes adversarial learning methods and datasets (e.g., a synthetic dataset and CI-MNIST) without fairness implications and uses dp and eodd as the fairness metrics. In contrast, our benchmark is more comprehensive in terms of algorithms, datasets, and fairness evaluation metrics."
        },
        {
            "heading": "B DETAILS OF THE GROUP FAIRNESS",
            "text": "In this section, we provide the details of the group fairness. We first introduce the definition of group fairness. Then, we introduce the existing group fairness metrics and algorithms.\n\u2022 dp (Demographic Parity or Statistical Parity) (Zemel et al., 2013). A classifier satisfies demographic parity if the predicted outcome Y\u0302 is independent of the sensitive attribute S, i.e., P (Y\u0302 | S = 0) = P (Y\u0302 | S = 1).\n\u2022 prule (Zafar et al., 2017). A classifier satisfies p%-rule if the ratio between the probability of subjects having a certain sensitive attribute value assigned the positive decision outcome and the probability of subjects not having that value also assigned the positive outcome should be no less than p/100, i.e., |P (Y\u0302 = 1 | S = 1)/P (Y\u0302 = 1 | S = 0)| \u2264 p/100.\n\u2022 eopp (Equality of Opportunity) (Hardt et al., 2016). A classifier satisfies equalized opportunity if the predicted outcome Y is independent of the sensitive attribute S when the label Y = 1, i.e., P (Y\u0302 | S = 0, Y = 1) = P (Y\u0302 | S = 1, Y = 1).\n\u2022 eodd (Equalized Odds) (Hardt et al., 2016). A classifier satisfies equalized odds if the predicted outcome Y is independent of the sensitive attribute S conditioned on the label Y , i.e., P (Y\u0302 | S = 0, Y = y) = P (Y\u0302 | S = 1, Y = y), y \u2208 {0, 1}.\n\u2022 acc (Accuracy Parity). A classifier satisfies accuracy parity if the error rates of different sensitive attribute values are the same, i.e., P (Y\u0302 \u0338= Y | S = 0) = P (Y\u0302 \u0338= Y | S = 1), y \u2208 {0, 1}.\n\u2022 aucp (ROC AUC Parity). A classifier satisfies ROC AUC parity if its area under the receiver operating characteristic curve of w.r.t. different sensitive attribute values are the same.\n\u2022 ppv (Predictive Parity Value Parity) A classifier satisfies predictive parity value parity if the probability of a subject with a positive predictive value belonging to the positive class w.r.t. different sensitive attribute values are the same, i.e., P (Y = 1 | Y\u0302 , S = 0) = P (Y = 1 | Y\u0302 , S = 1).\n\u2022 bnegc (Balance for Negative Class). A classifier satisfies balance for the negative class if the average predicted probability of a subject belonging to the negative class is the same w.r.t. different sensitive attribute values, i.e., E[f(X) | Y = 0, S = 0] = E[f(X) | Y = 0, S = 1].\n\u2022 bposc (Balance for Positive Class). A classifier satisfies balance for the negative class if the average predicted probability of a subject belonging to the positive class is the same w.r.t. different sensitive attribute values, i.e., E[f(X) | Y = 1, S = 0] = E[f(X) | Y = 1, S = 1].\n\u2022 abcc (Area Between Cumulative density function Curves) (Han et al., 2023) is proposed to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic groups"
        },
        {
            "heading": "C EXPERIMENTAL SETTING",
            "text": "For tabular datasets, we use a two-layer Multi-layer Perceptron with 256 neurons each for all datasets. We use different bath sizes for different datasets based on the total number of instances of each dataset. For image datasets, we use various neural networks (such as ResNet-18 (He et al., 2016) and ResNet-152) as the backbone. We don\u2019t use weight decay for all datasets and all fairness methods. We use linear learning rate decay for all datasets and all fairness methods. We use Adam (Diederik P. Kingma, 2014) as the optimizer with a learning rate of 0.001 for both tabular and image data. As these objectives, utility and fairness, often present trade-offs, it can be challenging to determine when to stop model training, and this issue is rarely discussed in the previous literature. In this work, we adopted a straightforward stopping strategy based on our experience. We employ a linear decay strategy for the learning rate, halving it every 50 training steps. The model training is stopped when the learning rate decreases to a value below 1e\u22125."
        },
        {
            "heading": "D DETAILS OF THE BENCHMARKING DATASETS",
            "text": "In this section, we provide the details of the benchmarking datasets. We first introduce the benchmarking datasets. Then, we introduce the data preprocessing and data splitting.\n\u2022 Tabular Datasets\n\u2013 Adult3 (Kohavi & Becker, 1996). The Adult dataset is widely used in machine learning and data mining research. It contains 1994 U.S. census data. The task of the dataset is to predict whether a person makes over $50K a year, given an individual\u2019s demographic and financial information. The dataset includes sensitive information such as age and gender. In the literature, gender is mostly used as the (binary) sensitive attribute to evaluate group fairness. \u2013 COMPAS4 (Larson et al., 2016). The COMPAS dataset contains records of criminal defendants, and it is used to predict whether the defendant will recidivate within two years. The dataset includes attributes related to the defendant, such as their criminal history, and demographic information, such as gender and race. \u2013 German5 (Dua & Graff, 2017). The German Credit dataset contains information on individuals who applied for credit at a German bank, including their financial status, credit history, and demographic information (e.g., gender and age). It is used to predict whether an individual should receive a positive or negative credit risk rating. \u2013 Bank6 (Dua & Graff, 2017). The bank marketing dataset is used to analyze the effectiveness of marketing strategies of a Portuguese banking institution by predicting if the client will subscribe to a term deposit. The input variables of the dataset include the bank client\u2019s personal information and other bank marketing activities related to the client. Age was studied as the sensitive attribute in (Zafar et al., 2017). \u2013 KDDCensus7 (Dua & Graff, 2017). Similar to the Adult dataset, the task of the KDD Census dataset is to predict whether the individual\u2019s income is above $50k with more instances. The sensitive attributes are gender and race. \u2013 ACS-I/E/P/M/T8 (Ding et al., 2021). The ACS dataset provides several prediction tasks (e.g., predict whether an individual\u2019s income is above $50K or whether an individual is employed). It is constructed from the American Community Survey (ACS) Public Use Microdata Sample (PUMS). All tasks contain features for race, gender, and other task-related features.\n\u2022 Image Datasets\n\u2013 CelebA-A/W/S9 (Liu et al., 2015). The CelebFaces Attributes dataset comprises 20k face images from 10k celebrities. Each image is annotated with 40 binary labels indicating specific facial attributes such as gender, hair color, and age. \u2013 UTKFace10 (Zhang et al., 2017). The UTKFace dataset is a large-scale face dataset that contains over 20k face images of people from different ethnicities and ages. The images are annotated with age, gender, and ethnicity information.\n\u2022 Text Datasets\n\u2013 Jigsaw11 (Jigsaw, 2018). The CelebFaces Attributes dataset comprises 20k face images from 10k celebrities. Each image is annotated with 40 binary labels indicating specific facial attributes such as gender, hair color, and age.\n3https://archive.ics.uci.edu/ml/datasets/adult 4https://github.com/propublica/compas-analysis 5https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data 6https://archive.ics.uci.edu/dataset/222/bank+marketing 7https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD) 8https://github.com/zykls/folktables 9https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n10https://susanqq.github.io/UTKFace/ 11https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
        },
        {
            "heading": "E DETAILED EXPERIMENTAL SETTINGS",
            "text": "We present the details of the experimental setting in Tables 5 to 7. Table 5 presents the common hyperparameters used by both Tabular and Image datasets, including an initial learning rate of 0.01, Adam as the optimizer, zero weight decay, StepLR as the scheduler with a step of 50, a gamma value of 0.1, and 150 training steps in total. Table 6 presents the range of control hyperparameters used for various fairness methods. Each method has a unique range of these parameters. Table 7 indicates the batch sizes chosen for various datasets during training, ranging from 32 for the German and COMPAS datasets to a large 4096 for the KDDCensus and ACS-I/E/P/M/T datasets, with CelebA-A/W/S and UTKFace datasets using a batch size of 128, which are determined by the number of instances of the datasets.\nF MORE EXPERIMENT RESULTS ON Adult\nIn this appendix, we present the experimental results on Adult datasets. The same experiment results are presented in http://TODO.\nF.1 UTILITY-FAIRNESS TRADE-OFFS\nWe plot the utility-fairness trade-offs for the Adult dataset with gender as the sensitive attribute and present the results in Figures 7 and 8.\nF.2 TRAINING CURVES AND HYPERPARAMETERS FOR CONTROLLING FAIRNESS\nWe plot the utility and fairness training curves for varying fairness control hyperparameters on the Adult dataset, and present the results in Figure 9. The intensity of the color represents the size of the control parameters. In most cases, the larger value of control parameters yields better fairness performance, while small ones have worse fairness performance.\nG MORE EXPERIMENT RESULTS ON CelebA-A\nIn this appendix, we present the experimental results on the CelebA-A dataset.\nG.1 UTILITY-FAIRNESS TRADE-OFFS\nWe plot the utility-fairness trade-offs for the CelebA-A dataset with gender as the sensitive attribute and present the results in Figure 10. The results show the utility-fairness trade-offs in CelebA-A dataset.\nG.2 TRAINING CURVES AND HYPERPARAMETERS FOR CONTROLLING FAIRNESS\nWe plot the utility and fairness training curves for varying fairness control hyperparameters on the CelebA-A dataset, and present the results in Figure 11. The intensity of the color represents the size of the control parameters. In most cases, the larger value of control parameters yields better fairness performance, while small ones have worse fairness performance.\nH MORE EXPERIMENT RESULTS ON Jigsaw\nWe present the results on fairness performance using various fairness control hyperparameters. We observed that the fairness metrics abcc and dp cannot be influenced by the fairness control hyperparameters, whereas prule can.\nH.1 HOW DOES MODEL SIZE INFLUENCE FAIRNESS PERFORMANCE?\nWe conducted experiments to explore the influence of model size on fairness performance. We use various neural networks with the number of neural network trainable parameters spanning from 11.6M to 126.9M.12 The results are presented in Figure 13.\n\u2787 The architecture does not significantly influence fairness performance. An increase in neural network parameters does not yield significant changes in utility or fairness performance. This suggests larger models do not naturally mitigate that dataset bias. The results observed from the DiffDP method indicate a potential correlation between utility performance and bias: fairness performance may deteriorate as utility increases, exhibiting trade-offs between them across different models.\n12ResNet-18 (11.6M), ResNet-34 (21.8M), ResNet-50 (25.6M), ResNet-101 (44.5M), ResNet-152 (60.2M), ResNext-50 (25.0M), ResNext101 (88.8M), wide_ResNet-50 (68.9M), and wide_ResNet101 (126.9M).\nI IMPLEMENTATION COMPARISON WITH AIF360 AND FAIRLEARN\nIn this section, we provide the implementations of adversarial debiasing in AIF360, FairLearn, and FFB to demonstrate FFB are extensible, minimalistic, and research-oriented compared to existing fairness packages. Algorithm 1, Algorithm 2, and Algorithm 3 show the implementation of adversarial debiasing in AIF360, FairLearn, and FFB, respectively.\nWe can see that both AIF360 and FairLearn use Scikit-learn API design (e.g., the use of fit() function), whereas FFB use Pytorch-style implementation, which provides a unified data preprocessing pipeline and fairness evaluation interface in a single script. Thus, researchers using FFB can use the bias mitigation method and reproduce the experimental results using one line of command. Additionally, AIF360 and FairLearn use complicated class inheritance (e.g., AdversarialFairnessClassifier in FairLearn inherents AdversarialFairness and ClassifierMixin), AdversarialDebiasing in AIF360 inherents Transformer), and other external dependencies (e.g., AdversarialFairness in FairLearn uses backendEngine_ to implement the training step), making the implementation hard to read. This makes researchers hard to understand and re-implement the bias mitigation methods.\nAlgorithm 1 AdvDebias in AIF360\nclass AdversarialDebiasing(Transformer):\ndef __init__(self): ...\ndef _classifier_model(self, features, features_dim, keep_prob): ... # deine classifier\ndef _adversary_model(self, pred_logits, true_labels): ... # deine adversary model\ndef predict(self, dataset): ...\ndef fit(self, dataset):\nwith tf.variable_scope(self.scope_name):\n# tf graph construction ...\nself.sess.run(tf.global_variables_initializer()) self.sess.run(tf.local_variables_initializer())\nfor epoch in range(self.num_epochs): # training for i in range(num_train_samples//self.batch_size):\nbatch_feed_dict = {self.features_ph: batch_features, self.true_labels_ph: batch_labels, self.protected_attributes_ph: batch_protected_attributes, self.keep_prob: 0.8} if self.debias: _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([\nclassifier_minimizer, adversary_minimizer, pred_labels_loss, pred_protected_attributes_loss], feed_dict=batch_feed_dict)\nif i % 200 == 0: ... # logging\nelse: _, pred_labels_loss_value = self.sess.run(\n[classifier_minimizer, pred_labels_loss], feed_dict=batch_feed_dict)\nif i % 200 == 0: ... # logging\nAlgorithm 2 AdvDebias in FairLearn\nclass _AdversarialFairness(BaseEstimator):\ndef __init__(self): ...\ndef __setup(self, X, Y, A): ...\ndef fit(self, X, y, *, sensitive_features=None): X, Y, A = self._validate_input(X, y, sensitive_features, reinitialize=True)\n...\nfor epoch in range(epochs): batch_slice = slice(\nbatch * batch_size, min((batch + 1) * batch_size, X.shape[0]),\n) (LP, LA) = self.backendEngine_.train_step(\nX[batch_slice], Y[batch_slice], A[batch_slice] ) predictor_losses.append(LP) adversary_losses.append(LA)\n...\ndef partial_fit(self, X, y, *, sensitive_features=None): ...\ndef decision_function(self, X): ...\ndef predict(self, X): ...\ndef _validate_input(self, X, Y, A, reinitialize=False): ...\ndef _validate_backend(self): ...\ndef _set_predictor_function(self): ...\nclass AdversarialFairnessClassifier(_AdversarialFairness, ClassifierMixin): def __init__(self):\n\"\"\"Initialize model by setting the predictor loss and function.\"\"\" self._estimator_type = \"classifier\" super(AdversarialFairnessClassifier, self).__init__()\nAlgorithm 3 AdvDebias in FFB\nclass Adversary(nn.Module): ...\nclass MLP(nn.Module): ...\ndef test(model, test_loader, criterion, device, args=None): ...\ndef train(clf, adv, data_loader, clf_criterion, adv_criterion, clf_optimizer, adv_optimizer): ...\nif __name__ == \u2019__main__\u2019: parser = argparse.ArgumentParser() parser.add_argument(\u2019--dataset\u2019, type=str, default=\"adult\") parser.add_argument(\u2019--model\u2019, type=str, default=\"MLP\") ... args = parser.parse_args()\n# Dataset selection if args.dataset == \"adult\":\nX, y, s = load_adult_data(sensitive_attribute=args.sensitive_attr) elif args.dataset == \"compas\":\nX, y, s = load_compas_data( sensitive_attribute=args.sensitive_attr) ...\n# Unified Dataset preprocessing (e.g., train/test split, ) ...\n# define network architecture, etc. optimizer clf = MLP(n_features=n_features, num_classes=1, mlp_layers=[512, 256, 64]).to(device) clf_criterion = nn.BCELoss() clf_optimizer = optim.Adam( clf.parameters(), lr=args.lr)\nadv = Adversary( n_sensitive = 1 ).to(device) adv_criterion = nn.BCELoss(reduction=\"mean\") adv_optimizer = optim.Adam(adv.parameters(), lr=args.lr)\nfor epoch in range(1, args.num_epochs+1): # begin training train(clf, adv, train_loader, clf_criterion, adv_criterion, clf_optimizer, adv_optimizer)\nif epoch % args.logging_steps == 0 or epoch == args.num_epochs: test_metrics = test(model=clf, test_loader=test_loader, criterion=clf_criterion, device=device) # logging metrics ..."
        }
    ],
    "year": 2023
}