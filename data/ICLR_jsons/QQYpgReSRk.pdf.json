{
    "abstractText": "We present MOFI1, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: (i) pre-training data, and (ii) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. It\u2019s a simple, cost-effective method that can scale to handle billions of web-mined image-text pairs. Through this method, we have created Image-to-Entities (I2E), a new dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes like supervised pre-training, contrastive pre-training, and multi-task learning. For constrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI\u2019s CLIP model. Further experiments on zeroshot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-text data, demonstrating the effectiveness of the I2E dataset in learning strong image representations.",
    "authors": [],
    "id": "SP:ce2b671700f13fa1f8f1808701bfa87cb9040fe7",
    "references": [
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Songhao Piao",
                "Furu Wei"
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "arXiv preprint arXiv:2106.08254,",
            "year": 2021
        },
        {
            "authors": [
                "Liangliang Cao",
                "Bowen Zhang",
                "Chen Chen",
                "Yinfei Yang",
                "Xianzhi Du",
                "Wencong Zhang",
                "Zhiyun Lu",
                "Yantao Zheng"
            ],
            "title": "Less is more: Removing text-regions improves clip training efficiency and robustness",
            "venue": "arXiv preprint arXiv:2305.05095,",
            "year": 2023
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Chen",
                "Bowen Zhang",
                "Liangliang Cao",
                "Jiguang Shen",
                "Tom Gunter",
                "Albin Madappally Jose",
                "Alexander Toshev",
                "Jonathon Shlens",
                "Ruoming Pang",
                "Yinfei Yang"
            ],
            "title": "Stair: Learning sparse text and image representation in grounded tokens",
            "venue": "arXiv preprint arXiv:2301.13081,",
            "year": 2023
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jeff Donahue",
                "Yangqing Jia",
                "Oriol Vinyals",
                "Judy Hoffman",
                "Ning Zhang",
                "Eric Tzeng",
                "Trevor Darrell"
            ],
            "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
            "venue": "In ICML,",
            "year": 2014
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Dimakis",
                "Jenia Jitsev",
                "Yair Carmon",
                "Vaishaal Shankar",
                "Ludwig Schmidt"
            ],
            "title": "Datacomp: In search of the next generation of multimodal datasets",
            "venue": "arXiv preprint arXiv:2304.14108,",
            "year": 2023
        },
        {
            "authors": [
                "Zhe Gan",
                "Linjie Li",
                "Chunyuan Li",
                "Lijuan Wang",
                "Zicheng Liu",
                "Jianfeng Gao"
            ],
            "title": "Vision-language pre-training: Basics, recent advances, and future trends",
            "venue": "Foundations and Trends\u00ae in Computer Graphics and Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Deepti Ghadiyaram",
                "Du Tran",
                "Dhruv Mahajan"
            ],
            "title": "Large-scale weakly-supervised pre-training for video action recognition",
            "year": 2019
        },
        {
            "authors": [
                "Ross Girshick",
                "Jeff Donahue",
                "Trevor Darrell",
                "Jitendra Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "In CVPR,",
            "year": 2014
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "year": 2021
        },
        {
            "authors": [
                "Da-Cheng Juan",
                "Chun-Ta Lu",
                "Z. Li",
                "Futang Peng",
                "Aleksei Timofeev",
                "Yi-Ting Chen",
                "Yaxi Gao",
                "Tom Duerig",
                "Andrew Tomkins",
                "Sujith Ravi"
            ],
            "title": "Graph-rise: Graph-regularized image semantic embedding",
            "year": 1902
        },
        {
            "authors": [
                "Janghyeon Lee",
                "Jongsuk Kim",
                "Hyounguk Shon",
                "Bumsoo Kim",
                "Seung Hwan Kim",
                "Honglak Lee",
                "Junmo Kim"
            ],
            "title": "Uniclip: Unified framework for contrastive language-image pre-training",
            "venue": "arXiv preprint arXiv:2209.13430,",
            "year": 2022
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Object-semantics aligned pre-training for vision-language",
            "year": 2020
        },
        {
            "authors": [
                "Yangguang Li",
                "Feng Liang",
                "Lichen Zhao",
                "Yufeng Cui",
                "Wanli Ouyang",
                "Jing Shao",
                "Fengwei Yu",
                "Junjie Yan"
            ],
            "title": "Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm",
            "venue": "arXiv preprint arXiv:2110.05208,",
            "year": 2021
        },
        {
            "authors": [
                "Yanghao Li",
                "Haoqi Fan",
                "Ronghang Hu",
                "Christoph Feichtenhofer",
                "Kaiming He"
            ],
            "title": "Scaling languageimage pre-training via masking",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Dhruv Mahajan",
                "Ross Girshick",
                "Vignesh Ramanathan",
                "Kaiming He",
                "Manohar Paluri",
                "Yixuan Li",
                "Ashwin Bharambe",
                "Laurens Van Der Maaten"
            ],
            "title": "Exploring the limits of weakly supervised pretraining",
            "year": 2018
        },
        {
            "authors": [
                "Carlos Roig Mari",
                "David Varas Gonzalez",
                "Elisenda Bou-Balust"
            ],
            "title": "Multi-scale transformer-based feature combination for image retrieval",
            "venue": "In ICIP,",
            "year": 2022
        },
        {
            "authors": [
                "Norman Mu",
                "Alexander Kirillov",
                "David Wagner",
                "Saining Xie"
            ],
            "title": "Slip: Self-supervision meets language-image pre-training",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Musgrave",
                "Serge Belongie",
                "Ser-Nam Lim"
            ],
            "title": "A metric learning reality check",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Maximillian Nickel",
                "Douwe Kiela"
            ],
            "title": "Poincar\u00e9 embeddings for learning hierarchical representations",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Maxime Oquab",
                "Timoth\u00e9e Darcet",
                "Th\u00e9o Moutakanni",
                "Huy Vo",
                "Marc Szafraniec",
                "Vasil Khalidov",
                "Pierre Fernandez",
                "Daniel Haziza",
                "Francisco Massa",
                "Alaaeldin El-Nouby"
            ],
            "title": "Dinov2: Learning robust visual features without supervision",
            "venue": "arXiv preprint arXiv:2304.07193,",
            "year": 2023
        },
        {
            "authors": [
                "Hieu Pham",
                "Zihang Dai",
                "Golnaz Ghiasi",
                "Kenji Kawaguchi",
                "Hanxiao Liu",
                "Adams Wei Yu",
                "Jiahui Yu",
                "Yi-Ting Chen",
                "Minh-Thang Luong",
                "Yonghui Wu",
                "Mingxing Tan",
                "Quoc V. Le"
            ],
            "title": "Combined scaling for zero-shot transfer learning",
            "venue": "arXiv preprint arXiv:2111.10050,",
            "year": 2023
        },
        {
            "authors": [
                "Filip Radenovic",
                "Ahmet Iscen",
                "Giorgos Tolias",
                "Yannis Avrithis",
                "Ond\u0159ej Chum"
            ],
            "title": "Revisiting oxford and paris: Large-scale image retrieval benchmarking",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "NeurIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "year": 2015
        },
        {
            "authors": [
                "Konstantin Schall",
                "Kai Uwe Barthel",
                "Nico Hezel",
                "Klaus Jung"
            ],
            "title": "Gpr1200: A benchmark for general-purpose content-based image retrieval",
            "venue": "arXiv preprint arXiv:2111.13122,",
            "year": 2021
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki"
            ],
            "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
            "venue": "arXiv preprint arXiv:2111.02114,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Sharif Razavian",
                "Hossein Azizpour",
                "Josephine Sullivan",
                "Stefan Carlsson"
            ],
            "title": "Cnn features off-the-shelf: an astounding baseline for recognition",
            "venue": "In CVPR workshops,",
            "year": 2014
        },
        {
            "authors": [
                "Sheng Shen",
                "Chunyuan Li",
                "Xiaowei Hu",
                "Yujia Xie",
                "Jianwei Yang",
                "Pengchuan Zhang",
                "Zhe Gan",
                "Lijuan Wang",
                "Lu Yuan",
                "Ce Liu"
            ],
            "title": "K-lite: Learning transferable visual models with external knowledge",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Mannat Singh",
                "Laura Gustafson",
                "Aaron Adcock",
                "Vinicius de Freitas Reis",
                "Bugra Gedik",
                "Raj Prateek Kosaraju",
                "Dhruv Mahajan",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Laurens Van Der Maaten"
            ],
            "title": "Revisiting weakly supervised pre-training of visual perception models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Mannat Singh",
                "Laura Gustafson",
                "Aaron Adcock",
                "Vinicius De Freitas Reis",
                "Bugra Gedik",
                "Raj Prateek Kosaraju",
                "Dhruv Mahajan",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Laurens Van Der Maaten"
            ],
            "title": "Revisiting weakly supervised pre-training of visual perception models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Sun",
                "Abhinav Shrivastava",
                "Saurabh Singh",
                "Abhinav Gupta"
            ],
            "title": "Revisiting unreasonable effectiveness of data in deep learning era",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Hao Wang",
                "Yitong Wang",
                "Zheng Zhou",
                "Xing Ji",
                "Zhifeng Li",
                "Dihong Gong",
                "Jin Zhou",
                "Wei Liu"
            ],
            "title": "Cosface: Large margin cosine loss for deep face recognition",
            "year": 2018
        },
        {
            "authors": [
                "Floris Weers",
                "Vaishaal Shankar",
                "Angelos Katharopoulos",
                "Yinfei Yang",
                "Tom Gunter"
            ],
            "title": "Self supervision does not help natural language supervision at scale",
            "venue": "arXiv preprint arXiv:2301.07836,",
            "year": 2023
        },
        {
            "authors": [
                "Chen Wei",
                "Haoqi Fan",
                "Saining Xie",
                "Chao-Yuan Wu",
                "Alan Yuille",
                "Christoph Feichtenhofer"
            ],
            "title": "Masked feature prediction for self-supervised visual pre-training",
            "year": 2022
        },
        {
            "authors": [
                "Tobias Weyand",
                "Araujo Araujo",
                "Bingyi Cao",
                "Jack Sim"
            ],
            "title": "Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval",
            "venue": "In Proc. CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Bichen Wu",
                "Ruizhe Cheng",
                "Peizhao Zhang",
                "Peter Vajda",
                "Joseph E Gonzalez"
            ],
            "title": "Data efficient language-supervised zero-shot recognition with optimal transport distillation",
            "venue": "arXiv preprint arXiv:2112.09445,",
            "year": 2021
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X. Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "year": 2018
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Bin Xiao",
                "Ce Liu",
                "Lu Yuan",
                "Jianfeng Gao"
            ],
            "title": "Unified contrastive learning in image-text-label space",
            "year": 2022
        },
        {
            "authors": [
                "Min Yang",
                "Dongliang He",
                "Miao Fan",
                "Baorong Shi",
                "Xuetong Xue",
                "Fu Li",
                "Errui Ding",
                "Jizhou Huang"
            ],
            "title": "Dolg: Single-stage image retrieval with deep orthogonal fusion of local and global features",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Lewei Yao",
                "Runhui Huang",
                "Lu Hou",
                "Guansong Lu",
                "Minzhe Niu",
                "Hang Xu",
                "Xiaodan Liang",
                "Zhenguo Li",
                "Xin Jiang",
                "Chunjing Xu"
            ],
            "title": "Filip: fine-grained interactive language-image pre-training",
            "venue": "arXiv preprint arXiv:2111.07783,",
            "year": 2021
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Legg Yeung",
                "Mojtaba Seyedhosseini",
                "Yonghui Wu"
            ],
            "title": "Coca: Contrastive captioners are image-text foundation models",
            "year": 1917
        },
        {
            "authors": [
                "Lu Yuan",
                "Dongdong Chen",
                "Yi-Ling Chen",
                "Noel Codella",
                "Xiyang Dai",
                "Jianfeng Gao",
                "Houdong Hu",
                "Xuedong Huang",
                "Boxin Li",
                "Chunyuan Li"
            ],
            "title": "Florence: A new foundation model for computer vision",
            "venue": "arXiv preprint arXiv:2111.11432,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Joan Puigcerver",
                "Alexander Kolesnikov",
                "Pierre Ruyssen",
                "Carlos Riquelme",
                "Mario Lucic",
                "Josip Djolonga",
                "Andre Susano Pinto",
                "Maxim Neumann",
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Olivier Bachem",
                "Michael Tschannen",
                "Marcin Michalski",
                "Olivier Bousquet",
                "Sylvain Gelly",
                "Neil Houlsby"
            ],
            "title": "A large-scale study of representation learning with the visual task adaptation benchmark",
            "venue": "arXiv preprint arXiv:1910.04867,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Alexander Kolesnikov",
                "Neil Houlsby",
                "Lucas Beyer"
            ],
            "title": "Scaling vision transformers",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Xiao Wang",
                "Basil Mustafa",
                "Andreas Steiner",
                "Daniel Keysers",
                "Alexander Kolesnikov",
                "Lucas Beyer"
            ],
            "title": "Lit: Zero-shot transfer with locked-image text tuning",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Xiao Wang",
                "Basil Mustafa",
                "Andreas Steiner",
                "Daniel Keysers",
                "Alexander Kolesnikov",
                "Lucas Beyer"
            ],
            "title": "Lit: Zero-shot transfer with locked-image text tuning",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Jinghao Zhou",
                "Li Dong",
                "Zhe Gan",
                "Lijuan Wang",
                "Furu Wei"
            ],
            "title": "Non-contrastive learning meets language-image pre-training",
            "venue": "In CVPR,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Over the past decade, the research community has devoted significant efforts to studying the acquisition of high-quality, general-purpose image representations (Donahue et al., 2014; Sun et al., 2017; Juan et al., 2019; Dosovitskiy et al., 2021). An effective image representation can yield impressive results on downstream tasks such as image classification and image retrieval across various domains, without requiring further customization.\nArguably, the most classical image representation learning method is based on supervised image classification (Deng et al., 2009; Sun et al., 2017), often using datasets like ImageNet and ImageNet21K (Deng et al., 2009). However, these datasets usually require expensive and difficult human labeling of precise class labels, which makes them less scalable. While some industrial labs have created large classification datasets using semi-automatic pipelines like JFT (Sun et al., 2017) or private data sources like IG hashtags (Singh et al., 2022b), how to further scale the datasets remains a challenge for the research community. Another prevailing approach to learn general image representations is leveraging the weakly supervised signals from text, which is easier to acquire and scale. For instance, state-of-the-art models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) learn from billions of web-mined image-text pairs using a contrastive learning objective. Such pre-trained models can achieve strong zero-shot generalization results on various downstream tasks including image-text retrieval and image classification.\nDespite the great success of CLIP and ALIGN, they have not been able to explore the classification objective due to the typically varying associated text for each image. However, recent studies have demonstrated that incorporating supervised data (Pham et al., 2023; Zhai et al., 2022b) or improving\n1Manifold OF Images.\nModel GPR1200 ImageNet-ZSmAP@all (%) Acc@1 (%)\nCLIP-L/14OpenAI 72.19 75.27 MOFI-L/14 86.15 77.17 MOFI-H/14 86.66 78.46\n(a) Comparison of MOFI and CLIP (Radford et al., 2021) on GPR1200 image retrieval and ImageNet zero-shot classification tasks.\nDataset # Images # Classes\nImageNet-1K 1.2M 1K ImageNet-21K 14M 21K JFT-300M 300M 18K JFT-3B 3B 30K IG-3.6B 3.6B 27K\nI2E (Ours) 1.1B 2M\n(b) I2E and existing large image classification datasets.\nFigure 1: MOFI is trained on the new Image-to-Entities (I2E) dataset, which has 66x more classes than the previous datasets, and achieves significantly better performance on the image retrieval tasks.\ndata quality (Gadre et al., 2023; Cao et al., 2023) can enhance the performance of contrastive models. With these motivations in mind, we (i) investigate the potential of extracting entity labels from noisy image-text pairs, and (ii) training models to learn from these extracted labels.\nFirst, we present a simple approach to automatically label images with entities at scale. Our method leverages existing noisy image-text datasets used for CLIP training. Given an image-text pair, a named entity recognition model is first applied to extract entities from the text. Each extracted entity is then paired with the original image and scored by a pre-trained CLIP model, and those image-entity pairs with low CLIP scores are filtered out. The constructed dataset, termed Image-to-Entities (I2E), consists of 1.1B images with 2M unique entities. To our best knowledge, I2E has the largest number of class labels documented thus far, 66 times more than JFT-3B (Zhai et al., 2022a) and IG-3.6B datasets (Singh et al., 2022a) (Table 1b). Compared with original noisy image-text data, entity labels contain more structured knowledge, which can potentially lead to better pre-trained models.\nWe study different training recipes to learn from the I2E dataset, including supervised pre-training, contrastive pre-training (CLIP), and multi-task learning. For the latter two, we treat entity names as free-form text and add entity descriptions of the entity to the text. The models are first evaluated on the image retrieval2 benchmark GPR1200 (Schall et al., 2021), and a modified image retrieval task from ImageNet. Experimental results show that the CLIP model trained on the I2E data significantly outperforms the model trained on the original image-text data. Changing the training objective to supervised classification boosts performance even more. This shows both the I2E data and the classification objective are very effective for image retrieval tasks. The multi-task model reaches a new state-of-the-art of 86.15% mAP@all on GPR1200, beating the previous record of 72.19% from OpenAI\u2019s CLIP model (Table 1a). We observe a similar performance gain on the ImageNet image retrieval task. Given its strong performance on image retrieval, we name the multi-task model MOFI, standing for Manifold OF Images.\nWe further evaluate the models on standard ImageNet (Deng et al., 2009) and VTAB (Zhai et al., 2020) image classification tasks.3 MOFI trained on the I2E data performs strongly compared to the CLIP model trained on the original image-text data. Specifically, for the ViT B/16 architecture (Dosovitskiy et al., 2021), MOFI achieves 72.99% zero-shot and 81.32% linear probe top-1 accuracy on ImageNet, significantly outperforming CLIP by 4.27% and 1.78%, respectively (evidenced later in Table 3 and 4 ). It also achieves the best linear probe and competitive zero-shot performance on VTAB tasks, with significant improvements on fine-grained recognition tasks like OxPet and OxFlowers.\nOur contributions are summarized as follows.\n\u2022 In terms of data, we introduce Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. \u2022 In terms of model training, we study various learning approaches from the constructed I2E dataset, including supervised pre-training, contrastive pre-training, and multi-task learning. \u2022 In terms of performance, we advance the image retrieval SoTA on the GPR1200 dataset by a significant margin, and show that learning from the I2E data leads to strong image representations, with improved zero-shot performance on ImageNet and VTAB benchmarks.\n2Image retrieval by default means retrieving images that are similar to a query image, which is different from the image-text retrieval tasks used to evaluate CLIP. Image based retrieval has wide industrial use cases.\n3For VTAB evaluation, we employ the 8 tasks from nature and specialized categories."
        },
        {
            "heading": "2 NOISY IMAGE-TO-ENTITIES DATA AT WEB SCALE",
            "text": "In this section, we introduce our new approach to construct the Image-to-Entities (I2E) dataset."
        },
        {
            "heading": "2.1 METHOD",
            "text": "Image-text datasets are easily accessible through large-scale web crawled corpus. This has been widely used for training vision foundation models such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). The texts are typically gathered from webpages, e.g., image alt-text and page titles, and are often unstructured and in free-form. We build the structured I2E dataset on top of this. At a high level, the steps are: 1. Construct image-text dataset from crawled web corpus; 2. Extract entities from text; 3. Entity filtering; 4. Sample the resulting dataset. Next, we describe each step in detail.\nConstructing image-text dataset. Following CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), we remove pornographic images based on a NSFW model and images whose shorter dimension is less than 200 pixels from crawled web corpus. The images are further deduplicated based on image hash. For each selected image, we select both alt-text and page title as text candidates. The text candidates are further filtered based on text length, full text frequency, unigram and bigram frequency, etc. Similar to Schuhmann et al. (2021), we use OpenAI CLIP-B/16 model to compute image and text embeddings, and compute the cosine similarity between them and remove image-text pairs below a threshold of 0.24. From our crawled web corpus, we constructed a large-scale image-text dataset which contains 8.4B image-text pairs. As one image may have multiple texts, it contains 5.5B images.\nExtracting entities from text. In order to extract entities from text which can be used as labels for model pre-training, we not only need to locate the named entities in the text (via named entity recognition), but also need to assign a unique identifier to each entity (via named entity linking).\nTo do this, we first find all possible entity candidate based on all n-grams and compute its probability based on its popularity from wikipedia. As entities may have ambiguity if purely based on text, e.g., Apple company vs. Apple fruit, we mitigate this problem by using other entities in the same text. Specifically, we first pre-compute an embedding for every entity as described later. For each candidate, we update its probability in an iterative way until convergence. In each iteration, we first compute the embedding for the full text by combining all entity candidates\u2019 embedding based on its probability, then update the probability for each candidate based on its distance to the full text embedding. In the end, we select the best candidate with the highest probability.\nSimilar to Nickel & Kiela (2017), the entity embeddings are hyperbolic graph embedding trained on the Wikipedia link graph and click stream data. The loss is to minimize the distance between two entities occurring together on Wikipedia. For simplicity, we utilize the Wikidata knowledge base and its associated identifiers for items. This approach has reasonable precision and recall, and can be run efficiently at large scale. We are able to annotate 8.4B texts in less than 10 hours with 256 machines, each has 30GB memory.\n4Entities can be found through https://www.wikidata.org/wiki/[Entity_id].\nEntity filtering. The entities extracted from text may not be always related to the corresponding image: We may not be able to disambiguate entities when there is not enough context, especially when most texts are short. Even if the entities are correctly extracted from text, it may not be always relevant to the image. Take an image of Honda Civic with text \u201cUsed Honda Civic for Sale in Los Angeles, CA\u201d as an example. The text is considered as relevant based on the image-text embedding, and we can extract two entities from the text, Honda Civic and Los Angeles, both are correct from the text perspective, but clearly the latter is not relevant to the image.\nIn order to reduce the noise in the dataset, we use CLIP model to compute the CLIP embedding for the text representation of entities, and filter out the entities which have lower cosine similarity with the image embedding. To further leverage external knowledge for enhanced performance (Shen et al., 2022), the entity names are further enriched with entity descriptions, e.g., from Wikidata. Therefore, the text representation of an entity is formed as entity name, entity description.\nSampling the resulting dataset. After filtering, the dataset contains 1.24B images with around 3.1M entities. Although it contains a large number of fine-grained entities, the distribution of number of images per entity is highly skewed to the popular entities (see Table 1). To ease pre-training, we removed all the images associated with entities which have less than 5 images.\n2.2 STATISTICS\nTable 1: Distribution of number of images per entity. Note that entities in the [0, 5) range are removed in our final dataset.\nRange # Entities\n[0, 5) 955, 651 [5, 10) 418, 895 [10, 100) 1, 127, 198 [100, 1000) 506, 753 [1000, 10000) 117, 802 [10000, inf) 18, 768\nThe constructed dataset contains 1.1B images with 2.1M entities. To our knowledge, this is one of the largest weakly labelled datasets available, with the largest number of entities, approximately 66 times more than JFT-3B (Zhai et al., 2022a) and IG-3.6B (Singh et al., 2022a), and around 100 time more than ImageNet-21k (Deng et al., 2009) and JFT-300M (Sun et al., 2017).\nTo differentiate the constructed dataset with the original imagetext dataset, we call the new dataset I2E (Image-to-Entities), and the original dataset I2T (Image-to-Text) in the rest of the paper. The distribution of number of images per entity is shown in Table 1. It is highly skewed to popular entities. We observe that the apparel entities are the most popular ones, e.g., Q131151(T-shirt) and Q83363(jeans) have 4.7M and 2.3M images, respectively. The tail entities with less than 10 images include Q1070890(Georgian Civil War), Q1030323(Pearled Treerunner), and Q10320201(Lucas Gau\u0301cho). Examples of the I2E dataset are provided in Figure 2."
        },
        {
            "heading": "3 LEARNING FROM IMAGE-TO-ENTITIES DATA",
            "text": "In this section, we describe how the I2E dataset is used for learning general-purpose image representations. We explore three types of learning approaches, as illustrated in Figure 3."
        },
        {
            "heading": "3.1 SUPERVISED PRE-TRAINING",
            "text": "The image classification task is one of the simplest, but very effective methods of using labeled data for learning image representations. In our context, each entity is considered to be a separate label, and the classification task is to predict which labels correspond to the given image.\nThere are multiple choices of loss functions which can be used to promote embedding properties such as separability etc. In experiments, we use the large margin cosine loss in Wang et al. (2018), as it was shown to be simple and effective, on par with other more complicated methods (Musgrave et al., 2020). Since the I2E dataset has an immense number of entities (over 2 million), predicting all the entities in each batch is computationally costly. Similar to sampled softmax (Tensorflow Authors), a fixed number of entities is used for each batch - entities of the in-batch images plus entities randomly sampled to be used as additional negatives. The exact number of entities used for each batch was selected based on a trade-off between performance and quality5. Formally,\nLclass = \u2211 k\u2208B log e(\u27e8e im k ,wck \u27e9\u2212m)/t e(\u27e8e im k ,wck \u27e9\u2212m)/t + \u2211 c\u2208Ck e \u27e8eimk ,wc\u27e9/t , (1)\nwhere eimk denotes the image embedding for the k-th sample, with ck the corresponding class label. Ck = C\n\u2032 \\{ck}, C \u2032 = Cbatch\u222aCrandom, Cbatch = {ci|i \u2208 B}, Crandom = {c|c \u223c U [C \\Cbatch]}, and U [C] denotes uniform distribution over classes. Size of Crandom is selected to achieve |C \u2032| = N . m = 0.15 and t = 132 are margin and temperature correspondingly. wc is the embedding of class c."
        },
        {
            "heading": "3.2 CONTRASTIVE PRE-TRAINING",
            "text": "Contrastive learning of image and text correspondence is another popular way of weakly-supervised pre-training of image representations (Radford et al., 2021; Jia et al., 2021). Given a set of imagetext pairs (Ik, Tk), the goal is to learn embedding eimk = fim(Ik) and e txt k = ftxt(Tk) such that the similarity \u27e8eimk , etxtk \u27e9 is larger than \u27e8eimk , etxtj \u27e9 and \u27e8eimj , etxtk \u27e9 for j \u0338= k. Thus, the following cross-entropy loss is used for model training for a batch B = {Ik, Tk}Kk=1. Specifically,\nLimcontrast = \u2211 k\u2208B log e\u27e8e im k ,e txt k \u27e9/\u03c4\u2211 j\u2208B e \u27e8eimk ,e txt j \u27e9/\u03c4 , (2)\nLtxtcontrast = \u2211 k\u2208B log e\u27e8e im k ,e txt k \u27e9/\u03c4\u2211 j\u2208B e \u27e8eimj ,etxtk \u27e9/\u03c4 , (3)\nLcontrast = Limcontrast + Ltxtcontrast , (4) where \u03c4 is a temperature which is also learned during model training."
        },
        {
            "heading": "3.3 MOFI: MULTI-TASK PRE-TRAINING",
            "text": "In the final setup, we combine the entity-based image classification loss with the image-text-based contrastive loss to learn a universal image representation. In this setup, a text embedding is produced, which is compatible with the learned image representation, and can be used for zero-shot image classification, etc. Since entities are extracted directly from text, each training example already consists of a triple of aligned image, text(s) and entities. Thus, it is straightforward to train the model with the above losses together. Specifically,\nLcombined = \u03bbLclass + (1\u2212 \u03bb)Lcontrast , (5) where \u03bb is a hyper-parameter to balance the two loss terms. For simplicity, we set \u03bb to 0.5 in all experiments. Note that compared with the plain alt-text used for model training, we explore the use of entity name and entity descriptions as external knowledge for better performance.6\nWe name the multi-task learned model MOFI, standing for Manifold OF Images, and the supervised learned model MOFISupOnly. We still use CLIP to refer to the constrastive model learned from I2E.\n5More details and ablation study can be found in Appendix C. 6Similar ideas have also been explored in K-Lite (Shen et al., 2022); however, the scale in K-Lite is much smaller (28M images compared with 1B images used in our training), while ours serves as the first evidence to show external knowledge can be useful at scale."
        },
        {
            "heading": "12 MOFI-H/14 I2E 86.67 96.96 98.62 65.52 84.0 86.82 88.05 82.73 77.60 89.34",
            "text": ""
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We perform comprehensive experiments to evaluate the performance of MOFI models learned from the I2E dataset. We compare with SoTA models under image retrieval (Section 4.2) and image classification tasks (both linear probe and zero-shot evaluation as detailed in Section 4.3)."
        },
        {
            "heading": "4.1 SETUP",
            "text": "We employ a transformer (Vaswani et al., 2017) architecture as the backbone in all experiments. The default MOFI-B/16 model adopts the CLIP-B/16 (Radford et al., 2021) configuration for the image encoder, which consists of a 12-layer transformer with 12 attention heads and 768-dimension hidden feature, which is projected to 512 dimension as the final image embedding. When training with contrastive objectives, we also employ the text encoder configuration from CLIP-B/16, which is 12 transformer layers with 8 attention heads and 512 feature dimension. The input text is tokenized by the OPT tokenizer (Zhang et al., 2022) with a vocabulary size of 50,265. The maximum input sequence length is set to 76.\nFor MOFI-L/14, the image encoder is a 24-layer transformer with 16 heads and 1024-dimension hidden feature, which is projected to 512 dimension as output; the text encoder is a 12-layer transformer with 12 heads and 768 dimension feature. For MOFI-H/14 model, the image encoder is a 32-layer transformer with 16 heads and 1280-dimension hidden feature, which is projected to 1024 dimension as output; the text encoder is a 24-layer transformer with 16 heads and 1024 dimension feature.\nAll models are trained with 224x224 input image size using the AdamW optimizer (Loshchilov & Hutter, 2017) with weight decay 0.1 and learning rate 0.0008, except that MOFI-L/14 uses a learning rate of 0.0006. The learning rate is first warmed up to 10,000 steps, and cosine decay is applied until the last training step. Due to the computation limit, we train the CLIP models for 600k steps with global batch size 32,768, and train the other models for 1.2M steps with global batch size 16,384, so all the models have seen the same number of training examples. The number of entities N used in classification for each batch is set to 512k."
        },
        {
            "heading": "4.2 RESULTS ON IMAGE RETRIEVAL",
            "text": "We first evaluate the models on image retrieval tasks on GPR1200 (Schall et al., 2021) and ImageNet1K (Russakovsky et al., 2015)7. GPR1200 is a general-purpose content-based image retrieval benchmark, which consists of subsets from six diverse domains. In total, there are 1200 categories, and each category has 10 images. Follows its original paper, images are not split as query and index sets for evaluation, we retrieve the nearest neighbor for every image and use the rest as index set. We report the full mean Average Precision (mAP) @all for the entire dataset and each domain. For\n7We report addtional image retrieval tasks ROxford and RParis (Radenovic et al., 2018) in Appendix A.\nImageNet, we modify its validation set to use as an image retrieval evaluation set. We randomly select one image from each category as query set, and use the rest as index set. For each retrieval result, we consider it as positive if it has the same category as the query; otherwise, we consider it as negative. We report the top-1 and top-5 accuracy for this dataset. For kNN metric, we follow Wu et al. (2018) and train a kNN classifier on the training set of ImageNet-1K (Russakovsky et al., 2015). The best accuracy on the validation set over a hyper-parameters sweep was reported.\nResults are summarized in Table 2. On GPR1200,MOFI outperforms existing supervised model (+17.13, Row 8 vs. Row 2) and CLIP model (+16.0, Row 8 vs. Row 3) by a significant margin. It is interesting to see that on the Landmark domain, MOFI outperforms Swin-B (+3.75, Row 8 vs. Row 2), which is pre-trained on ImageNet21k and then finetuned on a clean domain specific dataset, i.e., Google LandmarkV2 dataset. It is also worthwhile to mention that our model performs worse (-4.5, Row 8 vs. Row 4) on the SOP domain when compared to our in-house CLIP model. We hypothesize this is due to that using our current data mining approach, it may be hard to extract fine-grained entities from product-related text. We leave further exploration on this as future work. On ImageNet1k, we observe +10.84 and +4.64 improvement on Acc@1 and Acc@5 compared to our own CLIP model (Row 8 vs. Row 4).\nFor the sake of interest, we also compare our model with DINOv2 (Oquab et al., 2023), a recent model trained on a curated dataset (LDV-142M) consisting of 142 million images close to domains of a list of tasks. The DINOv2 model is reported with strong image retrieval performance. Table 2 shows that MOFI significantly outperforms DINOv2 on GPR1200, but is worse on ImageNet-1K. We believe this is due to that DINOv2\u2019s training data is curated for a target list of tasks. Apparently, ImageNet-1K is included in the targeted list and other domains are not included."
        },
        {
            "heading": "4.3 RESULTS ON IMAGE CLASSIFICATION",
            "text": "Zero-shot evaluation. MOFI can also be used for zero-shot image classification, as it is trained with a text encoder using contrastive loss. Table 3 summarizes results on ImagetNet and VTAB, employing the same prompt set from CLIP (Radford et al., 2021). MOFI achieves better or similar performance compared to CLIP on most datasets. For example, MOFI-B/16 improved over CLIP-B/16ours-I2T by 4.27 and 0.3 points on ImageNet and VTAB, respectively. Notably, MOFI models excel in most natural tasks within VTAB, particularly in the domains of OxPet and OxFlowers, where precise object recognition is crucial. Nevertheless, these models struggle with specialized tasks, possibly because the images in those tasks present greater challenges in terms of entity description. Note that zero-shot classification also requires a strong text encoder. The primary goal of the MOFI model is to learn better image embeddings, and it takes a significant part of the computation budget.\nLinear probe. We extract image features before the linear projection to the shared embedding space following CLIP (Radford et al., 2021), and train a logistic regression classifier on top of them for each dataset. We use the AdamW optimizer without weight decay. We conducted a hyper-parameter sweep on training epochs (i.e., 10, 20, 40, 80, 160) and learning rates (i.e., 1e-1, 1e-2, 1e-3, 1e-4) to identify the optimal configurations. Table 4 shows that MOFI performs the best, outperforming CLIPours-I2T by an average of 1.78 and 0.63 on ImageNet and VTAB, respectively. When CLIP was trained using I2E data, or MOFI was trained with only the classification objective, both models performed better than CLIP using I2T data, but worse than MOFI. This highlights the significance of a multi-task setup in achieving better performance."
        },
        {
            "heading": "4.4 QUALITATIVE ANALYSIS",
            "text": "In order to illustrate the difference between the learned embeddings, images from different subcategories of GPR1200 (Schall et al., 2021) as well as images from ROxford and RParis (Radenovic et al., 2018) are used to retrieve the most similar images based on the embedding from different models. Results are shown in Figure 4. For each query image and each model, we show the most similar image from the corresponding index set. Images with the correct label (i.e., the same label as the query) have a green frame and a check-mark (\u2713), others have a red frame and an x-mark(\u2717)."
        },
        {
            "heading": "4.5 COMPARING THE I2E AND I2T DATASETS",
            "text": "We quantitatively compare the I2E and I2T datasets for CLIP and MOFI model training on image retrieval and zero-shot classification tasks. Similar to the other experiments, we put the entity name together with its original text and sample the text with equal probability when using contrastive objectives. Results are reported in Table 5 for models trained on I2T, I2E, or combined. Even for the CLIP model, switching the dataset from I2T to I2E leads to a significant performance improvement on both image retrieval tasks. The performance on the classification tasks are close, with wins on ImageNet and loss on VTAB. The model trained on the combined I2E and I2T dataset is better than the model trained on I2T, but worse than the model trained on I2E. These results indicate that we can also use the entity mining process as a data cleaning and selection process to improve the original image-text dataset, which is aligned with the observation from Gadre et al. (2023); Cao et al. (2023)."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Supervised pre-training on extensive human-labelled datasets, such as ImageNet and ImageNet21k (Deng et al., 2009), has emerged as a widely adopted approach to acquire transferable visual representations. This approach has greatly expedited progress in various computer vision\ntasks like image classification (Donahue et al., 2014; Sharif Razavian et al., 2014), object detection/segmentation (Girshick et al., 2014; Ren et al., 2015), and visual question answering (Chen et al., 2020b; Li et al., 2020). Nevertheless, the effectiveness of learned representations is frequently constrained by the scale and diversity of supervision within the pre-training dataset. For larger-scale pre-training, noisy labels can be also derived from noisy image-text pairs crawled from the web (Ghadiyaram et al., 2019; Mahajan et al., 2018), and certain industrial laboratories have successfully built comprehensive classification datasets by utilizing semi-automatic pipelines, such as JFT (Zhai et al., 2022a), or private data sources like Instagram hashtags (Singh et al., 2022a).\nContrastive pre-training is another prominent approach for acquiring transferable image representations through text supervision. In particular, models such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021) and Florence (Yuan et al., 2021) have showcased impressive zero-shot image classification and image-text retrieval capabilities by mapping images and text into a shared embedding space. In addition, CoCa (Yu et al., 2022) incorporates an additional image captioning objective alongside the contrastive loss, LiT (Zhai et al., 2022c) proposes the freezing of a pre-trained image encoder for contrastive training, and subsequent studies (Yao et al., 2021; Lee et al., 2022; Li et al., 2021; Mu et al., 2022; Wu et al., 2021; Yang et al., 2022; Weers et al., 2023) further enhance the contrastive training objective. Furthermore, research has been conducted on various aspects including non-contrastive learning (Zhou et al., 2023), the integration of external knowledge (Shen et al., 2022), masking (Li et al., 2023), sparse embedding (Chen et al., 2023), and more. A comprehensive review for vision-language pre-training can be found in Gan et al. (2022).\nOur work distinguishes itself from previous studies in two key aspects. Firstly, in terms of data, we have curated a new dataset consisting of 1 billion images and 2 million distinct entities, making it the largest dataset of its kind. Secondly, regarding model training, we propose a new approach that combines supervised and contrastive pre-training, where supervised training treats entities as labels, while contrastive training utilizes entity names as text and augments them with entity descriptions.\nNote that the community also explore image-only self-supervised learning methods, such as imageonly contrastive learning (Chen et al., 2020a; He et al., 2020), non-contrastive learning (Grill et al., 2020; Caron et al., 2021; Chen & He, 2021) and masked image modeling (Bao et al., 2021; He et al., 2022; Wei et al., 2022).We focus on learning from language supervision in this paper."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper introduces MOFI, a new vision foundation model derived from billion-scale noisy entity annotated images. To train MOFI, we first construct a large-scale dataset, called Image-to-Entities (I2E), consisting of 1 billion images and 2 million distinct entities derived from noisy image-text pairs. Subsequently, we explore three different training approaches: supervised pre-training, contrastive pre-training, and multi-task learning. Extensive experiments show that supervised pre-training on a large number of entities significantly enhances the performance of image retrieval tasks, and multitask learning achieves the best performance. Additionally, we demonstrate that the MOFI model, trained on the I2E dataset, yields robust image representations, as evidenced by enhanced zero-shot and linear probe performance on benchmarks such as ImageNet and VTAB, outperforming CLIP."
        },
        {
            "heading": "A. ADDITIONAL IMAGE RETRIEVAL RESULTS",
            "text": "Table 6 reports results on two additional benchmarks, ROxford and RParis (Radenovic et al., 2018), which are also included in DINOv2\u2019s target task list. On these two tasks, MOFI outperforms CLIP models by a significant margin. When trained with GLDv2 (Weyand et al., 2020) data, which is in a similar domain as ROxford and RPairs, MOFI (f)8 model achieves comparable performance to DINOv2 across all metrics. It is worth to note that DINOv2 uses smaller patch size and is also distilled from a larger g/14 model. Both models are still worse than specialized models that are designed for these tasks.\nB. VISUALIZATION OF THE MOFI IMAGE REPRESENTATION SPACE\nWe show the distribution of fine-grained classes in GPR1200 eval set in the feature space from MOFI in Figure 5. We represent each class with the average feature vector of its examples. We first reduce the feature dimension to 48, and then run t-SNE with a perplexity of 20, a learning rate of 50 for 300 iterations. The left figure shows that the six domains are grouped together. The imdb domain has the most concentrated distribution, as it primarily consists of face images, while the stfproduct domain has a more dispersed distribution, as it encompasses a wider range of diverse categories. The right figure shows the distribution of different product categories in stfproduct domain. The categories that are similar to each other are located closer together in the embedding space compared to those that are not similar, e.g., coffee maker and kettle are more closely related than fan and sofa."
        },
        {
            "heading": "C. ABLATION STUDY",
            "text": "In this section, we perform ablation study on the number of in-batch negatives for supervised pretraining, and the number of entities needed to achieve the best performance. Limited by time and computation, we train models with 300k steps, and the learning rate cosine decay to 0 at the 300k step. At last, we also compare the I2E and I2T data. We evaluate the model on two image retrieval tasks (GPR1200 and ImageNet), and two zero-shot classification tasks, including VTAB averaged across 9 tasks and ImageNet.\nIn-batch negatives for supervised pre-training. As described in Section 3.1, the supervised learning objective is required to predict from 2M entities, which is very computation costly and slow. To make training more efficient, a simple strategy that samples N entities in each batch is used, which indicates\n8(f) indicates that the full image is resized to 224x224 regardless of its original aspect ratio, which is similar to DINOv2 setup. All other MOFI and CLIP models resize the shortest edge to 224 first and then perform a center crop.\nN \u2212 1 effective negatives for each loss calculation. N = 512k is used as the default setting, and Figure 6a shows the ablation of using different number of negative entities during training. Results on image retrieval and zero-shot classification consistently demonstrate a similar pattern, indicating that the continuous addition of negative samples up to 32k leads to substantial improvements in the model\u2019s performance. However, introducing additional negatives beyond 32k only brings marginal improvements, lacking significant impact.\nHow many entities are needed in I2E? As indicated in Table 1, the I2E dataset consists of 2 million entities, excluding those with fewer than 5 images. We conduct a more detailed investigation by examining various quantities of entities (Figure 6b). In particular, we select the top number of entities depending on the number of associated images. We start from 20k entities which is similar to the scale of ImageNet 21k, and then select top 50k, 100k, 250k, 1M, and All (2M) entities. Adding more entities consistently improves both image retrieval and zero-shot classification performances until reaching 1M entities. Adding more entities after 1M does not improve the model but also not hurt. We hypothesis the model size may not be large enough to leverage the tail entities, or the current evaluation cannot reveal the potential performance improvement brought up by adding the tail data. Thus, we keep the 2M entities in the dataset, and leave further study for future work."
        }
    ],
    "year": 2023
}