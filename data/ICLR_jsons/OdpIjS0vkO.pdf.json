{
    "abstractText": "In our era of enormous neural networks, empirical progress has been driven by the philosophy that more is better. Recent deep learning practice has found repeatedly that larger model size, more data, and more computation (resulting in lower training loss) improves performance. In this paper, we give theoretical backing to these empirical observations by showing that these three properties hold in random feature (RF) regression, a class of models equivalent to shallow networks with only the last layer trained. Concretely, we first show that the test risk of RF regression decreases monotonically with both the number of features and samples, provided the ridge penalty is tuned optimally. In particular, this implies that infinite width RF architectures are preferable to those of any finite width. We then proceed to demonstrate that, for a large class of tasks characterized by powerlaw eigenstructure, training to nearzero training loss is obligatory: near-optimal performance can only be achieved when the training error is much smaller than the test error. Grounding our theory in real-world data, we find empirically that standard computer vision tasks with convolutional neural kernels clearly fall into this class. Taken together, our results tell a simple, testable story of the benefits of overparameterization and overfitting in random feature models.",
    "authors": [],
    "id": "SP:97cadaf1b56fd8990f4c766f800f6cd25f15fbdd",
    "references": [
        {
            "authors": [
                "Alexander Atanasov",
                "Blake Bordelon",
                "Sabarish Sainathan",
                "Cengiz Pehlevan"
            ],
            "title": "The onset of variance-limited behavior for networks in the lazy and rich regimes",
            "venue": "arXiv preprint arXiv:2212.12147,",
            "year": 2022
        },
        {
            "authors": [
                "Francis Bach"
            ],
            "title": "High-dimensional analysis of double descent for linear regression with random projections",
            "venue": "arXiv preprint arXiv:2303.01372,",
            "year": 2023
        },
        {
            "authors": [
                "Yasaman Bahri",
                "Ethan Dyer",
                "Jared Kaplan",
                "Jaehoon Lee",
                "Utkarsh Sharma"
            ],
            "title": "Explaining neural scaling laws",
            "venue": "arXiv preprint arXiv:2102.06701,",
            "year": 2021
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Philip M Long",
                "G\u00e1bor Lugosi",
                "Alexander Tsigler"
            ],
            "title": "Benign overfitting in linear regression",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Daniel Hsu",
                "Partha Mitra"
            ],
            "title": "Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate",
            "venue": "arXiv preprint arXiv:1806.05161,",
            "year": 2018
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Daniel Hsu",
                "Siyuan Ma",
                "Soumik Mandal"
            ],
            "title": "Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Blake Bordelon",
                "Cengiz Pehlevan"
            ],
            "title": "Dynamics of finite width kernel and prediction fluctuations in mean field neural networks",
            "venue": "arXiv preprint arXiv:2304.03408,",
            "year": 2023
        },
        {
            "authors": [
                "Blake Bordelon",
                "Abdulkadir Canatar",
                "Cengiz Pehlevan"
            ],
            "title": "Spectrum dependent learning curves in kernel regression and wide neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Leo Breiman",
                "David Freedman"
            ],
            "title": "How many variables should be entered in a regression equation",
            "venue": "Journal of the American Statistical Association,",
            "year": 1983
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Mark Sellke"
            ],
            "title": "A universal law of robustness via isoperimetry",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Abdulkadir Canatar",
                "Blake Bordelon",
                "Cengiz Pehlevan"
            ],
            "title": "Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks",
            "venue": "Nature communications,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Cheng",
                "Andrea Montanari"
            ],
            "title": "Dimension free ridge regression",
            "venue": "arXiv preprint arXiv:2210.08571,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Cheng",
                "John Duchi",
                "Rohith Kuditipudi"
            ],
            "title": "Memorize to generalize: on the necessity of interpolation in high dimensional linear regression",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Cui",
                "Bruno Loureiro",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "George Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function",
            "venue": "Mathematics of control, signals and systems,",
            "year": 1989
        },
        {
            "authors": [
                "Kenneth R. Davidson",
                "Stanislaw J. Szarek"
            ],
            "title": "Chapter 8 - local operator theory, random matrices and banach spaces",
            "venue": "Elsevier Science B.V.,",
            "year": 2001
        },
        {
            "authors": [
                "Edgar Dobriban",
                "Stefan Wager"
            ],
            "title": "High-dimensional asymptotics of predictions: Ridge regression and classification",
            "venue": "The Annals of Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Nikhil Ghosh",
                "Mikhail Belkin"
            ],
            "title": "A universal trade-off between the model size, test loss, and training loss of linear predictors",
            "venue": "arXiv preprint arXiv:2207.11621,",
            "year": 2022
        },
        {
            "authors": [
                "Trevor Hastie",
                "Andrea Montanari",
                "Saharon Rosset",
                "Ryan J. Tibshirani"
            ],
            "title": "Surprises in highdimensional ridgeless least squares interpolation, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "venue": "arXiv preprint arXiv:2203.15556,",
            "year": 2022
        },
        {
            "authors": [
                "Like Hui",
                "Mikhail Belkin"
            ],
            "title": "Evaluation of neural architectures trained with square loss vs crossentropy in classification",
            "year": 2006
        },
        {
            "authors": [
                "Arthur Jacot",
                "Cl\u00e9ment Hongler",
                "Franck Gabriel"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Arthur Jacot",
                "Berfin \u015eim\u015fek",
                "Francesco Spadaro",
                "Cl\u00e9ment Hongler",
                "Franck Gabriel"
            ],
            "title": "Kernel alignment risk estimator: risk prediction from training data",
            "venue": "arXiv preprint arXiv:2006.09796,",
            "year": 2020
        },
        {
            "authors": [
                "Arthur Jacot",
                "Berfin Simsek",
                "Francesco Spadaro",
                "Cl\u00e9ment Hongler",
                "Franck Gabriel"
            ],
            "title": "Implicit regularization of random feature models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Dmitry Kobak",
                "Jonathan Lomond",
                "Benoit Sanchez"
            ],
            "title": "The optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Frederic Koehler",
                "Lijia Zhou",
                "Danica J Sutherland",
                "Nathan Srebro"
            ],
            "title": "Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Yasaman Bahri",
                "Roman Novak",
                "Samuel S. Schoenholz",
                "Jeffrey Pennington",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Deep neural networks as gaussian processes",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Lechao Xiao",
                "Samuel S. Schoenholz",
                "Yasaman Bahri",
                "Roman Novak",
                "Jascha SohlDickstein",
                "Jeffrey Pennington"
            ],
            "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Samuel S. Schoenholz",
                "Jeffrey Pennington",
                "Ben Adlam",
                "Lechao Xiao",
                "Roman Novak",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Finite versus infinite neural networks: an empirical study",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Tengyuan Liang",
                "Alexander Rakhlin"
            ],
            "title": "Just interpolate: Kernel \u201cridgeless\u201d regression can generalize",
            "venue": "The Annals of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Chaoyue Liu",
                "Libin Zhu",
                "Mikhail Belkin"
            ],
            "title": "Loss landscapes and optimization in over-parameterized non-linear systems and neural networks",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Bruno Loureiro",
                "Cedric Gerbelot",
                "Hugo Cui",
                "Sebastian Goldt",
                "Florent Krzakala",
                "Marc Mezard",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Learning curves of generic features maps for realistic datasets with a teacher-student model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhou Lu",
                "Hongming Pu",
                "Feicheng Wang",
                "Zhiqiang Hu",
                "Liwei Wang"
            ],
            "title": "The expressive power of neural networks: A view from the width",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Neil Mallinar",
                "James B Simon",
                "Amirhesam Abedsoltan",
                "Parthe Pandit",
                "Mikhail Belkin",
                "Preetum Nakkiran"
            ],
            "title": "Benign, tempered, or catastrophic: A taxonomy of overfitting",
            "venue": "arXiv preprint arXiv:2207.06569,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Maloney",
                "Daniel A Roberts",
                "James Sully"
            ],
            "title": "A solvable model of neural scaling laws",
            "venue": "arXiv preprint arXiv:2210.16859,",
            "year": 2022
        },
        {
            "authors": [
                "Song Mei",
                "Andrea Montanari"
            ],
            "title": "The generalization error of random features regression: Precise asymptotics and the double descent curve",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 2019
        },
        {
            "authors": [
                "Song Mei",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Vidya Muthukumar",
                "Kailas Vodrahalli",
                "Vignesh Subramanian",
                "Anant Sahai"
            ],
            "title": "Harmless interpolation of noisy data in regression",
            "venue": "IEEE Journal on Selected Areas in Information Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Preetum Nakkiran",
                "Yamini Bansal"
            ],
            "title": "Distributional generalization: A new kind of generalization",
            "venue": "arXiv preprint arXiv:2009.08092,",
            "year": 2020
        },
        {
            "authors": [
                "Preetum Nakkiran",
                "Prayaag Venkat",
                "Sham M. Kakade",
                "Tengyu Ma"
            ],
            "title": "Optimal regularization can mitigate double descent",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29\u201353",
            "year": 1996
        },
        {
            "authors": [
                "Roman Novak",
                "Lechao Xiao",
                "Jiri Hron",
                "Jaehoon Lee",
                "Alexander A. Alemi",
                "Jascha Sohl-Dickstein",
                "Samuel S. Schoenholz"
            ],
            "title": "Neural tangents: Fast and easy infinite neural networks in python",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Pratik Patil",
                "Jin-Hong Du"
            ],
            "title": "Generalized equivalences between subsampling and ridge regularization",
            "venue": "arXiv preprint arXiv:2305.18496,",
            "year": 2023
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2008
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2007
        },
        {
            "authors": [
                "Dominic Richards",
                "Jaouad Mourtada",
                "Lorenzo Rosasco"
            ],
            "title": "Asymptotics of ridge (less) regression under general source condition",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel A. Roberts",
                "Sho"
            ],
            "title": "Yaida, and Boris Hanin. Frontmatter",
            "year": 2022
        },
        {
            "authors": [
                "Alessandro Rudi",
                "Lorenzo Rosasco"
            ],
            "title": "Generalization properties of learning with random features",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Vaishaal Shankar",
                "Alex Fang",
                "Wenshuo Guo",
                "Sara Fridovich-Keil",
                "Jonathan Ragan-Kelley",
                "Ludwig Schmidt",
                "Benjamin Recht"
            ],
            "title": "Neural kernels without tangents",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "James B Simon",
                "Madeline Dickens",
                "Dhruva Karkada",
                "Michael R DeWeese"
            ],
            "title": "The eigenlearning framework: A conservation law perspective on kernel ridge regression and wide neural networks",
            "venue": "arXiv preprint arXiv:2110.03922,",
            "year": 2021
        },
        {
            "authors": [
                "Peter Sollich"
            ],
            "title": "Gaussian process regression with mismatched models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "Stefano Spigler",
                "Mario Geiger",
                "Matthieu Wyart"
            ],
            "title": "Asymptotic learning curves of kernel methods: empirical data versus teacher\u2013student paradigm",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2020
        },
        {
            "authors": [
                "Terence Tao"
            ],
            "title": "Topics in random matrix theory, volume 132",
            "venue": "American Mathematical Society,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Tsigler",
                "Peter L Bartlett"
            ],
            "title": "Benign overfitting in ridge regression",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Wei",
                "Wei Hu",
                "Jacob Steinhardt"
            ],
            "title": "More than a toy: Random matrix models predict how real-world neural representations generalize",
            "venue": "In International Conference on Machine Learning, Proceedings of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Denny Wu",
                "Ji Xu"
            ],
            "title": "On the optimal weighted l2 regularization in overparameterized linear regression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Greg Yang",
                "Edward J Hu"
            ],
            "title": "Tensor programs iv: Feature learning in infinite-width neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Greg Yang",
                "Edward J Hu",
                "Igor Babuschkin",
                "Szymon Sidor",
                "Xiaodong Liu",
                "David Farhi",
                "Nick Ryder",
                "Jakub Pachocki",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer",
            "venue": "arXiv preprint arXiv:2203.03466,",
            "year": 2022
        },
        {
            "authors": [
                "Tian-Le Yang",
                "Joe Suzuki"
            ],
            "title": "Dropout drops double descent",
            "venue": "arXiv preprint arXiv:2305.16179,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob A Zavatone-Veth",
                "Cengiz Pehlevan"
            ],
            "title": "Learning curves for deep structured gaussian feature models",
            "venue": "arXiv preprint arXiv:2303.00564,",
            "year": 2023
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "In International Conference on Learning Representations (ICLR). OpenReview.net,",
            "year": 2017
        },
        {
            "authors": [
                "Lijia Zhou",
                "James B Simon",
                "Gal Vardi",
                "Nathan Srebro"
            ],
            "title": "An agnostic view on the cost of overfitting in (kernel) ridge regression",
            "venue": "arXiv preprint arXiv:2306.13185,",
            "year": 2023
        },
        {
            "authors": [
                "Novak"
            ],
            "title": "2020)) to compute the NNGP kernel matrix of CIFAR10, and then diagonalize it to extract the eigenstructure. (We diagonalize an n = 30000 subset of the kernel matrix since this is the largest matrix we can diagonalize on a single A100 GPU without resorting to distributed eigensolvers",
            "year": 2020
        },
        {
            "authors": [
                "Wei"
            ],
            "title": "Second, it acts as an eigenvalue threshold: modes with \u03bbi \u226b \u03ba are learned, and modes with \u03bbi \u226a \u03ba are not",
            "venue": "In the RF eigenframework,",
            "year": 2022
        },
        {
            "authors": [
                "Simon"
            ],
            "title": "The more interesting case is perhaps the data-and-feature-averaged bias and variance",
            "venue": "Jacot et al. (2020a); Canatar et al",
            "year": 2020
        },
        {
            "authors": [
                "Mei"
            ],
            "title": "HIGH-DIMENSIONAL ISOTROPIC ASYMPTOTICS: THE SETTING OF MEI ET AL",
            "venue": "b, and a,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Concretely, we first show that the test risk of RF regression decreases monotonically with both the number of features and samples, provided the ridge penalty is tuned optimally. In particular, this implies that infinite width RF architectures are preferable to those of any finite width. We then proceed to demonstrate that, for a large class of tasks characterized by powerlaw eigenstructure, training to nearzero training loss is obligatory: near-optimal performance can only be achieved when the training error is much smaller than the test error. Grounding our theory in real-world data, we find empirically that standard computer vision tasks with convolutional neural kernels clearly fall into this class. Taken together, our results tell a simple, testable story of the benefits of overparameterization and overfitting in random feature models."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "It is an empirical fact that more is better in modern machine learning. State-of-the-art models are commonly trained with as many parameters and for as many iterations as compute budgets allow, often with little regularization. This ethos of enormous, underregularized models jars sharply with the old wisdom of classical statistics, which prescribes small, parsimonious models and strong regularization. The development of new theoretical results consistent with the success of overparameterized, underregularized modern machine learning has been a central goal of the field for many years.\nHow might such theoretical results look? Consider the well-tested observation that wider networks virtually always achieve better performance, so long as they are properly tuned (Kaplan et al., 2020; Hoffmann et al., 2022; Yang et al., 2022). Let Ete(n,w,\u03b8) denote the expected test error of a network with width w and training hyperparameters \u03b8 when trained on n samples from an arbitrary distribution. A satisfactory explanation for this observation might be a hypothetical theorem which states the following:\nIf w\u2032 > w, then min \u03b8 Ete(n,w\u2032,\u03b8) < min \u03b8 Ete(n,w,\u03b8).\nSuch a result would do much to bring deep learning theory up to date with practice. In this work, we take a first step towards this general result by proving it in the special case of RF regression \u2014 that is, for shallow networks with only the second layer trained. Our Theorem 1 states that, for RF regression, more features (as well as more data) is better, and thus infinite width is best. To our knowledge, this is the first analysis directly showing that for arbitrary tasks, wider is better for networks of a certain architecture.\nHow might a suitable result for overfitting look? It is by now established wisdom that optimal performance in many domains is achieved when training deep networks to nearly the point of interpolation, with train error many times smaller than test loss ( Zhang et al. (2017); Belkin et al. (2018), Appendix F of Hui & Belkin (2020)). However, unlike the statement \u201cwider is better,\u201d \u201cinterpolation is optimal\u201d cannot be true for generic task distributions: we can see this a priori by\nnoting that any fitting at all can only harm us if the task is pure noise, and indeed Nakkiran & Bansal (2020); Mallinar et al. (2022) empirically observe that training to interpolation does harm the performance of real deep networks on sufficiently noisy tasks. This suggests that we instead ought to seek an appropriate class C of model-task pairs \u2014 ideally general enough to include realistic tasks \u2014 such that the following hypothetical statement is true:\nFor model-task pairs in C, at optimal regularization, it holds that Rtr/te := Etr Ete \u226a 1.\nHere we have defined the fitting ratio (or the train-test error ratio) Rtr/te to be the ratio of train and test errors. We take a step towards this general result as well by proving a sharp statement to this effect for kernel ridge regression (KRR), including infinite-feature RF regression and infinite-width neural networks of any depth in the kernel regime (Jacot et al., 2018; Lee et al., 2019). Letting C be the set of tasks with powerlaw eigenstructure (Definition 2), our Theorem 2 states that under mild conditions on the powerlaw exponents, not only is Rtr/te \u226a 1 at optimal regularization, but in fact this overfitting is obligatory: attaining optimal test error requires that Rtr/te \u226a 1.1 Crucially, we put our proposed explanation to the experimental test: we clearly find that the eigenstructure of standard computer vision tasks with convolutional neural kernels satisfies our \u201cobligatory overfitting\u201d criteria, and indeed optimality occurs at Rtr/te \u2248 0 for these tasks. All our main results rely on closed form estimates for the train and test risk of RF regression and KRR in terms of task eigenstructure. We derive such an estimate for RF regression, and our \u201cmore data and features are better\u201d conclusion (Theorem 1) follows quickly from this general result. This estimate relies on a Gaussian universality ansatz (which we confirm with real data) and becomes exact in an appropriate asymptotic limit, though we see excellent agreement with experiment even at modest size. When we study overfitting in KRR, which is the infinite-feature limit of RF regression, we use the infinite-feature limit of our eigenframework, which recovers a well-known risk estimate for (kernel) ridge regression obtained many times in the recent literature (Sollich, 2001; Bordelon et al., 2020; Jacot et al., 2020a; Dobriban & Wager, 2018; Hastie et al., 2020). We solve this eigenframework for powerlaw task eigenstructure, obtaining an expression for test error in terms of the powerlaw exponents and the fitting ratio Rtr/te (Lemma 10), and our \u201cobligatory overfitting\u201d conclusion (Theorem 2 and Corollary 1) follows from this general result. Remarkably, we find that real datasets match our proposed powerlaw structure so well that we can closely predict test error as a function of Rtr/te purely from experimentally-extracted values of the powerlaw exponents \u03b1, \u03b2 Figure 2.\nConcretely, our contributions are as follows:\n\u2022 We obtain general closed-form estimates for the test risk of RF regression in terms of task eigenstructure.\n\u2022 We conclude that, at optimal ridge parameter, more features and more data are strictly beneficial (Theorem 1).\n\u2022 We study KRR for tasks with powerlaw eigenstructure, finding that for a subset of such tasks, overfitting is obligatory: optimal performance is only achieved at small or zero regularization (Theorem 2).\n\u2022 We demonstrate that standard image datasets with convolutional kernels satisfy our criteria for obligatory overfitting."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "The benefits of overparameterization. Much theoretical work has aimed to explain the benefits of overparameterization. Belkin et al. (2019) identify a \u201cdouble-descent\u201d phenomenon in which, for certain underregularized learning rules, increasing overparameterization improves performance. Ghosh & Belkin (2022) show that only highly overparameterized models can both interpolate noisy data and generalize well. Roberts et al. (2022); Atanasov et al. (2022); Bordelon & Pehlevan (2023) show that neural networks of wide but finite width can be viewed as (biased) noisy approximations to their infinite-width counterparts, with the noise decreasing as width grows, which is consistent\n1This is in contrast with the proposed phenomenon of \u201cbenign overfitting\u201d (Bartlett et al., 2020; Tsigler & Bartlett, 2023) in which interpolation is merely harmless, incurring only a sub-leading-order cost relative to optimal regularization. In our \u201cobligatory overfitting\u201d regime, interpolation is necessary, and not interpolating incurs a leading-order cost.\nwith our findings for RF regression. Nakkiran et al. (2021) prove that more features only benefit RF regression in the special case of isotropic covariates; our Theorem 1 extends their results to the general case, confirming a conjecture of theirs. Concurrent work (Patil & Du, 2023) also resolves this conjecture, showing sample-wise monotonicity for ridge regression. Yang & Suzuki (2023) also show that sample-wise monotonicity holds for isotropic linear regression given optimal dropout regularization. It has also been argued that overparameterization provides benefits in terms of allowing efficient optimization (Jacot et al., 2018; Liu et al., 2022), network expressivity (Cybenko, 1989; Lu et al., 2017), and adversarial robustness (Bubeck & Sellke, 2021).\nThe generalization of RF regression. RF (ridge) regression was first proposed by Rahimi et al. (2007) as a cheap approximation to KRR. Its generalization was first studied using classical capacitybased bounds Rahimi & Recht (2008); Rudi & Rosasco (2017). In the modern era, RF regression has seen renewed theoretical attention due to its analytical tractability and variable parameter count. Jacot et al. (2020b) show that the average RF predictor for a given dataset resembles a KRR predictor with greater ridge parameter. Mei & Montanari (2019); Mei et al. (2022) found closed-form equations for the average-case test error of RF regression in the special case of high-dimensional isotropic covariates. Maloney et al. (2022) found equations for the average test error of a general model of RF regression under a special \u201cteacher = student\u201d condition on the task eigenstructure, and Bach (2023) similarly solved RF regression for the case of zero ridge. We report a general eigenframework that subsumes all these closed-form solutions as special cases. Our eigenframework can also be extracted, with some algebra, from replica calculations reported by Atanasov et al. (2022) (Section D.5.2) and Zavatone-Veth & Pehlevan (2023) (Proposition 3.1).\nInterpolation is optimal. Many recent works have aimed to identify settings in which optimal generalization on noisy data may be achieved by interpolating methods, including local interpolating schemes Belkin et al. (2018) and ridge regression Liang & Rakhlin (2020); Muthukumar et al. (2020); Koehler et al. (2021); Bartlett et al. (2020); Tsigler & Bartlett (2023). However, it is not usually the case in these works that (near-)interpolation is required to generalize optimally, as seen in practice. We argue that this is because these works focus entirely on the model, whereas one must also identify suitable conditions on the task being learned in order to make such a claim. Several papers have described ridge regression settings in which a negative ridge parameter is in fact optimal Kobak et al. (2020); Wu & Xu (2020); Tsigler & Bartlett (2023); Zhou et al. (2023). We consider only nonnegative ridge in this work to align with deep learning, but our findings are consistent with the task criterion found by Wu & Xu (2020).2 In a similar spirit, Cheng et al. (2022) prove in a Bayesian linear regression setting that for low noise, algorithms must fit substantially below the noise floor to avoid being suboptimal."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "We work in a standard supervised setting: our dataset consists of n samples X = {xi}ni=1 sampled i.i.d. from a measure \u00b5x over Rd. We wish to learn a target function f\u2217 (which we assume to be square-integrable with respect to \u00b5x), and are provided noisy training labels y = (yi)ni=1 where yi = f\u2217(xi) +N (0, \u03c32) with noise level \u03c32 \u2265 0. We evaluate a predicted function f returned by a learning rule using the train and test mean-squared error, given by MSEtr = 1n \u2211 i(f(xi)\u2212 yi)2 and\nMSEte = Ex\u223c\u00b5x [ (f(x)\u2212 f\u2217(x))2 ] + \u03c32.\nRF regression is defined by the following procedure. First, we draw k weight vectors {wi}ki=1 sampled i.i.d. from some measure \u00b5w over Rp. We then define the featurization transformation \u03c8 : x 7\u2192 (g(wi, x))ni=1, where g : Rp \u00d7 Rd \u2192 R is an embedding function which is squareintegrable with respect to \u00b5w and \u00b5x. Finally, we perform standard linear regression over the featurized data: that is, we output the function f(x) = aT\u03c8(x), where the weights a are given by a = (\u03a8\u03a8T + \u03b4kIn)\n\u22121\u03a8y with \u03a8 = [\u03c8(x1), \u00b7 \u00b7 \u00b7 ,\u03c8(xn)] and \u03b4 \u2265 0 a ridge parameter. If the embedding function has the form g(w, x) = h(wTx) with d = p and some nonlinearity h, then this model is precisely a shallow neural network with only the second layer trained.\nRF regression is equivalent to kernel ridge regression\nf(x) = k\u0302xX (K\u0302XX + \u03b4In) \u22121y, (1)\n2While we consider only nonnegative ridge, certain results \u2014 for example, Corollary 1 \u2014 give inequalities which, if satisfied, imply that zero ridge is optimal. It is generally the case that, when the inequality is satisfied strictly (i.e. we do not have equality), a negative ridge would be optimal if we allowed it.\nwhere [k\u0302xX ]i = K\u0302(x, xi) and [K\u0302XX ]ij = K\u0302(xi, xj) with the (stochastic) random feature kernel K\u0302(x, x\u2032) = 1k \u2211 i g(wi, x)g(wi, x\n\u2032). Note that as k \u2192 \u221e, the kernel converges in probability to its expectation K\u0302(x, x\u2032) k\u2212\u2212\u2192 K(x, x\u2032) := Ew[g(wi, x)g(wi, x\u2032)] and RF regression converges to KRR with the deterministic kernel K.\n3.1 SPECTRAL DECOMPOSITION OF g AND THE GAUSSIAN UNIVERSALITY ANSATZ Here we say what we mean by \u201ctask eigenstructure\u201d in RF regression. By the spectral theorem for compact operators, the feature function g admits a singular value decomposition g(w, x) = \u2211 i \u221a \u03bbi\u03b6i(w)\u03d5i(x), where {\u03bbi}\u221ei=1 are nonnegative scalars indexed in decreasing order and {\u03b6i}\u221ei=1, {\u03d5i}\u221ei=1 are complete bases of eigenfunctions which are orthonormal with respect to \u00b5w and \u00b5x, respectively. The decomposition of the limiting kernel is given in this basis by K(x, x\u2032) = \u2211 i \u03bbi\u03d5i(x)\u03d5i(x\n\u2032). Note that the learning problem is specified entirely by (n, k, \u03b4, {\u03bbi}\u221ei=1, {vi}\u221ei=1, {\u03b6i}\u221ei=1, {\u03d5i}\u221ei=1). The functions {\u03b6i}\u221ei=1, {\u03d5i}\u221ei=1 are uncorrelated and have second moments equal to one \u2014 that is, Ew\u223c\u00b5w [\u03b6i(w)\u03b6j(w)] = Ex\u223c\u00b5x [\u03d5i(x)\u03d5j(x)] = \u03b4ij \u2014 but they may have complicated higher moments. To make progress, throughout the text we will use the following Gaussian universality ansatz that we may disregard all higher moments:\nAssumption A (Gaussian universality ansatz). The expected train and test error are unchanged if we replace {\u03b6i}\u221ei=1, {\u03d5i}\u221ei=1 by random Gaussian functions {\u03b6\u0303i}\u221ei=1, {\u03d5\u0303i}\u221ei=1 sharing the same first two moments. By this, we mean that when sampling w \u223c \u00b5w the values {\u03b6\u0303i(w)}\u221ei=1 are i.i.d. samples from N (0, 1), and likewise for x \u223c \u00b5x and {\u03d5\u0303i}\u221ei=1. Assumption A seems strong at first glance. It is made plausible by many comparable universality results in random matrix theory which state that, when computing certain scalar quantities derived from large random matrices, only the first two moments of the elementwise distribution matter (up to some technical conditions), and the elements can thus be replaced by Gaussians to simplify analysis (Davidson & Szarek, 2001; Tao, 2023). Assumption A holds provably for RF regression in certain asymptotic settings \u2014 see, for example, (Mei & Montanari, 2019; Mei et al., 2022). Most compellingly, results for the test error of KRR derived using a comparable condition show excellent agreement with the test error computed on real data at moderate sample size Bordelon et al. (2020); Jacot et al. (2020a); Simon et al. (2021); Loureiro et al. (2021); Wei et al. (2022). Given that our ultimate goal is to understand learning from real data, this empirical agreement is reassuring. We will shortly validate Assumption A in the same way, demonstrating excellent agreement between predictions for Gaussian statistics and RF experiments with real data (Figure 1).\nUnder this universality ansatz, the learning task is specified entirely by (n, k, \u03b4, {\u03bbi}\u221ei=1, {vi}\u221ei=1). We will now give a set of closed-form equations which estimate train and test risk in terms of these quantities."
        },
        {
            "heading": "4 MORE DATA AND FEATURES ARE BETTER IN RF REGRESSION",
            "text": ""
        },
        {
            "heading": "4.1 THE OMNISCIENT RISK ESTIMATE FOR RF REGRESSION",
            "text": "In this section, we will first state our risk estimate for RF regression, which will enable us to conclude that more data and features are better.\nLet \u03ba, \u03b3 \u2265 0 be the unique nonnegative scalars such that\nn = \u2211 i \u03bbi \u03bbi + \u03b3 + \u03b4 \u03ba and k = \u2211 i \u03bbi \u03bbi + \u03b3 + k\u03ba \u03b3 . (2)\nLet z = \u2211\ni \u03bbi \u03bbi+\u03b3 and q = \u2211 i ( \u03bbi \u03bbi+\u03b3 )2 . The test and train error of RF regression are given\napproximately by\nMSEte \u2248 Ete = 1\n1\u2212 q(k\u22122z)+z 2\nn(k\u2212q) [\u2211 i ( \u03b3 \u03bbi + \u03b3 \u2212 \u03ba\u03bbi (\u03bbi + \u03b3)2 k k \u2212 q ) v2i + \u03c3 2 ] , (3)\nMSEtr \u2248 Etr = \u03b42\nn2\u03ba2 Ete. (4)\nFollowing Breiman & Freedman (1983); Wei et al. (2022), we refer to Equation 3 as the omniscient risk estimate for RF regression because it relies on ground-truth information about the task (i.e., the true eigenvalues {\u03bbi} and target eigencoefficients {vi}). We refer to the boxed equations, together with estimates for the bias, variance, and expectation of f given in Appendix D.5, as the RF eigenframework. Several comments are now in order.\nLike the framework of Bach (2023), our RF eigenframework is expected to become exact (with the error hidden by the \u201c\u2248\u201d going to zero) in a proportional limit where n, k, and the number of eigenvalues diverge together. 3 However, we will soon see that it agrees well with experiment even at modest n, k.\nThis eigenframework generalizes and unifies several known results. In particular, we recover the established KRR eigenframework when k \u2192 \u221e, we recover the ridgeless RF risk estimate of Bach (2023) when \u03b4 \u2192 0+,4 we recover the RF risk estimate of Maloney et al. (2022) when \u03bbi = v 2 i , \u03c3\n2 = 0 and \u03b4 \u2192 0+, and we recover the main result of Mei et al. (2022) for highdimensional RF regression by inserting an appropriate gapped eigenspectrum.We work out these and other limits in Appendix E.\nDeriving the omniscient risk estimate for RF regression. We give a complete derivation of our RF eigenframework in Appendix D and give a brief sketch here. It is obtained from the fact that RF regression is KRR (c.f. Equation 1) with a stochastic kernel. Many recent works have converged on an omniscient risk estimate for KRR under the universality ansatz,5 and if we knew certain eigenstatistics of the stochastic RF kernel K\u0302, we could simply insert them into this known estimate for KRR and be done. Our main effort is in estimating these eigenstatistics using a handful of random matrix theory facts, after which we may read off the omniscient risk estimate for RF regression. Our derivation is nonrigorous, but we conjecture that it can be made rigorous with explicit error bounds decaying with n, k as in Cheng & Montanari (2022).\nPlan of attack. Our approach for the rest of the paper will be to rigorously prove facts about the deterministic quantities Ete and Etr given by our omniscient risk estimates in Eqs. (3, 4)."
        },
        {
            "heading": "4.2 THE \u201cMORE IS BETTER\u201d PROPERTY OF RF REGRESSION",
            "text": "We now state the main result of this section.\nTheorem 1 (More is better for RF regression). Let Ete(n, k, \u03b4) denote Ete with n samples, k features, and ridge \u03b4 with any task eigenstructure {\u03bbi}\u221ei=1, {vi}\u221ei=1. Let n\u2032 \u2265 n \u2265 0 and k\u2032 \u2265 k \u2265 0. It holds that\nmin \u03b4 Ete(n\u2032, k\u2032, \u03b4) \u2264 min \u03b4 Ete(n, k, \u03b4), (5)\nwith strict inequality so long as (n, k) \u0338= (n\u2032, k\u2032) and \u2211\ni \u03bbiv 2 i > 0 (i.e., the target has nonzero\nlearnable component).\nThe proof, given in Appendix F, is elementary and follows directly from Equation 3. Theorem 1 states that the addition of either more data or more random features can only improve generalization error so long as we are free to choose the ridge parameter \u03b4. It is counterintuitive from the perspective of classical statistics, which warns against overparameterization: by contrast, we see that performance increases with additional overparameterization, with the limiting KRR predictor being the optimal learning rule. However, this is sensible if one views RF regression as a stochastic approximation to KRR: the more features, the better the approximation to the ideal limiting process. This interpretation lines up nicely with recent theoretical intuitions viewing infinite-width deep networks as noiseless limiting processes (Bahri et al., 2021; Atanasov et al., 2022; Yang et al., 2022).\nMore prosaically, Theorem 1 is also sensible from the point of view that additional i.i.d. features simply provide the model with more information from which it can generate useful predictions. We might expect a reasonable learning rule to generate strictly better predictions when given strictly\n3More precisely: in this limit, one can consider taking n \u2192 \u03b7n, k \u2192 \u03b7k, and duplicating each eigenmode \u03b7 times for an integer \u03b7, and then taking \u03b7 \u2192 \u221e. Alternatively, new eigenmodes can be sampled from a fixed spectral distribution.\n4We note the technicality that our equations Equation 2 have no solutions when \u03b4 = 0 and n > k, and one should instead take \u03b4 \u2192 0+.\n5We refer to the risk estimate of Hastie et al. (2020); Bordelon et al. (2020). We give a full citation list in Appendix D.\nmore information. From this viewpoint, unrealized fears about double-descent peaks and overparameterization are phantasms, and we should by default trust in the model to handle additional information in a sensible way."
        },
        {
            "heading": "4.3 EXPERIMENTS: VALIDATING THE RF EIGENFRAMEWORK",
            "text": "We perform RF regression experiments using real and synthetic data. Synthetic experiments use Gaussian data x \u223c N (0, diag({\u03bbi})) with \u03bbi = i\u22121.5 and simple projection features g(w, x) = wTx with Gaussian weights w \u223c N (0, Id). Experiments with real datasets use random ReLU features g(w, x) = ReLU(wTx) with Gaussian weights w \u223c N (0, 2Id); the corresponding theoretical predictions use task eigenstructure extracted numerically from the full dataset. The results, shown in Figure 1 and elaborated in Appendix A, show excellent agreement between measured test and train errors and our theoretical predictions. The good match to real data justifies the Gaussian universality ansatz used to derive the framework (Assumption A)."
        },
        {
            "heading": "5 OVERFITTING IS OBLIGATORY FOR KRR WITH POWERLAW EIGENSTRUCTURE",
            "text": "Having established that more data and more features are better for RF regression, we now seek an explanation for why \u201cmore fitting\u201d \u2014 that is, little to no regularization \u2014 is also often optimal. As we now know that infinite-feature models are always best, in our quest for optimality we simply take k \u2192 \u221e for the remainder of the paper and study the KRR limit. When k \u2192 \u221e, our RF eigenframework reduces to the well-established omniscient risk estimate for KRR, which we write explicitly in Appendix H.1. We will demonstrate that overfitting is obligatory for a class of tasks with powerlaw eigenstructure.\nDefining \u201coverfitting.\u201d How should one quantify the notion of \u201coverfitting\u201d? In some sense, we mean that the optimal ridge parameter \u03b4\u2217 which minimizes test error Ete is \u201csmall.\u201d However, \u03b4\u2217 will usually decay with n, so it is unclear how to define \u201csmall.\u201d In this work, we define overfitting via the fitting ratio Rtr/te := Ete/Etr \u2208 [0, 1], which has the advantage of remaining order unity even as n diverges. The fitting ratio is a strictly increasing function of ridge, and so rather than minimizing Ete over ridge, we can equivalently minimize Ete over Rtr/te. We will take \u201coverfitting\u201d to mean that Rtr/te \u226a 1. Definition 1 (Optimal ridge, test error, and fitting ratio). The optimal ridge \u03b4\u2217, optimal test error E\u2217te, and optimal fitting ratio R\u2217tr/te are equal to the values of these quantities at the ridge that minimizes test error. That is,\n\u03b4\u2217 := argmin \u03b4\nEte, E\u2217te = Ete|\u03b4=\u03b4\u2217 , R\u2217tr/te = Rtr/te|\u03b4=\u03b4\u2217 . (6)\nIf the optimal fitting ratio R\u2217tr/te := argminRtr/te Ete is small \u2014 that is, if R \u2217 tr/te \u226a 1 \u2014 we may say that overfitting is optimal. If it is also true that any Rtr/te > R\u2217tr/te gives higher test error, we say that overfitting is obligatory.\nIn this section, we will describe a class of tasks with powerlaw eigenstructure for which overfitting is provably obligatory. For all powerlaw tasks, we will find that R\u2217tr/te is bounded away from 1 (Theorem 2). Given an additional condition \u2014 namely, noise not too big, and an inequality satisfied\nby the exponents \u2014 we find that R\u2217tr/te = 0 (Corollary 1). Remarkably, when we examine real datasets with convolutional kernels, we will observe powerlaw structure in satisfaction of this obligatory overfitting condition, and indeed we will see that R\u2217tr/te \u2248 0.\nDefinition 2 (\u03b1, \u03b2 powerlaw eigenstructure). A KRR task has \u03b1, \u03b2 powerlaw eigenstructure if there exist exponents \u03b1 > 1, \u03b2 \u2208 (1, 2\u03b1 + 1) and an integer i0 > 0 such that, for all i \u2265 i0, the task eigenvalues and eigencoeffients obey \u03bbi = i\u2212\u03b1 and v2i = i \u2212\u03b2 .\nIn words, a task has \u03b1, \u03b2 powerlaw eigenstructure if the task eigenvalues {\u03bbi}\u221ei=1 and squared eigencoefficients {\u03bb2i }\u221ei=1 have powerlaw tails with exponents \u03b1, \u03b2. The technical condition \u03b2 < 2\u03b1+ 1 needed for our proofs is mild, and we will find it is comfortably satisfied in practice.\nTarget noise. We will permit tasks to have nonzero noise. However, we must be mindful of a subtle but crucial point: unlike is conventional, we do not want to take a fixed noise variance \u03c32 = \u0398(1), independent of n. The reason is that the unlearned part of the signal will decay with n, so if \u03c32 does not decay, the noise will eventually overwhelm the uncaptured signal. At this point, we might as well be training on pure noise, which cannot possibly benefit from overfitting, as discussed in the Introduction. We give further justification of this key point in Appendix G.\nTo obtain a nontrivial answer, we must instead scale down \u03c32 proportional to the uncaptured signal. To do so, we set\n\u03c32 = \u03c32rel \u00b7 Ete|\u03c32=\u03b4=0, (7)\nwhere \u03c32rel = \u0398(1) is the relative noise level and Ete|\u03c32=\u03b4=0 is the test error at zero noise and zero ridge. This scaling also has the happy benefit of simplifying many of our final expressions. (We note that this question of noise scaling will ultimately prove purely academic \u2014 when we turn to real datasets, we will find that all are very well described with \u03c32 identically zero.)\nWe now state our main results.\nTheorem 2. Consider a KRR task with \u03b1, \u03b2 powerlaw eigenstructure. Let the optimal fitting ratio and optimal test risk be given by Definition 1. At optimal ridge, the fitting ratio is\nR\u2217tr/te = r 2 \u2217 +O(n \u2212\u03b3) (8)\nwhere r\u2217 is either the unique solution to\n\u03b1\u2212 \u03b2 \u2212 (\u03b1\u2212 1)\u03b2r\u2217 + (\u03b1\u2212 1) (1\u2212 r\u2217)\u03b2 \u03c32rel = 0 (9)\nover r\u2217 \u2208 [0, 1) or else zero if no such solution exists, and \u03b3 = min(1, 2\u03b1+ 1\u2212 \u03b2). Furthermore, this fitting ratio is the unique optimum (up to error decaying in n) in the sense that\nEte E\u2217te\n\u2265 1 + C\u03b1 (\u221a Rtr/te \u2212 \u221a R\u2217tr/te )2 +O(n\u2212\u03b3) (10)\nwhere C\u03b1 = (\u03b1\u22121)2\n\u03b12 .\nThe first part of Theorem 2 gives an equation whose solution is the optimal fitting ratio R\u2217tr/te under powerlaw eigenstructure. The second part is a strong-convexity-style guarantee that, unless we are indeed tuned near R\u2217tr/te, we will obtain test error Ete worse than optimal by a constant factor greater than one.\nThe proof of Theorem 2 consists of direct computation of Ete at large n, together with the observation that Rtr/te = \u03b4 2\nn2\u03ba2 . The difficulty lies largely in the technical task of bounding error terms. We give the proof in Appendix H.\nThe optimal fitting ratio R\u2217tr/te given by Theorem 2 will always be bounded away from 1. This implies that some overfitting will always be obligatory in order to reach near-optimal test error. The following corollary, which follows immediately from Theorem 2, gives a condition under which R\u2217tr/te \u2248 0 and interpolation is obligatory.\nCorollary 1. Consider a KRR task with \u03b1, \u03b2 powerlaw eigenstructure. The optimal fitting ratio vanishes \u2014 that is, R\u2217tr/te n\u2212\u2192 0 \u2014 if and only if\n\u03c32rel \u2264 \u03b2 \u2212 \u03b1\n\u03b1(\u03b1\u2212 1) . (11)\nCorollary 1 gives an elegant picture of what makes a task interpolation-friendly. First, we must have \u03b2 \u2265 \u03b1; otherwise, the RHS of Equation 11 is negative. Larger \u03b2 means that the error decays faster with n (Cui et al., 2021), so \u03b2 \u2265 \u03b1 amounts to a requirement that the task is sufficiently easy.6 Second, we must have sufficiently small noise relative to the uncaptured signal Ete|\u03c32=\u03b4=0. More noise is permissible when \u03b2 \u2212 \u03b1 is greater, which makes sense as noise serves to make a task harder. The fact that zero regularization can be the unique optimum even with nonzero label noise is surprising from the perspective of classical statistics, which cautions against fitting below the noise level. Corollary 2 in the appendix, which also follows directly from Theorem 2, gives the value of R\u2217tr/te explicitly when \u03c3 2 rel = 0, as we will find in experiment."
        },
        {
            "heading": "5.1 EXPERIMENTS: OVERFITTING IS OBLIGATORY FOR IMAGE DATASETS",
            "text": "We ultimately set out to understand an empirical phenomenon: the optimality of interpolation in many deep learning tasks. Having proposed a model for this phenomenon, is crucial that we now turn back to experiment and put it to the empirical test. We do so now with standard image datasets learned by convolutional neural kernels. Running KRR with varying amounts of regularization, we observe that R\u2217tr/te \u2248 0 and (near-)interpolation is indeed obligatory for all three datasets (Figure 2). We also find that adding label noise increases R\u2217tr/te in accordance with our theory.\nEven more remarkably, we find an excellent quantitative fit to our Lemma 10, which predicts Ete as a function of Rtr/te, \u03b1, \u03b2, \u03c32rel up to a multiplicative constant. This surprising agreement attests that, insofar as the effect of regularization is concerned, the structure of these datasets can be summarized by just the two scalars \u03b1, \u03b2. These experiments resoundingly validate our theoretical picture for the examined tasks.\nMeasuring the exponents \u03b1,\u03b2. We examine three standard image datasets \u2014 MNIST, SVHN, and CIFAR10 \u2014 and run KRR with the Myrtle convolutional neural tangent kernel (Shankar et al., 2020). We wish to check for powerlaw decay \u03bbi \u221d i\u2212\u03b1 and v2i \u221d i\u2212\u03b2 and estimate the exponents \u03b1, \u03b2. The naive method is to simply diagonalize the kernel matrix evaluated on the full dataset and directly fit powerlaws to the resulting eigenvalues and coefficients. This method is the current standard in the literature (Spigler et al., 2020; Lee et al., 2020; Bahri et al., 2021). However, this method is crucially defective: we show in Appendix B that finite-n effects cause significant error in the fitted exponents.7\nWe get around this with new methods. To extract \u03b1, we take inspiration from the random matrix theory analysis of Wei et al. (2022) and fit \u03ba \u2248 tr [ K\u22121XX ]\u22121 \u221d n\u2212\u03b1, which exhibits excellent powerlaw decay with no finite-size effects. To extract \u03b2, we simply plot the ridgeless learning curve \u2014 also an excellent powerlaw \u2014 and use the fact that MSEte \u223c n\u2212(\u03b2\u22121) (Cui et al., 2021). The good powerlaw fit here implies that \u03c32 \u2248 0. This is an important methodological point for future works attempting to measure powerlaw structure in KRR data, and we discuss it in detail in Appendix B. We are able to extract clean measurements of \u03b1, \u03b2 in this way for all datasets, finding in each case that \u03b2 \u2273 \u03b1. Values of \u03b1, \u03b2 are reported in Figure 2."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "The present work is part of the grand quest to understand the failures of classical learning theory and to develop updated ideas suitable to machine learning as it exists today. We have presented two solvable models capturing the \u201cmore is better\u201d ethos of deep learning, but we cannot consider this quest done until we have not only transparent models but also coherent new intuitions to take the place of appealing but outdated classical ones. To that end, we propose a few here.\n6More precisely, a larger \u03b1 corresponds to stronger inductive biases, so \u03b2 \u2265 \u03b1 means that in some sense \u201cthe task is no harder than the model anticipated.\u201d\n7As proof of this, we note that Lee et al. (2020) several times extract exponents \u03b1 \u2208 [0.5, 0.9], which is impossible: \u2211 i \u03bbi would diverge if \u03b1 < 1.\nOne once-well-believed classical nugget of wisdom is the following: Overparameterization is harmful because it permits models to express complicated, poorly-generalizing solutions. Taking inspiration from our RF models, perhaps we ought instead to believe the following: Overparameterization is desirable because bigger models more closely approximate an ideal limiting process with infinitely many parameters. Overparameterization does permit a model to express complicated, poorlygeneralizing solutions, but it does not mean that it will actually learn such a thing: models are simple creatures and tend not to learn unnecessarily complication without a good reason to.\nAnother classical view is that interpolation is undesirable because if a model interpolates, it has fit the noise, and so will generalize poorly. Our story with KRR suggests that perhaps we should instead hold the following belief: So long as the inductive bias of the model is a good match for the task, additional regularization is unnecessary, and interpolation is optimal. We might augment this with the following: At least some types of modern data contain very little noise to begin with!\nWhat lessons might we take away for future deep learning theory? Any progress made in this paper relied entirely on closed-form average case estimates of risk. Unlike the complexity-based bounds of classical learning theory, these estimates permitted us to easily resolve interesting O(1) factors, and we were not left wondering how tight final bounds were. In fact, no notion of capacity or model complexity was needed: we simply studied the model\u2019s behavior in the typical case and drew conclusions from our findings. This approach is consistent with that of notable recent successes in deep learning theory, including the \u201cneural tangent kernel\u201d (Jacot et al., 2018) and \u201c\u00b5-parameterization\u201d (Yang & Hu, 2021) avenues of study, both of which rely on exact descriptions of model behavior in regimes in which it concentrates about its mean. As we develop theory for deep learning, then, perhaps we ought to stop fretting about our models\u2019 capacity to fail us and instead study the behavior they actually display."
        },
        {
            "heading": "A EXPERIMENTAL VALIDATION OF OUR THEORY OF RF REGRESSION",
            "text": "We validate our RF eigenframework by comparing its predictions against two examples of random feature model:\nRandom Gaussian projections. We draw latent data vectors x \u223c N (0, Im) from an isotropic Gaussian in a high dimensional ambient space (dimension m = 104).8 The target function is linear as y = vTx+ \u03be, where the target coefficients vi follow a powerlaw vi = \u221a i\u2212\u03b2 with \u03b2 = 1.5, and \u03be \u223c N (0, \u03c32) is Gaussian label noise with \u03c32 = 0.5.\nWe construct random features as \u03c8(x) = W\u039b1/2x. Here, W \u2208 Rk\u00d7m has elements drawn i.i.d. from N (0, 1k ). \u039b = diag(\u03bb1 . . . \u03bbm) is a diagonal matrix of kernel eigenvalues, \u03bbi = i\n\u2212\u03b1 with \u03b1 = 1.5. With this construction, the full-featured kernel matrix is (in expectation over the features) EW [K] =XT\u039bX , with [X] = [x1, \u00b7 \u00b7 \u00b7 ,xn]. Random ReLU features. CIFAR10 input images are normalized to global mean 0 and standard deviation 1. The labels are binarized (with y = \u00b11) into two classes: things one can ride (airplane, automobile, horse, ship, truck) and things one ought not to ride (bird, cat, deer, dog, frog). Thus the target function is scalar.\nThe features are given by \u03c8(x) = ReLU(W Tx) whereW \u2208 Rk\u00d7m has elements drawn i.i.d. from N (0, 2din ), with din = 3072. With this construction, the limiting infinite-feature kernel is in fact the \u201cNNGP kernel\u201d of an infinite-width 1-hidden-layer ReLU network (Neal, 1996; Lee et al., 2018).\nTheoretical predictions. The RF framework is an omniscient risk estimate, so to use it, we must have on hand the eigenvalues of the infinite-feature kernel K and the eigencoefficients of the target function w.r.t to K. For the synthetic data, we dictate the eigenstructure by construction: \u03bbi = i\u22121.5 and v2i = i\n\u22121.5. For random ReLU features, we use the neural tangents library (Novak et al. (2020)) to compute the NNGP kernel matrix of CIFAR10, and then diagonalize it to extract the eigenstructure. (We diagonalize an n = 30000 subset of the kernel matrix since this is the largest matrix we can diagonalize on a single A100 GPU without resorting to distributed eigensolvers.)\nExperimental details. We vary n \u2208 [101, 104], k \u2208 [101, 104], \u03b4 \u2208 [10\u22123, 102]. We perform 45 trials of each experimental run at a given (n, k, \u03b4); however, in each trial we fix the size-n dataset as we vary the random features.\nAdditional plots. We report additional comparisons between RF experiments and our theory in Figures 3 and 4.\n8In the main text, we purported to draw the data anisotropically as x \u223c N (0,\u039b). For our explanation here, we introduce \u039b at the random projection stage, which amounts to the same thing."
        },
        {
            "heading": "B MEASURING POWERLAW EXPONENTS IN KRR TASKS",
            "text": "It is a widely-held belief that natural data have powerlaw structure. Recent works have conjectured that natural data distributions are powerlaw-like with respect to practical neural tangent kernels (Spigler et al., 2020; Lee et al., 2020; Bahri et al., 2021), in the sense that the statistics of kernel regression on a given natural task is well-modelled by KR using a kernel with powerlaw eigenstructure (see Definition 2). For tasks with this structure, the precise numerical values of the powerlaw exponents \u03b1 and \u03b2 characterize the train and test errors of the kernel learner (c.f. Theorem 2). We must measure these exponents carefully.\nA naive procedure for measuring powerlaw exponents is as follows. First, we compute an empirical kernel matrixKn on a dataset of size n. SinceKn is real symmetric, we may numerically diagonalize it, Kn = \u03a6\u039b\u03a6T . Now we project the labels onto the kernel eigenvectors to find the target eigencoefficients, v = \u03a6Ty. Finally, we plot the computed eigenvalues and target eigencoefficients on a log-log plot and perform a linear fit to extract the powerlaw exponent from the slope. This procedure, to our knowledge, is the current standard in the literature.\nThis procedure suffers from serious inherent flaws. For one, sinceKn is a submatrix of an idealized infinitely-large kernel matrix, finite-size effects introduce aberrations in the tail of the measured eigenvalues and eigencoefficients. Furthermore, it is difficult to know at which mode index i a given spectrum transitions from its non-powerlaw front end to its powerlaw tail. As an example, examine the spectrum in the top left of Figure 5. Given that the finite-size spectra deviate from the idealized powerlaw tail in some unknown way, any proposed powerlaw fitting procedure (e.g. \u201cfit a line on the segment of spectrum between i = 100 and i = 1000\u201d) can be disputed. To make matters worse, different fitting procedures yield dramatically different measurements of the exponent \u03b1. The problem is further exacerbated for the eigencoefficients, which do not even decrease monotonically.\nTo summarize, directly fitting powerlaws onto measured spectra is a dubious endeavor.\nThe fact that the true exponents are inaccessible through direct measurement seems to thwart any attempt to make quantitative predictions about real data. Fortunately, there exist techniques to measure the exponents by proxy. These techniques yield precise measurements and we employ them throughout our experiments. As seen in Figure 2, the excellent agreement between theory and experiment using these measurements suggests that these measurement techniques are both precise and accurate. We now describe them.\nUsing effective regularization \u03ba\u0303(n) to measure \u03b1.\nIn the study of the generalization error of KRR, many authors have introduced an \u201ceffective regularization\u201d \u03ba (Simon et al., 2021; Bordelon et al., 2020; Jacot et al., 2020a; Wei et al., 2022). Roughly speaking, \u03ba represents a learning \u201ccutoff\u201d for the kernel: the kernel fails to resolve any component of the target function whose corresponding eigenvalue \u03bbi < \u03ba. Since each additional training sample roughly allows the kernel to resolve one more mode, \u03ba should decrease with n as \u03ba(n) \u223c \u03bbn \u2248 n\u2212\u03b1. Wei et al. (2022) use random matrix theory to approximate \u03ba(n) \u2248 \u03ba\u0303(n) := (TrK\u22121n )\u22121. We compute this quantity for kernel matrices of varying size n and find a clean powerlaw, shown in the bottom left of Figure 5. To measure \u03b1 we fit a line to the segment of the powerlaw that starts around n \u2248 800 and goes to the end. We plot the resulting powerlaw in the top left panel in dotted lines. We observe that the powerlaw is visually distinct from the powerlaw one would expect by simply inspecting the spectrum. This is another strong indication that direct fitting is fallible and untrustworthy.\nUsing test error Ete(n) to measure \u03b2. We perform KRR using trainset kernel matrices of varying size n, with 10 trials at each n. Since KRR fails to resolve the components of the target function with mode indices roughly greater than n, the test error measures the total power of the target function in these modes, which goes as n\u2212(\u03b2\u22121). The test error as a function of n converges to a clean powerlaw at very modest n as seen in the bottom right of Figure 5. To measure \u03b2 we fit a line to the segment of the powerlaw that starts around n \u2248 800 and goes to the end. We now highlight two major benefits to the proposed exponent measurement techniques:\n\u2022 High precision. It is difficult to methodically and robustly estimate the measurement imprecision for these proxy techniques. Nonetheless, we observe that changing the starting location of the linear fit changes the exponent estimate by a meager \u2248 0.01, which is more than an order of magnitude less error than what we obtain by doing similar stress tests on direct powerlaw fits.\n\u2022 Low computational cost. Clean powerlaw behavior in the proxy techniques is visually distinguishable at very modest n. We contrast this with direct measurements: although one may expect the powerlaw tail in the numerical spectrum to become more visually obvious as one extends the spectrum by diagonalizing a larger kernel matrix, each additional decade of empirical spectrum multiplies the computational cost by 1000, making long empirical spectra practically unobtainable. This highlights the importance of methods which yield powerlaw behavior as early in the spectrum as possible.\nUltimately, the accuracy of these proxy measurement techniques is validated by the excellent match between theory and experiment in Figure 2.\nIntuitions about powerlaw eigenstructure. Informally, powerlaw eigenstructure constrains the structure of natural data in two ways:\n\u2022 The eigenvalues of the kernel roughly decay as a powerlaw: \u03bbi \u223c i\u2212\u03b1. (Equivalently, the spectrum of the covariance matrix of the data distribution in the kernel\u2019s feature space decay as a powerlaw.) Since \u03bbi represents a kernel\u2019s \u201cwillingness\u201d to learn eigenmode i, we may interpret \u03b1 as representing the kernel\u2019s parsimony: a kernel with large \u03b1 tends to fit the training data using only a small portion of the eigenfunctions at its disposal.\n\u2022 The target function, expressed as a vector in the kernel\u2019s eigenbasis, has components whose squares roughly decay as a powerlaw: v2i \u223c i\u2212\u03b2 . We may interpret \u03b2 as representing the target function\u2019s comprehensibility (to the kernel learner): targets with large \u03b2 are easier to learn.\nThese informal interpretations immediately suggest that a kernel is well-suited to learn a task if \u03b1 \u2272 \u03b2. Otherwise, the kernel is simply too conservative to learn the intricacies of the target function and will therefore generalize poorly in the absence of regularization. This intuition is made precise in Corollary 1."
        },
        {
            "heading": "Eigenstructure of image datasets.",
            "text": "For completeness, in Figure 6 we include plots depicting the eigenstructure of depth-10 convolutional NTKs on three standard computer vision datasets."
        },
        {
            "heading": "C EMPIRICALLY VERIFYING OUR THEORY OF INTERPOLATION",
            "text": "We compute neural tangent kernel matrices for the Myrtle-10 convolutional architecture on three standard computer vision datasets: MNIST, SVHN, and CIFAR10. For each, we perform kernel regression with varying ridge and added label noise. We additionally measure the eigenstructure exponents \u03b1 and \u03b2 using the techniques described in Appendix B. We use these measurements to predict the train and test error of these kernel learners and find excellent match with experiment.\nExperimental details. We use the neural tangents library (Novak et al. (2020)) to compute the convolutional NTK matrices (CNTKs). It takes about four A100 GPU-days to compute each CNTK. We normalize each dataset to have global mean 0 and standard deviation 1. We do not binarize the labels: the learning task is one-hot 10-class regression. For experiments with label noise, the added noise is a Gaussian vector whose norm has total variance \u03c32 = \u03c32rel \u00b7 Ete|\u03c32=\u03b4=0 (see Equation 7). After adding noise, we normalize all label vectors to have unit norm so that all curves in Figure 2 intersect at (Rtr/te = 1, Ete = 1). In Figure 2, when plotting theoretical predictions, we fit the multiplicative prefactor of the theory curve such that it agrees with the experimental data at Rtr/te = 0. This only provides an anchor for the theory curve; the excellent agreement at Rtr/te > 0 indicates the success of the theory. Additional plots. In Figure 7 and Figure 8 we provide different traces of the experiment depicted in Figure 2"
        },
        {
            "heading": "D DERIVATION OF THE RF EIGENFRAMEWORK",
            "text": "In this appendix, we give a derivation of the eigenframework giving the train and test risk of RF regression which we report in Section 4. Our plan of attack is as follows. First we will recall from the literature the comparable eigenframework for KRR, expressing it in a manner convenient for our task. We will then explicitly write RF regression as an instance of KRR with a stochastic kernel. This framing will make it clear that, if we could understand certain statistics of the (stochastic) eigenstructure of the RF regression kernel, we could directly plug them into the KRR eigenframework. We will then use a single asymptotic random matrix theory identity to compute the various desired statistics. Inserting them into the KRR eigenframework, we will arrive at our RF eigenframework.\nOur derivation will be nonrigorous in that we will gloss over technical conditions for the applicability of the KRR eigenframework for the sake of simplicity. Nonetheless, we will have strong evidence that our final answer is correct by its recovery of known results in various limits of interest (Appendix E) and by its strong agreement with real data (Figure 1)."
        },
        {
            "heading": "D.1 RECALLING THE KRR EIGENFRAMEWORK.",
            "text": "We now state the omniscient risk estimate for KRR (or equivalently linear ridge regression) under Gaussian design which has been converged upon by many authors in recent years (Sollich, 2001; Bordelon et al., 2020; Jacot et al., 2020a; Simon et al., 2021; Loureiro et al., 2021; Dobriban & Wager, 2018; Wu & Xu, 2020; Hastie et al., 2020; Richards et al., 2021). We phrase the framework in a slightly different way than in Appendix H.1 which will be more suitable to our current agenda. As in the main text, let the Mercer decomposition of the kernel K be K(x, x\u2032) =\u2211\u221e i=1 \u03bbi\u03d5i(x)\u03d5i(x\n\u2032), where {\u03d5i}\u221ei=1 are a complete basis of eigenfunctions which are orthonormal with respect to the data measure \u00b5x. We still assume our Gaussian universality ansatz (Assumption A) over the eigenfunctions {\u03d5i}\u221ei=1.\nWe will find it useful to pack the eigenvalues into the (infinite) matrix \u039b = diag(\u03bb1, \u03bb2, . . .), the target eigencoefficients into the (infinite) vector v = (v1, v2, . . .), and the set of eigenfunctions evaluated on any given data point into the (infinite) vector \u03d5(x) = (\u03d51(x), \u03d52(x), . . .). Using this notation, the kernel function is given by\nK(x, x\u2032) = \u03d5(x)T\u039b\u03d5(x\u2032). (12)\nThe KRR eigenframework is as follows. First, let \u03ba \u2265 0 be the unique nonnegative solution to the equation9\nn = Tr [ \u039b\n\u039b+ \u03baI\n] + \u03b4\n\u03ba . (13)\nThen test and train MSE are well-approximated by\nEte = n n\u2212 Tr [(\n\u039b \u039b+\u03baI\n)2] ( vT ( \u03ba\n\u039b+ \u03baI\n)2 v + \u03c32 ) , (14)\nEtr = \u03b42\nn2\u03ba2 Ete. (15)\nThe \u201c\u2248\u201d in 14 can be given several meanings. Firstly, it becomes an equivalence in an asymptotic limit in which n and the number of eigenmodes in a given eigenvalue range (or the number of duplicate copies of any given eignemode) both grow large proportionally (Hastie et al., 2020; Bach, 2023). This is often phrased as sampling a proportional number of new eigenmodes from a fixed measure. Secondly, with fixed task eigenstructure, the error incurred can be bounded by a decaying function of n (Cheng & Montanari, 2022). Thirdly, numerical experiments find small error even at quite modest n (Canatar et al., 2021; Simon et al., 2021). For the purposes of this derivation, we will simply treat it as an equivalence.\n9For two commuting matrices A,B, we will sometimes abuse notation slightly to write A B in place of AB\u22121 = B\u22121A."
        },
        {
            "heading": "D.2 REFRAMING RF REGRESSION AS KRR WITH STOCHASTIC EIGENSTRUCTURE.",
            "text": "We now turn to RF regression. Recall from the main text that RF regression is equivalent to KRR with the random feature kernel\nK\u0302(x, x\u2032) = 1\nk k\u2211 j=1 g(wi, x)g(wi, x \u2032) (16)\nwith wi sampled i.i.d. from some measure \u00b5w. Recall also that there exists a spectral decomposition\ng(w, x) = \u221e\u2211 i=1 \u221a \u03bbi\u03d5i(x)\u03b6i(w), (17)\nwhere (\u03d5i) \u221e i=1 and (\u03b6i) \u221e i=1 are sets of eigenfunctions which are orthonormal over \u00b5x and \u00b5w, respectively, and (\u03bbi) \u221e i=1 are a decreasing sequence of nonnegative scalars. Note that the limiting kernel\nas k \u2192 \u221e is limk\u2192\u221e K\u0302(x, x\u2032) = K(x, x\u2032) = \u2211 i \u03bbi\u03d5i(x)\u03d5i(x \u2032) in probability, from which we see that we are indeed justified in reusing the notation (\u03bbi, \u03d5i) \u221e i=1 from the previous subsection.\nNote that we can write this kernel as\nK\u0302(x, x\u2032) = 1\nk \u221e\u2211 i,i\u2032=1 k\u2211 j=1 \u221a \u03bbi\u03bbi\u2032\u03d5i(x)\u03d5i\u2032(x)\u03b6i(wj)\u03b6i\u2032(wj) (18)\n= \u03d5(x)T\u039b1/2 ZZT\nk \u039b1/2\u03d5(x\u2032), (19)\nwhere we define the projection matrix (Z)ij = \u03b6i(wj). Comparing with Equation 12 and examining our KRR eigenframework, we see that, under Assumption A, we can predict the risk of RF regression as follows.\nFirst, define\n\u039b\u0303 := \u039b1/2 ZZT\nk \u039b1/2. (20)\nThen, let \u03ba be the unique nonnegative solution to the equation\nn = Tr\n[ \u039b\u0303\n\u039b\u0303+ \u03baI\n] + \u03b4\n\u03ba . (21)\nThen test and train MSE will be well-approximated by\nEte = n n\u2212 Tr [(\n\u039b\u0303 \u039b\u0303+\u03baI\n)2] ( vT ( \u03baI\n\u039b\u0303+ \u03baI\n)2 v + \u03c32 ) , (22)\nEtr = \u03b42\nn2\u03ba2 Ete. (23)\nWe refer to this boxed set of equations as the partially-evaluated RF eigenframework because they are written in terms of the random projection Z, which we still have to deal with."
        },
        {
            "heading": "D.3 BUILDING UP SOME USEFUL STATISTICS OF \u039b\u0303",
            "text": "The problem with the partially-evaluated RF eigenframework is of course that we do not know the stochastic eigenstructure matrix \u039b\u0303. To make progress, we again turn to our Gaussian universality ansatz (Assumption A). Under this assumption, we may replace the columns of Z with i.i.d. isotropic Gaussian vectors, which amounts to replacing the whole of Z with i.i.d. samples from N (0, 1).\nWe now leverage a basic random matrix theory fact for such Gaussian matrices leveraged in many recent analyses of ridge regression with random design (Jacot et al., 2020a; Simon et al., 2021; Bach,\n2023). First, let \u03b3 \u2265 0 be the unique nonnegative solution to\nk = \u2211 i \u03bbi \u03bbi + \u03b3 + k\u03ba \u03b3 . (24)\nThe final term is k\u03ba\u03b3 instead of simply \u03ba \u03b3 as might be expected from the form of this fact in other works because of the factor of 1k in Equation 20. Then, under the Gaussian design assumption on Z, we have that\nEZ\n[ \u039b\u0303\n\u039b\u0303+ \u03baI\n] \u2248 \u039b\n\u039b+ \u03b3I . (25)\nThis equation is useful because it reduces a statistic of the stochastic eigenstructure matrix \u039b\u0303 into a function of the known eigenvalue matrix \u039b.\nThe meaning of \u201c\u2248.\u201d The \u201c\u2248\u201d in Equation 25 can be given various technical interpretations. It generally becomes an equivalence the proportional limit described in the following sense: consider fixing an integer \u03b7 > 1 and increasing n \u2192 \u03b7n, k \u2192 \u03b7k, and also duplicating each eigenmode \u03b7 times. As \u03b7 \u2192 \u221e, we reach the proportional limit. For the purposes of this derivation, we will simply treat it as an equivalence.\nNG: How is this bootstrapping done? We now bootstrap this relation to derive four more: EZ [ \u03baI\n\u039b\u0303+ \u03baI\n] \u2248 \u03b3I\n\u039b+ \u03b3I , (26)\nEZ ( \u039b\u0303 \u039b\u0303+ \u03baI )2 \u2248 \u039b \u039b+ \u03b3I \u2212 \u03ba\u039b (\u039b+ \u03b3I)2 \u2202\u03ba\u03b3, (27)\nEZ  \u03ba\u039b\u0303( \u039b\u0303+ \u03baI )2  \u2248 \u03ba\u039b (\u039b+ \u03b3I)2 \u2202\u03ba\u03b3, (28)\nEZ\n[( \u03baI\n\u039b\u0303+ \u03baI\n)2] \u2248 \u03b3I\n\u039b+ \u03b3I \u2212 \u03ba\u039b (\u039b+ \u03b3I)2 \u2202\u03ba\u03b3. (29)\nTaking a derivative of Equation 24 and performing some algebra, we have that\n\u2202\u03ba\u03b3 = k k \u2212 \u2211\ni\n( \u03bbi\n\u03bbi+\u03b3 )2 . (30) We obtain Equation 26 by simply subtracting both sides of Equation 25 from I. We obtain Equation 28 by taking a derivative of Equation 25 with respect to \u03ba. We obtain Equation 29 by taking a derivative\nof Equation 26 with respect to \u03ba. Finally, we obtain Equation 27 from the identity (\n\u039b\u0303 \u039b\u0303+\u03baI\n)2 =\nI\u2212 2 \u03ba\u039b\u0303 (\u039b\u0303+\u03baI)\n2 \u2212 (\n\u03baI \u039b\u0303+\u03baI\n)2 .\nD.4 INSERTING IDENTITIES INTO THE PARTIALLY-EVALUATED RF EIGENFRAMEWORK. We are now in a position to insert Equations 25-29 into the partially-evaluated RF eigenframework to get closed-form results. We will generally trust that scalar quantities concentrate \u2014 that is, for some matrix M and vector z of interest, we will have that Tr[M] \u2248 E[Tr[M]] and zTMz \u2248 E[zTMz], with small enough error that we can neglect it.\nWe start with Equation 21 defining \u03ba. Inserting Equation 25 into the trace, it becomes\nn = \u2211 i \u03bbi \u03bbi + \u03b3 + \u03b4 \u03ba . (31)\nInserting Equations (27) and (29) into Equation 22, we get that\nEte = n n\u2212 \u2211\ni\n( \u03bbi\n\u03bbi+\u03b3 \u2212 \u03ba\u03bbi(\u03bbi+\u03b3)2 \u2202\u03ba\u03b3\n)(\u2211 i ( \u03b3 \u03bbi + \u03b3 \u2212 \u03ba\u03bbi (\u03bbi + \u03b3)2 \u2202\u03ba\u03b3 ) v2i + \u03c3 2 ) . (32)\nInserting Equation 30 for \u2202\u03ba\u03b3 and simplifying with the definitions z = \u2211 i \u03bbi\n\u03bbi+\u03b3 and q =\u2211\ni\n( \u03bbi\n\u03bbi+\u03b3\n)2 and the fact that \u03ba = \u03b3k (k\u2212 z) as per Equation 24, we arrive at the RF eigenframework\nwe report in the main text.\nRemark on implicit constants. The KRR eigenframework we started with had only one implicit constant \u03ba, which could be understood two ways. First, it is given by the inverse of the trace of the inverse of the empirical kernel matrix: \u03ba \u2248 tr[K\u22121XX ]\u22121 Wei et al. (2022). Second, it acts as an eigenvalue threshold: modes with \u03bbi \u226b \u03ba are learned, and modes with \u03bbi \u226a \u03ba are not. In the RF eigenframework, we have two implicit constants, \u03ba and \u03b3. This new \u03ba still serves the first role:\n\u03ba \u2248 Tr [ K\u0302\u22121XX ]\u22121 . However, it is now \u03b3 that acts as the learnability threshold for eigenvalues."
        },
        {
            "heading": "D.5 ADDITIONAL ESTIMATES: BIAS, VARIANCE, AND MEAN PREDICTOR",
            "text": "Test mean squared error is canonically split into a bias term (equal to the error of the \u201caverage\u201d predictor) and a variance term (equal to the rest of the error). In the case of RF regression, a subtle question is: the average with respect to what? We could consider an average with respect to only random datasets, only random feature sets, or both at the same time. Jacot et al. (2020b) take a features-only average. Here we will take the other two.\nIn the main text, we denote the random dataset by X . Let us also denote the random RF weights as W . We then denote the data-averaged bias and variance to be\nBIASd := EW [ Ex\u223c\u00b5x [ (EX [f(x)]\u2212 f\u2217(x))2 ]] + \u03c32, (33)\nVARd := EW,X [ Ex\u223c\u00b5x [ (f(x)\u2212 EX [f(x)])2 ]] . (34)\nSimilarly, we let the data-and-feature-averaged bias and variance to be BIASd,f := Ex\u223c\u00b5x [ (EW,X [f(x)]\u2212 f\u2217(x))2 ] + \u03c32, (35)\nVARd,f := EW,X [ Ex\u223c\u00b5x [ (f(x)\u2212 EW,X [f(x)])2 ]] . (36)\nFortunately, the KRR eigenframework gives us an equation for the data-averaged bias and variance: the data-averaged bias is the term in big parentheses in Equation 14, while the variance is the rest (see Simon et al. (2021)). This tells us that the data-averaged bias and variance are the following:\nBIASd \u2248 Bd := \u2211 i ( \u03b3 \u03bbi + \u03b3 \u2212 \u03ba\u03bbi (\u03bbi + \u03b3)2 k k \u2212 q ) v2i + \u03c3 2, (37)\nVARd \u2248 Vd := Ete \u2212 BIASd. (38)\nThe more interesting case is perhaps the data-and-feature-averaged bias and variance. Jacot et al. (2020a); Canatar et al. (2021); Simon et al. (2021) found that the data-averaged predictor in the KRR case is simply EX [f(x)] = vT \u039b\u039b+\u03baI\u03d5(x), so in our case it will be\nEX [f(x)] = vT \u039b\u0303\n\u039b\u0303+ \u03baI \u03d5(x). (39)\nTaking the feature average, we conclude that\nEW,X [f(x)] = EW [ vT \u039b\u0303\n\u039b\u0303+ \u03ba \u03d5(x)\n] = vT \u039b\n\u039b+ \u03b3I \u03d5(x). (40)\nThat is, EX [f(x)] \u2248 \u2211 i \u03bbi \u03bbi+\u03b3 vi\u03d5i(x). We thus conclude that the data-and-feature-averaged bias and variance are given as follows: NG: Is the bias equation correct, there appear to be two \u03c32 terms? also any interesting conclusions from the bias and variance terms?\nBIASdf \u2248 Bdf := \u2211 i ( \u03b3 \u03bbi + \u03b3 )2 v2i + \u03c3 2 + \u03c32, (41)\nVARdf \u2248 Vdf := Ete \u2212 BIASdf. (42)"
        },
        {
            "heading": "E TAKING LIMITS OF THE RF EIGENFRAMEWORK",
            "text": "In Section 4, we report an omniscient risk estimator giving the expected test risk of RF regression in terms of task eigenstructure. Here we demonstrate that, by taking certain limits, we can recover several previously-reported results from our general framework. For easier reference, we repeat Equations 2 here:\nn = \u2211 i \u03bbi \u03bbi + \u03b3 + \u03b4 \u03ba , (43)\nk = \u2211 i \u03bbi \u03bbi + \u03b3 + k\u03ba \u03b3 . (44)\nE.1 THE LIMIT OF LARGE k: RECOVERING THE KRR EIGENFRAMEWORK RF regression converges to ordinary KRR in the limit of large k, and so we expect to recover the known KRR eigenframework. As k \u2192 \u221e, we find that \u03ba\u03b3 \u2197 1. Therefore we can discard Equation 44 and find that \u03ba simply satisfies n = \u2211 i \u03bbi \u03bbi+\u03ba + \u03b4\u03ba as we get in the case of KRR.\nEquation 3 reduces to\nEte = 1\n1\u2212 qn [\u2211 i ( \u03ba \u03bbi + \u03ba )2 v2i + \u03c3 2 ] , (45)\nwhich is precisely the omniscient risk estimator for KRR (compare with e.g. Simon et al. (2021))."
        },
        {
            "heading": "E.2 THE LIMIT OF ZERO RIDGE: RECOVERING THE RIDGELESS FRAMEWORK OF BACH (2023)",
            "text": "Bach (2023) report an omniscient risk estimator for ridgeless RF regression. We should recover this result from our framework when we take \u03b4 \u2192 0+. (Note that we cannot simply set \u03b4 = 0 as Equations (43) and (44) do not always have solutions, but we will have no trouble taking the limit \u03b4 \u2192 0+.) Like Bach (2023), we will handle this in two cases. Case 1: n < k. When n < k, we will still have \u03ba > 0 even as \u03b4 \u2192 0+. Observe that \u03b3 is determined by the constraint that n = z = \u2211 i \u03bbi \u03bbi+\u03b3 . Plugging into Equation 43 gives us that\nk = n+ k\u03ba\n\u03b3 (46)\n\u21d2 \u03ba = k \u2212 n k \u03b3. (47)\nSticking in these substitutions into Equation 3 and simplifying substantially, we find that\nEte = n\nn\u2212 q [\u2211 i ( \u03b3 \u03bbi + \u03b3 )2 v2i + \u03c3 2 ] + n k \u2212 n [\u2211 i \u03b3 \u03bbi + \u03b3 v2i + \u03c3 2 ] , (48)\nwhich matches the result of Bach (2023).\nCase 2: n > k. When n > k, we have that \u03ba \u2198 0. Therefore \u03b3 is determined by the constraint that k = \u2211 i \u03bbi \u03bbi+\u03b3 . We also have that k = z = \u2211 i \u03bbi \u03bbi+\u03b3 . Inserting these facts into Equation 3, we find that\nEte = 1\n1\u2212 k2\u2212kqn(k\u2212q) [\u2211 i \u03b3 \u03bbi + \u03b3 v2i + \u03c3 2 ] = n n\u2212 k [\u2211 i \u03b3 \u03bbi + \u03b3 v2i + \u03c3 2 ] . (49)\nThis also matches the result of Bach (2023)."
        },
        {
            "heading": "E.3 STUDENT EQUALS TEACHER: RECOVERING THE RF RISK ESTIMATOR OF MALONEY",
            "text": "ET AL. (2022)\nMaloney et al. (2022) work out a risk estimator for ridgeless RF regression under the \u201cstudent equals teacher\u201d condition that \u03bbi = v2i and \u03c3\n2 = \u03b4 = 0. Inserting these into Equations (48) and (49) and exploiting the fact that z = min(n, k) when \u03b4 = 0 to simplify the resulting expressions, we find that\nEte = { k k\u2212nn\u03b3 for n < k, n\nn\u2212kk\u03b3 for n > k. (50)\nAgain using the fact that z = min(n, k), these can be further unified (using the notation of Maloney et al. (2022)) as\nEte = { k k\u2212n\u2206 for n < k, n\nn\u2212k\u2206 for n > k, (51)\nwhere \u2206 \u2265 0 is the unique nonnegative solution to 1 = \u2211\ni \u03bbi m\u03bbi+\u2206 with m := min(n, k). This is\nthe main result of Maloney et al. (2022).\nE.4 HIGH-DIMENSIONAL ISOTROPIC ASYMPTOTICS: THE SETTING OF MEI & MONTANARI (2019)\nHere we compute the predictions of our framework in the setting of Mei & Montanari (2019), who study RF regression with isotropic covariates on the high-dimensional hypersphere. The essential feature of their setting is that task eigenvalues group into degenerate sets: we first have one eigenvalue of size \u0398(1), then d eigenvalues of size \u0398(d\u22121), then \u0398(d2) eigenvalues of size \u0398(d\u22122), and so on. We take d\u221e with n/d \u2192 rn/d, k/d \u2192 rk/d with rn/d, rk/d = \u0398(1). In this setting, we expect to perfectly learn the 0th-order modes, partially learn the 1st-order modes, and completely fail to learn the higher-order modes. Let I\u2113 denote the set of eigenvalue indices corresponding to degenerate level \u2113 \u2265 0. Let \u00b5\u2113 =\u2211 i\u2208I\u2113 \u03bbi be the total kernel power in level \u2113 and \u00b5\u2265\u2113 to denote \u2211 m\u2265\u2113 \u00b5\u2113. Let p\u2113 = \u2211 i\u2208I\u2113 v 2 i be\nthe total target power in level \u2113 and p\u2265\u2113 to denote \u2211\nm\u2265\u2113 p\u2113. Let us write \u03bb0\u0303, \u03bb1\u0303, etc. to denote the value of the degenerate eigenvalue at level \u2113.\nIn this setting, we will find that \u03b3, \u03ba = \u0398(d\u22121). Equation 2 simplify in this setting to\nn d \u2248 \u03bb1\u0303 \u03bb1\u0303 + \u03b3 + \u03b4 + \u00b5\u22652 d\u03ba\nand k d \u2248 \u03bb1\u0303 \u03bb1\u0303 + \u03b3 + k\u03ba d\u03b3 . (52)\nHere the \u2248 hides O(1) terms that asymptotically vanish. These equations can be solved analytically or numerically for \u03b3, \u03ba. Note that if one wishes to work in the large-d limit, one might prefer to solve for \u03b3\u0303 := d\u03b3 and \u03ba\u0303 := d\u03ba as follows:\nrn/d \u2248 \u00b51\n\u00b51 + \u03b3\u0303 + \u03b4 + \u00b5\u22652 \u03ba\u0303\nand rk/d \u2248 \u00b51\n\u00b51 + \u03b3\u0303 +\n\u03ba\u0303 \u03b3\u0303 rk/d. (53)\nThe advantage of the above equations is that all quantities are \u0398(1) after one replaces n/d, k/d with the appropriate \u0398(1) ratios.\nOne then has z = d \u03bb1\u0303\u03bb1\u0303+\u03b3 = d \u00b51 \u00b51+\u03b3\u0303 and q = d\n( \u03bb1\u0303\n\u03bb1\u0303+\u03b3\n)2 = d ( \u00b51\n\u00b51+\u03b3\u0303\n)2 . Let z\u0303 := z/d = \u0398(1) and\nq\u0303 := q/d = \u0398(1). Equation 3 then reduces to\nEte \u2248 1\n1\u2212 q\u0303(1\u22122z\u0303)+z\u0303 2\nrn/d(1\u2212q\u0303)\n[( \u00b51\n\u00b51 + \u03b3\u0303 \u2212 \u03ba\u0303\u00b51 (\u00b51 + \u03b3\u0303)2 1 1\u2212 q\u0303\n) p1 + p\u22652 + \u03c3 2 ] . (54)\nWe expect this is equivalent to the risk estimate of Mei & Montanari (2019)\u2019s Definition 1, as they solve the same problem, but we have not directly confirmed equivalence."
        },
        {
            "heading": "E.5 HIGH-DIMENSIONAL ISOTROPIC ASYMPTOTICS: THE SETTING OF MEI ET AL. (2022)",
            "text": "In followup work to Mei & Montanari (2019) in the linear regime, Mei et al. (2022) study RF regression in the same hyperspherical setting, but in a polynomial regime in which n \u221d da, k \u221d db with a, b > 0, a \u0338= b, and a, b /\u2208 Z.10\nIn this regime, working through our equations, we find the following. First, let c = \u230amin(a, b)\u230b. When a > b and thus n \u226b k, we find that \u03b3 \u2248 \u00b5>ck and \u03ba \u2248 \u03b4 n \u226a \u03b3. When a < b and thus k \u226b n, we find that \u03b3 \u2248 \u00b5>c+\u03b4n and \u03ba \u2248 \u03b3. In either case, we find that z, q \u226a min(k, n), and so the prefactor in Equation 3 becomes equal to 1, with the whole estimate simplifying to\nEte \u2248 p>c + \u03c32. (55)\nThat is, all signal of order \u2113 \u2264 c is perfectly captured and all signal of order \u2113 > c is fully missed (but not overfit), with c set by the minimum of n and k. This is precisely the conclusion of Mei et al. (2022)."
        },
        {
            "heading": "E.6 SANITY CHECK: THE LIMIT OF INFINITE RIDGE",
            "text": "It is easily verified that, as \u03b4 \u2192 \u221e, we find that \u03ba, \u03b3 \u2192 \u221e, and so z, q \u2198 0 and thus Ete = \u2211 i v 2 i +\u03c3 2 as expected.\nE.7 INTERESTING CASE: THE LIMIT OF LARGE n Here we report an additional limit which, to our knowledge, has not appeared in the literature. When n \u2192 \u221e with k finite, we have that \u03ba \u2198 0 and thus z \u2197 k. The risk estimator then reduces to\nEte = \u2211 i \u03b3 \u03bbi + \u03b3 v2i + \u03c3 2. (56)\nThis resembles the bias term of the KRR risk estimator with k samples, with the sole difference being the replacement (\n\u03bbi \u03bbi+\u03b3 )2 \u2192 \u03bbi\u03bbi+\u03b3 ."
        },
        {
            "heading": "F PROOF OF THEOREM 1",
            "text": "Proof. First consider increasing n \u2192 n\u2032. Examining Equations 2, we see that we may always increase \u03b4 such that \u03ba and \u03b3 remain unchanged and still satisfy both equations, and thus z and q are also unchanged. Turning to Equation 3, we see that increasing n \u2192 n\u2032 while keeping \u03ba, \u03b3, k, z, q fixed can only decrease Ete, as this only serves to decrease the (always positive) prefactor.\nNext consider increasing k \u2192 k\u2032. From Equations 2, we see that it is always possible to increase \u03b4 so that \u03b3 remains unchanged (and so z, q remain unchanged) and \u03ba increases. Increasing k decreases the prefactor in Equation 3 because ddk q(k\u22122z)+z2 n(k\u2212q) = \u2212 (z\u2212q)2 (k\u2212q)2 \u2264 0, and increasing k and \u03ba manifestly decreases the term in square brackets, and so overall Ete decreases.\nTo show that this inequality is strict in both cases, all we require is that q > 0, which only requires that the optimal \u03b4 is not positive infinity. To show this, we first observe that as \u03b4 \u2192 \u221e, then \u03ba, \u03b3 grow proportionally, with \u03ban\u03b4 , \u03b3n \u03b4 \u2198 1. It is then easy to expand Equation 3 in terms of large \u03b4, using\nthe facts that z = \u03b3\u22121 \u2211\ni \u03bbi +O(\u03b3 \u22122) and q = \u03b32 + \u2211 i \u03bb 2 i +O(\u03b3\n\u22123). Inserting these expansions, we find that\nEte = ( 1 + \u03b3\u22122n\u22122\n\u2211 i \u03bb2i +O(\u03b3 \u22123) )[\u2211 i v2i + \u03c3 2 \u2212 2\u03b3\u22121 \u2211 i \u03bbiv 2 i +O(\u03b3 \u22122) ] (57)\n= lim \u03b4\u2192\u221e Ete \u2212 2\u03b3\u22121 \u2211 i \u03bbiv 2 i +O(\u03b3 \u22122). (58)\nFrom the negative \u03b3\u22121 term in this expansion, it is clear that the optimal \u03b3 is finite so long as\u2211 i \u03bbiv 2 i > 0, and thus the inequality in the theorem is strict.\n10Technically they abstract the hyperspherical setting to a generic setting with a suitably gapped eigenspectrum, but the essential features are the same."
        },
        {
            "heading": "G NOISE SCALING AND COMPARISON WITH BENIGN OVERFITTING",
            "text": "Here we give further justification of our decision to scale the noise down with n as \u03c32 = \u03c32rel \u00b7 Ete|\u03c32=\u03b4=0, which amounts to \u03c32 \u221d n\u2212(\u03b2\u22121). This turns out to be a subtle but very important point which differentiates our approach and conclusions from those of the \u201cbenign overfitting\u201d literature (Bartlett et al., 2020; Tsigler & Bartlett, 2023). These other works take a fixed, positive noise level \u03c32 = \u0398(1). This is well-motivated \u2014 after all, there appears to be no reason why the noise should change with n \u2014 and is the standard setup in the statistics literature. One essential consequence of this setup \u2014 which we argue here is in fact a crucial shortcoming \u2014 is that, once n grows large, the noise inevitably dominates the signal. Concretely, one can decompose the portion of test risk coming from the (deterministic) signal and that coming from the noise, and so long as the signal lies in the RKHS of the model and the ridge is not very large, at large n the contribution to test error due to the noise dominates that coming from the signal.\nOnce we reach this \u201cnoise-dominating\u201d regime, our task was irrelevant: to leading order, we might as well assume it is pure noise. This simplifies the question of overfitting dramatically and, we argue, too much. Indeed, when training on pure noise, the best one can hope to do is not overfit it. Large ridge is optimal, and zero ridge is suboptimal: the best we can hope for is that the suboptimality of zero ridge costs us only a on(1) penalty in test error. That is, at best, interpolation is permissible; it is never preferable. This is emphatically not the regime we actually see in deep learning: we find that training interpolation is often indeed strictly beneficial (practitioners wouldn\u2019t train for so many extra iterations if they weren\u2019t helping), and indeed Nakkiran & Bansal (2020); Mallinar et al. (2022) simply measure the fitting-to-interpolation of neural networks of noisy data and find that interpolation is not benign but rather incurs a finite cost in test error.\nThe essential cause of this is that finite noise inevitably grows to dominate uncaptured signal. The solution is to scale down noise proportional to the uncaptured signal as we do in Section 5. This puts both sources of error on an equal footing and enables us to make a nontrivial comparison between the two. As we show in Section 5, we do in fact need to consider the details of the target function to understand overfitting in modern machine learning (or at least KRR on image datasets), and so choosing a scaling that does not wash out all signal is crucial, and target-function-agnostic analyses as performed by (Bartlett et al., 2020; Tsigler & Bartlett, 2023; Zhou et al., 2023) cannot see this effect.\nIt may seem aesthetically repugnant to have \u03c32 scale in any way with n. The noise level is surely a fixed quantity; how can it change? In reality, this is just a way of viewing whatever noise level we see in an experiment. As we increase n, we will indeed find that \u03c32rel indeed increases (because the noise remains the same, but the uncaptured signal decays). Our scaling boils down to a choice to treat \u03c32rel as an order-unity quantity that one must consider carefully. A scaling with \u03c3\n2 = \u0398(1) amounts to a choice to treat \u03c32rel as infinite and dominating in the large-n regime in which one will prove theorems. We will, of course, never operate at truly infinite n in practice \u2014 this limit is taken purely to arrive at a theoretical picture \u2014 and she choice of which scaling is preferable ought thus to be put to the empirical test. In our experiments, we find that \u03c32rel is indeed small for the datasets we examine, which we view as affirming our choice to do theory in a regime in which the noise is not dominant."
        },
        {
            "heading": "H PROOFS: KRR WITH POWERLAW STRUCTURE",
            "text": "Here we will derive a picture of the train and test risk of KRR under powerlaw eigenstructure. The ultimate goal is to arrive at Theorem 2 giving the optimal train-test error ratio for exponents \u03b1, \u03b2 and noise level. Along the way, we will derive closed-form equations for many quantities of interest, including the test risk Ete (Lemma 10), which are plotted as theory curves in Figure 2. All results in this appendix will assume powerlaw task eigenstructure as per Definition 2: that is, there exists some positive integer i0 = O(1) such that, for all i \u2265 i0, it holds that \u03bbi = i\u2212\u03b1 and v2i = i\u2212\u03b2 . (We do still assume that the eigenvalues are indexed in decreasing order, even though the first i0 \u2212 1 will not be exactly powerlaw.)"
        },
        {
            "heading": "H.1 RECALLING THE KRR EIGENFRAMEWORK",
            "text": "We begin by stating the k \u2192 \u221e limit of the RF eigenframework of Section 4. In this limit, we recover a known risk estimate for KRR (or equivalently linear ridge regression) that has been converged upon by many authors in recent years (Sollich, 2001; Bordelon et al., 2020; Jacot et al., 2020a; Simon et al., 2021; Loureiro et al., 2021; Dobriban & Wager, 2018; Wu & Xu, 2020; Hastie et al., 2020; Richards et al., 2021) \u2014 a result which we actually bootstrapped to derive our RF eigenframework (Appendix D). This risk estimate as follows:\nLet \u03ba \u2265 0 be the unique nonnegative solution to\u2211 i \u03bbi \u03bbi + \u03ba + \u03b4 \u03ba = n. (59) Then test risk is given approximately by MSEte \u2248 Ete := E0B, (60) where the overfitting coefficient E0 is given by E0 := n\nn\u2212 \u2211\ni\n( \u03bbi\n\u03bbi+\u03ba )2 (61) and the bias is given by\nB = \u2211 i ( \u03ba \u03bbi + \u03ba )2 v2i + \u03c3 2. (62)\nAs discussed in Appendix D, the \u201c\u2248\u201d in 60 can be given several meanings. Firstly, it becomes an equivalence in an asymptotic limit in which n and the number of eigenmodes in a given eigenvalue range (or the number of duplicate copies of any given eignemode) both grow large proportionally (Hastie et al., 2020; Bach, 2023). This is often phrased as sampling a proportional number of new eigenmodes from a fixed measure. Secondly, with fixed task eigenstructure, the error incurred can be bounded by a decaying function of n (Cheng & Montanari, 2022). Thirdly, numerical experiments find small error even at quite modest n (Canatar et al., 2021; Simon et al., 2021). As with the RF eigenframework, in this paper we will simply treat it as an equivalence, formally proving facts about the risk estimate Ete. Recall that all sums run from i = 1 to \u221e. Train risk is given by\nMSEtr \u2248 Etr := \u03b42\nn2\u03ba2 Ete, (63)\nand so the train-test error ratio is given roughly by MSEtr MSEte \u2248 Rtr/te := Etr Ete = \u03b42 n2\u03ba2 . (64) Recall that the noise level is defined relative to the zero-noise-zero-ridge risk as \u03c32 = \u03c32rel \u00b7 Ete|\u03c32=\u03b4=0. (65)\nWe will assume throughout that \u03b1 > 1 and \u03b2 \u2208 (1, 2\u03b1 + 1). We will also assume n \u2265 1. We will generally use an asterisk to demarcate quantities which occur at the optimal ridge w.r.t. test risk. For example, \u03b4\u2217 = argmin\u03b4 Ete, and \u03ba\u2217 = \u03ba|\u03b4=\u03b4\u2217 , and R\u2217tr/te = Rtr/te|\u03b4=\u03b4\u2217 .\nIt is worth noting that we have two varying parameters in our system: the sample size n and the ridge \u03b4. Other quantities, like \u03ba or Rtr/te, may be seen as functions of n and \u03b4. When we state scaling results using big-O-style notation, they will describe scaling with respect to n \u2014 that is, x = O(f(n, \u03b4)) means that there exist constants n0, C > 0 such that, for all n \u2265 n0, it holds that x \u2264 Cf(n, \u03b4). We will allow \u03b4 to vary arbitrarily, including as a function of n."
        },
        {
            "heading": "H.2 CONTINUUM APPROXIMATIONS TO SUMS",
            "text": "Our main trick will be approximating the sums appearing in the eigenframework by integrals. The primary technical difficulty will be in bounding the error of these approximations (though we will ultimately find that these errors do not present a problem). Upon a first reading of this appendix, a reader may wish to simply ignore all error terms to grasp the overall flow of the argument.\nVarious quantities in this appendix will have complicated prefactors depending on the exponents \u03b1 and \u03b2. These prefactors often cancel and simplify in the final accounting11. It is useful to pay less attention to these prefactors and more attention to how quantities scale \u2014 with respect to, for example, n (which will be large) and \u03ba (which will be small).\nHere we state continuum approximations to the three eigensums of the eigenframework.\nLemma 1 (Continuum approximations to eigensums). The sums appearing in the KRR eigenframework can be approximated as follows:\n\u221e\u2211 i=1 \u03bbi \u03bbi + \u03ba = \u03c0 \u03b12 sin(\u03c0/\u03b1) \u03ba\u22121/\u03b1 +O(1), (66)\n\u221e\u2211 i=1 ( \u03bbi \u03bbi + \u03ba )2 = \u03c0(\u03b1\u2212 1) \u03b12 sin(\u03c0/\u03b1) \u03ba\u22121/\u03b1 +O(1), (67)\n\u221e\u2211 i=1 ( \u03ba \u03bbi + \u03ba )2 v2i = \u03c0(\u03b1\u2212 \u03b2 + 1) \u03b1\u03b2 sin ( \u03c0 (\u03b2\u22121)\u03b1 )\u03ba \u03b2\u22121\u03b1 +O(\u03ba2 + \u03ba \u03b2a ). (68) Proof. First, we argue that we may disregard the first i0 = O(1) terms in these sums and replace them with their ideal powerlaw values \u03bbi = i\u2212\u03b1 and v2i = i\n\u2212\u03b2 . For the first two sums, note that replacing the first i0 terms (or neglecting the first i0 terms entirely) only incurs an O(1) error, which is the size of the error terms in Equations (66) and (67) anyways. For the third sum, note that replacing the first i0 terms (or neglecting the first i0 terms entirely) only incurs an O(\u03ba2) error, which is at most the size of the error term in Equation 68 anyways. We may thus proceed assuming perfect powerlaw structure, with i0 = 1.\nTo prove the first clause, we note that the summand is monotonically decreasing, and thus use integrals to bound the sum as\u222b \u221e\n0\ni\u2212\u03b1\ni\u2212\u03b1 + \u03ba > \u221e\u2211 i=1 \u03bbi \u03bbi + \u03ba > \u222b \u221e 1 i\u2212\u03b1 i\u2212\u03b1 + \u03ba > \u222b \u221e 0 i\u2212\u03b1 i\u2212\u03b1 + \u03ba \u2212 1. (69)\nThe LHS integral evaluates to \u03c0\u03b12 sin(\u03c0/\u03b1)\u03ba \u22121/\u03b1, which is sufficient to give Equation 66.\nEquation 67 is obtained in exactly the same way.\nEquation 68 is obtained in the same way, except that the summand is no longer monotonicallydecreasing if \u03b2 \u2208 (2\u03b1, 2\u03b1+ \u03b2), instead monotonically increasing to a maximum of size \u0398(\u03ba\u03b2/\u03b1\u22122) at index imax = \u03b2\u22121/\u03b1(2\u03b1\u2212 \u03b2)1/\u03b1\u03ba\u22121/\u03b1 +O(1) before monotonically decreasing to zero. Splitting the sum into increasing and decreasing parts and again using integrals to bound the sum gives Equation 68.\nRemark. We will more or less never again need to worry about the constant cutoff index i0 and can forget about it now. Our conclusions will live in the regime of large n, and in this regime, it will be sufficient to have powerlaw tails, and we can simply neglect a constant number of eigenmodes at low index. One should usually expect the sub-leading-order error terms we will carry around to be smaller the smaller i0 is and the less the task deviates from perfect powerlaw eigenstructure, however (though one likely ought to consider some measure of total deviation rather than simply the cutoff i0).\nRemark. In several places in this appendix, including Equation 68, we will encounter the fraction (\u03b1 \u2212 \u03b2 + 1)/ sin(\u03c0(\u03b2 \u2212 1)/\u03b1). This is nominally undefined when \u03b2 = \u03b1 + 1. However, this\n11The authors are grateful for the existence of computer algebra systems.\ndiscontinuity disappears with an application of L\u2019Hopital\u2019s rule, simplifying to \u03c0/\u03b1. (Indeed, when evaluating the integral to arrive at the RHS of Equation 68, if we specify that \u03b2 = \u03b1+ 1, then e.g. Mathematica indeed informs us that the integral evaluates to the result of applying L\u2019Hopital\u2019s rule to this general form.) Rather than treat the case \u03b2 = \u03b1+ 1 specially, we will simply gloss over it with the understanding that L\u2019Hopital\u2019s rule ought to be used to resolve this undefined fraction."
        },
        {
            "heading": "H.3 THE ZERO-NOISE-ZERO-RIDGE TEST RISK",
            "text": "Because the noise level is defined in a normalized fashion by \u03c32 = \u03c32rel \u00b7 Ete|\u03c32=\u03b4=0, we need to find the zero-noise-zero-ridge risk Ete|\u03c32=\u03b4=0 before we can study the noise in the general case. (This is also an interesting object in its own right.)\nLemma 2 (Zero-ridge implicit regularization). At \u03b4 = 0, the implicit regularization \u03ba is given by \u03ba|\u03b4=0 = ( \u03c0\n\u03b12 sin(\u03c0/\u03b1)\n)\u2212\u03b1 n\u2212\u03b1 +O ( n\u2212(\u03b1+1) ) . (70)\nProof. This follows straightforwardly from inserting Equation 66 into Equation 59.\nLemma 3 (Zero-noise-zero-ridge test risk). At \u03c32 = 0 and \u03b4 = 0, the test risk Ete is given by\nEte|\u03c32=\u03b4=0 = \u03c0\u03b2(\u03b1\u2212 \u03b2 + 1) sin\n( \u03c0 \u03b1 ) \u03b1\u03b2 sin ( \u03c0(\u03b2\u22121)\n\u03b1 ) n\u2212(\u03b2\u22121) +O(n\u22122\u03b1 + n\u2212\u03b2). (71) Proof. This follows from inserting Equation 70 into the continuum approximations of Lemma 1, inserting the results into the the eigenframework giving Ete, and simplifying.\nRemark. While doing this, one finds (after a surprising cancellation) that E0|\u03b4=0 = \u03b1+O(n\u22121), as previously reported by Mallinar et al. (2022).\nH.4 TEST RISK IN TERMS OF \u03ba We now turn back to the general case in which noise and regularization are nonzero. The following lemma gives test risk in terms of the implicit regularization \u03ba.\nLemma 4.\nEte = n\nn\u2212 \u03c0(\u03b1\u22121)\u03b12 sin(\u03c0/\u03b1)\u03ba\u22121/\u03b1 \u00b7  \u03c0(\u03b1\u2212 \u03b2 + 1) \u03b1\u03b2 sin ( \u03c0 (\u03b2\u22121)\u03b1 )\u03ba \u03b2\u22121\u03b1 + \u03c32rel \u00b7 \u03c0\u03b2(\u03b1\u2212 \u03b2 + 1) sin (\u03c0\u03b1) \u03b1\u03b2 sin ( \u03c0(\u03b2\u22121) \u03b1 ) \u00b7 n\u2212(\u03b2\u22121) \n+O(n\u2212\u03b2 + \u03ba2 + \u03ba\u03b2/\u03b1). (72)\nProof. This lemma follows from inserting Lemmas 1 and 3 into the definition of Ete. Lemma 5. \u03ba = \u2126(n\u2212\u03b1) , Ete = \u2126 ( min ( \u03ba \u03b2\u22121 \u03b1 , 1 )) , and thus Ete = \u2126 ( n\u2212(\u03b2\u22121) ) .\nProof. The fact that \u03ba = \u2126(n\u2212\u03b1) follows from the fact that \u03ba|\u03b4=0 = \u0398(n\u2212\u03b1) and \u03ba is a monotonically increasing function of \u03b4.\nTo see that Ete = \u2126 ( min ( \u03ba \u03b2\u22121 \u03b1 , 1 )) , we return to the definition of Ete. It is easily seen that\nE0 = \u0398(1), because E0|\u03b4=0 = \u0398(1), E0 \u2265 1, and E0 monotonically decreases with \u03b4, so we need only examine the bias B to understand the size of Ete in a scaling sense. Because \u03c32rel = O(1), we have that \u03c32 = \u03c32rel \u00b7 Ete|\u03c32=\u03b4=0 = O(n\u2212(\u03b2\u22121)). Examining the eigensum \u2211 i(1\u2212Li)2v2i , it is easily seen that at all modes i > i\u2032 for some i\u2032 = \u0398(max(1, \u03ba\u22121/\u03b1)), we will have that Li = i \u2212\u03b1 i\u2212\u03b1+\u03ba \u2264 1 2 ,\nsay, which tells us that Ete = \u2126( \u222b\u221e i\u2032 i\u2212\u03b2di) = \u2126 ( (i\u2032)\u2212(\u03b2\u22121) ) 12. The second clause of the lemma follows from this.\nThe third clause of the lemma follows from the insertion of the first statement into the second. 12It might seem that we need to be mindful of the cutoff index i0 up to which we do not necessarily have powerlaw eigenstructure, but we do not actually: because i0 = \u0398(1), we will still have i\u2032 = \u0398(max(1, \u03ba\u22121/\u03b1)).\nLemma 6. At optimal regularization, we have that E\u2217te = \u0398(n\u2212(\u03b2\u22121), \u03ba\u2217 = \u0398(n\u2212\u03b1), and thus \u03b4\u2217 = O(n \u2212(\u03b1\u22121))\nProof. The first clause of this lemma follows from the lower bound on Ete given by Lemma 5 and the fact that we can saturate this lower bound in a scaling sense because we do so already at zero ridge, Ete|\u03b4=0 = \u0398(n\u2212(\u03b2\u22121)) (which itself follows from inserting Lemma 2 into Lemma 4). Given this, the second clause of Lemma 5 assures us that \u03ba\u2217 = \u0398(n\u2212\u03b1) as desired.13 The bound on \u03b4\u2217 follows from Equation 66 and Equation 59.\nRemark. Lemma 5 is useful because it tells us that, at optimal ridge, the error terms in Lemma 4 will indeed decay faster with n than Ete itself.\nH.5 RELATING \u03ba AND Rtr/te\nLemma 7. The train-test error ratio is given in terms of \u03ba by\u221a Rtr/te = 1\u2212 n\u22121 \u03c0\n\u03b12 sin(\u03c0/\u03b1) \u03ba\u22121/\u03b1 +O(n\u22121). (73)\nSolving for \u03ba, we have\n\u03ba =\n( \u03c0\n\u03b12 sin(\u03c0/\u03b1)\n)\u2212\u03b1 n\u2212\u03b1 ( 1\u2212 \u221a Rtr/te +O(n\u22121) )\u2212\u03b1 . (74)\nProof. This lemma follows directly from Equations (59), (64) and (66).\nRemark. A minor annoyance in the usage of Equation 74 is that, when Rtr/te is too close to 1, the O(n\u22121) term can affect \u03ba to leading order. As a patch to avoid this, we will initially restrict the domain of study to Rtr/te \u2208 [0, 1\u2212 c] for some n-independent constant c > 0. We are then assured that\n\u03ba =\n( \u03c0\n\u03b12 sin(\u03c0/\u03b1)\n)\u2212\u03b1 n\u2212\u03b1 ( 1\u2212 \u221a Rtr/te) )\u2212\u03b1 +O(n\u2212(\u03b1+1)). (75)\nThe fact that \u03ba increases monotonically with Rtr/te (Lemma 8) will let us extend results of interest to the small region Rtr/te \u2208 (1\u2212 c, 1). As far as the study of the optimal regularization point go, we need not consider this small edge region in the following sense:\nLemma 8. The train-test error ratio Rtr/te \u2208 [0, 1) and implicit regularization \u03ba \u2208 [\u03ba|\u03b4=0,\u221e) are monotonically-increasing functions of each other. Proof. This lemma is apparent from the fact that Rtr/te = ( 1\u2212 n\u22121 \u2211 i \u03bbi \u03bbi+\u03ba )2 .\nLemma 9. The optimal value of the train-test error ratio is bounded away from 1 in the sense that there exists a constant c\u2032 > 0 such that R\u2217tr/te \u2264 1\u2212 c \u2032 +O(n\u22121).\nProof. We know from Lemma 6 that \u03ba\u2217 = \u0398(n\u2212\u03b1). We then obtain the desired result from Equation 73.\nH.6 PUTTING IT ALL TOGETHER: TEST RISK IN TERMS OF Rtr/te The following lemma gives the test risk Ete in terms of the train-test error ratio Rtr/te and is the basis for our ultimate conclusions about the optimal train-test error ratio.\n13Seeing this is made easier by writing out the scaling notation explicitly \u2014 e.g., Ete = \u2126(n\u2212(\u03b2\u22121)) means that there exists a constant A1 > 0 such that Ete >1 \u00b7n\u2212(\u03b2\u22121) for sufficiently large n, and so on \u2014 and then solving for constants which are sufficient to give the desired new statements.\nLemma 10. Let c > 0 be an n-independent constant. Then for Rtr/te \u2208 [0, 1\u2212 c], we have that\nEte = \u03c0\u03b2\u03b1\u2212\u03b2(\u03b1\u2212 \u03b2 + 1) ( sin (\u03c0 \u03b1 ))\u2212(\u03b2\u22121) csc ( \u03c0(\u03b2 \u2212 1) \u03b1 ) \u00d7 1\n1 + (\u03b1\u2212 1) \u221a Rtr/te\n( \u03b1\u03c32rel + ( 1\u2212 \u221a Rtr/te )\u2212(\u03b2\u22121)) n\u2212(\u03b2\u22121)\n+O(n\u2212\u03b2 + n\u22122\u03b1), (76)\nwhere the suppressed constants in the O(n\u2212\u03b2 + n\u22122\u03b1) term may depend on c.\nProof. This lemma follows from insertion of Equation 75 into Lemma 4. Lemma 11. Let us abbreviate r = \u221a Rtr/te, and let c be the n-independent constant from Lemma 10. Then for Rtr/te \u2208 [0, 1\u2212 c], we have that the second derivative of the log of the train error obeys\nd2\ndr2 log Ete =\n(\u03b1\u2212 1)2\n(1 + (1\u2212 \u03b1)r)2 + \u03b2 \u2212 1 + \u03b1\u03b2(1\u2212 r)\u03b2\u22121\u03c32rel (1\u2212 r + \u03b1(1\u2212 r)\u03b2\u03c32rel) 2 +O(n \u22121 + n\u22122\u03b1+\u03b2\u22121) (77)\n\u2265 (\u03b1\u2212 1) 2\n\u03b12 +O(n\u22121 + n\u22122\u03b1+\u03b2\u22121), (78)\nand thus Ete is strongly logarithmically convex w.r.t. r. Proof. This lemma follows from taking two derivatives of Ete as given by Lemma 10. Lemma 12. For all Rtr/te \u2208 [0, 1), we have that\nEte E\u2217te \u2265 1 + (\u03b1\u2212 1) 2 2\u03b12\n(\u221a Rtr/te \u2212 \u221a R\u2217tr/te )2 +O(n\u22121 + n\u22122\u03b1+\u03b2\u22121). (79)\nProof. It is easy to see that this holds for all Rtr/te \u2208 [0, 1 \u2212 c] for any constant c > 0. Indeed, it follows directly from Lemma 11. Note that\nlog Ete \u2212 log E\u2217te \u2265 (\u03b1\u2212 1)2\n2\u03b12\n(\u221a Rtr/te \u2212 \u221a R\u2217tr/te )2 +O(n\u22121 + n\u22122\u03b1+\u03b2\u22121), (80)\nso Ete E\u2217te\n\u2265 exp { (\u03b1\u2212 1)2\n2\u03b12\n(\u221a Rtr/te \u2212 \u221a R\u2217tr/te )2} +O(n\u22121 + n\u22122\u03b1+\u03b2\u22121), (81)\nwhich yields Equation 79 using the fact that ex > 1 + x for x \u2208 R. We now need to assure ourselves that this bound still holds for Rtr/te \u2208 (1\u2212 c, 1). Intuitively, the train-test error ratio will generally explode as Rtr/te \u2192 1, so we should expect this lower bound to hold near Rtr/te = 1 with little trouble.\nFirst, observe that there exist constants B1, n1 such that, if\nEte \u2265 B1n\u2212(\u03b2\u22121) for alln \u2265 n1 (82) on the region Rtr/te \u2208 (1\u2212 c, 1), then Equation 79 will hold on this region. (For example, one might set Rtr/te = 1 on the RHS of Equation 79, then use the fact that E\u2217te = \u0398(n\u2212(\u03b2\u22121)) (Lemma 6), then increase the constant a small amount to absorb the O(n\u22121 +n\u22122\u03b1+\u03b2\u22121) error term. Note that we are writing out the meaning of the big-\u0398 scaling notation explicitly here.) Then note that, by Lemma 5, there exist constants B2, n2 such that\nEte \u2265 B2\u03ba\u2212(\u03b2\u22121)/\u03b1 for alln \u2265 n2. (83) Thus, provided that n > max(n1, n2), we are assured of Equation 82 so long as c is small enough that \u03ba \u2265 (B2/B1)\u03b1/(\u03b2\u22121)n\u2212\u03b1. We can identify such a sufficiently small c by looking at Equation 73, which tells us that so long as n \u2265 n3 for some n3, we can be assured that \u03ba is sufficiently large when Rtr/te = 1\u2212 c for c = (B1/B2)1\u2212\u03b2\u03c0/(\u03b12 sin(\u03c0/\u03b1)). The fact that \u03ba monotonically increases with Rtr/te (Lemma 8) assures us that \u03ba will remain sufficiently large for all Rtr/te \u2208 (1\u2212 c, 1), and thus Equation 82 will continue to hold. This patches over our edge case in Lemma 12 and completes the proof.\nLemma 13. The value of the train-test error ratio that minimizes the test error is\nR\u2217tr/te = r 2 \u2217 +O(n \u22121 + n\u22122\u03b1+\u03b2\u22121), (84)\nwhere r\u2217 is either the unique solution to\n\u03b1\u2212 \u03b2 \u2212 (\u03b1\u2212 1)\u03b2r + (\u03b1\u2212 1)(1\u2212 r\u2217)\u03b2\u03c32rel = 0 (85)\nover r\u2217 \u2208 [0, 1) or else zero if no such solution exists. Proof. First, let the constant c in Lemma 10 be less than c\u2032/2, where c\u2032 is the constant prescribed by Lemma 9, so that we are assured that R\u2217tr/te \u2208 [0, 1 \u2212 2c]. Because the error terms are small relative to the quantity itself in the region Rtr/te \u2208 [0, 1 \u2212 c], we may simply take a derivative of Ete in terms of Rtr/te as given by Lemma 10, setting dEtedr = 0. This yields Equations (84) and (85). We are assured that this equation can have only one solution on the domain of interest because the function we differentiated to obtain it is strongly log-convex for r \u2208 [0, 1), as shown in proving Lemma 11. If this equation has no solution, this implies that there must be no local minimum on the domain, so the minimum over the domain must lie at (or more precisely, because we have error terms, close to) an endpoint \u2014 that is, at either Rtr/te = 0 +O(n\u2212\u03b3) or Rtr/te = 1\u2212 c+O(n\u2212\u03b3), where \u03b3 = min(1, 2\u03b1\u2212 \u03b2 + 1). However, we chose c to be small enough that Rtr/te < 1\u2212 2c+O(n\u2212\u03b3), so we can eliminate the right endpoint, and we have that R\u2217tr/te = O(n \u2212\u03b3) as desired.\nRemark. It is worth emphasizing that, because we were free from the beginning to choose c to be quite small, the rather technical patch business in the preceding proof is not really all that important and can be glossed over on a first reading. It is, however, nice to have for completeness, as covering the whole region Rtr/te \u2208 [0, 1) permits the final theorem statement to be simpler."
        },
        {
            "heading": "H.7 STATING THE FINAL THEOREM",
            "text": "Putting together Lemmas 12 and 13 gives us Theorem 2."
        },
        {
            "heading": "H.8 AN ADDITIONAL COROLLARY TO THEOREM 2",
            "text": "With zero noise, Theorem 2 simplifies to the following corollary for the optimal fitting ratio: Corollary 2. Consider a KRR task with \u03b1, \u03b2 powerlaw eigenstructure. When \u03c32 = 0, the optimal train-test risk ratio at large n converges to\nR\u2217tr/te n\u2212\u2212\u2192\n{ (\u03b1\u2212\u03b2)2\n(\u03b1\u22121)2\u03b22 if \u03b2 < \u03b1, 0 if \u03b2 \u2265 \u03b1.\n(86)\nCorollary 2 implies in particular that even if \u03b2 is slightly smaller than \u03b1, we will still find that R\u2217tr/te \u2248 0."
        }
    ],
    "year": 2023
}