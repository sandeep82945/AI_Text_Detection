{
    "abstractText": "The ability to engineer novel proteins with higher fitness for a desired property would be revolutionary for biotechnology and medicine. Modeling the combinatorially large space of sequences is infeasible; prior methods often constrain optimization to a small mutational radius, but this drastically limits the design space. Instead of heuristics, we propose smoothing the fitness landscape to facilitate protein optimization. First, we formulate protein fitness as a graph signal then use Tikunov regularization to smooth the fitness landscape. We find optimizing in this smoothed landscape leads to improved performance across multiple methods in the GFP and AAV benchmarks. Second, we achieve state-of-the-art results utilizing discrete energy-based models and MCMC in the smoothed landscape. Our method, called Gibbs sampling with Graph-based Smoothing (GGS), demonstrates a unique ability to achieve 2.5 fold fitness improvement (with in-silico evaluation) over its training set. GGS demonstrates potential to optimize proteins in the limited data regime. Code: https://github.com/kirjner/GGS",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrew Kirjner"
        },
        {
            "affiliations": [],
            "name": "Jason Yim"
        },
        {
            "affiliations": [],
            "name": "Tommi Jaakkola"
        },
        {
            "affiliations": [],
            "name": "Regina Barzilay"
        }
    ],
    "id": "SP:9157c9d549c669091a9878306b6d901be6b4163d",
    "references": [
        {
            "authors": [
                "Dave W Anderson",
                "Florian Baier",
                "Gloria Yang",
                "Nobuhiko Tokuriki"
            ],
            "title": "The adaptive landscape of a metallo-enzyme is shaped by environment-dependent epistasis",
            "venue": "Nature Communications,",
            "year": 2021
        },
        {
            "authors": [
                "Christof Angermueller",
                "David Dohan",
                "David Belanger",
                "Ramya Deshpande",
                "Kevin Murphy",
                "Lucy Colwell"
            ],
            "title": "Model-based reinforcement learning for biological sequence design",
            "year": 2020
        },
        {
            "authors": [
                "Frances H Arnold"
            ],
            "title": "Design by directed evolution",
            "venue": "Accounts of chemical research,",
            "year": 1998
        },
        {
            "authors": [
                "David Brookes",
                "Hahnbeom Park",
                "Jennifer Listgarten"
            ],
            "title": "Conditioning by adaptive sampling for robust design",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "David H Brookes",
                "Amirali Aghazadeh",
                "Jennifer Listgarten"
            ],
            "title": "On the sparsity of fitness functions and implications for learning",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Drew H Bryant",
                "Ali Bashir",
                "Sam Sinai",
                "Nina K Jain",
                "Pierce J Ogden",
                "Patrick F Riley",
                "George M Church",
                "Lucy J Colwell",
                "Eric D Kelsic"
            ],
            "title": "Deep diversification of an aav capsid protein by machine learning",
            "venue": "Nature Biotechnology,",
            "year": 2021
        },
        {
            "authors": [
                "Jeffrey Buzas",
                "Jeffrey Dinitz"
            ],
            "title": "An analysis of nk landscapes: Interaction structure, statistical properties, and expected number of local optima",
            "venue": "IEEE Transactions on Evolutionary Computation,",
            "year": 2013
        },
        {
            "authors": [
                "Christian Dallago",
                "Jody Mou",
                "Kadina E Johnston",
                "Bruce J Wittmann",
                "Nicholas Bhattacharya",
                "Samuel Goldman",
                "Ali Madani",
                "Kevin K Yang"
            ],
            "title": "Flip: Benchmark tasks in fitness landscape inference for proteins",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Emami",
                "Aidan Perreault",
                "Jeffrey Law",
                "David Biagioni",
                "Peter St John"
            ],
            "title": "Plug & play directed evolution of proteins with gradient-based discrete mcmc",
            "venue": "Machine Learning: Science and Technology,",
            "year": 2023
        },
        {
            "authors": [
                "Nathan C Frey",
                "Daniel Berenberg",
                "Karina Zadorozhny",
                "Joseph Kleinhenz",
                "Julien Lafrance-Vanasse",
                "Isidro Hotzel",
                "Yan Wu",
                "Stephen Ra",
                "Richard Bonneau",
                "Kyunghyun Cho"
            ],
            "title": "Protein discovery with discrete walk-jump sampling",
            "venue": "arXiv preprint arXiv:2306.12360,",
            "year": 2023
        },
        {
            "authors": [
                "Mahan Ghafari",
                "Daniel B. Weissman"
            ],
            "title": "The expected time to cross extended fitness plateaus",
            "venue": "Theoretical Population Biology,",
            "year": 2019
        },
        {
            "authors": [
                "Will Grathwohl",
                "Kevin Swersky",
                "Milad Hashemi",
                "David Duvenaud",
                "Chris Maddison"
            ],
            "title": "Oops i took a gradient: Scalable sampling for discrete distributions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Nate Gruver",
                "Samuel Stanton",
                "Nathan C Frey",
                "Tim GJ Rudner",
                "Isidro Hotzel",
                "Julien Lafrance-Vanasse",
                "Arvind Rajpal",
                "Kyunghyun Cho",
                "Andrew Gordon Wilson"
            ],
            "title": "Protein design with guided discrete diffusion",
            "year": 2000
        },
        {
            "authors": [
                "Elvin Isufi",
                "Fernando Gama",
                "David I Shuman",
                "Santiago Segarra"
            ],
            "title": "Graph filters for signal processing and machine learning on graphs",
            "venue": "arXiv preprint arXiv:2211.08854,",
            "year": 2022
        },
        {
            "authors": [
                "Stuart A Kauffman",
                "Edward D Weinberger"
            ],
            "title": "The nk model of rugged fitness landscapes and its application to maturation of the immune response",
            "venue": "Journal of theoretical biology,",
            "year": 1989
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Natalie Maus",
                "Haydn Jones",
                "Juston Moore",
                "Matt J Kusner",
                "John Bradshaw",
                "Jacob Gardner"
            ],
            "title": "Local latent space bayesian optimization over structured inputs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Meier",
                "Roshan Rao",
                "Robert Verkuil",
                "Jason Liu",
                "Tom Sercu",
                "Alex Rives"
            ],
            "title": "Language models enable zero-shot prediction of the effects of mutations on protein function",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel M\u00fcllner"
            ],
            "title": "Modern hierarchical, agglomerative clustering algorithms",
            "venue": "arXiv preprint arXiv:1109.2378,",
            "year": 2011
        },
        {
            "authors": [
                "Pascal Notin",
                "Mafalda Dias",
                "Jonathan Frazer",
                "Javier Marchena Hurtado",
                "Aidan N Gomez",
                "Debora Marks",
                "Yarin Gal"
            ],
            "title": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Vishakh Padmakumar",
                "Richard Yuanzhe Pang",
                "He He",
                "Ankur P Parikh"
            ],
            "title": "Extrapolative controlled sequence generation via iterative refinement",
            "venue": "arXiv preprint arXiv:2303.04562,",
            "year": 2023
        },
        {
            "authors": [
                "Roshan Rao",
                "Nicholas Bhattacharya",
                "Neil Thomas",
                "Yan Duan",
                "Xi Chen",
                "John Canny",
                "Pieter Abbeel",
                "Yun S Song"
            ],
            "title": "Evaluating protein transfer learning with tape",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "S James Remington"
            ],
            "title": "Green fluorescent protein: a perspective",
            "venue": "Protein Science,",
            "year": 2011
        },
        {
            "authors": [
                "Zhizhou Ren",
                "Jiahan Li",
                "Fan Ding",
                "Yuan Zhou",
                "Jianzhu Ma",
                "Jian Peng"
            ],
            "title": "Proximal exploration for model-guided protein sequence design",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Karen S Sarkisyan",
                "Dmitry A Bolotin",
                "Margarita V Meer",
                "Dinara R Usmanova",
                "Alexander S Mishin",
                "George V Sharonov",
                "Dmitry N Ivankov",
                "Nina G Bozhanova",
                "Mikhail S Baranov",
                "Onuralp Soylemez"
            ],
            "title": "Local fitness landscape of the green fluorescent protein",
            "year": 2016
        },
        {
            "authors": [
                "Sam Sinai",
                "Richard Wang",
                "Alexander Whatley",
                "Stewart Slocum",
                "Elina Locane",
                "Eric D Kelsic"
            ],
            "title": "Adalead: A simple and robust adaptive greedy search algorithm for sequence design",
            "year": 2010
        },
        {
            "authors": [
                "Zhenqiao Song",
                "Lei Li"
            ],
            "title": "Importance weighted expectation-maximization for protein sequence design",
            "venue": "arXiv preprint arXiv:2305.00386,",
            "year": 2023
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning",
            "year": 1929
        },
        {
            "authors": [
                "Samuel Stanton",
                "Wesley Maddox",
                "Nate Gruver",
                "Phillip Maffettone",
                "Emily Delaney",
                "Peyton Greenside",
                "Andrew Gordon Wilson"
            ],
            "title": "Accelerating bayesian optimization for biological sequence design with denoising autoencoders",
            "venue": "arXiv preprint arXiv:2203.12742,",
            "year": 2022
        },
        {
            "authors": [
                "Brandon Trabucco",
                "Aviral Kumar",
                "Xinyang Geng",
                "Sergey Levine"
            ],
            "title": "Conservative objective models for effective offline model-based optimization",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Brandon Trabucco",
                "Xinyang Geng",
                "Aviral Kumar",
                "Sergey Levine"
            ],
            "title": "Design-bench: Benchmarks for data-driven offline model-based optimization",
            "venue": "CoRR, abs/2202.08450,",
            "year": 2022
        },
        {
            "authors": [
                "James T Wilson",
                "Riccardo Moriconi",
                "Frank Hutter",
                "Marc Peter Deisenroth"
            ],
            "title": "The reparameterization trick for acquisition functions",
            "venue": "arXiv preprint arXiv:1712.00424,",
            "year": 2017
        },
        {
            "authors": [
                "Giacomo Zanella"
            ],
            "title": "Informed proposals for local mcmc in discrete spaces",
            "venue": "Journal of the American Statistical Association,",
            "year": 2020
        },
        {
            "authors": [
                "Brandon S Zerbe",
                "David R Hall",
                "Sandor Vajda",
                "Adrian Whitty",
                "Dima Kozakov"
            ],
            "title": "Relationship between hot spot residues and ligand binding hot spots in protein\u2013protein interfaces",
            "venue": "Journal of chemical information and modeling,",
            "year": 2012
        },
        {
            "authors": [
                "Dengyong Zhou",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "A regularization framework for learning from graph data",
            "venue": "In ICML 2004 Workshop on Statistical Relational Learning and Its Connections to Other Fields (SRL",
            "year": 2004
        },
        {
            "authors": [
                "Dallago"
            ],
            "title": "We utilize a 1D convolutional neural network (CNN) architecture in our model and oracle. The CNN takes in a one-hot encoded sequence as input then applies a 1D convolution with kernel width 5 followed by max-pooling and a dense layer to a single node that outputs a scalar value. It uses 256 channels throughout for a total of 157,000",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In protein engineering, fitness can be defined as performance on a desired property or function. Examples of fitness include catalytic activity for enzymes (Anderson et al., 2021) and fluorescence for biomarkers (Remington, 2011). Protein optimization seeks to improve protein fitness by altering the underlying sequences of amino acids. However, the number of possible proteins increases exponentially with sequence length, rendering it infeasible to perform brute-force search to engineer novel functions, which often require multiple mutations from the starting sequence (i.e. at least 3 (Ghafari & Weissman, 2019)). Directed evolution (Arnold, 1998) has been successful in improving protein fitness, but it requires substantial labor and time.\n\u2217Contributed equally to this work. Authors agreed ordering can be changed for their respective interests. \u2020Advised equally to this work.\nWe aim to computationally generate high-fitness proteins by optimizing a learned model of the fitness landscape, but face several challenges. Proteins can be notorious for highly non-smooth fitness landscapes1: fitness can change dramatically with single mutations, fitness measurements contain experimental noise, and most protein sequences have zero fitness (Brookes et al., 2022). Furthermore, protein fitness datasets are scarce and difficult to generate due to their high costs (Dallago et al., 2021). As a result, machine learning (ML) methods are susceptible to predicting false positives and getting stuck in local optima (Brookes et al., 2019). The 3D protein structure, if available, can provide information in navigating the noisy fitness landscape such as identifying hot spot residues (Zerbe et al., 2012), but high quality structures are not available in many cases.\nOne way to deal with noisy and limited data is to regularize the fitness landscape model2. Our work considers a smoothing regularizer in which similar sequences (based on a distance measure) are predicted to have similar predicted fitness. While actual fitness lanscapes are not smooth, smoothing can be an important tool in the context of optimization, allowing gradient-based methods to reach higher peaks by avoiding local optima, especially in discrete optimization (Zanella, 2020). A few works have studied properties of protein fitness landscapes (Section 2), but none have directly applied smoothing with a graph framework during optimization.\nWe propose a novel method for applying smoothing to protein sequence and fitness data together with an optimization technique that takes advantage of the smoothing. First, we formulate sequences as a graph with fitness values as node attributes and apply Tikunov regularization to smooth the topological signal measured by the graph Laplacian. The smoothed data is then fitted with a neural network to be used as a model for discrete optimization (Figure 1 top). Second, we sample over the energy function for high fitness sequences by using the model\u2019s gradients in a Gibbs With Gradients (GWG) procedure (Grathwohl et al., 2021). In GWG, a discrete distribution is constructed based on the model\u2019s gradients where mutations with improved fitness will correlate with higher probability. The process of taking gradients and sampling mutations is performed in an iterative fashion where subsequent mutations will guide towards higher fitness (Figure 1 bottom).\n1Landscape refers to the sequence to fitness mapping. 2In the sequel, we will use \u201cmodel\u201d when referring to the fitness landscape model.\nFigure 1 shows an overview of the method. We refer to the procedure of smoothing then sampling as Gibbs sampling with Graph-based Smoothing (GGS). To evaluate our method, we introduce a set of tasks using the well studied Green Fluorescent Proteins (GFP) (Sarkisyan et al., 2016) and Adeno-Associated Virus (AAV) (Bryant et al., 2021) proteins. We chose GFP and AAV because of their real-world importance and availability of large mutational data. We design a set of tasks that emulate starting with noisy and limited data and evaluate with a trained model (as done in most prior works). We evaluate GGS and prior works on our proposed benchmarks to show that GGS is state-of-the-art in GFP and AAV fitness optimization. Our contributions are summarized as follows:\n\u2022 We develop a novel sequence-based protein optimization algorithm, GGS, which uses graph-based smoothing to train a smoothed fitness model. The model is used as a discrete energy function to progressively sample mutations towards higher-fitness sequences with GWG (Section 3).\n\u2022 We develop a set of tasks that measure a method\u2019s ability to extrapolate towards higher fitness. We use publicly available GFP and AAV datasets to emulate difficult optimization scenarios of starting with limited and noisy data (Section 4.1).\n\u2022 Our benchmark shows prior methods fail to extrapolate towards higher fitness. However, we show graph-based smoothing can drastically improve their performance; in one baseline, the fitness jumps from 18% to 39% in GFP and 4% to 44% in AAV after smoothing (Section 4.2).\n\u2022 Our method GGS directly exploits smoothness to achieve state-of-the-art results with 5 times higher fitness in GFP and 2 times higher in AAV compared to the next best method (Section 4.2)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Protein optimization and design. Approaches can broadly be categorized using sequence, structure or both. Sequence-based methods have been explored through the lens of reinforcement learning (Angermueller et al., 2020), latent space optimization (Stanton et al., 2022; Lee et al.; Maus et al., 2022), generative models (Notin et al., 2022; Meier et al., 2021; Jain et al., 2022; Gruver et al., 2023), and model-based directed evolution (Sinai et al., 2020; Padmakumar et al., 2023; Ren et al., 2022). Together they face the issue of a noisy fitness landscape to optimize. We focus on sequence-based methods using Gibbs With Gradients (GWG) (Grathwohl et al., 2021) which can perform stateof-the-art in discrete optimization but requires a smooth energy function for strong performance. Concurrently, Emami et al. (2023) used GWG for protein optimization with a product of experts distribution using a protein language model. However, they achieved subpar results.\nPrevious methods focused on developing new sampling and optimization techniques. Our work is complimentary by addressing the need for improved regularization and smoothing. We show in our experiments that our smoothing technique can enhance the performance of prior methods.\nProtein fitness regularization. The NK model was an early attempt to model smoothness of protein fitness through a statistical model of epistasis (Kauffman & Weinberger, 1989). Brookes et al. (2022) proposed a framework to approximate the sparsity of protein fitness using a generalized NK model (Buzas & Dinitz, 2013). Concurrently, dWJS (Frey et al., 2023) is most related to our work by utilizing Gaussian noise to regularize the discrete energy function during Langevin MCMC. dWJS trains by denoising to smooth a energy-based model whereas we apply discrete regularization using graph-based smoothing techniques.\nFinally, we distinguish our smoothing method from traditional regularizers applied during training such as dropout (Srivastava et al., 2014). Our goal is to smooth the fitness landscape in a way that is amenable for iterative optimization. We enforce similar sequences to have similar fitness which is not guaranteed with dropout or similar regularizers applied in minibatch training. Evaluating multiple smoothing strategies is not the focus of our work, but rather to demonstrate their importance."
        },
        {
            "heading": "3 METHOD",
            "text": "The following describes our method. Section 3.1 details the problem formulation. Next section 3.2 describes the procedure for training a smoothed model. Lastly, section 3.3 provides background on Gibbs With Gradients (GWG) which is adapted for protein optimization. The full algorithm, Gibbs sampling with Graph-based Smoothing (GGS), is presented in Algorithm 1."
        },
        {
            "heading": "3.1 PROBLEM FORMULATION",
            "text": "We denote the starting set of N proteins as D = (X,Y ) where X = {x1, . . . , xN} \u2282 VM are the sequences and Y = {y1, . . . , yN} are corresponding real-valued scalar fitness measurements. Each sequence xi \u2208 VM is composed of M residues from a vocabulary V of 20 amino acids. Subscripts refer to different sequences. Note our method can be extended to other modalities, e.g. nucleic acids.\nFor in-silico evaluation, we denote the set of all known sequences and fitness measurements as D\u2217 = (X\u2217, Y \u2217). We assume there exists a unknown black-box function g : VM \u2192 R such that g(x\u2217) = y\u2217. In practice, g needs to be approximated by a evaluator model, g\u03d5, trained with weights \u03d5 to minimize prediction error on D\u2217. g\u03d5 poses a limitation to evaluation since the true fitness needs to be verified with biological experiments. Nevertheless, an in-silico approximation provides a accessible way for evaluation and is done in all prior works. The starting dataset is a strict subset of the known dataset D \u2282 D\u2217 to simulate fitness optimization scenarios. Given D, our task is to generate a set of sequences with higher fitness than the starting set."
        },
        {
            "heading": "3.2 GRAPH-BASED SMOOTHING ON PROTEINS",
            "text": "Our goal is to develop a model of the sequence-to-fitness mapping that can be utilized when sampling higher fitness sequences. Unfortunately, the high-dimensional sequence space coupled with few data points and noisy labels can result in a noisy model that is prone to sampling false positives or getting stuck in local optima. To address this, we use smoothing techniques from graph signal processing.\nThe smoothing process is depicted in Figure 2. First, we train a noisy fitness model f\u03b8\u0303 : VM \u2192 R with weights \u03b8\u0303 on the initial dataset D using Mean-Squared Error (MSE). D is usually very small in real-world scenarios. We augment the dataset by using f\u03b8\u0303 to infer the fitness of neighboring sequences which we do not have labels for \u2013 known as transductive inference. Neighboring sequences are generated by randomly applying point mutations to each sequence in X . The augmented and original sequences become nodes, V , in our graph while their fitness labels are node attributes. Edges, E , are constructed with a k-nearest neighbor (kNN) graph around each node based on the Levenshtein distance3. The graph construction algorithm can be found in Algorithm 4.\nThe following borrows techniques from Isufi et al. (2022). The smoothness of the fitness variability in our protein graph is defined as the sum over the square of all local variability,\nTV2(Y ) = 1\n2 \u2211 i\u2208V (\u2206yi) 2, \u2206yi = \u221a \u2211 (i,j)\u2208E (yi \u2212 yj)2.\nTV refers to Total Variation and \u2206yi is the local variability of node i that measures local changes in fitness. Using TV2 as a regularizer, we solve the following optimization problem, known as Tikhunov regularization (Zhou & Scho\u0308lkopf, 2004), for a new set of smoothed fitness labels,\nargmin Y\u0302 \u2208R|V |\n\u2225Y \u2212 Y\u0302 \u222522 + \u03b3 TV2(Y\u0302 ). (1)\nWith abuse of notation, we represent Y as a vector with each node\u2019s fitness. \u03b3 is a hyperparameter set to control the smoothness; too high can lead to underfitting. We experiment with different \u03b3\u2019s in Section 4. Since eq. (1) is a quadratic convex problem, it has a closed form solution, Y\u0302 = (I+\u03b3L)\u22121Y where L is the graph Laplacian and I is the identity matrix. The final step is to retrain the model on the sequences in the graph and their smoothed fitness labels. The result will be a model f\u03b8 with lower TV2 than before and thus improved smoothness. The smoothing algorithm is in Algorithm 2."
        },
        {
            "heading": "3.3 SAMPLING IMPROVED FITNESS WITH GIBBS",
            "text": "Equipped with model f\u03b8 from section 3.2, we apply it in a procedure to sample mutations that improve the starting sequences\u2019 fitness. f\u03b8 can also be viewed as an energy-based model (EBM) that defines a Boltzmann distribution log p(x) = f\u03b8(x)\u2212 logZ where Z is the normalization constant. Higher fitness sequences will be more likely under this distribution, while sampling will induce diversity and novelty. To sample from p(x), we use Gibbs With Gradients (GWG) Grathwohl et al. (2021) which\n3Defined as the minimum number of mutations between two sequences.\nhas attracted significant interest due to its simplicity and state-of-the-art performance in discrete optimization. In this section, we describe the GWG procedure for protein sequences. GWG uses Gibbs sampling with approximations of locally informed proposals (Zanella, 2020):\nq(x\u2032|x) \u221d exp\n( 1\n2 \u2211 i (x\u2032i) \u22a4d\u03b8(x)i ) 1(x\u2032 \u2208 H(x)), d\u03b8(x)i = [\u2207xf\u03b8(x)]i \u2212 xi \u2299 [\u2207f\u03b8(x)]i .\n(2) With slight abuse of notation, we use the one-hot sequence representation x \u2208 {0, 1}M\u00d7|V| where xi \u2208 {0, 1}|V| represents the ith index of the sequence with 1 at its amino acid index and 0 elsewhere. \u2299 is the element wise product. H(x) = {y \u2208 VM : dHamming(x, y) \u2264 1} is the 1-ball around x using Hamming distance. The core idea of GWG is to use d\u03b8(x)i as the first order approximation of a continuous gradient of the change in likelihood from mutating the ith index of x to a different amino acid. The quality of the proposals in eq. (2) rely on the smoothness of the energy f\u03b8 (Theorem 1 in Grathwohl et al. (2021)). If the gradients, \u2207f\u03b8, are noisy, then the proposal distributions are ineffective in sampling better sequences. Hence, smoothing f\u03b8 is desirable (see section 4).\nThe choice of H(\u00b7) as the 1-Hamming ball limits x\u2032 to point mutations from x and only requires O (M \u00d7 |V|) compute to construct. Let the point mutation where x and x\u2032 differ be defined by the residue location, iloc \u2208 {1, . . . ,M}, and amino acid substitution, jsub \u2208 {1, . . . , |V|}. By limiting x\u2032 to point mutants (iloc, jsub), sampling q(x\u2032|x) is equivalent to sampling the following,\n(iloc, jsub) \u223c q(\u00b7|x) = Cat ( Softmax ({ d\u03b8(x)i,j\n\u03c4\n}M,|V| i=1,j=1 )) (3)\nwhere \u03c4 is the sampling temperature and d\u03b8(x)i,j is the logits of mutating to (i, j). The proposal sequence x\u2032 is constructed by setting its iloc residue to jsub and equal to x elsewhere. Each proposed sequence is accepted or rejected using Metropolis-Hasting (MH),\nmin ( exp(f\u03b8(x\n\u2032)\u2212 f\u03b8(x)) q(x|x\u2032) q(x\u2032|x) , 1\n) . (4)\nWe provide the GWG algorithm in Algorithm 3.\nClustered sampling. GWG requires a starting sequence to start mutating. A reasonable starting set are the sequences X used to train the model. On each round r, we use eq. (3) to propose Nprop mutations for each sequence. If accepted via eq. (4), then the mutated sequence will be added to the next round. However, this procedure can lead to an intractable number of sequences to consider.\nTo control compute bandwidth, we perform hierarchical clustering (Mu\u0308llner, 2011) on all the sequences in a round and take the sequence of each cluster with the highest predicted fitness using f\u03b8. Let C be the number of clusters which we set based on amount of available compute. This procedure, known as Reduce, is,\nReduce(X; \u03b8) = C\u22c3\nc=1\n{argmax x\u2208Xc f\u03b8(x)} where {Xc}Cc=1 = Cluster(X; C). (5)\nEach round r reduces the sequences from the previous round and performs GWG sampling.\nX\u0303r = Reduce(Xr; \u03b8), Xr+1 = GWG(X\u0303r; \u03b8)\nTo summarize, we adapted GWG for protein optimization by developing a smoothed model to satisfy GWG\u2019s smoothness assumptions and use clustering during sampling to reduce redundancy and compute. An illustration of clustered sampling is provided in Figure 5.\nThe full algorithm for smoothing and clustered sampling is provided in Algorithm 1.\nAlgorithm 1 GGS: Gibbs sampling with Graph-based Smoothing Require: Starting dataset: D = (X,Y )\n1: \u03b8\u0303 \u2190 argmax\u03b8\u0303 E(x,y)\u223cD [ (y \u2212 f\u03b8\u0303(x))2 ] \u25b7 Initial training 2: \u03b8 \u2190 Smooth(D; \u03b8\u0303) \u25b7 GS algorithm 2 3: for r = 0, . . . , R\u2212 1 do 4: X\u0303r \u2190 Reduce(Xr; \u03b8) 5: Xr+1 \u2190 GWG(X\u0303r; \u03b8) \u25b7 GWG algorithm 3 6: end for 7: Return TopK(XR) \u25b7 Return Top-K best sequences based on predicted fitness f\u03b8"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Our experiments demonstrate the benefits of smoothing in protein optimization. Section 4.1 presents a set of challenging tasks based on the GFP and AAV proteins that emulate starting with experimental noise and a sparsely sampled fitness landscape. Section 4.2 evaluates the performance of baselines and our method, GGS, on our benchmark. In addition, we find applying smoothing improves performance for two baselines. Section 4.3 provides sweeps over hyperparameters and analysis of GGS.\nBaselines. We choose a representative set of prior works that evaluated on GFP and AAV: GFlowNets (GFN-AL) (Jain et al., 2022), model-based adaptive sampling (CbAS) (Brookes et al., 2019), greedy search (AdaLead) (Sinai et al., 2020), bayesian optimization (BO-qei) (Wilson et al., 2017), conservative model-based optimization (CoMs) (Trabucco et al., 2021), and proximal exploration (PEX) (Ren et al., 2022). NOS (Gruver et al., 2023) performs protein optimization with diffusion models. However, their framework is tailored to antibody optimization and requires non-trivial modifications for general proteins. We were unable to evaluate Song & Li (2023) due to unrunnable public code.\nGGS implementation. We use a 1D CNN (see Appendix B.1 for architecture and training) for model f\u03b8. To ensure a fair comparison, we use the same model architecture in baselines when possible. In graph-based smoothing (GS), we augment the graph until it has Nnodes = 250, 000 nodes. We found larger graphs to not give improvements. Similarly, we use \u03c4 = 0.1, R = 15 rounds and Nprop = 100 proposals per round during GWG at which sequences would converge and more sampling did not give improvements. We choose the smoothing weight \u03b3 = 1.0 through grid search. We study sensitivity to hyperparameters, especially \u03b3, in Section 4.3."
        },
        {
            "heading": "4.1 BENCHMARK",
            "text": "We develop a set of tasks based on two well-studied protein systems: Green Fluoresent Protein (GFP) and Adeno-Associated Virus (AAV) (Sarkisyan et al., 2016; Bryant et al., 2021). These were chosen due to their relatively large amount of measurements, 56,806 and 44,156 respectively, with sequence variability of up to 15 mutations from the wild-type. Other datasets are either too small or do not have enough sequence variability. GFP\u2019s fitness is its fluorescence properties as a biomarker while for AAV\u2019s is the ability to package a DNA payload, i.e. for gene delivery. We found GFP and AAV to suffice in demonstrating how prior methods fail to extrapolate.\nOne measure of difficulty is the number of mutations required to achieve the highest known fitness; this assesses a method\u2019s exploration capability. We designate the set of optimal proteins, X99th, as any sequence in the 99th fitness percentile in the entire dataset4. Quantitatively, we compute the minimum number of mutations required from the training set to achieve the optimal fitness:\nGap(X0;X99th) = min({dist(x, x\u0303) : x \u2208 X, x\u0303 \u2208 X99th}). (6) 4This may differ from the true optimal protein found in nature. Unfortunately, we must work with existing\ndatasets since every possible protein cannot be experimentally measured.\nA high mutational gap would require the method discovering many mutations in a high dimensional space. A second measure of difficulty is the fitness range of the starting set of sequences. Starting with a low range of fitness requires the method to learn from barely functional proteins and exploit limited knowledge to find mutations that confer higher fitness. Appendix A shows Gap and starting rate are necessary as we found the previous GFP benchmark (Trabucco et al., 2022) as too \u201ceasy\u201d by only requiring one mutation to achieve the optimal fitness.\nRecall the protein optimization task is to use the starting set D to propose a set of sequences with higher fitness. We design two difficulties, medium and hard, for GFP and AAV based on the properties of D. We restricted the range and the mutational gap to modulate task difficulty. We found Gap= 7 and Range < 30% to suffice in finding where our baseline methods fail to discover better proteins. We use this setting as the hard difficulty and sought to develop GGS to solve it.\nTable 1: GFP tasks\nDifficulty Range (%) Gap |D|\nMedium 20th-40th 6 2828 Hard < 30th 7 2426\nTable 2: AAV tasks\nDifficulty Range (%) Gap |D|\nMedium 20th-40th 6 2139 Hard < 30th 7 3448\nIn-silico evaluation. We follow prior works in using a trained evaluator model as a proxy for realworld experimental validation. A popular model choice is the TAPE transformer (Rao et al., 2019). However, we noticed a poor performance of the transformer compared to a simpler CNN that matches the findings of Dallago et al. (2021). We use CNN architecture for the evaluator due to its superior performance. Following Jain et al. (2022), each method generates 128 samples X\u0302 = {x\u0302i}128i=1 whose approximated fitness is predicted with the evaluator. We additionally report Diversity and Novelty that are also used in Jain et al. (2022). Descriptions of these metrics can be found in Appendix B.2 We emphasize that higher diversity and novelty are not equivalent to better performance, but provide insight into the exploration and exploitation trade-offs of different methods. For instance, a random algorithm would achieve maximum diversity and novelty."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "We run 5 seeds and report the average metric across all seeds including the standard deviation in parentheses. We evaluate GGS and previously described baselines. To ensure a fair comparison, we use the same CNN architecture as the model across all methods \u2013 all our baselines (and GGS) perform model-based optimization. Since graph-based smoothing (GS) is a general technique, we sought to evaluate its effectiveness in each of our baselines. To incorporate GS, we used the smoothed predictor as a replacement in each baseline which will be denoted with \u201c+ GS\u201d. Table 3 summarizes GFP results while table 4 summarizes AAV.\nGGS substantially outperforms all unsmoothed baselines, consistently achieving a improvement in fitness from the starting range of fitness in each difficulty. However, the smoothed baselines (lines with + GS) demonstrated a up to three fold improvement for CbAS, AdaLead. We find larger improvements in GFP where the sequence space is far larger than AAV \u2013 suggesting the GFP fitness landscape is harder to optimize over.\nThe most difficult task is clearly hard difficulty on GFP where all the baselines without smoothing cannot achieve fitness higher than the training set. With smoothing, GGS achieves the best fitness since the sampling procedure uses gradient-based proposals that benefit from a smooth model. Appendix C.2.1 presents results on additional difficulties to analyze GGS beyond hard..\nWe observe GGS is able to achieve the highest fitness while exhibiting respectable diversity and novelty. Notably, GGS\u2019s novelty falls within the range of the mutational gap in each difficulty, suggesting it is extrapolating an appropriate amount for each task. Our sampling procedure, GWG, fails to perform without smoothing which agrees with its theoretical requirements of requiring a smooth model for good performance. We conclude smoothing is a beneficial technique not only for GGS but also for some baselines. GGS is able to achieve state-of-the-art results in our benchmark."
        },
        {
            "heading": "4.3 ANALYSIS",
            "text": "We analyze the effect of varying the following hyperparameters: number of nodes Nnodes in the protein graph, smoothness weight \u03b3 in eq. (1), and number of sampling rounds R during GWG sampling. For space, we leave the analysis of the sampling temperature \u03c4 in appendix C.1. Figure 3 presents the results of running GGS with different hyperparameters on the hard difficulty of GFP and AAV. Along the X-axis, we plot the median performance of the sequences during each round of GWG where r = 0 is initialization and r = 15 are the sequences and the end of GWG. The Y-axis shows the predicted fitness of the smoothed model in blue while the fitness scored with our is shown in red. Interestingly, we find in the majority of cases the smoothed model\u2019s predictions are highly correlated\nwith the evaluator along the sampling trajectory. This is despite the model being trained on 4% of the data with the hard filtering. Appendix C.2.2 shows the prediction error where we find smoothing greatly improves in predicting the fitness of unseen sequences despite having higher train error.\nGraph size. We find Nnodes = 250, 000 nodes to have the best performance over a smaller graph with 100,000 nodes. Larger graphs allow for better approximation of the fitness landscape. However, larger graphs require more compute. A future direction could be to determine optimal graph size with different node augmentations strategies than random mutations.\nSmoothing. Too much smoothing \u03b3 = 10.0 can lead to worse performance in AAV while GFP is not sensitive. This suggests the optimal \u03b3 is dependent on the particular fitness landscape. Since real proteins landscapes are unknown, the biggest limitation of our method is determining the optimal \u03b3. An important extension of GGS is to theoretically characterize landscapes (Buzas & Dinitz, 2013) and provide guidelines of selecting \u03b3.\nSampling convergence. We find a set number of rounds are required for GWG sampling to converge when the landscape is smooth enough (middle and right column). We find additional rounds are unnecessary; in practice, more rounds can be ran to ensure convergence. Results on sweeping the temperature are in Appendix C.1 where we see 0.1 clearly performs the best for GFP and AAV."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "We present Gibbs sampling with Graph-based Smoothing (GGS) for protein optimization with a smoothed fitness landscape. Our main contribution and insight is a novel application of graph signal processing to protein optimization. We show smoothing is not only beneficial to our method but also to our baselines. To evaluate, we designed a suite of tasks around two measure of difficulty: number of edits to achieve the 99th percentile (mutational gap) and starting range of fitness. All baselines struggled to achieve good performance on our tasks. However, some baselines showed a three fold improvement with smoothing. GGS performed the best by combining Gibbs with gradients\nwith a smoothed model \u2013 demonstrating the synergy of gradient-based sampling with a smooth discrete energy-based model. Our results highlight the benefits of optimizing over a smooth landscape that may not be reflective of the true fitness landscape. We believe it\u2019s important to investigate how regularization can be used to transform protein fitness data to be compatible with modern optimization algorithms. Our goal is to not learn the excess biological noise, but find the signal in the data to discover the best protein. We conclude with limitations.\nEvaluation limitations. The results demonstrate strong evidence of using smoothing given its improvement in multiple methods. Despite this, our evaluations follow prior works by utilizing an trained model for evaluation. This can be unreliable compared to testing out sequences with wet-lab validation. Unfortunately, wet-lab validation can be cost and time intensive. The ultimate test would be to use GGS in an active learning or experimental pipeline with wet-lab validation in the loop.\nMethod limitations. Our method utilizes several hyperparameters such as the graph size and smoothing parameter \u03b3. We demonstrated the effects of each hyperparameter in Section 4.3. Given the success of smoothing, it is desirable to find systematic ways to determine optimal hyperparameters based on an approximation of the underlying fitness landscape. We demonstrated our hyperparameter choices are not specific to either AAV or GFP, but this does not guarantee optimality for new landscapes. We believe the connections between spectral graph theory and protein optimization has more to give in advancing the important problem of protein optimization."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors thank Hannes Sta\u0308rk, Rachel Wu, Nathaniel Bennett, Sean Murphy, Jaedong Hwang, Josef S\u030civic, and Toma\u0301s\u030c Pluskal for helpful discussion and feedback.\nJY was supported in part by an NSF-GRFP. JY, RB, and TJ acknowledge support from NSF Expeditions grant (award 1918839: Collaborative Research: Understanding the World Through Code), Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, the Abdul Latif Jameel Clinic for Machine Learning in Health, the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program, the DARPA Accelerated Molecular Discovery program and the Sanofi Computational Antibody Design grant. IF is supported by the Office of Naval Research, the Howard Hughes Medical Institute (HHMI), and NIH (NIMH-MH129046). RS was partly supported by the European Regional Development Fund under the project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468), the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90254), and the MISTI Global Seed Funds under the MIT-Czech Republic Seed Fund."
        },
        {
            "heading": "A ADDITIONAL GFP ANALYSIS",
            "text": "Design-bench difficulty. Prior works have used the GFP task introduced by design-bench (DB), a suite of model-based reinforcement learning tasks (Trabucco et al., 2022), which samples a starting set of 5,000 sequences from the 50-60th percentile fitness range. However, we found this task to be too easy in the sense only one mutation was required from sequences in the training set to achieve the 99th percentile. We quantify this difficulty using the mutational gap described in eq. (6). Our proposed medium and hard difficulties (Section 4.1) require many more mutations to reach the top fitness percentile, see Figure 4. Similar issues may be present in other benchmarks."
        },
        {
            "heading": "B ADDITIONAL METHODS",
            "text": ""
        },
        {
            "heading": "B.1 CNN ARCHITECTURE",
            "text": "We utilize a 1D convolutional neural network (CNN) architecture in our model and oracle. The CNN takes in a one-hot encoded sequence as input then applies a 1D convolution with kernel width 5 followed by max-pooling and a dense layer to a single node that outputs a scalar value. It uses 256 channels throughout for a total of 157,000 parameters. Despite its simplicity, we find the CNN to outperform Transformers. Indeed, this corroborates the results in Dallago et al. (2021) that a simple CNN can be effective in low data regimes.\nTraining is performed with batch size 1024, ADAM optimizer (Kingma & Ba, 2014) (with \u03b21 = 0.9, \u03b22 = 0.999), learning rate 0.0001, and 50 epochs, using a single A6000 Nvidia GPU."
        },
        {
            "heading": "B.2 METRICS",
            "text": "We provide mathematical definitions of each metric. Note g\u03d5 is the evaluator trained to predict the approximate fitness as a proxy for experimental validation.\n\u2022 (Normalized) Fitness = median({\u03be(x\u0302i;Y \u2217)} Nsamples i=1 ) where \u03be(x\u0302;Y \u2217) = g\u03d5(x\u0302i)\u2212min(Y \u2217) max(Y \u2217)\u2212min(Y \u2217) is\nthe min-max normalized fitness based on the lowest and highest known fitness in Y \u2217.\n\u2022 Diversity = median({dist(x, x\u0303) : x, x\u0303 \u2208 X\u0302, x \u0338= x\u0303}) is the average sample similarity.\n\u2022 Novelty = median({\u03b7(x\u0302i;X)} Nsamples i=1 ) where \u03b7(x;X) = min({dist(x, x\u0303) : x\u0303 \u2208 X\u2217, x\u0303 \u0338= x})\nis the minimum distance of sample x to any of the starting sequences X .\nAlgorithm 2 Smooth: Graph-based Smoothing Require: Sequences: X Require: Noisy model weights: \u03b8\u0303\n1: V,E \u2190 CreateGraph(X) \u25b7 Construct graph (Algorithm 4). 2: L\u2190 GraphLaplacian(V,E) \u25b7 Compute graph Laplacian. 3: Y \u2190 [f\u03b8\u0303(x1), . . . , f\u03b8\u0303(xNnodes)]\u22a4 4: Y\u0302 \u2190 (I+ \u03b3L)\u22121Y \u25b7 Compute smoothed fitness labels. 5: \u03b8 \u2190 argmax\u03b8 E(x,y\u0302)\u223c(V,Y\u0302 ) [ (y\u0302 \u2212 f\u03b8(x))2 ] \u25b7 Train on smoothed dataset. 6: Return \u03b8\nAlgorithm 3 GWG: Gibbs With Gradients Require: Parent sequences: X Require: Model weights: \u03b8\n1: X \u2032 \u2190 \u2205 2: for x \u2208 X do 3: for i = 1, . . . , Nprop do \u25b7 Number of proposals per sequence. 4: x\u2032 \u2190 x 5: (iloc, jsub) \u223c q(\u00b7|x) \u25b7 Sample index and token eq. (3) 6: x\u2032iloc \u2190 Vjsub \u25b7 Apply mutation 7: if accept using eq. (4) then 8: X \u2032 \u2190 X \u2032 \u222a {x\u2032} 9: end if 10: end for 11: end for 12: Return X \u2032 \u25b7 Return accepted sequences.\nAlgorithm 4 CreateGraph Require: Sequences: X\n1: V \u2190 X \u25b7 Construct nodes. 2: while |V | \u2264 Nnodes do 3: x \u223c U(V ) 4: x\u2032 \u2190 PointMutation(x) \u25b7 Sample a point mutation uniformly at random. 5: end while 6: E \u2190 \u22c3 x\u2208V kNN(x;V ) \u25b7 Construct edges (Algorithm 5). 7: Return (V,E)\nAlgorithm 5 kNN Require: Current node: x Require: All nodes: V\n1: D(x)\u2190 \u22c3\nx\u2032\u2208V/{x} dist(x \u2032, x) \u25b7 Levenstein distance between every pair of sequences.\n2: X \u2032 \u2190 TopK(D(x), V ) \u25b7 Compute K closest sequences to x. 3: E(x)\u2190 \u22c3 x\u2032\u2208X \u2032(x, x\n\u2032) \u25b7 Construct neighborhood around x. 4: Return E(x)"
        },
        {
            "heading": "C ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 SAMPLING TEMPERATURE SWEEP",
            "text": "We determine the effect of different tmperatures \u03b3 when running GGS on the hard difficulty for GFP and AAV. All other hyperparameters follow those used in the main results, see Section 4.2. Table 5 shows the results where clearly \u03b3 = 0.1 performs the best for both AAV and GFP."
        },
        {
            "heading": "C.2 SMOOTHING ANALYSIS",
            "text": "In this section, we provide further analyses into the effect of smoothing on performance of GGS, extrapolation to unseen data, and acceptance rate of the GWG sampling procedure. Throughout, we use the same parameters \u03c4 = 0.1, \u03b3 = 1, r = 15, Nnodes = 250, 000 as in the main text."
        },
        {
            "heading": "C.2.1 ADDITIONAL BENCHMARKS",
            "text": "We first define additional benchmarks, one easier, and three harder, for each protein dataset.\nTable 6: GFP extra tasks\nDifficulty Range (%) Gap |D|\nEasy 50th-60th 0 5609 Harder1 < 30th 8 1129 Harder2 < 20th 8 792 Harder3 < 10th 8 397\nTable 7: AAV extra tasks\nDifficulty Range (%) Gap |D|\nEasy 50th-60th 0 4413 Harder1 < 30th 13 1157 Harder2 < 20th 13 920 Harder3 < 10th 13 476\nWe note that the \u201ceasy\u201d GFP task is equivalent to the design-bench baseline that is sometimes used as a benchmark in protein engineering tasks. Due to experimental noise, protein variants are assayed multiple times, and can be assigned multiple fitness values, which means the fitness values of one sequence may occupy a large percentile range. In the case of this task, multiple measurements of the wildtype GFP fitness are found in the 50th-60th percentile range. Because WT GFP is also a \u201ctop sequence,\u201d this task necessarily has a mutational gap of 0. Due to this leakage, we develop our own benchmarks in the main text, and extend those to AAV."
        },
        {
            "heading": "C.2.2 HOW SMOOTHING AFFECTS PERFORMANCE",
            "text": "The following two tables show how a smoothed model outperforms its unsmoothed counterpart according to our evaluator across all GFP/AAV benchmarks except AAV Harder2 (see (\u2217)), and GFP Harder3, where the smoothing was not sufficient to induce successful GWG sampling (see Table 10).\nFor the GFP task, our model fails (achieves 0 median fitness) when we restrict the data to the 10th percentile and mutation gap 8 for GFP where |D| = 397.\n(\u2217): The unsmoothed model only outperforms its smoothed counterpart when applying GWG to the unsmoothed model generates only a few unique sequences nearby to the starting set (as evidenced by the low novelty for this benchmark)\nFor AAV, we find the model is able to still find signal and achieve 0.384 evaluated fitness despite the data being limited to the 10th percentile and mutation gap of 13 where |D| = 476. It is notable, though, that the performance improvements gained from smoothing are smaller than in the case of GFP. Presumably, this is due to the vastly reduced dimension of the AAV sequence space in comparison to that of GFP, which may result in a neural network to learn a smoother landscape without any regularization."
        },
        {
            "heading": "C.2.3 HOW SMOOTHING AFFECTS EXTRAPOLATION + SAMPLING",
            "text": "The following tables show the benefits of smoothing on extrapolation to held out ground truth experimental data, up to a certain difficulty benchmark, as well as how smoothing vastly improves the acceptance rate for the GWG sampling procedure.\nFor each benchmark category, we evaluated the impact of smoothing on extrapolation abilities by analyzing the Mean Absolute Error (MAE) of the models on that benchmark\u2019s training and holdout datasets from the experimental ground truth. The effectiveness of smoothing was indicated by reduced MAE values on the holdout set. We also find that the MAE on the training set is lower for the unsmoothed models, as expected. In line with the results of the previous section, the effect of smoothing is reduced for AAV. As task difficulty increases, for both proteins, the effectiveness of smoothing on extrapolation decreases, which we expect as any signal leading from the training set to the fitter sequences gets obscured as training set size decreases.\nFinally, we note that in every case except two, smoothing dramatically increases acceptance rate of the GWG sampling procedure, which aligns with the inversely proportional relationship between smoothness of the energy function and sampling efficiency. In the case of the hardest GFP task, even the the smoothed model had overfit to the training set. As for the GFP medium task, we suspect that this particular section of the experimental dataset allowed the unsmoothed model to learn a smooth landscape initially."
        }
    ],
    "title": "IMPROVING PROTEIN OPTIMIZATION WITH SMOOTHED FITNESS LANDSCAPES",
    "year": 2024
}