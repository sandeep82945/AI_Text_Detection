{
    "abstractText": "Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts k\u2217 is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when k\u2217 becomes unknown and the true model is over-specified by a Gaussian mixture of k experts where k > k\u2217, our findings suggest that the number of experts selected from the top-K sparse softmax gating function must exceed the total cardinality of a certain number of Voronoi cells associated with the true parameters to guarantee the convergence of the density estimation. Moreover, while the density estimation rate remains parametric under this setting, the parameter estimation rates become substantially slow due to an intrinsic interaction between the softmax gating and expert functions.",
    "authors": [
        {
            "affiliations": [],
            "name": "GATING MIXTURE"
        },
        {
            "affiliations": [],
            "name": "Huy Nguyen"
        },
        {
            "affiliations": [],
            "name": "Pedram Akbarian"
        }
    ],
    "id": "SP:8018da34ca8049d533ff3e6f665704965dd3f2b5",
    "references": [
        {
            "authors": [
                "S. Balakrishnan",
                "M.J. Wainwright",
                "B. Yu"
            ],
            "title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
            "venue": "Annals of Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "H. Bao",
                "W. Wang",
                "L. Dong",
                "Q. Liu",
                "O-K. Mohammed",
                "K. Aggarwal",
                "S. Som",
                "S. Piao",
                "F. Wei"
            ],
            "title": "VLMo: Unified vision-language pre-training with mixture-of-modality-experts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Bengio"
            ],
            "title": "Deep learning of representations: Looking forward",
            "venue": "In International Conference on Statistical Language and Speech Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Faicel Chamroukhi",
                "Allou Sam\u00e9",
                "G\u00e9rard Govaert",
                "Patrice Aknin"
            ],
            "title": "Time series modeling by a regression approach based on a latent process",
            "venue": "Neural Networks,",
            "year": 2009
        },
        {
            "authors": [
                "T. Chen",
                "Z. Zhang",
                "A. Jaiswal",
                "S. Liu",
                "Z. Wang"
            ],
            "title": "Sparse moe as the new dropout: Scaling dense and self-slimmable transformers",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning. arxiv preprint arxiv",
            "year": 2014
        },
        {
            "authors": [
                "Y. Chow",
                "A. Tulepbergenov",
                "O. Nachum",
                "D. Gupta",
                "M. Ryu",
                "M. Ghavamzadeh",
                "C. Boutilier"
            ],
            "title": "A mixture-of-expert approach to rl-based dialogue management",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "A.P. Dempster",
                "N.M. Laird",
                "D.B. Rubin"
            ],
            "title": "Maximum Likelihood from Incomplete Data Via the EM Algorithm",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1977
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "N. Du",
                "Y. Huang",
                "A.M. Dai",
                "S. Tong",
                "D. Lepikhin",
                "Y. Xu",
                "M. Krikun",
                "Y. Zhou",
                "A. Yu",
                "O. Firat",
                "B. Zoph",
                "L. Fedus",
                "M. Bosma",
                "Z. Zhou",
                "T. Wang",
                "E. Wang",
                "K. Webster",
                "M. Pellat",
                "K. Robinson",
                "K. Meier-Hellstern",
                "T. Duke",
                "L. Dixon",
                "K. Zhang",
                "Q. Le",
                "Y. Wu",
                "Z. Chen",
                "C. Cui"
            ],
            "title": "Glam: Efficient scaling of language models with mixture-of-experts",
            "year": 2022
        },
        {
            "authors": [
                "R. Dwivedi",
                "N. Ho",
                "K. Khamaru",
                "M.J. Wainwright",
                "M.I. Jordan",
                "B. Yu"
            ],
            "title": "Sharp analysis of expectation-maximization for weakly identifiable models",
            "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "R. Dwivedi",
                "N. Ho",
                "K. Khamaru",
                "M.J. Wainwright",
                "M.I. Jordan",
                "B. Yu"
            ],
            "title": "Singularity, misspecification, and the convergence rate of EM",
            "venue": "Annals of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "W. Fedus",
                "J. Dean",
                "B. Zoph"
            ],
            "title": "A review of sparse expert models in deep learning. arxiv preprint arxiv",
            "year": 2022
        },
        {
            "authors": [
                "W. Fedus",
                "B. Zoph",
                "N. Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "A. Guha",
                "N. Ho",
                "XL. Nguyen"
            ],
            "title": "On posterior contraction of parameters and interpretability in Bayesian mixture modeling",
            "year": 2021
        },
        {
            "authors": [
                "A. Gulati",
                "J. Qin",
                "C. Chiu",
                "N. Parmar",
                "Y. Zhang",
                "J. Yu",
                "W. Han",
                "S. Wang",
                "Z. Zhang",
                "Y. Wu",
                "R. Pang"
            ],
            "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
            "venue": "In Proc. Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "S. Gupta",
                "S. Mukherjee",
                "K. Subudhi",
                "E. Gonzalez",
                "D. Jose",
                "A. Awadallah",
                "J. Gao"
            ],
            "title": "Sparsely activated mixture-of-experts are robust multi-task learners",
            "venue": "arXiv preprint arxiv 2204.0768,",
            "year": 2022
        },
        {
            "authors": [
                "X. Han",
                "H. Nguyen",
                "C. Harris",
                "N. Ho",
                "S. Saria"
            ],
            "title": "Fusemoe: Mixture-of-experts transformers for fleximodal fusion",
            "venue": "arXiv preprint arXiv:2402.03226,",
            "year": 2024
        },
        {
            "authors": [
                "H. Hazimeh",
                "Z. Zhao",
                "A. Chowdhery",
                "M. Sathiamoorthy",
                "Y. Chen",
                "R. Mazumder",
                "L. Hong",
                "Ed H. Chi"
            ],
            "title": "Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "N. Ho",
                "X. Nguyen"
            ],
            "title": "On strong identifiability and convergence rates of parameter estimation in finite mixtures",
            "venue": "Electronic Journal of Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "N. Ho",
                "C.Y. Yang",
                "M.I. Jordan"
            ],
            "title": "Convergence rates for Gaussian mixtures of experts",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "N. Ho",
                "K. Khamaru",
                "R. Dwivedi",
                "M.J. Wainwright",
                "M.I. Jordan",
                "B. Yu"
            ],
            "title": "Instability, computational efficiency and statistical accuracy",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "R.A. Jacobs",
                "M.I. Jordan",
                "S.J. Nowlan",
                "G.E. Hinton"
            ],
            "title": "Adaptive mixtures of local experts",
            "venue": "Neural Computation,",
            "year": 1991
        },
        {
            "authors": [
                "M.I. Jordan",
                "R.A. Jacobs"
            ],
            "title": "Hierarchical mixtures of experts and the EM algorithm",
            "venue": "Neural Computation,",
            "year": 1994
        },
        {
            "authors": [
                "J. Kwon",
                "C. Caramanis"
            ],
            "title": "EM converges for a mixture of many linear regressions",
            "venue": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "J. Kwon",
                "W. Qian",
                "C. Caramanis",
                "Y. Chen",
                "D. Davis"
            ],
            "title": "Global convergence of the em algorithm for mixtures of two component linear regression",
            "venue": "Proceedings of the Thirty-Second Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "J. Kwon",
                "N. Ho",
                "C. Caramanis"
            ],
            "title": "On the minimax optimality of the em algorithm for learning twocomponent mixed linear regression",
            "venue": "Proceedings of The 24th International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "D. Lepikhin",
                "H. Lee",
                "Y. Xu",
                "D. Chen",
                "O. Firat",
                "Y. Huang",
                "M. Krikun",
                "N. Shazeer",
                "Z. Chen"
            ],
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "B. Li",
                "Y. Shen",
                "J. Yang",
                "Y. Wang",
                "J. Ren",
                "T. Che",
                "J. Zhang",
                "Z. Liu"
            ],
            "title": "Sparse mixture-of-experts are domain generalizable learners",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "T. Manole",
                "N. Ho"
            ],
            "title": "Refined convergence rates for maximum likelihood estimation under finite mixture models",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "T. Manole",
                "A. Khalili"
            ],
            "title": "Estimating the number of components in finite mixture models via the group-sort-fuse procedure",
            "venue": "The Annals of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "H. Nguyen",
                "TT. Nguyen",
                "N. Ho"
            ],
            "title": "Demystifying softmax gating function in Gaussian mixture of experts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "H. Nguyen",
                "TT. Nguyen",
                "K. Nguyen",
                "N. Ho"
            ],
            "title": "Towards convergence rates for parameter estimation in Gaussian-gated mixture of experts",
            "venue": "In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics,",
            "year": 2024
        },
        {
            "authors": [
                "F. Peng",
                "R. Jacobs",
                "M. Tanner"
            ],
            "title": "Bayesian Inference in Mixtures-of-Experts and Hierarchical Mixtures-of-Experts Models With an Application to Speech Recognition",
            "venue": "Journal of the American Statistical Association,",
            "year": 1996
        },
        {
            "authors": [
                "Q. Pham",
                "G. Do",
                "H. Nguyen",
                "TT. Nguyen",
                "C. Liu",
                "M. Sartipi",
                "B.T. Nguyen",
                "S. Ramasamy",
                "X. Li",
                "S. Hoi",
                "N. Ho"
            ],
            "title": "Competesmoe \u2013 effective training of sparse mixture of experts via competition",
            "venue": "arXiv preprint arXiv:2402.02526,",
            "year": 2024
        },
        {
            "authors": [
                "C. Riquelme",
                "J. Puigcerver",
                "B. Mustafa",
                "M. Neumann",
                "R. Jenatton",
                "A. Susano Pint",
                "D. Keysers",
                "N. Houlsby"
            ],
            "title": "Scaling vision with sparse mixture of experts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A. Rives",
                "J. Meier",
                "T. Sercu",
                "S. Goyal",
                "Z. Lin",
                "J. Liu",
                "D. Guo",
                "M. Ott",
                "C.L. Zitnick",
                "J. Ma",
                "R. Fergus"
            ],
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "N. Shazeer",
                "A. Mirhoseini",
                "K. Maziarz",
                "A. Davis",
                "Q. Le",
                "G. Hinton",
                "J. Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "B. Sturmfels"
            ],
            "title": "Solving Systems of Polynomial Equations",
            "year": 2002
        },
        {
            "authors": [
                "H. Teicher"
            ],
            "title": "On the mixture of distributions",
            "venue": "Annals of Statistics,",
            "year": 1960
        },
        {
            "authors": [
                "H. Teicher"
            ],
            "title": "Identifiability of mixtures",
            "venue": "Annals of Statistics,",
            "year": 1961
        },
        {
            "authors": [
                "H. Teicher"
            ],
            "title": "Identifiability of finite mixtures",
            "venue": "Ann. Math. Statist.,",
            "year": 1963
        },
        {
            "authors": [
                "S. van de Geer"
            ],
            "title": "Empirical Processes in M-estimation",
            "year": 2000
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Topics in Optimal Transportation",
            "venue": "American Mathematical Society,",
            "year": 2003
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Optimal transport: Old and New",
            "year": 2008
        },
        {
            "authors": [
                "Z. You",
                "S. Feng",
                "D. Su",
                "D. Yu"
            ],
            "title": "Speechmoe2: Mixture-of-experts model with improved routing",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhou",
                "N. Du",
                "Y. Huang",
                "D. Peng",
                "C. Lan",
                "D. Huang",
                "S. Shakeri",
                "D. So",
                "A. Dai",
                "Y. Lu",
                "Z. Chen",
                "Q. Le",
                "C. Cui",
                "J. Laudon",
                "J. Dean"
            ],
            "title": "Brainformers: Trading simplicity for efficiency",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "\u2264 c"
            ],
            "title": "exp(\u2212n\u03b42/c2), for any \u03b4 \u2265 \u03b4n Proof of Lemma 4 can be found in van de Geer (2000). Now, we are ready to provide the proof for convergence rate of density estimation in Theorem 1 in Appendix A.1.1",
            "year": 2000
        },
        {
            "authors": [
                "Kwon"
            ],
            "title": "Section 3, to guarantee the convergence of density estimation to the true density, we need to select K = 2 experts from the density estimation",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Introduced by Jacobs et al. (1991) and Jordan & Jacobs (1994), the mixture of experts (MoE) has been known as a statistical machine learning design that leverages softmax gating functions to blend different expert networks together in order to establish a more intricate model. Recently, there has been a huge interest in a variant of this model called top-K sparse softmax gating MoE, which activates only the best K expert networks for each input based on sparse gating functions (Shazeer et al., 2017; Fedus et al., 2022a; Chen et al., 2023). Thus, this surrogate can be seen as a form of conditional computation (Bengio, 2013; Cho & Bengio, 2014) since it significantly scales up the model capacity while avoiding a proportional increase in the computational cost. These benefits have been empirically demonstrated in several deep learning applications, including natural language processing (Lepikhin et al., 2021; Du et al., 2022; Fedus et al., 2022b; Zhou et al., 2023; Pham et al.,\n2024), speech recognition (Peng et al., 1996; Gulati et al., 2020; You et al., 2022), computer vision (Dosovitskiy et al., 2021; Riquelme et al., 2021; Liang et al., 2022; Bao et al., 2022), multi-task learning (Hazimeh et al., 2021; Gupta et al., 2022) and other applications (Rives et al., 2021; Chow et al., 2023; Li et al., 2023; Han et al., 2024). Nevertheless, to the best of our knowledge, the full theoretical analysis of the top-K sparse softmax gating function has remained missing in the literature. In this paper, we aim to shed new light on the theoretical understanding of that gating function from a statistical perspective via the convergence analysis of maximum likelihood estimation (MLE) under the top-K sparse softmax gating Gaussian MoE.\nThere have been previous efforts to study the parameter estimation problem in Gaussian MoE models. Firstly, Ho et al. (2022) utilized the generalized Wasserstein loss functions Villani (2003; 2008) to derive the rates for estimating parameters in the input-free gating Gaussian MoE. They pointed out that due to an interaction among expert parameters, these rates became increasingly slow when the number of extra experts rose. Subsequently, Nguyen et al. (2023) and Nguyen et al. (2024) took into account the Gaussian MoE with softmax gating and Gaussian gating functions, respectively. Since these gating functions depended on the input, another interaction between gating parameters and expert parameters arose. Therefore, they designed Voronoi loss functions to capture these interactions and observe that the convergence rates for parameter estimation under these settings can be characterized by the solvability of systems of polynomial equations (Sturmfels, 2002).\nTurning to the top-K sparse softmax gating Gaussian MoE, the convergence analysis of the MLE, however, becomes substantially challenging due to the sophisticated structure of the top-K sparse softmax gating function compared to those of softmax gating and Gaussian gating functions. To comprehend these obstacles better, let us introduce the formal formulation of that model.\nProblem setup. Suppose that (X1, Y1), . . . , (Xn, Yn) \u2208 Rd \u00d7 R are i.i.d. samples of size n from the top-K sparse softmax gating Gaussian MoE of order k\u2217 with the conditional density function\ngG\u2217(Y |X) = k\u2217\u2211 i=1 Softmax(TopK((\u03b2\u22171i) \u22a4X,K;\u03b2\u22170i)) \u00b7 f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ), (1)\nwhere G\u2217 := \u2211k\u2217\ni=1 exp(\u03b2 \u2217 0i)\u03b4(\u03b2\u22171i,a\u2217i ,b\u2217i ,\u03c3\u2217i ) is a true but unknown mixing measure of order k\u2217 (i.e.,\na linear combination of k\u2217 Dirac measures \u03b4) associated with true parameters (\u03b2\u22170i, \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ) for i \u2208 {1, 2, . . . , k\u2217}. Here, h1(X, a, b) := a\u22a4X + b is referred to as an expert function, while we denote f(\u00b7|\u00b5, \u03c3) as an univariate Gaussian density function with mean \u00b5 and variance \u03c3 (The results for other settings of f(\u00b7|\u00b5, \u03c3) are in Appendix E). Additionally, we define for any vectors v = (v1, . . . , vk\u2217) and u = (u1, . . . , uk\u2217) in Rk\u2217 that Softmax(vi) := exp(vi)/ \u2211k\u2217 j=1 exp(vj) and\nTopK(vi,K;ui) := { vi + ui, if vi is in the top K elements of v; \u2212\u221e, otherwise.\nMore specifically, before applying the softmax function in equation (1), we keep only the top K values of (\u03b2\u221711) \u22a4X, . . . , (\u03b2\u22171k\u2217) \u22a4X and their corresponding bias vectors among \u03b2\u221701, . . . , \u03b2 \u2217 0k\u2217\n, while we set the rest to \u2212\u221e to make their gating values vanish. An instance of the top-K sparse softmax gating function is given in equation (3). Furthermore, linear expert functions considered in equation (1) are only for the simplicity of presentation. With similar proof techniques, the results in this work can be extended to general settings of the expert functions, including deep neural networks. In order to obtain an estimate of G\u2217, we use the following maximum likelihood estimation (MLE):\nG\u0302n \u2208 argmax G\n1\nn n\u2211 i=1 log(gG(Yi|Xi)). (2)\nUnder the exact-specified settings, i.e., when the true number of expert k\u2217 is known, the maximum in equation (2) is subject to the set of all mixing measures of order k\u2217 denoted by Ek\u2217(\u2126) := {G =\u2211k\u2217\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) : (\u03b20i, \u03b21i, ai, bi, \u03c3i) \u2208 \u2126}. On the other hand, under the over-specified settings, i.e., when k\u2217 is unknown and the true model is over-specified by a Gaussian mixture of k experts where k > k\u2217, the maximum is subject to the set of all mixing measures of order at most k, i.e., Ok(\u2126) := {G = \u2211k\u2032 i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) : 1 \u2264 k\u2032 \u2264 k, (\u03b20i, \u03b21i, ai, bi, \u03c3i) \u2208 \u2126}.\nUniversal assumptions. For the sake of theory, we impose four main assumptions on the parameters:\n(U.1) Convergence of MLE: To ensure the convergence of parameter estimation, we assume that the parameter space \u2126 is compact subset of R\u00d7Rd \u00d7Rd \u00d7R\u00d7R+, while the input space X is bounded. (U.2) Identifiability: Next, we assume that \u03b2\u22171k\u2217 = 0d and \u03b2 \u2217 0k\u2217\n= 0 so that the top-K sparse softmax gating Gaussian mixture of experts is identifiable. Under that assumption, we show in Appendix C that if G and G\u2032 are two mixing measures such that gG(Y |X) = gG\u2032(Y |X) for almost surely (X,Y ), then it follows that that G \u2261 G\u2032. Without that assumption, the result that G \u2261 G\u2032 does not hold, which leads to unncessarily complicated loss functions (see [Proposition 1,Nguyen et al. (2023)]).\n(U.3) Distinct Experts: To guarantee that experts in the mixture (1) are different from each other, we assume that parameters (a\u22171, b \u2217 1, \u03c3 \u2217 1), . . . , (a \u2217 k\u2217 , b\u2217k\u2217 , \u03c3 \u2217 k\u2217 ) are pairwise distinct.\n(U.4) Input-dependent Gating Functions: To make sure that the gating functions depend on the input X , we assume that at least one among parameters \u03b2\u221711, . . . , \u03b2 \u2217 1k\u2217\nis different from zero. Otherwise, the gating functions would be independent of the input X , which simplifies the problem significantly. In particular, the model (1) would reduce to an input-free gating Gaussian mixture of experts, which was already studied in Ho et al. (2022).\nChallenge discussion. In our convergence analysis, there are two main challenges attributed to the structure of the top-K sparse softmax gating function. (1) First, since this gating function divides the input space into multiple regions and each of which has different convergence behavior of density estimation, there could be a mismatch between the values of the top-K sparse softmax gating function in the density estimation gG\u0302n and in the true density gG\u2217 (see Figure 1). (2) Second, under the overspecified settings, each component of G\u2217 could be fitted by at least two components of G\u0302n. Therefore, choosing only the best K experts in the formulation of gG\u0302n(Y |X) is insufficient to estimate the true density gG\u2217(Y |X). As a result, it is of great importance to figure out the minimum number of experts selected in the top-K sparse softmax gating function necessary for ensuring the convergence of density estimation.\nContributions. In this work, we provide rigorous statistical guarantees for density estimation and parameter estimation in the top-K sparse softmax gating Gaussian MoE under both the exact-specified and over-specified settings. Our contributions are two-fold and can be summarized as follows (see also Table 1):\n1. Exact-specified settings: When the true number of experts k\u2217 is known, we point out that the density estimation gG\u0302n converges to the true density gG\u2217 under the Hellinger distance h at the parametric rate, that is, EX [h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X))] = O\u0303(n \u22121/2). Then, we propose a novel Voronoi metric D1 defined in equation (5) to characterize the parameter estimation rates. By establishing the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D1(G,G\u2217) for any mixing measure G \u2208 Ek\u2217(\u0398), we arrive at another bound D1(G\u0302n, G\u2217) = O\u0303(n\u22121/2), which indicates that the rates for estimating true parameters exp(\u03b2\u22170i), \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i are of the optimal order O\u0303(n\u22121/2).\n2. Over-specified settings: When k\u2217 is unknown and the true model is over-specified by a Gaussian of k experts where k > k\u2217, we demonstrate that the density estimation gG\u0302n converges to the true density gG\u2217 only if the number of experts chosen from gG\u0302n , denoted by K, is greater than the total cardinality of a certain number of Voronoi cells generated by the support of G\u2217. Given these results, the density estimation rate is shown to remain parametric on the sample size. Additionally, by designing a novel Voronoi loss function D2 in equation (8) to capture an interaction between parameters of the softmax gating and expert functions, we prove that the MLE G\u0302n converges to the true mixing measure G\u2217 at a rate of O\u0303(n\u22121/2). Then, it follows from the formulation of D2 that the estimation rates for true parameters \u03b2\u22171j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j depend on the solvability of a system of polynomial equations arising from the previous interaction, and turn out to be significantly slow.\nHigh-level proof ideas. Following from the challenge discussion, to ensure the convergence of density estimation under the exact-specified settings (resp. over-specified settings), we show that the input regions divided by the true gating functions match those divided by the estimated gating functions in Lemma 1 (resp. Lemma 2). Then, we leverage fundamental results on density estimation for M-estimators in (van de Geer, 2000) to derive the parametric density estimation rate under the Hellinger distance in Theorem 1 (resp. Theorem 3). Regarding the parameter estimation problem, a key step is to decompose the density discrepancy gG\u0302n(Y |X)\u2212 gG\u2217(Y |X) into a combination of\nlinearly independent terms. Thus, when the density estimation gG\u0302n(Y |X) converges to the true density gG\u2217(Y |X), the coefficients in that combination also tend to zero, which leads to our desired parameter estimation rates in Theorem 2 (resp. Theorem 4).\nOrganization. The rest of our paper is organized as follows. In Section 2, we establish the convergence rates of density estimation and parameter estimation for the top-K sparse softmax gating Gaussian MoE under the exact-specified settings, while the results for the over-specified settings are presented in Section 3. Subsequently, we provide practical implications of those results in Section 4 before concluding the paper and discussing some future directions in Section 5. Finally, full proofs, numerical experiments and additional results are deferred to the supplementary material.\nNotations. For any natural numbers m \u2265 n, we denote [n] := {1, 2, . . . , n} and ( m n ) := m!n!(m\u2212n!) .\nNext, for any vector u, v \u2208 Rd, we let |u| := \u2211d\ni=1 ui, u! := u1! . . . ud!, u v := uv11 . . . u vd d\nand denote \u2225u\u2225 as its 2-norm value. Meanwhile, we define |A| as the cardinality of some set A. Then, for any two probability densities f1 and f2 dominated by the Lebesgue measure \u03bd, we denote V (f1, f2) := 12 \u222b |f1 \u2212 f2|d\u03bd as their Total Variation distance, whereas h2(f1, f2) := 1 2 \u222b ( \u221a f1 \u2212 \u221a f2)\n2d\u03bd represents the squared Hellinger distance. Finally, for any two sequences of positive real numbers (an) and (bn), the notations an = O(bn) and an \u2272 bn both stand for an \u2264 Cbn for all n \u2208 N for some constant C > 0, while the notation an = O\u0303(bn) indicates that the previous inequality may depend on some logarithmic term."
        },
        {
            "heading": "2 EXACT-SPECIFIED SETTINGS",
            "text": "In this section, we characterize respectively the convergence rates of density estimation and parameter estimation in the top-K sparse softmax gating Gaussian MoE under the exact-specified settings, namely when the true number of experts k\u2217 is known.\nTo begin with, let us introduce some essential notations and key results to deal with the top-K sparse softmax gating function. It can be seen from equation (1) that whether (a\u2217i )\n\u22a4X+b\u2217i belongs to the top K experts in the true density gG\u2217(Y |X) or not is determined based on the ranking of (\u03b2\u22171i)\u22a4X + \u03b2\u22170i,\nwhich depends on the input X . Additionally, it is also worth noting that there are a total of q := ( k\u2217 K ) different potential choices of top K experts. Thus, we first partition the input space X into q regions in order to specify the top K experts and obtain an according representation of gG\u2217(Y |X) in each region. In particular, for each \u2113 \u2208 [q], we denote {\u21131, \u21132, . . . , \u2113K} as an K-element subset of [k\u2217] and {\u2113K+1, . . . , \u2113k\u2217} := [k\u2217] \\ {\u21131, \u21132, . . . , \u2113K}. Then, the \u2113-th region of X is defined as\nX \u2217\u2113 := { x \u2208 X : (\u03b2\u22171i)\u22a4x \u2265 (\u03b2\u22171i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } ,\nfor any \u2113 \u2208 [q]. From this definition, we observe that if X \u2208 X \u2217\u2113 for some \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = [K], then it follows that TopK((\u03b2\u22171i)\u22a4X,K;\u03b2\u22170i) = (\u03b2\u22171i)\u22a4X+\u03b2\u22170i for any i \u2208 [K]. As a result, (a\u22171) \u22a4X + b\u22171, . . . , (a \u2217 K)\n\u22a4X + b\u2217K become the top K experts, and the true conditional density gG\u2217(Y |X) is reduced to:\ngG\u2217(Y |X) = K\u2211 i=1 exp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i)\u2211K j=1 exp((\u03b2 \u2217 1j) \u22a4X + \u03b2\u22170j) \u00b7 f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ). (3)\nSubsequently, we assume that the MLE G\u0302n takes the form G\u0302n := \u2211k\u2217\ni=1 exp(\u03b2\u0302 n 0i)\u03b4(\u03b2\u0302n1i,a\u0302ni ,\u0302bni ,\u03c3\u0302ni ) ,\nwhere the MLE\u2019s component \u03c9\u0302ni := (\u03b2\u0302 n 0i, \u03b2\u0302 n 1i, a\u0302 n i , b\u0302 n i , \u03c3\u0302 n i ) converges to the true component \u03c9 \u2217 i := (\u03b2\u22170i, \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ) for any i \u2208 [k\u2217]. We figure out in the following lemma a relation between the values of the TopK function in gG\u2217(Y |X) and gG\u0302n(Y |X): Lemma 1. For any i \u2208 [k\u2217], let \u03b21i, \u03b2\u22171i \u2208 Rd such that \u2225\u03b21i \u2212 \u03b2\u22171i\u2225 \u2264 \u03b7i for some sufficiently small \u03b7i > 0. Then, for any \u2113 \u2208 [q], unless the set X \u2217\u2113 has measure zero, we obtain that X \u2217\u2113 = X\u2113 where\nX\u2113 := {x \u2208 X : (\u03b21i)\u22a4x \u2265 (\u03b21i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}}.\nProof of Lemma 1 is in Appendix A.3. This lemma indicates that for almost surely X , TopK((\u03b2\u22171i) \u22a4X,K;\u03b2\u22170i) = (\u03b2 \u2217 1i) \u22a4X + \u03b2\u22170i is equivalent to TopK((\u03b2\u0302 n 1i) \u22a4X,K; \u03b2\u0302n0i) = (\u03b2\u0302 n 1i) \u22a4X + \u03b2\u0302n0i, for any i \u2208 [k\u2217] and sufficiently large n. Based on this property, we provide in Theorem 1 the rate for estimating the true conditional density function gG\u2217 :\nTheorem 1 (Density estimation rate). Given the MLE G\u0302n defined in equation (2), the convergence rate of the conditional density estimation gG\u0302n to the true conditional density gG\u2217 under the exactspecified settings is illustrated by the following inequality:\nP ( EX [h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X))] > C \u221a log(n)/n ) \u2272 n\u2212c,\nwhere C > 0 and c > 0 are some universal constants that depend on \u2126 and K.\nProof of Theorem 1 can be found in Appendix A.1. It follows from the result of Theorem 1 that the conditional density estimation gG\u0302n converges to its true counterpart gG\u2217 under the Hellinger distance at the parametric rate of order O\u0303(n\u22121/2). This rate plays a critical role in establishing the convergence rates of parameter estimation. In particular, if we are able to derive the following lower bound: EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D1(G,G\u2217) for any mixing measure G \u2208 Ek\u2217(\u2126), where the metric D1 will be defined in equation (5), we will achieve our desired parameter estimation rates. Before going into further details, let us introduce the formulation of Voronoi metric D1 that we use for our convergence analysis under the exact-specified settings.\nVoronoi metric. Given an arbitrary mixing measure G with k\u2032 components, we distribute those components to the following Voronoi cells generated by the components of G\u2217 Manole & Ho (2022): Cj \u2261 Cj(G) := {i \u2208 [k\u2032] : \u2225\u03b8i \u2212 \u03b8\u2217j \u2225 \u2264 \u2225\u03b8i \u2212 \u03b8\u2217j\u2032\u2225,\u2200j\u2032 \u0338= j}, (4) where \u03b8i := (\u03b21i, ai, bi, \u03c3i) and \u03b8\u2217j := (\u03b2 \u2217 1j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ) for any j \u2208 [k\u2217]. Recall that under the exact-specified settings, the MLE G\u0302n belongs to the set Ek\u2217(\u2126). Therefore, we consider k\u2032 = k\u2217 in this case. Then, the Voronoi metric D1 is defined as follows:\nD1(G,G\u2217) := max {\u2113j}Kj=1\u2282[k\u2217] K\u2211 j=1 [ \u2211 i\u2208C\u2113j exp(\u03b20i) ( \u2225\u2206\u03b21i\u2113j\u2225+ \u2225\u2206ai\u2113j\u2225+ \u2225\u2206bi\u2113j\u2225+ \u2225\u2206\u03c3i\u2113j\u2225 )\n+ \u2223\u2223\u2223 \u2211 i\u2208C\u2113j exp(\u03b20i)\u2212 exp(\u03b2\u22170\u2113j ) \u2223\u2223\u2223]. (5)\nHere, we denote \u2206\u03b21i\u2113i := \u03b21i \u2212 \u03b2\u22171\u2113j , \u2206ai\u2113j := ai \u2212 a \u2217 \u2113j , \u2206bi\u2113j := bi \u2212 b\u2217\u2113j and \u2206\u03c3i\u2113j := \u03c3i \u2212\u03c3 \u2217 \u2113j . Additionally, the above maximum operator helps capture the behaviors of all input regions separated by the top-K sparse softmax gating function. Now, we are ready to characterize the convergence rates of parameter estimation in the top-K sparse softmax gating Gaussian MoE. Theorem 2 (Parameter estimation rate). Under the exact-specified settings, the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D1(G,G\u2217) holds for any mixing measure G \u2208 Ek\u2217(\u2126). Consequently, we can find a universal constant C1 > 0 depending only on G\u2217, \u2126 and K such that\nP ( D1(G\u0302n, G\u2217) > C1 \u221a log(n)/n ) \u2272 n\u2212c1 ,\nwhere c1 > 0 is a universal constant that depends only on \u2126.\nProof of Theorem 2 is in Appendix A.2. This theorem reveals that the Voronoi metric D1 between the MLE G\u0302n and the true mixing measure G\u2217, i.e. D1(G\u0302n, G\u2217), vanishes at the parametric rate of order O\u0303(n\u22121/2). As a result, the rates for estimating ground-truth parameters exp(\u03b2\u22170i), \u03b2\u22171i, a\u2217i , b\u2217i , \u03c3\u2217i are optimal at O\u0303(n\u22121/2) for any i \u2208 [k\u2217]."
        },
        {
            "heading": "3 OVER-SPECIFIED SETTINGS",
            "text": "In this section, we continue to carry out the same convergence analysis for the top-K sparse softmax gating Gaussian MoE as in Section 2 but under the over-specificed settings, that is, when the true number of experts k\u2217 becomes unknown.\nRecall that under the over-specified settings, we look for the MLE G\u0302n within Ok(\u2126), i.e. the set of all mixing measures with at most k components, where k > k\u2217. Thus, there could be some components (\u03b2\u22171i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ) of the true mixing measure G\u2217 approximated by at least two fitted components (\u03b2\u0302n1i, a\u0302 n i , b\u0302 n i , \u03c3\u0302 n i ) of the MLE G\u0302n. Moreover, since the true density gG\u2217(Y |X) is associated with top K experts and each of which corresponds to one component of G\u2217, we need to select more than K experts in the formulation of density estimation gG\u0302n(Y |X) to guarantee its convergence to gG\u2217(Y |X). In particular, for any mixing measure G = \u2211k\u2032 i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ok(\u2126), let us consider a new formulation of conditional density used for estimating the true density under the over-specified settings as follows:\ngG(Y |X) := k\u2032\u2211 i=1 Softmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) \u00b7 f(Y |(ai)\u22a4X + bi, \u03c3i).\nHere, gG(Y |X) is equipped with top-K sparse softmax gating, where 1 \u2264 K \u2264 k\u2032 is fixed and might be different from K. Additionally, the definition of MLE in equation (2) also changes accordingly to this new density function. Then, we demonstrate in Proposition 1 an interesting phenomenon that the density estimation gG\u0302n converges to gG\u2217 under the Hellinger distance only if\nK \u2265 max{\u21131,...,\u2113K}\u2282[k\u2217] \u2211K j=1 |C\u2113j |, where C\u2113j is a Voronoi cell defined in equation (4).\nProposition 1. If K < max{\u21131,...,\u2113K}\u2282[k\u2217] \u2211K j=1 |C\u2113j |, then the following inequality holds true:\ninf G\u2208Ok(\u2126) EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] > 0.\nProof of Proposition 1 is deferred to Appendix B.3. Following from the result of Proposition 1, we will consider only the regime when max{\u2113j}Kj=1\u2282[k\u2217] \u2211K j=1 |C\u2113j | \u2264 K \u2264 k in the rest of this section to ensure the convergence of density estimation. As the number of experts chosen in the density estimation changes from K to K, it is necessary to partition the input space X into q := ( k K ) regions. More specifically, for any \u2113 \u2208 [q], we denote {\u21131, \u21132, . . . , \u2113K} as an K-element subset of [k] and {\u2113K+1, . . . , \u2113k} := [k] \\ {\u21131, \u21132, . . . , \u2113K}. Then, we define the \u2113-th region of X as follows:\nX \u2113 := {x \u2208 X : (\u03b21i) \u22a4x \u2265 (\u03b21i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i \u2032 \u2208 {\u2113K+1, . . . , \u2113k}}. Inspired by the result of Lemma 1, we continue to present in Lemma 2 a relation between the values of the TopK functions in the density estimation gG\u0302n and the true density gG\u2217 .\nLemma 2. For any j \u2208 [k\u2217] and i \u2208 Cj , let \u03b21i, \u03b2\u22171j \u2208 Rd that satisfy \u2225\u03b21i \u2212 \u03b2\u22171j\u2225 \u2264 \u03b7j for some sufficiently small \u03b7j > 0. Additionally, for max{\u2113j}Kj=1\u2282[k\u2217] \u2211K j=1 |C\u2113j | \u2264 K \u2264 k, we assume that there exist \u2113 \u2208 [q] and \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = C\u21131 \u222a . . .\u222a C\u2113K . Then, if the set X \u2217\u2113 does not have measure zero, we achieve that X \u2217\u2113 = X \u2113.\nProof of Lemma 2 is in Appendix B.4. Different from Lemma 1, we need to impose on Lemma 2 an assumption that there exist indices \u2113 \u2208 [q] and \u2113 \u2208 [q] that satisfy {\u21131, . . . , \u2113K} = C\u21131 \u222a . . . \u222a C\u2113K . This assumption means that each component (\u03b2\u0302n1i, a\u0302 n i , b\u0302 n i , \u03c3\u0302 n i ) corresponding to the top K experts in gG\u0302n must converge to some true component which corresponds to the top K experts in gG\u2217 . Consequently, for X \u2208 X \u2217\u2113 , if TopK((\u03b2\u22171j)\u22a4X,K;\u03b2\u22170j) = (\u03b2\u22171j)\u22a4X + \u03b2\u22170j holds true, then we achieve that TopK((\u03b2\u0302n1i) \u22a4X,K; \u03b2\u0302n0i) = (\u03b2\u0302 n 1i)\n\u22a4X + \u03b2\u0302n0i and vice versa, for any j \u2208 [k\u2217] and i \u2208 Cj . Given the result of Lemma 2, we are now able to derive the convergence rate of the density estimation gG\u0302n to its true counterpart gG\u2217 under the over-specified settings in Theorem 3.\nTheorem 3 (Density estimation rate). Under the over-specified settings, the conditional density estimation gG\u0302n converges to the true density gG\u2217 under the Hellinger distance at the following rate:\nP ( EX [h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X))] > C \u2032 \u221a log(n)/n ) \u2272 n\u2212c \u2032 ,\nwhere C \u2032 > 0 and c\u2032 > 0 are some universal constants that depend on \u2126 and K.\nProof of Theorem 3 is in Appendix B.1. Although there is a modification in the number of experts chosen from gG\u0302n , Theorem 3 verifies that the convergence rate of this density estimation to gG\u2217 under the over-specified settings remains the same as that under the exact-specified settings, which is of order O\u0303(n\u22121/2). Subsequently, we will leverage this result for our convergence analysis of parameter estimation under the over-specified settings, which requires us to design a new Voronoi metric.\nVoronoi metric. Regarding the top-K sparse softmax gating function, challenges comes not only from the TopK function but also from the Softmax function. In particular, there is an intrinsic interaction between the numerators of softmax weights and the expert functions in the Gaussian density. Moreover, Gaussian density parameters also interacts with each other. These two interactions are respectively illustrated by the following partial differential equations (PDEs):\n\u22022s(X,Y )\n\u2202\u03b21\u2202b =\n\u2202s(X,Y )\n\u2202a ;\n\u2202s(X,Y )\n\u2202\u03c3 =\n1 2 \u00b7 \u2202\n2s(X,Y )\n\u2202b2 , (6)\nwhere we denote s(X,Y ) := exp(\u03b2\u22a41 X) \u00b7 f(Y |a\u22a4X+ b, \u03c3). These PDEs arise when we decompose the density difference gG\u0302n(Y |X) \u2212 gG\u2217(Y |X) into a linear combination of linearly independent terms using Taylor expansions. However, the above PDEs indicates that derivative terms which admit the forms in equation (6) are linearly dependent. Therefore, we have to group these terms by taking the summation of their coefficients, and then arrive at our desired linear combination of linearly independent elements. Consequently, when gG\u0302n(Y |X)\u2212 gG\u2217(Y |X) converges to zero, all the coefficients in this combination also tend to zero, which leads to the following system of polynomial equations with unknown variables {z1j , z2j , z3j , z4j , z5j}mi=1:\nm\u2211 i=1 \u2211 (\u03b11,\u03b12,\u03b13,\u03b14)\u2208J\u03b71,\u03b72 z25i z \u03b11 1i z \u03b12 2i z \u03b13 3i z \u03b14 4i \u03b11! \u03b12! \u03b13! \u03b14! = 0, (7)\nfor all (\u03b71, \u03b72) \u2208 Nd \u00d7 N such that 0 \u2264 |\u03b71| \u2264 r, 0 \u2264 \u03b72 \u2264 r \u2212 |\u03b71| and |\u03b71| + \u03b72 \u2265 1, where J\u03b71,\u03b72 := {(\u03b11, \u03b12, \u03b13, \u03b14) \u2208 Nd \u00d7 Nd \u00d7 N\u00d7 N : \u03b11 + \u03b12 = \u03b71, |\u03b12|+ \u03b13 + \u03b14 = \u03b72}. Here, a solution to the above system is called non-trivial if all of variables z5i are different from zero, whereas at least one among variables z3i is non-zero. For any m \u2265 2, let r(m) be the smallest natural number r such that the above system does not have any non-trivial solution. In a general case when d \u2265 1 and m \u2265 2, it is non-trivial to determine the exact value of r(m) Sturmfels (2002). However, when m is small, Nguyen et al. (2023) show that r(2) = 4 and r(3) = 6. Additionally, since r(m) is a monotonically increasing function of m, they also conjecture that r(m) = 2m for any m \u2265 2 and d \u2265 1.\nGiven the above results, we design a Voronoi metric D2 to capture the convergence rates of parameter estimation in the top-K sparse softmax gating Gaussian MoE under the over-specified settings as\nD2(G,G\u2217) := max {\u2113j}Kj=1\u2282[k\u2217] { \u2211 j\u2208[K], |C\u2113j |=1 \u2211 i\u2208C\u2113j exp(\u03b20i) [ \u2225\u2206\u03b21i\u2113j\u2225+ \u2225\u2206ai\u2113j\u2225+ |\u2206bi\u2113j |+ |\u2206\u03c3i\u2113j | ]\n+ \u2211\nj\u2208[K], |C\u2113j |>1\n\u2211 i\u2208C\u2113j exp(\u03b20i) [ \u2225\u2206\u03b21i\u2113j\u2225 r\u0304(|C\u2113j |) + \u2225\u2206ai\u2113j\u2225 r\u0304(|C\u2113j |) 2 + |\u2206bi\u2113j | r\u0304(|C\u2113j |) + |\u2206\u03c3i\u2113j | r\u0304(|C\u2113j |) 2 ]\n+ K\u2211 j=1 \u2223\u2223\u2223 \u2211 i\u2208C\u2113j exp(\u03b20i)\u2212 exp(\u03b2\u22170\u2113j ) \u2223\u2223\u2223}, (8)\nfor any mixing measure G \u2208 Ok(\u2126). The above maximum operator allows us to capture the behaviors of all input regions partitioned by the top-K sparse softmax gating function in gG\u2217 . Then, we show in the following theorem that parameter estimation rates vary with the values of the function r(\u00b7) Theorem 4 (Parameter estimation rate). Under the over-specified settings, the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D2(G,G\u2217) holds for any mixing measure G \u2208 Ok(\u2126). As a consequence, we can find a universal constant C2 > 0 depending only on G\u2217, \u2126 and K such that\nP ( D2(G\u0302n, G\u2217) > C2 \u221a log(n)/n ) \u2272 n\u2212c2 ,\nwhere c2 > 0 is a universal constant that depends only on \u2126.\nProof of Theorem 4 is in Appendix B.2. A few remarks on this theorem are in order.\n(i) Under the over-specified settings, the MLE G\u0302n converges to the true mixing measure G\u2217 at the rate of order O\u0303(n\u22121/2) under the Voronoi metric D2. Let us denote Cnj = Cj(G\u0302n), then this result indicates that the estimation rates for ground-truth parameters exp(\u03b2\u22170j), \u03b2 \u2217 1j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j fitted by only one component, i.e. |Cnj | = 1, remain the same at O\u0303(n\u22121/2) as those in the exact-specified settings. (ii) However, for ground-truth parameters which are approximated by at least two components, i.e. |Cnj | > 1, the rates for estimating them depend on their corresponding Voronoi cells, and become significantly low when the cardinality of those Voronoi cells increase. In particular, both \u03b2\u22171j and b \u2217 j admit the estimation rate of order O\u0303(n\u22121/2r(|C n j |)). Meanwhile, the convergence rates of estimating a\u2217j and \u03c3 \u2217 j are of the same order O(n\u22121/r(|C n j |)). For instance, assume that (\u03b2\u22171j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ) has three fitted components, which means that |Cnj | = 3 and therefore, r(|Cnj |) = 6. Then, the estimation rates for \u03b2\u22171j , b \u2217 j and a \u2217 j , \u03c3 \u2217 j are O\u0303(n\u22121/12) and O\u0303(n\u22121/6), respectively."
        },
        {
            "heading": "4 PRACTICAL IMPLICATIONS",
            "text": "In this section, we present three main practical implications of our theoretical results for the use of top-K sparse softmax gating function in mixture of experts as follows:\n1. No trade-off between model capacity and model performance: In the top-K sparse softmax gating Gaussian mixture of experts, since the gating function is discontinuous and only a portion of experts are activated for each input to scale up the model capacity, the parameter estimation rates under that model are expected to be slower than those under the dense softmax gating Gaussian mixture of experts (Nguyen et al., 2023). However, from our theories it turns out that the parameter estimation rates under these two models are the same under both the exact-specified and over-specified settings. As a result, we point out that using the top-K sparse softmax gating function allows us to scale up the model capacity without sacrificing the computational cost as well as the convergence rates of parameter and density estimation.\n2. Favourable gating function: As mentioned in Section 3, due to an intrinsic interaction between gating parameters and expert parameters via the first PDE in equation (6), the rates for estimating those parameters under the over-specified settings are determined by the solvability of the system of\npolynomial equations (7), which are significantly slow. However, if we use the top-1 sparse softmax gating function, i.e. activating only a single expert for each input, then the gating value is either one or zero. As a result, the previous interaction no longer occurs, which helps improve the parameter estimation rates. This partially accounts for the efficacy of top-1 sparse softmax gating mixture of experts in scaling up deep learning architectures (see (Fedus et al., 2022b)).\n3. True/ Minimal number of experts: Another challenge is to choose the true/ minimal number of experts, which can be partially addressed using theories developed from the paper. In particular, suppose that the MLE G\u0302n consists of k\u0302n components. When the sample size n goes to infinity, every Voronoi cell among Cn1 , . . . , Cnk\u2217 contains at least one element. Since the total number of elements of those Voronoi cells is k\u0302n, the maximum cardinality of a Voronoi cell turns out to be k\u0302n \u2212 k\u2217 + 1. This maximum value is attained when there is exactly one ground-truth component (\u03b2\u22171j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ) fitted by more than one component. An instance for this scenario is when |Cn1 | = k\u0302n \u2212 k\u2217 + 1 and |Cn2 | = . . . = |Cnk\u2217 | = 1. Under this setting, the first true parameters \u03b2 \u2217 11, b \u2217 1 and a \u2217 1, \u03c3 \u2217 1 achieve their worst possible estimation rates of order O\u0303(n\u22121/2r(k\u0302n\u2212k\u2217+1)) and O\u0303(n\u22121/r(k\u0302n\u2212k\u2217+1)), respectively, which become significantly slow when the difference k\u0302n \u2212 k\u2217 increases. As a consequence, the estimated number of experts k\u0302n should not be very large compared to the true number of experts k\u2217."
        },
        {
            "heading": "5 CONCLUSION AND FUTURE DIRECTIONS",
            "text": "In this paper, we provide a convergence analysis for density estimation and parameter estimation in the top-K sparse softmax gating Gaussian MoE. To overcome the complex structure of top-K sparse softmax gating function, we first establish a connection between the outputs of TopK function associated with the density estimation and the true density in each input region partitioned by the gating function, and then construct novel Voronoi loss functions among parameters to capture different behaviors of these input regions. Under the exact-specified settings, we show that the rates for estimating the true density and true parameters are both parametric on the sample size. On the other hand, although the density estimation rate remains parametric under the over-specified settings, the parameter estimation rates witness a sharp drop because of an interaction between the softmax gating and expert functions.\nBased on the results of this paper, there are a few potential future directions. Firstly, as we mentioned in Section 4, a question of how to estimate the true number of experts k\u2217 and the number of experts selected in the top-K sparse softmax gating function K naturally arises from this work. Since the parameter estimation rates under the over-specified settings decrease proportionately to the number of fitted experts k, one possible approach to estimating k\u2217 is to reduce k until these rates reach the optimal order O\u0303(n\u22121/2). That reduction can be done by merging close estimated parameters within their convergence rates to the true parameters (Guha et al., 2021) or by penalizing the loglikelihood function of the top-K sparse softmax gating Gaussian MoE using the differences among the parameters (Manole & Khalili, 2021). As a result, the number of experts chosen in the density estimation K also decreases accordingly and approximates the value of K. Secondly, the theoretical results established in the paper are under the assumption that the data are assumed to be generated from a top-K sparse softmax gating Gaussian MoE, which can be violated in real-world settings when the data are not necessarily generated from that model. Under those misspecified settings, the MLE converges to a mixing measure G \u2208 argminG\u2208Ok(\u2126) KL(P (Y |X)||gG(Y |X)) where P (Y |X) is the true conditional distribution of Y given X and KL stands for the Kullback-Leibler divergence. As the space Ok(\u2126) is non-convex, the existence of G is not unique. Furthermore, the current analysis of the MLE under the misspecified settings of statistical models is mostly conducted when the function space is convex (van de Geer, 2000), which is inapplicable to the current non-convex misspecified settings. Thus, it is necessary to develop a new analysis and a new metric to establish the convergence rate of the MLE to the set of G. Finally, since the log-likelihood function of the top-K sparse softmax gating Gaussian MoE is highly non-concave, the EM algorithm is used to approximate the MLE. While the statistical guarantee of the EM has been established for vanilla Gaussian mixture models (Balakrishnan et al., 2017; Dwivedi et al., 2020b;a), to the best of our knowledge such guarantee has not been studied in the setting of top-K sparse softmax gating Gaussian MoE. A potential approach to this problem is to utilize the population-to-sample analysis that has been widely used in previous works to study the EM algorithm Balakrishnan et al. (2017); Ho et al. (2023). We leave that direction for the future work."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "NH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning."
        },
        {
            "heading": "Supplement to \u201cStatistical Perspective of",
            "text": "Top-K Sparse Softmax Gating Mixture of Experts\u201d\nIn this supplementary material, we first provide rigorous proofs for all results under the exact-specified settings in Appendix A, while those for the over-specified settings are then presented in Appendix B. Next, we study the identifiability of the top-K sparse softmax gating Gaussian mixture of experts (MoE) in Appendix C. We then carry out several numerical experiments in Appendix D to empirically justify our theoretical results. Finally, we establish the theories for parameter and density estimation beyond the settings of top-K sparse softmax Gaussian MoE in Appendix E."
        },
        {
            "heading": "A PROOF FOR RESULTS UNDER THE EXACT-SPECIFIED SETTINGS",
            "text": "In this appendix, we present the proofs for Theorem 1 in Appendix A.1, while that for Theorem 2 is then given in Appendix A.2. Lastly, the proof of Lemma 1 is provided in Appendix A.3."
        },
        {
            "heading": "A.1 PROOF OF THEOREM 1",
            "text": "In this appendix, we conduct a convergence analysis for density estimation in the top-K sparse softmax gating Gaussian MoE using proof techniques in (van de Geer, 2000). For that purpose, it is necessary to introduce some essential notations and key results first.\nLet Pk\u2217(\u0398) := {gG(Y |X) : G \u2208 Ek\u2217(\u2126)} be the set of all conditional density functions of mixing measures in Ek\u2217(\u2126). Next, we denote by N(\u03b5,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) the covering number of metric space (Pk\u2217(\u2126), \u2225 \u00b7 \u22251). Meanwhile, HB(\u03b5,Pk\u2217(\u2126), h) represents for the bracketing entropy of Pk\u2217(\u2126) under the Hellinger distance. Then, we provide in the following lemma the upper bounds of those terms. Lemma 3. If \u2126 is a bounded set, then the following inequalities hold for any 0 < \u03b7 < 1/2:\n(i) logN(\u03b7,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) \u2272 log(1/\u03b7);\n(ii) HB(\u03b7,Pk\u2217(\u2126), h) \u2272 log(1/\u03b7).\nProof of Lemma 3 is in Appendix A.1.2. Subsequently, we denote\nP\u0303k\u2217(\u2126) := {g(G+G\u2217)/2(Y |X) : G \u2208 Ek\u2217(\u2126)};\nP\u03031/2k\u2217 (\u2126) := {g 1/2 (G+G\u2217)/2 (Y |X) : G \u2208 Ek\u2217(\u2126)}.\nIn addition, for each \u03b4 > 0, we define a Hellinger ball centered around the conditional density function gG\u2217(Y |X) and met with the set P\u0303 1/2 k\u2217 (\u2126) as\nP\u03031/2k\u2217 (\u2126, \u03b4) := {g 1/2 \u2208 P\u03031/2k\u2217 (\u2126) : h(g, gG\u2217) \u2264 \u03b4}.\nTo capture the size of the above Hellinger ball, van de Geer (2000) suggest using the following quantity:\nJB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)) := \u222b \u03b4 \u03b42/213 H 1/2 B (t, P\u0303 1/2 k\u2217 (\u2126, t), \u2225 \u00b7 \u2225)dt \u2228 \u03b4, (9)\nwhere t\u2228 \u03b4 := max{t, \u03b4}. Given those notations, let us recall a standard result for density estimation in van de Geer (2000).\nLemma 4 (Theorem 7.4, van de Geer (2000)). Take \u03a8(\u03b4) \u2265 JB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)) such that \u03a8(\u03b4)/\u03b4 2 is a non-increasing function of \u03b4. Then, for some sequence (\u03b4n) and universal constant c which satisfy\u221a n\u03b42n \u2265 c\u03a8(\u03b4), we obtain that\nP ( EX [ h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X)) ] > \u03b4 ) \u2264 c exp(\u2212n\u03b42/c2),\nfor any \u03b4 \u2265 \u03b4n\nProof of Lemma 4 can be found in van de Geer (2000). Now, we are ready to provide the proof for convergence rate of density estimation in Theorem 1 in Appendix A.1.1."
        },
        {
            "heading": "A.1.1 MAIN PROOF",
            "text": "It is worth noting that for any t > 0, we have\nHB(t, P\u03031/2k\u2217 (\u2126, t), \u2225 \u00b7 \u2225) \u2264 HB(t,Pk\u2217(\u2126, t), h). Then, the integral in equation (9) is upper bounded as follows:\nJB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)) \u2264 \u222b \u03b4 \u03b42/213 H 1/2 B (t,Pk\u2217(\u2126, t), h)dt \u2228 \u03b4 \u2272 \u222b \u03b4 \u03b42/213 log(1/t)dt \u2228 \u03b4, (10)\nwhere the second inequality follows from part (ii) of Lemma 3. As a result, by choosing \u03a8(\u03b4) = \u03b4 \u00b7 \u221a log(1/\u03b4), we can verify that \u03a8(\u03b4)/\u03b42 is a non-increasing\nfunction of \u03b4. Furthermore, the inequality in equation (10) indicates that \u03a8(\u03b4) \u2265 JB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)). Next, let us consider a sequence (\u03b4n) defined as \u03b4n := \u221a log(n)/n. This sequence can be validated to satisfy the condition \u221a n\u03b42n \u2265 c\u03a8(\u03b4) for some universal constant c. Therefore, by Lemma 4, we reach the conclusion of Theorem 1:\nP ( EX [h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X))] > C \u221a log(n)/n ) \u2272 n\u2212c,\nfor some universal constant C depending only on \u2126."
        },
        {
            "heading": "A.1.2 PROOF OF LEMMA 3",
            "text": "Part (i). In this part, we will derive the following upper bound for the covering number of metric space (Pk\u2217(\u2126), \u2225 \u00b7 \u22251) for any 0 < \u03b7 < 1/2 given the bounded set \u2126:\nlogN(\u03b7,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) \u2272 log(1/\u03b7).\nTo begin with, we define \u0398 := {(a, b, \u03c3) \u2208 Rd \u00d7 R \u00d7 R+ : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}. Note that \u2126 is a bounded set, then \u0398 also admits this property. Thus, there exists an \u03b7-cover of \u0398, denoted by \u0398\u03b7. Additionally, we also define \u2206 := {(\u03b20, \u03b21) \u2208 R\u00d7 Rd : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}, and \u2206\u03b7 be an \u03b7-cover of \u2206. Then, it can be validated that |\u0398\u03b7| \u2264 O(\u03b7\u2212(d+1)k\u2217) and |\u2206\u03b7| \u2264 O(\u03b7\u2212(d+3)k\u2217).\nSubsequently, for each G = \u2211k\u2217\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ek\u2217(\u2126), we take into account two other mixing measures. The first measure is G\u2032 = \u2211k\u2217 i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i), where (ai, bi, \u03c3i) \u2208 \u0398\u03b7 is the closest points to (ai, bi, \u03c3i) in this set for all i \u2208 [k\u2217]. The second one is G := \u2211k\u2217 i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) in which (\u03b20i, \u03b21i) \u2208 \u2206\u03b7 for any i \u2208 [k\u2217]. Next, let us define\nT := {gG \u2208 Pk\u2217(\u2126) : (\u03b20i, \u03b21i) \u2208 \u2206\u03b7, (ai, bi, \u03c3i) \u2208 \u0398\u03b7,\u2200i \u2208 [k\u2217]}, then it is obvious that gG \u2208 T . Now, we will show that T is an \u03b7-cover of metric space (Pk\u2217(\u2126), \u2225\u00b7\u22251) with a note that it is not necessarily the smallest cover. Indeed, according to the triangle inequality,\n\u2225gG \u2212 gG\u22251 \u2264 \u2225gG \u2212 gG\u2032\u22251 + \u2225gG\u2032 \u2212 gG\u22251. (11) The first term in the right hand side can be upper bounded as follows:\n\u2225gG \u2212 gG\u2032\u22251 \u2264 k\u2217\u2211 i=1 \u222b X\u00d7Y \u2223\u2223\u2223f(Y |a\u22a4i X + bi, \u03c3i)\u2212 f(Y |a\u22a4i X + bi, \u03c3i)\u2223\u2223\u2223d(X,Y ) \u2272\nk\u2217\u2211 i=1 \u222b X\u00d7Y ( \u2225ai \u2212 ai\u2225+ \u2225bi \u2212 bi\u2225+ \u2225\u03c3i \u2212 \u03c3i\u2225 ) d(X,Y )\n= k\u2217\u2211 i=1 ( \u2225ai \u2212 ai\u2225+ \u2225bi \u2212 bi\u2225+ \u2225\u03c3i \u2212 \u03c3i\u2225 ) \u2272 \u03b7. (12)\nNext, we will also demonstrate that \u2225gG\u2032 \u2212 gG\u22251 \u2272 \u03b7. For that purpose, let us consider q := ( k K ) K-element subsets of {1, . . . , k}, which are assumed to take the form {\u21131, \u21132, . . . , \u2113K} for any \u2113 \u2208 [q].\nAdditionally, we also denote {\u2113K+1, . . . , \u2113k} := {1, . . . , k} \\ {\u21131, . . . , \u2113K} for any \u2113 \u2208 [q]. Then, we define\nX\u2113 := {x \u2208 X : \u03b2\u22a41ix \u2265 \u03b2\u22a41i\u2032x : i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}}, X\u0303\u2113 := {x \u2208 X : \u03b2 \u22a4 1ix \u2265 \u03b2 \u22a4 1i\u2032x : i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}}.\nBy using the same arguments as in the proof of Lemma 1 in Appendix A.3, we achieve that either X\u2113 = X\u0303\u2113 or X\u2113 has measure zero for any \u2113 \u2208 [q]. As the Softmax function is differentiable, it is a Lipschitz function with some Lipschitz constant L \u2265 0. Since X is a bounded set, we may assume that \u2225X\u2225 \u2264 B for any X \u2208 X . Next, we denote\n\u03c0\u2113(X) := ( \u03b2\u22a41\u2113ix+ \u03b2 \u22a4 0\u2113i )K i=1 ; \u03c0\u2113(X) := ( \u03b2 \u22a4 1\u2113ix+ \u03b2 \u22a4 0\u2113i )K i=1 ,\nfor any K-element subset {\u21131, . . . \u2113K} of {1, . . . , k\u2217}. Then, we get \u2225Softmax(\u03c0\u2113(X))\u2212 Softmax(\u03c0\u2113(X))\u2225 \u2264 L \u00b7 \u2225\u03c0\u2113(X)\u2212 \u03c0\u2113(X)\u2225\n\u2264 L \u00b7 K\u2211 i=1 ( \u2225\u03b21\u2113i \u2212 \u03b21\u2113i\u2225 \u00b7 \u2225X\u2225+ |\u03b20\u2113i \u2212 \u03b20\u2113i | ) \u2264 L \u00b7\nK\u2211 i=1 ( \u03b7B + \u03b7 ) \u2272 \u03b7.\nBack to the proof for \u2225gG\u2032 \u2212 gG\u22251 \u2272 \u03b7, it follows from the above results that \u2225gG\u2032 \u2212 gG\u22251 = \u222b X\u00d7Y |gG\u2032(Y |X)\u2212 gG(Y |X)| d(X,Y )\n\u2264 q\u2211\n\u2113=1\n\u222b X\u2113\u00d7Y |gG\u2032(Y |X)\u2212 gG(Y |X)| d(X,Y )\n\u2264 q\u2211\n\u2113=1\n\u222b X\u2113\u00d7Y K\u2211 i=1 \u2223\u2223\u2223Softmax(\u03c0\u2113(X)i)\u2212 Softmax(\u03c0\u2113(X)i)\u2223\u2223\u2223 \u00b7 \u2223\u2223\u2223f(Y |a\u22a4\u2113iX + b\u2113i , \u03c3\u2113i)\u2223\u2223\u2223 d(X,Y ) \u2272 \u03b7, (13)\nFrom the results in equations (11), (12) and (13), we deduce that \u2225gG \u2212 gG\u22251 \u2272 \u03b7. This implies that T is an \u03b7-cover of the metric space (Pk\u2217(\u2126), \u2225 \u00b7 \u22251). Consequently, we achieve that\nN(\u03b7,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) \u2272 |\u2206\u03b7| \u00d7 |\u0398\u03b7| \u2264 O(1/\u03b7(d+3)k), which induces the conclusion of this part\nlogN(\u03b7,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) \u2272 log(1/\u03b7).\nPart (ii). Moving to this part, we will provide an upper bound for the bracketing entropy of Pk\u2217(\u2126) under the Hellinger distance:\nHB(\u03b7,Pk\u2217(\u2126), h) \u2272 log(1/\u03b7). Recall that \u0398 and X are bounded sets, we can find positive constants \u2212\u03b3 \u2264 a\u22a4X + b \u2264 \u03b3 and u1 \u2264 \u03c3 \u2264 u2. Let us define\nQ(Y |X) :=\n{ 1\u221a 2\u03c0u1 exp ( \u2212 Y 2 8u2 ) , for |Y | \u2265 2\u03b3\n1\u221a 2\u03c0u1 , for |Y | < 2\u03b3\nThen, it can be validated that f(Y |a\u22a4X + b, \u03c3) \u2264 Q(X,Y ) for any (X,Y ) \u2208 X \u00d7 Y . Next, let \u03c4 \u2264 \u03b7 which will be chosen later and {g1, . . . , gN} be an \u03c4 -cover of metric space (Pk\u2217(\u2126), \u2225 \u00b7 \u22251) with the covering number N := N(\u03c4,Pk\u2217(\u2126), \u2225 \u00b7 \u22251). Additionally, we also consider brackets of the form [\u03a8Li (Y |X),\u03a8Ui (Y |X)] where\n\u03a8Li (Y |X) := max{gi(Y |X)\u2212 \u03c4, 0} \u03a8Ui (Y |X) := max{gi(Y |X) + \u03c4,Q(Y |X)}.\nThen, we can check that Pk\u2217(\u2126) \u2286 \u22c3N i=1[\u03a8 L i (Y |X),\u03a8Ui (Y |X)] and \u03a8Ui (Y |X) \u2212 \u03a8Li (Y |X) \u2264\nmin{2\u03b7,Q(Y |X)}. Let S := max{2\u03b3, \u221a 8u2} log(1/\u03c4), we have for any i \u2208 [N ] that\n\u2225\u03a8Ui \u2212\u03a8Li \u22251 = \u222b |Y |<2\u03b3 [\u03a8Ui (Y |X)\u2212\u03a8Li (Y |X)] dXdY + \u222b |Y |\u22652\u03b3 [\u03a8Ui (Y |X)\u2212\u03a8Li (Y |X)] dXdY\n\u2264 S\u03c4 + exp ( \u2212 S 2\n2u2\n) \u2264 S\u2032\u03c4,\nwhere S\u2032 is some positive constant. This inequality indicates that\nHB(S \u2032\u03c4,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) \u2264 logN(\u03c4,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) \u2264 log(1/\u03c4).\nBy setting \u03c4 = \u03b7/S\u2032, we obtain that HB(\u03b7,Pk\u2217(\u2126), \u2225 \u00b7 \u22251) \u2272 log(1/\u03b7). Finally, since the norm \u2225 \u00b7 \u22251 is upper bounded by the Hellinger distance, we reach the conclusion of this part:\nHB(\u03b7,Pk\u2217(\u2126), h) \u2272 log(1/\u03b7). Hence, the proof is completed."
        },
        {
            "heading": "A.2 PROOF OF THEOREM 2",
            "text": "Since the Hellinger distance is lower bounded by the Total Variation distance, that is h \u2265 V , we will prove the following Total Variation lower bound:\nEX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D1(G,G\u2217), which is then respectively broken into local part and global part as follows:\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0, (14)\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)>\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0, (15)\nfor some constant \u03b5\u2032 > 0.\nProof of claim (14): It is sufficient to show that\nlim \u03b5\u21920 inf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0.\nAssume that this inequality does not hold, then since the number of experts k\u2217 is known in this case, there exists a sequence of mixing measure Gn := \u2211k\u2217 i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ek\u2217(\u2126) such that both D1(Gn, G\u2217) and EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D1(Gn, G\u2217) approach zero as n tends to infinity. Now, we define\nCnj = Cj(Gn) := {i \u2208 [k\u2217] : \u2225\u03c9ni \u2212 \u03c9\u2217j \u2225 \u2264 \u2225\u03c9ni \u2212 \u03c9\u2217s\u2225, \u2200s \u0338= j},\nfor any j \u2208 [k\u2217] as k\u2217 Voronoi cells with respect to the mixing measure Gn, where we denote \u03c9ni := (\u03b2 n 1i, a n i , b n i , \u03c3 n i ) and \u03c9 \u2217 j := (\u03b2 \u2217 1j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ). As we use asymptotic arguments in this proof, we can assume without loss of generality (WLOG) that these Voronoi cells does not depend on n, that is, Cj = Cnj . Next, it follows from the hypothesis D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e that each Voronoi cell contains only one element. Thus, we continue to assume WLOG that Cj = {j} for any j \u2208 [k\u2217], which implies that (\u03b2n1j , a n j , b n j , \u03c3 n j ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) and exp(\u03b2n0j) \u2192 exp(\u03b2\u22170j) as n \u2192 \u221e.\nSubsequently, to specify the top K selection in the formulations of gGn(Y |X) and gG\u2217(Y |X), we divide the covariate space X into some subsets in two ways. In particular, we first consider q := ( k\u2217 K ) different K-element subsets of [k\u2217], which are assumed to take the form {\u21131, . . . , \u2113K}, for \u2113 \u2208 [q]. Additionally, we denote {\u2113K+1, . . . , \u2113k\u2217} := [k\u2217] \\ {\u21131, . . . , \u2113K}. Then, we define for each \u2113 \u2208 [q] two following subsets of X :\nXn\u2113 := { x \u2208 X : (\u03b2n1j)\u22a4x \u2265 (\u03b2n1j\u2032)\u22a4x : \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } ,\nX \u2217\u2113 := { x \u2208 X : (\u03b2\u22171j)\u22a4x \u2265 (\u03b2\u22171j\u2032)\u22a4x : \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } .\nSince (\u03b2n0j , \u03b2 n 1j) \u2192 (\u03b2\u22170j , \u03b2\u22171j) as n \u2192 \u221e for any j \u2208 [k\u2217], we have for any arbitrarily small \u03b7j > 0 that \u2225\u03b2n1j \u2212 \u03b2\u22171j\u2225 \u2264 \u03b7j and |\u03b2n0j \u2212 \u03b2\u22170j | \u2264 \u03b7j for sufficiently large n. By applying Lemma 1, we obtain that Xn\u2113 = X \u2217\u2113 for any \u2113 \u2208 [q] for sufficiently large n. WLOG, we assume that\nD1(Gn, G\u2217) = K\u2211 i=1 [ exp(\u03b2n0i) ( \u2225\u2206\u03b2n1i\u2225+ \u2225\u2206ani \u2225+ \u2225\u2206bni \u2225+ \u2225\u2206\u03c3ni \u2225 ) + \u2223\u2223\u2223 exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i)\u2223\u2223\u2223], where we denote \u2206\u03b2n1i := \u03b2 n 1i \u2212 \u03b2\u22171i, \u2206ani := ani \u2212 a\u2217i , \u2206bni := bni \u2212 b\u2217i and \u2206\u03c3ni := \u03c3ni \u2212 \u03c3\u2217i .\nLet \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. Then, for almost surely (X,Y ) \u2208 X \u2217\u2113 \u00d7 Y , we can rewrite the conditional densities gGn(Y |X) and gG\u2217(Y |X) as\ngGn(Y |X) = K\u2211 i=1 exp((\u03b2n1i) \u22a4X + \u03b2n0i)\u2211K j=1 exp((\u03b2 n 1j) \u22a4X + \u03b2n0j) \u00b7 f(Y |(ani )\u22a4X + bni , \u03c3ni ),\ngG\u2217(Y |X) = K\u2211 i=1 exp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i)\u2211K j=1 exp((\u03b2 \u2217 1j) \u22a4X + \u03b2\u22170j) \u00b7 f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ).\nNow, we break the rest of our arguments into three steps:\nStep 1 - Taylor expansion: In this step, we take into account Hn := [\u2211K i=1 exp((\u03b2 \u2217 1i) \u22a4X + \u03b2\u22170i) ] \u00b7 [gGn(Y |X)\u2212 gG\u2217(Y |X)]. Then, Hn can be represented as follows:\nHn = K\u2211 i=1 exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171i)\u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) ]\n\u2212 K\u2211 i=1 exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)gGn(Y |X)\u2212 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) ]\n+ K\u2211 i=1 [ exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i) ][ exp((\u03b2\u22171i) \u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\u2212 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) ] .\nBy applying the first-order Taylor expansion to the first term in the above representation, which is denoted by An, we get that\nAn = K\u2211 i=1 \u2211 |\u03b1|=1 exp(\u03b2n0i) \u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b11+\u03b12 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202|\u03b12|+\u03b13+\u03b14f\n\u2202h |\u03b12|+\u03b13 1 \u2202\u03c3\n\u03b14 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y ),\nwhere R1(X,Y ) is a Taylor remainder that satisfies R1(X,Y )/D\u20321(X,Y ) \u2192 0 as n \u2192 \u221e. Recall that f is the univariate Gaussian density, then we have\n\u2202\u03b14f \u2202\u03c3\u03b14 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) = 1 2\u03b14 \u00b7 \u2202 2\u03b14f \u2202h2\u03b141 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ),\nwhich leads to\nAn = K\u2211 i=1 \u2211 |\u03b1|=1 exp(\u03b2n0i) 2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b11+\u03b12 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202|\u03b12|+\u03b13+2\u03b14f\n\u2202h |\u03b12|+\u03b13+2\u03b14 1\n(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y )\n= K\u2211 i=1 2\u2211 |\u03b71|+\u03b72=1 \u2211 \u03b1\u2208J\u03b71,\u03b72 exp(\u03b2n0i) 2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y ), (16)\nwhere we denote \u03b71 = \u03b11 + \u03b12 \u2208 Nd, \u03b72 = |\u03b12|+ \u03b13 + 2\u03b14 \u2208 N and an index set\nJ\u03b71,\u03b72 := {(\u03b1i)4i=1 \u2208 Nd \u00d7 Nd \u00d7 N\u00d7 N : \u03b11 + \u03b12 = \u03b71, \u03b13 + 2\u03b14 = \u03b72 \u2212 |\u03b12|}. (17) By arguing in a similar fashion for the second term in the representation of Hn, we also get that\nBn := \u2212 K\u2211 i=1 \u2211 |\u03b3|=1 exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R2(X,Y ),\nwhere R2(X,Y ) is a Taylor remainder such that R2(X,Y )/D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Putting the above results together, we rewrite the quantity Hn as follows:\nHn = K\u2211 i=1 \u2211 0\u2264|\u03b71|+\u03b72\u22642 Uni,\u03b71,\u03b72 \u00b7X \u03b71 exp((\u03b2\u22171i) \u22a4X) \u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ K\u2211 i=1 \u2211 0\u2264|\u03b3|\u22641 Wni,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) +R1(X,Y ) +R2(X,Y ), (18)\nin which we respectively define for each i \u2208 [K] that\nUni,\u03b71,\u03b72 := \u2211\n\u03b1\u2208J\u03b71,\u03b72\nexp(\u03b2n0i)\n2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14 ,\nWni,\u03b3 := \u2212 exp(\u03b2n0i)\n\u03b3! (\u2206\u03b2n1i) \u03b3 ,\nfor any (\u03b71, \u03b72) \u0338= (0d, 0) and |\u03b3| =\u0338 0d. Otherwise, Uni,0d,0 = \u2212W n i,0d := exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i). Step 2 - Non-vanishing coefficients:\nMoving to the second step, we will show that not all the ratios Uni,\u03b71,\u03b72/D1(Gn, G\u2217) tend to zero as n goes to infinity. Assume by contrary that all of them approach zero when n \u2192 \u221e, then for (\u03b71, \u03b72) = (0d, 0), it follows that\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 \u2223\u2223\u2223 exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i)\u2223\u2223\u2223 = K\u2211 i=1 |Unj,\u03b71,\u03b72 | D1(Gn, G\u2217) \u2192 0. (19)\nAdditionally, for tuples (\u03b71, \u03b72) where \u03b71 \u2208 {e1, e2, . . . , ed} with ej := (0, . . . , 0, 1\ufe38\ufe37\ufe37\ufe38 j\u2212th , 0, . . . , 0) and \u03b72 = 0, we get\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i)\u2225\u2206\u03b2n1i\u22251 = K\u2211 i=1 |Unj,\u03b71,\u03b72 | D1(Gn, G\u2217) \u2192 0.\nBy using similar arguments, we end up having\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i) [ \u2225\u2206\u03b2n1i\u22251 + \u2225\u2206ani \u22251 + |\u2206bni |+ |\u2206\u03c3ni | ] \u2192 0.\nDue to the topological equivalence between norm-1 and norm-2, the above limit implies that\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i) [ \u2225\u2206\u03b2n1i\u2225+ \u2225\u2206ani \u2225+ |\u2206bni |+ |\u2206\u03c3ni | ] \u2192 0. (20)\nCombine equation (19) with equation (20), we deduce that D1(Gn, G\u2217)/D1(Gn, G\u2217) \u2192 0, which is a contradiction. Consequently, at least one among the ratios Uni,\u03b71,\u03b72/D1(Gn, G\u2217) does not vanish as n tends to infinity.\nStep 3 - Fatou\u2019s contradiction:\nLet us denote by mn the maximum of the absolute values of Uni,\u03b71,\u03b72/D1(Gn, G\u2217) and Wni,\u03b3/D1(Gn, G\u2217). It follows from the result achieved in Step 2 that 1/mn \u0338\u2192 \u221e.\nRecall from the hypothesis that EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Thus, by the Fatou\u2019s lemma, we have\n0 = lim n\u2192\u221e EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))] D1(Gn, G\u2217) \u2265 1 2 \u00b7 \u222b lim inf n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| D1(Gn, G\u2217) dXdY.\nThis result indicates that |gGn(Y |X)\u2212 gG\u2217(Y |X)|/D1(Gn, G\u2217) tends to zero as n goes to infinity for almost surely (X,Y ). As a result, it follows that\nlim n\u2192\u221e Hn mnD1(Gn, G\u2217) = lim n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| mnD1(Gn, G\u2217) = 0.\nNext, let us denote Uni,\u03b71,\u03b72/[mnD1(Gn, G\u2217)] \u2192 \u03c4i,\u03b71,\u03b72 and W n i,\u03b3/[mnD1(Gn, G\u2217)] \u2192 \u03bai,\u03b3 with a note that at least one among them is non-zero. From the formulation of Hn in equation (18), we deduce that\nK\u2211 i=1 \u2211 0\u2264|\u03b71|+\u03b72\u22642 \u03c4i,\u03b71,\u03b72 \u00b7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ K\u2211 i=1 \u2211 0\u2264|\u03b3|\u22641 \u03bai,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gG\u2217(Y |X) = 0, (21)\nfor almost surely (X,Y ). This equation is equivalent to\nK\u2211 i=1 \u2211 0\u2264|\u03b71|\u22641  \u2211 0\u2264\u03b72\u22642\u2212|\u03b3| \u03c4i,\u03b71,\u03b72 \u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) + \u03bai,\u03b71gG\u2217(Y |X)  \u00d7 X\u03b71 exp((\u03b2\u22171i)\u22a4X) = 0,\nfor almost surely (X,Y ). Note that \u03b2\u221711, . . . , \u03b2 \u2217 1K admits pair-wise different values, then {exp((\u03b2\u22171i)\u22a4X) : i \u2208 [K]} is a linearly independent set, which leads to\u2211 0\u2264|\u03b71|\u22641  \u2211 0\u2264\u03b72\u22642\u2212|\u03b3| \u03c4i,\u03b71,\u03b72 \u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) + \u03bai,\u03b71gG\u2217(Y |X) X\u03b71 = 0, for any i \u2208 [K] for almost surely (X,Y ). It is clear that the left hand side of the above equation is a polynomial of X belonging to the compact set X . As a result, we get that\u2211\n0\u2264\u03b72\u22642\u2212|\u03b3|\n\u03c4i,\u03b71,\u03b72 \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) + \u03bai,\u03b71gG\u2217(Y |X) = 0,\nfor any i \u2208 [K], 0 \u2264 |\u03b71| \u2264 1 and almost surely (X,Y ). Since (a\u22171, b\u22171, \u03c3\u22171), . . . , (a\u2217K , b\u2217K , \u03c3\u2217K) have pair-wise distinct values, those of ((a\u22171) \u22a4X + b\u22171, \u03c3 \u2217 1), . . . , ((a \u2217 K) \u22a4X + b\u2217K , \u03c3 \u2217 K) are also pair-wise\ndifferent. Thus, the set {\n\u2202\u03b72f \u2202h\n\u03b72 1\n(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ), gG\u2217(Y |X) : i \u2208 [K] }\nis linearly independent. Consequently, we obtain that \u03c4i,\u03b71,\u03b72 = \u03bai,\u03b3 = 0 for any i \u2208 [K], 0 \u2264 |\u03b71|+\u03b72 \u2264 2 and 0 \u2264 |\u03b3| \u2264 1, which contradicts the fact that at least one among these terms is different from zero.\nHence, we can find some constant \u03b5\u2032 > 0 such that\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0.\nProof of claim (15): Assume by contrary that this claim is not true, then we can seek a sequence G\u2032n \u2208 Ek\u2217(\u2126) such that D1(G\u2032n, G\u2217) > \u03b5\u2032 and\nlim n\u2192\u221e EX [V (gG\u2032n(\u00b7|X), gG\u2217(\u00b7|X))] D1(G\u2032n, G\u2217) = 0,\nwhich directly implies that EX [V (gG\u2032n(\u00b7|X), gG\u2217(\u00b7|X))] \u2192 0 as n \u2192 \u221e. Recall that \u2126 is a compact set, therefore, we can replace the sequence G\u2032n by one of its subsequences that converges to a mixing measure G\u2032 \u2208 Ek\u2217(\u2126). Since D1(G\u2032n, G\u2217) > \u03b5\u2032, this result induces that D1(G\u2032, G\u2217) > \u03b5\u2032.\nSubsequently, by means of the Fatou\u2019s lemma, we achieve that\n0 = lim n\u2192\u221e\nEX [2V (gG\u2032n(\u00b7|X), gG\u2217(\u00b7|X))] \u2265 \u222b\nlim inf n\u2192\u221e \u2223\u2223\u2223gG\u2032n(Y |X)\u2212 gG\u2217(Y |X)\u2223\u2223\u2223 d\u00b5(Y )\u03bd(X). It follows that gG\u2032(Y |X) = gG\u2217(Y |X) for almost surely (X,Y ). From Proposition 2, we know that the top-K sparse softmax gating Gaussian mixture of experts is identifiable, thus, we obtain that G\u2032 \u2261 G\u2217. As a consequence, D1(G\u2032, G\u2217) = 0, contradicting the fact that D1(G\u2032, G\u2217) > \u03b5\u2032 > 0. Hence, the proof is completed."
        },
        {
            "heading": "A.3 PROOF OF LEMMA 1",
            "text": "Let \u03b7i = Mi\u03b5, where \u03b5 is some fixed positive constant and Mi will be chosen later. For an arbitrary \u2113 \u2208 [q], since X and \u2126 are bounded sets, there exists some constant c\u2217\u2113 \u2265 0 such that\nmin x,i,i\u2032\n[ (\u03b2\u22171i) \u22a4x\u2212 (\u03b2\u22171i\u2032)\u22a4x ] = c\u2217\u2113\u03b5, (22)\nwhere the minimum is subject to x \u2208 X \u2217\u2113 , i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}. We will point out that c\u2217\u2113 > 0. Assume by contrary that c \u2217 \u2113 = 0. For x \u2208 X \u2217\u2113 , we may assume for any 1 \u2264 i < j \u2264 k\u2217 that\n(\u03b2\u22171\u2113i) \u22a4x \u2265 (\u03b2\u22171\u2113j ) \u22a4x.\nSince c\u2217\u2113 = 0, it follows from equation (22) that (\u03b2 \u2217 1\u2113K )\u22a4x\u2212 (\u03b2\u22171\u2113K+1) \u22a4x = 0, or equivalently\n(\u03b2\u22171\u2113K \u2212 \u03b2 \u2217 1\u2113K+1) \u22a4x = 0.\nIn other words, X \u2217\u2113 is a subset of\nZ := {x \u2208 X : (\u03b2\u22171\u2113K \u2212 \u03b2 \u2217 1\u2113K+1) \u22a4x = 0}.\nSince \u03b21\u2113K \u2212 \u03b21\u2113K+1 \u0338= 0d and the distribution of X is continuous, it follows that the set Z has measure zero. Since X \u2217\u2113 \u2286 Z , we can conclude that X \u2217\u2113 also has measure zero, which contradicts the hypothesis of Lemma 1. Therefore, we must have c\u2217\u2113 > 0.\nAs X is a bounded set, we assume that \u2225x\u2225 \u2264 B for any x \u2208 X . Let x \u2208 X \u2217\u2113 , then we have for any i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} that\n\u03b2\u22a41ix = (\u03b21i \u2212 \u03b2\u22171i)\u22a4x+ (\u03b2\u22171i)\u22a4x \u2265 \u2212Mi\u03b5B + (\u03b2\u22171i\u2032)\u22a4x+ c\u2217\u2113\u03b5 = \u2212Mi\u03b5B + c\u2217\u2113\u03b5+ (\u03b2\u22171i\u2032 \u2212 \u03b21i\u2032)\u22a4x+ \u03b2\u22a41i\u2032x \u2265 \u22122Mi\u03b5B ++c\u2217\u2113\u03b5+ \u03b2\u22a41i\u2032x.\nBy setting Mi \u2264 c\u2217\u2113 2B\n, we get that x \u2208 X\u2113, which means that X \u2217\u2113 \u2286 X\u2113. Similarly, assume that there exists some constant c\u2113 \u2265 0 that satisfies\nmin x,i,i\u2032\n[ (\u03b2\u22171i) \u22a4x\u2212 (\u03b2\u22171i\u2032)\u22a4x ] = c\u2217\u2113\u03b5.\nHere, the above minimum is subject to x \u2208 X\u2113, i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}. If Mi \u2264\nc\u2113 2B , then we also receive that X\u2113 \u2286 X \u2217\u2113 .\nHence, if we set Mi = 1\n2B min{c\u2217\u2113 , c\u2113}, we reach the conclusion that X \u2217\u2113 = X\u2113."
        },
        {
            "heading": "B PROOF FOR RESULTS UNDER OVER-SPECIFIED SETTINGS",
            "text": "In this appendix, we first provide the proofs of Theorem B.1 and Theorem 4 in Appendix B.1 and Appendix B.2, respectively. Subsequently, we present the proof for Proposition 1 in Appendix B.3, while that for Lemma 2 is put in Appendix B.4."
        },
        {
            "heading": "B.1 PROOF OF THEOREM 3",
            "text": "In this appendix, we follow proof techniques presented in Appendix A.1 to demonstrate the result of Theorem 3. Recall that under the over-specified settings, the MLE G\u0302n belongs to the set of all mixing measures with at most k > k\u2217 components, i.e. Ok(\u2126). Interestingly, if we can adapt the result of part (i) of Lemma 3 to the over-specified settings, then other results presented in Appendix A.1 will also hold true. Therefore, our main goal is to derive following bound for any 0 < \u03b7 < 1/2 under the over-specified settings:\nlogN(\u03b7,Pk(\u2126), \u2225 \u00b7 \u22251) \u2272 log(1/\u03b7), where Pk(\u2126) := {gG(Y |X) : G \u2208 Ok(\u2126)}. For ease of presentation, we will reuse the notations defined in Appendix A.1 with Ek\u2217(\u2126) being replaced by Ok(\u2126). Now, let us recall necessary notations for this proof.\nFirstly, we define \u0398 = {(a, b, \u03c3) \u2208 Rd \u00d7 R\u00d7 R+ : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}, and \u0398\u03b7 is an \u03b7-cover of \u0398. Additionally, we also denote \u2206 := {(\u03b20, \u03b21) \u2208 Rd \u00d7 R : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}, and \u2206\u03b7 be an \u03b7-cover of \u2206. Next, for each mixing measure G = \u2211k i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ok(\u2126), we denote\nG\u2032 = \u2211k\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) in which (ai, bi, \u03c3i) \u2208 \u0398\u03b7 is the closest point to (ai, bi, \u03c3i) in this set for any i \u2208 [k]. We also consider another mixing measure G := \u2211k i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ok(\u2126) where (\u03b20i, \u03b21i) \u2208 \u2206\u03b7 is the closest point to (\u03b20i, \u03b21i) in this set for any i \u2208 [k]. Subsequently, we define\nL := {gG \u2208 Pk(\u2126) : (\u03b20i, \u03b21i) \u2208 \u2206\u03b7, (ai, bi, \u03c3i) \u2208 \u0398\u03b7}. We demonstrate that L is an \u03b7-cover of the metric space (Pk(\u2126), \u2225 \u00b7 \u22251), that is, for any gG \u2208 Pk(\u2126), there exists a density gG \u2208 L such that \u2225gG \u2212 gG\u22251 \u2264 \u03b7. By the triangle inequality, we have\n\u2225gG \u2212 gG\u22251 \u2264 \u2225gG \u2212 gG\u2032\u22251 + \u2225gG\u2032 \u2212 gG\u22251. (23) From the formulation of G\u2032, we get that\n\u2225gG \u2212 gG\u2032\u22251 \u2264 k\u2211\ni=1\n\u222b X\u00d7Y \u2223\u2223\u2223f(Y |a\u22a4i X + bi, \u03c3i)\u2212 f(Y |a\u22a4i X + bi, \u03c3i)\u2223\u2223\u2223d(X,Y ) \u2272\nk\u2211 i=1 \u222b X\u00d7Y ( \u2225ai \u2212 ai\u2225+ |bi \u2212 bi|+ |\u03c3i \u2212 \u03c3i| ) d(X,Y )\n\u2272 \u03b7 (24)\nBased on inequalities in equations (23) and (24), it is sufficient to show that \u2225gG\u2032 \u2212 gG\u22251 \u2272 \u03b7. For any \u2113 \u2208 [q], let us define\nX \u2113 := {x \u2208 X : (\u03b21i) \u22a4x \u2265 (\u03b21i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i \u2032 \u2208 {\u2113K+1, . . . , \u2113k}}, X \u2032\n\u2113 := {x \u2208 X : (\u03b21i)\u22a4x \u2265 (\u03b21i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i \u2032 \u2208 {\u2113K+1, . . . , \u2113k}}. Since the Softmax function is differentiable, it is a Lipschitz function with some Lipschitz constant L \u2265 0. Assume that \u2225X\u2225 \u2264 B for any X \u2208 X and denote\n\u03c0\u2113(X) := ( \u03b2\u22a4 1\u2113i x+ \u03b2\u22a4 0\u2113i )K i=1 ; \u03c0\u2113(X) := ( \u03b2 \u22a4 1\u2113ix+ \u03b2 \u22a4 0\u2113i )K i=1 ,\nfor any K-element subset {\u21131, . . . \u2113K} of {1, . . . , k}. Then, we have \u2225Softmax(\u03c0\u2113(X))\u2212 Softmax(\u03c0\u2113(X))\u2225 \u2264 L \u00b7 \u2225\u03c0\u2113(X)\u2212 \u03c0\u2113(X)\u2225\n\u2264 L \u00b7 K\u2211 i=1 ( \u2225\u03b21\u2113i \u2212 \u03b21\u2113i\u2225 \u00b7 \u2225X\u2225+ |\u03b20\u2113i \u2212 \u03b20\u2113i | ) \u2264 L \u00b7 K\u2211 i=1 ( \u03b7B + \u03b7\n) \u2272 \u03b7.\nBy arguing similarly to the proof of Lemma 2 in Appendix B.4, we receive that either X \u2113 = X \u2032\u2113 or X \u2113 has measure zero for any \u2113 \u2208 [q]. As a result, we deduce that\n\u2225gG\u2032 \u2212 gG\u2217\u22251 \u2264 q\u2211\n\u2113=1\n\u222b X \u2113\u00d7Y |gG\u2032(Y |X)\u2212 gG(Y |X)|d(X,Y )\n\u2264 q\u2211\n\u2113=1\n\u222b X \u2113\u00d7Y K\u2211 i=1 \u2223\u2223\u2223Softmax(\u03c0\u2113(X)i)\u2212 Softmax(\u03c0\u2113(X)i)\u2223\u2223\u2223 \u00b7 \u2223\u2223\u2223f(Y |a\u22a4\u2113iX + b\u2113i , \u03c3\u2113i)\u2223\u2223\u2223d(X,Y ) \u2272 \u03b7.\nThus, L is an \u03b7-cover of the metric space (Pk(\u2126), \u2225 \u00b7 \u22251), which implies that\nN(\u03b7,Pk(\u2126), \u2225 \u00b7 \u22251) \u2272 |\u2206\u03b7| \u00d7 |\u0398\u03b7| \u2264 O(\u03b7\u2212(d+1)k)\u00d7O(\u03b7\u2212(d+3)k) = O(\u03b7\u2212(2d+4)k). (25)\nHence, logN(\u03b7,Pk(\u2126), \u2225 \u00b7 \u22251) \u2272 log(1/\u03b7)."
        },
        {
            "heading": "B.2 PROOF OF THEOREM 4",
            "text": "Similar to the proof of Theorem 2 in Appendix A, our objective here is also to derive the Total Variation lower bound adapted to the over-fitted settings:\nEX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D2(G,G\u2217).\nSince the global part of the above inequality can be argued in the same fashion as in Appendix A, we will focus only on demonstrating the following local part via the proof by contradiction method:\nlim \u03b5\u21920 inf G\u2208Ok(\u0398):D2(G,G\u2217)\u2264\u03b5 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D2(G,G\u2217) > 0. (26)\nAssume that the above claim does not hold true, then we can find a sequence of mixing measures Gn := \u2211kn i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ok(\u2126) such that both D2(Gn, G\u2217) and EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D2(Gn, G\u2217) vanish when n goes to infinity. Additionally, by abuse of notation, we reuse the set of Voronoi cells Cj , for j \u2208 [k\u2217], defined in Appendix A. Due to the limit D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e, it follows that for any j \u2208 [k\u2217], we have \u2211 i\u2208Cj exp(\u03b2 n 0i) \u2192 exp(\u03b2\u22170j) and (\u03b2n1i, a n i , b n i , \u03c3 n i ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) for all i \u2208 Cj . WLOG, we may assume that\nD2(Gn, G\u2217) = \u2211\nj\u2208[K], |Cj |>1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ \u2225\u2206\u03b2n1ij\u2225r\u0304(|Cj |) + \u2225\u2206anij\u2225 r\u0304(|Cj |) 2 + |\u2206bnij |r\u0304(|Cj |) + |\u2206\u03c3nij | r\u0304(|Cj |) 2 ]\n+ \u2211\nj\u2208[K], |Cj |=1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ \u2225\u2206\u03b2n1ij\u2225+ \u2225\u2206anij\u2225+ |\u2206bnij |+ |\u2206\u03c3nij | ] + K\u2211 j=1 \u2223\u2223\u2223 \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j) \u2223\u2223\u2223.\nRegarding the top-K selection in the conditional density gG\u2217 , we partition the covariate space X in a similar fashion to Appendix A. More specifically, we consider q = ( k\u2217 K ) subsets {\u21131, . . . , \u2113K} of {1, . . . , k\u2217} for any \u2113 \u2208 [q], and denote {\u2113K+1, . . . , \u2113k\u2217} := [k\u2217] \\ {\u21131, . . . , \u2113K}. Then, we define\nX \u2217\u2113 := { x \u2208 X : (\u03b2\u22171j)\u22a4x \u2265 (\u03b2\u22171j\u2032)\u22a4x, \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } ,\nfor any \u2113 \u2208 [q]. On the other hand, we need to introduce a new partition method of the covariate space for the weight selection in the conditional density gGn . In particular, let K \u2208 N such that max{\u2113j}Kj=1\u2282[k\u2217] \u2211K j=1 |C\u2113j | \u2264 K \u2264 k and q := ( k K ) . Then, for any \u2113 \u2208 [q], we denote (\u21131, . . . , \u2113k) as a subset of [k] and {\u2113K+1, . . . , \u2113k} := [k] \\ {\u21131, . . . , \u2113K}. Additionally, we define\nXn \u2113 := { x \u2208 X : (\u03b2n1i)\u22a4x \u2265 (\u03b2n1i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i \u2032 \u2208 {\u2113K+1, . . . , \u2113k} } .\nLet X \u2208 X \u2217\u2113 for some \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. If {\u21131, . . . \u2113K} =\u0338 C1 \u222a . . .\u222aCK for any \u2113 \u2208 [q], then EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D2(Gn, G\u2217) \u0338\u2192 0 as n tends to infinity. This contradicts the fact that this term must approach zero. Therefore, we only need to consider the scenario when there exists \u2113 \u2208 [q] such that {\u21131, . . . \u2113K} = C1 \u222a . . . \u222a CK . Recall that we have (\u03b2n0i, \u03b2 n 1i) \u2192 (\u03b2\u22170j , \u03b2\u22171j) as n \u2192 \u221e for any j \u2208 [k\u2217] and i \u2208 Cj . Thus, for any arbitrarily small \u03b7j > 0, we have that \u2225\u03b2n1i \u2212 \u03b2\u22171j\u2225 \u2264 \u03b7j and |\u03b2n0i \u2212 \u03b2\u22170j | \u2264 \u03b7j for sufficiently large n. Then, it follows from Lemma 2 that X \u2217\u2113 = Xn\u2113 for sufficiently large n. This result indicates that X \u2208 X n \u2113 .\nThen, we can represent the conditional densities gG\u2217(Y |X) and gGn(Y |X) for any sufficiently large n as follows:\ngG\u2217(Y |X) = K\u2211 j=1 exp((\u03b2\u22171j) \u22a4X + \u03b2\u22170j)\u2211K j\u2032=1 exp((\u03b2 \u2217 1j\u2032) \u22a4X + \u03b2\u22170j\u2032) \u00b7 f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ), gGn(Y |X) = K\u2211 j=1 \u2211 i\u2208Cj exp((\u03b2n1i) \u22a4X + \u03b2n0i)\u2211K j\u2032=1 \u2211 i\u2032\u2208Cj\u2032 exp((\u03b2n1i\u2032) \u22a4X + \u03b2n0i\u2032) \u00b7 f(Y |(ani )\u22a4X + bni , \u03c3ni ).\nNow, we reuse the three-step framework in Appendix A.\nStep 1 - Taylor expansion:\nFirstly, by abuse of notations, let us consider the quantity\nHn := [ K\u2211 j=1 exp((\u03b2\u22171j) \u22a4X + \u03b2\u22170j) ] \u00b7 [gGn(Y |X)\u2212 gG\u2217(Y |X)].\nSimilar to Step 1 in Appendix A, we can express this term as\nHn = K\u2211 j=1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171j)\u22a4X)f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ) ]\n\u2212 K\u2211 j=1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)gGn(Y |X)\u2212 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) ]\n+ K\u2211 j=1 [ \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j) ][ exp((\u03b2\u22171j) \u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\u2212 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) ] := An +Bn + En.\nNext, we proceed to decompose An based on the cardinality of the Voronoi cells as follows: An = \u2211\nj:|Cj |=1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171j)\u22a4X)f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ) ]\n+ \u2211\nj:|Cj |>1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171j)\u22a4X)f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ) ] .\nBy applying the Taylor expansions of order 1 and r\u0304(|Cj |) to the first and second terms of An, respectively, and following the derivation in equation (16), we arrive at\nAn = \u2211\nj:|Cj |=1 \u2211 i\u2208Cj \u2211 1\u2264|\u03b71|+\u03b72\u22642 \u2211 \u03b1\u2208J\u03b71,\u03b72 exp(\u03b2n0i) 2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R3(X,Y )\n+ \u2211\nj:|Cj |>1 \u2211 i\u2208Cj \u2211 1\u2264|\u03b71|+\u03b72\u22642r\u0304(|Cj |) \u2211 \u03b1\u2208J\u03b71,\u03b72 exp(\u03b2n0i) 2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R4(X,Y ),\nwhere the set J\u03b71,\u03b72 is defined in equation (17) while Ri(X,Y ) is a Taylor remainder such that Ri(X,Y )/D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e for i \u2208 {3, 4}. Similarly, we also decompose Bn according to the Voronoi cells as An but then invoke the Taylor expansions of order 1 and 2 to the first term and the second term, respectively. In particular, we get\nBn = \u2212 \u2211\nj:|Cj |=1 \u2211 i\u2208Cj \u2211 |\u03b3|=1 exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R5(X,Y )\n\u2212 \u2211\nj:|Cj |>1 \u2211 i\u2208Cj \u2211 1\u2264|\u03b3|\u22642 exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R6(X,Y ),\nwhere R5(X,Y ) and R6(X,Y ) are Taylor remainders such that their ratios over D2(Gn, G\u2217) approach zero as n \u2192 \u221e. Subsequently, let us define\nSnj,\u03b71,\u03b72 := \u2211 i\u2208Cj \u2211 \u03b1\u2208J\u03b71,\u03b72 exp(\u03b2n0i) 2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14 ,\nTnj,\u03b3 := \u2211 i\u2208Cj exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1i) \u03b3 ,\nfor any (\u03b71, \u03b72) \u0338= (0d, 0) and |\u03b3| =\u0338 0d, while for (\u03b71, \u03b72) = (0d, 0) we set\nSni,0d,0 = \u2212T n i,0d := \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i).\nAs a consequence, it follows that\nHn = K\u2211 j=1 2r\u0304(|Cj |)\u2211 |\u03b71|+\u03b72=0 Snj,\u03b71,\u03b72 \u00b7X \u03b71 exp((\u03b2\u22171i) \u22a4X) \u00b7 \u2202 \u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ K\u2211 j=1 1+1{|Cj |>1}\u2211 |\u03b3|=0 Tnj,\u03b3 \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R5(X,Y ) +R6(X,Y ). (27)\nStep 2 - Non-vanishing coefficients:\nIn this step, we will prove by contradiction that at least one among the ratios Snj,\u03b71,\u03b72/D2(Gn, G\u2217) does not converge to zero as n \u2192 \u221e. Assume that all these terms go to zero, then by employing arguments for deriving equations (19) and (20), we get that\n1 D2(Gn, G\u2217) \u00b7 [ K\u2211 j=1 \u2223\u2223\u2223 \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j) \u2223\u2223\u2223\n+ \u2211\nj:|Cj |=1 \u2211 i\u2208Cj exp(\u03b2n0i) ( \u2225\u2206\u03b2n1ij\u2225+ \u2225\u2206anij\u2225+ |\u2206bnij |+ |\u2206\u03c3nij | )] \u2192 0.\nCombine this limit with the representation of D2(Gn, G\u2217), we have that 1 D2(Gn, G\u2217) \u00b7 \u2211 j:|Cj |>1 \u2211 i\u2208Cj exp(\u03b2n0i) ( \u2225\u2206\u03b2n1ij\u2225r\u0304(|Cj |) + \u2225\u2206anij\u2225 r\u0304(|Cj |) 2 + |\u2206bnij |r\u0304(|Cj |) + |\u2206\u03c3nij | r\u0304(|Cj |) 2 ) \u0338\u2192 0.\nThis result implies that we can find some index j\u2032 \u2208 [K] : |Cj\u2032 | > 1 that satisfies 1 D2(Gn, G\u2217) \u00b7 \u2211 i\u2208Cj\u2032 exp(\u03b2n0i) ( \u2225\u2206\u03b2n1ij\u2032\u2225r\u0304(|Cj\u2032 |) + \u2225\u2206anij\u2032\u2225 r\u0304(|C j\u2032 |) 2 + |\u2206bnij\u2032 |r\u0304(|Cj\u2032 |) + |\u2206\u03c3nij\u2032 | r\u0304(|C j\u2032 |) 2 ) \u0338\u2192 0.\nFor simplicity, we may assume that j\u2032 = 1. Since Sn1,\u03b71,\u03b72/D2(Gn, G\u2217) vanishes as n \u2192 \u221e for any (\u03b71, \u03b72) \u2208 Nd \u00d7 N such that 1 \u2264 |\u03b71|+ \u03b72 \u2264 r\u0304(|Cj |), we divide this term by the left hand side of the above equation and achieve that\u2211\ni\u2208C1 \u2211\n\u03b1\u2208J\u03b71,\u03b72\nexp(\u03b2n0i)\n2\u03b14\u03b1! (\u2206\u03b2n1i1) \u03b11(\u2206ani1) \u03b12(\u2206bni1) \u03b13(\u2206\u03c3ni1) \u03b14\u2211\ni\u2208C1 exp(\u03b2 n 0i) ( \u2225\u2206bni1\u2225r\u0304(|C1|) + \u2225\u2206ani1\u2225 r\u0304(|C1|) 2 + |\u2206bni1|r\u0304(|C1|) + |\u2206\u03c3ni1| r\u0304(|C1|) 2 ) \u2192 0, (28)\nfor any (\u03b71, \u03b72) \u2208 Nd \u00d7 N such that 1 \u2264 |\u03b71|+ \u03b72 \u2264 r\u0304(|C1|).\nSubsequently, we define Mn := max{\u2225\u2206\u03b2n1i1\u2225, \u2225\u2206ani1\u22251/2, |\u2206bni1|, |\u2206\u03c3ni1|1/2 : i \u2208 C1} and pn := max{exp(\u03b2n0i) : i \u2208 C1}. As a result, the sequence exp(\u03b2n0i)/pn is bounded, which indicates that we can substitute it with its subsequence that admits a positive limit z25i := limn\u2192\u221e exp(\u03b2 n 0i)/pn. Therefore, at least one among the limits z25i equals to one. Furthermore, we also denote\n(\u2206\u03b2n1i1)/Mn \u2192 z1i, (\u2206ani1)/Mn \u2192 z2i, (\u2206bni1)/Mn \u2192 z3i, (\u2206\u03c3ni1)/(2Mn) \u2192 z4i.\nFrom the above definition, it follows that at least one among the limits z1i, z2i, z3i and z4i equals to either 1 or \u22121. By dividing both the numerator and the denominator of the term in equation (28) by pnM\n|\u03b71|+\u03b72 n , we arrive at the following system of polynomial equations:\u2211\ni\u2208C1 \u2211 \u03b1\u2208J\u03b71,\u03b72 z25i z \u03b11 1i z \u03b12 2i z \u03b13 3i z \u03b14 4i \u03b11! \u03b12! \u03b13! \u03b14! = 0,\nfor all (\u03b71, \u03b72) \u2208 Nd \u00d7 N : 1 \u2264 |\u03b71| + \u03b72 \u2264 r\u0304(|C1|). Nevertheless, from the definition of r\u0304(|C1|), we know that the above system does not admit any non-trivial solutions, which is a contradiction. Consequently, not all the ratios Snj,\u03b71,\u03b72/D2(Gn, G\u2217) tend to zero as n goes to infinity. Step 3 - Fatou\u2019s contradiction:"
        },
        {
            "heading": "It follows from the hypothesis that EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e.",
            "text": "Then, by applying the Fatou\u2019s lemma, we get\n0 = lim n\u2192\u221e EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))] D2(Gn, G\u2217) = 1 2 \u00b7 \u222b lim inf n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| D2(Gn, G\u2217) dXdY,\nwhich implies that |gGn(Y |X)\u2212gG\u2217(Y |X)|/D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ). Next, we define mn as the maximum of the absolute values of Snj,\u03b71,\u03b72/D2(Gn,G\u2217). It follows from Step 2 that 1/mn \u0338\u2192 \u221e. Moreover, by arguing in the same way as in Step 3 in Appendix A, we receive that\nHn/[mnD2(Gn, G\u2217)] \u2192 0 (29)\nas n \u2192 \u221e. By abuse of notations, let us denote\nSnj,\u03b71,\u03b72/[mnD2(Gn, G\u2217)] \u2192 \u03c4j,\u03b71,\u03b72 , Tnj,\u03b3/[mnD2(Gn, G\u2217)] \u2192 \u03baj,\u03b3 .\nHere, at least one among \u03c4j,\u03b71,\u03b72 , \u03baj,\u03b3 is non-zero. Then, by putting the results in equations (27) and (29) together, we get\nK\u2211 i=1 2r\u0304(|Cj |)\u2211 |\u03b71|+\u03b72=0 \u03c4i,\u03b71,\u03b72 \u00b7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ K\u2211 i=1 1+1{|Cj |>1}\u2211 |\u03b3|=0 \u03bai,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gG\u2217(Y |X) = 0.\nArguing in a similar fashion as in Step 3 of Appendix A, we obtain that \u03c4j,\u03b71,\u03b72 = \u03baj,\u03b3 = 0 for any j \u2208 [K], 0 \u2264 |\u03b71| + \u03b72 \u2264 2r\u0304(|Cj |) and 0 \u2264 |\u03b3| \u2264 1 + 1{|Cj |>1}. This contradicts the fact that at least one among them is non-zero. Hence, the proof is completed."
        },
        {
            "heading": "B.3 PROOF OF PROPOSITION 1",
            "text": "Since the Hellinger distance is lower bounded by the Total Variation distance, i.e. h \u2265 V , it is sufficient to show that\ninf G\u2208Ok(\u2126) EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] > 0.\nFor that purpose, we first demonstrate that\nlim \u03b5\u21920 inf G\u2208Ok(\u2126):D2(G,G\u2217)\u2264\u03b5 EX [V (g(\u00b7|X), gG\u2217(\u00b7|X))] > 0. (30)\nAssume by contrary that the above claim is not true, then we can find a sequence Gn =\u2211kn i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ok(\u2126) that satisfies D2(Gn, G\u2217) \u2192 0 and\nEX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))] \u2192 0\nwhen n tends to infinity. By applying the Fatou\u2019s lemma, we have\n0 = lim n\u2192\u221e EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]\n\u2265 1 2 \u222b X\u00d7Y lim inf n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)|d(X,Y ). (31)\nThe above results indicates that gGn(Y |X)\u2212 gG\u2217(Y |X) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ). WLOG, we may assume that\nmax {\u21131,...,\u2113K} K\u2211 j=1 |C\u2113j | = |C1|+ |C2|+ . . .+ |CK |.\nLet us consider X \u2208 X \u2217\u2113 , where \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. Since D2(Gn, G\u2217) converges to zero, it follows that (\u03b2n1i, a n i , b n i , \u03c3 n i ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) and \u2211 i\u2208Cj exp(\u03b2 n 0i) \u2192 exp(\u03b2\u22170j) for any i \u2208 Cj and j \u2208 [k\u2217]. Thus, we must have that X \u2208 X \u2113 for some \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = C1 \u222a . . . \u222a CK . Otherwise, gGn(Y |X)\u2212 gG\u2217(Y |X) \u0338\u2192 0, which is a contradiction. However, as K < \u2211K j=1 |Cj |, the fact that {\u21131, . . . , \u2113K} = C1 \u222a . . . \u222a CK cannot occur. Therefore, we reach the claim in equation (30). Consequently, there exists some positive constant \u03b5\u2032 such that\ninf G\u2208Ok(\u2126):D2(G,G\u2217)\u2264\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] > 0.\nGiven the above result, it suffices to point out that\ninf G\u2208Ok(\u2126):D2(G,G\u2217)>\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] > 0. (32)\nWe continue to use the proof by contradiction method here. In particular, assume that the inequality (32) does not hold, then there exists a sequence of mixing measures G\u2032n \u2208 Ok(\u2126) such that D2(G\u2032n, G\u2217) > \u03b5\u2032 and\nEX [V (gG\u2032n(\u00b7|X), gG\u2217(\u00b7|X))] \u2192 0.\nBy invoking the Fatou\u2019s lemma as in equation (31), we get that gG\u2032n(Y |X) \u2212 gG\u2217(Y |X) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ). Since \u2126 is a compact set, we can substitute (Gn) with its subsequence which converges to some mixing measure G\u2032 \u2208 Ok(\u2126). Then, the previous limit implies that gG\u2032(Y |X) = gG\u2217(Y |X) for almost surely (X,Y ). From the result of Proposition 2 in Appendix C, we know that the top-K sparse softmax gating Gaussian MoE is identifiable. Therefore, we obtain that G\u2032 \u2261 G\u2217, or equivalently, D2(G\u2032, G\u2217) = 0 On the other hand, due to the hypothesis D2(G\u2032n, G\u2217) > \u03b5\u2032 for any n \u2208 N, we also get that D2(G\u2032, G\u2217) > \u03b5\u2032 > 0, which contradicts the previous result. Hence we reach the claim in equation (32) and totally completes the proof."
        },
        {
            "heading": "B.4 PROOF OF LEMMA 2",
            "text": "Let \u03b7j = Mj\u03b5, where \u03b5 is some fixed positive constant and Mi will be chosen later. As X and \u2126 are bounded sets, we can find some constant c\u2217\u2113 \u2265 0 such that\nmin x,j,j\u2032\n[ (\u03b2\u22171j) \u22a4x\u2212 (\u03b2\u22171j\u2032)\u22a4x ] = c\u2217\u2113\u03b5,\nwhere the above minimum is subject to x \u2208 X \u2217\u2113 , j \u2208 {\u21131, . . . , \u2113K} and j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}. By arguing similarly to the proof of Lemma 1 in Appendix A.3, we deduce that c\u2217\u2113 > 0.\nSince X is a bounded set, we may assume that \u2225x\u2225 \u2264 B for any x \u2208 X . Let x \u2208 X \u2217\u2113 and \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = C\u21131 \u222a . . . \u222a C\u2113K . Then, for any i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k}, we have that\n\u03b2\u22a41ix = (\u03b21i \u2212 \u03b2\u22171j)\u22a4x+ (\u03b2\u22171j)\u22a4x \u2265 \u2212Mi\u03b5B + (\u03b2\u22171j\u2032)\u22a4x+ c\u2217\u2113\u03b5 = \u2212Mi\u03b5B + c\u2217\u2113\u03b5+ (\u03b2\u22171j\u2032 \u2212 \u03b21i\u2032)\u22a4x+ \u03b2\u22a41i\u2032x \u2265 \u22122Mi\u03b5B + c\u2217\u2113\u03b5+ \u03b2\u22a41i\u2032x,\nwhere j \u2208 {\u21131, . . . , \u2113K} and j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} such that i \u2208 Cj and i\u2032 \u2208 Cj\u2032 . If Mj \u2264 c\u2217\u2113 2B , then we get that x \u2208 X\u2113, which leads to X \u2217\u2113 \u2286 X \u2113. Analogously, assume that there exists some constant c\u2113 \u2265 0 such that\nmin x,j,j\u2032\n[ (\u03b2\u22171j) \u22a4x\u2212 (\u03b2\u22171j\u2032)\u22a4x ] = c\u2217\u2113\u03b5,\nwhere the minimum is subject to x \u2208 X \u2113, i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k}. Then, if Mj \u2264\nc\u2113 2B , then we receive that X \u2113 \u2286 X \u2217\u2113 .\nAs a consequence, by setting Mj = 1\n2B min{c\u2217\u2113 , c\u2113}, we achieve the conclusion that X \u2113 = X \u2217\u2113 .\nC IDENTIFIABILITY OF THE TOP-K SPARSE SOFTMAX GATING GAUSSIAN MIXTURE OF EXPERTS\nIn this appendix, we study the identifiability of the top-K sparse softmax gating Gaussian MoE, which plays an essential role in ensuring the convergence of the MLE G\u0302n to the true mixing measure G\u2217 under Voronoi loss functions.\nProposition 2 (Identifiability). Let G and G\u2032 be two arbitrary mixing measures in Ok(\u0398). Suppose that the equation gG(Y |X) = gG\u2032(Y |X) holds for almost surely (X,Y ) \u2208 X \u00d7 Y , then it follows that G \u2261 G\u2032.\nProof of Proposition 2. First, we assume that two mixing measures G and G\u2032 take the following forms: G = \u2211k i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) and G \u2032 = \u2211k\u2032 i=1 exp(\u03b2 \u2032 0i)\u03b4(\u03b2\u20321i,a\u2032i,b\u2032i,\u03c3\u2032i). Recall that gG(Y |X) = gG\u2032(Y |X) for almost surely (X,Y ), then we have k\u2211\ni=1\nSoftmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) \u00b7 f(Y |a\u22a4i X + bi, \u03c3i)\n= k\u2032\u2211 i=1 Softmax(TopK((\u03b2\u20321i) \u22a4X,K;\u03b2\u20320i)) \u00b7 f(Y |(a\u2032i)\u22a4 + b\u2032i, \u03c3\u2032i). (33)\nDue to the identifiability of the location-scale Gaussian mixtures Teicher (1960; 1961; 1963), we get that k = k\u2032 and{ Softmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) : i \u2208 [k] } \u2261 { Softmax(TopK((\u03b2\u20321i) \u22a4X,K;\u03b2\u20320i)) : i \u2208 [k] } ,\nfor almost surely X . WLOG, we may assume that\nSoftmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) = Softmax(TopK((\u03b2 \u2032 1i) \u22a4X,K;\u03b2\u20320i)), (34)\nfor almost surely X for any i \u2208 [k]. Since the Softmax function is invariant to translations, it follows from equation (34) that \u03b21i = \u03b2\u20321i + v1 and \u03b20i = \u03b2 \u2032 0i + v0 for some v1 \u2208 Rd and v0 \u2208 R. Notably, from the assumption of the model, we have \u03b21k = \u03b2\u20321k = 0d and \u03b20k = \u03b2 \u2032 0k = 0, which implies that v1 = 0d and v0 = 0. As a result, we obtain that \u03b21i = \u03b2\u20321i and \u03b20i = \u03b2 \u2032 0i for any i \u2208 [k].\nLet us consider X \u2208 X\u2113 where \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. Then, equation (33) can be rewritten as\nK\u2211 i=1 exp(\u03b20i) exp(\u03b2 \u22a4 1iX)f(Y |a\u22a4i X + bi, \u03c3i) = K\u2211 i=1 exp(\u03b20i) exp(\u03b2 \u22a4 1iX)f(Y |(a\u2032i)\u22a4X + b\u2032i, \u03c3\u2032i),\n(35)\nfor almost surely (X,Y ). Next, we denote J1, J2, . . . , Jm as a partition of the index set [k], where m \u2264 k, such that exp(\u03b20i) = exp(\u03b20i\u2032) for any i, i\u2032 \u2208 Jj and j \u2208 [m]. On the other hand, when i and i\u2032 do not belong to the same set Jj , we let exp(\u03b20i) \u0338= exp(\u03b20i\u2032). Thus, we can reformulate equation (35) as\nm\u2211 j=1 \u2211 i\u2208Jj exp(\u03b20i) exp(\u03b2 \u22a4 1iX)f(Y |a\u22a4i X + bi, \u03c3i)\n= m\u2211 j=1 \u2211 i\u2208Jj exp(\u03b20i) exp(\u03b2 \u22a4 1iX)f(Y |(a\u2032i)\u22a4X + b\u2032i, \u03c3\u2032i),\nfor almost surely (X,Y ). This results leads to {((ai)\u22a4X + bi, \u03c3i) : i \u2208 Jj} \u2261 {((a\u2032i)\u22a4X + b\u2032i, \u03c3\u2032i) : i \u2208 Jj}, for almost surely X for any j \u2208 [m]. Therefore, we have\n{(ai, bi, \u03c3i) : i \u2208 Jj} \u2261 {(a\u2032i, b\u2032i, \u03c3\u2032i) : i \u2208 Jj},\nfor any j \u2208 [m]. As a consequence,\nG = m\u2211 j=1 \u2211 i\u2208Jj exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) = m\u2211 j=1 \u2211 i\u2208Jj exp(\u03b2\u20320i)\u03b4(\u03b2\u20321i,a\u2032i,b\u2032i,\u03c3\u2032i) = G \u2032.\nHence, we reach the conclusion of this proposition."
        },
        {
            "heading": "D NUMERICAL EXPERIMENTS",
            "text": "In this appendix, we conduct a few numerical experiments to illustrate the theoretical convergence rates of the MLE G\u0302n to the true mixing measure G\u2217 under both the exact-specified and the overspecified settings."
        },
        {
            "heading": "D.1 EXPERIMENTAL SETUP",
            "text": "Synthetic Data. First, we assume that the true mixing measure G\u2217 = \u2211k\u2217\ni=1 exp(\u03b2 \u2217 0i)\u03b4(\u03b2\u22171i,a\u2217i ,b\u2217i ,\u03c3\u2217i )\nis of order k\u2217 = 2 and associated with the following ground-truth parameters:\n\u03b2\u221701 = \u22128, \u03b2\u221711 = 25, a\u22171 = \u221220, b\u22171 = 15, \u03c3\u22171 = 0.3, \u03b2\u221702 = 0, \u03b2 \u2217 12 = 0, a \u2217 2 = 20, b \u2217 2 = \u22125, \u03c3\u22172 = 0.4.\nThen, we generate i.i.d samples {(Xi, Yi)}ni=1 by first sampling Xi\u2019s from the uniform distribution Uniform[0, 1] and then sampling Yi\u2019s from the true conditional density gG\u2217(Y |X) of top-K sparse softmax gating Gaussian mixture of experts (MoE) given in equation (1). In Figure 2, we visualize the relationship between X and Y when the numbers of experts chosen from gG\u2217(Y |X) are K = 1 (Figure 2a) and K = 2 (Figure 2b), respectively. However, throughout the following experiments, we will consider only the scenario when K = 1, that is, we choose the best expert from the true conditional density gG\u2217(Y |X).\nMaximum Likelihood Estimation (MLE). A popular approach to determining the MLE G\u0302n for each set of samples is to use the EM algorithm Dempster et al. (1977). However, since there are not any closed-form expressions for updating the gating parameters \u03b20i, \u03b21i in the maximization steps, we have to leverage an EM-based numerical scheme, which was previously used in Chamroukhi et al. (2009). In particular, we utilize a simple coordinate gradient descent algorithm in the maximization steps. Additionally, we select the convergence criterion of \u03f5 = 10\u22126 and run a maximum of 2000 EM iterations.\nInitialization. For each k \u2208 {k\u2217, k\u2217 + 1}, we randomly distribute elements of the set {1, 2, ..., k} into k\u2217 different Voronoi cells C1, C2, . . . , Ck\u2217 , each contains at least one element. Moreover, we repeat this process for each replication. Subsequently, for each j \u2208 [k\u2217], we initialize parameters \u03b21i by sampling from a Gaussian distribution centered around its true counterpart \u03b2\u22171j with a small variance, where i \u2208 Cj . Other parameters \u03b20i, ai, bi, \u03c3i are also initialized in a similar fashion."
        },
        {
            "heading": "D.2 EXACT-SPECIFIED SETTINGS",
            "text": "Under the exact-specified settings, we conduct 40 sample generations for each configuration, across a spectrum of 200 different sample sizes n ranging from 102 to 105. It can be seen from Figure 3a that the MLE G\u0302n empirically converges to the true mixing measure G\u2217 under the Voronoi metric D1 at the rate of order O\u0303(n\u22121/2), which perfectly matches the theoretical parametric convergence rate established in Theorem 2."
        },
        {
            "heading": "D.3 OVER-SPECIFIED SETTINGS",
            "text": "Under the over-specified settings, we continue to generate 40 samples of size n for each setting, given 100 different choices of sample size n \u2208 [102, 105]. As discussed in Section 3, to guarantee the convergence of density estimation to the true density, we need to select K = 2 experts from the density estimation. As far as we know, existing works, namely Kwon et al. (2019); Kwon & Caramanis (2020); Kwon et al. (2021), only focus on the global convergence of the EM algorithm for parameter estimation under the input-free gating MoE, while that under the top-K sparse softmax gating MoE has remained poorly understood. Additionally, it is worth noting that the sample size must be sufficiently large so that the empirical convergence rate of the MLE returned by the EM algorithm aligns with the theoretical rate of order O\u0303(n\u22121/2) derived in Theorem 4."
        },
        {
            "heading": "E ADDITIONAL RESULTS",
            "text": "In this appendix, we study the convergence rates of parameter estimation under the model (1) when f is a probability density function of an arbitrary location-scale distribution. For that purpose, we first characterize the family of probability density functions of location-scale distributions\nF := {f(Y |h1(X, a, b), \u03c3) : (a, b, \u03c3) \u2208 \u0398}, (36)\nwhere h1(X, a, b) := a\u22a4X + b stands for the location, \u03c3 denotes the scale and \u0398 is a compact subset of Rd\u00d7R\u00d7R+, based on the following notion of strong identifiability, which was previously studied in Manole & Ho (2022) and Ho & Nguyen (2016):\nDefinition 1 (Strong Identifiability). We say that the family F is strongly identifiable if the probability density function f(Y |h1(X, a, b), \u03c3) is twice differentiable w.r.t its parameters and the following assumption holds true:\nFor any k \u2265 1 and k pairwise different tuples (a1, b1, \u03c31), . . . , (ak, bk, \u03c3k) \u2208 \u0398, if there exist real coefficients \u03b1(i)\u21131,\u21132 , for i \u2208 [k\u2217] and 0 \u2264 \u21131 + \u21132 \u2264 2, such that\nk\u2211 i=1 2\u2211 \u21131+\u21132=0 \u03b1 (i) \u21131,\u21132 \u00b7 \u2202 \u21131+\u21132f \u2202h\u211311 \u2202\u03c3 \u21132 (Y |h1(X, ai, bi), \u03c3(X,\u03c3i)) = 0,\nfor almost surely (X,Y ), then we obtain that \u03b1(i)\u21131,\u21132 = 0 for any i \u2208 [k\u2217] and 0 \u2264 \u21131 + \u21132 \u2264 2.\nExample 1. The families of Student\u2019s t-distributions and Laplace distributions are strongly identifiable, while the family of location-scale Gaussian distributions is not.\nIn high level, we need to establish the Total Variation lower bound EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D(G,G\u2217) for any G \u2208 Ok(\u2126). Then, this bound together with the density estimation rate in Theorem 1 (resp. Theorem 3) leads to the parameter estimation rates in Theorem 2 (resp. Theorem 4). Here, the key step is to decompose the difference gG\u0302n(Y |X) \u2212 gG\u2217(Y |X) into a combination of linearly independent terms using Taylor expansions. Therefore, we have to involve the above notion of strong identifiability, and separate our convergence analysis based on that notion.\nSubsequently, since the convergence rates of maximum likelihood estimation when F is a family of location-scale Gaussian distributions, which is not strongly identifiable, have already been studied in Section 2 and Section 3, we will focus on the scenario when the family F is strongly identifiable in the sequel. Under that assumption, the density f(Y |h1, \u03c3) is twice differentiable in (h1, \u03c3), therefore, it is also Lipschitz continuous. As a consequence, the density estimation rates under both the exact-specified and over-specified in Theorem 1 and Theorem 3 still hold true. Therefore, we aim to establish the parameter estimation rates under those settings in Appendix E.1 and Appendix E.2, respectively."
        },
        {
            "heading": "E.1 EXACT-SPECIFIED SETTINGS",
            "text": "In this appendix, by using the Voronoi loss function D1(G,G\u2217) defined in equation (5), we demonstrate in the following theorem that the rates for estimating true parameters exp(\u03b2\u22170i), \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i are of parametric order O\u0303(n\u22121/2), which totally match those in Theorem 2. Theorem 5. Under the exact-specified settings, if the family F is strongly identifiable, then the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D1(G,G\u2217) holds for any mixing measure G \u2208 Ek\u2217(\u2126). Consequently, we can find a universal constant C3 > 0 depending only on G\u2217, \u2126 and K such that\nP ( D1(G\u0302n, G\u2217) > C3 \u221a log(n)/n ) \u2272 n\u2212c3 ,\nwhere c3 > 0 is a universal constant that depends only on \u2126.\nProof of Theorem 5 is in Appendix E.3.1."
        },
        {
            "heading": "E.2 OVER-SPECIFIED SETTINGS",
            "text": "In this appendix, we capture the convergence rates of parameter estimation under the over-specified settings when the family F is strongly identifiable. Voronoi metric. It is worth noting that when F is strongly identifiable, the interaction among expert parameters in the second PDE (6) no longer holds true. As a result, it is not necessary to involve the solvability of the system (7) in the formulation of the Voronoi loss as in equation (8). Instead, let us consider the Voronoi loss function D3(G,G\u2217) defined as\nD3(G,G\u2217) := max {\u2113j}Kj=1\u2282[k\u2217] { \u2211 j\u2208[K], |C\u2113j |=1 \u2211 i\u2208C\u2113j exp(\u03b20i) [ \u2225\u2206\u03b21i\u2113j\u2225+ \u2225\u2206ai\u2113j\u2225+ |\u2206bi\u2113j |+ |\u2206\u03c3i\u2113j | ]\n+ \u2211\nj\u2208[K], |C\u2113j |>1\n\u2211 i\u2208C\u2113j exp(\u03b20i) [ \u2225\u2206\u03b21i\u2113j\u22252 + \u2225\u2206ai\u2113j\u22252 + |\u2206bi\u2113j |2 + |\u2206\u03c3i\u2113j |2 ]\n+ K\u2211 j=1 \u2223\u2223\u2223 \u2211 i\u2208C\u2113j exp(\u03b20i)\u2212 exp(\u03b2\u22170\u2113j ) \u2223\u2223\u2223}, (37)\nfor any mixing measure G \u2208 Ok(\u2126). Equipped with loss function, we are ready to illustrate the convergence behavior of maximum likelihood estimation in the following theorem:\nTheorem 6. Under the over-specified settings, if the family F is strongly identifiable, then the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D2(G,G\u2217) holds for any mixing measure G \u2208 Ok(\u2126). As a consequence, we can find a universal constant C4 > 0 depending only on G\u2217, \u2126 and K such that\nP ( D3(G\u0302n, G\u2217) > C4 \u221a log(n)/n ) \u2272 n\u2212c4 ,\nwhere c4 > 0 is a universal constant that depends only on \u2126.\nProof of Theorem 6 is in Appendix E.3.2. Theorem 6 indicates that true parameters \u03b2\u22171i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i , which are fitted by a single component, share the same estimation rates of order O\u0303(n\u22121/2) as those in Theorem 4. By contrast, the estimation rates for true parameters \u03b2\u22171i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i , which are fitted by more than one component, are of order O\u0303(n\u22121/4). Notably, these rates are no longer determined by the solvability of the system (7). Thus, they are significantly faster than those in Theorem 4. The main reason for this improvement is when F is strongly identifiable, the interaction among expert parameters via the second PDE in equation (6) does not occur."
        },
        {
            "heading": "E.3 PROOFS OF ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "E.3.1 PROOF OF THEOREM 5",
            "text": "Following from the result of Theorem 1 and since the Hellinger distance is lower bounded by the Total Variation distance, i.e. h \u2265 V , it is sufficient to demonstrate for any mixing measure G \u2208 Ok(\u0398) that\nEX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D1(G,G\u2217),\nwhich is then respectively broken into local part and global part as follows:\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0, (38)\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)>\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0, (39)\nfor some constant \u03b5\u2032 > 0. Subsequently, we will prove only the local part (38), while the proof of the global part (39) can be done similarly to that in Appendix A.2.\nProof of claim (38): It is sufficient to show that\nlim \u03b5\u21920 inf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0.\nAssume that this inequality does not hold, then since the number of experts k\u2217 is known in this case, there exists a sequence of mixing measure Gn := \u2211k\u2217 i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ek\u2217(\u2126) such that both D1(Gn, G\u2217) and EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D1(Gn, G\u2217) approach zero as n tends to infinity. Since D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e, each Voronoi cell contains only one element. Thus, we may assume WLOG that Cj = {j} for any j \u2208 [k\u2217], which implies that (\u03b2n1j , anj , bnj , \u03c3nj ) \u2192 (\u03b2\u22171j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ) and exp(\u03b2 n 0j) \u2192 exp(\u03b2\u22170j) as n \u2192 \u221e. WLOG, we assume that D1(Gn, G\u2217) = K\u2211 i=1 [ exp(\u03b2n0i) ( \u2225\u2206\u03b2n1i\u2225+ \u2225\u2206ani \u2225+ \u2225\u2206bni \u2225+ \u2225\u2206\u03c3ni \u2225 ) +\n\u2223\u2223\u2223 exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i)\u2223\u2223\u2223], where we denote \u2206\u03b2n1i := \u03b2 n 1i \u2212 \u03b2\u22171i, \u2206ani := ani \u2212 a\u2217i , \u2206bni := bni \u2212 b\u2217i and \u2206\u03c3ni := \u03c3ni \u2212 \u03c3\u2217i .\nSubsequently, by arguing in the same fashion as in Appendix A.2, we obtain that Xn\u2113 = X \u2217\u2113 , where\nXn\u2113 := { x \u2208 X : (\u03b2n1j)\u22a4x \u2265 (\u03b2n1j\u2032)\u22a4x : \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } ,\nX \u2217\u2113 := { x \u2208 X : (\u03b2\u22171j)\u22a4x \u2265 (\u03b2\u22171j\u2032)\u22a4x : \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } ,\nfor any \u2113 \u2208 [q] for sufficiently large n. Let \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. Then, for almost surely (X,Y ) \u2208 X \u2217\u2113 \u00d7 Y , we can rewrite the conditional densities gGn(Y |X) and gG\u2217(Y |X) as\ngGn(Y |X) = K\u2211 i=1 exp((\u03b2n1i) \u22a4X + \u03b2n0i)\u2211K j=1 exp((\u03b2 n 1j) \u22a4X + \u03b2n0j) \u00b7 f(Y |(ani )\u22a4X + bni , \u03c3ni ),\ngG\u2217(Y |X) = K\u2211 i=1 exp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i)\u2211K j=1 exp((\u03b2 \u2217 1j) \u22a4X + \u03b2\u22170j) \u00b7 f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ).\nNow, we break the rest of our arguments into three steps:\nStep 1 - Taylor expansion:\nIn this step, we take into account Hn := [\u2211K i=1 exp((\u03b2 \u2217 1i) \u22a4X + \u03b2\u22170i) ] \u00b7 [gGn(Y |X)\u2212 gG\u2217(Y |X)]. Note that the quantity Hn can be represented as follows:\nHn = K\u2211 i=1 exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171i)\u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) ]\n\u2212 K\u2211 i=1 exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)gGn(Y |X)\u2212 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) ]\n+ K\u2211 i=1 [ exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i) ][ exp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\u2212 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) ] .\nBy applying the first-order Taylor expansion to the first term in the above representation, which is denoted by An, we get that\nAn = K\u2211 i=1 \u2211 |\u03b1|=1 exp(\u03b2n0i) \u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b11+\u03b12 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202|\u03b12|+\u03b13+\u03b14f\n\u2202h |\u03b12|+\u03b13 1 \u2202\u03c3\n\u03b14 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y ),\nwhere R1(X,Y ) is a Taylor remainder that satisfies R1(X,Y )/D1(X,Y ) \u2192 0 as n \u2192 \u221e. Let \u03b71 = \u03b11 + \u03b12 \u2208 Nd, \u03b72 = |\u03b12|+ \u03b13 \u2208 N and \u03b73 = \u03b14 \u2208 N, then we can rewrite An as follows:\nAn = K\u2211 i=1 1\u2211 \u03b73=0 2\u2212\u03b73\u2211 |\u03b71|+\u03b72=1\u2212\u03b73 \u2211 \u03b1\u2208I\u03b71,\u03b72,\u03b73 exp(\u03b2n0i) \u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202\u03b72+\u03b73f\n\u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y ), (40)\nwhere we define\nI\u03b71,\u03b72,\u03b73 := {(\u03b1i)4i=1 \u2208 Nd \u00d7 Nd \u00d7 N\u00d7 N : \u03b11 + \u03b12 = \u03b71, |\u03b12|+ \u03b13 = \u03b72, \u03b14 = \u03b73}. (41)\nBy arguing in a similar fashion for the second term in the representation of Hn, we also get that\nBn := \u2212 K\u2211 i=1 \u2211 |\u03b3|=1 exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R2(X,Y ),\nwhere R2(X,Y ) is a Taylor remainder such that R2(X,Y )/D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Putting the above results together, we rewrite the quantity Hn as follows:\nHn = K\u2211 i=1 1\u2211 \u03b73=0 2\u2212\u03b73\u2211 |\u03b71|+\u03b72=0 Uni,\u03b71,\u03b72,\u03b73 \u00b7X \u03b71 exp((\u03b2\u22171i) \u22a4X) \u2202\u03b72+\u03b73f \u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ K\u2211 i=1 \u2211 0\u2264|\u03b3|\u22641 Wni,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) +R1(X,Y ) +R2(X,Y ), (42)\nin which we respectively define for each i \u2208 [K] that\nUni,\u03b71,\u03b72,\u03b73 := \u2211\n\u03b1\u2208I\u03b71,\u03b72,\u03b73\nexp(\u03b2n0i)\n\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14 ,\nWni,\u03b3 := \u2212 exp(\u03b2n0i)\n\u03b3! (\u2206\u03b2n1i) \u03b3 ,\nfor any (\u03b71, \u03b72, \u03b73) \u0338= (0d, 0, 0) and |\u03b3| =\u0338 0d. Additionally, Uni,0d,0,0 = \u2212W n i,0d := exp(\u03b2n0i) \u2212 exp(\u03b2\u22170i).\nStep 2 - Non-vanishing coefficients:\nMoving to the second step, we will show that not all the ratios Uni,\u03b71,\u03b72,\u03b73/D1(Gn, G\u2217) tend to zero as n goes to infinity. Assume by contrary that all of them approach zero when n \u2192 \u221e, then for (\u03b71, \u03b72, \u03b73) = (0d, 0, 0), it follows that\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 \u2223\u2223\u2223 exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i)\u2223\u2223\u2223 = K\u2211 i=1 |Unj,\u03b71,\u03b72,\u03b73 | D1(Gn, G\u2217) \u2192 0. (43)\nAdditionally, for tuples (\u03b71, \u03b72, \u03b73) where \u03b71 \u2208 {e1, e2, . . . , ed} with ej := (0, . . . , 0, 1\ufe38\ufe37\ufe37\ufe38 j\u2212th , 0, . . . , 0) and \u03b72 = \u03b73 = 0, we get\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i)\u2225\u2206\u03b2n1i\u22251 = K\u2211 i=1 \u2211 \u03b71\u2208{e1,...,ed} |Unj,\u03b71,0,0| D1(Gn, G\u2217) \u2192 0.\nBy using similar arguments, we end up with\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i) [ \u2225\u2206\u03b2n1i\u22251 + \u2225\u2206ani \u22251 + |\u2206bni |+ |\u2206\u03c3ni | ] \u2192 0.\nDue to the topological equivalence between norm-1 and norm-2, the above limit implies that\n1\nD1(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i) [ \u2225\u2206\u03b2n1i\u2225+ \u2225\u2206ani \u2225+ |\u2206bni |+ |\u2206\u03c3ni | ] \u2192 0. (44)\nCombine equation (43) with equation (44), we deduce that D1(Gn, G\u2217)/D1(Gn, G\u2217) \u2192 0, which is a contradiction. Consequently, at least one among the ratios Uni,\u03b71,\u03b72,\u03b73/D1(Gn, G\u2217) does not vanish as n tends to infinity.\nStep 3 - Fatou\u2019s contradiction:\nLet us denote by mn the maximum of the absolute values of Uni,\u03b71,\u03b72,\u03b73/D1(Gn, G\u2217) and Wni,\u03b3/D1(Gn, G\u2217). It follows from the result achieved in Step 2 that 1/mn \u0338\u2192 \u221e.\nRecall from the hypothesis that EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Thus, by the Fatou\u2019s lemma, we have\n0 = lim n\u2192\u221e EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))] D1(Gn, G\u2217) = 1 2 \u00b7 \u222b lim inf n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| D1(Gn, G\u2217) dXdY.\nThis result indicates that |gGn(Y |X)\u2212 gG\u2217(Y |X)|/D1(Gn, G\u2217) tends to zero as n goes to infinity for almost surely (X,Y ). As a result, it follows that\nlim n\u2192\u221e Hn mnD(Gn, G\u2217)\n= [ K\u2211\ni=1\nexp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i) ] \u00b7 lim n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| mnD1(Gn, G\u2217) = 0.\nNext, let us denote Uni,\u03b71,\u03b72,\u03b73/[mnD1(Gn, G\u2217)] \u2192 \u03c4i,\u03b71,\u03b72,\u03b73 and W n i,\u03b3/[mnD1(Gn, G\u2217)] \u2192 \u03bai,\u03b3 with a note that at least one among them is non-zero. From the formulation of Hn in equation (42), we deduce that\nK\u2211 i=1 1\u2211 \u03b73=0 2\u2212\u03b73\u2211 |\u03b71|+\u03b72=0 \u03c4i,\u03b71,\u03b72,\u03b73 \u00b7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u2202\u03b72+\u03b73f \u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ K\u2211 i=1 \u2211 0\u2264|\u03b3|\u22641 \u03bai,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gG\u2217(Y |X) = 0,\nfor almost surely (X,Y ). This equation is equivalent to 1\u2211\n|\u03b71|=0\n[ K\u2211 i=1 2\u2212|\u03b71|\u2211 \u03b72+\u03b73=0 \u03c4i,\u03b71,\u03b72,\u03b73 exp((\u03b2 \u2217 1i) \u22a4X) \u2202\u03b72+\u03b73f \u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ \u03bai,\u03b71 exp((\u03b2 \u2217 1i) \u22a4X)gG\u2217(Y |X) ] \u00d7X\u03b71 = 0,\nfor almost surely (X,Y ). It is clear that the left hand side of the above equation is a polynomial of X belonging to the compact set X . As a result, we get that\nK\u2211 i=1 2\u2212|\u03b71|\u2211 \u03b72+\u03b73=0 \u03c4i,\u03b71,\u03b72,\u03b73 exp((\u03b2 \u2217 1i) \u22a4X) \u2202\u03b72+\u03b73f \u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ \u03bai,\u03b71gG\u2217(Y |X) exp((\u03b2\u22171i)\u22a4X) = 0,\nfor any i \u2208 [K], 0 \u2264 |\u03b71| \u2264 1 and almost surely (X,Y ). Since (a\u22171, b\u22171, \u03c3\u22171), . . . , (a\u2217K , b\u2217K , \u03c3\u2217K) have pair-wise distinct values and the family F is strongly identifiable, the set{ \u2202\u03b72+\u03b73f\n\u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) : i \u2208 [K], 0 \u2264 \u03b72 + \u03b73 \u2264 2\u2212 |\u03b71| } is linearly independent w.r.t (X,Y ). Consequently, we obtain that \u03c4i,\u03b71,\u03b72,\u03b73 = \u03bai,\u03b71 = 0 for any i \u2208 [K], 0 \u2264 |\u03b71| \u2264 1 and 0 \u2264 \u03b72 + \u03b73 \u2264 2\u2212 |\u03b71|, which contradicts the fact that at least one among these terms is different from zero.\nHence, we reach the desired conclusion."
        },
        {
            "heading": "E.3.2 PROOF OF THEOREM 6",
            "text": "Similar to the proof of Theorem 5 in Appendix E.3.1, our objective here is also to derive the local part of the following Total Variation lower bound:\nEX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] \u2273 D3(G,G\u2217),\nfor any G \u2208 Ok(\u0398). In particular, we aim to show that\nlim \u03b5\u21920 inf G\u2208Ok(\u0398):D3(G,G\u2217)\u2264\u03b5 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D2(G,G\u2217) > 0. (45)\nAssume that the above claim does not hold true, then we can find a sequence of mixing measures Gn := \u2211kn i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ok(\u2126) such that both D3(Gn, G\u2217) and EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D3(Gn, G\u2217) vanish when n goes to infinity. Then, it follows that for any j \u2208 [k\u2217], we have \u2211 i\u2208Cj exp(\u03b2 n 0i) \u2192 exp(\u03b2\u22170j) and (\u03b2n1i, ani , bni , \u03c3ni ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) for all i \u2208 Cj . WLOG, we may assume that\nD3(Gn, G\u2217) = \u2211\nj\u2208[K], |Cj |>1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ \u2225\u2206\u03b2n1ij\u22252 + \u2225\u2206anij\u22252 + |\u2206bnij |2 + |\u2206\u03c3nij |2 ]\n+ \u2211\nj\u2208[K], |Cj |=1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ \u2225\u2206\u03b2n1ij\u2225+ \u2225\u2206anij\u2225+ |\u2206bnij |+ |\u2206\u03c3nij | ] + K\u2211 j=1 \u2223\u2223\u2223 \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j) \u2223\u2223\u2223.\nSubsequently, let X \u2208 X \u2217\u2113 for some \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}, where\nX \u2217\u2113 := { x \u2208 X : (\u03b2\u22171j)\u22a4x \u2265 (\u03b2\u22171j\u2032)\u22a4x, \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } .\nThen, for any \u2113 \u2208 [q], we denote (\u21131, . . . , \u2113k) as a permutation of (1, . . . , k) and\nXn \u2113 := { x \u2208 X : (\u03b2n1i)\u22a4x \u2265 (\u03b2n1i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i \u2032 \u2208 {\u2113K+1, . . . , \u2113k} } .\nIf {\u21131, . . . \u2113K} =\u0338 C1 \u222a . . . \u222a CK for any \u2113 \u2208 [q], then V (gGn(\u00b7|X), gG\u2217(\u00b7|X))/D3(Gn, G\u2217) \u0338\u2192 0 as n tends to infinity. This contradicts the fact that this term must approach zero. Therefore, we only need to consider the scenario when there exists \u2113 \u2208 [q] such that {\u21131, . . . \u2113K} = C1 \u222a . . . \u222a CK . By using the same arguments as in Appendix B.2, we obtain that X \u2208 Xn\n\u2113 .\nThen, we can represent the conditional densities gG\u2217(Y |X) and gGn(Y |X) for any sufficiently large n as follows:\ngG\u2217(Y |X) = K\u2211 j=1 exp((\u03b2\u22171j) \u22a4X + \u03b2\u22170j)\u2211K j\u2032=1 exp((\u03b2 \u2217 1j\u2032) \u22a4X + \u03b2\u22170j\u2032) \u00b7 f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ), gGn(Y |X) = K\u2211 j=1 \u2211 i\u2208Cj exp((\u03b2n1i) \u22a4X + \u03b2n0i)\u2211K j\u2032=1 \u2211 i\u2032\u2208Cj\u2032 exp((\u03b2n1i\u2032) \u22a4X + \u03b2n0i\u2032) \u00b7 f(Y |(ani )\u22a4X + bni , \u03c3ni ).\nNow, we reuse the three-step framework in Appendix E.3.1.\nStep 1 - Taylor expansion:\nFirstly, by abuse of notations, let us consider the quantity Hn := [ K\u2211 j=1 exp((\u03b2\u22171j) \u22a4X + \u03b2\u22170j) ] \u00b7 [gGn(Y |X)\u2212 gG\u2217(Y |X)].\nSimilar to Step 1 in Appendix A, we can express this term as\nHn = K\u2211 j=1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171j)\u22a4X)f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ) ]\n\u2212 K\u2211 j=1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)gGn(Y |X)\u2212 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) ]\n+ K\u2211 j=1 [ \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j) ][ exp((\u03b2\u22171j) \u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\u2212 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) ] := An +Bn + En.\nNext, we proceed to decompose An based on the cardinality of the Voronoi cells as follows: An = \u2211\nj:|Cj |=1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171i)\u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) ]\n+ \u2211\nj:|Cj |>1 \u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171i)\u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) ] .\nBy applying the Taylor expansions of first and second orders to the first and second terms of An, respectively, and following the derivation in equation (40), we arrive at An = \u2211\nj:|Cj |=1 \u2211 i\u2208Cj 1\u2211 \u03b73=0 2\u2212\u03b73\u2211 |\u03b71|+\u03b72=1\u2212\u03b73 \u2211 \u03b1\u2208I\u03b71,\u03b72,\u03b73 exp(\u03b2n0i) \u03b1! \u00b7 (\u2206\u03b2n1ij)\u03b11(\u2206anij)\u03b12(\u2206bnij)\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171j)\u22a4X) \u00b7 \u2202\u03b72+\u03b73f\n\u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ) +R3(X,Y )\n+ \u2211\nj:|Cj |>1 \u2211 i\u2208Cj 2\u2211 \u03b73=0 4\u2212\u03b73\u2211 |\u03b71|+\u03b72=1\u22121{\u03b73>0} \u2211 \u03b1\u2208I\u03b71,\u03b72,\u03b73 exp(\u03b2n0i) \u03b1! \u00b7 (\u2206\u03b2n1ij)\u03b11(\u2206anij)\u03b12(\u2206bnij)\u03b13(\u2206\u03c3nij)\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171j)\u22a4X) \u00b7 \u2202\u03b72+\u03b73f\n\u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ) +R4(X,Y ),\nwhere the set I\u03b71,\u03b72,\u03b73 is defined in equation (41) while Ri(X,Y ) is a Taylor remainder such that Ri(X,Y )/D3(Gn, G\u2217) \u2192 0 as n \u2192 \u221e for i \u2208 {3, 4}. Similarly, we also decompose Bn as\nBn = \u2212 \u2211\nj:|Cj |=1 \u2211 i\u2208Cj \u2211 |\u03b3|=1 exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) +R5(X,Y )\n\u2212 \u2211\nj:|Cj |>1 \u2211 i\u2208Cj \u2211 1\u2264|\u03b3|\u22642 exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) +R6(X,Y ),\nwhere R5(X,Y ) and R6(X,Y ) are Taylor remainders such that their ratios over D3(Gn, G\u2217) approach zero as n \u2192 \u221e. Subsequently, let us define\nSnj,\u03b71,\u03b72,\u03b73 := \u2211 i\u2208Cj \u2211 \u03b1\u2208I\u03b71,\u03b72,\u03b73 exp(\u03b2n0i) \u03b1! \u00b7 (\u2206\u03b2n1ij)\u03b11(\u2206anij)\u03b12(\u2206bnij)\u03b13(\u2206\u03c3nij)\u03b14 ,\nTnj,\u03b3 := \u2212 \u2211 i\u2208Cj exp(\u03b2n0i) \u03b3! (\u2206\u03b2n1ij) \u03b3 = \u2212Snj,\u03b3,0,0,\nfor any (\u03b71, \u03b72, \u03b73) \u0338= (0d, 0, 0) and |\u03b3| =\u0338 0d, while for (\u03b71, \u03b72, \u03b73) = (0d, 0) we set\nSni,0d,0 = \u2212T n i,0d := \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i).\nAs a consequence, it follows that\nHn = K\u2211 j=1 1+1{|Cj |>1}\u2211 \u03b73=0 2(1+1{|Cj |>1})\u2212\u03b73\u2211 |\u03b71|+\u03b72=0 Snj,\u03b71,\u03b72,\u03b73 \u00b7X \u03b71 exp((\u03b2\u22171j) \u22a4X) \u00b7 \u2202 \u03b72f \u2202h\u03b721 (Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j )\n+ K\u2211 j=1 1+1{|Cj |>1}\u2211 |\u03b3|=0 Tnj,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) +R5(X,Y ) +R6(X,Y ). (46)\nStep 2 - Non-vanishing coefficients:\nIn this step, we will prove by contradiction that at least one among the ratios Snj,\u03b71,\u03b72,\u03b73/D3(Gn, G\u2217) does not converge to zero as n \u2192 \u221e. Assume that all these terms go to zero, then by employing arguments for deriving equations (43) and (44), we get that\n1 D3(Gn, G\u2217) \u00b7 [ K\u2211 j=1 \u2223\u2223\u2223 \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j) \u2223\u2223\u2223\n+ \u2211\nj:|Cj |=1 \u2211 i\u2208Cj exp(\u03b2n0i) ( \u2225\u2206\u03b2n1ij\u2225+ \u2225\u2206anij\u2225+ |\u2206bnij |+ |\u2206\u03c3nij | )] \u2192 0.\nNext, let ej := (0, . . . , 0, 1\ufe38\ufe37\ufe37\ufe38 j\u2212th , 0, . . . , 0) for any j \u2208 [d]. Then, we have\n1\nD3(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i)\u2225\u2206\u03b2n1ij\u22252 = K\u2211 i=1 \u2211 \u03b71\u2208{2e1,...,2ed} |Unj,\u03b71,0,0| D3(Gn, G\u2217) \u2192 0.\nSimilarly, we also get that\n1\nD3(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i)\u2225\u2206bnij\u22252 \u2192 0, 1 D3(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i)\u2225\u2206\u03c3nij\u22252 \u2192 0\nMoreover, note that\n1\nD3(Gn, G\u2217) \u00b7 K\u2211 i=1 exp(\u03b2n0i)\u2225\u2206anij\u22252 = K\u2211 i=1 \u2211 \u03b71\u2208{2e1,...,2ed} |Unj,\u03b71,2,0| D3(Gn, G\u2217) \u2192 0.\nGathering all the above limits, we obtain that 1 = D3(Gn, G\u2217)/D3(Gn, G\u2217) \u2192 0 as n \u2192 \u221e, which is a contradiction. Thus, at least one among the terms Snj,\u03b71,\u03b72,\u03b73/D3(Gn, G\u2217) does not converge to zero as n \u2192 \u221e Step 3 - Fatou\u2019s contradiction:"
        },
        {
            "heading": "It follows from the hypothesis that EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D3(Gn, G\u2217) \u2192 0 as n \u2192 \u221e.",
            "text": "Then, by applying the Fatou\u2019s lemma, we get\n0 = lim n\u2192\u221e EX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))] D3(Gn, G\u2217) = 1 2 \u00b7 \u222b lim inf n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| D3(Gn, G\u2217) dXdY,\nwhich implies that |gGn(Y |X)\u2212gG\u2217(Y |X)|/D3(Gn, G\u2217) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ). Next, we define mn as the maximum of the absolute values of Snj,\u03b71,\u03b72/D3(Gn,G\u2217). It follows from Step 2 that 1/mn \u0338\u2192 \u221e. Moreover, by arguing in the same way as in Step 3 in Appendix E.1, we receive that\nHn/[mnD3(Gn, G\u2217)] \u2192 0 (47)\nas n \u2192 \u221e. By abuse of notations, let us denote\nSnj,\u03b71,\u03b72,\u03b73/[mnD3(Gn, G\u2217)] \u2192 \u03c4j,\u03b71,\u03b72,\u03b73 Here, at least one among \u03c4j,\u03b71,\u03b72,\u03b73 is non-zero. Then, by putting the results in equations (46) and (47) together, we get\nK\u2211 j=1 1+1{|Cj |>1}\u2211 \u03b73=0 2(1+1{|Cj |>1})\u2212\u03b73\u2211 |\u03b71|+\u03b72=0 \u03c4j,\u03b71,\u03b72,\u03b73 \u00b7X\u03b71 exp((\u03b2\u22171j)\u22a4X) \u2202\u03b72+\u03b73f \u2202h\u03b721 \u2202\u03c3 \u03b73 (Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j )\n+ K\u2211 j=1 1+1{|Cj |>1}\u2211 |\u03b3|=0 \u2212\u03c4nj,\u03b3,0,0 \u00b7X\u03b3 exp((\u03b2\u22171j)\u22a4X)gG\u2217(Y |X) = 0,\nfor almost surely (X,Y ). Arguing in a similar fashion as in Step 3 of Appendix E.1, we obtain that \u03c4j,\u03b71,\u03b72,\u03b73 = 0 for any j \u2208 [K], 0 \u2264 |\u03b71|+ \u03b72 + \u03b73 \u2264 2(1+1{|Cj |>1}) and 0 \u2264 |\u03b3| \u2264 1+1{|Cj |>1}. This contradicts the fact that at least one among them is non-zero. Hence, the proof is completed."
        }
    ],
    "year": 2024
}