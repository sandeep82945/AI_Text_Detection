{
    "abstractText": "Optimal Control of dynamic systems involving hybrid actions is a challenging task in robotics. To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor approximation technique called Tensor Train (TT) to approximate the state-value and advantage function which enables us to efficiently handle hybrid action space. We demonstrate the superiority of our approach over previous baselines for some benchmark problems with hybrid action spaces. Additionally, the robustness and generalization of the policy for hybrid systems are showcased through a real-world robotics experiment involving a non-prehensile manipulation task. Robotic systems often exhibit complex nonlinear dynamics that may involve hybrid actions. The need for real-time control, high precision, and adequate robustness to cope with disturbances or changes in the environment can result in demanding computational requirements that are challenging to meet with classical control methods. Optimal Control (OC) based on the principles of Dynamic Programming (DP) is a popular tool in robotics but they are still limited to systems with continuous actions and differentiable dynamics. Approximate DP (ADP) and Reinforcement Learning (RL) overcome the curse of dimensionality faced by classical DP algorithms by using function approximation techniques (Sutton & Barto, 2005; Bertsekas, 2012). OC is synonymous with ADP and uses the system\u2019s model to obtain an optimal policy, while RL focuses on learning a policy through trial-and-error interactions with the environment. Both methods aim to find a compact representation of the value functions to obtain a control policy. ADP faces difficulty in approximating the value function throughout the entire state space, conversely, RL restricts its approximation to a smaller region where data is collected, resulting in limited generalizability but greater scalability. However, the existing approaches for both ADP and RL face challenges in handling hybrid action space. Furthermore, existing ADP approaches also find it challenging to cope with large action spaces and hybrid states. In this paper, we present a novel ADP algorithm, called Generalized Policy Iteration using Tensor Train (TTPI) which overcomes the challenges faced by existing ADP methods for hybrid system control. TTPI is an approximate version of the Generalized Policy Iteration (GPI) algorithm\u2014a DP algorithm that encompasses both Value Iteration (VI) and Policy Iteration (PI) algorithms. We use Tensor Train (TT) (Oseledets, 2011), a low-rank tensor approximation technique (Grasedyck et al., 2013), to model the state-value and the advantage function. TT is a versatile function approximator that allows us to simultaneously handle continuous and discrete state and action variables. It approximates a given function as a sum of products of univariate functions, allowing for fast algebraic operations and interpretation. The use of TT-Cross (Oseledets & Tyrtyshnikov, 2010; Savostyanov & Oseledets, 2011), a powerful gradient-free method to approximate functions in TT format in a nonparametric manner, allows us to achieve TT approximation of statevalue and advantage function with a desired accuracy in a fast manner, thus exploiting the knowledge of the system model and the reward function. Moreover, the TT representation of the advantage function enables us to use optimization techniques such as TTGO (Shetty et al., 2022) to retrieve policies for hybrid action spaces. The TT representation is particularly effective when the function being approximated is smooth, resulting in a low-rank representation in the TT format. Our experiments demonstrate that such property is frequently observed in ADP while dealing with hybrid systems. Indeed, even though the",
    "authors": [],
    "id": "SP:0bbc49a8fef2df7d7b75fcb579b26c52895724d2",
    "references": [
        {
            "authors": [
                "Rajeev Alur",
                "Thao Dang",
                "Franjo"
            ],
            "title": "Ivan\u010di\u0107. Predicate abstraction for reachability analysis of hybrid systems",
            "venue": "ACM transactions on embedded computing systems (TECS),",
            "year": 2006
        },
        {
            "authors": [
                "Andr\u00e1s Antos",
                "Csaba Szepesv\u00e1ri",
                "R\u00e9mi Munos"
            ],
            "title": "Fitted q-iteration in continuous action-space mdps",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Dimitri Bertsekas"
            ],
            "title": "Dynamic programming and optimal control: Volume I, volume 1",
            "venue": "Athena scientific,",
            "year": 2012
        },
        {
            "authors": [
                "Alexey I. Boyko",
                "I. Oseledets",
                "Gonzalo Ferrer"
            ],
            "title": "Tt-qi: Faster value iteration in tensor train format for stochastic optimal control",
            "venue": "Computational Mathematics and Mathematical Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Andrei Chertkov",
                "Gleb Ryzhakov",
                "Georgii Novikov",
                "Ivan Oseledets"
            ],
            "title": "Optimization of functions given in the tensor train format",
            "venue": "arXiv preprint arXiv:2209.14808,",
            "year": 2022
        },
        {
            "authors": [
                "Andrzej Cichocki",
                "Namgil Lee",
                "Ivan V. Oseledets",
                "Anh Huy Phan",
                "Qibin Zhao",
                "Danilo P. Mandic"
            ],
            "title": "Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions",
            "venue": "Foundations and Trends in Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Sergey Dolgov",
                "Dmitry Savostyanov"
            ],
            "title": "Parallel cross interpolation for high-precision calculation of high-dimensional integrals",
            "venue": "Computer Physics Communications,",
            "year": 2020
        },
        {
            "authors": [
                "Neel Doshi",
                "Francois R Hogan",
                "Alberto Rodriguez"
            ],
            "title": "Hybrid differential dynamic programming for planar manipulation primitives",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Zhou Fan",
                "Ruilong Su",
                "Weinan Zhang",
                "Yong Yu"
            ],
            "title": "Hybrid actor-critic reinforcement learning in parameterized action space",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Juan Del Aguila Ferrandis",
                "Joao Pousa De Moura",
                "Sethu Vijayakumar"
            ],
            "title": "Nonprehensile planar manipulation through reinforcement learning with multimodal categorical exploration",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Haotian Fu",
                "Hongyao Tang",
                "Jianye Hao",
                "Zihan Lei",
                "Yingfeng Chen",
                "Changjie Fan"
            ],
            "title": "Deep multiagent reinforcement learning with discrete-continuous hybrid action spaces",
            "venue": "In Proceedings of the 28th International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Sergei A. Goreinov",
                "I. Oseledets",
                "Dmitry V. Savostyanov",
                "Eugene E. Tyrtyshnikov",
                "Nickolai Zamarashkin"
            ],
            "title": "How to find a good submatrix. In Matrix Methods: Theory, Algorithms And Applications: Dedicated to the Memory of Gene",
            "venue": "World Scientific,",
            "year": 2010
        },
        {
            "authors": [
                "Alex Gorodetsky",
                "Sertac Karaman",
                "Youssef Marzouk"
            ],
            "title": "Efficient high-dimensional stochastic optimal motion control using tensor-train decomposition",
            "venue": "In Proc. Robotics: Science and Systems (R:SS),",
            "year": 2015
        },
        {
            "authors": [
                "Lars Grasedyck",
                "Daniel Kressner",
                "Christine Tobler"
            ],
            "title": "A literature survey of low-rank tensor approximation techniques",
            "venue": "GAMM-Mitteilungen,",
            "year": 2013
        },
        {
            "authors": [
                "Zhao-Yu Han",
                "Jun Wang",
                "Heng Fan",
                "Lei Wang",
                "Pan Zhang"
            ],
            "title": "Unsupervised generative modeling using matrix product states",
            "venue": "Phys. Rev. X,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew J. Hausknecht",
                "Peter Stone"
            ],
            "title": "Deep reinforcement learning in parameterized action space",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Francois Robert Hogan",
                "Alberto Rodriguez"
            ],
            "title": "Feedback control of the pusher-slider system: A story of hybrid and underactuated contact dynamics",
            "venue": "In Algorithmic Foundations of Robotics XII: Proceedings of the Twelfth Workshop on the Algorithmic Foundations of Robotics,",
            "year": 2020
        },
        {
            "authors": [
                "Matanya B. Horowitz",
                "Anil Damle",
                "Joel W. Burdick"
            ],
            "title": "Linear Hamilton Jacobi Bellman equations in high dimensions",
            "venue": "IEEE Conference on Decision and Control (CDC),",
            "year": 2014
        },
        {
            "authors": [
                "T. Kolda",
                "B. Bader"
            ],
            "title": "Tensor decompositions and applications",
            "venue": "SIAM Review,",
            "year": 2009
        },
        {
            "authors": [
                "Namgil Lee",
                "Andrzej Cichocki"
            ],
            "title": "Fundamental tensor operations for large-scale data analysis using tensor network formats",
            "venue": "Multidimensional Systems and Signal Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Boyan Li",
                "Hongyao Tang",
                "YAN ZHENG",
                "Jianye HAO",
                "Pengyi Li",
                "Zhen Wang",
                "Zhaopeng Meng",
                "LI Wang"
            ],
            "title": "HyAR: Addressing discrete-continuous action reinforcement learning via hybrid action representation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Lutter",
                "Boris Belousov",
                "Shie Mannor",
                "Dieter Fox",
                "Animesh Garg",
                "Jan Peters"
            ],
            "title": "Continuoustime fitted value iteration for robust policies",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Tobia Marcucci",
                "Russ Tedrake"
            ],
            "title": "Warm start of mixed-integer programs for model predictive control of hybrid systems",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2020
        },
        {
            "authors": [
                "Georgii Sergeevich Novikov",
                "Maxim Panov",
                "I. Oseledets"
            ],
            "title": "Tensor-train density estimation",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Ivan Oseledets",
                "Eugene Tyrtyshnikov"
            ],
            "title": "TT-cross approximation for multidimensional arrays",
            "venue": "Linear Algebra and its Applications,",
            "year": 2010
        },
        {
            "authors": [
                "Ivan V. Oseledets"
            ],
            "title": "Tensor-train decomposition",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2011
        },
        {
            "authors": [
                "Dmitry V. Savostyanov",
                "Ivan V. Oseledets"
            ],
            "title": "Fast adaptive interpolation of multi-dimensional arrays in tensor train format",
            "venue": "The 2011 International Workshop on Multidimensional (nD) Systems,",
            "year": 2011
        },
        {
            "authors": [
                "S. Shetty",
                "T. Lembono",
                "T. L\u00f6w",
                "S. Calinon"
            ],
            "title": "Tensor trains for global optimization problems in robotics",
            "year": 2022
        },
        {
            "authors": [
                "Nikos D. Sidiropoulos",
                "Lieven De Lathauwer",
                "Xiao Fu",
                "Kejun Huang",
                "Evangelos E. Papalexakis",
                "Christos Faloutsos"
            ],
            "title": "Tensor decomposition for signal processing and machine learning",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Konstantin Sozykin",
                "Andrei Chertkov",
                "Roman Schutski",
                "A. Phan",
                "Andrzej Cichocki",
                "I. Oseledets"
            ],
            "title": "TTOpt: A maximum volume quantized tensor train-based optimization and its application to reinforcement",
            "venue": "learning. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Edwin Miles Stoudenmire",
                "David J. Schwab"
            ],
            "title": "Supervised learning with tensor networks",
            "venue": "In NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2005
        },
        {
            "authors": [
                "Jiechao Xiong",
                "Qing Wang",
                "Zhuoran Yang",
                "Peng Sun",
                "Lei Han",
                "Yang Zheng",
                "Haobo Fu",
                "Tong Zhang",
                "Ji Liu",
                "Han Liu"
            ],
            "title": "Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space",
            "venue": "arXiv preprint arXiv:1810.06394,",
            "year": 2018
        },
        {
            "authors": [
                "Teng Xue",
                "Hakan Girgin",
                "Teguh Santoso Lembono",
                "Sylvain Calinon"
            ],
            "title": "Demonstration-guided optimal control for long-term non-prehensile planar manipulation",
            "venue": "In Proc. IEEE Intl Conf. on Robotics and Automation (ICRA),",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Robotic systems often exhibit complex nonlinear dynamics that may involve hybrid actions. The need for real-time control, high precision, and adequate robustness to cope with disturbances or changes in the environment can result in demanding computational requirements that are challenging to meet with classical control methods. Optimal Control (OC) based on the principles of Dynamic Programming (DP) is a popular tool in robotics but they are still limited to systems with continuous actions and differentiable dynamics.\nApproximate DP (ADP) and Reinforcement Learning (RL) overcome the curse of dimensionality faced by classical DP algorithms by using function approximation techniques (Sutton & Barto, 2005; Bertsekas, 2012). OC is synonymous with ADP and uses the system\u2019s model to obtain an optimal policy, while RL focuses on learning a policy through trial-and-error interactions with the environment. Both methods aim to find a compact representation of the value functions to obtain a control policy. ADP faces difficulty in approximating the value function throughout the entire state space, conversely, RL restricts its approximation to a smaller region where data is collected, resulting in limited generalizability but greater scalability. However, the existing approaches for both ADP and RL face challenges in handling hybrid action space. Furthermore, existing ADP approaches also find it challenging to cope with large action spaces and hybrid states.\nIn this paper, we present a novel ADP algorithm, called Generalized Policy Iteration using Tensor Train (TTPI) which overcomes the challenges faced by existing ADP methods for hybrid system control. TTPI is an approximate version of the Generalized Policy Iteration (GPI) algorithm\u2014a DP algorithm that encompasses both Value Iteration (VI) and Policy Iteration (PI) algorithms. We use Tensor Train (TT) (Oseledets, 2011), a low-rank tensor approximation technique (Grasedyck et al., 2013), to model the state-value and the advantage function.\nTT is a versatile function approximator that allows us to simultaneously handle continuous and discrete state and action variables. It approximates a given function as a sum of products of univariate functions, allowing for fast algebraic operations and interpretation. The use of TT-Cross (Oseledets & Tyrtyshnikov, 2010; Savostyanov & Oseledets, 2011), a powerful gradient-free method to approximate functions in TT format in a nonparametric manner, allows us to achieve TT approximation of statevalue and advantage function with a desired accuracy in a fast manner, thus exploiting the knowledge of the system model and the reward function. Moreover, the TT representation of the advantage function enables us to use optimization techniques such as TTGO (Shetty et al., 2022) to retrieve policies for hybrid action spaces.\nThe TT representation is particularly effective when the function being approximated is smooth, resulting in a low-rank representation in the TT format. Our experiments demonstrate that such property is frequently observed in ADP while dealing with hybrid systems. Indeed, even though the\nsystem dynamics and reward functions may be non-smooth and discontinuous, the optimal value functions typically exhibit low-rank structures.\nContributions: We introduce TTPI, a novel ADP algorithm for optimal control that leverages TT as a function approximator to address the challenges of hybrid system control in robotics. Our approach is interpretable and eliminates the need for differentiability of the system dynamics and reward function which is a common assumption in the existing ADP algorithms. Our experiments demonstrate that TTPI outperforms state-of-the-art algorithms in terms of both training time and performance on various benchmark control tasks for hybrid control. To showcase the practicality and generalization of our approach, we conducted a real-world robotic experiment where we successfully tackled a non-prehensile planar manipulation task that is notoriously difficult for existing control methods. Our results demonstrate the robustness of the policy and highlight the potential of our approach to addressing complex control problems in robotics."
        },
        {
            "heading": "1 GENERALIZED POLICY ITERATION USING ADVANTAGE FUNCTION",
            "text": ""
        },
        {
            "heading": "1.1 THE OPTIMAL CONTROL PROBLEM",
            "text": "We consider a discrete-time dynamic system with d-dimensional state space and m-dimensional action space. For ease of presentation, we assume the dynamic system to be deterministic, however, our approach can also handle a stochastic model (see Section 2.9).\nWe denote the state at time t by st = (s1t , . . . , s d t ) , and action by at = (a 1 t , . . . , a m t ). The dynamics of the system is given by:\nst+1 = f(st, at),\ns.t. sit \u2208 \u2126si ,\u2200i \u2208 {1, . . . , d}, ajt \u2208 \u2126aj ,\u2200j \u2208 {1, . . . ,m},\n(1)\nwhere the domain of each state \u2126si and action \u2126ai can be a bounded real interval or a discrete set. Let \u2126s denote the state space and \u2126a denote the action-space.\nLet r(s, a) represent the reward function and \u2206t be the time step for the discrete time control. We define R(s, a) = r(s, a)\u2206t. Our goal is to obtain an optimal policy \u03c0\u2217 for the following infinite horizon optimal control problem for any given initial state in the state space s0 \u2208 \u2126s:\n\u03c0\u2217 =argmax \u03c0 \u221e\u2211 t=0 \u03b3tR(st, \u03c0(st)), \u2200s0\nwhere st+1 = f(st, \u03c0(st))\n(2)\nwhere \u03b3 is the discount factor 0 \u2264 \u03b3 < 1. We do not make any assumption on the structure or differentiability of the dynamics f and the reward function r. For example, a black box deterministic simulator that returns the next state and the reward for the state-action pair satisfies our requirement. However, for a fast implementation of our algorithm described in Section 2.8, the simulator should ideally process a batch of state-action pairs for parallel implementation."
        },
        {
            "heading": "1.2 DYNAMIC PROGRAMMING",
            "text": "The state-value function V \u03c0 corresponding to a policy \u03c0, with discount factor \u03b3, is defined as follows:\nV \u03c0(s0) = \u221e\u2211 t=0 \u03b3tR(st, \u03c0(st)), \u2200s0\nwhere st+1 = f(st, \u03c0(st)), \u2200t. (3)\nGiven a state-value function V : \u2126s \u2192 R, a policy \u03c0 and the discount factor \u03b3, the Bellman operator B\u03c0 is a functional that is defined as B\u03c0V (s) = R(s, \u03c0(s)) + \u03b3V (f(s, \u03c0(s))),\u2200s \u2208 \u2126s where B\u03c0 : V \u2192 V. We define the advantage function AV corresponding to the value function V as follows:\nAV (s, a) = R(s, a) + \u03b3(V (f(s, a))\u2212 V (s)), \u2200(s, a) \u2208 \u2126s \u00d7 \u2126a. (4)"
        },
        {
            "heading": "1.3 CHALLENGES IN APPROXIMATE DYNAMIC PROGRAMMING",
            "text": "Algorithm 1 describes the value iteration (VI) algorithm (Sutton & Barto, 2005), a popular DP algorithm. One of the challenges in implementing the VI algorithm and other similar DP algorithms including the Policy Iteration (PI) algorithm (Sutton & Barto, 2005) in practice is the curse of dimensionality in representing the value function when the involved state space is either high-dimensional or includes continuous states. ADP addresses this challenge by using function approximation techniques. In addition, retrieving the policy \u03c0k from the advantage function is difficult if it is nonconvex, if there are bounds on the actions, if the action space is large, or if the action space is hybrid. An inefficient optimization technique for policy retrieval increases the overall time of the algorithm, as it must be repeated for each state in every iteration, and it results in a sub-optimal policy. The lack of such policy retrieval techniques is a bottleneck in the development of ADP algorithms for hybrid control.\nAlgorithm 1 VI Algorithm\nInput: Initial value function V 0, convergence threshold \u03f5 Output: Optimal policy \u03c0\u2217 1: Set k = 0 2: repeat 3: \u03c0k(s) := argmax\na AV k(s, a)\n4: V k+1 = B\u03c0kV k 5: if \u2225V k+1 \u2212 V k\u2225\u221e < \u03f5 then 6: break 7: end if 8: Set k \u2190 k + 1 9: until convergence\n10: V \u2217 = V k 11: \u03c0\u2217(s) = argmax a AV \u2217(s, a)"
        },
        {
            "heading": "2 GENERALIZED POLICY ITERATION USING TENSOR TRAIN (TTPI)",
            "text": "In this section, we briefly describe the proposed approach and the related concepts used to tackle the previously described challenges in ADP for handling hybrid actions and large action spaces. Further details are provided in the Appendix. In summary, we overcome the challenges mentioned in the ADP algorithms using TT as a function approximator. We propose to model the advantage function explicitly in TT format and use TTGO, a technique for optimization of functions in TT format proposed by Shetty et al. (2022); Chertkov et al. (2022), for policy retrieval."
        },
        {
            "heading": "2.1 TENSORS AS DISCRETE ANALOGUE OF A FUNCTION",
            "text": "A multivariate function P (x1, . . . , xd) defined over a rectangular domain made up of the Cartesian product of intervals (or discrete sets) I1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Id can be discretized by evaluating it at points in the set X = {(xi11 , . . . , x id d ) : x ik k \u2208 Ik, ik \u2208 {1, . . . , nk}}. This gives us a tensor P , a discrete version of P , where P(i1,...,id) = P (x i1 1 , . . . , x id d ),\u2200(i1, . . . , id) \u2208 IX , and IX = {(i1, . . . , id) : ik \u2208 {1, . . . , nk}, k \u2208 {1, . . . , d}}. The value of P at any point in the domain can then be approximated by interpolating between the elements of the tensor P ."
        },
        {
            "heading": "2.2 TENSOR DECOMPOSITION",
            "text": "Representing a high-dimensional tensor is difficult because of the limitation in storage. Tensor decomposition techniques (Kolda & Bader, 2009; Sidiropoulos et al., 2017) solve this problem by representing the tensor using a smaller number of lower-dimensional tensors, known as factors, which occupy less memory. These factors are combined with certain algebraic operations, depending on the decomposition method, to represent the elements of the original tensor. In addition to the compact representation, they also enable efficient algebraic operations in the compressed format.\nThe accuracy of a tensor representation is usually controlled by its rank in the decomposition, which is proportional to the number of elements in the factorization. The rank of the tensor is closely related to the separability of the underlying function. In practice, the smoothness of the underlying function often corresponds to a low rank."
        },
        {
            "heading": "2.3 TT DECOMPOSITION",
            "text": "TT decomposition, also known as Matrix Product State (MPS) or Tensor Networks (TN) in physics (Cichocki et al., 2016), is a widely used tensor decomposition technique due to its versatility and effective methods for determining the approximation.\nTT decomposition approximates a given tensor (a multidimensional array) compactly using a set of third-order tensors called cores. A d-th order tensor P \u2208 Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nd in TT format is represented using a tuple of d third-order tensors (P1, . . . ,Pd). The dimension of the cores are given as P1 \u2208 R1\u00d7n1\u00d7r1 ,Pk \u2208 Rrk\u22121\u00d7nk\u00d7rk , k \u2208 {2, . . . , d\u22121}, and Pd \u2208 Rrd\u22121\u00d7nd\u00d71 with r0 = rd = 1. As shown in Figure 1, we can access the element (i1, . . . , id) of the tensor in this format simply given by multiplying matrix slices from the cores:\nP(i1,...,id) = P 1 :,i1,:P 2 :,i2,: \u00b7 \u00b7 \u00b7P d :,id,: , (5) where Pk:,ik,: \u2208 R\nrk\u22121\u00d7rk represents the ik-th frontal slice (a matrix) of the third-order tensor Pk. The dimensions of the cores are such that the above matrix multiplication yields a scalar. The TT-rank of the tensor in TT representation is then defined as the tuple r = (r1, r2, . . . , rd\u22121). We call r = max (r1, . . . , rd\u22121) as the maximal rank. For any given tensor, there always exists a TT decomposition equation 5 (Oseledets, 2011)."
        },
        {
            "heading": "2.4 TT-CROSS",
            "text": "TT-Cross (Oseledets & Tyrtyshnikov, 2010; Savostyanov & Oseledets, 2011) efficiently computes a TT approximation of a tensor with controlled accuracy by evaluating only a small number of its elements, without requiring the entire tensor to be stored in memory. It does this by computing only specific tensor fibers at a time, in a black-box manner. A noteworthy feature of TT-Cross is that it is an unsupervised and nonparametric approach as it directly takes the function being modeled as its input and the number of parameters in its TT representation is adjusted based on the structure of the underlying function until a specified accuracy of the approximation is obtained.\nConsider a tensor P that is a discrete analog of a function P with domain \u2126, using a discretization set \u2126\u0302. Instead of evaluating the entire tensor, TT-Cross selects a subset of elements by evaluating function P at various points in the discretization set of \u2126\u0302. The approximate tensor in TT format, P\u0302 = TT-cross(P, \u2126\u0302, rmax, \u03f5, ), requires only O(ndr2) evaluations, rather than O(nd) evaluations of the full tensor. Here, rmax is the upper-bound on the rank of the TT, \u03f5 is the accuracy of the approximation, and r is the actual rank of the tensor in TT format. TT-Cross is efficient when the TT-rank r is low, which is often the case for state-value functions in many problems involving hybrid states. We can then obtain the continuous approximation to P from P\u0302 using equation 7."
        },
        {
            "heading": "2.5 ALGEBRAIC OPERATIONS IN TT FORMAT",
            "text": "In addition to representing a tensor compactly, the TT format enables the efficient execution of various tensor algebraic operations in its compact form. Operations such as addition, subtraction, and inner product of TT tensors can be performed efficiently (Lee & Cichocki, 2018). The mean, norm, and gradient can also be calculated efficiently. The compactness of the representation can be improved by using TT-rounding operations, with a trade-off in accuracy. The availability of these\nalgebraic tools allows a thorough analysis of the functions represented in TT format, thus making them highly interpretable."
        },
        {
            "heading": "2.6 CONTINUOUS APPROXIMATION USING TT",
            "text": "Given the discrete analogue tensor P of a function P , we obtain the continuous approximation by spline-based interpolation of the TT cores corresponding to the continuous variables only. For example, we can use linear interpolation for the cores (i.e., between the matrix slices of the core) and define a matrix-valued function corresponding to each core k \u2208 {1, . . . , d},\nP k(xk) = xk \u2212 xikk\nxik+1k \u2212 x ik k Pk:,ik+1,: + xik+1k \u2212 xk xik+1k \u2212 x ik k Pk:,ik,:, (6)\nwhere xikk \u2264 xk \u2264 x ik+1 k and P k : Ik \u2282 R \u2192 Rrk\u22121\u00d7rk with r0 = rd = 1. This induces a continuous approximation of P given by P (x1, . . . , xd) \u2248 P 1(x1) \u00b7 \u00b7 \u00b7P d(xd). (7) This allows us to selectively do the interpolation only for the cores corresponding to continuous variables, and hence we can represent functions in TT format whose variables could be a mix of continuous and discrete elements."
        },
        {
            "heading": "2.7 TTGO: OPTIMIZATION OF FUNCTION IN TT-FORMAT",
            "text": "In addition to the availability of algorithms like TT-Cross for finding function approximation and the accompanying algebraic tools, an advantage of using TT decomposition for approximating functions in ADP is its ability to efficiently find optima over a mix of continuous and discrete variables. This was introduced as Tensor Train for Global Optimization (TTGO) in Shetty et al. (2022), in the form of a stochastic method. A deterministic version of this was proposed in Chertkov et al. (2022). In practice, the technique often yields globally optimal solutions, as demonstrated in Shetty et al. (2022) and Chertkov et al. (2022).\nThe basic idea behind TTGO is that it transforms the given function in TT format, using the accompanying necessary algebraic tools, into a nonnegative function in TT format that can be interpreted as a probability density function. The efficient sampling techniques for density functions in TT format allow us to pick samples of only high-density regions which in turn correspond to the optima. In practice, the chosen number of prioritized samples ns \u2265 1 and the sample(s) with the highest density (or least cost) is used to represent the optima.The solution obtained from such a procedure can be refined further using local optimization techniques such as Newton-type optimization for continuous variables. But, in practice, as in this paper, the refinement procedure is often not required.\nIn this paper, we identify and exploit TTGO\u2019s ability to handle a mix of continuous and discrete variables. In addition, we perform optimization in the batch form: we propose to model the advantage function A(s, a) in ADP in TT format, and adapt TTGO to obtain the optimal actions a corresponding to a batch of states s (i.e. parallel computation of argmax\na A(s, a) ) in an efficient manner."
        },
        {
            "heading": "2.8 TTPI ALGORITHM",
            "text": "By combining the conceptual ideas proposed so far, Algorithm 2 presents the TTPI algorithm, which addresses the previously mentioned challenges in ADP using TT as the function approximator for state-value and advantage functions and TTGO for policy retrieval.\nIn the TTPI algorithm, the value update step involves computing \u03c0k(s) (i.e., argmaxa AV k(s, a)) numerous times across several iterations. To compute V kj in TT-format, the function B\u03c0kV kj\u22121 is queried iteratively using TT-Cross(B\u03c0kV kj\u22121, rmax, \u03f5), with batches of states (usually ranging from 10,000 to 100,000 in practice). This requires computing the policy \u03c0k for each of these states in batch form. We use TT-round to compress the value functions in TT format at the end of every policy evaluation (i.e., after updating the value function for the current policy). We use cubic spline-based interpolation for continuous variables which reduces the number of discretization points required by TT-cross to construct the TT model.\nTo resolve the bottleneck in policy retrieval, we propose to compute the advantage function AV k in TT format using TT-Cross. This is efficient as the calculation only requires evaluating V k and R(s, a), which are cheap to compute. This enables the numerical optimization of variables for\nAlgorithm 2 TTPI: Generalized Policy Iteration using Tensor Train Input:\nnv: Number of value update steps \u03f5: Accuracy of TT representation rmax: Maximum TT-rank \u03b4max: Convergence tolerance r(s, a): Reward function \u2206t: Time Discretization f(s, a): Forward simulation \u2126\u0302s: Discretization of state space \u2126\u0302: Discretization of state-action space (\u2126\u0302s \u00d7 \u2126\u0302a) ns: Number of candidate samples for optima used in TTGO.\nInitialize: 1: Initialize V 0 = 0 in TT-format 2: Initialize Advantage model: 3: AV 0 = TT-Cross(R(s, a), \u2126\u0302, rmax, \u03f5) 4: (alternatively, initialize arbitrarily), 5: Set k = 0\nOutput: Policy \u03c0\u2217 1: while \u03b4 \u2264 \u03b4max do 2: k = k + 1 3: \u03c0k(s) := argmax\na AV k\u22121(s, a) (Use TTGO)\n4: V k0 = V k\u22121 5: for j \u2190 1 to nv do 6: V kj (s) = TT-Cross(B\u03c0 k\nV kj\u22121, \u2126\u0302s, rmax, \u03f5) 7: end for 8: V k = TT-round(V knv , \u03f5) 9: Ak(s, a) = R(s, a)+ \u03b3(V k(f(s, a))\u2212V k(s))\n10: AV k = TT-Cross(Ak, \u2126\u0302, rmax, \u03f5) 11: \u03b4 = \u2225V\nk\u2212V k\u22121\u22252 \u2225V k\u22121|2\n12: end while 13: Set V \u2217 = V k 14: \u03c0\u2217(s) = argmax\na AV \u2217(s, a)\nfunctions in TT format using TTGO, as outlined in Section 2.7. As a result, \u03c0k(s) over batches of states can be obtained quickly. Most importantly, this allows us to handle hybrid action space. The computational cost involved in retrieving a solution is O(nsmdr2max) which is linear in the number of discretizations (d) of an action variable and the dimension of action space (m).\nA PyTorch-based GPU-accelerated implementation of these algorithms is provided along with the supplementary material. The computational cost of the algorithm increases linearly with the number of dimensions in both state and action spaces and grows quadratically with the rank of the functions represented in TT format, thanks to the properties of TT-Cross and TT-representation."
        },
        {
            "heading": "2.9 TTPI FOR STOCHASTIC SYSTEMS",
            "text": "In this section, we show how our approach can be extended to consider stochastic system dynamics. Instead of relying on deterministic system dynamics of the form s\u2032 = f(s, a), we consider the transition probability P (s\u2032, s, a) and the reward function R(s, a) in TT format. The transition probability P (s\u2032, s, a) can be obtained by fitting a density model to data collected from the robot. To achieve this, we can employ the TT format for density modeling as suggested by Novikov et al. (2021) and Han et al. (2018). Alternatively, if the function P is available in a different format such as NN, we can utilize TT-Cross. By leveraging the algebraic tools provided in TT format, we can normalize P such that \u2211 s\u2032 P (s\n\u2032, s, a) = 1 (or integrate if s\u2032 is continuous). The following outlines the procedure to update the value function and policy under this approach:\nV k =TT-Cross(Uk, \u2126\u0302s, rmax, \u03f5),\nUk(s) =R(s, \u03c0k(s)) + \u03b3W k(s, \u03c0k(s)), W k(s, a) = \u2211 s\u2032 P (s\u2032, s, a)V k(s),\nAV k(s, a) =R(s, a) + \u03b3(W k(s, a)\u2212 V k(s)),\n\u03c0k(s) = argmax a AV k(s, a).\n(8)\nIn the above algorithm, as P and V k are both in TT format, we can obtain W k efficiently by using algebraic operation over TT format (namely, element-wise product and contraction operations over s\u2032). Then AV k can be readily computed in TT-format using addition operations over the TT tensors as R, W k, and V k are also in TT format. We only need TT-cross to find V k. Hence the algorithm would be very efficient if P is known in TT format. However, we acknowledge that, in practice, obtaining a stochastic model of a system P from data is a challenging problem and it is still an ongoing area of research."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "In our experiments, we utilized an NVIDIA GeForce RTX 3090 GPU with 24GB of memory. For the applications considered, we discretized each continuous variable with 100 points using uniform discretization. To approximate the value and advantage functions in TT format using TT-Cross, an accuracy of \u03f5 = 10\u22123 proved sufficient. We set rmax to a large value of 100. The discount factor was chosen in the range of 0.99 to 0.9999, depending on the time step \u2206t which ranged from 0.01 to 0.001. The rank of the value function in the applications considered ranged between 5 to 50, and the rank of the advantage function was roughly twice that of the value function.\nFigure 2: The tasks considered in this study involve controlling an agent to reach a target point in a 2D space. In the first task, called \u201cCatchPoint\u201d, the agent has control over its heading direction (continuous) and the option to either stop or move toward the target (binary variable). In the second task, known as \u201cHard-Move\u201d, the agent is equipped with n actuators, and it can decide to activate or deactivate each actuator (n binary actions) and specify acceleration along each actuator (n continuous variables)."
        },
        {
            "heading": "3.1 SIMULATION EXPERIMENTS:",
            "text": "d m HPPO PDQN HyAR TTPI T \u00b5 S T \u00b5 S T \u00b5 S T \u00b5 S\nCP 4 2 0.5h 0.13\u00b10.01 86% \u00b16% 1.9h 0.16 \u00b10.05 84% \u00b16% 4h 0.15 \u00b10.02 92% \u00b14% 30s 1 100%\nHM(8) 4 16 1.4h 0.15\u00b10.01 8% \u00b12% 2.1h 0.19 \u00b10.03 8% \u00b13% 8h 0.92 \u00b10.01 100% 850s 0.93 \u00b10.01 100% HM(12) 4 24 NA NA NA NA NA NA 10h 0.92\u00b10.05 12% \u00b15% 946s 0.92 \u00b10.01 100% HM(16) 4 32 NA NA NA NA NA NA 10h NA 0% 1743s 0.92\u00b10.02 100%\nTable 1: We used the success rate (S) for reaching the target position as one of the metrics. We note that the primary objective of both approaches, in the problems considered here, is to reach the goal in the shortest possible time or path. So as a second metric (\u00b5), we calculate the square of the ratio between the length of the trajectory generated by each policy and the length of the shortest path for HM task. For CP task, \u00b5 is the inverse of the number of catch motions till reaching the goal. The table includes the training time (T ) required to obtain the policy used for evaluation. The number of states is d and the number of actions is m.\nBaseline: To the best of our knowledge, there are no established approaches for OC based on ADP algorithms that can handle hybrid actions. To evaluate our algorithm performance, we compared it against Deep RL techniques for hybrid action spaces such as HyAR, HPPO and PDQN (Li et al., 2022; Fan et al., 2019; Xiong et al., 2018). The HyAR algorithm has shown superiority over other Deep RL techniques for high-dimensional hybrid action spaces. It is important to acknowledge that TTPI assumes access to the system dynamics and the reward function, whereas Deep RL techniques, in theory, are agnostic to the system model and implicitly address a more challenging problem than TTPI. However, many of these methods are data-inefficient and, like TTPI, assume access to a simulator.\nEvaluation: We evaluated our algorithm on two benchmark problems involving systems with hybrid action spaces: the Catch-Point (CP) Problem and the Hard-Move (HM) problem, as proposed by Li et al. (2022). The Catch-Point Problem has four states and an action space with one discrete and one continuous action. The Hard-Move problem has n actuators, resulting in a total of 2n action variables, with n binary and n continuous variables. Thus, this problem allows testing the scalability for high-dimensional action spaces by increasing n.\nThe results, as presented in the table, provide strong evidence of TTPI\u2019s superior performance compared to the baseline method. TTPI demonstrates faster training times and generates highly performant policies. In contrast, the baseline method struggles with generalization and produces lower-quality solutions, particularly for the Hard-Move problem with a number of actuators m > 12. This is attributed to TT-Cross accurately modeling the value functions by leveraging the system model and reward function, in a fast manner and efficient policy retrieval using TTGO."
        },
        {
            "heading": "3.2 REAL ROBOT EXPERIMENTS",
            "text": "We demonstrate the effectiveness of our proposed method for hybrid system control on a planar pushing task with a face-switching mechanism (Xue et al., 2023) and involves discrete states and actions. The objective is to push a block with freedom in switching both the contact modes and faces. It is modeled using 6 states and 4 actions. The action includes a discrete variable representing the index of next contact face. Its underactuated and hybrid nature, coupled with multiple discrete contact modes, makes it difficult to design effective control strategies, and it has been a test-bed problem for the control of hybrid systems. Previous approaches, such as mixed integer programming (Hogan & Rodriguez, 2020) and hybrid Differential Dynamic Programming (Doshi et al., 2020), have struggled with the high computational cost required for solving the problem, which requires robust algorithms that can handle the complexity of hybrid systems with both continuous and discrete variables. Note that typically such a non-prehensile manipulation problem is formulated differently as continuous control (Ferrandis et al., 2023) due to a lack of methodologies to handle hybrid actions and is not representative of hybrid control in robotics applications.\nOur algorithm achieves robust performance 100% success rate (reaching the goal) in both simulation and real-world experiments for this task. The experiments demonstrate successful reaching of the target position and orientation, even in the presence of additional weight and external disturbances, as shown in Fig. 3. This indicates the potential of TTPI for solving complex hybrid system control."
        },
        {
            "heading": "4 LIMITATION",
            "text": "TTPI approximates the state-value and advantage function over the entire state-action space, resulting in a highly generalizable policy. However, computational complexity and storage issues may arise when these functions are not low-rank in the TT representation. For instance, systems involving highly agile movements like the acrobat (double pendulum swing-up) can lead to high-rank in the TT representation. Nonetheless, decreasing the time step \u2206t has been observed to reduce the rank of these functions which may enable the approach to handle such systems at the expense of longer training time.\nTTPI may be well-suited for commonly encountered systems with discontinuities and hybrid characteristics, such as manipulation and legged robotics. However, a drawback is its reliance on highly parallelized simulators. Hand-coding the system\u2019s dynamics and reward function, as demonstrated in this paper, may not be practical for more complex dynamics involving contact. While existing simu-\nlators like Mujoco and Raisim are not parallelizable and may slow down the process, the availability of recently introduced GPU-based simulators like NVIDIA Isaac Gym presents an opportunity to test the algorithm on more intricate applications.\nConcerning scalability, although existing Deep RL techniques struggle to handle hybrid action space, they can cope well with high-dimensional state space (e.g., images as states). On the other hand, TTPI can handle high-dimensional hybrid actions and perform better compared to existing ADP methods, it may not be suitable for very high-dimensional state spaces. However, we could potentially enable our method to handle such high-dimensional problems by formulating our approach as an RL problem instead of ADP or OC. In such cases, instead of TT-Cross, gradient-based methodologies (Novikov et al., 2017; Stoudenmire & Schwab, 2016) could be used to find the TT model of the value and advantage functions. We will investigate this in our future work."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "In recent years, research has surged in the domain of optimal control for hybrid systems which involve a mix of discrete and continuous state and action variables. Classical techniques, like Mixed-Integer Programming (MIP) (Marcucci & Tedrake, 2020), unify continuous and discrete variables in a single optimization problem. Abstraction and reachability analysis methods (Alur et al., 2006) help adapt hybrid systems for traditional solvers. However, they often involve high computational complexity and are not suitable for real-time decision-making. This motivates the development of Approximate Dynamic Programming (ADP) techniques, which involve approximating value functions to alleviate computational burdens and handle high-dimensional settings.\nThe use of low-rank tensor approximation techniques for solving ADP was previously proposed in Horowitz et al. (2014), Gorodetsky et al. (2015), and Boyko et al. (2021). In Gorodetsky et al. (2015) and Boyko et al. (2021), they proposed a TT-based value iteration algorithm, where the TT was used to approximate the value function, and the policy was retrieved using Newton-type optimization technique based on the value function. This limits the application and speed of the algorithm, as the policy retrieval procedure demands the system dynamics and the reward function to be differentiable and the action space to be continuous.\nSome of the NN-based ADP for continuous state and action space have been proposed in fitted-Q iteration (Antos et al., 2007) and fitted-value iteration (Lutter et al., 2022). However, these methods are limited to small dimensional state space and action space. Moreover, they have not been successful in handling hybrid action space. The NN-based ADP methods have been overshadowed by the rise of Deep RL as they have demonstrated scalability to problems with high dimensional state and action space. To overcome the issues in Deep RL for handling hybrid actions several improvements were proposed by Hausknecht & Stone (2016); Fu et al. (2019); Fan et al. (2019) and Li et al. (2022)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we presented TTPI, an ADP algorithm that can handle hybrid action space. Through simulation experiments, we showed that the algorithm is superior to state-of-the-art algorithms for dealing with hybrid action spaces in terms of training time, generalization, and the quality of the policy. We demonstrated the robustness of the policy of TTPI through real-world experiments. The results demonstrate that our approach could be promising in robotics for solving challenging hybrid control tasks."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 TT VS NN\nIn this section, we provide a comprehensive analysis of the performance of Tensor Train (TT) compared to Neural Networks (NN) for function approximation. We employed a Gaussian Mixture Model (GMM) as a representative target function, varying the number of mixture components (2 to 20) and dimensionality (2 to 40). The results in Figure 4 demonstrate that TT-Cross could accurately represent the target function in TT format, achieving orders of magnitude better accuracy compared to NN, and requiring significantly less time. Additionally, the nonparametric and unsupervised nature of TT-Cross offers flexibility with minimal intervention, eliminating the need for careful selection, unlike the case of NN which requires careful hyperparameter tuning such as NN architecture, learning rate, and batch size. The superiority of TT-Cross is due to the fact that it directly takes the function to be modeled as the input and exploits the low-rank structure for the approximation. This motivates the use of TT for modeling value functions and advantage functions in Approximate Dynamic Programming (ADP).\nIn our experiment, TT-Cross was able to find the TT representation of the GMM with less than 10\u22126 error as specified in TT-Cross in under 20 seconds for each test case, while NN took several minutes and had a significantly higher error (often several orders of magnitude higher). Furthermore, NN required significant effort to tune the hyperparameters, whereas TT-Cross, as it is a non-parametric and unsupervised approach, was much easier to use. This is because TT-Cross finds the approximation by querying data (the function values at various points) intelligently (Savostyanov & Oseledets, 2011) and exploits the structure in the function (i.e., low-rank or separability). It can do so as TT-Cross directly takes the function to be approximated as the input. On the other hand, NN takes a fixed set of samples from the function and does supervised learning to find the function approximation. We acknowledge that the approximation error in NN in our experiments could potentially be reduced by using more training data, and using a more exhaustive search for best hyperparameters. However, this would increase the training time and manual effort.\nAlthough NN is an established tool for supervised learning over datasets, it is inefficient, compared to TT-Cross, when we need to approximate a known low-rank function accurately. Unlike TT-Cross, NN works with data collected from the function for the approximation and does not have a feedback mechanism to query points from the function during the approximation procedure. Thus, choosing NN as a function approximation technique in ADP, where we need to repeatedly approximate value functions from the previous estimations, comes with a drawback. The software code for this comparison is provided in the supplementary material.\nA.2 ADDITIONAL SIMULATION EXPERIMENTS In addition to the benchmark problems on hybrid actions provided in the main section, we performed further experiments to evaluate the performance of our approach on some benchmark optimal control problems involving continuous states including Point-mass control with obstacles, Cart-Pole Swingup, and Box-pivoting. The video associated with the paper shows the performance of the policy obtained by TTPI on these tasks.\nA.3 EXPERIMENTS WITH REAL ROBOT: PLANAR PUSHING TASK In this section, we discuss the performance of our proposed method for a planar pushing task (?). It is considered to be challenging in the field of model-based planning and control, due to its underactuated and hybrid nature with several discrete contact modes.\nThe objective of the task is to push a block with the option of switching the face of the block to be pushed, as well as the contact mode used for pushing. We demonstrate that the proposed algorithms can achieve this task robustly in both simulation and the real world. A video of the experiments is provided in the supplementary material.\nThe state of the system is denoted as [q\u22a4s q \u22a4 p cc] \u22a4, where qs = [sx sy s\u03b8]\u22a4 is the position and orientation of the block, qp = [px py]\u22a4 is the position of end-effector, and cc \u2208 {0, 1, 2, 3} is the current contact face. The action is expressed as [v\u22a4 cn]\u22a4, where v = [vn vt]\u22a4 is the velocity of the end-effector, and cn \u2208 {0, 1, 2, 3} is the next contact face. The system, therefore, has n = 6 states and m = 3 control variables in total, including both continuous and discrete variables.\nWe first trained the control policy in simulation based on the predefined motion equation. The continuous variables in state and action spaces are discretized into 100 bins. The domain is set in the range from [\u22120.5m,\u22120.5m,\u2212\u03c0] to [0.5m, 0.5m,\u03c0], with maximum velocity defined as 0.1 m/s. The accuracy of TT-cross is defined as 10\u22123. The rank of the final value function was found to be 4 and the rank of the advantage function was 40. Each iteration of the VI procedure took about 10 seconds on average. To test the generalization capability of the policy, we randomly selected 1000 initialization points in the domain. A success rate of 100% was obtained in under 10 minutes of training. Fig. 5 shows the simulation results. The reward function is defined as:\nR(s, a) = \u22122\u2225qs\u2225 \u2212 (1\u2212 \u03b4(cc \u2212 cn)), (9)\nwhere qs represent the block pose, \u03b4(cc \u2212 cn) will return 1 if cc = cn (no face switching), otherwise, 0. Note that the flexibility offered by our method allows us to utilize such reward functions.\nWe then tested the trained policy on the real robot setup (Fig. 3), using a 7-axis Franka Emika robot and a RealSense D435 camera. The slider (rs = 6 cm) is a 3D-printed prismatic object with PLA, lying on a flat plywood surface, with an Aruco Marker on the top face. A wooden pusher (rp = 0.5cm) is attached to the robot to move the object. The motion of the object is tracked by the camera at 30 HZ, and the policy is updated at 100 HZ, with a low-level Cartesian velocity controller (1000 HZ) actuating the robot.\nThree experiments were conducted to assess the robustness of our policy: a) Reaching task: The robot pushes the slider from qs0 = [0.05m 0.16m 0]\n\u22a4 to the origin (Fig. 3a); b) Reaching with additional weight: The robot pushes the block from the same initialization as before, but with an additional weight, 3 times heavier than the block (Fig. 3b); c) Reaching with external disturbance: The same initialization like before, but with a significant external disturbance of qdist = [0.1m 0.03m 90\u25e6]\u22a4 exerted by a human (Fig. 3c).\nThe results of these experiments are shown in Table 2. The results show that in all experiments, the policy successfully reaches the final target in terms of both position and orientation. The error increases with the disturbance, while orientation errors remain less than 4\u25e6 and position errors remain less than 5cm even under significant disturbance. Experiment 3 demonstrates that the policy is able to dynamically select the contact face based on the current state, as evidenced by the change in contact face after a 90\u25e6 rotation. This highlights the ability of our method to handle both continuous and discrete variables in hybrid systems.\nA.3.1 CROSS APPROXIMATION METHODS\nThe popular methods to find the TT decomposition of a tensor are TT-SVD (Oseledets, 2011), TT-DMRG (Dolgov & Savostyanov, 2020), and TT-cross (Savostyanov & Oseledets, 2011). TT-SVD and TT-DMRG, like matrix SVD, require the full tensor in memory to find the decomposition, and hence they are infeasible for higher-order tensors. TT-cross approximation (TT-cross) is an extension of the matrix cross approximation technique for obtaining the TT decomposition of a tensor. It is appealing for many practical problems as it approximates the given tensor with a controlled accuracy, by evaluating only a small number of its elements and without having to compute and store the entire tensor in the memory. The method needs to compute only certain fibers of the original tensor at a time and hence works in a black-box fashion. In this section, we describe the matrix cross-approximation algorithm to provide an intuition about TT-Cross and we refer the readers to (Sozykin et al., 2022; Oseledets & Tyrtyshnikov, 2010; Savostyanov & Oseledets, 2011) for more detail on how it can be adapted to find the TT decomposition of higher-dimensional tensors using TT-cross.\nSuppose we have a rank-r matrix P \u2208 Rn1\u00d7n2 . Using cross-approximation (a.k.a. CUR decomposition or skeleton decomposition), this matrix can be exactly recovered using r independent rows (given by the index vector i1 \u2282 {1, . . . , n1}) and r independent columns (given by the index vector i2 \u2282 {1, . . . , n2}) of the matrix P as\nP\u0302 = P:,i2 P \u22121 i1,i2\nPi1,:, provided the intersection matrix Pi1,i2 (called submatrix) is non-singular. Thus, the matrix P , which has n1n2 elements, can be reconstructed using only (n1 + n2 \u2212 r)r of its elements (see Figure 6).\nNow suppose we have a noisy version of the matrix P = P\u0303 + E with \u2225E\u2225 < \u03f5 and P\u0303 is of low rank. For a sufficiently small \u03f5, rank(P\u0303 ) = r so that the matrix P can be approximated with a lower rank r (i.e., rank(P ) \u2248 r). Then, the choice of the submatrix Pi1,i2 (or index vectors i1, i2) for the cross approximation requires several considerations. The maximum volume principle can be used in choosing the submatrix which states that the submatrix with maximum absolute value of the determinant is the optimal choice. If Pi\u22171 ,i\u22172 is chosen to have the maximum volume, then by skeleton decomposition we have an approximation of the matrix P given by P\u0302 = P:,i\u22172P \u22121 i\u22171 ,i \u2217 2 Pi\u22171 ,:. This results in a quasi-optimal approximation: \u2225P \u2212 P\u0302 \u22252 < (r + 1)2 \u03c3r+1(P ), where \u03c3r+1(P ) is the (r + 1)-th singular value of P (i.e., the approximation error in the best rank r approximation in the spectral norm). Thus, we have an upper bound on the error incurred in the approximation which is slightly higher than the best rank r approximation (Eckart\u2013Young\u2013Mirsky theorem).\nFinding the maximum volume submatrix is, however, an NP-hard problem. However, many heuristic algorithms that work well exist in practice by using a submatrix with a sufficiently large volume, trading off the approximation accuracy for the computation speed. One of the widely used methods is the MAXVOL algorithm (Goreinov et al., 2010) which can provide, given a tall matrix P \u2208 Rr\u00d7n2 (or Rn1\u00d7r), the maximum volume submatrix Pi\u22171 ,i\u22172 \u2208 R\nr\u00d7r. The cross approximation algorithm uses the MAXVOL algorithm in an iterative fashion to find the skeleton decomposition as follows:\n1. Input: P \u2208 Rn1\u00d7n2 , the approximation rank r for the skeleton decomposition. 2. Find the columns index set i\u22172 and the row index set i \u2217 1 corresponding to the maximum\nvolume submatrix.\n2.1 Randomly choose r columns i2 of the matrix P and repeat the following until convergence:\n\u2022 Use MAXVOL to find r row indices i1 so that Pi1,i2 is the submatrix with maximum volume in P:,i2 . \u2022 Use MAXVOL to find r column indices i2 so that Pi1,i2 is the submatrix with maximum volume in Pi1,:.\n3. Output: Using the column index set i\u22172 and the row-index set i \u2217 1 corresponding to the\nmaximum volume submatrix, we have the skeleton decomposition P\u0302 \u2248 P:,i\u22172P \u22121 i\u22171 ,i \u2217 2 Pi\u22171 ,:.\nIn the above algorithm, during the iterations, the matrices P:,i2 (or Pi1,:) might be singular. Thus, a more practical implementation uses the QR decomposition of these matrices and the MAXVOL algorithm is applied to the corresponding Q factor to find the columns (or rows) of the submatrix. Furthermore, instead of a random choice in step 2.1, one can choose the r columns from the multinomial distribution given by p(i2) = \u2225P:,i2\u2225 \u2225P \u2225 , i2 \u2208 {1, . . . , n1} without sample replacement.\nNote that, in the above algorithm, the input is only a function to evaluate the elements of the matrix P (i.e., we do not need the whole matrix P in computer memory). Some features of cross approximation algorithms are highlighted below:"
        }
    ],
    "year": 2023
}