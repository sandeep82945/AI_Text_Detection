{
    "abstractText": "Diffusion-based generative models have significantly advanced text-to-image generation but encounter challenges when processing lengthy and intricate text prompts describing complex scenes with multiple objects. While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs. In response, we present a novel approach leveraging Large Language Models (LLMs) to extract critical components from text prompts, including bounding box coordinates for foreground objects, detailed textual descriptions for individual objects, and a succinct background context. These components form the foundation of our layout-to-image generation model, which operates in two phases. The initial Global Scene Generation utilizes object layouts and background context to create an initial scene but often falls short in faithfully representing object characteristics as specified in the prompts. To address this limitation, we introduce an Iterative Refinement Scheme that iteratively evaluates and refines box-level content to align them with their textual descriptions, recomposing objects as needed to ensure consistency. Our evaluation on complex prompts featuring multiple objects demonstrates a substantial improvement in recall compared to baseline diffusion models. This is further validated by a user study, underscoring the efficacy of our approach in generating coherent and detailed scenes from intricate textual inputs. Our iterative framework offers a promising solution for enhancing text-to-image generation models\u2019 fidelity with lengthy, multifaceted descriptions, opening new possibilities for accurate and diverse image synthesis from textual inputs.",
    "authors": [],
    "id": "SP:266e719fdb34b9c2ded25d2e7ed8b3b2aeef7e53",
    "references": [
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Omri Avrahami",
                "Dani Lischinski",
                "Ohad Fried"
            ],
            "title": "Blended diffusion for text-driven editing of natural images",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Arpit Bansal",
                "Hong-Min Chu",
                "Avi Schwarzschild",
                "Soumyadip Sengupta",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Universal guidance for diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Brock",
                "Jeff Donahue",
                "Karen Simonyan"
            ],
            "title": "Large scale gan training for high fidelity natural image synthesis",
            "venue": "arXiv preprint arXiv:1809.11096,",
            "year": 2018
        },
        {
            "authors": [
                "Minghao Chen",
                "Iro Laina",
                "Andrea Vedaldi"
            ],
            "title": "Training-free layout control with cross-attention guidance",
            "venue": "arXiv preprint arXiv:2304.03373,",
            "year": 2023
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ming Ding",
                "Zhuoyi Yang",
                "Wenyi Hong",
                "Wendi Zheng",
                "Chang Zhou",
                "Da Yin",
                "Junyang Lin",
                "Xu Zou",
                "Zhou Shao",
                "Hongxia Yang"
            ],
            "title": "Cogview: Mastering text-to-image generation via transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Dave Epstein",
                "Allan Jabri",
                "Ben Poole",
                "Alexei A Efros",
                "Aleksander Holynski"
            ],
            "title": "Diffusion selfguidance for controllable image generation",
            "venue": "arXiv preprint arXiv:2306.00986,",
            "year": 2023
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K.I. Williams",
                "J. Winn",
                "A. Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International Journal of Computer Vision,",
            "year": 2010
        },
        {
            "authors": [
                "Wan-Cyuan Fan",
                "Yen-Chun Chen",
                "DongDong Chen",
                "Yu Cheng",
                "Lu Yuan",
                "Yu-Chiang Frank Wang"
            ],
            "title": "Frido: Feature pyramid diffusion for complex scene image synthesis",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Weixi Feng",
                "Wanrong Zhu",
                "Tsu-jui Fu",
                "Varun Jampani",
                "Arjun Akula",
                "Xuehai He",
                "Sugato Basu",
                "Xin Eric Wang",
                "William Yang Wang"
            ],
            "title": "Layoutgpt: Compositional visual planning and generation with large language models",
            "venue": "arXiv preprint arXiv:2305.15393,",
            "year": 2023
        },
        {
            "authors": [
                "Oran Gafni",
                "Adam Polyak",
                "Oron Ashual",
                "Shelly Sheynin",
                "Devi Parikh",
                "Yaniv Taigman"
            ],
            "title": "Makea-scene: Scene-based text-to-image generation with human priors",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Shuyang Gu",
                "Dong Chen",
                "Jianmin Bao",
                "Fang Wen",
                "Bo Zhang",
                "Dongdong Chen",
                "Lu Yuan",
                "Baining Guo"
            ],
            "title": "Vector quantized diffusion model for text-to-image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville"
            ],
            "title": "Improved training of wasserstein gans",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "Clipscore: A referencefree evaluation metric for image captioning",
            "venue": "arXiv preprint arXiv:2104.08718,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wonjun Kang",
                "Kevin Galim",
                "Hyung Il Koo"
            ],
            "title": "Counting guidance for high fidelity text-to-image synthesis",
            "venue": "arXiv preprint arXiv:2306.17567,",
            "year": 2023
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Gwanghyun Kim",
                "Taesung Kwon",
                "Jong Chul Ye"
            ],
            "title": "Diffusionclip: Text-guided diffusion models for robust image manipulation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Naveen Kodali",
                "Jacob Abernethy",
                "James Hays",
                "Zsolt Kira"
            ],
            "title": "On convergence and stability of gans",
            "venue": "arXiv preprint arXiv:1705.07215,",
            "year": 2017
        },
        {
            "authors": [
                "Boyi Li",
                "Rodolfo Corona",
                "Karttikeya Mangalam",
                "Catherine Chen",
                "Daniel Flaherty",
                "Serge Belongie",
                "Kilian Q. Weinberger",
                "Jitendra Malik",
                "Trevor Darrell",
                "Dan Klein"
            ],
            "title": "Does unsupervised grammar induction need pixels",
            "venue": "arXiv preprint arXiv:2212.10564,",
            "year": 2022
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang",
                "Kai-Wei Chang",
                "Jianfeng Gao"
            ],
            "title": "Grounded language-image pre-training, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Yuheng Li",
                "Haotian Liu",
                "Qingyang Wu",
                "Fangzhou Mu",
                "Jianwei Yang",
                "Jianfeng Gao",
                "Chunyuan Li",
                "Yong Jae Lee"
            ],
            "title": "Gligen: Open-set grounded text-to-image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Long Lian",
                "Boyi Li",
                "Adam Yala",
                "Trevor Darrell"
            ],
            "title": "Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models",
            "venue": "arXiv preprint arXiv:2305.13655,",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013 ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Xingchao Liu",
                "Dahun Hwang Park",
                "Samaneh Azadi",
                "Guandao Zhang",
                "Armen Chopikyan",
                "Yizhe Hu",
                "Trevor Darrell"
            ],
            "title": "More control for free! image synthesis with semantic diffusion guidance",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Shilin Lu",
                "Yanzhu Liu",
                "Adams Wai-Kin Kong"
            ],
            "title": "Tf-icon: Diffusion-based training-free crossdomain image composition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Yun Luo",
                "Zhen Yang",
                "Fandong Meng",
                "Yafu Li",
                "Jie Zhou",
                "Yue Zhang"
            ],
            "title": "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
            "venue": "arXiv preprint arXiv:2308.08747,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2112.10741,",
            "year": 2021
        },
        {
            "authors": [
                "Taesung Park",
                "Ming-Yu Liu",
                "Ting-Chun Wang",
                "Jun-Yan Zhu"
            ],
            "title": "Semantic image synthesis with spatially-adaptive normalization",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Quynh Phung",
                "Songwei Ge",
                "Jia-Bin Huang"
            ],
            "title": "Grounded text-to-image synthesis with attention refocusing",
            "venue": "arXiv preprint arXiv:2306.05427,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Scott Reed",
                "Zeynep Akata",
                "Xinchen Yan",
                "Lajanugen Logeswaran",
                "Bernt Schiele",
                "Honglak Lee"
            ],
            "title": "Generative adversarial text to image synthesis",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Sun",
                "Tianfu Wu"
            ],
            "title": "Image synthesis from reconfigurable layout and style",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Tristan Sylvain",
                "Pengchuan Zhang",
                "Yoshua Bengio",
                "R Devon Hjelm",
                "Shikhar Sharma"
            ],
            "title": "Objectcentric image generation from layouts",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Ming Tao",
                "Hao Tang",
                "Fei Wu",
                "Xiao-Yuan Jing",
                "Bing-Kun Bao",
                "Changsheng Xu"
            ],
            "title": "Df-gan: A simple and effective baseline for text-to-image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Xu",
                "Pengchuan Zhang",
                "Qiuyuan Huang",
                "Han Zhang",
                "Zhe Gan",
                "Xiaolei Huang",
                "Xiaodong He"
            ],
            "title": "Attngan: Fine-grained text to image generation with attentional generative adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Binxin Yang",
                "Shuyang Gu",
                "Bo Zhang",
                "Ting Zhang",
                "Xuejin Chen",
                "Xiaoyan Sun",
                "Dong Chen",
                "Fang Wen"
            ],
            "title": "Paint by example: Exemplar-based image editing with diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Jianfeng Wang",
                "Zhe Gan",
                "Linjie Li",
                "Kevin Lin",
                "Chenfei Wu",
                "Nan Duan",
                "Zicheng Liu",
                "Ce Liu",
                "Michael Zeng"
            ],
            "title": "Reco: Region-controlled text-to-image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Zuopeng Yang",
                "Daqing Liu",
                "Chaoyue Wang",
                "Jie Yang",
                "Dacheng Tao"
            ],
            "title": "Modeling image composition for complex scene generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan"
            ],
            "title": "Scaling autoregressive models for contentrich text-to-image generation",
            "venue": "arXiv preprint arXiv:2206.10789,",
            "year": 2022
        },
        {
            "authors": [
                "Han Zhang",
                "Tao Xu",
                "Hongsheng Li",
                "Shaoting Zhang",
                "Xiaogang Wang",
                "Xiaolei Huang",
                "Dimitris N Metaxas"
            ],
            "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Han Zhang",
                "Tao Xu",
                "Hongsheng Li",
                "Shaoting Zhang",
                "Xiaogang Wang",
                "Xiaolei Huang",
                "Dimitris N Metaxas"
            ],
            "title": "Stackgan++: Realistic image synthesis with stacked generative adversarial networks",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Han Zhang",
                "Jing Yu Koh",
                "Jason Baldridge",
                "Honglak Lee",
                "Yinfei Yang"
            ],
            "title": "Cross-modal contrastive learning for text-to-image generation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A. Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric, 2018b",
            "year": 2018
        },
        {
            "authors": [
                "Bo Zhao",
                "Lili Meng",
                "Weidong Yin",
                "Leonid Sigal"
            ],
            "title": "Image generation from layout",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Guangcong Zheng",
                "Xianpan Zhou",
                "Xuewei Li",
                "Zhongang Qi",
                "Ying Shan",
                "Xi Li"
            ],
            "title": "Layoutdiffusion: Controllable diffusion model for layout-to-image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "arXiv preprint arXiv:2304.10592,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": ""
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Modern generative diffusion models, e.g. Rombach et al. (2022); Ho et al. (2020); Saharia et al. (2022); Ruiz et al. (2023), provided a massive leap forward in the problem of text-to-image generation and have emerged as powerful tools for creating diverse images and graphics from plain text prompts. Their success can be attributed to several factors, including the availability of internet-scale multimodal datasets, increased computational resources, and the scaling up of model parameters. These models are trained using shorter prompts and especially excel at generating images of one prominent foreground object. However, as the description length and the number of objects in the scene increase, modern diffusion models tend to ignore parts of the prompt often leading to critical omissions, misrepresentations, or the generation of objects that do not align with the nuanced details described in the prompts. Fig. 1 shows a scenario where existing state-of-the-art text-to-image diffusion models struggle to follow all the details. This failure can partly be ascribed to the diffusion model\u2019s CLIP text encoder Radford et al. (2021) which can only process the first 77 text tokens, effectively truncating longer prompts and potentially omitting critical details. Indeed, a single prompt describing a complex scene can span far beyond these token limits, making it a challenge for existing models to process and translate long prompts comprehensively.\nRecent efforts (Epstein et al., 2023; Kang et al., 2023) have been dedicated to improving the capabilities of pre-trained diffusion models to faithfully follow the intricate details within text prompts. These works predominantly revolve around aspects such as object count (e.g. \u201c2 oranges and 4 apples on the table\u201d), and/or capturing spatial relationships among objects (e.g. \u201can orange on the left of an apple\u201d). In the context of longer and more complex prompts, these models still tend to struggle to generate coherent images that faithfully reflect the complexity of the text prompts, especially when tasked with the placement of object instances at considerable spatial separations, often falling short of comprehensively capturing all instances of objects as intended. More recently layout-based diffusion models Feng et al. (2023); Li et al. (2023); Yang et al. (2023b) have proven to be effective in capturing the count and spatial characteristics of the objects in the prompt. Such models first generate bounding boxes of all the objects and then condition the diffusion model jointly on the bounding boxes and the text prompt to generate the final image. While effective in the case of small prompts, these models still struggle when presented with long text descriptions that feature multiple diverse objects and hence fail to generate the desired output (See Fig. 1).\nTo address these challenges, our approach seeks to improve text-to-image generation from lengthy prompts. We introduce an iterative framework that divides image generation into two phases: generating a global scene, followed by refinement of individual object representations. We exploit LLMs to break down long prompts into smaller components organized in a data structure that we call Scene Blueprint. This allows us to generate the image in a step-wise manner. Our framework ensures that the final image faithfully adheres to the details specified in lengthy and complex text prompts.\nWe evaluate our framework on challenging prompts containing 3 to 10 unique foreground objects in varied scenes. Our results showcase a significant improvement in recall (\u223c85%) compared to the baseline Feng et al. (2023) (\u223c69%) (+16 % improvement). We also include a user study that demonstrates that our proposed method consistently produces coherent images that closely align with their respective textual descriptions, whereas existing approaches struggle to effectively handle longer text prompts (See Fig. 4)\nIn summary, our main contributions are:\n\u2022 Iterative image generation from long prompts: We introduce a two-phase framework for generating images from long textual descriptions, ensuring a faithful representation of details.\n\u2022 Scene Blueprints using LLMs: We propose Scene Blueprints as a structured scene representation encompassing scene layout and object descriptions that enable a coherent step-wise generation of images from lengthy prompts.\n\u2022 State-of-the-art results: We present quantitative and qualitative results showcasing the effectiveness of our method, in terms of adherence to textual descriptions, demonstrating its applicability and superiority in text-to-image synthesis from lengthy prompts."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Text-to-Image Diffusion. Over the years, Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) have been the default choice for image synthesis (Brock et al., 2018; Reed et al., 2016; Xu et al., 2018; Zhang et al., 2017; 2021; Tao et al., 2022; Zhang et al., 2018a; Karras et al., 2019). However, more recently, the focus has shifted towards text conditioned autoregressive models (Ding et al., 2021; Gafni et al., 2022; Ramesh et al., 2021; Yu et al., 2022) and diffusion models (Rombach et al., 2022; Gu et al., 2022; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022) which have exhibited impressive capabilities in producing high-quality images while avoiding the training challenges, such as instability and mode collapse, commonly associated with GANs (Arjovsky et al., 2017; Gulrajani et al., 2017; Kodali et al., 2017). In particular, diffusion models are trained on large-scale multi-modal data and are capable of generating high-resolution images conditioned on text input. Nevertheless, effectively conveying all the nuances of an image solely from a text prompt can present a considerable hurdle. Recent studies have demonstrated the effectiveness of classifier-free guidance (Ho & Salimans, 2022) in improving the faithfulness of the generations in relation to the input prompt. However, all these approaches are designed to accept shorter text prompts, but they tend to fail in scenarios where the prompt describing a scene is longer. In contrast, our proposed approach generates images from longer text prompts, offering an efficient solution to address this challenge.\nLayout-to-Image Generation. Generating images from layouts either in the form of labeled bounding boxes or semantic maps was recently explored in (Sun & Wu, 2019; Sylvain et al., 2021; Yang et al., 2022; Fan et al., 2023; Zhao et al., 2019; Park et al., 2019). Critically, these layout to image generation methods are only conditioned on bounding boxes and are closed-set, i.e., they can only generate limited localized visual concepts observed in the training set. With the inception of large multi-modal foundational models such as CLIP (Radford et al., 2021), it has now been possible to generate images in an open-set fashion. Diffusion-based generative models can be conditioned on multiple inputs, however, they have been shown to struggle in following the exact object count and spatial locations in the text prompts Chen et al. (2023); Kang et al. (2023). More recently layout conditioned diffusion models have been proposed to solve this problem (Chen et al., 2023; Li et al., 2023; Yang et al., 2023b; Phung et al., 2023). Chen et al. (2023) manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the reconstruction in the desired user-specified layout. GLIGEN (Li et al., 2023) uses a gated self-attention layer that enables additional inputs (e.g., bounding boxes) to be processed. ReCo (Yang et al., 2023b) achieves layout control through regional tokens encoded as part of the text prompt. Zheng et al. (2023) introduce LayoutDiffusion which treats each patch of the image as a special object for multimodal fusion of layout and image and generates images with both high quality and diversity while maintaining precise control over the position and size of multiple objects. In addition to this, there have been few works on LLM-based layout generation (Feng et al., 2023; Lian et al., 2023). These works exploit the LLMs\u2019 abilities to reason over numerical and spatial concepts in text conditions (Li et al., 2022a). Building upon these works, we extend LLMs\u2019 powerful generalization and reasoning capabilities to extract layouts, background information, and foreground object descriptions from longer text prompts.\nDiffusion Based Image Editing and Composition. Diffusion-based image editing has received overwhelming attention due to its ability to condition on multiple modalities. Recent works utilize text-based image editing using diffusion models to perform region modification. DiffusionCLIP (Kim et al., 2022) uses diffusion models for text-driven global multi-attribute image manipulation on varied domains. Liu et al. (2023) provides both text and semantic guidance for global image manipulation. In addition, GLIDE (Nichol et al., 2021) trains a diffusion model for text-to-image synthesis, as well as local image editing using text guidance. Image composition refers to a form of image manipulation where a foreground reference object is affixed onto a designated source image. A naive way to blend a foreground object on a background image may result in an unrealistic composition. However, more recent works (Avrahami et al., 2021; Yang et al., 2023a; Ho & Salimans, 2022; Lu et al., 2023) use diffusion models to overcome the challenges posed due to fusion inconsistency and semantic inharmony for efficient image composition. Avrahami et al. (2021) takes the target region mask and simply blends the noised version of the input image with local text-guided diffusion latent. Yang et al. (2023a) trains a diffusion model to blend an exemplar image on the source image at the position specified by an arbitrary shape mask and leverages the classifier-free guidance (Ho & Salimans, 2022) to increase the similarity to the exemplar image. Lu et al. (2023) introduces TF-ICON which leverages off-the-shelf diffusion models to perform cross-domain image-guided composition without\nrequiring additional training, fine-tuning, or optimization. Based on these works, we build an iterative refining scheme, which performs region-based composition at the layout level, utilizing a given mask shape and modifies each layout based on the object characteristics guided by a multi-modal loss signal."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARIES ON DIFFUSION MODELS",
            "text": "Diffusion models are generative models that learn the data distribution of complex datasets. They consist of a forward diffusion process and a reverse diffusion process. During the forward process, noise is added to the input data point x0 for T steps, until the resulting vector xt is almost distributed according to a standard Gaussian distribution. Each step in the forward process is a Gaussian transition q(xt | xt\u22121) := N ( \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI), where {\u03b2t}Tt=0 is a fixed or learned variance schedule. The resulting latent variable xt can be expressed as:\nxt = \u221a \u03b1tx0 + \u221a 1\u2212 \u03b1t\u03f5, \u03f5 \u223c N (0, I), (1)\nwhere \u03b1t := \u220ft\ns=1 (1\u2212 \u03b2s). The reverse process q(xt\u22121 | xt) is parametrized by another Gaussian transition p\u03b8(xt\u22121 | xt) := N (xt\u22121;\u00b5\u03b8(xt, t), \u03c3\u03b8(xt, t)I). \u00b5\u03b8(xt, t) can be decomposed into the linear combination of xt and a noise approximation model \u03f5\u03b8(xt, t), which is trained so that for any pair (x0, t) and any sample of \u03f5,\n\u03f5\u03b8(xt, t) \u2248 \u03f5 = xt \u2212 \u221a \u03b1tx0\u221a\n1\u2212 \u03b1t . (2)\nAfter training \u03f5\u03b8(x, t), different works (Song et al., 2020a; Song & Ermon, 2019; Song et al., 2020b) study different approximations of the unknown q(xt\u22121|xt,x0) to perform sampling. In our work, we utilize the denoising diffusion implicit model (DDIM) Song et al. (2020a) to predict the clean data point."
        },
        {
            "heading": "3.2 OVERVIEW",
            "text": "Our core objective is to generate images from long textual descriptions, ensuring that the resulting images faithfully represent the intricate details outlined in the input text. We generate the output image in a multi-step manner; generating an initial image that serves as a template at a global level, followed\nFigure 3: Effect of interpolation factor \u03b7: We interpolate the k bounding boxes for each object and control the interpolation by the factor \u03b7. We visualize the change in the bounding box location of \u201da white cat\u201d highlighted in the text for different \u03b7 values from 0.1 to 0.9 with increments of 0.1.\nby a box-level refinement phase that serves as a corrective procedure. 1) Global scene generation: We begin by decomposing lengthy text prompts into \u201cScene Blueprints.\u201d Scene Blueprints provide a structured representation of the scene containing: object bounding boxes in image space, detailed text descriptions for each box, and a background text prompt. We use an LLM to extract the Scene Blueprint from the given long prompt and use layout-conditioned text-to-image models to generate the initial image. Unlike prior works, we support additional user control on the box layouts by generating multiple proposal blueprints and providing an option to smoothly interpolate between the candidate layouts. This interpolation not only facilitates (optional) user control but also helps mitigate any potential errors introduced by the LLM. 2) Box-level content refinement: In the second phase, we iterate through all the boxes and evaluate and refine their content in terms of quality and adherence to the prompt. We use a multi-modal guidance procedure that maximizes a quality score for each box."
        },
        {
            "heading": "3.3 GLOBAL SCENE GENERATION",
            "text": "Textual representation of a scene contains various characteristics that provide information about objects, their spatial properties, semantics, attributes, and more. To coherently capture these properties, we employ an off-the-shelf pre-trained large language model (LLM)(OpenAI, 2021). We instruct the LLM with an appropriately engineered prompt (see supplementary Sec. A.4) to generate a Scene Blueprint containing the following three components:\n1. Layout: Bounding box coordinates for each object - {object : (x, y, width, height)}\n2. Object description: Description associated with each object - {object : description}\n3. Background Prompt: A general prompt describing the overall essence of the scene.\nThe layout and the background prompt generated by the LLM are then used to condition the diffusion model to generate an image. We follow recent work by Lian et al. (2023) which generates the image from the layouts in two steps, 1) generating masked latent inversion for each object bounding box, and 2) composing the latent inversion as well as generating the corresponding background from the background prompt.\nLayouts Interpolation and Noise Correction. While LLMs have advanced spatial reasoning abilities, we observed that they struggle to model the spatial positions of the objects when presented with longer descriptions, often resulting in abnormal relative sizes of the objects and unnatural placement of object boxes in the scene (Sec.4.1). A naive approach to resolve these issues is to fine-tune an LLM on handcrafted data of text descriptions and bounding boxes. However, this approach requires extensive resources in terms of human annotators and compute requirements, and risks catastrophic forgetting (Luo et al., 2023). On the other hand, correcting these errors manually by adjusting the boxes is also a daunting task and defeats the purpose of using an LLM to extract layouts. To address these challenges, we propose a simple yet effective solution - layout interpolation. Instead of generating only one proposal layout, we query the LLM to generate k layouts. Subsequently, we employ linear interpolation to compute the coordinates for each object\u2019s final bounding box, denoted as oj = (x\u0302j , y\u0302j , w\u0302j , h\u0302j), where for any coordinate z\u0302j \u2208 oj , z\u0302j =Interpolation(z(1)j , z (2) j , z (3) j . . . z (k) j ). The interpolation function recursively updates each coordinate such that at any iteration l of bounding boxes, z\u0302(l)j := (1\u2212 \u03b7) \u00b7 z\u0302 (l\u22121) j + \u03b7 \u00b7 z (l) j , where \u03b7 is the interpolation factor which controls the influence of the individual bounding boxes on the final interpolated box.\nAs the long complex prompts may result in a large number of boxes, the images generated by the layout guidance tend to have color noise and small artifacts (See Fig. A.2 in supplementary). Therefore, we optionally perform an image-to-image translation step using a diffusion model Rombach et al. (2022), resulting in a cleaner image while preserving the semantics. We refer to this image as xinitial.\nDespite conditioning on layout, we observe that the diffusion model is unable to generate all scene objects effectively. It struggles to compose multiple diverse objects having varied spatial and semantic properties in one shot. Consequently, xinitial often has missing objects or fails to represent them accurately in accordance with their descriptions. Therefore, we employ a box-level refinement strategy that evaluates and refines the content within each bounding box of the layout in terms of quality and adherence to the prompt."
        },
        {
            "heading": "3.4 BOX-LEVEL REFINEMENT",
            "text": "Current diffusion models have certain limitations in terms of composing multiple objects in the image when presented with longer text prompts, a problem that is still unexplored. To overcome this issue and ensure faithful generation of all the objects, we introduce an Iterative Refinement Scheme (IRS). Our proposed IRS works at the bounding box level and ensures the corresponding object at each bounding box is characterized by its properties given in the textual description. To achieve this, we iterate across each object\u2019s bounding box in xinitial and compare the visual characteristics of the object with its corresponding description extracted from the text prompt. Consider an object j with its bounding box oj = (x\u0302j , y\u0302j , w\u0302j , h\u0302j) and its textual characteristics denoted by dj , we use CLIP score Hessel et al. (2021) as a metric to get the similarity between the object and its description such that, score(s) = CLIP(object(j), dj). If the CLIP score s is below a certain threshold, we modify the content of the bounding box such that it follows its corresponding description.\nAny reasonable modification of the content within a bounding box must improve its fidelity and adherence to the prompt. Since diffusion models, e.g. stable diffusion (Rombach et al., 2022), are already good at generating high-fidelity images from shorter prompts, we exploit this ability and follow a paint-by-example approach. We generate a new object for the designated box by passing the object description dj to a Text-to-Image stable diffusion model. The generated image x ref j acts as a reference content for the bounding box oj . We then use an image composition model Yang et al. (2023a) conditioned on the reference image xrefj , mask mj (extracted from the bounding box), and source image xinitial to compose the reference object at the designated position on the source image specified by the mask. To ensure the composition generates the object that follows the properties described in the text prompt as closely as possible, we guide the sampling process by an external multi-modal loss signal.\nBox-level Multi-modal Guidance. Given the initial image xinitial, a reference image xrefj , a guiding text prompt dj and a binary mask mj that marks the region of interest in the image corresponding to bounding box oj , our goal is to generate a modified image x\u0302, s.t. the content of the region x\u0302\u2299mj is consistent with the prototype image xrefj and adheres to the text description dj , while the complementary area remains as close as possible to the source image, i.e., xinitial \u2299 (1 \u2212mj) \u2248 x\u0302\u2299 (1\u2212mj), where \u2299 is the element-wise multiplication. Dhariwal & Nichol (2021) use a classifier trained on noisy images to guide generation towards a target class. In our case, we use an external function in the form of CLIP to guide the generation in order to adhere to the prototype image xrefj and text description dj . However, CLIP is trained on noise-free data samples, we estimate a clean image x0 from each noisy latent xt during the denoising diffusion process via Eqn. 1 as follows,\nx\u03020 = xt \u2212 ( \u221a 1\u2212 \u03b1t)\u03f5\u03b8(xt, t)\u221a\n\u03b1t . (3)\nOur CLIP based multimodal guidance loss can then be expressed as, LCLIP = Lcosine ( CLIPimage(x\u03020 \u2299mj),CLIPtext(dj) ) +\n\u03bb \u00b7 Lcosine ( CLIPimage(x\u03020 \u2299mj),CLIPimage(xrefj ) ) ,\n(4)\nwhere Lcosine denotes the cosine similarity loss and \u03bb is a hyperparameter. The first part of the equation measures the cosine loss between the composed object at the region specified by the mask\nmj and its corresponding text characteristics dj . The second part of the equation measures the cosine loss between the composed object and its corresponding prototype xrefj . A similar approach using CLIP text-based guidance is used in Avrahami et al. (2021) for region-based modification. However, in contrast, we also include a CLIP image-based guidance to steer the generation toward the prototype image xrefj to account for the fine-grained details that may not be captured in the text description. In order to confine the modification within the given bounding box, we optionally employ a background preservation loss Lbg which is a summation of the L2 norm of the pixel-wise differences and Learned Perceptual Image Patch Similarity metric Zhang et al. (2018b) between x\u03020 \u2299 (1\u2212mj) and xinitial \u2299 (1\u2212mj) The final diffusion guidance loss is thus the weighted sum of LCLIP and Lbg given as, Lguidance = LCLIP + \u03b3 \u00b7 Lbg. (5) The gradient of the resultant loss\u2207x\u03020Lguidance is used to steer the sampling process to produce an object at the bounding box oj which follows the properties of prototype x ref j and description dj ."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Settings: Our framework uses a combination of several components. For acquiring the long text descriptions, we ask ChatGPT to generate scenes on various themes. In addition to this, we also use the textual descriptions from some COCO (Lin et al., 2014) and PASCAL (Everingham et al., 2010) images by querying an image captioning model (Zhu et al., 2023) to generate a detailed description spanning 80-100 words. For extracting layouts, bounding boxes, and background prompt, we make use of ChatGPT completion API (OpenAI, 2021) with an appropriate instruction template (See Supplementary Sec. A.4). We generate 3 layouts for each text prompt and interpolate them to a single layout to account for the spatial location correctness. To avoid layout overlap, we push the boxes away from each other until there is minimal contact wherever feasible. For base layout-to-image generation, we use the work of Lian et al. (2023) and scale it for longer text prompts. We use 20 diffusion steps at this point. For box refinement, we use the image composition model of Yang et al. (2023a) which conditions on a reference image to compose the object in order to capture the fine-grained details. For each box refinement, we use 50 diffusion steps. For implementation, we use Pytorch 2.0. Finally, our entire pipeline runs on a single Nvidia A100 40GB GPU.\nQuantitative Results: Our work stands as a first effort to address the challenge of generating images from extensive text prompts. As discussed in Section 1, current diffusion-based text-to-image generation methods typically utilize the CLIP tokenizer to condition the diffusion model for image generation. However, this approach can lead to inconsistent images when confronted with lengthy text prompts. To the best of our knowledge, there is currently no established metric for assessing the performance of diffusion models in handling lengthy text descriptions. Hence we propose to use the Prompt Adherence Recall (PAR) score to quantify adherence to the prompt defined as Mean of object presence over all objects over all prompts where we use an off-the-shelf object detector (Li et al., 2022b) to check if the object is actually present in the generated image such that object presence is 1 if present and 0 otherwise. We achieve a PAR score of 85% which is significantly better than Stable Diffusion (49%) and GLIGEN (57%). We also conducted an extensive user study to assess the effectiveness of our method in comparison to four established baseline approaches: Stable Diffusion (Rombach et al., 2022), GLIGEN (Li et al., 2023), LayoutGPT (Feng et al., 2023) and LLM-Grounded Diffusion (Lian et al., 2023). To mitigate any bias, participants were instructed to select one image from a pair of images randomly selected from two distinct approaches. Their goal was to choose the image that most accurately represented the provided textual descriptions regarding\nspatial arrangement, object characteristics, and overall scene dynamics. The outcomes of the user study are presented in Fig. 4. Our findings demonstrate that, on average, our proposed method consistently produces coherent images that closely align with their respective textual descriptions, whereas existing approaches struggle to effectively handle longer text prompts.\nQualitative Analysis: Fig. 5 presents a qualitative assessment of our method in comparison to established state-of-the-art methods. The text descriptions include specific phrases denoted by underlined italics, conveying information about objects, their attributes, and spatial arrangements. Notably, the red text beneath each image highlights instances of missing objects, while the purple text indicates spatial inaccuracies, and the black text identifies elements of implausibility or distortion. From the figure, the stable diffusion baseline (column 1) frequently falls short in incorporating certain\nFigure 6: Effect of Layout Interpolation: Our layout interpolation method (last column) significantly improves object spatial positioning compared to non-interpolated cases (first two columns).\nFigure 7: Effect of Guidance: Without guidance signal, the composed image does not follow the properties corresponding to its description and visual appearance. In contrast, the one with the guidance (right) adheres to the visual prototype and description.\nobjects from the prompt due to its inability to efficiently handle lengthy text prompts. In some instances (rows 2 and 4), the generated images exhibit unrealistic features. Layout-based approaches (columns 2, 3, and 4) also encounter difficulties in fully capturing the nuances of the text prompt, resulting in occasional omissions of objects and instances of deformations (column 3, row 5). In contrast, our approach excels by accurately capturing all objects from the text, including their intricate details and precise spatial positions."
        },
        {
            "heading": "4.1 ABLATIONS",
            "text": "Effect of Layout Interpolation. We query the LLM to generate multiple layouts from the input text prompt and then employ linear interpolation to merge them into a single layout. However, for lengthy prompts, LLMs can occasionally generate layouts with random object placements, resulting in unnatural images. As shown in Fig. 6, the first two columns depict images without layout interpolation, while the last column shows the interpolated image. The underlined phrases in the text prompt indicate object spatial characteristics. In contrast, the last column demonstrates the improved result with interpolation, aligning nearly every object with its textual spatial description.\nEffect of Guidance The external guidance in the form of CLIP multi-modal loss used in the refinement stage steers sampling of the specific box proposal towards its corresponding description and reference prototype. We present a visual illustration of this phenomenon in Fig. 7. As seen from the figure, the properties of the cat get more aligned with the prototype image and text description in the presence of a guidance signal."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we identified the limitations of prior text-to-image models in handling lengthy text prompts. In response, we introduced a framework involving a data structure (Scene Blueprint) and a multi-step procedure involving global scene generation followed by an iterative refinement scheme to generate images that faithfully adhere to the details in lengthy prompts. Our framework offers a promising solution for accurate and diverse image synthesis from complex text inputs, bridging a critical gap in text-to-image synthesis capabilities.\nWhile we presented a simple interpolation technique to combine various bounding box proposals, we maintained fixed box layouts for the second phase. A promising avenue for future research lies in exploring dynamic adjustments of boxes within the iterative refinement loop. Another area warranting further examination pertains to the handling of overlapping boxes. While we currently address this challenge by sorting boxes by size prior to the box-level refinement phase, there is an opportunity to explore more advanced techniques for managing overlaps. Additionally, our current approach to box-level refinement treats each object in isolation, overlooking the relationships that exist among objects within a scene. A compelling avenue for future research is to incorporate and leverage these object relationships, with the aim of achieving more comprehensive and contextually aware image generation."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 ALGORITHM",
            "text": ""
        },
        {
            "heading": "A.2 NOISE AND ARTIFACT CORRECTION",
            "text": "As discussed in Sec. 3.3, the generated image after the first phase sometimes suffers from noise or artifacts. We do an optional noise correction step after Global scene generation, which essentially removes the unwanted noise and artifacts. Specifically, we utilize the image-to-image translation method of stable diffusion Rombach et al. (2022) which instead of starting from random noise, starts from an input image, adds noise to it and then denoises it in the reverse process. The idea is to enhance the quality of the image while maintaining its semantics. We notice that this process removes the unwanted noise and artifacts present in the image (See Figure 8)."
        },
        {
            "heading": "A.3 FURTHER QUALITATIVE COMPARISONS",
            "text": "We provide further qualitative comparisons of our approach with state-of-the-art baselines in Fig. 9.\nAlgorithm 1 LLM Blueprint Input: long textual description C, diffusion steps k, sampling iterations n, layout-to-image model ML, LLM , image composition modelMcomp (\u00b5\u03b8(xt),\u03a3\u03b8(xt)), LLM queries q, stable diffusion model SD, CLIP model, Interpolation function Output: composed image x\u0302 encompassing all the elements of the textual description C // Get layouts, object details, and background prompt from LLM. O|q,D,Pb = LLM(C) // Interpolate layouts. O =Interpolation(O|q) // Generate initial image from interpolated layout. xinitial =ML(O,D,Pb) // Optional noise correction. xinitial \u2190 SDimage\u2212image(xinitial) // Iterative Refinement. for oj , dj in {O,D} do xrefj \u2190 SDtext\u2212image(dj) ; mj \u2190 GetMask(oj) xk \u223c N ( \u221a \u03b1\u0304kx0, (1\u2212 \u03b1\u0304k)I)\nfor t from k to 1 do for iter from 1 to n do\n\u00b5,\u03a3\u2190 \u00b5\u03b8(xt),\u03a3\u03b8(xt) x\u03020 \u2190 xt\u221a\u03b1\u0304t \u2212 \u221a 1\u2212\u03b1\u0304t\u03f5\u03b8(xt,t)\u221a \u03b1\u0304t L \u2190 LCLIP(x\u03020, dj ,mj) + \u03bbLCLIP(x\u03020, xrefj ,mj) + \u03b3Lbg(x, x\u03020,mj) \u03f5\u0302\u2190 \u03f5\u03b8(xt)\u2212 \u221a 1\u2212 \u03b1\u0304t\u2207x\u03020L xfg \u2190 \u221a \u03b1\u0304t\u22121 ( xt\u2212 \u221a 1\u2212\u03b1\u0304t \u03f5\u0302\u221a \u03b1\u0304t ) + \u221a 1\u2212 \u03b1\u0304t\u22121\u03f5\u0302 xt\u22121 \u2190 mj \u2299 xfg + (1\u2212mj)\u2299N ( \u221a \u03b1\u0304tx0, (1\u2212 \u03b1\u0304t)I)\nend for end for x0 \u2190 xt\u22121\nend for return x0\nA.4 INSTRUCT PROMPTS FOR LLM\nOur approach utilizes LLMs\u2019 advanced spatial and reasoning capabilities to derive layouts, object descriptions, and background prompt from a long textual description. For extracting layouts and background prompts, we use the prompt designed by Lian et al. (2023) and scale it to work on longer textual descriptions. For extracting object descriptions, we designed our own unique prompt. Below are the instruct prompts utilized in our work.\nInstruct prompt for extracting layouts and background prompt.\nYou are an intelligent bounding box generator. I will provide you with a caption for a photo, image, a detailed scene, or a painting. Your task is to generate the bounding boxes for the objects mentioned in the caption, along with a background prompt describing the scene. The images are of size 512x512. The top-left corner has coordinates [0, 0]. The bottom-right corner has coordinates [512, 512]. The bounding boxes should not overlap or go beyond the image boundaries. Each bounding box should be in the format of (object name, [top-left x coordinate, top-left y coordinate, box width, box height]) and include exactly one object (i.e., start the object name with \u201da\u201d or \u201dan\u201d if possible). Do not put objects that are already provided in the bounding boxes into the background prompt. Do not include non-existing or excluded objects in the background prompt. If needed, you can make reasonable guesses. Please refer to the example below for the desired format.\nCaption: In the quiet countryside, a red farmhouse stands with an old-fashioned charm. Nearby, a weathered picket fence surrounds a garden of wildflowers. An antique tractor,\nthough worn, rests as a reminder of hard work. A scarecrow watches over fields of swaying crops. The air carries the scent of earth and hay. Set against rolling hills, this farmhouse tells a story of connection to the land and its traditions Objects: [(\u2019a red farmhouse\u2019, [105, 228, 302, 245]), (\u2019a weathered picket fence\u2019, [4, 385,"
        },
        {
            "heading": "504, 112]), (\u2019an antique tractor\u2019, [28, 382, 157, 72]), (\u2019a scarecrow\u2019, [368, 271, 66, 156]) ] Background prompt: A realistic image of a quiet countryside with rolling hills",
            "text": "Caption: A realistic image of landscape scene depicting a green car parking on the left of a blue truck, with a red air balloon and a bird in the sky Objects: [(\u2019a green car\u2019, [21, 181, 211, 159]), (\u2019a blue truck\u2019, [269, 181, 209, 160]), (\u2019a red air balloon\u2019, [66, 8, 145, 135]), (\u2019a bird\u2019, [296, 42, 143, 100])] Background prompt: A realistic image of a landscape scene\nInstruct prompt for extracting object descriptions.\nYou are an intelligent description extractor. I will give you a list of the objects and a corresponding text prompt. For each object, extract its respective description or details mentioned in the text prompt. The description should strictly contain fine details about the object and must not contain information regarding location or abstract details about the object. The description must also contain the name of the object being described. For objects that do not have concrete descriptions mentioned, return the object itself in that case. The output should be a Python dictionary with the key as object and the value as description. The description should start with \u2019A realistic photo of object\u2019 followed by its characteristics. Sort the entries as per objects that are spatially behind (background) followed by objects that are spatially ahead (foreground). For instance object \u201da garden view\u201d should precede the \u201dtable\u201d. Make an intelligent guess if possible. Here are some examples:\nlist of objects: [a Golden Retriever,a white cat,a wooden table,a vase of vibrant flowers,a sleek modern television] text prompt: In a cozy living room, a heartwarming scene unfolds. A friendly and affectionate Golden Retriever with a soft, golden-furred coat rests contently on a plush rug, its warm eyes filled with joy. Nearby, a graceful and elegant white cat stretches leisurely, showcasing its pristine and fluffy fur. A sturdy wooden table with polished edges stands gracefully in the center, adorned with a vase of vibrant flowers adding a touch of freshness. On the wall, a sleek modern television stands ready to provide entertainment. The ambiance is warm, inviting and filled with a sense of companionship and relaxation.\noutput: {a sleek modern television: A realistic photo of a sleek modern television., a wooden table: A realistic photo of a sturdy wooden table with polished edges., vase of vibrant flowers: A realistic photo of a vase of vibrant flowers adding a touch of freshness., a Golden Retriever: \u2019A realistic photo of a friendly and affectionate Golden Retriever with a soft, golden-furred coat and its warm eyes filled with joy., a white cat: \u2019A realistic photo of a graceful and elegant white cat stretches leisurely, showcasing its pristine and fluffy fur.}"
        },
        {
            "heading": "A.5 ADDITIONAL COMPARISONS WITH DEEPFLOYD AND DENSEDIFFUSION",
            "text": "As recommended we provide visual comparison of our method with DeepFloyd and DenseDiffusion in Fig. 10. As seen from the figure, DeepFloyd with a strong T5 text encoder struggles to generate coherent images with complex compositional prompts. The same is true for DenseDiffusion. We conclude that our scene blueprint augmented with iterative refinement is necessary for generating coherent images from complex compositional prompts.\nWe further present quantitative comparison in terms of Prompt Adherence Recall (PAR) (see Sec. 4 of main paper) of our approach with all the baselines in Table 1. We also report average inference time for each approach. As seen from the table, DeepFloyd with a PAR score of 60% is highly inefficient as it takes around 8 min to generate a 256x256 image from a long textual prompt. We notice that while other approaches are slightly efficient in time, they report a lower PAR score, thus rendering\nthem ineffective on complex compositional prompts. Our approach with an average inference time of around 3 min (including blue print generation and iterative refinement process) has the highest PAR score of 85%, thus validating the effectiveness of our approach. Therefore, a discernible trade-off emerges between addressing complexity and ensuring the faithful reproduction of images."
        },
        {
            "heading": "A.6 EFFECT OF NUMBER OF LAYOUTS ON FINAL GENERATED IMAGE",
            "text": "We present an analysis to study the effect of number of layouts on the final generated image in Fig 11. Consistent with the findings of Li et al. (2022b), the layouts generated by LLM (ChatGPT) most\nof the times align with the textual prompts. The interpolation of multiple layouts (K>1) produces coherent images preserving the spatial relationships between the objects i.e. dog is always towards the left while cat is on the right. However, in extreme cases with only one layout, such as in Fig 12, the ChatGPT can sometimes generate spatially incorrect boxes, such as that for dog and cat, leading to missing objects or incorrect spatial positions of the objects in the final generated image (Fig. 6 of main paper)"
        },
        {
            "heading": "A.7 EFFECTIVENESS OF CHATGPT IN MODELING SPATIAL RELATIONSHIPS",
            "text": "Consistent with the findings of Li et al. (2022b), we observed that proprietary LLMs such as ChatGPT are exceptionally good at following the object positions from the textual prompt. We conducted an analysis in Table 2 with ChatGPT to verify its effectiveness on well-defined and ambiguous prompts (position of some objects is unclear). Specifically we prompted ChatGPT to generate bounding boxes of cat and dog with three different prompts: \u201dA living room with a cat and a dog sitting on each side\u201d, \u201dA living room with a cat sitting towards right and a dog sitting towards left\u201d, \u201dA living room with a dog sitting towards right and a cat sitting towards left\u201d. Our analysis reveals that on an average 60% of the times for the ambiguous prompt \u201dA living room with a cat sitting towards right and a dog sitting towards left\u201d, the chatGPT generates bounding box on the right for the cat and on left for the dog. While for other two prompts, the ChatGPT generates correct location of bounding boxes for cat and dog. This shows that ChatGPT works exceptionally well for unambiguous prompts with clearly defined spatial relationships. For the ambiguous prompt, we notice an inherent bias inside ChatGPT, which leads to it generating dog on the left and cat on the right. To account for the errors and to provide a meaningful fix to the inherent bias inside ChatGPT, we provide a hyperparameter \u03b7 in the interpolation, which can be controlled to adjust for the bounding box of each object (see Fig. 3 in the main paper for a visual illustration of effect of \u03b7 parameter)."
        },
        {
            "heading": "A.8 SAMPLING PROCESS WITH GUIDANCE",
            "text": "Kindly note that the guidance function enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any specific components Bansal et al. (2023). Since ours is a training-free approach, therefore, to ensure the objects at each bounding box location follow their corresponding description in the long textual prompt, we employ CLIP-based loss LCLIP at each step t during sampling process (Eq. 5, main paper). During sampling, we first compute the gradient of this loss with respect to the estimated clean sample x\u03020 from Eq. 3 of the main paper and denote it as\u2207x\u03020LCLIP . Using this gradient, we update the noise \u03f5 such that the updated noise at time step t is expressed as,\n\u03f5\u0302\u2190 \u03f5\u03b8(xt)\u2212 \u221a 1\u2212 \u03b1\u0304t\u2207x\u03020LCLIP (6)\nBased on the above-updated noise, the sampling step is given as,\nxfg \u2190 \u221a \u03b1\u0304t\u22121\n( xt \u2212\n\u221a 1\u2212 \u03b1\u0304t\u03f5\u0302\u221a \u03b1\u0304t\n) + \u221a 1\u2212 \u03b1\u0304t\u22121\u03f5\u0302, (7)\nwhere xfg represents the latent variable of the image containing the specific foreground object being refined. This is combined with the unaltered background to get the final output latent as shown below,\nxt\u22121 \u2190 m\u2299 xfg + (1\u2212m)\u2299N ( \u221a \u03b1\u0304tx0, (1\u2212 \u03b1\u0304t)I), (8)\nwhere m represents the mask for the object being refined. The process is repeated n iterations across T time steps and finally decoded to get the image with refined object. Further please refer to the Algorithm A1 in Appendix A1 for a full end-to-end pipeline."
        },
        {
            "heading": "A.9 STUDY ON FIDELITY",
            "text": "Additionally, we present an extensive user study in Fig. 13 on the fidelity of generated images. For the user study, we compared our method with Stable Diffusion, GLIGEN, LayoutGPT and LLM Grounded Diffusion.\nThe subjects for human study were recruited from a pool of academic students with backgrounds in computer science and engineering. To ensure an unbiased evaluation and maintain objectivity, the subjects participating in the evaluation remained anonymous with no conflicts of interest with the authors. Furthermore, we prioritized user confidentiality by refraining from collecting personal details such as names, addresses etc.\nThe entire survey process was anonymized, ensuring that there was no leakage of data to any user. Following a 2-AFC (two-alternative forced choice) design, each participant was tasked with selecting one image out of a pair of images, having highest fidelity. Each pair of images was randomly sampled from a pool containing images from baselines as well as our approach. This system mitigates biases which can originate from user-curated surveys. Further, the pool of images was shuffled before sampling each time. The system was further equipped with the feature of randomly flipping the position of the images, to remove the user-specific position bias in case any pair of images is repeated. Additionally, a 2-second delay between questions was introduced to facilitate more thoughtful decision-making. The system yielded approximately 90 responses from 15 subjects, with each user answering an average of 6 questions. From the Fig. 13, its clear that our approach compares favorably against baselines in terms of image fidelity while maintaining highest alignment with the text."
        }
    ],
    "year": 2023
}