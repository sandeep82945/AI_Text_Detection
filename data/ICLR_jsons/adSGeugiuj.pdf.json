{
    "abstractText": "Denoisers play a central role in many applications, from noise suppression in lowgrade imaging sensors, to empowering score-based generative models. The latter category of methods makes use of Tweedie\u2019s formula, which links the posterior mean in Gaussian denoising (i.e., the minimum MSE denoiser) with the score of the data distribution. Here, we derive a fundamental relation between the higherorder central moments of the posterior distribution, and the higher-order derivatives of the posterior mean. We harness this result for uncertainty quantification of pre-trained denoisers. Particularly, we show how to efficiently compute the principal components of the posterior distribution for any desired region of an image, as well as to approximate the full marginal distribution along those (or any other) one-dimensional directions. Our method is fast and memory-efficient, as it does not explicitly compute or store the high-order moment tensors and it requires no training or fine tuning of the denoiser. Code and examples are available on the project website.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hila Manor"
        },
        {
            "affiliations": [],
            "name": "Tomer Michaeli"
        }
    ],
    "id": "SP:258ac7c376c21ec79d112575e9382a154c1b7bdc",
    "references": [
        {
            "authors": [
                "Eirikur Agustsson",
                "Radu Timofte"
            ],
            "title": "NTIRE 2017 challenge on single image super-resolution: Dataset and study",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Michal Aharon",
                "Michael Elad",
                "Alfred Bruckstein"
            ],
            "title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation",
            "venue": "IEEE Transactions on signal processing,",
            "year": 2006
        },
        {
            "authors": [
                "Anastasios N Angelopoulos",
                "Amit Pal Kohli",
                "Stephen Bates",
                "Michael Jordan",
                "Jitendra Malik",
                "Thayer Alshaabi",
                "Srigokul Upadhyayula",
                "Yaniv Romano"
            ],
            "title": "Image-to-image regression with distribution-free uncertainty quantification and applications in imaging",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Pablo Arbelaez",
                "Michael Maire",
                "Charless Fowlkes",
                "Jitendra Malik"
            ],
            "title": "Contour detection and hierarchical image segmentation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2010
        },
        {
            "authors": [
                "Peter Arbenz"
            ],
            "title": "Lecture notes on solving large scale eigenvalue problems",
            "year": 2016
        },
        {
            "authors": [
                "Dmitry Baranchuk",
                "Andrey Voynov",
                "Ivan Rubachev",
                "Valentin Khrulkov",
                "Artem Babenko"
            ],
            "title": "Labelefficient semantic segmentation with diffusion models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Zdravko I Botev",
                "Dirk P Kroese"
            ],
            "title": "The generalized cross entropy method, with applications to probability density estimation",
            "venue": "Methodology and Computing in Applied Probability,",
            "year": 2011
        },
        {
            "authors": [
                "Alon Brifman",
                "Yaniv Romano",
                "Michael Elad"
            ],
            "title": "Turning a denoiser into a super-resolver using plug and play priors",
            "venue": "In 2016 IEEE International Conference on Image Processing (ICIP),",
            "year": 2016
        },
        {
            "authors": [
                "Antoni Buades",
                "Bartomeu Coll",
                "J-M Morel"
            ],
            "title": "A non-local algorithm for image denoising",
            "venue": "IEEE computer society conference on computer vision and pattern recognition (CVPR\u201905),",
            "year": 2005
        },
        {
            "authors": [
                "Guangyong Chen",
                "Fengyuan Zhu",
                "Pheng Ann Heng"
            ],
            "title": "An efficient statistical method for image noise level estimation",
            "venue": "In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Kostadin Dabov",
                "Alessandro Foi",
                "Vladimir Katkovnik",
                "Karen Egiazarian"
            ],
            "title": "Image denoising by sparse 3-d transform-domain collaborative filtering",
            "venue": "IEEE Transactions on image processing,",
            "year": 2007
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Tim Dockhorn",
                "Arash Vahdat",
                "Karsten Kreis"
            ],
            "title": "GENIE: Higher-order denoising diffusion solvers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bradley Efron"
            ],
            "title": "Tweedie\u2019s formula and selection bias",
            "venue": "Journal of the American Statistical Association,",
            "year": 2011
        },
        {
            "authors": [
                "Rich Franzen"
            ],
            "title": "Kodak lossless true color image suite. source: http://r0k",
            "venue": "us/graphics/kodak,",
            "year": 1999
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "In international conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "R\u00e9mi Gribonval"
            ],
            "title": "Should penalized least squares regression be interpreted as maximum a posteriori estimation",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2011
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Eliahu Horwitz",
                "Yedid Hoshen"
            ],
            "title": "Conffusion: Confidence intervals for diffusion models",
            "venue": "arXiv preprint arXiv:2211.09795,",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Asem Khmag",
                "Abd Rahman Ramli",
                "SA Al-Haddad",
                "Noraziahtulhidayu Kamarudin"
            ],
            "title": "Natural image noise level estimation based on local statistics for blind noise reduction",
            "venue": "The Visual Computer: International Journal of Computer Graphics,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Priyanka Kokil",
                "Turimerla Pratap"
            ],
            "title": "Additive white gaussian noise level estimation for natural images using linear scale-space features",
            "venue": "Circuits, Systems, and Signal Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Krull",
                "Tim-Oliver Buchholz",
                "Florian Jug"
            ],
            "title": "Noise2void-learning denoising from single noisy images",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Krull",
                "Tom\u00e1\u0161 Vi\u010dar",
                "Mangal Prakash",
                "Manan Lalit",
                "Florian Jug"
            ],
            "title": "Probabilistic noise2void: Unsupervised content-aware denoising",
            "venue": "Frontiers in Computer Science,",
            "year": 2020
        },
        {
            "authors": [
                "Gilad Kutiel",
                "Regev Cohen",
                "Michael Elad",
                "Daniel Freedman",
                "Ehud Rivlin"
            ],
            "title": "Conformal prediction masks: Visualizing uncertainty in medical imaging",
            "venue": "In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare,",
            "year": 2023
        },
        {
            "authors": [
                "Yan LeCun"
            ],
            "title": "The MNIST database of handwritten digits",
            "venue": "URL http://yann.lecun. com/exdb/mnist/",
            "year": 1998
        },
        {
            "authors": [
                "Cheng-Han Lee",
                "Ziwei Liu",
                "Lingyun Wu",
                "Ping Luo"
            ],
            "title": "Maskgan: Towards diverse and interactive facial image manipulation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jiezhang Cao",
                "Guolei Sun",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Bee Lim",
                "Sanghyun Son",
                "Heewon Kim",
                "Seungjun Nah",
                "Kyoung Mu Lee"
            ],
            "title": "Enhanced deep residual networks for single image super-resolution",
            "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Gwo Dong Lin"
            ],
            "title": "Recent developments on the moment problem",
            "venue": "Journal of Statistical Distributions and Applications,",
            "year": 2017
        },
        {
            "authors": [
                "Wei Liu",
                "Weisi Lin"
            ],
            "title": "Additive white gaussian noise level estimation in svd domain for images",
            "venue": "IEEE Transactions on Image processing,",
            "year": 2012
        },
        {
            "authors": [
                "Xinhao Liu",
                "Masayuki Tanaka",
                "Masatoshi Okutomi"
            ],
            "title": "Single-image noise level estimation for blind denoising",
            "venue": "IEEE transactions on image processing,",
            "year": 2013
        },
        {
            "authors": [
                "Cheng Lu",
                "Kaiwen Zheng",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Maximum likelihood training for score-based diffusion odes by high order denoising score matching",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Kede Ma",
                "Zhengfang Duanmu",
                "Qingbo Wu",
                "Zhou Wang",
                "Hongwei Yong",
                "Hongliang Li",
                "Lei Zhang"
            ],
            "title": "Waterloo exploration database: New challenges for image quality assessment models",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yang Song",
                "Wenzhe Li",
                "Stefano Ermon"
            ],
            "title": "Estimating high order gradients of the data distribution by denoising",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Koichi Miyasawa"
            ],
            "title": "An empirical bayes estimator of the mean of a normal population",
            "venue": "Bull. Inst. Internat. Statist,",
            "year": 1961
        },
        {
            "authors": [
                "Wenlong Mou",
                "Yi-An Ma",
                "Martin J Wainwright",
                "Peter L Bartlett",
                "Michael I Jordan"
            ],
            "title": "Highorder langevin diffusion yields an accelerated MCMC algorithm",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Elias Nehme",
                "Omer Yair",
                "Tomer Michaeli"
            ],
            "title": "Uncertainty quantification via neural posterior principal components",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Luis Oala",
                "Cosmas Hei\u00df",
                "Jan Macdonald",
                "Maximilian M\u00e4rz",
                "Wojciech Samek",
                "Gitta Kutyniok"
            ],
            "title": "Interval neural networks: Uncertainty scores",
            "venue": "arXiv preprint arXiv:2003.11566,",
            "year": 2020
        },
        {
            "authors": [
                "Javier Portilla",
                "Vasily Strela",
                "Martin J Wainwright",
                "Eero P Simoncelli"
            ],
            "title": "Image denoising using scale mixtures of gaussians in the wavelet domain",
            "venue": "IEEE Transactions on Image processing,",
            "year": 2003
        },
        {
            "authors": [
                "Herbert Robbins"
            ],
            "title": "An empirical bayes approach to statistics",
            "venue": "In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1954-1955,",
            "year": 1956
        },
        {
            "authors": [
                "Yaniv Romano",
                "Michael Elad",
                "Peyman Milanfar"
            ],
            "title": "The little engine that could: Regularization by denoising (RED)",
            "venue": "SIAM Journal on Imaging Sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Leonid I Rudin",
                "Stanley Osher",
                "Emad Fatemi"
            ],
            "title": "Nonlinear total variation based noise removal algorithms",
            "venue": "Physica D: nonlinear phenomena,",
            "year": 1992
        },
        {
            "authors": [
                "Yousef Saad"
            ],
            "title": "Numerical methods for large eigenvalue problems: revised edition",
            "year": 2011
        },
        {
            "authors": [
                "Sotirios Sabanis",
                "Ying Zhang"
            ],
            "title": "Higher order langevin monte carlo algorithm",
            "venue": "Electronic Journal of Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Swami Sankaranarayanan",
                "Anastasios Nikolas Angelopoulos",
                "Stephen Bates",
                "Yaniv Romano",
                "Phillip Isola"
            ],
            "title": "Semantic uncertainty intervals for disentangled latent spaces",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Charles M Stein"
            ],
            "title": "Estimation of the mean of a multivariate normal distribution",
            "venue": "The Annals of Statistics,",
            "year": 1981
        },
        {
            "authors": [
                "Tom Tirer",
                "Raja Giryes"
            ],
            "title": "Image restoration by iterative denoising and backward projections",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Singanallur V Venkatakrishnan",
                "Charles A Bouman",
                "Brendt Wohlberg"
            ],
            "title": "Plug-and-play priors for model based reconstruction",
            "venue": "IEEE Global Conference on Signal and Information Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural computation,",
            "year": 2011
        },
        {
            "authors": [
                "Yinhuai Wang",
                "Jiwen Yu",
                "Jian Zhang"
            ],
            "title": "Zero-shot image restoration using denoising diffusion null-space model",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Zhang",
                "Wangmeng Zuo",
                "Yunjin Chen",
                "Deyu Meng",
                "Lei Zhang"
            ],
            "title": "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising",
            "venue": "IEEE transactions on image processing,",
            "year": 2017
        },
        {
            "authors": [
                "Kai Zhang",
                "Wangmeng Zuo",
                "Shuhang Gu",
                "Lei Zhang"
            ],
            "title": "Learning deep CNN denoiser prior for image restoration",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Kai Zhang",
                "Yawei Li",
                "Wangmeng Zuo",
                "Lei Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Plug-and-play image restoration with deep denoiser prior",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Lei Zhang",
                "Xiaolin Wu",
                "Antoni Buades",
                "Xin Li"
            ],
            "title": "Color demosaicking by local directional interpolation and nonlocal adaptive thresholding",
            "venue": "Journal of Electronic Imaging,",
            "year": 2011
        },
        {
            "authors": [
                "Yide Zhang",
                "Yinhao Zhu",
                "Evan Nichols",
                "Qingfei Wang",
                "Siyuan Zhang",
                "Cody Smith",
                "Scott Howard"
            ],
            "title": "A poisson-gaussian denoising dataset with real fluorescence microscopy images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Meng"
            ],
            "title": "2021), a similar relation links the second-order score with the second posterior moment (i.e., the posterior covariance",
            "year": 2021
        },
        {
            "authors": [
                "Meng"
            ],
            "title": "Moving on to higher-order moments, following our notations, Lemma 1 of Meng et al. (2021) states that E[\u2297x|y",
            "year": 2021
        },
        {
            "authors": [
                "e.g",
                "Chen"
            ],
            "title": "However, we find that even with the naive method described above, we get quite accurate results. Particularly, the impact of small inaccuracies in \u03c3 on our uncertainty estimation turn out to be very small. To illustrate this, we applied our method with a SwinIR model that was trained for \u03c3 = 50, on images with noise levels",
            "year": 2015
        },
        {
            "authors": [
                "Baranchuk"
            ],
            "title": "2022), a subset of CelebAMask-HQtest",
            "venue": "Lee et al",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Denoisers serve as key ingredients in solving a wide range of tasks. Indeed, along with their traditional use for noise suppression (Aharon et al., 2006; Buades et al., 2005; Dabov et al., 2007; Krull et al., 2019; Liang et al., 2021; Portilla et al., 2003; Roth & Black, 2009; Rudin et al., 1992; Zhang et al., 2017a; 2021), the last decade has seen a steady increase in their use for solving other tasks. For example, the plug-and-play method (Venkatakrishnan et al., 2013) demonstrated how a denoiser can be used in an iterative manner to solve arbitrary inverse problems (e.g., deblurring, inapinting). This approach was extended by many, and has led to state-of-the-art results on various restoration tasks (Brifman et al., 2016; Romano et al., 2017; Tirer & Giryes, 2018; Zhang et al., 2017b). Similarly, the denoising score-matching work (Vincent, 2011) showed how a denoiser can be used for constructing a generative model. This approach was later improved (Song & Ermon, 2019), and highly related ideas (originating from (Sohl-Dickstein et al., 2015)) served as the basis for diffusion models (Ho et al., 2020), which now achieve state-of-the-art results on image generation.\nMany of the uses of denoisers rely on Tweedie\u2019s formula (often attributed to Robbins (1956), Miyasawa et al. (1961), Stein (1981), and Efron (2011)) which connects the MSE-optimal denoiser for white Gaussian noise, with the score function (the gradient of the log-probability w.r.t the observations) of the data distribution. The MSE-optimal denoiser corresponds to the posterior mean of the clean signal conditioned on the noisy signal. Therefore, Tweedie\u2019s formula in fact links between the first posterior moment and the score of the data. A similar relation holds between the second posterior moment (i.e., the posterior covariance) and the second-order score (i.e., the Hessian of the log-probability w.r.t the observations) (Gribonval, 2011), which is in turn associated with the derivative (i.e., Jacobian) of the posterior moment. Recent works used this relation to quantify uncertainty in denoising (Meng et al., 2021), as well as to improve score-based generative models (Dockhorn et al., 2022; Lu et al., 2022; Meng et al., 2021; Mou et al., 2021; Sabanis & Zhang, 2019).\nIn this paper we derive a relation between higher-order posterior central moments and higher-order derivatives of the posterior mean in Gaussian denoising. Our result provides a simple mechanism that, given the MSE-optimal denoiser function and its derivatives at some input, allows determining\nthe entire posterior distribution of clean signals for that particular noisy input (under mild conditions). Additionally, we prove that a similar result holds for the posterior distribution of the projection of the denoised output onto a one-dimensional direction.\nWe leverage our results for uncertainty quantification in Gaussian denoising by employing a pretrained denoiser. Specifically, we show how our results allow computing the top eigenvectors of the posterior covariance (i.e., the posterior principal components) for any desired region of the image. We further use our results for approximating the entire posterior distribution along each posterior principal direction. As we show, this provides valuable information on the uncertainty in the restoration. Our approach uses only forward passes through the pre-trained denoiser and is thus advantageous over previous uncertainty quantification methods. In particular, it is training-free, fast, memory-efficient, and applicable to high-resolution images. We illustrate our approach with several pre-trained denoisers on multiple domains, showing its practical benefit in uncertainty visualization."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Many works studied theoretical properties of MSE-optimal denoisers for signals contaminated by additive white Gaussian noise. Perhaps the most well-known result is Tweedie\u2019s formula (Efron, 2011; Miyasawa et al., 1961; Robbins, 1956; Stein, 1981), which connects the MSE-optimal denoiser with the score function of noisy signals. Another interesting property, shown by Gribonval (2011), is that the MSE-optimal denoiser can be interpreted as a maximum-a-posteriori (MAP) estimator, but with a possibly different prior. The work most closely related to ours is that of Meng et al. (2021), who studied the estimation of high-order scores. Specifically, they derived a relation between the high-order posterior non-central moments in a Gaussian denoising task, and the high-order scores of the distribution of noisy signals. They discussed how these relations can be used for learning high-order scores of the data distribution. But due to the large memory cost of storing high-order moment tensors, and the associated computational cost during training and inference, they trained only second-order score models and only on small images (up to 32 \u00d7 32). They used these models for predicting the posterior covariance in denoising tasks, as well as for improving the mixing speed of Langevin dynamics sampling. Their result is based on a recursive relation, which they derived, between the high-order derivatives of the posterior mean and the highorder non-central moments of the posterior distribution in Gaussian denoising. Specifically, they showed that the non-central posterior moments m1,m2,m3, . . ., admit a recursion of the form mk+1 = f(mk,\u2207mk,m1). In many settings, central moments are rather preferred over their non-central counterparts. Indeed, they are more numerically stable and relate more intuitively to uncertainty quantification (being directly linked to variance, skewness, kurtosis, etc.). Unfortunately, the result of (Meng et al., 2021) does not trivially translate into a useful relation for central moments. Specifically, one could use the fact that the kth central moment, \u00b5k, can be expressed in terms of {mj}ki=1, and that each mj can be written in terms of {\u00b5i}ji=1. But naively substituting these relations into the recursion of Meng et al. (2021) leads to an expression for \u00b5k that includes all lower-order central-moments and their high-order derivatives. Here, we manage to prove a very simple recursive form for the central moments, which takes the form \u00b5k+1 = f(\u00b5k,\u2207\u00b5k,\u00b52). Another key contribution, which we present beyond the framework studied by Meng et al. (2021), relates to marginal posterior distributions along arbitrary cross-sections. Specifically, we prove that the central posterior moments of any low-dimensional projection of the signal, also satisfy a similar recursion. Importantly, we show how these relations can serve as very powerful tools for uncertainty quantification in denoising tasks.\nUncertainty quantification has drawn significant attention in the context of image restoration. Many works focused on per-pixel uncertainty prediction (Angelopoulos et al., 2022; Gal & Ghahramani, 2016; Horwitz & Hoshen, 2022; Meng et al., 2021; Oala et al., 2020), which neglects correlations between the uncertainties of different pixels in the restored image. Recently, several works forayed into more meaningful notions of uncertainty, which allow to reason about semantic variations (Kutiel et al., 2023; Sankaranarayanan et al., 2022). For example, a concurrent work by Nehme et al. (2023) presented a method for learning the posterior principal components of arbitrary inverse problems. However, all existing methods either require a pre-trained generative model with a disentangled latent space (e.g., StyleGAN (Karras et al., 2020) for face images) or, like many of their per-pixel\ncounterparts, require training. Here we present a training-free, computationally efficient, method that only requires access to a pre-trained denoiser."
        },
        {
            "heading": "3 MAIN THEORETICAL RESULT",
            "text": "We now present our main theoretical result, starting with scalar denoising and then extending the discussion to the multivariate setting. The scalar case serves two purposes. First, it provides intuition. But more importantly, the formulae for moments of orders higher than three are different for the univariate and multivariate settings, and therefore the two cases require separate treatment."
        },
        {
            "heading": "3.1 THE UNIVARIATE CASE",
            "text": "Consider the univariate denoising problem corresponding to the observation model y = x + n, (1)\nwhere x is a scalar random variable with probability density function px and the noise n \u223c N (0, \u03c32) is statistically independent of x. The goal in denoising is to provide a prediction x\u0302 of x, which is a function of the measurement y. It is well known that the predictor minimizing the MSE, E[(x\u2212 x\u0302)2], is the posterior mean of x given y. Specifically, given a particular measurement y = y, the MSEoptimal estimate is the first moment of the posterior density px|y(\u00b7|y), which we denote by\n\u00b51(y) = E[x|y = y]. (2)\nWhile optimal in the MSE sense, the posterior mean provides very partial knowledge on the possible values that x could take given that y = y. More information is encoded in higher-order moments of the posterior. For example, the posterior variance provides a measure of uncertainty about the MSE-optimal prediction, the posterior third moment provides knowledge about the skewness of the posterior distribution, and the posterior fourth moment can already reveal a bimodal behavior.\nLet us denote the higher-order posterior central moments by \u00b5k(y) = E [ (x\u2212 \u00b51(y))k \u2223\u2223 y = y] , k \u2265 2. (3) Our key result is that knowing the posterior mean function \u00b51(\u00b7) and its derivatives at y can be used to recursively compute all higher-order posterior central moments at y (see proof in App. A). Theorem 1 (Posterior moments in univariate denoising). In the scalar denoising setting of (1), the high-order posterior central moments of x given y satisfy the recursion\n\u00b52(y) = \u03c3 2 \u00b5\u20321(y), \u00b53(y) = \u03c3 2 \u00b5\u20322(y),\n\u00b5k+1(y) = \u03c3 2 \u00b5\u2032k(y) + k\u00b5k\u22121(y)\u00b52(y), k \u2265 3. (4)\nThus, \u00b5k+1(y) is uniquely determined by \u00b51(y), \u00b5\u20321(y), \u00b5 \u2032\u2032 1(y), . . . , \u00b5 (k) 1 (y).\nFigure 1 illustrates this result via a simple example. Here, the distribution of x is a mixture of two Gaussians. The left pane depicts the posterior density px|y(\u00b7|\u00b7) as well as the posterior mean function \u00b51(\u00b7). We focus on the measurement y = y\u2217, shown as a vertical dashed line, for which the posterior px|y(\u00b7|y\u2217) is bimodal (right pane). This property cannot be deduced by merely examining the MSEoptimal estimate \u00b51(y\u2217). However, this information does exist in the derivatives of \u00b51(\u00b7) at y\u2217. To demonstrate this, we numerically differentiated \u00b51(\u00b7) at y\u2217, used the first three derivatives to extract the first four posterior moments using Theorem 1, and computed the maximum entropy distribution that matches those moments (Botev & Kroese, 2011). As can be seen, this already provides a good approximation of the general shape of the posterior (dashed red line).\nTheorem 1 has several immediate implications. First, it is well known that if the moments do not grow too fast, then they uniquely determine the underlying distribution (Lin, 2017). This is the case e.g., for distributions with a compact support and is thus relevant to images, whose pixel values typically lie in [0, 1]. For such settings, Theorem 1 implies that knowing the posterior mean at the neighborhood of some point y, allows determining the entire posterior distribution for that point. A second interesting observation, is that Theorem 1 can be evoked to show that the posterior is Gaussian whenever all high-order derivatives of \u00b51(\u00b7) vanish (see proof in App. F). Corollary 1. Assume that \u00b5(k)1 (y\u2217) = 0 for all k > 1. Then the posterior px|y(\u00b7|y\u2217) is Gaussian."
        },
        {
            "heading": "3.2 THE MULTIVARIATE CASE",
            "text": "We now move on to treat the multivariate denoising problem. Here x is a random vector taking values in Rd, the noise n \u223c N (0, \u03c32Id) is a white multivariate Gaussian vector that is statistically independent of x, and the noisy observation is\ny = x+ n. (5)\nAs in the scalar setting, given a noisy measurement y = y, we are interested in the posterior distribution px|y(\u00b7|y). The MSE-optimal denoiser is, again, the first-order moment of this distribution,\n\u00b51(y) = E[x |y = y], (6) which is a d dimensional vector. The second-order central moment is the posterior covariance\n\u00b52(y) = Cov(x |y = y), (7) which is a d\u00d7 d matrix whose (i1, i2) entry is given by\n[\u00b52(y)]i1,i2 = E [(xi1 \u2212 [\u00b51(y)]i1) (xi2 \u2212 [\u00b51(y)]i2) |y = y] . (8) For any k \u2265 3, the posterior kth-order central moment is a d \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 d array with k indices (a kth order tensor), whose component at multi-index (i1, . . . , ik) is given by\n[\u00b5k(y)]i1,...,ik = E [(xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) |y = y] . (9)\nAs we now show, similarly to the scalar case, having access to the MSE-optimal denoiser and its derivatives, allows to recursively compute all higher order posterior moments (see proof in App. B). Theorem 2 (Posterior moments in multivariate denoising). Consider the multivariate denoising setting of (5) with dimension d \u2265 2. For any k \u2265 1 and any k+1 indices i1, . . . , ik+1 \u2208 {1, . . . , d}, the high-order posterior central moments of x given y satisfy the recursion\n[\u00b52(y)]i1,i2 = \u03c3 2 \u2202[\u00b51(y)]i1\n\u2202yi2 ,\n[\u00b53(y)]i1,i2,i3 = \u03c3 2 \u2202[\u00b52(y)]i1,i2\n\u2202yi3 ,\n[\u00b5k+1(y)]i1,...,ik+1 = \u03c3 2 \u2202[\u00b5k(y)]i1,...,ik\n\u2202yik+1 + k\u2211 j=1 [\u00b5k\u22121(y)]\u2113j [\u00b52(y)]ij ,ik+1 , k \u2265 3, (10)\nwhere \u2113j \u225c (i1, . . . , ij\u22121, ij+1 . . . , ik). Thus, \u00b5k+1(y) is uniquely determined by \u00b51(y) and by the derivatives up to order k of its elements with respect to the elements of the vector y.\nNote that the first line in (10) can be compactly written as\n\u00b52(y) = \u03c3 2 \u2202\u00b51(y)\n\u2202y , (11)\nAlgorithm 1 Efficient computation of posterior principal components Input: N (number of PCs), K (number of iterations), \u00b51(\u00b7) (MSE-optimal denoiser), y (noisy input), \u03c32 (noise variance), c\u226a 1 (linear approx. constant)\n1: Initialize {v(i)0 }Ni=1 \u2190 N (0, \u03c32I) 2: for k \u2190 1 to K do 3: for i\u2190 1 to N do 4: v(i)k \u2190 1c ( \u00b51(y + cv (i) k\u22121)\u2212 \u00b51(y)\n) 5: Q,R\u2190 QR DECOMPOSITION([v(1)k \u00b7 \u00b7 \u00b7 v (N) k ]) 6: [v(1)k \u00b7 \u00b7 \u00b7 v (N) k ]\u2190 Q 7: v(i) \u2190 v(i)K 8: \u03bb(i) \u2190 \u03c32c \u2225\u00b51(y + cv (i) K\u22121)\u2212 \u00b51(y)\u2225\nwhere \u2202\u00b51(y)\u2202y denotes the Jacobian of \u00b51 at y. This suggests that, in principle, the posterior covariance of an MSE-optimal denoiser could be extracted by computing the Jacobian of the model using e.g., automatic differentiation. However, in settings involving high-resolution images, even storing this Jacobian is impractical. In Sec. 4.1, we show how the top eigenvectors of \u00b52(y) (i.e., the posterior principal components) can be computed without having to ever store \u00b52(y) in memory.\nMoments of order greater than two pose an even bigger challenge, as they correspond to higherorder tensors. In fact, even if they could somehow be computed, it is not clear how they would be visualized in order to communicate the uncertainty of the prediction to a user. A practical solution could be to visualize the posterior distribution of the projection of x onto some meaningful onedimensional space. For example, one might be interested in the posterior distribution of x projected onto one of the principal components of the posterior covariance. The question, however, is how to obtain the posterior moments of the projection of x onto a deterministic d-dimensional vector v.\nLet us denote the first posterior moment of v\u22a4x (i.e., its posterior mean) by \u00b5v1 (y). This moment is given by the projection of the denoiser\u2019s output onto v,\n\u00b5v1 (y) = E [ v\u22a4x \u2223\u2223y = y] = v\u22a4E [x|y = y] = v\u22a4\u00b51(y). (12) Similarly, let us denote the kth order posterior central moment of v\u22a4x by\n\u00b5vk(y) = E [( v\u22a4x\u2212 v\u22a4\u00b51(y) )k\u2223\u2223\u2223y = y] , k \u2265 2. (13) As we show next, the scalar-valued functions {\u00b5vk(y)}\u221ek=1 satisfy a recursion similar to (4) (see proof in App. C). In Sec. 5, we use this result for uncertainty visualization. Theorem 3 (Directional posterior moments in multivariate denoising). Let v be a deterministic d-dimensional vector. Then the posterior central moments of v\u22a4x are given by the recursion\n\u00b5v2 (y) = \u03c3 2Dv\u00b5 v 1 (y),\n\u00b5v3 (y) = \u03c3 2Dv\u00b5 v 2 (y),\n\u00b5vk+1(y) = \u03c3 2Dv\u00b5 v k(y) + k\u00b5 v k\u22121(y)\u00b5 v 2 (y), k \u2265 3. (14)\nHere Dvf(y) denotes the directional derivative of a function f : Rd \u2192 R in direction v at y."
        },
        {
            "heading": "4 APPLICATION TO UNCERTAINTY VISUALIZATION",
            "text": "We now discuss the applicability of our results in the context of uncertainty visualization. We start with efficient computation of posterior principal components (PCs), and then illustrate the approximation of marginal densities along those directions."
        },
        {
            "heading": "4.1 EFFICIENT COMPUTATION OF POSTERIOR PRINCIPAL COMPONENTS",
            "text": "The top eigenvectors of the posterior covariance, \u00b52(y), capture the main modes of variation around the MSE-optimal prediction. Thus, as we illustrate below, they reveal meaningful information regarding the uncertainty of the restoration. Had we had access to the matrix \u00b52(y), computing these\ntop eigenvectors could be done using the subspace iteration method (Arbenz, 2016; Saad, 2011). This technique maintains a set ofN vectors, which are repeatedly multiplied by \u00b52(y) and orthonormalized using the QR decomposition. Unfortunately, storing the full covariance matrix is commonly impractical. To circumvent the need for doing so, we recall from (11) that \u00b52(y) corresponds to the Jacobian of the denoiser \u00b51(y). Thus, every iteration of the subspace method corresponds to a Jacobian-vector product. For neural denoisers, such products can be calculated using automatic differentiation (Dockhorn et al., 2022). However, this requires computing a backward pass through the model in each iteration, which can become computationally demanding for large images1. Instead, we propose to use the linear approximation\n\u2202\u00b51(y) \u2202y v \u2248 \u00b51(y + cv)\u2212 \u00b51(y) c , (15)\nwhich holds for any v \u2208 Rd when c \u2208 R is sufficiently small. This allows applying the subspace iteration using only forward passes through the denoiser, as summarized in Alg. 1. As we show in App. H, this approximation has a negligible effect on the calculated eigenvectors, but leads e.g., to a 6\u00d7 reduction in memory footprint for a 80\u00d792 patch with the SwinIR denoiser (Liang et al., 2021). We note that to compute the PCs for a user-chosen region of interest, all that is required is to mask out all entries of v outside that region in each iteration.\nFigure 2 illustrates this technique in the context of denoising of face images contaminated by white Gaussian noise with standard deviation \u03c3 = 122. We use the denoiser from (Baranchuk et al., 2022), which was trained as part of a DDPM model (Ho et al., 2020) on the FFHQ dataset (Karras et al., 2019). Note that here we use it as a plain denoiser (as used within a single timestep of the\n1Note that backward passes for whole images are also often avoided during training of neural denoisers. Indeed, typical training procedures use limited-sized crops.\nDDPM). We showcase examples from the CelebAMask-HQ dataset (Lee et al., 2020). As can be seen, different posterior principal components typically capture uncertainty in different localized regions of the image. Note that this approach can be applied to any region-of-interest within the image, chosen by the user at test time. This is in contrast to a model that is trained to predict a lowrank approximation of the covariance, as in (Meng et al., 2021). Such a model is inherently limited to the specific input size on which it was trained, and cannot be manipulated at test time to produce eigenvectors corresponding to some user-chosen region (cropping a patch from an eigenvector is not equivalent to computing the eigenvector of the corresponding patch in the image). In App. K we report quantitative comparisons to the naive baseline of estimating the PCs using a posterior sampler, and quantitatively evaluate the accuracy of the eigenvalues predicted by our method."
        },
        {
            "heading": "4.2 ESTIMATION OF MARGINAL DISTRIBUTIONS ALONG CHOSEN DIRECTIONS",
            "text": "A more fine-grained characterization of the posterior can be achieved by using higher-order moments along the principal directions. These can be calculated using Theorem 3, through (high-order) numerical differentiation of the one-dimensional function f(\u03b1) = v\u22a4\u00b51(y + \u03b1v) at \u03b1 = 0. Once we obtain all moments up to some order, we compute the probability distribution with maximum entropy that fits those moments. In practice, we compute derivatives up to third order, which allows us to obtain all moments up to order four.\nFigure 3 illustrates this approach on a two-dimensional Gaussian mixture example with a noise level of \u03c3 = 2. On the left, we show a heatmap corresponding to px(\u00b7), as well as a noisy input y (red point) and its corresponding MSE-optimal estimate (black point). The two axes of the ellipse are the posterior principal components computed using Alg. 1 using numerical differentiation of the closed-form expression of the denoiser (see App. E). The bottom plot on the second pane shows the function f(\u03b1) corresponding to the largest eigenvector. We numerically computed its derivatives up to order three at \u03b1 = 0 (black point), from which we estimated the moments up to order four according to Theorem 3. The top plot on that pane shows the ground-truth posterior distribution of v\u22a41 x, along with the maximum entropy distribution computed from the moments. The right half of the figure shows the same experiment only with a neural network that was trained on pairs of noisy (pink) samples and their clean (blue) counterparts. This denoiser comprises 5 layers with (100, 200, 200, 100) hidden features and SiLU (Hendrycks & Gimpel, 2016) activation units. We trained the network using Adam (Kingma & Ba, 2015) for 300 epochs, with a learning rate of 0.005.\nFigure 4 illustrates the approach on a handwritten digit from the MNIST (LeCun, 1998) dataset. Here, we train and use a simple CNN with 10 layers of 64 channels, separated by ReLU activation layers followed by batch normalization layers. As can be seen, fitting the maximum entropy distribution reveals more than just the main modes of variation, as it also reveals the likelihood of each reconstruction along that direction. It is instructive to note that although the two extreme reconstructions, \u00b51(y)\u00b1 \u221a \u03bb3v3, look realistic, they are not probable given the noisy observation. This is the\nreason their corresponding estimated posterior density is nearly zero. In App. K, we quantitatively validate the advantage of using higher-order moments for estimating the marginal distribution.\nOur theoretical analysis applies to non-blind denoising, in which \u03c3 is known. However, we empirically show in Sec. 5 and Fig. 5 that using an estimated \u03c3 is also sufficient for obtaining qualitatively plausible results. This can either be obtained from a noise estimation method (Chen et al., 2015) or even from the naive estimate \u03c3\u03022 = 1d\u2225\u00b51(y)\u2212 y\u22252, where \u00b51(y) is the output of a blind denoiser. Here we use the latter. We further discuss the impact of using an estimated \u03c3 in App. I."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We conduct experiments with our proposed approach for uncertainty visualization and marginal posterior distribution estimation on additional real data in multiple domains using different models.\nWe showcase our method on the MNIST dataset, natural images, human faces, and on images from the microscopy domain. For natural images, we use SwinIR (Liang et al., 2021) that was pre-trained on 800 DIV2K (Agustsson & Timofte, 2017) images, 2650 Flickr2k (Lim et al., 2017) images, 400 BSD500 (Arbelaez et al., 2010) images and 4,744 WED (Ma et al., 2016) images, with patch sizes 128 \u00d7 128 and window size 8 \u00d7 8. We experiment with two SwinIR models, trained separately for noise levels \u03c3 = {25, 50}, and showcase examples on test images from the CBSD68 (Martin et al., 2001) and Kodak (Franzen, 1999) datasets. For the medical and microscopy domain we use Noise2Void (Krull et al., 2019), trained and tested for blind-denoising on the FMD dataset (Zhang et al., 2019) in the unsupervised manner described by Krull et al. (2020). The FMD dataset was collected using real microscopy imaging, and as such its noise is most probably not precisely white nor Gaussian, and the noise level is unknown in essence (the ground truth images are considered as the average of 50 burst images). Accordingly, N2V is a blind-denoiser, and we have no access to the \u201creal\u201d \u03c3, therefore, for this dataset we used an estimated \u03c3 in our method, as described in Sec. 4.2.\nExamples for the different domains can be seen in Figs. 2, 4, and 5. As can be seen, in all cases, our approach captures interesting uncertainty directions. For natural images, those include cracks, wrinkles, eye colors, stripe shapes, etc. In the biological domain, visualizations reveal uncertainty in the size and morphology of cells, as well as in the (in)existence of septums. Those constitute important geometric features in cellular analysis. More examples can be found in App. L.\nOne limitation of the proposed method is that it relies on high-order numerical differentiation. As this approximation can be unstable with low-precision computation, we use double precision during the forward pass of the networks. Another method that can be used to mitigate this is to fit a low degree polynomial to f(\u03b1) = v\u22a4\u00b51(y + \u03b1v) around the point of derivation, \u03b1 = 0, and then use the smooth polynomial fit for the high-order derivatives calculation. Empirically we found the polynomial fitting to also be sensitive, highly-dependant on the choice of the polynomial degree and the fitted range. This caused bad fits even for the simple two-component GMM example, whereas the numerical derivatives approximations worked better."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Denoisers constitute fundamental ingredients in a variety of problems. In this paper we derived a relation in the denoising problem between higher-order derivatives of the posterior mean to higherorder posterior central moments. These results were then used in the application of uncertainty visualisation of pre-trained denoisers. Specifically, we proposed a method for efficiently computing the principal components of the posterior distribution, in any chosen region of an image. Additionally, we presented a scheme to use higher-order moments to estimate the full marginal distribution along any one-dimensional direction. Finally, we demonstrated our method on multiple denoisers across different domains. Our method allows examining semantic directions of uncertainty by using only pre-trained denoisers, in a fast and memory-efficient way. While the theoretical basis of our method applies only to additive white Gaussian noise, we show empirically that our method provides qualitatively satisfactory results also in blind denoising on real-world microscopy data.\nREPRODUCIBILITY STATEMENT\nAs part of the ongoing effort to make the field of deep learning more reproducible and open, we publish our code at https://hilamanor.github.io/GaussianDenoisingPosterior/. The repository includes scripts to regenerate all figures. Researchers that want to re-implement the code from scratch can use Alg. 1 and our published code as guidelines. In addition, we provide full and detailed proofs for all claims in the paper in Appendices A, B, C, E, and F of the supplementary material. Finally, we provide in Appendix D a translation from our notation to the notation of Meng et al. (2021) to allow future researchers to use both methods conveniently.\nETHICS STATEMENT\nIn many scientific and medical domains, signals are contaminated by noise, and deep learning based denoising models have emerged as popular tools for restoring such low-fidelity data. However, denoising problems are inherently ill-posed. Therefore, a system that presents users with only a single restored signal, may mislead the data-analyst, researcher, or physician into making flawed decisions. To avoid such situations, it is of utmost importance for systems to also report and conveniently visualize the uncertainties in their predictions. Such systems would be much more trustworthy and interpretable, and will thus support making credible deductions and decisions. The method we presented in this paper, can help visualize the uncertainty in a denoiser\u2019s prediction, by allowing users to explore the dominant modes of possible variations around that prediction, accompanied by their likelihood (given the noisy measurements). Such interactive denoising systems, would allow users to take into consideration other, and sometimes even more likely possibilities than e.g., the minimum MSE reconstruction that is often reported as a single solution."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "The Miriam and Aaron Gutwirth Memorial Fellowship supported the research of HM. The research of TM was supported by the Israel Science Foundation (grant no. 2318/22), by the Ollendorff Miverva Center, ECE faculty, Technion, and by a gift from Elbit. The authors are grateful to Elias Nehme, Rotem Mulayoff, and Matan Kleiner for their insightful discussions and input throughout this work, and Noa Cohen for her invaluable help with the algorithm."
        },
        {
            "heading": "A PROOF OF THEOREM 1",
            "text": "We start with the case k \u2265 2 (bottom two lines in (4)). In this case, the conditional moment \u00b5k(y) can be expressed using Bayes\u2019 formula as\n\u00b5k(y) = E [ (x\u2212 \u00b51(y))k \u2223\u2223 y = y] = \u222b (x\u2212 \u00b51(y))kpx|y(x|y)dx\n=\n\u222b (x\u2212 \u00b51(y))kpy|x(y|x)px(x)dx\npy(y)\n= (2\u03c0\u03c32)\u2212 1 2\n\u222b (x\u2212 \u00b51(y))k exp{\u2212 12\u03c32 (y \u2212 x)2}px(x)dx\npy(y) . (S1)\nDenoting the numerator by q(y) \u225c (2\u03c0\u03c32)\u2212 1 2 \u222b (x\u2212 \u00b51(y))k exp{\u2212 12\u03c32 (y \u2212 x)2}px(x)dx, we can write the derivative of \u00b5k(y) as\n\u00b5\u2032k(y) = q\u2032(y)py(y)\u2212 q(y)p\u2032y(y)\np2y(y)\n= q\u2032(y)\npy(y) \u2212 q(y) py(y)\np\u2032y(y)\npy(y)\n= q\u2032(y)\npy(y) \u2212 \u00b5k(y)\np\u2032y(y)\npy(y)\n= q\u2032(y)\npy(y) \u2212 \u00b5k(y)\nd log py(y)\ndy\n= q\u2032(y)\npy(y) \u2212 1 \u03c32 \u00b5k(y)(\u00b51(y)\u2212 y), (S2)\nwhere we used the fact that d log py(y)dy = 1 \u03c32 (\u00b51(y) \u2212 y) (see e.g., (Efron, 2011; Miyasawa et al., 1961; Stein, 1981)). The first term in this expression is given by\nq\u2032(y) py(y) =\n(2\u03c0\u03c32)\u2212 1 2 \u222b d dy [ (x\u2212 \u00b51(y))k exp{\u2212 12\u03c32 (y \u2212 x)2} ] px(x)dx\npy(y)\n= (2\u03c0\u03c32)\u2212 1 2\n\u222b ( \u2212k(x\u2212 \u00b51(y))k\u22121\u00b5\u20321(y)\u2212 (x\u2212 \u00b51(y))k 1\u03c32 (y \u2212 x) ) exp{\u2212 12\u03c32 (y \u2212 x)2}px(x)dx\npy(y)\n=\n\u222b ( \u2212k(x\u2212 \u00b51(y))k\u22121\u00b5\u20321(y)\u2212 (x\u2212 \u00b51(y))k 1\u03c32 (y \u2212 x) ) py|x(y|x)px(x)dx\npy(y)\n= \u222b ( \u2212k(x\u2212 \u00b51(y))k\u22121\u00b5\u20321(y)\u2212 (x\u2212 \u00b51(y))k 1\n\u03c32 (y \u2212 x)\n) px|y(x|y)dx\n= E [ \u2212k(x\u2212 \u00b51(y))k\u22121\u00b5\u20321(y)\u2212 (x\u2212 \u00b51(y))k 1\n\u03c32 (y\u2212 x) \u2223\u2223\u2223\u2223 y = y] . (S3) To allow unified treatment of the cases k = 2 and k > 2, let us denote\n\u03c8k(y) \u225c E [ (x\u2212 \u00b51(y))k \u2223\u2223 y = y] = {0 k = 1, \u00b5k(y) k \u2265 2.\n(S4)\nWe therefore have q\u2032(y)\npy(y) = \u2212k\u03c8k\u22121(y)\u00b5\u20321(y)\u2212\n1\n\u03c32 \u03c8k(y)y +\n1 \u03c32 E [ (x\u2212 \u00b51(y))kx \u2223\u2223 y = y] = \u2212k\u03c8k\u22121(y)\u00b5\u20321(y)\u2212 1\n\u03c32 \u03c8k(y)y +\n1\n\u03c32 E[(x\u2212 \u00b51(y))k(x\u2212 \u00b51(y) + \u00b51(y)) | y = y]\n= \u2212k\u03c8k\u22121(y)\u00b5\u20321(y)\u2212 1\n\u03c32 \u03c8k(y)y +\n1\n\u03c32 (\u03c8k+1(y) + \u03c8k(y)\u00b51(y))\n= \u2212k\u03c8k\u22121(y)\u00b5\u20321(y) + 1\n\u03c32 \u03c8k+1(y) +\n1\n\u03c32 \u03c8k(y) (\u00b51(y)\u2212 y) . (S5)\nSubstituting this back into (S2), we obtain that\n\u00b5\u2032k(y) = \u2212k\u03c8k\u22121(y)\u00b5\u20321(y) + 1\n\u03c32 \u03c8k+1(y) +\n1\n\u03c32 \u03c8k(y) (\u00b51(y)\u2212 y)\u2212\n1\n\u03c32 \u00b5k(y) (\u00b51(y)\u2212 y)\n= \u2212k\u03c8k\u22121(y)\u00b5\u20321(y) + 1\n\u03c32 \u03c8k+1(y), (S6)\nwhere we used the fact that \u03c8k(y) = \u00b5k(y) for all k \u2265 2. Now, for k = 2 this equation reads\n\u00b5\u20322(y) = 1\n\u03c32 \u00b53(y), (S7)\nand for k \u2265 3, it reads \u00b5\u2032k(y) = \u2212k\u00b5k\u22121(y)\u00b5\u20321(y) + 1\n\u03c32 \u00b5k+1(y). (S8)\nWe thus have that \u00b53(y) = \u03c3 2\u00b5\u20322(y),\n\u00b5k+1(y) = \u03c3 2\u00b5\u2032k(y) + k\u03c3 2\u00b5k\u22121(y)\u00b5 \u2032 1(y), k \u2265 3. (S9)\nNote that an equivalent expression for the last line is obtained by replacing \u03c32\u00b5\u20321(y) with \u00b52(y), as we prove below. This completes the proof for k \u2265 2. The case k = 1 can be treated similarly. Here,\n\u00b51(y) = E[x | y = y]\n= (2\u03c0\u03c32)\u2212 1 2\n\u222b x exp{\u2212 12\u03c32 (y \u2212 x)2}px(x)dx\npy(y) , (S10)\nso that we define q(y) \u225c (2\u03c0\u03c32)\u2212 1 2 \u222b x exp{\u2212 12\u03c32 (y \u2212 x)2}px(x)dx. We thus have\nq\u2032(y) py(y) =\n(2\u03c0\u03c32)\u2212 1 2 \u222b d dy [ x exp{\u2212 12\u03c32 (y \u2212 x)2} ] px(x)dx\npy(y)\n= (2\u03c0\u03c32)\u2212 1 2\n\u222b 1 \u03c32 (x\u2212 y) exp{\u2212 12\u03c32 (y \u2212 x)2}px(x)dx\npy(y)\n= 1\n\u03c32 E[x(x\u2212 y) | y = y]\n= 1 \u03c32 ( E[x2 | y = y]\u2212 \u00b51(y)y ) . (S11)\nTherefore,\n\u00b5\u20321(y) = q\u2032(y) py(y) \u2212 1 \u03c32 \u00b51(y)(\u00b51(y)\u2212 y)\n= 1 \u03c32 ( E[x2 | y = y]\u2212 \u00b51(y)y ) \u2212 1 \u03c32 \u00b51(y)(\u00b51(y)\u2212 y) = 1\n\u03c32 ( E[x2 | y = y]\u2212 \u00b521(y) ) = 1\n\u03c32 ( E[x2 | y = y]\u2212 E[x | y = y]2 ) = 1\n\u03c32 \u00b52(y), (S12)\nwhich demonstrates that\n\u00b52(y) = \u03c3 2\u00b5\u20321(y). (S13)\nThis completes the proof for k = 1."
        },
        {
            "heading": "B PROOF OF THEOREM 2",
            "text": "We begin with the case k = 1 (first line in (10)), by directly deriving the matrix form (11). Using Bayes\u2019 formula, the posterior mean \u00b51(y) can be expressed as\n\u00b51(y) = E[x|y = y]\n= \u222b Rd xpx|y(x|y)dx\n=\n\u222b Rd xpy|x(y|x)px(x)dx\npy(y)\n=\n1\n(2\u03c0\u03c32) d 2\n\u222b Rd x exp{\u2212 12\u03c32 \u2225y \u2212 x\u22252}px(x)dx\npy(y) . (S14)\nTherefore, denoting the numerator by q(y) \u225c 1 (2\u03c0\u03c32) d 2\n\u222b Rd x exp{\u2212 12\u03c32 \u2225y\u2212x\u22252}px(x)dx, we can\nwrite the Jacobian of \u00b51 at y as\n\u2202\u00b5(y)\n\u2202y =\n\u2202q(y) \u2202y py(y)\u2212 q(y) (\u2207py(y))\n\u22a4\np2y(y)\n=\n\u2202q(y) \u2202y\npy(y) \u2212 q(y) py(y) (\u2207py(y))\u22a4 py(y)\n=\n\u2202q(y) \u2202y\npy(y) \u2212 \u00b51(y) (\u2207py(y))\u22a4 py(y)\n=\n\u2202q(y) \u2202y\npy(y) \u2212 \u00b51(y) (\u2207 log py(y))\u22a4\n=\n\u2202q(y) \u2202y\npy(y) \u2212 1 \u03c32\n\u00b51(y) ( \u00b51(y) \u22a4 \u2212 y\u22a4 ) . (S15)\nHere, \u2202q(y)\u2202y \u2208 Rd\u00d7d denotes the Jacobian of q : Rd \u2192 Rd at y, and we used the fact that \u2207 log py(y) = 1\u03c32 (\u00b51(y)\u2212 y) (Efron, 2011; Miyasawa et al., 1961; Stein, 1981). The first term in (S15) can be further simplified as\n\u2202q(y) \u2202y\npy(y) =\n1\n(2\u03c0\u03c32) d 2\n\u222b Rd x exp{\u2212 12\u03c32 \u2225y \u2212 x\u22252} 1\u03c32 (x\u2212 y)\u22a4px(x)dx\npy(y)\n=\n\u222b Rd 1 \u03c32x(x\u2212 y)\u22a4py|x(x|y)px(x)dx\npy(y)\n= \u222b Rd 1 \u03c32 x(x\u2212 y)\u22a4px|y(x|y)dx = 1\n\u03c32 ( E[xx\u22a4|y = y]\u2212 E[x|y = y]y\u22a4 ) = 1\n\u03c32 ( E[xx\u22a4|y = y]\u2212 \u00b51(y)y\u22a4 ) . (S16)\nSubstituting (S16) back into (S15), we obtain\n\u2202\u00b51(y)\n\u2202y =\n1 \u03c32 ( E[xx\u22a4|y = y]\u2212 \u00b51(y)y\u22a4 ) \u2212 1 \u03c32 \u00b51(y) ( \u00b51(y) \u22a4 \u2212 y\u22a4 )\n= 1 \u03c32 ( E[xx\u22a4|y = y]\u2212 \u00b51(y)\u00b51(y)\u22a4 ) = 1\n\u03c32 ( E[xx\u22a4|y = y]\u2212 E[x|y = y]E[x|y = y]\u22a4 ) = 1\n\u03c32 Cov(x|y = y)\n= 1\n\u03c32 \u00b52(y). (S17)\nThis completes the proof for k = 1.\nWe now move on to the cases k = 2 and k \u2265 3 (second and third lines in (10)). Element (i1, . . . , ik) of the posterior kth order central moment can be expressed as\n[\u00b5k(y)]i1,...,ik = E [ (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) \u2223\u2223y = y] = 1 (2\u03c0\u03c32) d 2 \u222b Rd (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) exp{\u2212 12\u03c32 \u2225y \u2212 x\u22252}px(x)dx\npy(y)\n= q(y)\npy(y) , (S18)\nwhere q(y) \u225c 1 (2\u03c0\u03c32) d 2\n\u222b Rd (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) exp{\u2212 12\u03c32 \u2225y \u2212 x\u22252}px(x)dx.\nTherefore, for any ik+1 \u2208 {1, . . . , d}, the derivative of [\u00b5k(y)]i1,...,ik with respect to yik+1 is given by\n\u2202[\u00b5k(y)]i1,...,ik \u2202yik+1 =\n\u2202q(y) \u2202yik+1 py(y)\u2212 q(y) \u2202py(y)\u2202yik+1 p2y(y)\n=\n\u2202q(y) \u2202yik+1\npy(y) \u2212 q(y) py(y)\n\u2202py(y) \u2202yik+1\npy(y)\n=\n\u2202q(y) \u2202yik+1 py(y) \u2212 [\u00b5k(y)]i1,...,ik \u2202 log py(y) \u2202yik+1\n=\n\u2202q(y) \u2202yik+1\npy(y) \u2212 1 \u03c32 [\u00b5k(y)]i1,...,ik\n( [\u00b51(y)]ik+1 \u2212 yik+1 ) , (S19)\nwhere in the last line we used the fact that \u2207 log py(y) = 1\u03c32 (\u00b51(y)\u2212 y) (Efron, 2011; Miyasawa et al., 1961; Stein, 1981). The first term here can be written as\n\u2202q(y) \u2202yik+1 py(y) =\n1\n(2\u03c0\u03c32) d 2\n\u222b \u2202\n\u2202yik+1\n[ (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) exp{\u2212 12\u03c32 \u2225y \u2212 x\u22252} ] px(x)dx\npy(y)\n=\n\u222b \u2212\u2202[\u00b51(y)]i1\u2202yik+1 (xi2 \u2212 [\u00b51(y)]i2) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) exp{\u2212 1 2\u03c32 \u2225y \u2212 x\u22252}px(x)dx\n(2\u03c0\u03c32) d 2 py(y)\n+ \u00b7 \u00b7 \u00b7\n+\n\u222b \u2212 (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 ( xik\u22121 \u2212 [\u00b51(y)]ik\u22121 ) \u2202[\u00b51(y)]ik \u2202yik+1 exp{\u2212 12\u03c32 \u2225y \u2212 x\u22252}px(x)dx\n(2\u03c0\u03c32) d 2 py(y)\n+\n\u222b (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) 1\u03c32 ( xik+1 \u2212 yik+1 ) exp{\u2212 12\u03c32 \u2225y \u2212 x\u22252}px(x)dx\n(2\u03c0\u03c32) d 2 py(y)\n.\n(S20)\nLet us treat the cases k = 2 and k \u2265 3 separately. When k = 2, the above expression contains precisely three terms, but the first two vanish. Indeed, the first term reduces to \u2212\u2202[\u00b51(y)]i1\u2202yi3 E[xi2 \u2212 [\u00b51(y)]i2 |y = y] = \u2212 \u2202[\u00b51(y)]i1 \u2202yi3\n([\u00b51(y)]i2 \u2212 [\u00b51(y)]i2) = 0 and the second term to \u2212\u2202[\u00b51(y)]i2\u2202yi3 E[xi1 \u2212 [\u00b51(y)]i1 |y = y] = \u2212 \u2202[\u00b51(y)]i1 \u2202yi2\n([\u00b51(y)]i1 \u2212 [\u00b51(y)]i1) = 0. Therefore, when k = 2 we are left only with the last term, which simplifies to\n\u2202q(y) \u2202yi3\npy(y) =\n1 \u03c32 E [ (xi1 \u2212 [\u00b51(y)]i1) (xi2 \u2212 [\u00b51(y)]i2) (xi3 \u2212 yi3) \u2223\u2223y = y] = 1\n\u03c32 E [ (xi1 \u2212 [\u00b51(y)]i1) (xi2 \u2212 [\u00b51(y)]i2)xi3 \u2223\u2223y = y]\u2212 1 \u03c32 [\u00b52(y)]i1,i2yi3\n= 1 \u03c32 E [ (xi1 \u2212 [\u00b51(y)]i1) (xi2 \u2212 [\u00b51(y)]i2) (xi3 \u2212 [\u00b51(y)]i3 + [\u00b51(y)]i3) \u2223\u2223y = y] \u2212 1 \u03c32 [\u00b52(y)]i1,i2yi3 = 1\n\u03c32 [\u00b53(y)]i1,i2,i3 +\n1\n\u03c32 [\u00b52(y)]i1,i2 [\u00b51(y)]i3 \u2212\n1\n\u03c32 [\u00b52(y)]i1,i2yi3 (S21)\nSubstituting this back into (S19), we obtain \u2202[\u00b5k(y)]i1,i2 \u2202yik+1 = 1 \u03c32 [\u00b53(y)]i1,i2,i3 + 1 \u03c32 [\u00b52(y)]i1,i2 [\u00b51(y)]i3 \u2212 1 \u03c32 [\u00b52(y)]i1,i2yi3\n\u2212 1 \u03c32 [\u00b52(y)]i1,i2 ( [\u00b51(y)]i3 \u2212 yi3 ) = 1\n\u03c32 [\u00b53(y)]i1,i2,i3 . (S22)\nThis demonstrates that\n[\u00b53(y)]i1,i2,i3 = \u03c3 2 \u2202[\u00b5k(y)]i1,i2\n\u2202yik+1 , (S23)\nwhich completes the proof for k = 2.\nWhen k \u2265 3, none of the terms in (S20) vanish, and the expression reads \u2202q(y) \u2202yik+1\npy(y) = \u2212\n( [\u00b5k\u22121(y)]i2,...,ik\n\u2202[\u00b51(y)]i1 \u2202yik+1 + \u00b7 \u00b7 \u00b7+ [\u00b5k\u22121(y)]i1,...,ik\u22121 \u2202[\u00b51(y)]ik \u2202yik+1 ) \u2212 1 \u03c32 [\u00b5k(y)]i1,...,ik yik+1 + 1 \u03c32 E [ (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik)xik+1\n\u2223\u2223y = y] = \u2212\nd\u2211 j=1 [\u00b5k\u22121(y)]\u2113j \u2202[\u00b51(y)]ij \u2202yik+1 \u2212 1 \u03c32 [\u00b5k(y)]i1,...,ik yik+1\n+ 1 \u03c32 E [ (xi1 \u2212 [\u00b51(y)]i1) \u00b7 \u00b7 \u00b7 (xik \u2212 [\u00b51(y)]ik) ( xik+1 \u2212 [\u00b51(y)]ik+1 + [\u00b51(y)]ik+1 ) \u2223\u2223y = y] = \u2212\nk\u2211 j=1 [\u00b5k\u22121(y)]\u2113j \u2202[\u00b51(y)]ij \u2202yik+1 \u2212 1 \u03c32 [\u00b5k(y)]i1,...,ik yik+1 + 1 \u03c32 [\u00b5k+1(y)]i1,...,ik+1\n+ 1\n\u03c32 [\u00b5k(y)]i1,...,ik [\u00b51(y)]ik+1\n= \u2212 k\u2211\nj=1\n[\u00b5k\u22121(y)]\u2113j \u2202[\u00b51(y)]ij \u2202yik+1 + 1 \u03c32 [\u00b5k+1(y)]i1,...,ik+1\n+ 1\n\u03c32 [\u00b5k(y)]i1,...,ik\n( [\u00b51(y)]ik+1 \u2212 yik+1 ) , (S24)\nwhere we used the definition \u2113j \u225c (i1, . . . , ij\u22121, ij+1 . . . , ik). Substituting this expression back into (S19), we obtain\n\u2202[\u00b5k(y)]i1,...,ik \u2202yik+1\n= \u2212 k\u2211\nj=1\n[\u00b5k\u22121(y)]\u2113j \u2202[\u00b51(y)]ij \u2202yik+1 + 1 \u03c32 [\u00b5k+1(y)]i1,...,ik+1 . (S25)\nThis demonstrates that\n[\u00b5k+1(y)]i1,...,ik+1 = \u03c3 2 \u2202[\u00b5k(y)]i1,...,ik\n\u2202yik+1 + \u03c32 k\u2211 j=1 [\u00b5k\u22121(y)]\u2113j \u2202[\u00b51(y)]ij \u2202yik+1\n= \u03c32 \u2202[\u00b5k(y)]i1,...,ik\n\u2202yik+1 + k\u2211 j=1 [\u00b5k\u22121(y)]\u2113j [\u00b52(y)]ij ,ik+1 , (S26)\nwhere we used (S17). This completes the proof for k \u2265 3."
        },
        {
            "heading": "C PROOF OF THEOREM 3",
            "text": "We will use the fact that for any k \u2265 1, the posterior kth order central moment of v\u22a4x can be written explicitly by expanding brackets as\nE [( v\u22a4(x\u2212 \u00b51(y)) )k\u2223\u2223\u2223y = y] = E ( d\u2211\ni=1\nvi [x\u2212 \u00b51(y)]i )k\u2223\u2223\u2223\u2223\u2223\u2223y = y \n= d\u2211 i1=1 \u00b7 \u00b7 \u00b7 d\u2211 ik=1 vi1 . . .vikE [(xi1 \u2212 [\u00b51(y)]i1) . . . (xi1 \u2212 [\u00b51(y)]ik)|y = y]\n= d\u2211 i1=1 \u00b7 \u00b7 \u00b7 d\u2211 ik=1 vi1 . . .vik [\u00b5k(y)]i1,...,ik . (S27)\nLet us start with the second moment. From (S27), it is given by\n\u00b5v2 (y) = d\u2211 i1=1 d\u2211 i2=1 vi1vi2 [\u00b52(y)]i1,i2\n= v\u22a4\u00b52(y)v\n= \u03c32v\u22a4 \u2202\u00b51(y)\n\u2202y v\n= \u03c32\u2207y ( v\u22a4\u00b51(y) )\u22a4 v\n= \u03c32Dv ( v\u22a4\u00b51(y) ) = \u03c32Dv\u00b5 v 1 (y). (S28)\nThis proves the first line of (14).\nNext, we derive the third moment. From (S27), it is given by\n\u00b5v3 (y) = d\u2211 i1=1 d\u2211 i2=1 d\u2211 i3=1 vi1vi2vi3 [\u00b53(y)]i1,i2,i3\n= \u03c32 d\u2211\ni1=1 d\u2211 i2=1 d\u2211 i3=1 vi1vi2vi3 \u2202[\u00b52(y)]i1,i2 \u2202yi3\n= \u03c32 d\u2211\ni3=1\nvi3 \u2202 ( v\u22a4\u00b52(y)v ) \u2202yi3\n= \u03c32v\u22a4\u2207y ( v\u22a4\u00b52(y)v ) = \u03c32Dv ( v\u22a4\u00b52(y)v\n) = \u03c32Dv\u00b5 v 2 (y), (S29)\nwhere in the last line we used (S28). This proves the second line of (14).\nFinally, we derive the (k + 1)th moment for any k \u2265 3. From (S27), it is given by\n\u00b5vk+1(y) = d\u2211 i1=1 \u00b7 \u00b7 \u00b7 d\u2211 ik+1=1 vi1 . . .vik+1 [\u00b5k+1(y)]i1,...,ik+1\n= d\u2211 i1=1 \u00b7 \u00b7 \u00b7 d\u2211 ik+1=1 vi1 . . .vik+1 \u03c32 \u2202[\u00b5k(y)]i1,...,ik \u2202yik+1 + k\u2211 j=1 [\u00b5k\u22121(y)]\u2113j [\u00b52(y)]ij ,ik+1  = \u03c32\nd\u2211 ik+1=1 vik+1 \u2202 \u2202yik+1\n( d\u2211\ni1=1\n\u00b7 \u00b7 \u00b7 d\u2211\nik=1\nvi1 . . .vik [\u00b5k(y)]i1,...,ik\n) +\nk\u2211 j=1  d\u2211 i1=1 \u00b7 \u00b7 \u00b7 d\u2211 ij\u22121=1 d\u2211 ij+1=1 \u00b7 \u00b7 \u00b7 d\u2211 ik+1=1 vi1 . . .vij\u22121vij+1 . . .vik [\u00b5k\u22121(y)]\u2113j d\u2211 ij=1 d\u2211 ik+1=1 vjvik+1 [\u00b52(y)]ij ,ik+1  = \u03c32\nd\u2211 ik+1=1 vik+1 \u2202\u00b5vk(y) \u2202yik+1 + k\u2211 j=1 \u00b5vk\u22121(y)\u00b5 v 2 (y)\n= \u03c32v\u22a4\u2207y\u00b5vk(y) + k\u00b5vk\u22121(y)\u00b5v2 (y) = \u03c32Dv\u00b5 v k(y) + k\u00b5 v k\u22121(y)\u00b5 v 2 (y), (S30)\nwhere in the second line we used (10). This completes the proof of the third line of (14)."
        },
        {
            "heading": "D RELATED WORK: ESTIMATION OF HIGHER ORDER SCORES BY DENOISING",
            "text": "The work most related to ours is that of Meng et al. (2021). Here, we present their results while translating to our notation. Given a probability density py over Rd, they define the kth order score sk(y) as the tensor whose entry at multi-index (i1, i2, ..., ik) is\n[sk(y)]i1,i2,...,ik \u225c \u2202k\n\u2202yi1\u2202yi2 . . . \u2202yik log py(y), (S31)\nfor every i1, . . . , ik \u2208 {1, . . . , d}k. Using our notation, and under the assumption (5) that y is a noisy version of x \u223c px, the denoising score matching method estimates the first-order score s1(y), which is simply the gradient of the log-probability, \u2207y log py(y). This is done by using Tweedie\u2019s formula, which links s1 with the first posterior moment (the MSE-optimal denoiser) as\n\u00b51(y) = E[x|y = y] = y + \u03c32s1(y). (S32)\nAs noted by Meng et al. (2021), a similar relation links the second-order score with the second posterior moment (i.e., the posterior covariance) as\n\u00b52(y) = Cov(x|y = y) = \u03c34s2(y) + \u03c32I. (S33) Note from (S31) that s2(y) is the Hessian of the log-probability, \u22072y log py(y), or equivalently the Jacobian of the gradient of the log probability, \u2202\u2202y\u2207y log py(y). And since we have from (S32) that \u2207y log py(y) = 1\u03c32 (\u00b51(y)\u2212 y), Eq. (S33) can be equivalently written as\n[\u00b52(y)]i1,i2 = \u03c3 4 \u2202\n\u2202yi2\n[ \u00b51(y)\u2212 y\n\u03c32\n] i1 + \u03c32I = \u03c32 \u2202[\u00b51(y)]i1 \u2202yi2 . (S34)\nThis illustrates that the second-order formula of Meng et al. (2021) is equivalent to (10).\nMoving on to higher-order moments, following our notations, Lemma 1 of Meng et al. (2021) states that\nE[\u2297k+1x|y = y] = \u03c32 \u2202 \u2202y\nE[\u2297kx|y = y]+\u03c32E[\u2297kx|y = y]\u2297 ( s1(y) + y\n\u03c32\n) , \u2200k \u2265 1, (S35)\nwhere \u2297k+1x \u2208 Rdk denotes k-fold tensor multiplication. This lemma is used in Theorem 3 of Meng et al. (2021), to derive a recursion relating higher-order moments and scores. Substituting (S32), this relation can be written as\nE[\u2297k+1x|y = y] = \u03c32 \u2202 \u2202y E[\u2297kx|y = y] + E[\u2297kx|y = y]\u2297 \u00b51(y), \u2200k \u2265 1. (S36)\nDenoting the non-central posterior moment of order k by mk(y), Eq. (S36) can be written compactly as\nmk+1(y) = \u03c3 2 \u2202\n\u2202y mk(y) +mk(y)\u2297 \u00b51(y), \u2200k \u2265 1. (S37)\nWriting out the elements of mk+1(y) explicitly, this relation reads\n[mk+1(y)]i1,...,ik+1 = \u03c3 2 \u2202[mk(y)]i1,...,ik\n\u2202yik+1 + [mk(y)]i1,...,ik [\u00b51(y)]ik+1 , \u2200k \u2265 1. (S38)\nIt is interesting to compare this expression with the recursion for the central moments in Theorem 2. We see that the non-central moments satisfy a sort of one-step recursion (if we disregard the dependence on \u00b51), in the sense that mk+1 depends only on mk. In contrast, as can be seen in Theorem 2, the central moments satisfy a sort of two-step recursion (if we disregard the dependence on \u00b52), in the sense that \u00b5k+1(y) depends on both \u00b5k(y) and \u00b5k\u22121(y)."
        },
        {
            "heading": "E POSTERIOR DISTRIBUTION FOR A GAUSSIAN MIXTURE PRIOR",
            "text": "In Fig. 1 and Fig. 3, we demonstrated our approach on one-dimensional and two-dimensional Gaussian mixtures, respectively. In both cases, we showed plots of the marginal posterior distribution in the direction of the first posterior principal component, as well as the posterior mean for a particular noisy input sample. Those simulations relied on the closed-form expressions of the posterior distribution and the marginal posterior distribution along some direction for a Gaussian mixture prior. In addition, Fig. 1 and Fig. 3 also contain the maximum entropy distribution estimated using our method, which uses the numerical derivatives of the posterior mean. Here as well we used the numerical derivatives of the posterior mean function, which we computed in closed-from. We now present these closed-form expressions for completeness.\nSuppose px is a mixture of L Gaussians,\npx(x) = L\u2211 \u2113=1 \u03c0\u2113N (x;m\u2113,\u03a3\u2113). (S39)\nLet c be a random variable taking values in {1, . . . , L} with probabilities \u03c01, . . . , \u03c0L. Then we can think of x as drawn from the \u2113th Gaussian conditioned on the event that c = \u2113. Therefore,\npx|y(x|y) = L\u2211\n\u2113=1\npx|y,c(x|y, \u2113)pc|y(\u2113|y)\n= L\u2211 \u2113=1 px|y,c(x|y, \u2113) py|c(y|\u2113)pc(\u2113) py(y)\n= L\u2211 \u2113=1 N (x; m\u0304\u2113, \u03a3\u0304\u2113) \u03c1\u2113\u03c0\u2113\u2211L \u2113\u2032=1 \u03c1\u2113\u2032\u03c0\u2113\u2032 , (S40)\nwhere we denoted\n\u03c1i = N (y;mi,\u03a3i + \u03c32I), m\u0304i = \u03a3i(\u03a3i + \u03c3\n2I)\u22121(y \u2212mi) +mi, \u03a3\u0304i = \u03a3i \u2212 \u03a3i(\u03a3i + \u03c32I)\u22121\u03a3i. (S41)\nAs for the marginal posterior distribution along some direction v, it is easy to show that\npv\u22a4x|y(\u03b1|y) = L\u2211\n\u2113=1\npv\u22a4x|y,c(\u03b1|y, \u2113)pc|y(\u2113|y)\n= L\u2211 \u2113=1 pv\u22a4x|y,c(\u03b1|y, \u2113) py|c(y|\u2113)pc(\u2113) py(y)\n= L\u2211 \u2113=1 N (\u03b1;v\u22a4m\u0304\u2113,v\u22a4\u03a3\u0304\u2113v) \u03c1\u2113\u03c0\u2113\u2211L \u2113\u2032=1 \u03c1\u2113\u2032\u03c0\u2113\u2032 . (S42)"
        },
        {
            "heading": "F PROOF OF COROLLARY 1",
            "text": "We start by reminding the reader of (4) :\n\u00b52(y) = \u03c3 2 \u00b5\u20321(y), \u00b53(y) = \u03c3 2 \u00b5\u20322(y),\n\u00b5k+1(y) = \u03c3 2 \u00b5\u2032k(y) + k\u00b5k\u22121(y)\u00b52(y), k \u2265 3.\nWe will prove by complete induction that\n\u00b5 (m) k = 0 for all k \u2265 2 and m \u2265 1. (S43)\nBase Note that since for any m \u2265 2 we have \u00b5(m)1 (y\u2217) = 0, for any m \u2265 1 we have \u00b5 (m) 2 (y \u2217) = \u03c32\u00b5 (m+1) 1 (y \u2217)\n= 0\n\u00b5 (m) 3 (y \u2217) = \u03c32\u00b5 (m+1) 2 (y \u2217)\n= \u03c34\u00b5 (m+2) 1 (y \u2217)\n= 0\n\u00b5 (m) 4 (y \u2217) = \u03c32\u00b5 (m+1) 3 (y\n\u2217) + 3 \u2202m \u2202ym ( \u00b522(y) )\u2223\u2223\u2223\u2223 y=y\u2217\n(1) = \u03c32\u00b5 (m+1) 3 (y \u2217) + 3 m\u2211 l=0 ( m l ) \u00b5 (m\u2212l) 2 (y \u2217)\u00b5 (l) 2 (y \u2217)\n= \u03c32\u00b5 (m+1) 3 (y \u2217) + 3 ( \u00b5 (m) 2 (y \u2217)\u00b52(y \u2217) + \u00b7 \u00b7 \u00b7+ \u00b52(y\u2217)\u00b5(m)2 (y\u2217) ) = \u03c32\u00b5\n(m+1) 3 (y \u2217)\n= 0, (S44)\nwhere (1) results from the general Leibniz rule.\nInduction Assume that \u00b5(m)n (y\u2217) = 0 for all 4 \u2264 n < k + 1 and m \u2265 1. Then,\n\u00b5 (m) k+1(y\n\u2217) = \u2202m \u2202ym ( \u03c32 \u00b5\u2032k(y) + k\u00b5k\u22121(y)\u00b52(y) )\u2223\u2223\u2223\u2223 y=y\u2217\n= \u03c32 \u00b5 (m+1) k (y\n\u2217) + k \u2202m\n\u2202ym (\u00b5k\u22121(y)\u00b52(y)) \u2223\u2223\u2223\u2223 y=y\u2217\n(1) = \u03c32 \u00b5 (m+1) k (y \u2217) + k m\u2211 l=0 ( m l ) \u00b5 (m\u2212l) k\u22121 (y \u2217)\u00b5 (l) 2 (y \u2217)\n= \u03c32 \u00b5 (m+1) k (y \u2217) + k\u00b5 (m) k\u22121(y \u2217)\u00b52(y \u2217) + ...+ k\u00b5k\u22121(y \u2217)\u00b5 (m) 2 (y \u2217)\n(2) = 0, (S45)\nwhere for (1) the general Leibniz rule was used again, and in (2) we used our induction assumption. This concludes the induction.\nUsing (S43) we therefore obtain for all k \u2265 3 that \u00b5k+1(y \u2217) = k\u00b5k\u22121(y \u2217)\u00b52(y \u2217),\n= k(k \u2212 2)\u00b522(y\u2217)\u00b5k\u22123(y\u2217) = k(k \u2212 2)(k \u2212 4)\u00b532(y\u2217)\u00b5k\u22125(y\u2217) = ...\n=\n{ k!!\u00b5 k+1 2\n2 (y \u2217) k is odd,\n0 k is even. (S46)\nSince \u00b53(y\u2217) = \u03c32\u00b52(y\u2217) = 0 as well, the posterior moments are the same as those of a Gaussian distribution. Indeed, the central moments of a random variable z \u223c N (E[z], \u03c32) are given by\nE[(z\u2212 E[z])d] = { \u03c3d(d\u2212 1)!! d is even, 0 d is odd.\n(S47)\nTo conclude the proof, all that remains to show is moment-determinacy (i.e., that the sequence of moments uniquely determines the distribution). This is the case, since the moments of a Gaussian distribution are trivially verified to satisfy e.g., Condition (h6) of (Lin, 2017). This implies that the posterior is moment-determinate, and is Gaussian."
        },
        {
            "heading": "G EXPERIMENTAL DETAILS",
            "text": "Algorithm 1 requires three hyper-parameters as input. The first is the small constant c, which is used for the linear approximation in (15). The second is N , which is the number of principal components we seek. The last is K, which is the number of iterations to preform. In all our experiments we used c = 10\u22125 and N = 3. For the N2V experiments we used K = 100 while for the rest we used K = 50.\nFigure S1 depicts the convergence of the subspace iteration method for two different domains. For each noisy image and patch for which we find the principal components (marked in red), the plot to the right shows the convergence of the first N = 3 principal components. Specifically, for each principal component vi, we calculate its inner product with the same principal component in the previous iteration. As the graph shows, K = 50 iterations suffice for convergence."
        },
        {
            "heading": "H THE IMPACT OF THE JACOBIAN-VECTOR DOT-PRODUCT LINEAR APPROXIMATION",
            "text": "As described in Sec 4.1, Alg. 1 calls for calculating the Jacobian-vector dot-product of the denoiser. While for neural denoisers this calculation can be done via automatic differentiation, we propose using a linear approximation instead (See Eq. (15)). This can reduce the computational burden, while retaining high-accuracy in the computed eigenvectors. For example, in an experiment using SwinIR and \u03c3 = 50, the cosine similarity between the principal components computed with the approximation and those computed with automatic differentiation typically reaches around 0.97 at the 50th iteration. However, in terms of computational burden, the differences can sometimes be dramatic. For example, with the SwinIR model, when calculating one eigenvector for a patch of size 80\u00d7 92, the memory footprint using automatic differentiation reaches 12GB, while using the linear approximation method it only reaches 2GB. These differences will increase for running on larger images. A visual comparison of the resulting principal component can be found in Fig. S2.\nI THE IMPACT OF ESTIMATING \u03c3\nOur theoretical analysis is developed for non-blind denoising, and accordingly, most of our experiments conform to this setting. These include the experiments on faces (Fig. 2 and Fig. S7), on\nNoisy image Subspace iterations convergence\n0 10 20 30 40 50 0.2\n0.4\n0.6\n0.8\n1.0\nv1 v2 v3\n0 10 20 30 40 50\n0.4\n0.6\n0.8 1.0 v1 v2 v3\nFigure S1: Convergence of the subspace iteration method. In each row one noisy image is shown with a red patch marking the region for which the posterior principal components are calculated. To the right, we plot for each of the first 3 principal components the inner product between the principal component in consecutive iterations. As the graph shows, K = 50 iterations suffice to guarantee convergence in those domains.\nMNIST digits (Fig. 4), on natural images (top part of Fig. 5, S5 and S6), and the toy problem of Fig. 3. Namely, in all those experiments the noise level \u03c3 was assumed known.\nNevertheless, we show empirically that our method can also work well in the blind setting. This is the case in the real microscopy images (bottom part of Fig. 5). In this experiment, we estimated \u03c3 using the naive formula \u03c3\u03022 = 1d\u2225\u00b51(y) \u2212 y\u22252, where \u00b51(y) is the (blind) N2V denoiser. It is certainly possible to employ more advanced noise-level estimation methods in order to obtain an even more accurate estimate for \u03c3. Indeed, noise-level estimation, particularly for white Gaussian noise, has been heavily researched, and as of today state-of-the-art methods reach very-high precision (Chen et al., 2015; Khmag et al., 2018; Kokil & Pratap, 2021; Liu & Lin, 2012; Liu et al., 2013). For example, when the real \u03c3 equals 10, the error in estimating sigma is around 0.05 (see e.g., Chen et al. (2015)). However, we find that even with the naive method described above, we get quite accurate results. Particularly, the impact of small inaccuracies in \u03c3 on our uncertainty estimation turn out to be very small. To illustrate this, we applied our method with a SwinIR model that was trained for \u03c3 = 50, on images with noise levels of \u03c3 = 47.5, 52.5. This accounts for 5% errors in \u03c3, that are significantly higher than typical 0.5% errors of good noise level estimation techniques. Despite the inaccuracies in \u03c3, the eigenvectors produced using our method are quite similar, as can be seen in Fig. S3."
        },
        {
            "heading": "J USE IN NON-GAUSSIAN SETTINGS",
            "text": "In Sec. 5 we empirically show our method provides sensible results also on real microscopy images (bottom part of Fig. 5), where the noise model is not known. In this setting, the noise distribution in each pixel is likely close to Poisson-Gaussian, the noise level is unknown, and it is not even clear if the noise is completely white. However, the theory developed in this paper holds only for the nonblind Gaussian denoising case. We therefore aim to provide here intuition as to why our method can still find meaningful results for blind Gaussian denoising.\nSuppose that the observation model is y = x+ \u03c3n, (S48)\nUsing Linear Approximation Using Backpropegation\nFigure S2: The impact of the linear approximation on the calculated principal component. The first principal component calculated with SwinIR and \u03c3 = 50, using the linear approximation in Eq. (15), and using automatic differentiation (Backpropegation). Both methods achieve similar results, with a cosine similarity of 0.96 over 50 iterations. However, the linear approximation methods uses drastically less memory.\n\ud835\udf0e = 5 2 .5\n\ud835\udf0e = 5 0\n\ud835\udf0e = 4 7 .5\n\u22123 \ud835\udf06\ud835\udc56\ud835\udc97\ud835\udc56 \ud835\udf411(\ud835\udc9a) +3 \ud835\udf06\ud835\udc56\ud835\udc97\ud835\udc56\ud835\udc971Noisy Image\nFigure S3: The effect of small inaccuracies in \u03c3 on uncertainty estimation. The first principal component calculated using SwinIR, for an assumed \u03c3 = 50, for three different actual noise levels in the image.\nwhere x is a random vector taking values in Rd, the noise n \u223c N (0, Id) is a multivariate Gaussian vector that is statistically independent of x, and the noise level \u03c3 is a random variable sampled from some distribution p\u03c3. The noise level is unknown to the denoiser (all that is known is the distribution of noise levels p\u03c3).\nIn this case, the Jacobian of the MSE-optimal denoiser is given by \u2202\u00b51(y)\n\u2202y = \u2202E[x|y = y] \u2202y =\n= \u2202\n\u2202y E [E[x|y = y,\u03c3 = \u03c3]|y = y] = = E [ \u2202\n\u2202y E[x|y = y,\u03c3 = \u03c3] \u2223\u2223\u2223\u2223y = y] = = E [ Cov(x|y = y,\u03c3)\n\u03c32 \u2223\u2223\u2223\u2223y = y] , (S49) where we used the law of total expectation in the second line, and Theorem 2 in the last line. Namely, instead of Cov(x|y)\u03c32 , which we had in the Gaussian setting, here the Jacobian reveals the mean of the posterior covariance divided by \u03c32, where the mean is taken over all possible noise levels \u03c3. This matrix is a linear combination of the posterior covariances corresponding to different noise levels, so that it captures some notion of spread about the posterior mean, similarly to the regular posterior covariance that arises in the non-blind setting. Thus, intuitively, we expect that the top eigenvectors of this matrix capture meaningful uncertainty directions, similarly to the non-blind setting.\nK VALIDATION OF THE PREDICTED PRINCIPAL COMPONENTS\nIt is impossible to directly measure the quality of our estimated posterior PCs, since denoising datasets contain only one clean image x for each noisy image y. This single x is just one sample from the posterior px|y and therefore it cannot be used to extract a ground-truth posterior covariance matrix or ground-truth PCs to compare against. To validate our method beyond the controlled toyexperiment of Fig. 3, in which the ground-truth posterior distribution was known analytically and thus so were the PCs, here we provide the two following experiments.\nFirst, we employ the use of a diffusion-based posterior sampler for inverse-problems to generate many posterior samples for a noisy image y. The posterior principal components can then by extracted by performing PCA on those samples. We note that this approach is impractical for realworld applications because of its very high computational cost, and is brought here only for evaluating our method against some baseline. Indeed, when using a diffusion-based posterior sampler, each sample requires many neural function evaluations (NFEs) to generate, and many samples are needed for obtaining accurate PCs. This is while our method can faithfully extract each posterior PC with only 10 NFEs, as shown in the convergence graphs in Fig. S1.\nFor each noisy image, we generated many posterior samples using DDNM (Wang et al., 2023) and used them to calculate the PCs of the posterior. As can be seen in Fig. S4, as the number of posterior samples increases, the PCs estimated using this baseline become cleaner and more similar to our PCs. However, even with 500 samples, the PCs of this baseline do not seem to have fully converged, and generating 500 posterior samples using DDNM requires 50,000 NFEs. Therefore, for extracting e.g., 5 PCs, our method is roughly 1000\u00d7 faster than this naive approach. We further supply quantitative results in Tab. 1, over 100 randomly selected images from CelebA19 Baranchuk et al. (2022), a subset of CelebAMask-HQtest Lee et al. (2020). First, the empirical mean of the samples generated by the posterior samples should theoretically approximate the posterior mean, which is the MSE-optimal restoration. As we verify, this estimate is indeed very close to our denoiser\u2019s output, and they both achieve practically the same RMSE to the ground-truth images. Second, we compare the PCs of our method to those generated by the suggested baseline by measuring the norm of the error after projecting it onto these PCs. The larger this norm, the larger the portion of the error that these PCs account for. Mathematically, this measure is defined as \u2225V T (x\u2212 \u00b51(y))\u222522, where V is a matrix containing the PCs as columns. We compute the ratio of this norm relative to the measured MSE, and find that the mean of this measure for both methods is also very close.\n\ud835\udc97\ud835\udfd1\n\ud835\udc97\ud835\udfd0\n\ud835\udc97\ud835\udfcf\n\ud835\udc97\ud835\udfd2\n50 Samples 100 Samples 200 Samples 500 Samples 500 Samples\nRotated Ours\nFigure S4: Comparison to PCs computed by applying SVD on different numbers of posterior samples. The posterior samples were produced using DDNM Wang et al. (2023). In the second column to the right, we rotate the produced PCs to best match our estimated PCs, while constraining them to remain orthonormal (using Procrustes analysis). The fact that the rotated PCs are quite similar to our PCs, shows that both sets of PCs span similar subspaces. However, as can be seen, our PCs are more disentangled within that subspace.\nFinally, we verify the predicted eigenvalues by comparing the projected test error over the first PC, vT1 (x \u2212 \u00b51(y)), to the predicted 1st eigenvalue \u03bb1. The average of the ratio between those two quantities should theoretically be 1. For the same 100 randomly sampled face images, we found that the average of this ratio is 1.03. We also verified the predicted eigenvalues by calculating the ratio for the natural images domain. For this, we randomly selected 100 natural images from the CBSD (Martin et al., 2001), Kodak (Franzen, 1999) and McMaster (Zhang et al., 2011) datasets, and applied our algorithm using SwinIR (Liang et al., 2021). For each image we calculated the PCs on\nFigure S5: Additional examples on natural images using SwinIR (Liang et al., 2021). In each row, one of the first three PCs corresponding to the noisy image is shown on the left. On the right, images along the PC are shown above the marginal posterior distribution estimated for this direction. The principal components reveal uncertainty in delicate parts of the wall-painting, such as the thin rays of the sun, or the existence of mullions in the windows.\na 100 \u00d7 100 sized patch, located randomly within the image. For these images, the ratio computed was 0.93.\nIn addition, to quantitatively verify that the marginal posterior distributions we estimate along the PCs are accurate, we measure the negative log likelihood (NLL) of the ground-truth images projected onto those directions (lower is better). We compared this to the NLL of a Gaussian distribution defined by only the first two estimated moments. Tab. 2 provides the results for the same 100 randomly selected face images and natural images. In both cases, the NLL of our estimation is lower."
        },
        {
            "heading": "L ADDITIONAL RESULTS",
            "text": "Figures S5 and S6 provide additional results on test images from the McMaster (Zhang et al., 2011) dataset and images from ImageNet (Deng et al., 2009). In the supplementary material we attach a video showing more examples on face images, demonstrating different semantic principal components.\n\u22123 \ud835\udf06\ud835\udc56\ud835\udc97\ud835\udc56 \ud835\udf411(\ud835\udc9a) +3 \ud835\udf06\ud835\udc56\ud835\udc97\ud835\udc56\ud835\udc971 \ud835\udc972 \ud835\udc973Noisy Image\n\u22123 \ud835\udf06\ud835\udc56 \u22122 \ud835\udf06\ud835\udc56 3 \ud835\udf06\ud835\udc562 \ud835\udf06\ud835\udc560\u22121 \ud835\udf06\ud835\udc56 1 \ud835\udf06\ud835\udc56\nFigure S6: Additional examples on natural images using SwinIR (Liang et al., 2021). In each row, the first three PCs corresponding to the noisy image are shown on the left, and one is marked in blue. On the right, images along the marked PC are shown above the marginal posterior distribution estimated for this direction. The principal components catch semantic directions such as the pattern on the owl\u2019s feathers, the embroidery pattern, or the length of the Axolotl\u2019s gills.\nL.1 POLYNOMIAL FITTING EXAMPLES\nAs discussed briefly in Sec. 5, we experimented with fitting a polynomial to the function f(\u03b1) = v\u22a4\u00b51(y + \u03b1v), and using the derivatives of the polynomial at \u03b1 = 0 instead of using numerical derivatives of f(\u03b1) itself at \u03b1 = 0. Here, we provide the results of an experiment where we fit a polynomial of degree six over the range [ \u2212 \u221a \u03bbi, \u221a \u03bbi ]\nfor the ith principal component. As can be seen in Fig. S7, the marginal distribution estimates are quite smooth. Presumably, these posterior estimates are smoother than the true posterior, as the low degree polynomial smooths the directional posterior mean function.\n\u22123 \ud835\udf06\ud835\udc56\ud835\udc97\ud835\udc56 \ud835\udf411(\ud835\udc9a) +3 \ud835\udf06\ud835\udc56\ud835\udc97\ud835\udc56\ud835\udc971 \ud835\udc972 \ud835\udc973Noisy Image\n\u22123 \ud835\udf06\ud835\udc56 \u22122 \ud835\udf06\ud835\udc56 3 \ud835\udf06\ud835\udc562 \ud835\udf06\ud835\udc560\u22121 \ud835\udf06\ud835\udc56 1 \ud835\udf06\ud835\udc56\n1\n1\n1\n1\nFigure S7: Additional examples on face images, using a polynomial fit marginal distribution estimate. In each row, the first three PCs corresponding to the noisy image are shown on the left, and one is marked in blue. On the right, images along the marked PC are shown above the marginal posterior distribution estimated for this direction. The principal components highlight meaningful uncertainty, such as eyes shape or the existence of wrinkles. Note as an example in the first row how the optimal-MSE restoration is the mean of the more probable mode, depicting no hair on the forehead, and the distribution\u2019s tail, yielding the less-probable semi-translucent hair."
        }
    ],
    "year": 2024
}