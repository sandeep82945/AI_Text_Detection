{
    "abstractText": "Hard-thresholding is an important type of algorithm in machine learning that is used to solve l0 constrained optimization problems. However, the true gradient of the objective function can be difficult to access in certain scenarios, which normally can be approximated by zeroth-order (ZO) methods. The SZOHT algorithm is the only algorithm tackling l0 sparsity constraints with ZO gradients so far. Unfortunately, SZOHT has a notable limitation on the number of random directions due to the inherent conflict between the deviation of ZO gradients and the expansivity of the hard-thresholding operator. This paper approaches this problem by considering the role of variance and provides a new insight into variance reduction: mitigating the unique conflicts between ZO gradients and hard-thresholding. Under this perspective, we propose a generalized variance reduced ZO hard-thresholding algorithm as well as the generalized convergence analysis under standard assumptions. The theoretical results demonstrate the new algorithm eliminates the restrictions on the number of random directions, leading to improved convergence rates and broader applicability compared with SZOHT. Finally, we illustrate the utility of our method on a ridge regression problem as well as black-box adversarial attacks.",
    "authors": [
        {
            "affiliations": [],
            "name": "EXPANSIVITY CONTRADICTIONS"
        },
        {
            "affiliations": [],
            "name": "Xinzhe Yuan"
        },
        {
            "affiliations": [],
            "name": "William de Vazelhes"
        },
        {
            "affiliations": [],
            "name": "Bin Gu"
        },
        {
            "affiliations": [],
            "name": "Huan Xiong"
        }
    ],
    "id": "SP:7db1b2d3b85984ed41d1e883eb24a501031a31c8",
    "references": [
        {
            "authors": [
                "Krishnakumar Balasubramanian",
                "Saeed Ghadimi"
            ],
            "title": "Zeroth-order (non)-convex stochastic optimization via conditional gradient and gradient updates",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Blumensath",
                "Mike E Davies"
            ],
            "title": "Iterative hard thresholding for compressed sensing",
            "venue": "Applied and computational harmonic analysis,",
            "year": 2009
        },
        {
            "authors": [
                "Peter B\u00fchlmann",
                "Sara Van De Geer"
            ],
            "title": "Statistics for high-dimensional data: methods, theory and applications",
            "venue": "Springer Science & Business Media,",
            "year": 2011
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In 2017 ieee symposium on security and privacy (sp),",
            "year": 2017
        },
        {
            "authors": [
                "PinYu Chen",
                "Huan Zhang",
                "Yash Sharma",
                "Jinfeng Yi",
                "ChoJui Hsieh"
            ],
            "title": "Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models",
            "venue": "In Proceedings of the 10th ACM workshop on artificial intelligence and security,",
            "year": 2017
        },
        {
            "authors": [
                "Xiangyi Chen",
                "Sijia Liu",
                "Kaidi Xu",
                "Xingguo Li",
                "Xue Lin",
                "Mingyi Hong",
                "David Cox"
            ],
            "title": "Zoadamm: Zeroth-order adaptive momentum method for black-box optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Krzysztof Choromanski",
                "Aldo Pacchiano",
                "Jack Parker-Holder",
                "Yunhao Tang",
                "Deepali Jain",
                "Yuxiang Yang",
                "Atil Iscen",
                "Jasmine Hsu",
                "Vikas Sindhwani"
            ],
            "title": "Provably robust blackbox optimization for reinforcement learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "William de Vazelhes",
                "Hualin Zhang",
                "Huimin Wu",
                "Xiaotong Yuan",
                "Bin Gu"
            ],
            "title": "Zeroth-order hardthresholding: Gradient error vs. expansivity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Defazio",
                "Francis Bach",
                "Simon Lacoste-Julien"
            ],
            "title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "SM Moosavi Dezfooli",
                "F Alhussein",
                "F Omar",
                "F Pascal",
                "S Stefano"
            ],
            "title": "Analysis of universal adversarial perturbations",
            "venue": "arXiv preprint arXiv:1705.09554,",
            "year": 2017
        },
        {
            "authors": [
                "Dheeru Dua",
                "Casey Graff"
            ],
            "title": "UCI machine learning repository, 2017",
            "venue": "URL http://archive. ics.uci.edu/ml",
            "year": 2017
        },
        {
            "authors": [
                "Jianqing Fan",
                "Runze Li"
            ],
            "title": "Variable selection via nonconcave penalized likelihood and its oracle properties",
            "venue": "Journal of the American statistical Association,",
            "year": 2001
        },
        {
            "authors": [
                "X. Gao",
                "B. Jiang",
                "S. Zhang"
            ],
            "title": "On the information-adaptive variants of the admm: An iteration complexity perspective",
            "venue": "Journal of Scientific Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Bin Gu",
                "Wenhan Xian",
                "Zhouyuan Huo",
                "Cheng Deng",
                "Heng Huang"
            ],
            "title": "A unified q-memorization framework for asynchronous stochastic optimization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "T. Hofmann",
                "A. Lucchi",
                "S. Lacoste-Julien",
                "B. Mcwilliams"
            ],
            "title": "Variance reduced stochastic gradient descent with neighbors",
            "year": 2015
        },
        {
            "authors": [
                "Feihu Huang",
                "Bin Gu",
                "Zhouyuan Huo",
                "Songcan Chen",
                "Heng Huang"
            ],
            "title": "Faster gradient-free proximal stochastic methods for nonconvex nonsmooth optimization",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Prateek Jain",
                "Ambuj Tewari",
                "Purushottam Kar"
            ],
            "title": "On iterative hard thresholding methods for high-dimensional m-estimation",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Rie Johnson",
                "Tong Zhang"
            ],
            "title": "Accelerating stochastic gradient descent using predictive variance reduction",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Xingguo Li",
                "Raman Arora",
                "Han Liu",
                "Jarvis Haupt",
                "Tuo Zhao"
            ],
            "title": "Nonconvex sparse learning via stochastic optimization with progressive variance reduction",
            "venue": "arXiv preprint arXiv:1605.02711,",
            "year": 2016
        },
        {
            "authors": [
                "Sijia Liu",
                "Bhavya Kailkhura",
                "PinYu Chen",
                "Paishun Ting",
                "Shiyu Chang",
                "Lisa Amini"
            ],
            "title": "Zerothorder stochastic variance reduction for nonconvex optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Horia Mania",
                "Aurelia Guy",
                "Benjamin Recht"
            ],
            "title": "Simple random search of static linear policies is competitive for reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sahand N Negahban",
                "Pradeep Ravikumar",
                "Martin J Wainwright",
                "Bin Yu"
            ],
            "title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
            "year": 2012
        },
        {
            "authors": [
                "Yurii Nesterov",
                "Vladimir Spokoiny"
            ],
            "title": "Random gradient-free minimization of convex functions",
            "venue": "Foundations of Computational Mathematics,",
            "year": 2017
        },
        {
            "authors": [
                "Lam M Nguyen",
                "Jie Liu",
                "Katya Scheinberg",
                "Martin Tak\u00e1\u010d. Sarah"
            ],
            "title": "A novel method for machine learning problems using stochastic recursive gradient",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Nam Nguyen",
                "Deanna Needell",
                "Tina Woolf"
            ],
            "title": "Linear convergence of stochastic iterative greedy algorithms with sparse constraints",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2017
        },
        {
            "authors": [
                "Fabian Pedregosa",
                "Ga\u00ebl Varoquaux",
                "Alexandre Gramfort",
                "Vincent Michel",
                "Bertrand Thirion",
                "Olivier Grisel",
                "Mathieu Blondel",
                "Peter Prettenhofer",
                "Ron Weiss",
                "Vincent Dubourg"
            ],
            "title": "Scikit-learn: Machine learning in python",
            "venue": "Journal of machine Learning research,",
            "year": 2011
        },
        {
            "authors": [
                "Garvesh Raskutti",
                "Martin J Wainwright",
                "Bin Yu"
            ],
            "title": "Minimax rates of estimation for high-dimensional linear regression over lq-balls",
            "venue": "IEEE transactions on information theory,",
            "year": 2011
        },
        {
            "authors": [
                "Ohad Shamir"
            ],
            "title": "An optimal algorithm for bandit and zero-order convex optimization with two-point feedback",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Jie Shen",
                "Ping Li"
            ],
            "title": "A tight bound of hard thresholding",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "ChunChen Tu",
                "Paishun Ting",
                "PinYu Chen",
                "Sijia Liu",
                "Huan Zhang",
                "Jinfeng Yi",
                "ChoJui Hsieh",
                "ShinMing Cheng"
            ],
            "title": "Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Joaquin Vanschoren",
                "Jan N Van Rijn",
                "Bernd Bischl",
                "Luis Torgo"
            ],
            "title": "Openml: networked science in machine learning",
            "venue": "ACM SIGKDD Explorations Newsletter,",
            "year": 2014
        },
        {
            "authors": [
                "Xiaotong Yuan",
                "Ping Li"
            ],
            "title": "Stability and risk bounds of iterative hard thresholding",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "XiaoTong Yuan",
                "Ping Li",
                "Tong Zhang"
            ],
            "title": "Gradient hard thresholding pursuit",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2017
        },
        {
            "authors": [
                "CunHui Zhang"
            ],
            "title": "Nearly unbiased variable selection under minimax concave penalty",
            "year": 2010
        },
        {
            "authors": [
                "Pan Zhou",
                "Xiaotong Yuan",
                "Jiashi Feng"
            ],
            "title": "Efficient stochastic gradient hard thresholding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "\u21130 constrained optimization is a fundamental method in large-scale machine learning, particularly in high-dimensional problems. This approach is widely favored for achieving sparse learning. It offers numerous advantages, notably enhancing efficiency by reducing memory usage, computational demands, and environmental impact. Additionally, this constraint plays a crucial role in combatting overfitting and facilitating precise statistical estimation (Negahban et al., 2012; Raskutti et al., 2011; B\u00fchlmann and Van De Geer, 2011; Yuan and Li, 2021). In this study, we focus on the following problem:\nmin \u03b8\u2208Rd F(\u03b8) = 1 n n\u2211 i=1 fi(\u03b8), s.t. \u2225\u03b8\u22250 \u2264 k, (1)\nHere, F(\u03b8) is the (regularized) empirical risk. \u2225\u03b8\u22250 represents the number of non-zero directions. d is the dimension of \u03b8. Unfortunately, due to the \u21130 constraint, (1) becomes an NP-hard problem, rendering traditional methods unsuitable for its analysis.\nTherefore, we consider using the hard-threshold iterative algorithm (Raskutti et al., 2011; Jain et al.,"
        },
        {
            "heading": "2014; Nguyen et al., 2017b; Yuan et al., 2017), which is a widely used technique for obtaining",
            "text": "approximate solutions to NP-hard\u2019s \u21130 constrained optimization problems. Specifically, this technique alternates between the gradient step and the application of the hard threshold operator Hk(\u03b8). Operator\n\u2217Corresponding authors.\nHk(\u03b8) retains the top k elements of \u03b8 while setting all other directions to zero. The advantage of hardthresholding over its convex relaxations is that it can achieve similar precision without the need for computationally intensive adjustments, such as tuning \u21131 penalties or constraints. Hard-thresholding was first used for its full gradient form(Jain et al., 2014). Nguyen (Nguyen et al., 2017b) developed a stochastic gradient descent SGD version of hard thresholding known as StoIHT. Nevertheless, StoIHT\u2019s convergence condition is overly stringent for practical applications(Li et al., 2016). To address this issue, (Zhou et al., 2018), (Shen and Li, 2017) and (Li et al., 2016) implemented variance reduction techniques to improve the performance of StoIHT in real-world problem-solving.\nHowever, this type of stolHT is still not suitable for many problems. For example, in certain graphical modeling tasks (Blumensath and Davies, 2009), obtaining the gradient is computationally hard. Even worse, in some settings, the gradient is inaccessible by nature, for instance in bandit problems (Shamir, 2017), black-box adversarial attacks(Tu et al., 2019; Chen et al., 2017; 2019), or reinforcement learning (Salimans et al., 2017; Mania et al., 2018; Choromanski et al., 2020). To address these challenges, zeroth-order (ZO) optimization methods have been developed(Nesterov and Spokoiny, 2017). These methods commonly replace the inaccessible gradient with its finite difference approximation which can be calculated by simply using the function evaluations. Subsequently, ZO methods have been adapted to handle convex constraint sets, rendering them suitable for solving the \u21131 convex relaxation of the problem (1)(Liu et al., 2018; Balasubramanian and Ghadimi, 2018). However, it\u2019s essential to highlight that in the context of sparse optimization, \u21131 regularization or constraints can introduce substantial estimation bias and result in inferior statistical properties when compared to \u21130 regularization and constraints(Fan and Li, 2001; Zhang, 2010).\nTo tackle this issue, a recent development introduced the Stochastic Zeroth-Order Hard-Thresholding algorithm (SZOHT)(de Vazelhes et al., 2022), specifically designed for \u21130 sparsity constraints and gradient-free optimization. Unfortunately, as the only available algorithm in zeroth-order hardthresholding so far, SZOHT has notable limitations due to the inherent conflict between the deviation of ZO estimators and the expansivity of the hard-thresholding. This limitation makes the algorithm difficult to use in practice, and a natural question is proposed: Could we have a simple ZO hardthresholding algorithm whose convergence does not rely on the number of q (the number of random directions used to estimate the gradient, further defined in Section 2)?\nIn this paper, we provide a positive response to this question. Our approach centers on the role of variance in addressing this problem. We firmly believe that variance reduction can offer a dual benefit. It not only holds the potential to accelerate convergence speed but, more importantly, it can effectively mitigate the unique conflicts associated with zero-order hard-thresholding. From this perspective, SZOHT is characterized by its limitation in restricting the sampling of zero-order gradients, essentially representing an incomplete approach to variance reduction. This incompleteness leads to strict conditions for SZOHT. In contrast, we have developed better algorithms by using historical gradients to reduce variance thoroughly. We then provide the convergence and complexity analysis for the generalized variance reduce algorithm under the standard assumptions of sparse learning, which are restricted strong smoothness (RSS), and restricted strong convexity (RSC) (Nguyen et al., 2017b; Shen and Li, 2017) to retain generality. These algorithms eliminate the restrictions on zero-order gradient steps, leading to improved convergence rates and broader applicability. Crucial to our analysis is to provide how variance reduction mitigates contradictions on the parameters q and k. Finally, we demonstrate the effectiveness of our method by applying it to both ridge regression problems and black-box adversarial attacks. Our results highlight that our method can achieve competitive performance when compared to state-of-the-art methods for zeroth-order algorithms designed to enforce sparsity.\nThe majority of our work can be summarized in three parts:\n1. New Perspective on Resolving Conflicts Between Zeroth-Order Methods and HardThresholding. Our paper acknowledges the necessity of mitigating this contradiction, emphasizing the demand for a more flexible and resilient approach. By employing the perspective of variance to analyze this issue, our paper presents a more practical and effective solution.\n2. Variance Reduction: Another key innovation presented in the paper is the introduction of variance reduction. This concept provides a unique solution to \u21130-constrained zeroth-order optimization. By employing data-driven techniques to reduce variance, the paper not only\nenhances the algorithm\u2019s convergence but also expands its utility across a wider range of scenarios.\n3. General Analysis: The introduction of a general analysis framework is another contribution to the paper. This framework systematically evaluates the performance and behavior of varying variance reduced algorithms under \u21130-constraint and ZO gradient."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Throughout this paper, we use \u2225\u03b8\u2225 to denote the Euclidean norm for a vector, \u2225\u03b8\u2225\u221e to denote the maximum absolute component of that vector, and \u2225\u03b8\u22250 to denote the \u21130 norm (which is not a proper norm). The following two assumptions are widely adopted (Li et al., 2016; Nguyen et al., 2017b) and are needed in this paper.\nAssumption 1 (Restricted strong convexity (RSC) (Li et al., 2016; Nguyen et al., 2017b)). A differentiable function F is restricted \u03c1\u2212s -strongly convex at sparsity s if there exists a generic constant \u03c1\u2212s > 0 such that for any \u03b8, \u03b8 \u2032 \u2208 Rd with \u2225\u03b8 \u2212 \u03b8\u2032\u22250 \u2264 s, we have:\nF(\u03b8)\u2212F(\u03b8\u2032)\u2212 \u27e8\u2207F(\u03b8\u2032), \u03b8 \u2212 \u03b8\u2032\u27e9 \u2265 \u03c1 \u2212 s\n2 \u2225\u03b8 \u2212 \u03b8\u2032\u222522. (2)\nAssumption 2 (Restricted strong smoothness (RSS) (Li et al., 2016; Nguyen et al., 2017b)). For any i \u2208 [n], a differentiable function fi is restricted \u03c1+s -strongly smooth at sparsity level s if there exists a generic constant \u03c1+s > 0 such that for any \u03b8, \u03b8 \u2032 \u2208 Rd with \u2225\u03b8 \u2212 \u03b8\u2032\u22250 \u2264 s, we have\n\u2225\u2207fi(\u03b8)\u2212\u2207fi(\u03b8\u2032)\u2225 \u2264 \u03c1+s \u2225\u03b8 \u2212 \u03b8\u2032\u2225.\nWe assume that the objective function F(\u03b8) satisfies the RSC condition and that each component function fi(\u03b8) n i=1 satisfies the RSS condition. We also define the restricted condition number as \u03bas = \u03c1 + s /\u03c1 \u2212 s . This assumption ensures that the objective function behaves like a strongly convex and smooth function over a sparse domain, even when it is non-convex."
        },
        {
            "heading": "2.1 ZO ESTIMATE",
            "text": "Then, we give our zeroth-order gradient estimator below adopted by (de Vazelhes et al., 2022):\n\u2207\u0302f(\u03b8) = d q\u00b5 q\u2211 i=1 (f(\u03b8 + \u00b5ui)\u2212 f(\u03b8))ui, (3)\nwhere each random direction ui is a unit vector sampled uniformly from the set {u \u2208 Rd : \u2225u\u22250 \u2264 s2, \u2225u\u2225 = 1}, q is the number of random unit vectors, and \u00b5 > 0 is a constant called the smoothing radius (typically taken as small as possible, but no too small to avoid numerical errors). To obtain these vectors, we can first sample a random set of coordinates S of size s2 from [d]. Following, we sample a random vector u supported on S, in other words, uniformly sampled from the set {u \u2208 Rd : u[d]\u2212S = 0, \u2225u\u2225 = 1}. Especially, if s2 = d, the general estimator is the usual vanilla estimator with uniform smoothing on the sphere (Gao et al., 2018). Additionally, for convenience, we\ndefine I\u2217 = supp(\u03b8\u2217) as the support of \u03b8\u2217. Let \u03b8(r) be a sparse vector with \u2225\u03b8(r)\u22250 \u2264 k and support I(r) = supp(\u03b8(r)). Define, with H2k(\u00b7) the hard-thresholding operator of sparsity 2k:\nI\u0303 = supp(H2k(\u2207\u0302F(\u03b8\u2217)) \u222a supp(\u03b8\u2217).\nand let I = I(r) + I(r+1) + I\u0303. \u03b5\u00b5 = \u03c1+s 2 sd, \u03b5I = 2dq(s2+2)\n( (s\u22121)(s2\u22121) d\u22121 + 3 ) + 2, \u03b5Ic =\n2d q(s2+2)\n( s(s2\u22121)\n\u22121\n) , \u03b5abs = 2d\u03c1+s 2 ss2\nq\n( (s\u22121)(s2\u22121) d\u22121 + 1 ) + \u03c1+s 2 sd."
        },
        {
            "heading": "2.2 REVISIT OF SZOHT",
            "text": "Based on this assumption and ZO estimation, de Vazelhes proposed the SZOHT algorithm (de Vazelhes et al., 2022). The iteration relationship of this algorithm is:\n\u03b8(r+1) = Hk(\u03b8(r) \u2212 \u03b7\u2207\u0302F(\u03b8(r)))\nwhere Hk(\u00b7) is the hard-thresholding operator and \u2207\u0302F(\u03b8(r)) is ZO gradient estimate defined by (3), \u03b7 represents learning rate. SZOHT can address some ZO \u21130-constrained problems under specific conditions. However, it\u2019s important to note that the hard-thresholding operator, unlike the projection onto the \u21131 ball, lacks non-expansiveness. Consequently, it has the potential to divert the algorithm\u2019s iteration away from the desired solution. To deal with this challenge, SZOHT imposes stringent limitations on both k (hard-thresholding coefficients) and q. That is,(\n1\u2212 \u03c1 \u2212 s 2\n(4\u03b5I + 1)\u03c1 + s 2\n) k\u2217(4\u03b5I + 1) 2\u03c1+s 4\n\u03c1\u2212s 4 \u2264 k \u2264\nd\u2212 k\u2217\n2\nand\n\u2022 if s2 > 1: q \u2265 16d(s2\u22121)k \u2217\u03ba2\n(s2+2)(d\u22121)\n[ 18\u03ba2 \u2212 1 + 2 \u221a 9\u03ba2(9\u03ba2 \u2212 1) + 12 \u2212 1 2k\u2217 + 3 2 d\u22121 k\u2217(s2\u22121) ] \u2022 if s2 = 1: q \u2265 8\u03ba\n2d\u221a d k\u2217 +1\nEvidently, these conditions are exceedingly stringent and may not be suitable for numerous real-world problems. Therefore, we urgently need an algorithm with fewer constraints."
        },
        {
            "heading": "3 GENERAL ANALYSIS WITH VARIANCE",
            "text": "In this section, we will analyze the random ZO hard-thresholding algorithm from the perspective of variance and provide a positive response to the above questions. These algorithms can be described using the following general iterative expression:\n\u03b8(r+1) = Hk(\u03b8(r) \u2212 \u03b7g\u0302(r)(\u03b8(r))), (4)\nwhere g\u0302(r)(\u03b8(r)) is the generalized gradient estimate (applicable to all ZO hard-thresholding algorithms). Let \u03b1 = 1 + 2 \u221a k\u2217\u221a\nk\u2212k\u2217 . Then, we have:\nTheorem 1. Assume that each fi is (\u03c1+s\u2032 , s \u2032)\u2212RSS and that F is (\u03c1\u2212s , s)\u2212RSC. For any stochastic ZO hard-thresholding algorithm capable of expressing its iterative relationships as described in (4), we can establish the following:\nE||\u03b8(r+1) \u2212 \u03b8\u2217||22 \u2264 (1 + \u03b72\u03c1\u2212s 2 )\u03b1E||\u03b8(r) \u2212 \u03b8\u2217||22 + \u03b72\u03b1E||g\u0302 (r) I (\u03b8 (r))||22 \u2212 2\u03b7\u03b1 [ F(\u03b8(r))\u2212F(\u03b8\u2217) ] + \u03b1 n2\u03b5\u00b5\u00b5 2\n\u03c1\u2212s 2\n(5)\nRemark 1. Differing from the approach in (Yuan et al., 2017; Nguyen et al., 2017b; de Vazelhes et al., 2022), where the convergence inequality is segregated into linear convergence terms\n(represented as (1 + \u03b72\u03c1\u2212s2)\u03b1E||\u03b8(r) \u2212 \u03b8\u2217||22 in (5) and error terms (represented as \u03b1 n2\u03b5\u00b5\u00b52\n\u03c1\u2212s 2 \u2212 2\u03b7\u03b1 [ F(\u03b8(r))\u2212F(\u03b8\u2217) ] in (5), we have introduced the gradient squared term \u03b72\u03b1E||g\u0302(r)(\u03b8(r))||22 to elucidate the role of variance better. We can transform (5) into the form of (Yuan et al., 2017; Nguyen et al., 2017b; de Vazelhes et al., 2022) by establishing an upper bound for the gradient squared term, which is often feasible for specific algorithms.\nConflict analysis through variance. It is worth noting that among these three components, only the gradient squared term \u03b72\u03b1E\u2225g\u0302(r)(\u03b8(r))\u222522 encompasses both the hard-thresholding parameter (included by \u03b1) and the ZO gradient parameter (included by \u2225g\u0302(r)(\u03b8(r))\u222522). In essence, this means that the conflict between expansivity and zeroth-order error can be fully encapsulated through the gradient squared term. More importantly, when our attention is directed towards the gradient squared term, we discover that in cases where the gradient estimation is unbiased, we obtain E\u2225g\u0302(r)(\u03b8(r))\u222522 = Var\u2225g\u0302(r)(\u03b8(r))\u22252 +\n\u2225\u2225\u2207F(\u03b8(r))\u2225\u22252, which means that E\u2225g\u0302(r)(\u03b8(r))\u222522 only related to the variance of gradient estimation. This indicates that the conflict between the expansionary of hard-thresholding and ZO error is actually between hard-thresholding and the variance of gradient estimation. In SZOHT, we have g\u0302(r)(\u03b8(r)) = \u2207\u0302F(\u03b8(r)). Then, the gradient squared term becomes \u03b72\u03b1E||\u2207\u0302F(\u03b8(r))||22. In this scenario, to guarantee algorithm convergence, it becomes essential to ensure that the gradient squared term remains within a reasonable upper bound. Due to the fact that \u03b1 is already required to satisfy certain conditions (which are generated by linear convergence terms and error terms), therefore, the sampling method for ZO gradients must be restricted, which leads to a reduction in the variance. However, due to the technique of sampling used to reduce the variance, the limitation on the number q of random directions is introduced into SZOHT.\nImprovement plan. A natural idea is to use a more comprehensive variance reduction approach instead of only using sampling technique to reduce E||g\u0302(r)(\u03b8(r))||22, which could effectively alleviate the conflict between ZO estimation and hard-thresholding, ultimately enabling the design of algorithms with fewer constraints, broader applicability, and enhanced convergence speed. Based on this perspective, we have developed a generalized variance reduction ZO hard-thresholding algorithm that leverages historical gradients. We will provide a detailed explanation of this algorithm in the next section.\n4 pM-SZHT ALGORITHM FRAMEWORK\nThis section mainly presents the pM-SZHT algorithm framework along with its convergence analysis. This framework encompasses the majority of unbiased stochastic variance-reduction ZO hard-thresholding methods, providing a generalized result.Subsequently, we introduce the VR-SZHT algorithm, a special case under this framework. Additionally, we extend our discussion by introducing SARAH-ZHT (please note that the gradient estimate in this algorithm is biased) and providing its convergence analysis in the appendix.\n4.1 pM-SZHT\nWe now present our generalized algorithm to solve the target problem (1), which we name pM-SZHT (p Memorization Stochastic Zeroth-Order Hard-Thresholding). Each iteration of our algorithm is composed of two steps: (i) the gradient estimation step, and (ii) the hard thresholding step, where the gradient estimation step includes the variance reduce estimation and zeroth-order estimation. We give the full formal description of our algorithm in Algorithm (1).\nIn the gradient estimation step, we are utilizing the p-Memorization framework, which was originally proposed by Hofmann (Hofmann et al., 2015) to analyze the sequential stochastic gradient algorithm for convex and smooth optimization problems. It\u2019s worth noting that our gradient estimation can be seen as its zeroth-order variant (the zeroth-order estimation is shown in Section 2.2). Here, we select in each iteration a random index set J \u2286 [n] of memory locations to update according to:\n\u2200j \u2208 [n] : a\u0302+j := { \u2207\u0302fj(\u03b8), if j \u2208 J a\u0302j , otherwise\nsuch that any j has the same probability of p/n being updated1, where p is the number of directions updated each time (see (Hofmann et al., 2015)). The value of p set J , \u2200j, \u2211 J\u220bj P {J} \u2212 p n . Its\nprobability is determined by some specific algorithm. For example, if P {J} = 1/ ( n p ) if |J | = p, and P {J} = 0 otherwise, we obtain the p-SAGA-ZHT algorithm. If P {\u2205} = 1\u2212 pn and P {[1 : n]} = p n , we obtain a variant of the VR-SZHT algorithm from Section 4.2. Those are the ZO hard-thresholding versions of the algorithms mentioned in Hofmann et al. (2015); Gu et al. (2020).\nIn the hard thresholding step, we only keep the k largest (in magnitude) components of the current iterate \u03b8(r). This ensures that all our iterates (including the last one) are k-sparse. This hardthresholding operator has been studied for instance in (Shen and Li, 2017), and possesses several interesting properties. Firstly, it can be seen as a projection on the \u21130 ball. Second, importantly, it is not non-expansive, contrary to other operators like the soft-thresholding operator (Shen and Li, 2017).\nAlgorithm 1 Stochastic variance reduced zeroth-order Hard-Thresholding with p-Memorization (pM-SZHT)\nInput: Learning rate \u03b7, maximum number of iterations T , initial point \u03b8(0), number of random directions q, and number of coordinates to keep at each iteration k. Output: \u03b8(r). 1: for r = 1, . . . , T do 2: Update a\u0302(r\u22121) 3: Randomly sample ir \u2208 {1, 2, . . . , n} 4: g\u0302(r\u22121)(\u03b8(r\u22121)) = \u2207\u0302fir (\u03b8(r\u22121))\u2212 a\u0302 (r\u22121) ir + 1n \u2211n j=1 a\u0302 (r\u22121) j\n5: \u03b8(r) = Hk(\u03b8(r\u22121) \u2212 \u03b7g(r\u22121)(\u03b8(r\u22121))) 6: end for\nConvergence Analysis: We provide the convergence analysis of pM-SZHT, using the assumptions from Section 2, and demonstrate the correctness of the conclusions made in Section 3 by assessing whether the algorithm converges independently of q. Theorem 2. Suppose F(\u03b8) satisfies the RSC condition and that the functions {fi(\u03b8)}ni=1satisfy the RSS condition with s = 2k+k\u2217. For Algorithm 1, suppose that we run SZOHT with random supports of size s2, q random directions, a learning rate of \u03b7, and k coordinates kept at each iteration. We have:\n[EF(\u03b8(r+1))\u2212F(\u03b8\u2217)] \u2264 \u03b3[EF(\u03b8(r))\u2212F(\u03b8\u2217)] + 2L\u00b5 + Lr (6)\nhere L\u00b5 = \u03b1 n2\u03b5\u00b5\u00b5 2\n\u03c1\u2212s 2 +6\u03b1\u03b5abs\u00b5\n2+6\u03b72\u03b1Ar, Lr = \u221a s||\u2207F(\u03b8\u2217)||\u221eE||\u03b8(r)\u2212 \u03b8\u2217||2+\u03b72(3\u03b1((4\u03b5Is+\n2) + \u03b5Ic(d\u2212 k))E\u2225\u2207fit(\u03b8\u2217)\u22252\u221e), \u03b3 = ( 2\u03b2\n\u03c1\u2212s + 48\u03b72\u03b1\u03c1+s \u03b5I \u2212 2\u03b7\u03b1+ 1\u2212 p n\n) .\nRemark 2. (System error). This format of result is similar to the ones in (Yuan et al., 2017; Nguyen et al., 2017b; de Vazelhes et al., 2022), the right of (25) contains a linear convergence term \u03b3[EF(\u03b8(r))\u2212F(\u03b8\u2217)], and system error 2L\u00b5 +Lr. We note that if F has a k\u2217 -sparse unconstrained minimizer, which could happen in sparse reconstruction, or with overparameterized deep networks, then we would have \u2225\u2207F(\u03b8\u2217)\u2225\u221e = 0 and ||\u2207fir (\u03b8\u2217)||2\u221e = 0, and hence that part of the system error Lrwould vanish. In addition, we also have another system error Lr, which depends on the smoothing radius \u00b5, due to the error from the ZO estimate and the iterative method of a\u0302.\nFrom this theorem, we know that if the algorithm converges, \u03b7 needs to lie in some specific interval. Corollary 1. If\n\u03b7\u2032 \u2212 \u221a \u2206\n2(48\u03b5I\u03b1\u03c1 + s + \u03c1 \u2212 s )\n\u2264 \u03b7 \u2264 max { \u03b7\u2032 +\n\u221a \u2206\n2(48\u03b5I\u03b1\u03c1 + s + \u03c1 \u2212 s )\n, 1\n48\u03b5I\u03c1 + s\n} (7)\nalgorithm 1 converges. Here \u03b7\u2032 = \u03b1 48\u03b5I\u03b1\u03c1 + s +\u03c1 \u2212 s , \u2206 = 4\u03b12 \u2212 4(48\u03b5I\u03b1\u03c1+s + \u03c1\u2212s )(1\u2212 p n + 2 \u03c1\u2212s ). 1Originally, p-Memorization is called q-Memorization. We change it to p to avoid conflicting with random directions in zeroth order\nRemark 3. (Independence of q) When k > k\u2217, for any q > 0 the necessary condition \u2206 > 0 for (7) holds. We emphasize here that variance reduction can only make q unable to determine whether to converge, but q can still affect the convergence speed. In other words, variance reduction can mitigate the conflict, but cannot resolve it."
        },
        {
            "heading": "4.2 VR-SZHT",
            "text": "To offer a specific analysis, we introduce the VR-SZHT algorithm, which is the adaptation of the original SVRG method Johnson and Zhang (2013) to our ZO hard-thresholding setting. In addition to the previously mentioned convergence analysis, we will also provide a complexity analysis to demonstrate the advantages of this algorithm, which extend beyond existing algorithm.\nAlgorithm 2 Stochastic variance reduced zeroth-order Hard-Thresholding (VR-SZHT)\nInput: Learning rate \u03b7, maximum number of iterations T , initial point \u03b80, SVRG update frequency m, number of random directions q, and number of coordinates to keep at each iteration k. Output: \u03b8T . 1: for r = 1, . . . , T do 2: \u03b8(0) = \u03b8r\u22121; 3: \u00b5\u0302 = 1n \u2211n i=1 \u2207\u0302fi(\u03b8(0));\n4: for t = 0, 1, . . . ,m\u2212 1 do 5: Randomly sample it \u2208 {1, 2, . . . , n}; 6: Compute ZO estimate \u2207\u0302fit(\u03b8(r)), \u2207\u0302fit(\u03b8(0)); 7: \u03b8\u0304(r+1) = \u03b8(r) \u2212 \u03b7(\u2207\u0302fit(\u03b8(r))\u2212 \u2207\u0302fit(\u03b8(0)) + \u00b5\u0302)); 8: \u03b8(r+1) = Hk(\u03b8\u0304(r+1)); 9: end for\n10: \u03b8r = \u03b8(r+1), random t\u2032 \u2208 [m\u2212 1]; 11: end for\nTheorem 3. Suppose F(\u03b8) satisfies the RSC condition and that the functions {fi(\u03b8)}ni=1 satisfy the RSS condition with s = 2k + k\u2217.When \u03b7 = \u03b1\u03c1 \u2212 S\n2(48\u03b5I\u03b1\u03c1 \u2212 s \u03c1 + s +\u03c1 \u2212 s 2 ) , we have:\n\u03b4 [ F(\u03b8\u0303(r))\u2212F(\u03b8\u2217) ] \u2264 \u03b3\u2032E[F(\u03b8\u0303(r\u22121))\u2212F(\u03b8\u2217)] + L\u2032\u00b5 + L. (8)\nHere \u03b2 = (1 + \u03b72\u03c1\u2212s 2 )\u03b1, \u03b4 = \u03b2 m\u22121 \u03b2\u22121 (2\u03b7 \u2212 48\u03b5I\u03b7 2\u03c1+s )\u03b1, \u03b3 \u2032 = ( 2\u03b2 m\n\u03c1\u2212s +\n48\u03b72\u03c1+s \u03b5I\u03b1(\u03b2 m\u22121)\n\u03b2\u22121 ),\nL\u2032\u00b5 = 2\u03b2m\n\u03c1\u2212s\n\u221a s\u2225\u2207F(\u03b8\u2217)\u2225\u221eE\u2225\u03b8\u0303(r\u22121)\u2212\u03b8\u2217\u22252+6\u03b72 \u03b2 m\u22121 \u03b2\u22121 \u03b1((4\u03b5Is+2)+\u03b5Ic(d\u2212k))E||\u2207fit(\u03b8 \u2217)||2\u221e+\n3||\u2207IF(\u03b8\u2217)||22), and L\u2032 = \u03b2m\u22121 \u03b2\u22121 \u03b1(72\u03b7 2\u03b5abs\u00b5 2 + n2\u03b5\u00b5\u00b5 2\n\u03c1\u2212s 2 ).\nThis theorem is similar to Theorem 2. And it is worth noting that q is also independent in VR-SZHT, and can be found in the appendix due to space limitations.\nCorollary 2. The ZO query complexity of the algorithm is O ( [n+ \u03ba 3 \u03ba2+1 ] log ( 1 \u03b5 ) ) . And the hard-\nthresholding query complexity is O ( log( 1\u03b5 ) ) . When comparing VR-SZHT with SZOHT, where the ZO query complexity of SZOHT is O ( (k + ds2 )\u03ba 2 log (1\u03b5 ) ) and the hard-thresholding query complexity is O ( \u03ba2 log( 1\u03b5 ) ) , it becomes evident that the hard-thresholding query complexity of VR-SZHT is significantly reduced. Furthermore, as k becomes large, the ZO complexity is also reduced."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We now compare the performance of VR-SZHT, SAGA-SZHT, and SARAH-SZHT (an adaptation of the SARAH variance reduction method (Nguyen et al., 2017a) to our ZO hard-thresholding setting, for which we provide the convergence analysis in Appendix 6) with that of the following algorithms,\nin terms of IZO (iterative zeroth-order oracle, i.e. number of calls to fi) and NHT (number of hard-thresholding operations):\n\u2022 SZOHT (de Vazelhes et al., 2022): a vanilla stochastic ZO hard-thresholding algorithm. \u2022 FGZOHT: the full gradient version of SZOHT.\nRidge Regression We first consider the following ridge regression problem, where malfunctions fi are defined as follows: fi(\u03b8) = (x\u22a4i \u03b8 \u2212 yi)2 + \u03bb2 \u2225\u03b8\u2225 2 2, where \u03bb is some regularization parameter. We generate each xi randomly from a unit norm ball in Rd, and a true random model \u03b8\u2217 from a normal distribution N (0, Id\u00d7d). Each yi is defined as yi = x\u22a4i \u03b8\u2217. We set the constants of the problem as such: n = 10, d = 5, \u03bb = 0.5. Before training, we preprocess each column by subtracting its mean and dividing it by its empirical standard deviation. We run each algorithm with k = 3, q = 200, \u00b5 = 10\u22124, s2 = d = 5, and for the variance reduced algorithms, we choose m = 10. For all algorithms, the learning rate \u03b7 is found through grid-search in {0.005, 0.01, 0.05, 0.1, 0.5}: we choose the learning rates giving the lowest function value (averaged over several runs) at the end of training. We stop each algorithm once its number of IZO reaches 80,000. All curves are averaged over 3 runs, and we plot their mean and standard deviation in Figure 2. As we can observe, SZOHT converges to higher function values than other algorithms: this illustrates the advantage of the variance reduction techniques, which can allow to attain smaller function values than plain SZOHT, but at a cheaper cost than FGZOHT.\nFew Pixels Universal Adversarial Attacks Finally, we consider a few-pixel universal adversarial attacks problem. Let some classifier be trained on a dataset of images. We assume that it can only be accessed as a black box, i.e. it only returns the log probabilities of each estimated class, given an input image. This is a typical real-life scenario, where for instance the model can only be accessed through a provider\u2019s API. We seek to find a single perturbation \u03b8 \u2208 Rd, to apply to several images at once, (we denote those images by xi, i = {1, . . . , n}, and their true label as yi) to make the predicted class for those images different than their true class. Further discussion on universal perturbations can be found in (Dezfooli et al., 2017). In addition, we seek an adversarial perturbation that is sparse, to preserve as much as possible the original image. As is usual in black-box adversarial attacks, we maximize the following Carlini-Wagner loss (Carlini and Wagner, 2017; Chen et al., 2017), which encourages the prediction from the model to be different from the true class:\nfi(\u03b8) =max{Fyi(clip(xi + \u03b8))\u2212max j \u0338=yi Fj(clip(xi + \u03b8)), 0},\nwhere xi is the original i-th image (rescaled to have values in [\u22120.5, 0.5]), of true class yi, clip denotes the clipping operation into [\u22120.5, 0.5], \u03b8 is the universal perturbation that we seek to optimize, and\neach function Fk outputs the log-probability of image xi being of class k as predicted by the model, for k \u2208 {1, ..,K}, with K the number of classes (similarly to (Chen et al., 2017; Liu et al., 2018; Huang et al., 2019)). Similarly to Liu et al. (2018) (Appendix A.11), we evaluate the algorithm on a dataset of n = 10 images from the test-set of the CIFAR-10 dataset(Krizhevsky et al., 2009), of dimensionality 32\u00d7 32\u00d7 3 = 3, 072, from the same class \u2019airplane\u2019, which we display in Table 1. We take as model F a fixed neural network, already trained on the train-set of CIFAR-10, obtained from the supplementary material of (de Vazelhes et al., 2022). We set k = 60, \u00b5 = 0.001, q = 10, s2 = d = 3, 072, and the number of inner iterations of the variance reduced algorithms to m = 10. We check at each iteration the number of IZO, and we stop training if it exceeds 600. Finally, for each algorithm, we grid-search the learning rate \u03b7 in {0.001, 0.005, 0.01, 0.05}. The best learning rates (giving the curve which obtained the smallest minimum function value), are respectively: FGZOHT: 0.05, SZOHT: 0.005, VR-SZHT: 0.01, SAGA-SZHT: 0.05, SARAH-SZHT: 0.05. Our experiments are conducted on a workstation of 128 CPU cores. The training curves are presented in Figure 3: SAGA-SZHT obtains the lowest function value at the end of the training, followed by SARAH-SZHT. In terms of attack success rate, SARAH-SZHT presents the highest success rate, as it has successfully attacked 7/10 images. We provide further results, on 3 more classes (\u2019ship\u2019, \u2019bird\u2019, and \u2019dog\u2019) in the appendix, which demonstrate even further the advantage of variance reduction methods in our setting."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we introduce a novel approach to address sparse zero-order optimization problems and leverage it to enhance existing algorithms. We perform a comprehensive convergence analysis of the generalized variance reduction algorithm, showcasing how variance reduction can effectively mitigate the limitations inherent in existing algorithms. To substantiate our claims, we validate our algorithm through experiments involving ridge regression and adversarial attacks."
        },
        {
            "heading": "Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Deepali Jain, Yuxiang",
            "text": "Yang, Atil Iscen, Jasmine Hsu, and Vikas Sindhwani. Provably robust blackbox optimization for reinforcement learning. In Conference on Robot Learning, pages 683\u2013696. PMLR, 2020.\nWilliam de Vazelhes, Hualin Zhang, Huimin Wu, Xiaotong Yuan, and Bin Gu. Zeroth-order hardthresholding: Gradient error vs. expansivity. Advances in Neural Information Processing Systems, 35:22589\u201322601, 2022.\nAaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. Advances in neural information processing systems, 27, 2014."
        },
        {
            "heading": "SM Moosavi Dezfooli, F Alhussein, F Omar, F Pascal, and S Stefano. Analysis of universal",
            "text": "adversarial perturbations. arXiv preprint arXiv:1705.09554, 2017.\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive. ics.uci.edu/ml.\nJianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American statistical Association, 96(456):1348\u20131360, 2001."
        },
        {
            "heading": "X. Gao, B. Jiang, and S. Zhang. On the information-adaptive variants of the admm: An iteration",
            "text": "complexity perspective. Journal of Scientific Computing, 76(1):327\u2013363, 2018."
        },
        {
            "heading": "Bin Gu, Wenhan Xian, Zhouyuan Huo, Cheng Deng, and Heng Huang. A unified q-memorization",
            "text": "framework for asynchronous stochastic optimization. The Journal of Machine Learning Research, 21(1):7761\u20137813, 2020."
        },
        {
            "heading": "T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. Mcwilliams. Variance reduced stochastic gradient",
            "text": "descent with neighbors. Mathematics, 2015.\nFeihu Huang, Bin Gu, Zhouyuan Huo, Songcan Chen, and Heng Huang. Faster gradient-free proximal stochastic methods for nonconvex nonsmooth optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 1503\u20131510, 2019.\nPrateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-dimensional m-estimation. Advances in neural information processing systems, 27, 2014.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. Advances in neural information processing systems, 26, 2013.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nXingguo Li, Raman Arora, Han Liu, Jarvis Haupt, and Tuo Zhao. Nonconvex sparse learning via stochastic optimization with progressive variance reduction. arXiv preprint arXiv:1605.02711, 2016.\nSijia Liu, Bhavya Kailkhura, PinYu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zerothorder stochastic variance reduction for nonconvex optimization. Advances in Neural Information Processing Systems, 31, 2018.\nHoria Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is competitive for reinforcement learning. Advances in Neural Information Processing Systems, 31, 2018."
        },
        {
            "heading": "Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework",
            "text": "for high-dimensional analysis of m-estimators with decomposable regularizers. 2012.\nYurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17:527\u2013566, 2017.\nLam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak\u00e1c\u030c. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In International conference on machine learning, pages 2613\u20132621. PMLR, 2017a.\nNam Nguyen, Deanna Needell, and Tina Woolf. Linear convergence of stochastic iterative greedy algorithms with sparse constraints. IEEE Transactions on Information Theory, 63(11):6869\u20136895, 2017b."
        },
        {
            "heading": "Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier",
            "text": "Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825\u20132830, 2011.\nGarvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high-dimensional linear regression over \u2113q-balls. IEEE transactions on information theory, 57(10):6976\u20136994, 2011."
        },
        {
            "heading": "Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a",
            "text": "scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.\nOhad Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. The Journal of Machine Learning Research, 18(1):1703\u20131713, 2017.\nJie Shen and Ping Li. A tight bound of hard thresholding. The Journal of Machine Learning Research, 18(1):7650\u20137691, 2017."
        },
        {
            "heading": "ChunChen Tu, Paishun Ting, PinYu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, ChoJui Hsieh, and",
            "text": "ShinMing Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, number 01, pages 742\u2013749, 2019.\nJoaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. ACM SIGKDD Explorations Newsletter, 15(2):49\u201360, 2014.\nXiaotong Yuan and Ping Li. Stability and risk bounds of iterative hard thresholding. In International Conference on Artificial Intelligence and Statistics, pages 1702\u20131710. PMLR, 2021.\nXiaoTong Yuan, Ping Li, and Tong Zhang. Gradient hard thresholding pursuit. J. Mach. Learn. Res., 18(1):6027\u20136069, 2017.\nCunHui Zhang. Nearly unbiased variable selection under minimax concave penalty. 2010.\nPan Zhou, Xiaotong Yuan, and Jiashi Feng. Efficient stochastic gradient hard thresholding. Advances in Neural Information Processing Systems, 31, 2018."
        }
    ]
}