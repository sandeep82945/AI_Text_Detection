{
    "abstractText": "Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks pre-trained as general restoration operators. The method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. Our theoretical result analyzes its convergence to a stationary point of a global functional associated with the restoration operator. Numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively. Overall, this work offers a step forward for solving inverse problems by enabling the use of powerful pre-trained restoration models as priors.",
    "authors": [
        {
            "affiliations": [],
            "name": "AS AN"
        },
        {
            "affiliations": [],
            "name": "IMPLICIT PRIOR"
        }
    ],
    "id": "SP:d4df7ae517eb09674102bd2732783e82eab7d123",
    "references": [
        {
            "authors": [
                "H.K. Aggarwal",
                "M.P. Mani",
                "M. Jacob"
            ],
            "title": "Modl: Model-based deep learning architecture for inverse problems",
            "venue": "IEEE Trans. Med. Imag.,",
            "year": 2019
        },
        {
            "authors": [
                "E. Agustsson",
                "R. Timofte"
            ],
            "title": "Ntire 2017 challenge on single image super-resolution: Dataset and study",
            "venue": "In Proc. IEEE Conf. Comput. Vis. and Pattern Recognit. (CVPR) workshops,",
            "year": 2017
        },
        {
            "authors": [
                "R. Ahmad",
                "C.A. Bouman",
                "G.T. Buzzard",
                "S. Chan",
                "S. Liu",
                "E.T. Reehorst",
                "P. Schniter"
            ],
            "title": "Plugand-play methods for magnetic resonance imaging: Using denoisers for image recovery",
            "venue": "IEEE Sig. Process. Mag.,",
            "year": 2020
        },
        {
            "authors": [
                "H. Attouch",
                "J. Bolte",
                "B.F. Svaiter"
            ],
            "title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward\u2013backward splitting, and regularized Gauss\u2013Seidel methods",
            "venue": "Math. Program. Ser. A,",
            "year": 2013
        },
        {
            "authors": [
                "A. Beck"
            ],
            "title": "First-Order Methods in Optimization",
            "venue": "MOS-SIAM Series on Optimization",
            "year": 2017
        },
        {
            "authors": [
                "A. Beck",
                "M. Teboulle"
            ],
            "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
            "venue": "SIAM J. Imaging Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "S.A. Bigdeli",
                "M. Zwicker",
                "P. Favaro",
                "M. Jin"
            ],
            "title": "Deep mean-shift priors for image restoration",
            "venue": "Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2017
        },
        {
            "authors": [
                "A. Bora",
                "A. Jalal",
                "E. Price",
                "A.G. Dimakis"
            ],
            "title": "Compressed sensing using generative priors",
            "venue": "In Int. Conf. Mach. Learn.,",
            "year": 2017
        },
        {
            "authors": [
                "G.T. Buzzard",
                "S.H. Chan",
                "S. Sreehari",
                "C.A. Bouman"
            ],
            "title": "Plug-and-play unplugged: Optimization free reconstruction using consensus equilibrium",
            "venue": "SIAM J. Imaging Sci.,",
            "year": 2018
        },
        {
            "authors": [
                "S.H. Chan",
                "X. Wang",
                "O.A. Elgendy"
            ],
            "title": "Plug-and-play ADMM for image restoration: Fixed-point convergence and applications",
            "venue": "IEEE Trans. Comp. Imag.,",
            "year": 2017
        },
        {
            "authors": [
                "H. Chen",
                "Y. Zhang",
                "M.K. Kalra",
                "F. Lin",
                "Y. Chen",
                "P. Liao",
                "J. Zhou",
                "G. Wang"
            ],
            "title": "Low-dose CT with a residual encoder-decoder convolutional neural network",
            "venue": "IEEE Trans. Med. Imag.,",
            "year": 2017
        },
        {
            "authors": [
                "H. Chung",
                "J. Kim",
                "M.L.K.M.T. Mccann",
                "J.C. Ye"
            ],
            "title": "Diffusion posterior sampling for general noisy inverse problems",
            "venue": "In Int. Conf. on Learn. Represent.,",
            "year": 2023
        },
        {
            "authors": [
                "R. Cohen",
                "Y. Blau",
                "D. Freedman",
                "E. Rivlin"
            ],
            "title": "It has potential: Gradient-driven denoisers for convergent solutions to inverse problems",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst",
            "year": 2021
        },
        {
            "authors": [
                "R. Cohen",
                "M. Elad",
                "P. Milanfar"
            ],
            "title": "Regularization by denoising via fixed-point projection (red-pro)",
            "venue": "SIAM Journal on Imaging Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "M. Delbracio",
                "P. Milanfar"
            ],
            "title": "Inversion by direct iteration: An alternative to denoising diffusion for image restoration",
            "venue": "Trans. on Mach. Learn. Research,",
            "year": 2023
        },
        {
            "authors": [
                "M. Delbracio",
                "H. Talebei",
                "P. Milanfar"
            ],
            "title": "Projected distribution loss for image enhancement",
            "venue": "In 2021 Int. Conf. on Comput. Photography (ICCP),",
            "year": 2021
        },
        {
            "authors": [
                "W. Dong",
                "P. Wang",
                "W. Yin",
                "G. Shi",
                "F. Wu",
                "X. Lu"
            ],
            "title": "Denoising prior driven deep neural network for image restoration",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2019
        },
        {
            "authors": [
                "M. Elad",
                "P. Milanfar",
                "R. Rubinstein"
            ],
            "title": "Analysis versus synthesis in signal priors",
            "venue": "Inverse Problems,",
            "year": 2007
        },
        {
            "authors": [
                "W. Gan",
                "S. Shoushtari",
                "Y. Hu",
                "J. Liu",
                "H. An",
                "U.S. Kamilov"
            ],
            "title": "Block coordinate plug-and-play methodsfor blind inverse problems",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst",
            "year": 2023
        },
        {
            "authors": [
                "D. Gilton",
                "G. Ongie",
                "R. Willett"
            ],
            "title": "Deep equilibrium architectures for inverse problems in imaging",
            "venue": "IEEE Trans. Comput. Imag.,",
            "year": 2021
        },
        {
            "authors": [
                "R. Gribonval"
            ],
            "title": "Should penalized least squares regression be interpreted as maximum a posteriori estimation",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2011
        },
        {
            "authors": [
                "R. Gribonval",
                "P. Machart"
            ],
            "title": "Reconciling \u201cpriors",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst",
            "year": 2013
        },
        {
            "authors": [
                "R. Gribonval",
                "M. Nikolova"
            ],
            "title": "On Bayesian estimation and proximity operators",
            "venue": "Appl. Comput. Harmon. Anal.,",
            "year": 2021
        },
        {
            "authors": [
                "A. Hauptmann",
                "F. Lucka",
                "M. Betcke",
                "N. Huynh",
                "J. Adler",
                "B. Cox",
                "P. Beard",
                "S. Ourselin",
                "S. Arridge"
            ],
            "title": "Model-based learning for accelerated, limited-view 3-d photoacoustic tomography",
            "venue": "IEEE Trans. Med. Imag.,",
            "year": 2018
        },
        {
            "authors": [
                "S. Hurault",
                "A. Leclaire",
                "N. Papadakis"
            ],
            "title": "Gradient step denoiser for convergent plug-and-play",
            "venue": "In Int. Conf. on Learn. Represent.,",
            "year": 2022
        },
        {
            "authors": [
                "S. Hurault",
                "A. Leclaire",
                "N. Papadakis"
            ],
            "title": "Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization",
            "venue": "In Int. Conf. Mach. Learn.,",
            "year": 2022
        },
        {
            "authors": [
                "K.H. Jin",
                "M.T. McCann",
                "E. Froustey",
                "M. Unser"
            ],
            "title": "Deep convolutional neural network for inverse problems in imaging",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Kadkhodaie",
                "E.P. Simoncelli"
            ],
            "title": "Stochastic solutions for linear inverse problems using the prior implicit in a denoiser",
            "venue": "In Proc. Adv.Neural Inf. Process. Syst",
            "year": 2021
        },
        {
            "authors": [
                "U.S. Kamilov",
                "C.A. Bouman",
                "G.T. Buzzard",
                "B. Wohlberg"
            ],
            "title": "Plug-and-play methods for integrating physical and learned models in computational imaging",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2023
        },
        {
            "authors": [
                "E. Kang",
                "J. Min",
                "J.C. Ye"
            ],
            "title": "A deep convolutional neural network using directional wavelets for low-dose x-ray CT reconstruction",
            "venue": "Med. Phys.,",
            "year": 2017
        },
        {
            "authors": [
                "R. Laumont",
                "V. De Bortoli",
                "A. Almansa",
                "J. Delon",
                "A. Durmus",
                "M. Pereyra"
            ],
            "title": "Bayesian imaging using plug & play priors: When Langevin meets Tweedie",
            "venue": "SIAM J. Imaging Sci.,",
            "year": 2022
        },
        {
            "authors": [
                "J. Liang",
                "J. Cao",
                "G. Sun",
                "K. Zhang",
                "L. Van G",
                "R. Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "In Proc. IEEE Int. Conf. Comp. Vis. (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "B. Lim",
                "S. Son",
                "H. Kim",
                "S. Nah",
                "K. Mu L"
            ],
            "title": "Enhanced deep residual networks for single image super-resolution",
            "venue": "In Proc. IEEE Conf. Comput. Vis. and Pattern Recognit. (CVPR) workshops,",
            "year": 2017
        },
        {
            "authors": [
                "J. Liu",
                "Y. Sun",
                "C. Eldeniz",
                "W. Gan",
                "H. An",
                "U.S. Kamilov"
            ],
            "title": "RARE: Image reconstruction using deep priors learned without ground truth",
            "venue": "IEEE J. Sel. Topics Signal Process.,",
            "year": 2020
        },
        {
            "authors": [
                "J. Liu",
                "S. Asif",
                "B. Wohlberg",
                "U.S. Kamilov"
            ],
            "title": "Recovery analysis for plug-and-play priors using the restricted eigenvalue condition",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst",
            "year": 2021
        },
        {
            "authors": [
                "J. Liu",
                "X. Xu",
                "W. Gan",
                "S. Shoushtari",
                "U.S. Kamilov"
            ],
            "title": "Online deep equilibrium learning for regularization by denoising",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "A. Lucas",
                "M. Iliadis",
                "R. Molina",
                "A.K. Katsaggelos"
            ],
            "title": "Using deep neural networks for inverse problems in imaging: Beyond analytical methods",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2018
        },
        {
            "authors": [
                "M.T. McCann",
                "K.H. Jin",
                "M. Unser"
            ],
            "title": "Convolutional neural networks for inverse problems in imaging: A review",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2017
        },
        {
            "authors": [
                "T. Meinhardt",
                "M. Moeller",
                "C. Hazirbas",
                "D. Cremers"
            ],
            "title": "Learning proximal operators: Using denoising networks for regularizing inverse imaging problems",
            "venue": "In Proc. IEEE Int. Conf. Comp. Vis.,",
            "year": 2017
        },
        {
            "authors": [
                "C. Metzler",
                "P. Schniter",
                "A. Veeraraghavan",
                "R. Baraniuk"
            ],
            "title": "prDeep: Robust phase retrieval with a flexible deep network",
            "venue": "In Proc. 36th Int. Conf. Mach. Learn.,",
            "year": 2018
        },
        {
            "authors": [
                "K. Miyasawa"
            ],
            "title": "An empirical bayes estimator of the mean of a normal population",
            "venue": "Bull. Inst. Internat. Statist,",
            "year": 1961
        },
        {
            "authors": [
                "V. Monga",
                "Y. Li",
                "Y.C. Eldar"
            ],
            "title": "Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Nesterov"
            ],
            "title": "Introductory Lectures on Convex Optimization: A Basic Course",
            "venue": "Kluwer Academic Publishers,",
            "year": 2004
        },
        {
            "authors": [
                "G. Ongie",
                "A. Jalal",
                "C.A. Metzler",
                "R.G. Baraniuk",
                "A.G. Dimakis",
                "R. Willett"
            ],
            "title": "Deep learning techniques for inverse problems in imaging",
            "venue": "IEEE J. Sel. Areas Inf. Theory,",
            "year": 2020
        },
        {
            "authors": [
                "E.T. Reehorst",
                "P. Schniter"
            ],
            "title": "Regularization by denoising: Clarifications and new interpretations",
            "venue": "IEEE Trans. Comput. Imag.,",
            "year": 2019
        },
        {
            "authors": [
                "H. Robbins"
            ],
            "title": "An empirical Bayes approach to statistics",
            "venue": "Proc. Third Berkeley Symp. on Math. Statist. and Prob.,",
            "year": 1956
        },
        {
            "authors": [
                "Y. Romano",
                "M. Elad",
                "P. Milanfar"
            ],
            "title": "The little engine that could: Regularization by denoising (RED)",
            "venue": "SIAM J. Imaging Sci.,",
            "year": 2017
        },
        {
            "authors": [
                "E.K. Ryu",
                "J. Liu",
                "S. Wang",
                "X. Chen",
                "Z. Wang",
                "W. Yin"
            ],
            "title": "Plug-and-play methods provably converge with properly trained denoisers",
            "venue": "In Proc. 36th Int. Conf. Mach. Learn.,",
            "year": 2019
        },
        {
            "authors": [
                "I.W. Selesnick",
                "M.A.T. Figueiredo"
            ],
            "title": "Signal restoration with overcomplete wavelet transforms: comparison of analysis and synthesis priors",
            "venue": "In Proc. SPIE 7446,",
            "year": 2009
        },
        {
            "authors": [
                "S. Sreehari",
                "S.V. Venkatakrishnan",
                "B. Wohlberg",
                "G.T. Buzzard",
                "L.F. Drummy",
                "J.P. Simmons",
                "C.A. Bouman"
            ],
            "title": "Plug-and-play priors for bright field electron tomography and sparse interpolation",
            "venue": "IEEE Trans. Comput. Imaging,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Sun",
                "B. Wohlberg",
                "U.S. Kamilov"
            ],
            "title": "An online plug-and-play algorithm for regularized image reconstruction",
            "venue": "IEEE Trans. Comput. Imag.,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Sun",
                "S. Xu",
                "Y. Li",
                "L. Tian",
                "B. Wohlberg",
                "U.S. Kamilov"
            ],
            "title": "Regularized Fourier ptychography using an online plug-and-play algorithm",
            "venue": "In Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process. (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Y. Sun",
                "Z. Wu",
                "B. Wohlberg",
                "U.S. Kamilov"
            ],
            "title": "Scalable plug-and-play ADMM with convergence guarantees",
            "venue": "IEEE Trans. Comput. Imag.,",
            "year": 2021
        },
        {
            "authors": [
                "A.M. Teodoro",
                "J.M. Bioucas-Dias",
                "M.A.T. Figueiredo"
            ],
            "title": "A convergent image fusion algorithm using scene-adapted Gaussian-mixture-based denoising",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2019
        },
        {
            "authors": [
                "T. Tirer",
                "R. Giryes"
            ],
            "title": "Image restoration by iterative denoising and backward projections",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2019
        },
        {
            "authors": [
                "S.V. Venkatakrishnan",
                "C.A. Bouman",
                "B. Wohlberg"
            ],
            "title": "Plug-and-play priors for model based reconstruction",
            "venue": "In Proc. IEEE Global Conf. Signal Process. and Inf. Process.,",
            "year": 2013
        },
        {
            "authors": [
                "S. Wang",
                "Z. Su",
                "L. Ying",
                "X. Peng",
                "S. Zhu",
                "F. Liang",
                "D. Feng",
                "D. Liang"
            ],
            "title": "Accelerating magnetic resonance imaging via deep learning",
            "venue": "In Proc. Int. Symp. Biomedical Imaging,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "J. Yu",
                "J. Zhang"
            ],
            "title": "Zero-shot image restoration using denoising diffusion null-space model",
            "venue": "arXiv preprint arXiv:2212.00490,",
            "year": 2022
        },
        {
            "authors": [
                "K. Wei",
                "A. Aviles-Rivero",
                "J. Liang",
                "Y. Fu",
                "C.-B. Sch\u00f6nlieb",
                "H. Huang"
            ],
            "title": "Tuning-free plug-andplay proximal algorithm for inverse imaging problems",
            "venue": "In Proc. 37th Int. Conf. Mach. Learn.,",
            "year": 2020
        },
        {
            "authors": [
                "X. Xu",
                "Y. Sun",
                "J. Liu",
                "B. Wohlberg",
                "U.S. Kamilov"
            ],
            "title": "Provable convergence of plug-and-play priors with mmse denoisers",
            "venue": "IEEE Signal Process. Lett.,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Yue",
                "Q. Zhao",
                "J. Xie",
                "L. Zhang",
                "D. Meng",
                "K. K Wong"
            ],
            "title": "Blind image super-resolution with elaborate degradation modeling on noise and kernel",
            "venue": "In Proc. IEEE Conf. Comput. Vis. and Pattern Recognit. (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "B. Ghanem"
            ],
            "title": "ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing",
            "venue": "In Proc. IEEE Conf. Comput. Vision Pattern Recognit.,",
            "year": 2018
        },
        {
            "authors": [
                "K. Zhang",
                "W. Zuo",
                "Y. Chen",
                "D. Meng",
                "L. Zhang"
            ],
            "title": "Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2017
        },
        {
            "authors": [
                "K. Zhang",
                "W. Zuo",
                "S. Gu",
                "L. Zhang"
            ],
            "title": "Learning deep CNN denoiser prior for image restoration",
            "venue": "In Proc. IEEE Conf. Comput. Vis. and Pattern Recognit.,",
            "year": 2017
        },
        {
            "authors": [
                "K. Zhang",
                "W. Zuo",
                "L. Zhang"
            ],
            "title": "Deep plug-and-play super-resolution for arbitrary blur kernels",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2019
        },
        {
            "authors": [
                "K. Zhang",
                "L. Van G",
                "R. Timofte"
            ],
            "title": "Deep unfolding network for image super-resolution",
            "venue": "In Proc. IEEE Conf. Comput. Vis. and Pattern Recognit. (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "K. Zhang",
                "J. Liang",
                "L. Van G",
                "R. Timofte"
            ],
            "title": "Designing a practical degradation model for deep blind image super-resolution",
            "venue": "In Proc. IEEE Int. Conf. Comp. Vis. (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "K. Zhang",
                "Y. Li",
                "W. Zuo",
                "L. Zhang",
                "L. Van Gool",
                "R. Timofte"
            ],
            "title": "Plug-and-play image restoration with deep denoiser prior",
            "venue": "IEEE Trans. Patt. Anal. and Machine Intell.,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhu",
                "K. Zhang",
                "J. Liang",
                "J. Cao",
                "B. Wen",
                "R. Timofte",
                "L. Van G"
            ],
            "title": "Denoising diffusion models for plug-and-play image restoration",
            "venue": "In Proc. IEEE Conf. Comput. Vis. and Pattern Recognit. (CVPR),",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Many problems in computational imaging, biomedical imaging, and computer vision can be formulated as inverse problems, where the goal is to recover a high-quality images from its low-quality observations. Imaging inverse problems are generally ill-posed, thus necessitating the use of prior models on the unknown images for accurate inference. While the literature on prior modeling of images is vast, current methods are primarily based on deep learning (DL), where a deep model is trained to map observations to images (Lucas et al., 2018; McCann et al., 2017; Ongie et al., 2020).\nImage denoisers have become popular for specifying image priors for solving inverse problems (Venkatakrishnan et al., 2013; Romano et al., 2017; Kadkhodaie & Simoncelli, 2021; Kamilov et al., 2023). Pre-trained denoisers provide a convenient proxy for image priors that does not require the description of the full density of natural images. The combination of state-of-the-art (SOTA) deep denoisers with measurement models has been shown to be effective in a number of inverse problems, including image super-resolution, deblurring, inpainting, microscopy, and medical imaging (Metzler et al., 2018; Zhang et al., 2017b; Meinhardt et al., 2017; Dong et al., 2019; Zhang et al., 2019; Wei et al., 2020; Zhang et al., 2022) (see also the recent reviews (Ahmad et al., 2020; Kamilov et al., 2023)). This success has led to active research on novel methods based on denoiser priors, their theoretical analyses, statistical interpretations, as well as connections to related approaches such as score matching and diffusion models (Chan et al., 2017; Romano et al., 2017; Buzzard et al., 2018; Reehorst & Schniter, 2019; Sun et al., 2019; Sun et al., 2019; Ryu et al., 2019; Xu et al., 2020; Liu et al., 2021; Cohen et al., 2021a; Hurault et al., 2022a;b; Laumont et al., 2022; Gan et al., 2023).\nDespite the rich literature on the topic, the prior work has narrowly focused on leveraging the statistical properties of denoisers. There is little work on extending the formalism and theory to priors specified using other types of image restoration operators, such as, for example, deep image super-resolution models. Such extensions would enable new algorithms that can leverage SOTA pre-trained restoration networks for solving other inverse problems. In this paper, we address this gap by developing the Deep Restoration Priors (DRP) methodology that provides a principled approach for using restoration operators as priors. We show that when the restoration operator is a minimum mean-squared error (MMSE) estimator, DRP can be interpreted as minimizing a composite objective function that includes log of the density of the degraded image as the regularizer. Our interpretation extends the recent formalism based on using MMSE denoisers as priors (Bigdeli et al., 2017; Xu et al., 2020; Kadkhodaie & Simoncelli, 2021; Laumont et al., 2022; Gan et al., 2023). We present a theoretical convergence analysis of DRP to a stationary point of the objective function under a set of clearly specified assumptions. We show the practical relevance of DRP by solving several inverse problems by using a super-resolution network as a prior. Our numerical results show\nthe potential of DRP to adapt the super-resolution model to act as an effective prior that can outperform image denoisers. This work thus addresses a gap in the current literature by providing a new principled framework for using pre-trained restoration models as priors for inverse problems.\nAll proofs and some details that have been omitted for space appear in the supplementary material."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Inverse Problems. Many imaging problems can be formulated as inverse problems that seek to recover an unknown image x \u2208 Rn from from its corrupted observation\ny = Ax+ e, (1)\nwhere A \u2208 Rm\u00d7n is a measurement operator and e \u2208 Rm is the noise. A common strategy for addressing inverse problems involves formulating them as an optimization problem\nx\u0302 \u2208 argmin x\u2208Rn f(x) with f(x) = g(x) + h(x) , (2)\nwhere g is the data-fidelity term that measures the fidelity to the observation y and h is the regularizer that incorporates prior knowledge on x. For example, common functionals in imaging inverse problems are the least-squares data-fidelity term g(x) = 12 \u2016Ax\u2212 y\u2016 2 2 and the total variation (TV) regularizer h(x) = \u03c4 \u2016Dx\u20161, where D is the image gradient, and \u03c4 > 0 a regularization parameter. Deep Learning. DL is extensively used for solving imaging inverse problems (McCann et al., 2017; Lucas et al., 2018; Ongie et al., 2020). Instead of explicitly defining a regularizer, DL methods often train convolutional neural networks (CNNs) to map the observations to the desired images (Wang et al., 2016; Jin et al., 2017; Kang et al., 2017; Chen et al., 2017; Delbracio et al., 2021; Delbracio & Milanfar, 2023). Model-based DL (MBDL) is a widely-used sub-family of DL algorithms that integrate physical measurement models with priors specified using CNNs (see reviews (Ongie et al., 2020; Monga et al., 2021)). The literature of MBDL is vast, but some well-known examples include plug-and-play priors (PnP), regularization by denoising (RED), deep unfolding (DU), compressed sensing using generative models (CSGM), and deep equilibrium models (DEQ) (Bora et al., 2017; Romano et al., 2017; Zhang & Ghanem, 2018; Hauptmann et al., 2018; Gilton et al., 2021; Liu et al., 2022). These approaches come with different trade-offs in terms of imaging performance, computational and memory complexity, flexibility, need for supervision, and theoretical understanding.\nDenoisers as Priors. PnP (Venkatakrishnan et al., 2013; Sreehari et al., 2016) is one of the most popular MBDL approaches for inverse problems based on using deep denoisers as imaging priors (see recent reviews (Ahmad et al., 2020; Kamilov et al., 2023)). For example, the proximal-gradient method variant of PnP can be written as (Hurault et al., 2022a)\nxk \u2190 prox\u03b3g(zk) with zk \u2190 xk\u22121 \u2212 \u03b3\u03c4(xk\u22121 \u2212 D\u03c3(xk\u22121)), (3) where D\u03c3 is a denoiser with a parameter \u03c3 > 0 for controlling its strength, \u03c4 > 0 is a regularization parameter, and \u03b3 > 0 is a step-size. The theoretical convergence of PnP methods has been established for convex functions g using monotone operator theory (Sreehari et al., 2016; Sun et al., 2019; Ryu et al., 2019), as well as for nonconvex functions based on interpreting the denoiser as a MMSE estimator (Xu et al., 2020) or ensuring that the term (I\u2212D\u03c3) in (3) corresponds to a gradient\u2207h of a function h parameterized by a deep neural network (Hurault et al., 2022a;b; Cohen et al., 2021a). Many variants of PnP have been developed over the past few years (Romano et al., 2017; Metzler et al., 2018; Zhang et al., 2017b; Meinhardt et al., 2017; Dong et al., 2019; Zhang et al., 2019; Wei et al., 2020), which has motivated an extensive research on its theoretical properties (Chan et al., 2017; Buzzard et al., 2018; Ryu et al., 2019; Sun et al., 2019; Tirer & Giryes, 2019; Teodoro et al., 2019; Xu et al., 2020; Sun et al., 2021; Cohen et al., 2021b; Hurault et al., 2022a; Laumont et al., 2022; Hurault et al., 2022b; Gan et al., 2023).\nThis work is most related to two recent PnP-inspired methods using restoration operators instead of denoisers (Zhang et al., 2019; Liu et al., 2020). Deep plug-and-play super-resolution (DPSR) (Zhang et al., 2019) was proposed to perform image super-resolution under arbitrary blur kernels by using a bicubic super-resolver as a prior. Regularization by artifact removal (RARE) (Liu et al., 2020) was proposed to use CNNs pre-trained directly on subsampled and noisy Fourier data as priors for\nmagnetic resonance imaging (MRI). These prior methods did not leverage statistical interpretations of the restoration operators to provide a theoretical analysis for the corresponding PnP variants.\nIt is also worth highlighting the work of Gribonval and colleagues on theoretically exploring the relationship between MMSE restoration operators and proximal operators (Gribonval, 2011; Gribonval & Machart, 2013; Gribonval & Nikolova, 2021). Some of the observations and intuition in that prior line of work is useful for the theoretical analysis of the proposed DRP methodology.\nOur contribution. (1) Our first contribution is the new method DRP for solving inverse problems using the prior implicit in a pre-trained deep restoration network. Our method is a major extension of recent methods (Bigdeli et al., 2017; Xu et al., 2020; Kadkhodaie & Simoncelli, 2021; Gan et al., 2023) from denoisers to more general restoration operators.(2) Our second contribution is a new theory that characterizes the solution and convergence of DRP under priors associated with the MMSE restoration operators. Our theory is general in the sense that it allows for nonsmooth datafidelity terms and expansive restoration models. (3) Our third contribution is the implementation of DRP using the popular SwinIR (Liang et al., 2021) super-resolution model as a prior for two distinct inverse problems, namely deblurring and super-resolution. We publicly share our implementation that shows the potential of using restoration models to achieve SOTA performance."
        },
        {
            "heading": "3 DEEP RESTORATION PRIOR",
            "text": "Image denoisers are currently extensively used as priors for solving inverse problems. We extend this approach by proposing the following method that uses a more general restoration operator.\nAlgorithm 1 Deep Restoration Priors (DRP) 1: input: Initial value x0 \u2208 Rn and parameters \u03b3, \u03c4 > 0 2: for k = 1, 2, 3, . . . do 3: zk \u2190 xk\u22121 \u2212 \u03b3\u03c4G(xk\u22121) where G(x) := x\u2212 R(Hx) 4: xk \u2190 sprox\u03b3g(zk) 5: end for\nThe prior in Algorithm 1 is implemented in Line 3 using a deep model R : Rp \u2192 Rn pre-trained to solve the following restoration problem\ns = Hx+ n with x \u223c px, n \u223c N (0, \u03c32I), (4) where H \u2208 Rp\u00d7n is a degradation operator, such as blur or downscaling, and n \u2208 Rp is the additive white Gaussian noise (AWGN) of variance \u03c32. The density px is the prior distribution of the desired class of images. Note that the restoration problem (4) is only used for training R and doesn\u2019t have to correspond to the inverse problem in (1) we are seeking to solve. When H = I, the restoration operator R reduces to an AWGN denoiser used in the traditional PnP methods (Romano et al., 2017; Kadkhodaie & Simoncelli, 2021; Hurault et al., 2022a). The goal of DRP is to leverage a pre-trained restoration network R to gain access to the prior.\nThe measurement consistency is implemented in Line 4 using the scaled proximal operator\nsprox\u03b3g(z) := prox HTH \u03b3g (z) = argmin\nx\u2208Rn\n{ 1\n2 \u2016x\u2212 z\u20162HTH + \u03b3g(x)\n} , (5)\nwhere \u2016v\u2016HTH := vTHTHv denotes the weighted Euclidean seminorm of a vector v. When HTH is positive definite and g is convex, the functional being minimized in (5) is strictly convex, which directly implies that the solution is unique. On the other hand, when g is not convex or HTH is positive semidefinite, there might be multiple solutions and the scaled proximal operator simply returns one of the solutions. It is also worth noting that (5) has an efficient solution when g is the least-squares data-fidelity term (see for example the discussion in (Kamilov et al., 2023) on efficient implementations of proximal operators of least-squares).\nThe fixed points of Algorithm 1 can be characterized for subdifferentiable g (see Chapter 3 in (Beck, 2017) for a discussion on subdifferentiability). When DRP converges, it converges to vectors x\u2217 \u2208 Rn that satisfy (see formal analysis in Supplement A.1)\n0 \u2208 \u2202g(x\u2217) + \u03c4HTHG(x\u2217) (6)\nwhere \u2202g is the subdifferential of g and G is defined in Line 3 of Algorithm 1. As discussed in the next section, under additional assumptions, one can associate the fixed points of DRP with the stationary points of a composite objective function f = g + h for some regularizer h."
        },
        {
            "heading": "4 CONVERGENCE ANALYSIS OF DRP",
            "text": "In this section, we present a theoretical analysis of DRP. We first provide a more insightful interpretation of its solutions for restoration models that compute MMSE estimators of (4). We then discuss the convergence of the iterates generated by DRP. Our analysis will require several assumptions.\nWe will consider restoration models that perform MMSE estimation of x \u2208 Rn for the problem (4) R(s) = E [x|s] = \u222b xpx|s(x; s) dx = \u222b x ps|x(s;x)px(x)\nps(s) dx. (7)\nwhere we used the probability density of the observation s \u2208 Rp\nps(s) = \u222b ps|x(s;x)px(x) dx = \u222b G\u03c3(s\u2212Hx)px(x) dx. (8)\nThe function G\u03c3 in (8) denotes the Gaussian density function with the standard deviation \u03c3 > 0. Assumption 1. The prior density px is non-degenerate over Rn.\nFor example, a probability density px is degenerate over Rn, if it is supported on a space of lower dimensions than n. Our goal is to establish an explicit link between the MMSE restoration operator (7) and the following regularizer\nh(x) = \u2212\u03c4\u03c32 log ps(Hx), x \u2208 Rn, (9) where \u03c4 is the parameter in Algorithm 1, ps is the density of the observation (8), and \u03c32 is the AWGN level used for training the restoration network. We adopt Assumption 1 to have a more intuitive mathematical exposition, but one can in principle generalize the link between MMSE operators and regularization beyond non-degenerate priors (Gribonval & Machart, 2013). It is also worth observing that the function h is infinitely continuously differentiable, since it is obtained by integrating px with a Gaussian density G\u03c3 (Gribonval, 2011; Gribonval & Machart, 2013). Assumption 2. The scaled proximal operator sprox\u03b3g is well-defined in the sense that there exists a solution to the problem (5) for any z \u2208 Rn. The function g is subdifferentiable over Rn.\nThis mild assumption is needed for us to be able to run our method. There are multiple ways to ensure that the scaled proximal operator is well defined. For example, sprox\u03b3g is always welldefined for any g that is proper, closed, and convex (Parikh & Boyd, 2014). This directly makes DRP applicable with the popular least-squares data-fidelity term g(x) = 12\u2016y \u2212 Ax\u201622. One can relax the assumption of convexity by considering g that is proper, closed, and coercive, in which case sprox\u03b3g will have a solution (see for example Chapter 6 of (Beck, 2017)). Note that we do not require the solution to (5) to be unique; it is sufficient for sprox\u03b3g to return one of the solutions.\nWe are now ready to theoretically characterize the solutions of DRP. Theorem 1. Let R be the MMSE restoration operator (7) corresponding to the restoration problem (4) under Assumptions 1-3. Then, any fixed-point x\u2217 \u2208 Rn of DRP satisfies\n0 \u2208 \u2202g(x\u2217) +\u2207h(x\u2217), where h is given in (9).\nThe proof of the theorem is provided in the supplement and generalizes the well-known Tweedie\u2019s formula (Robbins, 1956; Miyasawa, 1961; Gribonval, 2011) to restoration operators. The theorem implies that the solutions of DRP satisfy the first-order conditions for the objective function f = g+ h. If g is a negative log-likelihood py|x, then the fixed-points of DRP can be interpreted as maximuma-posteriori probability (MAP) solutions corresponding to the prior density ps. The density ps is related to the true prior px through eq. (8), which implies that DRP has access to the prior px through the restoration operator R via density ps. As H\u2192 I and \u03c3 \u2192 0, the density ps approaches the prior distribution px.\nThe convergence analysis of DRP will require additional assumptions.\nAssumption 3. The data-fidelity term g and the implicit regularizer h are bounded from below.\nThis assumption implies that there exists f\u2217 > \u2212\u221e such that f(x) \u2265 f\u2217 for all x \u2208 Rn. Assumption 4. The function h has a Lipschitz continutous gradient with constant L > 0. The degradation operator associated with the restoration network is such that \u03bb HTH \u00b5 > 0.\nThis assumption is related to the implicit prior associated with a restoration model and is needed to ensure the monotonic reduction of the objective f by the DRP iterates. As stated under eq. (9), the function h is infinitely continuously differentiable. We additionally adopt the standard optimization assumption that\u2207h is Lipschitz continuous (Nesterov, 2004). It is also worth noting that the positive definiteness of HTH in Assumption 4 is a relaxation of the traditional PnP assumption that the prior is a denoiser, which makes our theoretical analysis a significant extension of the prior work (Bigdeli et al., 2017; Xu et al., 2020; Kadkhodaie & Simoncelli, 2021; Gan et al., 2023).\nWe are now ready to state the following results. Theorem 2. Run DRP for for t \u2265 1 iterations under Assumptions 1-4 using a step-size \u03b3 = \u00b5/(\u03b1L) with \u03b1 > 1. Then, for each iteration 1 \u2264 k \u2264 t, there exists w(xk) \u2208 \u2202f(xk) such that\n1\nt t\u2211 k=1 \u2016w(xk)\u201622 \u2264 C(f(x0)\u2212 f\u2217) t ,\nwhere C > 0 is an iteration independent constant.\nThe exact expression for the constant C is given in the proof. Theorem 2 shows that the iterates generated by DRP satisfy w(xk) \u2192 0 as t \u2192 \u221e. Theorems 1 and 2 do not explicitly require convexity or smoothness of g, and non-expansiveness of R. They can thus be viewed as a major generalization of the existing theory from denoisers to more general restoration operators."
        },
        {
            "heading": "5 NUMERICAL RESULTS",
            "text": "We now numerically validate DRP on several distinct inverse problems. Due to space limitations in the main paper, we have included several additional numerical results in the supplementary material.\nWe consider two inverse problems of form y = Ax+e: (a) Image Deblurring and (b) Single Image Super Resolution (SISR). For both problems, we assume that e is the additive white Gaussian noise (AWGN). We adopt the traditional `2-norm loss as the data-fidelity term in (2) for both problems. We use the Peak Signal-to-Noise Ratio (PSNR) for quantitative performance evaluation.\nIn the main manuscript, we compare DRP with several variants of denoiser-based methods, including SD-RED (Romano et al., 2017), PnP-ADMM (Chan et al., 2017), IRCNN (Zhang et al., 2017b), and DPIR (Zhang et al., 2022). SD-RED and PnP-ADMM refer to the steepest-descent variant of RED and the ADMM variant of PnP, both of which incorporate AWGN denoisers based on DnCNN (Zhang et al., 2017a). IRCNN and DPIR are based on half-quadratic splitting (HQS) iterations that use the IRCNN and the DRUNet denoisers, respectively.\nIn the supplement, we present several additional comparisons, namely: (a) evaluation of the performance of DRP on the task of image denoising; (b) additional comparison of DRP with the recent provably convergent variant of PnP called gradient-step plug-and-play (GS-PnP) (Hurault et al., 2022a); (c) comparison of DRP with the diffusion posterior sampling (DPS) (Chung et al., 2023) method that uses a denoising diffusion model as a prior; (d) illustration of the improvement of DRP using SwinIR as a prior over the direct application of SwinIR on SR using the Gaussian kernel; (e) presentation of quantitative results using SSIM and LPIPS metrics; (f) additional comparison with DPIR using with SwinIR trained as denoiser; (g) additional experiments of DRP using with DRUNet trained as SR model; (h) additional evaluation of DRP with a non-MMSE restoration priors and priors based on other network architectures; and (i) evaluation of robustness of our numerical results to random seed."
        },
        {
            "heading": "5.1 SWIN TRANSFORMER BASED SUPER RESOLUTION PRIOR",
            "text": "Super Resolution Network Architecture. We pre-trained a q\u00d7 super resolution model Rq using the SwinIR (Liang et al., 2021) architecture based on Swin Transformer. Our training dataset comprised\nboth the DIV2K (Agustsson & Timofte, 2017) and Flick2K (Lim et al., 2017) dataset, containing 3450 color images in total. During training, we applied q\u00d7 bicubic downsampling to the input images with AWGN characterized by standard deviation \u03c3 randomly chosen in [0, 10/255]. We used three SwinIR SR models, each trained for different down-sampling factors: 2\u00d7, 3\u00d7 and 4\u00d7. Prior Refinement Strategy for the Super Resolution prior. Theorem 1 suggests that as H \u2192 I, the prior in DRP converges to px. This process can be approximated for SwinIR by controlling the down-sampling factor q of the SR restoration prior Rq(\u00b7). We observed through our numerical experiments that gradual reduction of q leads to less reconstruction artifacts and enhanced fine details. We will denote the approach of gradually reducing q as prior refinement strategy. We initially set q to a larger down-sampling factor, which acts as a more aggressive prior; we then reduce q to a smaller value leading to preservation of finer details. This strategy is conceptually analogous to the gradual reduction of \u03c3 in the denoiser in the SOTA PnP methods such as DPIR (Zhang et al., 2022)."
        },
        {
            "heading": "5.2 IMAGE DEBLURRING",
            "text": "Image deblurring is based on the degradation operator of the form A = K, where K is a convolution with the blur kernel k. We consider image deblurring using two 25\u00d7 25 Gaussian kernels (with the standard deviations 1.6 and 2) used in (Zhang et al., 2019), and the AWGN vector e corresponding to noise level of 2.55/255. For fair comparison, we use the official implementations provided by each baseline method and use the same random seed to ensure consistency of random noise for all methods. The restoration model used as a prior in DRP is SwinIR introduced in Section 5.1, so that the operation H corresponds to the standard bicubic downsampling. The scaled proximal operator sprox\u03bbg in (5) with data-fidelity term g(x) = 1 2 \u2016y \u2212Kx\u2016 2 2 can be written as\nsprox\u03b3g(z) = (K TK + \u03b3HTH)\u22121[KTy + \u03b3HTHz]. (10)\nWe adopt a standard approach of using a few iterations of the conjugate gradient (CG) method (see for example (Aggarwal et al., 2019)) to implement the scaled proximal operator (10) by avoiding the\ndirect inversion of (KTK + \u03b3HTH). As bicubic SR model is adopted as prior, the R(Hx) in Step 3 of Algorithm 1 performs a bicubic downsampling of the intermediate image xk\u22121 and inputs it into the bicubic SR SwinIR. In each DRP iteration, we run three steps of a CG solver, starting from a warm initialization from the previous DRP iteration. We fine-turned the hyper-parameter \u03b3, \u03c4 and SR restoration prior rate q to achieve the highest PSNR value on the Set5 dataset and then apply the same configuration to the other three datasets.\nFigure 1 (a)-(b) illustrates the convergence behaviour of DRP on the Set3c dataset for two blur kernels. Table 1 presents the quantitative evaluation of the reconstruction performance on two different blur kernels, showing that DRP outperforms the baseline methods across four widely-used datasets. Figure 2 visually illustrates the reconstructed results on the same two blur kernels. Note how DRP can reconstruct the fine details of the tiger and starfish, as highlighted within the zoom-in boxes, while all the other baseline methods yield either oversmoothed reconstructions or noticeable artifacts. These results show that DRP can leverage SwinIR as an implicit prior, which not only ensures stable convergence, but also leads to competitive performance when compared to denoisers priors.\nFigure 3 illustrates the impact of the prior-refinement strategy described in Section 5.1. We compare three settings: (i) use of only 3\u00d7 prior, (ii) use of only 2\u00d7 prior, and (iii) use of the prior-refinement strategy to leverage both 3\u00d7 and 2\u00d7 priors. The subfigure on the left shows the convergence of DRP for each configuration, while the ones on the right show the final imaging quality. Note how"
        },
        {
            "heading": "2\u00d7 LR Image PnP-ADMM IRCNN+ DPIR DRP (Ours) Ground Truth",
            "text": "the reduction of q leads to better performance, which is analogous to what was observed with the reduction of \u03c3 in the SOTA PnP methods (Zhang et al., 2022)."
        },
        {
            "heading": "5.3 SINGLE IMAGE SUPER RESOLUTION",
            "text": "We apply DRP using the bicubic SwinIR prior to Single Image Super Resolution (SISR) task. The measurement operator in SISR can be written as A = SK, where K is convolution with the blur kernel k and S performs standard d-fold down-sampling with d2 = n/m. The scaled proximal operator sprox\u03bbg in (5) with data-fidelity term g(x) = 1 2 \u2016y \u2212 SKx\u2016 2 2 can be write as:\nsprox\u03b3g(z) = (K TSTSK + \u03b3HTH)\u22121[KTSTy + \u03b3HTHz], (11)\nwhere H is the bicubic downsampling operator. Similarly to deblurring in Section 5.2, we use CG to efficiently compute (11). We adjust the hyper-parameter \u03b3, \u03c4 , and the SR restoration prior factor q for the best PSNR performance on Set5, and then use these parameters on the remaining datasets.\nWe evaluate super-resolution performance across two 25 \u00d7 25 Gaussian blur kernels, each with distinct standard deviations (1.6 and 2.0), and for two distinct downsampling factors (2\u00d7 and 3\u00d7),\nincorporating an AWGN vector e corresponding to noise level of 2.55/255. For fair comparison, we use the official implementations provided by each baseline method and use the same random seed to ensure consistency of random noise for all methods.\nFigure 1 (c)-(d) illustrates the convergence behaviour of DRP on the Set3c dataset for 2\u00d7 and 3\u00d7 SISR. Figure 4 shows the visual reconstruction results for the same downsampling factors. Table 2 summarizes the PSNR values achieved by DRP relative to other baseline methods when applied to different blur kernel and downsampling factors on four commonly used datasets.\nIt is worth highlighting that the SwinIR model used in DRP was pre-trained for the bicubic superresolution task. Consequently, the direct application of the pre-trained SwinIR to the setting considered in this section leads to the suboptimal performance due to mismatch between the kernels used. See Supplement B.4 to see how DRP improves over the direct application of SwinIR."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "The work presented in this paper proposes a new DRP method for solving imaging inverse problems by using pre-trained restoration operators as priors, presents its theoretical analysis in terms of convergence, and applies the method to two well-known inverse problems. The proposed method and its theoretical analysis extend the recent work using denoisers as priors by considering more general restoration operators. The numerical validation of DRP shows the improvements due to the use of learned SOTA super-resolution models. One conclusion of this work is the potential effectiveness of going beyond priors specified by traditional denoisers."
        },
        {
            "heading": "LIMITATIONS AND FUTURE WORK",
            "text": "The work presented in this paper comes with several limitations. The proposed DRP method uses pre-trained restoration models as priors, which means that its performance is inherently limited by the quality of the pre-trained model. As shown in this paper, pre-trained restoration models provide a convenient, principled, and flexible mechanism to specify priors; yet, they are inherently selfsupervised and their empirical performance can thus be suboptimal compared to priors trained in a supervised fashion for a specific inverse problem. Our theory is based on the assumption that the restoration prior used for inference performs MMSE estimation. While this assumption is reasonable for deep networks trained using the MSE loss, it is not directly applicable to denoisers trained using other common loss functions, such as the `1-norm or SSIM. Finally, as is common with most theoretical work, our theoretical conclusions only hold when our assumptions are satisfied, which might limit their applicability in certain settings.\nOur work opens several interesting directions for future research. First, the implicit prior in 9 can be seen as an analysis prior with a transform H (Elad et al., 2007; Selesnick & Figueiredo, 2009), which suggests a possibility of considering broader class of linear transforms for priors. Second, the excellent performance of the restoration priors beyond denoisers leads to an interesting open question: what is the optimal linear transform H for a given measurement operator A. Third, while in this paper we considered non-blind inverse problems, the extension of DRP to blind inverse problems would be an interesting future direction of research (Gan et al., 2023)."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We have provided the anonymous source code in the supplementary materials. The included README.md file contains detailed instructions on how to run the code and reproduce the results reported in the paper. The pseudo-code of DRP is outlined in Algorithm 1. The complete proofs and technical details for our theoretical analysis can be found in the supplement."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "To the best of our knowledge this work does not give rise to any significant ethical concerns."
        },
        {
            "heading": "A THEORETICAL ANALYSIS OF DRP",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF THEOREM 1",
            "text": "Theorem. Let R be the MMSE restoration operator (7) corresponding to the restoration problem (4) under Assumptions 1-3. Then, any fixed-point x\u2217 \u2208 Rn of DRP satisfies 0 \u2208 \u2202g(x\u2217) +\u2207h(x\u2217), where h is given in (9).\nProof. First note that any fixed point x\u2217 \u2208 Rn of the DRP method can be expressed as\nx\u2217 = sprox\u03b3g(x \u2217 \u2212 \u03b3\u03c4G(x\u2217)) = argmin\nx\u2208Rn\n{ 1\n2 \u2016x\u2212 (x\u2217 \u2212 \u03b3\u03c4G(x\u2217))\u20162HTH + \u03b3g(x)\n} , (12)\nwhere we used the definition of the scaled proximal operator. From the optimality conditions of the scaled proximal operator, we then get\n0 \u2208 \u2202g(x\u2217) + \u03c4HTHG(x\u2217). (13)\nOn the other hand, the gradient of ps, defined in (8), can be expressed as \u2207sps(s) = \u222b ( 1\n\u03c32 (Hx\u2212 s)\n) G\u03c3(s\u2212Hx)px(x) dx = 1\n\u03c32 (HR(s)\u2212 sps(s)) , (14)\nwhere we used the gradient of the Gaussian density with respect to s and the definition of the MMSE restoration operator in eq. (7). By rearranging the terms, we obtain the following relationship\nHR(s)\u2212 s = \u03c32\u2207 log ps(s), s \u2208 Rp. (15) By using the definitions G(x) = x\u2212 R(Hx) and h(x) = \u2212\u03c4\u03c32 log ps(Hx), with x \u2208 Rn, in (15), we obtain the following generalization of the well-known Tweedie\u2019s formula\nHTHG(x) = HTH (x\u2212 R(Hx)) = \u2212\u03c32\u2207 log ps(Hx) = 1\n\u03c4 \u2207h(x). (16)\nBy combining(12) and (16), we directly obtain the desired result 0 \u2208 \u2202g(x\u2217) +\u2207h(x\u2217)."
        },
        {
            "heading": "A.2 PROOF OF THEOREM 2",
            "text": "Theorem. Run DRP for for t \u2265 1 iterations under Assumptions 1-4 using a step-size \u03b3 = \u00b5/(\u03b1L) with \u03b1 > 1. Then, for each iteration 1 \u2264 k \u2264 t, there exists w(xk) \u2208 \u2202f(xk) such that\nmin 1\u2264k\u2264t\n\u2016w(xk)\u201622 \u2264 1\nt t\u2211 k=1 \u2016w(xk)\u201622 \u2264 C(f(x0)\u2212 f\u2217) t ,\nwhere C > 0 is an iteration independent constant.\nProof. Consider the iteration k \u2265 1 of DRP xk = sprox\u03b3g ( xk\u22121 \u2212 \u03b3\u03c4G(xk\u22121) ) with G := x\u2212 R(Hx),\nwhere R is the MMSE restoration operator specified in (7). This implies that xk minimizes\n\u03d5(x) := 1 2\u03b3 (x\u2212 xk\u22121)THTH(x\u2212 xk\u22121) +\n[ \u03c4HTHG(xk\u22121) ]T (x\u2212 xk\u22121) + g(x)\n= 1 2\u03b3 (x\u2212 xk\u22121)THTH(x\u2212 xk\u22121) +\u2207h(xk\u22121)T(x\u2212 xk\u22121) + g(x),\nwhere in the second inequality we used eq. (16) from the proof in Supplement A.1. By evaluating \u03d5 at xk and xk\u22121, we obtain the following useful inequality\ng(xk) \u2264 g(xk\u22121)\u2212 1 2\u03b3 (xk \u2212 xk\u22121)THTH(xk \u2212 xk\u22121)\u2212\u2207h(xk\u22121)T(xk \u2212 xk\u22121). (17)\nOn the other hand, from the L-Lipschitz continuity of\u2207h, we have the following bound\nh(xk) \u2264 h(xk\u22121) +\u2207h(xk\u22121)T(xk \u2212 xk\u22121) + L 2 \u2016xk \u2212 xk\u22121\u201622. (18)\nBy combining eqs. (17) and (18), we obtain\nf(xk) \u2264 f(xk\u22121)\u2212 1 2 (xk \u2212 xk\u22121)T\n[ 1\n\u03b3 HTH\u2212 LI\n] (xk \u2212 xk\u22121)\n\u2264 f(xk\u22121)\u2212 (\u03b1\u2212 1)L 2 \u2016xk \u2212 xk\u22121\u201622, (19)\nwhere we used \u03b3 = \u00b5/(\u03b1L) with \u03b1 > 1 and \u00b5 > 0 defined in Assumption 4.\nOn the other hand, from the optimality conditions for \u03d5, we also have\n0 \u2208 HTH(xk \u2212 xk\u22121 + \u03b3\u03c4G(xk\u22121)) + \u03b3\u2202g(xk)\n\u21d4 1 \u03b3 HTH(xk \u2212 xk\u22121) \u2208 \u2202g(xk) +\u2207h(xk\u22121),\nwhere we used eq. (16) from Supplement A.1. This directly implies that the following inclusion\nw(xk) := 1 \u03b3 HTH(xk \u2212 xk\u22121) +\u2207h(xk)\u2212\u2207h(xk\u22121) \u2208 \u2202f(xk)\nThe norm of the subgradient w(xk) can be bounded as follows\n\u2016w(xk)\u20162 \u2264 1 \u03b3 \u2016HTH(xk \u2212 xk\u22121)\u20162 + \u2016\u2207h(xk)\u2212\u2207h(xk\u22121)\u20162\n\u2264 L (\u03b1(\u03bb/\u00b5) + 1) \u2016xk \u2212 xk\u22121\u20162, (20) where we used the Lipschitz constant of\u2207h, \u03b3 = \u00b5/(\u03b1L), and \u03bb \u2265 \u00b5 > 0 defined in Assumption 4. By combining eqs. (19) and (20), we obtain the following inequality\n\u2016w(xk)\u201622 \u2264 A1\u2016xk \u2212 xk\u22121\u201622 \u2264 A2(f(xk\u22121)\u2212 f(xk)), (21) where A1 := L2(\u03b1(\u03bb/\u00b5) + 1)2 > 0 and A2 := 2A1/(L(\u03b1 \u2212 1)) > 0. Hence, by averaging over t \u2265 1 iterations, we can directly get the desired result\nmin 1\u2264k\u2264t\n\u2016w(xk)\u201622 \u2264 1\nt t\u2211 k=1 \u2016w(xk)\u201622 \u2264 A2(f(x 0)\u2212 f\u2217) t . (22)\nThis implies that w(xk)\u2192 0 as t\u2192\u221e."
        },
        {
            "heading": "B ADDITIONAL NUMERICAL RESULTS",
            "text": "B.1 IMAGE DENOISING\nIn this subsection, we show that DRP using the SwinIR prior trained for bicubic SR can perform Gaussian image denoising. The measurement model is y = x + e, where e is AWGN with the standard deviation \u03c3 and x is the unknown clean image. We use the same SwinIR SR model from Section 5.1, as the prior for DRP. The degradation model in the SwinIR prior is the operation H corresponding to bicubic downsampling. The scaled proximal operator sprox\u03bbg in (5) with datafidelity term g(x) = 12 \u2016y \u2212 x\u2016 2 2 can be written as\nsprox\u03b3g(z) := (I+ \u03b3H TH)\u22121[y + \u03b3HTHz], (23)\nwhich can be efficiently implemented using CG, as in Section 5.2.\nWe compare DRP with one of SOTA denoising model DRUNet (Zhang et al., 2019) on noise level (\u03c3 = 0.1). Figure 5 and Figure 6 illustrate the visual performance of DRP on the Set5 and CBSD68 datasets, respectively. Figure 7 further explores the impact of using different SR factors q as priors, elucidating how these choices influence the visual quality of denoising.\n<latexit sha1_base64=\"hzCd3Dyj+ochPbV9vHJvws6PsxI=\">AAACEnicbZDNSsNAFIVv6l+tf6ku3QwWwVVIin8boeDGZQXbCm0ok+mkHTqThJmJUkLfwq1bfQd34tYX8BV8CqdtENt6YODj3HvgzgkSzpR23S+rsLK6tr5R3Cxtbe/s7tnl/aaKU0log8Q8lvcBVpSziDY005zeJ5JiEXDaCobXk3nrgUrF4uhOjxLqC9yPWMgI1sbq2uVOILKOYn2B0RVyHW/ctSuu406FlsHLoQK56l37u9OLSSpopAnHSrU9N9F+hqVmhNNxqZMqmmAyxH3aNhhhQZWfTU8fo2Pj9FAYS/Mijabu30SGhVIjEZhNgfVAzc0C8WsvZibuf5l2qsNLP2NRkmoakdkBYcqRjtGkH9RjkhLNRwYwkcz8AZEBlpho02LJlOMtVrEMzarjnTtnt6eVWjWvqQiHcAQn4MEF1OAG6tAAAo/wDC/waj1Zb9a79TFbLVh55gDmZH3+AIMvnC0=</latexit>"
        },
        {
            "heading": "B.2 COMPARISON WITH GS-PNP",
            "text": "In this subsection, we will compare DRP with the recent gradient-step denoiser PnP method (GSPnP) (Hurault et al., 2022a). These comparisons were not included in the main paper due to space, but are provided here for completeness. GS-PnP provides comparable performance on image deblurring and single image super resolution as DPIR (Zhang et al., 2019), but comes with theoretical convergence guarantees.\nTable 3 shows that DRP outperforms both DPIR and GS-PnP on image deblurring in most settings in terms of PSNR. Similarly, Table 4 shows that DRP can achieve better SISR performance in terms of PSNR compared to both methods. Figure 8 provides additional visual results on SISR showing that DRP can recover intricate details and sharpen features."
        },
        {
            "heading": "DPIR DRP (Ours)GS-PnP Ground Truth3\u00d7 LR Image",
            "text": ""
        },
        {
            "heading": "B.3 COMPARISON WITH DIFFUSION POSTERIOR SAMPLING",
            "text": "There is a growing interest in using denoisers within diffusion models for solving inverse problems (Zhu et al., 2023; Wang et al., 2022; Chung et al., 2023). One of the most wiedely-adopted diffusion model in this context is the diffusion posterior sampling (DPS) method from (Chung et al., 2023), which integrates pre-trained denoisers and measurement models for posterior sampling. One may argue that DPS is related to PnP due to the use of image denoisers as priors. In this section, we present results comparing DRP with DPS for deblurring human faces. We used the public implementation of DPS on the GitHub page that uses the prior specifically trained on human face image dataset (Chung et al., 2023). DRP uses the same SwinIR model trained on general image datasets (see Section 5.1). DPS and DRP are related but very different classes of methods. While DPS seeks to use denoisers to generate perceptually realistic solutions to inverse problems, DRP enables the adaptation of pre-trained restoration models as priors for solving other inverse problems.\nTable 5 presents PSNR results obtained by DPS and DRP for human face deblurring. While we omitted the visual results from the paper for the privacy reasons, we will be happy to provide them if requested by the reviewers. Overall, DPS achieves more perceptually realistic images, while DRP achieves higher PSNR and more closely matches the ground truth images. This is not surprising when considering the generative nature of DPS. A similar observation is available in the original DPS publication, which reported better PSNR and SSIM performance of PnP-ADMM relative to DPS on SISR and deblurring (see Supplement E in (Chung et al., 2023))."
        },
        {
            "heading": "B.4 PERFORMANCE OF SWINIR TRAINED FOR BICUBIC SR ON A MISMATCHED SISR TASK",
            "text": "In this section, we make a noteworthy point: the SwinIR SR network we used as a prior in our DRP method is specifically trained for the bicubic SR task. Its direct application to the SISR task (which is SR under Gaussian blur kernels and additive white Gaussian noise) leads to sub-optimal performance. This implies that our DRP method has the capacity to use a mismatched restoration model as an implicit prior, effectively adapting it for other image restoration tasks.\nFigure 9 present qualitative and quantitative PSNR results on the Set3c dataset. Note how the direct use of SwinIR trained for bicubic SR does poorly on the SISR task, while using it within our DRP method as a prior leads to the SOTA performance. Furthermore, we want to highlight that DRP with mismatched prior can also outperform SwinIR trained for general SR tasks (named SwinIR (RealSR)), as shown in the Figure 9.\nDPR (Ours) Ground TruthSwinIR2\u00d7 LR Image\nSwinIR (RealSR)"
        },
        {
            "heading": "B.5 ADDITIONAL EVALUATION USING SSIM AND LPIPS QUALITY METRICS",
            "text": "In this section, we present results using two popular quality metrics\u2014SSIM and LPIPS\u2014for evaluating image restoration. We compare the proposed DRP method using SwinIR trained for bicubic SR as a prior against PnP-ADMM and DPIR. The results below complement those using PSRN in Table 1 and Table 2 in the main paper. As can be seen in Table 6 and Table 7, DRP always achieves better LPIPS and in most cases achieves better SSIM in all the debluring and SISR settings."
        },
        {
            "heading": "B.6 COMPARISON ON SISR TASK WITH ADDITIONAL BASELINES",
            "text": "In this section, we compare DRP with some other baseline methods on 2\u00d7 SISR task. Specificity, we consider DIP-based method BSRDM (Yue et al., 2022) and pure learning-based method BSRNet (Zhang et al., 2021) on the same setting in Section 5.3. Table 8 presents the results of our simulations. Note how DRP achieves better performance compared to both methods."
        },
        {
            "heading": "B.7 COMPARISON WITH DPIR (SWINIR) \u2013 DPIR EQUIPPED WITH THE SWINIR DENOISER",
            "text": "We present results showing that the performance improvements due to DRP are not due to the SwinIR network architecture. We substitute the DRUNet denoiser used in DPIR with the SwinIR network (Liang et al., 2021) trained for Gaussian denoising. We name this method DPIR (SwinIR). Note how both DRP and DPIR (SwinIR) use priors based on the same network architecture, but trained to perform different restroation tasks, namely SR for the priro in DRP and denoising for the prior in DPIR. Table 9 and Table 10 show that DRP achieves better performance than both DPIR (SwinIR) and DPIR (DRUNet) in most of the debluring and SISR settings, which suggests that the improvements are not due to the SwinIR network architecture."
        },
        {
            "heading": "B.8 COMPARISON WITH DRP USING THE DRUNET SR PRIOR",
            "text": "We present results demonstrating that DRP outperforms denoiser prior-based methods across various network architectures. We retrained a SR model using DRUNet architecture, as used in DPIR for denoising, within the same bicubic SR setting introduced in Section 5.1. We label DRP using the DRUNet SR prior as DRP (DRUNet SR).\nTable 11 and Table 12 show that DRP achieves better performance than both DPIR (SwinIR) and DPIR (DRUNet) in both debluring and SISR settings, which suggests that the improvements are robust under different network architectures."
        },
        {
            "heading": "B.9 EVALUATION OF DRP USING IMAGE DEBLURRING NETWORK AS A PRIOR",
            "text": "In this section, we show that DRP can be used with restoration priors and network architectures (beyond the SwinIR-based bicubic SR prior). To that end, we evaluate DRP under the deep unfolding network for image deblurring as a prior for solving Single Image Super Resolution (SISR) task."
        },
        {
            "heading": "B.9.1 RESTORATION PRIOR BASED ON DEBLURRING DEEP UNFOLDING NETWORK",
            "text": "Deep Unfolding Network Architecture. We pre-trained a deblurring model using the USRNet (Zhang et al., 2020) architecture based on deep unfolding network. Our training dataset consists of both the DIV2K (Agustsson & Timofte, 2017) and Flick2K (Lim et al., 2017) datasets, containing 3,450 color images in total. During training, we applied synthesized blur kernels to the input images, introducing Additive White Gaussian Noise (AWGN) characterized by \u03c3 randomly chosen in [0, 5/255]. This process uses the same synthesized blur kernels as detailed in (Zhang et al., 2020).\nThe restoration network is trained only for the deblurring task, which limits its performance on the SISR task. The direct application of the deblur prior on SISR is presented under the name \u201dDeblur\u201d in Table 13. Note how the direc use of the deblurring network on SISR without DRP does not work."
        },
        {
            "heading": "B.9.2 SINGLE IMAGE SUPER RESOLUTION USING DEBLURRING PRIOR",
            "text": "The measurement operator in SISR can be written as A = SK, where K is convolution with the blur kernel k and S performs standard d-fold down-sampling with d2 = n/m. The scaled proximal operator sprox\u03bbg in (5) with data-fidelity term g(x) = 1 2 \u2016y \u2212 SKx\u2016 2 2 can be write as:\nsprox\u03b3g(z) = (K TSTSK + \u03b3HTH)\u22121[KTSTy + \u03b3HTHz], (24)\nwhere H is the convolution operator with blur kernel k. Similarly to deblurring in Section 5.3, we use CG to efficiently compute (11). We adjust the hyper-parameter \u03b3, \u03c4 and then use these parameters on the remaining datasets.\nWe refer to DRP using the Deblurring Prior as DRP (Deblur) and DRP using the Super-Resolution Prior as DRP (SR). Table 13 provides a comprehensive quantitative evaluation of the reconstruction performance for the 2\u00d7 Super-Resolution (SISR) task, using two distinct blur kernels. The results show that DRP (Deblur) outperforms DPIR and achieves a comparable performance as DRP (SR) across four datasets."
        },
        {
            "heading": "B.10 EVALUATION OF DIFFERENT RANDOM SEEDS",
            "text": "In this section, we present results illustrating the influence of different random seeds on the performance. We use three different random seeds for the image deblurring task on the Set3c dataset. For each seed, DRP is compared with the best baseline method DPIR on the image deblurring task."
        },
        {
            "heading": "B.11 NUMERICAL EVALUATION USING NON-MMSE PRIORS",
            "text": "In this section, we show that although our theory relies on the assumption that the restoration prior used for inference performs MMSE estimation, in practice, DRP work even for priors trained with other loss functions, such as the `1-norm or SSIM.\nWe retrained the SwinIR model for bicubic SR using the same setting as in Section 5.1, but with the `1-norm loss instead of `2. Table 15 shows that there is not much difference between the two. This suggests that DRP can be stable in practice even without the MMSE assumption."
        }
    ],
    "title": "A RESTORATION NETWORK",
    "year": 2023
}