{
    "abstractText": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph\u2019s structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Anson Bastos"
        },
        {
            "affiliations": [],
            "name": "Kuldeep Singh"
        },
        {
            "affiliations": [],
            "name": "Abhishek Nadgeri"
        },
        {
            "affiliations": [],
            "name": "Manish Singh"
        },
        {
            "affiliations": [],
            "name": "Toyotaro Suzumura"
        }
    ],
    "id": "SP:35b60d96b156f5696a7e7381e9467751ee9cf4e3",
    "references": [
        {
            "authors": [
                "Einstein Albert",
                "W Perrett",
                "G Jeffery"
            ],
            "title": "The foundation of the general theory of relativity",
            "venue": "Ann. Der Phys,",
            "year": 1916
        },
        {
            "authors": [
                "Muhammet Balcilar",
                "Guillaume Renton",
                "Pierre H\u00e9roux",
                "Benoit Ga\u00fcz\u00e8re",
                "S\u00e9bastien Adam",
                "Paul Honeine"
            ],
            "title": "Analyzing the expressive power of graph neural networks in a spectral perspective",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Nikolaos Bastas",
                "Theodoros Semertzidis",
                "Apostolos Axenopoulos",
                "Petros Daras"
            ],
            "title": "evolve2vec: Learning network representations using temporal unfolding",
            "venue": "In MultiMedia Modeling: 25th International Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Anson Bastos",
                "Abhishek Nadgeri",
                "Kuldeep Singh",
                "Hiroki Kanezashi",
                "Toyotaro Suzumura",
                "Isaiah"
            ],
            "title": "Onando Mulang\u2019. How expressive are transformers in spectral domain for graphs",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Anson Bastos",
                "Abhishek Nadgeri",
                "Kuldeep Singh",
                "Toyotaro Suzumura",
                "Manish Singh"
            ],
            "title": "Learnable spectral wavelets on dynamic graphs to capture global interactions",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan M. Blackledge"
            ],
            "title": "Chapter 2 - 2d fourier theory",
            "venue": "Woodhead Publishing,",
            "year": 2005
        },
        {
            "authors": [
                "Defu Cao",
                "Yujing Wang",
                "Juanyong Duan",
                "Ce Zhang",
                "Xia Zhu",
                "Congrui Huang",
                "Yunhai Tong",
                "Bixiong Xu",
                "Jing Bai",
                "Jie Tong"
            ],
            "title": "Spectral temporal graph neural network for multivariate time-series forecasting",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Defu Cao",
                "Yujing Wang",
                "Juanyong Duan",
                "Ce Zhang",
                "Xia Zhu",
                "Conguri Huang",
                "Yunhai Tong",
                "Bixiong Xu",
                "Jing Bai",
                "Jie Tong",
                "Qi Zhang"
            ],
            "title": "Spectral temporal graph neural network for multivariate time-series forecasting, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jinyin Chen",
                "Xueke Wang",
                "Xuanheng Xu"
            ],
            "title": "Gc-lstm: Graph convolution embedded lstm for dynamic network link prediction",
            "venue": "Applied Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Cheng",
                "Yang Chen",
                "Yeon Ju Lee",
                "Qiyu Sun"
            ],
            "title": "Svd-based graph fourier transforms on directed product graphs",
            "venue": "IEEE Transactions on Signal and Information Processing over Networks,",
            "year": 2023
        },
        {
            "authors": [
                "Mihai Cucuringu",
                "Apoorv Vikram Singh",
                "D\u00e9borah Sulem",
                "Hemant Tyagi"
            ],
            "title": "Regularized spectral methods for clustering signed networks",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "da Xu",
                "chuanwei ruan",
                "evren korpeoglu",
                "sushant kumar",
                "kannan achan"
            ],
            "title": "Inductive representation learning on temporal graphs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Micha\u00ebl Defferrard",
                "Xavier Bresson",
                "Pierre Vandergheynst"
            ],
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Palash Goyal",
                "Nitin Kamra",
                "Xinran He",
                "Yan Liu"
            ],
            "title": "Dyngem: Deep embedding method for dynamic graphs. IJCAI Workshop on Representation Learning for Graphs, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Palash Goyal",
                "Sujit Rokka Chhetri",
                "Arquimedes Canedo"
            ],
            "title": "dyngraph2vec: Capturing network dynamics using dynamic graph representation learning",
            "venue": "Knowl. Based Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Grassi",
                "Andreas Loukas",
                "Nathana\u00ebl Perraudin",
                "Benjamin Ricaud"
            ],
            "title": "A time-vertex signal processing framework: Scalable processing and meaningful representations for time-series on graphs",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2017
        },
        {
            "authors": [
                "David K Hammond",
                "Pierre Vandergheynst",
                "R\u00e9mi Gribonval"
            ],
            "title": "Wavelets on graphs via spectral graph theory",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2011
        },
        {
            "authors": [
                "Bal\u00e1zs Hidasi",
                "Alexandros Karatzoglou"
            ],
            "title": "Recurrent neural networks with top-k gains for sessionbased recommendations",
            "venue": "In Proceedings of the 27th ACM international conference on information and knowledge management,",
            "year": 2018
        },
        {
            "authors": [
                "Shenyang Huang",
                "Farimah Poursafaei",
                "Jacob Danovitch",
                "Matthias Fey",
                "Weihua Hu",
                "Emanuele Rossi",
                "Jure Leskovec",
                "Michael Bronstein",
                "Guillaume Rabusseau",
                "Reihaneh Rabbany"
            ],
            "title": "Temporal graph benchmark for machine learning on temporal graphs",
            "venue": "arXiv preprint arXiv:2307.01026,",
            "year": 2023
        },
        {
            "authors": [
                "Junzheng Jiang",
                "Hairong Feng",
                "David B. Tay",
                "Shuwen Xu"
            ],
            "title": "Theory and design of joint time-vertex nonsubsampled filter banks",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 1968
        },
        {
            "authors": [
                "Mengyuan Jing",
                "Yanmin Zhu",
                "Yanan Xu",
                "Haobing Liu",
                "Tianzi Zang",
                "Chunyang Wang",
                "Jiadi Yu"
            ],
            "title": "Learning shared representations for recommendation with dynamic heterogeneous graph convolutional networks",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),",
            "year": 2022
        },
        {
            "authors": [
                "Wang-Cheng Kang",
                "Julian McAuley"
            ],
            "title": "Self-attentive sequential recommendation",
            "venue": "IEEE international conference on data mining (ICDM), pp. 197\u2013206",
            "year": 2018
        },
        {
            "authors": [
                "B\u00fcnyamin Kartal",
                "Eray \u00d6zg\u00fcnay",
                "Aykut Ko\u00e7"
            ],
            "title": "Joint time-vertex fractional fourier transform, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Seyed Mehran Kazemi",
                "Rishab Goel",
                "Kshitij Jain",
                "Ivan Kobyzev",
                "Akshay Sethi",
                "Peter Forsyth",
                "Pascal Poupart"
            ],
            "title": "Representation learning for dynamic graphs: A survey",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Takashi Kurokawa",
                "Taihei Oki",
                "Hiromichi Nagao"
            ],
            "title": "Multi-dimensional graph fourier transform, 2017",
            "year": 2017
        },
        {
            "authors": [
                "P. Lancaster",
                "H.K. Farahat"
            ],
            "title": "Norms on direct sums and tensor products",
            "venue": "Mathematics of Computation,",
            "year": 1972
        },
        {
            "authors": [
                "Jiacheng Li",
                "Yujie Wang",
                "Julian McAuley"
            ],
            "title": "Time interval aware self-attention for sequential recommendation",
            "venue": "In Proceedings of the 13th international conference on web search and data mining,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaohan Li",
                "Mengqi Zhang",
                "Shu Wu",
                "Zheng Liu",
                "Liang Wang",
                "S Yu Philip"
            ],
            "title": "Dynamic graph collaborative filtering",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2020
        },
        {
            "authors": [
                "Andreas Loukas",
                "Damien Foucard"
            ],
            "title": "Frequency analysis of time-varying graph signals",
            "venue": "IEEE Global Conference on Signal and Information Processing (GlobalSIP),",
            "year": 2016
        },
        {
            "authors": [
                "Chen Ma",
                "Peng Kang",
                "Xue Liu"
            ],
            "title": "Hierarchical gating networks for sequential recommendation",
            "venue": "In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2019
        },
        {
            "authors": [
                "Yao Ma",
                "Ziyi Guo",
                "Zhaocun Ren",
                "Jiliang Tang",
                "Dawei Yin"
            ],
            "title": "Streaming graph neural networks",
            "venue": "In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Arash Golibagh Mahyari",
                "Selin Aviyente"
            ],
            "title": "Fourier transform for signals on dynamic graphs",
            "venue": "In 2014 48th Asilomar Conference on Signals, Systems and Computers,",
            "year": 2014
        },
        {
            "authors": [
                "Franco Manessi",
                "Alessandro Rozza",
                "Mario Manzo"
            ],
            "title": "Dynamic graph convolutional networks",
            "venue": "Pattern Recognit.,",
            "year": 2020
        },
        {
            "authors": [
                "Pedro Mercado",
                "Francesco Tudisco",
                "Matthias Hein"
            ],
            "title": "Clustering signed networks with the geometric mean of laplacians",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Apurva Narayan",
                "Peter HO\u2019N Roe"
            ],
            "title": "Learning graph dynamics using deep neural networks. IFAC-PapersOnLine",
            "year": 2018
        },
        {
            "authors": [
                "Antonio Ortega",
                "Pascal Frossard",
                "Jelena Kova\u010devi\u0107",
                "Jos\u00e9 MF Moura",
                "Pierre Vandergheynst"
            ],
            "title": "Graph signal processing: Overview, challenges, and applications",
            "venue": "Proceedings of the IEEE,",
            "year": 2018
        },
        {
            "authors": [
                "Chao Pan",
                "Siheng Chen",
                "Antonio Ortega"
            ],
            "title": "Spatio-temporal graph scattering transform",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Aldo Pareja",
                "Giacomo Domeniconi",
                "Jie Chen",
                "Tengfei Ma",
                "Toyotaro Suzumura",
                "Hiroki Kanezashi",
                "Tim Kaler",
                "Tao B. Schardl",
                "Charles E. Leiserson"
            ],
            "title": "Evolvegcn: Evolving graph convolutional networks for dynamic graphs",
            "venue": "In The Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "Steffen Rendle",
                "Christoph Freudenthaler",
                "Zeno Gantner",
                "Lars Schmidt-Thieme"
            ],
            "title": "Bpr: Bayesian personalized ranking from implicit feedback",
            "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
            "year": 2009
        },
        {
            "authors": [
                "Steffen Rendle",
                "Christoph Freudenthaler",
                "Lars Schmidt-Thieme"
            ],
            "title": "Factorizing personalized markov chains for next-basket recommendation",
            "venue": "In Proceedings of the 19th international conference on World wide web,",
            "year": 2010
        },
        {
            "authors": [
                "Emanuele Rossi",
                "Ben Chamberlain",
                "Fabrizio Frasca",
                "Davide Eynard",
                "Federico Monti",
                "Michael Bronstein"
            ],
            "title": "Temporal graph networks for deep learning on dynamic graphs",
            "venue": "arXiv preprint arXiv:2006.10637,",
            "year": 2020
        },
        {
            "authors": [
                "Purnamrita Sarkar",
                "Deepayan Chakrabarti",
                "Michael Jordan"
            ],
            "title": "Nonparametric link prediction in dynamic networks",
            "venue": "arXiv preprint arXiv:1206.6394,",
            "year": 2012
        },
        {
            "authors": [
                "Youngjoo Seo",
                "Micha\u00ebl Defferrard",
                "Pierre Vandergheynst",
                "Xavier Bresson"
            ],
            "title": "Structured sequence modeling with graph convolutional recurrent networks",
            "venue": "CoRR, abs/1612.07659,",
            "year": 2016
        },
        {
            "authors": [
                "Min Shi",
                "Yu Huang",
                "Xingquan Zhu",
                "Yufei Tang",
                "Yuan Zhuang",
                "Jianxun Liu"
            ],
            "title": "Gaen: Graph attention evolving networks",
            "venue": "In IJCAI,",
            "year": 2021
        },
        {
            "authors": [
                "David I Shuman",
                "Sunil K Narang",
                "Pascal Frossard",
                "Antonio Ortega",
                "Pierre Vandergheynst"
            ],
            "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
            "venue": "IEEE signal processing magazine,",
            "year": 2013
        },
        {
            "authors": [
                "Duraisamy Sundararajan"
            ],
            "title": "The discrete fourier transform",
            "venue": "In Signals and Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaxi Tang",
                "Ke Wang"
            ],
            "title": "Personalized top-n sequential recommendation via convolutional sequence embedding",
            "venue": "In Proceedings of the eleventh ACM international conference on web search and data mining,",
            "year": 2018
        },
        {
            "authors": [
                "Terrence Tao"
            ],
            "title": "When are eigenvalues stable",
            "venue": "URL https://terrytao.wordpress.com/",
            "year": 2008
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Marisel Villafa\u00f1e-Delgado",
                "Selin Aviyente"
            ],
            "title": "Dynamic graph fourier transform on temporal functional connectivity networks",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2017
        },
        {
            "authors": [
                "Jianling Wang",
                "Kaize Ding",
                "Liangjie Hong",
                "Huan Liu",
                "James Caverlee"
            ],
            "title": "Next-item recommendation with sequential hypergraphs",
            "venue": "In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Minjie Wang",
                "Da Zheng",
                "Zihao Ye",
                "Quan Gan",
                "Mufei Li",
                "Xiang Song",
                "Jinjing Zhou",
                "Chao Ma",
                "Lingfan Yu",
                "Yu Gai",
                "Tianjun Xiao",
                "Tong He",
                "George Karypis",
                "Jinyang Li",
                "Zheng Zhang"
            ],
            "title": "Deep graph library: A graph-centric, highly-performant package for graph neural networks",
            "venue": "arXiv preprint arXiv:1909.01315,",
            "year": 2019
        },
        {
            "authors": [
                "Xiyuan Wang",
                "Muhan Zhang"
            ],
            "title": "How powerful are spectral graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Shu Wu",
                "Yuyuan Tang",
                "Yanqiao Zhu",
                "Liang Wang",
                "Xing Xie",
                "Tieniu Tan"
            ],
            "title": "Session-based recommendation with graph neural networks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Xintao Xiang",
                "Tiancheng Huang",
                "Donglin Wang"
            ],
            "title": "Learning to evolve on dynamic graphs (sa)",
            "venue": "In Thirty-Sixth AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Sijie Yan",
                "Yuanjun Xiong",
                "Dahua Lin"
            ],
            "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Bing Yu",
                "Haoteng Yin",
                "Zhanxing Zhu"
            ],
            "title": "Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting",
            "venue": "arXiv preprint arXiv:1709.04875,",
            "year": 2017
        },
        {
            "authors": [
                "Mengqi Zhang",
                "Shu Wu",
                "Xueli Yu",
                "Qiang Liu",
                "Liang Wang"
            ],
            "title": "Dynamic graph neural networks for sequential recommendation",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Kun Zhou",
                "Hui Yu",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "title": "Filter-enhanced mlp is all you need for sequential recommendation",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "T X\u0302f"
            ],
            "title": "Xf is the equivalent of X\u0302G in equation 6 that is the output of EFT. In practice, the fast Fourier transform is used that can perform the computations in order O(T log(T )). Hence, overall time complexity of the architecture is O((N +E)T +NTlogT ). To map the output back to the original space from the interpolated space we would need further mapping layers",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In numerous practical situations, graphs exhibit temporal characteristics, as seen in applications like social networks, citation graphs, and bank transactions, among others (Kazemi et al., 2020). These temporal graphs can be divided into two types: 1) temporal graphs with constant graph structure (Grassi et al., 2017; Cao et al., 2020), and 2) temporal graphs with dynamic structures (Zhou et al., 2022; Bastos et al., 2023; da Xu et al., 2020). Our focus in this work is the latter case.\nThe evolving graphs have been comprehensively studied from the spatio-temporal graph-neural network (GNN) perspective, focusing on propagating local information (Pareja et al., 2020; Shi et al., 2021; Xiang et al., 2022; da Xu et al., 2020). Albeit the success of spectral GNNs for static graphs for capturing non-local dependencies in graph signals (Wang & Zhang, 2022), they have not been applied to temporal graphs with evolving structure. To make spectral GNN work for temporal graphs effectively and efficiently, there is a necessity for an invertible transform that collectively captures evolving spectra along the graph vertex and time domain. To the best of our knowledge, there exists no such transform in the spectral domain for temporal graphs with evolving structures.\nIn the present literature, Graph Fourier Transform (GFT), which is a generalization of Fourier Transform, exists for static graphs but can not capture spectra of evolving graph structure (Shuman et al., 2013). Hence, it cannot be applied to temporal graphs due to the additional temporal aspect. One naive extension would be to treat the time direction as a temporal edge, construct a directed graph with newly added nodes at each timestep, and find the Eigenvalue Decomposition (EVD) of the joint graph. However, this would lose the distinction between variation along temporal and vertex domains. Moreover, such an approach would incur an added computational cost by a multiplicative factor of O(T 3), which would be prohibitively high for the temporal setting with a large number of timesteps. Thus, in this paper, we attempt to find an approximation to the dynamic graph transform that would capture its evolving spectra and be efficient to compute.\nWe aim to propose a novel transform for a temporal graph to its frequency domain. For this we consider the Laplacian of the dynamic graph and find the orthogonal basis of maximum variation to obtain the spectral transform (Hammond et al., 2011). We view this as an optimization of the variational form of the Laplacian such that the optimal value is within the \u03f5\u2212 pseudospectrum (Tao, 2008). We then show that such optimization gives us a simple and efficient to compute solution while also being close to the exact solution of the variational form under certain conditions of Lipschitz continuous dynamic graphs. Effectively, we propose a method to simultaneously perform spectral transform along both the time and vertex dimensions of a dynamic graph. This solves the following challenges over the natural extension of EVD over dynamic graphs: 1) The proposed transformation is computationally efficient compared to the direct eigendecomposition of the joint Laplacian. 2) Distinction between time and vertex domain frequency components with the proposed transform provides interpretability to the transformed spectral domain. We term the proposed concept as \"Evolving Graph Fourier Transform\" (EFT ).\nIn summary, we make the following key contributions:\n\u2022 We propose EFT (grounded on theoretical foundations), that transforms a temporal graph to its frequency domain for capturing evolving spectra.\n\u2022 We provide the theoretical bounds of the difference between EFT and the exact solution to the variational form and analyze its properties.\n\u2022 As a reference implementation, we develop a simple neural model induced with the proposed transform to process and filter the signals on the dynamic graphs for downstream tasks. We perform extensive experimentation on large-scale and standard datasets for dynamic graphs to show that our method can effectively filter out the noise signals and enhance task performance against baselines."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Spectral Graph Transforms: Work by (Hammond et al., 2011) was among the first to propose a computationally efficient algorithm to compute the Fourier Transform for static graphs. Loukas et al. (Loukas & Foucard, 2016) conceptualized Joint Fourier Transform (JFT) over graphs on which the signals change with time. JFT has been generalized in (Kartal et al., 2022) by proposing the Joint Fractional Fourier Transform (JFRT). However, JFT and JFRT does not consider graph structures evolving with time. (Cao et al., 2021) apply JFT and propose a model for time series forecasting. (Villafa\u00f1e-Delgado & Aviyente, 2017) summarized graphs over time by using Tucker decomposition to the dynamic graph Laplacian in order to obtain an orthogonal matrix and further applies it to a cognitive control experiment. However, this method does not fully capture the varying graph information in a lossless sense. Researchers have also proposed spectral methods for spatio-temporal applications such as action recognition Yan et al. (2018); Pan et al. (2020), traffic forecasting Yu et al. (2017) etc. Other works such as (Mahyari & Aviyente, 2014; Chen et al., 2022; Sarkar et al., 2012; Kurokawa et al., 2017; Jiang et al., 2021; Cheng et al., 2023) also consider temporal graphs, but ignore the evolving structure. We position our work as the novel spectral graph transform for temporal graphs which is currently a gap in existing literature.\nTemporal Graph Representation Learning: Since static graph methods do not work well with dynamic graphs (Pareja et al., 2020), researchers have proposed a slew of methods (Pareja et al., 2020; Goyal et al., 2020; Xiang et al., 2022), for learning on dynamic graphs for problems such as link prediction and node classification. One elementary way to adapt methods developed for static graphs on dynamic graphs is to use RNN modules in conjunction with GNN modules to capture the evolving graph dynamics. Researchers (Seo et al., 2016; Narayan & Roe, 2018; Manessi et al., 2020) have explored this idea extensively. Some other recent approaches model several real world phenomena, however, these methods rely on an RNN for encoding temporal information such as Bastas et al. (2019), (da Xu et al., 2020), Ma et al. (2020), etc. Most generic among these works is TGN (Temporal Graph Networks) (Rossi et al., 2020) that remembers nodes and connections it has seen in the past, and then uses that memory to update new nodes and connections that it hasn\u2019t seen before. However, the memory updater uses GRU which may have issues such as vanishing gradient limiting the ability to capture long term information. Also, these models have been studied for small-graphs spread over limited time duration (e.g., one month). Considering large scale temporal graphs with evolving structures, one such application is that of sequential recommendation (SR) with decades of temporal information (1996-2018) (Zhang et al.,\n2022; Huang et al., 2023). Researchers (Li et al., 2020b; Zhang et al., 2022; Jing et al., 2022) have attempted to model the sequential recommendation task as a link prediction over dynamic graphs. DGSR (Zhang et al., 2022) is a work that considers generic dynamic graphs over user-item interactions. However, the GNN-based methods described in this section including DGSR majorly employ low pass GNNs that limit the ability to model complex relations and are fundamentally restricted to focus on local neighborhood interactions (Balcilar et al., 2020)."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Discrete Fourier Transform (DFT) (Sundararajan, 2023) is employed to obtain the frequency representation of a sequence of signal values sampled at equal intervals of time. Consider a signal x sampled at N intervals of time t \u2208 [0, N \u2212 1] to obtain the sequence {xt}. The DFT of xt is then given by Xk = \u2211N\u22121 t=0 xte\n\u2212i\u03c9tk with \u03c9t = 2\u03c0tN . The transformed sequence Xk gives the values of the signal in the frequency domain. If we represent X as the vector form of the signal, we can define the DFT matrix \u03a8T such that Xk = \u03a8TX .\nGraph Fourier Transform (GFT) (Ortega et al., 2018) is a generalization of the Discrete Fourier Transform (DFT) to graphs. We represent a graph as (V, E) where V is the set of N nodes and E represents the edges between them. Denote the adjacency matrix by A. D is the degree matrix, defined as (D)ii = \u2211 j(A)ij , which is diagonal. The graph Laplacian graph is given by L\u0302 = D \u2212A and the normalized Laplacian L is defined as L = I \u2212 D\u2212 12AD\u2212 12 . The Laplacian(L) has the eigendecomposition as: L = \u03a8\u2217G\u039b\u03a8G. Let X \u2208 RN\u00d7d be the signal on the nodes of the graph. The Graph Fourier Transform X\u0302 of X is then given as: X\u0302 = \u03a8GX .\nPseudospectrum: The spectrum of a graph (of N nodes) is a finite set consisting of N points \u03bb that form the eigenvalues of the graph\u2019s matrix representation M i.e. {\u03bb \u2208 C | \u2225\u2225(M \u2212 \u03bbI)\u22121\u2225\u2225 = \u221e}. Similarly we can think of the (\u03f5-)pseudospectrum of a graph to be the larger set (containing these N points) such that A\u2212 \u03bbI has the least singular value at most \u03f5. Formally the pseudospectrum can be defined by the set {\u03bb \u2208 C |\n\u2225\u2225(M \u2212 \u03bbI)\u22121\u2225\u2225 \u2265 1\u03f5 }. Common Notations: We denote by \u2295, \u2297 the Kronecker sum, product respectively. (M)ji refers to the i-th row and j-th column of matrix M . {.} refers to a sequence, of elements, in time. \u22a0,\u229e refer to the Kronecker product and sum respectively, applied timestep wise."
        },
        {
            "heading": "4 THEORETICAL FRAMEWORK: AN OPTIMIZATION PERSPECTIVE",
            "text": "We begin by striving for a physical interpretation of frequency for dynamic graph systems. For this, we draw inspiration from energy diffusion processes and establish similarities with the variation of signals on static graphs. Consider graph Gt at time t with node ni \u2208 Vt and nj\nGt\u223c ni denoting the neighbors of ni at time t. We define a directed graph JD with the graphs at all timesteps taken as is and a directed edge added from a node at time t \u2212 1 (modulo T ) to its\ncorresponding node at time t. For continuous time dynamic graph the previous time would be represented by t\u2212 dt (modulo T ). Let Xni,t represent the energy of the signal on node ni at time t. The flow of energy to the node ni at time t can be represented by the divergence of the gradient (\u2206ni,tX) of the energy. We define the variation of the signals at time t and node ni as follows:\n\u2225\u2206ni,tX\u22252 = [\u2211\nnj JD\u223c ni\n( \u2202X\n\u2202eninj\n)2] 12 = [\u2211 nj Gt\u223cni ( Xnj ,t \u2212Xni,t )2 + ( \u2202Xni,t \u2202t dt )2] 12 , where\n\u2202X \u2202eninj is the discrete edge derivative on the collective dynamic graph JD. Considering \u2206 to be the finite difference between neighboring nodes in the joint graph, the global notion of variation (Sp(X)) can be given by the p-Dirichlet form as follows\nSp(X) = 1\np N\u2211 n=1 \u222b T t=0 \u2225\u2206ni,tX\u2225 p 2 dt = 1 p \u222b T t=0 N\u2211 n=1  \u2211 nj Gt\u223cni ( Xnj ,t \u2212Xni,t )2 + (\u03b4Xni,t) 2  p 2 dt\nDefine LT to be the Laplacian of the continuous ring graph representing the nodes at each timestep t \u2208 [0, T ] and connecting consequent nodes. Let LGt be the Laplacian of the sampled graph at time t. In the discrete case the Laplacian LJD of JD can be shown to be\n(LJD ) j i = (LT \u2297 IN ) j i + (IT \u2297 {LGt}) j\u230a jN \u230b i = (LT \u2295 LGt) j\u230a jN \u230b i (1)\nFor the case of continuous time, this can be generalized to (LJD ) = LT \u2297 IN + [IT \u22a0 {LGt}] = [LT \u229e LGt ] (2) where \u22a0,\u229e refers to the timestep wise Kronecker product and sum respectively and [.] refers to the matricization operation. In the discrete case this operation would convert RNT\u00d7T\u00d7N \u2212\u2192 RNT\u00d7NT , ordering from the last dimension first. We can now characterize the variation of signals on JD similar to static graphs by the following result: Lemma 1. (Variational Characterization of JD) The 2-Dirichlet S2(X) of the signals X on JD is the quadratic form of the Laplacian LJD of JD i.e.\nS2(X) = \u222b NT i=0 vec(X)(i) \u222b NT j=0 LJD (i, j)vec(X)(j)didj = vec(X) TLJDvec(X) (3)\nThis implies that LJD \u2ab0 0 since S2(X) \u2265 0, which assures us of the existence of the eigenvalue decomposition. Additionally, the value of S2(X) is lower when the signal changes slower along the dynamic graph and higher when the signal changes faster. Hence, we can define a notion of signal variation on the dynamic graph that is similar to the variation of signals on static graphs. Consequently, the eigendecomposition of LJDcharacterizes signals on the dynamic graph by projecting them onto the optimizers of S2(X). This means that high-frequency components of the evolving dynamic graph represent sharply varying signals, whereas smoother signals will have a higher magnitude in the low-frequency components. From an optimization perspective, we can view the maximum frequency as the optimal value for the below equation, i.e.,\nfmax = max x,\u2225x\u2225\u22641 \u222b NT i=0 x(i) \u222b NT j=0 LJD (i, j)x(j)didj = max x,\u2225x\u2225\u22641 xTLJD (i, j)x (4)\nThe optimal solution x provides the basis for transforming a dynamic graph signal to obtain its maximum frequency component, denoted by fmax. We can obtain the next frequency values by optimizing equation 4 in orthogonal directions. However, this approach has an issue - the eigenvalue decomposition would have to be performed over a large number of nodes. In a real world setting of temporal graphs with T timesteps, this method would have a complexity of O((NT )3), which would be prohibitive considering large number of timesteps. To address this issue, we relax the objective in equation 4 to include solutions in the pseudospectrum. The solution is presented in the following result, upon which we can formulate a transformation method for temporal graphs.\nLemma 2. Consider the variational form xTLJDx = \u222b NT i=0 x(i) \u222b NT j=0\nLJD (i, j)x(j)didj. The optimization problem f = max\nx,\u2225x\u2225\u22641 [|xTLJDx\u2212\u03bbs| \u2212 \u03f5]+ has the optimal solution as y\u03c9 \u2297 z\u03c9l , where\n\u03bbs is the optimal value of equation 4, y\u03c9 is the \u03c9-th optimal solution of the variational form of the ring graph, ztl is the l-th optimal solution to the variational form of the graph at time t, [s]+ = max(s, 0) and \u03f5 = O(\u03b4)."
        },
        {
            "heading": "5 CONSTRUCTING AN EVOLVING GRAPH FOURIER TRANSFORM",
            "text": "In the previous section, we have outlined the theoretical framework for the evolving graph Fourier transform. We also obtained a sketch of the transform as a solution to the optimization problem of the variational characterization with pseudospectrum relaxations. This enables us to obtain a simple and efficient form to compute. In this section, building upon the theoretical framework, we propose our formulation of the Evolving Graph Fourier Transform (EFT). From lemma 2, we obtain the orthogonal basis vectors of the desired transform matrix in terms of the kronecker product of the basis vectors of the Fourier Transform (\u03a8T ) and Graph Fourier Transform (\u03a8G). Thus, lemma 2 helps us to define the EFT in terms of the graph and time Fourier transforms:\nEFT (fg, \u03c9) = \u2211 n \u03a8G(fg, n) \u222b T t=0 fs(n, t)e \u2212j\u03c9tdt (5)\nwhere fg, \u03c9 are the graph and temporal frequency components respectively, fs(n, t) is the signal at node n and time t. In terms of the matrix representation, the EFT could be expressed, using the Einstein notation (Albert et al., 1916), as a Kronecker product of DFT and GFT as (\u03a8D) j i = (\u03a8T \u2297{\u03a8Gt}) j\u230a jN \u230b i , which when applied to the columnwise vectorized signal fs gives the transform in the spectral space.\nEFT is one of the solutions in the pseudospectrum of LJD as shown in lemma 2. There also exists other solutions and specifically considering the case where \u03f5 = 0 we obtain the solution to the exact EVD of LJD . Let \u03a8AD be the matrix whose rows form the right eigenvectors of LJD . Since \u03a8AD is the absolute decomposition of LJDwe term this as AD for brevity. We now define error bounds between \u03a8D and \u03a8AD.\nTheorem 1. Considering bounded changes in a graph G with N nodes over time T , the norm of the difference between EFT (\u03a8D) and AD (\u03a8AD) is bounded as follows: \u2225\u03a8D \u2212\u03a8AD\u2225 \u2264 O ( N 3 2T\u03b5(\u03c9max, (\u2206\u03bbG)min, (\u2206\u03bbJ)min) )(\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225) max where (\u2206\u03bbJ)min and (\u2206\u03bbJ)min refer to the minimum difference between the eigenvalues of matrices LG and LJD respectively, L\u0307G is the rate of change of LG and \u03c9max = 2\u03c0 and \u03b5(\u03c9max,\u2206\u03bbG,\u2206\u03bbJ) = \u03c9 1 2 max\n\u2206\u03bbG + \u03c92max \u2206\u03bbJ .\nThe above theorem states that as the structure on the graph evolves infinitesimally, the difference between \u03a8D and \u03a8AD is bounded from above by the change in the graph matrix representation (laplacian/adjacency). This property is desirable since it allows us to approximate \u03a8AD, which is formed by the eigendecomposition of LJD and has a physical interpretation, using the defined \u03a8D that is easy to compute. The above bound is finite if 1) The rate of change of the graph with time is bounded. 2) The eigenvalues have a multiplicity of 1. In such cases, EFT characterizes signals on the dynamic graph by their proximity (projection) to the optimizers of S2(X) defined in lemma 1. The physical implication of this is that applying EFT , the high-frequency components correspond to sharply varying signals on a dynamic graph, while low-frequency components correspond to smoother signals. Hence, the norm of the difference between EFT and AD are bounded from above by the rate of evolution of the graphs.\nFor computational purpose in real-world applications, the sampled form of EFT can be obtained by sampling T snapshots of the dynamic graph signal at uniform time intervals. We now get a dynamic graph {(Vt, Et)}, t \u2208 {0, T} the edges (Et) of which by definition evolves with time. We consider the node set V to be fixed, i.e., no new nodes are added. All the nodes (|V| = N ) are known from the start, and the graph may contain isolated nodes. In case of node editions, we could create dummy isolated nodes with varying node signals and edge connectivity information. Without loss of generality, consider a 1-dimensional temporal signal, uniformly sampled at T intervals, residing on the graph nodes. Let X \u2208 RN\u00d7T represent the temporal signal on the graph nodes. The Fourier transform (DFT) (with DFT matrix \u03a8T ) independently for each node is DFT (X) = X\u03a8\u22a4T . Further, the GFT for the graph Gt \u2261 (Vt, Et) at time t is given as GFT (Xt) = \u03a8GtXt, where Xt \u2208 RN is the signal on the graph nodes at time t. In order to compute the dynamic graph transform along the graph domain as well as the temporal dimension, we can collectively perform both the operations.\nConsider {\u03a8Gt} \u2208 RN\u00d7N\u00d7T as the tensor containing the graph Fourier basis at each timestep. Then using Einstein notation (Albert et al., 1916), we write EFT as\n(EFT({Gt};X))ji = (\u03a8GtX) kk i ( \u03a8\u22a4T )j k\n(6)\nwhere i, j, k are tensor indices. Next, we aim to define a transformation matrix for EFT as in DFT and GFT. For this we make use of the Kronecker product (\u2297) between tensors. We then get the matrix form of EFT as the following expression:\n(EFT({Gt};X))ji = ( X\u0302G )j i = (\u03a8GtX) kk i ( \u03a8\u22a4T )j k = (\u03a8T \u2297 {\u03a8Gt}) km (j\u2217N+i) xk (7)\nThus, we have x\u0302j\u2217N+i = (\u03a8T \u2297 {\u03a8Gt}) km (j\u2217N+i) xk or x\u0302 = \u03a8Dx. In the above equations, X\u0302G is the EFT of signal X over dynamic graph {Gt}, x, x\u0302 \u2208 RNT are the columnwise vectorized form of X, X\u0302G \u2208 RN\u00d7T and m = \u230a k N \u230b . \u03a8D \u2208 RNT\u00d7NT is the EFT matrix over dynamic graph {Gt} with\n(\u03a8D) j i = (\u03a8T \u2297\u03a8G) j\u230a jN \u230b i .\nWe remark from equation 6 of EFT , that the following desirable properties (over the exact eigendecompostion of the joint laplacian) are satisfied: 1) The equation imparts interpretibility to the frequency components, whether belonging to the time or vertex domain, as compared to the exact eigendecomposition. This is possible because we are able to decompose the transform into the individual transforms of each domain. 2) The transform equation is computationally efficient as compared to the exact eigendecomposition of the joint laplacian. Specifically EFT reduces the computational complexity for the dynamic graph (T timesteps) from a factor of O(T 3) to O(T + T log(T )). Having derived the EFT transform, we state and prove its properties in the appendix C. The illustration between EFT and other transforms is in Figure 1. The figure shows transforms (GFT, JFT, DFT, EFT) in a circle, and arrows from one transform to the next indicate that the source transform can be obtained by the destination transform using the simulation annotated on the edges. For example, the GFT of a ring graph (T ) gives the DFT and thus the DFT can be simulated by GFT using graph T . Similarly DFT can be simulated by EFT when the number of nodes N = 1. Also the GFT of the temporal ring of a static graph (topologically equivalent to a torus), where the nodes and edges remain constant with time, gives the EFT and vice versa (when time T = 1). However when the graph structure changes with time GFT cannot be used to simultae EFT . Thus, we can also look at the EFT as a generalization of the previous transforms. We briefly explain the task specific implementation of these modules in the below subsection and focus more on the representations and results in the following sections."
        },
        {
            "heading": "5.1 IMPLEMENTATION DETAILS",
            "text": "Having obtained the representations using the proposed transform, we intend to perform filtering in spectral space for dynamic graphs. Since our idea is to perform collective filtering along the vertex and temporal domain in EFT, we need two modules to compute \u03a8Gt (vertex aspect) and \u03a8T (temporal aspect), respectively, in equation 6 of EFT. We now briefly explain these modules with details in appendix D.2.\nFiltering along the Vertex Domain: This module computes the convolution matrix \u03a8Gt in equation 6. The frequency response of the desired filter is approximated as \u039b\u0302l = \u2211Of k=0 ckTk(\u039b\u0303), where Of is the polynomial/filter order, Tk is the Chebyshev polynomial basis, \u039b\u0303 = 2\u039b\u03bbmax \u2212 I , \u03bbmax is the maximum eigenvalue and ck is the corresponding filter coefficients. The convolution of the graph signal X with the filter (X \u2217 \u039bl) gives the desired filter response in the vertex domain. Filtering along the Temporal Domain: After performing filtering in the vertex domain, we aim to filter over the temporal signals using \u03a8T as in equation 6. Formally, let Xt \u2208 Rd be the signal of a node at time t. Let X = {Xt} \u2208 RT\u00d7d be the time ordered matrix of embeddings of the node. This is converted to the frequency domain (X\u0302 \u2208 RT\u00d7d) using the matrix \u03a8T as X\u0302 = \u03a8TX . Then we multiply X\u0302 element-wise by a temporal filter FT \u2208 RT\u00d7d to obtain the filtered signal X\u0302f = FT \u2299 X\u0302 which is then converted back to the temporal domain by using the inverse transform \u03a8\u2217T to get Xf = \u03a8 \u2217 T X\u0302f . Xf is the filtered signal in the time-vertex domain of the dynamic graph."
        },
        {
            "heading": "6 EXPERIMENTAL SETUP",
            "text": "Model Implementation and Datasets: Considering EFT is a spectral transform, we need a base model to induce EFT in it. We select transformer as the base model inspired from (Zhou et al., 2022; Bastos et al., 2022) that induce learnable filters into a vanilla transformer for downstream tasks (implementation is inspired from (Zhou et al., 2022), hence, details are in appendix). To illustrate the efficacy of the representations obtained from EFT, we consider eight datasets. We name our model EFT-T. The first three (Amazon Beauty, Games, CD in Table 3) are large continuous time dynamic graph datasets from sequential recommendation (SR) setting (Huang et al., 2023), spread over two decades. We inherit these datasets, dynamic graph construction process in SR setting, and metric from (Zhang et al., 2022). Other datasets (Pareja et al., 2020) (UCI, AS, SBM, Elliptic, Brain) are standard (discrete) dynamic graph datasets to understand the generalizability of our method and contain a sequence of time-ordered graphs. Details on datasets, metrics, and experiment settings are in Appendix (cf., Table 4). Experiment code and associated datasets are on Github: https://github.com/nadgeri14/ICLR_EFT.\nBaselines: We use baselines depending on the experiment setting for fairness. For SR link prediction, we use strong baselines from previous best (Zhang et al., 2022): BPR-MF (Rendle et al., 2009), FPMC (Rendle et al., 2010), GRU4Rec+ (Hidasi & Karatzoglou, 2018), Caser (Tang & Wang, 2018), SASRec (Kang & McAuley, 2018), HGN (Ma et al., 2019), TiSASRec (Li et al., 2020a), SRGNN (Wu et al., 2019), HyperRec (Wang et al., 2020), FMLPRec (Zhou et al., 2022), and DGSR (Zhang et al., 2022). For link prediction, node classification on discrete dynamic graph datasets, we rely on state of the art approaches of this setting (Xiang et al., 2022): GCN (Kipf & Welling, 2017), GAT (Velic\u030ckovic\u0301 et al., 2018), GCN-GRU (Pareja et al., 2020), DynGEM (Goyal et al., 2017), GAEN (Shi et al., 2021), EvolveGCN (Pareja et al., 2020), dyngraph2vec (dg2vec) (Goyal et al., 2020)."
        },
        {
            "heading": "7 RESULTS AND DISCUSSION",
            "text": "This section reports the various experiment results, supporting our theoretical contributions.\ntransforms collectively across time and vertex dimensions, will result in better denoising and signal reconstruction compared to using GFT or DFT, which only performs filtering in one dimension. Our hypothesis is confirmed in Figure 2, which shows a decrease in error as the spectral energy of the signal is preserved while noise is filtered. Moreover, EFT yields comparable results to absolute transform (AD ) while requiring less computational resources.\nCompactness of EFT : Compaction refers to the ability of the transform to summarize the data compactly. A transform with good compaction is desirable as it summarizes the signals well in the frequency components, which can be used for efficient processing by downstream models. In this experiment, we verify the compaction properties of the proposed transform for the time-vertex frequencies on the temporal mesh graphs (Grassi et al., 2017) concerning GFT and DFT. In order to test this, we remove varying percentile of the frequency components from the transformed frequency domain of signal X . We then apply the inverse transform to obtain the signal Xr. We plot the error \u2225X\u2212Xr\u2225F\n\u2225X\u2225F vs the percentile of components removed. From figure 3a, 3b we can see that EFT has a lower error and better compaction and thus is able to summarize the data better than the baselines that only transform along a single dimension of vertex or time.\nTable 1: For link prediction on large temporal graphs of sequential recommendation setting, table shows our model comparison (EFT-T) on the metrics Recall@10 and NDCG@10. The best results are shown in boldface. The second best result has been underlined. The improvement of our method over the best-performing baseline is statistically significant with p < 0.05.\nGRU4Rec+ Caser SASRec HGN TiSASRec FMLPRec SRGNN HyperRec DGSR EFT-T Recall@10\nBeauty 43.98 42.64 48.54 48.63 46.87 47.47 48.62 34.71 52.40 53.23 Games 67.15 68.83 73.98 71.42 71.85 73.62 73.49 71.24 75.57 77.78 CDs 67.84 61.65 71.32 71.42 71.00 72.41 69.63 71.02 72.43 75.42 NDCG@10 Beauty 26.42 25.47 32.19 32.47 30.45 32.38 32.33 23.26 35.90 37.10 Games 45.64 45.93 53.60 49.34 50.19 51.26 53.35 48.96 55.70 58.65 CDs 44.52 45.85 49.23 49.34 48.97 53.31 48.95 47.16 51.22 54.99\n(a) Dog (b) Dancer (c) Dancer\nFigure 3: Representations on dynamic mesh datasets. Left (a,b): Reconstruction error on the datasets illustrating the compactness of EFT . Right (c): Illustration of filtering using EFT on the dynamic mesh of a Dancer.\nIllustration of filtering on temporal mesh Figure 3c shows an example of collective filtering of a dynamic mesh representing a dancer (Grassi et al., 2017). Similar to Grassi et al. (2017), we implement the following filters: (a) a low-pass filter that\njointly attenuates high frequency components of the dynamic graph, and (b) a wave filter whose frequency response is described in Eq. (19) of Grassi et al. (2017). The former filter gives us the frame of the mesh with stiff manoeuvers, whereas the fluid filter produces fluid movements. This experiment shows that EFT can enhance the frequency components non-linearly. This also hints towards why EFT performs better on evolving temporal graphs in subsequent experiments.\nPerformance comparison on (continuous) large-scale temporal graph datasets: The results on the large-scale SR datasets are in Table 1 and EFT-T outperforms baselines on all datasets. We note that our gains to the best baseline are higher in CDs, followed by the Games and Beauty dataset. We observe that as the density of the graph and length of sequences in the data increases (e.g., CD dataset), the performance of EFTT enhances. We believe that as graph density increases, higherorder connections may encompass\nnoisy relations, a challenge conventional baselines struggle to filter out, whereas our method effectively handles this noise. Also, EFT effectively captures global interactions as it considers the temporal aspect in the collective filtering module. Furthermore, compared to the FMLPRec model that induces DFT into a transformer, EFT-T performs significantly better, concluding the necessity of capturing evolving spectra of temporal graphs. We also note that among the graph-based methods, SRGNN only considers connectivity information from the sequence graph, whereas HyperRec uses higher-order connectivity information. This indicates that not using the graph information effectively hampers performance but using higher-order connectivity without filtering to remove noise also degrades the results.\nPerformance comparison on discrete temporal graph datasets: Table 2 summarizes link prediction and node classification results. Across datasets, our model significantly outperforms all baselines, which focus on learning local dependencies. It illustrates our framework\u2019s effectiveness in filtering noise and amplifying useful signals in evolving temporal graphs.\nEffectiveness of filtering module (Figure 4): Our approach focuses on capturing useful frequencies along vertex and time dimensions collectively while filtering the noise. Hence, in this experiment, we aim to understand the effectiveness of the filters along both graphs (vertex) and time dimension in the presence of explicitly added noise.\nFirstly, we induce semantic noise into the system by adding a random vector (sampled from a normal distribution) to the node embeddings. Then, we run experiments on our model with and without learnable collective graph-time filters. To ensure a fair comparison, we keep the parameters in both models the same and simulate the no-filter configuration by using a uniform distribution for the frequency response (all-pass filter). In the presence of noise, the performance of configuration with filters is much better (p < 0.01) than that without any filtering. Next, we induce structural noise into the system by adding random nodes/edges. We observe that on inducing structural noise, the performance of the con-\nfiguration with graph filters is statistically better (p < 0.01 using a paired t-test) compared to the one without, confirming that collective filtering is needed to be robust to structural noise in dynamic graphs. Additionally, we plotted the filter frequency responses of EFT on the Games and CDs datasets in Figure 5. The figure shows dominating low-frequency response and higher-frequency components, indicating global aggregation for the long-range interactions."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "In this paper, we introduce a novel approach to transform temporal graphs into the frequency domain, grounded on theoretical foundations. We propose pseudospectrum relaxations to the variational objective obtaining a simplified transformation, making it computationally efficient for real-world applications. We show that the error between the proposed transform and the exact solution to the variational objective is bounded from above and study its properties. We further demonstrate the practical effectiveness for temporal graphs. In the current scope, we do not consider generic signed and directed graphs. To mitigate this, we suggest future works explore generalizing the Laplacian and the resulting transform to such graphs, leveraging techniques proposed in (Mercado et al., 2016; Cucuringu et al., 2021). Our work opens up new possibilities for dynamic graph analysis and representation learning, and we encourage researchers to explore potential of EFT as a spectral representation of the evolving graph in downstream graph representation learning models."
        },
        {
            "heading": "A PRELIMINARIES",
            "text": "Here we give an extended discussion of the preliminaries which could not be accommodated in the main paper due to space constraints.\nA.1 DISCRETE FOURIER TRANSFORM\nThe Discrete Fourier Transform (DFT) is used to obtain the frequency representation of a sequence of signal values sampled at equal intervals of time. The magnitude and phase of the frequency components are obtained by multiplying the signal values by complex sinusoids of the respective frequencies. Consider a signal x sampled at N intervals of time t \u2208 [0, N \u2212 1] to obtain the sequence {xt}. The DFT of xt is then given by,\nXk = N\u22121\u2211 t=0 xte \u2212i\u03c9tk, \u03c9t = 2\u03c0t N (8)\nThe transformed sequence Xk gives the values of the signal in the frequency domain. If we represent X as the vector form of the signal we can define the DFT matrix \u03a8T such that Xk = \u03a8TX . Thus Xk is the complex valued spectrum of {xt} at frequency \u03c9t. We can perform filtering by removal of noisy frequencies in this spectral domain. As required by signal processing applications we can then obtain the signal sequence in the time domain from the frequency domain {Xk} using the Inverse Discrete Fourier Transform (IDFT) as,\nxt = 1\nN N\u22121\u2211 k=0 Xke i\u03c9kt, \u03c9k = 2\u03c0k N (9)\nA.2 GRAPH FOURIER TRANSFORM\nGraph Fourier Transform (GFT) is a generalization of the Discrete Fourier Transform to graphs. We represent a graph as (V, E) where V is the set of N nodes and E represents the edges between them. Denote the adjacency matrix by A. In the setting of an undirected graph A would be a symmetric matrix. D is the degree matrix, defined as (D)ii = \u2211 j(A)ij , which is diagonal. The Laplacian of the graph is given by L\u0302 = D\u2212A and the normalized Laplacian L is defined as L = I\u2212D\u2212 12AD\u2212 12 . The Laplacian(L) can be decomposed into its orthogonal basis, namely the eigenvectors and eigenvalues as:L = \u03a8\u2217G\u039b\u03a8G, where U is an N \u00d7N matrix whose columns are the eigenvectors corresponding to the eigenvalues \u03bb1, \u03bb2, . . . , \u03bbN and \u039b = diag([\u03bb1, \u03bb2, . . . , \u03bbn]). Let X \u2208 RN\u00d7d be the signal on the nodes of the graph. The Fourier Transform X\u0302 of X is then given as: X\u0302 = \u03a8GX . Similarly, the inverse Fourier Transform is defined as: X = \u03a8\u2217GX\u0302 . Note \u03a8 \u2217 G is the transposed conjugate of \u03a8G. By the convolution theorem (Blackledge, 2005), the convolution of the signal X with a filter G having its frequency response as G\u0302 is given by (below, m represents the mth node in the graph, \u03a8Gk represents the kth eigenvector or column of \u03a8G):\n(X \u2217G)(m) = N\u2211\nk=1\nX\u0302(k)G\u0302(k)\u03a8Gk(m)\n(X \u2217G)(m) = N\u2211\nk=1\n(\u03a8GX)(k)G\u0302(k)\u03a8 \u2217 Gk (m)\nX \u2217G = \u03a8\u2217GG\u0302\u03a8GX\n(10)\nNote that a sequence can be considered as a grid graph and for this graph the GFT specializes to the DFT i.e \u03a8T = \u03a8G."
        },
        {
            "heading": "B THEORETICAL PROOFS",
            "text": "In this section we outline the proofs stated in the main paper and briefly discuss the implications etc. that could not be accommodated in the main paper due to space constraints. For completeness we restate the results.\nLemma 1. (Variational Characterization of JD) The 2-Dirichlet S2(X) of the signals X on JD is the quadratic form of the Laplacian LJD of JD i.e.\nS2(X) = vec(X) TLJDvec(X)\nProof. The p-Dirichlet form is given by\nSp(X) = 1\np N\u2211 n=1 \u222b T t=0 \u2225\u2206ni,tX\u2225 2 p\n= 1\np T\u2211 t=1 N\u2211 n=1  \u2211 nj Gt\u223cni ( Xnj ,t \u2212Xni,t )2 + ( \u2202Xni,t \u2202t dt )2 p 2\nThus the 2-Dirichlet form is\nS2(X) = 1\n2 N\u2211 n=1 \u222b T t=0 \u2225\u2206ni,tX\u2225 2 2\n= 1\n2 \u222b T t=0 N\u2211 n=1  \u2211 nj Gt\u223cni ( Xnj ,t \u2212Xni,t )2 + ( \u2202Xni,t \u2202t dt )2 = 1\n2 \u222b T t=0 N\u2211 n=1 \u2211 nj Gt\u223cni (\u03b4Xni,t) 2  + 1\n2 (\u222b T t=0 N\u2211 n=1 (Xni,t\u2212dt \u2212Xni,t) 2 ) We consider the above sum in two parts. Taking the first part we have\n1\n2 \u222b T t=0 N\u2211 n=1 \u2211 nj Gt\u223cni ( Xnj ,t \u2212Xni,t )2\n= 1\n2 \u222b T t=0 N\u2211 n=1 \u2211 nj Gt\u223cni ( X2nj ,t \u2212 2 \u2217Xnj ,t \u2217Xni,t +X 2 ni,t )\n= 1\n2 \u222b T t=0 N\u2211 n=1 \u2211 nj Gt\u223cni ( X2nj ,t \u2212Xnj ,t \u2217Xni,t \u2212Xnj ,t \u2217Xni,t +X 2 ni,t )\n= 1\n2 \u222b T t=0 N\u2211 n=1 \u2211 nj Gt\u223cni ( Xnj ,t ( Xnj ,t \u2212Xni,t ) +Xni,t ( Xni,t \u2212Xnj ,t ))\n= 1\n2 \u222b T t=0 N\u2211 n=1 2 \u2217 \u2211\nnj Gt\u223cni\n( Xnj ,t ( Xnj ,t \u2212Xni,t ))\n= \u222b T t=0 N\u2211 n=1 \u2211 nj Gt\u223cni ( Xnj ,t ( Xnj ,t \u2212Xni,t )) = vec(X)\u22a4(IT \u2297 LGt)vec(X)\nConsidering the second part which is the ring graph along the time dimension we have\n1\n2 (\u222b T t=0 N\u2211 n=1 (Xni,t\u2212dt \u2212Xni,t) 2 )\n= 1\n2 (\u222b T t=0 N\u2211 n=1 X2ni,t\u2212dt \u2212 2 \u2217Xni,t\u2212dt \u2217Xni,t +X 2 ni,t )\n= 1\n2 (\u222b T t=0 N\u2211 n=1 2X2ni,t\u2212dt \u2212 2 \u2217Xni,t\u2212dt \u2217Xni,t ) . . . Redistributing terms\n= \u222b T t=0 N\u2211 n=1 X2ni,t\u2212dt \u2212Xni,t\u2212dt \u2217Xni,t\n= \u222b T t=0 N\u2211 n=1 Xni,t\u2212dt (Xni,t\u2212dt \u2212Xni,t) = vec(X)\u22a4(LT \u2297 IN )vec(X)\nCombining the results of the 2 parts we get the below result\nS2(X) = vec(X) \u22a4(IT \u2297 LGt)vec(X) + vec(X)\u22a4(LT \u2297 IN )vec(X)\n= vec(X)\u22a4 (IT \u2297 LGt + LT \u2297 IN ) vec(X) = vec(X)\u22a4LJDvec(X)\nas required.\nThis implies that LJD \u2ab0 0. We can see that slower the changes in the signals along the dynamic graph smaller the value of S2(X) and vice versa. Thus we have a notion of variation of signals on the dynamic graph similar to the case of static graphs. The eigen decomposition of LJD therefore characterizes signals on the dynamic graph by its projection to the optimizers of S2(X). In other words, high collective dynamic graph frequency components inform of the presence of sharply varying signals and smoother signals will have higher magnitude in the low frequency components. Next we provide a solution to the relaxed pseudospectrum objective in 2.\nLemma 2. Consider the variational form xTLJDx = \u222b NT i=0 x(i) \u222b NT j=0\nLJD (i, j)x(j)didj. The optimization problem f = max\nx,\u2225x\u2225\u22641 [|xTLJDx\u2212\u03bbs| \u2212 \u03f5]+ has the optimal solution as y\u03c9 \u2297 z\u03c9l , where\n\u03bbs is the optimal value of equation 4, y\u03c9 is the \u03c9-th optimal solution of the ring graph, ztl is the l-th optimal solution of the graph at time t and \u03f5 = O(\u03b4).\nProof. We begin by considering the variational characterization of LJD which is given by the below equation\n\u03bbs = max x,\u2225x\u2225\u22641 \u222b NT i=0 x(i) \u222b NT j=0 LJD (i, j)x(j)didj max x,\u2225x\u2225\u22641 xTLJDx (11)\nNote that in the objective, xTLJDx is convex since LJD \u2ab0 0 and thus \u22072 ( xTLJDx ) = 2LJD \u2ab0 0. Also, we can check that \u2225x\u22252 \u2264 1 is convex. Thus applying the KKT conditions [cite] to the lagrangian L = xTLJDx+ \u03bb(\u2225x\u2225 2 \u2212 1), we get the below equation\nLJDx = \u03bbsx (12)\nWe recognize from the above equation that \u03bbs is the eigenvalue of LJD and x is the corresponding eigenvector. However the computation of this exact solution is computationally costly and here we are ineterested in finding an efficient form of the solution to the objective with the pseudospectrum relaxation. As already seen, the pseudospectrum can be defined by the set {\u03bb \u2208 C | \u2225\u2225(LJD \u2212 \u03bbI)\u22121\u2225\u2225 \u2265 1\u03f5 } or equivalently {\u03bb \u2208 C | \u2225(LJD \u2212 \u03bbI)\u2225 \u2264 \u03f5}, where\n\u2225.\u2225 is the operator norm. Thus we have that for the pseudospectrum, there exists a unit vector v such that |(LJD \u2212 \u03bbI)v| \u2264 \u03f5 and so |\u03bbs \u2212 \u03bb| \u2264 \u03f5. This shows that the \u03f5\u2212 neighborhood of the spectrum of LJD is contained in the pseudospectrum i.e. if \u03bb is in the pseudospectrum of LJD it is in the \u03f5\u2212 neighborhood of \u03bbs. We would now like to find a solution residing in the pseudospectrum of LJD .\nWe have {LGt} \u2208 RN\u00d7N\u00d7T to be the Laplacian of the graphs at each timestep with eigenvalues \u03bbti where i \u2208 N, t \u2208 [0, T ]. LT \u2208 RT\u00d7T be the Laplacian of the time adjacency matrix with eigenvalues \u00b5j where j \u2208 T . The Laplacian of the collective graph JD is expressed as\nLJD = LT \u2295 {LGt} = LT \u2297 IN + [IT \u22a0 {LGt}] In the above equation, \u22a0 is the timestep wise Kronecker product and operator [.] represents the vectorization. If T is discrete this vectorization can be thought of as a reordering from RNT\u00d7T\u00d7N \u2212\u2192 RNT\u00d7NT . Consider a1, a2, . . . ap to be the linearly independent right eigenvectors of LT and bt1, b t 2, . . . b t qt to be the linearly independent right eigenvectors of LGt . Consider the vector y = [ak \u22a0 {btl}], where {btl} represents the set of eigenvectors of Laplacian at time t i.e. LGt and the operator \u22a0 is again timestep wise followed by vectorization. Then we have\nLJDy = LT \u2297 INy + [IT \u22a0 {LGt}]y = (LT \u2297 {IN})[ak \u22a0 {btl}] + [IT \u22a0 {LGt}][ak \u22a0 {btl}] = (LT \u2297 {IN}\u25a1[ak \u22a0 {btl}]) + (IT \u2297 {LGt}\u25a1[ak \u22a0 {btl}]) = [LTak \u22a0 {IN}\u25a1{btl}] + [ITak \u22a0 {LGt\u25a1{btl}}] = (\u00b5k[ak \u22a0 {btl}]) + [ak \u22a0 {\u03bbtl{btl}}] = (\u00b5k[ak \u22a0 {btl}] + [ak \u22a0 {\u03bbtl{btl}}] = ([ak \u22a0 {btl}]diag({\u00b5k}) + [ak \u22a0 {btl}diag({\u03bbtl})] = ([ak \u22a0 {btl}]diag({\u00b5k + \u03bbtl}))\nwhere \u25a1 indicates timestep (column) wise product and diag(.) operator converts a vector to a diagonal matrix. Now considering the graph at the 0-th timestep having eigenvalue \u03bb0l , we are interested in verifying the pseudospectrum condition for \u00b5k + \u03bb0l }. We thus have to find the upper bound for\u2225\u2225LJD \u2212 (\u00b5k + \u03bb0l )I\u2225\u2225. In order to bound the above expression we consider the vector y = [ak \u22a0 {btl}]. We have from the above equations,\u2225\u2225LJDy \u2212 (\u00b5k + \u03bb0l )y\u2225\u2225 = ([ak \u22a0 {btl}]diag({\u00b5k + \u03bbtl \u2212 (\u00b5k + \u03bb0l )})) (13) We would like to study the rate of change of the eigenvalues as the graph changes. Consider a normal matrix A of which the eigenvectors v1, . . . , vn form a basis of Cn. Also we consider w1, . . . , wn be the dual basis, i.e. w\u2217j vk = \u03b4jk for all 1 \u2264 j, k \u2264 n, where \u03b4jk is the Kronecker delta and\n\u03b4jk = { 1, if j = k 0, otherwise\nSince the eigenvectors form a basis we can represent any vector u as a linear combination of v1, . . . , vn as u = \u2211n j=1 ajvj . Also we have w \u2217 ju = \u2211n j=1 ajw \u2217 j vj = aj . We thus have the below equation\nu = n\u2211 j=1 (w\u2217ju)vj (14)\nfor any vector u \u2208 Cn. We know the below relation due to vk being the eigenvector of A with eigen value \u03bbk Avk = \u03bbkvk (15) We also can write the following in terms of the dual basis (since A is a normal matrix)\nw\u2217kA = \u2211 j \u03bbjw \u2217 kvjw \u2217 j w\u2217kA = \u03bbkw \u2217 k\n(16)\nWe now differentiate 21 using the product rule of differentiation to get\nA\u0307vk +Av\u0307k = \u03bb\u0307kvk + \u03bbkv\u0307k (17)\nTaking the inner product of the equation 23 with w\u2217k, and using 22 we obtain:\nA\u0307vk +Av\u0307k = \u03bb\u0307kvk + \u03bbkv\u0307k\nw\u2217kA\u0307vk + w \u2217 kAv\u0307k = w \u2217 k\u03bb\u0307kvk + w \u2217 k\u03bbkv\u0307k\nw\u2217kA\u0307vk + \u03bbkw \u2217 kv\u0307k = \u03bb\u0307kw \u2217 kvk + \u03bbkw \u2217 kv\u0307k\nw\u2217kA\u0307vk = \u03bb\u0307k\n\u03bb\u0307k = w \u2217 kA\u0307vk\n(18)\nAssuming \u03bb0k to be the eigenvalue at the start, we can get the value after time t by simply integrating as follows,\n\u03bbtk = \u03bb 0 k + \u222b t 0 \u03c9kA\u0307vk0dt (19)\nThus from the above result and equation 13 we have\u2225\u2225LJDy \u2212 (\u00b5k + \u03bb0l )y\u2225\u2225 = \u2225\u2225[ak \u22a0 {btl}]diag({\u00b5k + \u03bbtl \u2212 (\u00b5k + \u03bb0l )})\u2225\u2225 = \u2225\u2225[ak \u22a0 {btl}]diag(\u03bbtl \u2212 \u03bb0l })\u2225\u2225\n= \u2225\u2225\u2225\u2225[ak \u22a0 {btl}]diag(\u222b t 0 \u03c9kA\u0307vk0dt) \u2225\u2225\u2225\u2225 \u2264 \u2225\u2225[ak \u22a0 {btl}]\u2225\u2225\u2225\u2225\u2225\u2225diag(\u222b t\n0\n\u03c9kA\u0307vk0dt) \u2225\u2225\u2225\u2225 \u2264\n\u2225\u2225\u2225\u2225\u2225 \u222b T 0 \u222b t 0 \u03c9kA\u0307vk0dtdt \u2225\u2225\u2225\u2225\u2225 \u2264\n\u2225\u2225\u2225\u2225\u2225 \u222b T 0 \u222b t 0 \u2225\u2225\u2225\u03c9kA\u0307vk0\u2225\u2225\u2225dtdt \u2225\u2225\u2225\u2225\u2225\n\u2264 \u2225\u2225\u2225\u2225\u2225 \u222b T 0 \u222b t 0 \u2225\u2225\u2225A\u0307\u2225\u2225\u2225dtdt\u2225\u2225\u2225\u2225\u2225 \u2264\n\u2225\u2225\u2225\u2225\u2225 \u222b T 0 \u222b t 0 \u03b4Ndtdt \u2225\u2225\u2225\u2225\u2225 \u2264\n\u2225\u2225\u2225\u2225\u2225 \u222b T 0 \u03b4NTdt \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4NT 2\n\u2264 O(\u03b4)\u2225\u2225LJDy \u2212 (\u00b5k + \u03bb0l )y\u2225\u2225 \u2264 \u03f5\u2225\u2225LJD \u2212 (\u00b5k + \u03bb0l )\u2225\u2225 \u2264 \u03f5 Thus \u00b5k + \u03bb0l is in the pseudospectrum of LJD and so y = [ak \u22a0 {btl}] is one of the solutions to the objective with the pseudospectrum relaxation. Thus it follows that [\u03a8T \u22a0{\u03a8Gt}] forms a basis of the solution to the defined objective, where \u03a8T and \u03a8Gt have a \u2217 k and b t l as their row spaces respectively.\nThe above result gives us the definition of EFT in terms of the Kronecker product of the Time Fourier Transform and the Graph Fourier Transform of the graph at each time. While both EFT and AD are solutions to the pseudospectrum relaxed objectives they are not equal in general. To see this, we\nfirst need to look at the eigenvectors of LJD . Let \u03a8AD be the matrix whose rows form the right eigenvectors of LJD . Below we state and prove the result of equivalence between \u03a8D and \u03a8AD for the general case of dynamic graphs using a counter example\nRemark 1. In general, the collective dynamic graph fourier transform as defined by the operator \u03a8D does not form the eigenspace of the spectrum of LJD i.e. \u03a8D \u0338= \u03a8AD.\nProof. It is sufficient to show a single counter example to conclude the statement.\nConsider the below weighted adjacency matrix for a certain graph at time t0\nG0 = [ 1 0.5 0.5 1 ] Let this change to the following in the next timestep t1\nG1 = [ 1 0.6 0.6 1 ] The Laplacian LJD is given by\nLJD =  1.5 \u22120.5 \u22121. \u22120.\u22120.5 1.5 \u22120. \u22121.\u22121. \u22120. 1.6 \u22120.6 \u22120. \u22121. \u22120.6 1.6  The EFT matrix \u03a8D is\n\u03a8D = 0.5 \u22120.5 \u22120.5 0.50.5 0.5 \u22120.5 \u22120.50.5 \u22120.5 0.5 \u22120.5 0.5 0.5 0.5 0.5  Similarly the matrix \u03a8AD comes out to be the following (upto sign and row wise permutations)\n\u03a8AD = 0.47 \u22120.47 \u22120.52 0.520.5 0.5 \u22120.5 \u22120.50.52 \u22120.52 0.47 \u22120.47 0.5 0.5 0.5 0.5  From the above we can see the two matrices differ and so we have a counter example.\nFrom the above result we can see that in the general case of dynamic graphs the defined EFT and the eigen decomposition of the defined Laplacian LJDare not the same. Thus we can have an alternate definition of the collective dynamic graph fourier transform in terms of the decomposition of the joint Laplacian LJD . We term \u03a8AD as the Absolute Drcomposition or AD for brevity.\nBoth EFT and AD have their own advantages. EFT has a simple primal definition and is easy to compute whereas AD has a beautiful physical interpretation. Even though EFT and AD are not exactly the same, in order to have desirable properties of both we can define approximation bounds that inform under what conditions the two transforms can be used interchangeably upto the approximation error. We work under the below assumptions for weighted graphs in order to bound the two transforms:\n1. The rate of change of the graph with time is bounded 2. The eigenvalues of the graph Laplacian at any given timestep and between timesteps has a\nmultiplicity of 1\nThe condition 2 is required for stability of the bound and can be enforced for example by adding random perturbations to the matrix.\nBased on these assumptions we state and prove the bounds between EFT and AD below\nTheorem 1. Considering bounded changes in a graph G with N nodes over time T , the norm of the difference between EFT (\u03a8D) and AD (\u03a8AD) is bounded as follows: \u2225\u03a8D \u2212\u03a8AD\u2225 \u2264 O ( N 3 2 T\u03c9 1 2 max\n(\u2206\u03bbG)min +\nN 3 2 T\u03c92max (\u2206\u03bbJ )min )(\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225) max where (\u2206\u03bbG)min and (\u2206\u03bbJ)min refer to the minimum\ndifference between the eigenvalues of matrices LG and LJD respectively, L\u0307G is the rate of change of LG and \u03c9max = 2\u03c0.\nProof. Consider LJ to be the Laplacian of collective graph when the graphs are static with time. We can show this, in a similar manner as LJD to be LJ = LT \u2297 IN + IT \u2297 LG, where LG is the Laplacian of the static graph. Let \u03a8J be the matrix whose rows form the left eigenvectors of LJ . Now we consider the graph to change infinitesimally with LG as the starting state of the Laplacian. We intend to bound the frobenius norm \u2225\u03a8D \u2212\u03a8AD\u2225. We can manipulate this as follows\n\u2225\u03a8D \u2212\u03a8AD\u2225 = \u2225\u03a8D \u2212\u03a8AD +\u03a8J \u2212\u03a8J\u2225 = \u2225\u03a8D \u2212\u03a8J +\u03a8J \u2212\u03a8AD\u2225 \u2264 \u2225\u03a8D \u2212\u03a8J\u2225+ \u2225\u03a8J \u2212\u03a8AD\u2225\nWe thus find the bound in two parts first for the error between \u03a8D,\u03a8J and second for the error between \u03a8J ,\u03a8AD.\nIn order to bound the matrices (which are formed by the eigenvectors) we first attempt to bound the vectors forming the matrix. For this we study the rate of change of the vectors with time (as the graph evolves) using the language of calculus. For deeper insights into this and the stability of eigenvectors/values we refer the interested reader to (Tao, 2008). Consider a normal matrix A of which the eigenvectors v1, . . . , vn form a basis of Cn. Also we consider w1, . . . , wn be the dual basis, i.e. w\u2217j vk = \u03b4jk for all 1 \u2264 j, k \u2264 n, where \u03b4jk is the Kronecker delta and\n\u03b4jk = { 1, if j = k 0, otherwise\nSince the eigenvectors form a basis we can represent any vector u as a linear combination of v1, . . . , vn as u = \u2211n j=1 ajvj . Also we have w \u2217 ju = \u2211n j=1 ajw \u2217 j vj = aj . We thus have the below equation\nu = n\u2211 j=1 (w\u2217ju)vj (20)\nfor any vector u \u2208 Cn. We know the below relation due to vk being the eigenvector of A with eigen value \u03bbk Avk = \u03bbkvk (21) We also can write the following in terms of the dual basis (since A is a normal matrix)\nw\u2217kA = \u2211 j \u03bbjw \u2217 kvjw \u2217 j w\u2217kA = \u03bbkw \u2217 k\n(22)\nWe now differentiate 21 using the product rule of differentiation to get\nA\u0307vk +Av\u0307k = \u03bb\u0307kvk + \u03bbkv\u0307k (23)\nTaking the inner product of the equation 23 with w\u2217k, and using 22 we obtain:\nA\u0307vk +Av\u0307k = \u03bb\u0307kvk + \u03bbkv\u0307k\nw\u2217kA\u0307vk + w \u2217 kAv\u0307k = w \u2217 k\u03bb\u0307kvk + w \u2217 k\u03bbkv\u0307k\nw\u2217kA\u0307vk + \u03bbkw \u2217 kv\u0307k = \u03bb\u0307kw \u2217 kvk + \u03bbkw \u2217 kv\u0307k\nw\u2217kA\u0307vk = \u03bb\u0307k\n\u03bb\u0307k = w \u2217 kA\u0307vk\n(24)\nIn our case since A is normal, we have the eigenbasis vk as an orthonormal set, and the dual basis wk is identical to vk.\nWe are interested in how the eigenvectors change with time. Taking the inner product of equation 23 with w\u2217j for j \u0338= k, we get\nA\u0307vk +Av\u0307k = \u03bb\u0307kvk + \u03bbkv\u0307k\nw\u2217j A\u0307vk + w \u2217 jAv\u0307k = w \u2217 j \u03bb\u0307kvk + w \u2217 j\u03bbkv\u0307k\nw\u2217j A\u0307vk + \u03bbjw \u2217 j v\u0307k = \u03bb\u0307kw \u2217 j vk + w \u2217 j\u03bbkv\u0307k\nw\u2217j A\u0307vk + \u03bbjw \u2217 j v\u0307k = w \u2217 j\u03bbkv\u0307k\nw\u2217j A\u0307vk + \u03bbjw \u2217 j v\u0307k \u2212 w\u2217j\u03bbkv\u0307k = 0\nw\u2217j A\u0307vk + (\u03bbj \u2212 \u03bbk)w\u2217j v\u0307k = 0\nw\u2217j v\u0307k = w\u2217j A\u0307vk\n(\u03bbk \u2212 \u03bbj)\n(25)\nUsing the above in 20 we obtain the following\nv\u0307k = n\u2211 j=1 (w\u2217j v\u0307k)vj\nv\u0307k = \u2211 j \u0338=k (w\u2217j v\u0307k)vj + (w \u2217 kv\u0307k)vk\nv\u0307k = \u2211 j \u0338=k w\u2217j A\u0307vk \u03bbk \u2212 \u03bbj vj + (w \u2217 kv\u0307k)vk\n(26)\nWe consider the change in A so that the resulting matrix is also normal. Thus the eigenvectors of the resulting matrix will also be orthonormal. This imples all the vectors lie on the surface of the unit sphere in Cn and so the change in the eigenvectors should be along the surface of this sphere. As such A\u0307vk will be tangential to the sphere at vk and so v\u0307\u22a4k vk = 0. Note this need not be the case in general if we consider non-unit vectors (that could also be eigenvectors). Thus we can represent v\u0307k = 0vk + \u2211 j \u0338=k bjvj . Thus we have\nv\u0307k = vk + \u2211 j \u0338=k bjvj\nw\u2217kv\u0307k = w \u2217 k \u2211 j \u0338=k bjvj  w\u2217kv\u0307k = \u2211 j \u0338=k bjw \u2217 kvj\n w\u2217kv\u0307k = 0\nUsing the above equations and the consideration that \u2225vj\u2225 = 1 we have\nv\u0307k = \u2211 j \u0338=k w\u2217j A\u0307vk \u03bbk \u2212 \u03bbj vj\n\u2225v\u0307k\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 j \u0338=k w\u2217j A\u0307vk \u03bbk \u2212 \u03bbj vj \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u2211 j \u0338=k \u2225\u2225\u2225\u2225\u2225 w\u2217j A\u0307vk\u03bbk \u2212 \u03bbj vj \u2225\u2225\u2225\u2225\u2225\n\u2264 \u2211 j \u0338=k \u2225\u2225\u2225\u2225\u2225 w\u2217j A\u0307vk\u03bbk \u2212 \u03bbj \u2225\u2225\u2225\u2225\u2225\u2225vj\u2225\nSince we consider orthonormal vectors \u2225vj\u2225 = 1\n\u2234 \u2225v\u0307k\u2225 \u2264 \u2211 j \u0338=k \u2225\u2225\u2225\u2225\u2225 w\u2217j A\u0307vk\u03bbk \u2212 \u03bbj \u2225\u2225\u2225\u2225\u2225\n\u2264 \u2211 j \u0338=k \u2225\u2225\u2225w\u2217j A\u0307vk\u2225\u2225\u2225 \u2225\u03bbk \u2212 \u03bbj\u2225\n\u2264 \u2211 j \u0338=k \u03c3(A\u0307) \u2225\u03bbk \u2212 \u03bbj\u2225 \u2264 \u2211 j \u0338=k \u2225\u2225\u2225A\u0307\u2225\u2225\u2225 \u2225\u03bbk \u2212 \u03bbj\u2225 \u2264 \u2211 j \u0338=k \u2225\u2225\u2225A\u0307\u2225\u2225\u2225 (\u2206\u03bb)min\n\u2264 N \u2212 1 (\u2206\u03bb)min\n\u2225\u2225\u2225A\u0307\u2225\u2225\u2225\nwhere \u03c3(.) is the operator norm and (\u2206\u03bb)min is the absolute of the minimum difference between the eigenvalues of A. in the above we have seen how the change in eigenvectors is bounded by the change in the matrix. Using this result we now attempt to bound the change in the required transform matrices.\nFor the first part we bound \u2225\u03a8D \u2212\u03a8J\u2225. Let \u2206v represent the (infinitesimal) change in the eigenvectors of LG in time t and let \u2206vi be the infinitesimal change per unit time in the vector at step i. Thus using the triangle inequality we have the below equations\n\u2225\u2206v\u2225 \u2264 \u2211 i \u2225\u2206vi\u2225\n\u2225\u2206v\u2225 \u2264 \u222b t \u2225v\u0307(t)\u2225dt\nUsing the above derived inequality \u2225v\u0307k\u2225 \u2264 N\u22121(\u2206\u03bb)min \u2225\u2225\u2225A\u0307\u2225\u2225\u2225 for LG we have \u2225\u2206v\u2225 \u2264\n\u222b t \u2225\u2225\u2225\u2225 N \u2212 1(\u2206\u03bb)min \u2225\u2225\u2225L\u0307G(t)\u2225\u2225\u2225\u2225\u2225\u2225\u2225dt\n\u2264 \u222b t N \u2212 1 (\u2206\u03bb)min (\u2225\u2225\u2225L\u0307G(t)\u2225\u2225\u2225) max dt\nFinally taking the maximum norm of the rate of change in LG over the entire time duration we have the following\n\u2225\u2206v\u2225 \u2264 N \u2212 1 (\u2206\u03bb)min (\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225) max \u222b t dt\n\u2225\u2206v\u2225 \u2264 N \u2212 1 (\u2206\u03bb)min (\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225) max t\n\u2264 O(N \u2212 1)T (\u2206\u03bb)min (\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225) max\nwhere ( L\u0307G ) max\nis the maximum of the norm of the rate of change of LG over all timesteps considered and we absorb the total time t into the constant factor considering finite time. Thus we have, \u2225\u2225\u2225\u03a8\u0307G\u2225\u2225\u2225 =\u221a\u2211\ni\n\u2225vi\u22252\n\u2264 \u221a N(N \u2212 1)T (\u2206\u03bbG)min\n( L\u0307G ) max\n\u2264 O(N 3 2T )\n(\u2206\u03bbG)min\n( L\u0307G ) max\nThus using theorem 8 from (Lancaster & Farahat, 1972) and the fact that \u2225\u03a8tT \u2225 = 1 we have, \u2225\u03a8D \u2212\u03a8J\u2225 = \u2225([\u03a8T \u22a0 ({\u03a8Gt} \u2212\u03a8G)])\u2225\n= (\u222b \u03c9 \u2225\u03a8\u03c9T \u2297 (\u03a8G\u03c9 \u2212\u03a8G)\u2225 2 ) 1 2\n\u2264 (\u222b\n\u03c9\n\u2225\u03a8\u03c9T \u2225 2\u2225(\u03a8G\u03c9 \u2212\u03a8G)\u2225 2\n) 1 2\n\u2264 (\u222b\n\u03c9\n\u2225(\u03a8G\u03c9 \u2212\u03a8G)\u2225 2\n) 1 2\n\u2264 (\u222b\n\u03c9\n\u2225\u2206\u03a8G\u03c9\u2225 2\n) 1 2\n\u2264 \u222b \u03c9 ( O(N 32T ) (\u2206\u03bbG)min \u2225\u2225\u2225L\u0307G\u03c9\u2225\u2225\u2225 max )2 12\n\u2264 \u03c9max( O(N 32T ) (\u2206\u03bbG)min \u2225\u2225\u2225L\u0307G\u2225\u2225\u2225 max )2 12\n\u2225\u03a8D \u2212\u03a8J\u2225 \u2264\n( O(N 32T\u03c9 1 2 max)\n(\u2206\u03bbG)min\n\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225 max\n)\nwhere T is the time duration over which the evolution ghof the graphs is considered.\nFor the second part we can show that \u2225\u03a8J \u2212\u03a8AD\u2225 = \u2225\u2225\u2225\u03a8\u0307J\u2225\u2225\u2225\n= \u221a\u222b N\u03c9max i=0 \u2225vi\u22252 \u2264 \u221a N\u03c9max(N\u03c9max \u2212 1)T\n(\u2206\u03bbJ)min\n( L\u0307J ) max\n\u2264 O((N\u03c9max) 3 2T )\n(\u2206\u03bbJ)min\n( L\u0307J ) max\nAlso we have, LJ = LT \u2295 LG\n= LT \u2297 IN + IT \u2297 LG L\u0307J = IT \u2297 L\u0307G\u2225\u2225\u2225L\u0307J\u2225\u2225\u2225 = \u2225\u2225\u2225IT \u2297 L\u0307G\u2225\u2225\u2225\u2225\u2225\u2225L\u0307J\u2225\u2225\u2225 = \u2225IT \u2225\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225\u2225\u2225\u2225L\u0307J\u2225\u2225\u2225 = \u222b\n\u03c9\n(d\u03c9) 1 2 \u2225\u2225\u2225L\u0307G\u2225\u2225\u2225\u2225\u2225\u2225L\u0307J\u2225\u2225\u2225 = \u221a\u03c9\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225 \u2234 \u2225\u03a8J \u2212\u03a8AD\u2225 \u2264\nO(N 32 ) (\u2206\u03bbJ)min\n( L\u0307J ) max\n\u2264 O(N 3 2\u03c92T )\n(\u2206\u03bbJ)min\n( L\u0307G ) max\nCombining the two parts we get the result \u2225\u03a8D \u2212\u03a8AD\u2225 \u2264 \u2225\u03a8D \u2212\u03a8J\u2225+ \u2225\u03a8J \u2212\u03a8AD\u2225\n\u2264 O\n( N 3 2T\u03c9 1 2 max\n(\u2206\u03bbG)min +\nN 3 2T\u03c92max\n(\u2206\u03bbJ)min\n)(\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225) max\n(27)\nFor the discrete case this bound becomes\n\u2225\u03a8D \u2212\u03a8AD\u2225 \u2264 O\n( (NT ) 3 2\n(\u2206\u03bbG)min +\n(NT 2) 3 2\n(\u2206\u03bbJ)min\n)(\u2225\u2225\u2225L\u0307G\u2225\u2225\u2225) max\n(28)\nWe thus see that as the graph evolves infinitesimally the difference between \u03a8D and \u03a8AD is bounded from above by the change in the graph matrix representation. This is desirable since it allows us to approximate \u03a8AD (formed by the eigendecomposition of LJD ) which has a physical interpretation using the defined \u03a8D which is simple to compute, when the graph changes in a stable manner. In such cases, EFT therefore characterizes signals on the dynamic graph by its proximity (projection) to the optimizers of S2(X) meaning high (collective dynamic graph) frequency components correspond to sharply varying signals and low frequency components to smoother signals. Having derived the transform, we next state and prove the properties of the proposed transform in the next section."
        },
        {
            "heading": "C PROPERTIES OF PROPOSED TRANSFORM",
            "text": "Having designed the Evolving Graph Fourier Transform, we now look at some of the properties of the transform. The below defines some properties of EFT before learning its representations and applying to downstream tasks (proofs are in appendix section B).\nProperty 1. (Equivalence in special case) Consider \u03a8T to be the time Fourier transform and \u03a8Gt to be the Graph Fourier transform at time t. Let \u03a8JD be the Graph fourier transform of JD. In the special case of Gti = Gtj\u2200i, j \u2208 {T} we have (\u03a8JD ) j i = (\u03a8D) j i = (\u03a8T \u2297 {\u03a8Gt}) j\u230a jN \u230b i . Property 2. EFT is an invertible transform and the inverse is given by EFT\u22121(X\u0302)ji =( \u03a8\u22121G X\u0302 )kk i ( \u03a8\u22a4 \u2217 T )j k in matrix form and EFT\u22121(x\u0302)j\u2217N+i = ( \u03a8\u2217T \u2297\u03a8 \u22121 G )k\u230a kN \u230b j\u2217N+i x\u0302k in vector form.\nProperty 3. EFT is a unitary transform if and only if GFT is unitary at all timesteps considered i.e. \u03a8D\u03a8\u2217D = INT iff \u03a8Gt\u03a8 \u2217 Gt\n= IN ,\u2200t. Property 4. EFT is invariant to the order of application of DFT or GFT on signal X.\nProperty 3 allows us to define the stability of the proposed transform. Consider the EFT matrix E and the signal vector x (normalized). The transform would be given by Ex. Now consider the perturbed matrix E + \u03f5, where \u03f5 is the (fixed) perturbation. The relative difference between the output would be \u2225(E + \u03f5)x\u2212 Ex\u2225/\u2225Ex\u2225 = \u2225\u03f5x\u2225/\u2225Ex\u2225. Since E is orthogonal, x is not in the null space of E and so the relative difference is bounded by \u03f5. So a small change in E should cause a small change in the output as desired.\nAs seen in property 1, EFT can be simulated by GFT in the special case that the graph structure does not change with time. The illustration between other transforms is in Figure 1. The figure shows transforms (GFT, JFT, DFT, EFT) in a circle, and arrows from one transform to the next indicate that the source transform can be obtained by the destination transform using the simulation annotated on the edges. Please note that the analysis has been performed for one-dimensional signals. However, the same holds true for higher dimensions as well by conducting the EFT dimension-wise. Here dimension-wise means the feature dimension of a node. Each node may have a multidimensional signal residing on it and the EFT can be independently applied to each channel or dimension of the node signals on the dynamic graph. Below subsection provides proofs for the above stated properties.\nC.1 PROOFS OF PROPERTIES\nIn this section we now prove the properties stated above. We repeat the statements for completeness. Though EFT and AD are not same in the general case they are equivalent when the graph structure does not change with time. Below result proves the result for the discrete case with graphs sampled at uniform timesteps\nProperty 5. (Special Equivalence between AD and EFT ) Consider \u03a8T to be the time fourier transform and \u03a8Gt to be the Graph fourier transform at time t. Let \u03a8JD be the Graph fourier transform of JD. In the special case of Gti = Gtj\u2200i, j \u2208 {T} we have (\u03a8JD ) j i = (\u03a8D) j i = (\u03a8T \u2297 {\u03a8Gt}) j\u230a jN \u230b i .\nProof. As before consider {LGt} \u2208 RN\u00d7N\u00d7T to be the Laplacian of the graphs at each timestep with eigenvalues \u03bbti where i \u2208 N, t \u2208 T . Let LT \u2208 RT\u00d7T be the Laplacian of the time adjacency matrix with eigenvalues \u00b5j where j \u2208 T . The Laplacian of the collective graph JD is expressed as\n(LJD ) j i = (LT \u2295 {LGt}) j\u230a jN \u230b i = LT \u2297 IN + (IT \u2297 {LGt}) j\u230a jN \u230b i\nConsider x1, x2, . . . xp to be the linearly independent right eigenvectors of LT and zt1, z t 2, . . . z t qt to be the linearly independent right eigenvectors of LGt . Consider the vector yj = (xk\u2297ztl ) \u230a jN \u230b j , y \u2208 RNT .\nThen we have\n(LJDy)i = (LT \u2297 IN ) j iyj + (IT \u2297 {LGt}) j\u230a jN \u230b i yj\n= (LT \u2297 {IN}) j\u230a jN \u230b i (xk \u2297 z t l ) \u230a jN \u230b j + (IT \u2297 {LGt}) j\u230a jN \u230b i (xk \u2297 z t l ) \u230a jN \u230b j\n= (LT \u2297 {IN}\u25a1xk \u2297 ztl ) \u230a iN \u230b i + (IT \u2297 {LGt\u25a1xk \u2297 z t l}) \u230a iN \u230b i = (LTxk \u2297 {IN}\u25a1ztl ) \u230a iN \u230b i + (ITxk \u2297 {LGt\u25a1z t l}) \u230a iN \u230b i = (\u00b5kxk \u2297 ztl ) \u230a iN \u230b i + (xk \u2297 {\u03bb t lz t l}) \u230a iN \u230b i = (\u00b5kxk \u2297 ztl + xk \u2297 {\u03bbtlztl}) \u230a iN \u230b i = (xk \u2297 ztldiag({\u00b5k}) + xk \u2297 {ztl}diag({\u03bbtl})) \u230a iN \u230b i = ((xk \u2297 ztl )diag({\u00b5k + \u03bbtl})) \u230a iN \u230b i\nwhere \u25a1 indicates timestep (column) wise product and diag(.) operator converts a vector to a diagonal matrix. In the special case where Gti = Gtj\u2200i, j \u2208 T we have \u03bbtil = \u03bb tj l . Thus we get\n(LJDy)i = ((xk \u2297 ztl )diag({\u00b5k + \u03bblIT })) \u230a iN \u230b i\n= (\u00b5k + \u03bbl(xk \u2297 ztl )diag({IT })) \u230a iN \u230b i = (\u00b5k + \u03bbl(xk \u2297 ztl )) \u230a iN \u230b i\n= (\u00b5k + \u03bbl)yi\nThus yj = (xk \u2297 ztl ) \u230a jN \u230b j is the eigenvector of LJD with eigenvalue \u00b5k + \u03bbl. But y is nothing but one of the columns of \u03a8\u2217D. By the rank nullity theorem, the row spaces of the transform matrices \u03a8D and GFT of JD share the same orthogonal basis. Thus the two transforms are equivalent in this case.\nNote the eigenvalues (\u039bT \u2295 \u039bG) obtained in the result above are exactly the ones used for plotting the frequency response of EFT as we compress the sequence of graphs into a single dynamic graph.\nNext we prove some properties of EFT as stated in the main paper Property 6. EFT is an invertible transform and the inverse is given by EFT\u22121(X\u0302)ji =( \u03a8\u22121G X\u0302 )kk i ( \u03a8\u22a4 \u2217 T )j k in matrix form and EFT\u22121(x\u0302)j\u2217N+i = ( \u03a8\u2217T \u2297\u03a8 \u22121 G )k\u230a kN \u230b j\u2217N+i x\u0302k in vector form.\nProof. We begin by noting the expression for EFT (\u03a8D)\n(\u03a8D) i j = (\u03a8T \u2297\u03a8Gt) j\u230a jN \u230b i\nwhere \u03a8Gt \u2208 RN\u00d7N is the graph fourier transform of the graph at time t, \u03a8T \u2208 RT\u00d7T is the time fourier transform. Let \u03a6Gt = \u03a8 \u22121 Gt\nbe the inverse graph fourier transform of the graph at timestep t and \u03a6T = \u03a8\u2217T be the inverse time fourier transform.\nWe can write \u03a8D as a block matrix in the following form \u03a8D = [ CB1, CB2, . . . CBT ] CBi = \u03a8iT \u2297\u03a8Gi\nwhere \u03a8iT is the i-th column of \u03a8T and CB i \u2208 RNT\u00d7N .\nConsider \u03a6D in a similar but row block format as follows\n\u03a6D =  RB1 RB2\n... RBT  (29) RBi = \u03a6Ti \u2297\u03a6Gi (30)\nwhere \u03a6Ti is the i-th row of \u03a6T and RBi \u2208 RN\u00d7NT . Now taking the matrix product of \u03a6D and \u03a8D we get\n\u03a6D\u03a8D =  RB1 RB2\n... RBT\n [CB1 CB2 . . . CBT ]\n=  RB1CB 1 RB1CB 2 . . . RB2CB 1 RB2CB\n2 . . . ...\nRBTCB 1 RBTCB 2 . . .  We can verify that RBiCBj evaluates to the following\nRBiCB j = (\u03a6Ti \u2297\u03a6Gi) ( \u03a8jT\u03a8Gj ) (31)\n= ( \u03a6Ti\u03a8 j T ) \u2297 ( \u03a6Gi\u03a8Gj ) (32)\n(33)\nNow the columns of \u03a6T form the eigenvectors of a circulant matrix (LT ). Also we know that if columns form basis of column space then rows form the basis of the row space. Thus we have\n\u03a6Ti\u03a8 j T = { 1, if i = j 0, otherwise\n(34)\n\u03a6Gi\u03a8Gi = IN (35)\n\u2234 RBiCB j = { IN , if i = j 0, otherwise\n(36)\n(37)\nThus we have shown that \u03a6D\u03a8D = INT . Thus \u03a6D is a left inverse of \u03a8D. We know that for a square matrix left inverse is also the right inverse and can be readily verified in a similar manner. Thus EFT is invertible and the inverse of the transformed signal in vector form is (\u03a6T \u2297 {\u03a6Gt}) j\u230a jN \u230b i x\u0302j =(\n\u03a8\u2217T \u2297 {\u03a8 \u22121 Gt } )j\u230a jN \u230b i x\u0302j . Similarly for the matrix form of the signal we have the inverse of the\ntransform given as ( {\u03a6Gt}X\u0302 )jj i ( \u03a6\u22a4T )k j = ( {\u03a8\u22121Gt }X\u0302 )jj i ( \u03a8\u22a4 \u2217 T )k j .\nProperty 7. EFT is a unitary transform if and only if GFT is unitary at all timesteps considered i.e. \u03a8D\u03a8 \u2217 D = INT iff \u03a8Gt\u03a8 \u2217 Gt = IN ,\u2200t\nProof. This property can be proved in a similar manner as in proof of property 6. The only difference here is we consider \u03a6D to be the transposed conjugate of \u03a8D rather than inverse i.e. \u03a6D = \u03a8\u2217D and\nalso \u03a6Gi = \u03a8 \u2217 Gi . Similar to the previous proof we have the following\n\u03a6D\u03a8D =  RB1 RB2\n... RBT\n [CB1 CB2 . . . CBT ]\n=  RB1CB 1 RB1CB 2 . . . RB2CB 1 RB2CB\n2 . . . ...\nRBTCB 1 RBTCB 2 . . .  RBiCB j = (\u03a6Ti \u2297\u03a6Gi) ( \u03a8jT\u03a8Gj\n) = ( \u03a6Ti\u03a8 j T ) \u2297 ( \u03a6Gi\u03a8Gj )\n\u2234 \u03a6D\u03a8D =  ( \u03a6T1\u03a8 1 T ) \u2297 (\u03a6G1\u03a8G1) ( \u03a6T1\u03a8 2 T ) \u2297 (\u03a6G1\u03a8G2) . . .( \u03a6T2\u03a8 1 T ) \u2297 (\u03a6G2\u03a8G1) ( \u03a6T2\u03a8 2 T ) \u2297 (\u03a6G2\u03a8G2) . . .\n...( \u03a6TT\u03a8 1 T ) \u2297 (\u03a6GT\u03a8G1) ( \u03a6TT\u03a8 2 T ) \u2297 (\u03a6GT\u03a8G2) . . .\n\n=  1\u2297 (\u03a6G1\u03a8G1) 0\u2297 (\u03a6G1\u03a8G2) . . . 0\u2297 (\u03a6G2\u03a8G1) 1\u2297 (\u03a6G2\u03a8G2) . . .\n... 0\u2297 (\u03a6GT\u03a8G1) 0\u2297 (\u03a6GT\u03a8G2) . . .\n\n=  ( \u03a8\u2217G1\u03a8G1 ) 0 . . . 0 ( \u03a8\u2217G2\u03a8G2 ) . . .\n... 0 0 . . .\n\nPart 1: If \u03a8G1 is unitary then \u03a8 \u2217 G1 = \u03a8\u22121G1 . Thus in this case \u03a6D\u03a8D = INT which implies \u03a6D = \u03a8 \u2217 D = \u03a8 \u22121 D implying \u03a8D is unitary.\nPart 2: Considering \u03a8D is unitary whic means \u03a6D = \u03a8\u2217D = \u03a8 \u22121 D . Thus \u03a6D\u03a8D = INT and so \u03a8\u2217Gi\u03a8Gi = IN \u2212\u2192 \u03a8 \u22121 Gi\n= \u03a8\u2217Gi . \u2234 \u03a8Gi is unitary proving the 2nd part and completing the proof.\nProperty 8. EFT is invariant to the order of application of DFT or GFT on signal X.\nThe above property can be observed from equation 6 using the fact that matrix multiplication is associative."
        },
        {
            "heading": "D DATASETS",
            "text": "Table 3: The statistics of the Large scale Dynamic graph datasets for link prediction.\nSR Datasets Beauty Games CDs # of Users 52,024 31,013 17,052 # of Items 57,289 23,715 35,118 # of Interactions 394,908 287,107 472,265 Average length 7.6 9.3 27.6\nDensity 0.01% 0.04% 0.08%\nContinuous Time Dynamic Graph link prediction dataset in sequential recommendation setting: For showing the efficacy of our method on large dynamic graphs, we perform experiments on three real-world e-commerce datasets (cf., Table 3) for sequential recommendation. Specifically, we pose the sequential recommendation as a link prediction problem on temporal graphs. The penultimate and last interactions are used for validation and testing, respectively. The graphs at each interaction timestamp is constructed as detailed in (Zhang et al., 2022) i.e., at time t, the subgraph (Gt) containing all interactions till t is consid-\nered. Then the m-hop neighborhood Gmt (u) around the user u is sampled from it. The next item to predict is the item (it+1) interacted with at time t + 1. Thus the training set would contain (Gm1 (u), i2), (G m 2 (u), i3) . . . (G m T\u22122(u), iT\u22121) and the test set would have (G m T\u22121(u), iT ). The graph construction is done in the preprocessing phase to speed-up training and testing.\n# Nodes # Edges # Time Steps Task (Train / Val / Test)\nSBM 1,000 4,870,863 35 / 5 / 10 LP UCI 1,899 59,835 62 / 9 / 17 LP AS 6,474 13,895 70 / 10 / 20 LP\nElliptic 203,769 234,355 31 / 5 / 13 NC Brain 5,000 1,955,488 10 / 1 / 1 NC\nspond to the pathways of monetary transfers. The Brain (Brn) dataset focuses on nodes representing minuscule cerebral regions or cubes, with the edges signifying their interconnections.\nSynthetic Dataset Consider the dynamic graph over T timesteps. Thus we have T graph snapshots. We compute the eigenvectors at each snapshot and place them over the graph\u2019s nodes. Moreover, for each node (spread over T timesteps) we compute a periodic signal that is added to the eigenvector component Evec(Gt). So the expression for the noise added to the signal would be: X(i, t) =\u2211\nk \u03b1kEvec(Gt)[i, k] + \u2211\nf \u03b2f \u2217 ei\u03c9tf [t]. In our experiments, we have used only one randomly chosen eigenvector. Also we consider only a single sinusoid frequency \u03c9. \u03b1k, \u03b2f are parameters and are set to 12 in our experiments. For noise, we add to X(i, t) a signal taken from a Gaussian distribution with 0 mean i.e. X(i, t) = X(i, t) +N (0, \u03b4), where \u03b4 is the standard deviation.\nD.1 EXPERIMENTAL SETUP\nWe implement our models using the DGL framework (Wang et al., 2019) in the pytorch library (Paszke et al., 2017). The hyperparameters are selected from the following search space: learning rate \u2208 [0.01, 0.0003], l2 regularization parameter \u03b1 \u2208 [0.01, 0.00001], embedding and hidden layer dimensions \u2208 {32, 64, 128}, filter order \u2208 {2, 4, 8, 16}, subgraph size \u2208 {1, 2, 3, 4}. The experiments are run on a single Tesla P100 GPU. We run our method for 5 runs per dataset and report the mean of the results. For the baselines we report the best results that have been reported unless mentioned otherwise. If results are not available we run baselines by using the implementation provided with default parameters and optimizing the hidden size (width) and layer number (depth) of the network. Regarding graph construction, for the Sequential Recommendation (SR) datasets we use similar to (Zhang et al., 2022). For the Session Based Recommendation (SBR) setting we use the transition graph of the items in the sequence as in (Wu et al., 2019). We also try with higher order graphs, albeit without any gains, as reported in (Wu et al., 2019). Moreover, since for SBR the last\nitem is of more significance to the prediction task and the datasets suffer from overfitting we modify the prediction layer accordingly and incorporate appropriate changes from baselines.\nD.2 IMPLEMENTATION\nWe intend to perform filtering in spectral space for dynamic graphs using EFT. Since our idea is to perform collective filtering along the vertex and temporal domain in EFT, we need two modules to compute \u03a8Gt (vertex aspect) and \u03a8T (temporal aspect), respectively, in equation 6 of EFT. We now explain these modules in detail.\nFiltering along the vertex domain: This module computes the convolution matrix \u03a8Gt in equation 6. Consider the filter response \u039b\u0302l which is a diagonal matrix with diagonal values representing the magnitude of the corresponding frequency(eigenvalue). In order to avoid the computational cost of the eigendecomposition, we choose to approximate the it using polynomials. In this work, we use the Chebyshev polynomials (Defferrard et al., 2016). Specifically, the frequency response of the desired filter is approximated as \u039b\u0302l = \u2211Of k=0 ckTk(\u039b\u0303), where Of is the polynomial/filter order, Tk is the Chebyshev polynomial basis, \u039b\u0303 = 2\u039b\u03bbmax \u2212 I , \u03bbmax is the maximum eigenvalue and ck is the corresponding filter coefficients. Thus, we can approximate the filtering operation as: X \u2217 \u039bl \u2248 U (\u2211Of k=0 ckTk(\u039b\u0303l) ) U\u2217X = \u2211Of k=0 ckTk(U \u039b\u0303lU \u2217)X = \u2211Of\nk=0 ckTk(L\u0303)X . Having the filter coefficients ck as learnable parameters enables learning of filter for the task. The convolution X \u2217 \u039bl gives the desired filtered response. Filtering along the temporal Domain: After performing filtering in the vertex domain, we aim to filter over the temporal signals using \u03a8T as in equation 6. To apply the \u03a8T (Fourier transform), we must first ensure that the signals in sequences are sampled at uniform intervals. In the continuous time setting, interactions between nodes could occur at anytime or the sampling could be nonuniform, Thus, we perform a mapping from RT\u00d7d \u2212\u2192 RT\u00d7d that aims to map the input space to a uniformly sampled space. For computational reasons, we select the current and next embeddings (with positional information) along with the timestamp information (Et(t) \u2208 Rd) for getting the mapped embedding akin to interpolation. Formally, let Xit \u2208 Rd be the embeddings of the node at time t. This is first mapped to the interpolated space using a universal approximator: Xt = W i2\u03c3 i(W i1[X i t ;X i t+1;Et(t)] + b i 1) + b i 2, where W i 1, W i 2, b i 1, b i 1 are learnable parameters and \u03c3\ni is a non-linearity. We call this module the time encoding layer , which is essential for applying Fourier transform along the temporal dimension. Let X = Xt \u2208 RT\u00d7d be the interpolated sequence of embeddings of the node. This is converted to the frequency domain (X\u0302 \u2208 RT\u00d7d) using the DFT matrix \u03a8T as X\u0302 = \u03a8TXt Then we multiply X\u0302 element-wise by a temporal filter FT \u2208 RT\u00d7d to obtain the filtered signal X\u0302f = FT \u2299 X\u0302 which is then converted back to the temporal domain by using the inverse transform \u03a8\u2217T to get Xf = \u03a8 \u2217 T X\u0302f . Xf is the equivalent of X\u0302G in equation 6 that is the output of EFT. In practice, the fast Fourier transform is used that can perform the computations in order O(T log(T )). Hence, overall time complexity of the architecture is O((N +E)T +NTlogT ). To map the output back to the original space from the interpolated space we would need further mapping layers. Similar to (Zhou et al., 2022), we use the standard layer normalization (LN) and feedforward (FFN) layers: XF = LN (LN (Xt + D(Xf )) + D (FFN (LN (Xt + D(Xf ))))), where W f2 ,W f 1 , b f 1 , b f 2 are learnable parameters and D(.) represents dropout. We could stack filter layers with the node embeddings obtained from previous layers as inputs. XF is the final filtered signal that is used in the downstream prediction. For the concerned node n we denote this as XnF .\nD.3 COMPUTATIONAL COMPLEXITY\nConsidering the spectral transform, the exact eigendecomposition of the joint laplacian would take order O((NT )3) whereas our method of EFT would take O(N3T +NT log(T )). Thus we reduce the complexity from a factor of T 3 to T log(T ). This would be beneficial in cases where there are many timesteps considered. For the model at the implementation level since we have made use of a function approximator that runs in time linear to the number of edges (\u03b5), the time complexity is O(\u03b5+NT log(T )). We have performed a wall clock run time analysis for the training of our method and the results in table 5 shows that it is comparable to a dynamic graph based baseline (that doesn\u2019t use any spectral transform):"
        },
        {
            "heading": "E ABLATION STUDY",
            "text": "Beauty Games R@10 NDCG@10 R@10 NDCG@10\nEFT 53.23 37.10 77.78 58.75 w/o Temporal filter 52.42 36.12 76.55 56.95 w/o Graph filter 38.27 24.39 58.36 40.06 +High Pass Filter 47.53 31.10 76.88 57.24 +Low Pass Filter 52.71 36.76 77.74 58.49 +Band Pass Filter 52.27 36.09 76.67 56.98 +Band Stop Filter 45.34 29.09 77.63 58.42\netc. Performance with static filters is less than that of dynamic filters, supporting our choice of having learnable filters in EFT .\nParameter Selection: In this experiment, we study the effect of filter and graph construction parameters that will help select optimal parameters for the model. Specifically, we run experiments for 1) the order of the graph filter and 2) subgraph size, which is the number of hops considered around the given user node for constructing the graph. The results are in Figure 7 with apt transform of the 2 metric scales for comprehension. For the filter order, we observe that for both datasets, there exists an optimal filter order at which the best performance is achieved. We observe that increasing filter order further causes overfitting on these datasets. For the subgraph size, we observe an increasing trend in the results, indicating that higher subgraph sizes (> 1) benefit the performance over a single hop (which is the sequence itself). This shows that modeling the SR as a graph learning problem is helpful over considering only the sequence. We conclude that beyond the subgraph size of two, the results saturate for these datasets."
        }
    ],
    "title": "BEYOND SPATIO-TEMPORAL REPRESENTATIONS: EVOLVING FOURIER TRANSFORM FOR TEMPORAL GRAPHS",
    "year": 2024
}