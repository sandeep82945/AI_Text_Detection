{
    "abstractText": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maximilian Seitzer"
        },
        {
            "affiliations": [],
            "name": "Sjoerd van Steenkiste"
        },
        {
            "affiliations": [],
            "name": "Thomas Kipf"
        },
        {
            "affiliations": [],
            "name": "Klaus Greff"
        },
        {
            "affiliations": [],
            "name": "Mehdi S. M. Sajjadi"
        }
    ],
    "id": "SP:2299b63aa0deaa73be864ef8ecbedd6b8deb6fe4",
    "references": [
        {
            "authors": [
                "Benjamin Attal",
                "Eliot Laidlaw",
                "Aaron Gokaslan",
                "Changil Kim",
                "Christian Richardt",
                "James Tompkin",
                "Matthew O\u2019Toole"
            ],
            "title": "T\u00f6rf: Time-of-flight radiance fields for dynamic scene view synthesis",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Angel X. Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "ShapeNet: An Information-Rich 3D Model Repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Danny Driess",
                "Zhiao Huang",
                "Yunzhu Li",
                "Russ Tedrake",
                "Marc Toussaint"
            ],
            "title": "Learning Multi-Object dynamics with compositional neural radiance fields",
            "venue": "CoRL,",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Du",
                "Yinan Zhang",
                "Hong-Xing Yu",
                "Joshua B. Tenenbaum",
                "Jiajun Wu"
            ],
            "title": "Neural radiance flow for 4d view synthesis and video processing",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "SM Ali Eslami",
                "Danilo Jimenez Rezende",
                "Frederic Besse",
                "Fabio Viola",
                "Ari S Morcos",
                "Marta Garnelo",
                "Avraham Ruderman",
                "Andrei A Rusu",
                "Ivo Danihelka",
                "Karol Gregor"
            ],
            "title": "Neural scene representation and rendering",
            "year": 2018
        },
        {
            "authors": [
                "Yang Fu",
                "Ishan Misra",
                "Xiaolong Wang"
            ],
            "title": "Mononerf: Learning generalizable nerfs from monocular videos without camera poses",
            "year": 2023
        },
        {
            "authors": [
                "Chen Gao",
                "Ayush Saraf",
                "Johannes Kopf",
                "Jia-Bin Huang"
            ],
            "title": "Dynamic view synthesis from dynamic monocular video",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Raghav Goyal",
                "Samira Ebrahimi Kahou",
                "Vincent Michalski",
                "Joanna Materzynska",
                "Susanne Westphal",
                "Heuna Kim",
                "Valentin Haenel",
                "Ingo Fr\u00fcnd",
                "Peter N. Yianilos",
                "Moritz Mueller-Freitag",
                "Florian Hoppe",
                "Christian Thurau",
                "Ingo Bax",
                "Roland Memisevic"
            ],
            "title": "The \u201csomething something\u201d video database for learning and evaluating visual common sense",
            "venue": "IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Klaus Greff",
                "Francois Belletti",
                "Lucas Beyer",
                "Carl Doersch",
                "Yilun Du",
                "Daniel Duckworth",
                "David J Fleet",
                "Dan Gnanapragasam",
                "Florian Golemo",
                "Charles Herrmann"
            ],
            "title": "Kubric: A Scalable Dataset Generator",
            "year": 2022
        },
        {
            "authors": [
                "Allan Jabri",
                "Sjoerd van Steenkiste",
                "Emiel Hoogeboom",
                "Mehdi SM Sajjadi",
                "Thomas Kipf"
            ],
            "title": "Dorsal: Diffusion for object-centric representations of scenes et",
            "venue": "al.. arXiv preprint arXiv:2306.08068,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Kosiorek",
                "Heiko Strathmann",
                "Daniel Zoran",
                "Pol Moreno",
                "Rosalia Schneider",
                "Sona Mokr\u00e1",
                "Danilo Rezende"
            ],
            "title": "NeRF-VAE: A Geometry Aware 3D Scene",
            "year": 2021
        },
        {
            "authors": [
                "Yunzhu Li",
                "Shuang Li",
                "Vincent Sitzmann",
                "Pulkit Agrawal",
                "Antonio Torralba"
            ],
            "title": "3d neural scene representations for visuomotor control",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengqi Li",
                "Simon Niklaus",
                "Noah Snavely",
                "Oliver Wang"
            ],
            "title": "Neural scene flow fields for space-time view synthesis of dynamic scenes",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Lun Liu",
                "Chen Gao",
                "Andreas Meuleman",
                "Hung-Yu Tseng",
                "Ayush Saraf",
                "Changil Kim",
                "Yung-Yu Chuang",
                "Johannes Kopf",
                "Jia-Bin Huang"
            ],
            "title": "Robust dynamic radiance fields",
            "year": 2023
        },
        {
            "authors": [
                "Ricardo Martin-Brualla",
                "Noha Radwan",
                "Mehdi S.M. Sajjadi",
                "Jonathan Barron",
                "Alexey Dosovitskiy",
                "Daniel Duckworth"
            ],
            "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
            "year": 2021
        },
        {
            "authors": [
                "Quan Meng"
            ],
            "title": "GNeRF: GAN-Based Neural Radiance Field Without Posed Camera",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul Srinivasan",
                "Matthew Tancik",
                "Jonathan Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Jonathan T Barron",
                "Ben Mildenhall",
                "Mehdi SM Sajjadi",
                "Andreas Geiger",
                "Noha Radwan"
            ],
            "title": "RegNeRF: Regularizing neural radiance fields for view synthesis from sparse inputs",
            "year": 2022
        },
        {
            "authors": [
                "Albert Pumarola",
                "Enric Corona",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Mehdi S.M. Sajjadi",
                "Daniel Duckworth",
                "Aravindh Mahendran",
                "Sjoerd van Steenkiste",
                "Filip Pavetic",
                "Mario Lucic",
                "Leonidas J. Guibas",
                "Klaus Greff",
                "Thomas Kipf"
            ],
            "title": "Object Scene Representation Transformer",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Mehdi S.M. Sajjadi",
                "Henning Meyer",
                "Etienne Pot",
                "Urs Bergmann",
                "Klaus Greff",
                "Noha Radwan",
                "Suhani Vora",
                "Mario Lucic",
                "Daniel Duckworth",
                "Alexey Dosovitskiy"
            ],
            "title": "Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Mehdi S.M. Sajjadi",
                "Aravindh Mahendran",
                "Thomas Kipf",
                "Etienne Pot",
                "Daniel Duckworth",
                "Mario Lu\u010di\u0107",
                "Klaus Greff"
            ],
            "title": "RUST: Latent Neural Scene Representations from Unposed Imagery",
            "year": 2023
        },
        {
            "authors": [
                "Johannes Lutz Sch\u00f6nberger",
                "Jan-Michael Frahm"
            ],
            "title": "Structure-from-Motion Revisited",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Semon Rezchikov",
                "William T Freeman",
                "Joshua B Tenenbaum",
                "Fredo Durand"
            ],
            "title": "Light field networks: Neural scene representations with single-evaluation rendering",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Liangchen Song",
                "Anpei Chen",
                "Zhong Li",
                "Zhang Chen",
                "Lele Chen",
                "Junsong Yuan",
                "Yi Xu",
                "Andreas Geiger"
            ],
            "title": "Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 2023
        },
        {
            "authors": [
                "Ayush Tewari",
                "Justus Thies",
                "Ben Mildenhall",
                "Pratul Srinivasan",
                "Edgar Tretschk",
                "Yifan Wang",
                "Christoph Lassner",
                "Vincent Sitzmann",
                "Ricardo Martin-Brualla",
                "Stephen Lombardi"
            ],
            "title": "Advances in neural rendering",
            "venue": "Computer Graphics Forum,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Trevithick",
                "Bo Yang"
            ],
            "title": "GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Wenqi Xian",
                "Jia-Bin Huang",
                "Johannes Kopf",
                "Changil Kim"
            ],
            "title": "Space-time neural irradiance fields for free-viewpoint video",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Xu",
                "Thiemo Alldieck",
                "Cristian Sminchisescu"
            ],
            "title": "H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans",
            "venue": "in motion. NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Yu",
                "Vickie Ye",
                "Matthew Tancik",
                "Angjoo Kanazawa"
            ],
            "title": "pixelNeRF: Neural Radiance Fields from One or Few Images",
            "venue": "In CVPR,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The majority of research in visual representation learning focuses on capturing the semantics and 2D-structure of individual images. In this work, we instead focus on simultaneously capturing the 3D structure as well as the dynamics of a scene, which is critical for planning, spatial and physical reasoning, and effective interactions with the real world. We draw upon the recent progress in generative modelling of 3D visual scenes, which has moved away from explicit representations such as voxel grids, point-clouds or textured meshes in favor of learning implicit representations by directly optimizing for novel view synthesis (NVS). For example, Neural Radiance Fields, though initially limited to single scenes with hundreds of input images with controlled lighting, precise camera pose and long processing times (Mildenhall et al., 2020), have since been extended to handle variations in lighting (Martin-Brualla et al., 2021), generalize across scenes (Trevithick & Yang, 2021), work with few images (Niemeyer et al., 2022), missing cameras (Meng, 2021), and even dynamic scenes (Pumarola et al., 2020).\nA related line of research focuses on learning global latent neural scene representations (Sitzmann et al., 2021; Sajjadi et al., 2022b; Kosiorek et al., 2021). These approaches offer several advantages, including the ability to generalize from few views, and improved scalability and efficiency due to amortized learning (Sajjadi et al., 2022b). Most importantly, their tangible latent representations can be readily used for downstream applications (Sajjadi et al., 2022a; Driess et al., 2022; Jabri et al., 2023). However, each of these models is limited to static scenes, which not only ignores the important aspect of scene dynamics, but also disqualifies the vast majority of potential real-world video datasets that contain dynamic scenes.\nIn this work, we present first steps towards learning latent dynamic neural scene representations from monocular real-world videos. Building up on recent work, our method learns a separation of the scene into global content and per-view camera pose & scene dynamics, thereby enabling independent control over these factors.\nOur core contributions are as follows:\n\u2022 We propose the Dynamic Scene Transformer (DyST), a model that learns latent neural scene representations from monocular video and provides controlled view generation. \u2217Work done during an internship at Google DeepMind. Project website: dyst-paper.github.io.\n\u2022 Through a unique training scheme, DyST learns a latent decomposition of the space into scene content as well as per-view scene dynamics and camera pose.\n\u2022 We present a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose.\n\u2022 Finally, we propose DySO, a novel synthetic dataset used for co-training DyST. We will publish DySO to the community for use in co-training and evaluation of future work on dynamic neural scene representations."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "The field of neural rendering for static 3D scenes has recently experienced a significant gain in popularity with the introduction of Neural Radiance Field (NeRF) (Mildenhall et al., 2020) which optimizes an MLP through the novel view synthesis (NVS) task on posed RGB imagery. NeRF and similar scene-specific methods typically require a very dense coverage of the scene along with accurate camera pose information. While there exists a great deal of works tackling these shortcomings (e.g. Yu et al., 2021; Meng, 2021; Niemeyer et al., 2022), this line of research largely focuses on view synthesis quality rather than learning latent scene representations. We refer the reader to Tewari et al. (2022) for a recent overview.\nMore closely related to our approach are methods that learn 3D-aware latent scene representations from the NVS task. Examples in this line of work include GQN (Eslami et al., 2018), NeRF-VAE (Kosiorek et al., 2021), LFN (Sitzmann et al., 2021), and SRT (Sajjadi et al., 2022b). These methods combine a visual encoder with a camera pose-conditioned decoder network for novel view synthesis. More recently, Sajjadi et al. (2023) proposed RUST, which uses a Pose Estimator to train the model without any camera pose information. Contrary to DyST, all of these methods are limited to static scenes, and, with the exception of RUST, require accurate camera poses for training.\nLearning scene representations from dynamic scenes comes with additional challenges: in the absence of calibrated multi-camera setups that record posed multi-view video, models have to learn to capture and disentangle scene dynamics from camera motion. In the easier case of posed multi-view video, high-fidelity solutions exist, such as D-NeRF (Pumarola et al., 2020), H-NeRF (Xu et al., 2021), NeRF-Dy (Li et al., 2022), and NeRFlow (Du et al., 2021). Acquiring the right data for these methods requires specialized recording setups and is not applicable to abundantly available monocular video. To overcome this limitation, several works including Neural Scene Flow Fields (Li et al., 2021), Spacetime Neural Irradiance Fields (Xian et al., 2021), T\u00f6RF (Attal et al., 2021), and Gao et al. (2021)\nmake use of additional priors obtained from optical flow or depth to support NVS for small camera displacements directly from monocular video. NerFPlayer (Song et al., 2023) and RoDynRF (Liu et al., 2023) are able to model dynamic scenes without depth or optical flow information by learning separate NeRFs for dynamic and static parts of the scene, though they require training a separate model for every video without generalizing between scenes. MonoNeRF (Fu et al., 2023) learns generalizable scene representations from pose-free monocular videos, but different from our method, it assumes a static underlying scene, i.e. it is not applicable to videos of dynamic scenes."
        },
        {
            "heading": "3 METHOD",
            "text": "Dynamic Scenes. A dynamic scene X consists of an arbitrary number of images or views x with associated camera pose ci and scene dynamics dj , i.e. X = {xc1d1 , x c2 d2 , . . .}. The cameras ci define the extrinsics and intrinsics of the camera that this particular image has been taken with, while the dynamics dj define the position and shape of entities in the scene, e.g. the position of a moving car.\nWe note two special cases thereof: in static scenes, there exist no scene dynamics, hence only the camera view varies across views: X = {xc1d , x c2 d , . . .}. This static-scene assumption significantly simplifies the setting by allowing the usage of basic photogrammetric constraints (Mildenhall et al., 2020), and it forms the basis for the majority of prior works on learned implicit representations (Sitzmann et al., 2021; Sajjadi et al., 2022b; Kosiorek et al., 2021).\nCommon real-life videos are monocular, i.e. both scene dynamics and camera vary together, but are strongly correlated over time, i.e. X = {xctdt , . . .} for time step t = 1, . . . , T , complicating the learning of disentangled representations from this source of data. Our goal is to learn latent neural scene representations on real-life video data while gaining independent control over both scene dynamics and the camera position.\nNeural Scene Representations. Given a dynamic scene X , we select a number of input views X = {x, . . .} \u2282 X that are encoded by the model into a set-based neural scene representation\nZ = {zk \u2208 Rd}k = Enc\u03b8(X). (1)\nWe note that this setting is in contrast to most common approaches based on Neural Radiance Fields (NeRF) (Mildenhall et al., 2020), which require training a model per scene and do not provide latent neural scene representations beyond the scene-specific MLP weights. Following Sajjadi et al. (2022b), the encoder Enc\u03b8 first applies a convolutional neural network to each input view x \u2208 X independently, then flattens the resulting feature maps into a set of tokens that is jointly processed with an encoder transformer (Vaswani et al., 2017).\nTo reproduce a novel target view ycd \u2208 X of the same scene, we use a transformer decoder Dec\u03b8 that repeatedly cross-attends into the scene representation Z to retrieve information about the scene relevant for this novel view. In order to do so, the decoder needs to be informed of the desired target view camera pose c and dynamics d. Most existing work only covers static scenes (Sitzmann et al., 2021; Kosiorek et al., 2021; Sajjadi et al., 2022b), and simply assumes a known ground-truth camera position c for querying novel views:\ny\u0302c = Dec\u03b8(c,Z). (2)\nInferred latent camera poses. Access to the ground-truth camera poses is a strong assumption that does regularly not extend to real-world settings. In practice, camera parameters are often estimated using external sensors such as LIDAR, or from the RGB views through Structure-from-Motion (SfM) methods such as COLMAP (Sch\u00f6nberger & Frahm, 2016). However, these methods are generally noisy and regularly fail altogether (Meng, 2021; Sajjadi et al., 2023), especially so in dynamic scenes such as natural videos.\nTo lift any requirements for explicit camera poses on static scenes, Sajjadi et al. (2023) introduce a Camera Estimator module CE\u03b8 that learns to extract a latent camera pose c\u0302 from the RGB data itself. More specifically, the camera estimator receives the target view y as input and queries parts of the scene representation Z \u2032 \u2282 Z to produce a low-dimensional camera control latent\nc\u0302 = CE\u03b8(y,Z \u2032) \u2208 RNc . (3)\nIn practice, Z \u2032 contains tokens belonging to the first input view, encouraging the learned camera control latent to be relative to that specific view. We note that c\u0302 lives in an abstract feature space, and it is not immediately compatible with explicit GT camera poses c, though such a mapping can be learned from data (Sajjadi et al., 2023). This estimated camera control latent is then used to condition the decoder when generating an image during training, i.e. y\u0302c = Dec\u03b8(c\u0302,Z), entirely removing the need for camera poses to train such a model. Sajjadi et al. (2023) show that the learned camera control latent distribution accurately follows the true poses, and that this model, on static scenes, achieves similar generation quality as baselines trained with ground truth pose information.\nThe network architecture of the camera estimator is similar to the encoder\u2019s: a CNN processes the target view, followed by a transformer that alternates between cross-attending into the scene representation, and self-attending on the flattened feature maps. The final camera control latent c\u0302 is then produced by a global average pooling of the output tokens and a linear projection to Nc dimensions."
        },
        {
            "heading": "3.1 STRUCTURING THE LATENT SPACE FOR DYNAMIC SCENE CONTROL",
            "text": "Up until this point, the setting in Eqs. (2) and (3) only admits static scenes. To apply our method on real-world videos, we need a way to allow variations in scene dynamics. We start with an approach similar to the learned camera control latents: a Dynamics Estimator DE sees the target view y and Z \u2032, and learns to extract a dynamics control latent d\u0302 from the RGB views:\nd\u0302 = DE\u03b8(y,Z \u2032) \u2208 RNd . (4) The dynamics control latent d\u0302 is used as an additional query for the decoder to reconstruct the target view: y\u0302cd = Dec\u03b8(c\u0302, d\u0302,Z).\nSeparating dynamics from camera pose. While this split between camera pose and scene dynamics is convenient in theory, we note that thus far, there exist no structural differences between the ways in which c\u0302 and d\u0302 are inferred or used. Hence, in practice, they likely will both learn to capture the camera pose and scene dynamics in a unified way, making it hard to control these aspects independently (as we confirm in practice in Sec. 4.4).\nWe propose to enforce this separation through a novel latent control swap training scheme that is outlined in Fig. 2. For training, we assume a scene X where views are available for all combinations of camera poses {ci}i and scene dynamics {di}i. As target frames, we choose the four views corresponding to all combinations of the camera poses c1, c2 and dynamics d1, d2, i.e. our set of target views is {yc1d1 , y c2 d1 , yc1d2 , y c2 d2 }. For all remaining camera poses and scene dynamics, we randomly sample a subset of views as input views.\nIn the following, we describe the decoding mechanism for the target view yc1d1 ; the scheme for the remaining target views follows analogously. To generate this particular target view, the decoder needs access to (a latent estimate for) the camera pose c1 and the scene dynamics d1. Our\nkey insight is that we do not necessarily need to estimate c\u03021 and d\u03021 from the target frame yc1d1 itself. Instead, we can estimate the camera pose and scene dynamics from their respective counterparts:\nc\u03021 = CE\u03b8(y c1 d2 ,Z \u2032), d\u03021 = DE\u03b8(yc2d1 ,Z \u2032). (5)\nCrucially, we note that the most salient information yc1d2 contains to render the target view y c1 d1 is the camera pose and it is hence natural for the camera estimator CE to learn to extract camera pose information from its inputs. Similarly, yc2d1 shares its scene dynamics d1 with the target view to be generated, encouraging the dynamics estimator DE to extract scene dynamics. From these estimates, we render the target view:\ny\u0302c1d1 = Dec\u03b8 ( CE\u03b8(y c1 d2 ,Z \u2032),DE\u03b8(yc2d1 ,Z \u2032),Z ) . (6)\nAs a consequence, the model is required to route all information regarding the camera pose through c\u03021, and all information regarding the scene dynamics through d\u03021, yielding the desired separation."
        },
        {
            "heading": "3.2 SIM-TO-REAL TRANSFER",
            "text": "The approach outlined requires the availability of a special kind of multi-view, multi-dynamics dataset. In the real world, multi-view video would satisfy the requirements: the scene dynamics are synced through time, while each separate video instantiates a different view onto the same scene. However, acquiring such a dataset for training would narrow the applicability of the method to a limited set of domains. Instead, we suggest a sim-to-real setup: we generate a synthetic dataset that contains the desired properties for inducing camera-dynamics separation in the learned latent representations, while co-training on natural, monocular videos.\nSynthetic dataset. We follow prior work using Kubric (Greff et al., 2022) and ShapeNet objects (Chang et al., 2015). Our dataset uses components from the MultiShapeNet (MSN) dataset (Sajjadi et al., 2022b) that has been introduced as a challenging test bed for 3D representation learning from static scenes. MSN uses photo-realistic ray tracing, high resolution images as backgrounds, and has a diverse set of realistic 3D objects.\nCompared to MSN, our dataset has some distinct modifications. We place one object into each scene, as our primary goal is the separation of scene dynamics, in this case the object position & orientation, from the camera. The object is initialized with a random position and pose on the floor. To integrate dynamics, for each time step, we randomly jitter the object\u2019s position and further apply a random rotation to the object. MSN samples cameras on a half-sphere around the scene. Since real videos rarely feature 360 degree views, we use a set of more realistic camera motions: horizontal shifts, panning, and zooming, as well as sampling random camera points nearby a fixed point.\nWe find this simple set of camera and object motions to be sufficient for successful sim-to-real transfer, and did not find the need to simulate more physically realistic object motions. Our dataset consists of 1M dynamic scenes, each generated with all 25 combinations of 5 distinct scene dynamics and 5 camera views. We call this new dataset Dynamic Shapenet Objects (DySO) and will make it publicly available, as we expect it to be a useful tool for the community for training and evaluating dynamic neural scene representations in the future.\nCo-training. To transfer the separation of dynamics and camera control latents from the synthetic scenes to natural monocular videos, we co-train on both types of data at the same time. More specifically, we take alternating optimization steps on batches sampled from each of the two datasets. On the synthetic batches, we train the model according to Eq. (6), i.e. the camera pose and scene dynamics are estimated from views yc1d2 and y c2 d1\n, different from the actual target view yc1d1 to be rendered. On the batches with monocular videos, those additional views are not available, so we simply use the target view itself for dynamics and camera pose estimation:\ny\u0302cd = Dec\u03b8 (CE\u03b8(y c d,Z \u2032),DE\u03b8(ycd,Z \u2032),Z) . (7)\nThe latent control swap outlined in Sec. 3.1 is implemented efficiently by estimating the cameras and dynamics for all four (randomly sampled) target views in parallel before swapping the estimated c\u0302i and d\u0302j control latents accordingly and decoding all four target views jointly.\nModel architecture. Our network architecture largely follows Sajjadi et al. (2023). For efficiency, we implement the Camera and Dynamics Estimators CE & DE as a single transformer that produces c\u0302 and d\u0302 simultaneously for a given target view (see also Fig. 8). The two control latents are produced differently: for the camera pose, we apply global average pooling to the transformer\u2019s output, and linearly project the result to produce c\u0302. For the scene dynamics, we add a learned token to the transformer\u2019s set of inputs, and apply a separate linear projection to d\u0302 to only the output for that token. This follows the intuition that the camera pose is a global property affecting all tokens (hence global pooling), whereas estimating the dynamics is a more localized task that only affects object tokens (hence the additional token that can learn to attend to the respective subset of tokens). For the decoder Dec, we simply concatenate the two control latents to form a single query [c\u0302, d\u0302] which is used to cross-attend into the SLSR Z to generate the target view. All model weights \u03b8 are optimized end-to-end using the novel view synthesis (NVS) objective. Given a dataset D = {Xi}i of training scenes, for an arbitrary ground truth target view ycd \u2208 Xi, the model is trained to minimize the L2 loss:\nLNVS(\u03b8) = EX\u223cD, (X,ycd)\u223cX [ \u2016Dec\u03b8(CE\u03b8(ycd\u2032 ,Z \u2032),DE\u03b8(yc \u2032 d ,Z \u2032),Enc\u03b8(X))\u2212 ycd\u201622 ] , (8)\nwhere c\u2032 6=c, d\u2032 6=d if X is synthetic, and c\u2032=c, d\u2032=d otherwise, i.e. on real-world videos."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Unless specified otherwise, DyST is always co-trained on our synthetic DySO dataset and on real world videos as described in Secs. 3.1 and 3.2. As a source of a real world videos, we use the Something-Something v2 dataset (SSv2) (Goyal et al., 2017), which consists of roughly 170K training videos of humans performing basic actions with everyday objects. The videos contain both nontrivial camera motion and dynamic scene manipulation. From each video, we sub-sample a clip of 64 frames. The model is trained using 3 randomly sampled input frames X to compute the scene representation Z = Enc(X) and 4 randomly sampled target views for reconstruction. Following Sajjadi et al. (2022b), we train using a batch size of 256 for 4M steps. For both control latents, we use a dimensionality of Nc = Nd = 8. We refer to Appendix A.1 for further implementation details."
        },
        {
            "heading": "4.1 NOVEL VIEW SYNTHESIS",
            "text": "We begin our investigations by testing the novel view synthesis (NVS) capabilities of the model on the test splits of both the synthetic DySO and the real SSv2 datasets. We show several examples in Fig. 3 (left). On DySO, the background is particularly well-modelled, while the object is often less sharp. We attribute this to the difficulty of capturing 3D object appearance from only 3 provided input views. For SSv2, we use the first, middle, and last frame of the videos as input frames and generate intermediate frames using control latents estimated from the intermediates. Qualitative results are shown in Fig. 4. We find that our model accurately captures changes in both camera viewpoint and object dynamics.\nQuantitatively, we evaluate the model on DySO by computing the PSNR against the ground truth target view ycd while varying the control latents, see Fig. 3 (right). We observe that there is only a minor difference in PSNR between using the target ycd itself to estimate the control latents (y c d/y c d), versus estimating the latent camera from a view with different scene dynamics (yc d\u2032\n) and vice versa (yc \u2032\nd ). This further confirms that DyST can successfully estimate and re-combine camera pose and scene dynamics to synthesize the desired novel views."
        },
        {
            "heading": "4.2 LEARNED CAMERA AND SCENE DYNAMICS CONTROL LATENTS",
            "text": "In this section, our experiments aim to answer three questions. First, does our training with latent control swapping induce a meaningful separation of camera pose and scene dynamics on synthetic data? Secondly, does the latent space separation transfer to real videos? And finally, what is the structure of the learned latent spaces?\nSeparation of Camera and Dynamics. To analyze the separation into camera and scene dynamics quantitatively, we develop a metric which we call contrastiveness. Let Dcam(y, y\u2032,Z \u2032) = \u2016CE(y,Z \u2032)\u2212 CE(y\u2032,Z \u2032)\u2016 denote the L2 distance in camera latent space between views y and y\u2032. Ddyn is defined accordingly for the dynamics latent space. Then, we compute the average ratio Rcam between the distance in camera latent space of a reference view ycd to a matching view y c d\u2032 , and to a non-matching view yc \u2032\nd :\nRcam = EX\u223cD, (X,ycd,ycd\u2032 ,yc \u2032 d )\u223cX\n[ Dcam(ycd, y c d\u2032 ,Z \u2032)\nDcam(ycd, y c\u2032 d ,Z \u2032)\n] , with c 6= c\u2032and d 6= d\u2032. (9)\nAnalogously, we define Rdyn using yc \u2032\nd as matching and y c d\u2032 as non-matching view. If the contrastive-\nness metric Rcam is \u22480, this indicates that views with the same camera are mapped onto the same point in camera latent space regardless of their dynamics; if Rcam is \u22481, views with non-matching camera can not be distinguished based on latent distance on average. If Rcam is >1, information from the dynamics latent space is \u201cleaking\u201d into the camera latent space, because views with the same dynamics d but different camera c\u2032 are on average closer to ycd than views with different dynamics d\n\u2032, but the same camera c.\nWe find that the learned latent spaces are well separated on average, with little influence of one control latent on the other. In particular, our model achieves a camera contrastiveness of Rcam=0.06 (corr. to 16.7\u00d7 less distance of matching to non-matching views), and a dynamics contrastiveness of Rdyn=0.42 (corr. to 2.4\u00d7 less distance). The fact that Rcam is notably smaller than Rdyn indicates that it is easier for the model to estimate the camera pose compared to the object pose. This might be intuitive, as the camera pose can be estimated directly from all background pixels (unaffected by the scene dynamics), while the appearance of scene dynamics in 2D images is the result of a combination of the actual scene dynamics and the camera pose. In contrast, when training without latent control swapping, the model only achieves Rcam=0.72 and Rdyn=1.26, indicating that the separation is drastically worse there (see also Tab. 1). This is further evidence that our latent control swap training scheme induces the desired disentanglement of camera pose and scene dynamics. We also visualize the average latent distances as a heatmap in Fig. 5, and find that matching (blue squares) and non-matching views (red squares) are easily distinguishable.\nTransfer to Real Video. To see whether the camera-dynamics separation transfers to real videos, we measure the frame-to-frame latent distances on an example video of one of the authors grasping,\nlifting and turning a coffee cup, before placing it back in its original position. The resulting distance matrix is visualized in Fig. 5. We find both the slow panning motion of the camera and the distinct events in the video are clearly visible in the distance maps. Moreover, we point out that similar object poses lead to similar dynamics latents despite the vastly different camera positions. We conclude that the model is able to successfully disentangle camera pose from scene dynamics in real-world videos.\nLearned Structure. To investigate the structure of the two learned control latent spaces, we apply principal component analysis (PCA) to a set of latent cameras c\u0302 and scene dynamics d\u0302 collected from 300 test scenes of the DySO dataset. Starting from the extracted latents c\u0302 & d\u0302 of some view, we then linearly interpolate along the principal components of each space and show the decoded images in Fig. 6. We find that the learned spaces capture meaningful movements: the first three components for the camera pose capture horizontal and vertical movements and planar rotation, mirroring similar observations by Sajjadi et al. (2023). For the scene dynamics, the first component captures object rotation, the second represents horizontal and the third vertical movements. Furthermore, we observe additional evidence that camera and dynamics are well separated: changing c\u0302 does not modify the object\u2019s pose; similarly, changing d\u0302 does not modify the camera. Lastly, we also estimate the principal components on the SSv2 dataset, and find that they capture similar effects for the camera; for the dynamics, movements are more constrained to plausible dynamics fitting to the input frames.\nWe conclude that our model is able to disentangle camera and scene dynamics, and that it learns a meaningful structure for both. We study how these capabilities can be used for test-time video control in the next section."
        },
        {
            "heading": "4.3 CONTROLLABLE VIDEO MANIPULATION",
            "text": "After establishing that DyST learns a disentangled representation of camera and scene dynamics, we now show how this capability can be used to manipulate real-world video post-hoc. After encoding a target video into the scene representation Z using the the first, middle and last frame, we synthesize counterfactual variations for this video by manipulating the control latents c\u0302 & d\u0302. We study two variants: motion freezing and video-to-video motion transfer. For the former, we compute c\u0302 or d\u0302 from a frame of the target video, and keep it fixed throughout the video. This freezes the corresponding camera or object motion from that frame throughout the video, creating a \u201cbullet time\u201d effect. For the latter, we replace c\u0302, d\u0302, or both, with the latents computed from a different video, effectively transferring the motion in that video to the target frame or video.\nFigure 7 shows examples of such manipulations. First, we find that inserting the latent camera c\u0302 or scene dynamics d\u0302 of a source frame correctly keeps the camera / object stable throughout the video, but with the original object / camera trajectory. Second, we find that transferring the camera from a video with a camera zoom or horizontal pan copies the corresponding motion onto a target frame. Note how on the shift motion, the left edge of the frame becomes blurred \u2014 the respective areas are not visible in the target video, and thus DyST fills in an average guess due to the L2 loss. It is notable that both types of manipulations work in this fashion, as the manipulated control latents are fully out-of-distribution for the decoder given the scene representation."
        },
        {
            "heading": "4.4 ABLATION STUDY",
            "text": "We investigate the importance of the latent control swap (Eq. (6)). We measure PSNR under latent control swapping as described in Sec. 4.1, and latent space contrastiveness as described in Sec. 4.2 on the DySO dataset. The results are listed in Tab. 1. We compare several variations: applying no swap at all (i.e., decoding the target yc1d1 using control latents of y c1 d1\nitself), swapping randomly with a 50% probability, or averaging the control latents of the target view and the swapped view together (e.g., averaging camera latents from yc1d1 and y c1 d2\nto decode yc1d1 ). The latter is motivated by the idea that the latents of matching views should eventually converge to the same point. We find that all variations perform significantly worse than the latent control swap: without swapping, latent space separation does not occur as expected because there is no structural incentive that enforces it \u2014 consequently, the recombination PSNR is low. When swapping only 50% of the time, some separation emerges, but the model still has some chance to mix information from the target view into both control latents. For averaging the latents, we also find substantial entanglement of camera and dynamics. Thus, our latent control swap is the most suitable training scheme for consistent separation of camera and dynamics in the latent space."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we propose DyST, a novel approach to generative modeling of dynamic 3D visual scenes that admits separate control over the camera and the content of the scene. We demonstrate how DyST can be applied to real-world videos of dynamic scenes despite not having access to ground-truth camera poses via sim-to-real transfer. DyST shows promising view synthesis and scene control capabilities on real-world videos of dynamic scenes.\nAs a next step, we would like to apply our method to more complex types of videos, for example with several independent moving objects, longer camera trajectories, or changing lighting conditions \u2014 challenges which exceed the scope of this work. Indeed, we expect that further model innovations are needed to tackle them. Another open problem concerns the novel view synthesis aspect of our model; there is still room for improvement in view generation quality, especially for dynamic objects, which is currently limited due to the L2 loss. Future work could improve the model\u2019s generative capabilities: using diffusion or GAN-like approaches should lead to more plausibly imputed missing information.\nWe believe that DyST contributes a significant step towards learning neural scene representations from real-world scenes encountered in the wild. This opens the door for exciting down-stream applications, especially when combined with the potential of training on large-scale video collections.\nAuthor contributions & Acknowledgements. Maximilian Seitzer: conception, implementation, datasets, experiments, evaluation, writing. Sjoerd van Steenkiste: writing, advising. Thomas Kipf: writing, advising. Klaus Greff: project co-lead, conception, evaluation, writing. Mehdi S. M. Sajjadi: project lead, conception, datasets, early experiments, evaluation, writing.\nWe thank Aravindh Mahendran for detailed feedback on the manuscript and Alexey Dosovitskiy for fruitful discussions on the project. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Maximilian Seitzer."
        }
    ],
    "title": "DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS",
    "year": 2024
}