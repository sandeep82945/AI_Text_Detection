{
    "abstractText": "Real-world deployment of machine learning models is challenging because data evolves over time. While no model can work when data evolves in an arbitrary fashion, if there is some pattern to these changes, we might be able to design methods to address it. This paper addresses situations when data evolves gradually. We introduce a time-varying propensity score that can detect gradual shifts in the distribution of data which allows us to selectively sample past data to update the model\u2014not just similar data from the past like that of a standard propensity score but also data that evolved in a similar fashion in the past. The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes.",
    "authors": [],
    "id": "SP:30d3c209f4c017ec933c51090fd27c55c63ee84a",
    "references": [
        {
            "authors": [
                "Alessandro Achille",
                "Michael Lam",
                "Rahul Tewari",
                "Avinash Ravichandran",
                "Subhransu Maji",
                "Charless C Fowlkes",
                "Stefano Soatto",
                "Pietro Perona"
            ],
            "title": "Task2vec: Task embedding for meta-learning",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Deepak Agarwal",
                "Lihong Li",
                "Alexander Smola"
            ],
            "title": "Linear-time estimators for propensity scores",
            "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Michael Ahn",
                "Henry Zhu",
                "Kristian Hartikainen",
                "Hugo Ponte",
                "Abhishek Gupta",
                "Sergey Levine",
                "Vikash Kumar"
            ],
            "title": "Robel: Robotics benchmarks for learning with low-cost robots",
            "venue": "In Conference on Robot Learning (CoRL),",
            "year": 2019
        },
        {
            "authors": [
                "Amr Alexandari",
                "Anshul Kundaje",
                "Avanti Shrikumar"
            ],
            "title": "Maximum likelihood with bias-corrected calibration is hard-to-beat at label shift adaptation",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Rahaf Aljundi",
                "Klaas Kelchtermans",
                "Tinne Tuytelaars"
            ],
            "title": "Task-free continual learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Baxter"
            ],
            "title": "A Model of Inductive Bias Learning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2000
        },
        {
            "authors": [
                "Shai Ben-David",
                "Reba Schuller"
            ],
            "title": "Exploiting task relatedness for learning multiple tasks",
            "venue": "In Proceedings of the 16th Annual Conference on Learning Theory,",
            "year": 2003
        },
        {
            "authors": [
                "Steffen Bickel",
                "Michael Br\u00fcckner",
                "Tobias Scheffer"
            ],
            "title": "Discriminative learning for differing training and test distributions",
            "venue": "In Proceedings of the 24th International Conference on Machine Learning,",
            "year": 2007
        },
        {
            "authors": [
                "Massimo Caccia",
                "Pau Rodriguez",
                "Oleksiy Ostapenko",
                "Fabrice Normandin",
                "Min Lin",
                "Lucas PageCaccia",
                "Issam Hadj Laradji",
                "Irina Rish",
                "Alexandre Lacoste",
                "David V\u00e1zquez",
                "Laurent Charlin"
            ],
            "title": "Online fast adaptation and knowledge accumulation (osaka): a new approach to continual learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Massimo Caccia",
                "Jonas Mueller",
                "Taesup Kim",
                "Laurent Charlin",
                "Rasool Fakoor"
            ],
            "title": "Task-agnostic continual reinforcement learning: In praise of a simple baseline",
            "venue": "arXiv preprint arXiv:2205.14495,",
            "year": 2022
        },
        {
            "authors": [
                "Ewen Callaway"
            ],
            "title": "The coronavirus is mutating \u2014 does it matter?, September 2020",
            "venue": "URL https: //www.nature.com/articles/d41586-020-02544-6",
            "year": 2020
        },
        {
            "authors": [
                "Xiangli Chen",
                "Mathew Monfort",
                "Anqi Liu",
                "Brian D. Ziebart"
            ],
            "title": "Robust covariate shift regression",
            "venue": "In AISTATS,",
            "year": 2016
        },
        {
            "authors": [
                "D.R. Cox"
            ],
            "title": "Regression models and life-tables",
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
            "year": 1972
        },
        {
            "authors": [
                "Rasool Fakoor",
                "Pratik Chaudhari",
                "Alexander J. Smola"
            ],
            "title": "P3o: Policy-on policy-off policy optimization",
            "venue": "In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Rasool Fakoor",
                "Pratik Chaudhari",
                "Alexander J Smola"
            ],
            "title": "Ddpg++: Striving for simplicity in continuous-control off-policy reinforcement learning",
            "year": 2020
        },
        {
            "authors": [
                "Rasool Fakoor",
                "Jonas Mueller",
                "Kavosh Asadi",
                "Pratik Chaudhari",
                "Alexander J. Smola"
            ],
            "title": "Continuous doubly constrained batch reinforcement learning",
            "venue": "In Thirty-Fifth Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Willliam Feller"
            ],
            "title": "An introduction to probability theory and its applications, vol 2",
            "year": 2008
        },
        {
            "authors": [
                "Chelsea Finn",
                "Aravind Rajeswaran",
                "Sham Kakade",
                "Sergey Levine"
            ],
            "title": "Online meta-learning",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yansong Gao",
                "Pratik Chaudhari"
            ],
            "title": "An information-geometric distance on the space of tasks",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Saurabh Garg",
                "Yifan Wu",
                "Sivaraman Balakrishnan",
                "Zachary C. Lipton"
            ],
            "title": "A unified view of label shift estimation",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Heitor Murilo Gomes",
                "Jesse Read",
                "Albert Bifet",
                "Jean Paul Barddal",
                "Jo\u00e3o Gama"
            ],
            "title": "Machine learning for streaming data: state of the art, challenges, and opportunities",
            "venue": "ACM SIGKDD Explorations Newsletter,",
            "year": 2019
        },
        {
            "authors": [
                "Arthur Gretton",
                "Alex Smola",
                "Jiayuan Huang",
                "Marcel Schmittfull",
                "Karsten Borgwardt",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Covariate Shift by Kernel Mean Matching",
            "year": 2008
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M Borgwardt",
                "Malte J Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Raia Hadsell",
                "Dushyant Rao",
                "Andrei Rusu",
                "Razvan Pascanu"
            ],
            "title": "Embracing change: Continual learning in deep neural networks",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "James Harrison",
                "Apoorva Sharma",
                "Chelsea Finn",
                "Marco Pavone"
            ],
            "title": "Continuous meta-learning without tasks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Xu He",
                "Jakub Sygnowski",
                "Alexandre Galashov",
                "Andrei A Rusu",
                "Yee Whye Teh",
                "Razvan Pascanu"
            ],
            "title": "Task agnostic continual learning via meta learning",
            "year": 1906
        },
        {
            "authors": [
                "James J. Heckman"
            ],
            "title": "Sample selection bias as a specification",
            "venue": "error. Econometrica,",
            "year": 1979
        },
        {
            "authors": [
                "Jiayuan Huang",
                "Arthur Gretton",
                "Karsten Borgwardt",
                "Bernhard Sch\u00f6lkopf",
                "Alex Smola"
            ],
            "title": "Correcting sample selection bias by unlabeled data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2006
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Zhiqiu Lin",
                "Jia Shi",
                "Deepak Pathak",
                "Deva Ramanan"
            ],
            "title": "The clear benchmark: Continual learning on real-world imagery",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2021
        },
        {
            "authors": [
                "Zachary C. Lipton",
                "Yu-Xiang Wang",
                "Alex Smola"
            ],
            "title": "Detecting and correcting for label shift with black box predictors, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Bo Lu"
            ],
            "title": "Propensity score matching with time-dependent",
            "venue": "covariates. Biometrics,",
            "year": 2005
        },
        {
            "authors": [
                "Jie Lu",
                "Anjin Liu",
                "Fan Dong",
                "Feng Gu",
                "Joao Gama",
                "Guangquan Zhang"
            ],
            "title": "Learning under concept drift: A review",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Jie Lu",
                "Anjin Liu",
                "Fan Dong",
                "Feng Gu",
                "Jo\u00e3o Gama",
                "Guangquan Zhang"
            ],
            "title": "Learning under concept drift: A review",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "Nan Lu",
                "Tianyi Zhang",
                "Tongtong Fang",
                "Takeshi Teshima",
                "Masashi Sugiyama"
            ],
            "title": "Rethinking importance weighting for transfer learning",
            "venue": "arXiv preprint arXiv:2112.10157,",
            "year": 2021
        },
        {
            "authors": [
                "Tianwei Ni",
                "Benjamin Eysenbach",
                "Ruslan Salakhutdinov"
            ],
            "title": "Recurrent model-free RL can be a strong baseline for many POMDPs",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "E.J.G. Pitman"
            ],
            "title": "Sufficient statistics and intrinsic accuracy",
            "venue": "Mathematical Proceedings of the Cambridge Philosophical Society,",
            "year": 1936
        },
        {
            "authors": [
                "Sashank Reddi",
                "Barnabas Poczos",
                "Alex Smola"
            ],
            "title": "Doubly robust covariate shift correction",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Sashank J. Reddi",
                "Barnab\u00e1s P\u00f3czos",
                "Alexander J. Smola"
            ],
            "title": "Doubly robust covariate shift correction",
            "venue": "In AAAI,",
            "year": 2015
        },
        {
            "authors": [
                "Sidney I Resnick"
            ],
            "title": "A probability path",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "C.J. Russell"
            ],
            "title": "Orthomyxoviruses: Structure of antigens",
            "venue": "In Reference Module in Biomedical Sciences. Elsevier,",
            "year": 2016
        },
        {
            "authors": [
                "Yuya Saito",
                "Koji Kamagata",
                "Peter A. Wijeratne",
                "Christina Andica",
                "Wataru Uchida",
                "Kaito Takabayashi",
                "Shohei Fujita",
                "Toshiaki Akashi",
                "Akihiko Wada",
                "Keigo Shimoji",
                "Masaaki Hori",
                "Yoshitaka Masutani",
                "Daniel C. Alexander",
                "Shigeki Aoki"
            ],
            "title": "Temporal progression patterns of brain atrophy in corticobasal syndrome and progressive supranuclear palsy revealed by subtype and stage inference (sustain)",
            "venue": "Frontiers in Neurology,",
            "year": 2022
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael I Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Shai Shalev-Shwartz"
            ],
            "title": "Online Learning and Online Convex Optimization",
            "year": 2012
        },
        {
            "authors": [
                "Hidetoshi Shimodaira"
            ],
            "title": "Improving predictive inference under covariate shift by weighting the loglikelihood function",
            "venue": "Journal of Statistical Planning and Inference,",
            "year": 2000
        },
        {
            "authors": [
                "Jun Shu",
                "Qi Xie",
                "Lixuan Yi",
                "Qian Zhao",
                "Sanping Zhou",
                "Zongben Xu",
                "Deyu Meng"
            ],
            "title": "Meta-weightnet: Learning an explicit mapping for sample weighting",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Vinicius Souza",
                "Denis M dos Reis",
                "Andre G Maletzke",
                "Gustavo EAPA Batista"
            ],
            "title": "Challenges in benchmarking stream learning algorithms with real-world data",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2020
        },
        {
            "authors": [
                "Masashi Sugiyama",
                "Matthias Krauledat",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Covariate shift adaptation by importance weighted cross validation",
            "venue": "Journal of Machine Learning Research,",
            "year": 2007
        },
        {
            "authors": [
                "Masashi Sugiyama",
                "Shinichi Nakajima",
                "Hisashi Kashima",
                "Paul Buenau",
                "Motoaki Kawanabe"
            ],
            "title": "Direct importance estimation with model selection and its application to covariate shift adaptation",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2007
        },
        {
            "authors": [
                "Masashi Sugiyama",
                "Shinichi Nakajima",
                "Hisashi Kashima",
                "Paul Von Bunau",
                "Motoaki Kawanabe"
            ],
            "title": "Direct importance estimation for covariate shift adaptation",
            "year": 2008
        },
        {
            "authors": [
                "Ryan J Tibshirani",
                "Rina Foygel Barber",
                "Emmanuel Candes",
                "Aaditya Ramdas"
            ],
            "title": "Conformal prediction under covariate shift",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Martin J. Wainwright",
                "Michael I. Jordan"
            ],
            "title": "Graphical models, exponential families, and variational inference",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Mei Wang",
                "Weihong Deng"
            ],
            "title": "Deep visual domain adaptation: A survey",
            "year": 2018
        },
        {
            "authors": [
                "Rongguang Wang",
                "Guray Erus",
                "Pratik Chaudhari",
                "Christos Davatzikos"
            ],
            "title": "Adapting machine learning diagnostic models to new populations using a small amount of data: Results from clinical neuroscience, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ziyu Wang",
                "Victor Bapst",
                "Nicolas Heess",
                "Volodymyr Mnih",
                "Remi Munos",
                "Koray Kavukcuoglu",
                "Nando de Freitas"
            ],
            "title": "Sample efficient actor-critic with experience replay",
            "year": 2016
        },
        {
            "authors": [
                "Junfeng Wen",
                "Chun-Nam Yu",
                "Russell Greiner"
            ],
            "title": "Robust learning under uncertain test distributions: Relating covariate shift to model misspecification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Ruihan Wu",
                "Chuan Guo",
                "Yi Su",
                "Kilian Q Weinberger"
            ],
            "title": "Online adaptation to label distribution shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Annie Xie",
                "James Harrison",
                "Chelsea Finn"
            ],
            "title": "Deep reinforcement learning amidst lifelong nonstationarity",
            "year": 2020
        },
        {
            "authors": [
                "Fan Yang",
                "Chao Yang",
                "Huaping Liu",
                "Fuchun Sun"
            ],
            "title": "Evaluations of the gap between supervised and reinforcement lifelong learning on robotic manipulation tasks",
            "venue": "In 5th Annual Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kun Zhang",
                "Bernhard Sch\u00f6lkopf",
                "Krikamol Muandet",
                "Zhikun Wang"
            ],
            "title": "Domain adaptation under target and conditional shift",
            "venue": "In International conference on machine learning,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Machine learning models are not expected to perform well when the test data is from a different distribution than the training data. There are many techniques to mitigate the consequent deterioration in performance (Heckman, 1979; Shimodaira, 2000; Huang et al., 2006; Bickel et al., 2007; Sugiyama et al., 2007b; 2008; Gretton et al., 2008). These techniques use the propensity score between the train and test data distributions to reweigh the data\u2014and they work well (Agarwal et al., 2011; Wen et al., 2014; Reddi et al., 2015b; Fakoor et al., 2020c;a; Tibshirani et al., 2019) when dealing with a single training and test dataset. But when machine learning models are deployed for real-world problems, they do not just undergo one distribution shift1 (from train to test), but instead suffer many successive distribution shifts (Lu et al., 2019) e.g., search queries to an online retailer or a movie recommendation service evolve as fashion and tastes of the population evolve, etc. Even for problems in healthcare, there are many situations where data drifts gradually, e.g., the different parts of the brain atrophy slowly as the brain ages for both healthy subjects and those with dementia; tracking these changes and distinguishing between them is very useful for staging the disease and deciding treatments (Saito et al., 2022; Wang et al., 2023). Viruses can mutate and drift over time, which can make them resistant to existing treatments (Russell, 2016; Callaway, 2020; Harvey et al., 2021).\nTo build a method that can account for changes in data, it is important to have some understanding of how data evolves. If data evolves in an arbitrary fashion, there is little that we can do to ensure that the model can predict accurately at the next time instant in general. Settings in which the data distribution drifts gradually over time (rather than experiencing abrupt changes) are particularly ubiquitous (Shalev-Shwartz, 2012; Lu et al., 2019). Assuming observations are not statistically dependent over time (as in time-series), how to best train models in such streaming/online learning settings remains a key question (Lu et al., 2019). Some basic options include: fitting/updating the model to only the recent data (which is statistically suboptimal if the distribution has not significantly shifted) or fitting the model to all previous observations (which leads to biased estimates if shift has occurred). Here we consider a less crude approach in which past data are weighted during training with continuous-valued weights that vary over time. Our proposed estimator of these weights generalizes standard two-sample\n1In this paper, we will not distinguish between covariate, label, and concept shifts; they will all be referred to as distribution shifts and the difference will be clear from context.\npropensity scores, allowing the training process to selectively emphasize past data collected at time t based on the distributional similarity between the present and time t.\nThis paper studies problems where data is continuously collected from a constantly evolving distribution such that the single train/test paradigm no longer applies and the model learns and makes predictions based on already seen data to predict accurately on test data from future where the learner does not have access to any training data from future. Our main contribution is to introduce a simple yet effective time-varying propensity score estimator to account for gradual shifts in data distributions. One important property of our method is that it can be utilized in many different kinds of settings\u2014from supervised learning to reinforcement learning\u2014and can be used in tandem with many existing approaches for these problems. Another key property of our method is that it neither has any assumption on data nor requires data to be generated from a specific distribution in order to be effective. We evaluate our proposed method in various settings involving a sequence of gradually changing tasks with slow repeating patterns. In such settings, the model not only must continuously adapt to changes in the environment/task but also learn how to select past data which may have become more relevant for the current task. Extensive experiments show our method efficiently detects data shifts and statistically account for such changes during learning and inference time."
        },
        {
            "heading": "2 APPROACH",
            "text": "For two probability densities p(x) and q(x) that are absolutely continuous with respect to the Lebesgue measure on Rd, observe that\nE x\u223cq [\u2113(x)] =\n\u222b dp(x)\ndp(x) dq (x)\u2113(x) = E x\u223cp [dq(x) dp(x) \u2113(x) ] = E x\u223cp [ \u03b2(x) \u2113(x) ] (1)\nwhere \u2113(x) is any function. The propensity score \u03b2(x) = dq(x)dp(x) is the Radon-Nikodym derivative between the two densities (Resnick, 2013). It measures the relative likelihood of a datum x coming from p versus q; in the sequel we will call \u03b2(x) the \u201cstandard propensity score\u201d. We may think of q(x) as the probability density corresponding to the test distribution and p(x) as that of the training distribution. If \u2113(x) is the loss of a datum, the identity here suggests that we can optimize the test loss, i.e., the left-hand-side, using a \u03b2-weighted version of the training loss, i.e., the right-hand-side. In practice, can estimate \u03b2(x) via samples from p and q (Agarwal et al., 2011) as follows. We can create a data set D = {(xi, zi)}Ni=1 with zi = 1 if xi \u223c p and zi = \u22121 if xi \u223c q. We fit a model g\u03b8, which can be linear or non-linear, parameterized by \u03b8 to classify these samples and obtain\n\u03b8\u0302 = \u2212min \u03b8\n1\nN N\u2211 i=1 log (1 + exp(\u2212zig\u03b8(xi))) . (2)"
        },
        {
            "heading": "2.1 MODELING AND ADDRESSING DRIFT IN THE DATA",
            "text": "Let us now consider the situation when the probability density evolves over time: (pt(x))t\u2264T where t \u2208 R. Assume that we have a dataset D = {(xi, ti)}Ni=1 where each datum xi was received at time ti and therefore xi \u223c pti(x). The dataset can contain multiple samples from any of the pt for t \u2264 T , and there may even be time instants for which there are no samples. Our theoretical development will be able to address such situations. Let p(t) = N\u22121 \u2211N i=1 \u03b4ti(t) be the empirical distribution of the time instants at which we received samples; here \u03b4 denotes the Dirac-delta distribution. We can now define the marginal on the data as\np(x) = \u222b T 0 dt p(t) pt(x).\nInspired by this expression, we will liken pt(x) \u2261 p(x | t). Assumption 1 (Data evolves gradually). The probability density of data at a later time\npt(x); t > T\ncan be arbitrarily different from the past data and in the absence of prior knowledge of how the distribution evolves, we cannot predict accurately far into the future. Suppose our goal is to build a\nmodel that can predict well on data from pT+dt(x), i.e., a small time-step into the future using the dataset D. For the loss \u2113(x), this amounts to minimizing\nEx\u223cpT+dt [\u2113(x)] . (3) This problem is challenging because we do not have any data from pT+dt, at training time or for adaptation. But observe Ex\u223cpT+dt(x) [\u2113(x)] equals to\nE x\u223cpT (x) [\u2113(x)] +\n\u222b dx (pT+dt(x)\u2212 pT (x)) \u2113(x) \u2264 E\nx\u223cpT (x) [\u2113(x)] + 2TV(pT+dt(x), pT (x)), (4) where TV(pT+dt, pT ) = 12 \u222b dx |pT+dt(x)\u2212 pT (x)| is the total variational divergence between the probability densities at time T and T +dt, and the integrand \u2113(x) is upper bounded by 1 in magnitude (without loss of generality, say after normalizing it). We therefore assume that the changes in the probability density pT are upper bounded by a constant (uniformly over time t); this can allow us to build a model for pT+dt using data from (pt)t\u2264T .\nValidating and formalizing Assumption 1 This assumption is a natural one in many settings and we can check it in practice using a two-sample test to estimate the total variation TV(pT+dt, pT ) (Gretton et al., 2012). We do so for some of our tasks in Sec. 3 (see Fig. 9). We can also formalize this assumption mathematically as follows. Consider a stochastic process (Xt)t\u2208R with a transition function Pt. By definition of the transition function, we have that\nE [\u03c6(Xt)] = \u222b \u03c6(y)Pt(Xs,dy) \u2200s \u2264 t,\nfor any measurable function \u03c6. The distribution of the random variable Xt corresponding to this Markov process is exactly the distribution of data pt at time instant t in our formulation. We can now define the semi-group of this transition function as the conditional expectation Kt\u03c6 =\u222b dy \u03c6(y)Pt(x, dy); this holds for any function \u03c6. Such a semi-group satisfies properties that are familiar from Markov chains in finite state-spaces, e.g., Kt+s = KtKs. The infinitesimal generator of the semi-group Kt is defined as A\u03c6 = limt\u21920(\u03c6\u2212Kt\u03c6)/t. This generator completely defines the local evolution of a Markov process (Feller, 2008)\npt = e \u2212tA p0 (5)\nwhere etA is the matrix exponential of the generator A. If all eigenvalues of A are positive, then the Markov process has a steady state distribution, i.e., the distribution of our data pt stops evolving (this could happen at very large times and the time-scale depends upon the smallest non-zero eigenvalue of A). On the other hand, if the magnitude of the eigenvalues of A is upper bounded by a small constant, then the stochastic process evolves gradually.\nThe above discussion suggests that our development in this paper, building upon Assumption 1, is meaningful, so long as the data does not change abruptly. Remark 2 (Evaluating the model learned from data from (pt)t\u2264T on test from pT+dt). We are interested in making predictions on future data, i.e., data from pT+dt. For all our experiments, we will therefore evaluate on test data from \u201cone time-step in the future\u201d. The learner does not have access to any training data from pT+dt in our experiments. This is reasonable if the data evolves gradually. Our setting is therefore different from typical implementations of online meta-learning (Finn et al., 2019; Harrison et al., 2020) and continual learning (Hadsell et al., 2020; Caccia et al., 2020).\nIn the sequel, motivated by Assumption 1, we will build a time-dependent propensity score for pT instead of pT+dt. Using a similar calculation as that of Eq. (1) (see Appendix A for a derivation), we can write our objective as\nEx\u223cpT (x) [\u2113(x)] = Et\u223cp(t)Ex\u223cpt(x) [\u03c9(x, T, t) \u2113(x)] , (6) where\n\u03c9(x, T, t) = dpT dpt (x) (7)\nis the time-varying propensity score. Remark 3 (Is the time-varying propensity score equivalent to standard propensity score on (x, t)?). We can define a new random variable z \u2261 (x, t) with a corresponding distribution p(z) \u2261 pt(x)p(t). However, doing so is not useful for estimating Ex\u223cpT (x) [\u2113(x)] because our objective Eq. (6) involves conditioning on a particular time T (see also Sec. 3.3 for numerical comparison)."
        },
        {
            "heading": "2.2 TIME-VARYING PROPENSITY SCORE",
            "text": "Let us first discuss a simple situation where we model the time-varying propensity score using an exponential family (Pitman, 1936; Wainwright & Jordan, 2008)\npt(x) = p0(x) exp (g\u03b8(x, t)) (8)\nwhere g\u03b8(x, t) is a continuous function parameterized by weights \u03b8. This gives\n\u03c9(x, T, t) = exp (g\u03b8(x, T )\u2212 g\u03b8(x, t)) (9)\nThe left-hand-side can be calculated using Eq. (2) to estimate g\u03b8(x, t) as follows. We create a modified dataset whose inputs are triplets (xi, t\u2032, t) with a label 1 if t\u2032 = ti, i.e., the time at which the corresponding xi was recorded, and a label -1 if t = ti.\nRemark 4 (Modeling the propensity score using an exponential family does not mean that the data is from an exponential family). Observe that the exponential family in Eq. (8) is used to model the propensity pt(x)/p0(x), this is different from pt(x) \u221d exp(g\u03b8(x, t)) which would amount to modeling the data distribution as belong to an exponential family.\nObserve that in the above method p0(x), which is the distribution of data at the initial time, is essentially unconstrained. We can exploit this \u201cgauge\u201d to choose p0(x) differently and build a more refined estimator for the time-varying propensity score. We can model pt(x) as deviations from the marginal on x:\npt(x) = p(x) exp (g\u03b8(x, t)) . (10)\nThis gives \u03c9(x, T, t) = exp (g\u03b8(x, T )\u2212 g\u03b8(x, t)) .\nIn this case, we create a modified dataset where for each of N samples, there is an input tuple (xi, t) with label 1 if t = ti, i.e., the correct time at which xi was recorded, and with label -1 if t \u0338= ti. While creating this modified dataset, we choose all the unique time instances at which some sample was recorded, i.e., t \u2208 {ti : (xi, ti) \u2208 D}. The logistic regression in Eq. (2) is fitted to obtain g\u03b8(x, t) using this modified dataset. Algorithm 2 gives more details.\nIt is important to note that the distinction between the two approaches is subtle. The Eq. (8) models the deviations of the probability density pt(x) from its value at the first time instant p0(x) while Eq. (10) models its deviations from the marginal p(x) = \u222b T 0 dt p(t)pt(x). The nuances between these approaches are subtle when deals with data with varying gradual shift, leading to comparable performance of both methods, as illustrated in Fig. 8."
        },
        {
            "heading": "2.3 A LEARNING-THEORETIC PERSPECTIVE",
            "text": "We next discuss a learning theoretic perspective on how we should learn models when the underlying data evolves with time. For the purposes of this section, suppose we have m input-output pairs Dt = {(xi, yi)}mi=1 where xi are the inputs and yi \u2208 {0, 1} are the binary outputs, that are independent and identically distributed from pt(x, y) for t = 1, . . . , T . Note that data from each time instant are IID, there can be correlations across time. Unlike the preceding sections when time was real-valued, we will only consider integer-valued time.\nConsider the setting where we seek to learn hypotheses h = (ht)t=1,...,T \u2208 HT that can generalize on new samples from distributions (pt)t=1,...,T respectively. A uniform convergence bound on the population risk of one ht suggests that if\nm = O ( \u03f5\u22122(VCH \u2212 log \u03b4) ) ,\nthen et(ht) \u2264 e\u0302t(ht) + \u03f5 with probability at least 1 \u2212 \u03b4; here et(ht) the population risk of ht on pt, e\u0302t(ht) is its average empirical risk on the m samples from Dt (Vapnik, 1998) and VC is the VC-dimension of H \u220b ht. If we want hypotheses that have a small average population risk et(h) = T\u22121 \u2211T t=1 et(ht). across all time instants, we may minimize the empirical risk e\u0302 t(h) =\nT\u22121 \u2211T t=1 e\u0302t(ht). As Baxter (2000) shows, if\nm = O ( \u03f5\u22122 ( VCH(T )\u2212 T\u22121 log \u03b4 )) (11)\nthen we have et(h) \u2264 e\u0302t(h) + \u03f5 for any h. The quantity VCH(T ) is a generalized VC-dimension that characterizes the number of distinct predictions made by the T different hypotheses; it depends upon the stochastic process underlying our individual distributions pt. Larger the time T , smaller the VCH(T ). Whether we should use the m samples from pT to train one hypothesis, or use all the mT samples to achieve a small average population risk across all the tasks, depends upon how related the tasks are. If our goal is only the latter, then we can just train on data from all the tasks because\nVCH(T ) \u2264 VCH , for all T \u2265 1.\nIn the literature on multi-task or meta-learning, this result is often the motivation for learning on samples from all the tasks. We will use this as a baseline in our experiments; we call this Everything in Sec. 3.1.1. In theory, this baseline is effective if tasks are related to each other (Baxter, 2000; Gao & Chaudhari, 2021; Achille et al., 2019) but it will not be able to exploit the fact that the data distribution evolves over time. A related baseline that we can glean from this argument that the one that only builds the hypothesis using data from pT ; we call this Recent) in Sec. 3.1.1.\nOur goal is however not to obtain a small population risk on all tasks, it is instead to obtain a small risk on pT+dt\u2014our proxy for it being the latest task pT . Even if the average risk on all tasks of the hypotheses trained above is small, the risk on a particular task pT can be large. (Ben-David & Schuller, 2003) studies this phenomena. They show that if we can find a hypothesis h \u25e6 ft such that h \u25e6 ft can predict accurately on pt for all t, then this effectively reduces the size of the hypothesis space h \u2208 HT and thereby entails a smaller sample complexity in Eq. (11) at the added cost of searching for the best hypothesis ft \u2208 F for each task pt (which requires additional samples that scale linearly with VCF ). The sample complexity of such a two-stage search can be better than training on all tasks, or training on pT in isolation, under certain cases, e.g., if VCH \u2265 log |F | for a finite hypothesis space F (Ben-David & Schuller, 2003).\nWhile the procedure that combines such hypotheses to get h \u25e6 ft in the above theory is difficult to implement in practice, this argument motivates our third baseline (Finetune) which first fits a hypothesis on all past tasks (pt)t\u2264T and then adapts it further using data from pT ."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "We provide a broad empirical comparison of our proposed method in both continuous supervised and reinforcement learning. We first evaluate our method on synthetic data sets with time-varying shifts. Next, we create continuous supervised image classification tasks utilizing CIFAR-10 (Krizhevsky & Hinton, 2009) and CLEAR (Lin et al., 2021) datasets. Finally, we evaluate our method on ROBEL D\u2019Claw (Ahn et al., 2019; Yang et al., 2021) simulated robotic environments and MuJoCo (Todorov et al., 2012). See also Appendices C and D for a description of our settings and more results."
        },
        {
            "heading": "3.1 CONTINUOUS SUPERVISED LEARNING",
            "text": "In these experiments, the goal is to build a model that can accurately predict future data (under continuous label shift) at t+ 1 using only historical data from {p0, . . . , pt} as described in Remark 2. It is important to note that all models, including baselines, are trained entirely from scratch at each time step. Also, we do not update the model continually, as our main focus is on understanding the the performance of the time-varying propensity score. The following sections will explain the baseline models, the data sets and simulated shift setups, and the results of the experiments."
        },
        {
            "heading": "3.1.1 BASELINE METHODS",
            "text": "1. Everything: To obtain model used at time t, we train on pooled data from all past times {ps(x) : s \u2264 t}, including the most recent data from pt(x). More precisely, this involves minimizing the objective\n\u2113t(\u03d5) = 1\nN t\u2211 s=1 \u2211 {xi:ti=s} \u2113(\u03d5;xi). (12)\nwhere the loss of the predictions of a model parameterized by weights \u03d5 is \u2113(\u03d5;xi) and we have defined \u2113t(\u03d5) as the cumulative loss of all tasks up to time t.\n2. Recent trains only on the most recent data, i.e., data from pt(x). This involves optimizing\n\u2113t(\u03d5) = 1\nN \u2032 \u2211 {xi:ti=t} \u2113(\u03d5;xi), where N \u2032 = \u2211 {xi:ti=t} 1. (13)\n3. Fine-tune: first trains on all data from the past using the objective\n\u03d5t\u2212 = argmin \u03d5\n1\nN t\u2211 s=1 \u2211 {xi:ti=s,s<t} \u2113(\u03d5;xi)\nand then finetunes this model using data from pt. We can write this problem as\n\u2113t+(\u03d5) = 1\nN \u2032 \u2211 {xi:ti=t} \u2113(\u03d5;xi) + \u2126(\u03d5\u2212 \u03d5t\u2212), (14)\nwhere \u2126(\u03d5\u2212\u03d5t\u2212) is a penalty that keeps \u03d5 close to \u03d5t\u2212 , e.g., \u2126(\u03d5\u2212\u03d5t\u2212) = \u2225\u03d5\u2212 \u03d5t\u2212\u222522. Sometimes this approach is called \u201cstandard online learning\u201d (Finn et al., 2019) and it resembles (but is not the same as) Follow-The-Leader (FTL) (Shalev-Shwartz, 2012). We call it \u201cFinetune\u201d to explicitly state the objective and avoid any confusion due to its subtle differences with FTL. In practice, we implement finetuning without the penalty \u2126 by initializing the weights of the model to \u03d5t\u2212 in Eq. (14). Remark 5 (Properties of the different baselines). Recall that our goal is to build a model that predicts on data from pt+1(x). It is important to observe that Everything minimizes the objective over all the past data; for data that evolves over time, doing so may be detrimental to performance at time t. If we think using a bias-variance tradeoff, the variance of the predictions of Everything is expected to be smaller than the variance of predictions made by Recent, although the former is likely to have a larger bias. For example, if the data shifts rapidly, then we should expect Everything to have a large bias and potentially a worse accuracy than that of Recent. But if the amplitude of the variations in the data is small (and drift patterns repeat) then Everything is likely to benefit from the reduced variance and perform better than Recent. Fine-tune strikes a balance between the two methods and although the finetuning procedure is not always easy to implement optimally, e.g., the finetuning hyper-parameters can be different for different times. As our results illustrate neither of these approaches show consistent trend in the experiments and highly depend on a given scenario which can be problematic in practice, e.g. sometimes Everything performs better than both but other times Fine-tune is better (compare their performance in Fig. 1). Remark 6 (How does the objective change when we have supervised learning or reinforcement learning?). We have written Eqs. (12) to (14) using the loss \u2113(\u03d5;xi) only for the sake of clarity. In general, the loss depends upon both the inputs xi and their corresponding ground-truth labels yi for classification problems. For problems in reinforcement learning, we can think of our data xi as entire trajectories from the system and thereby the loss \u2113(\u03d5;xi) as the standard 1-step temporal difference (TD) error which depends on the weights that parameterize the value function.\nWe incorporate our time-varying propensity score into Eq. (12) as follows:\n\u2113t\u03c9(\u03d5) = 1\nN t\u2211 s=1 \u2211 {xi:ti=s} \u03c9(xi, t, ti) \u2113(\u03d5;xi). (15)\nNote that the only difference between our method and Everything is introduction of \u03c9 into Eq. (12) and all other details (e.g. network architecture, etc.) are exactly the same. \u03c9 can be trained based on our algorithm explained in Sec. 2.1 and Algorithm 2. Important to note that, our method automatically detects shifts in the data without knowing them as a priori. This means that our method is able to detect these changes without being given any explicit information about when or where they occur."
        },
        {
            "heading": "3.1.2 DRIFTING GAUSSIAN DATA",
            "text": "Here, we design continuous classification tasks using a periodic shift where samples are generated from a Gaussian distribution with time-varying means (\u00b5t) and standard deviation of 1. In particular, we create a data set,Dt = {(xti, yti)}Ni=1, at each time step twhere xti \u223c N (\u00b5t, 1), \u00b5t = \u00b5t\u22121+d/10,\n\u00b50 = 0.5, and label 1 (yti = 1) is assigned if x t i > \u00b5t, otherwise y t i = 0. We change direction of shift every 50 time steps (i.e., set d \u2190 \u2212d) and use N = 2000 in our experiments. We run these experiments for 160 time steps. Fig. 1a displays that our method performs much better than Everything and similarly to others. We note that both Recent and Fine-tune are like oracle for this experiment as test time data from pt+1(x) are more similar to the recent data than other historical data. This experiment clearly illustrates how our method enables model training to selectively emphasize data similar to the current time and, importantly, data that evolved in a similar fashion in the past."
        },
        {
            "heading": "3.1.3 IMAGE CLASSIFICATION WITH EVOLVING LABELS",
            "text": "We adopt the experimental setting of (Wu et al., 2021) for classification under continuous label shift with images from CIFAR-10 (Krizhevsky & Hinton, 2009) and CLEAR (Lin et al., 2021).\nCIFAR-10. We split the original CIFAR-10 training data into 90% (train) and 10% (validation), and use original test split as is. Following (Wu et al., 2021), we pre-train Resent-18 (He et al., 2016) using training data for 100 epochs. In subsequent continuous label shift experiments, we utilize the pre-trained model and only fine-tune its final convolution block and classification (output) layer. We use two different shifts, called CIFAR-10a and CIFAR-10b (see Appendix D.1 for details) and 3 random data splits here.\nCLEAR. The data set contains 10 classes with 3000 samples per class. CLEAR is curated from a large public data set spanning 2004 to 2014. We utilize unsupervised pre-trained features provided by (Lin et al., 2021) and only use a linear layer classifier during continuous label shift experiments. As they stated, the linear model using pre-trained features is far more effective than training from scratch for this data set (for more, see (Lin et al., 2021)). We average all results over 3 random train-validation-test splits, of relative size 60% (train), 10% (validation), and 30% (test).\nContinuous label shifts. We create a sequence of classification tasks wherein the label distributions varies at each time step, according to a predefined pattern (refer to Appendix D.1 for details). The pattern repeats slowly, with each new task having a slightly different label distribution than previous task. Here, we train a model from scratch at each time step t using all training data encountered thus\nfar and evaluate the model on test data drawn from label distribution of time t+ 1. This resulted in a significant number of experiments, typically ranging from 100 to 500 different models per benchmark illustrated in Fig. 1. This setup presents a significant challenging scenario due to the occurrence of label shift at each time step. Consequently, it becomes crucial for a method to learn how to selectively utilize only data which are relevant for the current time step and task. It is important to note that we solely employ the test data during the evaluation phase and never use it to train a model. Results. Fig. 1 shows the results of these large experiments, where we compare our method against the baselines across different data sets and shifts. To give further insight into our method, we show in Fig. 9 that our method produces high quality estimates under shifted distribution. Also, in Fig. 4, we present a comparison between our method and Meta-Weight-Net (Shu et al., 2019). All these comprehensive results illustrate that not only does our method outperform others by achieving higher classification accuracy, but it also consistently performs well across various shifts and benchmarks, unlike other methods. For instance, Everything fares poorly in Gaussian experiment than others but it works better than Fine-tune and Recent on continuous CIFAR-10 benchmark. We also show the comparable performance of our method and Everything in the absence of data shifts, highlighting their consistent applicability and effectiveness (see Appendix C for more)."
        },
        {
            "heading": "3.2 REINFORCEMENT LEARNING WITH CHANGING TASKS",
            "text": "In what follows, we discuss how we adopt our method into existing RL methods. We use two challenging environments with various settings for experiments in this section. For all experiments, we report the undiscounted return averaged across 10 different random seeds. ROBEL D\u2019Claw. This is a robotic simulated manipulation environment containing 10 different valve rotation tasks and the goal is to turn the valve below 180 degrees in each task. All tasks have the same state and action spaces, and same episode length but each rotates a valve with a different shape. We build a random sequence of these tasks switching from one to another (after an episode finishes) in repeating patterns. We utilize SAC (Haarnoja et al., 2018) as our baseline. In order to incorporate our approach into SAC, we only change Q-value update with \u03c9 as follows:\n1 |B| \u2211\n(s,a,t,s\u2032)\n[ \u03c9(s, a, T, t) ( Q\u03c8(s, a)\u2212 r \u2212 \u03b3Q\u03c8\u0302(s \u2032, a\u2032) + \u03b1 log \u03c0\u03d5(a \u2032|s\u2032) )2] , a\u2032 \u223c \u03c0\u03d5(\u00b7|s\u2032) (16)\nwhere \u03b1 is entropy coefficient, T is the current episode, and (s, a) denote state and action, respectively.\nHalf-Cheetah WindVel. We closely follow the setup of (Xie et al., 2020) to create Half-Cheetah WindVel environment where direction and magnitude of wind forces on an agent keep changing but slowly and gradually. This is a difficult problem as an agent needs to learn a non-stationary sequence of tasks (i.e. whenever direction and magnitude of wind changes, it constitutes as a new task). Since task information is not directly available to the agent, we utilize the recent method of Caccia et al. (2022) and Fakoor et al. (2020c) to learn and infer task related information using context variable (called SAC + Context here). Similar to Eq. (16), we only change Q-update to add our method. We emphasize that SAC + Context is a strong method for non-stationary settings and leads to state-of-art performance in those scenarios (Fakoor et al., 2020c; Ni et al., 2022). Additionally, we use SAC in this experiment as well to underscore the challenging nature of the task.\nResults. We can see in Fig. 2 that our method offers consistent improvements over the baseline methods. This is particularly noteworthy that although the replay buffer contains data from various behavior policies, our method can effectively model how data evolve over time with respect to the current policy without requiring to have access to the behavior policies that were used to collect the data. It is particularly useful in situations where the data are often collected from unknown policies. Note that the only difference between our method and baseline methods is the introduction of \u03c9 term in the Q-update and all other details are exactly the same. See Fig. 10 and 11 for more results."
        },
        {
            "heading": "3.3 COMPARING STANDARD PROPENSITY WITH OUR METHOD",
            "text": "Now we compare our method with standard propensity scoring discussed in Sec. 2. Fig. 3 shows results of this experiment on CIFAR-10b benchmark. For a comparison with the Gaussian benchmark, please refer to Fig. 5. These result provide an empirical verification that our method works better than standard propensity method and gives us some insights why it performs better. Particularly, we can see in Fig. 3 and 5 that our method fairly detects shifts across different time steps, whereas standard propensity scoring largely ignores shifts across different time steps. Moreover, one major issue with standard propensity is which portion of the data is considered current and which is considered outdated when creating a binary classifier as Eq. (2). This challenge becomes more complex when data is continuously changing, and what was considered current in one step becomes old in the next. However, our method does not suffer from this issue as it organically depends on time."
        },
        {
            "heading": "4 DISCUSSION",
            "text": "We propose a straightforward yet powerful approach for addressing gradual shifts in data distributions: a time-varying propensity score. Much past work has focused on a single shift between training/test data (Lu et al., 2021; Wang & Deng, 2018; Fakoor et al., 2020c; Chen et al., 2016) as well as restricted forms of shift involving changes in only the features (Sugiyama et al., 2007a; Reddi et al., 2015a), labels (Lipton et al., 2018; Garg et al., 2020; Alexandari et al., 2020), or in the underlying relationship between the two (Zhang et al., 2013; Lu et al., 2018). Past approaches to handle distributions evolving over time have been considered in the literature on: concept drift (Gomes et al., 2019; Souza et al., 2020), survival analysis (Cox, 1972; Lu, 2005), reinforcement learning (shift between the target policy and behavior policy) (Schulman et al., 2015; Wang et al., 2016; Fakoor et al., 2020a;b; 2021), (meta) online learning (Shalev-Shwartz, 2012; Finn et al., 2019; Harrison et al., 2020; Wu et al., 2021), and task-free continual/incremental learning (Aljundi et al., 2019; He et al., 2019). However, to best our knowledge, existing methods for these settings do not employ time-varying propensity weights like we propose here. One of the key advantages of our method is its ability to automatically detect data shifts without prior knowledge of their occurrence. Additionally, our method is versatile and can be applied in various problem domains, including supervised learning and reinforcement learning and it can be seamlessly integrated with existing approaches. Through extensive experiments in continuous supervised learning and reinforcement learning, we demonstrate the effectiveness of our method. It is important to note that while we refer to \"time\" in this paper, our methodology can also be applied to situations where shifts occur gradually over space or any other continuous-valued index. The broader impact of this research lies in enhancing the robustness of machine learning techniques in real-world scenarios that involve continuous distribution shifts."
        },
        {
            "heading": "A DERIVATIONS",
            "text": "Using a similar calculation as that of Eq. (1), we can write our time-varying propensity score-based objective as follows\nEx\u223cpT (x) [ \u2113(x) ] = Ex\u223cp(x|T ) [ \u2113(x) ] = \u222b t dp(x|T ) \u2113(x) = \u222b t \u222b x dp(t) dp(x|T ) dp(x|t) dp(x|t) \u2113(x) (17)\n= \u222b t \u222b x dp(t) dp(x|t) dp(x|T ) dp(x|t) \u2113(x)\n= \u222b t dp(t) \u222b x dp(x|t) dp(x|T ) dp(x|t) \u2113(x)\n= Et\u223cp(t)Ex\u223cp(x|t) [dp(x|T ) dp(x|t) \u2113(x) ]\n= Et\u223cp(t)Ex\u223cpt(x) [dpT (x) dpt(x) \u2113(x) ]\n= Et\u223cp(t)Ex\u223cpt(x) [ \u03c9(x, T, t)\u2113(x) ] (18)\n\u03c9 can be parameterized by a neural network and is trained based on our algorithm explained in the Appendix B."
        },
        {
            "heading": "B ALGORITHM DETAILS",
            "text": "We learn \u03c9 by building a binary classifier by utilizing samples from different time steps. In particular, we need to create a triplet (xi, t\u2032, t) for each of samples in the dataset with label 1 if t\u2032 equals the time corresponding to xi, i.e., if ti = t\u2032 and label \u22121 if t equals the time corresponding to xi, i.e. ti = t. These steps are detailed in Algorithm 1.\nAlgorithm 1 GenerateData\n1: Input: D = {xj , tj}Nj=1 2: T = [1, .., T ] 3: Initialize Do = \u2205 4: for j=1...N do 5: tr is chosen uniformly at random from T such that tj \u0338= tr 6: if random() >= 0.5 then 7: z = 1 8: Do \u2190 Do \u222a {(xj , tj, tr, z)} 9: else\n10: z = \u22121 11: Do \u2190 Do \u222a {(xj , tr, tj, z)} 12: end if 13: end for 14: Return Do\nOnce the triplet (xi, t\u2032, t) are generated, we fit the binary classifier by solving the following:\n\u2212minimize \u03b8\n1\nN N\u2211 (x,t2,t1,z)\u2208B log ( 1 + e\u2212z(g\u03b8(x,t2)\u2212g\u03b8(x,t1)) ) (19)\nwhere \u03c9(x, T, t) = exp (g\u03b8(x, T )\u2212 g\u03b8(x, t)) indicates our time-varying propensity score described in Sec. 2.1. Algorithm 2 presents the detailed implementation steps of our method.\nIt should be noted that we use the exact same algorithm (Algorithm 2) to learn \u03c9 for both reinforcement learning and continuous supervised learning experiments. The main difference between these cases is using of (x, y, t) as inputs for supervised learning tasks whereas (s, a, t) are used as inputs in the reinforcement learning experiments. Here s, a, x, y, and t represent state, action, input data, label, and time respectively. Note although g\u03b8 can be parameterized differently considering its given task (e.g. image classification tasks require g\u03b8 to be a convolutional deep network; however, continuous control tasks in reinforcement learning experiments need fully connected networks), its implementation details remain the same. Moreover, in order to implement Eq. (10) described in Sec. 2.1, we only need to change line 8 of Algorithm 2 as follow (all other details remain the same):\n\u2207\u03b8J \u2190 \u2212\u2207\u03b8 \u2211\n(x,t2,z)\u2208B\nlog ( 1 + e\u2212z(g\u03b8(x,t2)) ) (20)\nAlgorithm 2 Train Time-varying Propensity Score Estimator\n1: Input: D = {xj , tj}Nj=1 2: Input M : Number of epochs 3: Initialize Do = \u2205 4: for j=1...M do 5: Do \u2190 GenerateData(D) (see Algorithm 1) 6: repeat 7: Sample mini-batch B = {(x, t2, t1, z)} \u223c Do 8: \u2207\u03b8J \u2190 \u2207\u03b8\n\u2211 (x,t2,t1,z)\u2208B log ( 1 + e\u2212z(g\u03b8(x,t2)\u2212g\u03b8(x,t1)) ) 9: \u03b8 \u2190 \u03b8 \u2212 \u03b1\u2207\u03b8J\n10: until convergence 11: end for\nIt is important to point out that although Eq. (20) is used for the implementation of Algorithm 2 in all experiments presented in this paper, either method could have been utilized as Fig. 8 illustrates that both method performs similarly."
        },
        {
            "heading": "C ABLATION STUDIES AND ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "Method 1 vs Method 2. We compare performance of our Method 1 Eq. (8) and Method 2 Eq. (10) on continuous CIFAR-10 and CLEAR benchmarks. We can see in Fig. 8 that both methods perform similarly, although Method 1 has a slight edge over Method 2. Note we use Method 2 for all the experiments in the paper.\nZoomed-in version of Fig. 1. We provide a zoomed-in version of Fig. 1 in Fig. 7 where we also remove Recent and Fine-tune to make the plots less crowded. Note that the experiments in these plots are exactly the same as the ones in Fig. 1 but only different visualizations.\nNo Shift. Here, we analyze how our method performs when there is no shift in data and see whether or not it performs as good as Everything (Eq. (12))? To answer this question, we run new experiments on the CIFAR-10 benchmark without shift. We build continuous classification tasks from this dataset where we used past data to predict future data points. This is the same setting as other supervised learning experiments in the paper except we do not apply any shift to the data. Results of this experiment are shown in Fig. 14. As this experiment shows, our method and Everything perform similarly when there is no shift in the data. This experiment provides further evidence about the applicability of our method and shows that our method works regardless of presence of shifts in the data and it has no negative effect on the performance.\nNumber of samples. Our approach involves utilizing fixed number of samples (i.e. N in Eq. (19)) to train our time-varying propensity score as explained in Algorithm 1. Here, we investigate the influence of the sample size on the overall performance of our approach. We conduct new experiments on the CIFAR-10 benchmark, where we vary the sample size to 50, 100, and 300, while keeping all other settings consistent with the other supervised learning experiments described in the paper. The results of this experiment can be observed in Fig. 13. These experiments demonstrate that our method consistently outperforms Everything across different sample sizes. This further validates the versatility and effectiveness of our approach in various settings and scenarios.\nAdditional experiments with more shifts. In order to further validate the effectiveness of our method, we conduct additional experiments on CIFAR-10c (see Appendix D.1 for more details). The results of this experiment are shown in Figure Fig. 6. Additionally, we run more experiments for our reinforcement learning settings, and the results of these experiments are illustrated in Fig. 10 and 11.\nNumber of seeds. To assess the sensitivity of supervised learning results to the number of seeds, we conduct supplementary experiments employing 8 different random seeds and compare them with the results obtained from 3 seeds. The outcomes presented in Fig. 12 maintain consistency with those derived from 3 seeds. It\u2019s worth noting that for all reinforcement learning experiments, we employ 10 seeds."
        },
        {
            "heading": "D EXPERIMENT DETAILS",
            "text": "Implementation Details. The hyper-parameters, computing infrastructure, and libraries utilized in the experiments of this paper are presented in Table 2, 1, and Table 3, respectively. It is worth mentioning that we conducted a minimal random hyper-parameters search for the experiments in this paper and we mostly follow standard and readily available settings for the experiments whenever it is applicable. We intend to make the code for this paper publicly available upon its publication.\nD.1 CONTINUOUS LABEL SHIFT SETTINGS\nIn this paper, we simulate the continuous label shift process for image classification experiments by adopting the setting introduced in (Wu et al., 2021). However, we modify and adapt their settings to create more challenging scenarios for our experiments. In particular, in contrast to the setting of simulating label shifts for only two classes as done in their study, we extend our label shift simulation to encompass all classes, totaling 10 classes in this case:\n\u2200i \u2208 [1, C] \u2200t \u2208 [1, T ], vt = (1\u2212 t\nT )qit + (\nt\nT )qi+1t , (21)\nwhere T is the number of shift steps per a label pair i and i+ 1, t \u2208 [1, T ], C denotes the number of classes (C = 10 in our experiments), qit \u2208 RC is vector of class probabilities, and vt \u2208 RT \u00d7C . For CIFAR-10 experiments, we use T = 9 (called CIFAR-10a), T = 6 (CIFAR-10b), and T = 30 (CIFAR-10c). We also use T = 30 for CLEAR experiment. Listing 1 demonstrates a simple Python implementation of the equation presented in Eq. (21).\n1 import numpy as np 2 def create_shift(T, q1, q2): 3 lamb = 1.0 / (T-1) 4 return np.concatenate([np.expand_dims(q1 * (1 - lamb * t) + q2 * lamb * t\n, axis=0)for t in range(T)], axis=0)\nListing 1: Python code of Eq. (21) for given two classes"
        }
    ],
    "year": 2023
}