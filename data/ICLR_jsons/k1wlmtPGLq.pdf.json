{
    "abstractText": "Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "TION IN"
        },
        {
            "affiliations": [],
            "name": "SPIKING NEURAL NETWORKS"
        },
        {
            "affiliations": [],
            "name": "Haiyan Jiang"
        },
        {
            "affiliations": [],
            "name": "Vincent Zoonekynd"
        },
        {
            "affiliations": [],
            "name": "Giulia De Masi"
        },
        {
            "affiliations": [],
            "name": "Bin Gu"
        },
        {
            "affiliations": [],
            "name": "Huan Xiong"
        },
        {
            "affiliations": [],
            "name": "Mohamed bin Zayed"
        }
    ],
    "id": "SP:71d23bf50db43526368679127fc94d3da92d652b",
    "references": [
        {
            "authors": [
                "Filipp Akopyan",
                "Jun Sawada",
                "Andrew Cassidy",
                "Rodrigo Alvarez-Icaza",
                "John Arthur",
                "Paul Merolla",
                "Nabil Imam",
                "Yutaka Nakamura",
                "Pallab Datta",
                "Gi-Joon Nam"
            ],
            "title": "Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip",
            "venue": "IEEE transactions on computer-aided design of integrated circuits and systems,",
            "year": 2015
        },
        {
            "authors": [
                "Nils Bjorck",
                "Carla P Gomes",
                "Bart Selman",
                "Kilian Q Weinberger"
            ],
            "title": "Understanding batch normalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Tong Bu",
                "Wei Fang",
                "Jianhao Ding",
                "PengLin Dai",
                "Zhaofei Yu",
                "Tiejun Huang"
            ],
            "title": "Optimal ANN-SNN conversion for high-accuracy and ultra-low-latency spiking neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Sayeed Shafayet Chowdhury",
                "Nitin Rathi",
                "Kaushik Roy"
            ],
            "title": "Towards ultra low latency spiking neural networks for vision and sequential tasks using temporal pruning",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Mike Davies",
                "Narayan Srinivasa",
                "Tsung-Han Lin",
                "Gautham Chinya",
                "Yongqiang Cao",
                "Sri Harsha Choday",
                "Georgios Dimou",
                "Prasad Joshi",
                "Nabil Imam",
                "Shweta Jain"
            ],
            "title": "Loihi: A neuromorphic manycore processor with on-chip learning",
            "venue": "Ieee Micro,",
            "year": 2018
        },
        {
            "authors": [
                "Michael V DeBole",
                "Brian Taba",
                "Arnon Amir",
                "Filipp Akopyan",
                "Alexander Andreopoulos",
                "William P Risk",
                "Jeff Kusnitz",
                "Carlos Ortega Otero",
                "Tapan K Nayak",
                "Rathinakumar Appuswamy"
            ],
            "title": "Truenorth: Accelerating from zero to 64 million neurons",
            "venue": "years. Computer,",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Fei-Fei Li"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Shikuang Deng",
                "Shi Gu"
            ],
            "title": "Optimal conversion of conventional artificial neural networks to spiking neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Shikuang Deng",
                "Yuhang Li",
                "Shanghang Zhang",
                "Shi Gu"
            ],
            "title": "Temporal efficient training of spiking neural network via gradient re-weighting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Shikuang Deng",
                "Hao Lin",
                "Yuhang Li",
                "Shi Gu"
            ],
            "title": "Surrogate module learning: Reduce the gradient error accumulation in training spiking neural networks",
            "year": 2023
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552,",
            "year": 2017
        },
        {
            "authors": [
                "Peter U Diehl",
                "Daniel Neil",
                "Jonathan Binas",
                "Matthew Cook",
                "Shih-Chii Liu",
                "Michael Pfeiffer"
            ],
            "title": "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing",
            "venue": "In 2015 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2015
        },
        {
            "authors": [
                "Jianhao Ding",
                "Zhaofei Yu",
                "Yonghong Tian",
                "Tiejun Huang"
            ],
            "title": "Optimal ANN-SNN conversion for fast and accurate inference in deep spiking neural networks",
            "venue": "In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jianhao Ding",
                "Tong Bu",
                "Zhaofei Yu",
                "Tiejun Huang",
                "Jian Liu"
            ],
            "title": "SNN-RAT: Robustness-enhanced spiking neural network through regularized adversarial training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chaoteng Duan",
                "Jianhao Ding",
                "Shiyan Chen",
                "Zhaofei Yu",
                "Tiejun Huang"
            ],
            "title": "Temporal effective batch normalization in spiking neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jason K Eshraghian",
                "Max Ward",
                "Emre Neftci",
                "Xinxin Wang",
                "Gregor Lenz",
                "Girish Dwivedi",
                "Mohammed Bennamoun",
                "Doo Seok Jeong",
                "Wei D Lu"
            ],
            "title": "Training spiking neural networks using lessons from deep learning",
            "venue": "arXiv preprint arXiv:2109.12894,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Fang",
                "Zhaofei Yu",
                "Yanqi Chen",
                "Tiejun Huang",
                "Timoth\u00e9e Masquelier",
                "Yonghong Tian"
            ],
            "title": "Deep residual learning in spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wulfram Gerstner",
                "Werner M Kistler",
                "Richard Naud",
                "Liam Paninski"
            ],
            "title": "Neuronal dynamics: From single neurons to networks and models of cognition",
            "year": 2014
        },
        {
            "authors": [
                "Yufei Guo",
                "Yuanpei Chen",
                "Liwen Zhang",
                "Xiaode Liu",
                "Yinglei Wang",
                "Xuhui Huang",
                "Zhe Ma"
            ],
            "title": "Imloss: information maximization loss for spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bing Han",
                "Gopalakrishnan Srinivasan",
                "Kaushik Roy"
            ],
            "title": "RMP-SNN: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zecheng Hao",
                "Jianhao Ding",
                "Tong Bu",
                "Tiejun Huang",
                "Zhaofei Yu"
            ],
            "title": "Bridging the gap between anns and snns by calibrating offset spikes",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Haiyan Jiang",
                "Srinivas Anumasa",
                "Giulia De Masi",
                "Huan Xiong",
                "Bin Gu"
            ],
            "title": "A unified optimization framework of ann-snn conversion: Towards optimal mapping from activation values to firing",
            "year": 2023
        },
        {
            "authors": [
                "Youngeun Kim",
                "Priyadarshini Panda"
            ],
            "title": "Revisiting batch normalization for training low-latency deep spiking neural networks from scratch",
            "venue": "Frontiers in neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Hongmin Li",
                "Hanchao Liu",
                "Xiangyang Ji",
                "Guoqi Li",
                "Luping Shi"
            ],
            "title": "Cifar10-DVS: an event-stream dataset for object classification",
            "venue": "Frontiers in neuroscience,",
            "year": 2017
        },
        {
            "authors": [
                "Yuhang Li",
                "Shikuang Deng",
                "Xin Dong",
                "Ruihao Gong",
                "Shi Gu"
            ],
            "title": "A free lunch from ANN: Towards efficient, accurate spiking neural networks calibration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhang Li",
                "Yufei Guo",
                "Shanghang Zhang",
                "Shikuang Deng",
                "Yongqing Hai",
                "Shi Gu"
            ],
            "title": "Differentiable spike: Rethinking gradient-descent for training spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Changze Lv",
                "Jianhan Xu",
                "Xiaoqing Zheng"
            ],
            "title": "Spiking convolutional neural networks for text classification",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Qingyan Meng",
                "Mingqing Xiao",
                "Shen Yan",
                "Yisen Wang",
                "Zhouchen Lin",
                "Zhi-Quan Luo"
            ],
            "title": "Training high-performance low-latency spiking neural networks by differentiation on spike representation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyan Meng",
                "Mingqing Xiao",
                "Shen Yan",
                "Yisen Wang",
                "Zhouchen Lin",
                "Zhi-Quan Luo"
            ],
            "title": "Towards memory-and time-efficient backpropagation for training spiking neural networks",
            "venue": "arXiv preprint arXiv:2302.14311,",
            "year": 2023
        },
        {
            "authors": [
                "Emre O Neftci",
                "Hesham Mostafa",
                "Friedemann Zenke"
            ],
            "title": "Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2019
        },
        {
            "authors": [
                "Nitin Rathi",
                "Gopalakrishnan Srinivasan",
                "Priyadarshini Panda",
                "Kaushik Roy"
            ],
            "title": "Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kaushik Roy",
                "Akhilesh Jaiswal",
                "Priyadarshini Panda"
            ],
            "title": "Towards spike-based machine intelligence with neuromorphic",
            "venue": "computing. Nature,",
            "year": 2019
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Andrew Ilyas",
                "Aleksander M\u0105dry"
            ],
            "title": "How does batch normalization help optimization",
            "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Abhronil Sengupta",
                "Yuting Ye",
                "Robert Wang",
                "Chiao Liu",
                "Kaushik Roy"
            ],
            "title": "Going deeper in spiking neural networks: VGG and residual architectures",
            "venue": "Frontiers in neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Wu",
                "Yueyi Zhang",
                "Wenming Weng",
                "Yongting Zhang",
                "Zhiwei Xiong",
                "Zheng-Jun Zha",
                "Xiaoyan Sun",
                "Feng Wu"
            ],
            "title": "Training spiking neural networks with accumulated spiking flow",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jibin Wu",
                "Chenglin Xu",
                "Xiao Han",
                "Daquan Zhou",
                "Malu Zhang",
                "Haizhou Li",
                "Kay Chen Tan"
            ],
            "title": "Progressive tandem learning for pattern recognition with deep spiking neural networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yujie Wu",
                "Lei Deng",
                "Guoqi Li",
                "Jun Zhu",
                "Luping Shi"
            ],
            "title": "Spatio-temporal backpropagation for training high-performance spiking neural networks",
            "venue": "Frontiers in neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "Yujie Wu",
                "Lei Deng",
                "Guoqi Li",
                "Jun Zhu",
                "Yuan Xie",
                "Luping Shi"
            ],
            "title": "Direct training for spiking neural networks: Faster, larger, better",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Mingqing Xiao",
                "Qingyan Meng",
                "Zongpeng Zhang",
                "Yisen Wang",
                "Zhouchen Lin"
            ],
            "title": "Training feedback spiking neural networks by implicit differentiation on the equilibrium state",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mingqing Xiao",
                "Qingyan Meng",
                "Zongpeng Zhang",
                "Di He",
                "Zhouchen Lin"
            ],
            "title": "Online training through time for spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ruibin Xiong",
                "Yunchang Yang",
                "Di He",
                "Kai Zheng",
                "Shuxin Zheng",
                "Chen Xing",
                "Huishuai Zhang",
                "Yanyan Lan",
                "Liwei Wang",
                "Tieyan Liu"
            ],
            "title": "On layer normalization in the transformer architecture",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Wenrui Zhang",
                "Peng Li"
            ],
            "title": "Temporal spike sequence learning via backpropagation for deep spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hanle Zheng",
                "Yujie Wu",
                "Lei Deng",
                "Yifan Hu",
                "Guoqi Li"
            ],
            "title": "Going deeper with directly-trained larger spiking neural networks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Shibo Zhou",
                "Xiaohua Li",
                "Ying Chen",
                "Sanjeev T Chandrasekaran",
                "Arindam Sanyal"
            ],
            "title": "Temporalcoded deep spiking neural network with easy training and robust performance",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Spiking Neural Networks (SNNs) are known to be biologically inspired artificial neural networks (ANNs) and have recently attracted great research interest (Chowdhury et al., 2022; Ding et al., 2022). The attraction of SNNs lies in their ability to deliver energy-efficient and fast-inference computations when implemented on neuromorphic hardware such as Loihi (Davies et al., 2018) and TrueNorth (Akopyan et al., 2015; DeBole et al., 2019). These advantages arise from the fact that SNNs utilize spikes to transmit information between layers, whereby the networks circumvent multiplication during inference (Roy et al., 2019). However, the discrete and non-differentiable nature of the binary firing functions makes it difficult to directly train deep SNNs. ANN-to-SNN conversion (Diehl et al., 2015; Bu et al., 2022; Jiang et al., 2023) and directly training with surrogate gradients back-propagation (Neftci et al., 2019; Deng et al., 2022; 2023) are two typical solutions.\nBatch Normalization (BN) has found extensive use in ANNs and has seen tremendous success in boosting their performance by reducing the internal covariate shift (ICS) and flattening the loss landscape (Ioffe & Szegedy, 2015; Santurkar et al., 2018). In ANNs, ICS refers to changes in the distribution of layer inputs caused by updates of preceding layers, while in SNNs, the Temporal Covariate Shift (TCS) phenomenon (Duan et al., 2022) has been identified due to updates of preceding layers and prior time-steps, which transpires along the additional temporal dimension. Within SNNs, synaptic currents are sequentially fed into spiking neurons, with spike-triggered asynchronous currents accumulating in the membrane potential. Whenever this accumulated membrane potential exceeds a threshold, a spike is generated. This temporal dependency on membrane accumulation\n\u2217Corresponding authors. Codes are available at https://github.com/HaiyanJiang/SNN-TAB.\nhas the potential to amplify the internal covariate shift across the temporal domain. The intertwining of this temporal dependency with the TCS phenomenon, presents a significant challenge in direct training of SNNs especially for the integration of BN techniques into SNNs.\nWhen it comes to BN techniques for SNNs, only a few methods have been proposed. These methods either normalize data jointly by aggregating data across the temporal dimension or perform independent normalization at each discrete time-step. For example, Kim & Panda (2021) conducts independent batch normalization separately at each time-step. However, this approach uses separate sets of mean, variance, and scale and shift parameters at each time-step, failing to account for the temporal dependencies of the input spikes. While Zheng et al. (2021) merges the data along the time dimension and utilizes shared batch statistics across all time-steps for normalization. Nonetheless, introducing such overall statistics may limit the flexibility to capture varying temporal characteristics at different time-steps. On the other hand, Duan et al. (2022) attempts to tackle the TCS issue by assigning different weights to each time-step, while still utilizing shared batch statistics across all time-steps for normalization. Although these methods improve upon the performance of the SNN models, they do not significantly address the alignment with the neuron dynamics, i.e., the membrane accumulation dependency, or provide a potential to do so.\nIn this paper, we propose TAB (Temporal Accumulated Batch Normalization) as a solution to effectively address these challenges by closely aligning with the neuron dynamics, specifically the accumulated membrane potential, and providing more accurate batch statistics. This alignment establishes a natural connection between neuronal dynamics and batch normalization in SNNs. Neuron dynamics refer to the changes in the membrane potential of a neuron over time as it integrates input signals and generates spikes. Here, \u201caligning with neuron dynamics\u201d means that TAB is tailored to mimic or capture neurons\u2019 behavior as closely as possible, normalizing data in line with the temporal dependencies and information accumulation within neurons. This alignment ensures that TAB\u2019s normalization process corresponds well with how neurons naturally operate in SNNs, thus leading to improved performance by addressing the temporal covariate shift problem."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 RELATED WORK",
            "text": "SNN Learning Methods. Many works have recently emerged and focused on the supervised training of SNNs (Wu et al., 2021a; Zhou et al., 2021; Meng et al., 2022; Xiao et al., 2021). These SNN learning methods can be mainly categorized into two classes: ANN-to-SNN conversion (Diehl et al., 2015; Deng & Gu, 2021; Ding et al., 2021; Han et al., 2020; Li et al., 2021a; Bu et al., 2022; Hao et al., 2023; Lv et al., 2023) and end-to-end training with back-propagation (Fang et al., 2021; Zhang & Li, 2020; Deng et al., 2022; Xiao et al., 2022; Guo et al., 2022; Meng et al., 2023). ANN-to-SNN conversion takes a pre-trained ANN and converts it into an SNN by preserving the weights and replacing the ReLU activation function with a spiking activation function. This approach can be efficient in obtaining an SNN since the ANN has already been trained and the weights can be directly copied to the SNN. However, the resulting performance of the converted SNN may not be as good as that of the original source ANN. It usually requires a large number of time-steps for the converted SNN to achieve performance comparable to the source ANN. Direct end-to-end training usually employs the surrogate gradients (Wu et al., 2018; 2019; Neftci et al., 2019; Zheng et al., 2021; Eshraghian et al., 2021) method to overcome the non-differentiable nature of the binary spiking function to directly train SNNs from scratch. This method can yield comparable performance to that of traditional ANNs with a few time-steps.\nBN Method in ANNs. Batch normalization methods have significantly contributed to the success of ANNs by boosting their learning and inference performance (Ioffe & Szegedy, 2015; Xiong et al., 2020; Bjorck et al., 2018). BN is a technique used to stabilize the distribution (over a mini-batch) of inputs to each network layer during training. This is achieved by introducing additional BN layers which set the first two moments (mean and variance) of the activation distribution to zero and one. Then, the batch-normalized inputs are scaled and shifted using learnable/trainable parameters to preserve model expressiveness. This normalization is performed before the non-linearity is applied. The BN layer can be formulated as,\nBN(xi) = \u03b3x\u0302i + \u03b2 , x\u0302i = xi \u2212 \u00b5\u221a \u03c32 + \u03f5 , i = 1, \u00b7 \u00b7 \u00b7 , b .\nThe mini-batch mean \u00b5 and variance \u03c32 are computed by \u00b5 = 1b \u2211b i=1 xi and \u03c3 2 = 1b \u2211b i=1(xi\u2212\u00b5)2.\nBN Method in SNNs. Due to the additional temporal dimension, several recent studies have proposed modifications to batch normalization to fit the training of SNNs. The threshold-dependent Batch Normalization (tdBN) method (Zheng et al., 2021) is introduced to alleviate the gradient vanishing or explosion during training SNNs. The tdBN utilizes shared BN statistics and parameters (as the conventional BN) by merging the data along the temporal dimension. Similar to tdBN, the TEBN method (Duan et al., 2022) employs shared BN statistics by merging the data along the temporal dimension, then scales using different weights to capture temporal dynamics. Different from them, BNTT (Kim & Panda, 2021) uses separate BN statistics and parameters at each time-step t independently, however, it ignores the temporal dependencies of the input spikes. Differently, our TAB method leverages the accumulated pre-synaptic inputs in the temporal domain, which is in alignment with the membrane potential accumulation in the LIF model."
        },
        {
            "heading": "2.2 SPIKING NEURON DYNAMICS AND NEURON MODEL",
            "text": "SNNs use binary spike trains to transmit information between layers. Each neuron maintains its membrane potential dynamics ui(t) over time, \u201cintegrates\u201d the received input with a leakage (much like an RC circuit), and fires a spike if the accumulated membrane potential value exceeds a threshold. We adopt the widely used leaky-integrate-and-fire (LIF) model. Neuron dynamics refer to the changes in the membrane potential of a neuron over time as it integrates input signals and generates spikes, which can be formulated as a first-order differential equation (ODE),\nLIF Neuron Dynamics: \u03c4 dui(t)\ndt = \u2212ui(t) +RIi(t), ui(t) < Vth, (1)\nwhere Ii(t) is the injected input current to the i-th neuron at time t, ui(t) is the membrane potential of the i-th neuron at time t in the current layer, Vth is the membrane threshold, and \u03c4 denotes the membrane time constant, and R denotes the resistor. For numerical simulations of LIF neurons, we consider a discrete version of the neuron dynamics. Similar to Wu & He (2018), the membrane potential ui[t] of the i-th neuron at time-step (discrete) t is represented as:\nui[t] = \u03bbui[t\u2212 1] + \u2211\nj\u2208pre(i)\nWijoj [t] . (2)\nWe adopt a simple current model RIi[t] = \u2211\nj\u2208pre(i) Wijoj [t], with R absorbed in weights Wij . Here, oi[t] denotes the binary spike of neuron i at time-step [t], taking a value of 1 when a spike occurs and 0 otherwise. The index j refers to pre-synaptic neurons. The membrane potential ui[t] increases with the summation of input spikes from all the pre-synaptic neurons pre(i) connecting the current i-th neuron through synaptic weight Wij . It also decreases with a leak factor \u03bb (0 < \u03bb \u2a7d 1), where \u03bb and the time constant \u03c4 are related by \u03bb = e\u2212 \u2206t \u03c4 . The discrete LIF model degenerates to the IF model when \u03bb = 1, therefore in the following, we only use the LIF model with 0 < \u03bb \u2a7d 1. When the neuron\u2019s membrane potential ui[t] exceeds the threshold Vth, the neuron will fire a spike with oi[t] = 1 and then reset the membrane potential to 0. By combining the sub-threshold dynamics Eq. (2) and hard reset mechanism, the whole iterative LIF model can be formulated by:\nDiscrete LIF Neuron Model: ui[t] = \u03bbui[t\u2212 1](1\u2212 oi[t\u2212 1]) + \u2211\nj\u2208pre(i)\nWijoj [t] , (3)\noi[t] = H(ui[t]\u2212 Vth) , (4) where H(x) is the Heaviside step function, i.e., the non-differentiable spiking activation function. H(x) = 1 if x > 0 and H(x) = 0 otherwise."
        },
        {
            "heading": "3 PROPOSED TAB METHOD",
            "text": "In this section, we will present our TAB method. We begin by introducing the Temporal Dependencies and Temporal Covariate Shift in SNNs which motivate our method. Following this, we introduce our TAB method, which addresses these challenges. Finally, we establish a theoretical connection between the neural dynamics and the TAB method by deriving the closed-form solution of LIF dynamics ODE."
        },
        {
            "heading": "3.1 MOTIVATION: TEMPORAL DEPENDENCIES AND TEMPORAL COVARIATE SHIFT",
            "text": "Temporal dependencies in SNNs arise naturally from the sequential nature of spike events, where synaptic currents (also known as spike trains) are sequentially fed into spiking neurons, playing a pivotal role in capturing the dynamic evolution of input spikes over time. These networks model the dynamics of biological neurons through ODEs and utilize spikes to transmit information (Eshraghian et al., 2021). In SNNs, each neuron maintains a membrane potential, continuously \u2018integrating\u2019 and accumulating received spikes over time. It emits a spike only when its accumulated membrane potential exceeds a threshold, remaining inactive otherwise in the current time-step (Li et al., 2021a). This process highlights the intrinsic influence of temporal dynamics on the temporally delayed accumulation of the membrane potential. We refer to this accumulation dependency over the time dimension as temporal dependencies.\nIn SNNs, a phenomenon known as Temporal Covariate Shift (TCS) has been identified (Duan et al., 2022), which represents ICS (Internal Covariate Shift) (Ioffe & Szegedy, 2015) across the additional temporal dimension, and it refers to changes in the distribution of layer inputs caused by updates of preceding layers, and prior time-steps. Within the framework of SNNs, synaptic currents are sequentially fed into spiking neurons, and spike-triggered asynchronous currents are accumulated into the membrane potential which will trigger a spike when it exceeds the membrane threshold. This temporal dependency on membrane potential accumulation intensifies the internal covariate shift along the temporal domain. This temporal dependency, together with the TCS phenomenon, presents a significant challenge when integrating BN techniques into SNNs.\nOur motivation comes along these lines, how to perform batch normalization in training of SNNs, but keeping in mind the temporal dependency of the data, as well as the temporal covariate shift. A simple, yet elegant, method that aligns closely with this underlying neuron dynamics comes with Temporal Accumulated Batch normalization (TAB). Generally speaking, our TAB method addresses the temporal covariate shift issue by aligning with the inherent temporal dependencies in SNNs. Fig. 1 illustrates the temporal dependencies and neuron dynamics and showcases the involvement of our proposed TAB method.\nNeuronal dynamics refers to the change in membrane potential over time as a neuron integrates input signals and generates spikes. This temporal accumulation of the membrane potential in SNNs enables neurons to process input data by taking into account both past and current time-steps (with no access to future information beyond t), and the TAB method aligns closely with this underlying neuron dynamics and alleviates the TCS issue."
        },
        {
            "heading": "3.2 TEMPORAL ACCUMULATED BATCH NORMALIZATION (TAB)",
            "text": "To address the temporal covariate shift issue and to model the temporal distributions in SNNs, our TAB method aligns with the inherent temporal dependencies by utilizing the temporal accumulated batch statistics (\u00b51:t, \u03c321:t) over an expanding window [1, t]. To achieve this, we establish the relationship between the expectations and variances across accumulated time-steps (\u00b51:t, \u03c321:t) and those of the\nsingle time-step (\u00b5[t], \u03c32[t]), as follows:\n\u00b51:t = 1\nt t\u2211 s=1 \u00b5[s] , \u03c321:t = 1 t t\u2211 s=1 \u03c32[s] . (5)\nOur proposed TAB method utilizes Temporal Accumulated Statistics (\u00b51:t, \u03c321:t) for data normalization, and then assigns different learnable weights \u03c9[t] > 0 to each time-step to distinguish their effect on the final result. The TAB method is given by\nx\u0302i[t] = TAB(xi[t]) = \u03c9[t] ( \u03b3[t]\nxi[t]\u2212 \u00b51:t\u221a \u03c321:t + \u03f5 + \u03b2[t]\n) = \u03b3\u0302[t]\nxi[t]\u2212 \u00b51:t\u221a \u03c321:t + \u03f5 + \u03b2\u0302[t] , \u03c9[t] > 0 . (6)\nGiven the pre-synaptic inputs xl[t] to layer l at time-step t, the spiking neuron with TAB is as follows,\nxl[t] = W lol\u22121[t] , (7)\nul[t] = \u03bbul[t\u2212 1](1\u2212 ol[t\u2212 1]) + x\u0302l[t] , (8)\nwhere x\u0302l[t] = TAB(xl[t]) = \u03b3\u0302[t] xl[t]\u2212 \u00b51:t\u221a\n\u03c321:t + \u03f5 + \u03b2\u0302[t] . (9)\nHere ul[t] and ol[t] denote the membrane potential and binary spike outputs of all neurons in l-th layer at time-step t, and W l denotes the synaptic weights between layer l \u2212 1 and layer l. We assign different positive weights \u03c9l[t] > 0 to each time-step which is different from Deng et al. (2022) and \u03b3\u0302[t] = \u03c9[t]\u03b3[t], \u03b2\u0302[t] = \u03c9[t]\u03b2[t]. The weights \u03c9[t] and parameters \u03b3l[t],\u03b2l[t] are learnable, which are trained during the training process. For details, refer to ?? and ??. Refer to ?? for the learning rules to compute the gradients.\nComputation of the temporal accumulated statistics is dynamically performed, in a moving averaging fashion, without the need to store batch data from all previous time-steps. This not only saves memory, but is also an important feature of our novel approach. For the algorithm details of the TAB method, please refer to ?? in the Appendix.\nThe rationale behind employing this accumulated spatial-temporal information in TAB comes from the sequential processing and temporal dependency characteristics intrinsic to spiking neurons. The TAB method utilizes the accumulated batch statistics (\u00b51:t, \u03c321:t) over an expanding window [1, t]. Fig. 2 illustrates an overview of four typical BN methods used in SNNs: default BN (Ioffe & Szegedy, 2015), BNTT (Kim & Panda, 2021), tdBN (Zheng et al., 2021), and TEBN (Duan et al., 2022). A comprehensive overview of statistics and parameters used by these methods is summarized in ?? in the ??.\nAs shown in ??, BNTT (Kim & Panda, 2021) considers BN statistics at each time-step individually and calculates different BN statistics (\u00b5[t], \u03c32[t]) and BN parameters (\u03b3[t]) at each time-step, which ignores the temporal dependencies of the input spikes. In contrast, tdBN (Zheng et al., 2021) computes the same overall BN statistics (\u00b51:T , \u03c321:T ) and BN parameters (\u03b3, \u03b2) across all time-steps, but overlooking the temporal differences. Similarly, TEBN (Duan et al., 2022) employs the same overall BN statistics (\u00b51:T , \u03c321:T ) as tdBN, but introduces distinct weight parameters p[t] at each timestep to capture time-specific variations. However, both tdBN and TEBN, computing BN statistics over T time-steps, implicitly assume access to data from all T time-steps, that is, even if the current time-step is t < T , future information up to time-step T can also be obtained, which is not true for the temporal accumulation of membrane potential nor the neural dynamics. As illustrated in Fig. 2, the input statistics of tdBN and TEBN consider the statistics of all the time-steps and all effective batches, while BNTT considers BN statistics at each time-step. Despite these differences, none of the existing methods have addressed the alignment with the membrane potential accumulation."
        },
        {
            "heading": "3.3 THEORETICAL CONNECTION BETWEEN TAB METHOD AND THE NEURAL DYNAMICS",
            "text": "TAB is tailored to capture the temporal dependencies of neurons as closely as possible by aligning with the neuron dynamics. To explore the theoretical connection between the TAB method and the neural dynamics, we need to delve into the LIF dynamics from the perspective of differential equations. In SNNs, each neuron maintains the dynamics of its membrane potential U(t) over time,\nby \u201cintegrating\u201d the received input current I(t) with a leakage term until a spike is triggered. This is described as a first-order linear differential equation (ODE),\nNeuron Dynamics as an ODE: \u03c4 dU(t)\ndt = \u2212U(t) +RI(t), U(t) < Vth , (10)\nwhere I(t) represents the input current injected into the neuron at time t, and it is a function of t (note that I(t) is not a constant value). The closed-form solution of the LIF neuron dynamics (as an ODE) can be derived with analytical and theoretical methods. Additional details are available in ?? and ??. Lemma 1. The analytical closed-form solution for the first-order IVP (Initial Value Problem) of the LIF dynamics ODE is as follows (Gerstner et al., 2014),\nU(t) = exp ( \u2212 t \u03c4 )(\u222b t 0 R \u03c4 I(s)exp ( s \u03c4 ) ds+ U0 ) . (11)\nRemark 1. When the neuron initiates at the value U0 with no further input, i.e., I(t) = 0, the closed-form solution of the ODE Eq. (11) shows that the membrane potential U(t) will start at U0 and exponentially decay with a time constant \u03c4 , U(t) = U0exp ( \u2212 t\u03c4 ) . Consequently, we can determine the membrane potential ratio, often referred to as the leak factor, denoted by \u03bb, as \u03bb = U(t+\u2206t)U(t) = U0exp(\u2212 t+\u2206t\u03c4 ) U0exp(\u2212 t\u03c4 ) = exp ( \u2212\u2206t\u03c4 ) . This relationship enables us to formulate the discretization scheme as: U [t+ 1] = \u03bbU [t].\nThis remark provides insights into the behavior of the membrane potential in the absence of input and establishes the discretization principle used for LIF modeling. Lemma 2. Through applying integration by parts, we derive another equivalent form of the closedform solution for the LIF dynamics, denoted as:\nU(t) = exponential decay term\ufe37 \ufe38\ufe38 \ufe37 (U0 \u2212RI0)exp ( \u2212 t \u03c4 ) + input current model\ufe37 \ufe38\ufe38 \ufe37 RI(t)\ufe38 \ufe37\ufe37 \ufe38\ncommonly considered in the discrete LIF model\n\u2212 \u222b t 0 Rexp ( s\u2212 t \u03c4 ) dI(s)\ufe38 \ufe37\ufe37 \ufe38\nabsent in the discrete LIF model\n. (12)\nWith the application of the Riemann\u2013Stieltjes integral, the discretization version of the closed-form solution is represented as:\nU [t] = (U0\u2212RI0)exp(\u2212 t\u03c4 )\ufe37 \ufe38\ufe38 \ufe37 \u03bbU [t\u2212 1] + WO[t]=RI[t]\ufe37\ufe38\ufe38\ufe37 X[t] \u2212 n\u2211 i=0\ngiX[si]\ufe38 \ufe37\ufe37 \ufe38 TAB method . (13)\nIn this formulation Eq. (13), the first exponential decay term, \u03bbU [t \u2212 1], captures the temporal dependency of the membrane potential from the preceding time-step. The second term, a simple current input model, RI[t] = WO[t], incorporates spikes from the pre-connected neurons at the current time-step [t]. Significantly, the third term, representing the temporal accumulated input across all previous time-steps through a weighted sum of the input currents X[si] with associated weights gi, introduces a novel concept. Here 0 = s0 < \u00b7 \u00b7 \u00b7 < si < \u00b7 \u00b7 \u00b7 < sn = t denotes a partition of the time interval [0, t] with a finite sequence of numbers. Refer to ?? for the details. Importantly, note that this accumulation mechanism of the inputs is a foundational component of the TAB method, providing a link that connects the TAB method and the neural dynamics. Remark 2. The commonly used discrete LIF model in Eq. (2), as denoted by U [t] = \u03bbU [t\u22121]+X[t], is derived from the first two terms of the discretization version of the closed-form solution Eq. (13). The third term, representing the temporal accumulated input across all previous time-steps, however, is not incorporated into the discrete LIF models typically used in practice. Remark 3. Note that the recursive application of the discrete LIF model, as denoted by U [t] = \u03bbU [t \u2212 1] + X[t], yields the temporal evolution of the membrane potential as U [t] = \u03bbtU [0] +\u2211t\ns=1 \u03bb t\u2212sX[s]. This result shows the temporal dependency of the membrane potential accumulation\nin LIF neuron dynamics.\nRecalling the TAB method introduced in Sect. 3.2, our TAB method normalizes data utilizing temporal accumulated batch statistics (\u00b51:t, \u03c321:t) across an expanding window [1, t], where \u00b51:t and \u03c321:t represent the temporal accumulated information up to time-step [t]. The utilization of the temporal accumulated batch statistics aligns well with the accumulation mechanism of the membrane potentialthrough Eq. (13). Consequently, it alleviates the temporal covariate shift issue which refers to the changes in the distribution of layer inputs resulting from updates of preceding layers and prior time-steps. The entire TAB method procedure and membrane updates can be linked through Eq. (13), derived by solving the LIF dynamics ODE. This equation naturally connects TAB batch normalization to neuron dynamics, as evident in Eq. (13).\nUpon comparing the commonly used discrete LIF model in Eq. (2) with the discrete closed-form solution in Eq. (13), it shows that the TAB method reintroduces the accumulation term into the normalization procedure. This is achieved by using temporal accumulated batch statistics from time-step 1 to t. While the temporal accumulated batch statistics employed by the TAB method do not replicate the exact term in Eq. (13), but as an approximation. Thus, there exists no one-to-one functional mapping between the two. The adjustment within TAB method brings the discrete LIF model closer to its analytical closed-form counterpart, thus, TAB can work well in addressing the temporal covariate shift issue. This establishes a natural connection between neuron dynamics and batch normalization."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we conduct extensive experiments on large-scale static and neuromorphic datasets, CIFAR-10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), and DVS-CIFAR10 (Li et al., 2017), to verify the effectiveness of our proposed TAB method. We utilize the VGG network architecture and ResNet architecture. Firstly, we perform a comparative analysis of our TAB method with other BN methods in the context of SNNs. Further, we compare our TAB method with other state-of-the-art approaches. For implementation details, refer to ??."
        },
        {
            "heading": "4.1 COMPARISON WITH OTHER BN METHODS",
            "text": "We conduct our evaluation by comparing the performance of the proposed TAB method and other batch normalization methods in the context of SNNs. To ensure fairness in our comparisons, we do not employ advanced data augmentation techniques like cutout (DeVries & Taylor, 2017). Table 1 provides a comprehensive overview of the the test accuracy on both traditional static dataset CIFAR10, CIFAR-100 and neuromorphic dataset DVS-CIFAR10. On the CIFAR-10 dataset, our TAB method demonstrates remarkable performance improvement, achieving a top-1 accuracy of 94.73% with the ResNet-19 network using only 2 time-steps. Notably, this surpasses the performance of TEBN using 6 time-steps. Furthermore, when using the same network architecture, TAB consistently outperforms other BN methods, even with fewer time-steps T . This pattern holds true for other\ndatasets as well. For instance, on the DVS-CIFAR10 dataset, our TAB method achieves 1.6% better performance (76.7% v.s. 75.10%) while utilizing fewer time-steps (4 v.s. 10) than TEBN. Similarly, on CIFAR-100, our method exhibits a 0.55% increase in accuracy (76.31% v.s. 75.86%) compared to TEBN when both use 2 time-steps. All the accuracy values for other methods reported in the table are drawn from the existing literature."
        },
        {
            "heading": "4.2 COMPARISON ON LARGE-SCALE IMAGENET DATASET",
            "text": "In this section, we investigate the effectiveness of our TAB method on the ImageNet dataset, renowned for its extensive collection of more than 1.25 million training images and 50, 000 test images (Deng et al., 2009). The training set of ImageNet offers 1, 280 training samples for each label, and we apply standard preprocessing and augmentation techniques (He et al., 2016) to the training data. Test data is centered and cropped to dimensions of 224\u00d7 224. The evaluation employs the ResNet-34 architecture, a widely recognized model. The network is trained using the AdamW optimizer with an initial learning rate of 0.00002 and a weight decay of 0.02. Training occurs on an NVIDIA RTX A6000 with 4 GPUs, each handling a batch size of 24. To ensure unbiased statistics, we follow Zheng et al. (2021) and synchronize batch mean and variance across devices.\nThe results, presented in Tables Table 1 and ??, reveal the efficacy of our TAB method. Notably, even with a modest training duration of 80 epochs for T = 4, the TAB method exhibits a 3.29% improvement on ResNet-34 over TEBN at T = 4 (TAB with 67.78% vs. TEBN 64.29%). Impressively, with only 2 time-steps (T = 2), our TAB method achieves an accuracy of 65.38% on ImageNet, showcasing its promising performance."
        },
        {
            "heading": "4.3 COMPARISON WITH THE STATE-OF-THE-ART APPROACHES",
            "text": "In this section, we present a comprehensive comparison of our TAB method with other state-of-the-art learning methods for SNNs using CIFAR-10 as the benchmark dataset, as illustrated in Table 2.\nOn the VGG-11 architecture, our TAB method achieves an impressive accuracy of 94.73% while utilizing 4 time-steps, outperforming all the ANN-to-SNN conversion and hybrid training methods that require more time-steps. Besides, we follow TEBN (Duan et al., 2022) and adopt the cutout augmentation (DeVries & Taylor, 2017) on static datasets denoted by \u201c*\u201d in the table. Compared to other surrogate gradient methods, our TAB method consistently performs better. On ResNet-19, our TAB method achieves an accuracy of 96.09% with 6 time-steps, which is better than Dspike (94.25%), TET (94.5%), TEBN (95.6%) while using the same number of time-steps. Even when using only 2 time-steps T = 2, our TAB method on ResNet-19 achieves a higher accuracy than TEBN (Duan et al., 2022) which utilizes 6 time-steps. We contribute this elevated performance to the better representation capability of TAB, achieved by its alignment with the neuron dynamics, thereby bridging the gap between the discrete LIF model and the underlying neuron dynamics and making the two closer. For clarity, all reported accuracy values for other methods in the tables are sourced from the literature. Further experimental results on CIFAR-100 and DVS-CIFAR10 datasets are detailed in ?? from ??. For a comprehensive comparison with state-of-the-art (SOTA) methods on ImageNet, please consult ?? provided in ??."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Directly training SNNs is extremely challenging, even when adopting BN techniques to enable more stable training. The presence of the Temporal Covariate Shift (TCS) phenomenon, coupled with the intrinsic temporal dependency of neuron dynamics, further compounds these challenges for directly training SNNs. To tackle this, we have introduced TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization approach. TAB closely aligns with the neuron dynamics, normalizing data using temporal accumulated statistics, effectively capturing historical temporal dependencies similar to that of the accumulation process of the membrane potential in the LIF neuron model. Neuron dynamics refer to the changes in the membrane potential of a neuron over time as it integrates input signals and generates spikes. The alignment with the neuron dynamics means that the TAB method is tailored to mimic or capture the behavior of neurons as closely as possible. It aims to normalize the data in a manner that is coherent with the temporal dependencies and accumulation of information that occur within neurons as they process input signals. This alignment ensures that TAB\u2019s normalization process corresponds well with the way neurons naturally operate in SNNs, thereby leading to improved training and performance by addressing the temporal covariate shift problem."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work is part of the research project (\u201cEnergy-based probing for Spiking Neural Networks\", Contract No. TII/ARRC/2073/2021) in collaboration between Technology Innovation Institute (TII, Abu Dhabi) and Mohamed bin Zayed University of Artificial Intelligence (MBZUAI, Abu Dhabi).\nREPRODUCIBILITY STATEMENT\nThe experiments and results presented in this research are reproducible, with all code, data, and detailed methodologies available in the supplementary materials. The codebase has been documented extensively, ensuring clarity and ease of implementation for future researchers. The datasets used in this study, including CIFAR-10, CIFAR-100, and DVS-CIFAR10, are publicly accessible, and we provide precise instructions on data preprocessing and augmentation procedures. Additionally, the hardware and software specifications utilized for conducting experiments are thoroughly documented, enabling researchers to replicate our results under similar computational environments. We are committed to supporting the scientific community\u2019s efforts in validating and building upon our work, thus promoting transparency and trustworthiness in the field of spiking neural networks and batch normalization techniques.\nETHICS STATEMENT\nThis research strictly adheres to ethical standards and guidelines governing scientific inquiry. All experiments involving living subjects or animals were not a part of this study, eliminating any ethical concerns in that regard. In terms of data usage, we employed publicly available datasets, ensuring no breach of privacy or data protection regulations. In terms of research conduct, this study promotes openness and transparency by making all code, data, and methodologies accessible to the wider scientific community. We also acknowledge and properly cite prior work, respecting intellectual property rights and academic integrity. Furthermore, this research focuses on improving the efficiency and effectiveness of spiking neural networks, which could potentially contribute to more energyefficient AI applications. We are committed to upholding the highest ethical standards in research and encourage responsible and transparent scientific practices within the field."
        }
    ],
    "title": "TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA-",
    "year": 2024
}