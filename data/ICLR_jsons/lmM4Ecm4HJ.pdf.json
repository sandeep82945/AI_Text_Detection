{
    "abstractText": "Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest. However, in the realworld where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability. In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes. We compute the box stability score (BoS score) to reflect this stability. Specifically, given an image, we compute a normal set of bounding boxes and a second set after feature map dropout. To obtain BoS score, we use bipartite matching to find the corresponding boxes between the two sets and compute the average Intersection over Union (IoU) across the entire test set. We contribute to finding that BoS score has a strong, positive correlation with detection accuracy measured by mean average precision (mAP) under various test environments. This relationship allows us to predict the accuracy of detectors on various real-world test sets without accessing test ground truths, verified on canonical detection tasks such as vehicle detection and pedestrian detection.",
    "authors": [],
    "id": "SP:871d899e0a227c04ddb8585cdbd8ecdff1d1911f",
    "references": [
        {
            "authors": [
                "Amir Bar",
                "Xin Wang",
                "Vadim Kantorov",
                "Colorado J Reed",
                "Roei Herzig",
                "Gal Chechik",
                "Anna Rohrbach",
                "Trevor Darrell",
                "Amir Globerson"
            ],
            "title": "Detreg: Unsupervised pretraining with region priors for object detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Markus Braun",
                "Sebastian Krebs",
                "Fabian Flohr",
                "Dariu M Gavrila"
            ],
            "title": "Eurocity persons: A novel benchmark for person detection in traffic scenes",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos"
            ],
            "title": "Cascade r-cnn: high quality object detection and instance segmentation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Chen",
                "Jiaqi Wang",
                "Jiangmiao Pang",
                "Yuhang Cao",
                "Yu Xiong",
                "Xiaoxiao Li",
                "Shuyang Sun",
                "Wansen Feng",
                "Ziwei Liu",
                "Jiarui Xu",
                "Zheng Zhang",
                "Dazhi Cheng",
                "Chenchen Zhu",
                "Tianheng Cheng",
                "Qijie Zhao",
                "Buyu Li",
                "Xin Lu",
                "Rui Zhu",
                "Yue Wu",
                "Jifeng Dai",
                "Jingdong Wang",
                "Jianping Shi",
                "Wanli Ouyang",
                "Chen Change Loy",
                "Dahua Lin"
            ],
            "title": "MMDetection: Open mmlab detection toolbox and benchmark",
            "venue": "arXiv preprint arXiv:1906.07155,",
            "year": 2019
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Scharw\u00e4chter",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset",
            "venue": "In CVPR Workshop on the Future of Datasets in Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Ciprian A Corneanu",
                "Sergio Escalera",
                "Aleix M Martinez"
            ],
            "title": "Computing the testing error without a testing set",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhigang Dai",
                "Bolun Cai",
                "Yugeng Lin",
                "Junying Chen"
            ],
            "title": "Up-detr: Unsupervised pre-training for object detection with transformers",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Kumari Deepshikha",
                "Sai Harsha Yelleni",
                "PK Srijith",
                "C Krishna Mohan"
            ],
            "title": "Monte carlo dropblock for modelling uncertainty in object detection",
            "venue": "arXiv preprint arXiv:2108.03614,",
            "year": 2021
        },
        {
            "authors": [
                "Weijian Deng",
                "Liang Zheng"
            ],
            "title": "Are labels always necessary for classifier accuracy evaluation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Weijian Deng",
                "Stephen Gould",
                "Liang Zheng"
            ],
            "title": "What does rotation prediction tell us about classifier accuracy under varying testing environments",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Pinar Donmez",
                "Guy Lebanon",
                "Krishnakumar Balasubramanian"
            ],
            "title": "Unsupervised supervised learning i: Estimating classification and regression errors without labels",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Kaiwen Duan",
                "Song Bai",
                "Lingxi Xie",
                "Honggang Qi",
                "Qingming Huang",
                "Qi Tian"
            ],
            "title": "Centernet: Keypoint triplets for object detection",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Alexandre Drouin",
                "Brady Neal",
                "Nitarshan Rajkumar",
                "Ethan Caballero",
                "Linbo Wang",
                "Ioannis Mitliagkas",
                "Daniel M Roy"
            ],
            "title": "In search of robust measures of generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International journal of computer vision,",
            "year": 2009
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "In international conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Saurabh Garg",
                "Sivaraman Balakrishnan",
                "Zachary C Lipton",
                "Behnam Neyshabur",
                "Hanie Sedghi"
            ],
            "title": "Leveraging unlabeled data to predict out-of-distribution performance",
            "venue": "arXiv preprint arXiv:2201.04234,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Christoph Stiller",
                "Raquel Urtasun"
            ],
            "title": "Vision meets robotics: The kitti dataset",
            "venue": "The International Journal of Robotics Research,",
            "year": 2013
        },
        {
            "authors": [
                "Gregory Griffin",
                "Alex Holub",
                "Pietro Perona"
            ],
            "title": "Caltech-256 object category dataset",
            "year": 2007
        },
        {
            "authors": [
                "Devin Guillory",
                "Vaishaal Shankar",
                "Sayna Ebrahimi",
                "Trevor Darrell",
                "Ludwig Schmidt"
            ],
            "title": "Predicting with confidence on unseen distributions",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "arXiv preprint arXiv:1610.02136,",
            "year": 2016
        },
        {
            "authors": [
                "Yiding Jiang",
                "Dilip Krishnan",
                "Hossein Mobahi",
                "Samy Bengio"
            ],
            "title": "Predicting the generalization gap in deep networks with margin distributions",
            "venue": "arXiv preprint arXiv:1810.00113,",
            "year": 2018
        },
        {
            "authors": [
                "Yiding Jiang",
                "Behnam Neyshabur",
                "Hossein Mobahi",
                "Dilip Krishnan",
                "Samy Bengio"
            ],
            "title": "Fantastic generalization measures and where to find them",
            "year": 1912
        },
        {
            "authors": [
                "Harold W Kuhn"
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly,",
            "year": 1955
        },
        {
            "authors": [
                "Yandong Li",
                "Xuhui Jia",
                "Ruoxin Sang",
                "Yukun Zhu",
                "Bradley Green",
                "Liqiang Wang",
                "Boqing Gong"
            ],
            "title": "Ranking neural checkpoints",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yuen Peng Loh",
                "Chee Seng Chan"
            ],
            "title": "Getting to know low-light images with the exclusively dark dataset",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2019
        },
        {
            "authors": [
                "Omid Madani",
                "David Pennock",
                "Gary Flake"
            ],
            "title": "Co-validation: Using model disagreement on unlabeled data to validate classification algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2004
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "David McAllester",
                "Nati Srebro"
            ],
            "title": "Exploring generalization in deep learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Emmanouil Platanios",
                "Hoifung Poon",
                "Tom M Mitchell",
                "Eric J Horvitz"
            ],
            "title": "Estimating accuracy from unlabeled data: A probabilistic logic approach",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Emmanouil Antonios Platanios",
                "Avinava Dubey",
                "Tom Mitchell"
            ],
            "title": "Estimating accuracy from unlabeled data: A bayesian approach",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Hamid Rezatofighi",
                "Nathan Tsoi",
                "JunYoung Gwak",
                "Amir Sadeghian",
                "Ian Reid",
                "Silvio Savarese"
            ],
            "title": "Generalized intersection over union: A metric and a loss for bounding box regression",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Kuniaki Saito",
                "Donghyun Kim",
                "Stan Sclaroff",
                "Trevor Darrell",
                "Kate Saenko"
            ],
            "title": "Semi-supervised domain adaptation via minimax entropy",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Shuai Shao",
                "Zijian Zhao",
                "Boxun Li",
                "Tete Xiao",
                "Gang Yu",
                "Xiangyu Zhang",
                "Jian Sun"
            ],
            "title": "Crowdhuman: A benchmark for detecting human in a crowd",
            "venue": "arXiv preprint arXiv:1805.00123,",
            "year": 2018
        },
        {
            "authors": [
                "Xiaoxiao Sun",
                "Yunzhong Hou",
                "Hongdong Li",
                "Liang Zheng"
            ],
            "title": "Label-free model evaluation with semi-structured dataset representations",
            "venue": "arXiv preprint arXiv:2112.00694,",
            "year": 2021
        },
        {
            "authors": [
                "Zhi Tian",
                "Chunhua Shen",
                "Hao Chen",
                "Tong He"
            ],
            "title": "Fcos: Fully convolutional one-stage object detection",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Longyin Wen",
                "Dawei Du",
                "Zhaowei Cai",
                "Zhen Lei",
                "Ming-Ching Chang",
                "Honggang Qi",
                "Jongwoo Lim",
                "Ming-Hsuan Yang",
                "Siwei Lyu"
            ],
            "title": "Ua-detrac: A new benchmark and protocol for multi-object detection and tracking",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2020
        },
        {
            "authors": [
                "Tete Xiao",
                "Colorado J Reed",
                "Xiaolong Wang",
                "Kurt Keutzer",
                "Trevor Darrell"
            ],
            "title": "Region similarity representation learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Fisher Yu",
                "Haofeng Chen",
                "Xin Wang",
                "Wenqi Xian",
                "Yingying Chen",
                "Fangchen Liu",
                "Vashisht Madhavan",
                "Trevor Darrell"
            ],
            "title": "Bdd100k: A diverse driving dataset for heterogeneous multitask learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Cityscapes (Cordts"
            ],
            "title": "2016), which are used as seeds to generate sample sets by various transformations. Each point \u201c\u2022\u201d of different colors represents a sample set generated from different seed",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Object detection systems use bounding boxes to locate objects within an image (Ren et al., 2015; Lin et al., 2017; He et al., 2017), and the quality of bounding boxes offers an intuitive understanding of how well objects of a class of interest, e.g., vehicles, are detected in an image. To measure detection performance, object detection tasks typically use the mean Average Precision (mAP) (Everingham et al., 2009) computed between predicted and ground-truth bounding boxes. However, it is challenging to determine the accuracy of bounding boxes in the absence of test ground truths in real-world scenarios, which makes it difficult to evaluate the detector generalization ability.\nIn this work, we start with a specific aspect of bounding box quality, i.e., bounding box stability under feature perturbation. Specifically, given an image and a trained detector, we apply Monte Carlo (MC) dropout (Gal & Ghahramani, 2016) in the testing processing, which randomly set some elements in certain backbone feature maps to zero. Afterward, we use bipartite matching to find the mapping of bounding boxes with and without dropout perturbations, and then compute their intersection over union (IoU), called box stability score (BoS score). As shown in Fig. 1(a) and (b), under feature perturbation, bounding boxes remaining stable (with high BoS score) would indicate a relatively good prediction, while bounding boxes changing significantly (with low BoS score) may suggest poor predictions.\nBased on this idea, we contribute to discovering strong correlation between bounding box stability and detection accuracy under various real-world test environments. Specifically, given a trained detector and a set of synthesized sample sets, we compute BoS score and actual mAP over each sample set. As the strong correlation (i.e., coefficients of determination R2 > 0.94) shown in Fig. 1(c), a detector overall would yield highly stable bounding boxes and thus have good accuracy (mAP) in relative environments, and in difficult environments where the detector exhibits low bounding box stability, object detection performance can be poor.\nThis finding endows us with the capacity to estimate detection accuracy in various test environments without the need to access ground truth. This is because the BoS score (1) does not rely on labels in computation, which encodes the difference between original predictions and predictions after feature dropout, and (2) is strongly correlated with detection accuracy. In fact, it is well known that detection accuracy drops when the test set has different distribution from the training set. That said, without ground truths, it is challenging how to estimate test accuracy under distribution shifts. As a use case, being able to obtain model accuracy without ground truths allows us to efficiently assess systems deployed in new environments.\nSeveral methods (Deng & Zheng, 2021; Guillory et al., 2021; Deng et al., 2021; Sun et al., 2021; Li et al., 2021; Garg et al., 2022), collectively referred to as AutoEval, have emerged to predict the classifier performance in new unlabeled environments. But these methods are poor at predicting detector performance because they do not take into account the regression characterization of the detection task. With the aforementioned box stability score measurement, we can effectively estimate the detection accuracy of a detector without ground truth. It is also the first method for AutoEval in object detection.\nIn summary, our main contributions are as follows:\n(1) We report a strong positive correlation between the detection mAP and BoS score. Our proposed BoS score calculates the stability degree for the bounding box with and without feature perturbation by bipartite matching, which is an unsupervised and concise measurement.\n(2) We demonstrate an interesting use case of this strong correlation: estimating detection accuracy in terms of mAP without test ground truths. To our best knowledge, we are the first to propose the problem of unsupervised evaluation of object detection. For the first time, we explore the feasibility of applying existing AutoEval algorithms for classification to object detection.\n(3) To show the effectiveness of the BoS score in estimating detection mAP, this paper conducts experiments on two detection tasks: vehicle detection and pedestrian detection. For both tasks, we collate existing datasets that have the category of interest and use leave-one-out cross-validation for mAP estimator building and test mAP estimation. On these setups, we show that our method yields very competitive mAP estimates of four different detectors on various unseen test sets. BoS score achieves only 2.25% RMSE on mAP estimation of the vehicle detection, outperforming confidencebased measurements by at least 2.21 points."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Mainstream object detectors are often categorized into two types, two-stage detectors (Ren et al., 2015; Cai & Vasconcelos, 2019) predict boxes w.r.t. proposals, and single-stage methods make predictions w.r.t. anchors (Lin et al., 2017) or a grid of possible object centers (Tian et al., 2019; Duan et al., 2019). Recently, transformer-based methods have shown an incredible capacity for object detection, including DETR (Carion et al., 2020), Deformable DETR (Zhu et al., 2020), and DINO (Zhang et al., 2022), etc. In the community, numerous popular datasets are available for benchmarking detector performance, such as COCO (Lin et al., 2014) and ADE20K (Zhou et al., 2019), and so\non. Nonetheless, due to the distribution shifts, the results on test sets in public benchmarks may not accurately reflect the performance of the model in unlabeled real-world scenarios. To address this issue, we focus on studying detector generalization ability on test sets from various distributions.\nLabel-free model evaluation predicts performance of a certain model on different test sets (Madani et al., 2004; Donmez et al., 2010; Platanios et al., 2016; 2017; Jiang et al., 2019). Some works (Guillory et al., 2021; Saito et al., 2019; Hendrycks & Gimpel, 2016) indicate classification accuracy by designing confidence-based measurements. In the absence of image labels, recent works (Deng & Zheng, 2021; Deng et al., 2021; Sun et al., 2021) estimated model performance for AutoEval with regression. Deng & Zheng (2021); Sun et al. (2021) used feature statistics to represent the distribution of a sample dataset to predict model performance. Deng et al. (2021) estimated classifier performance from the accuracy of rotation prediction. More recently, ATC (Garg et al., 2022) predicted accuracy as the fraction of unlabeled examples for which the model confidence exceeds the learned threshold. Different from the above works designed for the classification task, we aim to study a brand-new problem of estimating the detector performance in an unlabeled test domain.\nModel generalization aims to estimate the generalization error of a model on unseen images. Several works (Neyshabur et al., 2017; Jiang et al., 2018; Dziugaite et al., 2020) proposed various complexity measures on training sets and model parameters, to predict the performance gap of a model between a given pair of training and testing sets. Neyshabur et al. (2017) derived bounds on the generalization gap based on the norms of the weights across layers. Jiang et al. (2018) designed a predictor for the generalization gap based on layer-wise margin distributions. Corneanu et al. (2020) derived a set of topological summaries to measure persistent topological maps of the behavior of deep neural networks, and then computed the testing error without any testing dataset. Notably, these methods assume that the training and testing distributions are identical. In contrast, our work focuses on predicting the model performance on test sets from unseen distributions."
        },
        {
            "heading": "3 STRONG CORRELATION BETWEEN BOUNDING BOX STABILITY AND DETECTION ACCURACY",
            "text": ""
        },
        {
            "heading": "3.1 MEAN AVERAGE PRECISION: A BRIEF REVISIT",
            "text": "Mean average precision (mAP) (Everingham et al., 2009) is widely used in object detection evaluation. We first define a labeled test set Dl = {(xi, yi)}Mi=1, in which xi is an image, yi is its ground truth label, and M is the number of images. Given a trained detector f\u03b8 : xi \u2192 y\u0303i, which is parameterized by \u03b8 and maps an image xi to its predicted objects y\u0303i. Since the Dl is available, we can obtain the precision and recall curves of the detector by comparing the predicted objects y\u0303i with the ground truths yi. The Average Precision (AP) summarises the area under the precision/recall curve. mAP50 and mAP75 correspond to the average AP over all K categories under 0.50, 0.75 IoU threshold, respectively. mAP corresponds to the average AP for preconfigured IoU thresholds.\nIn unsupervised detector evaluation, given a detector f\u03b8 and an unlabeled dataset Du = {xi}Mi=1, we want to find a mAP predictor A : (f\u03b8,Du) \u2192 mAP, which can output an estimated mAP \u2208 [0, 1] of detector f\u03b8 on this test set:\nmAPunsup = A (f\u03b8,Du) . (1)"
        },
        {
            "heading": "3.2 MEASUREMENT OF BOUNDING BOX STABILITY",
            "text": "This paper uses an existing technique (MC dropout (Gal & Ghahramani, 2016)) for measurement. Let us define Nori as the number of objects detected in an image by the original detector, Nper as the number of objects detected in the same image by the perturbed detector with MC dropout. Let N and Nmax represent the smaller or larger value between Nori and Nper, respectively. On this basis, we denote the set with the fewest objects in the predicted set as y = {yj}Nj=1, and the most as y\u0302 = {y\u0302j}Nmaxj=1 . To find a bipartite matching between these two sets, we search for a permutation of N elements \u03c3 \u2208 SN with the lowest cost:\n\u03c3\u0302 = argmin \u03c3\u2208SN N\u2211 j Lmatch ( yj , y\u0302\u03c3(j) ) , (2)\nwhere Lmatch ( yj , y\u0302\u03c3(j) ) is a pair-wise matching cost between the original prediction and the perturbed prediction with index j and index \u03c3(j), respectively. This optimal assignment is computed efficiently with the Hungarian algorithm following previous works (Kuhn, 1955; Carion et al., 2020).\nThe matching cost considers the similarity between the predicted boxes of the original detector and the perturbed detector. However, using the widely-used l1 loss will lead to different scales for small and large boxes, even if their relative errors are similar. Therefore, we adopt the GIoU loss (Rezatofighi et al., 2019) and define the matching loss L(y, y\u0302) as:\nL(y, y\u0302) = \u2211N\nj Lgiou(bj , b\u0302\u03c3(j)) N , (3)\nwhere bj and b\u0302\u03c3(j) \u2208 [0, 1]4 denote a pair of matched boxes with index j and index \u03c3(j), respectively. After that, we average the matching loss over all M test images in Du and obtain the box stability score of the unseen test set Du:\nBS(Du) = \u2211M\ni=1 Li M , (4)"
        },
        {
            "heading": "4 USE CASE: LABEL-FREE DETECTOR EVALUATION",
            "text": "Following the practice in (Deng & Zheng, 2021), we formulate label-free evaluation as a regression problem. First, we create a training meta-dataset composed of fully labeled sample sets which are transformed versions of seed sets. Second, for each sample set, we compute the mAP and BoS score and train a regression model using BoS score as input and output mAP. Third, we use real-world test sets which are unseen during training, for mAP estimation."
        },
        {
            "heading": "4.1 TRAINING META-DATASET",
            "text": "Take mAP estimation on vehicle detection as an example. We generate 500 sample sets from 10 seed sets as mentioned in Section 5.4. We employ the leave-one-out evaluation procedure, where each time we use 450 labeled sample sets {Dl1,Dl2, ...} generated from 9 out of 10 sources as the training meta-set. All images from the remaining 1 source serve as a real-world test set Du. Therefore, there are 10 different permutations to construct the training meta-set and test set. We set the maximum number of testing images to 5,000, as the number of images in the test set differs in each permutation."
        },
        {
            "heading": "4.2 REGRESSION MODEL TRAINING AND REAL-WORLD TESTING",
            "text": "From the strong correlation of box stability score and mAP shown in Fig. 2(c), we train an mAP estimator (i.e., linear regression model) on the previously constructed training meta-set to fit this re-\nlationship. Given detector f\u03b8, the linear regression model Alinear uses its BoS score as input and the corresponding mAP as target. The i-th training sample of Alinear can be denoted as {BSi,mAPi}. The linear regression model Alinear is written as,\nmAPi = Alinear(f\u03b8, f \u2032\u03b8,Dli) = \u03c91BSi + \u03c90, (5) where f \u2032\u03b8 denotes the perturbed detector from f\u03b8, and \u03c90, \u03c91 are parameters of the linear regression model. We use a standard least square loss to optimize this regression model. During real-world testing, given an unlabelled test set Du and detector f\u03b8, we compute the BoS score and then predict detector accuracy m\u0303AP using the trained mAP estimator."
        },
        {
            "heading": "5 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL DETAILS",
            "text": "Datasets. We study label-free detector evaluation on two canonical object detection tasks in the main experiment: vehicle detection and pedestrian detection, to verify our method. For vehicle detection, we use 10 datasets, including COCO (Lin et al., 2014), BDD (Yu et al., 2020), Cityscapes (Cordts et al., 2015), DETRAC (Wen et al., 2020), Exdark (Loh & Chan, 2019) Kitti (Geiger et al., 2013), Self-driving (Kaggle, 2020a), Roboflow (Kaggle, 2022), Udacity (Kaggle, 2021), Traffic (Kaggle, 2020b). For pedestrian detection, we use 9 datasets, including COCO (Lin et al., 2014), Caltech (Griffin et al., 2007), Crowdhuamn (Shao et al., 2018), Cityscapes (Cordts et al., 2015), Self-driving (Kaggle, 2020a), Exdark (Loh & Chan, 2019), EuroCity (Braun et al., 2019), Kitti (Geiger et al., 2013), CityPersons (Zhang et al., 2017). Note that the annotation standard of the datasets on the category of interest should be uniform.\nMetrics. Following (Deng & Zheng, 2021), we use root mean squared error (RMSE) as the metric to compute the performance of label-free detector evaluation. RMSE measures the average squared difference between the estimated mAP and ground truth mAP. A small RMSE corresponds to good performance. Our main experiments are repeated 10 times due to the stochastic nature of dropout, after which mean and standard deviation are reported.\nDetectors. In the main experiment, we employ pre-trained ResNet-50 (He et al., 2016) as the backbone and one-stage RetinaNet (Lin et al., 2017) as the detection head. Some other detector architectures, including Faster R-CNN (Ren et al., 2015) with R50, Faster R-CNN with Swin (Liu et al., 2021), RetinaNet with Swin, are also compared in Table 4 (left).\nCompared methods. Existing methods in label-free model evaluation, including prediction score (PS) (Hendrycks & Gimpel, 2016), entropy score (ES) (Saito et al., 2019), average confidence (AC) (Guillory et al., 2021), ATC (Garg et al., 2022), and Fre\u0301chet distance (FD) (Deng & Zheng, 2021), are typically experimented on image classification. For comparison, we extend them to object detection. Because PS, ES, AC, and ATC are based on the softmax output, we use the softmax output of the detected bounding boxes to compute these scores. To compute FD, we use the feature from the last stage of the backbone to calculate the train-test domain gap. Hyperparameters of all the compared methods are selected on the training set, such that the training loss is minimal.\nVariant study: comparing bounding box stability with confidence stability. Using the same MC dropout and bipartite matching strategy, we can also measure the changes in bounding box softmax output instead of bounding box positions. In Fig. 3(c), we compare this variant with BoS score of their capability in estimating detection accuracy. Results show that the bounding box stability score is superior to confidence stability score. A probable reason is that confidence stability score does not change much due to over-confidence even if the bounding box changes significantly. So it is less indicative of detection performance."
        },
        {
            "heading": "5.2 MAIN EVALUATION",
            "text": "Comparison with state-of-the-art methods. We compare our method with existing accuracy evaluators that are adapted to the detection problem. After searching optimal thresholds on the training meta-set, we set \u03c41 = 0.4, 0.95, and 0.3 for ATC (Garg et al., 2022), PS (Hendrycks & Gimpel, 2016), and ES (Saito et al., 2019), respectively. As mAP estimation for vehicle detection is shown in Table 1, through the leave-one-out test procedure, our method achieves the lowest average RMSE\n= 2.25% over ten runs. The second-best mAP estimator, ATC, yields an average RMSE = 4.46%. Across the ten test sets, our method gives consistently good mAP estimations, evidenced by the relatively low standard deviation of 0.13%. Specifically, on COCO, BDD, and Cityscapes, our method yields the lowest RMSE of 1.26%, 1.90%, 0.89%, respectively.\nIn Table 2, our method is compared with existing ones on label-free evaluation of pedestrian detection. We have similar observations, where BoS score makes consistent predictions across the six test scenarios, leading to the lowest average RMSE. These results demonstrate the usefulness of our key contribution: the strong correlation between BoS score and mAP.\nUsing BoS score for multi-class detectors. BoS score can be easily extended to multi-class detectors theoretically because of its class-wise nature. To verify this, we should collect many object detection datasets that have diverse appearances and the same label space. We tried and found 5 datasets that allow us to do pedestrian and vehicle detection. We implement BoS and report results in Table 3, where BoS is shown to be competitive compared with existing methods. Besides, we observe that all methods might not work as well in multi-category as it does in single-class in some datasets. This is because when confronted with multi-class mAP predictions, the features between different categories may interfere with each other.\nEffectiveness of our method for different detector structures. Apart from the detector structure composed of RetinaNet (Lin et al., 2017) head with ResNet-50 (He et al., 2016) backbone evaluated in Table 1, we further apply this method for other structures such as Faster R-CNN (Ren et al., 2015) head with Swin (Liu et al., 2021) backbone on vehicle detection, and results are summarized in Table 4 (left). Compared with existing methods, our method yields very competitive mAP predictions. For example, using ResNet-50 backbone and RetinaNet as head, the mean RMSE of our method is 2.25%, while the second best method ATC gives 4.46%. Our method is less superior under the Swin-T backbone. It is perhaps because the transformer backbone is different from the convolutional neural networks w.r.t attention block, which may not be susceptible to dropout perturbation.\nEffectiveness of combining BoS score with existing dataset-level statistics. To study whether existing label-free evaluation methods are complementary, we combine each of Entropy Score, Prediction Score, ATC, and FD with the proposed BoS score. As shown in Fig. 3(c), combining existing statistics with BoS score does not result in a noticeable improvement. For example, the average RMSE of the \u201cbox stability score + prediction score\u201d is on the same level as using BoS score alone. It suggests that confidence in this form does not add further value to bounding box stability, which is somehow consistent with the findings in the previous paragraph. Performance even deteriorates when we use other measurements such as Entropy, FD, and ATC. While current measures based on confidence are not complementary, we believe it is an intriguing direction for further study.\nEstimating mAP50 and mAP75 without ground truths. This paper mainly predicts the mAP score, which averages over 10 IoU thresholds, i.e., 0.50, 0.55, ..., 0.95. Here, we further predict mAP50 and mAP75, which use a certain IoU threshold of 0.50 and 0.75, respectively. In Table 4 (right), we observe that BoS score is also very useful in predicting mAP50 and mAP75, yielding the mean RMSE of 4.84% and 3.47%, respectively, which are consistently lower than the compared methods."
        },
        {
            "heading": "5.3 FURTHER ANALYSIS",
            "text": "Impact of the stochastic nature of dropout. Because dropout disables some elements in the feature maps randomly, in main experiments such as Table 1, Table 2 and Table 4, we use multiple runs of dropout, train multiple regressors and report the mean and standard deviation. We observe that the\nstandard deviations are relatively small, indicating that the randomness caused by dropout does not have a noticeable impact on the system.\nImpact of test set size. We assume access to an unlabeled test set, so that a dataset-level BoS score can be computed. In the real world, we might access limited test samples, so we evaluate our system under varying test set sizes. Results are provided in Fig. 4(c). Our system gives relatively low RMSE (\u223c2.54%) when the number of test images is more than 50.\nHyperparameters selection: dropout position p \u2208 {[0], [1], [2], ..., [0, 1, 2, 3]} and dropout rate \u03f5 \u2208 [0, 1]. For the former, there are 15 possible permutations for inserting dropout layer in 4 backbone stages. \u201c[0, 1]\u201d means adding a dropout after stage 0 and stage 1 of the backbone respectively, and so on. We search the optimal configuration on the training meta set in a greedy manner, where the interval for \u03f5 is 0.05 and permutations for dropout position are evaluated one by one. We use the configuration which results in the highest determination coefficient R2 between mAP and BoS score during training. Noting that hyperparameters are selected in training (see Section 4.1 for training meta-set details). Only for the purpose visual-\nization convenience, we show how dropout rate \u03f5 and position p affect the training coefficient of determination in Fig. 3(a) and (b), respectively. When visualizing the impact of one hyperparameter, we fix the other at optimal value found in training. For the RetinaNet+R50 vehicle detector, the best dropout rate \u03f5 is 0.15, and the best dropout position setting is adding a dropout layer after Stage 1 and Stage 2 (p = [1, 2]) of the backbone, respectively.\nImpact of meta-set size and sample set size. The meta-set size is defined by the number of sample sets it contains, and the size of sample set is defined by the number of images it has. We test various meta-set and sample set sizes and report the results in Fig. 4(a) and (b), respectively. We observe that a larger meta-set size or sample set size leads to improved system performance measured by RMSE. Our results are consistent with (Deng & Zheng, 2021)."
        },
        {
            "heading": "5.4 STRONG CORRELATION",
            "text": "One of our key contributions is the discovery of this strong relationship between mAP and BoS score. To study the relationship between mAP and BoS score on various test sets, we first train a detector, where we adopt RetinaNet+R50 (Lin et al., 2017) trained on the COCO (Lin et al., 2014).\nSecond, we construct a meta set. The meta-set is a collection of sample sets. To construct the meta-set, we collect various real-world datasets that all contain the vehicle categories as seed sets, including BDD (Yu et al., 2020), Cityscapes (Cordts et al., 2016), Kitti (Geiger et al., 2013) and so on (all datasets are listed in Fig. 2). To increase the number of datasets, we follow prior work (Deng & Zheng, 2021) to synthesize sample sets using various geometric and photometric transforms. Specifically, we sample 250 images randomly for each seed set, and randomly select three transformations from {Sharpness, Equalize, ColorTemperature, Solarize, Autocontrast, Brightness, Rotate}. Later, we apply these three transformations with random magnitudes on the selected images. This practice generates 50 sample sets from each source, and the sample sets inherit labels from the seeds.\nThen, we simply compute the mAP scores and BoS scores (Eq. 4) of the detector on these test sets. Finally, we plot these mAP scores against the BoS scores in Fig. 2. We clearly observe from Fig. 2(c) that BoS score has a strong correlation with mAP, evidenced by the high R2 score and Spearman\u2019s Rank Correlation \u03c1. It means that, for a given detector, its detection accuracy would be high in environments where the predicted bounding boxes are resistant to feature dropout; Similarly, the mAP score would be low in test sets where its bounding box positions are sensitive to model perturbation. If we think about this from the perspective of test difficulty for a given detector, results suggest that the test environment is more difficult if detector outputs are less robust model perturbation, and vice versa.\nWe then compare the correlation produced by BoS score with prediction score (Hendrycks & Gimpel, 2016) and average thresholded confidence (Garg et al., 2022). We observe that the correlation strength of our method is much stronger than the competing ones. Apart from the study in Fig. 2, more correlation results, including other types of detectors, other detection metrics like mAP50, and other canonical detection tasks like pedestrian detection, are provided in the supplementary materials, where our method gives consistently strong correlations."
        },
        {
            "heading": "5.5 DISCUSSIONS AND LIMITATIONS",
            "text": "Is BoS score a kind of confidence/uncertainty measurement in object detection? It characterizes confidence on the dataset level, but might not operate on the level of individual bounding boxes/images. In our preliminary experiment, we tried to correlate BoS score with accuracy on individual images and even bounding boxes, but the correlation was a little weak. We find BoS score is correlated with accuracy when the number of images considered is statistically sufficient. Please see supplementary materials for more details.\nUsing MC dropout, can we measure the change of bounding box confidence? Yes. By bipartite matching, it is technically possible to compute the confidence difference between corresponding bounding boxes. But we find that this confidence stability score (CS score) does not correlate well with accuracy. Please refer to the bottom two pillars in Fig. 3(c) for this comparison.\nCan BoS score be a loss function for object detection during training? No. There are two reasons. (1) the BoS-mAP correlation might not hold when a model just starts training, whose performance on a test set is extremely poor. (2) BoS score is not differentiable, since its calculation involves the perturbed detection results which are constantly changing during training. However, we might improve detector performance from the insights of our paper. For example, we can apply a resistance module like MC-DropBlock (Deepshikha et al., 2021) to improve detector generalization.\nCan other self-supervised signals exhibit a good correlation with mAP? This work uses the pretext metric, box stability score, for label-free model evaluation, because it well correlates with mAP. There exist other existing pretext tasks for unsupervised object detection pre-training (Xiao et al., 2021; Dai et al., 2021; Bar et al., 2022). It is interesting to study in the future whether these methods also relate to mAP in target environments.\nLlimitations on the working scope of BoS score. Strong correlation is observed when the detector has reasonably good performance (e.g., mAP > 15%) on the sample sets. If a test set is extremely difficult on which mAP could be as low as 1-2%, the correlation might not hold. The reason is that when mAP is extremely low, there are too many missed detections. Without bounding boxes, BoS score cannot be computed. In fact, a similar observation is made in image classification: existing confidence-based indicators (Saito et al., 2019; Deng & Zheng, 2021; Garg et al., 2022) are less correlated with accuracy where classification accuracy is very low. Moreover, we might encounter images from unseen classes in the open world. To improve the mAP estimation performance under this scenario, a feasible solution is using out-of-distribution detection techniques to detect and reject such novel images. In addition, the effectiveness of the BoS score in predicting the mAP for multi-class detectors requires further investigation. This requires a large scale collection of object detection datasets across diverse appearances and the same label space. However, real-world datasets meeting these criteria are scarce, which limits us from exploring more findings."
        },
        {
            "heading": "6 CONCLUSION AND FUTURE WORK",
            "text": "In this paper, we report a very interesting finding: given a trained detector, its bounding box stability when feature maps undergo dropout positively correlates with its accuracy, measured on various test sets. The stability is measured by BoS score, computed as the intersection over union between corresponding bounding boxes found by bipartite matching. Because computing the BoS score does not require test labels, this finding allows us to predict detection mAP on unlabeled test sets. We perform extensive experiments to verify this capability and show that the mAP evaluator based on BoS score yields very competitive results. However, there is still a lack of explanation patterns of failure cases. To better understand this, we need to collect a large scale collection of object detection datasets specifically designed for the mAP estimation task and analyze the patterns of failure cases. Part of our future work is to collect such datasets to share with the community."
        },
        {
            "heading": "A EXPERIMENTAL SETUP",
            "text": "A.1 MODELS TO BE EVALUATED\nWe consider both one-stage and two-stage detection models with different backbones, including RetinaNet+R50 (Lin et al., 2017; He et al., 2016), RetinaNet+Swin (Liu et al., 2021), FasterRCNN+R50 (Ren et al., 2015) and FasterRCNN+Swin. We follow common practices (Liu et al., 2021) to initialize the backbone with pre-trained classification weights, and train models using a 3\u00d7 (36 epochs) schedule by default. The models are typically trained with stochastic gradient descent (SGD) optimizer with a learning rate of 10\u22123. Weight decay of 10\u22124 and momentum of 0.9 are used. We use synchronized SGD over 4 GPUs with a total of 8 images per minibatch (2 images per GPU). During the training process, we resize the input images such that the shortest side is at most 800 pixels while the longest is at most 1333 (Chen et al., 2019). We use horizontal image flipping as the only form of data augmentation. During the testing process, we resize the input images to a fixed size of 800\u00d71333.\nA.2 DATASETS\nThe datasets we use are publicly available and we have double-checked their license. Their open-source is listed as follows. For vehicle detection, we use 10 datasets, including COCO (Lin et al., 2014): https://cocodataset.org/#home; BDD (Yu et al., 2020): https://bdd-data.berkeley.edu/; Cityscapes (Cordts et al., 2015): https://www.cityscapes-dataset.com/; DETRAC (Wen et al., 2020): https://detrac-db.rit.albany.edu/; Exdark (Loh & Chan, 2019): https://github.com/cs-chan/ Exclusively-Dark-Image-Dataset; Kitti (Geiger et al., 2013): https://www.cvlibs.net/datasets/kitti/); Self-driving (Kaggle, 2020a): https://www.kaggle.com/datasets/ owaiskhan9654/car-person-v2-roboflow; Udacity (Kaggle, 2021): https://www.kaggle.com/datasets/alincijov/ self-driving-cars; Roboflow (Kaggle, 2022): https://www.kaggle.com/datasets/sshikamaru/ udacity-self-driving-car-dataset; Traffic (Kaggle, 2020b): https://www.kaggle.com/datasets/saumyapatel/ traffic-vehicles-object-detection. For pedestrian detection, we use 9 datasets, including COCO (Lin et al., 2014): https://cocodataset.org/#home; Caltech (Griffin et al., 2007): https://data.caltech.edu/records/f6rph-90m20; Crowdhuamn (Shao et al., 2018): https://www.crowdhuman.org/; Cityscapes (Cordts et al., 2015): https://www.cityscapes-dataset.com/; Self-driving (Kaggle, 2020a): https://www.kaggle.com/datasets/alincijov/ self-driving-cars; Exdark (Loh & Chan, 2019) : https://github.com/cs-chan/\nExclusively-Dark-Image-Dataset; EuroCity (Braun et al., 2019): https://eurocity-dataset.tudelft.nl/; Kitti (Geiger et al., 2013): https://www.cvlibs.net/datasets/kitti/; CityPersons (Zhang et al., 2017): https://www.cityscapes-dataset.com/ downloads/.\nA.3 COMPUTATIONAL RESOURCES\nWe conduct detector training experiments on four A100, and detector testing experiments on one A100, with PyTorch 1.9.0 and CUDA 11.1. The CPU is Intel(R) Xeon(R) Gold 6248R 29- Core Processor.\nA.4 HYPER-PARAMETER EXPLORATION FOR BASELINE METHODS.\nWe empirically find that different measurements have their own optimal thresholds. According to the effect of thresholds in terms of correlation strength (R2) in Fig. 6, we use thresholds of 0.4, 0.95, and 0.3 for ATC (Garg et al., 2022), PS (Hendrycks & Gimpel, 2016), and ES (Saito\net al., 2019), respectively. We have two observations. First, ATC and ES tend to choose centering thresholds as their optimal thresholds, while ES is particularly sensitive to the setting of thresholds. Second, when using extremely high threshold temperature values, PS achieves its own strongest correlation with mAP."
        },
        {
            "heading": "B DISCUSSION",
            "text": "Is BoS score a kind of confidence/uncertainty measurement in object detection? It characterizes confidence on the dataset level, but might not operate on the level of individual bounding\nboxes/images. As shown in Fig. 8(a), we tried to correlate box stability score with ground truth IoU (GT IoU) on individual bounding boxes. Compared to confidence score (coefficients of determination R2 > 0.49), BS achieves a stronger correlation (coefficients of determination R2 > 0.72) with GT IoU, but the correlation was still weak. We find BoS score is correlated with accuracy when the\nnumber of images considered is statistically sufficient (coefficients of determination R2 > 0.93). In Fig. 5(a)(b), we give some visual examples of where BoS score can reflect whether the dataset is hard for the detector. Under feature perturbation, bounding boxes remaining stable (with high BoS score) would indicate a relatively good prediction, while bounding boxes changing significantly (with low BoS score) may suggest poor predictions."
        },
        {
            "heading": "C MORE CORRELATION RESULTS",
            "text": "C.1 CORRELATION RESULTS FOR OTHER METRICS\nIn Fig. 7, we additionally show the correlation between different measurements and different detection metrics. We observe that BoS score also has a strong correlation with mAP50 and mAP75, yielding the other three measurements.\nC.2 CORRELATION RESULTS ON DIFFERENT DETECTORS\nIn this section, we additionally report the correlation results using other detectors: RetinaNet+Swin, FasterRCNN+R50, and FasterRCNN+Swin. As shown in Fig. 9, there is a very strong correlation between BoS score and mAP on FasterRCNN+Swin (R2 = 0.830 and \u03c1 = 0.783). But on RetinaNet+Swin and FasterRCNN+R50, the correlation is somehow weak, and we will try to improve on this in future work.\nC.3 CORRELATION RESULTS ON PEDESTRIAN DETECTION\nIn this section, we additionally report the correlation results on Pedestrian Detection. As shown in Fig. 10, the correlation between BoS score and mAP is significantly strong than the other methods when the mAP is greater than 15%. When mAP is less than 15%, the ATC method is slightly better among these four measures, but the sample sets are clustered together and hard to distinguish from each other."
        }
    ],
    "year": 2023
}