{
    "abstractText": "Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction (S), teacher prediction (T ), and ground truth (G). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher\u2019s global average prediction (T\u0304 ) for samples within the same class. A simple neural network then learns the implicit mapping from the intraand inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chengming Hu"
        },
        {
            "affiliations": [],
            "name": "Haolun Wu"
        },
        {
            "affiliations": [],
            "name": "Xuan Li"
        },
        {
            "affiliations": [],
            "name": "Chen Ma"
        },
        {
            "affiliations": [],
            "name": "Xi Chen"
        },
        {
            "affiliations": [],
            "name": "Jun Yan"
        },
        {
            "affiliations": [],
            "name": "Boyu Wang"
        },
        {
            "affiliations": [],
            "name": "Xue Liu"
        }
    ],
    "id": "SP:9d3536550d0110bbf125eb04c0244085de03d237",
    "references": [
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Urvashi Khandelwal",
                "Christopher D. Manning",
                "Quoc V. Le"
            ],
            "title": "BAM! born-again multi-task networks for natural language understanding",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Luca Franceschi",
                "Paolo Frasconi",
                "Saverio Salzo",
                "Riccardo Grazzi",
                "Massimiliano Pontil"
            ],
            "title": "Bilevel programming for hyperparameter optimization and meta-learning",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "I.J. Good"
            ],
            "title": "Rational decisions. Journal of the Royal Statistical Society",
            "venue": "Series B (Methodological),",
            "year": 1952
        },
        {
            "authors": [
                "Huifeng Guo",
                "Ruiming Tang",
                "Yunming Ye",
                "Zhenguo Li",
                "Xiuqiang He"
            ],
            "title": "Deepfm: A factorizationmachine based neural network for CTR prediction",
            "venue": "In Proceedings of the 26th International Joint Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Byeongho Heo",
                "Minsik Lee",
                "Sangdoo Yun",
                "Jin Young Choi"
            ],
            "title": "Knowledge transfer via distillation of activation boundaries formed by hidden neurons",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Chengming Hu",
                "Xuan Li",
                "Dan Liu",
                "Haolun Wu",
                "Xi Chen",
                "Ju Wang",
                "Xue Liu"
            ],
            "title": "Teacher-student architecture for knowledge distillation: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Tao Huang",
                "Shan You",
                "Fei Wang",
                "Chen Qian",
                "Chang Xu"
            ],
            "title": "Knowledge distillation from a stronger teacher",
            "venue": "arXiv preprint arXiv:2205.10536,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "James M. Joyce"
            ],
            "title": "Kullback-Leibler Divergence, pages 720\u2013722",
            "year": 2011
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Carlos Lassance",
                "Myriam Bontonou",
                "Ghouthi Boukli Hacene",
                "Vincent Gripon",
                "Jian Tang",
                "Antonio Ortega"
            ],
            "title": "Deep geometric knowledge distillation with graphs",
            "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Kimin Lee",
                "Kibok Lee",
                "Honglak Lee",
                "Jinwoo Shin"
            ],
            "title": "A simple unified framework for detecting out-of-distribution samples and adversarial attacks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Shiyu Liang",
                "Yixuan Li",
                "R Srikant"
            ],
            "title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Peng Lu",
                "Abbas Ghaddar",
                "Ahmad Rashid",
                "Mehdi Rezagholizadeh",
                "Ali Ghodsi",
                "Philippe Langlais"
            ],
            "title": "RW-KD: Sample-wise loss terms re-weighting for knowledge distillation",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP,",
            "year": 2021
        },
        {
            "authors": [
                "Michal Lukasik",
                "Srinadh Bhojanapalli",
                "Aditya Krishna Menon",
                "Sanjiv Kumar"
            ],
            "title": "Teacher\u2019s pet: understanding and mitigating biases in distillation",
            "venue": "arXiv preprint arXiv:2106.10494,",
            "year": 2021
        },
        {
            "authors": [
                "Fuyuan Lyu",
                "Xing Tang",
                "Huifeng Guo",
                "Ruiming Tang",
                "Xiuqiang He",
                "Rui Zhang",
                "Xue Liu"
            ],
            "title": "Memorize, factorize, or be naive: Learning optimal feature interaction methods for CTR prediction",
            "venue": "In Proceedings of 38th IEEE International Conference on Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Shengyi Pan",
                "Thomas Morris",
                "Uttam Adhikari"
            ],
            "title": "Developing a hybrid intrusion detection system using data mining for power systems",
            "venue": "IEEE Transactions on Smart Grid,",
            "year": 2015
        },
        {
            "authors": [
                "Wonpyo Park",
                "Dongju Kim",
                "Yan Lu",
                "Minsu Cho"
            ],
            "title": "Relational knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Xingchao Peng",
                "Qinxun Bai",
                "Xide Xia",
                "Zijun Huang",
                "Kate Saenko",
                "Bo Wang"
            ],
            "title": "Moment matching for multi-source domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Adriana Romero",
                "Nicolas Ballas",
                "Samira Ebrahimi Kahou",
                "Antoine Chassang",
                "Carlo Gatta",
                "Yoshua Bengio"
            ],
            "title": "Fitnets: Hints for thin deep nets",
            "venue": "arXiv preprint arXiv:1412.6550,",
            "year": 2014
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive representation distillation",
            "venue": "In Proceedings of the 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Apoorv Vyas",
                "Nataraj Jammalamadaka",
                "Xia Zhu",
                "Dipankar Das",
                "Bharat Kaul",
                "Theodore L. Willke"
            ],
            "title": "Out-of-distribution detection using an ensemble of self supervised leave-out classifiers",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yu Wang",
                "Jingjing Zou",
                "Jingyang Lin",
                "Qing Ling",
                "Yingwei Pan",
                "Ting Yao",
                "Tao Mei"
            ],
            "title": "Out-ofdistribution detection via conditional kernel independence model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "\u00c0gata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Helong Zhou",
                "Liangchen Song",
                "Jiajie Chen",
                "Ye Zhou",
                "Guoli Wang",
                "Junsong Yuan",
                "Qian Zhang"
            ],
            "title": "Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective",
            "venue": "arXiv preprint arXiv:2102.00650,",
            "year": 2021
        },
        {
            "authors": [
                "scenes"
            ],
            "title": "There are roughly 1.2 million training images, 50K validation images, and 150K testing images. HIL. The open-source benchmark dataset is generated from the HIL security testbed (Pan et al., 2015), the representative transmission network with rich operations in a realistic setting, which is widely used to develop proofs-of-concept for attack detection in industrial control systems",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Knowledge distillation (KD) (Hinton et al., 2015) is a widely used machine learning technique that aims to transfer the informative knowledge from a cumbersome model (i.e., teacher) to a lightweight model (i.e., student). The student is trained by both imitating the teacher\u2019s behavior and minimizing the difference between its own predictions and the ground truths. This is achieved by optimizing a convex combination of two losses: L = \u03b1LKD + (1 \u2212 \u03b1)LGT, where \u03b1 \u2208 [0, 1] is the knowledge fusion ratio balancing the trade-off between the two different supervision signals.\nDetermining the knowledge fusion ratio \u03b1 is critical for training. The most straightforward method is to pre-set an identical value for all training samples (Hinton et al., 2015; Huang et al., 2022; Clark et al., 2019; Romero et al., 2014; Park et al., 2019; Lassance et al., 2020). Other works, such as ANL-KD (Clark et al., 2019) and FitNet (Romero et al., 2014), gradually decrease \u03b1 from 1 to 0 through an annealing factor. Recent studies (Lukasik et al., 2021; Zhou et al., 2021; Lu et al., 2021) imply that a uniform knowledge fusion ratio across all samples is sub-optimal and cannot well capture the nuanced dynamics of the knowledge transfer process, thus designing the knowledge fusion ratio in a more fine-grained manner. For instance, ADA-KD (Lukasik et al., 2021) assigns a higher \u03b1 to a class if the teacher has a higher correctness on that class. WLS-KD (Zhou et al., 2021) takes\n\u2217Equal contribution with random order. \u2020To whom the correspondence should be addressed. Our source code can be found here.\nboth the teacher\u2019s and student\u2019s correctness into consideration, and \u03b1 is increased if the teacher outperforms the student on a sample, otherwise decreased. RW-KD (Lu et al., 2021) analyzes the same information as WLS-KD yet employs a meta-learning method to learn the sample-wise \u03b1.\nHowever, existing methods largely ignore the discrepancy between the student\u2019s prediction (S) and the teacher\u2019s prediction (T ), denoted as ST , when determining \u03b1. We argue that this oversight is significant, as making the student imitate the teacher lies at the heart of KD; thus intuitively, the ST discrepancy should offer valuable insights into balancing the two supervisory signals. Empirical results on CIFAR-100 (Krizhevsky, 2009) further verify our argument. The details of the motivation experiment are demonstrated at the end of this section. Derived from our observations, we draw the following insights:\n\u2022 If the teacher predicts correctly, a higher ST discrepancy indicates the higher learning potential from the teacher, favoring a larger \u03b1. A lower discrepancy indicates less potential to learn from the teacher and value in using the ground truth, thus a smaller \u03b1 is preferred.\n\u2022 If the teacher predicts incorrectly, knowledge from the teacher is misleading, and thus a smaller \u03b1 is advisable.\n\u2022 Regardless of the situation, determining a proper sample-wise value \u03b1 relies on not only the teacher\u2019s or student\u2019s performances but also the value of ST .\nConsequently, our findings suggest that the ST discrepancy offers valuable insights for determining the knowledge fusion ratio \u03b1. In light of the emphasized importance of student-ground truth (SG) and teacher-ground truth (T G) relations in existing studies (Zhou et al., 2021; Lu et al., 2021), we propose TGeo-KD, which captures all three relations aforementioned and naturally leads to model the intra-sample Trilateral Geometry among the signals from the student (S), teacher (T ), and ground truth (G). To enhance the model stability against outliers, we further incorporate the teacher\u2019s global average prediction for a given class as an additional reference, abbreviated as T\u0304 , enriching the geometric relations at both intra- and inter-sample levels. Based on the insights from the motivation experiment, we also argue that learning the sample-wise \u03b1 is quite involved and cannot be achieved by merely heuristic rules. To this end, we propose to learn the fusion ratio by a neural network (NN) and formulate KD as a bilevel objective that leverages the trilateral geometric information. As a result, the student is influenced by a tailored blend of knowledge from both the teacher and the ground truth. Our proposed TGeo-KD, an end-to-end solution, is versatile and proves superior to other re-weighting methods in various tasks, from image classification to click-through rate prediction. To summarize, the main contributions of our work are as follows:\n\u2022 We introduce TGeo-KD, a novel method for learning sample-wise knowledge fusion ratios in KD. Leveraging the trilateral geometry, our method encapsulates the geometric relations among the signals from the student, teacher, and ground truth.\n\u2022 We exploit the trilateral geometry at both intra-sample and inter-sample levels, mitigating the impact of outliers in training samples towards a more effective knowledge fusion.\n\u2022 We conduct comprehensive experiments across diverse domains to demonstrate the consistent superiority over other loss re-weighting methods, as well as to highlight its versatility and adaptability across different architectures and model sizes.\nDetails of Motivation Experiment. We conduct our motivation experiment on CIFAR100 (Krizhevsky, 2009). Specifically, we partition the dataset into two subsets, D and D\u2032, where D consists of samples on which the pre-trained teacher (ResNet-34) has correct predictions, whereas D\u2032 includes those with incorrect predictions. Initially, with \u03b1 = 0.5, we train a student model (ResNet18) over 50 epochs to imbibe preliminary knowledge. We then compute the Euclidean distance between the student\u2019s and teacher\u2019s predicted class probabilities across all samples, designating this as the ST discrepancy. Based on ascending ST values, we further split D and D\u2032 into five equalized groups g1 \u223c g5 and g\u20321 \u223c g\u20325, respectively. Subsequently, the student is further trained with varying \u03b1 values adjusted from {0.1, 0.3, 0.5, 0.7, 0.9}, yielding five distinct student models. Upon evaluating these students across all five g groups and five g\u2032 groups, we obtain 25 bins for each subfigure in Fig. 1, which reveals that: (i) For samples in D, students trained with smaller \u03b1 values (i.e., 0.1, 0.3) outperformed on groups with lower ST discrepancies (i.e., g1, g2), whereas larger \u03b1 values (i.e., 0.7, 0.9) were beneficial for groups with higher discrepancies (i.e., g4, g5). (ii) For samples in D\u2032, a smaller \u03b1 (i.e., 0.1, 0.3) demonstrated the best performance on all g\u2032 groups. Our observation shows a proper sample-wise value \u03b1 relies on not only the student\u2019s or teacher\u2019s performances but also their discrepancy, which motivates the design of our proposed method for knowledge fusion learning."
        },
        {
            "heading": "2 PRELIMINARY: REVISITING KNOWLEDGE FUSION RATIO IN KD",
            "text": "The vanilla KD (Hinton et al., 2015) transfers knowledge from a pre-trained teacher network to a student by reducing discrepancies between their predictions and aligning with the ground truth. The student learns through two losses: LKD, the Kullback\u2013Leibler (KL) divergence (Joyce, 2011) between student and teacher predictions, and LGT, the Cross-Entropy (CE) loss (Good, 1952) from the ground truth. Formally, denoting D = {(xi,yi)}Ni=1 as the data where yi is the ground truth label represented as a one-hot vector for each sample, C as the number of classes, zsi and z t i as the logits of the student and teacher, we formulate the two losses in a sample-wise manner as follows:\nLKDi = \u03c42KL(zsi , zti) = \u03c42 C\u2211\nj=1\n\u03c3j(z t i/\u03c4) log\n\u03c3j(z t i/\u03c4) \u03c3j(zsi/\u03c4) , (1)\nLGTi = CE(zsi ,yi) = \u2212 C\u2211\nj=1\nyi,j log ( \u03c3j(z s i ) ) , (2)\nwhere \u03c3 is the softmax function and the temperature \u03c4 controls the softness of logits. Then the overall training objective aims to optimize the student network (parameterized by \u03b8) through a convex combination of the two losses with a sample-wise fusion ratio \u03b1i:\nL = min \u03b8\n1\nN N\u2211 i=1 \u03b1iLKDi + (1\u2212 \u03b1i)LGTi . (3)\nWe present a comparison of prior knowledge fusion methods alongside our work in Fig. 2, emphasizing both the geometric relations captured for learning \u03b1 and their distinctive model attributes. Evidently, our proposed TGeo-KD overcomes the constraints observed in previous approaches, thus leading to enhanced performance."
        },
        {
            "heading": "3 TRILATERAL GEOMETRY GUIDED KNOWLEDGE FUSION",
            "text": ""
        },
        {
            "heading": "3.1 ADAPTIVE LEARNING FOR KNOWLEDGE FUSION RATIO",
            "text": "To address the limitation of prior works and employing the insights from the motivation experiment as depicted in Sec. 1, we propose to adaptively learn the knowledge fusion ratio based on trilateral geometry within (S, T , G) triplet using a separate network. For simplifying the notation, we consistently denote S := \u03c3(zs) \u2208 RN\u00d7C and T := \u03c3(zt) \u2208 RN\u00d7C as the prediction probabilities of the student and teacher, and G := y \u2208 RN\u00d7C as the ground truth (i.e., each row is an one-hot vector). Given a training sample (xi,yi), the knowledge fusion ratio can be correspondingly modeled as \u03b1i = f\u03c9(\u2206i), where f\u03c9 is one NN parameterized by \u03c9. The final layer of f\u03c9 employs a sigmoid activation, ensuring that \u03b1i \u2208 (0, 1). For brevity, we omit explicitly writing the sigmoid function. \u2206i represents the unique geometric relation among Si, Ti, and Gi. Our ultimate goal is to find the optimal sample-wise ratios \u03b1i = f\u03c9(\u2206i) that enable the student network parameterized by \u03b8 to generalize well on test data. This naturally implies a bilevel optimization problem (Franceschi et al., 2018) with \u03c9 as the outer level variable and \u03b8 as the inner loop variable:\nmin \u03c9\nJ outerval (\u03b8\u2217(\u03c9)) = 1\nNval Nval\u2211 i=1 LGTi , (4)\ns.t. \u03b8\u2217(\u03c9) = argmin \u03b8\nJ innertrain (\u03b8, \u03c9) := 1\nNtrain Ntrain\u2211 i=1 f\u03c9(\u2206i)LKDi + ( 1\u2212 f\u03c9(\u2206i) ) LGTi . (5)\nOn the inner level, we aim to train a student network given a fixed \u03c9 by minimizing the combined loss. On the outer level, the loss function on the validation set serves as a proxy for the generalization error of \u03c9. The goal for TGeo-KD is to find \u03c9 to minimize the validation loss. Note that Eq. 4 is an implicit function of \u03c9 as \u03b8\u2217 depends on \u03c9."
        },
        {
            "heading": "3.2 EXPLOITING TRILATERAL GEOMETRY",
            "text": "For modeling the trilateral geometry of \u2206i, we propose to capture both intra-sample and inter-sample geometric relations. The details are demonstrated as follows.\nIntra-sample relations. Given the ith sample, to capture the trilateral geometry of the (Si, Ti, Gi) triplet, denoting as \u2206ST Gi , we capture its three edges as outlined below:\nesgi := [Gi \u2212 Si] \u2208 R C , etgi := [Gi \u2212 Ti] \u2208 R C , esti := [Ti \u2212 Si] \u2208 RC . (6)\nThe three edges represent the student\u2019s correctness, the teacher\u2019s correctness, and the discrepancy between the student and teacher, respectively. Previous research (Zhou et al., 2021; Lu et al., 2021) has affirmed the efficacy of the first two edges in guiding the learning of \u03b1, while the third concept is our original contribution. We finally represent \u2206ST Gi by also incorporating the exact three vertices Si, Ti, Gi, to capture the exact probability across all classes for incorporating more information:\n\u2206ST Gi := [e sg i \u2295 e tg i \u2295 e st i \u2295 Si \u2295 Ti \u2295 Gi], (7)\nwhere \u2295 is the concatenation operation.\nInter-sample relations. In addition to intra-sample relations, we argue that inter-sample relations are also essential for knowledge fusion learning, especially considering the impact of outliers in training samples. Out-of-distribution samples, which are significantly different from normal training data, commonly behave as outliers to challenge the generalization capability of a model (Lee et al., 2018; Wang et al., 2022). In KD, the teacher network may perform poorly on these outliers, occasionally even with high absolute values of confidence margin. Therefore, blindly using the teacher\u2019s prediction as the supervisory signal can result in the propagation of misleading knowledge, thereby disturbing the student\u2019s training process.\nTo address this issue, we introduce inter-sample geometric relations. For each sample, we associate it with an additional vertex T\u0304ci \u2208 RC , representing the teacher\u2019s global average prediction for all\nsamples of the class (ci) that sample i belongs to. It is essential to understand that while each sample is linked to its respective class-specific vertex, samples within the same class refer to the same vertex T\u0304ci . Consequently, we incorporate an additional triplet, (Si, T\u0304ci , Gi), to encapsulate these inter-sample relations. This is achieved by a similar process as before, focusing on the three edges, as well as incorporating all the vertices as follows:\n\u2206ST\u0304 Gi := [e sg i \u2295 e t\u0304g i \u2295 e st\u0304 i \u2295 Si \u2295 T\u0304ci \u2295 Gi]. (8)\nAs such, by introducing the teacher\u2019s average prediction at the inter-sample level, the exploitation of more supportive knowledge can be further facilitated to effectively guide the student training process, particularly in addressing outliers.\nImproved distillation with trilateral geometry. Although we can fully explore the sample-wise trilateral geometry through intra- and inter-sample trilateral relations, it is still challenging to design an explicit formulation between these signals and a knowledge fusion ratio as depicted in Sec. 1. We thus use a simple network f\u03c9(\u00b7) parameterized by \u03c9, to adaptively learn a flexible and sample-wise knowledge fusion ratio with the input of geometric relations. The information captured for each sample can be represented as follows:\n\u2206i := \u2206 ST G i \u2295\u2206 ST\u0304 G i , (9)\n:= [esgi \u2295 e tg i \u2295 e st i \u2295 e t\u0304g i \u2295 e st\u0304 i \u2295 Si \u2295 Ti \u2295 T\u0304ci \u2295 Gi], (10)\nwhere the redundant terms are removed for brevity. Through inputting \u2206i into f\u03c9(\u00b7), the knowledge fusion ratio \u03b1i can be adaptively learned, and \u03c9 is optimized with \u03b8 in an end-to-end way."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 TASKS AND EXPERIMENT SETTINGS",
            "text": "Tasks and datasets. To demonstrate the broad applicability of our method, we conduct extensive experiments on three different tasks. Specifically, we use CIFAR-100 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009) for image classification in computer vision, HIL (Pan et al., 2015) for attack detection in cyber-physical systems, and Criteo (Jean-Baptiste Tien, 2014) for click-through rate (CTR) prediction in recommender systems. Details of datasets and task selection are shown in Appendix A.1.\nExperiment settings. In the experiment setup, the temperatures (\u03c4 ) are set as 4.0, 1.5, 1.5, and 2.0 on the four datasets, respectively. In the vanilla KD, the pre-set fusion ratios are 0.2, 0.3, 0.1, and 0.3, respectively. Considering that the original HIL (Pan et al., 2015) and Criteo (Jean-Baptiste Tien, 2014) are imbalanced, we conduct oversampling on the minority class of training set as the data pre-processing procedure, ensuring all classes have the equal number of samples in a balanced setting. We conduct the experiments on one NVIDIA RTX-3080 GPU and one RTX-3090 GPU. The detailed experiment settings can be found in Appendix A.2."
        },
        {
            "heading": "4.2 STUDENT CLASSIFICATION PERFORMANCE",
            "text": "Results on CIFAR-100. We evaluate our proposed TGeo-KD method against numerous established KD baselines, as illustrated in Table 1. To ascertain the significance of improvement, we conduct a statistical t-test across five repeated runs, with t-scores being calculated based on the top-1 classification accuracy of TGeo-KD and the baseline methods. All computed t-scores surpass the threshold value t0.05,5, indicating the acceptance of the alternative hypothesis with a statistical significance level of 5.0%. This furnishes compelling evidence that TGeo-KD consistently demonstrates a marked enhancement in performance.\nNotably, when the teacher (ResNet-56) and student (ResNet-20) models possess relatively similar architectures, ADA-KD (Lukasik et al., 2021) is the best baseline with a marginal improvement of 0.07% over the second-best baseline WLS-KD (Zhou et al., 2021). In comparison to ADAKD (Lukasik et al., 2021), TGeo-KD illustrates a substantial advantage of 0.76%. As the architectural gap increases, like between the student ResNet-32 and the teacher ResNet-110, our method\u2019s performance advantage increases to 0.97%, compared to the best baseline. This performance gain\nfurther escalates to 1.26% when the student ResNet-20 and the teacher ResNet-110, underscoring the advantage of our TGeo-KD particularly when dealing with the increasing architectural disparities between the student and teacher. Moreover, our technique excels in hetero-architecture KD scenarios, wherein knowledge is distilled from either ResNet-32\u00d74 or WRN-40-2 models into ShuffleNet. In these cases, our method consistently demonstrates performance enhancements of 1.37%, 0.96%, and 0.84%, respectively, compared to the strongest baseline WLS-KD (Zhou et al., 2021).\nResults on ImageNet. To further demonstrate the effectiveness of our approach on larger datasets, we extend our experiments to ImageNet, adhering to the setup outlined by Zhou et al. (2021). As depicted in Table 2, TGeo-KD consistently outperforms all competing baselines. Notably, compared with the strongest baseline WLS-KD (Zhou et al., 2021), TGeo-KD exhibits the performance improvement of 1.10% when the teacher (ResNet-34) and student (ResNet-18) share the same architecture style. Similarly, in a hetero-architecture KD experiment with ResNet-50 and MobileNetV1 as teacher and student respectively, our method realizes an improvement of 0.94%.\nResults on HIL and Criteo. To illustrate the broad applicability of our proposed TGeo-KD in diverse application scenarios, we also observe the similar superiority of our method on HIL for attack detection and Criteo for CTR prediction, as shown in Table 3. For instance, TGeo-KD not only relatively improves ACC and NLL over ADA-KD (Lukasik et al., 2021) by 2.48% and 5.05% on HIL, respectively, but also is better than the deeper teacher with the increasing ACC of 4.20%. Besides, WLS-KD (Zhou et al., 2021) and RW-KD (Lu et al., 2021) are the best methods among all the baselines on Criteo, approving the effectiveness of adopting sample-wise knowledge fusion ratio. More results on various network architectures are provided in Appendix A.4."
        },
        {
            "heading": "4.3 FUSION RATIO ANALYSIS WITH PREDICTION DISCREPANCY",
            "text": "To demonstrate the effectiveness of incorporating prediction discrepancy ST between the student and teacher on learning the knowledge fusion ratio \u03b1, we follow the same settings as the motivation experiment in Sec. 1, for a comprehensive analysis. We first categorize training samples based on the correctness of teacher predictions and ST on these samples. We then compare the distributions of fusion ratio learned with and without the consideration of ST , as depicted in Fig. 3. When the teacher predicts incorrectly, it may transfer misleading knowledge to the student, resulting in a decline in the student\u2019s performance. By incorporating ST , our proposed TGeo-KD decreases the fusion ratio on LKD when the discrepancy is either large or small (in Fig. 3(a) and Fig. 3(b)), which suggests that the student is encouraged to acquire more knowledge from the ground truths. In cases where the teacher predicts correctly, the fusion ratio typically ranges from 0.4 and 0.8 when not incorporating ST . The fusion ratio is greater when incorporating ST in situations with a significant discrepancy between the student and teacher (in Fig. 3(c)). This suggests that the student is expected to emulate the teacher more closely, as the teacher possesses a greater potential for offering valuable knowledge. When the discrepancy is smaller, the student is encouraged to rely more on the ground truth, leading to a decline in the fusion ratio (in Fig. 3(d)). On the contrary, as illustrated in Table 4, the fusion ratio steadily increases without the incorporation of ST as the training progresses, yet an insufficient potential can be learned from the teacher."
        },
        {
            "heading": "4.4 ANALYSIS ON NORMAL SAMPLES AND OUTLIERS",
            "text": "Fusion ratio on normal samples and outliers. To conduct analysis on fusion ratios between normal samples and outliers, we first create outliers by adding synthetic Gaussian noise as additional training samples following the setting outlied in the studies (Hendrycks and Gimpel, 2016; Liang et al., 2018; Vyas et al., 2018). In the context of Gaussian noise based outliers, each RGB value for every pixel is sampled from an independent and identically distributed Gaussian distribution with a mean of 0.5 and unit variance, and each pixel value is clipped to the range [0, 1]. As illustrated in Fig. 4, we compare the final fusion ratio distributions between sample-wise based baselines (i.e, WLS-KD (Zhou et al., 2021) and RW-KD (Lu et al., 2021)) and our TGeo-KD on CIFAR-100. Compared to the two baselines, TGeo-KD reports the final fusion ratios for normal samples within the range of 0.4 and 0.6 typically, which indicates that the student can be effectively guided with the valuable supervision signals from both the ground truth and the teacher on these normal samples. Furthermore, the teacher may make incorrect predictions on outliers, which provides misleading knowledge to the student. With the consideration of ST discrepancy and inter-sample relations, TGeo-KD reports the fusion ratios below 0.3 on outliers, which suggests that the student is expected to learn more informative knowledge from the ground truth, resulting in an increased fusion ratio on LGT. More comparisons about the effect of ST discrepancy and inter-sample relations on outliers can be found in Appendix A.4.\nPrediction discrepancy during the training and testing. Fig. 5 shows the prediction discrepancies between the student and teacher on normal samples and outliers during training and testing. Compared to baselines, in Fig. 5(a), our proposed TGeo-KD achieves the smallest discrepancies (i.e., 0.19 during the training and 0.26 during the testing) on normal samples, indicating that the student can effectively mimic the teacher predictions through learning the knowledge distilled from the teacher. Furthermore, in Fig. 5(b), although ADA-KD (Lukasik et al., 2021) and RW-KD (Lu et al., 2021) (i.e, the baselines without inter-sample relations) report fewer discrepancies on outliers during the training, the teacher may report poor performance on these outliers and transfer misleading knowledge to the student. With the power of our inter-sample relations, TGeo-KD surpasses these aforementioned studies during the testing, which indicates that inter-sample relations can protect the student training process from being disrupted by low-quality knowledge from the teacher, especially on those outliers."
        },
        {
            "heading": "4.5 ABLATION STUDIES",
            "text": "Effect of different relations captured. TGeo-KD adaptively learns the knowledge fusion ratio by leveraging both intra- and inter-sample relations. To illustrate the effectiveness of each relation, we conduct experiments to train students under different combinations. As summarized in Table 6, both intra- and inter-sample relations yield better performance than the standalone student and vanilla KD models. Specifically, when we incorporate the ST relation, there is a notable increase in top-1 accuracy from 73.92% to 75.28%. This suggests the importance of accounting for the discrepancies between the student and teacher models. The performance improves to 76.83% when the inter-sample\nrelations are further captured. A deeper dive into how various representations of these intra- and inter-sample relations affect performance can be found in Appendix A.4.\nComparison between attention mechanism and MLP. We assess various options for modeling the knowledge fusion ratio generator, f\u03c9(\u00b7), including attention mechanism as suggested in (Vaswani et al., 2017) and MLPs with different numbers of layers. For CIFAR-100, the teacher and student are ResNet-110 and ResNet-32. For ImageNet, the teacher and student are ResNet-34 and ResNet-18. Based on the results in Table 5, a 2-layer MLP is sufficient in capturing the valuable information embedded in the trilateral geometry, leading to superior performance across two datasets. Furthermore, MLP settings generally demonstrate a higher performance compared to attention mechanisms."
        },
        {
            "heading": "5 RELATED WORKS",
            "text": "Knowledge balance in KD. Knowledge distillation techniques have evolved from uniformly applying fusion ratios across samples (Hinton et al., 2015; Huang et al., 2022; Clark et al., 2019; Romero et al., 2014; Park et al., 2019; Lassance et al., 2020) to more refined strategies. Methods like ANL-KD (Clark et al., 2019) and FitNet (Romero et al., 2014) use annealing factors to adjust fusion ratios. Recent advancements include ADA-KD (Lukasik et al., 2021), which uses class-wise ratios, and WLS-KD (Zhou et al., 2021), which adjusts based on student\u2019s and teacher\u2019s performance. RW-KD (Lu et al., 2021) employs meta-learning for adaptive ratio learning. Yet, existing methods lack a consideration of the comprehensive trilateral relations among the signals from the student, teacher, and ground truth during the knowledge fusion learning.\nSample relations in KD. The exploitation of sample relations in knowledge distillation has been a key focus of numerous studies (Zagoruyko and Komodakis, 2017; Zhou et al., 2016; Park et al., 2019; Heo et al., 2019; Tian et al., 2020). For instance, AT (Zagoruyko and Komodakis, 2017; Hu et al., 2023) introduces attention transfer to transfer the attention maps from the teacher to the student, explicitly capturing the sample-level relation. CAMT (Zhou et al., 2016) extends this idea by incorporating both spatial and channel attention transfer to enhance the student\u2019s performance. Other studies have also emphasized relational reasoning and contrastive representation as key mechanisms for better understanding and improving knowledge distillation (Park et al., 2019; Tian et al., 2020). Despite these advancements in capturing sample relations, none of these studies target on the learning of the knowledge fusion ratio."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose an innovative approach named TGeo-KD for learning sample-wise knowledge fusion ratios during KD, which exploits the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations. Across diverse domains, TGeo-KD outperforms other re-weighting methods consistently. It offers a simple yet adaptable distillation solution that fits various architectures and model sizes, thereby easing deployment complexities and resource limitations associated with deep neural networks. For the broader impact, TGeo-KD is effective across a variety of domains, including image classification, attack detection, and click-through rate prediction. Future research on TGeo-KD will address its focus on inter-sample relations within classes, exploring cross-class relationships to enhance performance."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 TASKS AND DATASETS\nTo demonstrate the effectiveness of our method TGeo-KD, we conduct extensive experiments across three diverse tasks, including image classification, attack detection, and click-through rate (CTR) prediction. The selection of the three tasks is based on their representations of diverse domains with different data natures, underlining the versatility of our proposed method. We use a classic and widely used dataset for each of the above tasks. This section provides the details for each dataset.\nCIFAR-100. The open-source benchmark dataset (Krizhevsky, 2009) is widely used for image classification, which comprises 60,000 color images of 32\u00d732 pixels each. It is categorized into 100 classes, each containing 600 images. Each class is evenly split between the training and test sets, with 500 images for training and 100 for testing. This dataset includes a wide variety of objects, animals, and scenes, making it an ideal resource for developing and testing models and algorithms.\nImageNet. ImageNet (Deng et al., 2009) is a large-scale dataset commonly used for image classification, containing millions of labeled images spanning thousands of categories, such as animals and scenes, etc. There are roughly 1.2 million training images, 50K validation images, and 150K testing images.\nHIL. The open-source benchmark dataset is generated from the HIL security testbed (Pan et al., 2015), the representative transmission network with rich operations in a realistic setting, which is widely used to develop proofs-of-concept for attack detection in industrial control systems. Specifically, this dataset includes 128 multi-sourced features, including 118 synchrophasor measurements and 12 descriptive system logs. The event scenarios can be categorized as three classes: normal operation (class 0), natural fault (class 1), and malicious attack (class 2). Our task is a three-class classification to identify the class of each event scenario.\nAs an imbalanced dataset, the normal and fault samples are randomly oversampled to balance the attack samples in the training set, while the validation and testing sets are not oversampled and remain imbalanced. To avoid dominating features, data normalization is essential to be performed to improve the uniformity of features by adjusting all feature values into the range of [0, 1] (i.e., min-max normalization (Ioffe and Szegedy, 2015)).\nCriteo. Criteo is an online advertising dataset released by Criteo AI Lab . It contains feature values and click feedback for millions of display ads and serves as a benchmark for click-through rate (CTR) prediction. Each ad within the dataset is characterized by 40 attributes, providing a detailed description of the data. The first attribute acts as a label, indicating whether an ad was clicked (1) or not clicked (0), which also automatically forms the two classes in this dataset. The rest attributes consist of 13 integer features and 26 categorical features. To maintain anonymity, all feature values are hashed onto 32 bits. The dataset represents a full month of Criteo\u2019s traffic, offering extensive, real-world insights into online advertising dynamics.\nThe version we used in this paper is a well-known one that used in a competition on CTR prediction jointly hosted by Criteo and Kaggle in 2014 . Following prior works (Guo et al., 2017; Lyu et al., 2022), the continuous features are first normalized into the range of [0, 1]. Due to the imbalance of this dataset, we adopt the same strategy as HIL to conduct random oversampling to balance the two classes in this dataset.\nA.2 EXPERIMENT SETTINGS\nOur method TGeo-KD can remain robust performance on all datasets, using different hyperparameter settings as shown in Table 7. This paper introduces three metrics for evaluating the student\u2019s generalization ability: classification accuracy (ACC), area under the ROC curve (AUC), and negative log-likelihood (NLL). In order to avoid overfitting of the student, we conduct the early stopping\nhttps://ailab.criteo.com/ressources/ https://www.kaggle.com/competitions/criteo-display-ad-challenge/\nstrategy when the ACC does not improve on the validation set within 10 successive iterations. Our computation resources include one NVIDIA RTX-3080 GPU and one RTX-3090 GPU.\nA.3 GENERALIZATION ABILITY\nTo demonstrate the generalization ability of our method on unseen data, we conduct additional experiments in the Out-of-Distribution Detection (OOD) task using DomainNet dataset (Peng et al., 2019). DomainNet (Peng et al., 2019) is a domain adaptation dataset with six domains (including Clipart, Real, Sketch, Infograph, Painting, and Quickdraw) and 0.6 million images distributed across 345 categories. In the experiment setup, we adopt a standard leave-one-domain-out manner, and train our student (teacher: ResNet-50 and student: ResNet-18) on the training set of the source domains and evaluate the trained student on all images of the held-out target domain. As depicted in Table 8, our TGeo-KD can outperform all the relevant KD-based baseline methods.\nA.4 MORE ABLATION STUDIES\nStudent and teacher architectures. In the main paper, we have shown the superiority of TGeo-KD across different architectures and model sizes of teachers and students on CIFAR-100 and ImageNet. Here, we provide more results on the other two datasets. As shown in Table 9, our proposed TGeo-KD consistently demonstrates strong performance across a range of architectural discrepancies between the teacher and student networks.\nSensitivity to temperature \u03c4 . Table 10 compares the top-1 classification accuracy (%) of TGeo-KD using various temperatures (i.e., in the range of [0.5, 5.0]). It can be observed that our proposed TGeo-KD maintains stable performance using different temperatures \u03c4 (i.e., with the variances of 0.69, 0.26, 0.42, and 0.49), which further demonstrates the consistent performance of TGeo-KD across various temperature settings.\nSensitivity to oversampling. We employ oversampling when the dataset is imbalanced, which is a widely accepted practice in the research community, especially for classification tasks. To analyze the impact of oversampling on performance, we compare our TGeo-KD with five other baselines across varying imbalance ratios on the HIL and Criteo datasets. As shown in Table 11, our proposed TGeo-KD consistently outperforms all the baselines (e.g., the improved accuracy of 2.24% and 3.15% over the best baseline ADA-KD (Lukasik et al., 2021) with \u03c1 = 1 and without oversampling on HIL dataset, respectively), which underscores the robustness of our TGeo-KD on imbalanced datasets.\nRepresentations of Trilateral Geometry. To fully exploit intra-sample and inter-sample relations, we consider multiple representations of trilateral geometry as follows:\n\u2022 R1: Si \u2295 Ti \u2295 T\u0304ci \u2295 Gi, \u2022 R2: esgi \u2295 e tg i \u2295 esti \u2295 e t\u0304g i \u2295 est\u0304i ,\n\u2022 R3: esgi \u2295 e tg i \u2295 esti \u2295 e t\u0304g i \u2295 est\u0304i \u2295 Si \u2295 Ti \u2295 T\u0304ci \u2295 Gi,\nwhere the explanations of notations can be found in the main paper. We conduct this ablation experiment on all the datasets. As shown in Table 12, our proposed TGeo-KD can achieve the best performance by capturing both the vertex-wise and edge-wise trilateral geometric information (i.e., R3) at the intra-sample and inter-sample levels.\nEffect of prediction discrepancy and inter-samples relations on outliers. To analyze the impacts of prediction discrepancy (ST ) and inter-sample relations (\u2206ST\u0304 G) on outliers, we compare the Top-1 classification accuracy on outliers of CIFAR-100 when learning \u03b1 based on different relations. As shown in Table 13, our proposed TGeo-KD achieves the best performance on outliers when considering both the discrepancy and inter-sample relations. Furthermore, compared to solely exploiting\nthe discrepancy, inter-sample relations make a better contribution to the improved performance on outliers.\nMoreover, to analyze the impact of ST and \u2206ST\u0304 G on the fusion ratios for incorrectly predicted samples, Table 14 reports the mean and standard deviation of the fusion ratios across these samples when learning fusion ratio with and without the consideration of ST and \u2206ST\u0304 G . When considering both ST and \u2206ST\u0304 G , our TGeo-KD demonstrates the fusion ratios below 0.3 for incorrectly predicted samples. This suggests that the student acquires more knowledge from the ground truth, leading to decreased fusion ratios on LKD. Furthermore, in comparison to solely exploiting \u2206ST\u0304 G when learning fusion ratios, ST contributes more effectively to the decreased fusion ratios across incorrectly predicted samples.\nComputational cost. We compare the computational cost of our proposed TGeo-KD with the baselines including vanilla KD (Hinton et al., 2015), ANL-KD (Clark et al., 2019), ADA-KD (Lukasik et al., 2021), and WLS-KD (Zhou et al., 2021). We run experiments on CIFAR-100, and report the mean training time (in seconds) on a batch of 128 images per iteration. The student network is ResNet-32. The experiment is conducted on one NVIDIA RTX-3090 GPU. As shown in Table 15, our method achieves better training efficiency.\nA.5 DISCUSSION\nLimitation. Although our proposed TGeo-KD is a promising knowledge distillation approach, it could be limited by solely considering inter-sample relations within the same class, thereby neglecting relationships across different classes that could potentially enhance performance. This can be regarded as one future research direction of this work.\nBroader impact. As for the broader impact, with its capability of dynamically learning a samplewise knowledge fusion ratio across various model architectures, TGeo-KD holds the potential to\nsignificantly improve the performance of knowledge distillation across a variety of domains, including image classification, attack detection, and click-through rate prediction."
        }
    ],
    "title": "ERAL GEOMETRY FOR KNOWLEDGE DISTILLATION",
    "year": 2024
}