{
    "abstractText": "Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant challenge that existing approaches struggle to address without extensive modifications to vanilla NeRF framework. We introduce InsertNeRF, a method for INStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play HyperNet modules, InsertNeRF dynamically tailors NeRF\u2019s weights to specific reference scenes, transforming multi-scale sampling-aware features into scene-specific representations. This novel design allows for more accurate and efficient representations of complex appearances and geometries. Experiments show that this method not only achieves superior generalization performance but also provides a flexible pathway for integration with other NeRF-like systems, even in sparse input settings. Code will be available at: https://github.com/bbbbby-99/InsertNeRF.",
    "authors": [
        {
            "affiliations": [],
            "name": "INSTILLING GENERALIZABILITY"
        },
        {
            "affiliations": [],
            "name": "HYPERNET MODULES"
        },
        {
            "affiliations": [],
            "name": "Yanqi Bao"
        },
        {
            "affiliations": [],
            "name": "Tianyu Ding"
        },
        {
            "affiliations": [],
            "name": "Jing Huo"
        },
        {
            "affiliations": [],
            "name": "Wenbin Li"
        },
        {
            "affiliations": [],
            "name": "Yuxin Li"
        },
        {
            "affiliations": [],
            "name": "Yang Gao"
        }
    ],
    "id": "SP:479cf69495e4889651c0470763bc3a309a62b8fa",
    "references": [
        {
            "authors": [
                "Yuval Alaluf",
                "Omer Tov",
                "Ron Mokady",
                "Rinon Gal",
                "Amit Bermano"
            ],
            "title": "Hyperstyle: Stylegan inversion with hypernetworks for real image editing",
            "venue": "In Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yanqi Bao",
                "Yuxin Li",
                "Jing Huo",
                "Tianyu Ding",
                "Xinyue Liang",
                "Wenbin Li",
                "Yang Gao"
            ],
            "title": "Where and how: Mitigating confusion in neural radiance fields from sparse inputs",
            "venue": "arXiv preprint arXiv:2308.02908,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan T Barron",
                "Ben Mildenhall",
                "Matthew Tancik",
                "Peter Hedman",
                "Ricardo Martin-Brualla",
                "Pratul P Srinivasan"
            ],
            "title": "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan T Barron",
                "Ben Mildenhall",
                "Dor Verbin",
                "Pratul P Srinivasan",
                "Peter Hedman"
            ],
            "title": "Mip-nerf 360: Unbounded anti-aliased neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "SC Chan",
                "Heung-Yeung Shum",
                "King-To Ng"
            ],
            "title": "Image-based rendering and synthesis",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2007
        },
        {
            "authors": [
                "Vinod Kumar Chauhan",
                "Jiandong Zhou",
                "Ping Lu",
                "Soheila Molaei",
                "David A Clifton"
            ],
            "title": "A brief review of hypernetworks in deep learning",
            "venue": "arXiv preprint arXiv:2306.06955,",
            "year": 2023
        },
        {
            "authors": [
                "Anpei Chen",
                "Zexiang Xu",
                "Fuqiang Zhao",
                "Xiaoshuai Zhang",
                "Fanbo Xiang",
                "Jingyi Yu",
                "Hao Su"
            ],
            "title": "Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Anpei Chen",
                "Zexiang Xu",
                "Andreas Geiger",
                "Jingyi Yu",
                "Hao Su"
            ],
            "title": "Tensorf: Tensorial radiance fields",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Yunpeng Chen",
                "Marcus Rohrbach",
                "Zhicheng Yan",
                "Yan Shuicheng",
                "Jiashi Feng",
                "Yannis Kalantidis"
            ],
            "title": "Graph-based global reasoning networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Pei-Ze Chiang",
                "Meng-Shiun Tsai",
                "Hung-Yu Tseng",
                "Wei-Sheng Lai",
                "Wei-Chen Chiu"
            ],
            "title": "Stylizing 3d scene via implicit representation and hypernetwork",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Paul E Debevec",
                "Camillo J Taylor",
                "Jitendra Malik"
            ],
            "title": "Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach",
            "venue": "In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques,",
            "year": 1996
        },
        {
            "authors": [
                "Emilien Dupont",
                "Miguel Bautista Martin",
                "Alex Colburn",
                "Aditya Sankar",
                "Josh Susskind",
                "Qi Shan"
            ],
            "title": "Equivariant neural rendering",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kyle Genova",
                "Forrester Cole",
                "Avneesh Sud",
                "Aaron Sarna",
                "Thomas Funkhouser"
            ],
            "title": "Local deep implicit functions for 3d shape",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Xin Huang",
                "Qi Zhang",
                "Ying Feng",
                "Xiaoyu Li",
                "Xuan Wang",
                "Qing Wang"
            ],
            "title": "Local implicit ray function for generalizable radiance field representation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Satoshi Iizuka",
                "Edgar Simo-Serra",
                "Hiroshi Ishikawa"
            ],
            "title": "Globally and locally consistent image completion",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2017
        },
        {
            "authors": [
                "Brian KS Isaac-Medina",
                "Chris G Willcocks",
                "Toby P Breckon"
            ],
            "title": "Exact-nerf: An exploration of a precise volumetric parameterization for neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Nishant Jain",
                "Suryansh Kumar",
                "Luc Van Gool"
            ],
            "title": "Enhanced stable view synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Chiyu Jiang",
                "Avneesh Sud",
                "Ameesh Makadia",
                "Jingwei Huang",
                "Matthias Nie\u00dfner",
                "Thomas Funkhouser"
            ],
            "title": "Local implicit grid representations for 3d scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad Mahdi Johari",
                "Yann Lepoittevin",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Geonerf: Generalizing nerf with geometry priors",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Kania",
                "Artur Kasymov",
                "Maciej Zieba",
                "Przemys\u0142aw Spurek"
            ],
            "title": "Hypernerfgan: Hypernetwork approach to 3d nerf gan",
            "venue": "arXiv preprint arXiv:2301.11631,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Arno Knapitsch",
                "Jaesik Park",
                "Qian-Yi Zhou",
                "Vladlen Koltun"
            ],
            "title": "Tanks and temples: Benchmarking large-scale scene reconstruction",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2017
        },
        {
            "authors": [
                "Johannes Kopf",
                "Fabian Langguth",
                "Daniel Scharstein",
                "Richard Szeliski",
                "Michael Goesele"
            ],
            "title": "Imagebased rendering in the gradient domain",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2013
        },
        {
            "authors": [
                "Jon\u00e1\u0161 Kulh\u00e1nek",
                "Erik Derner",
                "Torsten Sattler",
                "Robert Babu\u0161ka"
            ],
            "title": "Viewformer: Nerf-free neural rendering from few images using transformers",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Yawei Li",
                "Shuhang Gu",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Dhp: Differentiable meta pruning via hypernetworks",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Zhouchen Lin",
                "Heung-Yeung Shum"
            ],
            "title": "A geometric analysis of light field rendering",
            "venue": "International Journal of Computer Vision,",
            "year": 2004
        },
        {
            "authors": [
                "Shichen Liu",
                "Shunsuke Saito",
                "Weikai Chen",
                "Hao Li"
            ],
            "title": "Learning to infer implicit surfaces without 3d supervision",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yuan Liu",
                "Sida Peng",
                "Lingjie Liu",
                "Qianqian Wang",
                "Peng Wang",
                "Christian Theobalt",
                "Xiaowei Zhou",
                "Wenping Wang"
            ],
            "title": "Neural rays for occlusion-aware image-based rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Liu",
                "Yao Feng",
                "Michael J Black",
                "Derek Nowrouzezahrai",
                "Liam Paull",
                "Weiyang Liu"
            ],
            "title": "Meshdiffusion: Score-based generative 3d mesh modeling",
            "venue": "arXiv preprint arXiv:2303.08133,",
            "year": 2023
        },
        {
            "authors": [
                "Lars Mescheder",
                "Michael Oechsle",
                "Michael Niemeyer",
                "Sebastian Nowozin",
                "Andreas Geiger"
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Jonathan T Barron",
                "Ben Mildenhall",
                "Mehdi SM Sajjadi",
                "Andreas Geiger",
                "Noha Radwan"
            ],
            "title": "Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Sida Peng",
                "Yunzhi Yan",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "Representing volumetric videos as dynamic mlp maps",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Songyou Peng",
                "Michael Niemeyer",
                "Lars Mescheder",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "Convolutional occupancy networks",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Ethan Perez",
                "Florian Strub",
                "Harm De Vries",
                "Vincent Dumoulin",
                "Aaron Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988,",
            "year": 2022
        },
        {
            "authors": [
                "Gernot Riegler",
                "Vladlen Koltun"
            ],
            "title": "Free view synthesis",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Sudipta Sinha",
                "Drew Steedly",
                "Rick Szeliski"
            ],
            "title": "Piecewise planar stereo for image-based rendering",
            "venue": "In 2009 International Conference on Computer Vision, pp",
            "year": 2009
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Michael Zollh\u00f6fer",
                "Gordon Wetzstein"
            ],
            "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien Martel",
                "Alexander Bergman",
                "David Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Przemys\u0142aw Spurek",
                "Sebastian Winczowski",
                "Jacek Tabor",
                "Maciej Zamorski",
                "Maciej Zieba",
                "Tomasz Trzci\u0144ski"
            ],
            "title": "Hypernetwork approach to generating point clouds",
            "venue": "arXiv preprint arXiv:2003.00802,",
            "year": 2020
        },
        {
            "authors": [
                "Przemys\u0142aw Spurek",
                "Maciej Zieba",
                "Jacek Tabor",
                "Tomasz Trzci\u0144ski"
            ],
            "title": "Hyperflow: Representing 3d objects as surfaces",
            "venue": "arXiv preprint arXiv:2006.08710,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammed Suhail",
                "Carlos Esteves",
                "Leonid Sigal",
                "Ameesh Makadia"
            ],
            "title": "Generalizable patch-based neural rendering",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Von Oswald",
                "Christian Henning",
                "Benjamin F Grewe",
                "Jo\u00e3o Sacramento"
            ],
            "title": "Continual learning with hypernetworks",
            "venue": "arXiv preprint arXiv:1906.00695,",
            "year": 2019
        },
        {
            "authors": [
                "Chu Wang",
                "Babak Samari",
                "Kaleem Siddiqi"
            ],
            "title": "Local spectral graph convolution for point set feature learning",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Peihao Wang",
                "Xuxi Chen",
                "Tianlong Chen",
                "Subhashini Venugopalan",
                "Zhangyang Wang"
            ],
            "title": "Is attention all nerf needs",
            "venue": "arXiv preprint arXiv:2207.13298,",
            "year": 2022
        },
        {
            "authors": [
                "Qianqian Wang",
                "Zhicheng Wang",
                "Kyle Genova",
                "Pratul P Srinivasan",
                "Howard Zhou",
                "Jonathan T Barron",
                "Ricardo Martin-Brualla",
                "Noah Snavely",
                "Thomas Funkhouser"
            ],
            "title": "Ibrnet: Learning multiview image-based rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing,",
            "year": 2004
        },
        {
            "authors": [
                "Muyu Xu",
                "Fangneng Zhan",
                "Jiahui Zhang",
                "Yingchen Yu",
                "Xiaoqin Zhang",
                "Christian Theobalt",
                "Ling Shao",
                "Shijian Lu"
            ],
            "title": "Wavenerf: Wavelet-based generalizable neural radiance fields",
            "venue": "arXiv preprint arXiv:2308.04826,",
            "year": 2023
        },
        {
            "authors": [
                "Jiawei Yang",
                "Marco Pavone",
                "Yue Wang"
            ],
            "title": "Freenerf: Improving few-shot neural rendering with free frequency regularization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Lingfeng Yang",
                "Xiang Li",
                "Renjie Song",
                "Borui Zhao",
                "Juntian Tao",
                "Shihao Zhou",
                "Jiajun Liang",
                "Jian Yang"
            ],
            "title": "Dynamic mlp for fine-grained image classification by leveraging geographical and temporal information",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yao Yao",
                "Zixin Luo",
                "Shiwei Li",
                "Tian Fang",
                "Long Quan"
            ],
            "title": "Mvsnet: Depth inference for unstructured multi-view stereo",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Alex Yu",
                "Vickie Ye",
                "Matthew Tancik",
                "Angjoo Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Maciej Zamorski",
                "Maciej Zieba",
                "Piotr Klukowski",
                "Rafa\u0142 Nowak",
                "Karol Kurach",
                "Wojciech Stokowiec",
                "Tomasz Trzci\u0144ski"
            ],
            "title": "Adversarial autoencoders for compact representations of 3d point clouds",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Zhang",
                "Gernot Riegler",
                "Noah Snavely",
                "Vladlen Koltun"
            ],
            "title": "Nerf++: Analyzing and improving neural radiance fields",
            "venue": "arXiv preprint arXiv:2010.07492,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Zicheng Zhang",
                "Yinglu Liu",
                "Congying Han",
                "Yingwei Pan",
                "Tiande Guo",
                "Ting Yao"
            ],
            "title": "Transforming radiance field with lipschitz network for photorealistic 3d scene stylization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Linqi Zhou",
                "Yilun Du",
                "Jiajun Wu"
            ],
            "title": "3d shape generation and completion through point-voxel diffusion",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Haidong Zhu",
                "Tianyu Ding",
                "Tianyi Chen",
                "Ilya Zharkov",
                "Ram Nevatia",
                "Luming Liang"
            ],
            "title": "Caesarnerf: Calibrated semantic representation for few-shot generalizable neural rendering",
            "venue": "arXiv preprint arXiv:2311.15510,",
            "year": 2023
        },
        {
            "authors": [
                "Dominik Zimny",
                "T Trzci\u0144ski",
                "Przemys\u0142aw Spurek"
            ],
            "title": "Points2nerf: Generating neural radiance fields from 3d point cloud",
            "venue": "arXiv preprint arXiv:2206.01290,",
            "year": 2022
        },
        {
            "authors": [
                "Dominik Zimny",
                "Jacek Tabor",
                "Maciej Zieba",
                "Przemys\u0142aw Spurek"
            ],
            "title": "Multiplanenerf: Neural radiance field with non-trainable representation",
            "venue": "arXiv preprint arXiv:2305.10579,",
            "year": 2023
        },
        {
            "authors": [
                "II (Wang"
            ],
            "title": "2022), we randomly sample 512 rays from Target-Reference pairs, and it trains",
            "year": 2022
        },
        {
            "authors": [
                "GAN (Kania"
            ],
            "title": "2023) and diffusion model (Liu et al., 2023). In this work, we focus on some 3D generative models with HyperNetwork. (Spurek et al., 2020a) is an early work that builds variable size representations of point clouds with HyperNetwork. Then, HyperFlow (Spurek et al., 2020b) uses a hypernetwork to model 3D objects as families of surfaces and Points2NeRF (Zimny et al., 2022) utilizes a HyperNetwork to generate NeRF from a 3D point cloud",
            "year": 2022
        },
        {
            "authors": [
                "pixelNeRF Yu"
            ],
            "title": "2021), especially in the 1-view setting. It\u2019s also evident that compared to NeRF Synthetic, LLFF, and DTU, InsertNeRF shows less improvement on ShapeNet. This might be due to the relatively simplistic appearance and geometry of ShapeNet-Scenes",
            "venue": "InsertNeRF",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Novel view synthesis, a fundamental discipline in computer vision and graphics, aspires to create photorealistic images from reference inputs. Early works (Debevec et al., 1996; Lin & Shum, 2004) primarily focused on developing explicit representations, facing challenges due to the absence of 3D supervision. This issue has been alleviated by recent advancements in implicit neural representation research, which have led to improved performance. In particular, Neural Radiance Fields (NeRF) (Mildenhall et al., 2021) has attracted significant interest. NeRF, and its derivative works, extract scene-specific implicit representations through overfitting training on posed scene images. Although NeRF uses neural scene representations effectively to yield realistic images, the scene-specific nature of these representations requires retraining when faced with novel scenarios.\nAn emerging topic known as Generalizable NeRF (GNeRF) has recently garnered considerable attention for this challenge. GNeRF aims to learn a scene-independent inference approach that facilitates the transition from references to target view. Current methods enhance the NeRF architecture by adding structures that aggregate reference-image features, or reference features. Examples include pixel-wise feature cost volumes (Johari et al., 2022), transformers (Wang et al., 2022; Suhail et al., 2022), and 3D visibility predictors (Liu et al., 2022). However, fitting these additions into conventional NeRF-like frameworks such as mip-NeRF (Barron et al., 2021), NeRF++ Zhang et al. (2020), and others, often proves challenging and may fail to effectively harness the guiding potential of reference features. Furthermore, the extensive use of transformers or cost volumes can be timeconsuming. Thus, an intriguing question arises: Is it possible to directly INStill gEneRalizabiliTy into NeRF (InsertNeRF) while staying faithful to the original framework?\nA straightforward way to accomplish this goal is to adaptively modify the NeRF network\u2019s weights, or implicit representations, for different reference scenes while preserving the original framework. The concept of hypernetwork (Ha et al., 2016), which conditionally parameterizes a target network, is an effective strategy in this scenario. The features extracted from the reference scene can be used as inputs to generate scene-specific network weights. However, initial experiments in Tab. 2 indicate that constructing a hypernetwork directly based on the NeRF framework can be inadequate, and often fails to predict different attributes like emitted color and volume density. To address this, we\n\u2217Corresponding author.\npropose to use HyperNet modules, which are designed to serve as easily integrable additions to existing NeRF-like frameworks. Owing to their flexibility, the resulting InsertNeRF excels at predicting the NeRF attributes by capitalizing on sampling-aware features and various module structures.\nIn InsertNeRF, we insert multiple HyperNet modules to instill generalizability throughout the framework\u2019s progression. This approach allows us to fully leverage the guiding role of scene features in determining the entire network\u2019s weights. Unlike existing works that solely utilize reference features as inputs, InsertNeRF exhibits a thorough grasp of reference scene knowledge. To further unlock the full potential of the HyperNet modules, it is crucial to aggregate scene features from a set of nearby reference images. To achieve this, we introduce a multi-layer dynamic-static aggregation strategy. Compared to existing works, it not only harnesses the inherent completion capabilities of global features, but it also implicitly models occlusion through dynamic-static weights, as demonstrated on the depth renderings shown in Fig. 1b. By feeding the aggregated scene features into the HyperNet modules, we can generate scene-related weights based on the well-understood reference scene.\nIn summary, we make the following specific contributions: \u2022 We introduce InsertNeRF, a novel paradigm that inserts multiple plug-and-play HyperNet mod-\nules into the NeRF-like framework, endowing NeRF-like systems with instilled generalizability. \u2022 We design two types of HyperNet module structures tailored to different NeRF attributes, aiming\nfor predicting scene-specific weights derived from sampling-aware scene features. For these features, we further propose a multi-layer dynamic-static aggregation strategy, which models the views-occlusion and globally completes information based on the multi-view relationships.\n\u2022 We demonstrate that InsertNeRF achieves state-of-the-art performance with extensive generalization experiments by integrating the modules into the vanilla NeRF. Furthermore, we show the significant potential of our modules in various NeRF-like systems, such as mip-NeRF (Barron et al., 2021), NeRF++ (Zhang et al., 2020), as shown in Fig. 1a, and in task with sparse inputs."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": ""
        },
        {
            "heading": "2.1 GENERALIZABLE NEURAL RADIANCE FIELDS",
            "text": "Neural Radiance Fields (NeRF) by (Mildenhall et al., 2021) and its subsequent derivatives (Barron et al., 2022; Isaac-Medina et al., 2023; Bao et al., 2023) have gained momentum and are capable of producing realistic images. However, a significant drawback is the need to retrain them for every new scene, which is not efficient in real-world applications. Recent works by (Wang et al., 2021; 2022) introduce Generalizable Neural Radiance Fields that can represent multiple scenes, regardless of whether they are in the training set. To achieve this, many studies have focused on understanding the relationships between reference views and refining NeRF\u2019s sampling-rendering mechanism. For instance, NeuRay (Liu et al., 2022) and GeoNeRF (Johari et al., 2022) use pre-generated depth maps or cost volumes as prior to alleviate occlusion issues. On the other hand, IBRNet (Wang et al., 2021) and GNT (Wang et al., 2022) implicitly capture these relationships through MLPs or transformers. Regarding the sampling-rendering process, most works (Xu et al., 2023; Suhail et al., 2022; Wang et al., 2021; Zhu et al., 2023) utilize the transformer-based architectures to aggregate the sampling point features and replace traditional volume rendering with a learnable technique. However, a\ncommon limitation is that most of these methods replace NeRF\u2019s network with transformers, making it challenging to apply to NeRF derivatives and leading to increased computational complexity. Our research aims to address this by instilling generalizability into NeRF-like systems with scene-related weights while preserving its original framework and efficiency."
        },
        {
            "heading": "2.2 HYPERNETWORKS",
            "text": "The hypernetwork (Ha et al., 2016; Chauhan et al., 2023), often abbreviated as hypernet, is invented to generate weights for a target neural network. Unlike traditional networks that require training from scratch, hypernets offer enhanced generalization and flexibility by adaptively parameterizing the target network (Alaluf et al., 2022; Yang et al., 2022; Li et al., 2020). Leveraging these benefits, hypernets have found applications in various domains including few-shot learning (Li et al., 2020), continual learning (Von Oswald et al., 2019), computer vision (Alaluf et al., 2022), etc. In the realm of NeRF, there have been efforts to incorporate hypernets to inform the training of the rendering process. For instance, (Chiang et al., 2022) propose to train a hypernet using style image features for style transfer, while (Zimny et al., 2022) employ encoded point-cloud features for volume rendering. On a related note, (Peng et al., 2023) utilize a dynamic MLP mapping technique to create volumetric videos and (Kania et al., 2023) use a hypernet for 3D-aware NeRF GAN. In our work, instead of using the hypernet in NeRF framework directly, we introduce a plug-and-play HyperNet module, with a focus on providing reference scene knowledge to enable generalization to new scenarios."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 BACKGROUND",
            "text": "Neural Radiance Fields. Neural radiance fields (NeRF) (Mildenhall et al., 2021) is a neural representation of scenes. It employs MLPs to map a 3D location x \u2208 R3 and viewing direction d \u2208 S2 to an emitted color c \u2208 [0, 1]3 and a volume density \u03c3 \u2208 [0,\u221e), which can be formalized as:\nF(x,d;\u0398) 7\u2192 (c, \u03c3) , (1)\nwhere F is the MLPs, and \u0398 is the set of learnable parameters of NeRF. Note that F can be further split into an appearance part Fapp and a geometry part Fgeo for the view-dependent attribute c and view-invariant attribute \u03c3, respectively (Zhang et al., 2023).\nVolume Rendering. Given a ray in a NeRF, r (t) = o + td, where o is the camera center and d is the ray\u2019s unit direction vector, we sample K points, {r (ti) |i = 1, ...,K}, along the ray and predict their color values ci and volume densities \u03c3i. The ray\u2019s color is then calculated by:\nC\u0302 (r) = K\u2211 i=1 wici, where wi = exp \u2212 i\u22121\u2211 j=1 \u03c3j\u03b4j  (1\u2212 exp (\u2212\u03c3i\u03b4i)) , (2) where \u03b4i is the distance between adjacent samples, and wi is considered to be the hitting probability or the weight of the i-th sampling point (Liu et al., 2022).\nGeneralizable NeRF. Given N reference scene views with known camera poses {In,Pn}Nn=1, the goal of GNeRF is to synthesize a target novel view IT based on these reference views, even for scenes not observed in the training set, thereby achieving generalizability. Current works (Wang et al., 2021; Liu et al., 2022; Wang et al., 2022) primarily focus on aggregating features along with the ray r (t) from multiple reference views. The overall process can be outlined as:\nFsample ({ Fview ( {Fn (\u03a0n(r(ti)))}Nn=1 )}K i=1 ) 7\u2192 (c, \u03c3). (3)\nHere, \u03a0n(x) projects x onto In, and Fn(z) queries the corresponding feature vectors according to the projected points in n-th reference. Fview and Fsample specifically denote the aggregation of multi-view features and the accumulation of multiple sampling point features along the ray. These aggregations are often carried out using common techniques such as MLPs and transformers."
        },
        {
            "heading": "3.2 INSERTNERF",
            "text": "We introduce InsertNeRF, a novel paradigm that instills generalizability into the NeRF framework, as illustrated in Fig. 2. While this method can be adapted to a variety of NeRF-based systems (Sec. 4.4), we focus on its application on the vanilla NeRF in this section.\nOverview. InsertNeRF achieves generalizability by inserting multiple HyperNet modules into NeRF. These modules dynamically generate scene-specific weights for NeRF that are tailored to specific reference scene, denoted by \u2126T . Based on it, by incorporating \u2126T into Eq. (1) and combining \u0398 as well as \u2126T , NeRF\u2019s implicit representation (or weights) gains the generalizability across multi-scenes, which is explained in Appendix D.3. Specifically, it can be described as follows:\nF(x,d;\u0398,\u2126T ) 7\u2192 (c, w) , where \u2126T = HyperNet ({ Fview ( {Fn (\u03a0n(r(ti)))}Nn=1 )}K i=1 ) .\n(4)\nComparing Eq. (4) to Eq. (3), the key to InsertNeRF is the newly introduced architectures with dynamic weights \u2126T , guided by the HyperNet modules based on specific reference inputs. The process begins with reference features extraction (Sec. 3.2.1), then a multi-layer dynamic-static aggregation strategy is employed to fuse reference features from multi-views into scene features (Sec. 3.2.2). Subsequently, these aggregated scene features are used to adaptively generate NeRF\u2019s samplingaware weights via the HyperNet modules, which consist of sampling-aware filters, dynamic MLPs and dynamic activation functions (Sec. 3.2.3). These novel HyperNet modules are inserted before each MLP layer in the original NeRF, serving as an enhancement to the original MLP layers.\nA notable aspect of InsertNeRF is its ability to directly calculate the hitting probability wi in Eq. (2) for volume rendering, rather than simply outputting the volume density from Fgeo. This capability stems from the implicit modeling of the relationships between spatial points and the advantage of using multi-scale features. By combining Fgeo with Fapp, the entire pipeline is trained end-toend. Our unique design not only leads to superior rendering performance in GNeRF but also offers improved computational efficiency compared to transformer-based structures (Wang et al., 2022)."
        },
        {
            "heading": "3.2.1 REFERENCE FEATURES EXTRACTION",
            "text": "In the exploration of reference images, generalizable methods often combine U-Net (Ronneberger et al., 2015) and ResNet (He et al., 2016) to extract local dense feature maps. These have proven effective in dealing with occlusion problems (Liu et al., 2022). Yet, there is a risk that an overemphasis on local dense features might neglect global features, which are key to occlusion completion\nand global inference (Iizuka et al., 2017). In our work, we take advantage of the spatial representation capabilities of multi-scale features to model complex geometry and detailed appearance. Specifically, we bring in global-local features to successively update the Hypernet module\u2019s weights for Fgeo and dense feature for Fapp. Here, geometry requires multi-scale information to deduce occluded portions, while appearance concentrates on dense fine-grained details. This process begins with multi-scale features Fl,n from U-Net for each reference input In, and can be expressed as:\nFl,n \u2208 R W 2l+1 \u00d7 H 2l+1 \u00d7Cl , l = 2, 1, 0; n = 1, \u00b7 \u00b7 \u00b7 , N. (5)\nHere, W \u00d7 H defines the image resolution, and Cl is the number of channels. During feature upsampling (as l decreases), we output each layer\u2019s features, transitioning from global to local."
        },
        {
            "heading": "3.2.2 MULTI-LAYER DYNAMIC-STATIC AGGREGATION STRATEGY",
            "text": "Following the feature extraction, the next essential step is the aggregation of scene features. This is not only foundational for scene generalizability but also significantly impacts the effectiveness of the HyperNet modules. Most existing techniques focus primarily on preserving local geometry and appearance consistency, often employing visibility to model occlusions. A straightforward approach is to deduce the view-weight based on differences between reference and target views (Wang et al., 2021). We refer to it as static weight, denoted by MST \u2208 RB\u00d7K\u00d7N , where B represents the batch size, and it assigns higher weights to closer views in a fixed manner. However, it may be unreliable as it overlooks the correlation among the features. To remedy this, we introduce a dynamic prediction of multi-layer weights based on multi-scale features, involving a blend of Maxpool-MLPs and Softmax layers, termed dynamic weights and denoted by MDYl \u2208 RB\u00d7K\u00d7N . Our approach hence adopts a dynamic-static aggregation strategy for more nuanced multi-view scene feature aggregation.\nFormally, given the corresponding features Fl \u2208 RB\u00d7K\u00d7N\u00d7dl of B \u00d7 K points in the space, where dl is the latent feature dimension, we calculate the weighted means and variances as \u00b5l = En [ Fl \u2299MDYl ] \u2208 RB\u00d7K\u00d7dl and vl = Vn [ Fl \u2299MDYl ] \u2208 RB\u00d7K\u00d7dl , respectively. After concatenating Fl for each reference view with \u00b5l and vl and halvely projecting its dimension, denoted as F\u0303l \u2208 RB\u00d7K\u00d7N\u00d7dl/2, it is applied to the static weight to obtain \u00b5\u0303l = En [ F\u0303l \u2299MST ] \u2208\nRB\u00d7K\u00d7dl/2 and v\u0303l = Vn [ F\u0303l \u2299MST ] \u2208 RB\u00d7K\u00d7dl/2. With Fmaxl \u2208 RB\u00d7K\u00d7dl representing the\nmaximum features among all the reference views, and by concatenating \u00b5\u0303l and v\u0303l, and adding it to Fmaxl , we accomplish the feature aggregation phase Fview in Eq. (4).1\nThe use of global-local dynamic weights leads to a significant enhancement in edge sharpness and the thorough completion of detail in the depth rendering images, as evidenced in Fig. 1b. Note that unlike static weights, dynamic weights are guided by the relationships between multi-scale reference features and are learned with auxiliary supervision (Sec. 3.3)."
        },
        {
            "heading": "3.2.3 HYPERNET MODULES",
            "text": "We now turn our attention to the HyperNet modules, the core element of InsertNeRF, integrated within both Fgeo and Fapp. These modules are composed of three basic components: samplingaware filters, dynamic MLPs (D-MLP), and dynamic activation functions.\nSampling-aware Filter. Unlike traditional hypernetworks, where reference features are generally stable, those based on pose-related epipolar geometric constraints in GNeRF are noisy. This noise complicates their direct use for weights generation. To address this challenge, we introduce a sampling-aware filter that seeks to implicitly find correlations between inter-samples and reduce noise within the reference features through graph reasoning. Specifically, following the aggregation phase Fview, each aggregated point-feature is regarded as a node within a graph structure. The relationships between these K points are then modeled using graph convolutions, formulated as:\nHl = (I \u2212Al)FviewW al , (6)\nwhere Fview \u2208 RB\u00d7K\u00d7dl denotes the aggregated K point-features after Fview, and Al and W al represent the K \u00d7K node adjacency matrix and the learnable state update function, respectively. I here denotes the identity matrix. This specific graph structure helps filter out noise by state-updating,\n1The advantages of this strategy in comparison with Wang et al. (2021) are discussed in the Appendix D.1.\nenabling the network to concentrate on key features more effectively. Additionally, for intricate tiny structures, we adopt an approach inspired by Chen et al. (2019), where linear layers across different dimensions are utilized instead of standard matrix multiplications within the graph convolutions.\nDynamic MLP. Using the filtered features Hl, the HyperNet module is designed to generate corresponding WeightHl and BiasHl within specific MLPs. This instills scene-awareness into vanilla NeRF, ensuring compatibility with Finput, the output of the previous layer in the original NeRF framework. To enhance efficiency, these MLPs are integrated within the sampling-aware filter.\nDynamic Activation Function. Activation functions plays an essential role in the NeRF framework (Sitzmann et al., 2020). Traditional options, such as the ReLU function, may struggle with detail rendering and hinder the performance of D-MLPs due to their static nature. To address this, we introduce a dynamic activation function. This function adaptively activates features in accordance with the unique characteristics of a given scene. Inspired by Perez et al. (2018), we propose the Dynamic Feature-wise Linear Modulation (DFiLM), in which the frequencies (FreqHl ) and phaseshifts (ShiftHl ) are dynamically determined from Hl, allowing for more responsive activation.\nThe entire MLP-Block, including both the D-MLP and the activation function, can be expressed as: Foutput = ShiftHl(FreqHl(WeightHl \u00d7 Finput + BiasHl)), (7) To insert the HyperNet modules into the NeRF framework, Foutput is subsequently fed into an original NeRF\u2019s MLP layer for the final result. This yields superior performance, as validated through experimental results. We remark that the parameters are not shared among the HyperNet modules. Moreover, their compact structures ensure that the impact on rendering efficiency is negligible.\nHyperNet Modules in Fgeo and Fapp. In vanilla NeRF, Fgeo and Fapp serve distinct purposes but employ similar MLP structures, albeit with varying complexities. Fgeo focuses on geometric properties, whereas Fapp encodes view-dependent features using a smooth BRDF prior for surface reflectance. This smoothness can be facilitated by progressively exploiting guided scene features, along with a reduction in both MLP parameters and activation functions for variable d (Zhang et al., 2020). Recognizing this need, we propose a modified HyperNet module architecture specifically for Fapp. Our design employs a progressive guidance mechanism within Fapp, incorporating multiple parallel dynamic branches into the NeRF framework. The weights of the D-MLP in each branch are progressively generated from the preceding branch, enabling the capture of reference features at different levels for complex appearance modeling. Finally, the results of all branches are summed and used as input to the original MLP for predicting the RGB value. In accordance with our analysis, the DFiLM is not used in Fapp, setting it apart from other elements in the architecture. [Appendix C.1]"
        },
        {
            "heading": "3.3 LOSS FUNCTIONS",
            "text": "The InsertNeRF pipeline is trained end-to-end utilizing three carefully designed loss functions.\nPhotometric loss. First, we employ the photometric loss in NeRF (Mildenhall et al., 2021), i.e., the Mean Square Error (MSE) between the rendered and true pixel colors:\nLMSE = \u2211 r\u2208R \u2225\u2225\u2225C\u0302(r)\u2212 C(r)\u2225\u2225\u22252 2 , (8)\nwhereR is the set of rays in a batch, and C(r) is the ground-truth RGB color for ray r \u2208 R. Backbone loss. During end-to-end training, optimizing the feature extraction without additional guidance poses considerable challenges. To address this, we draw inspiration from autoencoding (Kingma & Welling, 2013). By adding an additional upsampling layer and a small decoder (used exclusively for loss computation), we seek to reconstruct reference images from encoded features. The original images serve as supervision, and we refer to this particular loss term as Lbackbone. Dynamic weights loss. Initiating the learning of dynamic weights from scratch introduces difficulties in understanding the connections among multi-scale features. To tackle this issue, we introduce an auxiliary supervision to encompass global-local information. Specifically, we let C refn (r) \u2208 RB\u00d7K\u00d7N\u00d73 represent the ground-truth RGB values in corresponding reference images for K points in ray r within a batch R. We compute c\u2032i = \u2211 n,l,r\u2208R C ref n (r)\u2299MDYl , the weighted sum of these RGB values by dynamic weights. Utilizing c\u2032i, C\u0302 \u2032 (r) is subsequently calculated according to Eq. (2), and supervised by the true color C(r). We designate this loss term as LDY.\nTable 3: Results with sparse inputs.\nMethods 3-viewPSNR\u2191 SSIM\u2191 LPIPS\u2193 DietNeRF (ICCV 2021) 14.94 0.370 0.496 RegNeRF (CVPR 2022) 19.08 0.587 0.336 GeCoNeRF (ICML 2023) 18.77 0.596 0.338 FreeNeRF (CVPR 2023) 19.63 0.612 0.308 InsertNeRF (w/o retrain) 19.41 0.618 0.330\nWe formulate our final loss function as\nL = LMSE + \u03bb1Lbackbone + \u03bb2LDY (9)\nwhere \u03bb1 and \u03bb2 are hyperparameters controlling the relative importance of these terms."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We conduct comparative experiments with state-of-the-art (SOTA) methods across different settings on mainstream datasets. Additionally, we validate the effectiveness of the proposed paradigm in the context of derivative NeRF-like systems generalizations and tasks involving sparse inputs."
        },
        {
            "heading": "4.1 EXPERIMENTAL PROTOCOL AND SETTINGS",
            "text": "Following IBRNet (Wang et al., 2021), GNeRF exploits the target-reference pairs sampling strategy during both the training and inference phases. Here, reference views are selected from a set of nearby views surrounding the target view. Specifically, N reference views are chosen from a pool of P \u00d7 N (P \u2265 1) neighboring views of target, ensuring that the target view is excluded from the reference views. During the evaluation phase, we conduct evaluations using three metrics: PSNR, SSIM, and LPIPS, on well-established datasets such as NeRF Synthetic, LLFF, and DTU. More training and inference details are provided in the Appendix A and Algorithm in the Appendix C.2.\nIn our experiments, we follow two GNeRF settings of existing methods:\nSetting I. Following NeuRay (Liu et al., 2022), we use three types of training datasets for training GNeRF, including three forward-facing datasets, the synthetic Google Scanned Object dataset and the DTU dataset. Note that we only select training scenes in the DTU dataset, excluding the four evaluation scenes. Following their setting in the experiments, we set N = 8.\nSetting II. Following GNT (Wang et al., 2022), we train GNeRF using three forward-facing datasets and the Google Scanned Object dataset. Unlike Setting I, the DTU dataset is not used for either training or evaluation. In addition, we set N = 10 in this setting."
        },
        {
            "heading": "4.2 COMPARATIVE EXPERIMENTS",
            "text": "We evaluate InsertNeRF for its generalization based on the vanilla NeRF framework, comparing its performance with SOTA methods under two GNeRF settings. Through extensive quantitative and qualitative experiments, we explore the advantages of our approach, even with fewer references.\nQuantitative comparisons. We present quantitative comparisons with SOTA methods under Setting I and Setting II, as reported in Tab. 1 and Tab. 2. For Setting I, the quantitative comparisons\n\u2020GeoNeRF (Johari et al., 2022) and WaveNeRF (Xu et al., 2023) are trained on original rectified images and evaluated on the distinct scenes with us in the DTU dataset.\nTable 4: HyperNet modules ablations.\nMethods LLFFPSNR\u2191 SSIM\u2191 LPIPS\u2193 w/o D-MLP 23.33 0.774 0.198 w/o Sampling Filter 24.67 0.815 0.158 w/o DFiLM 25.04 0.832 0.152\nw/o original MLP 25.44 0.848 0.131 InsertNeRF (Ours) 25.68 0.861 0.126\nin Tab. 1 display our model\u2019s competitive results in evaluation datasets, with significant improvements in PSNR, SSIM and LPIPS in comparison to existing SOTA methods. Specifically, PSNR and LPIPS exhibit substantial enhancements by \u223c1.16dB \u2191 and \u223c23.6% \u2193 respectively. For Setting II, InsertNeRF consistently outperforms the SOTA method (Wang et al., 2022), as substantiated by the results in Tab. 2. We observe that these improvements become even more pronounced with fewer reference images, alongside higher efficiency, as demonstrated in subsequent sections.\nQualitative comparisons. Fig. 1 and Fig. 3 (a) show the qualitative performances of our method against baseline and SOTA methods. InsertNeRF achieves improved geometric fidelity and clear edges, attributable to the completion capability of global features and the modeling of sample spatial relationships from graph structures. For more analysis and results, please refer to the Appendix E.5."
        },
        {
            "heading": "4.3 ABLATION STUDIES",
            "text": "In Tab. 2, we analyze the core components of our method. The findings underscore the vital role of HyperNet modules in enhancing rendering performance, as further evidenced in Fig. 3 (b) that they instill scene-specific capabilities into NeRF\u2019s representation. Additionally, the multi-layer dynamicstatic aggregation strategy proves to be essential. By integrating both modules, our novel paradigm instills generalizability into the NeRF framework, leading to a performance boost of approximately two to three times compared to the baseline model, i.e., vanilla NeRF. Additionally, we explore the underlying mechanisms driving the effectiveness of these components. More experiments, including single-scene setting, fine-tuning and ablation studies about Fapp in the Appendix E\nHyperNet modules. Tab. 4 demonstrates that both the sampling-aware filters and dynamic activation functions are vital in the HyperNet modules, with the sampling-aware filters having a more substantial impact. This could be due to the need to consider relationships between sampled points in the rendering process, which implicitly models occlusions, as noted in Liu et al. (2022). Solely using dynamic activation functions without D-MLP leads to a marked decline in performance, highlighting the\nessential role of MLPs in neural representation. Furthermore, using only the HyperNet modules and omitting the original NeRF\u2019s MLP layers results in inferior performance, reducing training stability.\nMulti-layer dynamic-static aggregation strategy. In Tab. 5, ablation studies reveal the significance of dynamic-static weights and multi-layer features. Using only dynamic weights appears more effective than static weight, likely because they are adaptively generated to suit different scene features. The auxiliary supervision for dynamic weights and multi-layer global-local features also play essential roles in aggregating multi-view features, underlining their importance in this strategy.\nInput number (N ) and efficiency. Since feature extraction is time-consuming, reducing the number of reference images substantially improves the training and inference efficiency of the network. Fig. 4 illustrates the performance of InsertNeRF as the number of reference images (N ) varies for training on NeRF Synthetic. In comparison to GNT (Wang et al., 2022), InsertNeRF consistently demonstrates superior rendering performance and inference efficiency. This success can be attributed to our novel generalization paradigm and the compact structures of the HyperNet modules."
        },
        {
            "heading": "4.4 INSERT-NERF-LIKE FRAMEWORKS",
            "text": "Thanks to the plug-and-play advantage of the HyperNet modules, we extend the study of generalization to derived domains of NeRF, such as mip-NeRF (Barron et al., 2021) and NeRF++ (Zhang et al., 2020), areas that have rarely been discussed before. More details are provided in the Appendix A.\nInsert-mip-NeRF. Mip-NeRF is a multi-scale NeRF-like model used to address the inherent aliasing of NeRF, a significant challenge for GNeRF. Unlike Huang et al. (2023), we explore how to instill generalizability into mip-NeRF, following its original setup. We report the qualitative and quantitative performance of mip-NeRF, InsertNeRF, and Insert-mip-NeRF on multi-scale NeRF Synthetic in a cross-scene generalization setting (see Tab. 6, Fig. 1 and Fig. 5). One can observe that incorporating the HyperNet modules not only enhances generalization for mip-NeRF but also addresses the inherent aliasing of InsertNeRF and improves the performance in the task of multi-scale rendering.\nInsert-NeRF++. NeRF++, an unbounded NeRF. Fig. 1 and Fig. 13 depicts qualitative and quantitative rendering results of Insert-NeRF++. It is evident that our approach has successfully instilled generalizability into the NeRF++ framework, doubling its PSNR compared to the original.\nSparse Inputs. Training NeRF with sparse inputs has become a notable focus recently (Niemeyer et al., 2022; Yang et al., 2023). Unlike our nearby reference views setting (Sec. 4.1), this task often involves training from a limited number of fixed viewpoints to represent the entire scene. Under this setting, we relax constraints on selecting nearby viewpoints and uniformly select fixed sparse seen viewpoints to infer on arbitrary unseen viewpoints. Unlike existing works, our method trains on extensive auxiliary datasets, allowing us to represent the entire evaluation scene from\nsparse inputs without retraining (see Tab. 3). To ensure fairness, all scenes in evaluation are excluded in the training phase. In conclusion, InsertNeRF offers a novel insight that employs pre-training on auxiliary datasets to enhance representation capabilities with sparse inputs. We believe that, through fine-tuning on the evaluation scene and incorporating existing technologies like geometry and color regularization, our paradigm will achieve even better performance under sparse inputs."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We present InsertNeRF, a novel paradigm that instills generalizability into NeRF systems. Unlike popular transformer-based structures, our HyperNet modules are efficiently incorporated into the original NeRF-like framework, leveraging reference scene features to generate scene-specific network weights. To achieve this, we design a multi-layer dynamic-static feature aggregation strategy for extracting scene features from reference images and employ sampling-aware filters to explore relationships between sample points. Experiments on well-established datasets show that InsertNeRF and other Insert-NeRF-like frameworks can render high-quality images across different scenes without retraining. This offers insights for future works on: (i) generalization tasks for additional NeRFlike systems such as mip-NeRF 360; and (ii) sparse inputs tasks based on auxiliary datasets."
        },
        {
            "heading": "ACKNOWLEFGEMENTS",
            "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 62276128, Grant 62192783, and Grant 62106100; in part by the Jiangsu Natural Science Foundation under Grant BK20221441; in part by the Young Elite Scientists Sponsorship Program by CAST under Grant 2023QNRC001; in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization."
        },
        {
            "heading": "A EXPERIMENTAL DETAILS",
            "text": "During the training process, the feature extraction and InsertNeRF-systems are trained with different learning rates in an end-to-end manner, and we apply the Adam to optimize the entire network with an exponentially decaying learning rate. In our experiments, \u03bb1 is set as 0.1 and \u03bb2 is set as 1. During the cross-scene evaluation phase, we individually test the average PSNR, SSIM, and LPIPS metrics of all testing-view renderings for each scene and report the average values across all scenes, as shown in Tab. 1. For the sake of fair experimental comparison and efficiency with (Wang et al., 2022), we set the rendering stride size to 2 across all experiments. Empirical evidence demonstrates that this choice influences the evaluation of metrics, as shown in Tab. 8. Our model is implemented using PyTorch 1.11.0 and all experiments are conducted on Nvidia RTX 3090 GPUs with CUDA 11.4.\nMetrics. We calculate the Peak Signal to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) (Wang et al., 2004) to evaluate rendering quality for target novel viewpoints. Additionally, the Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018) is adopted as a perceptual metric. For all our experiments, we report the average of the PSNR, SSIM and LPIPS under different testing views in multiple scenes to verify the generalization. It is noteworthy that, similar to the majority of GNeRF research (Liu et al., 2022; Wang et al., 2022), our focus lies on foregroundcentric metrics for both the NeRF Synthetic and DTU datasets during the evaluation process."
        },
        {
            "heading": "A.1 EVALUATION DATASETS.",
            "text": "NeRF Synthetic. The dataset consists of 8 synthetic objects with viewpoints uniformly sampled on the upper hemisphere. Each scene comprises 200 test images, wherein a sampling strategy with an interval of 8 images is employed during the evaluation phase. Local Light Field Fusion (LLFF). The dataset consists of 8 complex real-world scenes. Each scene includes real images, 1/8 of which are used as the evaluation dataset. DTU Dataset. The dataset consists of 128 object-scenes, which is a classic dataset of MVS. In the experiments, we select 4 scenes (birds, tools, bricks and snowman) for evaluation (Liu et al., 2022)."
        },
        {
            "heading": "A.2 EXPERIMENTAL DETAILS FOR INSERTNERF",
            "text": "For all our experiments, we maintain the experimental protocols of NeuRay (Liu et al., 2022) and GNT (Wang et al., 2022). In Setting I, we also employ depth maps (Liu et al., 2022) as priors to assist the Hypernet modules to generate adaptive weights. Following (Liu et al., 2022), we randomly sample 2048 rays from Target-Reference pairs, and it trains for a total of 600,000 steps. In Setting II (Wang et al., 2022), we randomly sample 512 rays from Target-Reference pairs, and it trains for a total of 400,000 steps without any priors. In order to enhance training and inference efficiency, we sample K = 64 points along each ray and simplify the volume rendering process in our paradigm."
        },
        {
            "heading": "A.3 EXPERIMENTAL DETAILS FOR INSERT-MIP-NERF",
            "text": "Insert-mip-NeRF substitutes point-samplings with a sequence of conical-frustums and introduces the integrated positional encoding into the InsertNeRF framework. In contrast to InsertNeRF, we sample K = 65 positions for integrated positional encoding on conical-frustums. Throughout all experiments, we employ the multi-scale variants of NeRF Synthetic, such as Full, 1/2, 1/4 and 1/8 Resolutions, to simulate multi-resolution scenes for training and evaluation as (Barron et al., 2021). The remaining experimental configurations follow those of mipNeRF and our Setting II."
        },
        {
            "heading": "A.4 EXPERIMENTAL DETAILS FOR INSERT-NERF++",
            "text": "Insert-NeRF++ divides the InsertNeRF\u2019s scene representations into foreground and background components and combines both for finally rendering (Zhang et al., 2020). In Insert-NeRF++, we avoid using inverted sphere parametrization and chose standard parameterization across foregroundbackground spatial ranges, which possesses the capability to offer precise spatial positions for projection process \u03a0n(x). Following (Zhang et al., 2020), we employ the Tanks and Temples dataset (Knapitsch et al., 2017), a real-world unbound dataset captured with hand-held cameras, for training and evaluation under cross-scenes setting. During the evaluation process, we separately\nreport the renderings for the foreground and background as (Zhang et al., 2020). The remaining experimental configurations also follow those of vanilla NeRF++ and our Setting II."
        },
        {
            "heading": "B RELATED WORKS",
            "text": "B.1 IMAGE-BASED RENDERING\nImage-based rendering (IBR) (Chan et al., 2007) is a classic technique that aims to generate novel views within a specific scene by warping and integrating pixel information from reference-view images. To ensure spatial consistency, most existing works simplify this challenge by resorting to estimated explicit geometry or depth maps (Riegler & Koltun, 2020; Jain et al., 2023). In order to obtain proxy geometry, Structure-from-Motion (SfM) (Sinha et al., 2009) and MultiView Stereo (MVS) (Yao et al., 2018) have recently attracted the attention of researchers in the field of IBR. However, hints from explicit geometry without 3D supervision are unstable in most scenarios. Recently light field rendering (Lin & Shum, 2004) has become one of the alternatives to explicit representations, which considers the lighting and reflection properties of the scene to ensure visually plausible rendering. Moreover, some works (Kopf et al., 2013; Chen et al., 2021) focus on aggregating information from multiple reference views, which exploits the relationships between references and also implicitly solves the occlusion. As opposed to explicit geometry, these methods rely on constructing implicit representations to enable reasoning about novel views (Liu et al., 2019). Different from the above explicit or implicit scene-customized representations for IBR, our method can be used in a large number of scenes simultaneously without retraining."
        },
        {
            "heading": "B.2 NEURAL SCENE REPRESENTATION",
            "text": "Representing the geometry and appearance of a scenes with neural networks has been considered an alternative to 3D scene representations in recent years (Mescheder et al., 2019; Peng et al., 2020). Existing works demonstrate the potential of Multi-Layer Perceptrons (MLPs) in implicit representations, which activate spatial features by continuous functions (Genova et al., 2020; Jiang et al., 2020). Neural radiance fields (NeRF) (Mildenhall et al., 2021) apply such functions for coordinate-based representations, which use high-dimensional interpolation to produce photorealistic renderings of target views. On its basis, mip-NeRF (Barron et al., 2021) replaces rays with casting cones during volume rendering, changing the input of NeRF from points to cone frustums, and introduces an integrated positional encoding for multi-resolutions images. Subsequently, NeRF++ (Zhang et al., 2020) and mip-NeRF 360 (Barron et al., 2022) further improve NeRF and mip-NeRF to adapt to distant targets under unbounded scenes. To further enhance representation efficiency, (Chen et al., 2022) and (Zimny et al., 2023) proposed efficient representation methods to replace a large number of neural network parameters. Although derivative works has been a surge, similar to most IBR works, NeRF must also be trained for each novel scene, which is time-consuming in practice."
        },
        {
            "heading": "B.3 GENERATIVE MODELS",
            "text": "With the development of generative models, 3D generative models have been widely discussed, enabling the direct construction of 3D representations such as point clouds (Zamorski et al., 2020), surfaces (Spurek et al., 2020b), voxels (Zhou et al., 2021) and NeRF (Poole et al., 2022). A significant amount of works have leveraged techniques from image generative models and applied them to 3D generation, including GAN (Kania et al., 2023) and diffusion model (Liu et al., 2023). In this work, we focus on some 3D generative models with HyperNetwork. (Spurek et al., 2020a) is an early work that builds variable size representations of point clouds with HyperNetwork. Then, HyperFlow (Spurek et al., 2020b) uses a hypernetwork to model 3D objects as families of surfaces and Points2NeRF (Zimny et al., 2022) utilizes a HyperNetwork to generate NeRF from a 3D point cloud. Additionally, in recent years, there have been some NeRF works that focus on this technique, they directly incorporate HyperNetwork into NeRF, as described in Section 2.2. However, in the generalizable NeRF task, such idea is suboptimal, overlooking the characteristic of different attributes, such as volume density and color. Furthermore, they struggle to capture the relationship between the inputs (reference images) in the target\u2019s sampling process. Therefore, this paper proposes two types of HyperNet module structures for Fgeo and Fapp and Sampling-aware Filter separately to mitigate the aforementioned two issues.\nC IMPLEMENTATION DETAILS"
        },
        {
            "heading": "C.1 HYPERNET MODULE ARCHITECTURE",
            "text": "Diverging from the conventional HyperNetworks (Ha et al., 2016), the direct prediction of distinct attributes, such as emitted color and volume density, within the NeRF framework often proves suboptimal. Drawing an analogy to the vanilla NeRF (Mildenhall et al., 2021), where different network depths were assigned for Fgeo and Fapp, it is essential for InsertNeRF to discuss distinct HyperNet module structures for them. Fgeo plays a pivotal role in the NeRF\u2019s geometric representations, which necessitates accurate inference and completion of the relationship between references based on global-local features. Based on this analysis, multi-scale features Hl are systematically introduced into the Fgeo\u2019s HyperNet modules, as shown in Fig. 6(a). After Eq. (7), Foutput is subsequently fed into an original NeRF\u2019s MLP layer. Note that we also incorporate an additional ReLU and BatchNorm after MLP to ensure training stability for HyperNet modules.\nFfinal = ReLU(BatchNorm(MLPl(Foutput))), (10)\nNote that in Fgeo, as the network\u2019s depth increases, we leverage denser features for guiding weights prediction, i.e., global to dense. Additionally, when the number of MLP layers surpasses that of the feature layers, H0 will be recurrently utilized in the remaining MLP layers.\nUnlike Fgeo, Fapp exhibits a heightened focus on dense features (Wang et al., 2022) and the smooth BRDF prior for surface reflectance (Zhang et al., 2020). As shown in Fig. 6(b), Fapp\u2019s HyperNet modules employ a parallel progressive generation paradigm and residual connection that respond to the desired smoothness. Specifically, given dense feature H0,\nF\u0303final = MLP\n( Z\u2211\nz=0\n( WeightTz \u00d7 Finput ) + Finput ) , { Tz = WeightTz\u22121 z \u2265 1\nTz = H0 z = 0 , (11)\nwhere Z represents the number of parallel branches. Note that DFiLM and dynamic bias are not utilized in Fapp for the smooth BRDF prior."
        },
        {
            "heading": "C.2 PSEUDOCODE",
            "text": "In contrast to the scene-customized vanilla NeRF and its derivative works, GNeRF primarily concentrates on cross-scene rendering tasks without any retraining. As shown in Fig. 1, our HyperNet modules possess the capacity to instill generalizability into NeRF-like systems. Here, we delve further into the training processes of InsertNeRF-like systems. In Algorithm 1, due to \u2126TrSceneT \u2019s adaptive response to the stochastic sampling of scenes by DTrScene (that represents data of training scenes), InsertNeRF-like systems acquires inherent generalizability, where NeRF-like systems encompass diverse frameworks, including but not limited to mipNeRF, NeRF++, NeRF- -, and others. In the evaluation phase, for any given PT , we sample neighboring views {In,Pn}Nn=1 fromDTeScene, rendering for IT with the pretrained \u0398NeRF-like Systems and \u0398HyperNet, as shown in Algorithm 2."
        },
        {
            "heading": "C.3 GRAPH REASONING",
            "text": "Graph-based methods have been the focus of extensive research recently and shown to be an efficient way of relation reasoning (Wang et al., 2018). Following the spatial properties, we conceptualize\nAlgorithm 1: Training for InsertNeRF-like Systems Data: Training Datasets DTrain Result: \u0398NeRF-like Systems, \u0398HyperNet\n1 while t \u2264 it do 2 Sample: r(ti)\u2190 { {IT ,PT } , {In,Pn}Nn=1 } \u2190 DTrScene \u2190 DTrain\n3 \u2126TrSceneT \u2190 HyperNet ({ Fview ( {Fn (\u03a0n(r(ti)))}Nn=1 )}K i=1 ) 4 InsertNeRF-like Systems\u2190 NeRF-like Systems ( r(ti),\u2126 TrScene T\n) 5 \u0398NeRF-like Systems,\u0398HyperNet \u2190 L 6 t\u2190 t+ 1; 7 end\nAlgorithm 2: Testing for InsertNeRF-like Systems Input : \u0398NeRF-like Systems, \u0398HyperNet, DTest, and Random PT in Testing Scenes. Output: IT\n1 Sample: r(ti)\u2190 {{ {In,Pn}Nn=1 \u2190 DTeScene \u2190 DTest } , {PT } } 2 \u2126TeSceneT \u2190 HyperNet ({ Fview ( {Fn (\u03a0n(r(ti)))}Nn=1 )}K i=1\n) 3 IT \u2190 InsertNeRF-like Systems ( r(ti),\u2126 TeScene T\n) all sampled points along a ray in a fully connected graph to find correlations between inter-samples and further update node features. Specifically, as shown in Eq. (6), InsertNeRF initially predicts a learnable adjacency matrix Al to parameterize the edge weights between nodes, which models the relationships between sampled points. Subsequently, W al is employed to update node states, mitigating the noise from epipolar geometric constrains. Furthermore, the identity matrix I is introduced to guide the learning process to pay more attention to the intrinsic characteristics of node features. An naive approach is to calculate the adjacency matrix based on the similarity between node features or Euclidean distance, and update node states, similar to existing graph convolution works. However, this is computationally expensive, especially for a large number of MLP blocks in InsertNeRF. Inspired by Chen et al. (2019), Al and W al are replaced by two separate linear layers operating in different dimensions, while the identity matrix is represented as a residual connection,\nHl = Linear ( Linear (Fview) T )T + Fview. (12)"
        },
        {
            "heading": "D DISCUSSION",
            "text": ""
        },
        {
            "heading": "D.1 DIFFERENT FROM IBRNET ABOUT DYNAMIC-STATIC",
            "text": "For inputs, we employ the global-dense features as our multi-layer inputs, compared to IBRNet\u2019s single-layer dense feature, it not only retains the detailed information from dense features but also utilizes global features to predict occluded regions, as demonstrated on the depth renderings shown\nin Fig. 1b, where reports the rendering results of IBRNet (top) and InsertNeRF (bottom) in terms of color-depth. It is evident that InsertNeRF produces sharper edges and achieves more accurate depth predictions for background regions, even when occluded in the reference views.\nFor architecture, IBRNet generates the visual maps using features based on the MST, while our InsertNeRF directly predicts MDYl using multi-layer features before M\nST. Intuitively, our strategy prevents excessive reliance on the MST and contributes to the adaptive inference of relationships between reference-target images. To provide additional validation, we conduct ablation experiments. Concretely, we replace the Multi-layer Dynamic-Static with the aggregation strategy from IBRNet and introduce identical multi-layer inputs into IBR-InsertNeRF for fairness. As shown in Tab. 7, despite its simplicity, our approach yields significant improvements under GNeRF settings. In addition, we also observe that the Multi-layer inputs still contributes to a notable enhancement in IBR-InsertNeRF, which is consistent with the findings in Tab. 5.\nFor supervision, in contrast to the direct generation of visual maps without any supervision, we introduce auxiliary supervision to guide MDYl in fully encoding global-dense features. As shown in Tab. 5, the significance of our auxiliary supervision cannot be disregarded. In summary, as shown in Fig. 7, compared to IBRNet, the Multi-layer Dynamic-Static Aggregation Strategy focuses on predicting dynamic weights and combines them with static weight based on the multi-layer inputs and the auxiliary supervision to aggregate multiple reference features.\nD.2 SCENE REPRESENTATION ANALYSIS\nThe HyperNet modules instill generalizability into NeRF by generating scene-specific weights in the original framework. To verify this, we visualize the intermediate representations of InsertNeRF through a t-SNE plot. As shown in Fig. 3(b), it is noteworthy that the reduceddimensional features exhibit scene clustering characteristics in LLFF evaluation data, which may be attributed to the dynamic MLPs and activation functions in our HyperNet modules. NeRF Synthetic also exhibits a similar trend. In Fig. 8, although the scenes still possess clustering characteristics, they appear relatively more dispersed, which may be attributed to significant disparities between evaluation viewpoints in the NeRF Synthetic."
        },
        {
            "heading": "D.3 WHY INSERTNERF DESIGNS COULD HELP NERF GENERALIZATION",
            "text": "Vanilla NeRF can be considered as an implicit representation used to depict a scene through the parameters of a neural network, i.e. \u0398, as described in Eq. (1)\nA natural idea is how to alter \u0398 for different scenes s, so that \u0398s possesses the ability to represent this new scene. By sampling different s \u2208 S and generating different \u0398s, this can be considered as endowing vanilla NeRF representation with generalizability in multi-scenes. However, unlike explicit 3D representations such as voxels, meshes, and point clouds, constructing an implicit representation \u0398s directly for a given s is challenging.\nTherefore, in this paper, we introduce the HyperNet modules, which is invented to generate weights for a target neural network, to address this issue. Through two types of the HyperNet modules we propose, scene-customization weights (parameters) \u2126sT in the NeRF framework are generated in a given s. Here, we predict \u2126sT by combining the feature extraction from reference images and the multi-Layer dynamic-static aggregation strategy. Finally, by combining \u0398 and new weights \u2126sT within the NeRF framework, we obtain \u0398s that can adapt to different scenes s, as described in Sec. 3.2"
        },
        {
            "heading": "E ADDITIONAL RESULTS AND ANALYSIS",
            "text": ""
        },
        {
            "heading": "E.1 ADDITIONAL ABLATION STUDIES",
            "text": "Rendering Resolutions: During the evaluation phase, prior works employed different rendering resolutions, which has some impact on the metric. To investigate this issue, we evaluate the rendering performance at different resolutions without altering the training settings. In Tab. 8, reducing the rendering resolution not only improved rendering efficiency but also demonstrated performance enhancements in LLFF. However, in the DTU dataset, a contrasting trend is evident, which may be attributed to its emphasis on foreground rendering.\nParallel Branches Z: We further analyze the influence of the number of parallel branches Z on network rendering performance, as shown in Tab. 9. When Z = 1, i.e., Fapp is replaced by Fgeo with the same input, a significant performance drop occurs. This might be attributed to the implicit modeling of BRDF prior by Fapp. Furthermore, with an increase in the number of branches, Fapp endows the NeRF framework with enhanced fine-detail generalizability."
        },
        {
            "heading": "E.2 RESULTS FOR SHAPENET",
            "text": "In this section, we explore the performance of the our InsertNeRF in ShapeNet under Chairs and Cars scenes. Due to the primary emphasis of InsertNeRF on multi-view settings I and II, the validation for multi-layer dynamic-static aggregation strategy under few-views settings is unnecessary. Therefore, we integrate the HyperNet modules into the original pixelNeRF Yu et al. (2021), altering its training inputs accordingly. As shown in the Tab. 10, our modules exhibit significant improve-\nments compared to pixelNeRF Yu et al. (2021), especially in the 1-view setting. It\u2019s also evident that compared to NeRF Synthetic, LLFF, and DTU, InsertNeRF shows less improvement on ShapeNet. This might be due to the relatively simplistic appearance and geometry of ShapeNet-Scenes, and our work primarily focuses on multi-view settings as mentioned in (Kulha\u0301nek et al., 2022)."
        },
        {
            "heading": "E.3 RESULTS FROM FINE-TUNING",
            "text": "We also explore the rendering performance of InsertNeRF after fine-tuning in various scenes. In contrast to the fine-tuning methodology adopted by (Liu et al., 2022), we fine-tune directly on the pre-trained model. Tab. 11 presents the performance of InsertNeRF across different scenes and the results after fine-tuning in NeRF Synthetic."
        },
        {
            "heading": "E.4 SINGLE SCENE RESULTS",
            "text": "Existing works (Wang et al., 2022) also tend to focus on single-scene rendering within the framework of GNeRF. We conduct a quantitative comparison with existing works in the single-scene setting and achieve satisfactory performance, as shown in Tab. 12."
        },
        {
            "heading": "E.5 MORE QUALITATIVE RESULTS",
            "text": "We present additional qualitative results to further analyze the superiority of InsertNeRF. i). Fig. 9 and Fig. 10 report qualitative results in LLFF and NeRF Synthetic. ii). Fig. 11 showcase more Color-Depth results in LLFF and DTU datasets. iii). We also qualitatively analyze the generalizability of the InsertNeRF-systems including Insert-mip-NeRF Fig. 12, and Insert-NeRF++ Fig. 13. Note that the presence of color distortion in the Insert-NeRF++\u2019s foreground rendering is observed, yet it does not impact the combined results, possibly attributable to the replaced sampling process."
        },
        {
            "heading": "F LIMITATIONS",
            "text": "In the majority of scenarios, a higher number of sample points along the rays often leads to improved rendering performance. In essence, thanks to the transformer architecture, existing works (Wang et al., 2022; 2021) can be trained on a limited number of sample points (64 training samples) and evaluated on the more sample points (192 evaluation samples), resulting in elevated training efficiency and improved rendering performance. However, in the InsertNeRF-system, it is essential to maintain consistency in the number of sample points between the training and evaluation processes. In order to ensure fairness in comparative experiments and strike the trade-off between training efficiency and rendering performance, we set K = 64 both during training and evaluation, which imposes certain limitations on the rendering performance. Naturally, as we increase the number of sample points for training, the rendering performance will further improve."
        },
        {
            "heading": "G FUTURE WORK",
            "text": "We aspire to construct an all-encompassing InsertNeRF framework, endowing generalizability into various NeRF-derived works, such as TensoRF, NeRF\u2013, NeuS, and so forth. This can facilitate existing or future NeRF research to transcend the constraints of scene-customization.\nIn addition, we have provided a pre-trained model to address NeRF under sparse inputs in Sec. 4.4. While such models have demonstrated satisfactory performance without any retraining Tab. 3, we still plan to design a fine-tuning approach for sparse inputs to further enhance rendering quality."
        }
    ],
    "year": 2024
}