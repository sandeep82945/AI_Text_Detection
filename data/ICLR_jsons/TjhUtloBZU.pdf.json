{
    "abstractText": "Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training datasets, while inaccessible or too expensive to handle, often contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper aims to understand the nature of noise in pre-training datasets and then mitigate its impact on downstream tasks. Specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are different. We empirically ascertain that the reason behind is noise in pre-training shapes the feature space differently. We then propose a light-weight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering that one may not be able to access or fully fine-tune the pre-trained models. We conduct extensive experiments on popular vision and language models including APIs that are supervised and self-supervised pre-trained on real data for evaluation. Our results show the importance of this novel and fundamental research direction, which we term Noisy Model Learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "DOWNSTREAM TASKS"
        },
        {
            "affiliations": [],
            "name": "Hao Chen"
        },
        {
            "affiliations": [],
            "name": "Jindong Wang"
        },
        {
            "affiliations": [],
            "name": "Ankit Shah"
        },
        {
            "affiliations": [],
            "name": "Ran Tao"
        },
        {
            "affiliations": [],
            "name": "Hongxin Wei"
        },
        {
            "affiliations": [],
            "name": "Xing Xie"
        },
        {
            "affiliations": [],
            "name": "Masashi Sugiyama"
        },
        {
            "affiliations": [],
            "name": "Bhiksha Raj"
        }
    ],
    "id": "SP:a2f1ce5b1c98590aa685bf5af495142341cf3526",
    "references": [
        {
            "authors": [
                "Andrei Barbu",
                "David Mayo",
                "Julian Alverio",
                "William Luo",
                "Christopher Wang",
                "Dan Gutfreund",
                "Josh Tenenbaum",
                "Boris Katz"
            ],
            "title": "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Adrien Bardes",
                "Jean Ponce",
                "Yann LeCun"
            ],
            "title": "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Lucas Beyer",
                "Olivier J H\u00e9naff",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "A\u00e4ron van den Oord"
            ],
            "title": "Are we done with imagenet",
            "venue": "arXiv preprint arXiv:2006.07159,",
            "year": 2020
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Brendan O\u2019Connor"
            ],
            "title": "Racial disparity in natural language processing: A case study of social media african-american english",
            "venue": "arXiv preprint arXiv:1707.00061,",
            "year": 2017
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101 \u2013 mining discriminative components with random forests",
            "venue": "In European Conference on Computer Vision,",
            "year": 2014
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning"
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "arXiv preprint arXiv:1508.05326,",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Daphne Ippolito",
                "Matthew Jagielski",
                "Katherine Lee",
                "Florian Tramer",
                "Chiyuan Zhang"
            ],
            "title": "Quantifying memorization across neural language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Hongyan Chang",
                "Ta Duy Nguyen",
                "Sasi Kumar Murakonda",
                "Ehsan Kazemi",
                "Reza Shokri"
            ],
            "title": "On adversarial bias and the robustness of fair machine learning",
            "venue": "arXiv preprint arXiv:2006.08669,",
            "year": 2020
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut"
            ],
            "title": "Conceptual 12M: Pushing webscale image-text pre-training to recognize long-tail visual concepts",
            "year": 2021
        },
        {
            "authors": [
                "Hao Chen",
                "Ankit Shah",
                "Jindong Wang",
                "Ran Tao",
                "Yidong Wang",
                "Xing Xie",
                "Masashi Sugiyama",
                "Rita Singh",
                "Bhiksha Raj"
            ],
            "title": "Imprecise label learning: A unified framework for learning with various imprecise label configurations",
            "venue": "arXiv preprint arXiv:2305.12715,",
            "year": 2023
        },
        {
            "authors": [
                "Mayee Chen",
                "Karan Goel",
                "Nimit S Sohoni",
                "Fait Poms",
                "Kayvon Fatahalian",
                "Christopher R\u00e9"
            ],
            "title": "Mandoline: Model evaluation under distribution shift",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyang Chen",
                "Sinan Wang",
                "Mingsheng Long",
                "Jianmin Wang"
            ],
            "title": "Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Gong Cheng",
                "Junwei Han",
                "Xiaoqiang Lu"
            ],
            "title": "Remote sensing image scene classification: Benchmark and state of the art",
            "venue": "Proceedings of the IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "Hao Cheng",
                "Zhaowei Zhu",
                "Xing Sun",
                "Yang Liu"
            ],
            "title": "Mitigating memorization of noisy labels via regularization between representations",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Mehdi Cherti",
                "Romain Beaumont",
                "Ross Wightman",
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Cade Gordon",
                "Christoph Schuhmann",
                "Ludwig Schmidt",
                "Jenia Jitsev"
            ],
            "title": "Reproducible scaling laws for contrastive language-image learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Fran\u00e7ois Chollet"
            ],
            "title": "Xception: Deep learning with depthwise separable convolutions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "M. Cimpoi",
                "S. Maji",
                "I. Kokkinos",
                "S. Mohamed",
                "A. Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "Karan Desai",
                "Gaurav Kaul",
                "Zubin Aysola",
                "Justin Johnson"
            ],
            "title": "RedCaps: Web-curated image-text data created by the people, for the people",
            "venue": "In NeurIPS Datasets and Benchmarks,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Josip Djolonga",
                "Jessica Yung",
                "Michael Tschannen",
                "Rob Romijnders",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Joan Puigcerver",
                "Matthias Minderer",
                "Alexander D\u2019Amour",
                "Dan Moldovan",
                "Sylvain Gelly",
                "Neil Houlsby",
                "Xiaohua Zhai",
                "Mario Lucic"
            ],
            "title": "On robustness and transferability of convolutional neural networks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Rahim Entezari",
                "Mitchell Wortsman",
                "Olga Saukh",
                "M Moein Shariatnia",
                "Hanie Sedghi",
                "Ludwig Schmidt"
            ],
            "title": "The role of pre-training data in transfer learning",
            "venue": "arXiv preprint arXiv:2302.13602,",
            "year": 2023
        },
        {
            "authors": [
                "Li Fei-Fei",
                "Rob Fergus",
                "Pietro Perona"
            ],
            "title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
            "venue": "Computer Vision and Pattern Recognition Workshop,",
            "year": 2004
        },
        {
            "authors": [
                "Alexandros G. Dimakis",
                "Jenia Jitsev",
                "Yair Carmon",
                "Vaishaal Shankar",
                "Ludwig Schmidt"
            ],
            "title": "Datacomp: In search of the next generation of multimodal datasets",
            "year": 2023
        },
        {
            "authors": [
                "Aritra Ghosh",
                "Himanshu Kumar",
                "P. Shanti Sastry"
            ],
            "title": "Robust loss functions under label noise for deep neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Goldberger",
                "Ehud Ben-Reuven"
            ],
            "title": "Training deep neural-networks using a noise adaptation layer",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "Sachin Goyal",
                "Ananya Kumar",
                "Sankalp Garg",
                "Zico Kolter",
                "Aditi Raghunathan"
            ],
            "title": "Finetune like you pretrain: Improved finetuning of zero-shot vision models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Xingrui Yu",
                "Gang Niu",
                "Miao Xu",
                "Weihua Hu",
                "Ivor Wai-Hung Tsang",
                "Masashi Sugiyama"
            ],
            "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor Berg-Kirkpatrick",
                "Graham Neubig"
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "arXiv preprint arXiv:2110.04366,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity mappings in deep residual networks",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "year": 1911
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IGARSS",
            "year": 2018
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo",
                "Dawn Song",
                "Jacob Steinhardt",
                "Justin Gilmer"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV), Oct 2021a",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bingyi Kang",
                "Saining Xie",
                "Marcus Rohrbach",
                "Zhicheng Yan",
                "Albert Gordo",
                "Jiashi Feng",
                "Yannis Kalantidis"
            ],
            "title": "Decoupling representation and classifier for long-tailed recognition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Tushar Khot",
                "Ashish Sabharwal",
                "Peter Clark"
            ],
            "title": "Scitail: A textual entailment dataset from science question answering",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Taehyeon Kim",
                "Jongwoo Ko",
                "JinHwan Choi",
                "Se-Young Yun"
            ],
            "title": "Fine samples for learning with noisy labels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Joan Puigcerver",
                "Jessica Yung",
                "Sylvain Gelly",
                "Neil Houlsby"
            ],
            "title": "Big transfer (bit): General visual representation learning",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Simon Kornblith",
                "Jonathon Shlens",
                "Quoc V. Le"
            ],
            "title": "Do better imagenet models transfer better",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "IEEE International Conference on Computer Vision Workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Jones",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "title": "Finetuning can distort pretrained features and underperform out-of-distribution",
            "venue": "arXiv preprint arXiv:2202.10054,",
            "year": 2022
        },
        {
            "authors": [
                "Katherine Lee",
                "Daphne Ippolito",
                "Andrew Nystrom",
                "Chiyuan Zhang",
                "Douglas Eck",
                "Chris CallisonBurch",
                "Nicholas Carlini"
            ],
            "title": "Deduplicating training data makes language models better",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Richard Socher",
                "Steven C.H. Hoi"
            ],
            "title": "Dividemix: Learning with noisy labels as semisupervised learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Shikun Li",
                "Xiaobo Xia",
                "Shiming Ge",
                "Tongliang Liu"
            ],
            "title": "Selective-supervised contrastive learning with noisy labels",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "X. Li",
                "T. Liu",
                "B. Han",
                "G. Niu",
                "M. Sugiyama"
            ],
            "title": "Provably end-to-end label-noise learning without anchor points",
            "venue": "In Proceedings of 38th International Conference on Machine Learning (ICML2021),",
            "year": 2021
        },
        {
            "authors": [
                "Hong Liu",
                "Jeff Z HaoChen",
                "Adrien Gaidon",
                "Tengyu Ma"
            ],
            "title": "Self-supervised learning is more robust to dataset imbalance",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Liu",
                "Jonathan Niles-Weed",
                "Narges Razavian",
                "Carlos Fernandez-Granda"
            ],
            "title": "Early-learning regularization prevents memorization of noisy labels",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Sheng Liu",
                "Zhihui Zhu",
                "Qing Qu",
                "Chong You"
            ],
            "title": "Robust training under label noise by overparameterization",
            "venue": "Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Lam Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "Ptuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602,",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV), Oct 2021c",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Xingjun Ma",
                "Hanxun Huang",
                "Yisen Wang",
                "Simone Romano",
                "Sarah Monazam Erfani",
                "James Bailey"
            ],
            "title": "Normalized loss functions for deep learning with noisy labels",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts"
            ],
            "title": "Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics",
            "venue": "Human language technologies,",
            "year": 2011
        },
        {
            "authors": [
                "Inbal Magar",
                "Roy Schwartz"
            ],
            "title": "Data contamination: From memorization to exploitation",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen"
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y. Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning",
            "year": 2011
        },
        {
            "authors": [
                "Thao Nguyen",
                "Gabriel Ilharco",
                "Mitchell Wortsman",
                "Sewoong Oh",
                "Ludwig Schmidt"
            ],
            "title": "Quality not quantity: On the interaction between dataset design and robustness of clip",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "Sixth Indian conference on computer vision, graphics & image processing,",
            "year": 2008
        },
        {
            "authors": [
                "Curtis G. Northcutt",
                "Lu Jiang",
                "Isaac L. Chuang"
            ],
            "title": "Confident learning: Estimating uncertainty in dataset labels",
            "venue": "Journal of Artificial Intelligence Research (JAIR),",
            "year": 2021
        },
        {
            "authors": [
                "Changdae Oh",
                "Hyeji Hwang",
                "Hee-young Lee",
                "YongTaek Lim",
                "Geunyoung Jung",
                "Jiyoung Jung",
                "Hosik Choi",
                "Kyungwoo Song"
            ],
            "title": "Blackvip: Black-box visual prompting for robust transfer learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Samet Oymak",
                "Zalan Fabian",
                "Mingchen Li",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian",
            "venue": "arXiv preprint arXiv:1906.05392,",
            "year": 2019
        },
        {
            "authors": [
                "O.M. Parkhi",
                "A. Vedaldi",
                "A. Zisserman",
                "C.V. Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Xingchao Peng",
                "Qinxun Bai",
                "Xide Xia",
                "Zijun Huang",
                "Kate Saenko",
                "Bo Wang"
            ],
            "title": "Moment matching for multi-source domain adaptation",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "A. Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Recht",
                "Rebecca Roelofs",
                "Ludwig Schmidt",
                "Vaishaal Shankar"
            ],
            "title": "Do imagenet classifiers generalize to imagenet",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Tal Ridnik",
                "Emanuel Ben-Baruch",
                "Asaf Noy",
                "Lihi Zelnik-Manor"
            ],
            "title": "Imagenet-21k pretraining for the masses",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein",
                "Alexander C. Berg",
                "Li Fei-Fei"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2015
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki"
            ],
            "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
            "venue": "arXiv preprint arXiv:2111.02114,",
            "year": 2021
        },
        {
            "authors": [
                "Vaishaal Shankar",
                "Achal Dave",
                "Rebecca Roelofs",
                "Deva Ramanan",
                "Benjamin Recht",
                "Ludwig Schmidt"
            ],
            "title": "Do image classifiers generalize across time",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Hwanjun Song",
                "Minseok Kim",
                "Dongmin Park",
                "Yooju Shin",
                "Jae-Gil Lee"
            ],
            "title": "Learning from noisy labels with deep neural networks: A survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Trischler",
                "Tong Wang",
                "Xingdi Yuan",
                "Justin Harris",
                "Alessandro Sordoni",
                "Philip Bachman",
                "Kaheer Suleman"
            ],
            "title": "Newsqa: A machine comprehension dataset",
            "venue": "arXiv preprint arXiv:1611.09830,",
            "year": 2016
        },
        {
            "authors": [
                "Yun-Yun Tsai",
                "Pin-Yu Chen",
                "Tsung-Yi Ho"
            ],
            "title": "Transfer learning without knowing: Reprogramming black-box machine learning models with scarce data and limited resources",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Vijay Vasudevan",
                "Benjamin Caine",
                "Raphael Gontijo-Lopes",
                "Sara Fridovich-Keil",
                "Rebecca Roelofs"
            ],
            "title": "When does dough become a bagel? analyzing the remaining mistakes on imagenet",
            "venue": "arXiv preprint arXiv:2205.04596,",
            "year": 2022
        },
        {
            "authors": [
                "Bastiaan S Veeling",
                "Jasper Linmans",
                "Jim Winkens",
                "Taco Cohen",
                "Max Welling"
            ],
            "title": "Rotation equivariant CNNs for digital pathology",
            "year": 2018
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2018
        },
        {
            "authors": [
                "Haohan Wang",
                "Songwei Ge",
                "Eric P. Xing",
                "Zachary C. Lipton"
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Yue Fan",
                "Wang Sun",
                "Ran Tao",
                "Wenxin Hou",
                "Renjie Wang",
                "Linyi Yang",
                "Zhi Zhou",
                "Lan-Zhe Guo",
                "Heli Qi",
                "Zhen Wu",
                "Yu-Feng Li",
                "Satoshi Nakamura",
                "Wei Ye",
                "Marios Savvides",
                "Bhiksha Raj",
                "Takahiro Shinozaki",
                "Bernt Schiele",
                "Jindong Wang",
                "Xing Xie",
                "Yue Zhang"
            ],
            "title": "Usb: A unified semi-supervised learning benchmark",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Qiang Heng",
                "Wenxin Hou",
                "Yue Fan",
                "Zhen Wu",
                "Jindong Wang",
                "Marios Savvides",
                "Takahiro Shinozaki",
                "Bhiksha Raj",
                "Bernt Schiele",
                "Xing Xie"
            ],
            "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Yidong Wang",
                "Zhuohao Yu",
                "Jindong Wang",
                "Qiang Heng",
                "Hao Chen",
                "Wei Ye",
                "Rui Xie",
                "Xing Xie",
                "Shikun Zhang"
            ],
            "title": "Exploring vision-language models for imbalanced learning",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2023
        },
        {
            "authors": [
                "Yidong Wang",
                "Bowen Zhang",
                "Wenxin Hou",
                "Zhen Wu",
                "Jindong Wang",
                "Takahiro Shinozaki"
            ],
            "title": "Margin calibration for long-tailed visual recognition",
            "venue": "In Asian Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Yisen Wang",
                "Xingjun Ma",
                "Zaiyi Chen",
                "Yuan Luo",
                "Jinfeng Yi",
                "James Bailey"
            ],
            "title": "Symmetric cross entropy for robust learning with noisy labels",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Jiaheng Wei",
                "Hangyu Liu",
                "Tongliang Liu",
                "Gang Niu",
                "Masashi Sugiyama",
                "Yang Liu"
            ],
            "title": "To smooth or not? when label smoothing meets noisy labels",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Jiaheng Wei",
                "Zhaowei Zhu",
                "Hao Cheng",
                "Tongliang Liu",
                "Gang Niu",
                "Yang Liu"
            ],
            "title": "Learning with noisy labels revisited: A study using real-world human annotations",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyue Wen",
                "Jiaye Teng",
                "Jingzhao Zhang"
            ],
            "title": "Benign overfitting in classification: Provably counter label noise with larger models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ross Wightman"
            ],
            "title": "Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Ross Wightman",
                "Hugo Touvron",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Resnet strikes back: An improved training procedure in timm",
            "venue": "arXiv preprint arXiv:2110.00476,",
            "year": 2021
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman"
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426,",
            "year": 2017
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong"
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Tao Qi",
                "Yongfeng Huang"
            ],
            "title": "Noisytune: A little noise can help you finetune pretrained language models better",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Tong Xiao",
                "Tian Xia",
                "Yi Yang",
                "Chang Huang",
                "Xiaogang Wang"
            ],
            "title": "Learning from massive noisy labeled data for image classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V. Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Shoukai Xu",
                "Jiangchao Yao",
                "Ran Luo",
                "Shuhai Zhang",
                "Zihao Lian",
                "Mingkui Tan",
                "Yaowei Wang"
            ],
            "title": "Towards efficient task-driven model reprogramming with foundation models",
            "venue": "arXiv preprint arXiv:2304.02263,",
            "year": 2023
        },
        {
            "authors": [
                "Yihao Xue",
                "Kyle Whitecross",
                "Baharan Mirzasoleiman"
            ],
            "title": "Investigating why contrastive learning benefits robustness against label noise",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Linyi Yang",
                "Shuibai Zhang",
                "Libo Qin",
                "Yafu Li",
                "Yidong Wang",
                "Hanmeng Liu",
                "Jindong Wang",
                "Xing Xie",
                "Yue Zhang"
            ],
            "title": "Glue-x: Evaluating natural language understanding models from an out-ofdistribution generalization perspective",
            "venue": "In Findings of ACL,",
            "year": 2023
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Seong Joon Oh",
                "Byeongho Heo",
                "Dongyoon Han",
                "Junsuk Choe",
                "Sanghyuk Chun"
            ],
            "title": "Re-labeling imagenet: from single to multi-labels, from global to localized labels",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jure Zbontar",
                "Li Jing",
                "Ishan Misra",
                "Yann LeCun",
                "St\u00e9phane Deny. Barlow"
            ],
            "title": "twins: Self-supervised learning via redundancy reduction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Zhang",
                "Yidong Wang",
                "Wenxin Hou",
                "Hao Wu",
                "Jindong Wang",
                "Manabu Okumura",
                "Takahiro Shinozaki"
            ],
            "title": "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Jieyu Zhang",
                "Bohan Wang",
                "Zhengyu Hu",
                "Pang Wei Koh",
                "Alexander Ratner"
            ],
            "title": "On the trade-off of intra-/inter-class diversity for supervised pre-training",
            "venue": "arXiv preprint arXiv:2305.12224,",
            "year": 2023
        },
        {
            "authors": [
                "Li Zhang",
                "Steven R Wilson",
                "Rada Mihalcea"
            ],
            "title": "Multi-label transfer learning for multi-relational semantic similarity",
            "venue": "arXiv preprint arXiv:1805.12501,",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Zhang",
                "Bingyi Kang",
                "Bryan Hooi",
                "Shuicheng Yan",
                "Jiashi Feng"
            ],
            "title": "Deep long-tailed learning: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Yivan Zhang",
                "Gang Niu",
                "Masashi Sugiyama"
            ],
            "title": "Learning noise transition matrix from only noisy labels via total variation regularization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Zhilu Zhang",
                "Mert Sabuncu"
            ],
            "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler"
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading",
            "venue": "books. 2015 IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Cherti"
            ],
            "title": "PRE-TRAINING DATASETS AND HYPER-PARAMETERS For analysis in Section 2, we conduct pre-training of ResNet-50 on ImageNet-1K and YFCC15M. For ImageNet-1K pre-training, we follow the training recipe in Wightman et al. (2021). To introduce noise in ImageNet-1K, we use function cleanlab (Northcutt et al., 2021) to introduce symmetric noise in each class",
            "venue": "For YFCC15M CLIP pre-training,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The transfer learning paradigm of pre-training and fine-tuning (PT-FT) (Kornblith et al., 2019) has become the de facto standard in today\u2019s deep learning research and application. Instead of training a neural network from scratch for each individual task, which can be time-consuming, resourceintensive, and less adaptable, the PT-FT paradigm first pre-trains a relatively larger and more general model with huge volumes of datasets, and then transfers this pre-trained model (or the foundation model (Bommasani et al., 2021)) to various downstream tasks (He et al., 2019; Radford et al., 2021; He et al., 2022; Brown et al., 2020). For instance, ResNet (He et al., 2016a) and Vision Transformers (Dosovitskiy et al., 2020) pre-trained on ImageNet (Russakovsky et al., 2015) and larger but potentially noisy datasets (Kolesnikov et al., 2020; Xie et al., 2020; Ridnik et al., 2021) have been widely adopted in computer vision. The PT-FT paradigm has also become predominant in natural language processing (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2018; 2019; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a) and multi-modality (Radford et al., 2021; Schuhmann et al., 2022), where the pre-training is usually on large datasets scraped from the web.\nThe generalization and transferability of the pre-trained models are usually not guaranteed to be satisfying on downstream tasks, and the reason can lie in either the pre-training or the fine-tuning. Over the years, there have been tremendous efforts in improving the performance of fine-tuning in various practical downstream scenarios: out-of-distribution generalization (Chen et al., 2021; Kumar et al., 2022), semi-supervised learning (Sohn et al., 2020; Wang et al., 2022), imbalanced learning (Zhang et al., 2023b; Wang et al., 2023b), noisy label learning (Song et al., 2022; Li et al., 2022),\n\u2217haoc3@andrew.cmu.edu, work done during a research intern at MSRA. \u2020Correspondence to: jindong.wang@microsoft.com\nto name a few. While it is a common belief that scaling up the size of the pre-training data can benefit the downstream performance (Kaplan et al., 2020), its distribution also plays an essential role (Entezari et al., 2023; Zhang et al., 2023a). Recently, Nguyen et al. (2022) and Lee et al. (2022) found that the quality of the pre-training data is more important for robust generalization compared to the quantity. The bias in pre-training data created by the collection (and annotation) process, e.g., corrupted, poisoned, and false information (Blodgett & O\u2019Connor, 2017; Chang et al., 2020), can also impose malicious and unexpected influence to downstream tasks (Bommasani et al., 2021).\nTake label noise as an example. Training CLIP (Radford et al., 2021) on LAION-2B (Schuhmann et al., 2022), which is a billion-scale uncurated image-text pair dataset, can just match the performance of training it on WIT-400M (Radford et al., 2021), which is heavily cleaned and processed by OpenAI. The label noise in large-scale datasets inevitably exists owing to the data collection process by human annotators and web crawlers. It thus can be difficult to avoid or eliminate in pre-training (Ridnik et al., 2021; Vasudevan et al., 2022; Schuhmann et al., 2022). In fact, there are already numerous models pre-trained on large-scale noisy data and have been transferred on downstream tasks, such as Noisy Student (Xie et al., 2020), BiT (Kolesnikov et al., 2020), and Open CLIP (Cherti et al., 2023). Not to mention the enormous but noisy raw text (Yang et al., 2019; Lee et al., 2022) that has been utilized to pre-train language models such as BERT (Devlin et al., 2018) and GPT (Radford et al., 2019; Brown et al., 2020). As the pre-trained models and datasets have been growing significantly, it has become increasingly important and challenging to understand how the noise in pre-training data affects the performance of pre-trained models on downstream tasks.\nThis paper presents the first study on this unexplored problem, demystifying the label noise in pre-training data, understanding its effects on downstream tasks, and then mitigating such (malignant) effects. Notably, there are existing efforts under the name of \u201cnoisy label learning\u201d that train a robust model given noisy training data (Ghosh et al., 2017; Li et al., 2020; Northcutt et al., 2021). Our problem is inherently different since the noisy labels exist in the (usually black-box) pre-training data, and we do not make noise assumptions on the downstream data (while they can be used together as in Section 4.3; more discussion is in Section 5). Due to the increasing size of pre-trained models and datasets, it becomes notoriously difficult to alter the pre-training process or fine-tune the entire models (black-box or cannot be updated due to the large parameter size and the constrained computation).1 Therefore, given a pre-trained model, we should take special care of the fine-tuning to overcome the influence of noise in pre-training on downstream tasks.\nOur study aims to answer the following key questions: 1) Influence: Does the noise in pretraining data have an influence on downstream\nperformance? 2) Analysis: Why does such influence happen? and 3) Mitigation: How to mitigate such influence in a light-weight and black-box fine-tuning process? We present an in-depth analysis to answer the above questions, based on the popular supervised pre-training paradigm.2\n\u2022 Influence: The label noise in pre-training data has both benevolent and malignant influence on downstream tasks. In Sections 2.1 and 2.2, we conduct realistic experiments\n1Llama (Touvron et al., 2023a;b) model requires multiple V100 GPUs to fine-tune, which is not affordable to most ordinary researchers; and proprietary models like ChatGPT cannot be locally fine-tuned.\n2Supervised and self-supervised learning are the most popular pre-training schemes. The former learns the mapping from inputs to labels (He et al., 2016a; Radford et al., 2021), while the latter does not rely on labels, but predicts parts of the data itself (Devlin et al., 2018; He et al., 2022).\nwith ResNet-50 models (He et al., 2016a) fully-supervised and contrastive pre-trainied on synthetic noisy ImageNet-1K and YFCC15M (Thomee et al., 2016) with various noisy ratios (0, 5%, 10%, 20%, 30%) and then study the generalization performance on the downstream indomain (ID) and out-of-domain (OOD) tasks. We observe that, on ID tasks, slight noise (up to 5% or 10%) can benefit generalization performance. In contrast, even 5% noise can drastically deteriorate robustness and transferability on OOD tasks, as shown in Figure 1 and Figure 2.\n\u2022 Analysis: The label noise in pre-training shapes the feature space significantly of the pretrained model. In Section 2.3, we conduct empirical analysis from the singular value spectrum on the feature space of the pre-trained models. Noise in pre-training results in the decreasing largest singular value and flatter singular value distribution with a higher dimension span in the feature space. An initial increase in the spanning dimension of the feature space is beneficial to the discriminability on ID tasks. Still, it then becomes detrimental with the further increase, indicating more feature capacities are learned to fit to noise structure. The decrease in the dominant singular values leads to less transferability for OOD tasks (Chen et al., 2019), as shown in Figure 3.\n\u2022 Mitigation: We design a simple black-box fine-tuning algorithm to reshape the pre-trained feature space, reducing the influence of noisy pre-training data and boost the performance of downstream tasks. In Section 3, based on the analysis, we propose three regularization objectives on the singular value spectrum that help affine the feature space. We demonstrate the effectiveness of the proposed method on noisy ResNet-50 models with extensive analysis, as shown in Figure 1. In Section 4, we further validate our method on popular noisy pre-trained models (and APIs) and present superior generalization performance for both vision and language tasks.\nBeyond our analysis, we view this research as a novel and complementary topic to the classic noisy label learning setting, termed as Noisy Model Learning (NML). We think the value of this direction is even more significant in the era of large foundation models (Bommasani et al., 2021), where the downstream users only have access to the model weights or APIs. It would be of particular interest to explore how to eliminate the malignant influence of noise in pre-training on downstream tasks when adapting these models without full fine-tuning, since it may exist in broader applications such as the detection and segmentation in medical and autonomous driving. We hope that future research on this topic can facilitate a better understanding and application of large foundation models."
        },
        {
            "heading": "2 UNDERSTANDING THE LABEL NOISE IN PRE-TRAINED MODELS",
            "text": "In this section, we empirically and systemically investigate the effect of noisy labels in the supervised pre-training on the learned representations. We build our evaluation and analysis on the realistic motivating experiments of training ResNet-50 (He et al., 2016a) on synthetic noisy ImageNet-1K (Russakovsky et al., 2015) and YFCC15M (a subset of YFCC100M (Thomee et al., 2016))."
        },
        {
            "heading": "2.1 EXPERIMENTS DESIGN",
            "text": "Noisy pre-training datasets. We assume that the supervised pre-training dataset consists of inputs x \u223c X and supervisions y \u223c Y . We define a clean dataset D = {(xi, yi)}i\u2208[N ] of size N with accurate supervisions, where [N ] := {1, . . . , N}. We assume that y can exist in different formats in pre-training, e.g., an actual label for the input as in fully-supervised learning (Russakovsky et al., 2015; He et al., 2016a; Ridnik et al., 2021) or a text description for an input image as in contrastive learning of CLIP (Thomee et al., 2016; Radford et al., 2021; Jia et al., 2021; Changpinyo et al., 2021; Desai et al., 2021; Schuhmann et al., 2021; 2022). Due to the scale of data collection and the cost of data annotation, the pre-training dataset can usually contain noisy supervision y\u0302 that does not accurately match the corresponding x (Recht et al., 2019; Beyer et al., 2020; Northcutt et al., 2021; Yun et al., 2021; Vasudevan et al., 2022; Schuhmann et al., 2022). We define such noisy pre-training dataset as D\u0302 = {(xi, y\u0302i)}i\u2208[N ] and the noise ratio \u03b3 as the percentage of noisy supervision in D\u0302. Pre-trained models. The pre-trained models serve as a foundation for downstream tasks and usually can be abstracted as the stack of a feature extractor and a projection head. We define the feature extractor with learned parameters \u03d5 as a mapping function f\u03d5 from the input space to feature space of dimension D: f\u03d5 : X \u2192 F . The projection head g\u03b8 : F \u2192 Y is jointly pre-trained with the feature extractor, but not used when adapting f\u03d5 on downstream tasks. We consider two types of supervised pre-training on images for this motivating example: fully supervised pre-training where y is the actual class label and the projection head is a linear classifier (He et al., 2016a), and contrastive pre-\ntraining with text supervision (CLIP) where y is the text and the projection is a non-linear function maps the image and text to a common feature space (Radford et al., 2021; Cherti et al., 2023).\nIn-domain (ID) and out-of-domain (OOD) evaluation. To investigate the effect of noisy supervision comprehensively, we leverage both in-domain (ID) and out-of-domain (OOD) evaluation to assess the generalization capability of the pre-trained feature extractor (Djolonga et al., 2021) f\u03b3\u03d5 that are obtained from the pre-training data of different noise ratios. To evaluate the pre-trained models on a downstream dataset D\u2032 = {(xi, yi)}i\u2208[M ]3 and measure the quality of the learned representation, we conduct linear probing (LP)4, where only a C-way linear classification head is re-trained on the downstream dataset and the feature extractor is frozen. The linear probing can be viewed as a simple black-box tuning method for pre-trained models that are typically large and difficult or unable to fully fine-tune. For ID evaluation, we assume the same marginal distribution over X for both training and testing. In contrast, for OOD evaluation, we train the linear classifier on a source distribution and evaluate it on (multiple) different target distributions (Kumar et al., 2022).\nExperiment setup. We use ImageNet-1K (IN-1K) (Russakovsky et al., 2015) in fully supervised pre-training and YFCC15M (Thomee et al., 2016) in CLIP pre-training, with ResNet-50 (He et al., 2016a). To introduce noisy supervision in the datasets, we uniformly flip the ground truth class label into the other classes in IN-1K and randomly swap the text description from another image-text pair in YFCC15M. We set the noise ratio \u03b3 to {0%, 5%, 10%, 20%, 30%}, where 0% represents the clean dataset. For ID evaluation, we use 14 downstream datasets including CIFAR-10/100 (Krizhevsky et al., 2009), Flowers102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), OxfordPet (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), FGVCAircraft (Maji et al., 2013), SVHN (Netzer et al., 2011), DTD (Cimpoi et al., 2014), Caltech101 (Fei-Fei et al., 2004), EuroSAT (Helber et al., 2018; 2019), PatchCamelyon (Veeling et al., 2018), RESISC45 (Cheng et al., 2017), and Rendered SST2 (Socher et al., 2013), which cover various visual domains. For OOD evaluation, we use the \u201creal\u201d, \u201csketch\u201d, \u201cinpainting\u201d, and \u201cclippart\u201d of DomainNet (Peng et al., 2019), where we train on either \u201creal\u201d or \u201csketch\u201d and evaluate on the others. For CLIP pre-trained models, we additionally use 6 ImageNet variants (Recht et al., 2019; Hendrycks et al., 2021a; Wang et al., 2019a; Hendrycks et al., 2021b; Shankar et al., 2021; Barbu et al., 2019) for OOD evaluation while train on ImageNet-1K. We report the LP performance for both ID and OOD evaluation using {10%, 25%, 50%, 75%, 100%} percentage of downstream datasets. The setup can be extended to other architectures and pre-training proxy objectives, as shown in Section 4. Our pre-training primarily follows Wightman et al. (2021) and Cherti et al. (2023), with similar performance achieved as shown in Appendix A.1. More details of the setup are included in Appendix A.2."
        },
        {
            "heading": "2.2 RESULTS",
            "text": "In Figure 2, we plot the average accuracy for ID and OOD tasks of adapting the IN-1K fully supervised and YFCC15M CLIP pre-trained ResNet-50 models. With the extensive motivating experiments, we empirically find two important and counter-intuitive observations from the results:\n3We always treat y \u2208 [C] as an actual class label on downstream datasets. 4Linear probing is an evaluation protocol accessing feature quality (He et al., 2019; Liu et al., 2021a).\n\u2022 Proper noisy labels in pre-training (e.g., 5% or 10%) can benefit the performance on ID downstream tasks, while more noise results in inferior results;\n\u2022 The robustness of transferability on OOD downstream tasks constantly deteriorates as the noise increases, even with the improvement in ID tasks on 5% noise.\nWhile prior arts in noisy label learning mainly aim to correct/eliminate the noise or perform robust learning against noise (Ghosh et al., 2017; Li et al., 2020; Liu et al., 2022a; Xue et al., 2022), we show that the noise in pre-training can have both benevolent and malignant effects on downstream tasks. These observations raise a natural and fundamental question: where does the superior transferability (with slight noise) and the inferior robustness stem from? We further analyze the feature space to understand the change in the pre-trained feature extractor caused by noise."
        },
        {
            "heading": "2.3 FEATURE SPACE ANALYSIS",
            "text": "To understand the noise in pre-training data, we empirically analyze the singular value spectrum of the pre-trained feature space on downstream datasets, which is widely considered to be related to the generalization performance (Oymak et al., 2019; Chen et al., 2019; Xue et al., 2022). More specifically, we perform singular value decomposition (SVD) on the features F \u2208 RM\u00d7D 5 of pretrained feature extractors on a downstream dataset: F = U\u03a3V\u22a4.6. We plot the singular values in Appendix A.4, based on which we define two metrics that can help understand the observations:\nDefinition 2.1 (Singular Value Entropy). The singular value entropy (SVE) is defined as the entropy of normalized singular values. SVE measures the flatness of the singular value distribution.\nSVE = \u2212 D\u2211 i=1 \u03c3i\u2211D j=1 \u03c3j log \u03c3i\u2211D j=1 \u03c3j\n(1)\nLarger SVE values indicate that the feature space captures more structure in the data and thus spans more dimensions either due to more discriminated features are learned or memorization of the noise.\nDefinition 2.2 (Largest Singular Value Ratio). The largest singular value ratio (LSVR) is defined as the logarithm of the ratio of the largest singular value \u03c31 to the summation of all singular values:\nLSVR = \u2212 log \u03c31\u2211D i=1 \u03c3i . (2)\nLSVR measures the variations in data captured by the singular vector corresponding to the largest singular value \u03c31, which relates to the transferability of a model (Chen et al., 2019).\n5We denote M as the number of samples in downstream datasets and D as the feature dimension. 6We assume D \u2264 M (Kumar et al., 2022). U and V denotes the left and right singular vector matrices,\nrespectively, and \u03a3 denoting the diagonal singular value matrix {\u03c31, . . . , \u03c3D}.\nAnalysis. We plot the SVE for ID tasks and LSVR for OOD tasks, as shown in Figure 3. For ID tasks, as the noise ratio slightly increases, the learned representation usually presents slightly higher SVE, which indicates the pre-trained feature extractor captures more structure in data. Specifically, more capabilities of the feature space are assigned to fit the noise in data, resulting in a feature space spanning more dimensions, which provides better-initialized features on downstream tasks and facilitates generalization. Similar observations have also been found and explored in Wu et al. (2022). However, as the noise ratio further increases, the increased SVE indicates that a more noisy data structure is captured and memorized, thus leading to deteriorated generalization performance. When the labels in pre-training are random, the SVE of the feature extractor would further increase by memorizing all the noise but not generalize on downstream tasks, similar to Zhang et al. (2021b). For OOD tasks, the robustness performance is negatively correlated with the LSVR. As the noise ratio increases, the LSVR consistently increases with the decreasing largest singular value. A less transferable component is learned, thus resulting in worse generalization on unseen OOD tasks."
        },
        {
            "heading": "3 MITIGATING THE NOISE WITH REGULARIZATION ON SINGULAR VALUES",
            "text": "In this section, we propose a black-box fine-tuning method, which we call \u201cNoisy Model Tuning\u201d (NMTune, Figure 4) in response to the noisy model learning setting. We demonstrate that NMTune can boost the generalization on downstream tasks and provide the analysis for the reasons behind."
        },
        {
            "heading": "3.1 METHOD",
            "text": "Per analysis above, noise in pre-training can shape the feature space differently from pre-training on clean data, reducing the top dominant singular values with dampened transferability while increasing the spanning dimensions of the feature space to fit noise structure. Since the large pre-trained models are usually difficult to fully fine-tune due to the enormous parameter size and limited computation resources, we propose to alter the pre-trained feature space F in a light-weight and black-box fashion. More specifically, we introduce a multi-layer perceptron (MLP) h\u03c9 transforming the pretrained features into new feature space Z . We propose three regularization terms on Z, to encourage the pre-trained knowledge to be maintained and improving SVE and LSVR of the new feature space.\nConsistency regularization. To encourage the consistency of the pre-trained knowledge, we adopt a mean-square-error (MSE) loss between the normalized features F and Z:\nLMSE = \u2225\u2225\u2225\u2225 F\u2225F\u22252 \u2212 Z\u2225Z\u22252 \u2225\u2225\u2225\u22252 2 . (3)\nThis objective facilitates inheriting the pre-trained knowledge in the transformed features Z.\nCovariance regularization. We define the covariance loss to encourage the off-diagonal elements in the covariance matrix of the transformed feature C(Z) to be close to 0:\nLCOV = 1\nD \u2211 i\u0338=j [C(Z)]2i,j , where C(Z) = 1 M \u2212 1 M\u2211 i=1 (zi \u2212 z\u0304) (zi \u2212 z\u0304)T , z\u0304 = 1 M M\u2211 i=1 zi. (4)\nInspired by Zbontar et al. (2021) and Bardes et al. (2022), we use the covariance regularization term to improve the SVE of feature space by preventing the different coordinates of the features from encoding similar information. It also encourages more discriminative features to be learned.\nDominant singular value regularization. To help transferability, we use a more specific regularization to improve the LSVR by directly maximizing the ratio of the largest singular value:\nLSVD = \u2212 \u03c31\u2211D j=1 \u03c3j . (5)\nIn summary, the total objective on a downstream task becomes:\nL = LCE + \u03bbLNMTune = LCE + \u03bb (LMSE + LCOV + LSVD) , (6)\nwhere LCE is the cross-entropy loss for downstream classification. We set \u03bb = 0.01 and use 2 layers MLP for all our experiments. Ablation study on MLP architecture and \u03bb are in Appendix B.7."
        },
        {
            "heading": "3.2 EVALUATION ON NOISY IMAGENET-1K AND YFCC15M",
            "text": "Here, we evaluate the proposed NMTune on the noisy models and analyze the reason for its effectiveness. We compare against solely training the MLP without the regularization, termed as MLP tuning, to show the effectiveness stems from the regularization rather than the extra parameters.\nFor ID tasks, we plot the average F1 score and SVE in Figures 5(a) and 5(b), respectively. The F1 score of linear probing (LP) on different pre-training noise ratios follows the same trend as the accuracy: it first increases as the noise ratio goes up to 5% and then decreases. While adding an MLP can improve the F1 score in general, we find that it cannot mitigate the effect of noise, i.e., the clean pre-trained model underperforms the 5% noisy pre-trained models. Further introducing our method can rectify the effect of noise on ID tasks, leading the clean pre-trained feature extractor to achieve the best results. More interestingly, only adding a MLP to LP can result in a smaller SVE, especially on ImageNet-1K, corresponding to a much sparser feature structure. In contrast, our method provides a larger and flatter SVE. It indicates the transformed feature space not only maintains the pre-trained knowledge but also spans more dimensions. For OOD tasks, the F1 score and LSVR are shown in Figure 5(c) and 5(d), respectively. Similarly, one can observe significantly better generalization performance deploying NMTune, compared to the MLP and LP. We also notice a smaller performance gap for the clean pre-trained feature extractor and 5% noisy pre-trained,\nespecially on YFCC15M. On LSVR, MLP tuning usually imposes larger LSVR compared to LP, presenting smaller dominant singular values. Considering MLP tuning also presents smaller SVE, its resulting feature space is expected to present a more long-tailed spectrum than the original feature space. Maximizing the dominant singular values results in better transferability for OOD tasks."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We further validate NMTune on practical large-scale vision and language models that are pre-trained on noisy data, and discuss the noisy label learning and running time analysis in this section."
        },
        {
            "heading": "4.1 VISION MODELS AND DATASETS",
            "text": "Setup. For vision models, we use ResNet152 (He et al., 2016a) with dimensions widened by a factor of two (ResNet152x2) fully supervised pre-trained on ImageNet-21K (Kolesnikov et al., 2020), Swin-L (Liu et al., 2021c) fully supervised pre-trained on ImageNet-21K, EfficientNet-B3 semisupervised pre-trained on noisy JFT-300M (Hinton et al., 2015; Chollet, 2017) and ImageNet-1K, and ViT-L (Dosovitskiy et al., 2020) and ConvNext-L (Liu et al., 2022c) contrastive pre-trained on noisy Laion-2B (Cherti et al., 2023). All pre-trained models are adapted from TIMM (Wightman, 2019). We evaluate the models on the 14 downstream ID and 4 OOD vision datasets as in Section 2. The details of hyper-parameters are shown in Appendix B.1 due to space limit.\nResults. We present the average accuracy and F1 score across different datasets with three runs on vision models in Table 1. Our method improves the quality of the noisy pre-trained features with better accuracy and F1 score on both ID and OOD vision tasks. A large margin on downstream task across different pre-training architectures and datasets is present by NMTune, demonstrating better feature is learned. Noteworthy is that, although the MLP tuning also improves the performance in general, its performance gain is much smaller compared to our method, showing the effectiveness of the proposed regularization terms on mitigating the malicious effect of noise and improving generalization. More detailed results with error bars for each dataset are shown in Appendix B.2."
        },
        {
            "heading": "4.2 LANGUAGE MODELS AND DATASETS",
            "text": "Setup. We evaluate BERT-L (Devlin et al., 2018), RoBERTa-L (Liu et al., 2019), and GPT-2 (Radford et al., 2019) on the GLUE (Wang et al., 2018) and GLUE-X (Yang et al., 2023) benchmarks for ID and OOD performance.. BERT-L and RoBERTa-L are pre-trained on the combination of the BooksCorpus data (Zhu et al., 2015) and English Wikipedia with uncompressed raw text. It is found that the raw pre-training data of BERT can be reduced from 16GB to 12GB with data cleaning (Yang et al., 2019). GPT-2 is pre-trained on WebText (Radford et al., 2019), a scraped web dataset from Common Crawl that contains low-quality raw texts. We also leverage OpenAI\u2019s API service \u201ctext-ada-002\u201d7. Details of the hyper-parameters and evaluation metrics are in Appendix B.3.\n7We cannot use larger and more recent language models such as LLaMA (Touvron et al., 2023a), since they are unable to fit in a single V100 GPU and we are unsure whether GLUE is in their training data.\nResults. In Table 2, NMTune consistently achieves the best generalization performance. It presents superior performance gain, especially on OOD tasks of GLUE-X. On the \u201ctext-ada-002\u201d model with only API access, it also outperforms LP significantly, demonstrating the necessity of mitigating the effect of noise for better generalization. Interestingly, on the ID tasks of GLUE, we also observe a smaller gap of MLP tuning method to LP even with more parameters, showing that the MLP alone may not mitigate the influence of noisy data in pre-training. Full results are in Appendix B.4."
        },
        {
            "heading": "4.3 DISCUSSION",
            "text": "Noisy model learning with noisy label learning. We explore another setting, where these two paradigms occur together with both the pre-training and fine-tuning containing label noise, as shown in Appendix B.5. Our exploration in synthetic noisy CIFAR-10/100 presents similar observations of LP and NMtune as in clean downstream datasets, and they can work closely to achieve better performance on downstream datasets with slight noise. Running time analysis. We present the average GPU hours of NMTune, MLP tuning, and LP in Appendix B.6, showing that it introduces negligible computation. The ablation study and architecture of MLP are shown in Appendix B.7. Finally, our results may not be comparable to white-box full fine-tuning results, which is acceptable since we perform black-box tuning and the feature extractors are frozen. Our goal is not to pursue the best but to offer insights and discuss new research possibilities in the era of foundation models."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Noisy label learning. Prior arts on noisy label learning mainly focus on how to train robust models or how to adapt clean pre-trained models on noisy (downstream) datasets from scratch, including robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019b; Ma et al., 2020), noise estimation (Xiao et al., 2015; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Northcutt et al., 2021; Li et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Zhang et al., 2021c; Liu et al., 2022a; Kim et al., 2021; Chen et al., 2023). Perhaps more close to our work is the line of understanding noisy label learning. Ghosh et al. (2017) looked at theoretical conditions for a loss function to be noise-tolerant. CIFAR-N (Wei et al., 2022b) was built to understand the realworld instance-dependent label noise. Cheng et al. (2023) proposed to mitigate the memorization of noise labels by analyzing the regularization between representations. Wen et al. (2022) provably verified the failure of benign overfitting with label noise. Xue et al. (2022) investigated the robustness of contrastive pre-training with noisy labels on downstream tasks. Our work differs from the noisy label learning paradigm by focusing on the effect of pre-training noise on downstream.\nPre-training and fine-tuning. Pre-training and fine-tuning is the dominant transfer learning paradigm that allows a pre-trained model to adapt to a new, but similar, dataset. Many techniques are proposed for better transfer performance on the new dataset when it contains distribution shift (Cheng et al., 2023), unlabeled data (Sohn et al., 2020; Zhang et al., 2021a; Wang et al., 2023a), imbalanced data (Kang et al., 2019; Wang et al., 2023c), and noisy data (Wei et al., 2022a; Xue et al., 2022). There are also much relevant work studying and processing the pre-training data for better transfer performance by diversity trade-off (Kaplan et al., 2020; Zhang et al., 2023a), data selection (Entezari et al., 2023), quality-quantity trade-off (Magar & Schwartz, 2022; Nguyen et al., 2022; Lee et al., 2022; Carlini et al., 2022; Gadre et al., 2023), and specified fine-tuning methods (Tsai et al., 2020; Kumar et al., 2022; Wortsman et al., 2022; Goyal et al., 2023; Xu et al., 2023). Parameter-efficient transfer learning (He et al., 2021; Oh et al., 2023) is lightweight paradigms by adding adapters (Houlsby et al., 2019), low rank approximation (Hu et al., 2021), or prompt tuning (Liu et al., 2022b; 2021b). However, they all assume the availability of pre-trained models while we deal with black-box models. They also do not consider the noise in pre-training data."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We presented Noisy Model Learning, a new research direction for understanding and mitigating the effect of label noise in pre-training on downstream tasks. Extensive experiments demonstrate that proper noise in pre-training can benefit in-domain tasks and hurt out-of-domain tasks. We then proposed NMTune to mitigate the malignant effect of noise and improve the generalization performance of various noisy pre-trained models and APIs. While being the first study in this area, the explored models are still relatively small-scale in terms of pre-training, and we only use ResNet50 for analytical experiments, due to the limited computing resources. We hope our work can inspire more researchers on this important and challenging topic in more practical settings."
        },
        {
            "heading": "ACKNOWLEDGMENT AND DISCLAIMER",
            "text": "Masashi Sugiyama was supported by the Institute for AI and Beyond, UTokyo. In this paper, we generated some noisy pre-training images using ImageNet-1K to thoroughly study the noisy pre-training data. Such noisy data indeed could have malignant influence on downstream tasks, according to our findings. The only purpose of conducting this research is to study the noisy pre-training data, but not to claim their instability in real applications. Additionally, all the generated noisy images and our pre-trained models based on these data are for research purpose only, and will be released per request."
        },
        {
            "heading": "CONTENTS",
            "text": ""
        },
        {
            "heading": "A Understanding the Noisy Labels in Pre-training Data 19",
            "text": "A.1 Pre-training Datasets and Hyper-parameters . . . . . . . . . . . . . . . . . . . . . 19\nA.2 Downstream Vision Datasets and Hyper-parameters . . . . . . . . . . . . . . . . . 19\nA.3 Detailed ID and OOD Linear Probing Results . . . . . . . . . . . . . . . . . . . . 20\nA.4 Detailed ID and OOD Singular Value Spectrum . . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "B Experiments 22",
            "text": "B.1 Detailed Setup for Vision Models Experiments . . . . . . . . . . . . . . . . . . . 22\nB.2 Detailed Results for Vision Models Experiments . . . . . . . . . . . . . . . . . . . 24\nB.3 Detailed Setup for Language Models Experiments . . . . . . . . . . . . . . . . . . 24\nB.4 Detailed Results for Language Models Experiments . . . . . . . . . . . . . . . . . 27\nB.5 Transferring on Noisy Downstream Datasets . . . . . . . . . . . . . . . . . . . . . 28\nB.6 Runtime Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nB.7 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
        },
        {
            "heading": "C More Discussions 30",
            "text": "C.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nC.2 Potential Failure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31"
        },
        {
            "heading": "A UNDERSTANDING THE NOISY LABELS IN PRE-TRAINING DATA",
            "text": "We provide additional experiment details for the motivating example of ResNet-50 in this section. We also present the detailed results on each downstream dataset for noisy pre-trained models on both ImageNet-1K and YFCC15M. The SVD plots on each dataset are also shown here."
        },
        {
            "heading": "A.1 PRE-TRAINING DATASETS AND HYPER-PARAMETERS",
            "text": "For analysis in Section 2, we conduct pre-training of ResNet-50 on ImageNet-1K and YFCC15M.\nFor ImageNet-1K pre-training, we follow the training recipe in Wightman et al. (2021). To introduce noise in ImageNet-1K, we use function cleanlab (Northcutt et al., 2021) to introduce symmetric noise in each class. For YFCC15M CLIP pre-training, we follow the training recipe in Cherti et al. (2023). To introduce noise in YFCC15M, we swap the text description between two randomly sampled image-text pairs until the noise ratio is achieved. We show the validation accuracy on ImageNet-1K of the noisy ResNet-50 models pre-trained on ImageNet-1K and zero-shot accuracy on ImageNet-1K of the noisy ResNet-50 models pre-trained on YFCC15M in Table 3. The results show that our pre-training achieves the state-of-the-art results (Wightman et al., 2021; Cherti et al., 2023), as a basis for our further analysis."
        },
        {
            "heading": "A.2 DOWNSTREAM VISION DATASETS AND HYPER-PARAMETERS",
            "text": "We present the details of the in-domain (ID) vision datasets in Table 4 and out-of-domain vision datasets Table 5. For ID, we conduct training on the training set and test on the validation set of the downstream dataset. For OOD on DomainNet (Peng et al., 2019), we conduct training on the training set of DomainNet Real or DomainNet Sketch, and test on all the other three DomainNet\ndatasets not used in training. For OOD on ImageNet (Russakovsky et al., 2015), we conduct training on ImageNet training split and test on its variants.\nTo transfer a pre-trained model, we use linear probing (LP) for analysis as shown in Section 2. We train the linear classifier for 30 epochs on each downstream dataset, using AdamW (Kingma & Ba, 2014) optimizer with a cosine scheduler. We do not use weight decay for linear probing and set the learning rate to 0.1 for all tasks."
        },
        {
            "heading": "A.3 DETAILED ID AND OOD LINEAR PROBING RESULTS",
            "text": "We present the detailed ID and OOD linear probing results we analyzed in Section 2 here.\nThe ImageNet-1K and YFCC15M pre-trained ID results are in Figure 6 and Figure 8 respectively. On all the datasets, we can observe that the 5% or 10% noise pre-trained models outperform the clean pre-trained models, no matter which pre-training dataset and method is used.\nThe OOD results are in Figure 7 and Figure 9 respectively. On the validation split of the training dataset (ID), the trend follows the ID observations, where 5% noisy pre-trained model is better. However, on the OOD datasets, the model performance deteriorates as noise increases."
        },
        {
            "heading": "A.4 DETAILED ID AND OOD SINGULAR VALUE SPECTRUM",
            "text": "We plot the singular value spectrum for ID datasets and OOD datasets of the noisy ResNet-50 models. To better visualize the spectrum, we split the singular values into three groups: the top 20, 20-500, and the remaining.\nThe singular value spectrum of the ID datasets is shown in Figure 11 and Figure 13 respectively. From 20-500 singular values visualization, we can observe that the noisy pre-trained models in general have larger singular values in this range, corresponding to a feature space that spans more of its coordinates. We summarize this visualization as the SVE introduced Section 2. Here, we provide\nmore explanation how to make Figure 3. First, each color and marker represents a different pretraining noise ratio. We plot the average accuracy of different percentage of downstream datasets and the SVD (or LSVR) of the downstream test data for each downstream task. Thus each points corresponds to a downstream task. The results of different pre-training noise ratio for each task are thus clustered together.\nWe also provide a zoom-in version for Figure 3(a) and Figure 3(b) for better visualization, as shown in Figure 10.\nThe singular value spectrum of the OOD datasets is shown in Figure 12 and Figure 14 respectively. From the top 20 singular values visualization, we can observe that the clean pre-trained model tends to present larger singular values in this range, especially the largest singular value. We connect this observation with the transferability performance on OOD tasks (Chen et al., 2019), and summarize it as LSVR introduced in Section 2."
        },
        {
            "heading": "B EXPERIMENTS",
            "text": "More details of experiments in Section 4 are shown here."
        },
        {
            "heading": "B.1 DETAILED SETUP FOR VISION MODELS EXPERIMENTS",
            "text": "We provide a more detailed setup of evaluation on practical vision models. First, we summarize the noisy pre-trained models with their pre-trained dataset, parameter size, and validation accuracy on\nImageNet-1K we used in Table 6. We use the same ID vision and OOD vision datasets as in Table 4 and Table 5 for evaluation. Each experiment is run with three random seeds.\nTable 6: Noisy vision models we evaluated.\nModel Pre-trained Data Pre-trained Method Param. Size (M)\nEfficientNet-B3 (Tan & Le, 2019) JFT-300M (Hinton et al., 2015) Noisy Student (Xie et al., 2020) 12.23 ResNetv2-152x2 (He et al., 2016b) ImageNet-21K (Ridnik et al., 2021) BiT Kolesnikov et al. (2020) 236.34\nSwin-L (Liu et al., 2021c) ImageNet-21K (Ridnik et al., 2021) Supervised (Liu et al., 2021c) 196.74 ViT-L (Dosovitskiy et al., 2020) Laion-2B (Schuhmann et al., 2022) CLIP (Radford et al., 2021) 304.20 ConvNext-L (Liu et al., 2022c) Laion-2B (Schuhmann et al., 2022) CLIP (Radford et al., 2021) 200.13\nWe mainly compare our method with MLP tuning and LP, where we fine-tuning the modules using AdamW (Kingma & Ba, 2014) for 30 epochs with a cosine learning rate scheduler. We set the learning rate as 0.1 and weight decay of 0 for LP, and set the learning rate as 0.001 and weight decay of 1e\u22124 for MLP tuning and our method."
        },
        {
            "heading": "B.2 DETAILED RESULTS FOR VISION MODELS EXPERIMENTS",
            "text": "More results on each evaluated dataset are provided here. The ID results with standard deviation in accuracy on each ID datasets are shown in Table 7, and the OOD results with standard deviation in accuracy on the evaluated OOD datasets are shown in Table 8."
        },
        {
            "heading": "B.3 DETAILED SETUP FOR LANGUAGE MODELS EXPERIMENTS",
            "text": "The model details for natural language processing are shown in Table 9. We did not leverage larger language models mainly due to the limited computational resources. The recent open-sourced language models, such as Llama, have been trained on a very large-scale corpus of the web, evaluating them on GLUE and GLUE-X has the possibility to impose the problem of performing testing on the training samples.\nTable 8: Evaluation of our method on vision models in practice that are pre-trained on noisy datasets. We compare different methods on 4 DomainNet datasets for out-of-domain (OOD) evaluation. We perform training on either DomainNetSketch or DomainNetReal, and evaluate on DomainNetSketch, DomainNetReal, DomainNetPaining, DomainNetClipart without the training set.\nPre-trained Model Tuning Method DomainNet Sketch DomainNet Real Avg\nJFT-300M Semi-Supervised EfficientNet-B3 LP 46.22\u00b16.17 42.03\u00b17.07 44.13 MLP 48.29\u00b15.78 43.61\u00b17.53 45.95 Ours 48.84\u00b15.63 44.83\u00b17.45 46.84\nImageNet-21K Fully Supervised ResNetv2-152x2 LP 41.09\u00b14.81 40.55\u00b17.89 40.82 MLP 41.98\u00b15.08 41.47\u00b17.45 41.73 Ours 42.68\u00b14.85 42.15\u00b17.23 42.42\nImageNet-21K Fully Supervised Swin-L LP 54.57\u00b17.70 47.19\u00b17.45 50.88 MLP 53.81\u00b16.01 48.60\u00b18.12 51.21 Ours 55.68\u00b15.48 49.01\u00b16.15 52.35\nLaion-2B CLIP ConvNext-L\nLP 66.66\u00b17.69 67.05\u00b14.58 66.86 MLP 66.78\u00b17.00 70.07\u00b14.90 68.43 Ours 69.74\u00b16.82 70.85\u00b14.77 70.30\nLaion-2B CLIP ViT-L\nLP 67.00\u00b16.94 66.77\u00b14.74 66.89 MLP 68.99\u00b16.59 70.00\u00b14.65 69.50 Ours 70.48\u00b16.91 70.45\u00b14.98 70.47\nTable 9: Noisy language models we evaluated.\nModel Pre-trained Data Pre-trained Method\nBERT-L (Devlin et al., 2018) BooksCorpus and d English Wikipedia Masked Modeling (Devlin et al., 2018) RoBERTa-L (Liu et al., 2019) BooksCorpus and d English Wikipedia Masked Modeling (Liu et al., 2019) GPT-2 (Radford et al., 2019) WebText Autoregression\ntext-ada-002 - -\nNow, we present the dataset details here used in our analysis. For ID evaluation, we use CoLA, SST2, MRPC, STS-B, QQP, MNLI, QNLI, and RTE tasks of GLUE benchmark (Wang et al., 2018), as shown in Table 10. For OOD evaluation, following GLUE-X, we use Grammar Test (Yang et al., 2023) for CoLA, IMDB (Maas et al., 2011) for SST-2, QQP for MRPC, MNLI mismatched (Williams et al., 2017), SNLI (Bowman et al., 2015), SICK (Zhang et al., 2018) for MNLI, Reconstructed NewsQA (Trischler et al., 2016) for QNLI, SciTail (Khot et al., 2018) and HANS (McCoy et al., 2019) for RTE, as shown in Table 11.\nWe use the AdamW optimizer and set the learning rate for LP as 0.01 and for others as 0.001 for all the experiments of language models. For LP, we do not use weight decay, and for others we use a weight decay of 0.0001. All tuning methods are trained for 10 epochs with a linear learning rate scheduler."
        },
        {
            "heading": "B.4 DETAILED RESULTS FOR LANGUAGE MODELS EXPERIMENTS",
            "text": "The detailed ID and OOD results of language models evaluation are shown in Table 12 and Table 13 respectively. NMTune outperforms LP and MLP tuning across all the tasks, whereas MLP tuning sometimes fall short than LP, demonstrating the necessitate of using the proposed regularization terms to help mitigate the effect of noise in pre-training and improve generalization performance."
        },
        {
            "heading": "B.5 TRANSFERRING ON NOISY DOWNSTREAM DATASETS",
            "text": "We additionally study the setting where both pre-training and downstream datasets contain noisy labels. For pre-training noise, we use the ResNet-50 models pre-trained on noisy ImageNet-1K and YFCC15M with different noise ratios \u03b3 \u2208 {0%, 5%, 10%, 20%, 30%}, as in Section 2. For downstream noise, we adopt synthetic noise CIFAR-10 and CIFAR-100 which are usually used in noisy label learning (Liu et al., 2022a; Chen et al., 2023). We generate symmetric label noise by uniformly flipping labels for a percentage of the training set for all classes. We denote the noise ratio of downstream datasets as \u03b7, and set it to {0%, 10%, 20%, 30%, 40%, 50%}. We compare LP and NMTune in this setting, as shown in Figure 15 and Figure 16, respectively.\nOn the LP results in Figure 15, we find similar observations as our analysis in Section 2, where the 5% and 10% noisy pre-trained models usually outperforms the clean pre-trained model on downstream tasks, even the downstream tasks contain different level of noise. It indicates that the same conclusion from our main paper may extend and generalize to noisy downstream tasks, which highlights the importance of the proposed new topic - Noisy Model Learning - as the complementary to noisy label learning.\nMore importantly, we find that the proposed NMTune method has similar mitigation effect on noisy downstream tasks as the clean ones. On the NMTune results in Figure 16, we show that the clean pre-trained models now produce superior performance compared to noisy pre-trained models by utilizing the proposed regularization terms. It also improves the general performance when the noise ratio in downstream tasks is light, e.g., smaller than 40%. When the noise ratio in downstream tasks further increases, the performance of NMTune fall shorts to LP, which is acceptable because the regularization terms are not designed to be noise-tolerant. Noteworthy is that, even with slightly worse performance than LP, the performance of clean pre-trained mode still stays the best with NMTune. Devising NMTune to be more noise-tolerant on downstream tasks and experiments on practical asymmetric and instance-dependent noise (Wei et al., 2022b) would be very interesting and leave for the future exploration."
        },
        {
            "heading": "B.6 RUNTIME ANALYSIS",
            "text": "The runtime analysis for NMTune, in comparison to LP and MLP tuning is shown in Table 14. All of our experiments on downstream are conducted on single NVIDIA V100 GPU. Thus we report the average GPU hours for running the ID and OOD evaluation of vision and language tasks. From the results, the proposed NMTune introduces minimal computation, compared to MLP with the exactly the same parameters. The additional computation burden may involve in the SVD calculation and the covariance matrix calculation on the features."
        },
        {
            "heading": "B.7 ABLATION STUDY",
            "text": "The ablation study of NMTune is present here, where we run evaluation on the ID vision datasets. We use three ResNet-50 models from ImageNet-1K pre-training and YFCC15M pre-training for ablation, including the clean pre-trained, 5% noise pretrained, and 10% noise pretrained.\nWe study the MLP architecture, more specifically, the non-linearity and the number of layers in MLP in Table 15. From the results, one can observe that removing the non-linearity reduces the performance significantly. Adding more layers only improves the performance slightly but introduces much more parameters. Thus we adopt the 2-layer MLP architecture with ReLU activation. The overall structure is shown in Figure 17.\nWe also conduct ablation on the loss weight of different regularization terms we proposed in Table 16. From the results, we find that the proposed covariance regularization LCOV in general rectifies the effect of noise, improving the performance of clean pre-trained models to achieve better results than noisy pre-trained models. We can also observe that the dominant singular value regularization LSVD helps improve generalization. Solely adding LMSE or LSVD does not produces this behavior and yields slight worse results."
        },
        {
            "heading": "C MORE DISCUSSIONS",
            "text": "More discussions about our work are provided here."
        },
        {
            "heading": "C.1 LIMITATIONS",
            "text": "The limitation mainly lies in our empirical study of the noise in pre-training. Due to the limited computing resources, we could only conduct experiments on reltively small scale backbone and datasets, while most of the SOTA foundation models are of much more parameters and are trained on much larger datasets. Also, the empirical experiments is limited to actual supervised pre-training. Other pre-training objectives will be explored in our future work. That being said, we do believe the observation and conclusions from our practical experiments could scale and extend to larger datasets, stronger backbones, and other training objectives."
        },
        {
            "heading": "C.2 POTENTIAL FAILURE",
            "text": "We do observe some failure cases of the proposed methods. For example, from the results in Table.7, the proposed method falles short to LP on Caltech101 on almost all backbones we studied, while improving over MLP. Our hypothesis for the failure is that the SVD regularization term in the proposed method might need to optimize the top-K singular values instead of just the largest one. The optimal value of K might be different dataset. However, setting K = 1 can already achieves reasonable performance for most of the tasks."
        }
    ],
    "year": 2024
}