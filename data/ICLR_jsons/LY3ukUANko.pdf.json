{
    "abstractText": "Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. We analyze each model\u2019s ability to recall a token previously mentioned in-context, e.g. Hakuna Matata means no worries Hakuna Matata it means no \u2192 ??. On this task, termed associative recall, we find that attention outperforms gatedconvolutions: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions are capable of perfectly solving a synthetic version of AR. To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention, while maintaining subquadratic scaling.",
    "authors": [],
    "id": "SP:86bf0b187f5c11496ddb3bd26c15f8fd29ef768f",
    "references": [
        {
            "authors": [
                "Daniel Y. Fu",
                "Tri Dao",
                "Khaled K. Saab",
                "Armin W. Thomas",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "Hungry Hungry Hippos: Towards language modeling with state space models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Chunting Zhou",
                "Xiang Kong",
                "Junxian He",
                "Liangke Gui",
                "Graham Neubig",
                "Jonathan May",
                "Zettlemoyer Luke"
            ],
            "title": "Mega: Moving average equipped gated attention",
            "venue": "arXiv preprint arXiv:2209.10655,",
            "year": 2022
        },
        {
            "authors": [
                "Junxiong Wang",
                "Jing Nathan Yan",
                "Albert Gu",
                "Alexander M Rush"
            ],
            "title": "Pretraining without attention",
            "venue": "arXiv preprint arXiv:2212.10544,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Poli",
                "Stefano Massaroli",
                "Eric Nguyen",
                "Daniel Y Fu",
                "Tri Dao",
                "Stephen Baccus",
                "Yoshua Bengio",
                "Stefano Ermon",
                "Christopher R\u00e9"
            ],
            "title": "Hyena hierarchy: Towards larger convolutional language models",
            "venue": "arXiv preprint arXiv:2302.10866,",
            "year": 2023
        },
        {
            "authors": [
                "Yann N Dauphin",
                "Angela Fan",
                "Michael Auli",
                "David Grangier"
            ],
            "title": "Language modeling with gated convolutional networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Albert Gu",
                "Karan Goel",
                "Christopher R\u00e9"
            ],
            "title": "Efficiently modeling long sequences with structured state spaces",
            "venue": "arXiv preprint arXiv:2111.00396,",
            "year": 2021
        },
        {
            "authors": [
                "Bo Peng",
                "Eric Alcaide",
                "Quentin Anthony",
                "Alon Albalak",
                "Samuel Arcadinho",
                "Huanqi Cao",
                "Xin Cheng",
                "Michael Chung",
                "Matteo Grella",
                "Kranthi Kiran GV",
                "Xuzheng He",
                "Haowen Hou",
                "Przemyslaw Kazienko",
                "Jan Kocon",
                "Jiaming et al. Kong"
            ],
            "title": "Rwkv: Reinventing rnns for the transformer era",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Y. Fu",
                "Simran Arora",
                "Jessica Grogan",
                "Isys Johnson",
                "Sabri Eyuboglu",
                "Armin W. Thomas",
                "Benjamin Spector",
                "Michael Poli",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "Monarch mixer: A simple sub-quadratic gemm-based architecture, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Ivo Danihelka"
            ],
            "title": "Neural turing machines",
            "venue": "arXiv preprint arXiv:1410.5401,",
            "year": 2014
        },
        {
            "authors": [
                "Jimmy Ba",
                "Geoffrey E Hinton",
                "Volodymyr Mnih",
                "Joel Z Leibo",
                "Catalin Ionescu"
            ],
            "title": "Using fast weights to attend to the recent past",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Nelson Elhage",
                "Neel Nanda",
                "Catherine Olsson",
                "Tom Henighan",
                "Nicholas Joseph",
                "Ben Mann",
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Tom Conerly"
            ],
            "title": "A mathematical framework for transformer circuits",
            "venue": "Transformer Circuits Thread,",
            "year": 2021
        },
        {
            "authors": [
                "Catherine Olsson",
                "Nelson Elhage",
                "Neel Nanda",
                "Nicholas Joseph",
                "Nova DasSarma",
                "Tom Henighan",
                "Ben Mann",
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen"
            ],
            "title": "In-context learning and induction heads",
            "venue": "arXiv preprint arXiv:2209.11895,",
            "year": 2022
        },
        {
            "authors": [
                "Shahar Lutati",
                "Itamar Zimerman",
                "Lior Wolf"
            ],
            "title": "Focus your attention (with adaptive iir filters)",
            "year": 2023
        },
        {
            "authors": [
                "Michael Hahn"
            ],
            "title": "Theoretical limitations of self-attention in neural sequence models",
            "venue": "In Transactions of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "William Merrill",
                "Ashish Sabharwal",
                "Noah A. Smith"
            ],
            "title": "Saturated transformers are constant-depth threshold circuits",
            "venue": "In Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Feyza Duman Keles",
                "Pruthuvi Mahesakya Wijewardena",
                "Chinmay Hegde"
            ],
            "title": "On the computational complexity of self-attention",
            "venue": "In 34th International Conference on Algorithmic Learning Theory,",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The Pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "James W Cooley",
                "John W Tukey"
            ],
            "title": "An algorithm for the machine calculation of complex fourier series",
            "venue": "Mathematics of computation,",
            "year": 1965
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Donald Metzler"
            ],
            "title": "Efficient transformers: A survey",
            "venue": "ACM Computing Surveys,",
            "year": 2022
        },
        {
            "authors": [
                "Karan Goel",
                "Albert Gu",
                "Chris Donahue",
                "Christopher R\u00e9"
            ],
            "title": "It\u2019s raw! audio generation with statespace models",
            "venue": "Proceedings of the 39 th International Conference on Machine Learning,,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Zhang",
                "Khaled Saab",
                "Michael Poli",
                "Tri Dao",
                "Karan Goel",
                "Christopher R\u00e9"
            ],
            "title": "Effectively modeling time series with simple discrete state spaces",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Shuangfei Zhai",
                "Walter Talbott",
                "Nitish Srivastava",
                "Chen Huang",
                "Hanlin Goh",
                "Ruixiang Zhang",
                "Josh Susskind"
            ],
            "title": "An attention free transformer",
            "venue": "arXiv preprint arXiv:2105.14103,",
            "year": 2021
        },
        {
            "authors": [
                "Ramin Hasani",
                "Mathias Lechner",
                "Tsun-Huang Wang",
                "Makram Chahine",
                "Alexander Amini",
                "Daniela Rus"
            ],
            "title": "Liquid structural state-space models",
            "venue": "arXiv preprint arXiv:2209.12951,",
            "year": 2022
        },
        {
            "authors": [
                "Yutao Sun",
                "Li Dong",
                "Shaohan Huang",
                "Shuming Ma",
                "Yuqing Xia",
                "Jilong Xue",
                "Jianyong Wang",
                "Furu Wei"
            ],
            "title": "Retentive network: A successor to transformer for large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Sabri Eyuboglu",
                "Maya Varma",
                "Khaled Saab",
                "Jean-Benoit Delbrouck",
                "Christopher Lee-Messer",
                "Jared Dunnmon",
                "James Zou",
                "Christopher R\u00e9"
            ],
            "title": "Domino: Discovering systematic errors with crossmodal embeddings",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "John J Hopfield"
            ],
            "title": "Neural networks and physical systems with emergent collective computational abilities",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 1982
        },
        {
            "authors": [
                "P. B\u00fcrgisser",
                "T. Lickteig",
                "M. Clausen",
                "A. Shokrollahi"
            ],
            "title": "Algebraic Complexity Theory. Grundlehren der mathematischen Wissenschaften",
            "venue": "URL https://books.google.com/books?id=dYcgjfXsYk8C",
            "year": 1996
        },
        {
            "authors": [
                "Tri Dao",
                "Nimit S Sohoni",
                "Albert Gu",
                "Matthew Eichhorn",
                "Amit Blonder",
                "Megan Leszczynski",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "Kaleidoscope: An efficient, learnable representation for all structured linear maps",
            "venue": "arXiv preprint arXiv:2012.14966,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Y. Fu",
                "Elliot L. Epstein",
                "Eric Nguyen",
                "Armin W. Thomas",
                "Michael Zhang",
                "Tri Dao",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "Simple hardware-efficient long convolutions for sequence modeling",
            "venue": "arXiv preprint arXiv:2302.06646,",
            "year": 2023
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang"
            ],
            "title": "Big bird: Transformers for longer sequences",
            "venue": "Proceedings of NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509,",
            "year": 2019
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150,",
            "year": 2020
        },
        {
            "authors": [
                "Liliang Ren",
                "Yang Liu",
                "Shuohang Wang",
                "Yichong Xu",
                "Chenguang Zhu",
                "ChengXiang Zhai"
            ],
            "title": "Sparse modular activation for efficient sequence modeling",
            "venue": "arXiv preprint arXiv:2306.11197,",
            "year": 2023
        },
        {
            "authors": [
                "Tri Dao",
                "Daniel Y. Fu",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tri Dao"
            ],
            "title": "FlashAttention-2: Faster attention with better parallelism and work partitioning",
            "year": 2023
        },
        {
            "authors": [
                "A. Katharopoulos",
                "A. Vyas",
                "N. Pappas",
                "F. Fleuret"
            ],
            "title": "Transformers are rnns: Fast autoregressive transformers with linear attention",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML), 2020a. URL https://arxiv.org/abs/2006.16236",
            "year": 2006
        },
        {
            "authors": [
                "Chris Olah"
            ],
            "title": "Mechanistic interpretability, variables, and the importance of interpretable bases: An informal note on some intuitions related to mechanistic interpretability, 2022",
            "venue": "URL https: //transformer-circuits.pub/2022/mech-interp-essay/index.html",
            "year": 2022
        },
        {
            "authors": [
                "Alethea Power",
                "Yuri Burda",
                "Harri Edwards",
                "Igor Babuschkin",
                "Vedant Misra"
            ],
            "title": "Grokking: Generalization beyond overfitting on small algorithmic datasets",
            "year": 2022
        },
        {
            "authors": [
                "Dingquan Wang",
                "Jason Eisner"
            ],
            "title": "The galactic dependencies treebanks: Getting more data by synthesizing new languages",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2016
        },
        {
            "authors": [
                "Jennifer C White",
                "Ryan Cotterell"
            ],
            "title": "Examining the inductive bias of neural language models with artificial languages",
            "venue": "arXiv preprint arXiv:2106.01044,",
            "year": 2021
        },
        {
            "authors": [
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li"
            ],
            "title": "Physics of language models: Part 1, context-free grammar",
            "venue": "arXiv preprint arXiv:2305.13673,",
            "year": 2023
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yoav Goldberg",
                "Tal Linzen"
            ],
            "title": "Studying the inductive biases of rnns with synthetic variations of natural languages",
            "venue": "arXiv preprint arXiv:1903.06400,",
            "year": 2019
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Aditi Raghunathan",
                "Percy Liang",
                "Tengyu Ma"
            ],
            "title": "An explanation of in-context learning as implicit bayesian inference",
            "venue": "arXiv preprint arXiv:2111.02080,",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Kitaev",
                "\u0141ukasz Kaiser",
                "Anselm Levskaya"
            ],
            "title": "Reformer: The efficient transformer",
            "venue": "arXiv preprint arXiv:2001.04451,",
            "year": 2020
        },
        {
            "authors": [
                "JA Feldman",
                "GE Hinton",
                "JA Anderson"
            ],
            "title": "Parallel models of associative memory, 1981",
            "year": 1981
        },
        {
            "authors": [
                "Wei Zhang",
                "Bowen Zhou"
            ],
            "title": "Learning to update auto-associative memory in recurrent neural networks for improving sequence memorization",
            "venue": "arXiv preprint arXiv:1709.06493,",
            "year": 2017
        },
        {
            "authors": [
                "Stefano Massaroli",
                "Michael Poli",
                "Daniel Y Fu",
                "Hermann Kumbong",
                "David Romero",
                "Rom Parnichukun",
                "Aman Timalsina",
                "Quinn McIntyre",
                "Beidi Chen",
                "Atri Rudra",
                "Ce Zhang",
                "Christopher R\u00e9",
                "Stefano Ermon",
                "Yoshua Bengio"
            ],
            "title": "Laughing hyena distillery: Extracting compact recurrences from convolutions",
            "year": 2023
        },
        {
            "authors": [
                "David W. Romero",
                "Anna Kuzina",
                "Erik J. Bekkers",
                "Jakub M. Tomczak",
                "Mark Hoogendoorn"
            ],
            "title": "Ckconv: Continuous kernel convolution for sequential data",
            "year": 2022
        },
        {
            "authors": [
                "Ankit Gupta",
                "Albert Gu",
                "Jonathan Berant"
            ],
            "title": "Diagonal state spaces are as effective as structured state spaces, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Albert Gu",
                "Ankit Gupta",
                "Karan Goel",
                "Christopher R\u00e9"
            ],
            "title": "On the parameterization and initialization of diagonal state space models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Harsh Mehta",
                "Ankit Gupta",
                "Ashok Cutkosky",
                "Behnam Neyshabur"
            ],
            "title": "Long range language modeling via gated state spaces, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jimmy T.H. Smith",
                "Andrew Warrington",
                "Scott W. Linderman"
            ],
            "title": "Simplified state space layers for sequence modeling, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Eric Nguyen",
                "Michael Poli",
                "Marjan Faizi",
                "Armin Thomas",
                "Callum Birch-Sykes",
                "Michael Wornow",
                "Aman Patel",
                "Clayton Rabideau",
                "Stefano Massaroli",
                "Yoshua Bengio",
                "Stefano Ermon",
                "Stephen A. Baccus",
                "Chris R\u00e9"
            ],
            "title": "Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jue Wang",
                "Wentao Zhu",
                "Pichao Wang",
                "Xiang Yu",
                "Linda Liu",
                "Mohamed Omar",
                "Raffay Hamid"
            ],
            "title": "Selective structured state-spaces for long-form video understanding",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Brandon Yang",
                "Gabriel Bender",
                "Quoc V. Le",
                "Jiquan Ngiam"
            ],
            "title": "Condconv: Conditionally parametrized convolutions for efficient inference",
            "venue": "In 33rd Conference on Neural Information Processing Systems (NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Chrysoula Kosma",
                "Giannis Nikolentzos",
                "Michalis Vazirgiannis"
            ],
            "title": "Time-parameterized convolutional neural networks for irregularly sampled time series",
            "year": 2023
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Apoorv Vyas",
                "Nikolaos Pappas",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Transformers are rnns: Fast autoregressive transformers with linear attention, 2020b",
            "year": 2020
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Adaptive attention span in transformers",
            "venue": "Association of Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509,",
            "year": 2019
        },
        {
            "authors": [
                "Jiezhong Qiu",
                "Hao Ma",
                "Omer Levy",
                "Scott Wen tau Yih",
                "Sinong Wang",
                "Jie Tang"
            ],
            "title": "Blockwise self-attention for long document understanding",
            "venue": "arXiv preprint arXiv:1911.02972,",
            "year": 1911
        },
        {
            "authors": [
                "Michael T Heideman",
                "C Sidney Burrus"
            ],
            "title": "Multiplicative complexity, convolution, and the DFT",
            "year": 1988
        },
        {
            "authors": [
                "Ilya Volkovich"
            ],
            "title": "A guide to learning arithmetic circuits",
            "venue": "In Conference on Learning Theory,",
            "year": 2016
        },
        {
            "authors": [
                "Michael Poli",
                "Stefano Massaroli",
                "Eric Nguyen",
                "Daniel Y Fu",
                "Tri Dao",
                "Stephen Baccus",
                "Yoshua Bengio",
                "Stefano Ermon",
                "Christopher R\u00e9"
            ],
            "title": "Hyena hierarchy: Towards larger convolutional language models",
            "venue": "arXiv preprint arXiv:2302.10866,",
            "year": 2023
        },
        {
            "authors": [
                "Thathachar S Jayram",
                "Ravi Kumar",
                "Dandapani Sivakumar"
            ],
            "title": "The one-way communication complexity of hamming distance",
            "venue": "Theory of Computing,",
            "year": 2008
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A Smith",
                "Mike Lewis"
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "arXiv preprint arXiv:2108.12409,",
            "year": 2021
        },
        {
            "authors": [
                "Selim G Akl",
                "Henk Meijer"
            ],
            "title": "Parallel binary search",
            "venue": "IEEE Transactions on Parallel & Distributed Systems,",
            "year": 1990
        },
        {
            "authors": [
                "Thomas H Cormen",
                "Charles E Leiserson",
                "Ronald L Rivest",
                "Clifford Stein"
            ],
            "title": "Introduction to algorithms",
            "venue": "MIT press,",
            "year": 2022
        },
        {
            "authors": [
                "Mikl\u00f3s Ajtai",
                "J\u00e1nos Koml\u00f3s",
                "Endre Szemer\u00e9di"
            ],
            "title": "An 0 (n log n) sorting network",
            "venue": "In Proceedings of the fifteenth annual ACM symposium on Theory of computing,",
            "year": 1983
        },
        {
            "authors": [
                "Chris Chatfield"
            ],
            "title": "The analysis of time series: An introduction, fifth edition",
            "year": 1995
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Two advances \u2013 gating and long convolutions \u2013 have catalyzed a wave of excitement around gatedconvolution language models (Fu et al., 2023a; Ma et al., 2022; Wang et al., 2022; Poli et al., 2023a, inter alia.). These architectures combine gating (i.e. element-wise multiplication) with long convolutional filters (i.e. the length of the sequence) to enable interactions between distant tokens (Dauphin et al., 2017; Gu et al., 2021). There has been significant interest in gated convolution architectures since not only can they achieve asymptotically improved scaling in input sequence length compared to attention, but recent work suggests the models can also match attention in language modeling quality (Poli et al., 2023a; Peng et al., 2023; Fu et al., 2023b).\nWe pretrain and evaluate 17 language models across 4 scales (70M - 1.4Bn) and 5 architectures on the same data and infrastructure setup. We surprisingly find that there is still a perplexity gap of up to 2.1 points between state-of-the-art convolution-based architectures and Transformer baselines in language modeling on the Pile (Table 1). Through fine-grained analysis, we find a single, simple issue is responsible for much of the gap: recalling information seen in-context. For example: Hakuna Matata!\ufe38 \ufe37\ufe37 \ufe38\nKey-Value\nIt means no worries\ufe38 \ufe37\ufe37 \ufe38 Key-Value for the rest of your days! Hakuna\ufe38 \ufe37\ufe37 \ufe38 Query Matata\ufe38 \ufe37\ufe37 \ufe38 AR Hit means no\ufe38\ufe37\ufe37\ufe38 Query \u2192 worries\ufe38 \ufe37\ufe37 \ufe38 AR Hit\nWe find that errors on \u201cAR Hits\u201d (e.g. worries above) account for 82% of the perplexity gap to attention on average, despite only representing 6.4% of all tokens in the Pile dataset.1 A 1.4Bn parameter Hyena gated convolution model fails to close the AR gap to a 70M parameter (20\u00d7 smaller) Transformer (Table 1, Table 5). The AR gap persists at the 7Bn parameter scale when comparing RWKV and Llama-2 (Appendix G).\nThis task, associative recall (AR), has a long history in machine learning (Graves et al., 2014; Ba et al., 2016, inter alia.) (Appendix A.1). Prior work argues that a model\u2019s ability to perform\n1We measure AR on real data using a simple heuristic: n-gram tokens that are repeated in context.\nAR is predictive of in-context learning quality (Elhage et al., 2021; Olsson et al., 2022). As a result, AR has been adopted as a tool in designing new architectures and prior work has shown that gated convolution architectures match attention on AR synthetic tasks as proxies for real language modeling (Fu et al., 2023a; Poli et al., 2023a; Lutati et al., 2023). For this reason, the downstream AR perplexity gaps are surprising.\nThe key disparity is that prior synthetic formulations assume there is one query per input, at a fixed position in the sequence. Yet, language modeling often requires performing multiple recalls (e.g. for both \u201cHakuna Matata\u201d and \u201cno worries\u201d above, in a single forward pass), at varying positions. We thus propose the study of multi-query AR (MQAR). Compared to the prior AR formulations, MQAR better captures the persisting quality gaps on synthetic and real world data (Section 4). However, it is not clear why MQAR elucidates the gap and we explain this in the rest of our work.\nWe formalize the the MQAR gap across the architecture classes. Gated-convolutions process variable sequences using fixed filters, defined by the model weights rather than as functions of the input data (See Figure 1). We find it is inefficient for gated-convolutions to perform variable distance token-to-token interactions (e.g. at a distance of 10 tokens for Hakuna Matata and 9 for no worries) in a parameter inefficient way compared to attention. Attention achieves input-dependence since it computes all token-to-token interactions. We formally describe this limitation.\nWe first introduce a simple operator, BASECONV, that can provably simulate the class of architectures built from gating and convolution primitives. We show with theory and experiments that the model dimension for BASECONV to solve MQAR (and thus Hyena, H3, RWKV, RetNet, etc.) grows with the input sequence length (Theorem 4.4) while attention can solve MQAR with model dimension independent of sequence length (Proposition 4.3, Section 4.3).2 In practice, gated-convolutions appear to encode approximate solutions to AR that support only a subset of token-interaction distances. We note that theoretically analyzing deep learning architectures is challenging (Hahn, 2020; Merrill et al., 2022; Keles et al., 2023) \u2014 the fact that gating and convolutions are polynomial operations facilitates our precise analysis of the architecture class.\nWe show that input-dependent sequence mixing is important to solve MQAR efficiently. The scaling for gated convolutions with input-independent convolutions is undesirable so we next ask which architectural choices close the gap. We show that data-dependent sequence mixing helps an architecture solve MQAR efficiently (Theorem 4.5). The model needs to adapt the sequence mixing weights based on the token-interaction distances required for each new example.\nSeveral architectural modifications could satisfy the input-dependence property. Based on our analysis, we evaluate minimal modifications that only add input-dependent operations on exact-match repeated bigram tokens (our heuristic for measuring tokens that require recall). Note that language models often need to perform recall between fuzzier substrings (e.g. synonymous bigrams) or higher-dimensional concepts. However, we show that simply inserting input-dependent operator\n2Note that the runtime of Attention is quadratic, but gated convolutions is near linear.\n\u2014 e.g., a convolution filter that shifts the sequence based on the bigram positions or sparse attention placed only on repeated bigram positions \u2014 to the BASECONV architecture at < 10% of layers suffices to outperform the Transformer baseline on Pile language modeling (Section 5) and succeed on MQAR synthetic tasks (Section 4.3). Moreover, this closes > 80% of the MQAR perplexity gap on the Pile validation data. Finally, we prototype solutions that learn the positions at which to use input-dependent operations, validating that they also close the gap to attention on the Pile.\nIn this work, we analyze the increasingly-popular convolution-based architectures and identify fundamental limitations. We hope the MQAR tool and our analysis of the role of input-dependence for MQAR inform the design of future architectures. We release our code for reproducability."
        },
        {
            "heading": "2 BACKGROUND AND PRELIMINARIES",
            "text": "In this section, we describe the setting, introduce notation, and discuss important related work. See Appendix A for a broader discussion of related work.\nLanguage modeling. We study auto-regressive language models trained on the task of next token prediction (Gao et al., 2020). Given a sequence of N tokens x = {x0, ..., xN\u22121} drawn from a vocabulary C, the model outputs a probability distribution over C for each of xi given the preceding tokens P(xi|x0, ..., xi\u22121). The language models in this work share the same high-level architecture. First, each token xi in the input is embedded in d-dimensional space yielding a matrix u \u2208 RN\u00d7d. Next, u is passed through a stack of L layers, with layer \u2113 outputting u\u2113 \u2208 RN\u00d7d. Finally, the embeddings uL output by the last layer are mapped back to logits over C with a linear projection. Each layer transforms u with a sequence mixer (e.g. attention) followed by a state mixer (e.g. MLP). Unless specified, our models adhere to the implementation details of the LLaMA architecture (except the sequence mixer, which we vary throughout) (Touvron et al., 2023).\nSequence mixers. Our work evaluates how the choice of sequence mixer affects the quality and behavior of language models. Most sequence mixers aggregate the token embeddings in a sequence via a weighted sum. For example, y[i, :] = \u2211N\u22121 j=0 \u03c9(i, j)u[j, :]), where \u03c9 is a function outputting scalar weights. We study the differences between two classes of sequence mixers, discussed next.\nAttention. (Vaswani et al., 2017) We review attention, the de facto language model sequence mixer. An attention layer is parameterized by three learnable projection matrices Q,K,V \u2208 RN\u00d7d. To compute the output y given inputs u, attention applies the projections to the input: q = Qu, k = Ku, v = Vu. The projected embeddings are aggregated according to: y = softmax( 1\u221a\nd qk\u22a4)v. As\nshown in Fig. 1, the mixing matrix s(u) = 1\u221a d qk\u22a4 lacks explicit structure, so computing attention requires O(N2d) time, which is expensive for long sequences (large N ).\nGated-convolutions. A more efficient alternative to attention is the convolution, which is defined as y[i, :] = \u2211N\u22121 j=0 k[j, :] \u2299 u[i \u2212 j, :] where the kernel k \u2208 RN\u00d7d is a learnable weight matrix. Convolutions can be computed in time O(Nd logN) using the Fast Fourier Transform (FFT) and the\nconvolution theorem: y = u\u2217k = FFT\u22121(FFT(u)\u2299 FFT(k)) (Cooley and Tukey, 1965). While purely convolutional architectures match or outperform attention in certain domains (e.g. vision (Tay et al., 2022; Gu et al., 2021), audio (Goel et al., 2022), and time-series (Zhang et al., 2023)), they trail by a large margin on language (Fu et al., 2023a). Recent work has closed much of this gap by combining convolutions with gating (i.e. elementwise multiplication of the input with a transformed version of itself). Gating was first proposed in the context of convolutions by Dauphin et al. (2017), but more recently it has played a central role in state-of-the-art sub-quadratic language models, some of which claim attention-level quality (Fu et al., 2023a; Wang et al., 2022; Poli et al., 2023a; Peng et al., 2023; Zhai et al., 2021; Fu et al., 2023b, inter alia). Though they may appear different on the surface, these sub-quadratic architectures can all be expressed in terms of convolutions and gating. See Appendix H.2 for detailed descriptions of these architectures and their similarities. This work analyzes the differences between attention and the broad class of gated convolution mixers."
        },
        {
            "heading": "3 IDENTIFYING THE ASSOCIATIVE RECALL PROBLEM",
            "text": "In this section, we measure the perplexity gap between gated convolutions and attention and show that single skill termed associative recall accounts for 82% of the gap on average. This is surprising because prior work shows gated convolutions solve a synthetic version of associative recall perfectly. Informed by our analysis, we define a new synthetic formulation that better reflects real data. This task facilitates our analysis of why the gap occurs (Section 4) and how to fix it (Section 5)."
        },
        {
            "heading": "3.1 FINE-GRAINED ANALYSIS OF DOWNSTREAM QUALITY",
            "text": "Perplexity Gap We pretrain a suite of large language models with different sequence mixers across 3 scales (70M-360M) for 10B tokens on the standard Pile language modeling setting (Gao et al., 2020). In the main paper, we compare attention to three state-of-the-art gated convolution sequence mixers: H3, Hyena, and RWKV (Fu et al., 2023a; Poli et al., 2023a; Peng et al., 2023).3 We also include a pure long-convolution model to underscore the importance of gating. In Appendix F, we also include results on additional sequence mixers (Hasani et al., 2022; Sun et al., 2023). For experimental details and hyperparameters for all architectures see Appendix C.\nAcross scales, we find that attention outperforms the gated convolutions by at least a third of a perplexity point on average: the minimum gaps are +2.14, +0.59, +0.35 PPL at 70M, 160M, and 360M parameter scales, respectively. We report overall test perplexity in Table 1. Though these gaps are relatively small on average, models may still perform very differently on different subsets of data (Eyuboglu et al., 2022).\nAssociative Recall Perplexity To better understand the differences between attention and gated convolutions, we perform a fine-grained analysis of next token predictions and observe that convolution-based models struggle to recall associations previously seen in context. For example, in Fig. 1, the model must recall the association between \u201cTim\u201d and the last name \u201cRice\u201d. Following a long line of prior work, we call this skill associative recall (AR) (Willshaw et al., 1969; Hopfield, 1982) (see Appendix A for an extended discussion of AR\u2019s history in machine learning). In Appendix D.1.1, we provide annotated Pile examples to demonstrate the phenomenon qualitatively.\nQuantifying AR performance. It is challenging to derive a quantitative measure of associative recall performance on the Pile because we don\u2019t know which next token predictions in raw text require associative recall. We use a simple heuristic to identify these tokens, which we refer to as AR Hits. An AR Hit is the last token of an n-gram repeated in context (e.g. the second occurence of \u201cRice\u201d in Fig. 1). However, some common n-grams (e.g. \u201cof the\u201d) could have been memorized during training, so we factor in the frequency with which an n-grams appeared in the training data. This heuristic enables us to scale our analysis to over 10 million tokens of Pile validation data.\nWe stratify Pile validation tokens into two slices based on this heuristic and report perplexity in each slice in Table 1:\n1. AR Hits: (6.4% of tokens) Tokens in the final position of a bigram (a pair of consecutive tokens) which previously appeared in context, but \u2264 1250\u00d7 during training.\n2. Other tokens: (93.6% of tokens) Tokens in the final position of a bigram which did not previously appear in context or it appeared > 1, 250 times during training.\n3RWKV is commonly referred to as an RNN. We show it can be viewed as a convolution (Appendix H.2.2).\nIn Figure 1, we visualize these slices by plotting log-perplexity against the frequency with which bigrams appear during training. Strikingly, the gap between attention and gated convolutions is the largest on AR hits with the fewest occurrences. On the other tokens, there is no gap.\nIn Table 1, we also compute the percentage of the difference in perplexity between attention and each model that is due to AR tokens: \u2206 log(\u03d5AR)\u00b7|TAR|\u2206 log(\u03d5)\u00b7|T | , where \u03d5 is the perplexity and T is the set of tokens in the test set. This quantity can also be interpreted as the fraction of the overall gap that would close if a model matched attention on the AR slice. We find that the AR slice accounts for 82% of the average quality gap between the gated convolutions and attention.\nTo evaluate the AR capacity of larger models, we train two 1.4 billion parameter attention and Hyena models for 50 billion tokens on the Pile and repeat this analysis (see Table 5). Strikingly, a 70 million parameter attention model is a full perplexity point better in the AR slice than this 1.4B Hyena model 20 times its size (2.41 vs. 3.43). Between Attention and Hyena at 1.4B parameters, there is an overall perplexity gap of 1.46 of which 40.3% is accounted for by the AR slice. This substantial but comparatively less than the 98.2% observed at 350M. One explanation is that our AR hits heuristic is less effective for large language models that perform more complicated forms of AR (e.g. fuzzy matches). In Appendix G, we also evaluate open-source RWKV and attention models trained up to 7 billion parameters. We use a controlled semi-synthetic dataset to measure AR capacity and show that RWKV\u2019s performance degrades sharply as we increase the number of queries in an example while attention performs consistently well Appendix G."
        },
        {
            "heading": "3.2 FORMALIZING THE PROBLEM: MULTI-QUERY ASSOCIATIVE RECALL",
            "text": "This gap in associative recall perplexity is very surprising because prior work shows gatedconvolutions can perfectly solve a formalized version of the task (Fu et al., 2023a; Poli et al., 2023a; Olsson et al., 2022). In this synthetic task, the input x contains a sequence of bigrams representing key-value pairs from a random dictionary followed by a single query token. For example, the correct output for the input below would be 3:\nA 4 B 3\ufe38\ufe37\ufe37\ufe38 Key-Value C 6 E 2 F 1 C 6 G 8 \u2192 B ?\ufe38\ufe37\ufe37\ufe38 Query\nGated convolutions (e.g. H3, Hyena, RWKV) can solve this task perfectly for most sequence lengths.\nThis is inconsistent with our findings on the Pile described above, so we ask how this formulation of AR differs from the way AR manifests in real language. We identify a major difference. In real world inputs, the language model often needs to perform multiple associative recalls in a single forward pass, at varying positions in the sequence (e.g. \u201cTim Rice\u201d and \u201cMarch 2018\u201d in Fig. 1. We refer to this as Multi-Query AR (MQAR). We formally define the MQAR problem as follows: 4\nDefinition 3.1 (Multi-Query-AR (MQAR)). We are given an input sequence x = {x0, . . . , xN\u22121} where each xi \u2208 C is a token drawn from a vocabulary of size c = |C|. The task is to check, for every query 1 \u2264 i < N , whether there exists a 0 \u2264 j < i such that ui \u2261 uj . If so, output uj+1. For example, the correct output for input below would be 4, 6, 1, 2, 3:\nA 4 B 3 C 6 F 1\ufe38\ufe37\ufe37\ufe38 Key-Value E 2\u2192 A ? C ? F ?\ufe38\ufe37\ufe37\ufe38 Query E ? B ?\nIn Section 4, we use MQAR to explain the quality gap between gated convolutions and attention."
        },
        {
            "heading": "4 EXPLAINING THE ASSOCIATIVE RECALL PROBLEM",
            "text": "In this section, we provide an explanation for the gap in associative recall performance by analyzing the formal MQAR task theoretically and empirically. In Section 4.1, we define a simple gated-convolution architecture, called BASECONV, which we show can simulate a broad class of architectures built from gating and convolutions. This allows us to make general statements that apply to popular gated-convolution architectures like Hyena, RWKV, or H3. In Section 4.2, we show that there exist theoretical solutions to MQAR that could in principle be learned by BASECONV, and we analyze their complexity in terms of model width and depth. In Section 4.3, we use experiments on synthetic data to show that solving MQAR with BASECONV (and other gated-convolution architectures) requires model dimension to scale linearly with the sequence length. In contrast, attention\n4In Appendix H.7.1 we define a formal, general form of this definition, used in the theoretical analysis.\nsolves MQAR consistently in our experiments with model dimension scaling independently of sequence length. These empirical scaling laws provide a potential explanation for the AR gap and, alongside our theoretical analysis, point to the potential solutions discussed in Section 5."
        },
        {
            "heading": "4.1 BASECONV: A MINIMAL GATED CONVOLUTION OPERATOR",
            "text": "In this section, we define our minimal gated-convolution architecture, called BASECONV. Given a function, we would like to know the most efficient model (e.g. parameters, FLOPs) that can represent the solution. In this work, we show we can precisely reason about this question for representing polynomial functions with gated convolutions as gating and convolutions are both polynomial operations. The standard model defining computational complexity for polynomials is by the size of the smallest arithmetic circuit that can compute the polynomial. We define the BASECONV gated convolution operator which is exciting because (1) it is universal in that it that can simulate any arithmetic circuit C (with only a poly-log blowup in the corresponding parameters) and (2) it is simple to implement efficiently (19 lines of pure PyTorch including imports, see Appendix B).\nDefinition 4.1 (BASECONV Operator). Given an input u \u2208 RN\u00d7d, the BASECONV operator for layer \u2113 is defined as:\ny := ( u \u00b7W \u2113 + b\u21131 )\ufe38 \ufe37\ufe37 \ufe38 Linear Projection \u2299 ( h\u2113 \u2217 u+ b\u21132 )\ufe38 \ufe37\ufe37 \ufe38 Convolution\n(1)\nwhere the layer is parameterized by learnable filters h \u2208 RN\u00d7d, a linear projection W \u2113 \u2208 Rd\u00d7d, and \u2018bias\u2019 matrices b1, b2 \u2208 RN\u00d7d. The \u2299 is component-wise product and convolution of two matrices is computed as convolution of the corresponding columns.\nIn our experiments, each BASECONV layer uses O\u0303(Nd+ d2) parameters5 and can be computed in O\u0303(Nd2) operations. For our theoretical results, we can assume the weight matrix W \u2113 is restricted to a class of matrices that support near-linear time matrix multiplication (e.g. Kaleidoscope matrices, see Definition H.3). Under this assumption, BASECONV uses O\u0303(Nd) parameters and O\u0303(Nd) FLOPs (Proposition H.6). We now state the equivalency result between arithmetic circuits and BASECONV a \u201ccanonical\u201d representation of arithmetic circuits (Theorem H.21 in Appendix H.5):\nTheorem 4.2 (Equivalency to Arithmetic Circuits). For an arithmetic circuit C of size s and depth \u2206 that takes u \u2208 RN\u00d7d as input, there exists an equivalent BASECONV operator that uses O\u0303(s\u2206) parameters and O\u0303(\u2206) layers.6\nIn other words, any gated convolution model with small number of layers can be simulated by BASECONV with only a (poly)logarithmic blowup in parameters and layers. We note that arithmetic circuits are a very well studied computation model in computational complexity Bu\u0308rgisser et al. (1996). Many well-known efficient algorithms on matrices (e.g. the FFT or the current best known matrix-matrix multiplication algorithm) in fact give small arithmetic circuits. However, arithmetic circuits are inherently discrete objects \u2013 we cannot learn them via gradient descent. Theorem 4.2 shows that (up to poly-log loss in parameters), we can instead learn over BASECONV models. This result generalizes a similar result from Dao et al. (2020) for the special class of linear functions: we generalize the earlier result to the class of all polynomials.\nFor specific gated convolution layers, we can get rid of the poly-logarithmic factor blowup\u2013we observe in the appendix that BASECONV and Hyena models can simulate each other with only a small constant blowup in parameters (Proposition H.12 in Appendix H.5)."
        },
        {
            "heading": "4.2 THEORETICAL ANALYSIS OF GATED CONVOLUTION CAPACITY AND ASSOCIATIVE RECALL",
            "text": "In this section, we provide theoretical MQAR solutions that could in principle be learned by each architecture and analyze their complexity in terms of model width and depth. First, we note that attention solves MQAR with parameters independent of sequence length (Proposition H.27).\nProposition 4.3 (Attention). Given an input u \u2208 {0, 1}N\u00d73c, Attention (even without using softmax) solves MQAR for u usingO(c2) parameters,O(Nc2+N2c) time complexity andO(1) layers.\n5We use O\u0303(\u00b7) to hide poly-log factors. 6The formal statement in the Appendix has a sharper version of this result in terms of the circuit \u2018width\u2019.\nIt is natural to wonder then if all pairwise comparisons among tokens are necessary to solve MQAR. Indeed, in the RAM setting, a sequential algorithm can simply utilize N logarithmic insertion and membership queries to solve MQAR in subquadratic time. Unfortunately, any model attempting to emulate this would require \u2126(N) layers. Instead, we observe that we can parallelize this algorithm using dyadic intervals and achieve a depth of O\u0303(1) (Proposition H.30). We then convert this algorithm into an arithmetic circuit and apply Theorem 4.2 to derive an equivalent BASECONV model. This allows us to prove new upper bounds for BASECONV models applied to MQAR, which improves upon the quadratic time complexity of attention to near-linear runtime at the cost of using poly-log layers (Theorem H.37 in Appendix H.7).\nTheorem 4.4 (Data-Independent Filters7). Given an input u \u2208 {0, 1}N\u00d7O(log c) to MQAR (where we assume that distinct tokens are embedded into distinct vectors in {0, 1}O(log c)), there exists a BASECONV operator that solves MQAR for u using O\u0303(N log c) parameters as well as time complexity and O\u0303(1) layers.\nNevertheless, the poly-logarithmic number of layers in the above result is undesirable in practice. But, we show that using input-dependent convolution filters, one can get constant many layers (for a sub-class of inputs). Towards that end, we define the interaction distance between a query qi and the matching key kj as i \u2212 j. This then allows us to present the corresponding upper bound for data-dependent mixing (Theorem H.38 in Appendix H.8).\nTheorem 4.5 (Input-Dependent Filters). Given an input u \u2208 {0, 1}N\u00d7c to MQAR (where we assume that the tokens are embedded as one-hot encoding in {0, 1}c and there exists at most t distinct interaction distances),8 there exists a BASECONV operator that uses input-dependent kernels to solve the above case of MQAR using O(t \u00b7Nc) parameters and O(1) layers."
        },
        {
            "heading": "4.3 EMPIRICAL ANALYSIS OF GATED CONVOLUTION CAPACITY AND ASSOCIATIVE RECALL",
            "text": "In this section, we measure empirically how model dimension must scale in order for different sequence mixers to solve MQAR.\nSetup We train and evaluate models on a synthetic MQAR with vocabulary size 8, 192, varying model dimension and sequence length from 64 to 512. Appendix E provides further details on the formulation and construction of this synthetic task. Following Olsson et al. (2022), we train two layer models with a Transformer backbone that interleaves sequence mixing and state mixing (MLPs). For each architecture, we sweep four learning rates from log(\u22124) to log(\u22122)} for each architecture, and report maximum test accuracy.\n7We note here that existing architectures also use data-independent convolution filters, meaning the filter is defined as a function of the model parameters, independent of the input.\n8Note that the interaction distances can be arbitrary: there is just a bounded number of distinct distances.\nOur results, which are summarized in Figure 2, support two main claims:\nClaim 1 (Gated-convolutions and attention). Gated-convolution models with two layers require model dimension to scale at least linearly in sequence length in order to solve associative recall, while attention models can solve it with near-constant dimensionality. We compare attention and BASECONV as well as three popular instantiations of gated-convolution architectures: RWKV, H3, and Hyena (Peng et al., 2023; Fu et al., 2023c; Poli et al., 2023a). In the top row of Fig. 2, attention solves MQAR perfectly at all sequence lengths using a constant model dimension of 64. In contrast, MQAR does not achieve accuracy > 0.9 unless d \u2265 N . Claim 2 (Input-dependent filters). Using input-dependent filters in gated-convolution models can close some of the gap to attention. In Theorem 4.5, we show that BASECONV with input-dependent filters could solve MQAR with improved scaling. In this solution, we construct a filter that spikes at position j if matching keys are separated by j tokens. We evaluate two approaches for constructing this filter: (1) programatically (i.e. hard-coded comparisons between token ids) or (2) with autocorrelation, which could learn to perform fuzzy-matches (see Appendix H.8). In the bottom row of Fig. 2, we see that BASECONV with programmatic input-dependent filters achieves nearconstant scaling in model dimension and that BASECONV with autocorrelation input-dependent filters achieves improved scaling over BASECONV with input-independent filters.\nThese input-dependent filters cannot easily be made to satisfy causality and using an O(N logN) filter per gap could be expensive if each gap applies only to a small number of bigrams. A simpler and perhaps more efficient way to solve the problem would be to introduce a small amount of attention to an otherwise BASECONV model (Fu et al., 2023a). As the first natural baseline, we evaluate an attention hybrid on synthetic MQAR and show that it achieves improved scaling in Figure 2. Next, in Section 5, we put these empirical and theoretical insights into practice on Pile language modeling."
        },
        {
            "heading": "5 CLOSING THE ASSOCIATIVE RECALL GAP",
            "text": "In this section, we evaluate hybrid BASECONV-Attention models that leverage different sparsity patterns. We show that hybrids with input-dependent sparsity patterns can close most of the gap to attention, while maintaining sub-quadratic scaling. We also show that BASECONV hybrids can outperform attention-only models by up to a full perplexity point, all while being dramatically simpler to implement and analyze than prior hybrids (Fu et al., 2023a) (see implementation in Appendix B. We describe the architectures in Section 5 and results on Pile language modeling in Section 5.\nSparse BASECONV-Attention Hybrids We evaluate hybrids composed primarily of BASECONV layers and three attention layers (6.3% of layers at 354M parameters and 10% at 168M parameters). We augment the attention layers with operators that selectively apply attention to some tokens based on a selection function f : RN\u00d7d \u2192 {0, 1}N . They take as input u \u2208 RN\u00d7d and output y \u2208 RN\u00d7d:\ny[i, :] = softmax( 1\u221a d q[i, :]k\u22a4)v \u00b7 f(u)[i] (2)\nwhere q,k,v are query, key, and value projections, as in attention. We evaluate four choices for f :\n(1) Full attention. First, we evaluate the performance of full attention hybrids. These hybrids correspond to fixing f(u)[i] = 1 for all i in Eq. (2). Prior work has shown full attention hybrids to be effective, but they do not explain why supplementing gated convolutions with attention is necessary (Fu et al., 2023a). Based on our findings in Section 3, we hypothesize that sparse attention applied only to associative recall hits may suffice.\n(2) Random selection. As a control, we evaluate sparse attention randomly applied to tokens. This corresponds to a stochastic selection function where f(u)[i] is drawn from a Bernoulli. Many prior works have employed sparse attention patterns like this one, which are independent of the input (Zaheer et al., 2020; Child et al., 2019a; Beltagy et al., 2020, inter alia.).\n(3) Programmatic selection. Next, to evaluate our hypothesis that attention is needed for associative recall, we prototype a programmatic selection function that selects only those tokens that might be associative recall hits. Specifically, f(x[i, :]) is 1 if the token xi previously occurred in the sequence. In practice, we compare raw token ids, not token embeddings.\nf(x)[i] = { 1 if there exists j < i such that xi = xj 0 otherwise\n(3)\n(4) Learned selection. Finally, we prototype a learned selection function f(u)[i] = \u03c3(u[i, :] \u00b7W ) parameterized as a simple linear layer with sigmoid activation. We fix a hyperparameter k and select the top-k tokens with the highest score in each batch. This allows us to compute attention in O(ndk) time. During training, we add small amount of Gaussian noise to the topk calculation to encourage exploration and use an auxiliary loss to encourage sparse selection: \u2113f (u) = 1 N max(0, \u2211N i=1 f(u)[i] \u2212 k). This approach is most similar to the recently proposed SeqBoat architecture (Ren et al., 2023). We discuss the differences in Appendix A.\nDownstream Evaluations We evaluate the prototypes on Pile language modeling. We take BASECONV architectures at the 150M and 360M parameter scales and add input-dependent selection to three layers (details in Appendix C.2). Results are in Table 2. We validate that the prototypes close the overall and AR quality gaps to attention when added to the BASECONV backbone.\nAt 360M parameters, BASECONV with just 3 attention layers can outperform the Transformer, while requiring fewer FLOPs. Our hybrid attention-BASECONV models outperform attention only models by 0.85 perplexity points while enabling an 18% reduction in total FLOPs vs attention. However, this uses full quadratic attention. We next show that sparse attention localized to potential AR tokens, is also sufficient to close the gap, validating our insights on the role of input-dependence for MQAR. At 360M parameters, programmatic selection closes 85% of the gap between pure BASECONV and attention on the AR slice, in contrast to the random selection control. Learned selection closes 72% a of the gap using just k = 256 (sub-quadratic) attention positions per example."
        },
        {
            "heading": "6 DISCUSSION AND CONCLUSION",
            "text": "We present an extensive analysis of gated convolution architectures in light of their recent popularity. We identify a persisting quality gap between efficient convolution and inefficient attention based architectures, largely due to a single failure mode associative recall. We design a new multiquery associative recall (MQAR) analysis tool, which correlates with downstream AR quality. We theoretically and empirically explain the gap is due to insufficient data-dependent mixing in gated convolutions and we show minimal architectures that close the gap on the Pile.\nOur results in analyzing gated convolutions go beyond the conventional wisdom that attention is the \u201cright\u201d model. A significant amount of work focuses on improving the efficiency of attention (Dao et al., 2022; Dao, 2023; Katharopoulos et al., 2020a) and theoretically studying the exact power of attention (Hahn, 2020; Merrill et al., 2022; Keles et al., 2023). Attention is often used as the goalpost for what is needed downstream. We hope our contributions highlight the value of MQAR, and more broadly tasks tied to real language modeling, as a proxy to study."
        },
        {
            "heading": "APPENDIX",
            "text": "The appendix includes the following content:\n1. Appendix A provides an extended discussion of related work and concepts.\n2. Appendix B provides a code implementation of the BASECONV architecture.\n3. Appendix C gives details for the experiments, including model architectures and hyperparameters.\n4. Appendix D provides additional analysis of how MQAR appears in real data across a variety of language distributions.\n5. Appendix E provides a formal definition of the MQAR problem, synthetic construction procedure, and experimental details.\n6. Appendix F provides additional synthetic experiments and analysis.\n7. Appendix G provides experiments and analysis for how the MQAR gap changes as we scale the gated convolution and attention architectures.s\n8. Appendix H gives proofs and additional discussion for the theoretical analysis in our work."
        },
        {
            "heading": "A EXTENDED RELATED WORK",
            "text": "We are inspired by and build on prior work in mechanistic interpretability (Appendix A.1), efficient architecture design (Appendix A.2), and input-dependent architectures (Appendix A.3)."
        },
        {
            "heading": "A.1 MECHANISTIC INTERPRETABILITY, SYNTHETIC LANGUAGES, AND ASSOCIATIVE RECALL",
            "text": "Work in mechanistic interpretability aims to decompose the capabilities of a neural network into human-understandable algorithms that can be attributed to specific parameters in the model (Olah, 2022; Power et al., 2022; Elhage et al., 2021; Cammarata et al., 2020). Some of these works, use synthetic data to validate mechanistic interpretations of neural networks (Olsson et al., 2022). This relates to a broader line of work using synthetic languages to study language model architectures (Wang and Eisner, 2016; White and Cotterell, 2021; Allen-Zhu and Li, 2023; Ravfogel et al., 2019; Xie et al., 2021). Mechanistic design puts mechanistic interpretations to use in designing new architectures and learning algorithms. Several works in architecture research have used synthetic tasks to validate architecture designs (Kitaev et al., 2020).\nOur work is focused on one particular synthetic task: associative recall. Motivated by psychological models of how humans associate and retrieve information, work in the early days of neural network research focused on developing systems capable of associative recall (Willshaw et al., 1969; Feldman et al., 1981; Hopfield, 1982). For example, Hopfield networks, proposed in 1982, are a recurrent neural network explicitly designed to support associative (\u201ccontent-addressable\u201d) memory (Hopfield, 1982). More recently, several notable recurrent neural network mechanisms (e.g. neural turing machines, LSTMs, and RNNs with fast weights) were evaluated on a synthetic version of associative recall (Graves et al., 2014; Ba et al., 2016; Zhang and Zhou, 2017, inter alia.). These works use a formulation of associative recall very similar to the single-query associative recall in our work. Since the rise of large language models, several works have argued that LLMs ability to perform in-context learning is due, at least in part, to the associative recall capabilities of attention (Elhage et al., 2021; Olsson et al., 2022)."
        },
        {
            "heading": "A.2 EFFICIENT LANGUAGE MODELING ARCHITECTURES",
            "text": "We first briefly review the efficiency motivations for the recent excitement around gated convolution architectures. While attention requires compute that scales as O(N2) in sequence length N , convolutions scale as O(N logN) (Cooley and Tukey, 1965). Ideal, inference complexity is O(1) in sequence length, as provided by recurrent neural networks. State-space models can be computed either as a convolution or recurrence, to achieve both sub-quadratic training and constant inference complexity in sequence length (Gu et al., 2021). Architectures that use implicit convolutional filters (Poli et al., 2023a), can be converted to an SSM via a simple distillation step (Massaroli et al., 2023).\nA.3 INPUT-DEPENDENCE IN SEQUENCE MODELING ARCHITECTURES In an input-dependent sequence model, the way tokens are aggregated across the sequence is controlled by the data, not just the model parameters. We highlight several prior works related to our study that explore input-dependent sequence models.\n\u2022 Recurrent Architectures The pure long-convolution architectures can also be computed as recurrences. Letting si represent the hidden state at time i, ui be the input at time i, and A,B,C be projection matrices, the recurrence computes:\nsi+1 = f(Asi +Bui)\nyi = Csi In a linear RNN like S4, the A, B, and C matrices are input-independent and f is an identify function. However, recent work proposes input-dependent RNNs (e.g. RetNet Sun et al. (2023)), where a subset of these matrices is input-dependent (e.g. A = xWa). Note that linear attention mechanisms can also be expressed as input-dependent RNNs (Katharopoulos et al., 2020b). While these architectures improve over gated convolutions in MQAR synthetic experiments and downstream quality on the Pile (Appendix F), we observe and prove that the required RNN hidden state dimensionality grows with the number of key-value pairs that the model needs to recall in the sequence (Appendix H.6). In contrast, attention complexity does not scale with the number of key-value pairs to recall.\n\u2022 Hybrid Architectures Finally, we can combine architectural components that provide different capabilities. Prior work proposes architectures that hybridize sub-quadratic layers and attention (Fu et al., 2023a; Ma et al., 2022; Ren et al., 2023), but does motivate this choice from a mechanistic design perspective. Further, H3 without attention Fu et al. (2023a) and MEGA (Ma et al., 2022), which uses blocked attention, underperform attention on language modeling.\nSeqBoat (Ren et al., 2023) shares closest resemblance to our learned selection module. SeqBoat introduces a different module that learns where to use attention however, their model can end up using full attention at a layer, providing quadratic scaling in the worst case. Different from SeqBoat, we use an auxiliary loss to encourage sparsity in the selected attention positions and select the top-k positions to remain sub-quadratic.\nOverall, our work finds that prior sub-quadratic models do not fully solve the MQAR task, the focus of our work."
        },
        {
            "heading": "B CODE LISTING",
            "text": "In this section, we include code listings for the BASECONV operator. We begin with the a standard BASECONV implementation that uses an explicitly-parameterized long convolution. Below, we also provide the implementation for BASECONVwith an implicitly-parameterized long convolution.\n1 import torch 2 3 def fft_conv(u: torch.Tensor, k: torch.Tensor): 4 \"\"\" 5 Args: 6 u (torch.Tensor): (batch_size, d_model, seq_len) 7 k (torch.Tensor): (d_model, l_max) 8 Return: 9 y (torch.Tensor): (batch_size, d_model, seq_len)\n10 \"\"\" 11 seqlen = u.shape[-1] 12 fft_size = 2 * seqlen 13 k_f = torch.fft.rfft(k, n=fft_size) / fft_size 14 u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size) 15 y = torch.fft.irfft(u_f * k_f, n=fft_size, norm=\"forward\")[..., : seqlen] 16 return y 17 18 class BaseConv(torch.nn.Module): 19 20 def __init__(self, d_model: int, l_max: int, **kwargs): 21 super().__init__() 22 self.d_model, l_max = d_model, l_max, 23 self.projection = torch.nn.Linear(self.d_model, self.d_model) 24 self.filter = torch.nn.Parameter(torch.randn(self.d_model, l_max) , requires_grad=True) 25 26 def forward(self, u: torch.Tensor): 27 \"\"\" 28 Args: 29 u (torch.Tensor): (batch_size, d_model, seq_len)\n30 Return: 31 y (torch.Tensor): (batch_size, d_model, seq_len) 32 \"\"\" 33 u_conv = fft_conv(u.transpose(1, 2), self.filter).transpose(1, 2) 34 u_proj = self.projection(u) 35 y = u_conv * u_proj 36 return y + u\nListing 1: Explicit BASECONV implementation. Implementation of the BaseConv layer with explicitly-parameterized long convolutions. The implemtnation is 19 lines excluding comments and whitespace.\nIn some of our experiments, we interleave BASECONV layers that use explicit short convolution filters (like those in torch.nn.Conv1d) and with BASECONVlayers that use implicit long convolution filters (like those described in Poli et al. (2023a)).\nBelow we include the code for BASECONV with an implicit convolution.\n1 class PositionalEmbedding(nn.Module): 2 def __init__(self, emb_dim: int, seq_len: int, **kwargs): 3 \"\"\"Complex exponential positional embeddings for implicit long convolution filters.\"\"\" 4 super().__init__() 5 t = torch.linspace(0, 1, seq_len)[None, :, None] # 1, L, 1 6 bands = (emb_dim - 1) // 2 7 t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None] 8 w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1 9 f = torch.linspace(1e-4, bands - 1, bands)[None, None]\n10 z = torch.exp(-1j * f * w) 11 z = torch.cat([t, z.real, z.imag], dim=-1) 12 self.z = nn.Parameter(z, requires_grad=False) 13 14 def forward(self, L): 15 return self.z[:, :L] 16 17 class BaseImplicitConv(nn.Module): 18 \"\"\" 19 BaseConv with implicit filter parameterized by an MLP. 20 21 Args: 22 d_model (int): Number of expected features in input and output. 23 l_max (int): The maximum sequence length. 24 d_emb (int, optional): Dimension of the positional embeddings. Must be odd and $\\geq$ to 3 (time, sine and cosine). Defaults to 3. 25 d_hidden (int, optional): The number of features in the hidden layer of the MLP. Defaults to 16. 26 \"\"\" 27 28 def __init__(self, d_model: int, l_max: int, d_emb: int=3, d_hidden: int = 16,): 29 \"\"\" 30 Long convolution with implicit filter parameterized by an MLP. 31 \"\"\" 32 super().__init__() 33 self.pos_emb = PositionalEmbedding(d_emb, l_max) 34 self.filter_mlp = nn.Sequential(nn.Linear(d_emb, d_hidden), torch .nn.ReLU(), nn.Linear(d_hidden, d_model)) 35 self.projection = torch.nn.Linear(d_model, d_model) 36\n37 38 def forward(self, u: torch.Tensor, *args, **kwargs): 39 \"\"\" 40 Args: 41 u (torch.Tensor): (batch_size, seq_len, d_model)\n42 Return: 43 y (torch.Tensor): (batch_size, seq_len, d_model) 44 \"\"\" 45 filter = self.filter_mlp(self.pos_emb(u.shape[1])).transpose(1, 2) 46 u_conv = fft_conv(u.transpose(1, 2), filter).transpose(1, 2).to( dtype=u.dtype) 47 u_proj = self.projection(u) 48 y = u_conv * u_proj 49 return y + u\nListing 2: Implicit BASECONV implementation. Implementation of the BASECONV layer with implicitly-parameterized long convolutions. (The implementation is 34 lines excluding comments and whitespace)."
        },
        {
            "heading": "C DOWNSTREAM EXPERIMENTAL DETAILS",
            "text": "We use A100 80GB Nvidia GPUs to run all experiments. We use the reference training infrastructure from https://github.com/EleutherAI/gpt-neox for all pretraining runs. The Pile data is tokenized using the GPT2BPETokenizer and all models see the data in the same order.\nBelow we provide details on the hyperparameters and settings for training each architecture studied in the paper on the real-world Pile data. In Appendix C.1 we summarize and justify our method for measuring the quality gap due to associative recall between the convolution architectures and attention. In Appendix C.2, we provide details on the pure gated convolution architectures, attention baseline, and hybrid architectures studied in the paper."
        },
        {
            "heading": "C.1 MEASURING THE MQAR GAP ON REAL LANGUAGE DATA",
            "text": "Here we summarize our method for computing the amount of quality gap between the convolution and attention models that is ascribed to associative recall capability (e.g., in Table 1):\n1. Given an input sequence, we identify recurring bigrams (i.e. bigrams that have already appeared in the sequence at a prior position). Since bigrams that appear frequently during training may be memorized by the model, rather than requiring the model to perform recall at inference-time, we only measure AR log-probabilities with respect to bigrams that are seen fewer than a threshold number of times during training. The threshold used in all the experiments in our submission is 1, 250 training occurrences in the 10B tokens of pretraining data.\n2. We measure the log-probability assigned to the true bigram completion. This bigram completion is referred to as an AR Hit in our work. This protocol assumes that the model can produce the completion by recalling the prior occurrence of the bigram in the sequence.\n3. For the model being evaluated m, and the attention model M , we measure the % of the quality gap between m and M ascribed to associative recall capability as follows. Let the average log-probability for all AR Hits across validation sequences be lmH and l M H for m and\nM respectively. Let the average log-probabilities of all tokens in the validation sequences be lm and lM respectively. Let pH be the proportion of AR Hit tokens in the validation data. As the final gap ascribed to AR, we report:\nmin( (lmH \u2212 lMH )pH\nlm \u2212 lM , 1.0)\nShown above, if m is better than attention (M ) overall and M is better than m at AR, we ascribe 100% of the gap to AR.\nWe briefly discuss two important decisions in this protocol. First, we only measure explicit bigrams, i.e. bigrams are identified based on token ids in the sequence. However, intuitively, models may also perform associative recall between related concepts produced by a contextual language model. For instance, language may contain bigrams in which one word is swapped by a synonym. As another example, a model may see a sentence such as \u201cThe iPhone is outside my budget so I instead purchased an Android phone. ... It was much \u201d and predict \u201ccheaper\u201d for the blank, recalling that\nthe sequence is discussing cost. We note that our work does not measure recall in higher dimensional recall in favor of a more transparent procedure.\nNext, we measure the gap based on log-probabilities rather than perplexity. This is simply because we want to make our metrics independent of the number of tokens in each of the slices of the validation set. Approximately 6.4% of validation tokens are AR Hits with the threshold set to consider bigrams seen less than 1, 250\u00d7 during training."
        },
        {
            "heading": "C.2 GATED CONVOLUTION DOWNSTREAM ARCHITECTURES",
            "text": "We evaluate over 4 previously proposed architectures as well as BASECONV, the theoretically \u201ccanonical\u201d representation for gated convolutions, introduced in Section 4, for a total of 14 training runs. Here we provide details on the hyperaparamters and configurations used for training each architecture. We also provide details on the FLOPs computation.\n\u2022 Attention (Vaswani et al., 2017; Touvron et al., 2023) We train using the the specifications in Table 6. The parameters are sourced from the Transformer implementation in https://github.com/EleutherAI/gpt-neox.\n\u2022 Hyena (Poli et al., 2023a) We train using the specifications in Table 8. The parameters are sourced from the Appendix of Poli et al. (2023a) and the implementation is sourced from the provided reference at https://github.com/HazyResearch/safari.\n\u2022 H3 (Fu et al., 2023a) We train using the specifications in Table 10. The hyperparameters and implementation use the reference at https://github.com/HazyResearch/ H3.\n\u2022 RWKV (Peng et al., 2023) We train using the specifications in Table 11. The parameters are sourced from the Appendix of Peng et al. (2023) and the details provided in the reference implementation at https://github.com/BlinkDL/RWKV-LM. We specifically evaluate RWKV-V4.\n\u2022 Pure long convolution We train using the specifications in Table 13. We evaluate a simple long convolution based model with no gating as a reference point. While this is a generic architecture, we use the reference implementation and initialziations/regularizations from recent work Fu et al. (2023c). The implementation is provided at https://github. com/HazyResearch/safari.\n\u2022 BASECONV We train using the specifications in Table 15. The implementation, amounting to 19 lines of PyTorch, is shown in Appendix B.\nWe provide the equations used to compute the FLOPs for each model, letting D be the model width, H the head dimension, L the depth, N the sequence length, V the vocabulary size, and B the batch size. FLOPs equations for different architecture types are provided in Table 7 (attention), Table 9 (Hyena and adapted for H3), Table 14 (pure long convolution), and Table 16 (BASECONV). We compute FLOPs for RWKV as in the Appendix of (Peng et al., 2023), based on the number of linear layer parameters, plus input and language modeling head FLOPs.\nHybrid Architectures In the main paper and appendix, we evaluate a series of architectures with a hybrid of gated convolution and non gated convolution layers. For these architectures, we use the same number of layers as the pure gated convolution architecture (as reported in Appendix C.2). The hybridized (non gated convolution) layers are inserted as replacements of the gated convolution layer. For each architecture, we simply evenly intersperse the two types of layers. We evaluate with the following replacement layers in this work:\n\u2022 Full attention following the specification of attention in Appendix C.2.\n\u2022 Random selection We evaluate sparse attention randomly applied to tokens as introduced in Section 5.\n\u2022 Programmatic selection We apply sparse attention on AR hit tokens as introduced in Section 5. When processing the input sequence, we can causally determine if a bigram of raw token ids is repeated in the sequence to construct the attention pattern.\n\u2022 Learned selection We learn the positions on which to use attention by introducing a simple linear layer with sigmoid activation to the architecture that is trained to output high scores\nif attention should be applied to the token. We can take the top-k positions to control the computational complexity of the layer. This approach is introduced in Section 5.\nWe use these protocols to validate that input-dependence suffices to address the associative recall gap and highlight that there are varied approaches to incorporating input-dependence in an architecture."
        },
        {
            "heading": "D EXTENDED DOWNSTREAM ANALYSIS OF MQAR",
            "text": "In this section, we provide additional analysis of how MQAR manifests in real language data. We extend our discussion of associative recall on the Pile Gao et al. (2020). We also perform analysis on sources in RedPajama including ArXiv and StackOverflow Computer (2023)."
        },
        {
            "heading": "D.1 ADDITIONAL DISCUSSION OF MQAR IN THE PILE",
            "text": "The Pile is a widely popular language modeling corpus Gao et al. (2020).\n1. First, in Appendix D.1.1, we provide several real examples of how associative recall occurs in the Pile to demonstrate it\u2019s role in language modeling. We color code the tokens to highlight differences in the next token predictions from Attention, RWKV and Hyena models.\n2. Next, in Appendix D.1.2, we explain where associative recall hits tend to occur in sequences. This is useful to guide the design of sequence mixers \u2014 if associative recall tends to occur in specific places, the input-dependent sequence mixer may not need to compute all N2 token-to-token interactions for every sequence."
        },
        {
            "heading": "D.1.1 PILE EXAMPLES",
            "text": "For the examples below, the legend is: tokens are colored as both correct, both incorrect, Attention correct and Hyena incorrect, Attention incorrect and Hyena correct. For tokens where the models disagree, the predictions of each model are provided in parentheses.\nwhile lunch -ing at the Ma -ison Ber -gey b -ist -ro near his apartment : he had been mus -ing about(rwkv= about, attn= on) the ...(723 tokens)... the young waitress -\u2019s sigh at the Ma -ison Ber(rwkv= Bl, attn= Ber)\n-gey(rwkv=-nd, attn=-gey)\nExample D.1: Comparing next token predictions of a 350M parameter Attention model and a 350M parameter RWKV model. In this example, the models need to perform associative recall to correctly predict \u201cBergey\u201d in the 4-gram Ma-ison Ber-gey. The previous mention of the 4-gram was more than 700 tokens earlier in the passage.\nThe second(rwkv= first, attn= second) section is all about Pixar Fest , and the (rwkv= third, attn= the) final section is all(rwkv= about, attn= all) about Pixar Pier . ...(480 tokens)... - If(rwkv=-Disney, attn=-If) there wasn -a\u0302G\u0327 -t enough Pixar at Disneyland , Pixar Fest(rwkv= would, attn= Fest) is(rwkv= at, attn= is) coming to the Disneyland Resort on April 13 , 2018 .\nExample D.2: Comparing next token predictions of a 350M parameter Attention model and a 350M parameter RWKV model. In this example, the models need to perform associative recall to correctly predict \u201cFest\u201d in the bigram Pixar Fest.\nDavid Mus -a P -id -cock is an indigenous English revert to(rwkv=-ant, attn= to) Islam ,(rwkv=,, attn= who)\nwho formed the Islamic Party of Britain in 1989 , at London Central Mosque(rwkv= Mosque, attn= University) .(rwkv=., attn= in) ...(711 tokens)... -I have just found out that David Mus(rwkv= Cameron, attn= Mus) -a P(rwkv= (, attn= P) -id(rwkv=-asha, attn=-id) -cock(rwkv=-ham, attn=-cock) is(rwkv= (, attn= is) speaking at a Yorkshire Forum debate today in Bradford with\nExample D.3: Comparing next token predictions of a 350M parameter Attention model and a 350M parameter RWKV model. In this example, the models need to perform associative recall to correctly predict a middle and last name. Note that the middle and last names span several tokens.\n-The most common map elements(hyena= in, attn= elements) in Sub -Space are prizes , or \u201d -g(hyena=-points, attn=-g) -reens \u201d(hyena=-\",, attn=\") ( -for their green color -). Pri -zes allow players to upgrade their ships and gain(hyena= other, attn= gain) special weapons or abilities(hyena= abilities, attn= upgrades) . While prizes are generally plent -ifully scattered throughout the map(hyena= map, attn= game) , the upgrades or abilities(hyena= bonuses, attn= abilities) they award are randomly selected by the zone . -Energy -Rather than dealing with ammunition counts and hit points separately , Sub -Space combines both of these elements into a single unit of measure : energy . Each ship is equipped with a certain amount of energy , from which(hyena= which, attn= the) it must draw its health as well as its weapons power . ...(1,526 tokens)... Speed Zone proved to be less popular than the Jack -pot -/ -Running , Chaos , or \u201d -flag \u201d zone games and support was discontinued shortly after Sub -Space(hyena=-space, attn=-Space) went to retail .\nExample D.4: Comparing next token predictions of a 350M parameter Attention model and a 355M parameter Hyena model. In this example, the models need to perform associative recall to correctly predict the name of the game \u201cSubSpace\u201d. Note that both models correctly perform the recall when there is a short gap between tokens, but only Attention correctly performs recall when the gap is greater than 1,000 tokens.\nThus far , no systematic study has been published on is - olation(hyena=-olation, attn=-ot) via sequential centrif -ug -ation , and no systematic analysis is ...(491 tokens)... Fig . 1(hyena= 1, attn=-A\u0302 l) -is -olation via sequential(hyena= centrif, attn= sequential) centrif -ug -ation .\nExample D.5: Comparing next token predictions of a 350M parameter Attention model and a 355M parameter Hyena model. In this example, the models need to perform associative recall to correctly predict the technique \u201csequential centrifugation\u201d.\nMiss -ouri(hyena=-ouri, attn=-iss) Southern has had 14 Major League Baseball draft selections since the draft began in 1965 . ...(149 tokens)... Fred G . Hughes Stadium(hyena= Stadium, attn= Field)\n( -opened in 1975 ) is named(hyena= the, attn= named) after former(hyena= the, attn= former) J -op -lin Globe publisher and(hyena= and, attn= Fred) Missouri South-\nern(hyena= State, attn= Southern) board of reg -ents member\nExample D.6: Comparing next token predictions of a 350M parameter Attention model and a 355M parameter Hyena model. In this example, the models need to perform associative recall to correctly predict the name of the university \u201cMissouri Southern\u201d."
        },
        {
            "heading": "D.1.2 DISTRIBUTION OF MQAR HIT POSITIONS IN SEQUENCES",
            "text": "In Figure 3, we compute and plot the distances between AR hits and its prior bigram (key-value) occurrence in the sequence across the Pile training data. The distances follow a power law distribution where most AR hits are within 100 token positions from the prior bigram occurrence, and a long tail of AR hits requires long-range interactions.\nImplications This suggests that architectures which compute token-to-token interactions within windows, where the window size is less than the full sequence length, may suffice to handle MQAR in real data. For instance, sliding window attention may help with AR (Beltagy et al., 2020; Sun et al., 2023, inter alia.). Prior work leverages the observation that local attention captures most of the token dependencies to improve efficiency Sukhbaatar et al. (2019)."
        },
        {
            "heading": "D.2 ANALYZING MQAR ACROSS VARIED LANGUAGE DISTRIBUTIONS",
            "text": "We use the HuggingFace sample of the popular RedPajama language modeling corpus Computer (2023) to understand how the prevalence of MQAR hits and and ways in which the hits appear vary across language distributions. We analyze text sequences from the following sources: ArXiv papers, Books, C4, Common Crawl, GitHub, Stack Exchange, and Wikipedia.\nFirst we profile the prevalence of MQAR hits by computing the number of repeated bigram occurrences per sequence. Each sequence is 2, 048 tokens in length and exclude bigrams containing NLTK stop words or punctuation from the computation. The prevalence of hits and the distances between hit tokens and the prior bigram occurrence in context vary across distributions as shown in Figure 4 and 5.\nArxiv, Github, and Stack Exchange contain relatively structured or richly formatted langauge data. We observe that the prevalence of MQAR hits is relatively high in these three sources in Figure 4. We inspect why these documents contain many bigrams. In Arxiv, we find domain-specific terminology, groups of related citations, Latex commands, and mathematical equations are frequently repeated within the same document. In GitHub and Stack Exchange, we find function and variable names are frequently reused. Meanwhile, the C4, CC, Books, and Wikipedia bigrams are highly variable. The topics and documents in these corpora are relatively diverse and unstructured.\nIf the placement V of the hyperplanes is not generic, then the induced subdivision \u03a3 is not a triangulation. However, the above bijection is unaffected and, in particular, the fine type of a vertex of CDA can be read off the corresponding facet of \u03a3. We define the crosscut complex ccut\u03a3 \u2286 2[n]\u00d7[d] to be the unique simplicial complex with the same vertices-in-facets incidences as the polyhedral complex \u03a3. The crosscut complex is a standard notion in combinatorial topology and can be defined in more generality (see Bjo\u0308rner). The following observation is immediate from the definitions.\nProposition D.1. For V an ordered sequence of n points in T d\u22121 let A = A(V ) be the corresponding tropical arrangement and let \u03a3 be the regular subdivision of \u2206n\u22121 \u00d7 \u2206d\u22121 induced by V . Then the fine cotype ideal fcI is Alexander dual to the Stanley-Reisner ideal of ccut\u03a3.\nThe crosscut complex encodes the information of which collections of vertices lie in a common face. Hence, the crosscut complex is a purely combinatorial object and does not see the affine structure of the underlying polyhedral complex.\nExample D.7: ArXiv sequence where the initial bigram occurrence is highlighted in red and the repeated bigram with the MQAR hit is highlighted in blue.\n1 /** 2 * Adds a directed edge to the graph from pt1 to pt2. Precondition: Both 3 * GeographicPoints have already been added to the graph 4 * 5 * @param from The starting point of the edge 6 * @param to The ending point of the edge 7 * @param roadName The name of the road 8 * @param roadType The type of the road 9 * @param length The length of the road, in km\n10 * @throws IllegalArgumentException If the points have not already been 11 * added as nodes to the graph, if any of the arguments is null, or if the 12 * length is less than 0. 13 */ 14 public void addEdge(GeographicPoint from, GeographicPoint to, 15 String roadName, String roadType, double length) 16 throws IllegalArgumentException { 17 18 if (from == null || to == null || roadName == null 19 || roadType == null || length < 0 || !mapNodes. containsKey(from) 20 || !mapNodes.containsKey(to)) { 21 throw new IllegalArgumentException(); 22 } 23 24 if (!mapNodes.get(from).hasEdge(to)) { 25 mapNodes.get(from).addNeighbor(to, roadName, roadType, length ); 26 numEdges++; 27 } 28 }\nExample D.8: GitHub sequence where substrings such as \u201cIllegalArgumentException\u201d, \u201croadType\u201d (and other variable names), \u201cGeographicPoint\u201d (and other data types), \u201ccontainsKey\u201d (and other function calls) are repeated throughout."
        },
        {
            "heading": "E SYNTHETIC MQAR EXPERIMENTAL DETAILS",
            "text": "In this paper, we propose MQAR as a useful tool to help explain gaps between three popular sequence modeling layers \u2014 Transformers or Sparse Transformers, Convolutions, and Recurrences. In this section, we detail the motivation behind the design of MQAR and include extended experiments on additional architectures. We provide a procedure for generating MQAR synthetic data, which can help in the development of new architectures."
        },
        {
            "heading": "E.1 MQAR GENERATION PROCEDURE",
            "text": "Here we provide additional discussion on the properties of our MQAR synthetic data. The objective of the synthetic analysis is to help explain the differences in language modeling behavior between different classes of language modeling architectures, as observed on real-world data Section 3. Synthetic recall tasks were used in the development of Hyena Poli et al. (2023a) and H3 Fu et al. (2023a), and Olsson et al. (2022) to study Transformer in-context learning behavior.\n1. Attention: Attention can perform recall trivially (Proposition 4.3). The bound is independent of the sequence length N . 2. Convolutions: The gated convolution models use input-independent filters. We show in Section 4 that the dimensionality for solving recall grows with the input sequence length. Real language modeling can require performing O(N) recalls in one forward pass. Our intuition is that it is difficult to efficiently compute all token-interaction distances with such a filter. 3. Recurrences: Recurrent models include a single hidden state as an information bottleneck. We show in Appendix H.6 that the gated recurrence requires \u2126(N) bits to solve MQAR for d \u2264 \u221a N for model dimension d. Our intuition is that it is difficult to store large numbers\nof key-value pairs seen in the sequence in low-dimensional hidden states. Motivated by this analysis, we propose Procedure 1 for generating synthetic MQAR data. This procedure allows toggling two simple properties of the synthetic dataset, specified below, to reveal the differences between the sequence modeling architecture families. We also discuss deviations from the prior work that uses synthetic recall data.\n1. Size of the Key-Value Map: The number of unique key-value pairs appearing in each example in the dataset. In Procedure 1, we use the input parameter D to toggle this value. 2. Number of Gaps in the Data: The number of unique token-interaction distances required to perform the recall task. Prior work assumes there is a single query token that requires recall per example, failing to test whether the architecture can support multiple tokeninteraction distances. In Procedure 1, we use the parameters N (sequence length), D (size of the key-value map per example), and \u03b1 to toggle this property. We enforce that the tokeninteraction distances appearing in the synthetic examples follow a power-law distribution specified by \u03b1 (based on Figure 3). Keeping \u03b1 and D constant while varying N changes the number of unique token-interaction distances appearing in the example.\nFinally, noting that the vocabulary size appears in the theoretical bounds for all three architecture families (Section 4), we increase the vocabulary size to be much larger than the model dimension as typically seen in language modeling (30k - 50k tokens). Prior work uses vocab sizes \u2264 40 tokens.\nAlgorithm 1 MQAR Synthetic Procedure Input: Vocabulary C, Sequence length N , Power-law parameter \u03b1, Number of Key-Value Pairs D\nOutput: Synthetic sequence 1: Let the first half of C be keys K and the second half be values V . 2: Pair each key token k \u2208 K with a random value token v \u2208 V . 3: Sub-select D random key-value pairs to include in the sequence. 4: Place the D key-value pairs at the start of the sequence (i.e. consuming the first 2D positions). 5: Place a second occurrence of each d \u2208 D at a distance from the first occurrence in the sequence.\nThe distance for each d \u2208 D is selected at random from the positions [2D..N ], where the probability of choosing each position follows the power law distribution specified by \u03b1.\n6: Output the synthetic sequence."
        },
        {
            "heading": "E.2 TRAINING DETAILS",
            "text": "We first describe the architectures evaluated on MQAR synthetic data and then the training hyperparameters. We evaluate the following four architecture categories in the experiments:\n1. Attention Standard GPT-2 style multi-headed Transformer architecture with learned positional embeddings Brown et al. (2020). Attention, the core building block of Transformers, is defined in Section 2. The architecture for synthetics has 1 attention head. 2. Gated convolutions (Hyena Poli et al. (2023a), RWKV Peng et al. (2023)). Gating, convolutions, and the class of gated convolutions are defined in Section 2. 3. Gated recurrences (RetNet Sun et al. (2023)). RetNet proposes computing attention over chunks of the sequence and combining this with a recurrence over the sequence. We evaluate this architecture (\u201cChunked RNN\u201d) with chunk sizes of 32 and 8 in Figure 6. In RetNet, hidden states are updated as:\nSn = \u03b3Sn\u22121 +A T nVn\nOutputs at each timestep are defined as\nOn = CnSn\nwhere \u03b3 \u2208 R1, A = xWa for input x \u2208 N\u00d7 and learned weight matrix Wa \u2208 Rd\u00d7d, C = xWc for Wc \u2208 Rd\u00d7d, V = xWv for Wv \u2208 Rd\u00d7d. RetNet similar to a state-space-model (SSM) Gu et al. (2021), except for that the matrices A and C are input-dependent (i.e. they are functions of the input x). We note that RetNet is a special case of the recently proposed Mamba Anonymous (2023a) and GateLoop Anonymous (2023b) architectures, which replace the \u03b3 term in RetNet with yet another input-dependent matrix. The RetNet model is formally defined in Appendix H.2.3. 4. Sparse attention Several works have proposed methods for efficiently computing attention Tay et al. (2022). We evaluate two classical methods: sliding window attention Beltagy et al. (2020); Zaheer et al. (2020) and blocked window attention Child et al. (2019b); Qiu et al. (2019). Consider a window size w. Sliding window attention permits each token to attend to the prior w tokens in the sequence. Blocked window attention first splits the sequence into blocks of size w tokens, and then computes standard causal attention within the block.\nFor all synthetic runs in the main paper and the appendix, we use the following training protocol: \u2022 Optimizer and schedule: Weight decay 0.1, warmup duration 10%, linear warmup,\nAdamW optimizer. For each run, we sweep the learning rates in np.logspace(\u22124,\u22122, 4). We train for 64 epochs. \u2022 Training duration: The global batch size is 8 for input sequence length or model dimension \u2265 512, 16 for input sequence length or model dimension \u2265 256, and 64 otherwise. \u2022 Width and depth: For all synthetic experiments, we use exactly two layers (each with one sequence mixer and one MLP, interleaved with layer normalization). The model dimension, sequence length, and number of KV pairs are varied based on the relevant experiment (see Appendix F). \u2022 Position information: No position embeddings are used for the pure convolution runs (input-dependent nor input-independent filters). Position embeddings are used for the runs with any attention variant. \u2022 Data: We train and evaluate each model on 100, 000 and 3, 000 data points respectively. The data and data order for all runs are constant."
        },
        {
            "heading": "F EXTENDED RESULTS ON MQAR ACROSS ARCHITECTURES",
            "text": "In this section, we provide further validation that MQAR is a useful diagnostic tool to explain the behaviors of a wide variety of architecture families. We show:\n1. Gated convolutions In the main paper, we claim the gated convolution models with input-independent filters use larger dimensionality than attention as the number of tokeninteraction distances required for the task increases (N , holding \u03b1 and D constant in Procedure 1). 2. Gated recurrences In this section, we show gated recurrrent models use larger dimensionality than attention to solve the task as the number of unique key-value pairs to store (D in Procedure 1) increases. 3. Sparse attention In this section, we validate that sliding window attention helps close the MQAR gap when the token-interaction distances are relatively short, and degrades on\nlonger distances. Meanwhile, blocked window attention struggles to close the gap (intuitively tokens earlier in a block are able to interact with few other tokens in the sequence). Concretely, in Appendix D, we observe that repeated bigrams in the Pile and RedPajama corpora tend to occur within neighboring tokens in the context.\nWe finally show the architectures that perform well on MQAR also perform well on downstream realworld AR on the Pile. We use the experimental protocols outlined in Appendix E for the synthetic experiments and Appendix C for the downstream experiments in this section."
        },
        {
            "heading": "F.1 SYNTHETIC EXPERIMENTS",
            "text": "Experiment 1: Increasing the number of token-interaction distances per example. To construct the synthetic dataset following Procedure 1, we set \u03b1 = 0.1 and |C| = 8, 192 for all experiments. We vary N \u2208 {64, 128, 256, 512} to increase the number of token-interaction distances that will appear in the examples and we vary the model dimension \u2208 {64, 128, 256, 512}. The results of this experiment are shown in Fig. 2, validating that the gated convolutiosn use larger dimensionality than attention as the number of token-interaction distances required for the task increases.\nExperiment 2: Increasing the number of key-value pairs occurring per example. To construct the synthetic dataset following Procedure 1, we set \u03b1 = 0.1 and |C| = 8, 192 for all experiments. We fix N = 256 and vary D \u2208 {N32 , N 16 , N 8 , N 4 } to increase the number of key-value pairs occurring per example. For each number of key-value pairs per example, we additionally vary the model dimension \u2208 {64, 128, 256}. The results are shown in Fig. 6. We validate that as the number of associative recall keys and values exceeds the window size, the model again uses increased dimensionality relative to full O(N2) attention to solve the task. While Fig. 6 highlights RetNet, we note that it is a special case of the recently released Mamba Anonymous (2023a) and GateLoop Anonymous (2023b) architectures. Corresponding to this experiment, we theoretically show in Appendix H.6 that the gated recurrence uses \u2126(N) bits to solve MQAR for d \u2264 \u221a N . Intuitively, it is difficult to store a large number of key-value pairs in low dimensional hidden states.\nExperiment 3: Increasing the range of token-interaction distancs per example. To construct the synthetic dataset following Procedure 1, we set \u03b1 = 0.1 and |C| = 8, 192 for all experiments. We fix N = 256 and vary D \u2208 {N32 , N 16 , N 8 , N 4 } to increase the number of key-value pairs occurring per example and the range of token-interaction distances per example. For each number of key-value pairs per example, we additionally vary the model dimension \u2208 {64, 128, 256}. The results are shown in Fig. 6, validating that sliding window attention closes the gap to attention when the number of KV pairs is within the window size. I.e., 16 KVs means that there are 16 keys and 16 values, for 32 total tokens. Sliding attention with window size 32 performs well up until 16 KVs and degrades beyond this point relative to attention models of the same dimensionality. Sliding window attention with window size 8 does not perform well on any setting, as expected. Blocked\nattention also not perform well \u2013 intuitively tokens at the beginning of a block lack sufficient prior context to attend to."
        },
        {
            "heading": "F.2 DEMONSTRATING MQAR AS A TOOL FOR ARCHITECTURE DEVELOPMENT",
            "text": "In the main paper, we provide experiments showing that there is gap between the gated convolution and attention models on synthetic MQAR data and correspondingly in the downstream Pile results. In Table 4, we show that the trends of RetNet and sparse attention on MQAR also carry to the downstream results. We hope MQAR synthetic data may be useful in future architectural development."
        },
        {
            "heading": "G MQAR PERPLEXITY GAP AND MODEL SIZE",
            "text": "In this section, we investigate how the associative recall gap changes as we increase the model size to a billion parameters and beyond. Overall, the results suggest that the larger models improve in memorizing bigrams from the training data, but do not rapidly improve in in-context learning (perplexity on rare bigrams). Below, we pretrain at the 1.4Bn parameter scale and observe that the 70M parameter attention model is a full perplexity point better on AR than this 1.4Bn Hyena model (Table 1). We discuss these results further in (Section 3).\nOpen-source 7B parameter models We next evaluate the RWKV (gated convolution) Peng et al. (2023) and Llama 2 (attention) pretrained models at the 7B parameter scale Touvron et al. (2023).9 These are both popular models that took a significant amount of effort to train, towards maximizing quality. We find that there is a gap between RWKV and attention at the 7B scale and that it increases as the model needs to conduct more recalls per sequence (P below). We summarize the experimental protocol below.\n1. Justifying the experimental protocol. Since frequent bigrams may just be memorized by the model and not require in-context recall, our work measures AR quality on infrequent bigrams in validation sequences (Figure 1, Appendix C.1). We do not have access to the custom training data mixtures used in training RWKV or Llama 2 to measure bigram frequencies, so we use a synthetic test to fairly measure the AR capabilities. 2. Evaluation dataset. We construct the synthetic dataset following Algorithm 1, where each sequence contains P token pairs, or \u201cbigrams\u201d. Each bigram, containing a \u201ckey\u201d token and a \u201cvalue\u201d token, appears twice in the sequence. On the second occurrence of a \u201ckey\u201d token, the model should look back to the prior occurrence to output the corresponding \u201cvalue\u201d token. We measure the AR perplexity of the model based on its ability to predict the correct \u201cvalue\u201d token as the next word for these repeated keys. The sequences are constructed using the models\u2019 vocabulary tokens. 3. Evaluation details. We evaluate (inference only, no training) on sequence lengths of 1024 tokens. We measure the AR perplexity when the sequences contain P \u2208 {16, 32, 64, 128, 256} key-value pairs, using 1000 samples per P value. The tokens that do not contain a key or value are simply filled with a fixed token id (so this token is repeated frequently in the sequence). We plot perplexity for AR and non-AR tokens (fixed token) vs. P . We find RWKV quality degrades with P on the AR slice (blue line), while all other lines remain flat. MQAR remains problematic at scale.\nOverall our results suggest that simply scaling the model size may not close the MQAR quality gap between the gated convolutions and attention.\nMeasuring AR Gap with Scale In the main paper, we measure the AR perplexity on downstream data based on bigrams that are seen < 1, 250\u00d7 during pretraining. However, we hypothesize that larger models may memorize a larger number of bigram pairs seen in the training dataset, but do not rapidly gain the capability to perform associative recall as well as attention in-context. In-context learning is defined as learning from examples provided in context (Xie et al., 2021).\n9We specifically evaluate the RWKV-Raven model downloaded from https://huggingface. co/docs/transformers/model_doc/rwkv and the Llama 2 model downloaded from https:// github.com/facebookresearch/llama.\nConcretely, the gap between the Hyena / RWKV and attention models at the 350m scale is 1.85 / 1.84 perplexity points when we focus on bigrams seen < 1, 250\u00d7 during pretraining (Table 1). If we instead focus on bigrams seen 1\u00d7 during pretraining, the gap to attention quality is 12.0 / 13.2 perplexity points respectively. The gated convolutions appear to struggle in the regime of rare bigrams that require the model to use the context."
        },
        {
            "heading": "H DETAILS ON THEORETICAL ANALYSIS",
            "text": "This section provides proofs and extensions to the theoretical results in the main paper."
        },
        {
            "heading": "H.1 PRELIMINARIES AND NOTATION H.1.1 NOTATION",
            "text": "We denote the all 1 row vector of size k, given by [1 1 . . . 1 1], and the all 0 row vector of size k, given by [0 0 . . . 0 0], as 1k and 0k, respectively. We also construe the standard basis vector ei as a column vector in these notes, and adhere to the following matrix indexing convention: M[i, j] is the entry in the ith row and the jth column, M[i, :] \u2208 F1\u00d7n denotes the ith row, and M[:, j] \u2208 Fm\u00d71 denotes the jth column of M \u2208 Fm\u00d7n. We then use 1m\u00d7n,0m\u00d7n \u2208 Fm\u00d71 to denote the matrix of all 1s and 0s, respectively. Next, we denote the Hadamard product of vectors u,v \u2208 Fn as u\u2299v; the operation can be extended to matrices by applying the Hadamard product column-wise across the matrices. This is commonly referred to as (element-wise) gating. For vectors u,v \u2208 Fn, we also denote their linear (or acyclic) convolution as u \u2217 v and cyclic convolution as u\u229b v.\nPolynomial Notation. Because convolution is intimately tied to operations on polynomials, it is convenient to use them to discuss the inputs and outputs of gated convolution models. Let us define maps poly,poly\u2217 : Fn \u2192 F[X]/(Xn) such that\npoly(u) = n\u22121\u2211 i=0 u[i]Xi, and poly\u2217(u) = n\u22121\u2211 i=0 u[i]Xn\u22121\u2212i.\nThis allows us to map between vectors and polynomial. Accordingly, we also define coeff : F[X]/(Xn+1) \u2192 Fn as the map converting polynomials back to vectors: coeff(u(X)) = u with u[i] defined as the coefficient in u(X) at degree i. These operations allow us to interpret the convolution of vectors in terms of polynomial multiplication (Heideman and Burrus, 1988). More specifically, we have\nu \u2217 v = coeff (u(X) \u00b7 v(X) mod Xn) , and u\u229b v = coeff (u(X) \u00b7 v(X) mod Xn \u2212 1) .\nWe can similarly interpret the Hadamard product of vectors u \u2299 v as the Hadamard product of polynomials u(X)\u2299 v(X):\nu\u2299 v = coeff (u(X)\u2299 v(X)) = coeff ( n\u22121\u2211 i=0 (u[i] \u00b7 v[i]) \u00b7Xi ) .\nArithmetic Circuit Notation. We briefly introduce the notation of arithmetic circuits (Volkovich, 2016), the focus of Appendix H.5. An arithmetic circuit C with variables X \u225c {x1, x2, . . . , xn} over a field F is interpreted as a directed acyclic graph, where the input nodes are labelled by either the variables from X or constants from F and the internal nodes are labelled by + or \u00d7 with the output being the polynomial computed at the output node. We shall also refer to the size of the circuit as the number of nodes, the depth of the circuit as the length of the longest path between an input node and the output node, and the width of the circuit as the number of parallel operations in the circuit, or \u2018wires\u2019 which will be intersected by a horizontal \u2018cut\u2019 through the circuit. Moreover, the degree of a circuit is defined as the degree of the polynomial computed by the circuit. We summarize this with the following definition: Definition H.1. An arithmetic circuit C is an (n, s,\u2206, w)-circuit if C is an n-variate arithmetic circuit of size s and of depth at most \u2206, and width w.\nModel Notation. Now we introduce the notation we will be using for defining layers. In what follows, we denote u \u2208 RN\u00d7d as the model input; N as the sequence length; L as the number of stacked layers, indexed by \u2113; and d as the input (embedding) dimension."
        },
        {
            "heading": "H.1.2 SUMMARY OF THE RESULTS",
            "text": "The outline of our results are as follows: In Appendix H.2 we introduce gated convolution models and define H3, Hyena, RWKV, RetNet, and BASECONV. In Appendix H.3 we introduce a set of primitive operations that BASECONV can implement. We use these as tools in the subsequent proofs. Then, in Appendix H.5, we show that a general arithmetic circuit of size s and degree at most \u2206 can be simulated by BASECONV. We use the above results to show all models built from gating and convolutions can be simulated by BASECONV. This helps us analyze the class of gated convolutions, beyond a specific proposal (e.g. Hyena). Next, we study the representational power that gated convolutions and attention use to solve MQAR. In Appendix H.7, we derive a BASECONV model inspired from dyadic intervals to show the dimensionality for gated convolutions (with input-independent filters) solve MQAR. Next, we analyze the dimensionality for a BASECONV model with data-dependent kernels to solve MQAR in Appendix H.8."
        },
        {
            "heading": "H.2 GATED ATTENTION-FREE MODELS",
            "text": "We now present formal definitions of gated convolution and recurrence models with respect to the 5-tuple of parameters (N,L, d,N \u2032, d\u2032). Definition H.2. An (N,L, d,N \u2032, d\u2032)\u2212Gated Convolution Model is a stacked sequence to sequence model with L layers such that:\n1. Input and output are N \u00d7 d matrices, 2. Each layer\u2019s operations consist of element-wise gating, convolution, linear projection, and 3. All the individual gated convolution layers take in N \u2032 \u00d7 d\u2032 matrices and output N \u2032 \u00d7 d\u2032\nmatrices. We refer to the tuple (N \u2032, d\u2032) as the inner dimension of the model. We define the Hyena, RWKV, RetNet, and BASECONV layers to make step 2 more concrete. We also assume that the input u \u2208 RN\u00d7d is embedded into u\u2032 \u2208 RN \u2032\u00d7d\u2032 such that\nu\u2032[n, t] = { u[n, t] if n < N, t < d 0 otherwise.\nThe output from the last layer z \u2208 RN \u2032\u00d7d\u2032 is transformed into output y \u2208 RN\u00d7d by extracting the top left N \u00d7 d entries in z. Next, we define the class of weight matrices that we will use in the linear projections in the models: Definition H.3. The linear projection Linearm,m has its matrix representation as the weight matrix10W \u2208 Rm\u00d7m taken to be a K-matrix for W \u2208 (BB\u2217)poly- logmpoly- logm (Definition H.17). Consequently, each matrix W has O\u0303(m) parameters and runtime for matrix vector multiplication, and allows us to represent general linear transformations with low-depth linear arithmetic circuits (Dao et al., 2020). We will also need linear maps Linearm,n for m < n, where each take the corresponding square matrices from Linearm,m and note that such matrices has O\u0303(n) parameters and runtime for matrix vector multiplication. Remark H.4. We note that the weight matrix W \u2208 Rd\u00d7d above is taken to be a dense matrix in the experiments. However, for our theoretical results, we restrict the linear projection in our models as per Definition H.3. In the rest of the paper, unless mentioned otherwise all linear projections will follow Definition H.3. Next, we define three popular gated convolution models that we study in our work: Hyena Poli et al. (2023a), RWKV Peng et al. (2023), and RetNet Sun et al. (2023)."
        },
        {
            "heading": "H.2.1 THE HYENA LAYER",
            "text": "We will now outline the Hyena layer (Poli et al., 2023b). Hyena takes a sequence u \u2208 RN\u00d7d as input and produces L+ 1 projections p1, . . . , pL, v by passing y though a linear layer and applying a short convolution afterwards. The algorithm then recursively performs a point-wise multiplication of the projection with the convolution of the filter hl with the previous output. We summarize this process in Algorithm 3.\n10I.e. Linearm,m(z) = W \u00b7 z.\nAlgorithm 2 Projection (u,h)\nInput: Input sequence u \u2208 RN\u00d7d, a short convolution filter h \u2208 RN . 1: In parallel for 0 \u2264 n < N : z\u0302[n, :]\u2190 Lineard,(L+1)d (u[n, :]) so that z\u0302 \u2208 RN\u00d7(L+1)d 2: In parallel for 0 \u2264 t < (L+ 1)d : z[:, t]\u2190 h \u2217 z\u0302[:, t] 3: Reshape and split z \u2208 RN\u00d7(L+1)d into p1, . . . ,pL,v, where p\u2113,v \u2208 RN\u00d7d for \u2113 \u2208 [L]. 4: return p1, . . . ,pL,v.\nAlgorithm 3 Hyena (u,h,hs)\nInput: Input sequence u \u2208 RN\u00d7d, set of convolution filters h1, . . . ,hL \u2208 RN\u00d7d, short convolution filter hs \u2208 RN .\n1: p1, . . . ,pL,v \u2190 Projection(u,hs). 2: z0 \u2190 v 3: for \u2113 = 1, . . . , L do 4: In parallel for 0 \u2264 t < d : z\u2113[:, t]\u2190 p\u2113[t, :]\u2299 ( h\u2113[:, t] \u2217 z\u2113\u22121[:, t] ) .\n5: return zL\nRemark H.5. In Algorithm 3, L is used to denute the nuber of recursive applications of the Hadamard product and convolutions, not the number of layers Note that asymptotically, the recursive step does not make a difference. Henceforth, we will refer to a model consisting of L Hyena layers is a gated convolution model with associated tuple (N,L, d,N, (L+ 1)d) as (N,L, d,N, (L+ 1)d)\u2212 Hyena."
        },
        {
            "heading": "H.2.2 THE RWKV LAYER",
            "text": "We will now describe the RWKV layer (Peng et al., 2023). RWKV is typically referred to as an RNN, but, like some other recurrent models (e.g. S4 (Gu et al., 2021)), it can also be viewed as a convolution. Here, we present the convolutional view of RWKV. We will see that it is closely related to the Hyena layer. RWKV takes a sequence u \u2208 RN\u00d7d as input and applies a short convolution. Next, it produces three projections q, k, v by passing u through a linear layer. Then, it performs a convolution sandwiched by element-wise multiplication. We summarize this process in Algorithm 5.\nAlgorithm 4 RWKVProjection (u,h)\nInput: Input sequence u \u2208 RN\u00d7d, a short convolution filter h \u2208 RN . 1: In parallel for 0 \u2264 t < d : z\u0302[:, t]\u2190 h \u2217 u[:, t] 2: In parallel for 0 \u2264 n < N : z[n, :]\u2190 Lineard,3d (z\u0302[n, :]) so that z \u2208 RN\u00d73d 3: Reshape and split z \u2208 RN\u00d73d into q,k,v \u2208 RN\u00d7d. 4: return q,k,v.\nIn practice, the short convolution filter hs is restricted to a length two filter with hs[0] = \u00b5 and hs[1] = 1\u2212 \u00b5, where \u00b5 \u2208 [0, 1] is learned parameter. In the RWKV paper, the short convolution is referred to as a \u201ctime shift\u201d.\nAlgorithm 5 RWKV (u,h,hs)\nInput: Input sequence u \u2208 RN\u00d7d, set of convolution filters h1, . . . ,hL \u2208 RN\u00d7d, short convolution filter hs \u2208 RN .\n1: q,k,v \u2190 Projection(u,hs). 2: In parallel for 0 \u2264 t < d : z\u2113[:, t]\u2190 \u03c3(q[:, t])\u2299 ( h\u2113[:, t] \u2217 (softmax(k)\u2299 v)[:, t] ) .\n3: return zL\nIn practice, the long convolution filter h is also restricted to h[:, t] = ew(t\u22121), where w \u2208 R is a learnable parameter that controls how quickly the magnitudes of the filter decreases as t grows. To summarize, the differences between Hyena and RWKV are: (1) Hyena applies the short convolution after the linear projection whereas RWKV applies it before, (2) RWKV includes non-linear\nactivations (sigmoid and softmax) while Hyena does not, (3) RWKV and Hyena use different parameterizations for the convolutional filters, and (4) Hyena recursively performs a point-wise multiplication of the projections with the convolution filter whereas RWKV performs this operation only once (though, in practice, Hyena uses a recurrence depth of one, making them equivalent in this regard)."
        },
        {
            "heading": "H.2.3 THE RETNET LAYER",
            "text": "In this section, we introduce the RetNet layer Sun et al. (2023). To this end, we take an input sequence u \u2208 RN\u00d7d and project it using learnable weight matrices. We then compute the states zn recurrently as in line 5 and the output Out[n, :] as in line 6 for each n \u2208 [N ] (see Algorithm 6 below).\nAlgorithm 6 RetNet (u,h,hs)\nInput: Input sequence u \u2208 RN\u00d7d, a scalar \u03b3 \u2208 R, learnable weight matrices WA,WC ,WV \u2208 R\nd\u00d7d. 1: A,C,V\u2190 uWA,uWC ,uWV so that A,C,V \u2208 RN\u00d7d. 2: Initialize the output Out \u2208 RN\u00d7d 3: Initialize the state z0 \u2190 (A[0, :])\u22a4 V[0, :]. 4: for 1 \u2264 n < N do 5: zn \u2190 \u03b3zn\u22121 + (A[n, :])\u22a4 V[n, :] for zn \u2208 Rd\u00d7d 6: Out[n, :]\u2190 C[n, :]zn\n7: return Out"
        },
        {
            "heading": "H.2.4 BASECONV",
            "text": "Finally, we introduce the BASECONV here as follows:\nY = (uW + b1)\u2299 (u \u2217 h+ b2), (4)\nwith input u \u2208 RN \u2032\u00d7d\u2032 , weight matrix W \u2208 Rd\u2032\u00d7d\u2032 and bias matrices bi \u2208 RN \u2032\u00d7d\u2032 defining linear projections of the input sequence, and h \u2208 RN \u2032\u00d7d\u2032 is the a set of the d\u2032 mixed length filters. The corresponding pseudocode for BASECONV is as follows:\nAlgorithm 7 BASECONV (u,W ,b1,h,b2)\nInput: Input sequence u \u2208 RN \u2032\u00d7d\u2032 , linear mapping W \u2208 Rd\u2032\u00d7d\u2032 so that W \u2208 (BB\u2217)poly log d \u2032\npoly log d\u2032 , convolution filter h \u2208 RN \u2032\u00d7d\u2032 , bias matrices b1,b2 \u2208 RN\n\u2032\u00d7d\u2032 . 1: In parallel for 0 \u2264 n < N \u2032 : x[n, :] = Lineard\u2032,d\u2032 (u[n, :]) 2: In parallel for 0 \u2264 t < d\u2032 : z[:, t] = h[:, t] \u2217 u[:, t]\n3: In parallel for 0 \u2264 t < d\u2032 : y[:, t]\u2190 (x[:, t] + b1[:, t])\u2299 (z[:, t] + b2[:, t]). \u25b7 See equation 4 4: return y\nNote that the convolution in Algorithm 7 is not limited to causal convolution and allows circular convolution as well. For simplicity, we use \u2217 here and will disambiguate when required. We will start by specifying the parameter count and complexity of a single layer of BASECONV below. Proposition H.6. A single layer of BASECONV requires O\u0303(N \u2032d\u2032) parameters and runtime.\nProof. We know the parameters and runtime for the linear part of BASECONV via Definition H.3 is O\u0303(N \u2032d\u2032). Further, each convolution operation requires O(N \u2032) parameters and O(N \u2032 logN \u2032) runtime, and we employ d\u2032 such operations. Finally the Hadamard product step takes O(nd) time.\nSimilar to Hyena, we will refer to a model consisting of L BASECONV layers as (N,L, d,N \u2032, d\u2032)\u2212 BASECONV. In our experiments, we extend BASECONV by adding an MLP after Algorithm 7. For simplicity we will denote BASECONV (u,h, b1, b2) as BASECONV (u,h) when b1 = b2 = 0. We will now show that there exists a BASECONV model that can emulate each of the basic operations in Algorithm 7.\nLemma H.7. The functions Lineard,d (u) (Definition H.3), with d, d\u2032 defined as in Algorithm 7, convolution with filter h \u2208 RN\u00d7d, and element-wise gating can be computed with Algorithm 7 via a (N, 1, d,N, d\u2032)\u2212 BASECONV.\nProof. For each operation from Definition H.2 and Algorithm 7: 1. For any input u \u2208 RN \u2032\u00d7d\u2032 , Lineard,d\u2032 (u) with matrix representation W \u2208 RN\n\u2032\u00d7d\u2032 can be performed by a single BASECONV layer computing BASECONV (y,W ,h, b1, b2) with b1 and b2 being the matrix of all 0s and all 1s, respectively while and the convolution with the zero filter. That is, we have\nY = (uW+0N \u2032\u00d7d\u2032)\u2299(u\u22170N \u2032\u00d7d\u2032+1N \u2032\u00d7d\u2032) = (uW)\u22991N \u2032\u00d7d\u2032 = uW = Lineard,d\u2032 (u) .\n2. For any input u \u2208 RN\u00d7d, convolution with filter h \u2208 RN\u00d7d can be performed by a single BASECONV layer computing BASECONV (y,W ,h, b1, b2) where W, b2 are all zeroes, and b1 is the matrix of all 1s so that we get\nY = (u0N \u2032\u00d7d\u2032 + 1N \u2032\u00d7d\u2032)\u2299 (u \u2217 h+ 0N \u2032\u00d7d\u2032) = 1N \u2032\u00d7d\u2032 \u2299 (u \u2217 h) = u \u2217 h.\n3. We may compute element-wise gating between matrices u,v \u2208 RN\u00d7d, where v is some fixed factor, with a single layer computing BASECONV (y, ,)0N \u2032\u00d7d\u2032 , e0,v,0 N \u2032\u00d7d\u2032 where\ne1 is the identity filter, respectively, by Definition H.2.\nY = (u0N \u2032\u00d7d\u2032 + v)\u2299 (u \u2217 e0 + 0N \u2032\u00d7d\u2032) = v \u2299 u."
        },
        {
            "heading": "H.3 PRIMITIVES",
            "text": "In this section, we will establish some additional basic primitives that we expect a gated convolution model to emulate: shift,remember and add. We specify them below:\n1. Shift an sequential input of length N up or down by s entries:\nshift up(y, s), shift down(y, s)\n\u2022 Input: y \u2208 RN\u00d7d, s \u2265 0 \u2022 Output: z \u2208 RN\u00d7d where z+ = shift down(y, s) and z\u2212 = shift up(y, s)\ny \u2261  \u2190 y0 \u2192 ... \u2190 yi\u22121 \u2192 \u2190 yi \u2192 ...\n\u2190 yN\u22121 \u2192\n\nz+ \u2261  \u2190 0\u2192 ... \u2190 0\u2192 \u2190 y0 \u2192 ...\n\u2190 yN\u22121\u2212s \u2192\n\nz\u2212 \u2261  \u2190 ys \u2192 ... \u2190 yN\u22121 \u2192 \u2190 0\u2192 ...\n\u2190 0\u2192\n\n2. Add a sequence x \u2208 Rn\u00d7d to a running sum S \u2208 Rn\u00d7d for some 2n \u2264 N \u2032 with both x and S contained as subvectors in y \u2208 RN\u00d7d.\naddn(y : x,S)\n\u2022 Input: sequence y containing x,S \u2208 Rn\u00d7d for 2n \u2264 N such that y[0 : n\u22121] \u2261 x,y[n : 2n\u2212 1] \u2261 S and y[2n : N \u2212 1] \u2261 0N\u22122n. \u2022 Output: z \u2208 RN\u00d7d containing the sum y+S such that y[0 : n\u22121] \u2261 1n,y[n : 2n\u2212 1] \u2261 S + x and z[2n : N \u2212 1] \u2261 0N\u22122n.\ny \u2261  \u2190 x\u2192 \u2190 S \u2192 \u2190\u2212 0 \u2212\u2192 ...\n\u2190 0\u2192\n z \u2261  \u2190 1n \u2192 \u2190 S + x\u2192 \u2190\u2212 0 \u2212\u2192 ...\n\u2190 0\u2192\n\n3. Remember v \u2208 Rm\u00d7d as part of a sequence of input y \u2208 RN\u00d7d while performing gated convolution only on x \u2208 Rn\u00d7d for some m,n \u2264 N \u00d7 d.\nremembern,m,s,t(y : x, v, h, p)\n\u2022 Input: sequence y \u2208 RN\u00d7d containing x \u2208 Rn\u00d7d,v \u2208 Rm\u00d7d, and modifiers p,h \u2208 Rn\u00d7d such that y[0 : n \u2212 1] \u2261 x,y[n + s : n + s + m \u2212 1] \u2261 v and y[i] = 0 otherwise with x \u2217 h \u2208 R(n+s)\u00d7d and v \u2217 h \u2208 R(m+t)\u00d7d. \u2022 Output: z \u2208 RN\u00d7d containing (p\u2299 (x \u2217 h)) \u2208 R(n+s)\u00d7d, such that:\ny \u2261  \u2190 x\u2192 0s \u2190 v \u2192 ...\n0\n z \u2261  \u2190 p\u2299 (x \u2217 h)\u2192 \u2190 v \u2192 ...\n0\n\nThese primitives are building blocks of our proofs in the sequel. We will show that each of these primitives can be solved by some (N,L, d,N \u2032, d\u2032)\u2212 BASECONV model with a small constant L. Proposition H.8 (The Shift Primitive). For any y \u2208 RN\u00d7d, there exist (N, 1, d,N, d)\u2212BASECONV and (N, 3, d,N, d)\u2212 BASECONV that computes shift down(y, s) and shift up(y, s) for any s \u2264 N .\nProof. Define the following kernel dependent on s \u2264 N\nhs[n, :] \u2261 { 1d if n = s+ 1 0d otherwise.\nWe now deal with the down and up shifts separately: 1. We define W := 0N\u00d7d, b1 := 1N\u00d7d, b2 := 0N\u00d7d. Then, for input y \u2208 RN\u00d7d, BASECONV ( y,0N\u00d7d,hs,1 N\u00d7d,0N\u00d7d )\nfor BASECONV in Algorithm 7 is given by equation 4 as\nY \u2261 y \u2217 hs.\nNow, to perform shift down(y, s), we note that\nY[:, j] = y[:, j] \u2217 hs[:, j] = coeff(y[:, j](X) \u00b7 hs[:, j](X))\n= coeff (( N\u22121\u2211 i=0 y[i, j] \u00b7Xi ) \u00b7Xs mod XN )\n= coeff ( N\u22121\u2211 i=0 y[i, j] \u00b7Xi+s mod XN )\n= coeff\n( N\u22121+s\u2211\ni=s\ny[i\u2212 s, j] \u00b7Xi mod XN )\n= coeff ( N\u22121\u2211 i=s y[i\u2212 s, j] \u00b7Xi ) ,\nwhich implies that we exactly get what is specified in the output. 2. We again define W := 0N\u00d7d, b1 := 1N\u00d7d, b2 := 0N\u00d7d. Then, for input y \u2208 RN\u00d7d, BASECONV ( y,0N\u00d7d, e0,1 N\u00d7d,0N\u00d7d )\nfor BASECONV in Algorithm 7 is given in equation 4 as\nY0 \u2261 y \u229b e0.\nNow, to perform shift up(y, s), as before, we first apply the circular convolution to reverse the input\nY0[:, j] = y[:, j]\u229b e0 = coeff ( N\u22121\u2211 i=0 y[N \u2212 1\u2212 i, j] \u00b7Xi ) ,\nWe then apply Y1 \u2261 shift down(Y0, s) to get\nY1[:, j] \u2261 coeff ( N\u22121\u2211 i=s Y0[N \u2212 1\u2212 (i\u2212 s), j] \u00b7Xi ) ,\n\u2261 coeff ( N\u22121\u2211 i=s Y0[N \u2212 1\u2212 i+ s, j] \u00b7Xi ) .\nFinally, we apply another circular convolution with the identity filter to replace N \u2212 1\u2212 i with i to get\nY2[:, j] = Y1[:, j]\u229b e0 = coeff ( N\u22121\u2211 i=0 y[i+ s, j] \u00b7Xi ) ,\nHere, we note that we can compute both of these primitives in one and three layers, respectively (see Lemma H.11).\nNow, we present a BASECONV model with two layers that implements the addn(y : x,S), the purpose of which is to add some window of computation x to a running sum S. Proposition H.9 (The Running Sum Primitive). For any x,S \u2208 Rn\u00d7d contained in some y \u2208 R\nN\u00d7d, there exists a (N, 2, d,N, d)\u2212 BASECONV that computes addn(y : x,S) for BASECONV as in Algorithm 7.\nProof. We will show this for d\u2032 = 1 and the general case follows as we will explain at the end. We now specify the two layers that we use\nz1 \u2261 BASECONV ( y,0N\u00d71,h1, b11,0 N\u00d71) \u2261 b11 \u2299 (h1 \u2217 y) z \u2261 BASECONV ( z1,0N\u00d71,h2, b21, b 2 1 ) \u2261 b21 \u2299 ( h2 \u2217 y + b22 ) ,\nwhere we will specify the kernels as we go along. Let us start by defining the kernel and the bias for the first layer as\nh1 \u2261  e0 e0 0n \u00b7 \u00b7 \u00b7\n0n\n , b1 \u2261  0n 1n 0n \u00b7 \u00b7 \u00b7\n0n\n .\nLet us first compute h1 \u2217 y as follows:\nh1(X) \u00b7 y(X) = (Xn + 1) \u00b7 (S(X) \u00b7Xn + x(X)) = S(X) \u00b7X2n + (S + x)(X) \u00b7Xn + x(X).\nWe then have\nz1 \u2261 b11 \u2299 ( h1 \u2217 y ) \u2261  0n 1n 0n \u00b7 \u00b7 \u00b7\n0n\n \u2299  x S + x S \u00b7 \u00b7 \u00b7\n0n\n \u2261  0n S + x 0n \u00b7 \u00b7 \u00b7\n0n  Resetting for Next Phase. We now use the next layer to reset for the next phase. Here, we need the first vector to be 1n in order to start adding the next vector. We thus use the kernel and the biases h2, b21, b 2 2 defined as\nh2 \u2261  e0 0n 0n \u00b7 \u00b7 \u00b7\n0n\n , b21 \u2261  1n 1n 0n \u00b7 \u00b7 \u00b7\n0n\n , b22 \u2261  1n 0n 0n \u00b7 \u00b7 \u00b7\n0n\n .\nExplicitly, for the second layer, we compute the result of the convolution in terms of polynomials as follows:\nh2(X) \u00b7 z1(X) = 1 \u00b7 (S + x)(X) \u00b7Xn = (S + x)(X) \u00b7Xn.\nThus, the output for the second layer is given by\nz \u2261 b21\u2299 ( h2 \u2217 z1 + b22 ) \u2261  1n 1n 0n \u00b7 \u00b7 \u00b7\n0n\n \u2299   0n S + x 0n \u00b7 \u00b7 \u00b7\n0n\n +  1n 0n 0n \u00b7 \u00b7 \u00b7\n0n\n  \u2261  1n 1n 0n \u00b7 \u00b7 \u00b7\n0n\n \u2299  1n S + x 0n \u00b7 \u00b7 \u00b7\n0n\n \u2261  1n S + x 0n \u00b7 \u00b7 \u00b7\n0n\n .\nTherefore, we have used two BASECONV layers to add x to the running sum S and reset for the next phase. Here, we note that the only operations we perform and are convolutions and Hadamard product and they generalize in the obvious way to d > 1.\nNext, we show that a five layer BASECONV model can perform gated convolution on windows of the input (without changing the rest of the input). Proposition H.10 (The Remembering Primitive). For any x \u2208 Rn\u00d7d,v \u2208 Rm\u00d7d contained in some y \u2208 RN\u00d7d for some n + m + s + t \u2264 N so that for h \u2208 Rn\u00d7d and p \u2208 R(n+s)\u00d7d with x\u2217h \u2208 R(n+s)\u00d7d and v\u2217h \u2208 R(m+t)\u00d7d, there exists a (N, 5, d,N, d)\u2212BASECONV that computes remember(y : x,v,h,p) for BASECONV as in Algorithm 7.\nProof. We will again show this for d\u2032 = 1 and the general case should follow. We now specify the first two layers that we use\nz1 \u2261 BASECONV ( y,0N\u00d71,h1, b11,0 N\u00d7d) \u2261 b11 \u2299 (h1 \u2217 y) z2 \u2261 BASECONV ( z1,0N\u00d71,h2, b21,0\nN\u00d7d) \u2261 b21 \u2299 (h2 \u2217 y) , The kernel h1 and the bias b11 for the first layer are then given by\nh1 \u2261  h 0m es+t 0n 0n \u00b7 \u00b7 \u00b7\n0n\n , b11 \u2261  p 0m+t 0n 0s 1m \u00b7 \u00b7 \u00b7\n0n\n .\nwhere recall that x \u2217 h \u2208 R(n+s)\u00d7d and v \u2217 h \u2208 R(m+t)\u00d7d. We now want to first specify the result of applying the first kernel:\n( h1 \u2217 y ) = coeff ( (h(X) +Xn+m+s+t) \u00b7 ( v(X) \u00b7Xn+s + x(X) )) = coeff ( h \u2217 v(X) \u00b7Xn+s + h \u2217 x(X) + v(X) \u00b7X2n+2s+m+t + x(X) \u00b7Xn+m+s+t\n) We then have\nz1 \u2261 b11 \u2299 ( h1 \u2217 y ) \u2261  p 0m+t 0n 0s 1m \u00b7 \u00b7 \u00b7\n0n\n \u2299  h \u2217 x h \u2217 v x 0s v \u00b7 \u00b7 \u00b7\n0n\n \u2261  p\u2299 (h \u2217 x) 0m+t 0n 0s v \u00b7 \u00b7 \u00b7\n0n\n .\nWe now describe the second kernel h2 and the bias matrix b21 as follows:\nh2 \u2261  e0 0m+t e0 0n+s 0m \u00b7 \u00b7 \u00b7\n0\n , b21 \u2261  0n+s 0m+t 0n 1n+s 1m \u00b7 \u00b7 \u00b7\n0  This yields the following convolution computation:\nh2 \u2299 z1 \u2261 coeff (( Xm+n+s+t + 1 ) \u00b7 ( v(X) \u00b7X2n+2s+m+t + (p\u2299 (h \u2217 x)) (X) )) \u2261 coeff(v(X) \u00b7X3n+3s+2m+2t + v(X) \u00b7X2n+2s+m+t\n+ (p\u2299 (h \u2217 x)) (X) \u00b7Xm+n+s+t + (p\u2299 (h \u2217 x)) (X))\nThus we have\nz2 \u2261 b12 \u2299 ( h2 \u2217 z1 ) \u2261  0n+s 0m+t 0n 1n+s 1m \u00b7 \u00b7 \u00b7\n0\n \u2299  p\u2299 (h \u2217 x) 0m+t 0n p\u2299 (h \u2217 x) v \u00b7 \u00b7 \u00b7\n0\n \u2261  0n+s 0m+t 0n p\u2299 (h \u2217 x) v \u00b7 \u00b7 \u00b7\n0  We now shift this up by 2n+s+m+t entries using the primitive operation defined in Proposition H.8 that costs three additional layers so that we end up with\nz \u2261  p\u2299 (h \u2217 u) v \u00b7 \u00b7 \u00b7\n0  Again, we note that the only operations we perform and are convolutions and Hadamard product and they generalize in the obvious way to d > 1.\nFinally, we show that these primitives may be composed by \u2019stacking\u2019 models with matching inner dimension (N \u2032, d\u2032). Lemma H.11. For f, g : RN\u00d7d \u2192 RN\u00d7d that have (N,L1, d,N \u2032, d\u2032) and (N,L2, d,N \u2032, d\u2032) BASECONV models then their composition f \u25e6 g has an (N,L1 +L2, d,N \u2032, d\u2032) BASECONV model which can be computed by performing their models in succession, or \u2019stacking\u2019.\nProof. This result follows from noting that for any f(u) which requires L1 layers to compute and that we can compute f \u25e6 g(u) = g(f(u)) using the BASECONV model with L2 layers, yielding L1 + L2 layers in total."
        },
        {
            "heading": "H.3.1 BASECONV-HYENA EQUIVALENCE",
            "text": "We show that the equivalence between BASECONV and Hyena by showing that each layer can simulate the other\u2019s computation using a constant number of layers. Proposition H.12. For any input u \u2208 RN\u00d7d and (N,L, d,N \u2032, d)\u2212Hyena such that zHyena \u2261 Hyena(u) with a set of filters h\u2113 and linear projections p\u2113 as per Definition H.3 for \u2113 \u2208 [L], there exists a (N, 5L, d,N \u2032 +N, d)-BASECONVmodel such that zHyena \u2261 BASECONV(u). Similarly, for any input uBASECONV \u2208 RN\u00d7d and (N,L, d,N \u2032, d)\u2212Coyote such that zBASECONV \u2261 BASECONV(u) with a set of filters h\u2113 for \u2113 \u2208 [L], there exists a series of Hyena layers such that we have\nHyena (Hyena (. . .Hyena(uBASECONV,h)))\ufe38 \ufe37\ufe37 \ufe38 L layers \u2261 zBASECONV.\nProof. For the input uHyena \u2208 RN\u00d7d, the output of the \u2113th layer z\u2113Hyena \u2208 RN \u2032\u00d7d\u2032 for Hyena is given by (see Algorithm 3)\nz\u2113Hyena \u2261 p\u2113Hyena \u2299 (hl \u2217 z\u2113\u22121),\nwhere p\u2113Hyena \u2261 Linear(uHyena) \u2208 RN \u2032\u00d7d. Now, using the original input uHyena \u2208 RN\u00d7d to Hyena, we define the following input for BASECONVusing one layer:\nuBASECONV \u2261  uHyena 0(N \u2032\u2212N)\u00d7d\nuHyena  Then, we simply use the rememberN,N,N \u2032\u2212N,N \u2032\u2212N (uBASECONV : uHyena,uHyena,h\u2113Hyena,p \u2113 Hyena) primitive for BASECONV. Consequently, this allows us to \u201cremember\u201d the input uHyena in the output of the previous BASECONVlayer z\u2113\u22121BASECONV. We then use this to retrieve p \u2113 Hyena \u2261 linear(uHyena) with the projection used for BASECONV given by\np\u2113BASECONV \u2261 Linear(z\u2113\u22121BASECONV) \u2261  1N\u00d7d p\u2113Hyena.  Overall, the output of the \u2113th layer for BASECONV is given by\nz\u2113BASECONV \u2261  p\u2113Hyena \u2299 ( h\u2113Hyena \u2217 uHyena ) 0(M\u2212N)\u00d7d\nuHyena\n \u2261  z\u2113Hyena 0(M\u2212N)\u00d7d\nuHyena  Hence, we can reproduce the output of the \u2113th layer of Hyena using five layers of BASECONV after augmenting the input and using the remembering primitive (Proposition H.10) with internal dimension N \u2032 +N . Now, for the input uBASECONV \u2208 RN\u00d7d, the output of the \u2113th layer for BASECONV is given by\nz\u2113BASECONV \u2261 Linear(z\u2113\u22121BASECONV)\u2299 conv(h l, z\u2113\u22121BASECONV).\nHere, we show inductively that simply using \u2113-many Hyena models recursively simulates z\u2113BASECONV. For \u2113 = 1, we have\nHyena(uBASECONV,h) \u2261 Linear(uBASECONV)\u2299 (h1 \u2217 uBASECONV) \u2261 z1BASECONV.\nWe now assume that (\u2113\u22121)-many recursive Hyena models produce z(\u2113\u22121)BASECONV. For the \u2113th layer, we then have\nHyena (Hyena (. . .Hyena(uBASECONV,h))) \u2261 Hyena ( z (l\u22121) BASECONV ) \u2261 linear ( z (l\u22121) BASECONV ) \u2299 conv ( hl, z (l\u22121) BASECONV ) \u2261 zl\u22121BASECONV\n\u2261 z\u2113BASECONV."
        },
        {
            "heading": "H.4 LINEAR ARITHMETIC CIRCUITS",
            "text": "In this section we show the relation between linear arithmetic circuits and BASECONV. We recall a few definitions from (Dao et al., 2020). Definition H.13 (Linear Arithmetic Circuit (Bu\u0308rgisser et al., 1996)). An arithmetic circuit is called a linear arithmetic circuit if it only uses addition, subtraction and scalar multiplication. Further, every multiplication has a fixed constant from F as at least one of its two inputs. In other words, all gates in the circuit are linear functions of their inputs (i.e. of the form ax + by for fixed constants a, b \u2208 F). Definition H.14 (Butterfly Matrices (Dao et al., 2020)). A butterfly factor of size k \u2265 2 (denoted\nas Bk ) is a matrix of the form Bk = [\nD1 D2 D3 D4 ] where each Di is a k2 \u00d7 k 2 diagonal matrix. We\nrestrict k to be a power of 2 . A butterfly factor matrix of size n with block size k (denoted as B(n)k ) is a block diagonal matrix of n k (possibly different) butterfly factors of size k :\nB (n) k = diag ( [Bk]1 , [Bk]2 , . . . , [Bk]nk ) Finally, a butterfly matrix of size n (denoted as B(n) ) is a matrix that can be expressed as a product of butterfly factor matrices: B(n) = B(n)n B\n(n) n 2 . . .B (n) 2 . Equivalently, we may define B (n) recursively as a matrix that can be expressed in the following form:\nB(n) = B(n)n  [B(n2 )]1 0 0 [ B( n 2 ) ] 2  (Note that [ B( n 2 ) ] 1 and [ B( n 2 ) ] 2\nmay be different.) From Definition H.14, we observe that size n butterfly factor is comprised of three vectors d, d+, d\u2212 \u2208 Rn such that\nd = ( diag\u22121 (D1) ,diag \u22121 (D4) ) ,\nd+ = ( 0 n 2 ,diag\u22121 (D2) ) , and\nd\u2212 = ( diag\u22121 (D3) ,0 n 2 ) ,\nwhere diag\u22121(D) : Rn\u00d7n 7\u2192 Rn is the mapping from diagonal matrices to the vector of its diagonal entries. Let us define D1,D2,D3 \u2208 Rn\u00d7n as diag (d) ,diag ( d+ ) , and diag ( d\u2212 )\nrespectively. Then we note that\nD1 \u2261 [\nD1 0 0 D4\n] D2S n 2 \u2261 [ 0 D2 0 0 ] S n 2 D3 \u2261 [ 0 0 D3 0 ] (5)\nwhere Sk \u2208 Fn\u00d7n is a shift matrix for i \u2208 [n/2]. This gives us the following proposition: Proposition H.15. For any powers of 2, n = k \u2265 2, any butterfly factor matrix B(n)k is equivalent to\nB (n) k = S k 2D3 +D2S n 2 +D1\nwhere D3,D2,D1,S n 2 are defined as in equation 5. We use Proposition H.15 to show that butterfly matrices can easily be computed by BASECONV . Lemma H.16. For any n, d \u2265 2, k \u2265 1, and arbitrary vector x \u2208 Rnd:\n(1) there exists a (N,L, d,N \u2032, d\u2032)\u2212BASECONV that can represent B(nd)k \u00b7x with N = n,N \u2032 = O(N), L = O(1), and d\u2032 = O(d), and (2) there exists a (N,L, d,N \u2032, d\u2032)\u2212BASECONV that can represent B(nd)\u00b7x with N = n,N \u2032 = O(N), L = O(log nd), and d\u2032 = O(d).\nProof. (1) Given x \u2208 Rnd, construct u \u2208 Rn\u00d7d where x is the row-major form of u. We show that BASECONV can compute Bnd \u00b7 x column by column. Let A = S k 2D\u20323,C = D \u2032 2S n 2 , and D = D1 for Di,S k 2 \u2208 Rnd\u00d7nd for 1 \u2264 i \u2264 3 as\ndefined in Proposition H.15. We take d1 = 1ndD, d2 = 1ndC2,d3 = 1ndA, which\nextracts the diagonal entries of Di. With this we construct D\u2032i \u2208 Rn\u00d7d where di is the row major form of D\u2032i. This implies that Dix \u2261 D\u2032i \u2299 u. Then we can decompose Bnd \u00b7 x into Bndx \u2261 D1 \u2299 u+D2 \u2299 u+D3 \u2299 u. By Lemma H.7, each Hadamard product A \u2299 u,B \u2299 u,C \u2299 u can be trivially be performed with a single layer BASECONV model. Let each of these model outputs be denoted y1, y2, y3, respectively. Finally all that remains is to compute the y1 + y2 + y3. We achieve this using layers of add primitives11:\naddn(y 1 : y1,0)\naddn(y 2 : y2,y1)\naddn(y 3 : y3,y1 + y2),\nwhere using by Proposition H.9 and Lemma H.11, this requires six more layers, and we get y3 \u2261 y1 + y2 + y3 \u2261 Bndx.\nThen we can construct the (N,L, d,N \u2032, d\u2032)\u2212BASECONV as desired with L = O(1) layers. (2) From Definition H.14, B(nd) = B(nd)nd B\n(nd) nd 2 . . .B (nd) 2 . From (1), BASECONV can compute\nany butterfly matrix by simulating the log(nd) butterfly factor matrices which comprise B(nd). With Lemma H.11, this creates a BASECONV with 5 \u00b7 log(nd) = O(log(nd)) layers. Lemma H.11\nButterfly matrices comprise the kaleidoscope hierarchy, which we define below: Definition H.17 (The Kaleidoscope Hierarchy (Dao et al., 2020)).\n\u2022 Define B as the set of all matrices that can be expressed in the form B(n) (for some n). \u2022 Define (BB\u2217) as the set of matrices M of the form M = M1M\u22172 for some M1,M2 \u2208 B. \u2022 Define (BB\u2217)w as the set of matrices M that can be expressed as M = Mw . . .M2M1,\nwith each Mi \u2208 (BB\u2217) (1 \u2264 i \u2264 w). (The notation w represents width.) \u2022 Define (BB\u2217)we as the set of n \u00d7 n matrices M that can be expressed as M = SES\n\u22a4 for some en\u00d7 en matrix E \u2208 (BB\u2217)w, where S \u2208 Fn\u00d7en = [In 0 . . . 0]] (i.e. M is the upper-left corner of E). (The notation e represents expansion relative to n.)\nWe similarly show how BASECONV can simulate any kaleidoscope matrix. Lemma H.18. Given n, d \u2265 2, e > 0 for any nd \u00d7 nd matrix M \u2208 (BB\u2217)we , and x \u2208 Rnd there exists a (N,L, d,N \u2032, d\u2032) \u2212 BASECONV that can represent M \u00b7 x with N = n,L = O(w log(end)),N \u2032 = en, and d\u2032 = d. Proof. By Definition H.17, M can be decomposed with respect to size end\u00d7 end matrix\nE = E1 \u00b7E2 \u00b7 \u00b7 \u00b7Ew. Further, any Ei \u2208 (BB\u2217) can be expressed as a product of 2 log end butterfly factor matrices. Then by Lemma H.16 and Lemma H.11 we can compute Eix\u2032 in by stacking 2 log end (n, d, L, en, d)\u2212 BASECONV models each with L = O(1). Because E has width w, Lemma H.11 implies that composing with each Ei for 1 \u2264 i \u2264 w constructs a final model with O(w log(end)) layers. Finally, the kaleidoscope hierarchy is related to linear arithmetic circuits via the following result. We note that in (Dao et al., 2020) it is assumed that w = s, yet inspection of the proof yields the following stronger result: Theorem H.19 ( (Dao et al., 2020)). Let M be an n \u00d7 n matrix such that multiplication of M times an arbitrary vector u can be be represented as (n, s,\u2206, w)-linear arithmetic circuit C. Then, M \u2208 (BB\u2217)O(\u2206)O(w/n). We combine Theorem H.19 and Lemma H.18 to show that BASECONV can compute any linear arithmetic circuit with polylogarithmic factors in \u2206. Corollary H.20. For any (nd, s,\u2206, w)-linear arithmetic circuit C that can be represented by a matrix M \u2208 Rnd\u00d7nd multiplied by a vector x \u2208 Rnd, there exists an equivalent (n,\u2206\u2032, d, w, d) \u2212 BASECONV with \u2206\u2032 = O(\u2206 log(w)) such that Mx = BASECONV (u,h) where x is the row major form of u \u2208 Rn\u00d7d.\n11Recall that addn(y : x,S) adds the subvector x to S for the input y."
        },
        {
            "heading": "H.5 GENERAL ARITHMETIC CIRCUITS",
            "text": "We are now ready to prove the result that yields the equivalency between arithmetic circuits and BASECONV. Theorem H.21. For any (nd, s,\u2206, w)-arithmetic circuit C, there exists an equivalent (N,\u2206\u2032, d,N \u2032, d\u2032)\u2212BASECONV with N = n,\u2206\u2032 = O(\u2206 logw), N \u2032 = O(w), d\u2032 = d that simulates C. For the reader\u2019s convenience, we begin with a proof sketch and then provide the details afterwards.\nProof Sketch of Theorem H.21. Let us layer C so that each layer C\u2113 for \u2113 \u2208 [LC ] either only has linear gates or multiplication gates, where the number of such layers LC = O(\u2206). The composition of all C\u2113 layers results in C. We use z\u2113+1 \u2208 Rw to denote the output of the \u2113-th layer C\u2113 which feeds as the input to the (\u2113 + 1)-th layer C\u2113+1. Here, we note that if we can simulate each C\u2113 with BASECONV, then we can simulate the entire layered circuit C due to Lemma H.11. Now, if the layer Clin\u2113 is a linear layer (with only addition gates), then it can be represented by a matrix M \u2208 Rw\u00d7w multiplied by z\u2113 \u2208 Rw (We can append with 0s if necessary so that the input from the previous gates can be written as w-length vector). Thus, we can apply Corollary H.20 to simulate Clin\u2113 with an equivalent (n, logw, d,O(logw), d)\u2212 BASECONV model. Next, if Cmult\u2113 instead consists of only the multiplication gates. Then, we note here that the output z\u2113 may not exactly equal the input to Cmult\u2113 . Nevertheless, we can apply a O(w) sparse linear map R \u2208 Rw\u00d7w so that Rz\u2113 yields vectors v1,v2, and v3, where v1 constitutes the \u201cfirst\u201d input to all the multiplication gates and v2 constitutes all the \u201csecond\u201d inputs while v3 consists of all entries needed as inputs in the subsequent layers. That is, for the ith gate in Cmult\u2113 , we compute v1i \u00b7 v2i . This implies that for all the gates in Cmult\u2113 , we can simply compute v1 \u2299 v2. To this end, we use the remember primitive with constant number of layers from Proposition H.10 to define a (n,O(logw), d, w, d) \u2212 BASECONV model that remembers v3 while performing the Hadamard product of v1 with v2. Overall, we can then collect all the resulting BASECONV layers and compose them as in Lemma H.11 to simulate C. Overall, the number of layers used is given by O(\u2206 logw) while the internal dimension remains fixed at w.\nUsing the outline in the proof sketch, we now delve into a detailed analysis of the arithmetic circuit C to an equivalent BASECONVmodel. Proof of Theorem H.21. Let u \u2208 Rnd be the input to the arithmetic circuit C with depth \u2206 and width w. We begin by rearranging the circuit into layers of addition and multiplication gates. That is, each layer C\u2113 has either all addition gates or all multiplication gates. This allows us to readily apply the results from Appendix H.4. Note here that we can assert that the number of such layers LC = O(\u2206). Moreover, we construe the input to each such circuit as a vector of length w by appending with extra 0s if necessary so that the composition of the layers results in a circuit equivalent to C. See Fig. 8 for an example of such decomposition Let z\u2113 \u2208 Rw denote the input to each layer C\u2113 for \u2113 \u2208 [LC ] with z1 \u2261 (u,0w\u2212nd). It is important to note here that we may not have z\u2113+1 \u2261 C\u2113(z\u2113) since the inputs for gates at the (\u2113 + 1)-th layer may come from any of the previous layers. We now handle the case of layers with addition gates, multiplication gates, and the intermediate stage separately.\nAddition Gates. Let Clin\u2113 denote an arbitrary linear layer which only contains the addition gates that takes in z\u2113 as input. We know that there exists a matrix M \u2208 Rw\u00d7w\u2032 such that we have\nClin\u2113 (z\u2113) \u2261Mz\u2113.\nHere, note here that we may need entries from the vector z\u2113 in subsequent layers. Let s be the total number of such entries, then for at each index for such entries i \u2208 [w], we have the corresponding index is \u2208 [s]. We then append the ith standard row vector into the matrix M to get the matrix M\u2032 \u2208 Rw\u00d7(w\u2032+s) so that we have(\nM\u2032z\u2113 ) [j] =\n{( Mz\u2113 ) [j] if j < w\u2032\nz\u2113[i] if j = w\u2032 + is\nHere, note that we must have w\u2032 + s \u2264 w as w is the width of the circuit. If needed, we then append the matrix M\u2032 with all-zero rows 0w so that we get the matrix M\u2032\u2032 \u2208 Rw\u00d7w. Note here that we have thus preserved all entries needed in subsequent layers and incorporated the output of the \u2113th layer\n. . .\nwith the output M\u2032\u2032z\u2113 \u2261 z\u2113+1 serving as the input of the (\u2113+1)-th layer. Applying Corollary H.20 then yields an equivalent (n,O(logw), d, w, d)\u2212 BASECONV model.\nMultiplication Gates. Next, we deal with the layer C\u2113mult of multiplication gates by collecting the two inputs to each multiplication gates as v\u21131,v \u2113 2. Note that for the input z\n\u2113 \u2208 Rw from the previous layer, we will again have entries that need to be used in the subsequent layers that we will denote as v\u21133. We thus need to permute the entries of z \u2113 to get the vector [v\u21131 :: v \u2113 2 :: v \u2113 3] so that we can remember v\u21133 while taking the Hadamard product v \u2113 1 \u2299 v\u21132. To this end, we can achieve this permutation of entries using a O(w)-sparse linear matrix12 R\u2113 \u2208 Rw\u00d7w with which is equivalently represented by an (w,w, 1, w)-linear arithmetic circuit that simply moves the appropriate wires from the previous layer. This can again be achieved by a equivalent (n,O(logw), d, w, d)\u2212 BASECONV model. That is, R\u2113z\u2113 has the following form:\nR\u2113z\u2113 \u2261  v\u21131 v\u21132\nv\u21133  . Next, we can now define a (n, 1, d, n, d) \u2212 BASECONV which extracts v\u21131 as the projection with the input to the remember primitive given by y\u2113 \u2261 (v\u21132,v\u21133,0). We then specify the remember primitive as remember ( y\u2113 : v\u21132,v \u2113 3, e0,v \u2113 1 ) which computes the following\nz\u2113+1 \u2261  v\u21131 \u2299 (e0 \u2217 v\u21132) v\u21133\n0\n\n\u2261  v\u21131 \u2299 v\u21132 v\u21133\n0  \u2261 C\u2113mult(z\u2113) \u2208 Rw.\n12Each row of R\u2113 has exactly one non-zero entry.\nUsing Proposition H.10, we know that this requires a (n,O(logw), d, w, d) \u2212 BASECONV which remembers the entries that will serve as inputs in subsequent layers while performing the Hadamard product v1 \u2299 v2. Overall, we can now stack all the resulting BASECONV layers and compose them as in Lemma H.11 to simulate C. Overall, the number of layers blows up by O(\u2206 logw) while the internal dimension remains fixed at w."
        },
        {
            "heading": "H.6 THE RETNET REDUCTION AND SPACE COMPLEXITY FOR AR",
            "text": "We are now in a position to show that there exists an equivalent BASECONV model that can simulate a Retnet layer from Algorithm 6. Recall that for an input sequence u \u2208 RN\u00d7d, we project the input using weight matrices WA,WC ,WV \u2208 Rd\u00d7d to A,C,V \u2208 RN\u00d7d and compute the following recurrence (see line 5 in Algorithm 6):\nzn \u2261 \u03b3zn\u22121 + (A[n, :])\u22a4 V[n, :], (6)\nwhere zn \u2208 Rd\u00d7d. We unroll this recurrence to express each state zn in terms of u with coefficients given by \u03b3 and the weight matrices WA,WC ,WV as follows:\nz0 \u2261 ((uWA) [0, :])\u22a4 (uWV ) [0, :]\n\u2261 (u[0, :]WA)\u22a4 (u[0, :])WV \u2261W\u22a4A (u[0, :]) \u22a4 (u[0, :])WV\n\u2261W\u22a4A ( u\u22a4[:, 0]u[0, :] ) WV .\nSimilarly, we have z1 \u2261 \u03b3z0 +W\u22a4A ( u\u22a4[:, 1]u[1, :] ) WV\n\u2261 \u03b3 ( W\u22a4A ( u\u22a4[:, 1]u[0, :] ) WV ) +W\u22a4A ( u\u22a4[:, 1]u[1, :] ) WV\n\u2261W\u22a4A ( \u03b3 ( u\u22a4[:, 0]u[0, :] ) + u\u22a4[:, 1]u[1, :] ) WV\nWe can then generalize this to n \u2208 [N ] to get\nzn \u2261W\u22a4A\n( n\u2211\ni=0\n\u03b3n\u2212iu\u22a4[:, i]u[i, :] ) WV . (7)\nThe above helps us infer that zn can be expressed as a polynomial in {u[0, 0],u[0, 1], . . . ,u[N \u2212 1, d\u2212 1]}. For each polynomial, there exists an arithmetic circuit that computes the polynomial in a natural way, whence we can apply Theorem H.21 to get an equivalent BASECONV model. Corollary H.22. For a RetNet model withO(d2) parameters and N layers, there exists an equivalent BASECONVmodel that uses O(Nd) parameters and O(N log d) layers.\nProof. We will start by describing the arithmetic circuit that computes the states zn \u2208 Rd\u00d7d here. Indeed, each state requires exactly two alternative layers with only multiplication gates and only addition gates, respectively. First, we note that for each n \u2208 N , we have\nzn \u2261 \u03b3zn\u22121 + (A[n, :])\u22a4 V[n, :].\nSince we can compute z0 in O(1) layers and computing \u03b3 \u00b7 zn\u22121 needs exactly one layer with each entry from zn\u22121 serving as an input to a multiplication gate along with \u03b3, the depth of the circuit is O(N) with width d2. We can thus apply Theorem H.21 to get an (O(N),O(N log d), d2,O(d2),O(d2))\u2212BASECONV model that can compute zn for each n \u2208 N .\nThe Space Complexity of AR In the context of the Retnet Model and associative recall, it is worth exploring the space complexity of the associative recall (AR) problem:\nThe AR problem takes key-value pairs {ki,vi}n\u22121i=0 along with a query q appended at the end as input and the goal is to output vi if q = ki for some i \u2208 [0, N \u2212 1].\nIndeed, we will first provide a lower-bound for any model that purports to solve AR. To this end, we will require a randomized communication complexity lower bound result for the index problem:\nThe index problem has two agents, Alice and Bob, where Alice has a string x \u2208 {0, 1}n and Bob has an index i \u2208 [n], and the goal for the players is to output the i-th entry xi. Moreover, we also require the communication to be one-way: only Alice is allowed to send a single message to Bob and Bob needs to output the answer.\nWe will make use of the following lower-bound result. Theorem H.23 ((Jayram et al., 2008)). The one-way randomized communication complexity13 of the index problem for sending an n-length bit string is \u2126(n). We now use Theorem H.23 to provide a lower bound on the number of bits required by the Retnet model to solve AR. Corollary H.24. The RetNet model requires \u2126(N)-bits to solve AR for d \u2264 \u221a N .\nProof. Consider an instance (x, i) of the index problem with x \u2208 {0, 1}N . We now describe the corresponding instance of the AR problem:\n{i,xi}N\u22121i=0 , i. (8)\nNext, consider the following one-way protocol for solving the index problem using the RetNet model. Alice with their access of x \u2208 {0, 1}N generate an input for AR (without the query) as in equation 8. Alice then runs the RetNet model on {i,xi}N\u22121i=0 and sends the memory content of running the RetNet model to Bob. This should include the output zN\u22121 as we can reasonably assume that both have access weight matrices WA,WC ,WV and the scalar \u03b3. Since we assume that this model solves AR, the output Out[N, :] = xi should contain the associated value of i. Here, Bob can compute Out[N, :] by using the memory content sent by Alice along with the term \u03b3zN\u22121:\nxi = Out[N, :] = C[N, :]z N = C[N, :] ( \u03b3zN\u22121 + (A[N, :])\u22a4V[N, :] ) .\nThat is, the total number of bits that are communicated in this protocol is O(d2). For d \u2264 \u221a N , have shown that a one-way communication protocol exists for solving the index problem exists that uses o(N) communication complexity. This contradicts Theorem H.23 and hence, we conclude that the ReNet model solving AR also needs \u2126(N) bits."
        },
        {
            "heading": "H.7 THE MULTIPLE-QUERY ASSOCIATIVE RECALL PROBLEM",
            "text": "H.7.1 INTRODUCTION In this section, we consider a general version of the associative recall problem (Ba et al., 2016).\nSetup. Next, we will redefine the multiple-query associative recall problem (MQAR) from Definition 3.1 to a slightly more general problem:\nSuppose we are given an input sequence u[0 \u00b7 \u00b7 \u00b7N \u2212 1] \u225c{ (k0,v0, q0) , . . . , ( kN\n3 \u22121 ,vN 3 \u22121 , qN 3 \u22121\n)} with each ki,vi, qi \u2208 C is a\ntoken drawn from a vocabulary of size c = |C|. Our goal is then to check, for each 1 \u2264 i \u2264 N3 \u2212 1, whether there exists 0 \u2264 j < i such that qi \u2261 kj , and if so, output vj .\nHere, we note that it suffices to have d \u2248 log(c) so that ki,vi, qi is embedded in {0, 1}d. However, we will specify the specific embedding being used for the results below. Here, we construe the tokens ki, qi and vi to be the keys, the queries, and the associated values. Indeed, it might be helpful to think of the input u as a streaming sequence of key-value pairs for which we sequentially employ standard associative recall for every key that shows up in the sequence so far. To see that the above generalizes Definition 3.1, considers a sequence of length N3 : u[0 \u00b7 \u00b7 \u00b7N \u2212 1] := {x0, . . . ,xN\u22121}, where each xi \u2208 C. The goal of Definition 3.1 is then to check, for each 1 \u2264 i < N \u2212 1, whether there exists 0 \u2264 j < i such that xi \u2261 xj , and if so, output xj+1, and continue otherwise. We can reduce this problem to the above general formulation by taking the following sequence of tuples as the input {(xi,xi+1,xi)}. Remark H.25. As noted above, this version is more general than Definition 3.1. Thus, the results proven in the sequel, which are proven for the above general MQAR can be ported (with constant blowup in parameters) to get the results corresponding results for Definition 3.1.\n13The randomized communication complexity of function f is defined as min\u03c0 \u2225\u03c0\u2225, where \u03c0 ranges over all randomized protocols that can solve f with probability of success at least 2/3."
        },
        {
            "heading": "H.7.2 MQAR SOLUTION VIA ATTENTION",
            "text": "Before describing how BASECONV solves the multiple-query associative recall problem, we discuss how Attention solves it trivially using pairwise inner-products. To this end, we will specify how the input is presented to attention. Remark H.26. We note that the input for the multiple-query associative recall problem u \u2208 {0, 1}N\u00d7d has designated indices for the keys, queries, and values in the sequence. We gather these indices below:\nK = {i \u2208 {0, . . . , N \u2212 1}| i \u2261 0 mod 3}, V = {i \u2208 {0, . . . , N \u2212 1}| i \u2261 1 mod 3}, Q = {i \u2208 {0, . . . , N \u2212 1}| i \u2261 2 mod 3}, .\n(9)\nThe input u \u2208 RN\u00d7d to Attention for d = 3c is then given by\nu[i, :] \u2261  [ki : 0 c : 0c] if i \u2208 K [0c : vi : 0\nc] if i \u2208 V [0c : 0c : qi] if i \u2208 Q\nHere, each ki,vi, qi is embedded as a one-hot encoding in {0, 1}c. Without softmax, the output for an attention layer O \u2208 RN\u00d7d is given by\nO \u2261 ( QK\u22a4 ) V, (10)\nwhere Q,K,V \u2208 RN\u00d7d are defined as uWQ,uWK ,uWV for u \u2208 RN\u00d7d. Instead of position embeddings, we use ALiBi, a popular technique that biases the attention scores QK\u22a4 with a lowertriangular Toeplitz matrix B \u2208 RN\u00d7N (Press et al., 2021). The values in this matrix are controlled by a fixed hyperparameter so they do not count towards the number of parameters in the model.\nAlgorithm 8 ALiBi-without-softmax (u[0 \u00b7 \u00b7 \u00b7N \u2212 1],Oprev[0 \u00b7 \u00b7 \u00b7N \u2212 1],B))\nInput: Input sequence u[0 \u00b7 \u00b7 \u00b7N\u22121] \u225c {(ki,vi, qi)} N 3 \u22121 i=0 with each ki,vi, qi \u2208 {0, 1}3c, previous\nlayer\u2019s output Oprev[0 \u00b7 \u00b7 \u00b7N \u2212 1] \u2208 RN\u00d73c, and linear bias B \u2208 RN\u00d7N . 1: Add ucurr \u2190 u+Oprev as an input to this layer. 2: K,Q,V\u2190 ucurrWQ,ucurrWK ,ucurrWV . 3: O\u2190 ( QK\u22a4 +B ) V 4: return O as the output of this layer.\nProposition H.27. Given an input u \u2208 {0, 1}N\u00d7d (encoded as in Remark H.26) where d = 3c, Attention with linear biases (even without using soft-max) solves MQAR for u using O(c2) parameters, O(Nc2 +N2c) time complexity and O(1) layers.\nProof. We use two layers of attention. We will start by specifying the projection matrices for the first layer W1Q,W 1 K ,W 1 V \u2208 Rd\u00d7d as:\nW1K \u2261W1Q \u2261 0, W1V \u2261 ( 0 0 0 0 Ic\u00d7c 0 0 0 0 )\nAbove, W1V is meant to isolate the vi embeddings. For the first layer, we then have Q 1 \u2261 K1 \u2261 0, and V1[i, :] := u1[i, :]W1V \u2261 { [0c : vi : 0\nc] if i \u2208 V, 0d otherwise ,\nwhere K,Q,V are defined as in equation 9. The output for the first layer is given by the following: O1[i, :] = (( QK\u22a4 +B1 ) V ) [i, :]\n= B1V[i, :]\n=\n{ [0c : vi : 0\nc] if i \u2208 K 0d otherwise ,\nThe QKT is ignored (\u2261 0) and we isolate the shifted sequence by setting the bias matrix B appropriately. In particular, the last equality follows from the fact that B1 is an up-shift matrix that shifts each row of V by 1. We apply the residual connection at the end of the standard Transformer block to insert the ki adjacent to the vi. For the second layer, the input u2 \u2208 RN\u00d7d is given by\nu2[i, :] \u2261  [ki : vi : 0 c] if i \u2208 K [0c : vi : 0\nc] if i \u2208 V [0c : 0c : qi] if i \u2208 Q ,\nFurther, we take the following projection matrices:\nW2K \u2261 ( Ic\u00d7c 0 0 0 0 0 0 0 0 ) ,W2Q \u2261 ( 0 0 0 0 0 0\nIc\u00d7c 0 0\n) ,W2V \u2261 ( 0 0 0\nIc\u00d7c 0 0 0 0 0 ) We then have the following matrices as input to attention after applying projection:\nQ2[i, :] := u2[i, :]W2K \u2261 { [qi : 0\n2c] if i \u2208 Q, 0d otherwise ,\nK2[i, :] := u2[i, :]W2Q \u2261 { [ki : 0\n2c] if i \u2208 K, 0d otherwise ,\nV2[i, :] := u2[i, :]W2V \u2261 { [vi : 0\n2c] if i \u2208 K \u222a V, 0d otherwise .\n(11)\nHere, note that the values have been shifted to the corresponding key position. Next, we compute the term in the parenthesis for the second layer as(\nQ2K2\u22a4 ) [i, j] = ( Q2K2\u22a4 ) [i, j] = \u27e8Q2[i, :],K2\u22a4[:, j]\u27e9\n= \u27e8Q2[i, :],K2[j, :]\u27e9\n= { \u27e8qi,kj\u27e9 if i \u2208 Q, j \u2208 K 0 otherwise\n= { 1 if i \u2208 Q, j \u2208 K, qi \u2261 kj \u2261 ek for some k 0 otherwise.\nFinally, we compute the output as follows: O2[i, :] = (( Q2K2\u22a4 +B2 ) V2 ) [i, :]\n= (( Q2K2\u22a4 + 0 ) V2 ) [i, :]\n= ( Q2K2\u22a4 ) [i, :] \u00b7V2\n= N\u22121\u2211 j=0 ( Q2K2\u22a4 ) [i, j] \u00b7V2[j, :]\n= \u2211 j\u2208K ( Q2K2\u22a4 ) [i, j] \u00b7 [vj : 02c]\n=\n{ [vj : 0\n2c] if j \u2208 K, i \u2208 Q, qi \u2261 kj 0d otherwise ,\nwhere we use the fact that for each index j \u2208 K, the matrix V2 contains the associated value from equation 11. Thus, for each query qi, we solve the associated value problem yielding a match for the jth key. In total, we only needO(c2)-many parameters to perform these multiplications and the linear bias in the first layer is a hyperparameter that is static and unlearned; the time complexity comes from the multiplication of QK\u22a4 in O(N2c), and projections in O(Nc2). Finally, we only need O(1) layers for this solution.\nIn the sequel, we develop a parallel algorithm to solve the multiple-query associative recall problem with O(Nd \u00b7 log2 N) work complexity and O(d \u00b7 log2 N) time. We then convert the algorithm into a BASECONV model via the route of arithmetic circuits, which then solves the multiple-query associative recall problem with O\u0303(1) layers and O\u0303(Nd) parameters.\nH.7.3 INITIAL ATTEMPT: A SEQUENTIAL ALGORITHM\nWe will first discuss the algorithm that simply uses an associative array to solve the multiple-query associative recall problem. Specifically, we want to use a data structure that allows for logarithmic insertion and membership query. Here, we do not specify a choice but data structures including self-balancing binary search trees which allow for O(logN \u00b7 d) insert and find operations for d-bit entries should be sufficient.\nAlgorithm 9 Sequential-MQ-AR (u) [0 \u00b7 \u00b7 \u00b7N \u2212 1]\nInput: Input sequence u[0 \u00b7 \u00b7 \u00b7N \u2212 1] \u225c {(ki,vi, qi)} N 3 \u22121 i=0 with each ki,vi, qi \u2208 {0, 1}d.\n1: Initialize an associative array with insert and find and an output array out\u2190 []. 2: for i \u2208 {0, . . . , N3 \u2212 1} do 3: (kj ,vj)\u2190 find(qi) \u25b7 Query for qi in the data structure. 4: if kj is not null then 5: Add vj to out. 6: insert(ki,vi) \u25b7 Add the key-value pair to the data structure. 7: return out.\nProposition H.28. Algorithm 9 solves the multiple-query associative recall problem (MQAR) in O(dN logN) time for an input sequence u \u2208 {0, 1}N\u00d7d.\nProof. For any i \u2208 {0, . . . , N3 \u2212 1}, we know that both insertion and lookup operations take O(log(i) \u00b7 d) time. Overall, the runtime of the algorithm is\nN 3 \u22121\u2211 i=0 O(log(i) \u00b7 d) = O(log(N !) \u00b7 d) = O(N logN \u00b7 d)."
        },
        {
            "heading": "H.7.4 ALGORITHM VIA PARALLEL BINARY SEARCH",
            "text": "Our plan is to convert the algorithm for solving the multiple-query associative recall problem in the RAM model into an arithmetic circuit, which by Theorem H.21 will lead to a BASECONV model that solves the multiple-query associative recall problem. With respect to Algorithm 9, it may be the case that the arithmetic circuit has a large number of layers \u2126(N). Unfortunately, this would imply that the resulting BASECONV model may have near quadratic complexity in N . Instead, we now initiate our effort into designing a BASECONV model with both small enough number of parameters and number of layers. Here, we will first subdivide the problem using dyadic intervals into O(N) subproblems and reduce each such subproblem into a multiple search problem (Akl and Meijer, 1990). To this end, we briefly introduce the multiple search problem below.\nGiven two array of numbers A \u225c a0 \u2264 . . . \u2264 an\u22121 and B \u225c (b0 \u2264 . . . \u2264 bm\u22121) with n \u2264 m, for each aj \u2208 A, the goal is to find the smallest element in B that is larger than or equal to aj .\nThe multiple search problem is solved by a parallel binary search (pbs) algorithm in (Akl and Meijer, 1990) with work complexity O(n \u00b7 logm) and time O(log n logm). Specifically, for sorted arrays A[0 \u00b7 \u00b7 \u00b7n\u2212 1] and B[0 \u00b7 \u00b7 \u00b7m\u2212 1], pbs constructs the array C[0 \u00b7 \u00b7 \u00b7n\u2212 1] defined as\nC[i] \u225c { min0\u2264j<m{j| A[i] \u2264 B[j]} if A[i] \u2264 B[m\u2212 1] m otherwise.\n(12)\nThe algorithm itself runs in exclusive-read exclusive-write (EREW) PRAM model\u2014no two processors are allowed to read from or write into the same memory location at the same time. We now augment the algorithm copied from (Akl and Meijer, 1990) for our purposes below.\nAlgorithm 10 pbs-key-values (q[s \u00b7 \u00b7 \u00b7 t], k[x \u00b7 \u00b7 \u00b7 y], n,m) Input: sorted arrays q[s \u00b7 \u00b7 \u00b7 t] := {qi}ti=s, k[x \u00b7 \u00b7 \u00b7 y] := {(j,kj)} y j=x.\n1: Initialize n processors denoted P0, P1, . . . , Pn\u22121 \u25b7 {Sequential steps are assumed to be executed by Ps.} 2: Initialize the output array C := [m]ti=s. 3: if s \u2264 t then 4: mid\u2190 \u230a(s+ t)/2\u230b 5: if q[mid] \u2264 k[x][1] then 6: for i := s to mid in parallel do 7: C[i]\u2190 j \u25b7 Step executed in parallel by Pi 8: pbs-key-values (q[mid + 1 \u00b7 \u00b7 \u00b7 t], k[x \u00b7 \u00b7 \u00b7 y]) 9: else 10: if q[mid] > k[y][1] then 11: for i := mid to t in parallel do 12: C[i]\u2190 y + 1 \u25b7 Step executed in parallel by Pi 13: pbs-key-values (q[s \u00b7 \u00b7 \u00b7mid\u2212 1], k[x \u00b7 \u00b7 \u00b7 y]) 14: else \u25b7 C[mid] is determined using sequential binary search 15: z \u2190 minx\u2264j\u2264y{j| q[mid] \u2264 k[j][1]} 16: C[mid]\u2190 z 17: do steps 18 and 19 in parallel 18: pbs-key-values (q[s \u00b7 \u00b7 \u00b7mid\u2212 1], k[x \u00b7 \u00b7 \u00b7 z \u2212 1])] 19: pbs-key-values (q[mid + 1 \u00b7 \u00b7 \u00b7 t], k[z \u00b7 \u00b7 \u00b7 y]) 20: return C.\nLet \u03a3 be the set {0, 1} and denote the set of binary strings of size n as \u03a3n. We define prefix(x) for n-bit strings as the set of all initial substrings of x \u2208 \u03a3n which includes the empty string and x itself. Next, let dec : {0, 1}n \u2192 N be the decimal representation of an n-bit string x with x[0] denoting the least significant bit. We also use sort(A) as a procedure that sorts an array A. Finally, wlog, we assume that N is a power of 2. We are now ready to present a parallel algorithm that solves the multiple-query associative recall problem below.\nAlgorithm 11 Parallel-MQAR (u[0 \u00b7 \u00b7 \u00b7N \u2212 1])\nInput: Input sequence u[0 \u00b7 \u00b7 \u00b7N \u2212 1] \u225c {(ki,vi, qi)} N 3 \u22121 i=0 with each ki,vi, qi \u2208 {0, 1}d. 1: Initialize N3 log ( N 3 ) processors denoted P0, . . . , PN\n3 log( N 3 )\u22121 . 2: Initialize the index and output array idx, val\u2190 []. 3: for k := {0, . . . , log ( N 3 ) \u2212 1} do 4: for x := {x \u2208 \u03a3log( N 3 )\u2212k| x[log ( N 3 ) \u2212 k \u2212 1] = 0} do 5: {All the steps below are executed in parallel by {{{Px,ki }x}i\u2208[0,2k\u22121]}k} 6: Ixk \u2190 {y \u2208 \u03a3\nlog(N3 )| x \u2208 prefix(y)}. 7: kkxsorted, I kx permuted \u2190 sort ( {kdec(i)}i\u2208Ixk ) \u25b7\nIkxpermuted := {(j,dec(i))| kkxsorted[j] \u2261 kdec(i)} 8: x[log ( N 3 ) \u2212 k \u2212 1]\u2190 1 9: Jxk \u2190 {y \u2208 \u03a3 log(N3 )| x \u2208 prefix(y)}.\n10: qkxsorted, J kx permuted \u2190 sort ( {qdec(j)}j\u2208Jxk ) \u25b7\nJkxpermuted := {(dec(j), k)| qkxsorted[k] \u2261 qdec(j)} 11: Ck \u2190 pbs-key-values ( qkxsorted, k kx sorted, 2 k, 2k ) 12: for j \u2208 Jxk do 13: if Ck[dec(j)] \u0338= 2k then 14: ckxj \u2190 Ck[Jkxpermuted(dec(j))] 15: if ckxj \u0338= 2k then \u25b7 cf. equation 12 16: Add Ikxpermuted(c kx j ) to idx[dec(j)]. 17: for i \u2208 {1, . . . , N3 \u2212 1} do 18: {Executed in parallel by Pi.} 19: if \u2203 j \u2208 idx[i] then 20: Add vj+1 to val 21: return val.\nRemark H.29. In lines 7 and 10, we keep track of the sorted permutation of the indices of keys and queries, respectively. This helps us in the retrieval of the index of the matching key as in line 16."
        },
        {
            "heading": "H.7.5 CORRECTNESS AND COMPLEXITY",
            "text": "Proposition H.30. Algorithm 11 solves the multiple-query associative recall problem with work complexity O(Nd \u00b7 log2 N) and time O(d \u00b7 log2 N).\nProof. The correctness of pbs implies the correctness of Algorithm 11 if we can show that, for each 1 \u2264 i < N3 , we check for a match among the keys {kj}j\u2208[i\u22121]. To this end, for each 1 \u2264 i < N3 , let the set of all iterator indices associated with an index i be defined as Ki \u225c {(k, x)|i \u2208 Jxk } with Jxk as noted in line 9. Then, we define the corresponding set for keys as Ii \u225c \u22c3 (k,x)\u2208Ki I x k with I x k s defined as in line 6. That is, for all the calls to pbs-key-values that i is part of (given by Ki) where the algorithm checks for a match among the keys in Ii, it then suffices to show that Ii = [i\u2212 1]. Here, first note that if some index j \u2208 Ixk \u2286 Ii for some x \u2208 \u03a3\nlog(N3 )\u2212k, then, by definition, x \u2208 prefix(bin(j)). Here, let x1 := x|x[log(N3 )\u2212k]=1 where we set the (log ( N 3 ) \u2212 k)-th index of x to be 1. Consequently, as we have i \u2208 Jxk for the same k and x as in Ixk (cf. line 6), we must have x1 \u2208 prefix(bin(i)). Thus we get j < i, whence we can claim that Ii \u2286 [i\u2212 1]. For the other direction, for any i, let b denote the position of the most significant bit in bin(i) which differs from bin(j) for any j \u2208 [i\u2212 1]. Then, there must exist a binary string that is in the prefix set of both bin(i) and bin(i). That is, there exists x \u2208 prefix(bin(i)) \u2229 prefix(bin(j)) with x \u2208 \u03a3b. Thus, we then must have bin(j) \u2208 Ix\nlog(N3 )\u2212b and bin(i) \u2208 Jx log(N3 )\u2212b with x as the corresponding\nwitness. Hence, we have [i\u2212 1] \u2286 Ii. Overall, we have shown that Ii = [i\u2212 1]. Since this holds for all 1 \u2264 i < N3 , we can conclude that Algorithm 11 solves the multiple-query associative recall problem.\nNext, it is easy to see that we execute lines 6 to 16 \u2211log(N3 )\u22121\nk=0\nN 3\n2k+1 -many times. We note that\nsorting n values each of size d can be done with work complexity n log n \u00b7 d. We note that, at each instance, we are sorting sorting 2k values. Meanwhile, remembering 2k sorted permutation of indices can be done in linear time using arrays. Moreover, each call to pbs-key-values has n = m = 2k which has work complexity n logm. Finally, we know that the work complexity of lines 18 to 20 is O(N). Thus, the overall work complexity of Algorithm 11 is\nd \u00b7 log(N3 )\u22121\u2211\nk=0\nN 3\n2k+1 O(2k \u00b7 log 2k) = d \u00b7 O(N) \u00b7 logN/3\u22121\u2211 k=0 O(k) = O(Nd \u00b7 log2 N). (13)\nWe will now analyze the depth of Algorithm 11. We know that the depth of computation for Algorithm 10 is O(log n logm) for input sizes. Moreover, we have O(1) depth for the computation in 18 to 20 as each entry in idx can have at most one entry. Since the nested for loops iterating over ks and the associated xs runs in parallel, the depth of Algorithm 11 is dominated by the largest depth among all calls to pbs-key-values and to sort. The largest such call to pbs-key-values is of size n = m = 2log(\nN 3 )\u22121 = N/6 which yields a depth of d \u00b7 log2 N3 . Moreover, using sorting\nnetworks (Definition H.34), we know that the largest depth is for sorting N6 values of size d given by d \u00b7\u0398(logN) (Lemma H.35). Thus, we can conclude that Algorithm 11 takesO(d \u00b7 log2 N), time where N is the length of the input."
        },
        {
            "heading": "H.7.6 CONVERSION TO ARITHMETIC CIRCUIT",
            "text": "We will convert Algorithm 11 to an arithmetic circuits modularly. In particular, after writing out an explicit circuit for Algorithm 10, we will uses this circuit as a black-box along with circuits for sorting networks.\nCircuit for pbs-key-values. We will denote the corresponding arithmetic circuit for Algorithm 10 as pbs-key-values as well with the input gates comprising of each entry from q[s \u00b7 \u00b7 \u00b7 t] and k[x \u00b7 \u00b7 \u00b7 y] and the i-th output gate yielding the value C[i] as in Algorithm 10. Here, we first convert the comparisons for the if statements in Algorithm 10. To this end, we briefly introduce comparators. Definition H.31 (Comparators). A comparator is a device with inputs x and y and outputs x\u2032 and y\u2032, that performs the following function:\nx\u2032 = min(x,y),\ny\u2032 = max(x,y).\nUsing comparators, we can use bit-wise XOR and AND to define the result of the comparisons in lines 5 and 10 as the following fixed variables:\n\u2113x := 1{q[mid] \u2264 kx}, gy := 1{q[mid] > ky}, z := bin-search({ki}ti=s, q[mid]).\n(14)\nThis then allows us to infer the index of the key array for each recursive call to pbs-key-values in lines 8, 13, 18, and 19 from Algorithm 10. Specifically, let zs and zt \u2212 1 denote the starting and ending indices for the keys as inputs to the recursive calls in Algorithm 10) below:\npbs-key-values(q[mid + 1 \u00b7 \u00b7 \u00b7 t], k[zt \u00b7 \u00b7 \u00b7 y]); (lines 8 and 19) (15) pbs-key-values(q[s \u00b7 \u00b7 \u00b7mid\u2212 1], k[x \u00b7 \u00b7 \u00b7 zs \u2212 1]); (lines 13 and 18) (16)\nHere, zt and zs can assume values dependent on the results of the comparisons in lines 5 and 10. Specifically, we have\nzt = \u2113x \u00b7 x+ (1\u2212 \u2113x)(1\u2212 gy) \u00b7 z =  x if q[mid] \u2264 kx (line 8) z if q[mid] \u2208 (kx,ky] (line 19) 0 otherwise ,\nzs = gy \u00b7 (y + 1) + (1\u2212 \u2113x)(1\u2212 gy) \u00b7 z =  y + 1 if q[mid] > ky (line 13) z if q[mid] \u2208 (kx,ky] (line 18) 0 otherwise .\nHere, zs or zt getting a value 0 signifies that the branch is dead, and we do not execute the recursive call. Finally, let the arrays Ct[mid+1 \u00b7 \u00b7 \u00b7 t] and Cs[s \u00b7 \u00b7 \u00b7mid\u22121] denote the outputs to the recursive calls in equation 15 and equation 16, respectively. We can then succinctly express the outputs for each index of the output array C as\nC[i] =  \u2113x \u00b7 x+ zs \u00b7 (C1[i]) i \u2208 [s \u00b7 \u00b7 \u00b7mid\u2212 1] gy \u00b7 (y + 1) + zt \u00b7 (C2[i]) i \u2208 [mid + 1 \u00b7 \u00b7 \u00b7 t] \u2113x \u00b7 x+ (1\u2212 \u2113x)(1\u2212 gy) \u00b7 z + gy \u00b7 (y + 1) i = mid\n(17)\nWe can thus state the circuit schematically in Fig. 9. Now, before accounting for the complexity of the circuit for Algorithm 10, we must first assert the complexity of the comparators that we use in Fig. 9.\nLemma H.32 (Cormen et al. (2022)). For binary strings x,y \u2208 \u03a3d of length d, there exists a comparison network of size O(d), width O(d), and depth O(log d). We use this lemma to compute the complexity of the arhmetic circuit from Fig. 9. Proposition H.33. There exists an (O((n + m)d),O(nd \u00b7 (logm + log n)),O(log n(logm + log n log d)),O(nd))-arithmetic circuit14 equivalent to pbs-key-values (Algorithm 10) with inputs q and k of lengths n and m with d-bit entries.\nProof. The size of the circuit for pbs-key-values should equal the work-complexity of Algorithm 10 but we also need to account for the comparison gates in equation 14. Further, the circuit for binary search also has a size of O(d \u00b7 n) instead of O(d \u00b7 log n) work as in Algorithm 10. Using Lemma H.32, along with the fact that the comparison gates and the binary search are used at most O(log n) times, we deduce that we are adding O(nd log n) size to the circuit in addition to the work complexity of the parallel algorithm. Thus, the overall size of the arithmetic circuit for pbs-key-values isO(dn logm+nd log n). Further, the depth of the circuit here is determined\n14Recall that a (n, s,\u2206, w)-arithmetic circuit is an n-variate circuit with size s, depth at most \u2206, and width w.\nby the runtime of Algorithm 10 along with the depth of the comparison gates and binary search O(log n log d), and finally, at most n processors are used for the parallel algorithm in Algorithm 10, which yields the width of the circuit.\nCircuit for Parallel-MQAR. Now, we will call the circuit for Parallel-MQARwith the same name while the input gates contain the inputs of Algorithm 11. Indeed, we can directly \u201ctranslate\u201d Algorithm 11 to an arithmetic circuit as the values for Ixk and I x k for each x and k are predetermined from N . Thus, we start by placing the corresponding sorting networks which feeds into the pbs-key-values (q[s \u00b7 \u00b7 \u00b7 t], k[x \u00b7 \u00b7 \u00b7 y], n,m) circuit for Algorithm 10 in Fig. 9 so that the output values from the calls to pbs-key-values result in the checks as in line 16 of Algorithm 11. That is, we get outputs Ck[Jkxpermuted(dec(i))] from each call to pbs-key-values. We can then use a comparison gate to check if this value equals 2k, and if not, we have found a match Ck[Jkxpermuted(dec(i))] for the query qi which results in the output of the associated value vCk[Jkxpermuted(dec(i))]+1, exactly as in Algorithm 11. That is, we first define the following variable as the output of the comparison gate:\nckdec(i) := 1{Ck[J kx permuted(dec(i))] \u0338= 2k}. (18)\nHere, as Ck[Jkxpermuted(dec(i))] \u0338= 2k implies that Ikxpermuted ( Ck[J kx permuted(dec(i))] ) equals the\nindex of the matching key kj corresponding to the query qi, the ith output is then simply given by\nckdec(i) \u00b7 I kx permuted ( Ck[J kx permuted(dec(i))] ) , where the 0 output implies that there does not exist a matching key. Here, we also briefly introduce the the sorting networks that we use to sort the keys and queries: Definition H.34 (Informal). Sorting networks are circuits with gates and wires where the gates of the circuit are comparators (Definition H.31) connecting two wires. Each such circuit can perform sorting on a fixed number of values. We can then show the circuit schematically as in Fig. 10. We now dilineate the complexity of the circuit, starting with the complexity of the sorting networks. Lemma H.35 (Ajtai et al. (1983)). Let A be an array with d-bit entries of size n. Then, one can implement a sorting network to sort the array A with size O(d \u00b7 n log n) and depth O(log d log n). Proposition H.36. There exists an (N \u00b7 d,O(Nd \u00b7 log2 N),O(log d log2 N),O(Nd logN))arithmetic circuit that solves the multiple-query associative recall problem.\nProof. We note here that for each k, there are N 3\n2k+1 parallel problems of size 2k for both the sorting\nnetworks and the pbs\u2212 key\u2212 values circuit. Using Lemma H.35, the cumulative size of these sorting networks is O(d \u00b7N log2 N) (see equation 13) with overall depth O(log d logN). Similarly, the next layer again runs \u2211log N3 \u22121 k=0 N 3 2k+1 -many circuits for pbs\u2212 key\u2212 values each of which has size O(2kd(log 2k + log 2k)) = O(d \u00b7 2k log 2k), depth O(log2 2k log d) and width O(2k) (Proposition H.33). Again, the cumulative size of this layer is given by O(Nd \u00b7 log2 N) (see equation 13). Since we run each of these circuits in parallel, the depth of this layer is again O(log d log2(N)) while the width is O(N \u00b7 logN). Finally, we perform N3 log N 3 comparisons at the end of d-bit strings in parallel which results in size O(N logN \u00b7 d), depth O(log d) and width O(N logN \u00b7 d) (Lemma H.32). Therefore, the resulting arithmetic circuit has size O(d \u00b7N log2 N +Nd \u00b7 log2 N +N logN \u00b7 d) = O(Nd log2 N), depth O(log d log2 N) and width O(Nd logN)."
        },
        {
            "heading": "H.7.7 THE RESULTING BASECONV MODEL",
            "text": "As we have an arithemtic circuit for solving the multiple-query associative recall problem, we can now invoke Theorem H.21 to claim that there is a corresponding BASECONV model that solves the multiple-query associative recall problem with O\u0303(N log c) parameters and \u02dcO(1) layers. Theorem H.37. There exists a ( Nd, O\u0303(1), O\u0303(1), O\u0303(Nd), O\u0303(1) ) \u2212BASECONV solves the multiple-\nquery associative recall problem.\nProof. Directly applying Theorem H.21 yields a BASECONV model with the number of layers O(log d \u00b7 log2 N \u00b7 logNd logN) = O(1) layers while the claim on the input and inner dimensions follow trivially."
        },
        {
            "heading": "H.8 DATA-DEPENDENT CONVOLUTIONS",
            "text": "H.8.1 INTRODUCTION\nIn this section, we are again concerned with solving the multiple-query associative recall problem (Appendix H.7.1). However, in contrast to Appendix H.7.4, which yields a circuit that is unchanged and works for all inputs, we instead take the viewpoint of adapting the sequence mixing weights (Section 2) with respect to the particular sequence that the model receives as input. More specifically, we take the distance between the tokens in the sequence as a measure for designing data-dependent convolutions.\nSetup. To formally setup the problem, as in our discussion of designing a parallel algorithm, we consider the following problem description of the multiple-query associative recall problem.\nSuppose we are given an input u[0 \u00b7 \u00b7 \u00b7N \u2212 1] \u225c{ (k0,v0, q0) , . . . , ( kN\n3 \u22121 ,vN 3 \u22121 , qN 3 \u22121\n)} with each ki,vi, qi \u2208 C. Here,\neach token is embedded using the standard one-hot encoding in {0, 1}c (i.e. we assume d = c).15 Our goal is again to check, for each 1 \u2264 i \u2264 N3 \u2212 1, whether there exists 0 \u2264 j < i such that qi \u2261 kj , and if so, output vj .\n15Our arguments do need c = d. However we do not need d = N but we made this simplification for ease of presentation.\nHere, we define the interaction distance between the ith query qi and the matching key kj as i\u2212 j. We then also assume that number of distinct interaction distances is bounded by t.\nWe can identify these distances using an autocorrelation (Chatfield, 1995), which has an elegant underlying formulation. We will briefly introduce the relevant mathematical machinery in the context of elucidating the data-dependent model that we seek to develop in the sequel.\nAutocorrelations We introduce autocorrelation convolutions. Let u\u0303[t] := u[\u2212t], then the cross correlation of two vectors u and v is given by\nu \u22c6 v \u225c u\u0303 \u2217 v.\nThe autocorrelation of a vector u \u2208 Rn is the cross correlation of u with itself. Moreover, in terms of polynomials, we have u\u0303(X) = Xn\u22121 \u00b7 u(1/X). Thus, in analogy with our interpretation of convolution in terms of polynomial multiplication, we characterize the autocorrelation of a vector u \u2208 Rn, given by w \u2208 Rn as follows:\nw = coeff (u(X) \u00b7 u\u0303(X) mod Xn \u2212 1) ."
        },
        {
            "heading": "H.8.2 BASECONV WITH KERNELS GENERATED USING AUTO-CORRELATION",
            "text": "We are now ready to describe the model that solves the multiple-query associative recall problem using data-dependent kernels derived using auto-correlations.\nThe Input-Dependent Kernels. There are several potential strategies to identify the best tokeninteraction distances for an input using a convolution. Here we will focus on an autocorrelation convolution for exposition. Autocorrelation allows us to identify the top t distinct shifts of the sequence that result in highly overlapping values (e.g. matches between query and keys in our associative recall setting). We can then construct convolution filters that perform each of these t shifts. That is, we define a function Top such that Top (u \u22c6 u, t) returns a list of the top t shifts {s\u2113}\u2113\u2208[t]. We then use these top t distances {s\u2113}\u2113\u2208[t] to define the following two kernels:\nhk(X) \u2261 \u2211 \u2113\u2208[t] Xs\u2113+(\u2113\u22121)\u00b7 N 3 ,\nhv(X) \u2261 \u2211 \u2113\u2208[t] Xs\u2113\u22121+(\u2113\u22121)\u00b7 N 3 .\n(19)\nHere, we note that we only only have two kernels as we will assume N \u2032 = tN3 and the shift will be done in \u201dparallel.\u201d Obviously, one can instead define t distinct shift kernels but then there is a cost of O(t) in the number of layers.\nProjections. We define the following projections K,Q,V \u2208 {0, 1}N\u00d7d of the input that we shall use below using equation 9.\nK[i, :] := { u[i, :] if i \u2208 K, 0d otherwise ,\nQ[i, :] := { u[i, :] if i \u2208 Q, 0d otherwise ,\nV[i, :] := { u[i, :] if i \u2208 V, 0d otherwise ,\n(20)\nFinally, we present the BASECONV model that solves the multiple-query associative recall problem with input-dependent kernels using O(1) layer and O(t \u00b7Nd)-many parameters. Theorem H.38. There exists a BASECONV model with gated data-dependent convolutions that solves the multiple-query associative recall problem on inputs from {0, 1}3N\u00d7c with the total number of distinct interaction distances bounded by t in O(1) layers and O(t \u00b7Nc) total parameters.\nProof. We note that we have the input dimension d = c. Now, for any input sequence u \u2208 {0, 1}N\u00d7d, we get the input-dependent kernels as in equation 19 using autocorrelation of the in-\nput. We will now outline the following computations for the BASECONV layers:\ny = LinearQ(u)\u2299 ( hK \u2217 LinearK(u) ) = Q\u2299 ( hK \u2217K ) (21)\nz = LinearE(y)\u2299 ( hV \u2217 LinearV(u) ) = E\u2299 ( hV \u2217V ) , (22)\nwhere we have the linear projections LinearQ(u) = Q, LinearK(u) = K, LinearV(u) = V and LinearE(y) = E defined as\nE[i, :] := LinearE(y)[i, :] = { 1d if \u2203 j \u2208 [d] such that y[i, j] = 1 0d otherwise . (23)\nHere, we will first present the argument for the special case when we have t = 1 as that will help us elucidate the general case. To this end, as the kernels from equation 19 for t = 1 are given by\nhk(X) \u2261 Xs1 ; hv(X) \u2261 Xs1\u22121,\n(24)\nwe observe that convolving with these kernels h \u2217 y is equivalent to operating with the following primitives (Appendix H.3):\nshift down(y, s1); shift down(y, s1 \u2212 1). (25)\nWe note that we shift down instead of shifting up as the index of the top-left entry is (0, 0). We can then write down the computations performed in equation 21 and equation 22 as follows:\ny = Q\u2299 shift down(K, s1) (26) z = E\u2299 shift down(V, s1 \u2212 1), (27)\nWe begin by examining y below:\ny[i, :] = (Q\u2299 shift down(K, s1)) [i, :] = Q[i, :]\u2299 shift down(K, s1)[i, :] (28)\n= ({ u[i, :] if i \u2208 Q 0d otherwise ) \u2299 ({ u[i\u2212 s1, :] if i\u2212 s1 \u2208 K, 0d otherwise ) (29)\n= { u[i, :]\u2299 u[i\u2212 s1, :] if i \u2208 Q and i\u2212 s1 \u2208 K, 0d otherwise\nHere, we use the fact that the Hadamard product is row-independent in equation 28, and the definitions of the projections from equation 20 in equation 29. Examining the jth entry, we get\nu[i, j]\u2299 u[i\u2212 s1, j] = { 1 if i \u2208 Q, i\u2212 s1 \u2208 K and qi \u2261 ki\u2212s1 \u2261 ej 0 otherwise.\nThat is, we can express\ny[i, :] = { ej if i \u2208 Q, i\u2212 s1 \u2208 K and qi \u2261 ki\u2212s1 \u2261 ej 0d otherwise . (30)\nConsequently, as per the definition in equation 23, we get\nE[i, :] = { 1d if i \u2208 Q, i\u2212 s1 \u2208 K and qi \u2261 ki\u2212s1 0d otherwise\n(31)\nWe can now finally specify the output z from equation 27 as follows:\nz[i, :] = (E\u2299 shift down(V, s1 \u2212 1)) [i, :] = E[i, :]\u2299 shift down(V, s1 \u2212 1)[i, :] (32)\n= ({ 1d if i \u2208 Q, i\u2212 s1 \u2208 K and qi \u2261 ki\u2212s1 0d otherwise ) \u2299 ({ u[i\u2212 s1 + 1, :] if i\u2212 s1 + 1 \u2208 V, 0d otherwise ) (33)\n= { u[i\u2212 s1 + 1, :] if i \u2208 Q, i\u2212 s1 \u2208 K, i\u2212 s1 + 1 \u2208 V and qi \u2261 ki\u2212s1 0d otherwise\n= { vi\u2212s1 if qi \u2261 ki\u2212s1 0d otherwise\n(34)\nAgain, we use the fact that the Hadamard product is row-independent in equation 32, and the definitions of the projections from equation 20 in equation 33. Overall, we have solved associative recall for all queries that have interaction distance exactly equal to s1. In order to generalize this to arbitrary t \u2264 N3 , we first increase the internal dimension so that the input to the kernels u\u2032 \u2208 R(N \u00b7t)\u00d7d in equation 19 and the projections K\u2032,Q\u2032,V\u2032 \u2208 R(N \u00b7t)\u00d7d are given by\nu\u2032 \u2261  0d ... 0d\nu\n ,K \u2032 \u2261  0d ... 0d\nK\n ,Q\u2032 \u2261  Q ... Q\nQ\n ,V\u2032 \u2261  0d ... 0d\nV\n ,\nWe then observe that for hk\u2113 (X) := X s\u2113 and hv\u2113 (X) := X s\u2113\u22121, we have\nhk(X) \u2261 \u2211 \u2113\u2208[t] hk\u2113 (X) \u00b7X(\u2113\u22121)\u00b7 N 3 ,\nhv(X) \u2261 \u2211 \u2113\u2208[t] hk\u2113 (X) \u00b7X(\u2113\u22121)\u00b7 N 3 .\nIn analogy with equation 25, we can then equivalently write\n( hK \u2217K ) \u2261  hKt ... hK2\nhK1\n \u2217  0d ... 0d\nK\n\n\u2261  hKt \u2217K ... hK2 \u2217K\nhK1 \u2217K\n\n\u2261  shift down(K, st) ... shift down(K, s2)\nshift down(K, s1)\n .\nSimilarly, we also have\n( hV \u2217V\u2032 ) \u2261  shift down(V, st \u2212 1) ... shift down(V, s2 \u2212 1)\nshift down(V, s1 \u2212 1)\n .\nThat is, the argument for t = 1 now applies to each of the t shifts as we now have (cf. equation 21)\ny\u2032 \u2261 Q\u2032 \u2299 ( hV \u2217V )\n\u2261  Q ... Q\nQ\n \u2299  shift down(V, st \u2212 1) ... shift down(V, s2 \u2212 1)\nshift down(V, s1 \u2212 1)\n\n\u2261  Q\u2299 shift down(V, st \u2212 1) ... Q\u2299 shift down(V, s2 \u2212 1)\nQ\u2299 shift down(V, s1 \u2212 1)\n\n\u2261  yt ... y2\ny1\n ,\nwhere, for each \u2113 \u2208 [t], we have (cf. equation 30)\ny\u2113[i, :] \u2261 { ej if i \u2208 Q, i\u2212 s\u2113 \u2208 K and qi \u2261 ki\u2212s\u2113 \u2261 ej , 0d otherwise .\nWe then analogously get E\u2032 as follows:\nE\u2032 \u2261 LinearE(y\u2032) \u2261  LinearE(yt) ... LinearE(y2)\nLinearE(y1)\n \u2261  Et ... E2\nE1\n ,\nwhere, for each \u2113 \u2208 [t], we have (cf. equation 31)\nE\u2113[i, :] = { 1d if i \u2208 Q, i\u2212 s\u2113 \u2208 K and qi \u2261 ki\u2212s\u2113 0d otherwise\nThe output in the general case is then given by z\u2032 \u2261 E\u2032 \u2299 ( hV \u2217V\u2032 )\n\u2261  Et ... E2\nE1\n \u2299  shift down(V, st \u2212 1) ... shift down(V, s2 \u2212 1)\nshift down(V, s1 \u2212 1)\n\n\u2261  Et \u2299 shift down(V, st \u2212 1) ... E2 \u2299 shift down(V, s2 \u2212 1)\nE1 \u2299 shift down(V, s1 \u2212 1)\n\n\u2261  zt ... z2\nz1\n ,\nwhere, for each \u2113 \u2208 [t], we have (cf. equation 34) z\u2113[i, :] \u2261 { vi\u2212s\u2113 if qi \u2261 ki\u2212s\u2113 0d otherwise\nFinally, we define the last output layer to compute zout \u2261 Linearsum(z\u2032) \u2261 \u2211\n\u2113\u2208[t] z\u2113 so that we have zout[i, :] \u2261 { vi\u2212s\u2113 if qi \u2261 ki\u2212s\u2113 for some \u2113 \u2208 [t] 0d otherwise\nTo recall, we retrieved the top t interaction distances of the input u using auto-correlation and defined the corresponding convolution kernels (equation 19). We then shifted djown the keys K using the first kernel and gated with the corresponding queries Q so that we got a match exactly when there exists a key that is at s\u2113 interaction distance from the corresponding query. After \u201csmearing\u201d this match to get E, we used it as a mask to retrieve the value in the next layer. Overall, since we have t atomic kernels that perform t shifts with each of these kernels using O(Nd) parameters, we can conclude that the output solves the associative recall problem for all queries with exactly \u2113 interaction distance from the corresponding keys for all \u2113 \u2208 [t] using O(1) layers and O(t \u00b7 Nc) parameters as we have d = c."
        }
    ],
    "year": 2023
}