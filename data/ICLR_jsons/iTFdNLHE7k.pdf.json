{
    "abstractText": "Normalising Flows are non-parametric statistical models characterised by their dual capabilities of density estimation and generation. This duality requires an inherently invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve good results. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.",
    "authors": [
        {
            "affiliations": [],
            "name": "KERNELISED NORMALISING FLOWS"
        }
    ],
    "id": "SP:f5368cd9e07ad027a1eaadf772fc3a33aa6f116a",
    "references": [
        {
            "authors": [
                "Jens Behrmann",
                "Will Grathwohl",
                "Ricky T.Q. Chen",
                "David Duvenaud",
                "J\u00f6rn-Henrik Jacobsen"
            ],
            "title": "Invertible residual networks, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Clare Bycroft",
                "Colin Freeman",
                "Desislava Petkova",
                "Gavin Band",
                "Lloyd T. Elliott",
                "Kevin Sharp",
                "Allan Motyer",
                "Damjan Vukcevic",
                "Olivier Delaneau",
                "Jared O\u2019Connell",
                "Adrian Cortes",
                "Samantha Welsh",
                "Alan Young",
                "Mark Effingham",
                "Gil McVean",
                "Stephen Leslie",
                "Naomi Allen",
                "Peter Donnelly",
                "Jonathan Marchini"
            ],
            "title": "The uk biobank resource with deep phenotyping and genomic",
            "venue": "data. Nature,",
            "year": 2018
        },
        {
            "authors": [
                "Biwei Dai",
                "Uros Seljak"
            ],
            "title": "Sliced iterative generator",
            "venue": "CoRR, abs/2007.00674,",
            "year": 2020
        },
        {
            "authors": [
                "Biwei Dai",
                "Uros Seljak"
            ],
            "title": "Sliced iterative normalizing flows",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using real nvp, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Dheeru Dua",
                "Casey Graff"
            ],
            "title": "UCI machine learning repository, 2017",
            "venue": "URL http://archive. ics.uci.edu/ml",
            "year": 2017
        },
        {
            "authors": [
                "Conor Durkan",
                "Artur Bekasov",
                "Iain Murray",
                "George Papamakarios"
            ],
            "title": "Neural spline flows, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Eshant English",
                "Matthias Kirchler",
                "Christoph Lippert"
            ],
            "title": "Mixerflow for image modelling, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Mathieu Germain",
                "Karol Gregor",
                "Iain Murray",
                "Hugo Larochelle"
            ],
            "title": "Made: Masked autoencoder for distribution estimation",
            "year": 2015
        },
        {
            "authors": [
                "Will Grathwohl",
                "Ricky T.Q. Chen",
                "Jesse Bettencourt",
                "Ilya Sutskever",
                "David Duvenaud"
            ],
            "title": "Ffjord: Free-form continuous dynamics for scalable reversible generative models, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Derek Hansen",
                "Brian Manzo",
                "Jeffrey Regier"
            ],
            "title": "Normalizing flows for knockoff-free controlled feature selection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Xi Chen",
                "Aravind Srinivas",
                "Yan Duan",
                "Pieter Abbeel"
            ],
            "title": "Flow++: Improving flowbased generative models with variational dequantization and architecture design",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Zhichun Huang",
                "Rudrasis Chakraborty",
                "Vikas Singh"
            ],
            "title": "Forward operator estimation in generative models with kernel transfer operators, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Prafulla Dhariwal"
            ],
            "title": "Glow: Generative flow with invertible 1x1 convolutions, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Kirchler",
                "Christoph Lippert",
                "Marius Kloft"
            ],
            "title": "Training normalizing flows from dependent data",
            "venue": "arXiv preprint arXiv:2209.14933,",
            "year": 2022
        },
        {
            "authors": [
                "Ivan Kobyzev",
                "Simon J.D. Prince",
                "Marcus A. Brubaker"
            ],
            "title": "Normalizing flows: An introduction and review of current methods",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images. 2009b. URL https: //api.semanticscholar.org/CorpusID:18268744",
            "year": 2009
        },
        {
            "authors": [
                "V Laparra",
                "G Camps-Valls",
                "J Malo"
            ],
            "title": "Iterative gaussianization: From ICA to random rotations",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2011
        },
        {
            "authors": [
                "Sanggil Lee",
                "Sungwon Kim",
                "Sungroh Yoon"
            ],
            "title": "Nanoflow: Scalable normalizing flows with sublinear parameter complexity, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Juan Maro\u00f1as",
                "Oliver Hamelijnck",
                "Jeremias Knoblauch",
                "Theodoros Damoulas"
            ],
            "title": "Transforming gaussian processes with normalizing flows, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Chenlin Meng",
                "Linqi Zhou",
                "Kristy Choi",
                "Tri Dao",
                "Stefano Ermon"
            ],
            "title": "ButterflyFlow: Building invertible layers with butterfly matrices",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "George Papamakarios",
                "Theo Pavlakou",
                "Iain Murray"
            ],
            "title": "Masked autoregressive flow for density estimation, 2018",
            "year": 2018
        },
        {
            "authors": [
                "George Papamakarios",
                "Eric Nalisnick",
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Balaji Lakshminarayanan"
            ],
            "title": "Normalizing flows for probabilistic modeling and inference, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "John Platt"
            ],
            "title": "Sequential minimal optimization: A fast algorithm for training support vector machines",
            "year": 1998
        },
        {
            "authors": [
                "Joaquin Qui\u00f1onero-Candela",
                "Carl Edward Rasmussen"
            ],
            "title": "A unifying view of sparse approximate gaussian process regression",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Ralf Herbrich",
                "Alex J Smola"
            ],
            "title": "A generalized representer theorem",
            "venue": "In Computational Learning Theory: 14th Annual Conference on Computational Learning Theory, COLT 2001 and 5th European Conference on Computational Learning Theory, EuroCOLT",
            "year": 2001
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Alexander J Smola",
                "Francis Bach"
            ],
            "title": "Learning with kernels: support vector machines, regularization, optimization, and beyond",
            "venue": "MIT press,",
            "year": 2002
        },
        {
            "authors": [
                "S\u00f6ren Sonnenburg",
                "Gunnar R\u00e4tsch",
                "Christin Sch\u00e4fer",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Large scale multiple kernel learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2006
        },
        {
            "authors": [
                "Rhea Sanjay Sukthanker",
                "Zhiwu Huang",
                "Suryansh Kumar",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Generative flows with invertible attentions, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Li Wenliang",
                "Danica J Sutherland",
                "Heiko Strathmann",
                "Arthur Gretton"
            ],
            "title": "Learning deep kernels for exponential family densities",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Gordon Wilson",
                "Zhiting Hu",
                "Ruslan Salakhutdinov",
                "Eric P Xing"
            ],
            "title": "Deep kernel learning",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Maximum likelihood is a fundamental approach to parameter estimation in the field of machine learning and statistics. However, its direct application to deep generative modelling is rare due to the intractability of the likelihood function. Popular probabilistic generative models such as Diffusion Models (Dai & Seljak, 2020b) and Variational Autoencoders (Kingma & Welling, 2022) instead resort to optimising the Evidence Lower Bound (ELBO), a lower bound on the log-likelihood, due to the challenges in evaluating likelihoods.\nThe change of variables theorem offers a neat and elegant solution to compute the exact likelihood for deep generative modelling. These models, known as normalising flows, employ invertible architectures to transform complex probability distributions into simpler ones. Normalising flows (Papamakarios et al., 2021; Kobyzev et al., 2021) excel in efficient density estimation and exact sampling, making them suitable for various applications.\nWhilst flow-based models possess appealing properties rooted in invertibility, they also impose limitations on modelling choices, which can restrict their expressiveness. This limitation can be mitigated by employing deeper models with a higher number of parameters. For instance, the Glow model Kingma & Dhariwal (2018) utilised approximately 45 million parameters for image generation on CIFAR-10 Krizhevsky (2009a), whereas StyleGAN3 Karras et al. (2019), a method that doesn\u2019t use likelihood optimisation, achieved a superior FID score with only about a million parameters.\nThe issue of over-parameterisation in flow-based models hinders their effectiveness in domains with limited data, such as medical applications. For example, normalising flows can be used to model complex phenotypic or genotypic data in genetic association studies (Hansen et al., 2022; Kirchler et al., 2022); collection of high-quality data in these settings is expensive, with many studies only including data on a few hundred to a few thousand instances. In scenarios with low data availability, a flow-based network can easily memorise the entire dataset, leading to an unsatisfactory performance on the test set. While existing research has focused on enhancing the expressiveness of flows through clever architectural techniques, the challenge of achieving parameter efficiency has mostly been overlooked with few exceptions Lee et al. (2020). Most normalising flows developed to date rely on neural networks to transform complex distributions into simpler distributions. However, there is no inherent requirement for flows to use neural networks. Due to their over-parameterisation and low inductive bias, neural networks tend to struggle with generalisation in the low-data regime, making them inapplicable to many real-world applications.\nIn this work, we propose a novel approach to flow-based distribution modelling by replacing neural networks with kernels. Kernel machines work well in low-data regimes and retain expressiveness even at scale. We introduce Ferumal flows, a kernelised normalising flow paradigm that outperforms or achieves competitive performance in density estimation for tabular data compared to other efficiently invertible neural network baselines like RealNVP and Glow. Ferumal flows require up to 93% fewer parameters than their neural network-based counterparts whilst still matching or outperforming them in terms of likelihood estimation. We also investigate efficient training strategies for larger-scale datasets and show that kernelising the flows works especially well on small datasets."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 MAXIMUM LIKELIHOOD OPTIMISATION WITH NORMALISING FLOWS",
            "text": "A normalising flow is an invertible neural network, f : RD \u2192 RD that maps real data x onto noise variables z. The noise variable z is commonly modelled by a simple distribution with explicitly known density, such as a normal or uniform distribution, while the distribution of x is unknown and needs to be estimated. As normalising flows are maximum likelihood estimators, for a given data set of instances x1, . . . , xn, we want to maximise the log-likelihood over model parameters,\nmax f n\u2211 i=1 log (pX(xi)) .\nWith the change of variables formula,\npX(x) = pZ (f(x)) |det Jf (x)| , where Jf (x) is the Jacobian of f in x, we can perform this optimisation directly:\nmax f n\u2211 i=1 log (pZ (f (xi))) + log (|det Jf (xi)|) .\nThe first part, log(pZ(f(xi))), can be computed in closed form due to the choice of pZ . The second part, log(|det Jf (xi)|), can only be computed efficiently if f is designed appropriately."
        },
        {
            "heading": "2.2 COUPLING LAYERS",
            "text": "One standard way to design such an invertible neural network f with tractable Jacobian is affine coupling layers (Dinh et al., 2017). A coupling layer C\u2113 : RD \u2192 RD is an invertible layer that maps an input y\u2113\u22121 to an output y\u2113 (\u2113 is the layer index): first, permute data dimensions with a fixed permutation \u03c0\u2113, and split the output into the first d and second D \u2212 d dimensions:[\nu1\u2113 , u 2 \u2113 ] = [\u03c0\u2113(y\u2113\u22121)1:d, \u03c0\u2113(y\u2113\u22121)d+1:D] .\nCommonly, the permutation is either a reversal of dimensions or picked uniformly at random. Next, we apply two neural networks, s\u2113, t\u2113 : Rd \u2192 RD\u2212d, to the first output, u1\u2113 , and use it to scale and translate the second part, u2\u2113 :\ny2\u2113 = exp ( s\u2113 ( u1\u2113 )) \u2299 u2\u2113 + t\u2113 ( u1\u2113 ) .\nThe first part of y\u2113 remains unchanged, i.e., y1\u2113 = u 1 \u2113 , and we get the output of the coupling layer as: y\u2113 = C\u2113 (y\u2113\u22121) = [ y1\u2113 , y 2 \u2113 ] = [ u1\u2113 , exp ( s\u2113 ( u1\u2113 )) \u2299 u2\u2113 + t\u2113 ( u1\u2113 )] .\nThe Jacobian matrix of this transformation is a permutation of a lower triangular matrix resulting from u1\u2113 undergoing an identity transformation and u 2 \u2113 getting transformed elementwise by a function of u1\u2113 . The Jacobian of the permutation has a determinant with an absolute value of 1 by default. The diagonal of the remaining Jacobian consists of d elements equal to unity and the other D \u2212 d elements equal the scaling vector s\u2113 ( u1\u2113 ) . Thus, the determinant can be efficiently computed as the\nproduct of the elements of the scaling vector s\u2113 ( u1\u2113 ) .\nThe coupling layers are also efficiently invertible as only some of the dimensions are transformed and the unchanged dimensions can be used to obtain the scaling and translation factors used for the forward transformation to reverse the operation.\nMultiple coupling layers can be linked in a chain of L layers such that all dimensions can be transformed:\nf(x) = CL \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 C1(x), i.e., y0 = x and yL = f(x)."
        },
        {
            "heading": "2.3 KERNEL MACHINES",
            "text": "Kernel machines (Sch\u00f6lkopf et al., 2002) implicitly map a data vector u \u2208 Rp into a high-dimensional (potentially infinite-dimensional) reproducing kernel Hilbert space (RKHS), H, by means of a fixed feature map \u03d5 : Rp \u2192 H. The RKHS H has an associated positive definite kernel k(u, v) = \u27e8\u03d5(u), \u03d5(v)\u27e9H, where \u27e8\u00b7, \u00b7\u27e9H : H\u00d7H \u2192 R is the inner product of H. The kernel k can oftentimes be computed in closed form without requiring the explicit mapping of u, v into H, making computation of many otherwise intractable problems feasible. In particular, as many linear learning algorithms, such as Ridge Regression or Support Vector Machines, only require explicit computations of norms and inner products, these algorithms can be efficiently kernelised. Instead of solving the original learning problem in Rp, kernel machines map data into the RKHS and solve the problem with a linear algorithm in the RKHS, offering both computational efficiency (due to linearity and kernelisation) and expressivity (due to the nonlinearity of the feature map and the high dimensionality of the RKHS)."
        },
        {
            "heading": "3 FERUMAL FLOWS: KERNELISATION OF FLOW-BASED ARCHITECTURES",
            "text": "In this section, we extend standard coupling layers to use kernel-based scaling and translation functions instead. Whilst neural networks are known to perform well in large-data regimes or when transfer-learning from larger datasets can be applied, kernel machines perform well even at small sample sizes and naturally trade-off model complexity against dataset size without losing expressivity."
        },
        {
            "heading": "3.1 KERNELISED COUPLING LAYERS",
            "text": "We keep the definition of coupling layers in Section 2, and only replace the functions s\u2113 and t\u2113 by functions mapping to and from an RKHS H. We have to deal with two main differences to the kernelisation of many other learning algorithms: firstly, the explicit likelihood optimisation does not include a regularisation term that penalises the norm of the prediction function. And secondly, instead of a single mapping from origin space to RKHS to single-dimensional output, we aim to combine multiple layers, in each of which the scaling and translation map from origin space to RKHS to multi-dimensional output. As a result, the optimisation problem will not be convex in contrast to standard kernel learning, and we have to derive a kernelised and tractable representation of the learning objective.\nIn particular, in layer \u2113, we introduce RKHS elements V s\u2113 , V t \u2113 \u2208 HD\u2212d and define scaling and translation as\ns\u2113 ( u1\u2113 ) = [\u2329 V s\u2113,j , \u03d5 ( u1\u2113 )\u232a H ]D\u2212d j=1 \u2208 RD\u2212d and t\u2113 ( u1\u2113 ) = [\u2329 V t\u2113,j , \u03d5 ( u1\u2113 )\u232a H ]D\u2212d j=1 \u2208 RD\u2212d.\nWe summarise V\u2113 = [V s\u2113 , V t \u2113 ] and V = [V1, . . . , VL] for the full flow f(x) = CL \u25e6 \u00b7 \u00b7 \u00b7 \u25e6C1(x). Since elements V \u2208 H2L(D\u2212d) and H\u2019s dimensionality is potentially infinite, we cannot directly optimise the objective:\nmax V n\u2211 i=1 pZ (CL \u25e6 . . . \u25e6 C1(xi)) + log (|det JCL\u25e6...\u25e6C1(xi)|) = L(V ). (1)\nHowever, we can state a version of the representer theorem (Sch\u00f6lkopf et al., 2001) that allows us to kernelise the objective:\nProposition 3.1. Given the objective L in equation (1), for any V \u2032 = [V \u20321 , . . . , V \u2032L] \u2208 HL(D\u2212d) there also exists a V with L(V ) = L (V \u2032) such that\nV\u2113 = n\u2211 i=1 k ( \u00b7, u1\u2113,i ) A\u2113,i\nfor some A\u2113,i = [ As\u2113,i, A t \u2113,i ] \u2208 R2(D\u2212d). Here, u1\u2113,i = \u03c0\u2113(C\u2113\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 C1(xi))1:d, i.e., the first\npart of the permutated input to layer \u2113 for data point i. In particular, if there exists a solution V \u2032 \u2208 argmaxV L(V ), then there\u2019s also a solution V \u2217 of the form\nV \u2217\u2113 = n\u2211 i=1 k ( \u00b7, u1\u2113,i ) A\u2113,i.\nProof. Let V \u2032 \u2208 H2L(D\u2212d) and \u03a6\u2113 = span{\u03d5(u1\u2113,1), . . . , \u03d5(u1\u2113,n)} be the space spanned by the feature maps of layer \u2113 inputs, and let \u03a6\u22a5\u2113 denote its orthogonal complement in H. We can then represent each element V s\u2113,j\n\u2032 \u2208 H (j \u2208 {1, . . . , D \u2212 d}) as an orthogonal sum of an element of \u03a6\u2113 and \u03a6\u22a5\u2113 ,\nV s\u2113,j \u2032 = \u03d5\u2113,j + \u03d5 \u22a5 \u2113,j , with \u03d5\u2113,j = n\u2211 i=1 As\u2113,i,j\u03d5(u 1 \u2113,i) and \u27e8\u03d5\u22a5\u2113,j , \u03d5(u1\u2113,i)\u27e9H = 0 \u2200i = 1, . . . , n\nfor some values As\u2113,i,j \u2208 R. In the objective (1), we only use V s\u2113,j \u2032 to compute \u27e8V s\u2113,j \u2032, \u03d5(u1\u2113,i)\u27e9H as part of the computation of s\u2113(u1\u2113,i). But due to orthogonality, it holds that\n\u27e8V s\u2113,j \u2032, \u03d5(u1\u2113,i)\u27e9H = \u27e8\u03d5\u2113,j + \u03d5\u22a5\u2113,j , \u03d5(u1\u2113,i)\u27e9H = \u27e8\u03d5\u2113,j , \u03d5(u1\u2113,i)\u27e9H + \u27e8\u03d5\u22a5\u2113,j , \u03d5(u1\u2113,i)\u27e9H = \u27e8\u03d5\u2113,j , \u03d5(u1\u2113,i)\u27e9H.\nHence, replacing V s\u2113,j \u2032 by \u03d5\u2113,j = \u2211n i=1 A s l,i,j\u03d5(u 1 \u2113,i) keeps the objective unchanged. The reproducing property of the RKHS H now states that \u27e8\u03d5(u1\u2113,i), \u03d5(\u00b7)\u27e9H = k(u1\u2113,i, \u00b7).\nRepeating this for all \u2113 = 1, . . . , L, all j = 1, . . . , D \u2212 d and also for translations t\u2113 yields a version of V \u2032 that can be represented as a linear combination of the stated form.\nIn contrast to the classical representer theorem, the objective doesn\u2019t contain a regulariser of the model\u2019s norm, which would ensure that any solution can necessarily be represented as a linear combination of kernel evaluations. However, if a solution exists, Proposition 3.1 ensures that there also exists a solution that can be expressed as a linear combination of kernel evaluations. Therefore, we can re-insert this solution V \u2217 into the objective 1 to get a kernelised objective.\nFor layer \u2113 and arbitrary a \u2208 Rd,\ns\u2113(a) = [ \u27e8V s\u2217\u2113,j , \u03d5(a) \u232a ]D\u2212dj=1 =\n[ n\u2211\ni=1\nAs\u2113,i,jk ( u1\u2113,i, a )]D\u2212d j=1 = n\u2211 i=1 k ( u1\u2113,i, a ) As\u2113,i.\nAs in the objective (1), s\u2113 gets only evaluated in points a \u2208 { u1\u2113,i|i = 1, . . . , n } , this simplifies to\ns\u2113 ( u1\u2113,m ) = n\u2211 i=1 k ( u1\u2113,i, u 1 \u2113,m ) As\u2113,i = A s \u2113K ( U1\u2113 , U 1 \u2113 ) m ,\nwhere K ( U1\u2113 , U 1 \u2113 ) = [ k ( u1\u2113,i, u\u2113,m )1]n i,m=1\nis the kernel matrix at layer \u2113 and As\u2113 =[ As\u2113,1, . . . , A s \u2113,n ] \u2208 R(D\u2212d)\u00d7n is the weight matrix. A similar derivation holds for t\u2113.\nIn total, we can kernelise the objective (1) and optimise over parameters A \u2208 RL\u00d7n\u00d72(D\u2212d) instead of over V \u2208 H2L(D\u2212d). In contrast to neural network-based learning, the number of parameters, 2Ln(D \u2212 d), is fixed except for the number of layers (since d is usually set to \u230aD/2\u230b), but increases linearly with the dataset size, n. This makes kernelised flows especially promising for learning in the low-data regime, as their model complexity naturally scales with dataset size and does not over-parametrise as much as neural networks (as long as one does not employ an excessive number of layers).\nSince the resulting objective function is not convex, optimisers targeted to standard kernel machines such as Sequential Minimal Optimisation (Platt, 1998) are not applicable. Instead, we optimise (1) with variations of stochastic gradient descent (Kingma & Ba, 2014)."
        },
        {
            "heading": "3.2 EFFICIENT LEARNING WITH AUXILIARY POINTS",
            "text": "The basic kernelised formulation can be very computationally expensive even at moderate dataset sizes and can tend towards overfitting in the lower-data regime. In Gaussian Process (GP) regression, this problem is usually addressed via sparse GPs and the introduction of inducing variables (Qui\u00f1oneroCandela & Rasmussen, 2005). In a similar spirit, we introduce auxiliary points. In layer \u2113, instead of computing the kernel with respect to the full data u1\u2113,1, . . . , u 1 \u2113,n, we take N \u226a n new variables W 1\u2113 = [w 1 \u2113,1, . . . , w 1 \u2113,N ] \u2208 Rd\u00d7N and compute the scaling transform as\ns\u0302\u2113(u 1 \u2113,m) = A\u0302 s \u2113K(U 1 \u2113 ,W 1 \u2113 )m\nwith A\u0302s\u2113 \u2208 R(D\u2212d)\u00d7N (analogously for t\u0302\u2113). We make these auxiliary points learnable and initialise them to a randomly selected subset of u1\u2113,1, . . . , u 1 \u2113,n. This procedure reduces the learnable parameters from 2n(D \u2212 d)L (for both s\u2113 and t\u2113) to 2NDL. In another variation we make these auxiliary points shared between layers. In particular, instead of selecting L times N points w1\u2113,1, . . . , w 1 \u2113,N , we instead only select W 1 = [w11, . . . , w 1 N ] \u2208 Rd\u00d7N once and compute at layer \u2113\ns\u0304\u2113(u 1 \u2113,m) = A\u0302 s \u2113K(U 1 \u2113 ,W 1)m.\nThis further reduces the learnable parameters to 2N(D \u2212 d)L+ 2dN ."
        },
        {
            "heading": "4 RELATED WORKS",
            "text": "We are unaware of any prior work that attempts to replace neural networks with kernels in flow-based architectures directly. However, there is a family of flow models based on Iterative Gaussianisation (IG) Chen & Gopinath (2000) that utilise kernels. Notable works using Iterative Gaussianisation include Gaussianisation Flows Meng et al. (2020), Rotation-Based Iterative Gaussianisation (RBIG) Laparra et al. (2011), and Sliced Iterative Normalising Flows Dai & Seljak (2020a). These IG-based methods differ significantly from our methodology. They rely on kernel density estimation and inversion of the cumulative distribution function for each dimension individually and incorporate the dependence between input dimensions through a rotation matrix, which aims to reduce interdependence. In contrast, our method integrates kernels into coupling layer-based architectures. Furthermore, IG-based methods typically involve a large number of layers, resulting in inefficiency during training and a comparable number of parameters to neural network-based flow architectures. In contrast, the Ferumal flow approach of incorporating kernels can act as a drop-in replacement in many standard flow-based architectures, ensuring parameter efficiency without compromising effectiveness. Another generative model using kernels is the work on kernel transport operators (Huang et al., 2021). demonstrated promising results in low-data scenarios and favourable empirical outcomes. However, their approach differs from ours as they employed kernel mean embeddings and transfer operators, along with a pre-trained autoencoder.\nOther works focusing on kernel machines in a deep learning context are deep Gaussian processes Damianou & Lawrence (2013) and deep kernel learning (Wilson et al., 2016; Wenliang et al., 2019). Deep GPs concatenate multiple layers of kernelised GP operations; however, they are Bayesian, noninvertible models for prediction tasks instead of density estimation and involve high computational complexity due to operations that require inverting a kernel matrix. Deep kernel learning, on the other hand, designs new kernels that are parametrised by multilayer perceptrons.\nMaro\u00f1as et al. (2021) integrated normalising flows within Gaussian processes. Their approach differs significantly from ours as they aimed to exploit the invertibility property of flows by applying them to the prior or the likelihood. Their combined models consist of kernels in the form of GPs but also involve neural networks in the normalising flows network, resembling more of a hybrid model.\nNanoFlow Lee et al. (2020) also targets parameter efficiency in normalising flows. They rely on parameter sharing across different layers, whereas we utilise kernels. We also attempted to implement the naive parameter-sharing technique suggested by Lee et al. (2020), but we found no improvement in performance."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We assess the performance of our Ferumal flow kernelisation both on synthetic 2D toy datasets and on five real-world benchmark datasets sourced from Dua & Graff (2017). The benchmark datasets include Power, Gas, Hepmass, MiniBoone, and BSDS300. To ensure consistency, we adhere to the preprocessing procedure outlined by Papamakarios et al. (2018).\nImplementation details We kernelised the RealNVP and Glow architectures. For comparison purposes, RealNVP and Glow act as direct comparisons being the neural-net counterparts of our models. We have also included basic autoregressive methods, Masked Autoregressive Flows (Papamakarios et al., 2018), and Masked Autoregressive Distribution Estimation (Germain et al., 2015), FFJORD (Grathwohl et al., 2018) as a continuous normalising flow, and Gaussianisation Flows (Meng et al., 2020), Rotation-based Iterative Gaussianisation (Laparra et al., 2011), Sliced Iterative Normalising Flows (Dai & Seljak, 2020b) as iterative gaussianisation methods for our evaluations. Most autoregressive flow models outperform non-autoregressive flow models. However, they usually come with the trade-off of either inefficient sampling or inefficient density estimation, i.e., either the forward or the inverse computation is computationally very expensive.\nTraining details Our Ferumal Flow kernelisation has a negligible number of hyperparameters. Apart from learning rate hyperparameters (i.e., learning rate, \u03b21, \u03b22 for Adam) and the number of layers, that are central to both kernelised and neural-net-based flows, we only need to choose a kernel with its corresponding hyperparameters (and a number of auxiliary points for large-scale experiments). This is in contrast with neural-net-based flows where choices for a flow layer include a number of hidden sub-layers, the number of nodes in each sub-layer, residual connections, type of normalisation, activation function, dropout, and many more. Coupled with longer convergence times this necessitates considerably more time and resources in hyperparameter tuning than our proposed kernel methods. In our study,\nwe utilised either the Squared Exponential kernel or Matern Kernels exclusively for all experiments. We learnt all the kernel hyperparameters using the GPyTorch library for Python for the main experiments. Throughout the experiments, we used the Adam (Kingma & Ba, 2014) optimiser, whilst adjusting the \u03b21 and \u03b22 parameters of the optimiser. Additionally, we decayed the learning rate either with predefined steps (StepLR) or with cosine annealing. In all the experiments, we incorporated auxiliary points as we observed that they provided better results. In most cases, we persisted with 150 auxiliary points.\nWe coded our method in PyTorch (Paszke et al., 2019) and used existing implementations for the other algorithms. We ran all experiments for Ferumal flows and other baselines on CPUs (Intel Xeon 3.7 GHz). For more comprehensive training details, please refer to the Table 5\n5.1 2D TOY DATASETS Table 1: Results on toy datasets. Log Likelihood in nats, higher is better\nDataset Ours (#params) NN-based (#params)\nLine 3.75 (5K) 3.15 (44K) Pinwheel -2.44 (4K) -2.48 (44K) Moons -2.43 (5K) -2.54 (44K)\nInitially, we conducted density estimation experiments on three synthetic datasets that were sampled from twodimensional distributions exhibiting diverse shapes and numbers of modes.\nFigure 1 showcases the original data distribution alongside the samples generated using the Ferumal flow kernelisation and the corresponding neural network counterpart. The neural-net-based architecture clearly shows the distortion of density in many regions whereas the kernelised counterpart has much better modelling. Table 1 shows the corresponding log-likelihood in nats, quantitatively showing the enhancement from our kernelisation. The results demonstrate that Ferumal flow kernelisation can outperform its neural net counterpart on these toy datasets. All the toy datasets were trained with a batch size of 200 and for 10K iterations. We also investigated the effect of our kernelisation on highly discontinuous densities strengthening our argument for kernelisation. Please refer to Appendix B"
        },
        {
            "heading": "5.2 REAL-WORLD DATASETS",
            "text": "We conducted density estimation experiments on five real-world tabular benchmark datasets (description can be found in Appendix I), employing the preprocessing method proposed by Papamakarios et al. (2018). In our experiments, we kernelised two flow architectures, i.e., RealNVP and Glow, that utilise the coupling layer for efficient sampling and inference and making direct comparisons with them. Additionally, we also considered comparisons with GF (Gaussianisation Flows) (Meng et al., 2020), RBIG (Rotation-based Iterative Gaussianisation) (Laparra et al., 2011), GIS (Gaussianied Iterative Slicing/Sliced Iterative Normalising Flows) (Dai & Seljak, 2020b), MAF (Masked Autoregressive Flows) (Papamakarios et al., 2018), and MADE (Masked Autoregressive Distribution Estimation) (Germain et al., 2015), FFJORD (Grathwohl et al., 2018), architectures that do not use coupling layers. These methods are not directly comparable to the coupling-layer-based methods, as they have significantly higher computational costs. In particular, Gaussianisation-based methods, require many more layers (up to 100, in our settings), whilst autoregressive flows are slow to sample from, due to their autoregressive nature. In contrast to Gaussianisation-based methods, our kernelisation of coupling layers does not increase the computational complexity and the training time under a fixed number of epochs is similar to neural-net-based coupling layers. Run-time comparisons under a fixed number of epochs are provided in Table 10 in Appendix F),.\nTable 2 presents the results of our experiments, revealing that Ferumal flow kernelisation consistently achieves better or competitive outcomes across all five datasets. Despite its straightforward coupling layer architecture, our approach surpasses RBIG, GIS, and MADE on all the datasets and achieves competitive performance to the much more expensive MAF, GF, and FFJORD methods, underscoring the efficacy of integrating kernels. Please refer to Table 12 for error bars on coupling and non-coupling experiments."
        },
        {
            "heading": "5.3 INITIAL PERFORMANCE",
            "text": "Figure 7 presents the learning curves of the train and test loss for our Ferumal flow kernelisation and the two neural-network counterparts. These findings demonstrate that the Ferumal-flow-based\narchitecture exhibits faster convergence compared to the neural network baselines. This may be due to the parameter efficiency provided by our kernelisation or due to the stronger inductive biases provided by kernel machines. Throughout our experiments, we maintained default settings and ensured consistent batch sizes across all models.\n5.4 LOW-DATA REGIME\nIn certain applications, such as medical settings, data availability is often limited. Neural-network-based flows typically suffer from overparameterisation, leading to challenges in generalisation within lowdata regimes. To assess the generalisation capability of our model under such conditions, we trained our model using only 500 examples and evaluated its performance on the same benchmark\ndatasets. To address the challenges of limited data, we opted to tie the learned auxiliary variables across layers in this setting. This approach helped mitigate parameter complexity whilst maintaining the benefits of utilising auxiliary points.\nAs highlighted by Meng et al. (2020), Glow and RealNVP struggled to generalise in low-data regimes, evidenced by increasing validation and test losses whilst the training losses decreased. To provide a stronger benchmark, we included the FFJORD model (Grathwohl et al., 2018). FFJORD is a continuous normalising flow method with a full-form Jacobian and exhibits superior performance to Glow or RealNVP in density estimation and generation tasks. For our model, we used a kernelised version of RealNVP which is notably weaker than the Glow version. This also proves that kernelisation can make flow-based models more data-efficient.\nTable 3 presents the results, demonstrating that our method achieves superior generalisation. This may be attributed to the significantly lower number of parameters required compared to the continuous FFJORD method."
        },
        {
            "heading": "5.5 PARAMETER EFFICIENCY",
            "text": "Table 4 shows the parameter counts of Ferumal flows against the baseline methods. Kernelising the models results in a parameter reduction of up to 93%. The parameter efficiency consequently results in less data requirement, better generalisation, and faster convergence. This reduction can be further improved by implementing strategies such as sharing auxiliary variables between layers or potentially with low-rank approximations, particularly in scenarios where data is limited and concerns about overfitting arise (see Appendix C for additional details)."
        },
        {
            "heading": "6 DISCUSSION AND LIMITATIONS",
            "text": "We have introduced Ferumal flows, a novel approach to integrate kernels into flow-based generative models. Our study highlighted that Ferumal flows exhibit faster convergence rates, thanks to the inductive biases imparted by data-dependent initialisation and parameter efficiency. Moreover, we have demonstrated that kernels can significantly reduce the parameter count without compromising the expressive power of the density estimators. Especially in the low-data regime, our method shows superior generalisation capabilities, while Glow and RealNVP fail entirely, and FFJORD lags significantly in performance. We also demonstrate the application of our method in hybrid modelling. (Please refer to Appendix G)\nIn contrast to neural-network-based flows, kernelised flows require a different hyperparameter selection. In classical kernel machines, the choice of kernel usually implies a type of inductive bias (e.g., for specific data types (Vishwanathan et al., 2010)). Consequently, in this work, we mostly focus on Squared Exponential kernels and Matern kernels, but incorporating kernels with strong inductive biases may be a promising avenue for future research. In particular, parameter sharing for highly structured modalities such as images is another potential direction for future research.\nThe present work introduces kernels only for some affine coupling layer architectures such as RealNVP and Glow. However, the concepts also directly apply to other coupling-layer-type networks, such as neural spline flows (Durkan et al., 2019), ButterflyFlows (Meng et al., 2022), or invertible attention (Sukthanker et al., 2022) for greater expressiveness and parameter efficiency. Ferumal flow kernelisation can also be directly enhanced with other building blocks such as MixLogCDF-coupling layers (Ho et al., 2019).\nWhilst our method can be applied to coupling-type flow-based architectures, it poses challenges when it comes to ResFlow-like architectures Behrmann et al. (2019); Chen & Gopinath (2000), which require explicit control of Lipschitz properties of the residual blocks. As a result, extending our approach to ResFlow-like architectures is left as a direction for future research.\nOne major drawback of existing normalising flow algorithms is their dependence on an abundance of training data. The introduction of kernels into these models may allow the application of flows in low-data settings. Additionally, in the era of increasingly large and complex models, energy consumption has become a significant concern. Faster convergence can contribute to energy savings. Notably, our models, owing to their faster convergence and few hyperparameters needed fewer training runs than the neural-network counterparts. We anticipate that future research will continue to explore efficient methodologies and strive for reduced energy and data demands."
        },
        {
            "heading": "A ADDITIONAL EXPERIMENTAL DETAILS FOR OUR METHOD",
            "text": "We employed the Adam optimiser exclusively for all our experiments. The other hyperparameters are chosen using a random grid search, i.e. lr \u2208 [0.01, 0.005, 0.001], \u03b21, \u03b22 \u2208 [0.85, 0.9, 0.95, 0.99], kernel \u2208 [matern32,matern52, rbf ]. We used Cosine decay for all experiments with the minimum learning rate equalling zero. In initial experiments, we found that 150 auxiliary points performed satisfactorily and we persisted with it for the majority of datasets and tried 200 for datasets with high-dimensionality. For comprehensive information, please refer to Table 5.\nFor the experiments presented in Table 1, we employed Glow-based architectures for both approaches. We trained these datasets for 1000 training steps, with the data synthesised at every training step (as done in existing implementations). For experiments in Table 1, Table 3, we exclusively use the RBF/Squared-Exponential kernel and randomly sampled the kernel length scale from a log-uniform distribution, i.e., \u03b3 \u223c exp(U), where U \u223c U[\u22122,2]."
        },
        {
            "heading": "B MODELLING DISCONTINUOUS DENSITIES",
            "text": "We consider two toy datasets with discontinuous densities, CheckerBoard and Diamond. We used 4 flow steps for all the models and piecewise polynomial kernel and learnt its hyperparameters. Figure ?? showcases the original data samples(Left) alongside the modelled density(Middle) and the samples generated using the flow(Right). The neural-net-based architecture for\nthe checkerboard dataset in the first row shows blurry boundaries and ill-defined corners. The kernelised counterpart in the second row has better-defined boundaries in some cases. However, this effect is even more pronounced in the diamond dataset with the kernelised counterpart in the fourth row modelling the discontinuity way better. Table ?? shows the corresponding log-likelihood in nats, quantitatively showing the enhancement from our kernelisation. The results demonstrate that Ferumal flow kernelisation can outperform its neural net counterpart on these toy datasets. All the toy datasets were trained with a batch size of 512 and for 100K iterations. We found that the piecewise polynomial kernel was better suited for discontinuous densities than Matern kernels. This provides another evidence of better performance due to inductive biases of a kernel."
        },
        {
            "heading": "C LOW-RANK APPROXIMATIONS",
            "text": "For datasets characterised by high dimensionality and complex structures, relying solely on auxiliary points for the weight matrix (A \u2208 R2(D\u2212d)\u00d7N ) is inefficient. When half of the dimensions are transformed (as is the case in any coupling layer), this matrix becomes excessively large.\nTo preserve the desirable quality of generalisation in our models, we propose an alternative approach to obtaining the weight matrix for a kernelised layer. We suggest using the product of two smaller matrices (with fewer parameters) instead. For a weight matrix A \u2208 Rp\u00d7N , responsible for producing p affine parameters using N auxiliary points, we can learn two smaller matrices: A1 \u2208 Rc\u00d7N and A2 \u2208 Rc\u00d7p where c < p. By employing the outer product A\u0302 = A2\u22a4A1 \u2248 A as a proxy for a full-weight matrix, we can effectively reduce the number of parameters.\nIn Table 7, we present the effectiveness of employing a low-rank approximation on the identical subset of 500 samples, as depicted in Table 3 in the main manuscript. This technique ensures a minimal number of parameters while achieving satisfactory generalisation. During the experimental setup, we endeavoured to utilise the lowest feasible value of c (chosen via a hyperparameter grid of {4, 8, 12}) that would still yield reliable generalisation. Notably, our approach achieves good results while providing superior control over the parameters, in contrast to the shared auxiliary variable method.\nLearning without auxiliary points We also present a comparison of another variation of our method, i.e., learning without the use of auxiliary variables. Notably, the sharing of auxiliary variables yields the best results, followed by the utilisation of low-rank matrices in conjunction with auxiliary variables. Whilst the results obtained with low matrices are not significantly different from those obtained with shared auxiliary variables, they offer a further reduction in parameters. As seen in Table 7, not using auxiliary variables causes a high number of parameters and the model overfits easily causing comparatively poor results (notably fewer parameters than FFJORD, depicted in Table 3 in the main manuscript, while achieving somewhat similar performance)."
        },
        {
            "heading": "D DENSITY ESTIMATION ON REAL WORLD MEDICAL DATASET",
            "text": "Following our experiments within the low-data regime, as discussed in Section 5.4, we extend our analysis to a real-world medical dataset\u2014the UK Biobank (Bycroft et al., 2018; Kirchler et al., 2022). This dataset encompasses phenotype and genotype information for a substantial cross-section of the UK population, encompassing a total of 30 biomarkers. Notably, only 3,240 individuals within the dataset possess complete information on all biomarkers.\nIn line with our experiments in Section 5.4, we conducted a comparative analysis between our kernelised-RealNVP and FFJORD. The density estimation results presented in Table 8 exhibit better performance whilst training significantly faster, thereby reinforcing the findings in Table 3.\nTable 9: Results on the CIFAR-10 dataset. Log Likelihood in nats, higher is better\nMethod Ours params convergence step\nOurs(kernelised) -7644.39 4.85M 103K NN-based MixerFlow -7665.65 11.43M 195K\n(a) NN-based MixerFlow (b) Our kernelised MixerFlow\nFigure 4: Sampled images from MixerFlow\nE IMPROVING MIXERFLOW\u2019S IMAGE GENERATION\nMixerFlow (English et al., 2023) is a flow architecture for image modelling. Unlike Glow-based architectures that relies on convolutional neural networks for image generation, MixerFlow offers a flexible way to integrate any flow method, making it suitable for image generation with our kernelised coupling layers. In this section, we showcase the application of our kernelisation to the MixerFlow model on the CIFAR-10 dataset (Krizhevsky, 2009b). We employed small models(30 layers) and trained on a single gpu. For fair comparison, we kept the same model architecture with the sole change being the replacement of the neural-network-based coupling layers with our kernelised coupling layer. Our results as shown in Table 9 demonstrate kernelisation can yield better result with faster convergence attributed to reduction of parameters. The generated samples can be seen in Figure 4a and 4b."
        },
        {
            "heading": "F RUNTIME COMPARISONS",
            "text": "We picked the best models in each category from Table 2 and compared them with our kernelised Glow model. For fair comparisons, we ran the models for the same number of epochs and used the same number of flow steps for the neural-net counterpart, Glow model. It is worth noting that our model has faster convergence up to 3 times than the Glow model. Table 10 shows that despite using\nkernels, we have comparable run times with the neural-network counterpart. We perform significantly better than continuous time normalising flow, FFJORD, and iterative-gaussianisation-based kernel method, Gaussianisation flow, both take longer than a day on bigger datasets.\nG IMPROVING VAE\u2019S ELBO\nWe also try our kernelised flows in hybrid settings, demonstrating that we can integrate them with neural-net-based architectures. We apply our kernelised model, FF-GLow, to the variational autoencoder (Kingma & Welling, 2022) in the form of flexible prior and approximate posterior distributions. We apply the methods to Kuzushiji-MNIST, which is a variant of MNIST containing Japanese script. We investigate the capacity of our kernelisation to improve over the baseline of standard-normal prior and diagonal-normal approximate posterior, and its neural network counterpart, Glow. We use 6 flow steps for each flow-based model and the latent hidden dimension equals 16. The quantitative results are shown in Table 11 and generated image samples in Figure ??\nBoth models (Glow and ours FF-Glow) improve significantly over the standard baseline. However, there is no considerable quantitative gain by using the kernelised version. We believe that this might be due to the Glow model being sufficient to model the latent space on the dataset and having a little margin for kernelisation to shine. However, our kernelisation still helps in making the model parameter efficient with only a small increase in parameter complexity compared to the baseline."
        },
        {
            "heading": "H DISCUSSION ON OUR USE OF KERNELS",
            "text": "Effect of kernel length scale As with any kernel machine, the length scale serves as a highly sensitive hyperparameter in our method. During our investigation, we discovered that in identical settings, distinct kernel length scales produce varying outcomes. Certain scales have a tendency to overfit easily on the training set (e.g., -3.76 nats training likelihood on Miniboone), while some tend to underfit (e.g.,-40.23 nats training likelihood on Miniboone). This diverges from neural-net-based flows, where overfitting necessitates additional layers or nodes within each layer (bearing in mind that this results in an increase in parameters unlike our method). Such findings vividly demonstrate the high expressiveness of kernelisation in flow-based models.\nComposite kernels In our experiments, we mostly employed the Squared Exponential Kernel/RBF, Matern kernels. Nevertheless, it is feasible to employ a combination of kernels, also known as multiple kernel learning (Sonnenburg et al., 2006). We defer the comprehensive analysis of kernel composition and its application to future endeavours."
        },
        {
            "heading": "I DETAILS OF THE DATASETS",
            "text": "In the following paragraphs, a brief description of the five datasets used in Table 2 (POWER, GAS, HEPMASS, MINIBOONE, BSDS300) and their preprocessing methods is provided.\nPOWER: The POWER dataset comprises measurements of electric power consumption in a household spanning 47 months. Although it is essentially a time series, each example is treated as an independent and identically distributed (i.i.d.) sample from the marginal distribution. The time component was converted into an integer representing the number of minutes in a day, followed by the addition of uniform random noise. The date information is omitted and the global reactive power\nparameter, as it had numerous zero values that could potentially introduce large spikes in the learned distribution. Uniform random noise was also added to each feature within the interval [0, \u03f5i], where \u03f5i is chosen to ensure that there are likely no duplicate values for the i-th feature while maintaining the integrity of the data values.\nGAS: The GAS dataset records readings from an array of 16 chemical sensors exposed to gas mixtures over a 12-hour period. Like the POWER dataset, it is essentially a time series but was treated as if each example followed an i.i.d. distribution. The data selected represents a mixture of ethylene and carbon monoxide. After removing strongly correlated attributes, the dataset\u2019s dimensionality was reduced to 8.\nHEPMASS: The HEPMASS dataset characterizes particle collisions in high-energy physics. Half of the data correspond to particle-producing collisions (positive), while the remaining data originate from a background source (negative). In this analysis, we utilized the positive examples from the \"1000\" dataset, where the particle mass is set to 1000. To prevent density spikes and misleading results, five features with frequently recurring values were excluded.\nMINIBOONE: The MINIBOONE dataset is derived from the MiniBooNE experiment at Fermilab. Similar to HEPMASS, it comprises positive examples (electron neutrinos) and negative examples (muon neutrinos). In this case, only the positive examples were employed. Some evident outliers (11) with values consistently set to -1000 across all columns were identified and removed. Additionally, seven other features underwent preprocessing to enhance data quality.\nBSDS300: The dataset was created by selecting random 8x8 monochrome patches from the BSDS300 dataset, which contains natural images. Initially, uniform noise was introduced to dequantize the pixel values, after which they were rescaled to fall within the range [0, 1]. Furthermore, the average pixel value was subtracted from each patch, and the pixel located in the bottom-right corner was omitted."
        }
    ],
    "year": 2023
}