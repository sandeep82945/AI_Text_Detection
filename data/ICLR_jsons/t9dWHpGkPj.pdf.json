{
    "abstractText": "Language models produce a distribution over the next token; can we use this to recover the prompt tokens? We consider the problem of language model inversion and show that next-token probabilities contain a surprising amount of information about the preceding text. Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model\u2019s current distribution output. We consider a variety of model access scenarios, and show how even without predictions for every token in the vocabulary we can recover the probability vector through search. On Llama-2 7b, our inversion method reconstructs prompts with a BLEU of 59 and token-level F1 of 78 and recovers 27% of prompts exactly.1",
    "authors": [],
    "id": "SP:e0394753be538ecbf4fe9957ea82ce2a75c91351",
    "references": [
        {
            "authors": [
                "Florian Bordes",
                "Randall Balestriero",
                "Pascal Vincent"
            ],
            "title": "High fidelity visualization of what your self-supervised representation knows about",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Thomas Brox"
            ],
            "title": "Inverting visual representations with convolutional networks, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Haonan Duan",
                "Adam Dziedzic",
                "Mohammad Yaghini",
                "Nicolas Papernot",
                "Franziska Boenisch"
            ],
            "title": "On the privacy risk of in-context learning",
            "venue": "In ACL 2023 Workshop on Trustworthy Natural Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Vincent Dumoulin",
                "Ethan Perez",
                "Nathan Schucher",
                "Florian Strub",
                "Harm de Vries",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Feature-wise transformations. Distill, 2018",
            "venue": "doi: 10.23915/distill.00011",
            "year": 2018
        },
        {
            "authors": [
                "Adam Dziedzic",
                "Franziska Boenisch",
                "Mingjian Jiang",
                "Haonan Duan",
                "Nicolas Papernot"
            ],
            "title": "Sentence embedding encoders are easy to steal but hard to defend",
            "venue": "ICLR",
            "year": 2023
        },
        {
            "authors": [
                "Matt Fredrikson",
                "Somesh Jha",
                "Thomas Ristenpart"
            ],
            "title": "Model inversion attacks that exploit confidence information and basic countermeasures",
            "venue": "In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS",
            "year": 2015
        },
        {
            "authors": [
                "Matthew Fredrikson",
                "Eric Lantz",
                "Somesh Jha",
                "Simon Lin",
                "David Page",
                "Thomas Ristenpart"
            ],
            "title": "Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing",
            "venue": "In Proceedings of the USENIX Security Symposium,",
            "year": 2014
        },
        {
            "authors": [
                "Arnav Gudibande",
                "Eric Wallace",
                "Charlie Snell",
                "Xinyang Geng",
                "Hao Liu",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song"
            ],
            "title": "The false promise of imitating proprietary llms, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "year": 2020
        },
        {
            "authors": [
                "Or Honovich",
                "Thomas Scialom",
                "Omer Levy",
                "Timo Schick"
            ],
            "title": "Unnatural instructions: Tuning language models with (almost) no human labor",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Gaurav Singh Tomar",
                "Ankur P. Parikh",
                "Nicolas Papernot",
                "Mohit Iyyer"
            ],
            "title": "Thieves on sesame street! model extraction of bert-based apis, 2020",
            "year": 2020
        },
        {
            "authors": [
                "R. Lebret",
                "D. Grangier",
                "M. Auli"
            ],
            "title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2016
        },
        {
            "authors": [
                "Haoran Li",
                "Mingshi Xu",
                "Yangqiu Song"
            ],
            "title": "Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Lowd",
                "Christopher Meek"
            ],
            "title": "Adversarial learning",
            "venue": "In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining,",
            "year": 2005
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang"
            ],
            "title": "Wizardcoder: Empowering code large language models with evol-instruct",
            "venue": "arXiv preprint arXiv:2306.08568,",
            "year": 2023
        },
        {
            "authors": [
                "Aravindh Mahendran",
                "Andrea Vedaldi"
            ],
            "title": "Understanding deep image representations by inverting them",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher"
            ],
            "title": "Pointer sentinel mixture models, 2016",
            "year": 2016
        },
        {
            "authors": [
                "John X. Morris",
                "Volodymyr Kuleshov",
                "Vitaly Shmatikov",
                "Alexander M. Rush"
            ],
            "title": "Text embeddings reveal (almost) as much as text, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Arvind Neelakantan",
                "Tao Xu",
                "Raul Puri",
                "Alec Radford",
                "Jesse Michael Han",
                "Jerry Tworek",
                "Qiming Yuan",
                "Nikolas Tezak",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Johannes Heidecke",
                "Pranav Shyam",
                "Boris Power",
                "Tyna Eloundou Nekoul",
                "Girish Sastry",
                "Gretchen Krueger",
                "David Schnurr",
                "Felipe Petroski Such",
                "Kenny Hsu",
                "Madeleine Thompson",
                "Tabarak Khan",
                "Toki Sherbakov",
                "Joanne Jang",
                "Peter Welinder",
                "Lilian Weng"
            ],
            "title": "Text and code embeddings by contrastive pre-training, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Soham Pal",
                "Yash Gupta",
                "Aditya Shukla",
                "Aditya Kanade",
                "Shirish K. Shevade",
                "Vinod Ganapathy"
            ],
            "title": "A framework for the extraction of deep neural networks by leveraging public data",
            "venue": "ArXiv, abs/1905.09165,",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Reza Shokri",
                "Marco Stronati",
                "Congzheng Song",
                "Vitaly Shmatikov"
            ],
            "title": "Membership inference attacks against machine learning models, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Congzheng Song",
                "Ananth Raghunathan"
            ],
            "title": "Information leakage in embedding models, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Hideaki Takahashi",
                "Jingjing Liu",
                "Yang Liu"
            ],
            "title": "Breaching fedmd: Image recovery via paired-logits inversion attack, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https://github.com/tatsu-lab/stanford_alpaca,",
            "year": 2023
        },
        {
            "authors": [
                "Piotr Teterwak",
                "Chiyuan Zhang",
                "Dilip Krishnan",
                "Michael C. Mozer"
            ],
            "title": "Understanding invariance via feedforward inversion of discriminatively trained classifiers, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aurelien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Florian Tram\u00e8r",
                "Fan Zhang",
                "Ari Juels",
                "Michael K. Reiter",
                "Thomas Ristenpart"
            ],
            "title": "Stealing machine learning models via prediction apis, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Eric Wallace",
                "Mitchell Stern",
                "Dawn Song"
            ],
            "title": "Imitation attacks and defenses for black-box machine translation systems, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Karia",
                "Savan Doshi",
                "Shailaja Keyur Sampat",
                "Siddhartha Mishra",
                "Sujan Reddy A",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen"
            ],
            "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2023
        },
        {
            "authors": [
                "Minghao Wu",
                "Abdul Waheed",
                "Chiyu Zhang",
                "Muhammad Abdul-Mageed",
                "Alham Fikri Aji"
            ],
            "title": "Lamini-lm: A diverse herd of distilled models from large-scale instructions",
            "venue": "CoRR, abs/2304.14402,",
            "year": 2023
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "arXiv preprint arXiv:2304.12244,",
            "year": 2023
        },
        {
            "authors": [
                "Ann Yuan",
                "Daphne Ippolito",
                "Vitaly Nikolaev",
                "Chris Callison-Burch",
                "Andy Coenen",
                "Sebastian Gehrmann"
            ],
            "title": "Synthbio: A case study in human-ai collaborative curation of text datasets, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ruisi Zhang",
                "Seira Hidano",
                "Farinaz Koushanfar"
            ],
            "title": "Text revealer: Private text reconstruction via model inversion attacks against transformers, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yuheng Zhang",
                "Ruoxi Jia",
                "Hengzhi Pei",
                "Wenxiao Wang",
                "Bo Li",
                "Dawn Song"
            ],
            "title": "The secret revealer: Generative model-inversion attacks against deep neural networks, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Morris"
            ],
            "title": "This model performs quite poorly, indicating that projecting the probability vector down to a smaller rank discards significant information. We also test an identical parameterization that conditions on the raw outputs of the language model instead of log-normalized probabilities to determine if un-normalized outputs contain more usable information",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Language models produce a distribution over the next token; can we use this to recover the prompt tokens? We consider the problem of language model inversion and show that next-token probabilities contain a surprising amount of information about the preceding text. Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model\u2019s current distribution output. We consider a variety of model access scenarios, and show how even without predictions for every token in the vocabulary we can recover the probability vector through search. On Llama-2 7b, our inversion method reconstructs prompts with a BLEU of 59 and token-level F1 of 78 and recovers 27% of prompts exactly.1"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Language models are autoregressive, outputting the probability of each next token in a sequence conditioned on the preceeding text. This distribution is used to generate future tokens in the sequence. Can this distribution also be used to reconstruct the prompt?\nIn most contexts, this question is pointless, since we have already conditioned on this information. However, increasingly language models are being offered \u201cas a service\u201d where the user may have access to the outputs, but not all of the true prompt. In this context, it may be of interest to users to know the prompt and, perhaps, for the service provider to protect it. This goal has been the focus of \u201cjailbreaking\u201d approaches that attempt to use the forward text generation of the model to reveal the prompt.\nWe formalize this problem of prompt reconstruction as language model inversion, recovering the input prompt conditioned on the language model\u2019s next-token probabilities. Interestingly, work in computer vision has shown that probability predictions of image classifiers retain a surprising amount of detail (Dosovitskiy & Brox, 2016), so it is plausible that this also holds for language models. We propose an architecture that predicts prompts by\u201cunrolling\u201d the distribution vector into a sequence that can be processed effectively by a pretrained encoder-decoder language model. This method shows for the first time that language model predictions are mostly invertible: in many cases, we are able to recover very similar inputs to the original, sometimes getting the input text back exactly.\nWe additionally explore the feasibility of prompt recovery across a spectrum of real-world access patterns: full next-token probability outputs, top-K probabilities, probabilities per token upon request, and discrete sampling. We find that even in the case where we are only able to observe text output from the model (no probabilities), we can recover enough of the probability distribution to reconstruct the prompt.\nOur results show that systems that offer text-only access to a language model reveal information about their prompts. With enough queries, we can extract next-token probabilities at a given position, which can be used to reconstruct the input. Unlike text-based jailbreaks, our dense distribution inversion is less inhibited by post-pretraining reinforcement learning techniques such as RLHF to align the model. We also show that our inversion techniques transfer between models of the same family, and are not affected by language model scaling.\n1Code for reproducing all experiments is available at anonymous.4open.science/status/language-modelinversion. Our dataset of prompts will be provided upon paper publication."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Inverting deep embeddings. Several lines of work in computer vision have shown that inputs can be approximately reconstructed from the logits of an image classifier (Mahendran & Vedaldi, 2014; Dosovitskiy & Brox, 2016; Teterwak et al., 2021) or from a self-supervised representation vector (Bordes et al., 2021). Some recent work (Takahashi et al., 2023) has shown that outputs of computer vision models may reveal private information when shared in a federated learning setup. There is also work on inverting representations in NLP: Song & Raghunathan (2020); Li et al. (2023); Morris et al. (2023) investigate the privacy leakage of text embeddings from encoder models. Morris et al. (2023) succeeds in recovering full text sequences from their embeddings by conditioning the encoder of an encoder-decoder Transformer for inversion. Ours is the first work to inversion directly from the probability outputs of language models.\nModel inversion and membership inference. Given an output of a model, model inversion aims to construct an input that produces that output. This problem was investigated for simple regression classifiers in (Fredrikson et al., 2014; 2015) and extended to neural face-recognition classifiers in (Zhang et al., 2020). In some cases, model inversion can help recover training data. For example, in face-recognition classifiers each class corresponds to a single person, thus any image recovered by model inversion is visually similar to the training images for the same class label. (Zhang et al., 2022) used model inversion techniques to recover memorized training data from pretrained language models. A related problem is membership inference: given a data point, determine whether it was part of the model\u2019s training data or not (Shokri et al., 2017). Duan et al. (2023) demonstrated membership inference for in-context learning.\nPrompt inversion is a form of model inversion, but we work with significantly more complex language models, where dimensionality of inputs is much higher than in the classifiers studied in prior model-inversion work. Instead of recovering information about the training data, we aim to recover the specific prompt given to the model and filter out information related to the training data.\nModel stealing. As language models become more and more valuable, they are hidden behind increasingly stringent safeguards. Research into \u2018model stealing\u2019 aims to explore how models themselves can be replicated via API queries (Trame\u0300r et al., 2016). Stealing NLP models has been demonstrated in many domains: linear text-based classification (Lowd & Meek, 2005), language classification (Pal et al., 2019; Krishna et al., 2020), machine translation Wallace et al. (2021), and even text retreival (Dziedzic et al., 2023). Recent work Gudibande et al. (2023) suggests that this form of imitation may create models that replicate surface-level syntax but do not learn more complex attributes like knowledge or factuality. Different than these works, we do not focus on reconstructing model weights from third-party model outputs, but finding a hidden prompt from outputs of a third-party model."
        },
        {
            "heading": "3 PROMPT RECONSTRUCTION",
            "text": "Language models give the probability of the next token conditioned on the tokens that came before it, i.e. v = p(xT+1 | x1, ..., xT ; \u03b8), where v \u2208 \u2206|V|\u22121 gives the probability of each of |V| possible next tokens. Generally these models have relatively large vocabulary sizes; the vocabulary V may contain tens or hundreds of thousands of elements."
        },
        {
            "heading": "3.1 LOGITS CONTAIN RESIDUAL INFORMATION",
            "text": "We construct a simple experiment to demonstrate the amount of information LM logits may convey about the input. Given 100 text inputs from Wikipedia, we substitute a single word in the first sentence with a synonym2. Let x\u0302s be the synonym of word xs. To measure the change in language model output between the original sequence (containing xs) and the new sequence (containing x\u0302s), we compute two quantities: the KL divergence between the probability output of p for the original and synonym-swapped sequences, and the bit-level Hamming Distance between the two distributions when represented at 16-bit precision.\nWe plot KL and bitwise Hamming Distance relative to the position of the synonym swap in Figure 2. If LMs did not contain residual information about previous words, we would expect bitwise distance to decay to zero as we move away from the position of the swap. However, we observe that bits remain: although the vector v is only used to predict the next token, it clearly contains residual information about the prompt tokens x1, ..., xT . Since KL puts most of its weight on the highest-likelihood tokens, it can decay to zero, while much of the information remains in the low-probability tokens."
        },
        {
            "heading": "3.2 PROMPT RECONSTRUCTION",
            "text": "We now consider the problem of inverting the process: given the probability vector, we attempt to produce the prompt that led to these next-token probabilities. Given some unseen model f : VT \u2192 \u2206|V|\u22121 which gives next-token probabilities, we would like to learn to invert this function from samples: pairs of text prefixes and their associated next-token probability vectors (x11:T ,v 1) . . . (xJ1:T ,v J).\nInverting from outputs? Besides inverting from the probability vector, natural procedure to consider is predicting the prompt directly from the output of the language model. For example, given the model output \u201cBogota\u201d, we may predict the input \u201cWhat is the capital of Columbia?\u201d. We hypothesize that a single logit vector contains much more detailed information about the prompt then a single sequence sampled from the argmax of these vectors. However, we consider this scenario in our Sample inverter baseline described in Section 6.\nPrompt datasets. We construct Instructions-2M, a meta-dataset consisting of 2.33M instructions including user and system prompts for a wide variety of different problems. This includes prompts from Supernatural Instructions (Wang et al., 2022), Self Instruct (Wang et al., 2023), Alpaca (Taori et al., 2023), Dolly3, ShareGPT4, Unnatural Instructions (Honovich et al., 2023), ChatBot Arena 5, Stable Diffusion Dataset6, WizardLM instructions (Xu et al., 2023; Luo et al., 2023), GPTeacher7,\n2We perform synonym swaps using GPT-4. More information on this experiment is given in Appendix D 3https://huggingface.co/datasets/databricks/databricks-dolly-15k 4https://huggingface.co/datasets/anon8231489123/ShareGPT Vicuna unfiltered 5https://huggingface.co/datasets/lmsys/chatbot arena conversations 6https://huggingface.co/datasets/MadVoyager/stable diffusion instructional dataset 7https://github.com/teknium1/GPTeacher\nT0 (Chung et al., 2022), and LaMini instruction (Wu et al., 2023). In addition we collect out-ofdomain prompts to test the ability of the model to generalize in topical area.\nAssumptions of our threat model. We motivate this problem by the prevalence of language models as a service. In these use cases we assume that an unknown prefix sequence, known as the prompt, is prepended to the user input. We consider varying levels of model access: full distributional access, partial distributional access (top-K or by request), text output with user-defined logit bias, and text output access only. We assume no access to the model weights or activations."
        },
        {
            "heading": "4 METHOD: LEARNING TO INVERT PROBABILITIES",
            "text": "Our proposed approach is to learn a conditional language model that maps from next-token probabilities back to tokens: p(x1:T | v). We parameterize this distribution using a pretrained Transformer language model and train on samples from the unconditional model. Following work from Dumoulin et al. (2018) on feature-level conditioning, we use the cross-attention in an encoder-decoder Transformer to condition on the next-token vector.\nSince our encoder model is pretrained on text (we utilize T5), we must reformat v to be fed to a language encoder. The simplest method is to project v to Rd and feed it as an input hidden vector. However, given the large size of the vocabulary |V| and the fact that it has been passed through a softmax, this would cause a large reduction in rank and a loss of information8. We instead \u2018unroll\u2019 the vector into a sequence of pseudo-embeddings ci \u2208 Rd, so that we can condition transformer outputs on the full probability vector v,\nci = MLPi(log(vd(i\u22121):di)) \u2200 i \u2208 {1 . . . \u2308|V|/d\u2309} x\u2217 = argmax\nx Dec(x,Enc(c))\nWhere x\u2217 is the predicted inversion, d is the embedding dimension, and v is padded with zeros at the final position. In practice we use |V| = 32000 and d = 768 for all experiments which leads to a fixed-length input sequence of 42 words."
        },
        {
            "heading": "5 EXTRACTING LOGITS VIA API",
            "text": "The previous sections assume we have access to the full language model output probability vector. However, many language model platforms limit the information returned from API calls. For example, a well-known service\u2019s API only exposes either samples or the top-5 log probabilities, but does not expose all output probabilities for a given input.\nWe therefore propose a method for extracting next-token probabilities from APIs where the full probability distribution is not immediately available. We take advantage of the fact that even when API services do not return the full probabilities, they typically allow users to add a logit bias to adjust the distribution. In addition to providing a logit bias per API call, they typically allow setting the temperature parameter to zero to provide argmax of the distribution.\nThe probability of each token can therefore be recovered by finding its difference with the most likely word. We compute this difference by finding the smallest logit bias to make that word most likely. Algorithm 1 shows the approach, which relies on binary search to find the logit bias for each word.\nNote that binary search can be conducted independently for each word, enabling full parallelization and requiring only a single logit bias at a time. By running this procedure for each word in the vocabulary, we can then reconstruct the full distribution v = softmax(logits). The necessary number of queries to determine the distribution is |V| times the number of bits required to represent the desired precision.9\n8We explore variants of this projection through ablation experiments in Appendix G.1. 9We also provide an algorithm for logit extraction with an API that returns the top 2 log probabilities in\nAppendix F.\nAlgorithm 1 Logit Extraction via Binary Search for each word i procedure API ARGMAX(i, b)\nreturn argmax[logv0, . . . , logvi + b, . . .] \u25b7 Argmax of hidden v with bias b added to i procedure FINDLOGIT(i)\nU \u2190 \u03f5 \u25b7 Initialize upper bound to \u03f5 > 0 while API ARGMAX(i, U ) \u0338= i do \u25b7 Exponentiate to find upper bound\nU \u2190 2U L\u2190 0 \u25b7 Lower bound M \u2190 (L+ U)/2 while U \u2212 L > \u03b4 do \u25b7 Perform binary search to precision \u03b4\nif API ARGMAX(i,M) = i then \u25b7 Call API with i upweighted by M to obtain argmax U \u2190M else L\u2190M\nM \u2190 (L+ U)/2 return \u2212M\nprocedure EXTRACTLOGITS() logits[i]\u2190 FINDLOGIT(i) for each word i in vocab"
        },
        {
            "heading": "6 EXPERIMENTAL SETUP",
            "text": "Models. We train models to invert the distribution of Llama-2 (7B) and Llama-2 Chat (7B) (Touvron et al., 2023). We choose this model because, as of the time of publication, it is the bestperforming open-source LLM at the 7B parameter scale. We assume full access to the model output probabilities except for during the distribution extraction experiments in Section 7, in which we set temperature to 0 and provide a single logit bias argument.\nWe parameterize the inversion model using the method described in Section 4 and select T5-base (Raffel et al., 2020) as our encoder-decoder backbone, which has 222M parameters. We set the maximum sequence length to 64 for all experiments. We train models for 100 epochs with Adam optimizer with a learning rate of 2e \u2212 4. We use a constant learning rate with linear warmup over the first 25, 000 training steps. We train in bfloat16 precision.\nMetrics. We consider several metrics for prompt reconstruction: F1 score at the token level, BLEU score (Papineni et al., 2002) as a measure of string overlap, and exact match. We also consider the cosine similarity between the text embeddings of the original and recovered text as a measure of semantic relatedness. For cosine similarity, we use embeddings from the model text-embeddings-ada-002 available through the OpenAI API (Neelakantan et al., 2022). For each metric, we report error bounds as standard error of the mean (SEM).\nWe randomly hold out 1% of the training data for testing. We additionally evaluate on two datasets of human-written prompts: code prompts from Alpaca (Taori et al., 2023), and prompts extracted from the helpfulness and harmlessness data collected in Bai et al. (2022). Both datasets are drawn from different, more narrow distributions than our all-encompassing training dataset.\nBaselines. As we are the first work to consider inverting text directly from language model probabilities, there are no prior baselines to compare to. We therefore develop several baselines:\n\u2022 Jailbreak strings. Human-written sequences that attempt to persuade the language model to divulge information from earlier in the sequence. We aggregate jailbreak strings from a variety of sources, including writing some manually. We only show the top jailbreak string in the tables, and include more results in the appendix. We source 20 jailbreak strings and test them on all models. For pre-trained Llama models, jailbreak strings are simply appended to the prompt. For chat models, we input the hidden prompt as a system instruction, along with a basic system instruction that instructs the model not to divulge its hidden prompt. We then input the jailbreak string as a user prompt. When reporting results, we report the mean performance of all prompts as well as an oracle figure indicating the best-performing jailbreak string on the test dataset selected after evaluation.\n\u2022 GPT-4 Few-shot. We prompt GPT-4 with examples of the top-K tokens by probability from Llama2 input predictions. These example input-output pairs are prepended to the top probabilities for the hidden input.\n\u2022 Sample inverter. Instead of inverting from next-token probability, we consider whether we might predict prompts from samples of the text outputs from the LM. To train this model, we sample outputs from Llama-2 7b chat and train a T5-base encoder-decoder to predict the input prompt from a given language model output."
        },
        {
            "heading": "7 RESULTS",
            "text": "Table 1 shows the main results of the experiments on reversing prompts from the Instructions-2M test set on both a raw LLM and RLHF Chat variant. We find that our method is able to achieve high BLEU score with the true prompts and achieve reasonable high-exact match reproduction. This approach is significantly better than few-shot prompting approaches, even when using GPT-4. The other trained approach (Sample Inverter) has a reasonable BLEU but 0 exact recoveries. The failure of sample inversion indicates that we are able to extract more usable information about the prompt from the logit vector than from the argmax outputs alone.\nCompared to manually written jailbreak strings, our approach is significantly better than the average value, comparable with the oracle jailbreak method. Notably, while the best jailbreak method works well on the raw LM, none of the jailbreak approaches work on the RLHF chat version. We do observe that our method works slightly better on the non-chat model (59 vs. 52 mean BLEU), indi-\ncating that the RLHF procedure used to train the chat model may reduce the amount of information from what was initially available in the next-token probabilities.\nOut-of-domain. Table 2 shows the results when using prompts that are significantly different than the training distribution both in length and in content. For these domains, the model is significantly better than few-shot and jailbreaking on the RLHF model. With the chat model, we also observe that the jailbreak strings are especially ineffective on the Anthropic HH dataset, which contains a large amount of toxic content; this indicates that the chat model is less likely to obey the user\u2019s request when toxic content is present in the prompt. For the raw model, the inversion approach is a bit worse than jailbreaking on BLEU.\nAPI-Based Logits Extraction. Here we examine our ability to recover the next-token probability distribution. We simulate an API with a \u2018logit bias\u2019 argument and argmax outputs using a local LLAMA-2 model. We visualize results of our technique (blue) vs. a naive Monte Carlo sample baseline (red) in Figure 4 (left). Our deterministic algorithm extracts useful logits in fewer queries than the Monte Carlo baseline. This result follows our hypothesis (Section 3.1) that useful information is contained in the probabilities of very unlikely words, which almost never occur during random sampling.\nTransferability. We next investigate whether inversions transfer to models of different size by evaluating our inversion models trained on the 7B version of Llama-2 on the 13B version and 70B version. Results are shown in Table 3. We observe that inversions transfer reasonably well, performing best on code generation, and significantly better between non-RLHF\u2019d models than between the chat models. We speculate that models may need to be further fine-tuned to adapt to different models.\nInversion and Language Model Scale. To understand how dependent inversion results are to language model size, we consider inverting different sizes of GPT-2 (Radford et al., 2019) and show results in Table 9 (Left). Interestingly, the reconstructions perform very similarly (within 1 point of BLEU score) regardless of the size of the language model inverted. The fact that output probabilities contain similar amounts of information even after going through vastly different amounts of processing (dependent on the varying model size) differ from the findings of Dosovitskiy & Brox (2016) who note that more layers of computation make inversion more difficult in CNNs."
        },
        {
            "heading": "7.1 DEFENDING AGAINST PROMPT INVERSION",
            "text": "Language model providers may be interested in defending prompts from inversion. One simple defense is to add noise to the language model output distribution; instead of providing a deterministic (argmax) output, from which an attacker could trivially reconstruct the output probabilities, language model providers, could instead sample from the output distribution.\nWe consider three different LM sampling mechanisms as defenses against prompt inversion: adjusting the softmax temperature during sampling, adjusting the top-p parameter of nucleus sampling (Holtzman et al., 2020), and adjusting the total number of tokens considered (top-K). We sample from Llama-2 7b (non-chat) and feed probabilities into our inverter model according to the desired strategy. Results are visualized in Figure 3.\nIn each case, we observe a trade-off between language model fidelity and inversion performance. Interestingly, inversion performs best at temperature value \u03c4 = 1, and suffers when temperature\ndecreases, as the LM distribution anneals to argmax, as well as when temperature increases, as the distribution collapses to uniform. For both top-p and top-k we note that the model requires almost all of the distribution to perform well, indicating that there is significant information in the tails."
        },
        {
            "heading": "7.2 ANALYSIS",
            "text": "Qualitative examples. We showcase some randomly-selected qualitative examples from Instructions-2M in Table 4. Our inverted prompts are generally on topic and syntactically similar to the originals. Two prompts are perfectly reconstructed. We notice that proper nouns seem difficult; in one example, our model correctly recovers the structure of a question, but mixes up Steinbeck\u2019s Of Mice and Men with Salinger\u2019s The Catcher in the Rye10. (One might assume that this information is represented in the probability distribution, but interestingly the raw probabilities do not favor either title.) In all examples, the system correctly identifies whether or not the prompt ends in punctuation.\n10Of course, T5 knows Wikipedia well, and correctly states the year the incorrectly predicted book was published.\nWhich components of the distribution does the inverter need? To investigate whether our model focuses most on the largest components of its input, we iteratively remove (set to the mean) k components from the probability vector in both ascending and descending order. We also consider removing all but a random subset of k components from the input. Figure 4 highlights the difference in reconstruction performance across levels of component removal. It appears that the model focuses more on the more likely words. In particular, the smallest k probabilities are only slightly more useful than a random k. Reconstruction is poor in general until almost the entire distribution is re-included."
        },
        {
            "heading": "8 CONCLUSION & FUTURE WORK",
            "text": "We define the problem of inversion from language model outputs and analyze inversion approaches from an attack and defense perspective. We show that this attack vector can be used to elicit hidden prompts from LM systems, even when we do not have direct access to model output distributions.\nWhat are the limits of inversion? Our experiments show that much information about the input can be recovered from language model probabilities, but do not estimate the upper bound. The scaling analysis in Appendix G.1 implies that larger backbone models recover more information, but we do not run any experiments with backbone models larger than hundred-million parameter scale.\nHow can we keep our prompts safe? Our experiments show that when sampling is enabled, we can reconstruct model probability distributions given enough queries to the model. The only foolproof way to protect prompts while providing users access to generate text from a language model is to disable top-logits access (output only text) and set temperature to 0.\nSmarter parameterizations for inversion. Future work might consider exploiting the fact that inputting a single suffix into a LM outputs multiple next-token predictions, one at each position, not just at the end. Additional research may find that utilizing a parameterization that integrates token embeddings with probability values, so that the inversion model \u2018knows\u2019 which value corresponds which word, could be useful."
        },
        {
            "heading": "9 ETHICS",
            "text": "Our research on the inversion of language models underscores the ethical implications surrounding the deployment of such models as services, particularly when providers maintain prompts that are valuable or contain personal information. Users of language models may be affected as they rely on these services for various purposes, including content generation and information retrieval. Prompt secrecy can compromise users\u2019 trust in the systems they interact with, raising concerns about the transparency and integrity of the services themselves.\nLack of access to the underlying prompts hinders efforts to scrutinize, evaluate, and regulate language models effectively, thereby impeding advancements in the responsible and ethical development of AI technologies. Our research advances the field towards wider access to prompts while highlighting important privacy concerns for language-models-as-a-service providers."
        },
        {
            "heading": "10 REPRODUCIBILITY",
            "text": "Code for reproducing all experiments is available at anonymous.4open.science/status/languagemodel-inversion. Our dataset of prompts will be provided upon paper publication. All experiments are fully reproducible and documented in the Github repository, including all model-training, logit sampling, and evaluation."
        },
        {
            "heading": "A BASELINE: JAILBREAK PROMPTS",
            "text": "Jailbreak prompts were manually written by a panel of NLP experts. Prompts were tested with and without newlines prepended and with and without taking the first line of output. Table 5 contains a list of all jailbreak prompts tested in order of descending effectiveness. A plot of performance by model and dataset is available in Figure 5."
        },
        {
            "heading": "B BASELINE: FEW-SHOT",
            "text": "Table 7 shows a sample prompt for few-shot prompting an LLM to do inversion from LM probabilities. We choose a few-shot strategy. To format the input for an LLM, we take the top-100 predicted probabilities and subtract unigram probabilities to remove most common words. We show this input\nto the model along with a sample output. Because this takes many tokens, we only show a total of 3 examples per prompt before providing the log-probabilities for the true sample."
        },
        {
            "heading": "C ADDITIONAL ANALYSIS",
            "text": "Does our model accurately predict length? We plot the length of prompts vs their reconstructions by our model across datasets in Figure 6. Our model fits length of prompts in the training distribution (Instructions-2M) well, but struggles on the out-of-distribution datasets, tending to produce reconstructions with far too many tokens. On the Anthropic HH dataset, our method produces 39.5 tokens on average, while the true prompts have an average of 17.9 tokens."
        },
        {
            "heading": "D SYNONYM SWAP EXPERIMENTAL DETAILS",
            "text": "To perform the experiment illustrated in Figure 2, we sample 100 paragraphs from Wikipedia obtained via the Wikitext dataset (Merity et al., 2016). We prompt GPT-4 with the first ten words of each paragraph prepended by the text \u201cPlease update the sentence by replacing one word sentence with a close synonym. Respond with only the word to swap in the format word1 -\u00bf word2.\u201d. We then extract the word swap from GPT-4\u2019s response and apply it to the input to produce the transformed input x\u0302. The language model used for prompting is the 7-billion parameter version of LLAMA-2 (non-chat version).\nTo measure the change in language model output between the original sequence (containing xs) and the new sequence (containing x\u0302s), we compute two quantities:\nKL(x, x\u0302;T ) := DKL [p(xT+1 | x1, ..., xs, ..., xT ; \u03b8) || p(xT+1 | x1, ..., x\u0302s, ...xT ; \u03b8)]\nthat is, the KL divergence between the probability output of p for the original and synonym-swapped sequences, and\nHamming(x, x\u0302;T ) := \u2211 i |bin16(p(xT+1 | x1, ..., xs, ..., xT ; \u03b8))i\n\u2212 bin16(p(xT+1 | x1, ..., x\u0302s, ..., xT ; \u03b8))i|"
        },
        {
            "heading": "E DATASETS",
            "text": "Table 8 displays a breakdown of prompt datasets included in the Instructions-2M dataset. Prompts are the concatenation of the user prompt and an optional system prompt. T0 prompts are the longest, with an average of 32.4 words. Lamini prompts make up the majority of the training data, with a total of 1.8M included."
        },
        {
            "heading": "F LOGIT EXTRACTION WITH API ACCESS TO PROBABILITIES",
            "text": "In this section we offer another approach to extracting log probabilities when the API offers access to the probabilities of the top 2 most likely words. At a high level, to find the probability of a word in the original distribution, we first find a logit bias that makes that word most likely. We then use the change in probability of the most likely word to compute the normalizing constant of the original distribution, and use that to find the probability of the word of interest.\nFormally, we can extract the log probability of word log p(v) = f(v)\u2212 logZ as follows: First, find a logit bias bv that makes word v more probable than highest probability word v\u2217. Use the logit bias\nbv and change in probability \u2206 = log p(v\u2217) \u2212 log p(v\u2217; bv) of the highest probability word after adding the logit bias to word v \u0338= v\u2217 to solve for the normalizing constant:\n\u2206 = (log f(v\u2217)\u2212 logZ)\u2212 (log f(v\u2217)\u2212 log(Z + exp(bv))) = log(Z + exp(bv))\u2212 logZ\nexp(\u2206) = Z + exp(bv)\nZ\n= 1 + exp(bv)\nZ\nZ = exp(bv)\nexp(\u2206)\u2212 1 logZ = bv \u2212 log(exp(\u2206)\u2212 1)\nWith this, we can solve for the unnormalized log probability of the word f(v):\nlog p(v; bv) = f(v) + bv \u2212 log(Z + exp(bv)) f(v) = log p(v; bv) + log(Z + exp(bv))\u2212 bv,\nyielding log p(v) = f(v)\u2212 logZ. This allows us to extract log probabilities for each word with one call to find the probability of the most likely word, and one call for each other word with a large enough logit bias.\nG INITIAL EXPLORATIONS WITH ITERATIVE REFINEMENT\nA natural extension to this approach could be the iterative refinement approach proposed in vec2text (Morris et al., 2023). We parameterize an encoder-decoder that takes three inputs:\n\u2022 the model output probability vector for an unknown prompt (v) in Section 4) \u2022 a \u2018hypothesis\u2019 sequence, x\u03021, ..., x\u0302T \u2022 the model output probability vector p(x\u03021, ..., x\u0302T )\nand train it via language modeling on the true prompt x | p(x) = v. For training, we sample outputs from a checkpoint of our conditional LM (section 4) to use as hypotheses. We train the model on outputs from Llama-2 (7B) for 100 epochs using the same hyperparameters outlined in Section 6. This model is able to achieve a one-step BLEU score of 58.7 on Instructions-2M, essentially recovering the original model\u2019s BLEU performance of 59.2. However, we see no increase in BLEU score after applying multiple steps of correction; after 5 steps, our model achieves a BLEU score of 56.43.\nFigure 7 plots the BLEU scores in the hypotheses used to train the iterative refinement model. We note that these hypotheses do not cover a full spectrum of correctable texts up to a BLEU score of 100; this may make it difficult for the refinement model to learn to correct text at different\n\u2018distances\u2019 from the ground-truth text. Perhaps that iterative refinement also may be more difficult in the space of language model probability outputs than text embeddings, due to a lack of convexity; it is also plausible that a different architecture or set of hyperparameters may be needed to train a more powerful inverter using iterative refinement.\nG.1 ABLATIONS\nWe perform a variety of ablations of our model-training in a reduced setting: 1M training examples from the dataset with a maximum sequence length of 16, training for 40 epochs. Ablation results are shown in Table 9 (Left).\nParameterization. We consider one alternative model architecture, an encoder-decoder with projection as in Morris et al. (2023). This model performs quite poorly, indicating that projecting the probability vector down to a smaller rank discards significant information. We also test an identical parameterization that conditions on the raw outputs of the language model instead of log-normalized probabilities to determine if un-normalized outputs contain more usable information than the probabilities. This removal also makes a difference: without applying the softmax to the inputs, we observe over a 20% drop in BLEU.\nSince the main experiments are conducted in 16-bit precision, we test training in full precision (32- bit) to see if the additional bits improve performance. Training on full precision inputs gains about 1 BLEU point, indicating that we are not discarding significant information by storing probability vectors at half precision.\nScaling inverter. We train inverter models of varying size to see the effect of model scale on inversion performance. We note that the number of parameters in the inverter has a very large impact; with larger inverter models performing significantly better: under ablation settings, with the same number of training steps, the T5-large inverter achieves 24% higher BLEU score than T5small. This finding indicates that we may more accurately invert language models simply by scaling our system.\nReducing input dimensionality. When stored on disk in 32-bit precision, 10 million probability vectors for a vocabulary of size of 32, 000 take up 1.28 TB. Is it necessary to retain the full dimensionality of these input vectors? Surprisingly, Table 9 (Right) indicates that the majority of the probability vector is required to achieve good inversion performance. Even though the top 1000 predicted tokens contain 98% of the probability mass on average, training and evaluating with only the top 1000 tokens reduces performance by 45%."
        },
        {
            "heading": "H PERSONAL INFORMATION RECOVERY EXPERIMENT",
            "text": "We performed a small experiment to measure our system\u2019s performance at recovering entities from prompts. To do this, we created a dataset of prompts that include personal information from Wikibio and Synthbio. We extracted the entities themselves from the tabular portion of the bio datasets and inserted entities into manually-crafted template strings based on Instructions-2M. We release this dataset publicly to aid future research into PII reconstruction.\nWe consider our model\u2019s ability to reconstruct specific entities from prompts that have a high likelihood of containing personal information, such as names, dates, and nationalities. To test this, we generate a synthetic dataset of prompts that contain these attributes. We edit prompts from Instructions-2M with private attributes sourced from Wikibio (Lebret et al., 2016) and Synthbio (Yuan et al., 2022)11. We invert these prompts using both our LLAMA 7B and LLAMA-chat 7B models and measure accuracy at reconstructing private entities across categories.\nResults are displayed in Table 10. Our models are far better at reconstructing some private entities than others: countries and nationalities are particularly successful, while individual dates and years are typically lost during the inversion process. Future work might consider more wide-ranging training data for better performance, since Instructions-2M includes only a narrow distribution of personal information in its prompts."
        },
        {
            "heading": "I PRIVATE PROMPTS DATASET",
            "text": "11More details along with sample prompts are available in Appendix I."
        }
    ],
    "year": 2023
}