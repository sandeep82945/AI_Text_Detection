{
    "abstractText": "Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, and classifier signals. Walker hound, Walker foxhound in space Target Segmentation Map A headshot of a blonde woman as a sketch Target Identity A headshot of a woman with a dog in winter. Target Object Location A Portrait of a woman Target Style Image Figure 1: Diffusion guided by off-the-shelf networks. Top left: segmentation guidance, top-right: face recognition guidance, bottom-left: object detection guidance, bottom-right: style-transfer.",
    "authors": [],
    "id": "SP:99adef1ec236a4eb9b3ac88558ce27e9b9c837c0",
    "references": [
        {
            "authors": [
                "Jacob Austin",
                "Daniel D Johnson",
                "Jonathan Ho",
                "Daniel Tarlow",
                "Rianne van den Berg"
            ],
            "title": "Structured denoising diffusion models in discrete state-spaces",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Arpit Bansal",
                "Eitan Borgnia",
                "Hong-Min Chu",
                "Jie S Li",
                "Hamid Kazemi",
                "Furong Huang",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Cold diffusion: Inverting arbitrary image transforms without noise",
            "venue": "arXiv preprint arXiv:2208.09392,",
            "year": 2022
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Jeongsol Kim",
                "Michael T Mccann",
                "Marc L Klasky",
                "Jong Chul Ye"
            ],
            "title": "Diffusion posterior sampling for general noisy inverse problems",
            "venue": "arXiv preprint arXiv:2209.14687,",
            "year": 2022
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Byeongsu Sim",
                "Dohoon Ryu",
                "Jong Chul Ye"
            ],
            "title": "Improving diffusion models for inverse problems using manifold constraints",
            "venue": "arXiv preprint arXiv:2206.00941,",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Quinn Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "year": 2021
        },
        {
            "authors": [
                "Alexandros Graikos",
                "Nikolay Malkin",
                "Nebojsa Jojic",
                "Dimitris Samaras"
            ],
            "title": "Diffusion models as plug-and-play priors",
            "venue": "arXiv preprint arXiv:2206.09012,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Howard",
                "Mark Sandler",
                "Grace Chu",
                "Liang-Chieh Chen",
                "Bo Chen",
                "Mingxing Tan",
                "Weijun Wang",
                "Yukun Zhu",
                "Ruoming Pang",
                "Vijay Vasudevan"
            ],
            "title": "Searching for mobilenetv3",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Rongjie Huang",
                "Max WY Lam",
                "Jun Wang",
                "Dan Su",
                "Dong Yu",
                "Yi Ren",
                "Zhou Zhao"
            ],
            "title": "Fastdiff: A fast conditional diffusion model for high-quality speech synthesis",
            "venue": "arXiv preprint arXiv:2204.09934,",
            "year": 2022
        },
        {
            "authors": [
                "Bahjat Kawar",
                "Michael Elad",
                "Stefano Ermon",
                "Jiaming Song"
            ],
            "title": "Denoising diffusion restoration models",
            "venue": "arXiv preprint arXiv:2201.11793,",
            "year": 2022
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "arXiv preprint arXiv:2009.09761,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "Diffusion-lm improves controllable text generation",
            "venue": "arXiv preprint arXiv:2205.14217,",
            "year": 2022
        },
        {
            "authors": [
                "Yanghao Li",
                "Saining Xie",
                "Xinlei Chen",
                "Piotr Dollar",
                "Kaiming He",
                "Ross Girshick"
            ],
            "title": "Benchmarking detection transfer learning with vision transformers",
            "venue": "arXiv preprint arXiv:2111.11429,",
            "year": 2021
        },
        {
            "authors": [
                "Yuheng Li",
                "Haotian Liu",
                "Qingyang Wu",
                "Fangzhou Mu",
                "Jianwei Yang",
                "Jianfeng Gao",
                "Chunyuan Li",
                "Yong Jae Lee"
            ],
            "title": "Gligen: Open-set grounded text-to-image generation",
            "venue": "arXiv preprint arXiv:2301.07093,",
            "year": 2023
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2112.10741,",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Weilun Wang",
                "Jianmin Bao",
                "Wengang Zhou",
                "Dongdong Chen",
                "Dong Chen",
                "Lu Yuan",
                "Houqiang Li"
            ],
            "title": "Semantic image synthesis via diffusion models",
            "venue": "arXiv preprint arXiv:2207.00050,",
            "year": 2022
        },
        {
            "authors": [
                "Yinhuai Wang",
                "Jiwen Yu",
                "Jian Zhang"
            ],
            "title": "Zero-shot image restoration using denoising diffusion null-space model",
            "venue": "arXiv preprint arXiv:2212.00490,",
            "year": 2022
        },
        {
            "authors": [
                "Jay Whang",
                "Mauricio Delbracio",
                "Hossein Talebi",
                "Chitwan Saharia",
                "Alexandros G Dimakis",
                "Peyman Milanfar"
            ],
            "title": "Deblurring via stochastic refinement",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kaipeng Zhang",
                "Zhanpeng Zhang",
                "Zhifeng Li",
                "Yu Qiao"
            ],
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "venue": "IEEE signal processing letters,",
            "year": 2016
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.05543,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Diffusion models are powerful tools for creating digital art and graphics. Much of their success stems from our ability to carefully control their outputs, customizing results for each user\u2019s individual needs. Most models today are controlled through conditioning. With conditioning, the diffusion model is built from the ground up to accept a particular modality of input from the user, be it descriptive text, segmentation maps, class labels, etc. While conditioning is a powerful tool, it results in models that are handcuffed to a single conditioning modality. If another modality is needed, a new model has to be trained, often from scratch. Unfortunately, the high cost of training makes this prohibitive for most users.\nA more flexible approach to controlling model outputs is to use guidance. In this approach, the diffusion model acts as a generic image generator, and is not required to understand a user\u2019s instructions. The user pairs this model with a guidance function that measures whether some criterion has been met. For example, one could guide the model to minimize the CLIP score between the generated image and a text description of the user\u2019s choice. During each iteration of image creation, the iterates are nudged down the gradient of the guidance function, causing the final generated image to satisfy the user\u2019s criterion.\nIn this paper, we study guidance methods that enable any off-the-shelf model or loss function to be used as guidance for diffusion. Because guidance functions can be used without re-training or modification, this form of guidance is universal in that it enables a diffusion model to be adapted for nearly any purpose.\nFrom a user perspective, guidance is superior to conditioning, as a single diffusion network is treated like a foundational model that provides universal coverage across many use cases, both commonplace and bespoke. Unfortunately, it is widely believed that this approach is infeasible. While early diffusion models relied on classifier guidance (Dhariwal & Nichol, 2021), the community quickly turned to classifier-free schemes (Ho & Salimans, 2022) that require a model to be trained from scratch on class labels with a particular frozen ontology that cannot be changed (Nichol et al., 2021; Rombach et al., 2022; Bansal et al., 2022).\nThe difficulty of using guidance stems from the domain shift between the noisy images used by the diffusion sampling process and the clean images on which the guidance models are trained. When this gap is closed, guidance can be performed successfully. For example, Nichol et al. (2021) successfully use a CLIP model as guidance, but only after re-training CLIP from scratch using noisy inputs. Noisy retraining closes the domain gap, but at a very high financial and engineering cost. To avoid the additional cost, we study methods for closing this gap by changing the sampling scheme, rather than the model.\nTo this end, our contributions are summarized as follows:\n\u2022 We propose an algorithm that enables universal guidance for diffusion models. Our proposed sampler evaluates the guidance models only on denoised images, rather than noisy latent states. By doing so, we close the domain gap that has plagued standard guidance methods. This strategy provides the end-user with the flexibility to work with a wide range of guidance modalities and even multiple modalities simultaneously. The underlying diffusion model remains fixed and no fine-tuning of any kind is necessary. \u2022 We demonstrate the effectiveness of our approach for a variety of different constraints such as classifier labels, human identities, segmentation maps, annotations from object detectors, and constraints arising from inverse linear problems."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We first briefly review the recent literature on the core framework behind diffusion models. Then, we define the problem setting of controlled image generation and discuss previous related works."
        },
        {
            "heading": "2.1 DIFFUSION MODELS",
            "text": "Diffusion models are strong generative models that proved powerful even when first introduced for image generation (Song & Ermon, 2019; Ho et al., 2020). The approach has been successfully extended to a number of domains, such as audio and text generation (Kong et al., 2020; Huang et al., 2022; Austin et al., 2021; Li et al., 2022).\nWe introduce (unconditional) diffusion formally, as it is helpful in describing the nuances of different types of models. A diffusion model is defined as a combination of a T -step forward process and a T -step reverse process. Conceptually, the forward process gradually adds Gaussian noise of different magnitudes to a clean data point z0, while the reverse process attempts to gradually denoise a noisy input in hopes of recovering a clean data point. More concretely, given an array of scalars representing noise scales {\u21b5t}Tt=1 and an initial, clean data point z0, applying t steps of the forward process to z0 yields a noisy data point\nzt = p \u21b5tz0 + ( p 1 \u21b5t)\u270f, \u270f \u21e0 N (0, I). (1)\nA diffusion model is a learned denoising network \u270f\u2713. It is trained so that for any pair (z0, t) and any sample of \u270f,\n\u270f\u2713(zt, t) \u21e1 \u270f = zt p \u21b5tz0p\n1 \u21b5t . (2)\nThe reverse process takes the form q(zt 1|zt, z0) with various detail definitions, where q(\u00b7|\u00b7) is generally parameterized as a Gaussian distribution. Different works also studied different approximations of the unknown q(zt 1|zt, z0) used to perform sampling. For example, denoising diffusion implicit model (DDIM) (Song et al., 2021a) first computed a predicted clean data point\nz\u03020 = zt ( p 1 \u21b5t)\u270f\u2713(zt, t)p\n\u21b5t , (3)\nand sample zt 1 from q(zt 1|zt, z\u03020) by replacing unknown z0 with z\u03020. On the other hand, while the details of individual sampling methods vary, all sampling methods produce zt 1 based on current sample zt, current time step t and a predicted noise \u270f\u0302. To ease the notation burden, we define a function S(\u00b7, \u00b7, \u00b7) as an abstraction of the sampling method, where zt 1 = S(zt, \u270f\u0302, t)."
        },
        {
            "heading": "2.2 CONTROLLED IMAGE GENERATION",
            "text": "In this paper, we focus on controlled image generation with various constraints. Consider a differentiable guidance function f , for example a CLIP feature extractor or a segmentation network. When applied to an image, we obtain a vector c = f(x). We also consider a function `(\u00b7, \u00b7) that measures the closeness of two vectors c and c0. Given a particular choice of c, which we call a prompt, the corresponding constraint (based on c, `, and f ) is formalized as `(c, f(z)) \u21e1 0, and we aim to generate a sample z from the image distribution satisfying the constraint. In plain words, we want to generate an in-distribution image that matches the prompt.\nPrior work on controlled generative diffusion falls into two main categories. We refer to the first category as conditional image generation, and the second category as guided image generation. Next, we discuss the characteristics of each category and better situate our work among existing methods.\nConditional Image Generation. Methods from this category require training new diffusion models that accept the prompt as an additional input (Ho & Salimans, 2022; Bansal et al., 2022; Nichol et al., 2021; Whang et al., 2022; Wang et al., 2022a; Li et al., 2023; Zhang & Agrawala, 2023). For example, Ho & Salimans (2022) proposed classifier-free guidance using class labels as prompts, and trained a diffusion model by linear interpolation between unconditional and conditional outputs of the denoising networks. Bansal et al. (2022) studied the case where the guidance function is a known linear degradation operator, and trained a conditional model to solve linear inverse problems. Nichol et al. (2021) further extended classifier-free guidance to text-conditional image generation with descriptive phrases as prompts, and trained a diffusion model to enforce the similarity between the CLIP (Radford et al., 2021) representations of the generated images and the text prompts. These methods are successful across different types of constraints, however the requirement to retrain the diffusion model makes them computationally intensive.\nGuided Image Generation. Works in this category employed a frozen pre-trained diffusion model as a foundation model, but modify the sampling method to guide the image generation with feedback from the guidance function. Our method falls into this category. Prior work that studied guided image generation did so with a variety of restrictions and external guidance functions (Dhariwal & Nichol, 2021; Kawar et al., 2022; Wang et al., 2022b; Chung et al., 2022a; Lugmayr et al., 2022; Chung et al., 2022b; Graikos et al., 2022). For example, Dhariwal & Nichol (2021) proposed classifier guidance, where they trained a classifier on images of different noise scales as the guidance function f , and included gradients of the classifier during the sampling process. However, a classifier for noisy images is domain-specific and generally not readily available \u2013 an issue our method circumvents. Wang et al. (2022b) assumed the external guidance functions to be linear operators, and generated the component of images residing in the null space of linear operators with the foundation model. Unfortunately, extending that method to handle non-linear guidance functions is non-trivial. Chung et al. (2022a) studied general guidance functions, and modified the sampling process with the gradient of guidance function calculated on the expected denoised images. Nevertheless, the authors only presented results with simpler non-linear guidance functions such as non-linear blurring.\nIn this work, we study universal guidance algorithms for guided image generation with diffusion models using any off-the-shelf guidance functions f , such as object detection or segmentation networks."
        },
        {
            "heading": "3 UNIVERSAL GUIDANCE",
            "text": "We propose a guidance algorithm that augments the image sampling method of a diffusion model to include guidance from an off-the-shelf auxiliary network. Our algorithm is motivated by an empirical observation that the reconstructed clean image z\u03020 obtained by Eq. (3), while naturally imperfect, is still appropriate for a generic guidance function to provide informative feedback to guide the image generation. In Sec. 3.1, we motivate our forward universal guidance by extending classifier guidance Dhariwal & Nichol (2021) to leverage this observation and handle generic guidance functions. In Sec. 3.2, we propose a supplementary backward universal guidance to help enforce the generated image to satisfy the constraint based on the guidance function f . In Sec. 3.3, we discuss a simple yet helpful stepwise refinement trick to empirically improve the fidelity of generated images."
        },
        {
            "heading": "3.1 FORWARD UNIVERSAL GUIDANCE",
            "text": "To guide the generation with information from the external guidance function f and the loss function `, an immediate thought is to extend classifier guidance (Dhariwal & Nichol, 2021) to accept any general guidance function. Concretely, given a class prompt c, classifier guidance performs classification-guided sampling by replacing \u270f\u2713(zt, t) in each sampling step S(zt, t) with\n\u270f\u0302\u2713(zt, t) = \u270f\u2713(zt, t) p 1 \u21b5trzt log p(c|zt). (4)\nDefining `ce(\u00b7, \u00b7) to be the cross-entropy loss and fcl to be the guidance function that outputs classification probability, Eq. (4) can be re-written as\n\u270f\u0302\u2713(zt, t) = \u270f\u2713(zt, t) + p 1 \u21b5trzt`ce(c, fcl(zt)). (5)\nHowever, directly replacing fcl and `ce with any off-the-shelf guidance and loss functions does not work in practice, as f is most likely trained on clean images and fails to provide meaningful guidance when the input is noisy.\nTo address the issue, we observe that\np(c|zt) = Z p(c|z0, zt)p(z0|zt)dz0 = Ez0\u21e0p(z0|zt)[p(c|z0)]. (6)\nwhere c is conditionally-independent with zt given z0. Leveraging the fact that we can obtain a predicted clean image z\u03020 by Eq. (3) with \u270f\u2713(zt, t), we approximate the expectation in Eq. (6) as Ez0\u21e0p(z0|zt)[p(c|z0)] \u21e1 p(c|z\u03020). This leads to our proposed guided sampling procedure\n\u270f\u0302\u2713(zt, t) = \u270f\u2713(zt, t) + s(t) \u00b7rzt`(c, f(z\u03020)) (7) where s(t) controls the guidance strength for each sampling step and\nrzt`(c, f(z\u03020)) = rzt` \u2713 c, f \u2713 zt p 1 \u21b5t\u270f\u2713(zt, t)p\n\u21b5t\n\u25c6\u25c6\nas in Eq. (3). We term Eq. (7) forward universal guidance, or forward guidance in short. In practice, applying forward guidance effectively brings the generated image closer to the prompt while keeping the generation trajectory in the data manifold. We note that a related approach is also studied in Chung et al. (2022a), where the guidance step is computed based on E[z0|zt]. The approach drew inspiration from the score-based generative framework (Song et al., 2021b), but resulted in a different update method."
        },
        {
            "heading": "3.2 BACKWARD UNIVERSAL GUIDANCE",
            "text": "As will be shown in Sec. 4.2, we observe that forward guidance sometimes over-prioritizes maintaining the \u201crealness\u201d of the image, resulting in an unsatisfactory match with the given prompt. Simply increasing the guidance strength s(t) is suboptimal, as this often results in instability as the image moves off the manifold faster than the denoiser can correct it.\nAlgorithm 1 Universal Guidance Parameter: Refinement steps k, backward guidance steps m, and guidance strength s(t), Required: zt the noisy vector at a given time-step t, diffusion model \u270f\u2713, noise scale \u21b5t, guidance function f , loss function `, and prompt c for n = 1, 2, . . . , k do\nCalculate z\u03020 as in Eq. (3). Calculate \u270f\u0302\u2713 using forward universal guidance as in Eq. (7). if m > 0 then\nCalculate z0 by minimizing Eq. (8) with m steps of gradient descent. Perform backward universal guidance by \u270f\u0302\u2713 \u270f\u0302\u2713 p \u21b5t/(1 \u21b5t) z0 (see Eq. (10)).\nend if zt 1 S(zt, \u270f\u0302\u2713, t). \u270f0 \u21e0 N (0, I). zt p \u21b5t/\u21b5t 1zt 1 + p 1 \u21b5t/\u21b5t 1\u270f0.\nend for\nReturn zt 1\nTo address the issue, we propose backward universal guidance, or backward guidance in short, to supplement forward guidance and help enforce the generated image to satisfy the constraint. The key idea of backward guidance is to optimize for a clean image that best matches the prompt based on z\u03020, and linearly translate the guided change back to the noisy image space at step t. Concretely, instead of directly calculating rzt`(c, f(z\u03020)), we compute a guided change z0 in clean data space as\nz0 = argmin `(c, f(z\u03020 + )). (8)\nEmpirically, we solve Eq. (8) with m-step gradient descent, where we use = 0 as a starting point. Since z\u03020 + z0 minimizes `(c, f(z)) directly, z0 is the change in clean data space that best enforces the constraint. Then, we translate z0 back to the noisy data space of zt by calculating the guided denoising prediction \u270f\u0303 that satisfies\nzt = p \u21b5t(z\u03020 + z0) + p 1 \u21b5t\u270f\u0303. (9)\nReusing Eq. (3), we can rewrite \u270f\u0303 as an augmentation to the original denoising prediction \u270f\u2713(zt, t) by\n\u270f\u0303 = \u270f\u2713(zt, t) p \u21b5t/(1 \u21b5t) z0. (10)\nComparing to forward guidance, backward guidance (as Eq. (10)) produces an optimized direction for the generated image to match the given prompt, and hence prioritizes enforcing the constraint. Furthermore, calculation of a gradient step for Eq. (8) is computationally cheaper than forward guidance (Eq. (7)), and we can therefore afford to solve Eq. (8) with multiple gradient steps, further improving the match with the given prompt.\nWe note that the names \u201cforward\u201d and \u201cbackward\u201d are used analogously to the forward and backward Euler methods."
        },
        {
            "heading": "3.3 UNIVERSAL STEPWISE REFINEMENT",
            "text": "Upon applying our universal guidance to standard generation pipelines, we observe that in some instances, the images generated exhibit artifacts or unusual behaviors that differentiate them from natural images. Similar findings have been reported in (Lugmayr et al., 2022; Wang et al., 2022b), where linear guidance functions were explored. Although we attempted to enhance the realism by adjusting the guidance strength s(t), finding an optimal balance that both guarantees image realism and adherence to guidance constraints proved challenging, particularly with complex guidance functions. We\nhypothesize that the direction of guidance offered by our universal method might not always correlate with image realism, diverting the image from a natural image sampling trajectory.\nDrawing motivation from Lugmayr et al. (2022); Wang et al. (2022b), we propose a more nuanced approach, termed Universal Stepwise Refinement (USR), to navigate these challenges. Specifically,\nafter sampling zt 1 = S(zt, \u270f\u0302t, t), we reintroduce Gaussian noise \u270f0 \u21e0 N (0, I) to zt 1, deriving z0t as per the equation:\nz0t = p \u21b5t/\u21b5t 1 \u00b7 zt 1 + p 1 \u21b5t/\u21b5t 1 \u00b7 \u270f0. (11)\nEquation 11 ensures z0t retains the appropriate noise scale for input at time step t. Given a refinement step k, we repeat the Universal Stepwise Refinement k times before advancing with the sampling for step t 1. This refined process enables the exploration of various regions of the data manifold at a consistent noise scale to reach a solution that aligns with both the guidance and image quality requisites. Our empirical assessments indicate that using USR mechanism on top of proper guidance strength s(t) significantly enhanced the image realism while still maintaining the adherence to the prompt, as shown in Fig. 2.\nWe summarize our universal guidance algorithm composed of forward universal guidance, backward universal guidance and universal stepwise refinement for a single sampling step in Algorithm 1. For simplicity, the algorithm assumes only one guidance function, but can be easily adapted to handle multiple pairs of (f, l). Additionally, the objectives of the forward and backward guidance do not have to be identical, allowing different ways to simultaneously utilize multiple guidance functions."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we present results testing our proposed universal guidance algorithm against a wide variety of guidance functions. Specifically, we experiment with Stable Diffusion (Rombach et al., 2022), a diffusion model that is able to perform text-conditional generation by accepting text prompt as additional input, and experiment with a purely unconditional diffusion model trained on ImageNet (Deng et al., 2009), where we use pre-trained model provided by OpenAI (Dhariwal & Nichol, 2021). We note that Stable Diffusion, while being a text-conditional generative model, can also perform unconditional image generation by simply using an empty string as the text prompt. We first present the experiment on Stable Diffusion for different guidance functions in Sec. 4.1, and present the results on ImageNet diffusion model in Sec. 4.2. Hyper-parameters used for different guidance functions, further ablation studies and selection procedures for suitable guidance strength s(t) and refinement step k can be found in the appendix."
        },
        {
            "heading": "4.1 RESULTS FOR STABLE DIFFUSION",
            "text": "In this section, we present the results of guided image generation using Stable Diffusion as the foundation model. The guidance functions we experiment with include a segmentation network, a face recognition network, an object detection network and style guidance with CLIP feature extractor (Radford et al., 2021). For experiments on Stable Diffusion, we discover that applying forward guidance already produce high-quality images that match the given prompt, and hence set m = 0. To perform forward guidance on Stable Diffusion, we forward the predicted clean latent variable computed by Eq. (3) through the image decoder of Stable Diffusion to obtain predicted clean images. We discuss the qualitative results and implementation details for each guidance function in its corresponding subsection. We summarize our quantitative evaluation in Tab. 1, where we evaluate how well the generated images match the external guidance with task-specific metrics, and include the similarity of CLIP embedding between text prompts and images generated with and without guidance. For all tasks, the minimum difference between CLIP similarities indicates that our algorithm performs guidance without sacrificing the effectiveness of the underlying diffusion model. We also include additional quantitative and qualitative results for text-guided generation in the appendix.\nSegmentation Map Guidance. To guide image generation using a segmentation map as prompt, we use a MobileNetV3-Large (Howard et al., 2019) with a segmentation head, and a publicly available pre-trained model in PyTorch (Paszke et al., 2019). We use standard perpixel cross-entropy loss between a given prompt and the predicted segmentation of generated images as our loss function `.\nIn our experiment, we combine segmentation maps that depict objects of different shapes with new text prompts. We use the text prompt as a fixed additional input to Stable Diffusion to perform text-conditional sampling, and guide the text-conditional generated images to match the given segmentation maps. Qualitative results are presented in Fig. 3. From Fig. 3, we see that the generated images show a clear separation between object and background that matches the given segmentation map nearly perfectly. The generated object and background also each match their descriptive text (i.e. dog breed and environment description). Furthermore, the generated images are overall highly realistic. In Tab. 1, we evaluate mIoU between the ground truth segmentation map and the predicted segmentation of generated images to assess the match with constraint.\nFace Recognition Guidance. For guiding generation to include a specific person\u2019s likeness, we propose combining face detection and facial recognition modules into one guidance function. This setup produces a facial attribute embedding from an image of a face. We use multi-task cascaded convolutional networks (MTCNN) (Zhang et al., 2016) for face detection, and we use facenet (Schroff et al., 2015) for facial recognition. The guidance function f then crops out the detected face and outputs a facial attribute embedding as a prompt and we use an l1-loss between embeddings as the loss function `.\nWe explore different combinations of face guidance and text prompts. Similarly to the segmentation case, we use the text prompt as a fixed additional conditioning to Stable Diffusion and guide this text-conditional trajectory with our algorithm so that the face in the generated image looks similar to the face prompt. In Fig. 4, we clearly see that the facial characteristics of a given face prompt are reproduced almost perfectly on the generated images. The descriptive text of either background, material, or style is also realized correctly and blends nicely with the generated faces. We again summarize our quantitative evaluation in Tab. 1. We evaluate the similarity between facial attributes of ground-truth identity and the generated faces. In general, two faces are considered to be from the same person if the similarity is over 0.5, and our algorithm can effectively guide the generated face to meet the criteria.\nObject Location Guidance For Stable Diffusion, we also present the results of guided image generation with an object detection network. For this experiment, we use Faster-RCNN (Ren et al., 2015) with Resnet-50-FPN backbone (Li et al., 2021), a publicly available pre-trained model in\nPytorch, as our object detector. We use bounding boxes with class labels as our object location prompt. We construct a loss function ` by the sum of three individual losses, namely (1) anchor classification loss, (2) bounding box regression loss and (3) region label classification loss.\nWe again experiment with different combinations of text prompt and object location prompt, and similarly use the text prompt as a fixed conditioning to Stable Diffusion. Using our proposed guidance algorithm, we perform guided image generation that generates and matches the objects presented in the text prompt to the given object locations. The results are presented in Fig. 5. We observe from Fig. 5 that objects in the descriptive text all appear in the designated location with the appropriate size indicated by the given bounding boxes. Each location is filled with appropriate, high-quality generations that align with varied image content prompts, ranging from \u201cbeach\u201d to \u201coil painting\u201d. In Tab. 1, we use mAP@50 to measure how well the generated images satisfy the constraint.\nStyle Guidance Finally, we conclude our experiments on Stable Diffusion by guiding the image generation based on a reference style given by a style image. To achieve so, we capture the reference style from the style image by the image feature extractor from CLIP, and use the resulting image embedding as prompts. The loss function calculates the negative cosine similarity between the embedding of generated images and the embedding of the style image. Similar to previous experiments, we control the content using text input as additional conditioning to the Stable Diffusion model. We experiment with combinations of different style images and different text prompts, and present the results in Fig. 6. From Fig. 6, we can see that the generated images contain contents that match the given text prompts, while exhibiting style that matches the given style images."
        },
        {
            "heading": "4.2 RESULTS FOR IMAGENET DIFFUSION",
            "text": "In this section, we present results for guided image generation using an unconditional diffusion model trained on ImageNet. We experiment with object location guidance and a hybrid guided image generation task which we term segmentation-guided inpainting. We also include additional experiments for CLIP guidance in the appendix. We will discuss results of each guidance separately.\nObject Location Guidance. Similar to object location guidance for Stable Diffusion, we also use the same network architecture and the same pre-trained model as our object detection network, and construct an identical loss function ` for our guidance algorithm. However, unlike Stable Diffusion, object locations are the only prompts available for guided image generation. We experiment with different object location prompts using either (1) only forward universal guidance and (2) both\nObject Location Forward Only Forward + Backward\nFigure 7: Generation guided by object detection with the unconditional ImageNet model. While both forward and backward guidance produces realistic images with the desired objects in the designated locations, forward guidance alone produces the wrong objects or the wrong locations/sizes.\nforward and backward universal guidance. We observe from Fig. 7 that applying both forward and backward guidance generates images that are realistic and the objects match the prompt nicely. On the other hand, while images generated using only forward guidance remain realistic, they feature objects with mismatching categories and locations. The observation is further backed by quantitative evaluation presented in Tab. 2. The evaluation is based on mAP@50 between the ground truth object locations and the predicted bounding boxes of generated images, and clearly shows that the combination of forward and backward guidance leads to a much better match with the constraint. The results demonstrate the effectiveness of our universal guidance algorithm, and also validate the necessity of our backward guidance.\nSegmentation-Guided Inpainting. In this experiment, we aim to explore the ability of our algorithm to handle multiple guidance functions. We perform guided image generation with combined guidance from an inpainting mask, a classifier and a segmentation network. We first generate images with masked regions as the prompt for inpainting. We then pick an object class c as the prompt for classification and generate a segmentation mask where the masked regions\nare considered foreground objects of the same class c. We use `2 loss on the non-masked region as the loss function for inpainting, and set the corresponding s(t) = 0, or equivalently only use backward guidance for inpainting. We use the same segmentation network as described in Sec. 4.1. For classification guidance, we use the classifier that accepts noisy input (Dhariwal & Nichol, 2021), and perform the original classifier guidance as Eq. (4) instead of our forward guidance. The results in Fig. 8 show that when using both inpainting and classifier as guidance, our algorithm generates realistic images that both match the inpainting prompt and are classified correctly to the given object class. Adding in segmentation guidance, our algorithm further improves by making a near-perfect match to both the segmentation map and the inpainting prompt while maintaining realism, demonstrating that our algorithm effectively combines feedback from multiple guidance functions."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose a universal guidance algorithm that is able to perform guided image generation with any off-the-shelf guidance function based on a fixed foundation diffusion model. Our algorithm only needs guidance and loss functions to be differentiable, and avoids any retraining to adapt the guidance function or the foundation model to a specific type of prompt. We demonstrate promising results with our algorithm on complex guidance including segmentation, face recognition and object detection systems \u2013 and multiple guidance functions can even be used together.\nREPRODUCIBILITY STATEMENT\nwe describe guidance functions and foundation diffusion models for experiments presented in Sec. 4 in the corresponding subsections. Hyperparameters for experiments described in Sec. 4 of the main paper can be found in Sec. B in the appendix. We also include the source code used to conduct the experiments described in the paper in our supplementary material."
        }
    ],
    "year": 2023
}