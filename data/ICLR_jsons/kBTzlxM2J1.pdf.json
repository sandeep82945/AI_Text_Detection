{
    "abstractText": "There is increasing interest in methods for extracting interpretable rules from ML models trained to solve a wide range of tasks over knowledge graphs (KGs), such as KG completion, node classification, question answering and recommendation. Many such approaches, however, lack formal guarantees establishing the precise relationship between the model and the extracted rules, and this lack of assurance becomes especially problematic when the extracted rules are applied in safetycritical contexts or to ensure compliance with legal requirements. Recent research has examined whether the rules derived from the influential NEURAL-LP model exhibit soundness (or completeness), which means that the results obtained by applying the model to any dataset always contain (or are contained in) the results obtained by applying the rules to the same dataset. In this paper, we extend this analysis to the context of DRUM, an approach that has demonstrated superior practical performance. After observing that the rules currently extracted from a DRUM model can be unsound and/or incomplete, we propose a novel algorithm where the output rules, expressed in an extension of Datalog, ensure both soundness and completeness. This algorithm, however, can be inefficient in practice and hence we propose additional constraints to DRUM models facilitating rule extraction, albeit at the expense of reduced expressive power.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaxia Wang"
        },
        {
            "affiliations": [],
            "name": "David J. Tena Cucala"
        },
        {
            "affiliations": [],
            "name": "Bernardo Cuenca Grau"
        },
        {
            "affiliations": [],
            "name": "Ian Horrocks"
        }
    ],
    "id": "SP:db5f6af13dac02534f4996c64076b60702b16eb6",
    "references": [
        {
            "authors": [
                "Hamed Ayoobi",
                "Nico Potyka",
                "Francesca Toni"
            ],
            "title": "SpArX: Sparse argumentative explanations for neural networks",
            "venue": "In Proc. of the 26th European Conf. on Artificial Intelligence (ECAI 2023),",
            "year": 2023
        },
        {
            "authors": [
                "Adrien Benamira",
                "Tristan Gu\u00e9rand",
                "Thomas Peyrin",
                "Hans Soegeng"
            ],
            "title": "Neural network-based rule models with truth tables",
            "venue": "In Proc. of the 26th European Conf. on Artificial Intelligence (ECAI 2023),",
            "year": 2023
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garc\u0131\u0301a-Dur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko"
            ],
            "title": "Translating embeddings for modeling multi-relational data",
            "venue": "In Proc. of the 27th Conf. on Neural Information Processing Systems (NeurIPS",
            "year": 2013
        },
        {
            "authors": [
                "Evgeny Dantsin",
                "Thomas Eiter",
                "Georg Gottlob",
                "Andrei Voronkov"
            ],
            "title": "Complexity and expressive power of logic programming",
            "venue": "ACM Computing Surveys,",
            "year": 2001
        },
        {
            "authors": [
                "Tim Dettmers",
                "Pasquale Minervini",
                "Pontus Stenetorp",
                "Sebastian Riedel"
            ],
            "title": "Convolutional 2D knowledge graph embeddings",
            "venue": "In Proc. of the 32nd AAAI Conf. on Artificial Intelligence (AAAI 2018),",
            "year": 2018
        },
        {
            "authors": [
                "Richard Evans",
                "Edward Grefenstette"
            ],
            "title": "Learning explanatory rules from noisy data",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2018
        },
        {
            "authors": [
                "Jo\u00e3o Ferreira",
                "Manuel de Sousa Ribeiro",
                "Ricardo Gon\u00e7alves",
                "Jo\u00e3o Leite"
            ],
            "title": "Looking inside the black-box: Logic-based explanations for neural networks",
            "venue": "In Proc. of the 19th Int. Conf. on Principles of Knowledge Representation and Reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Guo",
                "Fuzhen Zhuang",
                "Chuan Qin",
                "Hengshu Zhu",
                "Xing Xie",
                "Hui Xiong",
                "Qing He"
            ],
            "title": "A survey on knowledge graph-based recommender systems",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Stanley Kok",
                "Pedro M. Domingos"
            ],
            "title": "Statistical predicate invention",
            "venue": "In Proc. of the 24th Int. Conf. on Machine Learning (ICML",
            "year": 2007
        },
        {
            "authors": [
                "Yunshi Lan",
                "Gaole He",
                "Jinhao Jiang",
                "Jing Jiang",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "title": "A survey on complex knowledge base question answering: Methods, challenges and solutions",
            "venue": "In Proc. of the 30th Int. Joint Conf. on Artificial Intelligence (IJCAI 2021),",
            "year": 2021
        },
        {
            "authors": [
                "Jan Portisch",
                "Heiko Paulheim"
            ],
            "title": "The DLCC node classification benchmark for analyzing knowledge graph embeddings",
            "venue": "In Proc. of the 21st Int. Semantic Web Conf. (ISWC 2022),",
            "year": 2022
        },
        {
            "authors": [
                "Meng Qu",
                "Junkun Chen",
                "Louis-Pascal A.C. Xhonneux",
                "Yoshua Bengio",
                "Jian Tang"
            ],
            "title": "RNNLogic: Learning logic rules for reasoning on knowledge graphs",
            "venue": "In Proc. of the 9th Int. Conf. on Learning Representations (ICLR 2021),",
            "year": 2021
        },
        {
            "authors": [
                "Ali Sadeghian",
                "Mohammadreza Armandpour",
                "Patrick Ding",
                "Daisy Zhe Wang"
            ],
            "title": "DRUM: endto-end differentiable rule mining on knowledge graphs",
            "venue": "In Proc. of the 33rd Conf. on Neural Information Processing Systems (NeurIPS 2019),",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang"
            ],
            "title": "RotatE: Knowledge graph embedding by relational rotation in complex space",
            "venue": "In Proc. of the 7th Int. Conf. on Learning Representations (ICLR",
            "year": 2019
        },
        {
            "authors": [
                "David Jaime Tena Cucala",
                "Bernardo Cuenca Grau",
                "Egor V. Kostylev",
                "Boris Motik"
            ],
            "title": "Explainable GNN-based models over knowledge graphs",
            "venue": "In Proc. of the 10th Int. Conf. on Learning Representations (ICLR 2022),",
            "year": 2022
        },
        {
            "authors": [
                "David Jaime Tena Cucala",
                "Bernardo Cuenca Grau",
                "Boris Motik"
            ],
            "title": "Faithful approaches to rule learning",
            "venue": "In Proc. of the 19th Int. Conf. on Principles of Knowledge Representation and Reasoning (KR 2022),",
            "year": 2022
        },
        {
            "authors": [
                "Komal K. Teru",
                "Etienne G. Denis",
                "William L. Hamilton"
            ],
            "title": "Inductive relation prediction by subgraph reasoning",
            "venue": "In Proc. of the 37th Int. Conf. on Machine Learning (ICML",
            "year": 2020
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Danqi Chen"
            ],
            "title": "Observed versus latent features for knowledge base and text inference",
            "venue": "In Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC",
            "year": 2015
        },
        {
            "authors": [
                "Meihong Wang",
                "Linling Qiu",
                "Xiaoli Wang"
            ],
            "title": "A survey on knowledge graph embeddings for link prediction. Symmetry",
            "year": 2021
        },
        {
            "authors": [
                "Zhuo Wang",
                "Wei Zhang",
                "Ning Liu",
                "Jianyong Wang"
            ],
            "title": "Learning interpretable rules for scalable data representation and classification",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Wang",
                "Yangqiu Song",
                "Ginny Y. Wong",
                "Simon See"
            ],
            "title": "Logical message passing networks with one-hop inference on atomic formulas",
            "venue": "In Proc. of the 11th Int. Conf. on Learning Representations (ICLR",
            "year": 2023
        },
        {
            "authors": [
                "Yikun Xian",
                "Zuohui Fu",
                "S. Muthukrishnan",
                "Gerard de Melo",
                "Yongfeng Zhang"
            ],
            "title": "Reinforcement knowledge graph reasoning for explainable recommendation",
            "venue": "In Proc. of the 42nd Int. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR 2019),",
            "year": 2019
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Thien Hoang",
                "William Yang Wang"
            ],
            "title": "DeepPath: A reinforcement learning method for knowledge graph reasoning",
            "venue": "In Proc. of the 2017 Conf. on Empirical Methods in Natural Language Processing (EMNLP 2017),",
            "year": 2017
        },
        {
            "authors": [
                "Fan Yang",
                "Zhilin Yang",
                "William W. Cohen"
            ],
            "title": "Differentiable learning of logical rules for knowledge base reasoning",
            "venue": "In Proc. of the 31st Conf. on Neural Information Processing Systems (NeurIPS 2017),",
            "year": 2017
        },
        {
            "authors": [
                "Shichang Zhang",
                "Jiani Zhang",
                "Xiang Song",
                "Soji Adeshina",
                "Da Zheng",
                "Christos Faloutsos",
                "Yizhou Sun"
            ],
            "title": "PaGE-Link: Path-based graph neural network explanation for heterogeneous link prediction",
            "venue": "In Proc. of the ACM Web Conf",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Numerous tasks over knowledge graphs (KGs) such as completion (Wang et al., 2021), node classification (Portisch & Paulheim, 2022), question answering (Lan et al., 2021) and recommendation (Guo et al., 2022; Xian et al., 2019) can be formulated as a transformation from an input to an output dataset (e.g., KG completion transforms an incomplete KG to its extension with the missing facts). ML models are widely employed to acquire these transformations from examples, as it is a more cost-effective approach than manual design and does not require domain expertise.\nML-based solutions face a challenge in explaining predictions. Recent studies (Yang et al., 2017; Evans & Grefenstette, 2018; Sadeghian et al., 2019; Qu et al., 2021; Ferreira et al., 2022; Zhang et al., 2023; Wang et al., 2023) propose techniques to extract interpretable Datalog (Abiteboul et al., 1995) rules from trained models. However, many of these approaches lack formal guarantees establishing the relationship between the model and the extracted rules. Instead, they often rely on informal claims that the rules \u201capproximate\u201d or \u201cexplain\u201d the model\u2019s behavior (Yang et al., 2017; Evans & Grefenstette, 2018; Sadeghian et al., 2019; Qu et al., 2021). Such claims may be substantiated by empirical evidence showcasing the similarity between the model\u2019s predictions and the outcomes of rule application (Ferreira et al., 2022). Nonetheless, the need for formal assurances regarding the alignment between model and rules becomes particularly critical when these rules are applied in safety-critical contexts or to ensure compliance with legal requirements for explainability.\nRecent studies have explored the formal relationship between models and extracted rules in the context of Feed-Forward Networks (Ayoobi et al., 2023; Wang et al., 2022), Convolutional Neural Networks (Benamira et al., 2023), and Graph Neural Networks (Tena Cucala et al., 2022a;b). These works examine whether the rules exhibit soundness (or completeness), which means that the results obtained by applying the model to any given dataset always contain (or are contained in) the results obtained by applying the rules to the same dataset. Faithful (i.e., both sound and complete) rule sets\ncan be used to explain the model\u2019s predictions. For example, assume that a modelMmed trained to suggest potential diagnoses predicts fact diagnose(alice, flu) when applied to the dataset\nDmed = {indicativeOf(fever, flu), indicativeOf(fever, tuberculosis), hasSymptom(alice, fever), contact(alice, bob), diagnose(bob, flu)} .\nA medical professional can examine the extracted rules Rmed to understand model predictions. An incomplete rule set may fail to support the predicted fact, leaving the prediction unexplained. An unsound rule, like diagnose(x, y) \u2190 hasSymptom(x, z) \u2227 indicativeOf(z, y), may generate facts not predicted by the model, such as diagnose(alice, tuberculosis). In contrast, a faithful rule set lacks spurious rules and guarantees an explanatory rule for each predicted fact. For example, the following rule rdiag explains alice\u2019s flu diagnosis due to her fever and contact with a flu patient:\ndiagnose(x, y)\u2190 hasSymptom(x, z1) \u2227 indicativeOf(z1, y) \u2227 contact(x, z2) \u2227 diagnose(z2, y) . The analysis in Tena Cucala et al. (2022b) revealed that soundness for NEURAL-LP can be ensured by selecting appropriate hyperparameters, but completeness cannot be guaranteed. Thus, the extracted rules may not be a faithful representation of the model.\nIn this paper, we study faithfulness guarantees in the context of DRUM (Sadeghian et al., 2019)\u2014an approach inspired by NEURAL-LP that has demonstrated superior empirical performance. DRUM exhibits significant differences with respect to NEURAL-LP, which makes the relationship between these approaches unclear. First, each DRUM model comprises multiple sub-models, initially applied independently to the input data, followed by the aggregation of their outputs; each DRUM sub-model resembles a NEURAL-LP model, but it contains much fewer parameters as it does not implement \u201cskip connections\u201d between network layers. Second, DRUM sub-models and NEURAL-LP employ different mechanisms for learning rules of varying lengths. Third, DRUM can generate a broader class of rules, including inverse rules such as parent(x, y) \u2190 child(y, x).1 Given these disparities, the results from Tena Cucala et al. (2022b) for NEURAL-LP do not directly apply to DRUM.\nIn Section 2, we revisit the definitions of Datalog, DRUM, and the concepts of soundness, completeness, and faithfulness introduced in Tena Cucala et al. (2022b). In Section 3, we analyse the faithfulness of rules extracted from a DRUM model. We observe that, much like in NEURAL-LP, the behavior of DRUM can be characterised in terms of \u201ccounting\u201d the distinct matches of a rule\u2019s body within the input data. We can thus establish results akin to those in Tena Cucala et al. (2022b) regarding NEURAL-LP. In Section 4, we present a method for extracting a faithful set of rules from a DRUM model expressed in an extension of Datalog with inequalities and disjunctions in the rule body, enabling the necessary counting operations. This represents an advancement compared to prior research because the faithful rule extraction method in Tena Cucala et al. (2022b) applies only to significantly restricted NEURAL-LP models. However, we note that achieving a practical implementation of our method may be challenging due to the time complexity associated with the rule extraction algorithm. In Section 5 we propose two solutions to this issue. Firstly, we introduce a mechanism that, given a DRUM model and a dataset, extracts a rule set that is sound for the model and derives all the model predictions on the given dataset. Secondly, we impose constraints on the DRUM models facilitating the extraction of a faithful rule set, at the expense of reduced expressive power. Specifically, we introduce two variants of the model, MMDRUM and SMDRUM, each striking a distinct balance between expressivity of the model and the effectiveness of rule extraction.\nIn Section 6, we conduct a comprehensive evaluation on KG completion tasks. Amongst other findings, our experiments show that SMDRUM and MMDRUM obtain competitive performance and confirm the practical feasibility of the rule extraction algorithms proposed in Section 5."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Datalog. A signature consists of disjoint, countable sets of constants and predicates, where each predicate is assigned a non-negative arity. A term is a variable or a constant. An atom is an expression of the form R(t1, \u00b7 \u00b7 \u00b7 , tn), where R is an n-ary predicate and t1, \u00b7 \u00b7 \u00b7 , tn are terms. A fact is a variable-free atom, and a dataset is a finite set of facts. A Datalog rule is an expression of the form:\nH \u2190 B1 \u2227 \u00b7 \u00b7 \u00b7 \u2227B\u2113 , \u2113 \u2265 0 , (1) 1There is an additional difference concerning the way in which the model\u2019s parameters are generated; however, it is irrelevant to our analysis, which does not rely on the method used to compute the model\u2019s parameters.\nwhere H and Bi for 1 \u2264 i \u2264 \u2113 are atoms. Typically, H is called the head atom, and each Bi is called a body atom. The value \u2113 \u2208 N is the length of the rule. We do not make the usual safety requirement that each variable in H must appear in some Bi. In fact, the rule body can be empty if \u2113 = 0, in which case we write \u22a4.2 A Datalog program is a finite set of rules. For a mapping \u03c3 of variables to constants and a term or a conjunctionX with variables in the domain of \u03c3, X\u03c3 is the result of replacing each x inX with \u03c3(x). For a rule r of the form (1), the immediate consequence operator Tr maps a dataset D to the smallest dataset Tr(D) containing H\u03c3 for each mapping \u03c3 of variables in r to constants inD that grounds the body of r inD i.e. that satisfiesBi\u03c3 \u2208 D for all 1 \u2264 i \u2264 \u2113. For a Datalog programR, TR is defined as TR(D) = \u22c3 r\u2208R Tr(D).\nVectors, Matrices and Tensors. For each n \u2208 N, we use the notion of a n-dimensional tensor (Yang et al., 2017; Sadeghian et al., 2019) over R (e.g., a matrix is a 2-tensor). For an n-tensor A, A(i1, i2, \u00b7 \u00b7 \u00b7 , in) is the element at position (i1, i2, \u00b7 \u00b7 \u00b7 , in). If M and N are matrices of dimension m \u00d7 n and n \u00d7 p, respectively, then the max-product of M and N, written M \u2297N, is a matrix of dimension m\u00d7 p whose element at position i and j is equal to maxnk=1 M(i, k) \u00b7N(k, j).\nThe DRUM Model. DRUM (Sadeghian et al., 2019) assumes a signature with \u03b4 binary predicates R1, R2, \u00b7 \u00b7 \u00b7 , R\u03b4 and is designed to learn rules of the form\nRh(x, y)\u2190 \u03c8k1 \u2227 \u03c8k2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u03c8k\u2113 with \u2113 \u2265 1 and h, ki \u2208 {1, \u00b7 \u00b7 \u00b7 , \u03b4} , (2)\nwhere \u03c8ki is of the form Rki(zi\u22121, zi) or Rki(zi, zi\u22121), with z0 = x and z\u2113 = y. We will show in Section 3, however, that rules of this form are not sufficiently expressive to faithfully characterise DRUM models. For instance, our example rule rdiag in Section 1 is not of this form.\nA DRUM model M of rank N \u2265 1 and depth L \u2265 1 is a tuple (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2), where each ah for 1 \u2264 h \u2264 \u03b4 is a 3-tensor over [0, 1]N\u00d7L\u00d7(2\u03b4+1) of (learnable) parameters and \u03b2 \u2208 R is a threshold for prediction. The rank determines the number of considered \u201csub-models\u201d, as mentioned in Section 1. Models with higher rank are more expressive and can simulate a richer set of rules (Sadeghian et al., 2019). The depth L determines the maximum length of a rule that can be learned. A DRUM modelM induces a transformation TM over datasets as we specify next. Given a dataset D, let c1, c2, \u00b7 \u00b7 \u00b7 , c\u03f5 be the constants in D arranged in a fixed but arbitrary order. For each 1 \u2264 k \u2264 2\u03b4 + 1, an \u03f5 \u00d7 \u03f5 binary matrix Mk is computed by setting to 1 all values Mk(i, j) satisfying one of the following conditions: (1) Rk(ci, cj) \u2208 D and 1 \u2264 k \u2264 \u03b4, (2) Rk\u2212\u03b4(cj , ci) \u2208 D and \u03b4 + 1 \u2264 k \u2264 2\u03b4, or (3) i = j and k = 2\u03b4 + 1; all remaining values are set to 0. Matrices Mk for k \u2264 \u03b4 are adjacency matrices representing facts in D. DRUM can be conceptualised as extending the input dataset with new facts over \u03b4 + 1 fresh predicates in the remaining matrices: a factR2\u03b4+1(c, c) for each constant c inD and a factR\u03b4+k(cj , ci) for eachRk(ci, cj) \u2208 D representing its inverse. The adjacency matrices M\u03b4+1, \u00b7 \u00b7 \u00b7 ,M2\u03b4+1 capture the new facts. Next, DRUM represents each constant cs with an one-hot vector vs \u2208 {0, 1}\u03f5 in which vs(s) = 1 and 0 elsewhere. Then, for 1 \u2264 h \u2264 \u03b4, DRUM computes a vector vhs as Equation (3).\n(vhs ) \u22ba = (vs) \u22ba \u00b7 N\u2211 i=1 L\u220f j=1 ( 2\u03b4+1\u2211 k=1 ah(i, j, k) \u00b7Mk ) . (3)\nVector vhs indicates, for each 1 \u2264 t \u2264 \u03f5, whether the fact Rh(cs, ct) is in TM(D), in the following way: Rh(cs, ct) \u2208 TM(D) if and only if vhs (t) is strictly greater than the prediction threshold \u03b2.\nRule Extraction in DRUM. Rules are extracted from a trained DRUM model M of depth L by first computing a confidence score \u03b1r \u2208 R for each rule r of the form (2) with length at most L. We next show how to compute such score. In the simple case where r has length exactly L, the score is \u03b1r = \u2211N i=1 \u03b1 i r where each \u03b1 i r is simply the product of the model parameters corresponding to each body atom of r for the i-th sub-model. In particular, for Rh the head predicate of r, parameter ah(i, j, kj) (respectively, ah(i, j, kj + \u03b4)) is associated to a body atom Rkj (zj\u22121, zj) (respectively, Rkj (zj , zj\u22121) at position 1 \u2264 j \u2264 L and sub-model 1 \u2264 i \u2264 N . Finally, if the length of r is smaller than L, we first consider all the ways in which r can be extended to an equivalent\n2Rules are applied to datasets with finitely many constants, and thus only finitely many facts can be derived.\nrule of the form (2) and length exactly L by padding the sequence of predicates in the body with the identity predicate R2\u03b4+1. For example, if L = 2, a rule R2(x, y)\u2190 R1(x, y) can be extended as\nR2(x, y)\u2190 R1(x, z1) \u2227R2\u03b4+1(z1, y) and R2(x, y)\u2190 R2\u03b4+1(x, z1) \u2227R1(z1, y) . We can compute a value for each of these rules as before and set \u03b1r as the maximum of these values.\nFor a rule r of the form (2), the set SLr contains all sequences (k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L) that can be obtained from (k1, \u00b7 \u00b7 \u00b7 , k\u2113) by replacing kj by kj + \u03b4 if \u03c8kj is of the form Rkj (zj , zj\u22121), and then padding it (if needed) with the value 2\u03b4 + 1. Then, the confidence score \u03b1r for r is\n\u03b1r = max (k\u20321,\u00b7\u00b7\u00b7 ,k\u2032L)\u2208SLr N\u2211 i=1 L\u220f j=1 ah(i, j, k\u2032j) . (4)\nFor each \u03b3 \u2208 R,RDRUMM,\u03b3 is the set of all rules in the form (2) with \u2113 \u2264 L and score higher than \u03b3.\nRelations between Models & Programs. ProgramR is sound for modelM if TR(D) \u2286 TM(D) for any datasetD. Conversely, R is complete forM if TM(D) \u2286 TR(D) for any datasetD. Finally, we say thatR is faithful forM if it is both sound and complete forM (Tena Cucala et al., 2022b)."
        },
        {
            "heading": "3 ANALYSING FAITHFULNESS OF RULE EXTRACTION IN DRUM",
            "text": "We begin by describing the behavior of a DRUM modelM of rank N and depth L. We show that whenM is applied to a dataset D, the decision made byM as to whether to return a fact Rh(cs, ct) can be characterised by considering each rule of the form (2) with \u2113 \u2264 L and head predicate Rh, and then counting the distinct matches of the rule body in D where x is mapped to cs and y to ct. Lemma 1. For a dataset D with constants c1, \u00b7 \u00b7 \u00b7 , c\u03f5, vector vhs computed by M for 1 \u2264 s \u2264 \u03f5 and 1 \u2264 h \u2264 \u03b4 is equal to \u2211 r\u2208RPATHh,L\n\u03c6M(r)\u00b7qr,D,s, whereRPATHh,L is the set of all rules of the form (2) with 1 \u2264 \u2113 \u2264 L (resp. \u2113 = 0) and head atom Rh(x, y) (resp. Rh(x, x)), \u03c6M(r) is a non-negative function of r that depends only on the parameters ofM, and qr,D,s is a vector of dimension \u03f5 such that, its t-th element is 1 (resp. 0) if the body of r is \u22a4 and t = s (resp. t \u0338= s), and otherwise it is the number of different mappings that ground the body of r in D and map x to cs and y to ct.\nConsider a DRUM model with N = 1, L = 2, \u03b2 = 1.5, a2(1, 1, 1) = 1, a2(1, 2, 2) = 1, and all other tensor elements equal to 0. Suppose contact and diagnose are the first and second predicates in the signature, respectively, and alice and flu are in positions s and t of the constant order, respectively. Equation (3) ensures that the t-th element of v2s represents the count of distinct constants c where both contact(alice, c) and diagnose(c, flu) are in the dataset. The value of \u03b2 ensures that diagnose(alice, flu) is derived if and only if alice has been in contact with at least two flu patients, indicating at least two valid paths from alice to flu.\nIn (3), the right-hand side computes a product of the one-hot vector vs and L adjacency matrices for an extended dataset D\u2032 with inverse and self-reflective facts. Each vector element corresponds to a constant ct and represents the count of paths within D\u2032 of length L from cs to ct. These paths traverse edges labeled with predicates corresponding to the adjacency matrices, following the specified sequence. Each path corresponds uniquely to a subset of D grounding of the body of a rule (2) and is associated with a weight derived from products and summations of elements of the tensor ah. The aggregation of such weights yields the expression in Lemma 1 showing that DRUM shares similarities with NEURAL-LP, where predictions depend also on the counts of rule matches. The approaches used by NEURAL-LP and DRUM to compute rule weights, however, differ significantly and it is unclear whether NEURAL-LP models can be simulated by DRUM models.\nThe aforementioned similarities allow us to derive results analogous to those obtained for NEURALLP concerning the connection between the model and the rules extracted from it. Theorem 1. Program RDRUMM,\u03b3 is sound forM = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) whenever \u03b3 \u2265 \u03b2. Furthermore, there is a DRUM modelM\u2032 = (a\u20321, \u00b7 \u00b7 \u00b7 ,a\u2032\u03b4, \u03b2\u2032) such thatRDRUMM\u2032,\u03b3 is unsound forM\u2032 for any \u03b3 < \u03b2\u2032.\nHence, by selecting a suitable value for \u03b3, it is possible to ensure the soundness of the extracted rules. Nevertheless, achieving completeness cannot be guaranteed. Theorem 2. There exists a DRUM model such that no Datalog program is faithful for it."
        },
        {
            "heading": "4 FAITHFUL RULE EXTRACTION FOR DRUM",
            "text": "Theorem 2 shows that DRUM models cannot be fully expressed using Datalog. This is unsurprising considering that DRUM models are characterised in terms of the counts of unique matches of rule bodies in datasets deriving relevant predictions. In contrast, Datalog rules derive a fact whenever there exists a match of the rule body in the data, regardless of the count of distinct relevant matches. Datalog can, however, be extended with counting (Dantsin et al., 2001) by introducing inequality atoms in rule bodies interpreted under the Unique Name Assumption (UNA), which stipulates that any two distinct constants must refer to separate entities. For instance, consider the following rules:\nR2(x, y) \u2190 R1(x, z11) \u2227R1(z11 , y) \u2227R1(x, z21) \u2227R1(z21 , y) . (5) R2(x, y) \u2190 R1(x, z11) \u2227R1(z11 , y) \u2227R1(x, z21) \u2227R1(z21 , y) \u2227 z11 \u0338\u2248 z21 . (6)\nOn dataset D1 = {R1(c1, c2), R1(c2, c3)}, rule (5) derives fact R2(c1, c3) through a match assigning x 7\u2192 c1, y 7\u2192 c3, z11 7\u2192 c2 and z21 7\u2192 c2. In contrast, rule (6) does not apply toD1 since variables z11 and z 2 1 cannot be mapped to the same constant. On dataset D2 = D1 \u222a {R1(c1, c4), R1(c4, c3)}, however, both rules derive R2(c1, c3). In particular, rule (6) admits the match x 7\u2192 c1, y 7\u2192 c3, z11 7\u2192 c2 and z21 7\u2192 c4 given that the inequality c2 \u0338\u2248 c4 holds by the UNA. In this section, we demonstrate that such extended Datalog programs can faithfully represent DRUM models. Additionally, we introduce disjunction in the rule bodies: this is a convenient extension that can (exponentially) reduce the number of rules required without increasing the expressive power of the language. Indeed, each rule with disjunction in the body is equivalent to multiple disjunctionfree rules. For example, the rule R3(x, y)\u2190 (R1(x, z1) \u2228 R2(x, z1)) \u2227 (R1(z1, y) \u2228 R2(z1, y)) is equivalent to the four rules of the formR3(x, y)\u2190 Rk1(x, z1)\u2227Rk2(z1, y), with {k1, k2} \u2286 {1, 2}. The basic building block in rule bodies is a multipath conjunction specified by a cardinality C \u2208 N and a core \u03a8 with the same structure as the rule body in Equation (2). Intuitively, a multipath conjunction consists of C copies of its core where variables other than x, y have been renamed; each such copy represents a \u201cpath\u201d. Meanwhile, inequalities are incorporated to guarantee that no two paths can be matched to the dataset in an identical manner. Thus, a multipath conjunction is satisfied if it admits at least C distinct matches of its core agreeing on the assignments of variables x and y. Definition 1. Let \u03a8 be a conjunction of length \u2113 \u2265 1 in the form of the rule body in (2). A multipath conjunction \u03d5 with core \u03a8 and cardinality C \u2208 N is of the form\n\u03d5 = C\u2227 j=1 \u03a8j \u2227 \u2227 1\u2264j<j\u2032\u2264C ( \u2113\u22121\u2228 i=1 zji \u0338\u2248 z j\u2032 i ) , (7)\nwhere \u03a8j replaces in \u03a8 each zi by z j i for 1 \u2264 i \u2264 \u2113\u2212 1. Its length is \u2113. A multipath rule of length L has head Rh(x, y) (resp. Rh(x, x)) and its body is a conjunction of P \u2265 1 (resp. P \u2265 0 and where x = y) multipath conjunctions of length \u2264 L and pairwise disjoint variables other than x and y.\nThe definition of the immediate consequence operator is standard and given by the well-known semantics of Datalog with inequalities and disjunction (Dantsin et al., 2001) under the UNA.\nCounting in multipath rules is limited by the cardinalities of their multipath conjunctions. In contrast, DRUM models face no such restrictions as the relevant counts are data-dependent and can become arbitrarily large. To address this challenge, we turn to Lemma 1 where M assigns a \u201cweight\u201d \u03c6M(r) to each rule of the form (2). There is no need to count the matches of a rule r with weight zero, as the resulting value will be multiplied by zero. Similarly, when there is at most one body atom, a single match is possible. For positive weights and rules with multiple body atoms, there is a minimum number of matches ensuring that the corresponding value in vector vhs exceeds the model\u2019s prediction threshold \u03b2, regardless of the other rules. The threshold will be met if the number of matches \u03c9(r) for r reaches \u230a \u03b2\u03c6M(r)\u230b+ 1. Thus, we can ignore rules featuring multipath conjunctions \u03d5p with cardinalities exceeding \u03c9(Rh(x, y) \u2190 \u03a8p), where \u03a8p is the core of \u03d5p; any such rule would be equivalent to the rule obtained by substituting Cp with \u03c9(Rh(x, y)\u2190 \u03a8p). This leads to a finite set of relevant multipath rules, and the only remaining challenge lies in constructing the specific subset characterising the model.\nAlgorithm 1 outlines the procedure for extracting a faithful program from a DRUM model. It begins by initializing the output program as an empty set (line 1). Additionally, it creates a list \u2126 encompassing all conjunctions serving as the body of rules in the form of (2) with length \u2113 \u2264 L, followed\nAlgorithm 1: Multipath Rule Extraction. Input: A DRUM modelM = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2), and a rule extraction threshold \u03b3. Output: A multipath program\n1 R := \u2205; 2 \u2126 := list of all conjunctions in the form of the body of (2) with 0 \u2264 \u2113 \u2264 L, ending with \u22a4; 3 foreach h \u2208 {1, \u00b7 \u00b7 \u00b7 , \u03b4} do 4 foreach [k1, \u00b7 \u00b7 \u00b7 , kL] with ki \u2208 {1, \u00b7 \u00b7 \u00b7 , 2\u03b4 + 1} do 5 [k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032\u2113] := remove all 2\u03b4 + 1; 6 foreach j \u2208 {1, \u00b7 \u00b7 \u00b7 , \u2113} do 7 if k\u2032j \u2264 \u03b4 then \u03c8j := Rk\u2032j (zj\u22121, zj); else \u03c8j := Rk\u2032j\u2212\u03b4(zj , zj\u22121); 8 if \u2113 \u2265 1 then r := Rh(x, y)\u2190 \u2227\u2113 j=1 \u03c8j ; else r := Rh(x, x)\u2190 \u22a4;\n9 if \u03c6r is undefined then \u03c6r := 0; 10 \u03c6r := \u03c6r + \u2211N i=1 \u220fL j=1 a\nh(i, j, kj); 11 foreach i \u2208 {1, \u00b7 \u00b7 \u00b7 , |\u2126| \u2212 1} do 12 ri := Rh(x, y)\u2190 \u2126(i); 13 if \u2126(i) has one atom then \u03c9i := 1; else \u03c9i := \u230a \u03b2\u03c6ri \u230b+ 1; 14 foreach [C1, \u00b7 \u00b7 \u00b7 , C|\u2126|\u22121] with Ci \u2208 {0, \u00b7 \u00b7 \u00b7 , \u03c9i} do 15 if Ci = 0 then \u03c1i := \u22a4 ; else \u03c1i := multip. conj. of core \u2126(i) and cardinality Ci; 16 if \u2211|\u2126|\u22121 i=1 Ci \u00b7 \u03c6ri > \u03b3 thenR := R\u222a {Rh(x, y)\u2190 \u2227|\u2126|\u22121 i=1 \u03c1i};\n17 if \u2211|\u2126|\u22121 i=1 Ci \u00b7 \u03c6ri + \u03c6r|\u2126| > \u03b3 thenR := R\u222a {Rh(x, x)\u2190 \u2227|\u2126|\u22121\ni=1 \u03c1i{y 7\u2192 x}}; 18 returnR;\nby \u22a4 (line 2). All elements of \u2126 except the last are possible cores of multipath conjunctions. The subsequent steps of the algorithm involve iterating over all predicates Rh in the signature (line 3). Within each iteration, it adds necessary multipath rules with head atom Rh(x, y) and Rh(x, x) to the output program. Specifically, it first computes the function \u03c6M from Lemma 1 for each rule r asRh(x, y)\u2190 \u03a8 with \u03a8 in \u2126 andRh(x, x)\u2190 \u22a4, with the results stored in \u03c6r (lines 4\u201310). Following this, for each rule r, the algorithm computes an upper bound \u03c9 on the cardinality of multipath conjunctions with core \u03a8 (lines 11\u201313). Subsequently, it enumerates all rules with head predicate Rh where the cardinality of each multipath conjunction does not exceed the computed bound for its core. This enumeration is performed by considering all combinations of cardinalities (line 14) for each core (i.e., each element of \u2126 except \u22a4) and constructing the corresponding rules for each combination (line 15). The algorithm calculates a score for each rule by summing the products of each multipath conjunction\u2019s cardinality with the weight assigned by \u03c6M to its core. The score is compared to a threshold \u03b3 \u2208 R and the rule is added if the threshold is exceeded (lines 16\u201317). The following theorem establishes the correctness and complexity of our rule extraction algorithm. Theorem 3. Program RMPM,\u03b3 extracted by Algorithm 1 on input M = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) is faithful toM for \u03b3 = \u03b2. Furthermore, Algorithm 1 terminates in O ( L \u00b7 (2\u03b4)L+1 ( N + \u03c9(2 \u03b4) L+1 )) steps,\nwhere \u03c9 is the maximum value of \u03c9(r) for r a rule of the form (2) with \u2113 \u2264 L.\nIn the example from Section 3, the extracted program would consist of the following rule of the form (7) with core contact(x, z) \u2227 diagnose(z, y) and cardinality 2:\ndiagnose(x, y)\u2190 contact(x, z1) \u2227 diagnose(z1, y) \u2227 contact(x, z2) \u2227 diagnose(z2, y) \u2227 z1 \u0338\u2248 z2 ."
        },
        {
            "heading": "5 PRACTICAL RULE EXTRACTION",
            "text": "The complexity of Algorithm 1 underscores the difficulty of extracting faithful programs from DRUM models. We next propose two approaches for addressing this challenge."
        },
        {
            "heading": "5.1 RULE EXTRACTION FOR A FIXED DATASET",
            "text": "Our first solution involves extracting a (usually small) subsetRDM of sound rules forM that explain all the model\u2019s predictions on a given datasetD. Focusing on a concrete dataset suffices in scenarios where data is not subject to frequent updates, and makes rule extraction practically feasible.\nAlgorithm 2: Multipath Rule Extraction for a Fixed Dataset. Input: A DRUM modelM = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2), and a dataset D. Output: A multipath program.\n1 R := \u2205; 2 Pnext := {[cs] | Rh(cs, ct) \u2208 TM(D)}, Pall := Pnext; 3 foreach j \u2208 [1, \u00b7 \u00b7 \u00b7 , L] do 4 Pcurrent := Pnext; 5 Pnext := \u2205 ; 6 while Pcurrent is not empty do 7 pop [\u00b7 \u00b7 \u00b7 , cs\u2032 ] from Pcurrent ; 8 foreach Rk(cs\u2032 , ct\u2032) \u2208 D do Pnext := Pnext \u222a {[\u00b7 \u00b7 \u00b7 , cs\u2032 , k, ct\u2032 ]}; 9 foreach Rk(ct\u2032 , cs\u2032) \u2208 D do Pnext := Pnext \u222a {[\u00b7 \u00b7 \u00b7 , cs\u2032 , k + \u03b4, ct\u2032 ]};\n10 Pall := Pall \u222a Pnext; 11 foreach Rh(cs, ct) \u2208 TM(D) do 12 \u03c1 := \u22a4; 13 count : \u2205 7\u2192 0; 14 foreach [cs, k1, \u00b7 \u00b7 \u00b7 , k\u2113, ct] \u2208 Pall do 15 foreach j \u2208 {1, \u00b7 \u00b7 \u00b7 , \u2113} if kj \u2264 \u03b4 then \u03c8j := Rkj (zj\u22121, zj); else Rk\u2032j\u2212\u03b4(zj , zj\u22121); 16 count( \u2227\u2113 j=1 \u03c8j) := count( \u2227\u2113 j=1 \u03c8j) + 1; 17 foreach \u03c8 : count(\u03c8) > 0 do 18 append to \u03c1 a multipath conj. of core \u03c8 and cardinality min (count(\u03c8), \u03c9(Rh(x, y)\u2190 \u03c8)); 19 if s \u0338= t then R := R\u222a {Rh(x, y)\u2190 \u03c1}; else R := R\u222a {Rh(x, x)\u2190 \u03c1{y 7\u2192 x}}; 20 returnR;\nAlgorithm 2 implements this idea. Given a modelM and a dataset D, the first part of the algorithm appliesM to D and computes all \u201cpaths\u201d Pall of length at most L in D that start from a constant cs featuring in a fact of the form Rh(cs, ct) \u2208 TM(D). The set Pall is initialised with paths of length 0 (i.e., all relevant constants). Subsequently, for each 1 \u2264 j \u2264 L, it iteratively determines the set of all paths of length j (lines 4\u201310). This is achieved by considering all paths with a length of j\u22121 (line 7) and examining all possible extensions they can undergo (lines 8\u20139). The second part of the algorithm adds a rule r toRDM for each fact Rh(cs, ct) in TM(D) so that the application of r to D derives this fact. Rule r is initialised with an empty body (line 12). Then, the algorithm counts all paths in Pall that start in cs, end in ct (line 14), and traverse the same sequence of predicates (line 15\u201316), and adds a corresponding multipath conjunction to the body of r (lines 17\u201319).\nTheorem 4. Program RDM extracted by Algorithm 2 for a DRUM model M = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) and a dataset D satisfies TRDM(D) = TM(D). Furthermore, Algorithm 2 terminates with time complexity O((2\u03b4)L \u00b7 (N \u00b7 L+ 2\u03b4 \u00b7 \u03f5L+2)).\nThe complexity of Algorithm 2 depends on the structure of D and TM(D). If their adjacency matrices are dense, the algorithm will exhibit exponential behaviour. In practice, however, the matrices are usually sparse (see Table 4). As shown in Section 6, programRDM can be efficiently computed."
        },
        {
            "heading": "5.2 FAITHFUL RULE EXTRACTION BY LIMITING MODEL EXPRESSIVITY",
            "text": "Our second approach involves simplifying DRUM models so that extracting faithful programs from them becomes practically feasible, at the expense of decreasing the expressive power of the model.\nA key source of complexity in Algorithm 1 is the need to enumerate all relevant multipath rules. In particular, the enumeration of all possible combinations of cardinalities causes an exponential blowup. This can be avoided by sacrificing the model\u2019s capacity to count distinct rule matches in the data. Instead, it goes back to Datalog, retaining only the capability to check whether the body atoms of a rule can be matched or not. This simplification can be achieved by pushing the vector (vs)\u22ba inside the product and sum operators in Equation (3)\u2014which yields an equivalent expression\u2014 and then replacing the sum over k with a max aggregation, and matrix products with matrix maxproducts. The resulting expression cannot be written in a compact form as in Equation (3), but we can write it concisely by using an inductive definition. We call the resulting model SMDRUM.\nDefinition 2. An SMDRUM model is defined as a DRUM model, but Equation (3) is replaced by (vhs ) \u22ba = \u2211N i=1 u L i , where u 0 i = (vs) \u22ba, and uji = max1\u2264k\u22642\u03b4+1 a h(i, j, k) \u00b7 uj\u22121i \u2297 Mk for 1 \u2264 j \u2264 L.\nThis restriction limits expressivity: each SMDRUM model is equivalent to a set of multipath rules where each rule has up toN multipath conjunctions without inequalities (i.e., of cardinality 1). Such rules, however, are still more expressive than those of form (2); for instance, they include rule rdiag from Section 1.They can be extracted with a simplified version of Algorithm 1 in exponential time.\nProposition 1. For an SMDRUM modelM there exists a programRSMM of inequality-free multipath rules computable in exponential time inM\u2019s size such that TM(D) = TRSMM(D) for any dataset D.\nUnfortunately, extracting a faithful program from an SMDRUM model remains challenging due to the large number of distinct combinations of N multipath conjunctions. To address this issue, we limit model expressivity further by replacing the sum over 1 \u2264 i \u2264 N by another max aggregation. Definition 3. An MMDRUM model is defined as an SMDRUM model, but where the sum over 1 \u2264 i \u2264 N is replaced by a max function.\nThe advantage of MMDRUM is that we can extract a faithful program by simply enumerating all rules of the form (2), then computing the score for each rule using a variant of expression (4),\n\u03b1M,r = max (k\u20321,\u00b7\u00b7\u00b7 ,k \u2032 L )\u2208SLr max 1\u2264i\u2264N L\u220f j=1 ah(i, j, k\u2032j) , (8)\nand then outputting those rules with score higher than a threshold \u03b3 \u2208 R. This can be achieved efficiently by taking advantage of structure-sharing between rules (see Appendix B). The downside is the further loss in expressivity, as we no longer can express commonly used rules with multiple atoms involving variables x and y in the body e.g., citizenOf(x, y)\u2190 bornIn(x, y) \u2227 livesIn(x, y). Theorem 5. For an MMDRUM model M of depth L, the program containing each rule r of the form (2) with \u2113 \u2264 L or R(x, x)\u2190 \u22a4 such that \u03b1M,r > \u03b2 is faithful forM."
        },
        {
            "heading": "6 EVALUATION",
            "text": "We evaluated MMDRUM, SMDRUM, DRUM, and NEURAL-LP (as a baseline) on the inductive KG completion task. Besides, we also applied and evaluated our rule extraction methods introduced in Section 5. All experiments were conducted on a Linux workstation with a Xeon E5-2670 CPU.\nWe followed the methodology of Yang et al. (2017) to evaluate DRUM models on KG completion tasks. We considered an inductive setting, where constants seen at test time may not have encountered during training. This is in contrast with the transductive setting, where all relevant constants occur already in the train set (Bordes et al., 2013; Sun et al., 2019).\nWe used the 13 benchmark datasets (Appendix C) for inductive KG completion by Teru et al. (2020) based on FB15k-237 (Toutanova & Chen, 2015), NELL-995 (Xiong et al., 2017), WN18RR Dettmers et al. (2018), and Family (Kok & Domingos, 2007), preserving the splits for train, validation and test. We selected L = 2 for all models and rank N = 3 for all DRUM-based models. Each model was trained up to 10 epochs. Threshold \u03b2 \u2208 (0, 1) for each model is a hyperparameter. We tried several values and picked the one maximising F1-score on validation sets.\nWe evaluated each model using precision, recall, accuracy, area under the precision-recall curve (AUPRC), and F1 score as metrics. The results are presented in Table 1. Notably, DRUM outperformed both MMDRUM and SMDRUM in terms of overall performance, demonstrating the ability to exploit higher expressive power in practice. For most FB15k-237 and NELL-995 datasets, the four models exhibited comparable performances, while for WN18RR datasets we see significant disparities. In particular, NEURAL-LP showed lower recall than all DRUM-based models, suggesting that the use of multiple \u201csub-models\u201d (N > 1) can be critical in practice, even for MMDRUM.\nWe also implemented the rule extraction algorithms in Section 5 and applied them to the inductive KG completion benchmarks. In all cases, both Algorithm 2 and the algorithm for MMDRUM (Appendix B) succeeded to compute the corresponding rule sets in less than 3 minutes, which suggest\nTop 10 rules extracted by MMDRUM In Top 10 rules of SMDRUM\nthe practical feasibility of our approach. Additionally, we verified empirically the theoretical guarantees for these algorithms provided in Theorem 4 and Theorem 5.\nWe implemented the incomplete Datalog rule extraction algorithm for DRUM described in Section 2. We computed the proportion of model predictions covered by the extracted rules for different prediction thresholds. The results in Table 2 show that the rules derive less than 7% of the facts predicted by the models, which suggests that they are insufficient to explain the predictions of the models.\nFinally, we examined the extracted rules on the Family dataset for MMDRUM and SMDRUM (the algorithm for SMDRUM is run best effort for a fixed length of time). Table 3 depicts the rules extracted from MMDRUM with the highest score. Some of these were also extracted for SMDRUM and the rankings obtained for both models are similar. We also show the highest ranked rules extracted by SMDRUM not of the form (2). Many of these are in practice irrelevant and suggest overfitting; this makes sense since the performance of MMDRUM in this dataset is close to that of SMDRUM."
        },
        {
            "heading": "7 LIMITATIONS AND FUTURE WORK",
            "text": "Rules generated by our approach may be challenging to interpret, as they contain many body atoms. Furthermore, obtaining a faithful program with inequalities for a DRUM model using Algorithm 1 can be challenging in practice. Our first priority will be to devise optimisations for Algorithm 1 that can significantly prune the search space of rules. We also aim at enhancing interpretability by devising algorithms that prioritise shorter rules. Finally, we will extend our analyses to models such as \u2202ILP (Evans & Grefenstette, 2018), and explore generalisations of DRUM to capture rules with higher-arity predicates."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "The proofs of all the lemmas, theorems, and propositions in this paper are provided in Appendix A. The datasets and source codes used in our experiments are available from the GitHub repository with documentation at https://github.com/xiaxia-wang/FaithfulRE."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the SIRIUS Centre for Scalable Data Access (Research Council of Norway, project 237889), Samsung Research UK, the EPSRC projects AnaLOG (EP/P025943/1), OASIS (EP/S032347/1), UKFIRES (EP/S019111/1), ConCur (EP/V050869/1), and the AIDA project (Alan Turing Institute)."
        },
        {
            "heading": "A PROOFS",
            "text": "Lemma 1. For a dataset D with constants c1, \u00b7 \u00b7 \u00b7 , c\u03f5, vector vhs computed by M for 1 \u2264 s \u2264 \u03f5 and 1 \u2264 h \u2264 \u03b4 is equal to \u2211 r\u2208RPATHh,L\n\u03c6M(r)\u00b7qr,D,s, whereRPATHh,L is the set of all rules of the form (2) with 1 \u2264 \u2113 \u2264 L (resp. \u2113 = 0) and head atom Rh(x, y) (resp. Rh(x, x)), \u03c6M(r) is a non-negative function of r that depends only on the parameters ofM, and qr,D,s is a vector of dimension \u03f5 such that, its t-th element is 1 (resp. 0) if the body of r is \u22a4 and t = s (resp. t \u0338= s), and otherwise it is the number of different mappings that ground the body of r in D and map x to cs and y to ct.\nProof. Let D be an arbitrary dataset with constants c1, \u00b7 \u00b7 \u00b7 , c\u03f5. The distributive and associative properties of the product and the sum allow us to rewrite Equation (3) as\n(vhs ) \u22ba = \u2211 (k1,\u00b7\u00b7\u00b7 ,kL)\u2208{1,\u00b7\u00b7\u00b7 ,2\u03b4+1}L (( N\u2211 i=1 L\u220f j=1 ah(i, j, kj) ) (vs) \u22ba \u00b7Mk1 \u00b7Mk2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7MkL ) . (9)\nIt is straightforward to see that each sequence (k1, \u00b7 \u00b7 \u00b7 , kL) of elements in {1, \u00b7 \u00b7 \u00b7 , 2\u03b4 + 1} corresponds to a rule of the form (2), namely, the unique rule with head atom Rh(x, y) (Rh(x, x) if all elements are 2\u03b4 + 1) and such that (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208 SLr . Furthermore, all sequences in SLr are over {1, \u00b7 \u00b7 \u00b7 , 2\u03b4 + 1}L and of length L. Hence, we can rewrite Equation (9) as\n(vhs ) \u22ba = \u2211 r\u2208RPATH\nh,L\n \u2211 (k1,\u00b7\u00b7\u00b7 ,kL)\u2208SLr (( N\u2211 i=1 L\u220f j=1 ah(i, j, kj) ) (vs) \u22ba \u00b7Mk1 \u00b7Mk2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7MkL ) . (10) Let D\u2032 be the extension of D with all inverse and identity facts over the extended signature, as described in Section 2. A simple inductive argument shows that the vector (vs)\u22ba \u00b7Mk1 \u00b7Mk2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 MkL describes the number of paths of length L inD\u2032 from cs to each constant. In particular, the t-th element of the vector is the number of paths of length L from cs to ct in D\u2032. Furthermore, for each rule r in RPATHh,L with \u2113 \u2265 1 and for each (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208 SLr , there is a one-to-one correspondence between each substitution that grounds the body of r mapping x to cs and y to ct, and each path of length L from cs to ct in D\u2032 through k1, k2, \u00b7 \u00b7 \u00b7 , kL; therefore, for each (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208 SLr , the vector (vs)\u22ba \u00b7Mk1 \u00b7Mk2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7MkL is always qr,D,s. Finally, for r = Rh(x, x) \u2190 \u22a4, there is a unique sequence in SLr , namely, that where all elements are 2\u03b4 + 1. For this sequence, the vector (vs)\u22ba \u00b7Mk1 \u00b7Mk2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7MkL has s-th component equal to 1 and all other components equal to 0, so it is also equal to qr,D,s. Hence, in Equation (10) we can factor out (vs)\u22ba \u00b7Mk1 \u00b7Mk2 \u00b7\u00b7 \u00b7 \u00b7\u00b7MkL , as it is equal for each (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208 SLr for a given r, and replace it by its value qr,D,s. Then the right side of the equation becomes \u2211 r\u2208RPATHh,L \u03c6M(r) \u00b7 qr,D,s, as we wanted to prove, with\n\u03c6M(r) = \u2211\n(k1,\u00b7\u00b7\u00b7 ,kL)\u2208SLr\nN\u2211 i=1 L\u220f j=1 ah(i, j, kj) . (11)\nTheorem 1. Program RDRUMM,\u03b3 is sound forM = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) whenever \u03b3 \u2265 \u03b2. Furthermore, there is a DRUM modelM\u2032 = (a\u20321, \u00b7 \u00b7 \u00b7 ,a\u2032\u03b4, \u03b2\u2032) such thatRDRUMM\u2032,\u03b3 is unsound forM\u2032 for any \u03b3 < \u03b2\u2032.\nProof. For the first claim, we consider an arbitrary fact Rh(cs, ct) \u2208 TRDRUMM,\u03b3 (D), and we show that Rh(cs, ct) \u2208 TM(D). Since Rh(cs, ct) \u2208 TRDRUMM,\u03b3 (D), R DRUM M,\u03b3 contains a rule of the form (2) where 0 \u2264 \u2113 \u2264 L, and there is a substitution \u03c3 such that x\u03c3 = cs, y\u03c3 = ct, and the grounding of the rule\u2019s body by this substitution is in D. Now, by hypothesis, \u03b3 \u2265 \u03b2. Moreover, the value \u03b1r from Equation (4) is strictly greater than \u03b3 because r \u2208 RDRUMM,\u03b3 . Furthermore, \u03c6M(r) \u2265 \u03b1r, as seen from comparing Equation (11) and Equation (4). Finally, we have vhs (t) \u2265 \u03c6M(r) from Lemma 1 and the fact that qr,D,s(t) \u2265 1 due to the existence of \u03c3. Combining all these inequalities yields vhs (t) > \u03b2, indicating Rh(cs, ct) \u2208 TM(D), as we intended to show. We prove the second claim with an example. Let D = {R1(c1, c1), R2(c2, c2)} be a dataset, and M = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) be a DRUM model in which N = L = 1, and \u03b2 = 0.5. All nonzero elements of a1, \u00b7 \u00b7 \u00b7 ,a\u03b4 are a2(1, 1, 1) = a2(1, 1, 2) = 0.5. According to Equation (3), the\nmodel does not predict the fact R2(c1, c1) since vhs (1) \u0338> \u03b2. However, for any \u03b3 < \u03b2, the program RDRUMM,\u03b3 contains the rule R2(x, y) \u2190 R1(x, y), and TRDRUMM,\u03b3 (D) contains the fact R2(c1, c1). Therefore,RDRUMM,\u03b3 is not sound forM.\nTheorem 2. There exists a DRUM model such that no Datalog program is faithful for it.\nProof. LetM = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) be the DRUM model in which N = 1, L = 2, and \u03b2 = 0.8. All non-zero elements of a1, \u00b7 \u00b7 \u00b7 ,a\u03b4 are a2(1, 1, 1) = 0.5 and a2(1, 2, 1) = 1. We show that there exists no Datalog program that is faithful forM using proof by contradiction. Assume that R is a faithful Datalog program for M, i.e., for any dataset D, TM(D) = TR(D). Consider two datasets D1 = {R1(c1, c1), R2(c3, c3)} and D2 = D1 \u222a {R1(c1, c2), R1(c2, c2)}, where constant c2 does not appear in R. According to Equation (3), the model M predicts no fact if it is applied to D1 (i.e., TM(D1) = \u2205) while it predicts the fact R2(c1, c2) if it is applied to D2 (i.e., TM(D2) = {R2(c1, c2)}). On the other hand, since R is complete for M, we have R2(c1, c2) \u2208 TR(D2). Let \u03c3 be a mapping from constants to constants with \u03c3(c2) = c1 and \u03c3(ci) = ci for any i \u0338= 2. Observe that \u03c3(D2) = D1. Since \u03c3(R2(c1, c2)) = R2(c1, c1), we have R2(c1, c2) \u2208 \u03c3(TR(D2)). It a well-known property of Datalog that for each Datalog program R\u2032 and each substitution \u00b5 that maps each constant in R\u2032 to itself, it holds that \u00b5(TR\u2032(D)) \u2286 TR\u2032(\u00b5(D)). Since constant c2 does not occur in R and \u03c3 is an identity mapping for other constants, we have \u03c3(TR(D2)) \u2286 TR(\u03c3(D2)) = TR(D1), indicating R2(c1, c2) \u2208 TR(D1). Notice that TR(D1) \u0338\u2286 TM(D1), so R is not sound for M, which contradicts our assumption that R is faithful forM. Thus, no faithful Datalog program exists forM.\nTheorem 3. Program RMPM,\u03b3 extracted by Algorithm 1 on input M = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) is faithful toM for \u03b3 = \u03b2. Furthermore, Algorithm 1 terminates in O ( L \u00b7 (2\u03b4)L+1 ( N + \u03c9(2 \u03b4) L+1 )) steps,\nwhere \u03c9 is the maximum value of \u03c9(r) for r a rule of the form (2) with \u2113 \u2264 L.\nProof. Let RMPM,\u03b2 be the output of the algorithm forM with \u03b3 = \u03b2. Let L and N be the depth and rank ofM, respectively.\n(Auxiliary Claim) We first prove an auxiliary claim: for each rule r \u2208 RPATHh,L with head atom Rh(x, y), the value \u03c6r is defined after line 9 by Algorithm 1 during the iteration of index h (line 3). Besides, \u03c6r = \u03c6M(r). Let \u03c8k1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u03c8k\u2113 be the body atoms of r, where 1 \u2264 ki \u2264 \u03b4 for each ki, and each \u03c8ki is of the form Rki(zi\u22121, zi) or Rki(zi, zi\u22121). Note that \u03c6r is only created and modified within the loop of h (line 3). Moreover, lines 4\u201310 ensure that \u03c6r = \u2211 (k\u20321,\u00b7\u00b7\u00b7 ,k\u2032L)\u2208S \u2211N i=1 \u220fL j=1 a\nh(i, j, k\u2032j), where S is the set of all sequences (k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L) with 1 \u2264 k\u2032i \u2264 2\u03b4+1. Therefore, when performing the iteration in lines 6\u20137 with index [k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L], rule r is produced in line 8, and so \u03c6r is defined. To complete the proof, by Equation (11) we need to show that S = SLr . This, however, follows straightforwardly from the definitions of S and SLr .\n(Soundness) We prove the soundness of RMPM,\u03b2 forM by considering an arbitrary dataset D and showing that TRMPM,\u03b2 (D) \u2286 TM(D). Let Rh(cs, ct) be an arbitrary fact in TRMPM,\u03b2 (D). Then, there exists a multipath rule r in RMPM,\u03b2 such that either (case 1) the body of r is \u22a4, or (case 2) the body of r is of the form \u03d51 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u03d5P for P \u2265 1, with each \u03d5p being a multipath conjunction with a distinct core (note that Algorithm 1 never outputs rules containing two multipath conjunctions with the same core (line 15\u201316)), and there exists a substitution \u03c3 from the variables in r to constants of D that grounds x to cs and y to ct such that, for each \u03d5p of the form (7), if its core is \u03a8p = \u03c8pk1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u03c8 p k\u2113p and its cardinality is Cp, then \u03c8 p ki \u03c3 \u2208 D for each 1 \u2264 i \u2264 \u2113p, and for each two 1 \u2264 j < j\u2032 \u2264 Cp, there exists 1 \u2264 i \u2264 \u2113p such that zji \u03c3 \u0338= z j\u2032 i \u03c3.\nCase 1. If the body of r is \u22a4, then r = r|\u2126|. The auxiliary claim shows that \u03c6r|\u2126| = \u03c6M(r). By Lemma 1, vhs (t) \u2265 \u03c6M(r) since \u03c6M(r\u2032) and qr\u2032,D,s are non-negative for each r\u2032 \u2208 RPATHh,L . Thus, vhs (t) \u2265 \u03c6r|\u2126| . Meanwhile, \u03c6r|\u2126| is precisely the value compared with \u03b3 in line 18, in the iteration starting from line 14 with index [C1, \u00b7 \u00b7 \u00b7 , C|\u2126|] = [0, \u00b7 \u00b7 \u00b7 , 0]. Since r \u2208 RMPM,\u03b2 , \u03c6r|\u2126| > \u03b2. Therefore, Rh(cs, ct) \u2208 TRMPM,\u03b2 (D).\nCase 2. For each 1 \u2264 p \u2264 P , if \u03c6p is of the form (7), we can use \u03c3 to produce Cp substitutions \u03c31, \u00b7 \u00b7 \u00b7 , \u03c3Cp defined as \u03c3j(z j i ) = zi\u03c3 for each 0 \u2264 i \u2264 \u2113 and 1 \u2264 j \u2264 Cp. If Cp > 1, then all substitutions are necessarily pairwise different (i.e., they differ in the assignment of at least one variable) because line 13 ensures that Cp can only be greater than 1 if \u03a8p contains at least two body atoms, and so \u2113 > 1. Thus, for each pair of j and j\u2032 satisfying 1 \u2264 j < j\u2032 \u2264 Cp, there exists 1 \u2264 i \u2264 \u2113 \u2212 1 such that zji \u03c3 \u0338= z j\u2032\ni \u03c3. Therefore, qrp,D,s(t) \u2265 Cp, for rp as the rule Rh(x, y) \u2190 \u03a8p. By Lemma 1, vhs (t) = \u2211 r\u2208RPATHh,L\n\u03c6M(r) \u00b7 qr,D,s(t). By construction, each rule rp is inRPATHh,L . Both \u03c6M(r) and qr,D,s are non-negative, so vhs (t) \u2265 \u2211 p \u03c6M(rp) \u00b7Cp if t \u0338= s\nor vhs (t) \u2265 \u2211\np \u03c6M(rp) \u00b7Cp+\u03c6M(Rh(x, x)\u2190 \u22a4) otherwise. By the auxiliary claim, this is equal to \u2211 p \u03c6rp \u00b7Cp or \u2211\np \u03c6rp \u00b7Cp+\u03c6r|\u2126| , respectively, which is precisely the value compared with \u03b3 in line 17 (if cs \u0338= ct), or line 18 (if cs = ct), in the iteration of the loop starting from line 14 with index [C \u20321, \u00b7 \u00b7 \u00b7 , C \u2032|\u2126|\u22121], where C \u2032 i = Cp if \u2126(i) = \u03a8\np and otherwise C \u2032i = 0. This list is well-defined since no two p and p\u2032 with 1 \u2264 p < p\u2032 \u2264 P correspond to the same elements of \u2126, as we already observed that each multipath conjunction in the body of r has a different core. Hence, r \u2208 RMPM,\u03b2 , and we have \u2211 p \u03c6rp \u00b7 Cp + \u03c6r|\u2126| > \u03b2. Thus, we have vhs (t) > \u03b2, indicating Rh(cs, ct) \u2208 TRMPM,\u03b2 (D).\n(Completeness) To prove completeness, we again consider an arbitrary dataset D and show that TM(D) \u2286 TRMPM,\u03b2 (D). Let Rh(cs, ct) be an arbitrary fact in TM(D). We show that there is a multipath rule in TRMPM,\u03b2 which derives the same fact from D. By Lemma 1, v h s (t) =\u2211\nr\u2208RPATHh,L \u03c6M(r) \u00b7 qr,D,s(t). Let R be the set of all rules r \u2208 RPATHh,L such that qr,D,s(t) > 0. Let r be an arbitrary such rule. By definition, if the body of r is empty, then qr,D,s(t) = 1. Otherwise, qr,D,s(t) is the number of different substitutions that ground the body of r in D mapping x to cs and y to ct. Furthermore, in the latter case, the body of r is an element of \u2126 and therefore line 13 ensures that some value \u03c9r is computed for it. We now consider two possible cases and show that completeness holds in both. In particular, in each case we construct a rule r\u2032 with headRh(x, y) and then show that, on the one hand, the body of r\u2032 can be grounded in D by a substitution that maps x to cs and y to ct. On the other hand, we show r\u2032 \u2208 RMPM,\u03b2 , and so Rh(cs, ct) \u2208 TRMPM,\u03b2 (D).\nCase 1. There exists r \u2208 R such that qr,D,s(t) > \u03c9r. This will not happen if the body of r is empty (since \u03c9r is not defined for it) or has a single atom, since in that case qr,D,s \u2264 1, but line 13 ensures \u03c9r = 1. Thus, the body of r has at least two atoms. Consider a multipath rule r\u2032 where the head is Rh(x, y) if cs \u0338= ct and otherwise it is Rh(x, x), and its body is a multipath conjunction of cardinality \u03c9r and its core is the body of r, with x = y if the head does not mention y. Since qr,D,s(t) > \u03c9r, there exist at least \u03c9r substitutions grounding the body of r in D mapping x to cs and y to ct. Since in a multipath conjunction of the form (7), each element \u03a8j shares no variables with the others other than x and y, we can take the union of those \u03c9r substitutions to obtain a new substitution \u03c3, and it is clear that this substitution grounds the body of r\u2032 inD mapping x to cs and y to ct. Now, note that there exists a unique element of \u2126 equal to the core of r\u2032 (i.e., the body of r). Consider the list [C1, \u00b7 \u00b7 \u00b7 , C|\u2126|\u22121] where Ci = \u03c9r for the unique i such that \u2126(i) is the body of r, and Ci = 0 otherwise. Consider the iteration in line 14 during the execution of the algorithm within the loop of h, with this list as an index. The value compared with \u03b3 in line 17 is \u03c6r|\u2126| + \u03c9r \u00b7 \u03c6r \u2265 \u03c9r \u00b7 \u03c6r (by the auxiliary claim and the fact that \u03c6M is non-negative). Since r has at least two body atoms, by line 13 we have \u03c9r \u00b7 \u03c6r > \u03b2, and so r \u2208 RMPM,\u03b2 .\nCase 2. There exists no r \u2208 R such that qr,D,s(t) > \u03c9r. Consider the multipath rule r\u2032 where the head is Rh(x, y) if cs \u0338= ct and otherwise it is Rh(x, x), and its body has a multipath conjunction for each element r \u2208 R with non-empty body, with core equal to the body of r (with x = y if y is not mentioned in the head) and cardinality qr,D,s(t). This is well defined by our assumption that qr,D,s(t) > 0. Therefore, for each r with non-empty body, there exist at least qr,D,s(t) different substitutions grounding the body of r in D mapping x to cs and y to ct. Since in a multipath conjunction of the form (7), each element \u03a8j shares no variables with the others other than x and y, and different multipath conjunctions in the body of r\u2032 also share no variables other than x and y, we can take the union of all those substitutions to obtain a new substitution \u03c3, and this substitution clearly grounds the body of r\u2032 in D mapping x to cs and y to ct. Let [C1, C2, \u00b7 \u00b7 \u00b7 , C|\u2126| \u2212 1] be the list which has Ci = 0 if Rh(x, y) \u2190 \u2126(i) is not in R and Ci = qr,D,s(t) if \u2126(i) is equal to the\nbody of r. This list is well-defined since each rule inR has a different body and hence there exists a unique element in \u2126 equal to its body. This list will be considered in the loop of line 14 during the execution of the algorithm, within the loop of h, because qr,D,s(t) \u2264 \u03c9r. The value compared with \u03b3 in line 17\u201318 for this iteration will be \u2211 r\u2208R qr,D,s(t) \u00b7 \u03c6r. By the auxiliary claim, this expression\nis equal to \u2211\nr\u2208R \u03c6M(r) \u00b7 qr,D,s(t). By Lemma 1 and the fact that for any r \u2208 RPATHh,L /\u2208 R, we have qr,D,s(t) = 0 (with our definition of R), we obtain that vhs (t) = \u2211 r\u2208R qr,D,s(t) \u00b7 \u03c6r. Hence, since vhs (t) > \u03b2 and our assumption that Rh(cs, ct) \u2208 TM(D), we have that line 17\u201318 ensures r\u2032 \u2208 RMPM,\u03b2 , as we intended to show.\n(Time Complexity) In line 2, each conjunction has size at most L, so the cost of this step isO(|\u2126|\u00b7 L). We next analyze the time complexity of the main loop of the algorithm, which runs \u03b4 times. The loop in line 4\u201310 considers (2\u03b4 + 1)L different lists. For each list, line 5 requires O(L) steps, and so does each iteration in line 6\u20137. The operation in line 9\u201310 requires N \u00b7 L steps, so the total cost of this part is O((2\u03b4 + 1)L \u00b7 (L+ L+ 1 +N \u00b7 L))) = O((2\u03b4)L \u00b7N \u00b7 L). The loop in line 11\u201313 is performed |\u2126| times, each of which requires a constant number of operations. The overall time cost is O(|\u2126|). For the loop in line 14\u201318, let \u03c9 = max1\u2264i\u2264|\u2126| \u03c9i. The number of possible combinations of cardinalities [C1, \u00b7 \u00b7 \u00b7 , C|\u2126|] with 0 \u2264 Ci \u2264 \u03c9i is bounded by (\u03c9 + 1)|\u2126|, which is a bound on the number of loop iterations. In each iteration, line 15\u201316 writes a rule with at most |\u2126| multipath conjunctions, where each conjunction has at most L \u00b7 \u03c9 atoms and ( \u03c9 2 ) \u00b7 L \u2264 \u03c92 \u00b7 L inequalities. Line 17\u201318 requires a number of operations linear in \u2126. Finally, the filtration in line 17\u201318 can be finished in constant time. The overall time cost of this loop is\nO ( (\u03c9 + 1)|\u2126| \u00b7 ( |\u2126| \u00b7 ( L \u00b7 \u03c9 + ( \u03c9\n2\n) \u00b7 L ) + |\u2126| )) ,\nwhich can be simplified to O ( \u03c9|\u2126|+2 \u00b7 |\u2126| \u00b7 L ) .\nTherefore, the overall time complexity of Algorithm 1 is O ( |\u2126| \u00b7 L+ \u03b4 ( (2\u03b4)L \u00b7N \u00b7 L+ |\u2126|+ \u03c9|\u2126|+2 \u00b7 |\u2126| \u00b7 L )) , (12)\nConsidering that the number of conjunctions in \u2126 for each length 0 \u2264 \u2113 \u2264 L is (2\u03b4)\u2113, we have\n|\u2126| = L\u2211\n\u2113=0\n(2\u03b4)\u2113 = L\u2211 \u2113=1 (2\u03b4)\u2113 + 1 = 2\u03b4 \u00b7\n( (2\u03b4)L \u2212 1 ) 2\u03b4 \u2212 1 + 1 = O ( (2\u03b4)L ) .\nHence, Expression (12) becomes O ( \u03b4(2\u03b4)L \u00b7N \u00b7 L+ L \u00b7 (2 \u03b4)L+1 \u00b7 \u03c9(2 \u03b4) L+2 ) = O ( L \u00b7 (2\u03b4)L+1 ( N + \u03c9(2 \u03b4) L+1 )) .\nTheorem 4. Program RDM extracted by Algorithm 2 for a DRUM model M = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) and a dataset D satisfies TRDM(D) = TM(D). Furthermore, Algorithm 2 terminates with time complexity O((2\u03b4)L \u00b7 (N \u00b7 L+ 2\u03b4 \u00b7 \u03f5L+2)).\nProof. To show TRDM(D) \u2286 TM(D), we simply point out that the rules in R D M are in RMPM,\u03b2 by construction, and so TRDM(D) \u2286 TRMPM,\u03b2 (D). Meanwhile, by Theorem 3, R MP M,\u03b2 is sound forM, so TRMPM,\u03b2 (D) \u2286 TM(D).\nTo show TM(D) \u2286 TRDM(D), we consider an arbitrary fact Rh(cs, ct) \u2208 TM(D) and then show that the rule produced during the iteration of the algorithm for this fact suffices to derive this fact in D. Let r be the rule added to RDM when Rh(cs, ct) is used as index in the iteration of loop 11. For each multipath conjunction in its body with core \u03c8, the existence of count(\u03c8) different paths from cs to ct in D and the fact that count(\u03c8) is greater or equal to the cardinality of this multipath conjunction ensure that the body of r matches the dataset mapping x to cs and y to ct. Hence the rule fires on D, and so Rh(cs, ct) \u2208 TRDM(D).\nAlgorithm 3: SMDRUM Rule Extraction. 1 R := \u2205; 2 \u2126 := list of all conjunctions in the body of the form (2), with 0 \u2264 \u2113 \u2264 L, and with no overlapping\nvariables other than x and y; 3 foreach h \u2208 {1, \u00b7 \u00b7 \u00b7 , \u03b4} do 4 foreach i \u2208 {1, \u00b7 \u00b7 \u00b7 , N} do 5 foreach [k1, \u00b7 \u00b7 \u00b7 , kL] : 1 \u2264 ki \u2264 2\u03b4+1 do 6 [k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032\u2113] := remove all 2\u03b4 +1; 7 foreach j \u2208 {1, \u00b7 \u00b7 \u00b7 , \u2113} do 8 if k\u2032j \u2264 \u03b4 then \u03c8j := Rk\u2032j (zj\u22121, zj); else \u03c8j := Rk\u2032j\u2212\u03b4(zj , zj\u22121); 9 if \u2113 \u2265 1 then r := Rh(x, y)\u2190 \u2227\u2113 j=1 \u03c8j ; else r := Rh(x, x)\u2190 \u22a4; 10 if \u03c6r,i is undefined then \u03c6r,i := 0; 11 \u03c6r,i := max(\u03c6r,i, \u220fL j=1 a\nh(i, j, kj)); 12 foreach [\u03c1e1 , \u00b7 \u00b7 \u00b7 , \u03c1en ] : 1 \u2264 e1 < \u00b7 \u00b7 \u00b7 < en \u2264 N, \u03c1ei \u2208 \u2126 do 13 if \u2211n i=1 \u03c6rei ,ei > \u03b3 then\n14 if \u22a4 /\u2208 {\u03c1e1 , \u00b7 \u00b7 \u00b7 , \u03c1en} thenR := R\u222a {Rh(x, y)\u2190 \u2227n\ni=1 \u03c1ei}; 15 elseR := R\u222a {Rh(x, x)\u2190 \u2227n i=1 \u03c1ei{y 7\u2192 x}}; 16 returnR;\n(Time Complexity) In the worst case, the number of possible paths in Pall reaches O(\u03f5L \u00b7 (2\u03b4)L), and the size of TM(D) reaches O(\u03f52 \u00b7 2\u03b4). Consider the iteration in lines 11\u201319 of Algorithm 2, each loop of a specific path [cs, k1, \u00b7 \u00b7 \u00b7 , ct] costs at most O(L) steps. Besides, to compute all the values \u03c9(r) for each possible rule r of the form (2) requires at mostO(N \u00b7L\u00b7(2\u03b4)L) steps. Therefore, the worst case time complexity of Algorithm 2 is\nO(\u03f5L \u00b7 (2\u03b4)L \u00b7 \u03f52 \u00b7 2\u03b4 +N \u00b7 L \u00b7 (2\u03b4)L) = O((2\u03b4)L \u00b7 (N \u00b7 L+ 2\u03b4 \u00b7 \u03f5L+2)) .\nProposition 1. For an SMDRUM modelM there exists a programRSMM of inequality-free multipath rules computable in exponential time inM\u2019s size such that TM(D) = TRSMM(D) for any dataset D.\nTo prove Proposition 1, we first present the algorithm extracting the relevant programRSMM fromM and then prove the faithfulness of the program and its complexity. The Proposition is then a trivial corollary from that result.\nAlgorithm 3 extracts a faithful program from an SMDRUM model. For each possible rule r of the form (2) with head atom Rh(x, y) (line 3), the algorithm first computes a score \u03c6r,i similar to the score \u03c6r from Algorithm 1, but computed separately for each sub-model (line 5\u201311) and using max instead of sum to aggregate different products of elements of ah. Then, it checks all possible combinations of n rules (which are not necessarily different) with headRh(x, y), and combines their bodies into a single rule (line 14\u201315).\nThe following result states that this algorithm extracts faithful programs from SMDRUM models. Theorem 6. The Datalog program RSMM extracted by Algorithm 3 for a SMDRUM model M = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) is faithful to M for \u03b3 = \u03b2. Furthermore, Algorithm 3 terminates in O(N \u00b7 (2\u03b4)L\u00b7N+1) steps.\nProof. (Soundness) We consider an arbitrary fact Rh(cs, ct) \u2208 TRSMM,\u03b2 (D) and show that Rh(cs, ct) \u2208 TM(D). Since Rh(cs, ct) \u2208 TRSMM,\u03b2 (D), there exists a rule r \u2208 R SM M,\u03b2 and either\n(1) it is Rh(x, x) \u2190 \u22a4 or (2) it is of the form Rh(x, y) \u2190 \u2227n\ni=1 bi, or analogously with x = y, with each bi a different element of \u2126, and a substitution \u03c3 grounding the body of this rule in D mapping x to cs and y to ct. Furthermore, the fact that r \u2208 RSMM,\u03b2 implies that \u2211n i=1 \u03c6ri,i > \u03b2, for ri = Rh(x, y)\u2190 bi. Simultaneously, we can rewrite the equation from Definition 2 as\n(vhs ) \u22ba = N\u2211 i=1  max (k1,\u00b7\u00b7\u00b7 ,kL)\u2208{1,\u00b7\u00b7\u00b7 ,2\u03b4+1}L (vs)\n\u22ba\u00b7Mk1\u2297\u00b7\u00b7\u00b7\u2297MkL=1\n L\u220f j=1 ah(i, j, kj)   , (13)\ndefining the result of max as 0 of no sequence (k1, \u00b7 \u00b7 \u00b7 , kL) satisfies the relevant conditions. For any (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208 {1, \u00b7 \u00b7 \u00b7 , 2\u03b4+1}L, the definition of the matrices Mkj implies that Mk1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 MkL(s, t) = 1 if either k1 = \u00b7 \u00b7 \u00b7 = kL = 2\u03b4 + 1, or there exists a substitution \u03c3 which grounds the body of r\u2032\u2014the unique rule such that (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208 SLr\u2032\u2014in D mapping x to cs and y to ct, or otherwise Mk1 \u2297 \u00b7 \u00b7 \u00b7 \u2297MkL(s, t) = 0. Hence,\n(vhs ) \u22ba(t) \u2265 n\u2211 i=1  L\u220f j=1 ah(ei, j, k i j)  , where (ki1, \u00b7 \u00b7 \u00b7 , kiL) is the element in SLri , (with ri = Rh(x, y) \u2190 bi if bi \u0338= \u22a4, and ri = Rh(x, x) \u2190 \u22a4 otherwise) that was used by the algorithm to produce the final value of \u03c6rei ,ei . But then, \u03c6rei ,ei = \u220fL j=1 a\nh(ei, j, k i j), and since the value compared with \u03b2 by the algorithm is\u2211n\ni=1 \u03c6rei ,ei , we have (v h s ) \u22ba(t) > \u03b2, which implies Rh(cs, ct) \u2208 TM(D).\n(Completeness) To show completeness, consider an arbitraryRh(cs, ct) \u2208 TM(D). Let e1 < \u00b7 \u00b7 \u00b7 < en be the indices 1 \u2264 i \u2264 N for which the sequence below is defined.\n(ki1, \u00b7 \u00b7 \u00b7 , kiL) = argmax (k1,\u00b7\u00b7\u00b7 ,kL)\u2208{1,\u00b7\u00b7\u00b7 ,2\u03b4+1}L Mk1\u2297\u00b7\u00b7\u00b7\u2297MkL (s,t)=1\n L\u220f j=1 ah(i, j, kj)  , (14) Let bei be the (unique) conjunction such that (k ei 1 , \u00b7 \u00b7 \u00b7 , k\nei L ) \u2208 SLrei] , with rei = Rh(x, y) \u2190 bei if bei \u0338= \u22a4, and otherwise rei = \u22a4. Consider the rule r = Rh(x, y) \u2190 \u2227n\ni=1 bei if none of the rei have a body equal to \u22a4, and otherwise r = Rh(x, x) \u2190 \u2227n i=1 bei{y 7\u2192 x}. By\nconstruction, \u220fL\nj=1 a h(ei, j, kj) = \u03c6rei ,ei , so Equation (13) and the choice of (k ei 1 , \u00b7 \u00b7 \u00b7 , k ei L ) to-\ngether ensure that (vhs ) \u22ba(t) = \u2211N i=1 \u03c6rei ,ei , and since (v h s )\n\u22ba(t) > \u03b2, then r \u2208 RSMM,\u03b2 . To see that Rh(cs, ct) \u2208 Tr(D), note simply that since one of the requirements in the equation above is Mk1 \u2297 \u00b7 \u00b7 \u00b7 \u2297MkL(s, t) = 1; thus, for any bei \u0338= \u22a4, there exists a substitution grounding bei in D mapping x to cs and y to ct. Since, by definition of \u2126, different bei share no variables other than x and y, we can take the union of these substitutions and obtain a substitution that grounds the body of r and maps x to cs and y to ct.\n(Time complexity) In Algorithm 3, there are a total of \u03b4 head atoms (line 2). In each rank i \u2208 {1, \u00b7 \u00b7 \u00b7 , N} (line 4), the number of predicate lists of length L (line 5) is:\n|{[k1, \u00b7 \u00b7 \u00b7 , kL] : 1 \u2264 ki \u2264 2\u03b4 + 1}| = (2\u03b4 + 1)L . (15)\nIn each iteration of [k1, \u00b7 \u00b7 \u00b7 , kL], the computation of \u03c6ri,i costs at most O(L) steps. Then in the combination of bodies bi, the overall number of iteration steps is O((2\u03b4 + 1)L\u00b7N ). The insertion of rules inR has a time cost of O(N). Therefore, the overall computational cost is:\n\u03b4 \u00b7 ( N \u00b7 (2\u03b4 + 1)L \u00b7 O(L) + (2\u03b4 + 1)L\u00b7N \u00b7 O(N) ) = O((2\u03b4)L\u00b7N+1) . (16)\nTheorem 5. For an MMDRUM model M of depth L, the program containing each rule r of the form (2) with \u2113 \u2264 L or R(x, x)\u2190 \u22a4 such that \u03b1M,r > \u03b2 is faithful forM.\nProof. Let RMMM,\u03b2 be the program stated in the theorem. To show soundness we consider a fact Rh(cs, ct) \u2208 TRMMM,\u03b2 (D) and show that it is in TM(D). We know that there exists a Datalog rule in RMMM,\u03b2 in the form of (2) or Rh(x, x) \u2190 \u22a4 and, if the body of this rule is not empty, a substitution \u03c3 that maps x to cs, y to ct, and grounds the body of the rule in D. Let\n(k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L) = argmax [k\u2032\u20321 ,\u00b7\u00b7\u00b7 ,k\u2032\u2032L]\u2208SLr max 1\u2264i\u2264N  L\u220f j=1 ah(i, j, k\u2032\u2032j )  .\nAlgorithm 4: MMDRUM Rule Extraction. 1 R := \u2205; 2 foreach h \u2208 {1, \u00b7 \u00b7 \u00b7 , \u03b4} and i \u2208 {1, \u00b7 \u00b7 \u00b7 , N} do 3 P := \u2205; P \u2032 := {(1, [])} ; 4 foreach j \u2208 {1, \u00b7 \u00b7 \u00b7 , L} do 5 P := P \u2032; P \u2032 := \u2205; 6 foreach (s, [p]) \u2208 P and k \u2208 {1, \u00b7 \u00b7 \u00b7 , \u03b4} do 7 if s \u00b7 ah(i, j, k) > \u03b3 then P \u2032 = P \u2032 \u222a {(s \u00b7 ah(i, j, k), [p, k])}; 8 foreach (s, [k1, .., kL]) \u2208 P \u2032 with s > \u03b3 do 9 [k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032\u2113] := remove all 2\u03b4 +1;\n10 foreach j \u2208 {1, \u00b7 \u00b7 \u00b7 , \u2113} do 11 if k\u2032j \u2264 \u03b4 then \u03c8j := Rk\u2032j (zj\u22121, zj); else \u03c8j := Rk\u2032j\u2212\u03b4(zj , zj\u22121); 12 if \u2113 \u2265 1 then r := Rh(x, y)\u2190 \u2227\u2113 j=1 \u03c8j ; else r := Rh(x, x)\u2190 \u22a4; 13 R := R\u222a {r}; 14 returnR;\nSince this rule is in RMMM,\u03b2 , we have \u220fL j=1 a h(i, j, k\u2032j) > \u03b2. Next, analogously to the case for MMDRUM, we can rewrite the equation that defines (vhs ) \u22ba in MMDRUM as\n(vhs ) \u22ba(t) = max\n1\u2264i\u2264N  max (k1,\u00b7\u00b7\u00b7 ,kL)\u2208{1,\u00b7\u00b7\u00b7 ,2\u03b4+1}L Mk1\u2297\u00b7\u00b7\u00b7\u2297MkL (s,t)=1  L\u220f j=1 ah(i, j, kj)   . (17)\nNow, note that if the body of r is not\u22a4, the existence of \u03c3 implies that Mk\u20321 \u2297\u00b7 \u00b7 \u00b7\u2297Mk\u2032L(s, t) = 1. If the body is \u22a4, then cs = ct and k\u2032i = 2\u03b4 + 1, so Mk\u20321 \u2297 \u00b7 \u00b7 \u00b7 \u2297Mk\u2032L(s, t) = 1 holds by definition of the MMDRUM model. This means that our sequence (k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L) is considered in the max computations, and so we obtain (vhs ) \u22ba(t) \u2265 \u220fL j=1 a h(i, j, k\u2032j). Thus, (v h s )\n\u22ba(t) > \u03b2 and hence Rh(cs, ct) \u2208 TM(D). To show completeness, we consider a fact Rh(cs, ct) \u2208 TM(D). Let\ni, (k1, \u00b7 \u00b7 \u00b7 , kL) = argmax 1\u2264i\u2264N\n[k\u2032\u20321 ,\u00b7\u00b7\u00b7 ,k \u2032\u2032 L]\u2208S L r\nMk1\u2297\u00b7\u00b7\u00b7\u2297MkL (s,t)=1\n L\u220f j=1 ah(i, j, k\u2032\u2032j )  . Clearly, at least one such i and sequence must exist forRh(cs, ct) to be in TM(D). By Equation (17), we have that (vhs ) \u22ba(t) = \u220fL j=1 a h(i, j, kj). Next, consider the (unique) rule r with (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208\nSLr . By our choice of i and (k1, \u00b7 \u00b7 \u00b7 , kL), we have \u03b1M,r = \u220fL j=1 a h(i, j, kj). Since Rh(cs, ct) \u2208 TM(D), we have (vhs )\u22ba(t) > \u03b2 and so \u03b1M,r > \u03b2. Thus, r \u2208 RMMM,\u03b2 , as we wanted to show. It remains to see that we can ground the body of r in D mapping x to cs and y to ct, but this follows from the definition of (k1, \u00b7 \u00b7 \u00b7 , kL), which has the condition Mk1 \u2297 \u00b7 \u00b7 \u00b7 \u2297MkL(s, t) = 1."
        },
        {
            "heading": "B EFFICIENT RULE EXTRACTION FOR MMDRUM",
            "text": "We present an optimised procedure to compute the faithful programRMMM,\u03b2 for an MMDRUM model M. The procedure is laid out in Algorithm 4. It relies on two techniques to optimise rule extraction. On the one hand, instead of first enumerating all rules and then calculating their scores, we interweave the construction of the rules with the computation of the scores. This allows us to take advantage of structure sharing. On the other hand, since the elements of ah are all between 0 and 1, computing the value of rules by multiplying different elements will result in a monotonically decreasing manner. Therefore, at each step, we can compare the value computed so far with the threshold, thus pruning the search space.\nThe following proposition ensures that the output of the algorithm corresponds to the target program.\nProposition 2. The Datalog program R extracted by Algorithm 4 for an MMDRUM model M = (a1, \u00b7 \u00b7 \u00b7 ,a\u03b4, \u03b2) is equal toRMMM,\u03b2 . Furthermore, Algorithm 4 terminates in O(N \u00b7 \u03b4L+1) steps.\nNotice that, although the time complexity given in Proposition 2 contains an exponential term \u03b4L+1, the real-world \u03b4 and L would be relatively small. Besides, the pruning in each step by threshold \u03b3 further improves the computational efficiency. The algorithm usually terminates in a short time in practice, which is also confirmed by the empirical results in Section 6.\nProof. We show the double inclusion. First, let RMMM,\u03b2 be the program from Theorem 5. If r \u2208 RMMM,\u03b2 with head predicate Rh, then there exists (k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L) \u2208 SLr and i \u2208 {1, \u00b7 \u00b7 \u00b7 , N} such that \u03b1M,r > \u03b2. Note that any prefix of factors in the expression will also be greater than \u03b2 since elements of ah(i, j, k) are in [0, 1]. But then, the execution of the algorithm ensures that for the iteration with h and i, the path (s, k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L, t) and all its subpaths are considered, and since lines 12-16 generate the rule r from (k\u20321, \u00b7 \u00b7 \u00b7 , k\u2032L), we have that r is in the output of the algorithm. Conversely, suppose that r is in the output of the algorithm, with head predicate Rh. Then we have that there exist i and (k1, \u00b7 \u00b7 \u00b7 , kL) \u2208 SLr such that in the iteration of line 2 with index i, the path (s, k1, \u00b7 \u00b7 \u00b7 , kL, t) is considered and \u220fL j=1 a h(i, j, kj) > \u03b2. But then,\nmax(k\u20321,\u00b7\u00b7\u00b7 ,k\u2032L)\u2208SLr max1\u2264i\u2264N \u220fL j=1 a h(i, j, k\u2032j) > \u03b2, and so the rule r is inRMMM,\u03b2 .\nTo prove the complexity, note that the outer loop iterates \u03b4 \u00b7N times. For each of these iterations, the number of rules that can be built is (2\u03b4 + 1)L. The remaining operations can be performed in linear time. Hence the complexity is O(\u03b4 \u00b7N \u00b7 (2\u03b4 + 1)L), which can be simplified as O(N \u00b7 \u03b4L+1)."
        },
        {
            "heading": "C EXPERIMENT DETAILS",
            "text": "This section provides additional details regarding the experimental set-up, including the used benchmarks and the configurations used for training and testing."
        },
        {
            "heading": "C.1 DATASETS",
            "text": "We used the benchmark datasets for inductive KG completion from Teru et al. (2020). Each dataset consists of train, validation and test sets, where the test set contains the same predicates but distinct constants w.r.t. the train and validation sets. To use them for the KG completion task, we further split each train set and test set into an \u201cincomplete\u201d dataset and a set of positive examples with a 3 : 1 ratio on a random basis. To evaluate the models prediction on negative examples, we randomly sampled the same number of negative facts as positive examples in the test set, and combined both positive and negative facts in the test process.\nTable 4 presents the statistics of each dataset used in our experiments. Apart from the number of facts and predicates, we also computed the in-degree and out-degree of nodes in each dataset by viewing it as a graph, where nodes are constants and edges are predicates. Compared with FB15k237 and WN18RR, NELL-995 datasets generally have smaller median in-degree and out-degree, indicating that they have relatively simpler graph structure and fewer paths within a given length constraint (e.g., L = 3).\nFamily (Kok & Domingos, 2007) is a dataset of blood relationship between individuals. It contains predicates such as sister, brother, mother, father, etc. We reused the split of train, validation and test sets in Sadeghian et al. (2019)."
        },
        {
            "heading": "C.2 SETTINGS",
            "text": "Model Implementation. The original implementation3 of DRUM by Sadeghian et al. (2019) relied on Python 2.74 and Tensorflow 1.135, which were relatively outdated and did not support most upto-date operators (e.g., the max-product of matrices). Therefore, we re-implemented the models\n3https://github.com/alisadeghian/DRUM 4https://www.python.org/download/releases/2.7/ 5https://pypi.org/project/tensorflow/1.13.1/\nwith Python 3.86 and PyTorch 2.0.17. For NEURAL-LP, we reused the source codes and settings provided in Tena Cucala et al. (2022b).\nModel Training. We followed Sadeghian et al. (2019) for all the default settings in training, such as the log-likelihood loss function, Adam optimizer and the use of 10 maximum training epochs. An early stop strategy was adopted for each model based on the prediction loss of the validation set. All experiments were conducted on a Linux workstation with a Xeon E5-2670 CPU. Our data and source codes are available at https://anonymous.4open.science/r/FaithfulRE\nRule Extraction. We conducted rule extraction for each model and benchmark. In addition to using threshold \u03b3 = \u03b2 in rule extraction, we further assessed their efficiency by comparing the time cost of rule extraction with different \u03b3.\nFor MMDRUM, we used the rule extraction threshold \u03b3 = 0.1 and 0.01, respectively. For SMDRUM, to avoid redundant body atoms brought by predicates with low scores such as Figure 1 (a), to make rule extraction feasible, and to balance the contribution of each sub-model, we required the score contributed by each sub-model to exceed a lower-bound (i.e., \u03b3\u2032 in Figure 1 (b)). We evaluated \u03b3\u2032 = 0.1 and 0.01 for SMDRUM in the experiments. Further, we combined duplicate chains of predicates learned by different sub-models in each SMDRUM rule by adding up their confidence scores and retaining the body atoms only once (as shown in Figure 1 (b)).\nFor rule extraction in DRUM, as discussed in Section 4, the relatively high complexity of Algorithm 1 makes it impractical to extract all multipath rules for a DRUM model. Instead, we used the approach from Section 5.1, extracting rules that explain the predictions on the test dataset.\n6https://www.python.org/downloads/release/python-3817/ 7https://pytorch.org/get-started/pytorch-2.0/"
        },
        {
            "heading": "D ADDITIONAL EVALUATION RESULTS",
            "text": "We provide more evaluation results below, including the training and rule extraction time of each model on the datasets, along with analyses of the extracted rules."
        },
        {
            "heading": "D.1 TRAINING TIME",
            "text": "Table 5 presents the training time of MMDRUM, SMDRUM and DRUM on the FB15k-237, NELL995, and WN18RR datasets. The three models used similar time for training, which generally increased in accordance with the number of constants and predicates in the dataset. All the models finished in a few hours for each dataset. Besides, the training time increased with the model depth L, as the number of learnable parameters (i.e., the elements of a1, \u00b7 \u00b7 \u00b7 ,a\u03b4) proportionally increased with L."
        },
        {
            "heading": "D.2 RULE EXTRACTION TIME",
            "text": "Table 6 shows the rule extraction time of each model for each dataset. Generally, all of them finished extraction in a few minutes for all the datasets, thereby demonstrating their practical viability. Besides, a lower threshold \u03b3 (or \u03b3\u2032) leads to an increase of rule extraction time, which is closely related to the increasing number of rules being extracted under \u03b3 (or \u03b3\u2032).\nNot surprisingly, MMDRUM finished extracting the faithful Datalog program within the shortest time, which is in accordance to its acceptable time complexity O(N \u00b7 \u03b4L+1). SMDRUM was run for a similar amount of time in best-effort mode. We in fact attempted to implement our faithful Datalog rule extraction for SMDRUM, Algorithm 3. However, with N = 3 and L = 3 the extraction process had not finished in 3 hours for dataset FB15k-237 V1, which suggested that extracting a faithful program for SMDRUM may not be feasible in practice.\nFor DRUM, the time cost of rule extraction (using the dataset-dependent Algorithm 2) was very influenced by the actual structure of the dataset. Observe that even though the numbers of predicted facts in different datasets were close, the NELL-995-V1 dataset required much longer time than the others (about 170 seconds for \u03b3 = 0.01 and 120 seconds for \u03b3 = 0.1). This was because the dataset contained some central constants with a fairly high in-degree or out-degree, thus leading to a large amount of paths with length \u2264 L (at most 1, 544, 589 with L = 3 in NELL-995-V1)."
        },
        {
            "heading": "D.3 COMPARISON OF RULE EXTRACTION UNDER DIFFERENT RANKS",
            "text": "We extended our analysis to assess the impact of rank N and depth L of the model to the derived rules. To facilitate the investigation, we contrasted the extracted rules from MMDRUM across varying configurations of N and L.\nTable 7 presents the highest ten rules along with their associated confidence scores for N = 1, 2 and L = 2, 3, respectively. For both L = 2 and L = 3, the algorithm extracted rules with length explicitly less than L, such as husband(x, y)\u2190 wife(y, x), which was in alignment with the analyses in Section 2. Regarding the rank N , the results suggested that a MMDRUM model with a greater rank can derive the same rules with higher confidence scores, due to the model acquiring the same rule independently in different sub-models."
        },
        {
            "heading": "D.4 ADDITIONAL EXAMPLES OF EXTRACTED RULES",
            "text": "We also evaluated the rule extraction performance of each model on other benchmark datasets from Teru et al. (2020). Table 8 shows example rules extracted by Algorithm 4 from the FB15k2378, NELL-995 and WN18RR datasets with N = 3 and L = 2. Most rules are correct and understandable. Many of them have lengths strictly smaller than L, which is in accordance with the model design of MMDRUM.\nAdditionally, we evaluate the rules extracted by Algorithm 3 on the FB15k-237, NELL-995 and WN18RR datasets with N = 3 and L = 2. Similarly to the results in Section 6 on the Family\n8For conciseness, we represent the predicates in FB15k-237 with their last parts separated by slash. For example, we write \u201clocation\u201d for \u201c/people/person/places lived./people/place lived/location\u201d\ndataset Kok & Domingos (2007), while some rules in the form of (2) can be both extracted by SMDRUM and MMDRUM, Algorithm 3 can also extract rules that are not in the form (2), as shown in Table 9. Many rules tend to have lengths smaller than L and focus on the same sequence of predicates among multiple sub-models of SMDRUM."
        }
    ],
    "title": "FAITHFUL RULE EXTRACTION FOR DIFFERENTIABLE RULE LEARNING MODELS",
    "year": 2024
}