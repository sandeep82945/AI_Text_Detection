{
    "abstractText": "We present Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022) for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree E(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ameya Daigavane"
        },
        {
            "affiliations": [],
            "name": "Song Kim"
        },
        {
            "affiliations": [],
            "name": "Mario Geiger"
        },
        {
            "affiliations": [],
            "name": "Tess Smidt"
        }
    ],
    "id": "SP:77e780a559acc60684c099d2cc9369dfedbc605c",
    "references": [
        {
            "authors": [
                "Dylan M. Anstine",
                "Olexandr Isayev"
            ],
            "title": "Generative Models as an Emerging Paradigm in the Chemical Sciences",
            "venue": "Journal of the American Chemical Society,",
            "year": 2023
        },
        {
            "authors": [
                "Simon Batzner",
                "Albert Musaelian",
                "Lixin Sun",
                "Mario Geiger",
                "Jonathan P. Mailoa",
                "Mordechai Kornbluth",
                "Nicola Molinari",
                "Tess E. Smidt",
                "Boris"
            ],
            "title": "Kozinsky. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
            "year": 2022
        },
        {
            "authors": [
                "Martin Buttenschoen",
                "Garrett M. Morris",
                "Charlotte M. Deane"
            ],
            "title": "PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ameya Daigavane",
                "Balaraman Ravindran",
                "Gaurav Aggarwal"
            ],
            "title": "Understanding Convolutions on Graphs",
            "venue": "Distill,",
            "year": 2021
        },
        {
            "authors": [
                "Niklas Gebauer",
                "Michael Gastegger",
                "Kristof Sch\u00fctt"
            ],
            "title": "Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Niklas W.A. Gebauer",
                "Michael Gastegger",
                "Stefaan S.P. Hessmann",
                "Klaus-Robert M\u00fcller",
                "Kristof T. Sch\u00fctt"
            ],
            "title": "Inverse design of 3d molecular structures with conditional generative neural networks",
            "venue": "URL https://doi.org/10.1038/s41467-022-28526-y",
            "year": 2022
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M. Borgwardt",
                "Malte J. Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander Smola"
            ],
            "title": "A Kernel Two-Sample Test",
            "venue": "Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Victor Garcia Satorras",
                "Cl\u00e9ment Vignac",
                "Max Welling"
            ],
            "title": "Equivariant Diffusion for Molecule Generation",
            "year": 2022
        },
        {
            "authors": [
                "Yeonjoon Kim",
                "Woo Youn Kim"
            ],
            "title": "Universal Structure Conversion Method for Organic Molecules: From Atomic Connectivity to Three-Dimensional Geometry",
            "venue": "Bulletin of the Korean Chemical Society,",
            "year": 2015
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Greg Landrum",
                "Paolo Tosco",
                "Brian Kelley",
                "Ric",
                "David Cosgrove",
                "sriniker",
                "gedeck",
                "Riccardo Vianello",
                "Nadine Schneider",
                "Eisuke Kawashima",
                "Dan N",
                "Gareth Jones",
                "Andrew Dalke",
                "Brian Cole",
                "Matt Swain",
                "Samo Turk",
                "Alexander Savelyev",
                "Alain Vaucher",
                "Maciej W\u00f3jcikowski",
                "Ichiru Take",
                "Daniel Probst",
                "Kazuya Ujihara",
                "Vincent F. Scalfani",
                "guillaume godin",
                "Juuso Lehtivarjo",
                "Axel Pahl",
                "Rachel Walker",
                "Francois Berenger"
            ],
            "title": "jasondbiggs, and strets123",
            "venue": "rdkit/rdkit:",
            "year": 2023
        },
        {
            "authors": [
                "Youzhi Luo",
                "Shuiwang Ji"
            ],
            "title": "An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Cameron J. Owen",
                "Steven B. Torrisi",
                "Yu Xie",
                "Simon Batzner",
                "Jennifer Coulter",
                "Albert Musaelian",
                "Lixin Sun",
                "Boris Kozinsky"
            ],
            "title": "Complexity of Many-Body Interactions in Transition Metals via Machine-Learned Force Fields from the TM23",
            "venue": "Data Set,",
            "year": 2023
        },
        {
            "authors": [
                "Sergey N. Pozdnyakov",
                "Michele Ceriotti"
            ],
            "title": "Incompleteness of graph neural networks for points clouds in three dimensions, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Bharath Ramsundar",
                "Peter Eastman",
                "Patrick Walters",
                "Vijay Pande",
                "Karl Leswing",
                "Zhenqin Wu"
            ],
            "title": "Deep Learning for the Life Sciences",
            "venue": "O\u2019Reilly Media,",
            "year": 2019
        },
        {
            "authors": [
                "A.K. Rappe",
                "C.J. Casewit",
                "K.S. Colwell",
                "W.A. III Goddard",
                "W.M. Skiff"
            ],
            "title": "UFF, a full periodic table force field for molecular mechanics and molecular dynamics simulations",
            "venue": "Journal of the American Chemical Society, 114(25):10024\u201310035,",
            "year": 1992
        },
        {
            "authors": [
                "Sereina Riniker",
                "Gregory A. Landrum"
            ],
            "title": "Better Informed Distance Geometry: Using What We Know To Improve Conformation Generation",
            "venue": "Journal of Chemical Information and Modeling,",
            "year": 2015
        },
        {
            "authors": [
                "M. Rupp",
                "A. Tkatchenko",
                "K.-R. M\u00fcller",
                "O.A. von Lilienfeld"
            ],
            "title": "Fast and accurate modeling of molecular atomization energies with machine learning",
            "venue": "Physical Review Letters,",
            "year": 2012
        },
        {
            "authors": [
                "Benjamin Sanchez-Lengeling",
                "Emily Reif",
                "Adam Pearce",
                "Alexander B. Wiltschko"
            ],
            "title": "A Gentle Introduction to Graph",
            "venue": "Neural Networks. Distill,",
            "year": 2021
        },
        {
            "authors": [
                "Kristof T. Sch\u00fctt",
                "Pieter-Jan Kindermans",
                "Huziel E. Sauceda",
                "Stefan Chmiela",
                "Alexandre Tkatchenko",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Gregor Simm",
                "Robert Pinsler",
                "Jose Miguel Hernandez-Lobato"
            ],
            "title": "Reinforcement learning for molecular design guided by quantum mechanics",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Gregor N.C. Simm",
                "Robert Pinsler",
                "G\u00e1bor Cs\u00e1nyi",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Symmetryaware actor-critic for 3d molecular design",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Martin Uhrin"
            ],
            "title": "Through the eyes of a descriptor: Constructing complete, invertible descriptions of atomic environments",
            "venue": "Phys. Rev. B,",
            "year": 2021
        },
        {
            "authors": [
                "Clement Vignac",
                "Nagham Osman",
                "Laura Toni",
                "Pascal Frossard"
            ],
            "title": "MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation, 2023",
            "year": 2023
        },
        {
            "authors": [],
            "title": "DETAILS OF MODELS C.1 EMBEDDERS Here, we describe E3SchNet and NequIP (Batzner et al., 2022) which we use to embed the atoms in each fragment into E(3)-equivariant features",
            "venue": "As shown in Appendix B,",
            "year": 2022
        },
        {
            "authors": [
                "Kingma",
                "Ba"
            ],
            "title": "2017) optimizer with a learning rate of 5 \u00d7 10\u22124. We chose the parameters that achieved the lowest loss on the validation set over 8000000 training steps with a batch size of 16 fragments",
            "venue": "DATA DETAILS Following EDM (Hoogeboom et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Simm"
            ],
            "title": "2021), which uses rejection sampling with a uniform base distribution. We perform some quantitative experiments with the parametrization of Simm et al",
            "venue": "E DETAILS OF METRICS",
            "year": 2021
        },
        {
            "authors": [
                "Simm"
            ],
            "title": "f(\u03b8, \u03c6) This corresponds to a simpler setting where we have only one radius r. We assess the KL divergence as a function of number of position channels ch and lmax in Figure 10. We see a consistent improvement across different lmax as the number of position channels",
            "year": 2021
        },
        {
            "authors": [
                "Simm"
            ],
            "title": "regularize the distribution so that it does not approach a delta function\u201d. In the left panel of Figure 11, we show that this regularization hurts the model. Even adding multiple channels does not help, because the regularization term \u2018switches\u2019 off multiples channels",
            "year": 2021
        },
        {
            "authors": [
                "Simm"
            ],
            "title": "Parametrization without Regularization Position Channels",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In silico generation of atomic systems with diverse geometries and desirable properties is important to many areas including fundamental science, materials design, and drug discovery (Anstine & Isayev, 2023). The direct enumeration and validation of all possible 3D structures is computationally infeasible and does not in itself lead to useful representations of atomic systems for guiding understanding or design. Thus, there is interest in \u2018generative models\u2019 that can generate 3D molecular structures using machine learning algorithms.\nEffective generative models of atomic systems must learn to represent and produce highly-correlated geometries that represent chemically valid and energetically favorable configurations. To do this, they must overcome several challenges:\n1. The validity of an atomic system is ultimately determined by quantum mechanics. Generative models of atomic systems are trained on 3D structures relaxed through computationallyintensive quantum mechanical calculations. These models must learn to adhere to chemical rules, generating stable molecular structures based solely on examples.\n2. The stability of atomic systems hinges on the precise placement of individual atoms. The omission or misplacement of a single atom can result in significant property changes and instability.\n3. Atomic systems have inherent symmetries. Atoms of the same element are indistinguishable, so there is no consistent way to order atoms within an atomic system. Additionally, atomic systems lack unique coordinate systems (global symmetry) and recurring geometric patterns occur in a variety of locations and orientations (local symmetry).\nTaking these challenges into consideration, the majority of generative models for atomic systems operate on point geometries and use permutation and Euclidean symmetry-invariant or equivariant methods. Thus far, two approaches have been emerged as effective for directly generating general 3D geometries of molecular systems: autoregressive models (Gebauer et al., 2019; 2022; Luo & Ji, 2022; Simm et al., 2020; 2021) and diffusion models (Hoogeboom et al., 2022).\n\u2217Work performed when at Massachusetts Institute of Technology.\nIn this work, we introduce Symphony, an autoregressive generative model that uses higher-degree equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. Similar to other autoregressive models, Symphony builds molecules sequentially by predicting and sampling atom types and locations of new atoms based on conditional probability distributions informed by previously placed atoms. However, Symphony stands out by using spherical harmonic projections to parameterize the distribution of new atom locations. This approach enables predictions to be made using features from a single \u2018focus\u2019 atom, which serves as the chosen origin for that step of the generation process. It allows for the simultaneous prediction of the radial and angular distribution of possible atomic positions in a direct manner without needing to use additional atoms.\nTo test our proposed architecture, we apply Symphony to the QM9 dataset and show that it outperforms previous autoregressive models and is competitive with existing diffusion models on a variety of metrics. We additionally introduce a metric based on the bispectrum for assessing the angular accuracy of matching generated local environments to similar environments in training sets. Finally, we demonstrate that Symphony can generate valid molecules at a high success rate, even when conditioned on unseen molecular fragments."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "E(3)-Equivariant Features: We say a E(3)-equivariant feature z \u2208 R2l+1 transforms as the irreducible representation l under rotation R and translation T:\nz R,T\u2212\u2212\u2212\u2192 Dl(R)T z\nwhere Dl is the irreducible representation of SO(3) of degree 2l + 1. Dl(R) \u2208 R(2l+1)\u00d7(2l+1) is referred to as the Wigner D-matrix of the rotation R. As D0(R) = 1 and D1(R) = R, invariant \u2018scalar\u2019 features correspond to degree l = 0 features, while \u2018vector\u2019 features correspond to l = 1 features. Note that these features are invariant under translation T.\nSpherical Harmonics: The real spherical harmonics Yl,m(\u03b8, \u03d5) are a set of real-valued orthogonal functions defined on the sphere S2, indexed by two integers l and m such that l \u2265 0, |m| \u2264 l. Here \u03b8 and \u03d5 come from the notation for spherical coordinates, where r is the distance from an origin, \u03b8 \u2208 [0, \u03c0] is the polar angle and \u03d5 \u2208 [0, 2\u03c0) is the azimuthal angle. The relation between Cartesian and spherical coordinates is given by: x = r sin \u03b8 cos\u03d5, y = r sin \u03b8 sin\u03d5, z = r cos \u03b8.\nl corresponds to an angular frequency: the higher the l, the more rapidly Yl,m changes over S2. This can intuitively be seen by looking at the functional form of the spherical harmonics. In their\nCartesian form, the spherical harmonics are proportional to simple polynomials. In one common choice of basis, l = 0 is proportional to 1, l = 1 is proportional to (x, y, z) and l = 2 is proportional to (xy, yz, 2z2 \u2212 x2 \u2212 y2, zx, x2 \u2212 y2), as seen in Figure 3D-F. One important property of the spherical harmonics is that they can be used to create E(3)-equivariant features. Let Yl(\u03b8, \u03d5) = [Yl,\u2212l(\u03b8, \u03d5), . . . , Yl,l(\u03b8, \u03d5)] \u2208 R2l+1 represent the collection of all spherical harmonics with the same l. Then, Yl(\u03b8, \u03d5) transforms as an E(3)-equivariant feature of degree l under rotation: Yl(R(\u03b8, \u03d5)) = Dl(R)TYl(\u03b8, \u03d5), where R is an arbitrary rotation, and (\u03b8, \u03d5) is interpreted as the coordinates of a point on S2.\nThe second important property of the spherical harmonics that we employ is the fact that they form an orthonormal basis for functions on the sphere S2. Thus, for any function f : S2 \u2192 R, we can express f as a linear combination of the Yl,m. Formally, there exists unique coefficients cl \u2208 R2l+1 for each l \u2208 N, such that f(\u03b8, \u03d5) = \u2211\u221e l=0 cl\nTYl(\u03b8, \u03d5). We term these coefficients cl as the spherical harmonic coefficients of f as they are obtained by projecting f onto the spherical harmonics."
        },
        {
            "heading": "3 METHODS",
            "text": "We first describe Symphony, our autoregressive model for 3D molecular structures, with a comparison to prior work in Section 3.6."
        },
        {
            "heading": "3.1 BUILDING MOLECULES VIA SEQUENCES OF FRAGMENTS",
            "text": "First, we create sequences of fragments using molecules from the training set via CREATEFRAGMENTSEQUENCE. Given a molecule M and random seed r, CREATEFRAGMENTSEQUENCE constructs a sequence of increasingly larger fragments {S1, . . .S |M|} such that |Sn| = n for all n \u2208 {1, . . . , |M|} and S |M| = M exactly. Of course, there are many ways to create such sequences of fragments; CREATEFRAGMENTSEQUENCE simply builds a molecule via a minimum spanning tree.\nSymphony attempts to recreate this sequence step-by-step, learning the (probabilistic) mapping Sn \u2192 Sn+1. In particular, we ask Symphony to predict the focus node fn+1, the target atomic number Zn+1 and the target position r\u20d7n+1, providing feedback at every step."
        },
        {
            "heading": "3.2 HANDLING THE SYMMETRIES OF FRAGMENTS",
            "text": "Here, we highlight several challenges that arise because Sn must be treated as an unordered set of atoms that live in 3D space. In particular, let RSn + T = {(Rr\u20d71 + T, Z1), . . . , (Rr\u20d7n + T, Zn)} be the description of the same set of atoms in Sn but in a coordinate frame rotated by R\u22121 and translated by T\u22121. Similarly, let \u03c0 be any permutation of {1, . . . , n} and \u03c0Sn = {(\u20d7r\u03c0(1), Z\u03c0(1)), . . . , (\u20d7r\u03c0(n), Z\u03c0(n))}. Fundamentally, RSn +T, Sn and \u03c0Sn all represent the same set of atoms. Thus, we would like Symphony to naturally accommodate the symmetries of fragment Sn, under the group E(3) of Euclidean transformations consisting of all rotations R and translations T, and the group of all permutations of constituent atoms. Formally, we wish to have:\n\u2022 Property (1): The focus distribution pfocus and the target species distribution pspecies should be E(3)-invariant:\npfocus(fn+1;RSn +T) = pfocus(fn+1;Sn) (1) pspecies(Zn+1 | fn+1;RSn +T) = pspecies(Zn+1 | fn+1;Sn) (2)\n\u2022 Property (2): The target position distribution pposition should be E(3)-equivariant:\npposition(Rr\u20d7n+1 +T | fn+1, Zn+1;RSn +T) = pposition(\u20d7rn+1 | fn+1, Zn+1;Sn) (3)\n\u2022 Property (3): With respect to the ordering of atoms in Sn, the map pfocus should be permutation-equivariant while pspecies and pposition should be permutation-invariant.\nWe represent pfocus, pspecies and pposition as probability distributions because there may be multiple valid choices of focus fn+1, species Zn+1 and position r\u20d7n+1."
        },
        {
            "heading": "3.3 THE DESIGN OF SYMPHONY",
            "text": "The overall working of Symphony is shown graphically in Figure 1. Symphony first computes atom embeddings via an EMBEDDER. Here, we assume that EMBEDDER(Sn) = {hv,l | v \u2208 Sn, 0 \u2264 l \u2264 lmax} returns a set of E(3)-equivariant features hv, l of degree l upto a predefined degree lmax, for each atom v in Sn. In Theorem B.1, we show that Symphony can guarantee Properties (1), (2) and (3) as long as EMBEDDER is permutation-equivariant and E(3)-equivariant. We provide further details about EMBEDDER in Appendix C.\nFrom Property (1), pfocus and pspecies should be invariant under rotation and translations of Sn. Since the atom types and the atom indices are discrete sets, we can represent both of these distributions as a vector of probabilities. Thus, we compute pfocus and pspecies by applying a multi-layer perceptron MLP on only the rotation and translation invariant features of EMBEDDER(Sn):\npfocus(fn+1;Sn) = MLP(EMBEDDER(Sn)fn+1,0) (4) pspecies(Zn+1 | fn+1;Sn) = MLP(EMBEDATOMTYPE(Zn+1) \u00b7 EMBEDDER(Sn)fn+1,0) (5)\nAlongside the node-wise probabilities for pfocus, we also predict a global STOP probability, indicating that no atom should be added.\nOn the other hand, Property (2) shows that pposition transforms non-identically under rotations and translations. We describe a novel parametrization of 3D probability densities such as pposition with spherical harmonic projections.\nThe position r\u20d7 is represented by spherical coordinates (r, \u03b8, \u03d5) where r is the distance from the focus f , \u03b8 is the polar angle and \u03d5 is the azimuthal angle. Any probability distribution pposition over positions must satisfy the normalization and non-negativity constraints: \u222b \u2126 pposition(r, \u03b8, \u03d5) dV = 1 and pposition(r, \u03b8, \u03d5) \u2265 0 where dV = rdr sin \u03b8d\u03b8d\u03d5 is the volume element and \u2126 = {r \u2208 [0,\u221e), \u03b8 \u2208 [0, \u03c0], \u03d5 \u2208 [0, 2\u03c0)} represents all space in spherical coordinates. Since these constraints are hard to incorporate directly into a neural network, we predict the unnormalized logits f position(r, \u03b8, \u03d5) instead, and take the softmax over all space: pposition(r, \u03b8, \u03d5) = 1Z exp f\nposition(r, \u03b8, \u03d5) To model these logits, we first discretize the radial component r into a set of discrete values. We choose 64 uniformly spaced values from 0.9A to 2.0A, which covers all of the bond lengths in QM9. For each fixed value of r, we obtain a function on the sphere S2, which we represent in the basis of\nspherical harmonic functions Yl,m(\u03b8, \u03d5), as described in Section 2 and similar to the construction of Cohen & Welling (2015). As we have a radial component r here, the coefficients cl also depend on r:\nf position(r, \u03b8, \u03d5 | fn+1, Zn+1;Sn) = \u221e\u2211 l=0 cl(r; fn+1, Zn+1,Sn)TYl(\u03b8, \u03d5)\nSymphony predicts these coefficients cl from the degree l features of the focus node EMBEDDER(Sn)fn+1,l, and the embedding of the target species Zn+1:\ncl(r; fn+1, Zn+1,Sn) = LINEAR(EMBEDDER(Sn)fn+1,l \u2297 EMBEDATOMTYPE(Zn+1)) (6)\nBy explicitly modelling the probability distributions pfocus, pspecies and pposition, Symphony learns to represent all possible options of completing Sn into a valid molecule."
        },
        {
            "heading": "3.4 BYPASSING THE ANGULAR FREQUENCY BOTTLENECK",
            "text": "For computational reasons, we are often limited to using a finite number of spherical harmonic projections (ie, up to some lmax). Due to the way the spherical harmonics are constructed, this means we can only represent signals upto some angular frequency. For example, to represent a signal on the sphere with peaks separated by d radians, we need spherical harmonic projections with lmax \u2265 2\u03c0d . This is similar to issues faced when using the first few terms of the Fourier series; we cannot represent high frequency components. To bypass the bottleneck of angular frequency, we propose using multiple channels of spherical harmonic projections, which are then summed over after a non-linearity: f position(r, \u03b8, \u03d5;Sn) = log \u2211 channel ch exp \u2211\u221e l=0 c ch l (r;Sn)TYl(\u03b8, \u03d5). See Appendix F for a concrete example where adding multiple channels effectively increases the angular frequency capacity of our model. For Symphony, we find that 2 channels is sufficient, as demonstrated in"
        },
        {
            "heading": "3.5 TRAINING AND INFERENCE",
            "text": "We utilize teacher forcing to train Symphony. At training time, the true focus fn+1 and atomic number Zn+1 are provided as computed in NEXTFRAGMENT. Thus, no sampling occurs at training time. The true probability distributions qfocus and qspecies are computed empirically from the set of unfinished atoms and their corresponding neighbors inM. The true probability distribution qposition is computed by smoothly approximating a Dirac delta distribution upto some cutoff frequency lmax\nat the target position r\u20d7n+1 around the focus atom. Further details about the training process and representing Dirac delta distributions are provided in Section C.2 and Appendix H.\nqposition(\u20d7r) = 1\nZ exp\n( \u2212\u2225\u20d7r\u2225 \u2212 \u2225\u20d7rn+1\u2225\n2\u03c32true \u00b7 \u03b4lmax (r\u0302\u2212 r\u0302n+1)\n) (7)\nAt inference time, both the focus fn+1 and atomic number Zn+1 are sampled from pfocus(\u00b7;Sn) and pspecies(\u00b7|fn+1;Sn) respectively. These are used to sample r\u20d7n+1 from pposition(\u00b7|fn+1, Zn+1;Sn). Molecules are generated by starting from an initial fragment S1, and repeatedly sampling from pfocus, pspecies and pposition until a STOP is predicted or Nmax = 35 iterations have occurred.1 We set S1 as a single hydrogen atom at the origin."
        },
        {
            "heading": "3.6 RELATION TO PRIOR WORK",
            "text": "Most methods for 3D molecular structure generation fall into one of two broad categories: autoregressive and end-to-end models. G-SchNet (Gebauer et al., 2019; 2022) and G-SphereNet (Luo & Ji, 2022) were the first successful attempts at autoregressive generation of molecular structures.\nG-SchNet uses the SchNet framework (Schu\u0308tt et al., 2017) to perform message-passing with rotationally invariant features and compute node embeddings. A focus node is then selected as the center of a 3D grid. All of the atoms in the current fragment then vote on where to place the next atom within this grid by specifying a radial distance to the next atom. Because of the use of only rotationally invariant features, at least three atoms are needed to be present in the current fragment to specify the exact position of the next atom without any degeneracy due to symmetry; this procedure is called triangulation. This requires several additional tokens to break symmetry. Similarly, G-SphereNet learns a normalizing flow to perform a triangulation procedure once there are atleast 3 atoms in Sn. We wish to highlight two observations that guided the development of Symphony:\n\u2022 Rotationally invariant features centered at a single point cannot capture the orientations of geometrical motifs (Pozdnyakov & Ceriotti, 2022). To handle the degeneracies inherent when using rotationally invariant features to predict positions, G-SchNet uses unphysical auxiliary tokens (which are multiple spatial positions that are not atoms) to break symmetry.\n\u2022 G-SchNet queries all of the atoms in Sn at each iteration, which means distant atoms can have an undue influence when placing the next atom. Similarly, G- SphereNet predictions are not a smooth function of the input fragment; when the input is perturbed slightly, the choice of atoms used in the triangulation procedure can change drastically.\nRecently, E(3)-equivariant neural networks that build higher-degree E(3)-equivariant features have demonstrated improved performance on a wide range of atomistic tasks (Batzner et al., 2022; Geiger & Smidt, 2022; Owen et al., 2023). Our key contribution is to show the benefit of higher-degree E(3)equivariant features for the molecular generation task allowing for a novel parametrization of 3D probability distributions using spherical harmonic projections. Simm et al. (2021) also uses spherical harmonic projections with a single channel for molecule generation, but trained with reinforcement learning. Their parametrization and sampling of the distribution differs from ours; we discuss these details in Appendix D.\nAmong end-to-end generation methods, Hoogeboom et al. (2022) developed EDM, a state-of-the-art E(3)-equivariant diffusion model. EDM significantly outperformed the previously proposed E(3)equivariant normalizing flow (ENF) model for molecule generation (Satorras et al., 2022a). EDM learns to gradually denoise a initial configuration of atoms into a valid molecular structure. Both EDM and ENF are built on the E(n)-Equivariant Graph Neural Networks (Satorras et al., 2022b) framework which can utilize only scalar and vector features (and interactions between them). A recent work (Vignac et al., 2023) improves EDM by utilizing bond order information (and hence, a 2D molecular graph to compare to), which we do not assume access to here. While expressive, diffusion models are expensive to train, requiring \u2248 3.5\u00d7 more training on the QM9 dataset to outperform autoregressive models. Unlike autoregressive models, diffusion models do not flexibly allow for completion of molecular fragments, because they are usually trained in setups where all atoms are free to move. To avoid recomputation of the neighbor lists during diffusion, current diffusion models\n1Nmax was set as 35 based on the maximum size of molecules in the QM9 dataset as 30 atoms.\nuse fully-connected graphs where all atoms interact with each other. This could potentially affect their scalability when building larger molecules. On the other hand, Symphony and other autoregressive models use distance cutoffs to restrict interactions and improve efficiency. Furthermore, diffusion models are significantly slower to sample from, because the underlying neural network is invoked \u2248 1000 times when sampling a single molecule."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "A major challenge with generative modelling is evaluating the quality of generated 3D structures. Ideally, a generative model should generate physically plausible structures, accurately capture training set statistics and generalize well to molecules outside of its training set. We propose a comprehensive set of tests to evaluate Symphony and other generative models along these three aspects."
        },
        {
            "heading": "4.1 VALIDITY OF GENERATED STRUCTURES",
            "text": "All of the generative models considered here output a set of atoms with 3D coordinates; bonding information is not generated by the model. Before we can use cheminformatics tools designed for molecules, we need to assign bonds between atoms. Multiple algorithms exist for bond order assignment: xyz2mol (Kim & Kim, 2015), OpenBabel (Banck et al., 2011) and a simple lookup table based on empirical pairwise distances in organic compounds (Hoogeboom et al., 2022). Here, we perform the first comparison between these algorithms for evaluating machine-learning generated 3D structures. In Table 1, we use each of these algorithms to infer the bonds and create a molecule from generated 3D molecular structure. We declare a molecule as valid if the algorithm could successfully assign bond order with no net resulting charge. We also measure the uniqueness to see how many repetitions were present in the set of SMILES (Weininger, 1988) strings of valid generated molecules. Ideally, we want both the validity and the uniqueness to be high.\nWhile EDM (Hoogeboom et al., 2022) is still superior on the validity and uniqueness metrics, we find that Symphony performs much better on both validity and uniqueness than existing autoregressive models, G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022), for the xyz2mol and OpenBabel algorithms. Note that the lookup table does not account for aromatic bonds and is quite sensitive to exact bond lengths; we believe this penalizes Symphony due to its coarser discretization compared to EDM and G-SchNet. Of note is that only xyz2mol finds almost all of the ground truth QM9 structures to be valid.\nRecently, Buttenschoen et al. (2023) showed that the predicted 3D structures from machine-learned protein-ligand docking models tend to be highly unphysical. For Table 2, we utilize their PoseBusters framework to perform the following sanity checks to count how many of the predicted 3D structures are reasonable. We see that the valid molecules from all models tend to be quite reasonable, with Symphony performing better than all baselines on generating structures with reasonable UFF (Rappe et al., 1992) energies and respecting the geometry constraints of double bonds. Further details about the PoseBusters tests are provided in Section E.1."
        },
        {
            "heading": "4.2 CAPTURING TRAINING SET STATISTICS",
            "text": "Next, we evaluate models on how well they capture bonding patterns and the geometry of local environments found in the training set molecules. In previous work (Luo & Ji, 2022; Hoogeboom et al., 2022), models were compared based on how well they capture the true bond length distributions\nobserved in QM9. However, such statistics only deal with pairwise bond lengths and cannot capture the geometry of how atoms are placed relative to each other. Here, we utilize the bispectrum (Uhrin, 2021) as a rotationally invariant descriptor of the geometry of local environments. Given a local environment with a central atom u, we first project all of the neighbors of u according to the inferred bonds onto the unit sphere S2. Then, we compute the signal f as a sum of Dirac delta distributions along the direction of each neighbor: f(r\u0302) = \u2211 v\u2208N(u) \u03b4lmax (r\u0302\u2212 r\u0302vu). The bispectrum B(f) of f is then defined as: B(f) = EXTRACTSCALARS(f \u2297 f \u2297 f). Thus, f captures the distribution of atoms around u, and the bispectrum B(f) captures the geometry of this distribution. The advantage of the bispectrum is that it varies smoothly when f is varied and is guaranteed to be rotationally invariant. We compute the bispectrum of local environments with atleast 2 neighboring atoms. Note that we exclude the pseudoscalars in the bispectra.\nFor comparing discrete distributions, we use the symmetric Jensen-Shannon divergence (JSD) as employed in Hoogeboom et al. (2022). Given the true distribution Q and the predicted distribution P , the Jensen-Shannon divergence between them is defined as: DJS(Q \u2225P ) = 1 2DKL (Q \u2225M) + 1 2DKL (P \u2225M) where DKL is the Kullback\u2013Leibler divergence and M = Q+P 2 is the mean distribution. For continuous distributions, estimating the Jensen-Shannon divergence from samples is tricky without further assumptions on the distributions. Instead, we use the Maximum Mean Discrepancy (MMD) score from Luo & Ji (2022) instead to compare samples from continuous distributions. The MMD score is the distance between means of features computed from samples from the true distribution Q and the predicted distribution P . A model with a smaller MMD score captures the true distribution of samples better. We provide details about the MMD score in Section E.2.\nFrom Table 3 we see that Symphony and other autoregressive models struggle to match the bond length distribution of QM9 as well as EDM. This is the case except for the single C-H and single N-H bonds. On the bispectra, however, Symphony attains the lowest MMD for several environments. To gain some intuition for these MMD numbers, we also plotted the bond length distributions, samples of the bispectra, atom type distributions and other statistics in Appendix A for each model."
        },
        {
            "heading": "4.3 GENERALIZATION CAPABILITIES",
            "text": "All of the metrics discussed so far can be maximized by simply memorizing the training set molecules. Now, we propose a new metric to evaluate how well the models have actually learned to generate valid chemical structures. We compare models by asking them to complete fragments of 1000 unseen molecules from the test set, with one hydrogen atom removed. We then check how many final molecules were deemed valid. Since the valid completion rate (VCR) depends heavily on the quality of the model, we compute the valid completion rate for fragments of molecules from the training set as well. If the performance is significantly different between the two sets of fragments, this indicates that the models do not generalize well. Diffusion models such as EDM are more challenging to evaluate for this task, since we would need a way to fix the initial set of atoms, so we compare only Symphony and G-SchNet. Encouragingly, both models are able to generalize well to unseen fragments, but Symphony\u2019s overall completion rate is higher for both seen and unseen fragments. However, we notice that the performance of Symphony on this task seems to decrease as training progresses, which we are currently investigating."
        },
        {
            "heading": "4.4 MOLECULE GENERATION THROUGHPUT",
            "text": "One of the major advantages of autoregressive models (such as Symphony) over diffusion models (such as EDM) is significantly faster inference speeds. As measured on a single NVIDIA RTX A5000 GPU, Symphony\u2019s inference speed is 0.293 seconds/molecule, compared to EDM\u2019s 0.930 sec/mol. Symphony is much slower than existing autoregressive models (G-SchNet is at 0.011 sec/mol, and G-SphereNet 0.006) because of the additional tensor products for generating higher-degree E(3)equivariant features, but is still approximately 3\u00d7 faster than EDM. However, our sampler is currently bottlenecked by some of the limitations of JAX (Bradbury et al., 2018); we believe that Symphony\u2019s inference speed reported here can be significantly improved to match its training speed."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We have proposed Symphony, a new method to autoregressively generate 3D molecular geometries with spherical harmonic projections and higher-degree E(3)-equivariant features. We show promising results on molecular generation and completion, relative to existing autoregressive models. However, one drawback of our current formulation is that the discretization of our radial components is too coarse, so our bond length distributions are not as accurate as EDM or G-SchNet. This affects our validity when using lookup tables to assign bond orders as they are particularly sensitive to exact bond lengths. Further, Symphony incurs increased computational cost due to the use of tensor products to create higher degree E(3)-equivariant features. As a highlight, Symphony is trained on only \u2248 80 epochs, while G-SchNet and EDM are trained for 330 and 1100 epochs respectively. Further exploring the data efficiency of Symphony remains to be seen. In the future, we plan to explore normalizing flows to smoothly model the radial distribution without any discretization, and placing entire local environment motifs at once which would speed up generation."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Ameya Daigavane was supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions), and the NSF Graduate Research Fellowship program. Song Kim was supported by Analog Devices as an Undergraduate Research and Innovation Scholar in the MIT Advanced Undergraduate Research Opportunities Program (SuperUROP). Mario Geiger and Tess Smidt were supported by the Integrated Computational and Data Infrastructure (ICDI) program of the U.S. Department of Energy, grant number DE-SC0022215. The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper."
        },
        {
            "heading": "6 REPRODUCIBILITY STATEMENT",
            "text": "We have uploaded an anonymized version of our code in JAX (Bradbury et al., 2018) containing all of the data preprocessing, model training and evaluation metrics. Section C.2 describes the hyperparameters used in the training process for Symphony. Details about the metrics used can be found in Appendix E. Section C.3 contains all of the information regarding the QM9 dataset used in this work. Further information about the baseline models and the sampled structures can be found in Section C.4."
        },
        {
            "heading": "7 ETHICS STATEMENT",
            "text": "Generative models for molecules such as Symphony have the potential to be used for discovering novel drugs and useful catalysts. While harmful uses of such generative models exist, the synthesis of a molecule given only its 3D geometry is still extremely challenging. Thus, we do not anticipate any negative consequences of our research."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A ADDITIONAL ANALYSES",
            "text": "For all of the analyses performed in this section, we used all the valid molecules for each model as computed by xyz2mol."
        },
        {
            "heading": "A.1 BISPECTRA OF LOCAL ENVIRONMENTS IN SAMPLED MOLECULES",
            "text": "As seen in Figure 4, we see that Symphony\u2019s sampled bispectra (second from left) have a slightly different distribution relative to those from QM9 in the two most frequent local environments."
        },
        {
            "heading": "A.2 BOND LENGTHS IN SAMPLED MOLECULES",
            "text": "From Figure 5 and Figure 6, we see that Symphony\u2019s bond length distribution tends to be wider than those of QM9, hurting its MMD score relative to EDM. Improving this aspect is an ongoing effort; but we believe that the bond lengths are still quite reasonable."
        },
        {
            "heading": "A.3 ATOM TYPE COUNTS",
            "text": "As seen in Figure 7, all models are able to reasonably capture the distribution of atom types in QM9; Symphony performs especially well here."
        },
        {
            "heading": "A.4 RING SIZES",
            "text": "We also extracted all rings using RDKit (Landrum et al., 2023) and counted their relative frequency, in Figure 8. G-SphereNet seems to produce either very large or very small rings. The other models seem to capture the distribution of ring sizes well."
        },
        {
            "heading": "B PROOF OF E(3)-EQUIVARIANCE",
            "text": "Theorem B.1. Suppose EMBEDDER produces O(3)-equivariant and translation-invariant features hv,l = EMBEDDER(Sn)v,l for every atom v. Then, pposition is O(3)-equivariant and translationinvariant (and hence, E(3)-equivariant):\npposition(Rr\u20d7n+1 +T | fn+1, Zn+1;RSn +T) = pposition(\u20d7rn+1 | fn+1, Zn+1;Sn)\nProof: We first show that pposition is O(3)-equivariant. We have:\nEMBEDDER(RSn)v,l = Dl(R)T EMBEDDER(Sn)v,l for every atom v and degree l. Note that because Zn+1 is rotationally invariant, it immediately follows from Equation 6 and the above, that cl is also E(3)-equivariant with degree l:\ncl(r;RSn, fn+1, Zn+1) = cl(r;Sn, fn+1, Zn+1) Now, as the Wigner D-matrices are always unitary, we have:\nf position(Rr\u20d7;RSn, fn+1, Zn+1) = \u221e\u2211 l=0 cl(r;RSn, fn+1, Zn+1)TYl(Rr\u0302ij)\n= \u221e\u2211 l=0 cl(r;Sn, fn+1, Zn+1)TDl(R)Dl(R)TYl(r\u0302ij)\n= \u221e\u2211 l=0 cl(r;Sn, fn+1, Zn+1)TYl(r\u0302ij)\n= f position(\u20d7r;Sn)\nby definition. Thus, we are guaranteed that f position is O(3)-equivariant. Note that applying a pointwise non-linearity (exp) to f position and a rotationally invariant normalization does not change O(3)-equivariance. Thus, pposition is O(3)-equivariant as well.\nFor translations, note that pposition is described relative to the focus atom fn+1. Thus, as EMBEDDER is translation-invariant:\nEMBEDDER(Sn +T)v,l = EMBEDDER(Sn)v,l pposition will be translation-equivariant:\npposition(\u20d7rn+1 +T | fn+1, Zn+1;Sn +T) = pposition(\u20d7rn+1 | fn+1, Zn+1;Sn)\nIn conclusion, pposition is O(3)-equivariant and translation-equivariant, and hence E(3)-equivariant. Thus, Property (2) is satisfied. \u25a0 Theorem B.2. Suppose EMBEDDER produces permutation-equivariant features hv,l = EMBEDDER(Sn)v,l for every atom v. Then, pfocus is permutation-equivariant, while pspecies and pposition are permutation-invariant:\npfocus(\u03c0(fn+1);\u03c0Sn) = pfocus(fn+1;Sn) pspecies(Zn+1 | \u03c0(fn+1);\u03c0Sn) = pspecies(Zn+1 | fn+1;Sn)\npposition(\u20d7rn+1 | \u03c0(fn+1), Zn+1;\u03c0Sn) = pposition(\u20d7rn+1 | fn+1, Zn+1;Sn) where \u03c0 represents a permutation of the atoms of Sn. Proof: Because EMBEDDER is permutation-equivariant:\nEMBEDDER(\u03c0Sn)\u03c0(v),l = EMBEDDER(Sn)v,l\nfor each atom v. Then, from Equation 5:\npfocus(\u03c0(fn+1);\u03c0Sn) = MLP(EMBEDDER(\u03c0Sn)\u03c0(fn+1),0) = MLP(EMBEDDER(Sn)fn+1,0) = pfocus(fn+1);Sn)\nas claimed. Similarly,\npspecies(Zn+1 | \u03c0(fn+1);\u03c0Sn) = MLP(EMBEDATOMTYPE(Zn+1) \u00b7 EMBEDDER(\u03c0Sn)\u03c0(fn+1),0) = MLP(EMBEDATOMTYPE(Zn+1) \u00b7 EMBEDDER(Sn)fn+1,0) = pspecies(Zn+1 | fn+1;Sn)\nFor pposition, it is sufficient to show that the coefficients cl(r) are permutation-equivariant: cl(r;\u03c0(fn+1), Zn+1, \u03c0Sn) = LINEAR(EMBEDDER(\u03c0Sn)\u03c0(fn+1),l \u2297 EMBEDATOMTYPE(Zn+1))\n= LINEAR(EMBEDDER(Sn)fn+1,l \u2297 EMBEDATOMTYPE(Zn+1)) = cl(r; fn+1, Zn+1,Sn)\nThus, all distributions transform as expected. \u25a0"
        },
        {
            "heading": "C DETAILS OF MODELS",
            "text": ""
        },
        {
            "heading": "C.1 EMBEDDERS",
            "text": "Here, we describe E3SchNet and NequIP (Batzner et al., 2022) which we use to embed the atoms in each fragment into E(3)-equivariant features. As shown in Appendix B, we require these models to be E(3)-equivariant.\nBoth of these models are geometric message-passing neural networks, a type of graph neural network (Sanchez-Lengeling et al., 2021; Daigavane et al., 2021) that respects the symmetries of 3D structures. In particular, E3SchNet as the EMBEDDER for the focus and atom type prediction, and NequIP as the EMBEDDER for the position prediction. Unlike previous autoregressive models which utilized a shared embedder for all tasks, we found that using different embedders for these two tasks performed much better in our experiments.\nGiven the fragment Sn, we define the neighbour of each atom i \u2208 Sn by a Euclidean distance cutoff \u2264 dmax:\nN (i) = {j \u2208 Sn | \u2225\u20d7rij\u2225 \u2264 dmax} (8)\nInitially, the features h(0)i of each atom i in Sn are set as the embedding of its atomic number Zi. At each iteration t, the features h(t)i is updated using the atom\u2019s features h (t\u22121) i and its neighbour\u2019s features h(t\u22121)j where j \u2208 N (i) from the previous round. The final embedding for atom i is returned as h(T )i where T is the number of message-passing iterations. Algorithm 2 formally shows the operations of a general message passing neural network.\nDifferent message-passing networks differ in their choice of UPDATE function. Following Batzner et al. (2022), the UPDATE for NequIP is defined as:\nUPDATE(h(t\u22121)i , h (t\u22121) N (i) ) = h (t\u22121) i +\n1\nC \u2211 j\u2208N (i) lmax\u2211 l=0 R\u0398(\u2225\u20d7rij\u2225)Y l(r\u0302ij)\u2297 h(t\u22121)j\nR\u0398(\u00b7) is a learned multi-layer perceptron (MLP). We set C = 20, dmax = 5A, lmax = 5, and T = 3 here. For clarity, we assume the decomposition of the tensor product into a direct sum of irreducible representations of O(3) above.\nE3SchNet is our generalization of the SchNet model (Schu\u0308tt et al., 2017) that was used in (Gebauer et al., 2019) to produce higher-degree E(3)-equivariant features. The UPDATE function for E3SchNet is defined as:\nUPDATE(h(t\u22121)i , h (t\u22121) N (i) ) = h (t\u22121) i + LINEAR  \u2211 j\u2208N (i) lmax\u2211 l=0 Wijl \u00b7 ( h (t\u22121) j \u2297 Y l(r\u0302ij) )\nAlgorithm 2 General Operation of a Message Passing Neural Network Input: Fragment Sn, Message Passing Iterations T , Cutoff dmax, Update Function UPDATE\nCompute neighbor lists for each atom in Sn according to Equation 8. for i = 1, 2, . . . , n do:\nh (0) i \u2190 SCALAREMBEDDING(Zi)\nfor t = 1, 2, . . . , T do: for i = 1, 2, . . . , n do:\nh (t\u22121) N (i) \u2190 {h (t\u22121) j | j \u2208 N (i)} h (t) i \u2190 UPDATE(h (t\u22121) i , h (t\u22121) N (i) )\nreturn {h(T )i }ni=1\nwhere Wijl are scalars computed via: Wijl = LINEAR(\u03c3(CUTOFF(\u2225\u20d7rij\u2225) \u00b7 RADIALBASIS(\u2225\u20d7rij\u2225)))\nWe use the Gaussian radial basis functions, following SchNet. In fact, for lmax = 0, E3SchNet reduces exactly to the standard SchNet. We set lmax = 2, as we find that the benefits of using even higher degree features for the focus and atom type prediction task are minimal. The cutoff is again 5A.\nWe see that NequIP and E3SchNet guarantee permutation-equivariance, translation invariance and O(3)-equivariance, and hence satisfy the requirements for EMBEDDER in Appendix B.\nWe implement Symphony with the e3nn-jax library that utilizes the JAX (Bradbury et al., 2018) framework for creating efficient E(3)-equivariant machine learning models."
        },
        {
            "heading": "C.2 TRAINING DETAILS",
            "text": "We set \u03c32true = 10 \u22125 and express the Dirac delta distribution in the spherical harmonic basis upto lmax = 5, as explained in Appendix H. The predicted distributions pfocus, pspecies and pposition are learned by minimizing the KL divergence to their true counterparts. We found that adding a small amount of zero-centered Gaussian noise \u03c32 = 2.5 \u00d7 10\u22123 to all input atom positions helped with robustness. All parameters in the EMBEDDER, MLP and LINEAR layers are trained with the Adam (Kingma & Ba, 2017) optimizer with a learning rate of 5 \u00d7 10\u22124. We chose the parameters that achieved the lowest loss on the validation set over 8000000 training steps with a batch size of 16 fragments."
        },
        {
            "heading": "C.3 DATA DETAILS",
            "text": "Following EDM (Hoogeboom et al., 2022), we obtained the QM9 (Rupp et al., 2012) dataset using the DeepChem library (Ramsundar et al., 2019), and filtered out 3054 \u2018uncharacterized\u2019 molecules (available at https://springernature.figshare.com/ndownloader/files/3195404) which rearranged significantly during geometry optimization, giving us exactly 130831 molecules. Symphony was trained used the same splits as EDM: 100000 molecules to train, 13083 molecules for validation and 17748 molecules for test, obtained from a random permutation of the molecules."
        },
        {
            "heading": "C.4 BASELINE MODEL DETAILS",
            "text": "For the baseline models, we used the pretrained EDM model at https://github. com/ehoogeboom/e3_diffusion_for_molecules and the pretrained G-SphereNet model at https://github.com/divelab/DIG/tree/dig-stable/examples/ ggraph3D/G_SphereNet. We retrained the G-SchNet model on the EDM splits following https://github.com/atomistic-machine-learning/G-SchNet. The samples (in .xyz format) of all models used for evaluation is available at this URL: https://figshare.com/s/a17ccface17f0c22f15a."
        },
        {
            "heading": "D LEARNING AND SAMPLING FROM POSITION DISTRIBUTIONS",
            "text": "In this section, we drop the superscript from pposition as it should be clear from context."
        },
        {
            "heading": "D.1 TRAINING",
            "text": "To recap Section 3.3, Symphony predicts coefficients cchl (r;Sn) to represent the position distribution p:\nf(r, \u03b8, \u03d5) = log \u2211\nchannel ch\nexp \u221e\u2211 l=0 cchl (r;Sn)TYl(\u03b8, \u03d5)\np(r, \u03b8, \u03d5) = 1\nZ exp f(r, \u03b8, \u03d5)\nwhere Z is the partition function.\nAs mentioned in Section C.2, the coefficients are learned by minimizing the KL divergence to the target distribution q:\nKL(q || p) = \u222b \u2126 q(\u20d7r) log q(\u20d7r) p(\u20d7r) d\u20d7r = \u222b \u2126 q(\u20d7r) log q(\u20d7r)d\u20d7r\u2212 \u222b \u2126 q(\u20d7r)f (\u20d7r)d\u20d7r+ logZ\nFollowing the notation of Section 3.3, \u2126 represents the set {r \u2208 [0,\u221e), \u03b8 \u2208 [0, \u03c0], \u03d5 \u2208 [0, 2\u03c0)} which is all space in spherical coordinates.\nFor training, we only need the unnormalized logits f and not the normalized distribution p. This is identical to the log-sum-exp trick when training with cross-entropy loss for a classification problem. Unlike the classification case where the number of classes is finite, the integral above must be computed over all of r, \u03b8 and \u03d5 which is an infinite set. To numerically approximate this integral, we use a uniform grid on r and a Spherical Gauss-Legendre quadrature on the sphere at each value of r. As discussed in Section 3.3, the uniform grid on r spans 64 values from 0.9A to 2.0A which is more than sufficient to cover all bond lengths in organic molecules. The Spherical Gauss-Legendre quadrature is a product of two quadratures: a 1D Gauss-Legendre quadrature with 180 points over cos \u03b8 \u2208 [\u22121, 1], and a uniform grid of 359 points over [0, 2\u03c0) for \u03d5. Symphony predicts the coefficients cl(r) of f which can be used to evaluate f(r, \u03b8, \u03d5) at any point. This evaluation for a spherical grid of (\u03b8, \u03d5) values can be done quickly via a Fast Fourier Transform (FFT) that is implemented in e3nn-jax. We perform this FFT procedure for each sphere defined by a radial grid point r."
        },
        {
            "heading": "D.2 SAMPLING",
            "text": "Once the model is learnt, we need to sample from the distribution p. A key advantage of predicting the coefficients cl(r) of f\u03b8(r, \u03b8, \u03d5) is that a different resolution of angular grid can be chosen for sampling than that of training. We simply evaluate f(r, \u03b8, \u03d5) on the quadrature grid as before, apply the exponential, and normalize via numerical integration to get p(r, \u03b8, \u03d5). We first marginalize over \u03b8, \u03d5 to obtain a distribution p(r) to sample a radius r. Then, we sample one of the angular grid points (\u03b8, \u03d5) for the sphere corresponding to this radius r. Overall, this procedure gives us a sample from p(r, \u03b8, \u03d5).\nIn Section G.2, we assess how the validity of molecules generated by Symphony varies as the grid resolution is varied.\nNote that our sampling procedure is much simpler than that of Simm et al. (2021), which uses rejection sampling with a uniform base distribution. We perform some quantitative experiments with the parametrization of Simm et al. (2021) in Section F.2."
        },
        {
            "heading": "E DETAILS OF METRICS",
            "text": ""
        },
        {
            "heading": "E.1 POSEBUSTERS",
            "text": "Table 5 provides details of the Posebusters tests used in Table 2. We use the default parameters from their framework."
        },
        {
            "heading": "E.2 MAXIMUM MEAN DISCREPANCY",
            "text": "The Maximum Mean Discrepancy (MMD), introduced in Gretton et al. (2012), measures how different two distributions pX and pY are, given a kernel function k. Formally, the MMD is defined as:\nMMD(pX , pY ) = \u221a\nE X,X\u2032\u223cpX [k(X,X \u2032)] + E Y,Y \u2032\u223cpX [k(Y, Y \u2032)]\u2212 E X\u223cpX ,Y\u223cpY [k(X,Y )]\nFrom the above equation, we see that the MMD can be easily estimated with samples from each distribution. We choose k as the sum of Gaussian kernels at different scales:\nk(X,X \u2032) = 29\u2211 i=0 exp(\u221210( i 5\u22123) \u00b7 \u2225X \u2212X \u2032\u22252)"
        },
        {
            "heading": "F THE ADVANTAGE OF USING MULTIPLE CHANNELS OF SPHERICAL HARMONICS",
            "text": ""
        },
        {
            "heading": "F.1 AN EXAMPLE WITH THE OCTAHEDRON",
            "text": "Figure 9 shows how adding a second channel helps reduce the effective lmax needed to represent pposition. The atoms depicted by red circles have been placed already, and the atom at the center of the octahedron has been chosen as the focus. To accurately capture the positions of the three remaining atoms (depicted by two stars and a square), we would need a projection upto lmax = 4, because the angle made by the \u2018star\u2019, central atom and the \u2018square\u2019 is \u03c02 radians. However, if we used one channel to represent the \u2018stars\u2019 and one to represent the \u2018square\u2019, we can get away by only using projections upto lmax = 2, because the \u2018stars\u2019 are diametrically opposite each other."
        },
        {
            "heading": "F.2 A STUDY ON LEARNING RANDOM SIGNALS",
            "text": "To quantitatively show the effect of having multiple channels, we see how well the model is able to learn a random distribution on the sphere. We randomly sample N = 5 target points with coordinates\n{r\u0302i}Ni=1 on the sphere, and then define the distribution:\nq(r\u0302) = N\u2211 i=1 exp(\u03b4lmax (r\u0302\u2212 r\u0302i))\nwith the same Dirac delta distribution approximation as described in Appendix H. We use lmax = 5 throughout this section. Then, we randomly initialize coefficients c to minimize the KL divergence to q:\nmin c\nKL(q || pc)\nwhere pc is the probability distribution defined by coefficients c, as before:\nf(\u03b8, \u03d5) = log \u2211\nchannel ch\nexp lmax\u2211 l=0 cchl T Yl(\u03b8, \u03d5)\np(\u03b8, \u03d5) = 1\nZ exp f(\u03b8, \u03d5)\nThis corresponds to a simpler setting where we have only one radius r.\nWe assess the KL divergence as a function of number of position channels ch and lmax in Figure 10. We see a consistent improvement across different lmax as the number of position channels are increased.\nWe also experimented with the parametrization from Simm et al. (2021), who define:\np(\u03b8, \u03d5) = 1\nZ exp ( \u2212\u03b2 k |f(\u03b8, \u03d5)|2 ) where k = \u2211lmax l=0 |cl|2. This extra factor of k was proposed by Simm et al. (2021) to \u201cregularize the distribution so that it does not approach a delta function\u201d. In the left panel of Figure 11, we show that this regularization hurts the model. Even adding multiple channels does not help, because the regularization term \u2018switches\u2019 off multiples channels. However, as shown in the right panel of Figure 11, removing this regularization significantly helps the model, with further improvement as the number of channels are increased. For lmax = 5, we see that our parametrization performs similarly to Simm et al. (2021) without the regularization term. Based on this experiment, we plan to experiment with non-linearities for the logits in future versions of Symphony."
        },
        {
            "heading": "G ABLATION STUDIES",
            "text": "G.1 lMAX AND NUMBER OF POSITION CHANNELS\nTo understand the practical effect of adding multiple position channels to Symphony, as well as the impact of increasing lmax, we trained variants of Symphony varying lmax for the focus embedder\nE3SchNet from 1 to 2, the number of position channels from 1 to 4, and lmax for the position embedder NequIP from 1 to 5.\nDue to computational constraints, we trained these models for 1, 000, 000 steps each, which is 8\u00d7 lesser than the original model reported in Section 4. Thus, the validity numbers are slightly lower overall. However, we believe we can still observe important trends from this experiment.\nWe report the validity as measured by xyz2mol for each of these models in Figure 12.\n\u2022 For the focus embedder E3SchNet, we do not see a significant increase in validity when going from lmax = 1 to lmax = 2.\n\u2022 For the position embedder NequIP, we find a large jump when going from lmax = 1 to lmax = 2. Further increasing lmax seemed to help slightly. For computational reasons, we kept lmax = 5.\n\u2022 Increasing the number of position channels helps for lmax = 1 in particular."
        },
        {
            "heading": "G.2 RESOLUTION",
            "text": "Here, we take the trained Symphony model, freeze all weights, and measure the validity of molecules across a range of grid resolutions. The original grid resolution for model training was (r\u03b8, r\u03d5) = (180, 359) as described above. From Figure 13, we see that the validity is within the expected variation even when using upto 10\u00d7 smaller grids. Further amplification of the resolution also does not seem to affect the validity. We hypothesize that this is due to sampling with a lower temperature than ideal making the target distribution more diffuse; future work will seek to understand this effect better.\nThe previous experiment measured the effect of the grid resolution for sampling. We also seeked to understand the effect of the grid resolution for training. For this, we reuse the task of Section F.2, and vary the grid resolution. All other hyperparameters were kept fixed, with lmax = 2 and 2 position channels. From Figure 14, we see that the learning is not affected even at low resolutions. In fact, from a KL divergence perspective, it is easier to learn at lower resolutions because localization is\neasier. However, lower resolutions come with decreased accuracy when sampling, as shown by the rightmost plot of Figure 14."
        },
        {
            "heading": "G.3 TEMPERATURE",
            "text": "Again, we take the trained Symphony model, freeze all weights, and measure the validity of molecules across a range of temperatures T . This means scaling all the logits by a factor of 1T . Higher temperatures make the model more diffuse, while lower temperatures make the model more peaked. We see that while the validity improves significantly at lower temperatures, the uniqueness tends to suffer. As seen in Figure 15, this experiment suggests a more careful sampling of the temperature to better understand a Pareto frontier between validity and uniqueness."
        },
        {
            "heading": "H REPRESENTING DIRAC DELTA DISTRIBUTIONS",
            "text": "Suppose we have the function f(r\u0302) = \u03b4(r\u0302\u2212 r\u03020) defined on the sphere S2, and we wish to compute its spherical harmonic coefficients cl,m:\nf(\u03b8, \u03d5) = lmax\u2211 l=0 cTl Yl(\u03b8, \u03d5) = lmax\u2211 l=0 l\u2211 m=\u2212l cl,mYl,m(\u03b8, \u03d5)\nBy orthonormality of the spherical harmonics, and the annihilation property of the Dirac delta:\ncl,m = \u222b f(\u03b8, \u03d5)Yl,m(\u03b8, \u03d5) sin \u03b8d\u03b8d\u03d5\n= \u222b \u03b4(r\u0302\u2212 r\u03020)Yl,m(\u03b8, \u03d5) sin \u03b8d\u03b8d\u03d5\n= Yl,m(r\u03020)\nThus, we can easily compute the spherical harmonic coefficients for the Dirac delta distribution upto any required lmax. This is implemented in the e3nn-jax package. Due to the frequency cutoff, the Dirac delta distribution thus obtained is a smooth approximation of a true Dirac delta."
        },
        {
            "heading": "I GENERATED MOLECULES FROM SYMPHONY",
            "text": "Figure 16 exhibits random non-cherry-picked samples from Symphony."
        }
    ],
    "title": "SYMPHONY: SYMMETRY-EQUIVARIANT POINT- CENTERED SPHERICAL HARMONICS FOR MOLECULE GENERATION",
    "year": 2024
}