{
    "abstractText": "We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.",
    "authors": [],
    "id": "SP:70f31b15c319a7719962bd679f85070a665d79d6",
    "references": [
        {
            "authors": [
                "Martin Abadi",
                "Andy Chu",
                "Ian Goodfellow",
                "H Brendan McMahan",
                "Ilya Mironov",
                "Kunal Talwar",
                "Li Zhang"
            ],
            "title": "Deep learning with differential privacy",
            "venue": "In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security,",
            "year": 2016
        },
        {
            "authors": [
                "Alessandro Achille",
                "Aditya Golatkar",
                "Avinash Ravichandran",
                "Marzia Polito",
                "Stefano Soatto"
            ],
            "title": "Lqf: Linear quadratic fine-tuning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro Achille",
                "Michael Kearns",
                "Carson Klingenberg",
                "Stefano Soatto"
            ],
            "title": "Ai model disgorgement: Methods and choices",
            "venue": "arXiv preprint arXiv:2304.03545,",
            "year": 2023
        },
        {
            "authors": [
                "Raef Bassily",
                "Adam Smith",
                "Abhradeep Thakurta"
            ],
            "title": "Private empirical risk minimization: Efficient algorithms and tight error bounds",
            "venue": "In 2014 IEEE 55th annual symposium on foundations of computer science,",
            "year": 2014
        },
        {
            "authors": [
                "Raef Bassily",
                "Crist\u00f3bal Guzm\u00e1n",
                "Michael Menart"
            ],
            "title": "Differentially private stochastic optimization: New results in convex and non-convex settings",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Bourtoule",
                "Varun Chandrasekaran",
                "Christopher A Choquette-Choo",
                "Hengrui Jia",
                "Adelin Travers",
                "Baiwu Zhang",
                "David Lie",
                "Nicolas Papernot"
            ],
            "title": "Machine unlearning",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Bowman",
                "Alessandro Achille",
                "Luca Zancato",
                "Matthew Trager",
                "Pramuditha Perera",
                "Giovanni Paolini",
                "Stefano Soatto"
            ],
            "title": "a-la-carte prompt tuning (apt): Combining distinct data via composable prompting",
            "venue": "arXiv preprint arXiv:2302.07994,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiqi Bu",
                "Yu-Xiang Wang",
                "Sheng Zha",
                "George Karypis"
            ],
            "title": "Differentially private optimization on large model at small cost",
            "venue": "arXiv preprint arXiv:2210.00038,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqi Bu",
                "Yu-Xiang Wang",
                "Sheng Zha",
                "George Karypis"
            ],
            "title": "Differentially private bias-term only fine-tuning of foundation models",
            "venue": "arXiv preprint arXiv:2210.00036,",
            "year": 2022
        },
        {
            "authors": [
                "Leshem Choshen",
                "Elad Venezian",
                "Noam Slonim",
                "Yoav Katz"
            ],
            "title": "Fusing finetuned models for better pretraining",
            "venue": "arXiv preprint arXiv:2204.03044,",
            "year": 2022
        },
        {
            "authors": [
                "Mircea Cimpoi",
                "Subhransu Maji",
                "Iasonas Kokkinos",
                "Sammy Mohamed",
                "Andrea Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Dukler",
                "Benjamin Bowman",
                "Alessandro Achille",
                "Aditya Golatkar",
                "Ashwin Swaminathan",
                "Stefano Soatto"
            ],
            "title": "Safe: Machine unlearning with shard graphs",
            "venue": "arXiv preprint arXiv:2304.13169,",
            "year": 2023
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Aaron Roth"
            ],
            "title": "The algorithmic foundations of differential privacy",
            "venue": "Foundations and Trends\u00ae in Theoretical Computer Science,",
            "year": 2014
        },
        {
            "authors": [
                "Huang Fang",
                "Xiaoyun Li",
                "Chenglin Fan",
                "Ping Li"
            ],
            "title": "Improved convergence of differential private sgd with gradient clipping",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Golatkar",
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "title": "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Golatkar",
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "title": "Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Golatkar",
                "Alessandro Achille",
                "Avinash Ravichandran",
                "Marzia Polito",
                "Stefano Soatto"
            ],
            "title": "Mixed-privacy forgetting in deep networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Golatkar",
                "Alessandro Achille",
                "Yu-Xiang Wang",
                "Aaron Roth",
                "Michael Kearns",
                "Stefano Soatto"
            ],
            "title": "Mixed differential privacy in computer vision",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Sharad Golatkar",
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "title": "Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Gregory Griffin",
                "Alex Holub",
                "Pietro Perona"
            ],
            "title": "Caltech-256 object category dataset",
            "year": 2007
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Like Hui",
                "Mikhail Belkin"
            ],
            "title": "Evaluation of neural architectures trained with square loss vs crossentropy in classification",
            "year": 2006
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Aditya Khosla",
                "Nityananda Jayadevaprakash",
                "Bangpeng Yao",
                "Fei-Fei Li"
            ],
            "title": "Novel dataset for fine-grained image categorization: Stanford dogs",
            "venue": "In Proc. CVPR workshop on fine-grained visual categorization (FGVC),",
            "year": 2011
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In Proceedings of the IEEE international conference on computer vision workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Hao Li",
                "Pratik Chaudhari",
                "Hao Yang",
                "Michael Lam",
                "Avinash Ravichandran",
                "Rahul Bhotika",
                "Stefano Soatto"
            ],
            "title": "Rethinking the hyperparameters for fine-tuning",
            "venue": "arXiv preprint arXiv:2002.11770,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Yu Liu",
                "Stefano Soatto"
            ],
            "title": "Tangent model composition for ensembling and continual fine-tuning",
            "venue": "arXiv preprint arXiv:2307.08114,",
            "year": 2023
        },
        {
            "authors": [
                "Tian Yu Liu",
                "Aditya Golatkar",
                "Stefano Soatto",
                "Alessandro Achille"
            ],
            "title": "Integral continual learning along the tangent vector field of tasks",
            "venue": "arXiv preprint arXiv:2211.13108,",
            "year": 2022
        },
        {
            "authors": [
                "S. Maji",
                "J. Kannala",
                "E. Rahtu",
                "M. Blaschko",
                "A. Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "Technical report,",
            "year": 2013
        },
        {
            "authors": [
                "Fangzhou Mu",
                "Yingyu Liang",
                "Yin Li"
            ],
            "title": "Gradients as features for deep representation learning",
            "venue": "arXiv preprint arXiv:2004.05529,",
            "year": 2020
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Ariadna Quattoni",
                "Antonio Torralba"
            ],
            "title": "Recognizing indoor scenes",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Herbert Robbins",
                "Sutton Monro"
            ],
            "title": "A stochastic approximation method",
            "venue": "The annals of mathematical statistics,",
            "year": 1951
        },
        {
            "authors": [
                "Andrew M Saxe",
                "James L McClelland",
                "Surya Ganguli"
            ],
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
            "venue": "arXiv preprint arXiv:1312.6120,",
            "year": 2013
        },
        {
            "authors": [
                "C. Wah",
                "S. Branson",
                "P. Welinder",
                "P. Perona",
                "S. Belongie"
            ],
            "title": "Caltech ucsd birds-200-2011",
            "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,",
            "year": 2011
        },
        {
            "authors": [
                "Di Wang",
                "Changyou Chen",
                "Jinhui Xu"
            ],
            "title": "Differentially private empirical risk minimization with non-convex loss functions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Puyu Wang",
                "Yunwen Lei",
                "Yiming Ying",
                "Hai Zhang"
            ],
            "title": "Differentially private sgd with non-smooth losses",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Zifeng Wang",
                "Zizhao Zhang",
                "Sayna Ebrahimi",
                "Ruoxi Sun",
                "Han Zhang",
                "Chen-Yu Lee",
                "Xiaoqi Ren",
                "Guolong Su",
                "Vincent Perot",
                "Jennifer Dy"
            ],
            "title": "Dualprompt: Complementary prompting for rehearsal-free continual learning",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Zifeng Wang",
                "Zizhao Zhang",
                "Chen-Yu Lee",
                "Han Zhang",
                "Ruoxi Sun",
                "Xiaoqi Ren",
                "Guolong Su",
                "Vincent Perot",
                "Jennifer Dy",
                "Tomas Pfister"
            ],
            "title": "Learning to prompt for continual learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Suchin Gururangan",
                "Shen Li",
                "Ali Farhadi",
                "Ludwig Schmidt",
                "Michael Rabbat",
                "Ari S Morcos"
            ],
            "title": "lo-fi: distributed fine-tuning without communication",
            "venue": "arXiv preprint arXiv:2210.11948,",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Ya Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaodong Yang",
                "Huishuai Zhang",
                "Wei Chen",
                "Tie-Yan Liu"
            ],
            "title": "Normalized/clipped sgd with perturbation for differentially private non-convex optimization",
            "venue": "arXiv preprint arXiv:2206.13033,",
            "year": 2022
        },
        {
            "authors": [
                "Da Yu",
                "Saurabh Naik",
                "Arturs Backurs",
                "Sivakanth Gopi",
                "Huseyin A Inan",
                "Gautam Kamath",
                "Janardhan Kulkarni",
                "Yin Tat Lee",
                "Andre Manoel",
                "Lukas Wutschitz"
            ],
            "title": "Differentially private fine-tuning of language models",
            "venue": "arXiv preprint arXiv:2110.06500,",
            "year": 2021
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Shauli Ravfogel",
                "Yoav Goldberg"
            ],
            "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "venue": "arXiv preprint arXiv:2106.10199,",
            "year": 2021
        },
        {
            "authors": [
                "Wen Zhang",
                "Lingfei Deng",
                "Lei Zhang",
                "Dongrui Wu"
            ],
            "title": "A survey on negative transfer",
            "venue": "IEEE/CAA Journal of Automatica Sinica,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep Networks are highly non-linear operators trained by optimizing highly non-convex functions, yet some of the training dynamics near convergence approximate those of linear over-parametrized systems (Saxe et al., 2013). Accordingly, linearization has been used as a tool for both the analysis of deep networks (Jacot et al., 2018), and their design (Achille et al., 2021). Linearization around an initial set of weights, however, is of limited practical relevance since the early learning dynamics are highly non-linear and decisive of final performance (Golatkar et al., 2019). On the other hand, linearization around a pre-trained point has been shown to be essentially equivalent to non-linear finetuning, and better in the case of few-shot fine-tuning (Achille et al., 2021). A linearized model has the same number of parameters as the original, but carries some distinct advantages: First, linearity allows straightforward model composition, whereby ensemble models can be formed by scalar combinations at essentially zero cost. Second, a monolithic training set can be partitioned into smaller \u201cshards,\u201d for instance for privacy or attribution purposes, and the resulting models combined to yield performance similar to a model trained on the monolith. This results in zero loss compartmentalization of separately trained models, and enables seamless parallel training. Third, since the linearized model can be trained by optimizing a convex loss, existing methods for private training via selective forgetting (Abadi et al., 2016) are effective and enjoy strong theoretical guarantees.\nDespite the benefits, model linearization is challenging at scale. To this date, only small-scale models have been successfully shown to operate comparably to their non-linear counterparts, typically in the ResNet family of architectures. To our knowledge, our work is the first to propose an efficient method to linearize models in the Transformer family of architectures, leading to what we call \u201cTangent Transformers.\u201d Tangent Transformers can be used to adapt Transformer models, as an alternative to prompt-tuning, fine-tuning, or adapter training, none of which are linear in weight space.\nThe key to enable practical linearization of Transformers is an efficient way to compute the JacobianVector product in a single forward pass, described in Sec. 3.2. As a result, training and inference costs are on the same order of magnitude as the corresponding non-linear Transformer model. In Sec. 4.2 we show that a Tangent Vision Transformer (T-ViT) can achieve similar accuracy to non-linear finetuning (NLFT) of the original ViT (Dosovitskiy et al., 2020) model. Given the comparable accuracy, we focus on illustrating some of the benefits of Tangent Transformers in Sec. 4. Specifically: Compositionality: Linearity yields equivalence between composition in weight space and composi-\ntion in activations, i.e., ensembling. This allows seamlessly combining independently trained models, with obvious benefits to parallel, incremental, and federated learning, while maintaining a constant inference time compared to traditional ensembling. Speedup: Specifically, we obtain up to 10\u00d7 and 50\u00d7 speed-up in parallel training, with only 3.7% and 9.3% drop in overall accuracy compared to non-linear fine-tuning on the full dataset, improving over the Model Soup (Wortsman et al., 2022b) approach by 9.1% and 13.5% respectively. Compartmentalization: Since training on disjoint shards yields the same performance, data removal, if it becomes necessary or desirable (Achille et al., 2023), can be performed deterministically in an exact fashion, at essentially zero cost. Privacy: Most theoretical results and practical methods concerning Differential Privacy (DP) (Abadi et al., 2016; Bassily et al., 2014; Fang et al., 2023; Yang et al., 2022; Bassily et al., 2021; Wang et al., 2019; 2022a) provide much better utility-privacy trade-offs when the optimization problem being solved is convex. While in general deep networks are not, if pre-training is conducted on safe data, linearized fine-tuning is convex and therefore strong results and effective methods for DP apply.\nIn Sec. 2 we briefly survey relevant related work, and in Sec. 3 we derive our Transformer linearization method. We illustrated its effectiveness in Sect. 3.2 for parallel training and composition, in Sec. 3.3 for selective forgetting, or \u201cunlearning.\u201d. and in Sec. 3.4 for privacy. Finally, we test Tangent Transformers empirically in Sec. 4."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Deep network linearization: Deep networks linearized using the first-order taylor approximation have various interpretations in literature - viewing gradients as features (Mu et al., 2020), learning along the tangent space of a neural network (Liu et al., 2022; Liu & Soatto, 2023), infinite-width networks (Jacot et al., 2018). Mu et al. (2020) shows that the Jacobian-Vector product (JVP) of linearized convolutional networks can be efficient computed in a single modified forward pass. Achille et al. (2021) shows that by using Leaky-ReLU activations and training with the rescaled square loss (Hui & Belkin, 2020) and gradient pre-conditioning, ResNets linearized around ImageNet pre-trained weights can achieve comparable performances to the original non-linear networks on downstream fine-tuning tasks. Most similar to our work, Liu & Soatto (2023) applies linearized convolutional networks to ensembling and continual fine-tuning. To the best of our knowledge, we are the first work to linearize transformer networks in a manner that is both computationally efficient and achieves competitive results when fine-tuned on various downstream tasks.\nComposition: We investigate compositionality of deep networks in weight space to yield a model that generalizes better than each individual component model. Weight averaging has been used to improve generalization of pre-trained weights (Choshen et al., 2022), and for distributed finetuning (Liu & Soatto, 2023; Wortsman et al., 2022a). Wortsman et al. (2022b) averages the weights of large pre-trained models fine-tuned with different hyperparameters to improve generalization. Compositionality has also been explored through prompts in continual learning (Wang et al., 2022b;c). However, these works do not develop any theoretically meaningful interpretations for composition, and often scale in inference time as number of component models increase. We introduce TAFT for linearly composing tangent transformer models trained, possibly in parallel, on multiple disjoint shards of data. Under our method, composition of linear weights is theoretically equivalent to output ensembling with constant inference cost.\nMachine Unlearning: Machine unlearning, or forgetting, methods aim to remove the influence of specific samples from a trained network (Achille et al., 2023; Bourtoule et al., 2021; Dukler et al., 2023; Golatkar et al., 2021; 2020a;b). We focus on methods that are zero-shot and yields theoretically guaranteed unlearning. These works (Bourtoule et al., 2021; Bowman et al., 2023; Koch & Soll) operate by splitting datasets into multiple disjoint shards, hence compartmentalizing each sample to only a single shard. Unlearning can then be done by simply removing the shard. However, such methods either incur high inference costs as a result of running inference across multiple models, and often incur significant trade-offs in generalization accuracy of the composed model. Instead, we show that as a result of linearity, we can compose tangent transformer networks simply by averaging network weights, to produce outputs that are equivalent to the ensemble of each network with an inference cost that is constant with respect to number of shards/models in the ensemble.\nPrivacy: Differential privacy (Dwork et al., 2014) seeks to limit the amount of information a trained model contains about the individual training samples. DP-SGD (Abadi et al., 2016) achieves this through clipping of individual gradients followed by addition of Gaussian noise. Bassily et al. (2021; 2014); Fang et al. (2023); Wang et al. (2019; 2022a); Yang et al. (2022) provide rigorous theoretical guarantees for the convergence and utility of DP algorithms, and show that convex (or strongly convex) models offer better utility compared to their non-convex counterparts. Per dimension Gaussian noise in DP-SGD reduces the utility of training large models in favour of fine-tuning parameter efficient adapters (Bu et al., 2022b;a; Golatkar et al., 2022; Yu et al., 2021). In this paper, we show that TAFT in the tangent space of these parameters provides better utility-privacy trade-off."
        },
        {
            "heading": "3 METHOD",
            "text": "We explore the most direct way to linearize a pre-trained transformer network fw - by replacing it with its first-order taylor approximation f linw about its pre-trained weights w.\nf linw (\u00b7) = fw(\u00b7) +\u2207wfw(\u00b7) \u00b7\u2206w (1)\nBy construction, f linw is now linear with respect to \u2206w, the new set of learnable parameters.\nThe new network can be trained easily using any loss function. For example, using the standard mean-squared error loss yields a quadratic objective function, reducing the training of such models to simple linear-quadratic optimization (Achille et al., 2021). We use the Rescaled Square Loss (RSL) (Hui & Belkin, 2020) given by\nL(x, y) = 1\nK\n( \u03b1(\u0307[f linw (x)]y \u2212 \u03ba)2 + K\u2211 i=1,i\u0338=y ([f linw (x)]i) 2 )\n(2)\nwhere \u03b1, \u03ba are hyper-parameters. We empirically found RSL performs better compared to crossentropy or regular MSE loss, corroborating the results of Achille et al. (2021); Liu & Soatto (2023).\nWe further note that how good a local approximation of the network about initial weights w depends on the \u201cdistance\u201d the network moves from its initial point. As such, we additionally regularize the training objective by adding a penalty on \u2225\u2206w\u222522. The resulting training objective is simply ridge regression, retaining the benefits of linear-quadratic optimization while obtaining better empirical results (Sec. 4.6). Note that due to the high dimensionality of the training set and gradient-based features, it is computationally prohibitive to obtain the solution in closed form.\nThis appears costly to compute for both inference and training, since evaluating the Jacobian-Vector Product (JVP) \u2207wfw(x) \u00b7\u2206w requires computing the gradient with respect to the original weights, for each input x. However, by computing the directional derivative, we can derive closed form equations for the linearized versions of the key building blocks of transformer networks. We show that they can be computed in a single modified forward pass through the original model where each layer of the network outputs the computed JVP (JVPout) in addition to the original output values, and takes as input the JVP from the previous layer (JVPin) in addition to the original input values."
        },
        {
            "heading": "3.1 LINEARIZING TRANSFORMERS",
            "text": "Here, we will derive the closed form linearization of a transformer network, and show that it can be easily computed by the modified forward propagation without explicitly computing any gradients. We break down transformer networks into attention, normalization, and fully-connected layers, and separately derive their linearizations (note that while fully-connected layers are already linear, we still need to handle the input JVP from the previous layer). These layers can be simply composed together to form the final Tangent Transformer network.\nWe parameterize the attention function A : Rd\u00d7n 7\u2192 Rd\u00d7n by the weights Wq,Wk,Wv \u2208 Rd\u00d7d corresponding to the key, query and value matrices respectively, and given by\nA(x) = \u03a6(x)V (x), where \u03a6(x) = \u03c3(Q(x)K(x)T ), (3) Q(x) = \u27e8Wq, x\u27e9,K(x) = \u27e8Wk, x\u27e9, V (x) = \u27e8Wv, x\u27e9 (4)\nwhere \u03c3 is the soft-max activation function. We will write Q,K, V,\u03a6 instead of Q(x),K(x), V (x),\u03a6(x) for ease of notation. For simplicity, we only consider single-headed attention in our derivations, but note that our definitions and derivations can be extended to multi-headed attention (which we use in the experiments section) with minimal modification. Now, we wish to compute the first-order approximation of A, denoted Alin : Rd\u00d7n 7\u2192 Rd\u00d7n, and parameterized by the linearized weights \u2206Wq,\u2206Wk,\u2206Wv for the key, query, and value matrices respectively. We can derive the closed form expression for Alin as follows:\nAlin(x) = A(x) + lim r\u21920\n\u2202\n\u2202r A(x,Wq + r\u2206Wq,Wk + r\u2206Wk,Wv + r\u2206Wv)\ufe38 \ufe37\ufe37 \ufe38\nJVPout\n(5)\n= A(x) + ( \u03a6\u2299\u03a8\u2212 (1\u2299 (\u03a6T\u03a8))\u03a6 )T V +\u03a6\u0393 (6)\nwhere\n\u03a8 := \u03a8(x) := \u2329 \u2206Q+WTq JVPin,K \u232a + \u2329 Q,\u2206K +WTk JVPin \u232a (7)\n\u0393 := \u0393(x) := \u2206V +WTv JVPin (8) \u2206Q := \u27e8\u2206Wq, x\u27e9, \u2206K := \u27e8\u2206Wk, x\u27e9, \u2206V := \u27e8\u2206Wv, x\u27e9 (9)\n\u2299 denote the Hadamard product, and JVPin = limr\u21920 \u2202x\u2202r is the Jacobian-Vector Product computed from the previous layer, obtained from the modified forward pass. The terms \u03a6, Q,K, V can be obtained for free as intermediate variables from computing A(x). Thus, computing the JVP term is done through simple matrix multiplication operations of similar computational complexity as the original attention mechanism.\nTransformer blocks also include several normalization layers. Similarly we can compute a closed form expression for their linearized versions that can be obtained in the modified forward propagation step. We show the derivation for Layer Norm, which we denote LN(\u03b3,\u03b2)(\u00b7) and parameterize by the affine transformation parameters (\u03b3, \u03b2), but note that the results can be easily generalized to other forms such as Batch Norm (Achille et al., 2021). In particular, the linearized Layer Norm LNlin parameterized by (\u2206\u03b3,\u2206\u03b2) and evaluated at x can be computed as\nLNlin(x) = LN(x) + LN(\u2206\u03b3,\u2206\u03b2)(x)+ (10)\n1\u221a V ar[x]\n( JVPin \u2212E[JVPin]\u2212\nE[(x\u2212 E[x])(JVPin \u2212E[JVPin])] \u00b7 (x\u2212 E[x]) V ar[x]\n) \u2217 \u03b3\n(11)\nwhere \u2217 is the element-wise scaling operation. Fully-connected (FC) layers parameterized by weight W and bias b can be easily modified to handle JVPin from the previous layer and has already been derived and used in prior works (Achille et al., 2021; Mu et al., 2020). We include it below for completeness.\nFClin(x) = FC(x) + \u2206W Tx+\u2206b+WT JVPin (12)\nNon-linearities are also conveniently handled by the same technique. We illustrate the derivation for the GeLU activation commonly used in transformer-based networks:\nGeLUlin(x) = GeLU(x) +\n( GeLU(x)\nx + x \u00b7Gaussian(x)\n) \u00b7 JVPin (13)\nwhere Gaussian(x) is probability obtained from evaluating the Standard Gaussian CDF at x. As before, all terms can be easily computed without any backpropagation steps, and JVPin is obtained from the modified forward propagation.\nThe final linearized transformer, which we term Tangent Transformer, is simply the composition of such layers, chained together using the modified forward pass. Since Tangent Transformers are linear only in the weights \u2206w, and highly non-linear in the original weights w, we only update \u2206w during fine-tuning, a process we term Tangent Attention Fine-Tuning (TAFT)."
        },
        {
            "heading": "3.2 PARALLEL TRAINING AND COMPOSITION",
            "text": "Given N models linearized about pre-trained weights w and a query x, the ensemble of these models, defined by the linear combination of their outputs, is equivalent to evaluating a single tangent model composed by taking the same linear combination of component models in weight space:\nN\u2211 i=1 \u03bbif lin w i (x) = fw(x) +\u2207wfw(x) \u00b7 N\u2211 i=1 \u03bbi\u2206wi (14)\nThis gives rise to a natural interpretation of weight space composition via output ensembling, while reducing the cost of ensembling N models from O(N) to O(1). In other words, we can train multiple Tangent Transformers on multiple different datasets in a completely parallel manner, and simply combine their output weights to yield a single model that performs as well as their ensemble but with constant inference time. Such compositions in weight space of transformer networks have also been previously explored by Wortsman et al. (2022b) combining multiple models trained on the same dataset with different configurations using weight averaging. However, as we show in Sec. 4.3, the lack of any theoretical relationship between combinations of models in weight space and their resulting output causes the resulting model to perform poorly when component models are trained on disjoint sets of data. On the other hand, we will show that the equivalence of weight averaging and ensembling allow the composition of up to 50 T-ViTs trained on different shards of data with relatively much smaller accuracy trade-offs compared to naively composing non-linear models."
        },
        {
            "heading": "3.3 ZERO-/LOW-COST FORGETTING WITH TANGENT TRANSFORMERS",
            "text": "\u201cLearning\u201d a model by combining the weights of component tangent models, each trained on disjoint shards of data, also allows for the subtraction of each component from the final model. Clearly, this subtraction operation completely removes the influence of samples contained within the shard used to train the component model from the final model. This is highly advantageous for machine unlearning.\nGiven a request to forget a training sample, the paragon unlearning method that guarantees forgetting of the target sample requires training the entire model from scratch on the remaining dataset samples. This is clearly impractical especially for large real-world transformer-based models like GPT-3 (Brown et al., 2020). With a Tangent Transformer composed from individual component models, we can simply remove the shard containing the sample to be forgotten by subtracting the weights of the associated component model. This theoretically guarantees forgetting while preserving accuracy when number of forgetting requests is small (Fig. 1(a)), all at essentially zero computational cost.\nWe note that this method of unlearning through shard removal is not scalable, since performance of the composed model degrades as number of forgetting requests increases. Instead, one can also optionally retrain the component model on the remaining samples in the shard, after removing the sample to be unlearned. Since shards are much smaller than the full dataset, this guarantees forgetting of the requested samples, while maintaining generalization performance of the \u201cforgotten\u201d model and enjoying orders of magnitude speedup compared to the paragon of re-training from scratch (Fig. 1(c))."
        },
        {
            "heading": "3.4 TAFT WITH DIFFERENTIAL PRIVACY",
            "text": "Differential privacy (Dwork et al., 2014) is a mathematical framework to design algorithms which protect the privacy of individual training samples. Given a training dataset D, and an algorithm M , we say that M is (\u03f5, \u03b4)-differentially private (DP) only if\nP (M(D) \u2208 E) \u2264 e\u03f5P (M(D\u2212i) \u2208 E) + \u03b4\nfor all E, D\u2212i, where D\u2212i is obtained by removing the ith sample from D. In simple words, DP enforces an algorithm to produce similar outputs when the dataset differs by a single sample. One of the most popular ways of enforcing DP in deep learning is to use DP-SGD (Abadi et al., 2016) during training. DP-SGD introduces two modifications over the standard stochastic gradient descent (Robbins & Monro, 1951), first it clips the gradient norm of every sample, and then it adds gaussian noise to the sum of the clipped gradients across a training batch. Thus the information pertaining to individual samples is contained with clipping with noise perturbation. It is well known (Bassily et al., 2021; 2014; Fang et al., 2023; Yang et al., 2022) that convex models have better convergence and utility guarantees with trained differentially private convex optimization algorithms (in our case DP-SGD). We show in Tab. 1 that TAFT on Tangent Transformers provide comparable (in some cases outperforms) results to non-linear fine-tuning. As we are now equipped with powerful linear models, we seek to understand if such models can be used in differentially private settings to reap the benefits of theoretical guarantees provided by private convex optimization."
        },
        {
            "heading": "3.5 CHOOSING A GOOD INITIALIZATION POINT",
            "text": "Strong pre-training objectives provide a natural initialization point on which we can compute the tangent model on. However, linearizing transformer models around the full pre-training weights might exhibit strong feature biases towards the source pre-training dataset that might not transfer well to downstream tasks, especially in the later layers. As such, we propose a simple method to overcome this, by linearizing about a randomized re-initialization for the later attention layers, while keeping the pre-training weights constant for earlier layers in the network. We show that this significantly improves results in Fig. 2(b). For Vision Transformer-based classifiers, we further show in Fig. 2(c) that the CLS token itself can also be linearized in the same manner. We will empirically show that this can be beneficial for certain downstream tasks which are \u201cfar\u201d from the pre-training initialization."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In Sec. 4.2, we show that TAFT on T-ViTs can attain similarly performances on downstream tasks compared to non-linear fine-tuning, while benefiting from linearity. We show the advantages that arise from linearity for composition and parallel training in Sec. 4.3, machine unlearning in Sec. 4.4, and privacy in Sec. 4.5. We describe our implementation details in Sec. 4.1, and carry out ablation studies on our implementation choices in Sec. 4.6. We further ablate on different pre-training schemes in Appendix C.4, and compare against Linearized ResNets in Appendix C.1."
        },
        {
            "heading": "4.1 IMPLEMENTATION DETAILS",
            "text": "We run all our experiments on Vision Transformers on image classification tasks. In particular, we use ViT-L/16 (Dosovitskiy et al., 2020) as our base model in all our experiments, and linearize around ImageNet pre-trained weights, the result of which we call T-ViT-L/16. We evaluate on the following datasets in increasing order of distance from the ImageNet pretraining task based on Li et al. (2020) - Caltech-256 (Griffin et al., 2007), MIT-67 (Quattoni & Torralba, 2009), Oxford Pets (Parkhi et al., 2012), Stanford Dogs (Khosla et al., 2011), CUB-200 (Wah et al., 2011), FGVC-Aircrafts (Maji et al., 2013), and Stanford Cars (Krause et al., 2013). We do not use any augmentations. The list of hyperparameters used can be found in the Appendix."
        },
        {
            "heading": "4.2 HOW WELL DOES THE TANGENT TRANSFORMER COMPARE TO THE ORIGINAL MODEL?",
            "text": "While linearity yields many benefits in terms of composition, privacy, forgetting, and even interpretability, there is one main drawback - Tangent Transformers are strictly less expressive compared to the original non-linear model. Hence, for such linear models to be practical, we wish to preserve as much performance as possible on downstream tasks. We show in Tab. 1 that in fact, due to the strong inductive priors from the ImageNet pre-trained initialization, the average downstream performance is highly comparable with that of non-linear fine-tuning of the original model, differing on average only by 1.0% and 0.7% respectively when fine-tuning multiple attention blocks and just the last attention block. In fact, for several tasks that are close to the pre-training dataset (ImageNet) such as MIT-67, Stanford Dogs, Oxford Pets, and CUB-200, we show that TAFT actually outperforms non-linear fine-tuning. We hypothesize that this is resultant from the additional implicit regularization imposed by the linear model. Furthermore, since most of the accuracy gains can be made from just fine-tuning the last attention block (NLFT-1, TAFT-1), this also allows for parameter-efficient fine-tuning since the number of resulting trainable parameters are < 5% of fine-tuning the entire original model.\nOn the other hand, for tasks that are far from the pre-training dataset, such as Stanford Cars and FGVC-Aircrafts, the local approximation becomes less accurate. As expected, the divergence between non-linear fine-tuning and TAFT increases. Compared to transfer learning that simply fine-tunes the classification head, TAFT is strictly more expressive and hence improves by up to 2.9% on average while maintaining linearity in weights. These experiments show that TAFT is highly practical for most downstream tasks, achieving comparable accuracy to non-linear fine-tuning while being parameter efficient. In the following sections, we explore several benefits that linearity yields."
        },
        {
            "heading": "4.3 COMPOSITIONALITY AND PARALLEL TRAINING",
            "text": "We evaluate our proposed method for parallel training and composition described in Sec. 3.2. We first shard a dataset into N disjoint subsets, and train individual models on each subset. Note that training can be done in parallel, yielding a N\u00d7 speed-up in training time that scales linearly as\nN grows. In Tab. 2, we show that across various sharding factors of each dataset, training and then linearly combining weights of models trained with TAFT significantly outperforms composing separately trained non-linear models via Model Soup (Wortsman et al., 2022b), which to the best of our knowledge, is the only method that yields a composed model with O(1) inference cost (with respect to number of component models). Indeed, naively composing non-linear models through weight averaging yields no theoretical guarantees regarding how the output space of the resulting model changes. However, composing the linear weights of Tangent Transformers trained via TAFT is theoretically equivalent to output ensembling, hence outperforming Model Soup by 9.1%, 13.0%, and 13.5% on 10, 25, and 50 shards respectively, while maintaining an O(1) inference cost."
        },
        {
            "heading": "4.4 MACHINE UNLEARNING",
            "text": "Tangent Transformers composed from component tangent models trained on disjoint shards of data enables forgetting \u201cfor free\u201d, since unlearning can be done by simply subtracting models without needing any further training. We show in Fig. 1(a) that for a model composed from 50 shards, one can drop up to half of the shards (25) with only 4.0% drop in accuracy. We also compare against SISA (Bourtoule et al., 2021), which also drops shards upon forgetting requests, and show that we perform uniformly better across all datasets and number of shards dropped, and on average by 11.0%.\nOptionally, one can retrain the shard containing the sample to be forgotten on the remaining samples in the shard. Even then, this still yields significant advantages compared to the baseline of re-training a model from scratch, since only the relevant shard needs to be retrained. In Fig. 1(c), we show that this yields a 50\u00d7 speed-up in our experiments, achieving close to the paragon performance (Appendix C.3) with only a 6.2% drop in accuracy after unlearning 50% of the entire dataset."
        },
        {
            "heading": "4.5 PRIVACY",
            "text": "In Tab. 3 we observe that training convex models in the tangent plane of non-convex models with differential privacy certainly results in a better utility privacy trade-off. To illustrate this we fine-tune various parameters of T-ViT-16 on two different fine-grained datasets (CUB200-easy and Stanford Cars-hard) for different privacy range. In almost all settings we observe that privately fine-tuning the linearized parameters performs much better than privately fine-tuning the non-linear parameters. While fine-tuning the entire last attention block (column \u201dFull\u201d in Tab. 3) we observe that the gradient noise significantly degrades the model utility compared only fine-tuning the last fully-connected layer (and biases/normalization layers) of the network. The linear nature of tangent transformers along with the results in Tab. 3 also inspires a simple private composition/continual learning algorithm i.e. train private models on shards of data, and linearly combining their weights."
        },
        {
            "heading": "4.6 ABLATION STUDIES",
            "text": "We also conduct ablation studies to show key implementation details needed to train tangent models to perform comparable to non-linear models in downstream tasks. In particular, Fig. 2(a) shows that using the rescaled square loss significantly improves average-case results across all datasets by an average of 9.0% and 1.5%, and on the hardest dataset by 23.8% and 2.7% compared to the MSE and\nCE loss respectively. In Fig. 2(b), we show that by resetting the weights of the final attention layer prior to linearization, average performance across datasets improves by 1.5%. We hypothesize that this is due to the negative transfer (Zhang et al., 2022) of later features learnt from the pre-training task to new downstream tasks. Indeed, we note that for datasets which are very close to ImageNet (i.e. Caltech-256, MIT-67), linearizing about the original pre-trained weights perform marginally better since they are highly transferrable to these downstream tasks. Similarly, we show in Fig. 2(c) that resetting and linearizing the CLS token in the last attention block of a vision transformer network can significantly improve performance on datasets which are far from the ImageNet pretraining task, improving results on FGVC-Aircrafts and Stanford Cars by 0.8% and 3.1% respectively."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "Linearization of Transformer architectures, when performed around a strong pre-trained point, can serve to facilitate a number of processes related to fine-tuning and ensembling. Independently trained linear components can be easily composed, thus realizing full parallelism, and disgorged if need be, thus realizing deterministic removal of data.\nHowever, linearization is not panacea: For the linearized models to work as advertised, the point around which the model is linearized is important, which can only be ascertained empirically. Once that is done, linear components can be trained with convex losses, which leads to overall models that enjoy strong guarantees for convergence, privacy, and composition. We also show in our experiments that this limitation can be partially mitigated by applying several techniques such as resetting certain pre-trained weights and linearizing the CLS tokens to improve results on downstream tasks which are far from the pre-training objective.\nAnother limitation of our method is that the inference cost of a Tangent Transformer can potentially be double that of the original model, since the modified forward pass requires an additional set of computations in original to the original forward pass. However, we show that linearizing the last attention block of a ViT-L/16 model is often sufficient to yield strong performances on several downstream tasks. Under this regime, training and inference is parameter efficient, and linearization only leads to a slight increase in inference cost compared to the original forward pass. Note that during training where the dataset is fixed, inference costs can actually be reduced to that of the original non-linear model by simply caching the activations from the static pre-trained weights for each training example, for each layer. Lastly, as observed by Koch & Soll, sharding naturally incurs significant trade-offs in performance on minority classes when training on highly imbalanced datasets.\nThe tasks on which we demonstrated how the linearity of transformers can be exploited through TAFT are certainly not exhaustive. Yet the encouraging empirical results of TAFT make it a candidate replacement for any applications of transfer learning or fine-tuning, while benefiting from the simplicity, composability, and interpretability of linear models."
        },
        {
            "heading": "B DERIVATION OF LINEAR ATTENTION",
            "text": "We note that when taking the Taylor approximation for any (multivariable) function f , f(w+\u2206w) = f(w)+\u2207wf(w)T\u2206w+\u2206wT\u22072wf(w)\u2206w+O(\u2225\u2206w\u22252) where O(\u00b7) notation hides the higher order terms, the first order term can be efficiently computed via its directional derivative\n\u2207wf(w)T\u2206w = lim r\u21920\n\u2202f \u2202r f(w + r\u2206w)\nwhere r is a scalar variable. We will use this technique to derive the linearized closed form for the attention layer.\nLet A denote the attention function parameterized by weights Wq,Wk,Wv. We wish to derive a closed form expression for the linear attention Alin, which is defined as the first-order Taylor approximation of A parameterized by the new linearized weights \u2206Wq,\u2206Wk,\u2206Wv .\nA(x) = \u03a6(x)V (x), where \u03a6(x) = \u03c3(Q(x)K(x)T ), (15) Q(x) = \u27e8Wq, x\u27e9,K(x) = \u27e8Wk, x\u27e9, V (x) = \u27e8Wv, x\u27e9 (16)\nwhere \u03c3 is the soft-max activation function. As in the main paper, we will write Q,K, V instead of Q(x),K(x), V (x) for ease of notation. We will derive the closed form for the single-headed attention, which can then be extended to multi-headed attention with minimal modification. Similarly, we will use n = 1 in the below proof (so x is a vector in Rd) for simplicity, but note that the final result extends to any n > 1.\nAlin(x) = A(x) + lim r\u21920\n\u2202\n\u2202r A(x,Wq + r\u2206Wq,Wk + r\u2206Wk,Wv + r\u2206Wv) (17)\n= A(x) + lim r\u21920 \u03c3(\u27e8Wq + r\u2206Wq, x\u27e9T \u27e8Wk + r\u2206Wk, x\u27e9)\u27e8Wv + r\u2206Wv, x\u27e9\ufe38 \ufe37\ufe37 \ufe38 :=s\n(18)\nDenote for ease of notation \u2206Q = \u27e8\u2206Wq, x\u27e9, \u2206K = \u27e8\u2206Wk, x\u27e9, \u2206V = \u27e8\u2206Wv, x\u27e9. Then for each component i of vector s, we can write\nsi = \u03c3 ((Q+ r\u2206Q)i(K + r\u2206K)) T (V + r\u2206V ) (19)\nApplying chain rule, we get\nlim r\u21920\n\u2202\n\u2202r si\n= lim r\u21920\n[ \u03c3\u2032 ((Q+ r\u2206Q)i (K + r\u2206K)) (( \u2206Q+WTq \u2202x\n\u2202r ) i K +Qi ( \u2206K +WTk \u2202x \u2202r ))]T V\n+ lim r\u21920\n\u03c3 ((Q+ r\u2206Q)i(K + r\u2206K)) T ( \u2206V +WTv lim\nr\u21920\n\u2202x\n\u2202r\n)\n= \u03c3\u2032 (QiK) (( \u2206Q+WTq lim r\u21920 \u2202x \u2202r ) i K +Qi ( \u2206K +WTk lim r\u21920 \u2202x \u2202r )) \ufe38 \ufe37\ufe37 \ufe38\n:=\u03a8i\n T V\n+ \u03c3 (QiK) T ( \u2206V +WTv lim\nr\u21920\n\u2202x\n\u2202r ) \ufe38 \ufe37\ufe37 \ufe38\n:=\u0393 = [ (diag(\u03c3(QiK))\u2212 \u03c3(QiK)\u03c3(QiK)T )\u03a8i ]T V + \u03c3 (QiK) T \u0393\n= [ diag(\u03a6i)\u03a8i \u2212 \u03a6i\u03a6Ti \u03a8i ]T V +\u03a6Ti \u0393\n= [ \u03a6i \u2299\u03a8i \u2212 (\u03a6Ti \u03a8i)\u03a6i ]T V +\u03a6Ti \u0393\nwhere \u2299 denote the Hadamard product. Hence, denoting \u03a8 as the matrix with rows \u03a8i and 1 the identity matrix, we obtain the desired result\nAlin(x) = A(x) + lim r\u21920\n\u2202\n\u2202r s (20) = A(x) + ( \u03a6\u2299\u03a8\u2212 (1\u2299 (\u03a6T\u03a8))\u03a6 )T V +\u03a6\u0393 (21)"
        },
        {
            "heading": "C ADDITIONAL COMPARISONS",
            "text": "We discuss additional comparisons to Linearized ResNets in Sec. C.1, and detail training and inference times in Sec. C.2. We also compare our unlearning method with the paragon of re-training from scratch in Sec. C.3, and ablate on the pre-training schemes for initializing the tangent transformer in Sec. C.4.\nC.1 COMPARISON TO LINEARIZED RESNET ARCHITECTURES\nThe benefits of linearization rely on the strength of the inductive prior obtained from pre-training. Since vision transformers are shown to learn better inductive priors than convolutional architectures as the scale of training data increases, we believe that linearized transformers yield a clear advantage over linearized ResNets by being able to leverage the better inductive priors learnt from pre-training. We compare with linearized ResNet-50 in Tab. 7, where we show that TAFT outperforms Linearized ResNet-50 by 7.3% on the standard fine-tuning task, and by 9.0% for the parallel training and composition task (10 shards) averaged across 3 datasets.\nC.2 TRAINING AND INFERENCE TIME COMPARISONS\nWe compare the per-example training and inference wall-clock timings for NLFT and TAFT in Tab. 8. The inference and training cost for the linearized transformer is potentially twice of the original model as discussed in Sec. 5. We note that the train timings reported would be much faster in practice due to large batch sizes and caching of intermediate features when limiting training to later layers.\nC.3 COMPARISON WITH FORGETTING PARAGON\nIn Fig. 3, we compare the shard re-training forgetting method using TAFT to the paragon of retraining from scratch. Both methods guarantee complete unlearning, but TAFT is able to achieve close-to-paragon performance while speeding up unlearning by up to 50x.\nC.4 ABLATION ON PRE-TRAINING SCHEME\nIn practice, the choice of a model to fine-tune for downstream tasks presupposes some relation between the latter tasks and those used in the pre-trained initialization. Since we focus on classification, we choose ImageNet classification pre-training as our initialization for all the experiments.\nHere, we compare TAFT with different pre-training schemes: (1) Self supervised learning via MAE (Masked Autoencoder Training), (2) Supervised/Classification pre-training, and (3) Contrastive Language-Image Pre-training (CLIP) followed by supervised pre-training.\nWe detail our results in Tab. 9. Indeed, the performance from fine-tuning depends on the discrepancy between the pre-training objective and the target task. (1) being the farthest to classification performs worse than a classification pre-training. However, by augmenting supervised classification pretraining using a contrastive language-image pre-training objective, (3) further boosts the performance of classification-only pre-training.\nC.5 INFLUENCE OF INDIVIDUAL COMPONENT MODELS\nSince models are composed via linear combinations of their weights, the influence of a single component model can be quantified in at least two ways: (A) based on the difference in performance on a validation dataset when the component model is added, and (B) based on the magnitude of the difference in weights with and without the component model. We explored (A) in Fig. 1(a), where we show that subtracting models have lower impact on the performance on downstream tasks when the number of remaining component models is large. However when there remain only few component models in the composition, the impact of each model becomes larger.\nIn Fig. 4, we show that as a result of linearity, this effect is also reflected in the weight space via measuring the L2 difference in weights before and after adding the component model.\nC.6 TEXTURE CLASSIFICATION\nIn the main paper, we primarily evaluated our method on object classification tasks. In Tab. 10, we evaluate our method on the Describable Textures Dataset (DTD) (Cimpoi et al., 2014), where we show that even on texture classification tasks, composing models trained with TAFT consistently outperforms non-linear models across all sharding factors.\nC.7 COMPARISON WITH PARAMETER-EFFICIENT FINE-TUNING\nIn this section, we compare against parameter-efficient fine-tuning methods. In particular, we compare against Adapters (Houlsby et al., 2019) and Low-Rank Adaptation (LoRA) (Hu et al., 2021) when applied to the same last attention block as non-linear fine-tuning and TAFT. Since the main use cases of such methods lie in parameter efficiency and training speed, we show in Tab. 11 that they typically exhibit lower performance on downstream tasks compared to full non-linear fine-tuning, and also lack the linearity of TAFT required to yield effective composition."
        },
        {
            "heading": "D TAFT WITH PROJECTED GRADIENT DESCENT",
            "text": "In the main paper, we constrain the distance that the f linw moves from its pretrained weights w by using the L2 weight decay penalty as an regularizer during training, since the first-order taylor expansion is only valid around some local neighborhood of w. However, we note that it is also possible to impose a hard constraint rather than soft constraint using projected gradient descent, where weights are projected onto a ball of radius R.\nIn Tab. 12, we disable weight decay and instead train with projected gradient descent. We compare using the RSL loss (with \u03ba = 5) and CE loss, since both losses differ in their effect on the final weight magnitude. We show that while RSL loss is more effective for the smaller radius R = 1,\nCE loss becomes more effective as the radius increases to R = 10. While imposing such hard constraints generally yield worse results compared to TAFT, we note that this can be useful in several applications, such as for estimating the smoothness constant of the Tangent Transformer. This can help to better bridge the gap between theoretical analysis - which generally require L-smoothness assumptions or convex loss objectives - and empirical applications."
        }
    ],
    "year": 2023
}