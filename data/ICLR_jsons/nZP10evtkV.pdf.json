{
    "abstractText": "Adversarial patch attacks, where a small patch is placed in the scene to fool neural networks, have been studied for numerous applications. Focusing on image classification, we consider the setting of a black-box transfer attack where an attacker does not know the target model. Instead of forcing corrupted image representations to cross the nearest decision boundaries or converge to a particular point, we propose a distribution-oriented approach. We rely on optimal transport to push the feature distribution of attacked images towards an already modeled distribution. We show that this new distribution-oriented approach leads to better transferable patches. Through digital experiments conducted on ImageNet-1K, we provide evidence that our new patches are the only ones that can simultaneously influence multiple Transformer models and Convolutional Neural Networks. Physical world experiments demonstrate that our patch can affect systems in deployment without explicit knowledge.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pol Labarbarie"
        },
        {
            "affiliations": [],
            "name": "Adrien Chan-Hon-Tong"
        },
        {
            "affiliations": [],
            "name": "St\u00e9phane Herbin"
        },
        {
            "affiliations": [],
            "name": "Milad Leyli-Abadi"
        }
    ],
    "id": "SP:a9d2e87efb9670b350124a4028f573bfd2a19e6e",
    "references": [
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Anish Athalye"
            ],
            "title": "Synthesizing robust adversarial examples",
            "venue": "In ICML. PMLR,",
            "year": 2018
        },
        {
            "authors": [
                "Yutong Bai",
                "Jieru Mei",
                "Alan L Yuille",
                "Cihang Xie"
            ],
            "title": "Are transformers more robust than cnns",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Battista"
            ],
            "title": "Biggio. Evasion attacks against machine learning at test time",
            "venue": "In Joint European conference on machine learning and knowledge discovery in databases,",
            "year": 2013
        },
        {
            "authors": [
                "Nicolas Bonneel",
                "Julien Rabin",
                "Gabriel Peyr\u00e9",
                "Hanspeter Pfister"
            ],
            "title": "Sliced and radon wasserstein barycenters of measures",
            "venue": "Journal of Mathematical Imaging and Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Casper",
                "Max Nadeau",
                "Dylan Hadfield-Menell",
                "Gabriel Kreiman"
            ],
            "title": "Robust feature-level adversaries are interpretability tools",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy others Cohen"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Bao Gia Doan",
                "Minhui Xue",
                "Shiqing Ma",
                "Ehsan Abbasnejad",
                "Damith C Ranasinghe"
            ],
            "title": "Tnt attacks! universal naturalistic adversarial patches against deep neural network systems",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Eykholt",
                "Ivan Evtimov",
                "Earlence Fernandes",
                "Bo Li",
                "Amir Rahmati",
                "Chaowei Xiao",
                "Atul Prakash",
                "Tadayoshi Kohno",
                "Dawn Song"
            ],
            "title": "Robust physical-world attacks on deep learning visual classification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Charlie Frogner",
                "Chiyuan Zhang",
                "Hossein Mobahi",
                "Mauricio Araya",
                "Tomaso A Poggio"
            ],
            "title": "Learning with a wasserstein loss",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Yonggan Fu",
                "Shunyao Zhang",
                "Shang Wu",
                "Cheng Wan",
                "Yingyan Lin"
            ],
            "title": "Patch-fool: Are vision transformers always robust against adversarial perturbations",
            "venue": "arXiv preprint arXiv:2203.08392,",
            "year": 2022
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville"
            ],
            "title": "Improved training of wasserstein gans",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In IEEE CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Zhanhao Hu"
            ],
            "title": "Adversarial texture for fooling person detectors in the physical world",
            "venue": "In Proceedings of the IEEE/CVF CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Gao Huang"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In IEEE CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Nathan Inkawhich",
                "Kevin Liang",
                "Binghui Wang",
                "Matthew Inkawhich",
                "Lawrence Carin",
                "Yiran Chen"
            ],
            "title": "Perturbing across the feature hierarchy to improve standard and strict blackbox attack transferability",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan Inkawhich"
            ],
            "title": "Feature space perturbations yield more transferable adversarial examples",
            "venue": "In Proceedings of the IEEE/CVF CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Danny Karmon",
                "Daniel Zoran",
                "Yoav Goldberg"
            ],
            "title": "Lavan: Localized and visible adversarial noise",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Kurakin"
            ],
            "title": "Adversarial machine learning at scale",
            "venue": "arXiv preprint arXiv:1611.01236,",
            "year": 2016
        },
        {
            "authors": [
                "Mark Lee",
                "Zico Kolter"
            ],
            "title": "On physical adversarial patches for object detection",
            "venue": "arXiv preprint arXiv:1906.11897,",
            "year": 2019
        },
        {
            "authors": [
                "Aishan Liu",
                "Xianglong Liu",
                "Jiaxin Fan",
                "Yuqing Ma",
                "Anlan Zhang",
                "Huiyuan Xie",
                "Dacheng Tao"
            ],
            "title": "Perceptual-sensitive gan for generating adversarial patches",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie. A convnet for the"
            ],
            "title": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp",
            "venue": "11976\u201311986, 2022.",
            "year": 2020
        },
        {
            "authors": [
                "Giulio Lovisotto",
                "Nicole Finnie",
                "Mauricio Munoz",
                "Chaithanya Kumar Mummadi",
                "Jan Hendrik Metzen"
            ],
            "title": "Give me your attention: Dot-product attention considered harmful for adversarial patch robustness",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aleksander Madry"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Muhammad Muzammal Naseer",
                "Salman H Khan",
                "Muhammad Haris Khan",
                "Fahad Shahbaz Khan",
                "Fatih Porikli"
            ],
            "title": "Cross-domain transferability of adversarial perturbations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Muzammal Naseer",
                "Salman Khan",
                "Fatih Porikli"
            ],
            "title": "Local gradients smoothing: Defense against localized adversarial attacks",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2019
        },
        {
            "authors": [
                "Muzammal Naseer",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Fatih Porikli"
            ],
            "title": "On generating transferable targeted perturbations",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Federico Nesti"
            ],
            "title": "Evaluating the robustness of semantic segmentation for autonomous driving against real-world adversarial patch attacks",
            "venue": "In Proceedings of the IEEE/CVF WACV,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Jalal Fadili",
                "Julien Rabin"
            ],
            "title": "Wasserstein active contours",
            "venue": "In 2012 19th IEEE International Conference on Image Processing,",
            "year": 2012
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Omid Poursaeed",
                "Isay Katsman",
                "Bicheng Gao",
                "Serge Belongie"
            ],
            "title": "Generative adversarial perturbations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Julien Rabin",
                "Gabriel Peyr\u00e9",
                "Julie Delon",
                "Marc Bernot"
            ],
            "title": "Wasserstein barycenter and its application to texture mixing",
            "venue": "In Scale Space and Variational Methods in Computer Vision: Third International Conference,",
            "year": 2011
        },
        {
            "authors": [
                "Andras Rozsa"
            ],
            "title": "Lots about attacking deep features",
            "venue": "In IJCB. IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "Aniruddha Saha"
            ],
            "title": "Role of spatial context in adversarial robustness for object detection",
            "venue": "In Proceedings of the IEEE/CVF CVPR Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Johannes Stallkamp",
                "Marc Schlipsing",
                "Jan Salmen",
                "Christian Igel"
            ],
            "title": "The german traffic sign recognition benchmark: a multi-class classification competition",
            "venue": "In The 2011 international joint conference on neural networks,",
            "year": 2011
        },
        {
            "authors": [
                "Christian Szegedy"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199,",
            "year": 2013
        },
        {
            "authors": [
                "Christian Szegedy"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In IEEE CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Simen Thys"
            ],
            "title": "Fooling automated surveillance cameras: adversarial patches to attack person detection",
            "venue": "In IEEE/CVF CVPR workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: old and new, volume 338",
            "year": 2009
        },
        {
            "authors": [
                "Anqi Zhao",
                "Tong Chu",
                "Yahao Liu",
                "Wen Li",
                "Jingjing Li",
                "Lixin Duan"
            ],
            "title": "Minimizing maximum model discrepancy for transferable black-box targeted attacks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Wen Zhou"
            ],
            "title": "Transferable adversarial perturbations",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Inkawhich"
            ],
            "title": "2019) and ours), during training, we randomly rotate the patch up to five degrees for the x and y-axis and up to 10 degrees for the z-axis",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural networks have shown vulnerability to adversarial examples, i.e., norm-bounded perturbations of their inputs designed to fool them (Szegedy et al., 2013; Biggio, 2013). Such vulnerability has motivated researchers to develop empirical robustification methods (Madry et al., 2017) or to provide some theoretical robustness guarantees (Cohen, 2019). Other research is dedicated to designing more powerful attacks (Kurakin et al., 2016). These attacks are invisible patterns added to the whole image \u2013 a pixel array \u2013 which, therefore, has to be accessible: a strong practical limitation.\nAdversarial patch attacks (APA) are a more realistic type of attack expected to be realizable in the physical world. They rely on adding a small textured patch to the scene. Since such a patch can be easily printed and localized on an object or in the environment, it poses a serious threat in various contexts and related tasks. For example, Brown et al. (2017) produce a patch capable of fooling multiple ImageNet-1K classification models. Patch attacks can also threaten other visual tasks (Thys et al., 2019; Lee & Kolter, 2019; Saha et al., 2020; Hu et al., 2022; Nesti et al., 2022). Saha et al. (2020) design a patch which, when placed on a stop sign or the roadway, may result in the missed detection of a pedestrian crossing the road. Another APA for a semantic segmentation task is proposed in (Nesti et al., 2022), which reduces the baseline model accuracy. Despite the good attacking performance of current APA in whitebox configuration (applied on the same model that they have been learned), their effectiveness is mitigated when transferring to unseen models (blackbox configuration). Prior works focus either on studying the whitebox performance of their patch against Transformer architecture (Fu et al., 2022; Lovisotto et al., 2022) or on studying the whitebox and/or the blackbox performance of their patch against classical CNNs (Brown et al., 2017; Karmon et al., 2018; Liu et al., 2019; Doan et al., 2022; Casper et al., 2022). Focusing on image classification, we propose a new attack perspective to improve the transfer of patch attacks to unseen models.\nMost previous work on APA for classification influence the network to output a target class with high confidence (Brown et al., 2017; Karmon et al., 2018; Liu et al., 2019; Doan et al., 2022; Casper et al., 2022). This strategy involves pushing the deep representation of images to cross the nearest decision boundary of the source model. The strategy has two drawbacks: it is highly dependent on\nthe model on which the attack is based, and the patch may push the corrupted image representations into unknown regions of the representation space. Instead of blindly maximizing the probability of a target class, research on invisible adversarial examples suggests considering the feature space of deep networks (Zhou et al., 2018; Rozsa et al., 2017; Inkawhich et al., 2019; 2020). For example, Inkawhich et al. (2019) propose optimizing the adversarial examples to match the deep feature representations of an existing target image.\nTo overcome the dependence of the attack on a single decision boundary of the source model and to relax the specificity of the selection of the target feature point, we propose a distribution-oriented approach. The learning principle of our patch attack is to globally alter the feature distribution of a set of images from a particular class to match another known distribution to be taken from another class. To do so, we propose to optimize with respect to the Wasserstein loss which has several practical and theoretical advantages (Peyre\u0301 et al., 2019; Frogner et al., 2015; Arjovsky et al., 2017). The role of the patch, when placed in the scene, is to push the feature distribution towards this known misleading class distribution (Fig 1). Such a global strategy in feature space is expected to allow a better transferability capability, as it is independent of the decision boundary constructed by the classifier and the choice of the target point (as proposed by Inkawhich et al. (2019)). By conducting extensive experiments on ImageNet-1K (Deng et al., 2009), we indeed show that our APA is more model transferable and is more physically feasible than previous APAs for a large ensemble of network architectures, including classical CNNs (Simonyan & Zisserman, 2014; Szegedy et al., 2016; He et al., 2016; Huang et al., 2017), recent CNNs (Tan & Le, 2019; Liu et al., 2022) and Vision Transformers (Touvron et al., 2021; Liu et al., 2021).\nThe main contributions of this work are to:\n\u2022 introduce a new framework based on optimal transport for creating patch attacks that are highly transferable to unknown networks. This framework is based on the idea of attacking feature distributions, which is independent to the classifier decision boundaries and more robust to optimization artifacts than the feature point method;\n\u2022 show that our attack works for the most extensive spectrum of deep networks considered in the patch attack literature: we deal with various versions of Convolutional Neural Networks, Transformers, and adversarially trained models and show transferability superiority through extensive experiments on ImageNet-1K;\n\u2022 provide digital and physical experiments demonstrating that our patch is potentially harmful in the physical world (including against state of the art defense)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 TRANSFERABLE INVISIBLE ADVERSARIAL EXAMPLES",
            "text": "After discovering adversarial examples, some works studied whether these invisible adversarial attacks were transferable from one model to another. To enhance the adversarial examples transferability, Inkawhich et al. (2019) consider the feature space of deep networks rather than their decision space. The rationale for such a strategy is that the feature space is expected to capture the useful information in an image more universally, allowing better attack transferability between models. They propose to build the attack to push the corrupted image features close to a single specific feature point (feature representation of a chosen sample of a pre-chosen class). To strengthen the transferability Inkawhich et al. (2020) train offline multiple specific auxiliary classifiers. Rather than independently train each adversarial examples by iterative methods, Poursaeed et al. (2018); Naseer et al. (2019a; 2021); Zhao et al. (2023) train a Generative Adversarial Network (GAN) to transform clean images to adversarial examples. Naseer et al. (2021) (called TTP method) learn their GAN by minimizing the Kullback-Leibler divergence between the probability-class distribution of adversarial examples and the probability-class distribution of target class images. From a generalization error bound for black-box targeted attacks, Zhao et al. (2023) derive an algorithm to train their GAN to minimize the maximum model discrepancy (M3D method) between two models. However, all these previously presented works study only the transferability of invisible adversarial examples, and, it is not clear that they work for APA."
        },
        {
            "heading": "2.2 ADVERSARIAL PATCH ATTACK",
            "text": "APAs were first introduced for image classification by Brown et al. (2017). Rather than finding a small additive adversarial noise, they instead constrain the optimization procedure to a small part of the image while also allowing the optimization to be unconstrained in magnitude. They design a patch (called GAP) by maximizing, under patch transformations, the log Softmax of a selected target class.\nThe resulting patch was capable of fooling five ImageNet-1K classification models. To increase the fooling effectiveness of the patch, Karmon et al. (2018) (LaVAN method) add a new term to the loss criterion initially proposed by Brown et al. (2017). By minimizing the log Softmax of image ground truth, this new term ensures the misclassification of the attacked image. Other methods propose to use generative-based approaches: Liu et al. (2019) (PS-GAN method) train a GAN to generate a background-harmonious patch that enhances both the visual fidelity and attacking ability of the patch. Instead of training a GAN, Doan et al. (2022) and Casper et al. (2022) use a\npre-trained GAN directly. To change the generated flower to an adversarial flower, Doan et al. (2022) (TnT method) modify the latent representation of the generator. Casper et al. (2022) perturb the latent representation at some chosen generator layer. All the above works optimize their patch to maximize the log Softmax of some classifiers."
        },
        {
            "heading": "2.3 TRANSFERABILITY EVALUATION",
            "text": "The previously presented APAs Brown et al. (2017); Karmon et al. (2018); Liu et al. (2019); Doan et al. (2022); Casper et al. (2022) use different evaluation settings in their experimentation. Some of these works measure the transferability of their patch to unknown models. Brown et al. (2017) and Doan et al. (2022) measure their patch transferability but only consider dated CNNs like ResNet trained with a less generalizing learning policy than recent ones. The GAP patch is also applied at random locations in these images, giving rise to possible occlusion of the main object. The PS-GAN\ngenerated patch (Liu et al., 2019) shows transferability among unseen models. However, the evaluated models are not state-of-the-art, and the experiments are conducted on the small GTRSB dataset (Stallkamp et al., 2011). Focused on the interpretability of decision-making of deep networks, Casper et al. (2022) do not provide quantitative results on network transferability. Table 1 contextualizes our approach with respect to others.\nOur work bridges the gap between the transferability studies of invisible adversarial examples and adversarial patch attacks. We follow the principle that the attack should be designed with an optimization metric defined in the feature space rather than in the decision space (Inkawhich et al., 2019). Moreover, we generalize the point-wise strategy proposed by Inkawhich et al. (2019), which presents several drawbacks. First, optimizing when the objective is to push multiple points to a unique point is likely to fail. When the optimization succeeds, the power of the attack depends highly on the choice of the target point (see Appendix B). Furthermore, a single feature point can be well-classified by one network but misclassified by another, thus limiting the transferability of the attack to multiple networks. We propose to avoid target point selection and to broaden the range of features under attack by considering target point distributions instead. This is done through optimal transport for the loss to be optimized."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 BACKGROUND",
            "text": "The theory of optimal transport (Peyre\u0301 et al., 2019; Villani et al., 2009) provides several techniques for efficient computation of distances between distributions. It has been shown that optimizing with respect to the Wasserstein loss has various practical benefits over the KL-divergence loss (Peyre\u0301 et al., 2012; Frogner et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017). Unlike the KLdivergence and its related dissimilarity measures (e.g. Jensen-Shannon divergence), the Wasserstein distance can provide a meaningful notion of closeness (i.e. distance) for distributions supported on non-overlapping low dimensional manifolds.\nLet Pp(Rd) = {\u00b5 \u2208 P(Rd) : \u222b R ||x||\npd\u00b5(x) < \u221e} be the set of probability measures on Rd with finite moment of order p, with p \u2208 [1,+\u221e). The p-Wasserstein distance is defined as\nWpp(\u00b5, \u03bd) = inf \u03c0\u2208\u03a0(\u00b5,\u03bd) \u222b Rd\u00d7Rd ||x\u2212 y||pd\u03c0(x, y), (1)\nwhere \u00b5, \u03bd \u2208 Pp(Rd), ||.|| is the Euclidean norm and \u03a0(\u00b5, \u03bd) is the set of probability measures on Rd \u00d7 Rd whose marginals with respect to the first and second variables are given by \u00b5 and \u03bd respectively. The quantity Wp(\u00b5, \u03bd) in not analytically avaible in general. To solve Eq 1, the standard methods are linear programs and have a worst-case computational complexity in O(n3 log(n)) where n is the number of samples (Peyre\u0301 et al., 2019).\nTo leverage the computational efficiency of Eq 1 ,Rabin et al. (2012); Bonneel et al. (2015) define a new metric named Sliced-Wasserstein distance. This new metric is based on the fact that for one-dimensional probability measure the p-Wasserstein distance (1) has the following closed-form\nWpp(\u00b5, \u03bd) = \u222b 1 0 |Q\u00b5(s)\u2212Q\u03bd(s)|pds, (2)\nwhere Q\u00b5 and Q\u03bd are the quantile functions of \u00b5 and \u03bd respectively. Let Sd\u22121 be the d-dimensional unit sphere and \u03c3 the uniform distribution on Sd\u22121. For \u03b8 \u2208 Sd\u22121, we define the linear form for all x \u2208 Rd by \u03b8\u2217(x) = \u27e8\u03b8, x\u27e9. The Sliced-Wasserstein distance is then defined by\nSWpp(\u00b5, \u03bd) = \u222b Sd\u22121 Wpp(\u03b8 \u2217 \u266f\u00b5, \u03b8 \u2217 \u266f \u03bd)d\u03c3(\u03b8), (3)\nwhere \u00b5, \u03bd \u2208 Pp(Rd), p \u2208 [1,+\u221e) and \u03b8\u2217\u266f\u00b5 and \u03b8\u2217\u266f \u03bd are the push-forward by \u03b8\u2217 of \u00b5 and \u03bd respectively. In practise, Eq 3 is approximated with a standard Monte Carlo method. We denote by SWpp(\u00b5, \u03bd)K its numerical approximation where K is the number of random projections. Since \u03b8 \u2217 \u266f\u00b5 and \u03b8\u2217\u266f \u03bd are univariate distributions, the resulting complexity of the approximation is general more efficient than resolving Eq 1. The corresponding computational complexity is O(Kdn+Kn log(n)). We show also in section A of Appendix that the empirical computation time of our approach remains similar to other methods."
        },
        {
            "heading": "3.2 OPTIMAL TRANSPORT BASED LOSS",
            "text": "We consider the standard notation where (xi, yi) \u2208 X \u00d7 Y , i = 1, ..., n, are samples drawn from a joint distribution of random variables X and Y . We are considering an image classification problem, where the input is sampled from X = Rh\u00d7w\u00d7c, where h \u00d7 w are the image dimensions and c is the number of channels and where the output Y is sampled from a set of M labels {y1, ..., yM}. Let F : X \u2192 Y be a given pre-trained neural network that we wish to attack. We assume that the functional architecture of F follows a classical encoder-decoder schema. We denote by f the encoder part of F , where f is composed by a set of J layers; L = {l1, ..., lJ}. Except for the last layer which usually directly result from an average pooling layer, we apply an average pooling layer to obtain a feature vector. For all l \u2208 L, f (l) maps x \u2208 X to the feature space S(l) = Rcl , where cl is the number of channels.\nFor a given target class y, we denote by \u03bd(l)y the multivariate target distribution of f (l)(X) when the class of X is y. The principle of our proposed method is to design a patch by moving the corrupted image distribution towards the target \u03bd(l)y and solve:\n\u03b4\u2217 = argmin \u03b4 EX [\u2211 l\u2208L OT (\u00b5 (l) X\u03b4 , \u03bd(l)y ) ] , (4)\nwhere \u00b5X\u03b4 is the estimated feature distribution of the corrupted source images and OT could be W p p or SWpp.\nIn practice, we solve a regularized version of Eq 4 using Expectation over Transformations (EoT) from Athalye et al. (2018). This regularization makes patches more physically realizable. Let T be a distribution over transformation (e.g., rotations, scaling, blur, ...) and E a distribution over locations. Following Brown et al. (2017); Casper et al. (2022) we denote by A(\u03b4, x, e, t) the patch applicator operator in an image x where \u03b4 is the patch, t are patch transformations and e is the patch location in the image x. Our patch is therefore trained to optimize the following objective:\n\u03b4\u2217 = argmin \u03b4 EX,t\u223cT ,e\u223cE [\u2211 l\u2208L OT (\u00b5 (l) A(\u03b4,X,e,t), \u03bd (l) y ) + TV (\u03b4) ] , (5)\nwhere TV is the total variation loss discouraging high-frequency patterns. We will denote by (Wpp) (N) and (SWpp) (N) when we attack N layers by solving the standard or the sliced version of the Wasserstein distance respectively. We choose by convention that for N = 1, l = lJ , i.e., we are attacking the last layer of f ."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "This section evaluates our APA through digital, hybrid and physical world experiments. In all experiments, the objective is to craft an APA with a high targeted success rate (tSuc).\nWe consider the single-source model setting and test attacking transferability to other models. Transferability is tested between ImageNet-1K (Deng et al., 2009) models; ResNet 18/34/50- V1/50-V2 (He et al., 2016), DenseNet 121/161/169/201 (Huang et al., 2017), EfficientNet B0/B1/B2/B3/B4 (Tan & Le, 2019), ConvNext Tiny/Small (Liu et al., 2022), VGG19 (Deng et al., 2009), Inception-V3 (Szegedy et al., 2016) and Swin Tiny/Small/Base (T/S/B) (Liu et al., 2021) from Pytorch Model Zoo, DeiT T/S/B from Timm Model Zoo, ResNet50 ReLU Adv and DeiT S Adv, adversarially trained models (trained against invisible adversarial examples) from Bai et al. (2021) and finally ResNet50 self-supervised learned from Caron et al. (2020). We regroup these models into the following categories depending on their architecture and their training recipes: CNNs-V1 = {ResNet 18/34/50-V1, DenseNet 121/161/169/201, VGG19, Inception-V3}, CNNs-V2 = {ResNet 50-V2/50-self}, ENet = {EfficientNet B0/B1/B2/B3/B4}, CNext = {ConvNext T/S}, DeiT = {DeiT T/S/B}, Swin = {Swin T/S/B} and AT = {ResNet50 ReLU Adv and DeiT S Adv}, where AT stands for Adversarially trained.\nEvaluated methods. We consider GAP (Brown et al., 2017), LaVAN (Karmon et al., 2018), TnT (Doan et al., 2022), Casper et al. (2022) and Logit (Zhao et al., 2021) as decision boundary-based\nbaselines. Because of its ease of computation compared to Inkawhich et al. (2020), which requires off-line training of multiple specific auxiliary models, we choose to adapt the proposed method by Inkawhich et al. (2019) as a baseline (we name it L2) to craft an APA based on attacking the feature space. We also adapt the recent state-of-the-art works on transferable invisible adversarial examples (Naseer et al., 2021; Zhao et al., 2023) to craft an APA. To do so, we convert generative methods (Naseer et al., 2021; Zhao et al., 2023) to iterative ones: the objective is to create one universal APA and not one for each image.\nExperimental setup. For the sake of comparison, baselines and our method are crafted using the same training recipes. To control the balance between the adversarial loss and the total variation loss, the gradient of each loss is computed individually, normalized, and combined using a weighted sum. Patch values are clipped into the image range at each iteration. Following prior works Brown et al. (2017); Lee & Kolter (2019); Casper et al. (2022), we choose and fix the sampling distributions from EoT (Eykholt et al., 2018) for all the methods. During training and evaluation, patches are randomly placed to the side of images (to avoid occluding the object of interest), and transformations and noises are applied to the patch to mimic real-world situations. Appendix E evaluates the robustness of models according to the patch position in the image. We randomly choose nine targeted classes (see Appendix A for details) and design a patch to fool the network targeting each of these classes. We split the ImageNet-1K validation set into a training set of 40000 images on which we train patches and a test set of 10000 images on which we evaluate their impact. The patch optimization is performed using 100 epochs (1 epoch equals 1000 iterations) with a batch size of 50 images and for three different learning rates (0.1, 0.5, 1). We choose for our method p = 2 and K = 500 (reasons are explained in Appendix I). We evaluate the APA with the best loss among the three learning rates, leading to one patch per method and per class. Finally, reported tSuc are the average over the classes and patch sizes (from 70 \u00d7 70 to 90 \u00d7 90 which is the standard setting consider in the literature (Brown et al., 2017; Poursaeed et al., 2018; Doan et al., 2022; Casper et al., 2022))."
        },
        {
            "heading": "4.1 DIGITAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1.1 TRANSFERABILITY AMONG NETWORKS",
            "text": "We select from the previously defined families the following models: ResNet34, ResNet50V1, ResNet50-V2, ResNet50-self, EfficientNet-B0, ConvNext-S, DeiT-S, Swin-T, Swin-S, SwinB. We design patches to attack one of these source models. Then we measure the attacking transferability when the resulting patch is used to fool the remaining models (target models).\nFor example, patches trained and tested on the Swin family provide three patches. Each one is trained individually on SwinT, Swin-S or Swin-B, and is evaluated against the two other Swin models. Table 2 summarizes, for each method, the best transferring attack performance. We select and report the results of the source family producing the highest mean targeted success rate (tSuc, rate at which the attacked images are classified as the patch target label) according to the method. Our method shows the best transferability capacity: highest mean, min and max tSuc. Table 3 details transferability results for\nall source families. From this table, we can make several conclusions: Networks trained with older training recipes (CNNs-v1) seem more vulnerable to attacks regardless of the attacking procedures. These networks are more sensitive to salient patches present in the image. As presented in Bai et al. (2021), new training recipes (scheduler, augmenting training data like RandAug and Mixup, ...) appear to robustify models for convolutional networks and transformers. Baseline methods were not able to create a patch that can strongly transfer to CNext and Swin models even when these patches are learned using the same model category. This indicates that rather than catch the useful common\ninformation in deep networks, baseline methods produce a patch that tends to overfit on the specific weights of the model. This is particularly the case for decision boundaries-based APAs (GAP (Brown et al., 2017), LaVAN (Karmon et al., 2018), TnT (Doan et al., 2022) and Casper et al. (2022)). Naseer et al. (2021) method seems to create a patch that better captures the overall source model decision boundaries. This method leads to better results than decision boundaries-based APAs on the DeiT family of networks (networks with more complex representations than CNNs-V1 networks). Creating a patch that minimizes the maximum discrepancy between two models (Zhao et al., 2023) is unstable and generally results in a patch that is not transferable. A possible explanation is that APA induces a higher shift in the feature space than invisible adversarial examples.\nA patch resulting from an optimization defined in the feature space reduces the patch overfitting and increases the transferability to other networks. This suggests that the patch has learned more about the common information to model the different classes rather than trying to cross the decision boundaries. However, the L2 methodology (Inkawhich et al., 2019) is unstable and is highly dependent on the choice of the target point, resulting in lower performance than our method (see Appendix B). Our two methods (exact and sliced version) outperform the previous methods on transferability. We remark that patches learned using Swin or CNext seem more universal as they can transfer to multiple models. When crafted on Swin models, we produce a patch capable of transferring uniformly well to all the models. We show in Appendix F that an ensemble of CNNs-v1 models can not reach the level of transferability obtained by our method when targeting Swin models. These results indicate that our method allows the patch to learn more about the common information shared across networks. The following experiments are performed on Swin patches as they lead to a more uniform transferability across networks."
        },
        {
            "heading": "4.1.2 EFFECTIVENESS AGAINST ROBUSTIFIED NETWORKS",
            "text": "We now consider a more realistic scenario in which the attacked system uses a defense mechanism. We propose to use Local Gradients Smoothing (LGS) (Naseer et al., 2019b), as it is one of the strongest defense mechanism against patch attacks.\nLGS smooths salient regions in images before feeding them to the network. We reproduce the previous experiments for three different smoothing factors \u03bb \u2208 {1.5, 1.9, 2.3} for LGS while fixing other parameters as in the article (we report here results for \u03bb = 1.5, see K for other results). For each method, we evaluate their Swin patches against networks robustified by LGS. Our method achieves the best transfer results demonstrating the criticality of our attack even for robustified networks (Table 4). We now suppose the target network has been adversarially trained (AT) against invisible adversarial examples. The patch at-\ntacks which are not learned on AT models could reduce their accuracy when transferred to these models. However, the AT models do not get fooled by the patch to predict the targeted class. (clean accuracy: 65.44, attacked accuracy: 57.65, tsuc: 0.72). AT models seem to have different class representations and are hard to force to predict a chosen class. When designed on one AT model and transferred to another model, our patches and GAP patches produce the best transfer performances (see Appendix G)."
        },
        {
            "heading": "4.2 HYBRID EXPERIMENTS",
            "text": "In this section, we propose to measure the physicality of patches through a hybrid experiment and to simulate the potential effect of patches in the real world. We consider the following steps: printing and digitalization. Scanned patches are placed numerically in images using the same procedure as in the previous section (physical transformations are applied to them). We use patches designed on Swin-T and the results for three different settings (i.e., digital, scan and scan with defense) are\nreported. Our patch obtains the best transfer results and performs well in a complex setting: scan with defense (see Table 5). This result confirms the potentially harmful behavior of our patch in the real-world."
        },
        {
            "heading": "4.3 QUALITATIVE PHYSICAL EXPERIMENTS",
            "text": "In this section, we give some qualitative results concerning the physicality of our attack. We select three objects present in ImageNet-1K (banana, cup, keyboard) and record videos of them when\none patch is placed or not next to the object. During the video, patches are moved around the object. Figure 2 shows examples of our patch near objects. In conducted experimentations, all the patches were not able to transfer (tSuc lower than 2%), except for L2 and our patches. The transfer results for the L2 method, our first ((SW22) (1) 500) and second ((W 2 2) (1) 500) methods are 9.3%, 23.4% and 29.3% respectively. These results confirm that real-world classifiers can be swayed without explicit knowledge of their architecture or their weights."
        },
        {
            "heading": "4.4 ABLATION STUDIES",
            "text": "We study the impact of the choice of the targeted layers on the patch transferability. We apply our attack ((W22) version) on different layers for Swin models and report results in Table 6. The last layer of the encoder (l = lJ ) seems essential to model and close the gap between the corrupted image distribution and the target distribution. It is coherent since this layer is expected to model and separate classes before linear classification. We observe that the multi-layer objective leads to better results as it helps the optimization to converge to a better local minimum, leading to a stronger patch. However, most layers fail to model the targeted distribution correctly. In Appendix I, we propose to study the effect of the power p and the number of slices K."
        },
        {
            "heading": "5 DISCUSSION AND CONCLUSION",
            "text": "This paper presents a distribution-oriented method based on optimal transport for designing APAs. This new method reduces patch overfitting to the source architecture and strengthens its transferability to Convolutional Neural Networks and Transformer architectures. When designed on Swin models, our patch is the only one capable of strongly fooling multiple architectures from different model families, even when the model robustness has been enhanced by a defense mechanism. Hybrid and physical experiments illustrate that our attack can disturb real-world classifiers without any knowledge of the system."
        },
        {
            "heading": "6 ETHICS STATEMENT",
            "text": "This paper presents how to make attacks potentially harmful to real-life deep networks. Ignoring the existence of attacks like the one presented in this work leaves systems with a false sense of security and may be disastrous to numerous applications like autonomous vehicles. Research promotes transparency and fosters a proactive approach to addressing potential vulnerabilities. We hope that our work empowers individuals and organizations to take necessary precautions, ultimately leading to a safer and more secure AI landscape.\nAlthough this paper is just one snapshot of the attack/defense race, we want to summarize some recommendations based on our experiments.\n\u2022 Despite Swin being a state-of-the-art model, we advise against relying on it for critical computer vision functions. Indeed, an attacker using our method would design its patch on Swin models to maximize its transferability across the different network families. However, as we show, patches designed on Swin models are the most critical for Swin models.\n\u2022 Conversely, using either of ConvNext or AT model seems a good shot: AT models combined with defense are quite resilient to patch not designed on them with the drawback of a moderate initial performance, and, ConvNext are the best trade-off today (good initial performance and moderate loss of performance against an attack even when designed on ConvNext)."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work has been supported by the French government under the \u201dFrance 2030\u201d program, as part of the SystemX Technological Research Institute within the Confiance.ai project. This work was granted access to the HPC resources of IDRIS under the allocation 20AD-011014245 made by GENCI."
        },
        {
            "heading": "B Feature point method instability 15",
            "text": ""
        },
        {
            "heading": "C Feature point method generalization 19",
            "text": ""
        },
        {
            "heading": "D Benefits of Optimal Transport 19",
            "text": ""
        },
        {
            "heading": "E Model robustness and patch position 20",
            "text": ""
        },
        {
            "heading": "F Ensemble methods 21",
            "text": ""
        },
        {
            "heading": "G Transferability on adversarially trained models 21",
            "text": ""
        },
        {
            "heading": "H Robustness according to physical transformations 22",
            "text": ""
        },
        {
            "heading": "I Ablation studies 23",
            "text": ""
        },
        {
            "heading": "J Decision boundary-based methods overfitting 24",
            "text": ""
        },
        {
            "heading": "K Complementary tables 27",
            "text": ""
        },
        {
            "heading": "L Printable patches 28",
            "text": "A IMPLEMENTATION DETAILS\nOur method training routine uses the PyTorch library (Paszke et al., 2019). For the training of each patch on medium and large models we consider a single NVIDIA V100-32G or a single NVIDIA A100 respectively. To train patches on smaller NVIDIA cards we should reduce the batch size. When not specified, patches are designed to target one the following classes: salamander, starfish, bird house, bullfrog, pinwheel, mongoose, brown bear, accordion and common iguana. We use Expectation over Transformations (EoT (Eykholt et al., 2018)) to obtain a more physically realizable patch, similarly to prior work on APAs (Brown et al., 2017; Lee & Kolter, 2019; Casper et al., 2022). For all the methods (GAP (Brown et al., 2017), LaVAN (Karmon et al., 2018), L2 (Inkawhich et al., 2019) and ours), during training, we randomly rotate the patch up to five degrees for the x and y-axis and up to 10 degrees for the z-axis. We also randomly scale the patch between 70 \u00d7 70 to 110 \u00d7 110 pixels, adjust patch brightness between [\u22120.1, 0.1] and patch blur between [0.8, 1.2], and apply normal noise of magnitude 0.1 on the patch. Patches are randomly translated in the image but not in the center. To control the balance between the adversarial loss and the total variation loss, the gradient of each loss is computed individually, normalized, and combined using a weighted sum. Following Nesti et al. (2022) we choose wadv = 1 and wTV = 0.1 where wadv is the weight for the adversarial loss and wTV is the weight for the TV loss.\nComputation time. We measure and report the computation time of each method in Table 7. This Table reports the averaged computational time for the different methods. Our method has a similar computational time as other methods. This result may be counterintuitive as OT losses are known to be slow, but in our setting, the number of samples is low. The M3D method (Zhao et al., 2023) is\nmuch slower than other methods. It is coherent, this method trains alternatively the patch and two models in a min-max game."
        },
        {
            "heading": "B FEATURE POINT METHOD INSTABILITY",
            "text": "To measure the stability of the L2 method (Inkawhich et al., 2019), we launch the optimization for three randomly selected target points. Patches are designed to sway ResNet50-v1 or Swin-T to output the class Australian terrier. Figure 3 plots the learning curves and the resulting patches for our distribution-based approach for Resnet50-v1 and Swin-T, respectively. Figure 4 and 5 plot the learning curves and the resulted patches of the L2 method for Resnet50-v1 and Swin-T, respectively. These four graphs show that our method is the easiest to optimize and is more robust to optimization artifacts. For the Swin-T model, the optimization for the L2 method becomes noisy. Table 8 reports the transfer results of the obtained patches from previous figures. Although the optimization has converged for the first target of the L2 method for ResNet50-v1, the obtained patch is harmless. Even if the APA works, its attacking capacity depends on the considered target point. For example, the mean transferability on Swin-T can decreased by a factor four. In general, our distribution-oriented approach outperforms the L2 method."
        },
        {
            "heading": "C FEATURE POINT METHOD GENERALIZATION",
            "text": "We provide in this section a proof that the exact 2-Wasserstein distance coincide with the L2-based method Inkawhich et al. (2019) when the source distribution is uniformly distributed and the targeted distribution is supported by a unique point. We recall that\nW22(\u00b5, \u03bd) = inf \u03c0\u2208\u03a0(\u00b5,\u03bd) \u222b Rd\u00d7Rd ||x\u2212 y||2d\u03c0(x, y), (6)\ndefines the 2-Wasserstein distance. This distance can be interpreted through a probabilistic point of view. If we name (X,Y ) a couple of random variables over Rd \u00d7 Rd with X \u223c \u00b5, Y \u223c \u03bd and (X,Y ) \u223c \u03c0 \u2208 \u03a0(\u00b5, \u03bd), we can write\nW22(\u00b5, \u03bd) = min (X,Y )\nE(X,Y ) [ ||X \u2212 Y ||2 ] . (7)\nIf we suppose that the target distribution is composed by a unique point, i.e., \u03bd = \u03b4y , then we have\nW22(\u00b5, \u03bd) = min X\nEX [ ||X \u2212 y||2 ] . (8)\nIn our problem we have empirical distribution based on samples, we name \u00b5\u0302n and \u03bd\u0302m the empirical distributions of \u00b5 based on n samples and \u03bd based on m samples respectively. We suppose that each sample from each distribution is uniformly distributed, i.e., \u00b5\u0302n = 1n \u2211n i=1 \u03b4xi and \u03bd\u0302m = 1 m \u2211m j=1 \u03b4yj , where \u03b4 is the Kronecker symbol. The estimated 2-Wasserstein distance is\nW22(\u00b5\u0302n, \u03bd\u0302m) = min \u03c0\u2208\u03a0(\u00b5,\u03bd) n\u2211 i=1 m\u2211 j=1 \u03c0ij ||xi \u2212 yj ||2. (9)\nIf we suppose that the target distribution is composed by a unique point, i.e., \u03bd\u0302 = \u03b4y , then we have\nW22(\u00b5\u0302n, \u03bd\u0302) = 1\nn n\u2211 i=1 ||xi \u2212 y||2. (10)\nwhich is equal to the L2-based criterion. Minimizing with respect to the 2-Wasserstein is equivalent to consider the L2-based criterion (Inkawhich et al., 2019). As a result, our method includes and generalizes the L2-based method."
        },
        {
            "heading": "D BENEFITS OF OPTIMAL TRANSPORT",
            "text": "Optimal transport-based losses (both exact and sliced) has the following advantages:\n\u2022 OT losses take into account the underlying metric space (through the cost matrix) on which the probability distributions are defined,\n\u2022 for non-overlapping distributions such as ours, the Kullback-Leibler divergence is infinite.\nTo illustrate the first point, we consider the toy example shown in Figure 6. We define four different one-dimensional distributions supported here by five points. We compute the 1-Wasserstein distance and the KL divergence between the red and the blue distributions for each column (results are shown between graphs). The blue mass has been moved near the first point from right to left. The 1- Wasserstein distance captures this mass shift, while the KL divergence does not and remains constant. This toy example highlights that OT losses capture the underlying geometry on which distributions are defined. More details concerning the advantages of OT over other methods can be found in (Arjovsky et al., 2017) (Part 2: Different Distances)."
        },
        {
            "heading": "E MODEL ROBUSTNESS AND PATCH POSITION",
            "text": "In this section, we evaluate the robustness of models according to the patch position in images. We consider the same families of models as before. We define nine patch positions and measure the patch transferability when the patch is fixed at one of these positions. Figure 7 represents the nine patch positions. We regroup these positions into three categories: Corner, Cross, and Center. We measure the patch transferability for a patch of size 40\u00d7 40 (\u2248 3% image size). Results are averaged over methods (GAP (Brown et al., 2017), LaVAN (Karmon et al., 2018), L2 (Inkawhich et al., 2019) and ours), classes, and categories of patch position. Table 9 reports the patch transferability according to its position. CNNs-v1 models are much more biased by the center of images than other network families. The accuracy of CNNs-v1 drops by a factor of 14 % when the patch is moved to corners to the center of images. This effect is not entirely due to the occluding of the object of interest since the patch is very small. Very recent families of networks (CNext and Swin models) are the more balanced networks in using context in images. For these models, the accuracy is nearly the same when the patch is placed in either corners or the center. To measure the actual efficiency of patches and to not occlude the object of interest in the case of large patches, its patches may not be placed in the center of images."
        },
        {
            "heading": "F ENSEMBLE METHODS",
            "text": "Ensemble methods train a single patch across an ensemble of models simultaneously. We determine if an attacker building his attack on an ensemble of CNN-v1 models can significantly increase its attacking performance on CNext or Swin models. We consider the following ordered list of models E-CNN-v1 = {ResNet50/34/18-v1, DenseNet121} in which networks are added to the ensemble in this order. Figure 8 plots the targeted success rate (tSuc) as a function of the number of models in the ensemble. Even with the largest ensemble of four models, patches failed to significantly increase their transferability performances on CNext and Swin models. This result confirms that an attacker expecting to sway all the models uniformly should design his attack on Swin models using our methodology. Figure 8 also shows that the feature point method becomes unstable with the increased number of models in the ensemble."
        },
        {
            "heading": "G TRANSFERABILITY ON ADVERSARIALLY TRAINED MODELS",
            "text": "In this section, we study the robustness of Adversarially Trained (AT) models. We consider two scenarios: when the patch is learned on AT models and when not. To strongly transfer on an AT model, the patch must be designed on an AT model (Table 10). None of the other source models can show good transferability results when applied to AT models. These results suggest that AT models learn different representations than other networks. From Table 11, we see that the GAP method (Brown et al., 2017) and our method are the best procedures to design a patch to target an AT model."
        },
        {
            "heading": "H ROBUSTNESS ACCORDING TO PHYSICAL TRANSFORMATIONS",
            "text": "In this section, we measure the robustness of patches according to physical transformations. We evaluate the L2 (Inkawhich et al., 2019), our exact Wasserstein (W22)\n(1) and Sliced-Wasserstein (SW22) (1) 500 patches as they are the only to transfer in the easiest scenario, i.e., without patch rotation, medium brightness and small distance patch-camera (Section 4.3 of the main article). Patch transferability is measured according to z-axis rotations (rotations in the image plane), variation of light (low and high) and distance between camera and the object (the patch is placed near the object). Results are reported in Table 12 and Figure 9. Our patches transfer even in the worst-case scenario (far from the camera or when rotated), while other patches do not. This indicates that our patches may be critical in real-world scenarios. Globally, our method produces patches with better transferability than other methods."
        },
        {
            "heading": "I ABLATION STUDIES",
            "text": "In this section, we study the effect of our method hyper-parameters. We solve the exact and the sliced Wasserstein distance for p \u2208 {1, 2} and report the results in Table 13. This Table shows that both values of p lead to the same transferability. To penalize higher feature values, we set the value of p = 2.\nWe launch the Sliced-Wasserstein distance (SW) for the following number of projections: K \u2208 {500, 1000, 5000, 10000, 50000}. There is no clear advantage to considering many projections (Table 16). The best transferability results are obtained with K = 500.\nWe now study the effect of the number of attacked layers (N ). In Table 14, we report the transferability results according to different numbers of targeted layers. We obtain better results for the exact Wasserstein distance when considering multiple layers. We observe that it helps the optimization to converge to a better local minimum, leading to stronger patches. For the Sliced-Wasserstein distance, targeting multiple layers seems counterproductive. Table 15 details the result presented in the article on the choice of the essential layer to target. The last layer of the encoder (l = lJ) seems essential to model and close the gap between the two distributions and, particularly, for the Sliced-Wasserstein distance.\nTo evaluate the data dependency of our method, we create different targeted distributions by changing the number of points which compose it (m = 1, 2, 10, 100, 300, 600, 900). We launch the optimization of patches for five different sampling seeds and three different classes. We consider the Swin-T model as the source model. We evaluate patches using the same procedure explained in the main article (Section 4). We report the results of the three runners-up baselines (GAP, LaVAN and TTP). As these methods do not consider distributions, they correspond to straight lines in the figure. From Figure 11 we see that the average targeted success rate (tSuc) increases with respect to the number of target samples. When considering multiple points, our method leads to better transfer results and is more stable than the L2-based method (see B). Our method performs better than decision-boundarybased methods (GAP, LaVAN and TTP). However, we would like to emphasize that our method requires multiple images of the target class to overcome the limitations of the L2-based approach (see Appendix B). This data dependency is a practical limitation of our method. This practical limitation may be simply leveraged by considering the training data of the source model when available."
        },
        {
            "heading": "J DECISION BOUNDARY-BASED METHODS OVERFITTING",
            "text": "In this section, we conduct an additional experiment to support that decision boundary-based methods learn a patch that tends to overfit on the source model classifier. For this purpose, we consider the transfer not between 2 different models but between 2 models sharing the same encoder but different classifiers. We select from the different methods patches trained to attack the source model Swin-T (Liu et al., 2021). On top of this Swin-T encoder, we train a new linear classifier from scratch on the ImageNet train set (Deng et al., 2009). This new linear classifier reaches the same level of clean accuracy as the previous classifier (from Pytorch (Paszke et al., 2019)) while being different. We measured the patch performance when targeting this new network (same encoder, different linear classifier). As expected, the transferability of decision boundary-based patches drops drastically (nearly by half) while our patches transferability remains almost the same."
        },
        {
            "heading": "K COMPLEMENTARY TABLES",
            "text": "In this section, we provide additional tables. Table 18 is the same as Table 4 present in the main paper but results are presented for different values of smoothing factors \u03bb."
        },
        {
            "heading": "L PRINTABLE PATCHES",
            "text": ""
        }
    ],
    "title": "OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY",
    "year": 2024
}