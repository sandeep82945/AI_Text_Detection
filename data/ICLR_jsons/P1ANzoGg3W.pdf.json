{
    "abstractText": "Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D indoor scene reconstruction. We introduce a novel two-phase learning approach, H2O-SDF, that discriminates between object and non-object regions within indoor environments. This method achieves a nuanced balance, carefully preserving the geometric integrity of room layouts while also capturing intricate surface details of specific objects. A cornerstone of our two-phase learning framework is the introduction of the Object Surface Field (OSF), a novel concept designed to mitigate the persistent vanishing gradient problem that has previously hindered the capture of high-frequency details in other methods. Our proposed approach is validated through several experiments that include ablation studies.",
    "authors": [
        {
            "affiliations": [],
            "name": "SURFACE FIELDS"
        },
        {
            "affiliations": [],
            "name": "Minyoung Park"
        },
        {
            "affiliations": [],
            "name": "Mirae Do"
        },
        {
            "affiliations": [],
            "name": "Yeon Jae Shin"
        },
        {
            "affiliations": [],
            "name": "Jaeseok Yoo"
        },
        {
            "affiliations": [],
            "name": "Jongkwang Hong"
        },
        {
            "affiliations": [],
            "name": "Joongrock Kim"
        },
        {
            "affiliations": [],
            "name": "Chul Lee"
        }
    ],
    "id": "SP:b9bae30a188ea482258b92638cb1feeaf04c936d",
    "references": [
        {
            "authors": [
                "Gwangbin Bae",
                "Ignas Budvytis",
                "Roberto Cipolla"
            ],
            "title": "Estimating and exploiting the aleatoric uncertainty in surface normal estimation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Anpei Chen",
                "Zexiang Xu",
                "Andreas Geiger",
                "Jingyi Yu",
                "Hao Su"
            ],
            "title": "Tensorf: Tensorial radiance fields",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Decai Chen",
                "Peng Zhang",
                "Ingo Feldmann",
                "Oliver Schreer",
                "Peter Eisert"
            ],
            "title": "Recovering fine details for neural implicit surface reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas Funkhouser",
                "Matthias Nie\u00dfner"
            ],
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Qiancheng Fu",
                "Qingshan Xu",
                "Yew Soon Ong",
                "Wenbing Tao"
            ],
            "title": "Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Amos Gropp",
                "Lior Yariv",
                "Niv Haim",
                "Matan Atzmon",
                "Yaron Lipman"
            ],
            "title": "Implicit geometric regularization for learning shapes",
            "venue": "In Proceedings of Machine Learning and Systems",
            "year": 2020
        },
        {
            "authors": [
                "Haoyu Guo",
                "Sida Peng",
                "Haotong Lin",
                "Qianqian Wang",
                "Guofeng Zhang",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "Neural 3d scene reconstruction with the manhattan-world assumption",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "John Lambert",
                "Zhuang Liu",
                "Ozan Sener",
                "James Hays",
                "Vladlen Koltun"
            ],
            "title": "Mseg: A composite dataset for multi-domain semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhihao Liang",
                "Zhangjin Huang",
                "Changxing Ding",
                "Kui Jia"
            ],
            "title": "Helixsurf: A robust and efficient neural implicit surface learning of indoor scenes with iterative intertwined regularization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "William E Lorensen",
                "Harvey E Cline"
            ],
            "title": "Marching cubes: A high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field",
            "year": 1998
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Zak Murez",
                "Tarrence Van As",
                "James Bartolozzi",
                "Ayan Sinha",
                "Vijay Badrinarayanan",
                "Andrew Rabinovich"
            ],
            "title": "Atlas: End-to-end 3d scene reconstruction from posed images",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Oechsle",
                "Songyou Peng",
                "Andreas Geiger"
            ],
            "title": "Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in PyTorch",
            "venue": "In NIPS Autodiff Workshop,",
            "year": 2017
        },
        {
            "authors": [
                "Johannes L Sch\u00f6nberger",
                "Enliang Zheng",
                "Jan-Michael Frahm",
                "Marc Pollefeys"
            ],
            "title": "Pixelwise view selection for unstructured multi-view stereo",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Jamie Shotton",
                "Ben Glocker",
                "Christopher Zach",
                "Shahram Izadi",
                "Antonio Criminisi",
                "Andrew Fitzgibbon"
            ],
            "title": "Scene coordinate regression forests for camera relocalization in rgb-d images",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Jiaming Sun",
                "Yiming Xie",
                "Linghao Chen",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "title": "Neuralrecon: Real-time coherent 3d reconstruction from monocular video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Tancik",
                "Pratul Srinivasan",
                "Ben Mildenhall",
                "Sara Fridovich-Keil",
                "Nithin Raghavan",
                "Utkarsh Singhal",
                "Ravi Ramamoorthi",
                "Jonathan Barron",
                "Ren Ng"
            ],
            "title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jiepeng Wang",
                "Peng Wang",
                "Xiaoxiao Long",
                "Christian Theobalt",
                "Taku Komura",
                "Lingjie Liu",
                "Wenping Wang"
            ],
            "title": "Neuris: Neural reconstruction of indoor scenes using normal priors",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "Lingjie Liu",
                "Yuan Liu",
                "Christian Theobalt",
                "Taku Komura",
                "Wenping Wang"
            ],
            "title": "Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yiqun Wang",
                "Ivan Skorokhodov",
                "Peter Wonka"
            ],
            "title": "Hf-neus: Improved surface reconstruction using high-frequency details",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yi Wei",
                "Shaohui Liu",
                "Yongming Rao",
                "Wang Zhao",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Qianyi Wu",
                "Xian Liu",
                "Yuedong Chen",
                "Kejie Li",
                "Chuanxia Zheng",
                "Jianfei Cai",
                "Jianmin Zheng"
            ],
            "title": "Object-compositional neural implicit surfaces",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Lior Yariv",
                "Jiatao Gu",
                "Yoni Kasten",
                "Yaron Lipman"
            ],
            "title": "Volume rendering of neural implicit surfaces",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zehao Yu",
                "Songyou Peng",
                "Michael Niemeyer",
                "Torsten Sattler",
                "Andreas Geiger"
            ],
            "title": "Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jingsen Zhu",
                "Yuchi Huo",
                "Qi Ye",
                "Fujun Luan",
                "Jifan Li",
                "Dianbing Xi",
                "Lisha Wang",
                "Rui Tang",
                "Wei Hua",
                "Hujun Bao"
            ],
            "title": "I2-sdf: Intrinsic indoor scene reconstruction and editing via raytracing in neural sdfs",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Wang et al",
                "2022a",
                "Liang"
            ],
            "title": "2023) and an additional 8 scenes, which were randomly selected. For each scene, we uniformly sample frames at a rate of one-tenth or one-sixth proportional to the video length. These frames are resized in 640\u00d7 480 resolution. We adopted SNU (Bae et al., 2021) as our normal estimation module, which takes a single image as input and produces a predicted normal map and uncertainty map",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The reconstruction of geometry and appearance in 3D indoor scenes using multi-view images has gained significant attention as an important field of research in both computer vision and graphics. Recent advancements in this field include techniques using Neural Radiance Fields (NeRF) (Mildenhall et al., 2020), Signed Distance Fields (SDF) (Wang et al., 2021; Yariv et al., 2021), and Occupancy Fields (Oechsle et al., 2021). Although these methods offer generally promising results, they still exhibit certain limitations. For instance, NeRF struggles to accurately capture object geometry due to an inadequate constraint between radiance and geometry (Oechsle et al., 2021). Similarly, the performance of neural implicit representation such as SDF and Occupancy Fields significantly deteriorates in indoor environments, which often contain a significant proportion of low-frequency regions like walls, ceilings, and floors (Guo et al., 2022). To address these shortcomings, subsequent research has incorporated additional constraints in the form of semantic (Guo et al., 2022) or geometric priors, such as depth or normal vectors (Wang et al., 2022a; Yu et al., 2022). These enhancements, though, have not fully resolved the issues associated with indoor scenes, particularly in reconstructing high-frequency areas, where a \u201csmoothness bias\u201d problem continues to affect the final quality of the results (Liang et al., 2023).\nNote that indoor scenes present unique challenges in terms of surface representation and learning. Specifically, they feature a mix of room layouts characterized by low-frequency, large-scale surfaces alongside various indoor objects that have high-frequency, small-scale surfaces. In technical terms, surfaces related to room layouts, which exhibit smooth characteristics, tend to converge more rapidly during the learning process than do complex, multiple object surfaces (Tancik et al., 2020). The latter is especially difficult to reconstruct, even in advanced stages of learning, due to the vanishing gradient issue. The recent approach aims to utilize a method that back-projects 2D depth information onto a 3D point cloud to guide SDF for preserving high-frequency details in multiple object regions (Zhu et al., 2023). However, directly guiding the SDF with incomplete geometric priors could negatively impact the quality of the reconstruction.\n\u2217Equal contribution. The order of these authors was determined randomly. \u2020Corresponding author.\nMotivated by the previous limitations and the inherent complexity of reconstructing indoor environments, we introduce a two-phase learning approach called H2O-SDF that stands for Holistic surface learning to Object surface learning for SDF. The first phase, focuses on global indoor scene geometry, while the second phase zooms in on the intricate geometrical and surface details of individual objects within the indoor space. For the initial phase, holistic surface learning, we present a novel rendering loss re-weighting scheme based on normal uncertainty. This effectively addresses the issues of over-smoothing and discontinuity that arise due to conflicting information from surface normals and colors. In the second phase, object surface learning, dedicated to indoor object surface learning, we recognize that rendering-based consistency alone cannot fully recover fine-grained surface details of the given objects. To overcome this, we propose a new object surface representation called the Object Surface Field (OSF). OSF still leverages SDF to capture object geometries and surfaces but they complement each other in inducing the 3D geometry reconstruction without the need for direct SDF value supervision. It effectively solves the vanishing gradient problem. Furthermore, we enhance the performance of reconstruction by leveraging an advanced OSF-guided sampling technique. As a result, object surface learning achieve more fine-grained capturing of surface details of individual objects. Fig. 1 illustrates the superiority of our approach in comparison to other existing methods like NeuS with additional normal priors (Fig. 1(b) vs Fig. 1(c,d)). Through comprehensive experiments conducted on the ScanNet dataset (Dai et al., 2017), we establish the superior performance of our method over other state-of-the-art solutions.\nIn summary, our main contributions are as follows:\n\u2022 We present a novel two-phase learning approach, which comprises holistic surface learning and object surface learning. This approach enables us to distinguish effectively between object and non-object regions in indoor scenes. Consequently, we preserve the overarching room layout while concurrently capturing intricate details within specific object areas.\n\u2022 The core driver of object surface learning is a novel concept known as the Object Surface Field (OSF). This concept enhances previous SDF to extract watertight surfaces of objects in 3D space by overcoming the vanishing gradient issue of SDF. We propose a targeted sampling strategy for OSF that can further improve the quality of reconstruction, particularly in areas requiring high-frequency detail.\n\u2022 Our methods substantially outperform previous approaches in the domain of indoor scene reconstruction. This superiority is empirically validated through extensive experimental studies, including a series of ablation tests that demonstrate the efficacy of each individual component of our two-phase learning approach."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 NEURAL IMPLICIT SURFACE REPRESENTATION",
            "text": "Inspired by the success of NeRF (Mildenhall et al., 2020), a neural field approach that encodes the geometry and radiance information of 3D coordinates using MLP has become popular. Subsequent\nworks like NeuS (Wang et al., 2021) and VolSDF (Yariv et al., 2021) enable more accurate surface reconstruction by representing SDF as an implicit function. Based on these studies, recent works(Fu et al., 2022; Wang et al., 2022b) show that the inconsistency between the rendered color depth and the 0-value surface of the SDF could lead to less desirable surface reconstructions in practice. To address this issue, Geo-NeuS (Fu et al., 2022) proposes a method that aligns sparse point clouds extracted from SFM with the zero-level set of SDF. Furthermore, HF-NeuS (Wang et al., 2022b) concludes that it is challenging to represent both high-frequency and low-frequency components with a single SDF. To overcome such a challenge, a new methodology is proposed to decompose the SDF into their respective base functions and displacement functions. Although these methods improve reconstruction quality at the object level, they face challenges in reconstructing indoor scenes. The main reason is that indoor scenes include a significant proportion of low-frequency areas (e.g., walls, floors, ceilings), making it difficult to restore them using only color supervision."
        },
        {
            "heading": "2.2 NEURAL 3D RECONSTRUCTION FOR INDOOR SCENES",
            "text": "Recently, other research works focus on inducing smoothness in the reconstruction process to improve low-frequency surfaces by incorporating geometric cues, including semantic priors (Guo et al., 2022), normal priors (Wang et al., 2022a) and depth priors (Yu et al., 2022) that are provided by pretrained models. Due to the inherent properties of the neural network, low-frequency surfaces tend to converge more rapidly than complex multiple-object surfaces characterized by high-frequency surfaces (Tancik et al., 2020). Moreover, the multiple object surfaces are hard to recover even during the advanced learning stage due to the vanishing gradient problem. To address this issue, I2-SDF proposes a method that back projects 2D depth into a 3D point cloud to guide the SDF explicitly. HelixSurf (Liang et al., 2023) shows the inaccuracy of 3D priors and proposes a method to leverage the benefits of both explicit and implicit methods. In contrast, our proposed method, H2O-SDF, captures all intricate geometrical and surface details of objects while maintaining the smoothness of room layouts by learning object surface field."
        },
        {
            "heading": "3 OUR METHOD",
            "text": "We provide an overview of our approach in Fig. 2. Our H2O-SDF pipeline consists of two phases: the first phase, holistic surface learning (Sec. 3.1), concentrates on the global scene geometry, and the second phase, object surface learning (Sec. 3.2), delves into the intricate geometrical and surface details of objects within the indoor scene. This dual-phase learning approach achieves a nuanced balance, carefully preserving the geometric integrity of room layouts while also capturing intricate surface details of specific objects."
        },
        {
            "heading": "3.1 HOLISTIC SURFACE LEARNING",
            "text": "Holistic surface learning is dedicated to reconstructing the overall geometry, including the smooth room layout, which constitutes a significant portion of the indoor scene and predominantly features low-frequency regions. We represent the scene\u2019s geometry as the signed distance function (SDF) d(x) for each spatial position x. Practically, the SDF function d is realized through a multi-layer perceptron (MLP). The geometry network is formulated as fg : x \u2208 R3 7\u2192 (d (x) \u2208 R, z (x) \u2208 R256), where z is a geometrical feature. The view-dependent color c is also implemented by an MLP. The color network is articulated as fc : (x \u2208 R3,v \u2208 R3,n (x) \u2208 R3, z(x) \u2208 R256) 7\u2192 c(x,v) \u2208 R3, where v is the view direction and n(x) = \u2207d (x) is the spatial gradient of SDF at point x. According to the volume rendering formula, the colors along a ray r are rendered by C\u0302(r) =\n\u2211N i=1 Ti \u00b7 \u03b1i \u00b7 c(xi,vi), where N is the number of sampled points along the ray, Ti =\u220fi\u22121\nj=1(1 \u2212 \u03b1j) represents the accumulated transmittance, and \u03b1i = 1 \u2212 exp(\u2212 \u222b ti+1 ti\n\u03c1(t)dt) is the discrete opacity, with the opaque density \u03c1(t) adhering to the original definition in NeuS (Wang et al., 2021). Similarly, we render the surface normal along a ray by n\u0302(r) = \u2211N i=1 Ti \u00b7 \u03b1i \u00b7 n (xi).\nHowever, as highlighted by NeuRIS (Wang et al., 2022a), there exists an inherent conflict between color and normal information. Normal priors can offer precise geometric guidance in texture-less regions, but color images might provide misleading appearance supervision in those regions due to insufficient visual features. Conversely, in regions with fine details, they exhibit the opposite behavior. That is, if color and normal losses are uniformly applied to all pixels, it results in discontinuities in planar regions, and details are overly smoothed. To mitigate this situation, we introduce a technique to re-weight normal and color loss using normal uncertainty, which exhibits low uncertainty in planar regions and high uncertainty in texture-rich surfaces.\nWe initially estimate the normal n\u0302(r) and the corresponding uncertainty ur of each pixel using a pre-existing monocular normal estimation model (Bae et al., 2021). Subsequently, we determine the weight of normal loss \u03bbn and color loss \u03bbc with (\u03b2n\u2212ur) and (\u03b2c+ur) respectively, where \u03b2n and \u03b2c are two trade-off hyperparameters and ur is the normal uncertainty of the ray r. Consequently, planar areas with low uncertainty, such as room layout and large objects, are predominantly influenced by normal rather than color loss, leading to more coherent and smoother planes. Conversely, texturerich areas are more influenced by color, preserving details and maintaining sharpness. The normal and color loss are defined below with each corresponding ground truth n(r) and C(r).\nLn = \u2211 r\u2208R \u2225n\u0302(r)\u2212 n(r)\u22251 \u00b7 (\u03b2n \u2212 ur), Lc = \u2211 r\u2208R \u2225\u2225\u2225C\u0302(r)\u2212C(r)\u2225\u2225\u2225 1 \u00b7 (\u03b2c + ur) (1)\nWe employ the Eikonal loss (Gropp et al., 2020) to further regularize the gradients of SDF: Leik =\u2211N i=1 1 N (\u2225\u2207d(xi)\u22252 \u2212 1) 2"
        },
        {
            "heading": "3.2 OBJECT SURFACE LEARNING",
            "text": "Object surface learning is propelled by a novel concept that we introduce as object surface field (OSF). OSF directs SDF to encapsulate small-scale geometry and high-frequency details on objects while preserving the smoothness of room layout. The 2D object surface loss is a preliminary step in obtaining the initial value of OSF. The 3D object surface loss, enables OSF to carefully learn the object surface from SDF, letting SDF to capture high-frequency details under the guidance of OSF. To further improve the reconstruction performance, we leverage OSF-guided sampling strategy to prioritize object surfaces.\n2D Object Surface Loss Object surface field (OSF) essentially represents the probability of object surface osf(x) for each spatial point x. It is realized by an MLP fo : (x \u2208 R3, z(x) \u2208 R256) 7\u2192 osf(x) \u2208 R, and the OSF of the ray r is rendered by: osf(r) = \u2211N i=1 Ti \u00b7 \u03b1i \u00b7 osf(xi). As a preliminary step in learning OSF, we define 2D object surface loss, L2dosf = BCE(osf(r),1o(r)) where BCE is the binary cross-entropy loss, and the indicator function 1o returns 1 if the object surface exists along the ray, and 0 otherwise. It is determined by the 2D object mask, which can be easily obtained using a pre-trained model (Lambert et al., 2020). L2dosf is utilized to minimize the discrepancy between the predicted OSF of the ray and the given 2D object mask, thereby establishing the initial value of OSF.\n3D Object Surface Loss 3D object surface loss encourages SDF to capture the high-frequency details of objects by interacting with OSF. As depicted in Fig. 3(a), while L2dosf trains OSF to learn the approximate object region, there is still a lack of information from the 2D object mask for OSF to learn the precise object boundary. Also, as it employs volume rendering to accumulate osf based on density-based weight, it struggles to provide supervision to empty spaces with low density, leading to a noisy OSF. Conversely, as Fig. 3(b) illustrates, the 3D object surface loss enables OSF to learn the object surface meticulously, allowing it to provide a supervision signal to the object surface of SDF. For this, we propose the 3D object surface loss as below:\nL3dosf = 1\nN N\u2211 i=1 [1o(r) \u00b7 osf(xi) \u00b7 |osf(xi)\u2212 \u03c3\u03b3(xi)|+ (1\u2212 1o(r)) \u00b7 osf(xi)] (2)\nwhere, the scaled sigmoid function \u03c3\u03b3(x) = 11+exp(\u03b3\u00b7d(x)) , \u03b3 is a hyperparameter determining the steepness of the function, and N is the number of sampled points along the ray r. This loss functions differently depending on the existence of the object surface along the ray.\nLGE Internal Use Only\nThe 3D object surface loss is designed for OSF to adhere to the object surface of SDF, simultaneously learning low probability on the non-object surfaces effectively. When the ray intersects with the object surface, the probability of the object surface should alter according to the sign of the SDF. In essence, during the traversal of the ray within the zero-level set of the SDF, it is anticipated that the object surface probability should escalate upon entry and diminish upon exit. It is motivated by ObjectSDF (Wu et al., 2022), which explains the correlation between the SDF and the 3D semantic field using a scaled sigmoid function. Unlike ObjectSDF, where SDF values are directly transformed into semantic field values and learned by 2D rendering-based loss, our method introduces a distinct object surface field to mutually guide the object surface of the SDF. When the ray intersects only with the room layout sur-\nface, the OSF of all points along the ray should exhibit low probability. Thus, L3dosf is structured to prevent OSF activation on the room-layout surface. In multi-view settings, it can effectively prompt OSF to decrease for occluded space located behind objects as well as the room layout. When the ray intersects with the room layout surface after colliding with the object surface, as the probability of OSF has already reduced, OSF remains unaffected by changes in SDF.\nMoreover, we observe that the reliability of point clouds extracted from multi-view stereo (MVS) is superior in object areas with abundant visual features, while it is inferior in texture-less areas. As a consequence, we propose a refinement loss to aid the learning of the OSF with the point clouds. The refinement loss is defined as follows:\nLref = \u2212 1\nNi \u2211 xj\u2208Pi 1o(xj) \u00b7 log(osf(xj)) (3)\nHere, Ni is the number of points in the point clouds Pi from view Vi, and 1o(xj) denotes whether an input point belongs to an object surface or not, returning a value of 1 or 0 respectively. This approach enhances the performance of the OSF network, allowing for more precise learning of intricate object regions with thin structures and high occlusion.\nMutual Induction of OSF and SDF In this section, we explain in more detail 1) how SDF directs OSF to learn the surface of the object and 2) how a well-learned OSF enables SDF to capture high-frequency geometric detail. As depicted in Fig. 4(a), during the initial stage of object surface learning, the OSF in regions with high rendering weight (w = T \u00b7\u03b1) are trained to converge towards 1 due to L2dosf (Eq. 3.2). Initially, the OSF represents high values not only on the surface but also inside the object. However, \u2202L3dosf /\u2202osf(x) is computed as follows:\n\u2202L3dosf \u2202osf(x) = { \u03c3\u03b3(x)\u2212 2 \u00b7 osf(x), if osf(x) < \u03c3\u03b3(x) 2 \u00b7 osf(x)\u2212 \u03c3\u03b3(x), otherwise\n(4)\nAs illustrated in Fig. 4(b), \u2202L3dosf /\u2202osf(x) yields negative values only in the red region, when osf(x) < \u03c3\u03b3(x) < 2 \u00b7 osf(x). In other words, the OSF is induced to have high values in the region where the SDF indicates that surfaces are nearby, but the OSF has not yet learned about those surfaces. Through the learning process, the OSF is trained to decrease both within and outside the object while it increases only near the surface. Consequently, the OSF is guided to represent high values exclusively on the object surfaces. The following process describes how the well-trained OSF encourages the SDF to learn high-frequency details that have not yet been captured (see Fig. 4(c)). Considering the case that a ray crosses a thin structure such as a chair leg, \u2202L3dosf /\u2202d(x) yields the following results:\n\u2202L3dosf \u2202d(x) = { \u03b3 \u00b7 osf(x) \u00b7 \u03c3\u03b3(x) \u00b7 (1\u2212 \u03c3\u03b3(x)), if \u03c3\u03b3(x) < osf(x) \u2212\u03b3 \u00b7 osf(x) \u00b7 \u03c3\u03b3(x) \u00b7 (1\u2212 \u03c3\u03b3(x)), otherwise\n(5)\nAs illustrated in Fig. 4(d), the SDF in regions of high OSF are pulled towards the negative side when \u03c3\u03b3(x) < osf(x). This loss generates gradients to the thin structure, effectively addressing the vanishing gradient problem. By means of the Eikonal loss, the SDF can be trained to learn highfrequency details while maintaining gradients and a water-tight shape. Please refer to our appendix (Sec. A.2) for the derivation of the vanishing gradient problem in our base model.\nObject Surface Field Guided Sampling Strategy In other approaches such as NeuS, hierarchical sampling relies on the volume rendering weight (w = T \u00b7 \u03b1), derived from density. However, in typical indoor scenes, expansive planar areas, particularly room layouts, tend to stabilize early in the training phase, leaving high-frequency details insufficiently captured. Consequently, densitybased sampling over-emphasizes already well-reconstructed room layout regions, ignoring points in high-frequency areas. The application of object surface field (OSF) has mitigated the overlooked density issues in high-frequency regions. There remains a necessity to prioritize object surfaces exhibiting more intricate structures compared to room layouts. To overcome this, we introduce a novel sampling strategy, guided by the OSF. After uniformly sampling points along the ray, we iteratively perform importance sampling on the probability distribution calculated from w(x) \u00b7 osf(x). This approach lets the network to concentrate more on areas with high density and object surface probability. Consequently, the OSF-guided sampling strategy enhances the overall model to more precisely capture high-frequency details. Details of the OSF-guided sampling strategy are provided in the Appendix (Sec. A.2)."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTING",
            "text": "Datasets To evaluate the effectiveness of our proposed algorithm in real-world scenarios, we conduct empirical analysis using ScanNet (Dai et al., 2017). ScanNet is a large-scale dataset, comprising 1613 scenes with 2.5 million views. Each scene contains a sequence of frames annotated with calibrated intrinsic and extrinsic camera parameters, in addition to surface reconstruction. We choose 4 scenes from (Guo et al., 2022) and randomly selected additional 8 scenes.\nImplementation Details We use the NeuS (Wang et al., 2021) as our base model, which comprises a geometry network with an 8-layer MLP and an appearance network with a 4-layer MLP. To learn the object surface field, we employ an object surface network with a 4-layer MLP. For model optimization, we adopt the Adam optimizer with an initial learning rate of 2e-4 and then train H2O-SDF with batches of 512 rays. We conducted 40k iterations of training during the holistic surface learning phase, followed by an additional 120k iterations during the object surface learning phase. We use the following hyperparameters in our experiments: \u03b2c = 1.0, \u03b2n = 2.0, \u03bbeik = 0.1, \u03bb2dosf = 0.5, \u03bb3dosf = 0.5, \u03bbref = 0.1. Our code is implemented with PyTorch (Paszke et al., 2017), conducting all experiments on a single NVIDIA RTX 3090Ti GPU. We provide more implementation details in the Appendix (Sec. A.1). Compared Methods We compare our H2O-SDF against other traditional MVS method such as COLMAP (Scho\u0308nberger et al., 2016) and neural volume rendering methods, including NeRF (Mildenhall et al., 2020), VolSDF (Yariv et al., 2021), NeuS (Wang et al., 2021), ManhattanSDF (Guo et al., 2022), NeuRIS (Wang et al., 2022a), MonoSDF (Yu et al., 2022), and HelixSurf (Liang et al., 2023).\nMetrics For a quantitative comparison of 3D surface reconstruction, we utilize five standard metrics introduced in (Murez et al., 2020): Accuracy, Completeness, Precision, Recall, and F-score. Following (Sun et al., 2021), we adopt F-score as the comprehensive metric to measure the quality of 3D reconstruction, as it considers both accuracy and completeness."
        },
        {
            "heading": "4.2 COMPARISONS",
            "text": "3D Reconstruction To demonstrate the reconstruction ability of H2O-SDF, we provide qualitative and quantitative comparisons with other state-of-the-art methods. Our method demonstrates significantly smoother reconstruction results in low-frequency areas compared to other methods, highlighting the benefits of our re-weighting scheme (See Fig. 5). Moreover, thanks to the object surface learning, H2O-SDF can better represent the fine-grained surface details of object geometries (e.g., chair legs or lamp) compared to other methods. Additionally, as shown in Tab. 1, our method outperforms previous methods, achieving a new state-of-the-art performance in every 3D geometry metric. These results, altogether, demonstrate the effectiveness of our two-phased learning approach by substantially improving the overall quality of the indoor scene reconstruction.\nNormal Predictions We compare the normal prediction quality of H2O-SDF against a monocular normal estimation method in (Bae et al., 2021) and other normal prior based reconstruction method in Wang et al. (2022a). As illustrated in Fig. 6, H2O-SDF excels at reconstructing both broad, low-frequency regions and fine-grained surface details compared to other methods. More normal comparison results are provided in the Appendix (Sec. A.3).\nGeneralizability To further investigate the applicability and generalization of H2O-SDF, we conducted additional experiments on the Replica (Straub et al., 2019) dataset and 7-Scenes (Shotton et al., 2013) dataset. As shown in Tab. 2, our method achieves superior 3D reconstruction results, irrespective of any specific domain of application."
        },
        {
            "heading": "4.3 ANALYSIS",
            "text": "Ablation Study In this section, to validate the effectiveness of individual components in H2O-SDF, we conduct ablation studies in different settings: (1) NeuS: Base model, (2) Model A: NeuS with normal priors, (3) Model B: the model undergoes training solely through our first stage, Holistic Surface Learning defined in Sec. 3.1, (4) Model C: Model B with Object Surface Field (OSF) defined in Sec. 3.2. Fig. 7 illustrates how the proposed components enhance reconstruction performance. Model-A demonstrates the ability to reconstruct low-frequency areas but still exhibits noise. With the proposed HSL, Model-B yields smoother and more consistent room layout surfaces than Model-A. By incorporating the OSF, we observe that it better captures intricate details within object regions, such as stands and chair legs. Finally, incorporating with OGS, our full model more suc-\ncessfully facilitates the reconstruction of fine-grained detailed regions. Quantitative results in Tab. 3 demonstrate the effectiveness of H2O-SDF through the progressive addition of different components to the baseline model. More ablation studies are provided in the Appendix (Sec. A.3).\nObject Surface Learning As shown in Fig. 8, we conduct additional analysis on the effectiveness of object surface learning in the case of thin structure regions. NeuS with a normal prior exhibit higher weights in the floor regions characterized by low-frequency surfaces rather than in thin structure regions due to the vanishing gradient problem. On the other hand, the introduction of object surface field (OSF) exhibits peak values in the thin structure regions with zero SDF values, as illustrated in Fig. 8(c). This demonstrates that OSF directs gradients to thin structures. By incorporating the OSF-Guided Sampling strategy (OGS), H2O-SDF resolves weight ambiguity more effectively."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We introduce a novel two-phase learning approach for 3D indoor reconstruction. The first phase, holistic surface learning, reconstructs the overall geometry of a scene, resolving conflicts in color and normal information. The second phase, object surface learning, addresses the vanishing gradient problem and enables the capturing of detailed object surface geometry by leveraging the object surface field (OSF). This concept is interesting as OSF extends beyond existing 2D priors, functioning as a 3D geometry cue that delivers 3D spatial information to the SDF. Our method has proven to achieve state-of-the-art reconstruction accuracy and smoothness on ScanNet. Future work will focus on integrating faster convergence radiance fields (e.g. TensoRF (Chen et al., 2022)) to expedite training and developing applications like scene-editing using OSF."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 ADDITIONAL IMPLEMENTATION DETAILS\nNetwork Architecture Our detailed network architecture is illustrated in Fig. 9. Our method uses three networks to encode the implicit radiance field, implicit signed distance field, and object surface field. The geometry network takes as input position x mapped by positional encoding (Mildenhall et al., 2020) and outputs SDF values and geometry features.\nExperimental Setting We test our method with 4 scenes as in the previous study (Wei et al., 2021; Wang et al., 2022a; Liang et al., 2023) and an additional 8 scenes, which were randomly selected. For each scene, we uniformly sample frames at a rate of one-tenth or one-sixth proportional to the video length. These frames are resized in 640\u00d7 480 resolution. We adopted SNU (Bae et al., 2021) as our normal estimation module, which takes a single image as input and produces a predicted normal map and uncertainty map as output. To facilitate the re-weighting scheme, we normalized the uncertainty map to a range between 0 and 1. Additionally, we take MSeg (Lambert et al., 2020) as our semantic segmentation estimation module, employing its officially provided pre-trained model. To create the object mask used in object surface learning, we masked the predicted segmentation results by assigning a value of 1 to categories related to objects and 0 to all other categories.\nEvaluation Metrics To evaluate the 3D reconstruction surface quality, we use the Accuracy, Completeness, Precision, Recall, and F-score with a threshold of 5cm. The definitions of 3D reconstruction metrics are shown in Tab. 4(a). P and P\u2217 are the set of points sampled from the predicted mesh and ground truth mesh. We use four standard metrics to evaluate the rendered normal quality, and definitions of normal evaluation metrics are shown in Tab. 4(b). n and n\u2217 are the predicted normal vector and ground truth normal vector.\nA.2 ADDITIONAL METHOD DETAILS\nTraining step We train H2O-SDF in two phases: Phase 1 - a holistic surface learning step, incorporating Lhol = \u03bbc \u00b7 Lc + \u03bbn \u00b7 Ln + \u03bbeik \u00b7 Leik, and Phase 2 - an object surface learning step, adding Lobj = \u03bb2dosf \u00b7 L2dosf + \u03bb3dosf \u00b7 L3dosf + \u03bbref \u00b7 Lref , with an OSF-guided sampling in the later stage. The final loss function is Lhol + Lobj .\nVanishing Gradient Problem In this section, we explain the vanishing gradient problem that exists in SDF-based neural representation. The explanation is motivated by I2-SDF (Zhu et al., 2023) which is based on VolSDF (Yariv et al., 2021). Unlike the paper, our model is based on NeuS (Wang et al., 2021), so we demonstrate the gradient vanishing problem on the NeuS based method.\nSuppose the loss for the view-dependent color c is a function L = c(\u03c1(d(x))), where \u03c1 is the density and d(x) is a signed distance function (SDF) for point x. The derivative of L with respect to x can be written as \u2202L\u2202c \u2202c \u2202\u03c1 \u2202\u03c1 \u2202d \u2202d \u2202x (Zhu et al., 2023). The following derivation is to show the derivative of L vanishes rapidly as point x is getting far from the surface.\nAccording to NeuS, for \u03c1(x) > 0, density is defined as\n\u03c1(x) = \u2212d\u03a6sdx (d(x)) \u03a6s(d(x))\n(6)\nwhere \u03a6s(d(x))) =\n1\n1 + e\u2212s\u00b7d(x) (7)\nd\u03a6s dx (d(x)) can be computed by\n\u2202\u03a6s \u2202d \u2202d \u2202x =\nse\u2212s\u00b7d(x)\n(e\u2212s\u00b7d(x) + 1)2 \u00b7 \u2202d \u2202x\n(8)\nNote that d follows the characteristic of SDF by Eikonal loss that,\n\u2202d \u2202x \u2248 1 (9)\nHence, d\u03a6s dx (d(x)) = se\u2212s\u00b7d(x) (e\u2212s\u00b7d(x) + 1)2 (10)\nTherefore, if we express \u03c1(x) in terms of d(x),\n\u03c1(x) =\n\u2212 se \u2212s\u00b7d(x)\n(e\u2212s\u00b7d(x) + 1)2\n1\n1 + e\u2212s\u00b7d(x)\n= \u2212 se \u2212s\u00b7d(x)\n1 + e\u2212s\u00b7d(x) (11)\nIn this case, the partial derivative of \u03c1 with respect to d can be calculated as:\n\u2202\u03c1 \u2202d =\ns2 \u00b7 es\u00b7d(x)\n(es\u00b7d(x) + 1)2 (12)\nStandard deviation 1/s is a trainable parameter that approaches to zero as the network training converges (Wang et al., 2021). In other words, s is trained with a sufficiently large value.\nThe graph of \u2202\u03c1/\u2202d is shown in Fig. 10, and as s increases, the width of the peak in the graph narrows. As a result, as d(x) gets larger, which means the point x becomes far from the surface, \u2202\u03c1/\u2202d becomes near zero rapidly. It indicates the gradient of loss vanishes fast with increasing d(x).\nObject Surface Field Guided Sampling Strategy Details In this section, we describe in detail about object surface field guided point sampling strategy with an example. Fig. 11(a) shows a simplified indoor scene scenario in which multiple objects and room layout exist. In this case, a ray passes by a green object while hitting an orange object and a room layout (gray).\nIn the previous methods like NeuS, importance sampling is performed based on volume rendering weight (w = T \u00b7 \u03b1) that can be derived from density. However, in this case, we observe the unnecessary weight generated near the green object surface (See Fig. 11(b) NeuS). It can be explained as a bias problem caused by inconsistency between the volume rendering weight and the SDF implicit surface. Inconsistency happened because there is a lack of constraint between the color field and\nthe geometry field (Chen et al., 2023). As a result, due to the bias problem, unnecessary points are sampled while thin structure (orange object) is ignored.\nHowever, as shown in Fig. 11(b) H2O-SDF w/o OGS, the introduction of the object surface field (OSF) shows its efficacy in alleviating the bias problem. The OSF learns a tight SDF surface and provides 3D supervision to the object surface and empty space, acting as a regularizer to geometry and volume rendering weight. Fig. 8 also demonstrates this effect, showing a reduction in unnecessary weight on the empty space.\nWe observe that the density-based sampling strategy still slightly over-emphasizes the already wellreconstructed room layout regions in the initial stage. To address this issue, we propose an OSFguided sampling strategy to prioritize object surfaces exhibiting more intricate structures compared to room layouts. The probability of sampling is computed based on w(x) \u00b7 osf(x), where x is a spatial position. As illustrated in Fig. 11(b) H2O-SDF, this strategy enhances the effectiveness of the OSF, inducing more densely sampled in regions where object surface probability and volume rendering weight are both high. Especially, it makes the learning process more focused on OSF and SDF collaboratively guiding each other, thereby improving the process of SDF capturing highfrequency detail.\nRefinement Loss Function Details In this section, we provide more details of the refinement function loss. The point mask 1o(xj) in Eq. 5 is defined as:\n1o(xj) = { 1, osf(xj) \u2265 \u03b8 0, osf(xj) < \u03b8\n(13)\nwhere \u03b8 is a hyper-parameter to estimate whether input points belong to an object surface or not, and we set \u03b8 = 0.5.\nA.3 ADDITIONAL EXPERIMENTS\nCurves of Standard Deviation We additionally provide the standard deviation curve of the SDF during the learning process. For training the SDF network, NeuS introduces the probability density function to apply the volume rendering method and is defined as follows: \u03d5s(x) = se\u2212sx/ (1 + e\u2212sx)\n2. Here, 1/s represents the standard deviation of \u03d5s(x). During the network training converges, 1/s gradually decreases and tends towards zero, indicating a higher level of certainty and accuracy in the learned SDF representation. As shown in Fig. 12, our method exhibits near-zero standard deviation values compared to NeuS with normal and segmentation priors. This visualization result demonstrates that our method provides a robust geometry cue, enabling the effective learning of SDF.\nAdditional Analysis on the Object Surface Learning To further analyze the effectiveness of our method, we provide additional experimental results that specifically focus on large object regions and layout regions pass through by empty space regions between multiple objects (See Fig. 13). As illustrated in Fig. 13(b), NeuS with a normal prior exhibits inconsistencies between the SDF implicit surface and the rendered surface. This inconsistency suggests the presence of a bias problem, highlighting the fact that simply incorporating geometry priors to guide the SDF representation does\nnot guarantee the accurate learning of the true SDF. In contrast, as illustrated in Fig. 13 (c) and (d), our method successfully builds the appropriate connection between the SDF implicit surface and the rendered surface. These results demonstrate the capability of object surface learning to accurately learn and represent the true SDF, surpassing the limitations of using geometry priors alone.\nAblation Study In this section, we report more detailed ablation study results for individual components in H2O-SDF. The notations used in the table are as follows: (1) NeuS : Base model, (2) Normal: integration with normal prior, (3) Reweight: re-weighting scheme for Holistic Surface Learning defined in Sec. 3.1 (4) 2D: 2D object surface loss L2dosf , (5) 3D: 3D object surface loss L3dosf , (6) Ref: refinement loss Lref and (7) OGS: Object Surface Field guided sampling strategy. Tab. 5 shows the progressive enhancement in performance with the integration of each component within the framework. This incremental improvement highlights the distinct effectiveness and contribution of each component to the overall model\u2019s efficacy.\nNormal Predictions Fig. 14 and Tab. 6 present additional results of normal prediction, which compare the proposed method with other state-of-the-art neural implicit surface learning methods. As shown in Fig. 14, our method exhibits superior normal predictions with smoother and more\nprecise geometry. Moreover, Tab. 6 also demonstrates that our approach outperformed the existing methods in all normal metrics.\n3D Reconstructions In this section, we provide additional qualitative and quantitative results of the selected 12 scenes on ScanNet. Results in Fig. 15 show that our proposed method achieves more smooth and fine-grained surface quality than other methods. According to the quantitative results (Tab. 7), our method outperforms the existing methods. However, for the scene0603 and scene0616, our method performs slightly worse compared to NeuRIS (Wang et al., 2022a) and MonoSDF (Yu et al., 2022). This discrepancy can be attributed to the absence of ground truth meshes in certain\nareas, as shown in Fig. 15 (see 5th and 6th row). Moreover, as demonstrated in the qualitative results Fig. 15, our method exhibits better reconstruction results for both scenes as well (see 5th and 6th row).\nTraining and Inference Time In Tab. 8, we present a comparison of training and inference times against baseline models. Our model demonstrates comparable speed to NeuRIS, outpacing the other models in terms of efficiency.\nObject Mesh Extraction To demonstrate the potential applications of our trained object surface field (OSF), we extracted object mesh from our final reconstruction (See Fig. 16). To extract object mesh, we find the intersection of the signed distance function (SDF) and OSF where the SDF is near the zero-level set (|d(x)| < \u03b8d) and OSF is above a certain threshold (osf(x) > \u03b8osf ). Then, the intersection of the SDF remains and the rest of the SDF is filtered out. Then the mesh extraction from SDF is accomplished using the Marching Cubes algorithm (Lorensen & Cline, 1998). The results show the effectiveness and possibility of OSF in extracting object meshes."
        }
    ],
    "year": 2024
}