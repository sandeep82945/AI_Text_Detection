{
    "abstractText": "Transformers have gained widespread usage in multivariate time series (MTS) forecasting, delivering impressive performance. Nonetheless, these existing transformer-based methods often neglect an essential aspect: the incorporation of uncertainty into the predicted series, which holds significant value in decisionmaking. In this paper, we introduce a Transformer-Modulated Diffusion Model (TMDM), uniting conditional diffusion generative process with transformers into a unified framework to enable precise distribution forecasting for MTS. TMDM harnesses the power of transformers to extract essential insights from historical time series data. This information is then utilized as prior knowledge, capturing covariate-dependence in both the forward and reverse processes within the diffusion model. Furthermore, we seamlessly integrate well-designed transformerbased forecasting methods into TMDM to enhance its overall performance. Additionally, we introduce two novel metrics for evaluating uncertainty estimation performance. Through extensive experiments on six datasets using four evaluation metrics, we establish the effectiveness of TMDM in probabilistic MTS forecasting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxin Li"
        },
        {
            "affiliations": [],
            "name": "Wenchao Chen"
        },
        {
            "affiliations": [],
            "name": "Xinyue Hu"
        },
        {
            "affiliations": [],
            "name": "Bo Chen"
        },
        {
            "affiliations": [],
            "name": "Baolin Sun"
        },
        {
            "affiliations": [],
            "name": "Mingyuan Zhou"
        }
    ],
    "id": "SP:5cfde683759859495dbea02181ce62e4142b8c2e",
    "references": [
        {
            "authors": [
                "Juan Lopez Alcaraz",
                "Nils Strodthoff"
            ],
            "title": "Diffusion-based time series imputation and forecasting with structured state space models",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Charles J Geyer"
            ],
            "title": "Practical markov chain monte carlo",
            "venue": "Statistical science, pp",
            "year": 1992
        },
        {
            "authors": [
                "Tilmann Gneiting",
                "Adrian E Raftery"
            ],
            "title": "Strictly proper scoring rules, prediction, and estimation",
            "venue": "Journal of the American statistical Association,",
            "year": 2007
        },
        {
            "authors": [
                "Alexandros Graikos",
                "Nikolay Malkin",
                "Nebojsa Jojic",
                "Dimitris Samaras"
            ],
            "title": "Diffusion models as plug-and-play priors",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xizewen Han",
                "Huangjie Zheng",
                "Mingyuan Zhou"
            ],
            "title": "Card: Classification and regression diffusion models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Irina Higgins",
                "Loic Matthey",
                "Arka Pal",
                "Christopher Burgess",
                "Xavier Glorot",
                "Matthew Botvinick",
                "Shakir Mohamed",
                "Alexander Lerchner"
            ],
            "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
            "venue": "International conference on learning representations,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Peter Dayan"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Lee Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT,",
            "year": 2019
        },
        {
            "authors": [
                "Kyoung-jae Kim"
            ],
            "title": "Financial time series forecasting using support vector",
            "venue": "machines. Neurocomputing,",
            "year": 2003
        },
        {
            "authors": [
                "Guokun Lai",
                "Wei-Cheng Chang",
                "Yiming Yang",
                "Hanxiao Liu"
            ],
            "title": "Modeling long-and short-term temporal patterns with deep neural networks",
            "venue": "The 41st international ACM SIGIR conference on research & development in information retrieval,",
            "year": 2018
        },
        {
            "authors": [
                "Yan Li",
                "Xinjiang Lu",
                "Yaqing Wang",
                "Dejing Dou"
            ],
            "title": "Generative time series forecasting with diffusion, denoise, and disentanglement",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yong Liu",
                "Haixu Wu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Non-stationary transformers: Exploring the stationarity in time series forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "James E Matheson",
                "Robert L Winkler"
            ],
            "title": "Scoring rules for continuous probability distributions",
            "venue": "Management science,",
            "year": 1976
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo",
            "year": 2011
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Kashif Rasul",
                "Calvin Seward",
                "Ingmar Schuster",
                "Roland Vollgraf"
            ],
            "title": "Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting",
            "venue": "International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kashif Rasul",
                "Abdul-Saboor Sheikh",
                "Ingmar Schuster",
                "Urs M Bergmann",
                "Roland Vollgraf"
            ],
            "title": "Multivariate probabilistic time series forecasting via conditioned normalizing flows",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kashif Rasul",
                "Young-Jin Park",
                "Max Nihl\u00e9n Ramstr\u00f6m",
                "Kyung-Min Kim"
            ],
            "title": "Vq-ar: Vector quantized autoregressive probabilistic time series forecasting",
            "venue": "arXiv preprint arXiv:2205.15894,",
            "year": 2022
        },
        {
            "authors": [
                "David Salinas",
                "Michael Bohlke-Schneider",
                "Laurent Callot",
                "Roberto Medico",
                "Jan Gasthaus"
            ],
            "title": "High-dimensional multivariate forecasting with low-rank gaussian copula processes",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas I Sapankevych",
                "Ravi Sankar"
            ],
            "title": "Time series prediction using support vector machines: a survey",
            "venue": "IEEE computational intelligence magazine,",
            "year": 2009
        },
        {
            "authors": [
                "Lifeng Shen",
                "James Kwok"
            ],
            "title": "Non-autoregressive conditional diffusion models for time series prediction",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Yusuke Tashiro",
                "Jiaming Song",
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Csdi: Conditional score-based diffusion models for probabilistic time series imputation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Toan Tran",
                "Thanh-Toan Do",
                "Ian Reid",
                "Gustavo Carneiro"
            ],
            "title": "Bayesian generative active deep learning",
            "venue": "International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural computation,",
            "year": 2011
        },
        {
            "authors": [
                "Zhiyuan Wang",
                "Xovee Xu",
                "Weifeng Zhang",
                "Goce Trajcevski",
                "Ting Zhong",
                "Fan Zhou"
            ],
            "title": "Learning latent seasonal-trend representations for time series forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Max Welling",
                "Yee W Teh"
            ],
            "title": "Bayesian learning via stochastic gradient langevin dynamics",
            "venue": "Proceedings of the 28th international conference on machine learning",
            "year": 2011
        },
        {
            "authors": [
                "Haixu Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising diffusion gans",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jiayu Yao",
                "Weiwei Pan",
                "Soumya Ghosh",
                "Finale Doshi-Velez"
            ],
            "title": "Quality of uncertainty quantification for bayesian neural network inference",
            "venue": "CoRR, abs/1906.09686,",
            "year": 2019
        },
        {
            "authors": [
                "San Francisco"
            ],
            "title": "Bay area freeways from January 2015 to December 2016",
            "year": 2020
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "Similar to most time series forecasting",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Time series forecasting plays a pivotal role in both the business and scientific domains of machine learning, serving as a vital tool for supporting decision-making in a wide array of downstream applications. These applications encompass, but are not limited to, financial pricing analysis (Kim, 2003), transportation planning (Sapankevych & Sankar, 2009), and weather pattern prediction (Chatfield, 2000), among various other fields (Rasul et al., 2022). The primary objective of time series forecasting is to predict the response variable y0:M \u2208 Rd\u00d7M based on a historical time series dataset represented as x0:N \u2208 Rd\u00d7N . This prediction process is characterized by the function f(x0:N ) \u2208 Rd\u00d7M , where f is a deterministic function that transforms the historical time series x0:N into the future time series y0:M .\nExisting time series forecasting methods commonly adopt an additive noise model to represent the future time series y0:M as the following: y0:M = f(x0:N ) + n0, where n0 follows a normal distribution N (0,\u03c32). Consequently, we can calculate the expected value of y0:M given x0:N as E[y0:M |x0:N ] = f(x0:N ). Classical time series forecasting methods (Liu et al., 2022; Wang et al., 2022; Zhou et al., 2021) that rely on this additive noise model typically provide univariate forecasts by accurately estimating the conditional mean E[y0:M |x0:N ]. These models have shown significant advancements in recent years, particularly with the adoption of transformer-based architectures. Transformers leverage self-attention mechanisms and designs tailored for handling time series characteristics effectively. This enhancement empowers transformers to excel in modeling long-term\n*Corresponding author.\ndependencies within sequential data (Wu et al., 2021), enabling the development of more potent, large-scale models (Kenton & Toutanova, 2019).\nHowever, the aforementioned methods pay less attention to whether the noise distribution can accurately capture the uncertainty of y0:M given x0:N . In time series forecasting, modeling uncertainty holds paramount importance as it directly affects our ability to assess the reliability of predictions for downstream applications (Rasul et al., 2021b). This uncertainty significantly impacts decision-making accuracy. For instance, if a point estimation model predicts that the conditional mean E[y0:M |x0:N ] for tomorrow\u2019s temperature is 12 \u25e6C, individuals still face a difficult decision regarding whether to cultivate plants today, as the morning temperature might plummet to just 4 \u25e6C, jeopardizing the plants\u2019 survival. Such models overlook the risks associated with uncertainty, which can be particularly crucial in certain contexts (Kim, 2003; Sapankevych & Sankar, 2009). As another example, if we assign the predicted temperature for tomorrow as a Gaussian distribution, the corresponding uncertainty, represented by N (12, 22) or N (12, 82), could directly influence decision-making processes. The primary objective of this paper is to recover the full distribution of future time series y0:M , conditioned on the representation captured by existing welldesigned transformer-based models. To achieve this goal,we introduce a novel framework called the Transformer-Modulated Diffusion Model (TMDM), which unifies the conditional diffusion generative process (Ho et al., 2020; Sohl-Dickstein et al., 2015; Song et al., 2020) with transformers, facilitating accurate distribution forecasting for time series.\nRecently, diffusion-based generative models have garnered significant attention due to their capacity to generate high-dimensional data and provide training stability (Han et al., 2022). These models can be viewed from various perspectives, including score matching (Hyva\u0308rinen & Dayan, 2005; Vincent, 2011) and Langevin dynamics (Neal et al., 2011; Welling & Teh, 2011). However, there has been a recent development in our understanding of these models through the lens of diffusion probabilistic models (Graikos et al., 2022). These models initially employ a forward process to transform data into noise and subsequently use a reverse process to regenerate the data from the noise (Ho et al., 2020).\nCurrent time-series diffusion models (Rasul et al., 2021a; Tashiro et al., 2021; Alcaraz & Strodthoff, 2022; Shen & Kwok, 2023) primarily concentrate on crafting effective conditional embeddings to be fed into the denoising network, which in turn guides the reverse process within the diffusion model. For instance, TimeGrad (Rasul et al., 2021a) employs the hidden state from an RNN as the conditional embedding, while TimeDiff (Shen & Kwok, 2023) constructs this embedding based on two features explicitly designed for time series data. In contrast to prevailing approaches that solely utilize conditional embeddings during the reverse process, TMDM employs conditional information as prior knowledge for both the forward and reverse processes. We believe this approach to be a more efficient means of leveraging the representations captured by existing transformer-based timeseries models (Liu et al., 2022; Wang et al., 2022) as conditions, given their proficiency in estimating the conditional mean E[y0:M | x0:N ]. Empowered by this potent prior knowledge, TMDM is geared toward capturing the uncertainty of future time series y0:M , ultimately providing a comprehensive estimate of the entire distribution.\nWe summarize our contributions as follows: (1) In the realm of probabilistic multivariate time series forecasting, we introduce TMDM, a transformer-based diffusion generative framework. TMDM harnesses the representations captured by well-designed transformer-based time series models as priors. We consider the covariate-dependence across both the forward and reverse processes within the diffusion model, resulting in a highly accurate distribution estimation for future time series. (2) TMDM integrates diffusion and transformer-based models within a cohesive Bayesian framework, employing a hybrid optimization strategy, it serves as a plug-and-play framework, seamlessly compatible with existing well-designed transformer-based forecasting models, leveraging their strong capability to estimate the conditional mean of time series, facilitating the estimation of complete distributions. (3) In our experimental evaluation, we explore the application of Prediction Interval Coverage Probability (PICP) (Yao et al., 2019) and Quantile Interval Coverage Error (QICE) (Han et al., 2022) as metrics in the probabilistic multivariate time series forecasting task. These metrics provide valuable insights into assessing the uncertainty estimation abilities of probabilistic multivariate time series forecasting models. Our study demonstrates TMDM\u2019s outstanding performance in four distribution metrics across six real-world datasets, emphasizing its effectiveness in probabilistic MTS forecasting."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 DIFFUSION MODEL",
            "text": "Diffusion probabilistic models (Sohl-Dickstein et al., 2015) take the form p\u03b8(y00:M ) :=\u222b p\u03b8(y 0:T 0:M )dy 1:T 0:M , where y 1 0:M , ...,y T 0:M represent latent variables (Ho et al., 2020). One wellknown diffusion model is the denoising diffusion probabilistic model (DDPM) (Ho et al., 2020), which consists of two processes: the forward (diffusion) process and the reverse process. Following the Markov chain, the forward process gradually adds noise, transforming an input vector y00:M into a Gaussian noise vector yT0:M over T steps:\nq(y1:T0:M | y00:M ) := \u220fT t=1 q(y t 0:M | yt\u221210:M ), q(y t 0:M | yt\u221210:M ) := N ( \u221a 1\u2212 \u03b2tyt\u221210:M , \u03b2 tI) (1)\nwhere \u03b2t represents a small positive constant denoting the noise level. In practical applications, we directly sample yt0:M from y 0 0:M as the following: q(y t 0:M | y00:M ) = N ( \u221a \u03b1ty00:M , (1 \u2212 \u03b1t)I),\nwhere \u03b1\u0304t := 1\u2212\u03b2t and \u03b1t := \u220fT\nt=1 \u03b1\u0304 t. The reverse process involves denoising yt0:M back to y 0 0:M\nand is defined as a Markov chain with a learned Gaussian transition:\np\u03b8(y 0:T 0:M ) := p(y T 0:M ) \u220fT t=1 p\u03b8(y t\u22121 0:M | y t 0:M ), p\u03b8(y t\u22121 0:M | y t 0:M ) := N (\u00b5\u03b8(yt0:M , t),\u03c3\u03b8(yt0:M , t))\n(2) In DDPM (Ho et al., 2020), the parameterization of p\u03b8(yt\u221210:M | yt0:M ) is defined as:\n\u00b5\u03b8(y t 0:M , t) = 1 \u03b1t (yt0:M \u2212 \u03b2 t\u221a\n1\u2212\u03b1t \u03f5\u03b8(y\nt 0:M , t)),\n\u03c3\u03b8(y t 0:M , t) = (\u03b2\u0304\nt)1/2, if t = 1 : \u03b2\u0304t = \u03b21, else : \u03b2\u0304t = 1\u2212\u03b1 t\u22121\n1\u2212\u03b1t \u03b2 t\n(3)\nwhere the \u03f5\u03b8 is denoising function and which can be trained by solving the following optimization problem:\nmin \u03b8 L(\u03b8) := Ey0 0:M \u223cq(y0 0:M ),\u03f5\u223cN (0,I),t \u2225\u03f5\u2212 \u03f5\u03b8(yt0:M , t))\u22252 (4)\nUsing the trained denoising function \u03f5\u03b8, we can generate samples step by step from N (0, I) randomly. However, in the context of time series forecasting, the objective is to generate the future time series y0:M conditioned on the historical time series x0:N . Several studies (Rasul et al., 2021a; Tashiro et al., 2021; Alcaraz & Strodthoff, 2022; Shen & Kwok, 2023) have explored adapting diffusion models for this task by injecting historical conditional information into the reverse process to guide the generative process."
        },
        {
            "heading": "2.2 PROBABILISTIC MULTIVARIATE TIME SERIES FORECASTING",
            "text": "Given an observed history MTS x0:N = {x1, x2, ..., xN | xt \u2208 Rd}, probabilistic multivariate time series forecasting tackles the problem of estimating the distribution of the subsequent future time series y0:M = {p(y1), p(y2), ..., p(yM ) | yt \u2208 Rd}. However, it\u2019s essential to note that the exact distribution of p(yt) is computationally intractable, prompting the development of various methods for its approximation. From the perspective of diffusion generation (Sohl-Dickstein et al., 2015), we establish a Markov chain (Geyer, 1992) with learned Gaussian transitions, initiating from p(yT0:M ) = N (0, I), to estimate the distribution of y0:M .\np\u03b8(y 0:T 0:M | x0:N ) := p(yT0:M ) \u220fT t=1 p\u03b8(y t\u22121 0:M | yt0:M ,x0:N )\np\u03b8(y t\u22121 0:M | yt0:M ,x0:N ) := N (\u00b5\u03b8(yt0:M ,x0:N , t),\u03c3\u03b8(yt0:M ,x0:N , t))\n(5)\nAn essential premise of probabilistic multivariate time series forecasting is the continuity and interrelation between the observed history x0:N and the future time series y0:M . However, two significant challenges arise: firstly, how to extract valuable time series information from x0:N (Shen & Kwok, 2023), and secondly, how to effectively employ this information to guide the generative process. Existing transformer-based time series forecasting models (Liu et al., 2022; Wu et al., 2021; Wang et al., 2022; Zhou et al., 2021) tend to overlook the estimation of time series uncertainty.However, they still have specially designed structures to capture information in the time series, which can be used as conditional information directly. Regarding the second challenge, recent models (Rasul et al., 2021a; Alcaraz & Strodthoff, 2022) attempt to inject conditional embeddings into the denoising network during the reverse process. In contrast to these methods, our proposed model,\nTMDM, employs conditional information as prior knowledge for both the forward and reverse processes within the diffusion model. By integrating conditional information into the forward process, TMDM can consider a richer set of conditional information during the denoising process. This enhancement enables TMDM to better learn the inherent time series properties between x0:N (captured by the condition) and y0:M (the generated target)."
        },
        {
            "heading": "2.3 PICP AND QICE FOR ASSESSING UNCERTAINTY ESTIMATION",
            "text": "To enhance the assessment of uncertainty estimation capabilities in probabilistic multivariate time series forecasting tasks, we introduce two novel metrics: Prediction Interval Coverage Probability (PICP) (Yao et al., 2019) and Quantile Interval Coverage Error (QICE) (Han et al., 2022). The computation of PICP is as the following:\nPICP := 1N \u2211N n=1 1yn\u2265y\u0302lown \u00b7 1yn\u2264y\u0302highn (6)\nwhere y\u0302lown and y\u0302 high n represent the low and high percentiles, respectively, of our choice for the predicted y0:M outputs given the same x0:N input. In cases where the learned distribution accurately represents the true distribution, this measurement should closely align with the difference between the selected low and high percentiles (Han et al., 2022). QICE can be viewed as an extension of PICP with higher granularity and without any uncovered quantile ranges. Its computation is as the following:\nQICE := 1M \u2211M m=1 \u2223\u2223rm \u2212 1M \u2223\u2223 , where rm = 1N \u2211Nn=1 1yn\u2265y\u0302low mn \u00b7 1yn\u2264y\u0302highmn . (7) With a sufficient number of y0:M samples, the first step involves dividing them into M quantile intervals (QIs), each with approximately equal sizes. Subsequently, quantile values corresponding to each QI boundary are determined. In contrast to PICP, QICE offers a more detailed evaluation. In cases where fewer true instances fall within one QI, another QI may capture more instances, potentially leading to increased absolute errors in both QIs. Additionally, we utilize the Continuous Ranked Probability Score (CRPS) (Matheson & Winkler, 1976; Gneiting & Raftery, 2007) and CRPSsum for evaluation on each dimension of the time series. CRPSsum represents the CRPS computed for the sum of all time series dimensions."
        },
        {
            "heading": "3 PROPOSED METHOD",
            "text": "In this section, we present TMDM, a novel framework that combines the diffusion generative process (Ho et al., 2020; Sohl-Dickstein et al., 2015) with well-designed transformer structures (Liu\net al., 2022; Wang et al., 2022). These transformer models excel at accurately estimating the conditional mean E[y0:M | x0:N ], while TMDM extends this capability to recover the full distribution of the future time series y0:M . As depicted in Fig. 1, TMDM consists of two main components: a transformer-powered conditional distribution learning model (condition generative model) and a conditional diffusion-based time series generative model. These two models are integrated into a unified Bayesian framework, leveraging a hybrid optimization approach. From a conceptual standpoint, TMDM can be viewed as a Bayesian generative model (Tran et al., 2019), where the generative process can be expressed as:\np(y00:M ) = \u222b y1:T 0:M \u222b z p(yT0:M | y\u03020:M ) \u220fT t=1 p(y t\u22121 0:M | y t 0:M , y\u03020:M )p(y\u03020:M | z)p(z)dzdy1:T0:M (8)\nIn this paper, we leverage well-designed transformers, including the Non-stationary transformer (Liu et al., 2022), Autoformer (Wu et al., 2021), and Informer (Zhou et al., 2021), to capture the information embedded within the historical time series x0:M . We utilize this information to model a latent variable z, which in turn generates a conditional representation y\u03020:M . This representation serves as a condition for the subsequent forward and reverse processes."
        },
        {
            "heading": "3.1 LEARNING TRANSFORMER POWERED CONDITIONS",
            "text": "Existing time-series diffusion models (Rasul et al., 2021a; Tashiro et al., 2021; Alcaraz & Strodthoff, 2022; Shen & Kwok, 2023) have primarily focused on designing effective conditional embeddings to guide the reverse process. In contrast, our approach advocates the utilization of representations captured by well-established transformer-based time series models. This shift offers several distinct advantages. Firstly, significant advancements have been made in point estimation time series forecasting tasks in recent years. Extensive research into time series properties has resulted in the proposal of dedicated transformers tailored for this purpose (Liu et al., 2022; Wu et al., 2021; Wang et al., 2022). We contend that employing conditions derived from such transformers is more efficient than relying on self-designed conditioning embeddings. Secondly, these specialized transformers exhibit a strong capability to estimate the conditional mean E[y0:M |x0:N ]. By employing this estimated mean as the condition, the diffusion model can more effectively focus on estimating uncertainty, simplifying the generative process. Conversely, using other specially designed conditions, such as future mixup (Shen & Kwok, 2023), may introduce new information but requires the diffusion model to simultaneously estimate both the mean and uncertainty, rendering generation more complex. Finally, TMDM serves as a versatile plug-and-play framework, bridging the gap between point estimates and distribution estimates. If improved transformer structures for point estimation emerge, we can seamlessly integrate these advancements into the distribution estimation domain.\nGiven the transformer structure T (\u00b7) and the historical time series x0:N , we can capture the representation by T (x0:N ). This representation serves as the guiding factor for approximating the true posterior distribution of z. This process is defined as the following:\nq(z | T (x0:N )) \u223c N (\u00b5\u0303z(T (x0:N )), \u03c3\u0303z(T (x0:N ))) (9) Given a well-learned z, we can generate the conditional representation y\u03020:M as the following:\nz \u223c N (0, 1) and y\u03020:M \u223c N (\u00b5z(z),\u03c3z) (10) Here, we model three nonlinearity functions, \u00b5\u0303z , \u03c3\u0303z , and \u00b5z , using neural networks. We initialize \u03c3z , representing the covariance matrix, to the identity matrix I . In this manner, we define a latent variable z to summarize the information captured by well-designed transformers. This latent variable is then used to generate the conditional representation y\u03020:M for the subsequent forward and reverse processes in TMDM."
        },
        {
            "heading": "3.2 CONDITIONAL DIFFUSION-BASED TIME SERIES GENERATIVE MODEL",
            "text": "Different from vanilla diffusion models that assume the endpoint of the diffusion process, yT0:M , adheres to the standard normal distribution N (0, 1), we incorporate the conditional representation y\u03020:M into p(yT0:M ) to better account for the conditional information in Eq. 10. Drawing inspiration from Han et al. (2022), we model the endpoint of our diffusion process as the following:\np(yT0:M | y\u03020:M ) = N (y\u03020:M , I) (11) where y\u03020:M , as defined in Eq. 10, incorporates the information captured by the transformer. In Eq. 11, y\u03020:M can be viewed as prior knowledge for estimating the conditional mean E[y0:M | x0:N ]\nAlgorithm 1 Training 1: Initialize the parameters; 2: repeat 3: Draw y00:M \u223c q(y00:M | x0:N ) 4: Draw t \u223c Uniform({1, 2, ..., T}) 5: Draw \u03f5 \u223c N (0, 1) 6: Compute the loss in Eq. 16 7: Take numerical optimization step\non: \u2207LELBO 8: until converged\nAlgorithm 2 Inference 1: yT0:M \u223c N (y\u03020:M , I) 2: for t = T to 1 do 3: Calculate reparameterize: Y t0:M = (1/\u03b1\nt)(yt \u2212 (1 \u2212\u221a \u03b1t)y\u03020:M \u2212 \u221a 1\u2212 \u03b1t\u03b5\u03b8(yt, y\u03020:M , t))\n4: if t > 1: draw \u03f5 \u223c N (0, 1)\n5: yt\u221210:M = \u03b30Y t 0:M + \u03b31y t 0:M + \u03b32y\u03020:M +\n\u221a \u03b2\u0303t\u03b5\n6: else: yt\u221210:M = Y t 0:M 7: end for\nbased on x0:N . With a diffusion schedule {\u03b2t}t=1:T \u2208 (0, 1), the conditional distributions for the forward process at all other time steps can be defined as:\nq ( yt0:M | y t\u22121 0:M , y\u03020:M ) \u223c N ( yt0:M | \u221a 1\u2212 \u03b2tyt\u221210:M + (1\u2212 \u221a 1\u2212 \u03b2t)y\u03020:M , \u03b2tI ) (12)\nIn practical applications, we sample yt0:M directly from y 0 0:M with an arbitrary timestep t:\nq ( yt0:M | y00:M , y\u03020:M ) \u223c N ( yt0:M | \u221a \u03b1ty00:M + (1\u2212 \u221a \u03b1t)y\u03020:M , (1\u2212 \u221a \u03b1t)I ) (13)\nHere, we define \u03b1\u0304t := 1 \u2212 \u03b2t and \u03b1t := \u220fT\nt=1 \u03b1\u0304 t. In Eq. 12 mean term, the diffusion process can\nbe conceptualized as an interpolation between the true data y00:M and the conditional representation y\u03020:M . It commences with the true data y00:M and gradually transitions to y\u03020:M . This approach effectively leverages the reliable conditional mean estimation E[y0:M |x0:N ] by transformers T (\u00b7). In the corresponding reverse process, initiated with y\u03020:M containing information capable of accurately estimating E[y0:M |x0:N ], the generative process is significantly simplified. If the provided condition is good enough, the model can then focus exclusively on uncertainty estimation.\nSimilar to many diffusion models designed for time series (Rasul et al., 2021a; Shen & Kwok, 2023), it is crucial for the reverse process to incorporate the conditional representation y\u03020:M . Considering the forward process in Eq. 12, the corresponding manageable posterior for the forward process is:\nq ( yt\u221210:M | y00:M ,yt0:M , y\u03020:M ) \u223c N ( yt\u221210:M | \u03b30y00:M + \u03b31yt0:M + \u03b32y\u03020:M , \u03b2\u0303tI ) \u03b30 = \u03b2t \u221a \u03b1t\u22121 1\u2212\u03b1t , \u03b31 = (1\u2212\u03b1t\u22121) \u221a \u03b1\u0304t 1\u2212\u03b1t , \u03b32 = 1 + ( \u221a \u03b1t\u22121)( \u221a \u03b1\u0304t+ \u221a \u03b1t\u22121) 1\u2212\u03b1t , \u03b2\u0303 t = (1\u2212\u03b1 t\u22121) 1\u2212\u03b1t \u03b2 t (14)\nThe derivation can be find in Appendix D."
        },
        {
            "heading": "3.3 HYBRID OPTIMIZATION",
            "text": "In this paper, we integrate the condition generative model and denoising model into a unified optimization objective. The condition generative model incorporates transformer T (\u00b7) structures and networks associated with the latent variable z. Within the diffusion model component, a denoising model is trained. As specified in Eq. 8, TMDM\u2019s optimization objective is to maximize the evidence lower bound (ELBO) of the log marginal likelihood, formulated as:\nlog p ( y00:M | x0:N ) \u2265 logEq(y1:T\n0:M ,z|y0 0:M ,y\u03020:M ,T (x0:N ))\n[ p(y0:T0:M |y\u03020:M )p(y\u03020:M |z)p(z)\nq(y1:T 0:M ,z|y0 0:M ,y\u03020:M ,T (x0:N )) ]\n= Eq(y1:T 0:M |y0 0:M ,y\u03020:M ) [log p(y0:T0:M |y\u03020:M ) q(y1:T0:M |y 0 0:M ,y\u03020:M) ] + Eq(z|T (x0:N ))[log p(y\u03020:M |z)p(z) q(z|T (x0:N )) ]\n(15) In Eq. 15, the first term, denoted as Ldiffusion, guides the denoising model to predict uncertainty while subtly adjusting the condition generative model to offer a more suitable conditional representation. We view this as an advantage of hybrid optimization. The second term, Lcond, is introduced to maintain the capacity for accurate estimation of the conditional mean E[y0:M |x0:N ] by the condition generative model. It also facilitates the generation of improved conditional representations by leveraging the capabilities of a well-designed transformer. Here, DKL(q\u2225p) represents the Kullback\u2013Leibler (KL) divergence from distribution p to distribution q. The aforementioned objective\ncan be expressed as:\nLELBO = Eq[\u2212 log p(y00:M | y10:M , y\u03020:M )] +DKL(q(yT0:M | y00:M , y\u03020:M )\u2225p(yT0:M | y\u03020:M ))\n+ \u2211T t=2 DKL(q ( yt\u221210:M | y 0 0:M ,y t 0:M , y\u03020:M ) \u2225p(yt\u221210:M | y t 0:M , y\u03020:M ))\n+ Eq(z|T (x0:N ))[\u2212 log p(y\u03020:M | z)] +DKL(q (z | T (x0:N )) \u2225p(z))\n(16)\nIn Eq. 16, the first two rows originate from Ldiffusion, while the last rows stem from Lcond. As depicted in Algorithm 1, the model parameters are optimized through stochastic gradient descent in an endto-end manner. The inference process is outlined in Algorithm 2.\n4 EXPERIMENTS\n4.1 EXPERIMENT SETUP\nTable 1: Summary of dataset statistics.\nDataset Dimension Freq. Time steps Pred. steps\nExchange 8 1 Day 7,588 192 ILI 7 1 Week 966 36\nETTm2 7 15 Min 69,680 192 Electricity 321 1 Hour 26,304 192\nTraffic 862 1 Hour 17,544 192 Weather 21 10 Min 52,695 192\nDataset: Six real-world datasets with diverse spatiotemporal dynamics were chosen, comprising Electricity, ILI, ETT, Exchange, Traffic, and Weather. Table 1 presents basic statistical information about these datasets. Further details can be found in Appendix A.\nImplementation details: In our experiments, we set the number of timesteps as T = 1000 and employed a linear noise schedule with \u03b21 = 10\u22124 and \u03b2T = 0.02, consistent with the setup in Ho et al. (2020). For the PICP, we selected the 2.5th and 97.5th percentiles. Therefore, an ideal PICP value for the learned model should be 95%. We employed 100 samples to approximate the estimated distribution, and all experiments were repeated 10 times, with mean and standard deviation recorded. More details can be find in Appendix B."
        },
        {
            "heading": "4.2 MAIN RESULT",
            "text": ""
        },
        {
            "heading": "4.2.1 BASELINES",
            "text": "We extensively compare our model with 14 baselines with different experiment settings. Including diffusion-based time series models: TimeGrad (Rasul et al., 2021a), CSDI (Tashiro et al., 2021), SSSD (Alcaraz & Strodthoff, 2022), D3VAE (Li et al., 2022) and TimeDiff (Shen & Kwok, 2023); Transformer-based models: Transformer-MAF (Rasul et al., 2021b), Transformer (Vaswani et al., 2017), Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021) and NSformer (Liu et al., 2022); VAE-based models: VAE (Higgins et al., 2016), cST-ML (Zhang et al., 2020) and DAC-ML (Zhang et al., 2021); and one additional well-designed method: GP-Copula (Salinas et al., 2019)."
        },
        {
            "heading": "4.2.2 QUALITATIVE ANALYSIS",
            "text": "To emphasize our distribution estimation capabilities, we present the predicted median and visualize the 50% and 90% distribution intervals in Fig. 2. We compare TMDM with three other models: TMDM-min: A simplified version of TMDM that employs a basic transformer in the condition generative model. TimeDiff : A recent time series prediction model that operates in a non-autoregressive setting. However, it was primarily designed for point-to-point forecasting tasks, which may not prioritize probabilistic forecasting. TimeGrad: A well-known diffusion-based autoregressive model.\nOverall, TMDM demonstrates superior distribution estimation performance compared to the other three models. While TMDM-min exhibits worse mean and uncertainty estimates than TMDM, we attribute this to the use of a different transformer in the condition generative model. The NSformer employed in TMDM is more powerful for mean estimation, facilitating better overall distribution estimation. TimeDiff, designed for point-to-point forecasting tasks, generates distribution intervals with varying width. Consequently, sample points generated farther away from the true value are sparse. In challenging scenarios (columns 3, 4, and 5), the 50% distribution intervals abruptly expand due to the absence of points in the middle section. This highlights the limitation of pointto-point forecasting in capturing real multivariate time series data, making it less practical in realworld applications. TimeGrad, which relies on an RNN to capture timing information, exhibits\npoor performance when estimating longer time series. For more detailed results, please refer to the Appendix E and F."
        },
        {
            "heading": "4.2.3 QUANTITATIVE COMPARISON",
            "text": "Probabilistic multivariate time series forecasting: To assess the performance of TMDM in probabilistic multivariate time series forecasting, we applied our model to six datasets alongside several competitive baselines. Four metrics (QICE, PICP, CRPS, and CRPSsum) were employed as performance indicators for probabilistic forecasting. Additionally, we used two metrics (MSE and MAE) to evaluate other aspects of model performance. As shown in Table 2, the NSformer integrated with our framework consistently achieved state-of-the-art (SOTA) performance across all benchmark datasets. Notably, when compared to previous SOTA results, TMDM achieved a remarkable 17% reduction in QICE (from 5.32 to 4.38) for the Exchange dataset, 11% reduction (from 7.6 to 6.74) for ILI, 23% reduction (from 4.88 to 3.75) for ETTm2, 27% reduction (from 5.26 to 3.81) for Electricity, 32% reduction (from 3.5 to 2.36) for Traffic, and 24% reduction (from 5.14 to 3.87) for Weather. It is worth noting that models like D3VAE (Li et al., 2022) and TimeDiff (Shen & Kwok, 2023) were originally designed for point-to-point forecasting tasks, where probabilistic forecasting was not their primary focus. Consequently, their performance on QICE and PICP was suboptimal. However, they still demonstrated competitive performance in CRPS, CRPSsum (see Appendix C), MSE, and MAE (refer to Table 3). As CRPS may not effectively evaluate the quality of distribution range, this underscores the importance of introducing new metrics (QICE and PICP) in the evaluation of probabilistic multivariate time series forecasting models. For additional results, please refer to the Appendix C.\nAblation study: To assess the impact of each component within our proposed framework, we conducted a comparative analysis of prediction results across three datasets using five models. Presented in Table 4, MLP-cond and Autoformer-cond serve as baseline models, employing a simple MLP or Autoformer in the condition generative model of TMDM. N (0, I)-Prior leverages NSformer to generate\nconditional embeddings while assuming yT0:M \u223c N (0, I) as a prior. When comparing our proposed model, TMDM, with MLP-cond and Autoformer-cond, we observed a substantial improvement, achieving an average QICE reduction of 42% and 23% respectively. This emphasizes the effectiveness of utilizing representations captured by existing well-designed transformer-based models as conditions. Furthermore, it demonstrates our capability to seamlessly integrate advancements in transformer structures for point estimation into the domain of distribution estimation. Comparing N (0, I)-Prior with TMDM, we noted an average 19% reduction in QICE, highlighting the advantage of considering condition information as a prior for both the forward and reverse processes. Finally, comparing No-hybrid with TMDM, we observed a significant 16% reduction in QICE, underscoring the effectiveness of our proposed hybrid optimization method.\nFramework generality: Diffusion models have gained widespread attention owing to their capacity to generate high-dimensional data and their training stability (Han et al., 2022). In the context of point-to-point forecasting tasks, TMDM operates as a versatile framework that enhances training stability when paired with various transformers. We apply our framework to four prominent transformers, showcasing the performance enhancements achieved by each model in Table 5. Our method consistently reduces variance across multiple experiments and enhances the point-to-point forecasting ability of most transformers."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we present TMDM, an innovative framework that merges diffusion generative process with existing well-designed transformer models. Our approach leverages the strengths of transformers, particularly their accuracy in estimating conditional means, and extends this capability as priors across both forward and reverse processes within the diffusion model. By employing this estimated mean as the condition, the diffusion model can more effectively focus on estimating uncertainty, simplifying the generative process. TMDM stands out as a versatile plug-and-play framework, effectively closing the gap between point estimates and distribution estimates. It enables seamless integration with advanced transformer models for point estimation, promising even better forecasting accuracy. We introduce two novel evaluation metrics, enriching the toolbox for assessing uncertainty in probabilistic multivariate time series forecasting models. Our comprehensive experiments on six real-world datasets consistently demonstrate TMDM\u2019s superior performance, underscoring its effectiveness in enhancing probabilistic prediction quality."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported in part by the National Natural Science Foundation of China under Grant U21B2006; in part by Shaanxi Youth Innovation Team Project; in part by the Fundamental Research Funds for the Central Universities QTZX23037 and QTZX22160; in part by the 111 Project under Grant B18039; The work of Wenchao Chen acknowledges the support of the stabilization support of National Radar Signal Processing Laboratory under Grant (JKW202X0X) and National Natural Science Foundation of China (NSFC) (6220010437)."
        },
        {
            "heading": "A DATASETS",
            "text": "Six real-world datasets with diverse spatiotemporal dynamics are selected, including: (1) Electricity\u2020 - records the hourly electricity consumption of 321 clients from 2012 to 2014. (2) ILI\u2021 - collects the ratio of influenza-like illness (ILI) patients versus the total patients in one week, which is reported weekly by the Centers for Disease Control and Prevention of the United States from 2002 to 2021. (3) ETT(Zhou et al., 2021) - contains the data collected from electricity transformers, including load and oil temperature that are recorded every 15 minutes between July 2016 and July 2018. (4) Exchange(Lai et al., 2018) - records the daily exchange rates of eight different countries ranging from 1990 to 2016. (5) Traffic\u00a7 - contains hourly road occupancy rates measured by 862 sensors on San Francisco Bay area freeways from January 2015 to December 2016. (6) Weather\u00b6 - includes meteorological time series with 21 weather indicators collected every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in 2020. Table 1 provides basic statistical information about these datasets.\nB IMPLEMENTATION DETAILS\nIn our experiments, we set the number of timesteps as T = 1000 and employed a linear noise schedule with \u03b21 = 10\u22124 and \u03b2T = 0.02, consistent with the setup in Ho et al. (2020). The latent states z were configured to have a dimension of 512 for all datasets. For the diffusion model, we adopted a simplified network architecture compared to prior work Xiao et al. (2021); Zheng (2022). Initially, we replaced the transformer\u2019s sinusoidal position embedding with a linear embedding for the timestep. We concatenated y0 : M t and y\u03020 : M and passed the resulting vector through three fully-connected layers, each with an output dimension of 128. We performed a Hadamard product between each output vector and the corresponding timestep embedding, followed by a Softplus non-linearity, before forwarding the resulting vector to the next fully-connected layer. Finally, we applied a fourth fully-connected layer to map the vector to a one-dimensional output for the forward diffusion noise prediction. We utilized the Adam optimizer with a learning rate of 0.0001 and a batch size of 32. All experiments were implemented in PyTorch (Paszke et al., 2019) and conducted on an NVIDIA RTX 3090 24GB GPU. The prediction length can be found in Table 1. The input sequence length for ILI is set to 36, while for other datasets, it is set to 96. For the Prediction Interval Coverage Probability (PICP), we selected the 2.5th and 97.5th percentiles. Therefore, an ideal PICP value for the learned model should be 95%. We employed 100 samples to approximate the estimated distribution, and all experiments were repeated 10 times, with mean and standard deviation recorded."
        },
        {
            "heading": "C MORE QUANTITATIVE RESULT",
            "text": "To assess the performance of TMDM in probabilistic multivariate time series forecasting, we applied our model to six datasets alongside several competitive baselines. Four metrics (QICE, PICP, CRPS,\n\u2020https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 \u2021https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html \u00a7http://pems.dot.ca.gov/ \u00b6https://www.bgc-jena.mpg.de/wetter/\nand CRPSsum) were employed as performance indicators for probabilistic forecasting. Additionally, we used two metrics (MSE and MAE) to evaluate other aspects of model performance. As shown in Table 6 and Table 7 , the NSformer integrated with our framework consistently achieved stateof-the-art (SOTA) performance across all benchmark datasets. Notably, when compared to previous SOTA results, TMDM achieved a remarkable 3.42 (from 71.12 to 74.54) improvement in PCIP for the Exchange dataset, 8.26 (from 79.57 to 87.83) improvement for ILI, 0.62 (from 72.58 to 73.2) improvement for ETTm2, 3.01 (from 79.34 to 82.35) improvement, b[i] for Electricity, 2.53 (from 84.3 to 86.83) improvement for Traffic, and 6.05 (from 66.92 to 72.97) improvement for Weather. It is worth noting that models like D3VAE (Li et al., 2022) and TimeDiff (Shen & Kwok, 2023) were originally designed for point-to-point forecasting tasks, where probabilistic forecasting was not their primary focus. Consequently, their performance on QICE and PICP was suboptimal. However, they still demonstrated competitive performance in CRPS, CRPSsum (see Appendix C), MSE, and MAE (refer to Table 3). This underscores the importance of introducing new metrics (QICE and PICP) into the evaluation of probabilistic multivariate time series forecasting models.\nEvaluating model performance with time-series data at different granularities is of significant importance in real-world applications. Our proposed TMDM can excel in such scenarios for the following reasons:\n1. Similar to most time series forecasting models Zhou et al. (2021); Wu et al. (2021); Liu et al. (2022), we incorporate the actual timestamps as learnable time embeddings for each data point. Leveraging a well-designed Transformer, we can effectively capture the temporal correlations within the data. This design ensures that TMDM can adapt to various granularities of time-series data.\n2. Within the diffusion model component, we also account for the time embedding in the data. This allows the model to generate multivariate time series with information from these embeddings, accommodating time series data at different granularities.\n3. As shown in Table 1 in our paper, the selected datasets covered different granularities ranging from 10 minutes to 1 day, and TMDM demonstrated competitive performance across all databases.\nThis confirms the model\u2019s ability to handle situations where time-series data is available at different granularities.\n4. To further evaluate TMDM\u2019s ability to handle varying time-series granularities, we conducted an experiment where we randomly removed D data points from a given time series y0:M and used this modified dataset to test TMDM with the same settings as described in the paper. This challenging experiment simulates scenarios where the time intervals in multivariate time series vary, making it a rigorous test of the model\u2019s performance under changing granularities.\nAs depicted in the table above (Table 8), TMDM-60 denotes our model with a prediction length of 192 + 60, where we randomly remove 60 samples from the data. In this setting, TMDM is challenged to forecast multivariate time series (MTS) with varying granularities based on the time embedding. The results obtained from the 30 and 60 settings exhibit similar scores compared to the original setting, demonstrating the effectiveness of the proposed TMDM in accommodating time-series data with differing granularities.\nThe variation in the CRPSsum results of the baseline models compared to the published results is primarily due to differences in the experimental settings, specifically concerning the history length and prediction length. We have included the experiments on the settings in TimeGrand and CSDI in our paper, and the results can also be found as follows (Table 9 and Table 10 ):\nCSDI occasionally matches or slightly surpasses TimeGrand, yet TMDM consistently outperforms both models across various datasets and metrics. TMDM showcases superior performance in probabilistic forecasting, reflected in its lower QICE and CRPSSUM values. These results emphasize TMDM\u2019s significant advancements in predictive accuracy and distributional modeling compared to TimeGrand and CSDI across diverse datasets and evaluation metrics."
        },
        {
            "heading": "D DERIVATION FOR FORWARD PROCESS POSTERIORS:",
            "text": "In this section, we derive the mean and variance of the forward process posteriors q ( yt\u221210:M | y t\u22121 0:M ,y 0 0:M , y\u03020:M ) in Eq. 14:\nq ( yt\u221210:M | y t\u22121 0:M ,y 0 0:M , y\u03020:M ) \u221d q ( yt0:M | yt\u221210:M , y\u03020:M ) q ( yt\u221210:M | y 0 0:M , y\u03020:M ) \u221d exp(\u22121\n2 ( (yt0:M \u2212 (1\u2212\n\u221a \u03b1\u0304t)y\u03020:M \u2212 \u221a \u03b1\u0304tyt\u221210:M ) 2\n\u03b2t\n+ (yt\u221210:M \u2212\n\u221a \u03b1t\u22121y00:M \u2212 (1\u2212 \u221a \u03b1t\u22121)y\u03020:M ) 2\n1\u2212 \u03b1t\u22121 ))\n\u221d exp(\u22121 2 ( \u03b1\u0304t(yt\u221210:M )\n2 \u2212 2 \u221a \u03b1\u0304t(yt0:M \u2212 (1\u2212 \u221a \u03b1\u0304t)y\u03020:M )y t\u22121 0:M\n\u03b2t\n+ (yt\u221210:M )\n2 \u2212 2( \u221a \u03b1t\u22121y00:M + (1\u2212 \u221a \u03b1t\u22121)y\u03020:M )y t\u22121 0:M\n1\u2212 \u03b1t\u22121 ))\n= exp(\u22121 2 (A1(y t\u22121 0:M ) 2 \u2212 2A2yt\u221210:M ))\nwhere\nA1 = \u03b1\u0304t(1\u2212 \u03b1t\u22121) + \u03b2t\n\u03b2t(1\u2212 \u03b1t\u22121) =\n1\u2212 \u03b1t\n\u03b2t(1\u2212 \u03b1t\u22121)\nA2 =\n\u221a \u03b1t\u22121\n1\u2212 \u03b1t\u22121 y00:M +\n\u221a \u03b1\u0304t\n\u03b2t yt0:M + (\n\u221a \u03b1\u0304t( \u221a \u03b1\u0304t \u2212 1) \u03b2t + 1\u2212 \u221a \u03b1t\u22121 1\u2212 \u03b1t\u22121 )y\u03020:M\nand we have the posterior variance:\n\u03b2\u0303t = 1/A1 = (1\u2212 \u03b1t\u22121) 1\u2212 \u03b1t \u03b2t\nMeanwhile, the following coefficients of the terms in the posterior mean through dividing each coefficient in A2 by A1\n\u03b30 =\n\u221a \u03b1t\u22121\n1\u2212 \u03b1t\u22121 /A1 =\n\u03b2t \u221a \u03b1t\u22121\n1\u2212 \u03b1t\n\u03b31 =\n\u221a \u03b1\u0304t\n\u03b2t /A1 =\n(1\u2212 \u03b1t\u22121) \u221a \u03b1\u0304t\n1\u2212 \u03b1t\n\u03b32 = (\n\u221a \u03b1\u0304t( \u221a \u03b1\u0304t \u2212 1) \u03b2t + 1\u2212 \u221a \u03b1t\u22121 1\u2212 \u03b1t\u22121 )/A1\n= \u03b1\u0304t \u2212 \u03b1t \u2212\n\u221a \u03b1\u0304t(1\u2212 \u03b1t\u22121) + \u03b2t \u2212 \u03b2t \u221a \u03b1t\u22121\n1\u2212 \u03b1t\u22121\n= 1 + ( \u221a \u03b1t \u2212 1)( \u221a \u03b1\u0304t + \u221a \u03b1t\u22121)\n1\u2212 \u03b1t which together give us the posterior mean\n\u00b5(y00:M ,y t 0:M , y\u03020:M ) = \u03b30y 0 0:M + \u03b31y t 0:M + \u03b32y\u03020:M\n0 200 400 600 800 1\n2\n3\n4\n5\n0 100 200 300 400 1\n2\n3\n4\n5\n0 100 200 300 1\n2\n3\n4\n5\n0 50 100 150 200 1\n2\n3\n4\n5\nPrediction length: 96 Prediction length: 192 Prediction length: 336 Prediction length: 720\nFigure 3: The prediction intervals in different predict lengths of the proposed TMDM"
        },
        {
            "heading": "E DIFFERENT PREDICT LENGTHS COMPARISON OF PREDICTION INTERVALS",
            "text": "As shown in Fig. 3, we have presented prediction intervals generated by TMDM for various prediction lengths. In this experiment, we maintained a consistent history length, and it becomes evident that the prediction intervals widen as the prediction length extends. This indicates TMDM\u2019s ability to provide different levels of uncertainty when dealing with more challenge prediction tasks, a valuable characteristic for real-world applications."
        },
        {
            "heading": "F MORE COMPARISON OF PREDICTION INTERVALS FOR THE EXCHANGE AND WEATHER DATASET",
            "text": "Fig. 4, Fig. 5, and Fig. 6 offer further comparisons of prediction intervals for the Exchange and Weather Datasets. These figures include examples of both successful and challenging prediction cases."
        }
    ],
    "title": "TRANSFORMER-MODULATED DIFFUSION MODELS FOR PROBABILISTIC MULTIVARIATE TIME SERIES FORECASTING",
    "year": 2024
}