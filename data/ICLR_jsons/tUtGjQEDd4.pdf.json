{
    "abstractText": "We introduce a novel generative modeling framework grounded in phase space dynamics, taking inspiration from the principles underlying Critically damped Langevin Dynamics and Bridge Matching. Leveraging insights from Stochastic Optimal Control, we construct a more favorable path measure in the phase space that is highly advantageous for efficient sampling. A distinctive feature of our approach is the early-stage data prediction capability within the context of propagating generative Ordinary Differential Equations or Stochastic Differential Equations. This early prediction, enabled by the model\u2019s unique structural characteristics, sets the stage for more efficient data generation, leveraging additional velocity information along the trajectory. This innovation has spurred the exploration of a novel avenue for mitigating sampling complexity by quickly converging to realistic data samples. Our model yields comparable results in image generation and notably outperforms baseline methods, particularly when faced with a limited Number of Function Evaluations. Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential in the realm of generative modeling.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tianrong Chen"
        },
        {
            "affiliations": [],
            "name": "Jiatao Gu"
        },
        {
            "affiliations": [],
            "name": "Laurent Dinh"
        },
        {
            "affiliations": [],
            "name": "Evangelos A. Theodorou"
        },
        {
            "affiliations": [],
            "name": "Josh Susskind"
        },
        {
            "affiliations": [],
            "name": "Shuangfei Zhai"
        }
    ],
    "id": "SP:f3448a1952dd9675ba3b8cedd2adda1b779b57d2",
    "references": [
        {
            "authors": [
                "Brian DO Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2201.06503,",
            "year": 2022
        },
        {
            "authors": [
                "Arthur Earl Bryson"
            ],
            "title": "Applied optimal control: optimization, estimation and control",
            "venue": "CRC Press,",
            "year": 1975
        },
        {
            "authors": [
                "Tianrong Chen",
                "Guan-Horng Liu",
                "Molei Tao",
                "Evangelos A Theodorou"
            ],
            "title": "Deep momentum multimarginal schr\\",
            "venue": "odinger bridge. arXiv preprint arXiv:2303.01751,",
            "year": 2023
        },
        {
            "authors": [
                "Yongxin Chen",
                "Tryphon Georgiou"
            ],
            "title": "Stochastic bridges of linear systems",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2015
        },
        {
            "authors": [
                "Yunjey Choi",
                "Youngjung Uh",
                "Jaejun Yoo",
                "Jung-Woo Ha"
            ],
            "title": "Stargan v2: Diverse image synthesis for multiple domains",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Valentin De Bortoli",
                "Guan-Horng Liu",
                "Tianrong Chen",
                "Evangelos A Theodorou",
                "Weilie Nie"
            ],
            "title": "Augmented bridge matching",
            "venue": "arXiv preprint arXiv:2311.06978,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alex Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "arXiv preprint arXiv:2105.05233,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dockhorn",
                "Arash Vahdat",
                "Karsten Kreis"
            ],
            "title": "Score-based generative modeling with criticallydamped langevin diffusion",
            "venue": "arXiv preprint arXiv:2112.07068,",
            "year": 2021
        },
        {
            "authors": [
                "Ulrich G Haussmann",
                "Etienne Pardoux"
            ],
            "title": "Time reversal of diffusions",
            "venue": "The Annals of Probability,",
            "year": 1986
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2006.11239,",
            "year": 2020
        },
        {
            "authors": [
                "Marlis Hochbruck",
                "Alexander Ostermann"
            ],
            "title": "Exponential integrators",
            "venue": "Acta Numerica,",
            "year": 2010
        },
        {
            "authors": [
                "HJ Kappen"
            ],
            "title": "Stochastic optimal control theory",
            "year": 2008
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusionbased generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Donald E Kirk"
            ],
            "title": "Optimal control theory: an introduction",
            "venue": "Courier Corporation,",
            "year": 2004
        },
        {
            "authors": [
                "Peter E Kloeden",
                "Eckhard Platen"
            ],
            "title": "Stochastic differential equations",
            "year": 1992
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Christian L\u00e9onard",
                "Sylvie R\u0153lly",
                "Jean-Claude Zambrini"
            ],
            "title": "Reciprocal processes. a measuretheoretical point of view",
            "year": 2014
        },
        {
            "authors": [
                "Yaron Lipman",
                "Ricky TQ Chen",
                "Heli Ben-Hamu",
                "Maximilian Nickel",
                "Matt Le"
            ],
            "title": "Flow matching for generative modeling",
            "venue": "arXiv preprint arXiv:2210.02747,",
            "year": 2022
        },
        {
            "authors": [
                "Guan-Horng Liu",
                "Arash Vahdat",
                "De-An Huang",
                "Evangelos A Theodorou",
                "Weili Nie",
                "Anima Anandkumar"
            ],
            "title": "I2sb: Image-to-image schr\\\u201d odinger bridge",
            "venue": "arXiv preprint arXiv:2302.05872,",
            "year": 2023
        },
        {
            "authors": [
                "Xingchao Liu",
                "Lemeng Wu",
                "Mao Ye",
                "Qiang Liu"
            ],
            "title": "Let us build bridges: Understanding and extending diffusion generative models",
            "venue": "arXiv preprint arXiv:2208.14699,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Bernt \u00d8ksendal"
            ],
            "title": "Stochastic differential equations",
            "venue": "In Stochastic differential equations,",
            "year": 2003
        },
        {
            "authors": [
                "Stefano Peluchetti. Non-denoising forward-time diffusions."
            ],
            "title": "Stefano Peluchetti",
            "venue": "Diffusion bridge mixture transports, schr\\\u201d odinger bridge problems and generative modeling. arXiv preprint arXiv:2304.00917, 2023.",
            "year": 2021
        },
        {
            "authors": [
                "Aram-Alexandre Pooladian",
                "Heli Ben-Hamu",
                "Carles Domingo-Enrich",
                "Brandon Amos",
                "Yaron Lipman",
                "Ricky Chen"
            ],
            "title": "Multisample flow matching: Straightening flows with minibatch couplings",
            "venue": "arXiv preprint arXiv:2304.14772,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Revuz",
                "Marc Yor"
            ],
            "title": "Continuous martingales and Brownian motion, volume 293",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Simo S\u00e4rkk\u00e4",
                "Arno Solin"
            ],
            "title": "Applied stochastic differential equations, volume 10",
            "year": 2019
        },
        {
            "authors": [
                "Yuyang Shi",
                "Valentin De Bortoli",
                "George Deligiannidis",
                "Arnaud Doucet"
            ],
            "title": "Conditional simulation using diffusion schr\u00f6dinger bridges",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Vignesh Ram Somnath",
                "Matteo Pariset",
                "Ya-Ping Hsieh",
                "Maria Rodriguez Martinez",
                "Andreas Krause",
                "Charlotte Bunne"
            ],
            "title": "Aligned diffusion schr\\\u201d odinger bridges",
            "venue": "arXiv preprint arXiv:2302.11419,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Conor Durkan",
                "Iain Murray",
                "Stefano Ermon"
            ],
            "title": "Maximum likelihood training of scorebased diffusion models",
            "venue": "arXiv e-prints, pp",
            "year": 2021
        },
        {
            "authors": [
                "Robert F Stengel"
            ],
            "title": "Optimal control and estimation",
            "venue": "Courier Corporation,",
            "year": 1994
        },
        {
            "authors": [
                "Alexander Tong",
                "Nikolay Malkin",
                "Kilian Fatras",
                "Lazar Atanackovic",
                "Yanlei Zhang",
                "Guillaume Huguet",
                "Guy Wolf",
                "Yoshua Bengio"
            ],
            "title": "Simulation-free schr\\\u201d odinger bridges via score and flow matching",
            "venue": "arXiv preprint arXiv:2307.03672,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Tong",
                "Nikolay Malkin",
                "Guillaume Huguet",
                "Yanlei Zhang",
                "Jarrid Rector-Brooks",
                "Kilian Fatras",
                "Guy Wolf",
                "Yoshua Bengio"
            ],
            "title": "Improving and generalizing flow-based generative models with minibatch optimal transport",
            "venue": "In ICML Workshop on New Frontiers in Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jiongmin Yong",
                "Xun Yu Zhou"
            ],
            "title": "Stochastic controls: Hamiltonian systems and HJB equations, volume 43",
            "venue": "Springer Science & Business Media,",
            "year": 1999
        },
        {
            "authors": [
                "Qinsheng Zhang",
                "Yongxin Chen"
            ],
            "title": "Fast sampling of diffusion models with exponential integrator",
            "venue": "arXiv preprint arXiv:2204.13902,",
            "year": 2022
        },
        {
            "authors": [
                "Qinsheng Zhang",
                "Molei Tao",
                "Yongxin Chen"
            ],
            "title": "gddim: Generalized denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2206.05564,",
            "year": 2022
        },
        {
            "authors": [
                "Qinsheng Zhang",
                "Jiaming Song",
                "Yongxin Chen"
            ],
            "title": "Improved order analysis and design of exponential integrator for diffusion models sampling",
            "venue": "arXiv preprint arXiv:2308.02157,",
            "year": 2023
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "degenerate to the point mass (see example of Brownian Bridge in Appendix.D.1). It is also the solution of the Schr\u00f6dinger Bridge when the optimal pairing is available see proposition.2",
            "venue": "De Bortoli et al",
            "year": 2023
        },
        {
            "authors": [
                "Song"
            ],
            "title": "2020b), One can rewrite such PDE, \u2202pt",
            "year": 2020
        },
        {
            "authors": [
                "E \u03b2t\u03bct"
            ],
            "title": "EXPERIMENTAL DETAILS Training: We stick with hyperparameters introduced in the section.4",
            "venue": "We use 8 \u00d7 Nvidia A100",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Diffusion Models (DMs;Song et al. (2020a); Ho et al. (2020)) constitute an instrumental technique in generative modeling, which formulate a particular Stochastic Differential Equation (SDE) linking the data distribution with a tractable prior distribution. Initially, a DM diffuses data towards the prior distribution via a predetermined linear SDE. In order to reverse the process, a neural network is used to approximate the score function which is analytically available. Subsequently, the approximated score is utilized to conduct time reversal (Anderson, 1982; Haussmann & Pardoux, 1986) of this diffusion process, ultimately generating data. Recently, the Critical-damped Langevin Dynamics (CLD;Dockhorn et al. (2021)) extends the SDE framework of DM into phase space (whereas DMs operate in the position space) by introducing an auxiliary velocity variable, which is defined by tractable Gaussian distributions at the initial and terminal time steps. This augmentation induces a trajectory in position space exhibiting enhanced smoothness, as stochasticity is solely introduced into the velocity space. The distinctive structure of CLD is shown to enhance the empirical performance and sample efficiency. However, despite the success of CLD, inefficient sampling still persists due to unnecessary curvature of the dynamics (Fig.1) as it has to converge to equilibrium for sampling from the tractable prior.\nThe remarkable accomplishments of DM have also catalyzed recent advancements in generative modeling, leading to the development of Bridge Matching (BM;(Peluchetti, 2021; Liu et al., 2022; 2023)) and Flow Matching (FM;models(Lipman et al., 2022)). These models leverage dynamic transport maps underpinned by the utilization of SDEs or ODEs. Unlike DM, Bridge and Flow Matching relaxes the reliance on a forward diffusion process with an asymptotic convergence to a prior distribution over an infinite time horizon. Moreover, they exhibit a heightened degree of versatility, enabling the construction of transport maps between two arbitrary distributions by drawing\n\u2217work done while Tianrong Chen is an intern at Apple\nupon insights from domains such as optimal transport (Pooladian et al., 2023), normalizing flow (Tong et al., 2023b), and optimal control (Liu et al., 2023).\nIn this paper, we focus on enhancing the sample efficiency of velocity based generative modeling (eg, CLD) by utilizing the Stochastic Optimal Control (SOC) theory. Specifically, we leverage the outcomes of stochastic bridge within the context of linear momentum systems (Chen & Georgiou, 2015) to construct a path measure bridging the data and prior distribution. The resulting path exhibits a more straight position and velocity trajectory compared to CLD (fig.1), making it more amenable to efficient sampling. Within the broader landscape of dynamic generative modeling (ie, ODE/SDE based generative models), data point can often be represented as linear combinations of scaled intermediate data of dynamics and Gaussian noise. In our work, we re-establish this property, enabling the estimation of target data points by leveraging both state and velocity information. In the case of DM and FM, the estimation of target data is exclusively reliant on position information, whereas our method incorporates the additional dimension of velocity data, enhancing the precision and comprehensiveness of our estimations. It is also worth noting that our model exhibits the capacity to generate high fidelity images at early time steps (fig.2). In addition, we propose a sampling technique which demonstrates competitive results with small Number of Function Evaluations (NFEs), eg, 5 to 10. Table.1 demonstrates the design differences among aforementioned models. In summary, our paper presents the following contributions:\n1. We propose Acceleration Generative Modeling (AGM) which is built on the SOC theory, enabling the favorable trajectories for efficient sampling over 2nd-order momentum dynamics generative modeling such as CLD.\n2. As a result of AGM structural characteristics, it becomes possible to estimate a realistic data point at an early time point, a concept we refer to as sampling-hop. This approach not only yields a significant reduction in sampling complexity but also offers a novel perspective on accelerating the sampling in generative modeling by leveraging additional information from the dynamics.\n3. We achieve competitive results compared to DM approaches equipped with specifically designed fast sampling techniques on image datasets, particularly in small NFE settings."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "Notation: Let xt \u2208 Rd and vt \u2208 Rd denote the d-dimensional position and velocity variable of a particle mt = [xt,vt]T \u2208 R2d at time t. We denote the discretized time series as 0 \u2264 t0 < ...tn < tN < 1. The Wiener Process is denoted as wt. The identity matrix is denoted as Id \u2208 Rd\u00d7d. We define \u03a3t as the covariance matrix of xt and vt at time step t."
        },
        {
            "heading": "2.1 DYNAMICAL GENERATIVE MODELING",
            "text": "The generative modeling approaches rooted in dynamical systems, including ODE and SDE, have garnered significant attention. Here, we present three noteworthy dynamical generative models: Diffusion Model (DM), Flow Matching (FM) and Bridge Matching (BM).\nDiffusion Model: In the framework of DM, given x0 drawn from a data distribution pdata, the model proceeds to construct a SDE,\ndxt = ft(xt)dt+ g(t)dwt x0 \u223c pdata(x) (1)\nwhose terminal distributions at t = 1 approach an approximate Gaussian, i.e. x1 \u223c N (0, Id). This accomplishment is realized through the careful selection of the diffusion coefficient gt and the base drift ft(xt). It is noteworthy that the time-reversal (Anderson, 1982) of (1) results in another SDE:\ndxt = [ ft(xt)\u2212 g2t\u2207x log p(xt, t) ] dt+ g(t)dwt, x1 \u223c N (0, Id) (2)\nwhere p(\u00b7, t) is the marginal density of (1) at time t and \u2207x log pt is known as the score function. SDE (2) can be regarded as the time-reversal of (1) in such a manner that the path-wise measure is almost surely equivalent to the one induced by (1). As a consequence, these two SDEs share identical marginal over time. In practice, it is feasible to analytically sample xt given t and x0. Additionally, we can leverage a neural network to learn the score function by regressing scaled Stein Score Ext,t\u2225s\u03b8t (xt, t; \u03b8)\u2212\u2207x log p(xt, t|x0)\u222522 for the purpose of propagating (2). This learned score can then be integrated into the solution of the aforementioned SDE(2) to simulate the generation of data that adheres to the target data distribution from the prior distribution. Meanwhile, (2) also corresponds to an ODE which shares the same path-wise measure:\ndxt = [ ft(xt)\u2212 1\n2 g2t\u2207x log p(xt, t)\n] dt, x1 \u223c N (0, Id) (3)\nwhich motivates the popular sampler introduced in (Zhang & Chen, 2022; Zhang et al., 2022; Bao et al., 2022) to solve the ODE (2) efficiently.\nBridge Matching and Flow Matching: An alternative approach to exploring the time-reversal of a forward noising process involves the concept of \u2019building bridges\u2019 between two distinct distributions p0(\u00b7) and p1(\u00b7). This method entails the learning of a mimicking diffusion process, commonly referred to as bridge matching, as elucidated in previous works (Peluchetti, 2021; Shi et al., 2022). Here we consider the SDE in the form of:\ndxt = vt(x, t)dt+ gtdwt s.t. (x0, x1) \u223c \u03a00,1(x0,x1) := p0 \u00d7 p1 (4)\nwhich is pinned down at an initial and terminal point x0, x1 which are independently samples from predefined p0 and p1. This is commonly known as the reciprocal projection of x0 and x1 in the literature (Shi et al., 2023; Peluchetti, 2023; Liu et al., 2022; Le\u0301onard et al., 2014). The construction of such SDE is accomplished by meticulous design of vt. A widely adopted choice for vt is vt := (x1 \u2212 xt)/(1 \u2212 t), which induces the well-known Brownian Bridge (Liu et al., 2023; Somnath et al., 2023). Similar to the approach in DM and owing to the linear structure of the dynamics, one can efficiently estimate this drift by employing a neural network parameterized by weights \u03b8 for regression on: Ext,t\u2225v\u03b8t (xt, t; \u03b8) \u2212 vt(xt, t)\u222522 given x1 and t. As extensively discussed in previous studies (Liu et al., 2023; Shi et al., 2022), this bridge matching framework takes on the characteristics of FM (Lipman et al., 2022) when the diffusion coefficient gt tends to zero.\nRemark 1. The practice of constraining a stochastic process to specific initial and terminal conditions is a well-established setup in SOC. For a gentle introduction of it\u2019s connection with Brownian Bridge, Schro\u0308dinger Bridge please see Appendix.C. From this perspective, one can derive Brownian Bridge, as elaborated in Appendix.D.1 for comprehensive elucidation. It is imperative to note that the SOC framework will serve as the fundamental basis upon which we will develop our algorithm."
        },
        {
            "heading": "3 ACCELERATION GENERATIVE MODEL",
            "text": "We apply SOC to characterize the twisted trajectory of momentum dynamics induced by CLD(Dockhorn et al., 2021). It becomes evident that the mechanisms encompassing flow matching, diffusion modeling, and Bridge matching collectively facilitate the construction of an estimated target data point, denoted as x1, by utilizing the intermediate state of the dynamics, xt. Our additional objective is to expedite the estimation of a plausible x1 by incorporating additional dynamics-related information, such as velocity, thereby curtailing the requisite time integration.\nIn this section, we introduce the proposed method, termed as the Acceleration Generative Model (AGM), rooted in SOC theory. Building upon (Chen & Georgiou, 2015), we extend the framework by incorporating a time-varying diffusion coefficient and accommodating arbitrary boundary conditions, ultimately arriving at an analytical solution suited for the generative modeling. We demonstrate its efficacy in rectifying the trajectory of CLD, concurrently showcasing its aptitude for accurately estimating the target data at an early timestep ti, thereby enabling expeditious sampling.\nAs suggested by BM approach, there is a necessity to formulate a trajectory that bridges the two data points sampled from p0 and p1 respectively. Desirably, the intermediate trajectory should exhibit optimal characteristics that facilitate smoothness and linearity. This is essential for the ease of simulating the dynamics system to obtain the solution. In our endeavor to tackle this challenge and enhance the estimation of the data point x1 by incorporating velocity components, we encapsulate the problem within a SOC framework, specifically formulated in the phase space which reads:\nDefinition 2 (Stochastic Bridge problem of linear momentum system (Chen & Georgiou, 2015)).\nmin at \u222b 1 0 \u2225at\u222522dt+ (m1 \u2212m1)TR(m1 \u2212m1) s.t [ dxt dvt ] \ufe38 \ufe37\ufe37 \ufe38\ndmt\n=\n[ vt\nat(xt,vt, t) ] \ufe38 \ufe37\ufe37 \ufe38\nf(m,t)\ndt+ [ 0 0 0 gt ] \ufe38 \ufe37\ufe37 \ufe38\ngt\ndwt,\nm0 := [ x0 v0 ] = [ x0 v0 ] , R = [ r 0 0 r ] \u2297 Id, x1 \u223c pdata.\n(5)\nIn this context, the matrix R is recognized as the terminal cost matrix, serving to assess the proximity between the propagated m1 and the ground truth m1 at the terminal time t = 1. As the parameter r approaches positive infinity, the trajectory converges toward the state x1, prompting a transition to constrained dynamics wherein the system becomes constrained by two predetermined boundaries, namely m0 and m1. This configuration aligns seamlessly with the principles of constructing a feasible bridge, as advocated by the tenets of BM. It is worth noting that this interpolation approach essentially represents a natural extension (Chen & Georgiou, 2015) of the well-established concept of the Brownian Bridge (Revuz & Yor, 2013), which has been employed in trajectory inference (Somnath et al., 2023; Tong et al., 2023a) and image inpainting tasks (Liu et al., 2023) and its connection with Diffusion has been discussed in Liu et al. (2023). Indeed, it is evident that the target velocity lacks a precise definition within this problem, allowing for flexibility in the design space for our approach. To address this, we opt for the linear interpolation of the intermediate point and the target point, represented as v1 = (x1 \u2212 xt)/(1 \u2212 t), as the chosen terminal velocity, which also is the optimal control in the original space (see Appendix..D.1). This choice is made due to its ability to construct a trajectory characterized by straightness. Conceptually, the acceleration at continually guides the dynamics towards the linear interpolation of the two data points, serving to mitigate the impact of introduced stochasticity. In contrast to previous bridge matching frameworks, the velocity\u2019s boundary condition in our approach varies over time since it depends on the state xt and t. The velocity variable serves solely as an auxiliary component aimed at straightening the trajectories. Regarding this SOC problem formulation, the solution is,\nProposition 3 (Phase Space Brownian Bridge). When r \u2192 +\u221e, The solution w.r.t optimization problem 5 is,\na\u2217(mt, t) = g 2 tP11 ( x1 \u2212 xt 1\u2212 t \u2212 vt ) where : P11 = \u22124 g2t (t\u2212 1) . (6)\nProof. Please see Appendix.D.2.\nRemark 4. P11 denotes the second diagonal component in the matrix Pt, a solution derived from the Lyapunov equation (see Lemma.7), serving as an implicit representation of the optimality of the control. This value is dependent upon the uncontrolled dynamics, where at is set to the zero vector in (5), and will vary accordingly when uncontrolled dynamics change."
        },
        {
            "heading": "3.1 TRAINING",
            "text": "By plugging the optimal control (6) back to the dynamics (5), we can obtain the desired SDE. As been suggested by (Song et al., 2020b; Dockhorn et al., 2021), such SDE has a corresponding probablistic ODE which shares the same marginal over time in which the drift term will have an additional score term \u2207v log p(mt, t). Here we summarize the force term for SDE and ODE as:[\ndxt dvt\n] = [ vt Ft ] dt+ [ 0 0 0 ht ] dwt s.t m0 := [ x0 v0 ] \u223c N (\u00b50,\u03a30),\nBridge Matching SDE : Ft := Fbt(mt, t) \u2261 a\u2217t (mt, t), h(t) := g(t),\nProbablistic ODE : Ft := F p t (mt, t) \u2261 a\u2217t (mt, t)\u2212\n1 2 g2t\u2207v log p(m, t), h(t) := 0.\n(7)\nHenceforth, we refer to the dynamics associated with the Bridge Matching SDE as AGM-SDE, and its corresponding ODE counterpart as AGM-ODE. Meanwhile, the linearity of the system implies the intermediate state mt and the close form solution of score term are analytically available. In particular, the mean \u00b5t and covariance matrix \u03a3t of the intermediate marginal pt(mt|x1) = N (\u00b5t,\u03a3t)\nof such a system can be analytically computed with \u03a3t = [ \u03a3xxt \u03a3 xv t\n\u03a3xvt \u03a3 vv t\n] \u2297 Id, and \u00b5t = [ \u00b5xt \u00b5vt ] , pro-\nvided we have the boundary conditions \u00b50 and \u03a30 in place, as outlined in Sa\u0308rkka\u0308 & Solin (2019). Please see Appendix.D.3 for detail. In order to sample from such multi-variant Gaussian, one need to decompose the covariance matrix by Cholesky decomposition, and mt is reparamertized as:\nmt = \u00b5t + Lt\u03f5 = \u00b5t +\n[ Lxxt \u03f50\nLxvt \u03f50 + L vv t \u03f51\n] ,\u2207v log pt := \u2212\u2113t\u03f51 (8)\nwhere \u03a3t = LtLTt ,\u03f5 = [ \u03f50 \u03f51 ] \u223c N (0, I2d) and \u2113t = \u221a \u03a3xxt \u03a3xxt \u03a3 vv t \u2212(\u03a3xvt )2 .\nParameterization: The Force term can be represented as a composite of the data point and Gaussian noise. Specifically,\na\u2217(mt, t) = 4x1(1\u2212 t)2 \u2212 g2tP11 [(\nLxxt 1\u2212 t + Lxvt\n) \u03f50 + L vv t \u03f51 ] . (9)\nWe express the force term as F\u03b8t = s \u03b8 t \u00b7 zt. Here, zt assumes the role of regulating the output of the network s\u03b8t , ensuring that the variance of the network output is normalized to unity. For the detailed formulation of the normalizer zt, please refer to Appendix.D.8. In a manner similar to the BM approach, one can formulate the objective function for regressing the force term as follows:\nmin \u03b8\nEt\u2208[0,1]Ex1\u223cpdataEmt\u223cpt(mt|x1)\u03bb(t) [ \u2225F\u03b8t (mt, t; \u03b8)\u2212 Ft(mt, t)\u222522 ] (10)\nWhere \u03bb(t) is known as the reweight of the objective function across the time horizon. We defer the derivation of \u2113t and the presentation of Lt, \u03bb(t) and at in Appendix.D."
        },
        {
            "heading": "3.2 SAMPLING FROM AGM",
            "text": "Once the paramterized force term F\u03b8t is trained, we are ready to simulate the dynamics to generate the samples by plugging it back to the dynamics (7). One can use any type of SDE or ODE sampler to propagate the learnt system. Here we list our choice of sampler for AGM-SDE and AGM-ODE.\nStochastic Sampler: To simulate the SDE, prior works are majorly relying on EulerMaruyama(EM) (Kloeden et al., 1992) and related methods. We adopt the Symmetric Splitting Sampler(SSS) from Dockhorn et al. (2021) in our AGM-SDE. This selection is based on the compelling performance it offers when dealing with momentum systems.\nDeterministic Sampler: It is imperative to acknowledge that this system is inherently underactuated because the force term is exclusively injected into the velocity component, while velocity serves as the driving factor for the position\u2014a variable of primary interest in generative modeling context. More specifically, at time step ti, the impact of force does not immediately manifest in the position but rather takes effect at a subsequent time step, denoted as ti+1 after discretizing time horizon. At time t0, it becomes undesirable to propagate the state x0 using an initially uncontrolled velocity over an extended time interval \u03b40. The presence of this delay phenomenon can also exert an influence when the time interval \u03b4t is large, thereby impeding our ability to reduce the NFE during sampling. We propose the adoption of an Exponential Integrator (EI) approach, as elaborated in Zhang & Chen (2022). Empirical evidence suggests that this method aligns well with our model. We provide an illustrative example of how the AGM-ODE, in conjunction with the EI technique, can be employed to inject the learnt network into both velocity and position channels simultaneously:[\nxti+1 vti+1\n] = \u03a6(ti+1, ti) [ xt vt ] + w\u2211 j=0 [\u222b ti+1 ti (ti+1 \u2212 \u03c4) z\u03c4 \u00b7Mi,j(\u03c4)d\u03c4 \u00b7 s\u03b8t (mti\u2212j , ti\u2212j))\u222b ti+1 ti z\u03c4 \u00b7Mi,j(\u03c4)d\u03c4 \u00b7 s\u03b8t (mti\u2212j , ti\u2212j) ]\nWhere Mi,j(\u03c4) = \u220f k \u0338=j ( \u03c4 \u2212 ti\u2212k ti\u2212j \u2212 ti\u2212k ) , and \u03a6(t, s) = [ 1 t\u2212 s 0 1 ] .\n(11)\nIn Eq.11, \u03a6(s, t) denotes the transition matrix for our system, while Mi,j(\u03c4) represents the w\u2212order multistep coefficient (Hochbruck & Ostermann, 2010). For a comprehensive derivation of these terms, please refer to Appendix.D.9. It is worth noting that the mapping of s\u03b8 into both the position and velocity channels significantly emulates the errors introduced by discretization delays. Sampling-hop: In the context of CLD (Dockhorn et al., 2021), their focus is on estimating the score function w.r.t. velocity, which essentially corresponds to estimating scaled \u03f51 in our notation. However, relying solely on the aforementioned information is not sufficient for estimating the data point x1. Additional knowledge regarding \u03f50 is also required in order to perform such estimation. In our case, the training objective implicitly includes both \u03f50 and \u03f51 (see eq.9), hence one can manage to recover x1 by Proposition.5. Remarkably, our observations have unveiled that when the network is equipped with additional velocity information, it acquires the capability to estimate the target data point during the early stages of the trajectory, as illustrated in fig.2. This estimation can be seamlessly integrated into AGM-SDE and AGM-ODE and we name it sampling-hop. Specifically,\nProposition 5 (Sampling-Hop). Given the state, velocity and trained force term F\u03b8t at time step t in sampling phase, The estimated data point x\u03031 can be represented as\nx\u0303SDE1 = (1\u2212 t)(F\u03b8t + vt)\ng2tP11 + xt, or x\u0303ODE1 =\nF\u03b8t + g 2 tP11(\u03b1txt + \u03b2tvt)\n4(t\u2212 1)2 + g2tP11(\u03b1t\u00b5xt + \u03b2t\u00b5vt ) (12)\nfor AGM-SDE and AGM-ODE dynamics respectively, and \u03b2t = Lvvt + 1 2P11 ,\u03b1t =\n( Lxxt 1\u2212t +L xv t )\u2212\u03b2tL xv t\nLxxt .\nProof. See Appendix.D.10\nThis property empowers us to allocate the NFE budget selectively within the time interval t \u2208 [0, ti], where ti < tN , effectively reducing the discretization error while maintaining the sampling quality. This insight paves the way for efficient low NFE sampling strategies later. Here we summarized the training and sampling procedure of our method in Algorithm.1 and Algorithm.2 respectively.\nAlgorithm 1 Training 1: Input: data distribution pdata(\u00b7) 2: while not converge do 3: t \u223c U([0, 1]), x1 \u223c pdata(x1) 4: Compute mean and covariance \u00b5t and \u03a3t.\n(Appendix.D.3) 5: Sample mt = \u00b5t + Lt\u03f5.(eq.8) 6: Compute target Ft (eq.7) using optimal acceleration (eq.9) 7: Compute loss E [ \u03bb\u2225F\u03b8t \u2212 Ft\u222522 ] (eq.10). 8: Take gradient descent with respect to F\u03b8t (mt, t; \u03b8). 9: end while\nAlgorithm 2 Sampling 1: Input: trained F(\u00b7, \u00b7; \u03b8), discretized time\nstep [t0,\u00b7 \u00b7 \u00b7 ,ti], Choose the sampler from [SSS(SDE), EI(ODE)]. Choose prior mean and covariance \u00b50, \u03a30\n2: Sample m0 \u223c p0(m;\u00b50,\u03a30). 3: for n = 0 to i do 4: estimate F\u03b8tn(mtn , tn) 5: mtn+1 = Sampler(mtn , F \u03b8tn , tn) 6: reconstruct x\u03021 using Proposition.5. 7: end for 8: Return x\u03021\n4 EXPERIMENTAL RESULTS\nArchitectures and Hyperparameters: We parameterize s\u03b8t (\u00b7, \u00b7; \u03b8) using modified NCSN++ model as provided in Karras et al. (2022). We employ six input channels, accounting for both position and velocity variables, as opposed to the standard three channels used in the CIFAR-10 (Krizhevsky et al., 2009), AFHQv2 (Choi et al., 2020) and ImageNet (Deng et al., 2009) which leads to a negligible increase of network parameters. For the purpose of comparison with CLD in the toy dataset, we adopt the same ResNet-based architecture utilized in CLD. Throughout all of our experiments, we maintain a monotonically decreasing diffusion coefficient, given by g(t) = 3(1 \u2212 t). For the detailed experimental setup, please refer further to Appendix.E.\nEvaluation: To assess the performance and the sampling speed of various algorithms, we employ the Fre\u0301chet Inception Distance score (FID;Heusel et al. (2017)) and the Number of Function Evaluations (NFE) as our metrics. For FID evaluation, we utilize reference statistics of all datasets obtained from EDM (Karras et al., 2022) and use 50k generated samples to evaluate. Additionally, we reevaluate the FID of CLD and EDM using the same reference statistics to ensure consistency in our comparisons. For all other reported values, we directly source them from respective referenced papers.\nSelection of \u03a30: The choice of initial covariance \u03a30 directly influences the path measure of the trajectory. In our case, we set \u03a30 := [ 1 k k 1 ] with hyperparameter k. We observe that trajectories tend to exhibit pronounced curvature under specific conditions: when\nthe k is positive, the absolute value of the position is large. This behavior is particularly noticeable when dealing with images, where the data scale ranges from -1 to 1. We aim for favorable uncontrolled dynamics, as this can potentially lead to better-controlled dynamics. Our strategy is to design k in such a way that the marginal distribution of uncontrolled dynamics at tN = 1 effectively covers the range of image data values meanwhile k keeps negative. We can express the marginal of uncontrolled dynamics by leveraging the transition matrix \u03a6(1, 0), which gives us x1 := x0 + v0. Figure 3 illustrates the standard deviation of x1 for various values of k. Based on our empirical observations, we choose k = \u22120.2 for all experiments, as it effectively covers the data range. The subsequent controlled dynamics (eq.7) will be constructed based on such desired uncontrolled dynamics as established.\nStochastic Sampling: In experiments, we emphasize the advantages of using the AGM-SDE compared with CLD. Firstly, we show that our model exhibits superior performance when NFE is significantly lower than that of CLD, particularly in toy dataset scenarios. For evaluation, we utilized the multi-modal Mixture of Gaussian and Multi-Swiss-Roll datasets. The results obtained from the toy dataset, as shown in Fig.9, demonstrate that AGM-SDE is capable of generating data that closely aligns with the ground truth, while requiring NFE that is around one order of magnitude lower than CLD. Furthermore, our findings reveal that AGM-SDE outperforms CLD in the context of CIFAR-10 image generation tasks, especially when faced with limited NFE, as illustrated in Table 2.\nDeterministic Sampling: We validate our algorithm on high-dimensional image generation with a deterministic sampler. We provide uncurated samples from CIFAR-10, AFHQv2 and ImageNet64 with varying NFE in Appendix.I. Regarding the quantitative evaluation, Table.3 and Table.4 summarize the FID together with NFE used for sampling on CIFAR-10 and ImageNet-64. Notably, AGM-ODE achieves 2.46 FID score with 50 NFE on CIFAR-10, and 10.55 FID score with 20 NFE in unconditional ImageNet-64 which is comparable to the existing dynamical generative modeling.\nWe underscore the effectiveness of sampling-hop, especially when faced with a constrained NFE budget, in comparison to baselines. We validate it on the CIFAR-10 and AFHQv2 dataset respectively. Fig.4 illustrates that AGM-ODE is able to generate plausible images even when NFE= 5 and outperforms EDM(Karras et al., 2022) when NFE is extremely small (NFE<15) visually and numerically on AFHQv2 dataset. We also compare with other fast sampling algorithms built upon DM in table.5 on CIFAR-10 dataset where AGM-ODE demonstrates competitive performance. Notably, AGM-ODE outperforms the baseline CLD with the same EI sampler by a large margin. We suspect that the improvement is based on the rectified trajectory which is more friendly for the ODE solver.\nConditional Generation We showcase the capability of AGM to generate conditional samples using an unconditional model (fig.5) by incorporating conditional information into the prior velocity variable v0. Instead of employing a randomly sampled v0, we use a linear combination of v0 and the desired velocity v1 = (x1 \u2212 xt0)/(1 \u2212 t0), where x1 is conditioned data. Thus, t0, the initial velocity is defined as vcond0 := (1\u2212 \u03be)v0+ \u03bev1, with \u03be serving as a mixing coefficient. Fig.5 shows that AGM can generate conditional data without augmentation and additional fine-tuning. Such property can be extended to the inpainting task as well and the detail can be found in appendix.F."
        },
        {
            "heading": "5 CONCLUSION AND LIMITATION",
            "text": "In this paper, we introduce a novel Acceleration Generative Modeling (AGM) framework rooted in SOC theory. Within this framework, we devise more favorable, straight trajectories for the momentum system. Leveraging the intrinsic characteristics of the momentum system, we capitalize on additional velocity to expedite the sampling process by using the sampling-hop technique, significantly reducing the time required to converge to accurate predictions of realistic data points. Our experimental results, conducted on both toy and image datasets in unconditional generative tasks, demonstrate promising outcomes for fast sampling.\nHowever, it is essential to acknowledge that our approach\u2019s performance lags behind state-of-the-art methods in scenarios with sufficient NFE. This observation suggests avenues for enhancing AGM performance. Such improvements could be achieved by enhancing the training quality through the adoption of techniques proposed in Karras et al. (2022) including data augmentation, fine-tuned noise scheduling, and network preconditioning, among others."
        },
        {
            "heading": "A SUPPLEMENTARY SUMMARY",
            "text": "We state the assumptions in Appendix.B. We provide the technique details appearing in Section.3 at Appendix.D. The details of the experiments can be found in Appendix.E. The visualization of generated figures can be found in Appendix.I."
        },
        {
            "heading": "B ASSUMPTIONS",
            "text": "We will use the following assumptions to construct the proposed method. These assumptions are adopted from stochastic analysis for SGM (Song et al., 2021; Yong & Zhou, 1999; Anderson, 1982),\n(i) p0 and p1 with finite second-order moment.\n(ii) gt is continuous functions, and |g(t)|2 > 0 is uniformly lower-bounded w.r.t. t. (iii) \u2200t \u2208 [0, 1], we have \u2207v log pt(mt, t) Lipschitz and at most linear growth w.r.t. x and v.\nAssumptions (i) (ii) are standard conditions in stochastic analysis to ensure the existence-uniqueness of the SDEs; hence also appear in SGM analysis (Song et al., 2021)."
        },
        {
            "heading": "C STOCHASTIC OPTIMAL CONTROL (SOC) IN THE WILD",
            "text": "In this section, we are going to provide a gentle introduction of the Stochastic Optimal Control (SOC). Our work is majorly relying on the prior work Chen & Georgiou (2015) in which some technical details are missing. Here we first clarify some core derivations that may help the broader audience to understand Chen & Georgiou (2015) and our work."
        },
        {
            "heading": "C.1 LINEAR QUADRATIC STOCHASTIC OPTIMAL CONTROL",
            "text": "SOC has wide applications in financial, robotics, and manufacturing. Here we will focus on Linear Quadratic SOC which usually refers to Linear Quadratic Regulator because the dynamic is linear and the objective function is quadratic (Bryson, 1975; Stengel, 1994). The problem states as:\nmin ut \u222b 1 0 1 2 \u2225ut\u222522dt+ xT1Rx1\ns.t dxt = [A(t)xt + gtut]dt+ gtdwt, x0 = x0. (13)\nIn this formulation, xt means the state, and ut is the control variable. Conceptually, the SOC problem is aiming to design the controller ut to drive the system from point x0 to x1 \u2261 0 with minimum effort. The presence of stochasticity, introduced by the Wiener Process denoted as dwt, prevents the system from precisely converging to the Dirac mass x1. In order to strike a balance between the objective of converging to x1 and minimizing overall control effort \u222b \u2225ut\u222522dt, the terminal cost xT1Rx1 has been imposed.\nOne special case is x\u2018 R \u2192 \u221e. Intuitively, it means the controlled dynamics should precisely converge to x1. However, one can notice that the stochastic trajectory which connects x0 and x1 is not unique in this case. Based on this constraint (pinned down at x1 and x0 at two boundaries), the optimization problem of SOC finds the optimal solution with minimum effort ut which can be understood as the regularization of the trajectories, hence, such stochastic trajectory is unique while the regularization of controller is still applied.\nThe classical procedure to solve the SOC problem includes:\n1. write down the Hamilton\u2013Jacobi\u2013Bellman equation (HJB PDE) which explicitly represents the propagation of value function over time.\n2. Construct the Ricatti/Lyapunov Equation.\n3. Solve Ricatti/Lyapunov Equation and obtain the optimal control.\nC.2 VALUE FUNCTION, HAMILTON-JACOBIAN (HAMILTON\u2013JACOBI\u2013BELLMAN EQUATION) AND RICATTI EQUATION\nWe adopt the classical notation in the SOC for the value function. Specifically, the underscript of the value function V represents for the partial derivative of it. For example, Vt, Vx and Vxx represent for the first order derivative of V w.r.t time t , state x and second order derivate of V w.r.t x. We first define the value function as:\nV (xt, t) = inf u\nE [\u222b 1\nt\n1 2 \u2225ut\u222522d\u03c4 + xT1Rx1 ] and the dynamics is,\ndxt = (Axt + gtut)dt+ gtdwt\nFrom Bellman\u2019s principle to the value function, one can get:\nV (t,xt) = inf u\nE [ V (t+ dt,xt+dt) + \u222b t+dt t 1 2 \u2225ut\u222522d\u03c4 ]\n= inf u\nE [ 1\n2 \u2225ut\u222522dt+ V (t,xt) + Vt(t,xt)dt+ Vx(t,x)dx+\n1 2 tr [ Vxxgg T ] dt ]\n= Plug in the dynamics dxt = \u00b7 \u00b7 \u00b7\n= inf u\nE [ 1\n2 \u2225ut\u222522dt+ V (t,xt) + Vt(t,xt)dt+ Vx(t,x)T((Axt + gtut)dt+ gdwt)\n+ 1 2 tr [ Vxxgg T ] dt ]\n= inf u\n[ 1\n2 \u2225ut\u222522dt+ V (t,xt) + Vt(t,xt)dt+ Vx(t,x)T(Axt + gtut)dt\n+ 1 2 tr [ Vxxgg T ] dt ]\nOne obtain:\nVt + inf u\n[ 1\n2 \u2225ut\u222522 + V Tx (Axt + gtut)\n] + 1 2 tr [ Vxxgg T ] = 0\nThe optimal control can be obtained by\nu\u2217t = \u2212gtVx Plugging it back, one can obtain the HJB PDE:\nVt \u2212 1\n2 Vxgg\nTVx + V T x Axt +\n1 2 tr [ Vxxgg T ] = 0\nWe assume that there exist certain matrix Q, s.t. V (x, t) \u2261 12x TQx + \u039e(t). By matching the different power term of HJB, one can write:\n\u2212\u039e\u0307\u2212 1 2 xTQ\u0307x = \u22121 2 xTQggTQxT + xTATQx+ 1 2 tr [ QggT ] (14)\nwith boundary condition:\n\u039e(1) = 0, Q(1) = R (15)\nDue to the fact that xTATQx = xTQAx, one arrives Riccati Equation:\n\u2212Q\u0307 = ATQ+QA\u2212QggTQ (16)\nRecall that the optimal solution is u\u2217t = \u2212gtVx and V := 12x TQx + \u039e(t), the optimal control can be expressed in the way of the solution of Ricatti equation: u\u2217t = \u2212gTQ(t)xt."
        },
        {
            "heading": "C.3 RICATTI EQUATION AND LYAPUNOV EQUATION",
            "text": "Here we provide the connection between Ricatti Equation and Lyapunov Equation in the current setup.\nLemma 6. Define P (t) := Q(t)\u22121 in which Q(t) is the solution of Ricatti equation (eq.16), Then P (t) solve the Lyapunov equation:\nP\u0307 = AP + PAT \u2212 ggT (17)\nProof. By plugging in the Lyapunov equation P (t) := Q(t)\u22121, one can get:\n\u02d9Q\u22121 = AQ\u22121 +Q\u22121AT \u2212 ggT\n\u21d4 \u2212Q\u22121Q\u0307Q\u22121 = AQ\u22121 +Q\u22121AT \u2212 ggT\n\u21d4 \u2212Q\u0307 = QA+ATQ\u2212QggTQ\nBy Lemma.6, the optimal control can also be represented as the solution of the Lyapunov equation: u\u2217t = \u2212gTP (t)\u22121xt which is indeed the optimal control term used in Chen & Georgiou (2015) after adopting their notation, and it is same as the optimal control term we used in the Lemma.10 without base dynamics compensation."
        },
        {
            "heading": "C.4 SOC CONNECTION WITH SCHRO\u0308DINGER BRIDGE",
            "text": "The optimal control solution is also the solution of Schro\u0308dinger Bridge when the terminal condition degenerate to the point mass (see example of Brownian Bridge in Appendix.D.1). It is also the solution of the Schro\u0308dinger Bridge when the optimal pairing is available see proposition.2 De Bortoli et al. (2023).\nSo in our case, we are not solving the momentum Schro\u0308dinger Bridge as shown in Chen et al. (2023) (also see. fig.6)."
        },
        {
            "heading": "D TECHNIQUE DETAILS IN SECTION.3",
            "text": ""
        },
        {
            "heading": "D.1 BROWNIAN BRIDGE AS THE SOLUTION OF STOCHASTIC OPTIMAL CONTROL",
            "text": "We adopt the presentation form Kappen (2008). We consider the control problem:\nmin vt \u222b 1 t0 1 2 \u2225vt\u222522dt+ r 2 \u2225x1 \u2212 x1\u222522\ns.t. dxt = vtdt, x0 = x0\nWhere r is the terminal cost coefficient. According to Pontryagin Maximum Principle (PMP;Kirk (2004)) recipe, one can construct the Hamiltonian:\nH(t,x,v, \u03b3) = \u22121 2 \u2225vt\u222522 + \u03b3vt\nHence the optimized Hamiltonian is:\nH(t,x,v, \u03b3)\u2217 = 1\n2 \u03b32, where vt = \u03b3\nThen we solve the Hamiltonian equation of motion:\ndxt dt = \u2202H\u2217 \u2202\u03b3 = \u03b3\nd\u03b3 dt = \u2202H\u2217 \u2202x = 0\nwhere x0 = x0 and \u03b31 = \u2212r \u00b7 (x1 \u2212 x1)\nOne can notice that the solution for \u03b3t is the constant \u03b3t = \u03b3 = \u2212r \u00b7 (x1 \u2212 x1), hence the solution for xt is xt = x1 + \u03b3t.\n\u03b3 = \u2212r(x1 \u2212 x1) = \u2212r(x0 + (1\u2212 t0)\u03b3 \u2212 x1)\n\u2192 v\u2217t := \u03b3 = r(x1 \u2212 x0) 1 + r(1\u2212 t0)\nWhen r \u2192 +\u221e, we arrive the optimal control as v\u2217t = x1\u2212x01\u2212t0 . Due to certainty equivalence, this is also the optimal control law for\ndxt = vtdt+ dwt\nBy plugging it back into the dynamics, we obtain the well-known Brownian Bridge:\ndxt = x1 \u2212 xt 1\u2212 t dt+ dwt"
        },
        {
            "heading": "D.2 PROOF OF PROPOSITION.3",
            "text": "Lemma 7. The Lyapunov equation corresponding to the optimization problem showed in Lemma.10:\nu\u2217t \u2208 argmin ut\u2208U E [\u222b T 0 1 2 \u2225ut\u22252 ] dt+ xT1Rx1\ns.t dmt = [ 0 1 0 0 ] \ufe38 \ufe37\ufe37 \ufe38\nA\nmtdt+ utdt+ gdwt\nm0 = m0, m1 = m1\nis depited as\nP\u0307 = AP+PAT \u2212 ggT . (18)\nWhen g = [ 0 g ] , the solution for Lyapunov equation above, with terminal condition\nP1 = R \u22121 = lim\nr\u2192inf\n[ r 0 0 r ]\u22121 = [ 0 0 0 0 ] (19)\nHowever, one does not need the force to converge exactly at v1 because we only care about the generate quality of x1. Here we give a general case in which the r keeps a small value \u03c9 for the velocity channel:\nP1 = R \u22121 = [ 0 0 0 \u03c9 ] (20)"
        },
        {
            "heading": "Then the solution is given by",
            "text": "Pt =\n[ \u03c9(t\u2212 1)2 \u2212 13g 2(t\u2212 1)3 \u03c9(t\u2212 1)\u2212 12g 2(t\u2212 1)2\n\u03c9(t\u2212 1)\u2212 12g 2(t\u2212 1)2 g2(1\u2212 t) + \u03c9 ] and the inverse of Pt is,\nP\u22121t = 1\ng2(\u22124\u03c9 + g2(t\u2212 1))(t\u2212 1)\n[ 12(\u03c9\u2212g2(t\u22121))\n(t\u22121)2 6(\u22122\u03c9+g2(t\u22121))\nt\u22121 6(\u22122\u03c9+g2(\u22121+t))\nt\u22121 12\u03c9 \u2212 4g 2(t\u2212 1)\n]"
        },
        {
            "heading": "Thus,",
            "text": "P10 = \u221212\u03c9 + 6g2(t\u2212 1)\ng2[\u22124\u03c9 + g2(t\u2212 1)](t\u2212 1)2 = \u221212\u03c9 g2[\u22124\u03c9 + g2(t\u2212 1)](t\u2212 1)2 + 6 [\u22124\u03c9 + g2(t\u2212 1)](t\u2212 1)\nP11 = 12\u03c9 \u2212 4g2(t\u2212 1)\ng2[\u22124\u03c9 + g2(t\u2212 1)](t\u2212 1) =\n12\u03c9\ng2[\u22124\u03c9 + g2(t\u2212 1)](t\u2212 1) + \u22124 [\u22124\u03c9 + g2(t\u2212 1)]\nProof. One can plug in the solution of Pt into the Lyapunov equation Pt and it validates Pt is indeed the solution.\nRemark 8. Here we provide a general form when the terminal condition of the Lyapunov function is not a zero matrix. It explicitly means that it allows that the velocity does not necessarily need to converge to the exact predefined v1. It will have the same results as shown in the paper by setting \u03c9 = 0.\nLemma 9. The state transition function \u03a6(t, s) of following dynamics, dmt = [ 0 1 0 0 ] mtdt\nis,\n\u03a6(t, s) = [ 1 t\u2212 s 0 1 ]\nProof. One can easily verify that such \u03a6 satisfies \u2202\u03a6/\u2202t = [ 0 1 0 0 ] \u03a6.\nLemma 10 (Chen & Georgiou (2015)). When R \u2192 \u221e, The optimal control u\u2217t of following problem,\nu\u2217t = [ 0 at ] \u2208 argmin\nut\u2208U \u222b T 0 1 2 \u2225ut\u22252dt+ xT1Rx1\ns.t dmt = [ 0 1 0 0 ] mtdt+ utdt\nm0 = m0\nis given by\nu\u2217t = \u2212ggTP\u22121t (mt \u2212 \u03a6(t, 1)m1) Where Pt follows Lyapunov equation (eq.18) with boundary condition P1 = 0. and function \u03a6(t, s) is the transition matrix from time-step s to time-step t given uncontrolled dynamics.\nAnd it is indeed the stochastic bridge of following system: dmt = [ 0 1 0 0 ] mtdt+ utdt+ gdwt (21)\nm0 = m0, m1 = m1 (22)\nProof. See page 8 in Chen & Georgiou (2015).\nProposition 11. The solution of the stochastic bridge problem of linear momentum system (Chen & Georgiou, 2015) is\na\u2217(mt, t) = g 2 tP11 ( x1 \u2212 xt 1\u2212 t \u2212 vt ) where : P11 = \u22124 g2t (t\u2212 1) . (23)\nProof. From Lemma.10, one can get the optimal control for this problem is\nu\u2217t = \u2212ggTP\u22121t (mt \u2212 \u03a6(t, 1)m1) where state transition function \u03a6 can be obtained from Lemma.9 and Pt is the solution of Lyapunov equation and P\u22121t can be found in Lemma.7.\nThen we have:\nu\u2217t = \u2212ggTP\u22121t (mt \u2212 \u03a6(t, 1)m1) = \u2212ggTP\u22121t mt + ggTP\u22121t \u03a6(t, 1)m1\n= \u2212 [ 0 0 0 g2 ] P\u22121t mt + gg TP\u22121t [ 1 t\u2212 1 0 1 ] m1\n= \u2212g2t [\n0 0 P10 P11\n] mt + [ 0 0 0 g2t ] [ P00 P01 P10 P11 ] [ 1 t\u2212 1 0 1 ] m1\n= \u2212g2t [\n0 0 P10 P11\n] mt + g 2 t [ 0 0 P10 P11 ] [ 1 t\u2212 1 0 1 ] m1\n= \u2212g2t [\n0 0 P10 P11\n] mt + g 2 t [ 0 0 P10 P10(t\u2212 1) + P11 ] m1\n=\n[ 0\ng2tP10(x1 \u2212 xt) + g2tP10(t\u2212 1) \u00b7 v1 + g2tP11(v1 \u2212 vt) ] Plug in v1 :=\nx1 \u2212 xt 1\u2212 t\n=\n[ 0\ng2tP11 ( x1\u2212xt 1\u2212t \u2212 vt\n)]"
        },
        {
            "heading": "D.3 MEAN AND COVARIANCE OF SDE",
            "text": "We follow the recipe of Sa\u0308rkka\u0308 & Solin (2019). The mean \u00b5t and variance \u03a3t of the matrix of random variable mt obey the following respective ordinary differential equations (ODEs):\nd\u00b5t = Ft\u00b5tdt+Dtdt\nd\u03a3t = Ft\u03a3tdt+ [\u03a3tFt] T dt+ ggTdt\nOne can solve it by numerically simulating two ODEs whose dimension is just two. Or one can use software such as Inc. (2022) to get analytic solutions."
        },
        {
            "heading": "D.4 DERIVATION FROM SDE TO ODE FOR PHASE DYNAMICS",
            "text": "One can represent the dynamics in the form of,[ dxt dvt ] = [ vt Ft ] dt+ [ 0 0 0 gt ] dwt s.t m0 := [ x0 v0 ] \u223c N (\u00b50,\u03a30) (24) as dmt = f(mt)dt+ gtdwt\nAnd its corresponding Fokker-Planck Partial Differential Equation \u00d8ksendal (2003) reads,\n\u2202pt \u2202t = \u2212 \u2211 d \u2202 \u2202mi [fi(m, t)pt(mt)] + 1 2 \u2211 d \u22022 \u2202mimj [\u2211 d gtg T t pt(mt) ] (25)\nAccording to eq.(37) in Song et al. (2020b), One can rewrite such PDE, \u2202pt \u2202t = \u2212 \u2211 d \u2202 \u2202mi { fi(mt, t)pt(mt)\u2212 1 2 [ p(mt)\u2207m \u00b7 (gtgTt ) + p(mt)gtgTt \u2207m log p(mt) ]} (26)\ndue to the fact gt \u2261 [ 0 0 0 gt ] (27)\n= \u2212 \u2211 d \u2202 \u2202mi { fi(mt, t)pt(mt)\u2212 1 2 p(mt) [ g2t\u2207v log p(mt) ]} (28)\nThen one can get the equivalent ODE: dmt = [ f(mt, t)\u2212 1\n2 g2t\u2207v log p(m, t)\n] dt (29)"
        },
        {
            "heading": "D.5 DECOMPOSITION OF COVARIANCE MATRIX AND REPRESENTATION OF SCORE",
            "text": "Here we follow the procedure in Dockhorn et al. (2021). Given the covariance matrix \u03a3t, the decomposition of the positive definite symmetric matrix is,\n\u03a3t = L T t Lt (30)\nWhere,\nLt =\n[ Lxxt L xv t\nLxvt L vv t\n] = [\u221a \u03a3xxt 0 \u03a3xvt\u221a \u03a3xxt \u221a \u03a3xxt \u03a3 vv t \u2212\u03a3vvt \u03a3xxt ] (31)\nWe borrow results from Dockhorn et al. (2021), the score function reads,\n\u2207m log p(mt|m1) = \u2212\u2207mt 1 2 (mt \u2212 \u00b5t)\u03a3t\u22121(mt \u2212 \u00b5t)\n= \u2212\u03a3t\u22121(mt \u2212 \u00b5t) Cholesky decomposition of \u03a3t = \u2212L\u2212TL\u22121(mt \u2212 \u00b5t) = \u2212L\u2212T \u03f5\nThe form of L reads,\nLt = [\u221a \u03a3xxt 0 \u03a3xvt\u221a \u03a3xxt \u221a \u03a3xxt \u03a3 vv t \u2212(\u03a3xvt )2 \u03a3xxt ] and the transpose inverse of L reads,\nL\u2212Tt =  1\u221a(\u03a3xxt +\u03f5xx) \u2212\u03a3 xv t\u221a (\u03a3xxt ) \u221a (\u03a3xxt )(\u03a3 vv t +)\u2212(\u03a3xvt )2\n0\n\u221a \u03a3xxt\u221a\n(\u03a3xxt )(\u03a3 vv t )\u2212(\u03a3xvt )2  Hence, the score function reads,\n\u2207v log p(mt|m1) = \u2212 \u221a\n\u03a3xxt\u221a (\u03a3xxt + \u03f5xx)(\u03a3\nvv t + \u03f5vv)\u2212 (\u03a3xvt )2\ufe38 \ufe37\ufe37 \ufe38 \u2113t\n\u03f51\nD.6 REPRESENTATION OF ACCELERATION at\nAs been shown in Proposition.3, the optimal control can be represented as,\na\u2217t = g 2 tP11 ( x1 \u2212 xt 1\u2212 t \u2212 vt )\n= g2tP11 x1\n1\u2212 t \u2212 g2tP11\n( xt\n1\u2212 t + vt ) = g2tP11\nx1 1\u2212 t\n\u2212 g2tP11 ( \u00b5xt + L xx t \u03f50\n1\u2212 t + (\u00b5vt + L xv t \u03f50 + L vv t \u03f51) ) = g2tP11 [( x1 \u2212 \u00b5xt 1\u2212 t \u2212 \u00b5vt ) \u2212 ( Lxxt 1\u2212 t \u03f50 + L xv t \u03f50 + L vv t \u03f51\n)] solving eq.D.3 we can get : xt = 1\n3 x1t 2(t2 \u2212 4t+ 6),vt = 4tx1 3 (t2 \u2212 3t+ 3)\nPlug inxt,vt\n= g2tP11\n[( x1 \u2212 13x1t 2 ( 6\u2212 4t+ t2 ) 1\u2212 t \u2212 4tx1 3 (t2 \u2212 3t+ 3) ) \u2212 ( Lxxt 1\u2212 t \u03f50 + L xv t \u03f50 + L vv t \u03f51 )]\n= g2tP11\n[( (\u2212t4 + 4t3 \u2212 6t2 + 3)\n3(1\u2212 t) \u2212 4t 3 (t2 \u2212 3t+ 3)\n) x1 \u2212 ( Lxxt 1\u2212 t \u03f50 + L xv t \u03f50 + L vv t \u03f51 )] = g2tP11 [( \u2212(t\u2212 1)(t3 \u2212 3t2 + 3t+ 3)\n3(1\u2212 t) \u2212 4t 3 (t2 \u2212 3t+ 3)\n) x1 \u2212 ( Lxxt 1\u2212 t \u03f50 + L xv t \u03f50 + L vv t \u03f51 )] = g2tP11 [( (t3 \u2212 3t2 + 3t+ 3)\n3 \u2212 1 3 (4t3 \u2212 12t2 + 12t)\n) x1 \u2212 ( Lxxt 1\u2212 t \u03f50 + L xv t \u03f50 + L vv t \u03f51 )] = g2tP11 [ (1\u2212 t)3x1 \u2212 ( Lxxt 1\u2212 t \u03f50 + L xv t \u03f50 + L vv t \u03f51\n)] = 4(1\u2212 t)2x1 + g2tP11 ( Lxxt 1\u2212 t \u03f50 + L xv t \u03f50 + L vv t \u03f51 )"
        },
        {
            "heading": "D.7 LOSS REWEIGHT",
            "text": "We use\n\u03bb(t) = 1\n1\u2212 t For all experiments for simplicity. We admit that this might not be an optimal selection."
        },
        {
            "heading": "D.8 NORMALIZER OF AGM-SDE AND AGM-ODE",
            "text": "Since the optimal control term can be represented as, a\u2217(mt, t) = 4x1(1\u2212 t)2 \u2212 g2tP11 [(\nLxxt 1\u2212 t + Lxvt\n) \u03f50 + L vv t \u03f51 ] .\nThen we introduce the normalizer as\nzSDE = \u221a\u221a\u221a\u221a(4(1\u2212 t)2 \u00b7 \u03c3data)2 + g2tP11 [(\nLxxt 1\u2212 t + Lxvt\n)2 + (Lvvt ) 2 ]\nzODE = \u221a\u221a\u221a\u221a(4(1\u2212 t)2 \u00b7 \u03c3data)2 + g2tP11 + g2tP11( Lxxt1\u2212 t + Lxvt )2 + [( g2tP11L vv t \u2212 1 2 g2t \u2113t )2] Where \u2113 := \u221a\n\u03a3xxt \u03a3xxt \u03a3 vv t \u2212(\u03a3xvt )2"
        },
        {
            "heading": "D.9 EXPONENTIAL INTEGRATOR DERIVATION",
            "text": "As suggested by Zhang & Chen (2022), one can write the discretized dynamics as,[ xti+1 vti+1 ] = \u03a6(ti+1, ti) [ xt vt ] + r\u2211 j=0 Ci,j [ 0 s\u03b8(mti\u2212j , ti\u2212j) ]\nWhere Ci,j = \u222b t+\u03b4t t \u03a6(t+ \u03b4t, \u03c4) [ 0 0 0 z\u03c4 ]\u220f k \u0338=j [ \u03c4 \u2212 ti\u2212k ti\u2212j \u2212 ti\u2212k ] d\u03c4, \u03a6(t, s) = [ 1 t\u2212 s 0 1 ] (32)\nAfter plugging in the transition kernel \u03a6(t, s), one can easily obtain the results shown in (11)."
        },
        {
            "heading": "D.10 PROOF OF PROPOSITION.5",
            "text": "The estimated data point x1 can be represented as\nx\u0303SDE1 = (1\u2212 t)(F\u03b8t + vt)\ng2tP11 + xt, or x\u0303ODE1 =\nF\u03b8t + g 2 tP11(\u03b1txt + \u03b2tvt)\n4(t\u2212 1)2 + g2tP11(\u03b1t\u00b5xt + \u03b2t\u00b5vt ) (33)\nfor SDE and probablistic ODE dynamics respectively, and \u03b2t = Lvvt + 1 2P11 ,\u03b1t =\n( Lxxt 1\u2212t +L xv t )\u2212\u03b2tL xv t\nLxxt .\nProof. It is easy to derive the representation of x1 of the SDE due to the fact that the network is essentially estimating:\nF\u03b8t \u2248 g2tP11 ( x1 \u2212 xt 1\u2212 t \u2212 vt )\n\u21d4 x1 \u2248 (1\u2212 t)(F\u03b8t + vt)\ng2tP11 + xt\nIt will become slightly more complicated for probabilistic ODE cases. We notice that\nmt = \u00b5t + L\u03f5\n\u21d4 xt = \u00b5xt + Lxxt \u03f51, vt = \u00b5vt + Lxvt \u03f50 + Lvvt \u03f51\nIn probabilistic ODE case, the force term can be represented as, F(mt, t) = 4x1(1\u2212 t)2 \u2212 g2tP11 [(\nLxxt 1\u2212 t + Lxvt\n) \u03f50 + L vv t \u03f51 ] \u2212 1\n2 g2t \u2113\u03f51\nIn order to use linear combination of xt and vt to represent F one needs to match the stochastic term in Ft by using\n\u03b1tL xx t + \u03b2tL xv t = Lxxt 1\u2212 t\n+ Lxvt\ufe38 \ufe37\ufe37 \ufe38 \u03a6t ,\n\u03b2tL vv t = L vv t +\n1\n2P11\ufe38 \ufe37\ufe37 \ufe38 \u03b6t .\nThe solution can be obtained by:\n\u03b2t = \u03b6t Lvvt \u03b1t = \u03a6t \u2212 \u03b2tLxvt\nLxxt\nBy subsitute it back to Ft, one can get:\nF(mt, t) = 4x1(1\u2212 t)2 \u2212 g2tP11 [\u03b1t(xt \u2212 \u00b5xt ) + \u03b2t(vt \u2212 \u00b5vt )] = [ 4(1\u2212 t)2 + g2tP11(\u03b1t\u00b5xt + \u03b2t\u00b5vt ) ] x1 \u2212 g2tP11 [\u03b1txt + \u03b2tvt]\n\u21d4 x1 = F\u03b8t + g 2 tP11(\u03b1txt + \u03b2tvt)\n4(t\u2212 1)2 + g2tP11(\u03b1t\u00b5xt + \u03b2t\u00b5vt )"
        },
        {
            "heading": "E EXPERIMENTAL DETAILS",
            "text": "Training: We stick with hyperparameters introduced in the section.4. We use AdamW(Loshchilov & Hutter, 2017) as our optimizer and Exponential Moving Averaging with the exponential decay rate of 0.9999. We use 8 \u00d7 Nvidia A100 GPU for all experiments. For further, training setup, please refer to Table.6.\nSampling: For Exponential Integrator, we choose the multistep order w = 2 consistently for all experiments. Different from previous work (Dockhorn et al., 2021; Karras et al., 2022; Zhang et al., 2023), we use quadratic timesteps scheme with \u03ba = 2:\nti = ( N \u2212 i N t 1 \u03ba 0 + i N t 1 \u03ba N )\u03ba Which is opposite to the classical DM. Namely, the time discretization will get larger when the dynamics is propagated close to data. For numerical stability, we use t0 = 1E\u22125 for all experiments. For NFE = 5, we use tN = 0.5 and NFE = 10, TN = 0.7. For the rest of the sampling, we use tN = 0.999.\nDue to the fact that EDM(Karras et al., 2022) is using second-order ODE solver, in practice, we allow it to have an extra one NFE as reported for all the tables."
        },
        {
            "heading": "F CONDITIONAL GENERATION DETAILS",
            "text": "Here we provide the detail of conditional generation details."
        },
        {
            "heading": "F.1 STORKE BASED GENERATION",
            "text": "For stroke based generation, we provide two types of conditional generation.\ninitial Velocity (IV):Please refer to section.4. Dynamics Velocity (dyn-V):Since the mean and variance of velocity and position is available, one can specify the velocity which is valid. In this case, we can set the velocity as\nvt = \u00b5 vt|xt t +\u03a3 vt|xt t \u03f5 (34)\nIn which,\n\u00b5 vt|xt t = \u00b5 v t + \u03a3xvt \u03a3xxt (xt \u2212 \u00b5xt ) (35)\n\u03a3 vt|xt t = \u03a3 vv t \u2212\n\u03a3xvt 2\n\u03a3xxt (36)\nwhen t \u2264 c. The c is the guidance length. We typically set it to be c = 0.25.\nF.2 INPAINTING\nIn the inpainting case, we apply the similar strategy as dyn-V. Specifically, in this case, the x\u03031 will be represented as:\nx\u03021 := MASK \u2299 \u00b5xt + (1\u2212 MASK)\u2299 x\u03031 (37) where MASK represents for the mask matrix which zero-out the pixel of the original image. Such x\u03021 will serve as the source to estimate \u00b5xt in eq.35.\nF.3 INPAINTING BASED GENERATION\nFor stroke based generation, we provide two types of conditional generation."
        },
        {
            "heading": "G ABLATION STUDY OF STOKE-BASED CONDITIONAL GENERATION",
            "text": "In order to investigate the diversity and faithfulness of stoke-based conditional generation, we conduct the ablation study with respect to the hyperparameter \u03be."
        },
        {
            "heading": "H ABLATION STUDY OF POSITION, VELOCITY AND ACCELERATION TRAJECTORIES",
            "text": ""
        },
        {
            "heading": "I ADDITIONAL FIGURES",
            "text": "We demonstrate the samples for different datasets with varying NFE."
        },
        {
            "heading": "I.1 TOY DATASET COMPARED WITH CLD",
            "text": "I.2 CIFAR-10"
        },
        {
            "heading": "I.3 AFHQV2",
            "text": "I.4 IMAGENET-64"
        }
    ],
    "title": "GENERATIVE MODELING WITH PHASE STOCHASTIC BRIDGES",
    "year": 2024
}