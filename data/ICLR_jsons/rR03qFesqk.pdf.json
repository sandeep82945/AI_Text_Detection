{
    "abstractText": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5\u2019s RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
    "authors": [
        {
            "affiliations": [],
            "name": "TIONS IMPROVES"
        },
        {
            "affiliations": [],
            "name": "LONG CONTEXT TRANSFORMERS"
        },
        {
            "affiliations": [],
            "name": "Shanda Li"
        },
        {
            "affiliations": [],
            "name": "Chong You"
        },
        {
            "affiliations": [],
            "name": "Guru Guruganesh"
        },
        {
            "affiliations": [],
            "name": "Joshua Ainslie"
        },
        {
            "affiliations": [],
            "name": "Santiago Ontanon"
        },
        {
            "affiliations": [],
            "name": "Manzil Zaheer"
        },
        {
            "affiliations": [],
            "name": "Sumit Sanghai"
        },
        {
            "affiliations": [],
            "name": "Yiming Yang"
        },
        {
            "affiliations": [],
            "name": "Sanjiv Kumar"
        },
        {
            "affiliations": [],
            "name": "Srinadh Bhojanapalli"
        }
    ],
    "id": "SP:9574634c42956ddfdf0defaf94e141c1ceca90d4",
    "references": [
        {
            "authors": [
                "Joshua Ainslie",
                "Tao Lei",
                "Michiel de Jong",
                "Santiago Onta\u00f1\u00f3n",
                "Siddhartha Brahma",
                "Yury Zemlyanskiy",
                "David Uthus",
                "Mandy Guo",
                "James Lee-Thorp",
                "Yi Tay"
            ],
            "title": "Colt5: Faster long-range transformers with conditional computation",
            "venue": "arXiv preprint arXiv:2303.09752,",
            "year": 2023
        },
        {
            "authors": [
                "Cem Anil",
                "Yuhuai Wu",
                "Anders Andreassen",
                "Aitor Lewkowycz",
                "Vedant Misra",
                "Vinay Ramasesh",
                "Ambrose Slone",
                "Guy Gur-Ari",
                "Ethan Dyer",
                "Behnam Neyshabur"
            ],
            "title": "Exploring length generalization in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mirelle Candida Bueno",
                "Carlos Gemmell",
                "Jeff Dalton",
                "Roberto Lotufo",
                "Rodrigo Nogueira"
            ],
            "title": "Induced natural language rationales and interleaved markup tokens enable extrapolation in large language models",
            "venue": "In Proceedings of the 1st Workshop on Mathematical Natural Language Processing (MathNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Pu-Chin Chen",
                "Henry Tsai",
                "Srinadh Bhojanapalli",
                "Hyung Won Chung",
                "Yin-Wen Chang",
                "ChunSung Ferng"
            ],
            "title": "A simple and effective positional encoding for transformers",
            "venue": "arXiv preprint arXiv:2104.08698,",
            "year": 2021
        },
        {
            "authors": [
                "Shouyuan Chen",
                "Sherman Wong",
                "Liangjian Chen",
                "Yuandong Tian"
            ],
            "title": "Extending context window of large language models via positional interpolation",
            "venue": "arXiv preprint arXiv:2306.15595,",
            "year": 2023
        },
        {
            "authors": [
                "Ta-Chung Chi",
                "Ting-Han Fan",
                "Peter J Ramadge",
                "Alexander Rudnicky"
            ],
            "title": "Kerple: Kernelized relative positional embedding for length extrapolation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ta-Chung Chi",
                "Ting-Han Fan",
                "Alexander Rudnicky",
                "Peter Ramadge"
            ],
            "title": "Dissecting transformer length extrapolation via the lens of receptive field analysis",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Krzysztof Marcin Choromanski",
                "Shanda Li",
                "Valerii Likhosherstov",
                "Kumar Avinava Dubey",
                "Shengjie Luo",
                "Di He",
                "Yiming Yang",
                "Tamas Sarlos",
                "Thomas Weingarten",
                "Adrian Weller"
            ],
            "title": "Learning a fourier transform for linear relative positional encodings in transformers",
            "venue": "arXiv preprint arXiv:2302.01925,",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea"
            ],
            "title": "Monotonic location attention for length generalization",
            "venue": "arXiv preprint arXiv:2305.20019,",
            "year": 2023
        },
        {
            "authors": [
                "Xiangxiang Chu",
                "Zhi Tian",
                "Bo Zhang",
                "Xinlong Wang",
                "Chunhua Shen"
            ],
            "title": "Conditional positional encodings for vision transformers",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jean-Baptiste Cordonnier",
                "Andreas Loukas",
                "Martin Jaggi"
            ],
            "title": "On the relationship between selfattention and convolutional layers",
            "venue": "arXiv preprint arXiv:1911.03584,",
            "year": 2019
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime G Carbonell",
                "Quoc Le",
                "Ruslan Salakhutdinov"
            ],
            "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Gregoire Deletang",
                "Anian Ruoss",
                "Jordi Grau-Moya",
                "Tim Genewein",
                "Li Kevin Wenliang",
                "Elliot Catt",
                "Chris Cundy",
                "Marcus Hutter",
                "Shane Legg",
                "Joel Veness",
                "Pedro A Ortega"
            ],
            "title": "Neural networks and the chomsky hierarchy",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Chao Dong",
                "Chen Change Loy",
                "Kaiming He",
                "Xiaoou Tang"
            ],
            "title": "Image super-resolution using deep convolutional networks",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yann Dubois",
                "Gautier Dagan",
                "Dieuwke Hupkes",
                "Elia Bruni"
            ],
            "title": "Location attention for extrapolation to longer sequences",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ximing Lu",
                "Melanie Sclar",
                "Xiang Lorraine Li",
                "Liwei Jian",
                "Bill Yuchen Lin",
                "Peter West",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jena D Hwang"
            ],
            "title": "Faith and fate: Limits of transformers on compositionality",
            "venue": "arXiv preprint arXiv:2305.18654,",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Mandy Guo",
                "Joshua Ainslie",
                "David C Uthus",
                "Santiago Ontanon",
                "Jianmo Ni",
                "Yun-Hsuan Sung",
                "Yinfei Yang"
            ],
            "title": "Longt5: Efficient text-to-text transformer for long sequences",
            "venue": "In Findings of the Association for Computational Linguistics: NAACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaotian Han",
                "Zhimeng Jiang",
                "Ninghao Liu",
                "Xia Hu"
            ],
            "title": "G-mixup: Graph data augmentation for graph classification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Adi Haviv",
                "Ori Ram",
                "Ofir Press",
                "Peter Izsak",
                "Omer Levy"
            ],
            "title": "Transformer language models without positional encodings still learn positional information",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Justin Johnson",
                "Alexandre Alahi",
                "Li Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Amirhossein Kazemnejad",
                "Inkit Padhi",
                "Karthikeyan Natesan Ramamurthy",
                "Payel Das",
                "Siva Reddy"
            ],
            "title": "The impact of positional encoding on length generalization in transformers",
            "venue": "arXiv preprint arXiv:2305.19466,",
            "year": 2023
        },
        {
            "authors": [
                "Guolin Ke",
                "Di He",
                "Tie-Yan Liu"
            ],
            "title": "Rethinking positional encoding in language pre-training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Zixuan Ke",
                "Yijia Shao",
                "Haowei Lin",
                "Tatsuya Konishi",
                "Gyuhak Kim",
                "Bing Liu"
            ],
            "title": "Continual pre-training of language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "He He",
                "Peng Qi",
                "Dan Jurafsky"
            ],
            "title": "Sharp nearby, fuzzy far away: How neural language models use context",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Nikita Kitaev",
                "Dan Klein"
            ],
            "title": "Constituency parsing with a self-attentive encoder",
            "venue": "arXiv preprint arXiv:1805.01052,",
            "year": 2018
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Ko\u010disk\u1ef3",
                "Jonathan Schwarz",
                "Phil Blunsom",
                "Chris Dyer",
                "Karl Moritz Hermann",
                "G\u00e1bor Melis",
                "Edward Grefenstette"
            ],
            "title": "The narrativeqa reading comprehension challenge",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Shanda Li",
                "Xiangning Chen",
                "Di He",
                "Cho-Jui Hsieh"
            ],
            "title": "Can vision transformers perform convolution",
            "venue": "arXiv preprint arXiv:2111.01353,",
            "year": 2021
        },
        {
            "authors": [
                "Bingbin Liu",
                "Jordan Ash",
                "Surbhi Goel",
                "Akshay Krishnamurthy",
                "Cyril Zhang"
            ],
            "title": "Exposing attention glitches with flip-flop language modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "Nelson F Liu",
                "Kevin Lin",
                "John Hewitt",
                "Ashwin Paranjape",
                "Michele Bevilacqua",
                "Fabio Petroni",
                "Percy Liang"
            ],
            "title": "Lost in the middle: How language models use long contexts",
            "venue": "arXiv preprint arXiv:2307.03172,",
            "year": 2023
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "arXiv preprint arXiv:2103.14030,",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Liutkus",
                "Ond\u0159ej C\u0131fka",
                "Shih-Lun Wu",
                "Umut Simsekli",
                "Yi-Hsuan Yang",
                "Gael Richard"
            ],
            "title": "Relative positional encoding for transformers with linear complexity",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Shengjie Luo",
                "Shanda Li",
                "Tianle Cai",
                "Di He",
                "Dinglan Peng",
                "Shuxin Zheng",
                "Guolin Ke",
                "Liwei Wang",
                "Tie-Yan Liu"
            ],
            "title": "Stable, fast and accurate: Kernelized attention with relative positional encoding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shengjie Luo",
                "Shanda Li",
                "Shuxin Zheng",
                "Tie-Yan Liu",
                "Liwei Wang",
                "Di He"
            ],
            "title": "Your transformer may not be as powerful as you expect",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Vinod Nair",
                "Geoffrey E Hinton"
            ],
            "title": "Rectified linear units improve restricted boltzmann machines",
            "venue": "In Proceedings of the 27th international conference on machine learning",
            "year": 2010
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Bowen Peng",
                "Jeffrey Quesnelle",
                "Honglu Fan",
                "Enrico Shippole"
            ],
            "title": "YaRN: Efficient context window extension of large language models",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Ofir Press",
                "Noah Smith",
                "Mike Lewis"
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "arXiv preprint arXiv:1511.06434,",
            "year": 2015
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683,",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Eric Michael Smith",
                "Y-Lan Boureau"
            ],
            "title": "Recipes for building an open-domain chatbot",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Anian Ruoss",
                "Gr\u00e9goire Del\u00e9tang",
                "Tim Genewein",
                "Jordi Grau-Moya",
                "R\u00f3bert Csord\u00e1s",
                "Mehdi Bennani",
                "Shane Legg",
                "Joel Veness"
            ],
            "title": "Randomized positional encodings boost length generalization of transformers",
            "venue": "In 61st Annual Meeting of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Uri Shaham",
                "Elad Segal",
                "Maor Ivgi",
                "Avia Efrat",
                "Ori Yoran",
                "Adi Haviv",
                "Ankit Gupta",
                "Wenhan Xiong",
                "Mor Geva",
                "Jonathan Berant"
            ],
            "title": "Scrolls: Standardized comparison over long language sequences",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Shaw",
                "Jakob Uszkoreit",
                "Ashish Vaswani"
            ],
            "title": "Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
            "venue": "URL https://aclanthology.org/N18-2074",
            "year": 2018
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Bo Wen",
                "Yunfeng Liu"
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "venue": "arXiv preprint arXiv:2104.09864,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Benyou Wang",
                "Lifeng Shang",
                "Christina Lioma",
                "Xin Jiang",
                "Hao Yang",
                "Qun Liu",
                "Jakob Grue Simonsen"
            ],
            "title": "On position embeddings in {bert",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chulhee Yun",
                "Srinadh Bhojanapalli",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Sanjiv Kumar"
            ],
            "title": "Are transformers universal approximators of sequence-to-sequence functions",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu"
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "William B Dolan"
            ],
            "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
            "year": 2020
        },
        {
            "authors": [
                "Wenting Zhao",
                "Mor Geva",
                "Bill Yuchen Lin",
                "Michihiro Yasunaga",
                "Aman Madaan",
                "Tao Yu"
            ],
            "title": "Complex reasoning in natural language. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)",
            "year": 2023
        },
        {
            "authors": [
                "Hattie Zhou",
                "Arwen Bradley",
                "Etai Littwin",
                "Noam Razin",
                "Omid Saremi",
                "Josh Susskind",
                "Samy Bengio",
                "Preetum Nakkiran"
            ],
            "title": "What algorithms can transformers learn? a study in length generalization",
            "venue": "arXiv preprint arXiv:2310.16028,",
            "year": 2023
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Uri Alon",
                "Xinyun Chen",
                "Xuezhi Wang",
                "Rishabh Agarwal",
                "Denny Zhou"
            ],
            "title": "Transformers can achieve length generalization but not robustly",
            "venue": "arXiv preprint arXiv:2402.09371,",
            "year": 2024
        },
        {
            "authors": [
                "Hendrycks",
                "Gimpel"
            ],
            "title": "2016) activation functions and present the experiment result in 6. The result shows that the model performance is not sensitive to the choice of activation function in the length generalization setting, while ReLU works better on normal sequence lengths. Thus, we use ReLU as our default activation function",
            "year": 2016
        },
        {
            "authors": [
                "choice. B"
            ],
            "title": "FIRE IS STILL STRONG WHEN TRAINED ON SEQUENCE LENGTH 512 In most of our pretraining experiments, the training sequence length is set to 2048 (see Appendix C.1). In this experiment we train models with different positional encodings on C4 with training sequence length 512 to confirm that the overall performance trends are not sensitive to the pretraining",
            "year": 2048
        },
        {
            "authors": [
                "Ainslie"
            ],
            "title": "inference datasets - ContractNLI; and summarization datasets - QMSum, SummScreenFD, and GovReport. Following existing works Shaham et al",
            "year": 2023
        },
        {
            "authors": [
                "Dubois"
            ],
            "title": "2023) introduce location attention for length generalization on synthetic tasks. Bueno et al. (2022) show that generating step-by-step rationales and using marker tokens as positional guides helps length generalization. Studying positional encoding approaches for length generalization is a main direction in this line of research",
            "year": 2022
        },
        {
            "authors": [
                "Long"
            ],
            "title": "Our work is based on a unified formulation of existing additive relative positional encoding approaches, and proposes new RPE variant aimed at improving length generalization. Interpolation techniques in deep learning. Interpolation techniques are successfully applied to many deep learning",
            "venue": "(Press et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Johnson"
            ],
            "title": "up-sampling layers of convolutional neural networks for dense visual prediction",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformer based Language Models have demonstrated state-of-the-art zero-shot performance on many natural language processing tasks (Brown et al., 2020), enabling increasingly longer context applications such as chat bots (Roller et al., 2021; Zhang et al., 2020b) and long document summarization and question answering (Zhang et al., 2020a; Guo et al., 2022; Ainslie et al., 2023). However, the accuracy of these models usually drops quickly for inputs longer than the ones used during training (Press et al., 2022; Anil et al., 2022; Deletang et al., 2023) \u2013 which are usually relatively short (e.g. 2048 for LLaMA (Touvron et al., 2023a;b)) to avoid the expensive quadratic attention cost during training. This has led to a significant interest in improving length generalization of Transformers - where we train the model using shorter inputs (e.g. 2048) and test the models performance on longer inputs (e.g. 8192) (Press et al., 2022; Anil et al., 2022; Chi et al., 2022; 2023; Chowdhury & Caragea, 2023; Chen et al., 2023).\nTransformers are fundamentally permutation equivariant, and are agnostic to input sequence ordering (Vaswani et al., 2017; Yun et al., 2019)1. They rely on position encodings to learn the ordering of input tokens. Popular position encodings such as Absolute Positional Encoding (APE) (Vaswani et al., 2017) and more recent Rotary Positional Encoding (RoPE) (Su et al., 2021) do not generalize to longer contexts than seen during training (Kazemnejad et al., 2023). T5\u2019s relative positional encoding (Raffel et al., 2019) generalizes to longer contexts by using the same representation for all out of distribution (OOD) sequence lengths, but suffers from slow vector operations on modern accelerators (Press et al., 2022). Another line of recent work promotes length generalization by encoding specific inductive biases on how attention should decay with sequence length (Press et al., 2022; Chi et al., 2022; 2023). More recently, Kazemnejad et al. (2023) show that having no position encodings in decoder-only models can have better length generalization, albeit for small-scale synthetic tasks.\nIn this work we take a functional approach to learn the relative position biases2, instead of having hard coded inductive biases, towards training language models with length generalization (focusing on decoder-only models). We propose FIRE (Functional Interpolation for Relative Positional Encoding) method that i) uses a learnable function to map the input positions to biases, and ii) uses a progressive\n\u2217Work done during internship at Google Research. 1Note that decoder-only models can infer position from the causal attention mask (Haviv et al., 2022). 2We consider relative position encodings for their superior performance over absolute position encodings\n(Raffel et al., 2019; Chen et al., 2021).\ninterpolation technique, which ensures bounded input for the position encoding function for all input sequence lengths, thereby enabling length generalization.\nC4 language modeling (base model)\nA functional approach to learn the biases allows the model to adapt to the given task instead of always having the same inductive bias, e.g. bias towards nearby tokens as in (Press et al., 2022; Chi et al., 2022; 2023). In particular we use an MLP to learn these biases, which we theoretically prove can represent several popular methods such as T5\u2019s RPE, Alibi, and Kerple in a parameter efficient manner. In fact, all our experiments use a tiny MLP with a hidden size of 32, which is also accelerator-friendly unlike T5\u2019s RPE. Next, our progressive interpolation technique normalizes the query-key relative distance by the query position. Since for causal attention in language models the relative distance is always between 0 and the query position, progressive interpolation results in an output that is always bounded between [0, 1]. This results in\na bounded input to the position encoding function for all input sequence lengths, leading to better generalization performance. As a result, with increasingly longer sequence lengths, the positional inputs will form progressively finer grids, interpolating the positional encoding function on [0, 1].\nInspired by the existing methods, we incorporate the following two transformations into FIRE, which we find helpful to improve the model quality. i) To encourage locality bias in FIRE, we apply the popular log transformation (Raffel et al., 2019; Chi et al., 2022) to the relative distance before feeding it to the MLP, which amplifies the input differences for local tokens. ii) Next we modify progressive interpolation with a learnable threshold in the normalizer to yield exact distances for shorter contexts. Note that both these transformations do not limit the ability of the model to learn arbitrary biases. In fact we show that FIRE learns to pay more attention to far away contexts in some attention heads.\nWe conduct an extensive empirical study to demonstrate the effectiveness of FIRE for length generalization. We benchmark FIRE as well as other positional encoding approaches on a wide range of real-world language modeling (C4, arXiv, and Github), long text benchmark (SCROLLS), zero-shot long-context question answering (NarrativeQA), and natural language understanding benchmarks (GLUE/SuperGLUE). Our empirical results show the strong length generalization performance and long text modeling capability of FIRE. Our experiments on standard natural language understanding benchmarks show that FIRE is competitive on short sequence tasks as well. We further visualize the learned positional encoding of FIRE showing that it learns diverse patterns, beyond just locality bias.\nThe main contributions of our paper are summarized below:\n\u2022 We propose FIRE, a new functional relative positional encoding method. Using progressive interpolation, FIRE is able to transform arbitrary input lengths into bounded domain, followed by a learned mapping.\n\u2022 We theoretically prove that FIRE can represent popular position encodings such as T5\u2019s RPE, Alibi, and Kerple, thereby unifying a class of existing position encoding approaches.\n\u2022 We empirically show strong length generalization behavior of FIRE, significantly improving over existing methods in zero-shot and finetuning settings on a wide range of datasets and benchmarks. For instance, it consistently delivers strongest performance on C4 language modeling across various sequence lengths, outperforming the best baseline by 2.28 perplexity points (Fig. 1). On SCROLLS long text benchmark, FIRE surpasses all the competing methods on average by over 1 point (Table 1).\n\u2022 We present visualization of learned position embeddings of FIRE model showing that it can learn both local and anti-local position biases."
        },
        {
            "heading": "2 POSITIONAL ENCODINGS AND LENGTH GENERALIZATION",
            "text": "We are interested in building Transformer models with length generalization ability, i.e., we expect that the model can be trained on sequences of length Ltrain and be directly applied to sequence length Ltest without performance degradation for Ltest > Ltrain (Press et al., 2022). Length generalization requires Transformers to generalize to unseen positions during training, and designing better position\nencodings is an active line of research towards improving the length generalization (Chi et al., 2022; 2023; Kazemnejad et al., 2023; Chen et al., 2023). In this section, we review existing positional encoding approaches with an emphasis on their length generalization abilities. More discussions on related work can be found in Appendix D."
        },
        {
            "heading": "2.1 ABSOLUTE POSITIONAL ENCODING",
            "text": "The Transformer paper (Vaswani et al., 2017) proposes Absolute Positional Encoding (APE) to endow Transformers with positional information. In particular, a (learnable or fixed sinusoidal) real-valued embedding ei \u2208 Rd is assigned to each position i, leading to an Absolute Positional Encoding matrix E = [e1, \u00b7 \u00b7 \u00b7 , en]\u22a4, which will be added to the input sequence. Though simple and straightforward, APE-based Transformers usually generalize poorly to longer sequences (Press et al., 2022)."
        },
        {
            "heading": "2.2 RELATIVE POSITIONAL ENCODING",
            "text": "Relative Positional Encoding (RPE) is an increasingly popular way to encode positional information for Transformers. Shaw et al. (2018) are the first to introduce RPE to Transformers and their proposed method adds position encodings to the key (and optionally the value) in the attention layer, instead of the input. Raffel et al. (2019) simplify the vector representations of relative positions to scalars and use them as a bias term added to the pre-softmax attention logits. They further map any OOD sequence lengths to the same position, resulting in length generalization. This form of additive RPE has proven to be highly effective in many applications (Dai et al., 2019; Liu et al., 2021; Ying et al., 2021). Following this, multiple additive RPE methods have been proposed to improve both length generalization and efficiency, such as Alibi (Press et al., 2022), Kerple (Chi et al., 2022), and Sandwich (Chi et al., 2023).\nAdditive RPE. For most of these additive RPE methods, the computation of the (pre-softmax) attention logits can be unified using the following formula:\nARPE(X) = XWQ(XWK) \u22a4 +B, (1)\nwhere the bias matrix B \u2208 Rn\u00d7n is induced by the position encoding function b : N\u22172 \u2192 R. Let the (i, j)-th entry of B be b(i, j). Different formulations and parameterizations of b lead to different RPE variants. A few examples that support arbitary sequence length include:\n\u2022 T5\u2019s RPE (Raffel et al., 2019): b(i, j) = rmin{i\u2212j,K}, where K is a hyper-parameter and {ri}Ki=0 are learnable scalars.3 \u2022 Alibi (Press et al., 2022): b(i, j) = \u2212r|i\u2212 j|, where r > 0 is a hyper-parameter. \u2022 Kerple (Chi et al., 2022): b(i, j) = \u2212r1 log(1+r2|i\u2212j|) (logarithmic variant) or \u2212r1|i\u2212j|r2\n(power variant), where r1, r2 > 0 are learnable scalars. \u2022 Sandwich (Chi et al., 2023): b(i, j) = r1 \u2211r2 k=1 cos ( (i\u2212 j)/10000 kd\u2032 ) , where r1 and r2\nare hyper-parameters.\nThe above methods can be applied to longer sequences than training, but they also have several limitations. T5\u2019s RPE uses the same attention bias for all query-key pairs with distance greater than K, lacking representational power to distinguish between different positions in long sequences. Furthermore, it relies on vector operations that are not accelerator-friendly, making its training and inference relatively slow (Press et al., 2022). Alibi, Kerple, and Sandwich significantly bias towards local attention, making it harder to attend to more distant query-key pairs (Chi et al., 2023). This property can prevent the model from capturing long-range dependencies and lead to performance degradation on some tasks. In the subsequent section, we will present our method to overcome these limitations.\nRotary Positional Encoding. In addition to the aforementioned methods, there are also several non-additive RPE variants. Among them, the most popular one in large language models is Rotary Position Encoding (RoPE) (Su et al., 2021; Chowdhery et al., 2022; Touvron et al., 2023a). RoPE\n3In practice, T5\u2019s RPE segments relative distances into distinct buckets with a logarithmic scale, each associated with a unique parameter. Refer to Appendix A.1 for further details.\nrotates the query and key vectors with an angle proportional to their absolute positions before the dot product attention, which results in attention being a function of the relative distance between the tokens, capturing the relative positional information.\nPress et al. (2022); Kazemnejad et al. (2023) find that RoPE-based language models have poor length generalization. To address this, Chen et al. (2023) propose RoPE with position interpolation, and show this allows better length generalization of these models. Such interpolation techniques ((Chen et al., 2023) for RoPE and (Dosovitskiy et al., 2021) for APE), usually requires 1) knowing the target sequence length a priori, which may not be feasible in practical generative applications, 2) finetuning the model at the new target sequence length, which can be challenging for larger scale models. In contrast, our proposed approach uses a progressive interpolation technique that does not require any prior information of the target sequence length. This property is appealing since the maximum sequence length can be hard to predict for auto-regressive language models. Further, our experiments show that the proposed approach does not require any additional finetuning to achieve strong zero-shot length generalization behavior."
        },
        {
            "heading": "2.3 NO POSITIONAL ENCODING",
            "text": "While encoder-only Transformer models (e.g., BERT (Devlin et al., 2019)) are permutation equivariant without positional encoding, Haviv et al. (2022) show that decoder-only Transformers with causal attention masks can learn positional information even without any explicit positional encoding. Recently, Kazemnejad et al. (2023) show that the no positional encoding (NoPE) model shows strong length generalization on small scale synthetic tasks."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we formally introduce FIRE (Functional Interpolation for Relative Positional Encoding), a new relative positional encoding approach for improving length generalization of Transformers."
        },
        {
            "heading": "3.1 FUNCTIONAL POSITION ENCODING WITH PROGRESSIVE INTERPOLATION",
            "text": "Our proposed approach FIRE uses a learnable continuous function to map input positions to biases. We implement the function using an MLP f\u03b8 : R \u2192 R,4 where \u03b8 denotes the MLP parameters. This avoids hard coding specific inductive biases and lets the position encoding be learnt jointly with the task at hand. A standard approach would be to feed the relative query-key distance as the input to the MLP. However this suffers from generalization issues when the inputs (the relative distances) are outside the training domain of the MLP.\nWe propose Progressive Interpolation to address this challenge. Instead of using the raw query-key relative distance as the input to the MLP, we normalize the distance by the query position index. Formally, we consider the following positional encoding function:\nb(i, j) = f\u03b8 ( i\u2212 j i ) where f\u03b8(x) = v\u22a43 \u03c3(V2\u03c3(v1x)), \u03b8 = {v1,V2,v3}.5 (2)\nHere \u03c3 is the ReLU activation function; i and j denote the query and key positions respectively. Note that in causal attention, the relative distance satisfies 0 \u2264 i\u2212 j < i. Therefore, the normalized relative distance is constrained to be in [0, 1] regardless of the sequence length. In particular, with increasingly longer sequence lengths, the positional inputs will form progressively finer grids, interpolating the positional encoding function on [0, 1]. Hence, this technique aligns inference domain with training domain for any sequence lengths, leading to better length generalization.\nDiscussion on the choice of the normalizer. FIRE uses the query position i to normalize the relative distance and implement interpolation. For auto-regressive generation with causal attention,\n4Here we focus on a single attention head. Generally, with H heads, FIRE learns an MLP f\u03b8 : R \u2192 RH and uses different attention biases for different heads.\n5v1,v3 \u2208 Rs,V2 \u2208 Rs\u00d7s, and s denotes the hidden size. Bias terms are omitted for brevity.\nthe query position index i corresponds to the length of current context. Another possible choice is to use some pre-defined max context length as the normalizer. In this case, the model will still suffer from unfamiliar (large) distances when the texts exceed the pre-defined max lengths, making such a choice suboptimal. Using the query position index as the normalizer avoids this issue."
        },
        {
            "heading": "3.2 ADDITIONAL TRANSFORMATIONS",
            "text": "Inspired by existing methods, we introduce two transformations on FIRE for further improvement. We note that these transformations do no limit the expressive power of FIRE to learn arbitrary biases.\nAmplifying the differences among local positions. Existing works show that RPE attention biases change more rapidly for the local tokens than for the distant tokens (Khandelwal et al., 2018; Wang et al., 2021). Thus, it\u2019s appealing to consider some monotonically increasing transformation \u03c8 : N \u2192 R+ with a monotonically decreasing slope (i.e., a concave function) to the relative distance, so that more modeling capacity can be allocated to learn RPE for local positions:\nb(i, j) = f\u03b8 ( \u03c8(i\u2212 j) \u03c8(i) ) . (3)\nFor example, in our experiments, we use \u03c8 : x 7\u2192 log(cx + 1) where c > 0 is a learnable parameter. This transformation \u03c8 amplifies the differences among local positions. Note that, the log transformation is applied to both the relative distance and the normalizer. Thus, the MLP inputs are still constrained to [0, 1] for any sequence lengths as long as \u03c8 is monotonically increasing.\nThresholding the normalizer for better short sequence modeling. While the progressive interpolation technique offers robust length generalization capabilities, our preliminary experiments indicate a marginal degradation in model performance for shorter sequences. We posit that it\u2019s because the actual relative distances are important in RPE of short sequences, while the normalization in progressive interpolation obfuscates this information. To address this, we introduce an adaptive thresholding mechanism, activating the progressive interpolation technique only for larger query position indices, i.e., long contexts. Specifically, we define a learnable threshold L and only apply progressive interpolation when i > L. For short sequences with less than L tokens, we use \u03c8(L) to normalize the relative distance.\nBased on the above, the positional encoding function of FIRE can be formulated as\nbFIRE(i, j) = f\u03b8\n( \u03c8(i\u2212 j)\n\u03c8(max{L, i})\n) , (4)\nwhere \u03c8 : N \u2192 R+ is monotonically increasing and L > 0 is a learnable scalar. Our main experiments of FIRE are based Eq. (4) with \u03c8 : x 7\u2192 log(cx+ 1). We present experiments ablating these design choices in Appendix B."
        },
        {
            "heading": "3.3 EXPRESSIVENESS OF FIRE",
            "text": "In this subsection, we theoretically prove that FIRE can represent all the existing additive RPE approaches discussed in Sec. 2.2. This expressiveness allows FIRE to learn suitable position encoding functions from the data. We state this formally in the theorem below. The proof can be found in Appendix A. Theorem 3.1. Let b0 be the positional encoding function of T5\u2019s RPE, Alibi, Kerple, or Sandwich as defined in Sec. 2.2. Consider FIRE function bFIRE(i, j) in Eq. (4). Given any sequence length L0 \u2208 N\u2217, there exist some transformation \u03c8, threshold L, and MLP configuration (weights \u03b8 and activation function \u03c3) such that bFIRE(i, j) = b0(i, j) for any 0 < j \u2264 i \u2264 L0.\nRemark. We point out that our proof is constructive, and does not leverage the universal approximation property of MLP, i.e., the MLP does not need to be extremely wide or deep. In fact, FIRE is parameter efficient in the sense that it represents T5\u2019s RPE, Alibi, and Kerple with nearly the same number of parameters (up to a constant factor). Further, in all our experiments with FIRE, we show that a small MLP with a hidden size of 32 suffices for strong performances."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section we present experimental results comparing our proposed unified relative encoding method FIRE with T5\u2019s RPE (Raffel et al., 2019), Alibi (Press et al., 2022), and Kerple (Chi et al., 2022), showing that the proposed approach significantly improves long context generalization while not sacrificing short context performance. We also include comparisons to other popular methods - Rotary Positional Encoding (RoPE) (Su et al., 2021) and no positional encoding (NoPE) (Kazemnejad et al., 2023). We use a hidden size of 32 for the MLPs in FIRE for all our experiments.\nWe consider language models trained on the C4 dataset (Raffel et al., 2019) with 2048 input length, with different positional encoding methods. We first compare the zero-shot perplexity values on inputs with different lengths (512 to 8192) from various datasets, comparing the long context generalization ability of different position encoding methods (Sec. 4.1). Later, we present finetuning results on both longer inputs of length 8192 on SCROLLS (Shaham et al., 2022) and shorter inputs of length 1024 on GLUE/SuperGLUE (Wang et al., 2019b;a) (Sec. 4.2 & 4.4).6 In addition, we conduct experiments on zero-shot long-context question answering on NarrativeQA (Koc\u030cisky\u0300 et al., 2018) with different context lengths from 512 to 32768 (Sec. 4.3). In Appendix B, we present some ablation experiments studying the design choices of FIRE. The complete experimental setup along with the hyper-parameters for each of the tasks and hardware details is provided in Appendix C."
        },
        {
            "heading": "4.1 LANGUAGE MODELING WITH LENGTH GENERALIZATION",
            "text": "Following Brown et al. (2020), we use the causal LM objective to pretrain decoder-only Transformers with different position encodings on C4 dataset (Raffel et al., 2019). We experiment with two model size settings, base (125M parameters) and large (350M parameters). The evaluation metrics are validation log perplexity on C4, arXiv, and Github (Raffel et al., 2019; Gao et al., 2020). We pretrain the models on sequence length to 2048, and evaluate their zero-shot perplexity on sequence lengths {512, 1024, 2048, 4096, 8192}. For base-sized models, we additionally compare our method with a concurrent work, YaRN (Peng et al., 2024), which improves length generalization of RoPE-based Transformer models.7 Model and training configurations are detailed in Appendix C.1.\nThe results are shown in Fig. 1, 2, & 7. We first notice that FIRE consistently achieves lower perplexity across different model sizes, validation sequence lengths, and datasets. In comparison to existing approaches, the performance gain is particularly significant for validation sequences that are longer than training sequences (out-of-distribution sequence lengths), showing better length generalization behavior. For example, for base models trained on sequence length 2048 and evaluated on sequence length 8192, FIRE outperforms the best baseline method, Kerple, by 2.28 points (21.24 v.s. 23.52 perplexity). Methods such as RoPE achieve strong performance for in-distribution sequence lengths, but their performances quickly degrade with longer inputs. YaRN requires knowledge of target sequence length and further finetuning, but we can see from Fig. 1 & 7 that it underperforms FIRE on long sequences and sacrifices model quality on short sequences (e.g., length 512). Note that in all our experiments, perplexity is computed in a single forward pass for a given input, and we do not use any sliding window tricks during inference (Press et al., 2022).\n6While finetuning is not the same as the zero-shot long-context generalization, it still measures the ability of the pre-trained model to adapt to longer inputs in the downstream applications.\n7We note that YaRN needs additional tuning on long sequences. All the other methods in this subsection, including FIRE, are evaluated on long context without any tuning."
        },
        {
            "heading": "4.2 FINETUNING ON LONG TEXT BENCHMARK",
            "text": "To further test the models\u2019 capability of learning and modeling long sequences, we conduct finetuning experiments on SCROLLS, a long text benchmark (Shaham et al., 2022) which contains 7 different datasets. We initialize the models with the C4 checkpoints pretrained on sequence length 2048, and finetune them on sequence length 8192 for each individual task. In addition to position encoding methods in Sec. 4.1, we also experiment with RoPE with positional interpolation (RoPE-PI) (Chen et al., 2023), which extends the context window of RoPE-based pretrained models given a downstream maximum sequence length. Following existing works by Shaham et al. (2022); Ainslie et al. (2023), we use three different evaluation metrics (Rgm, F1, and EM scores) for different datasets. We also compute the average score across different datasets as done in the SCROLLS benchmark. Detailed descriptions of the datasets and evaluation metrics are provided in Appendix C.2.\nThe results on SCROLLS benchmark are shown in Table 1. We first notice that FIRE attains the best average score, outperforming existing approaches by over 1.0 point on both model sizes. Even at the individual task level, FIRE achieves the best performances on 4/5 out of 7 tasks among the base/large models. RoPE-PI significantly improves RoPE as expected, but lags behind FIRE. One drawback though is that RoPE-PI requires the knowledge of maximum input sequence length beforehand, which is not always known in practice for decoder-only models."
        },
        {
            "heading": "4.3 ZERO-SHOT LENGTH GENERALIZATION ON NARRATIVEQA",
            "text": "We next evaluate the zero-shot length generalization capabilities of the finetuned models on the downstream NarrativeQA dataset. We use the NarrativeQA dataset (Koc\u030cisky\u0300 et al., 2018) with different input context lengths to test the model\u2019s ability to leverage long context in zero-shot learning settings. We use the base-sized model checkpoints pretrained on C4 (sequence length 2048) and finetuned on NarrativeQA (sequence length 8192). We evaluate the models on context lengths {512, 2048, 4096, 8192, 16384, 24576, 32768} without any further tuning on the target context lengths. For RoPE with position interpolation (Chen et al., 2023), we consider two variants with max sequence lengths set to 8192 or 32768. We use unigram overlap (F1) as the evaluation metric.\nWe compare FIRE with the most competitive baselines in the left panel of Fig. 3. Detailed results (including omitted baselines) can be found in Table 11. We notice that FIRE achieves top performances consistently across different sequence lengths. The plot also shows sensitivity of RoPE-PI to the max sequence length parameter in this zero-shot length generalization setting. Setting the max sequence length to a small value (8192) results in good performance until 8192, but with a steep drop for longer contexts. On the other hand, using a larger value for max sequence length (32768) gets rid of the steep drop for long contexts; but results in worse performance across all sequence lengths. In contrast, FIRE using progressive interpolation is able to generalize across all sequence lengths."
        },
        {
            "heading": "4.4 FINETUNING ON GLUE/SUPERGLUE",
            "text": "We next evaluate the C4 pre-trained models on GLUE/SuperGLUE benchmarks, to test these methods on shorter sequence lengths. We finetune on standard natural language understanding benchmarks, GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), with shorter sequence lengths (1024) to evaluate the general quality of the models. We use the average accuracy/exact match across all the tasks as our main evaluation metric. Detailed experiment results can be found in Table 12.\nThe results are shown in the right of Fig. 3. Among the baseline approaches, NoPE and Alibi slightly lag behind, while RoPE, Kerple, and T5\u2019s RPE all achieve similarly good accuracies. FIRE is on par with these approaches, demonstrating good performance on GLUE and SuperGLUE tasks. These results show that although FIRE is designed to enhance length generalization of Transformers, it does not sacrifice the accuracy on downstream tasks with shorter sequence lengths."
        },
        {
            "heading": "4.5 VISUALIZATION OF FIRE",
            "text": "In this subsection we present visualization of learned position encoding biases from a FIRE model pretrained on C4. We plot the learned position encoding bias for the query token at the 128th position, for all the attention heads from selected layers in Fig. 4. We notice that, in different attention heads, FIRE learns both local and \u201canti-local\u201d attention patterns that emphasize far away keys more, showing the advantage of functional approach, as opposed to a fixed local inductive bias (Press et al., 2022; Chi et al., 2022; 2023).\n4.6 LAYERWISE SHARING\nAnother important factor beyond length generalization is computational cost of these approaches. Most of FIRE\u2019s computation is based on matrix multiplication, which is more accelerator-friendly than the vector operations used in T5\u2019s RPE. To further improve the computational efficiency of FIRE, we consider FIRE-S, a weight-sharing version which uses the same position encoding bias for all the layers. This way the position encoding bias only needs to be computed once, and the cost is amortized over all the layers. Note that sharing position encoding across layers is a common inductive bias in many existing methods (Su et al., 2021; Press et al., 2022; Luo et al., 2022).\nWe conduct experiments to evaluate FIRE-S (with layerwise sharing) on C4 language modeling, SCROLLS long text benchmark, and GLUE/SuperGLUE. We also measure the inference speed of different methods. Experimental details are provided in C.6.\nModel quality. Table 2 compares the accuracy of FIRE-S and the standard FIRE. The results show that sharing position encoding function across layers only leads to a slight performance degradation. FIRE-S still outperforms other baselines in the long sequence regime. For example, on C4 language modeling with sequence length 8192, it outperforms Kerple, the best baseline in Fig. 1 (3.10 v.s. 3.16 log perplexity). On SCROLLS, its average score outperforms all the strong baseline methods including T5\u2019s RPE, RoPE with positional interpolation, and Kerple.\nInference speed. Fig. 5 compares the model speed of FIRE/FIRE-S with baselines. We first notice that FIRE and FIRE-S are both faster than T5\u2019s RPE while achieving stronger performances. Moreover, FIRE-S significantly improve the efficiency of FIRE and is faster than all the baselines but NoPE (no positional encoding). In conclusion, the experiments show that FIRE-S demonstrates good speed-accuracy trade-off."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose a functional interpolation for relative position encoding (FIRE) to improve Transformer\u2019s ability to generalize to longer contexts, and present theoretical and empirical results showing its effectiveness. We prove that FIRE unifies many existing additive RPE methods, while being adaptive enough to learn diverse position encoding biases in long context settings. Empirical results show strong length generalization behavior pushing the paradigm of train short test long. Our work does suffer from some limitations. 1) We only study decoder models. 2) We do not analyze the role of other components of Transformer and other training components (data, optimizer) in length generalization. These questions are interesting directions for future exploration."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported in part by the United States Department of Energy via the Brookhaven National Laboratory under Contract No. 384608."
        },
        {
            "heading": "A OMITTED PROOF",
            "text": "In this section, we first provide a more general formulation of T5\u2019s positional encoding function as mentioned in Sec. 2.2. Then we provide the proof of Theorem 3.1."
        },
        {
            "heading": "A.1 T5\u2019S RPE WITH BUCKETING",
            "text": "In Sec. 2.2, we use a simplified description for T5\u2019s RPE. In practice, T5\u2019s RPE does not assign different position bias for all different relative positions. Instead, all possible relative distances are partitioned into several buckets, and the relative distances in one bucket share a (learnable) attention bias. Formally speaking, T5\u2019s RPE pre-defines 0 = s0 < s1 < \u00b7 \u00b7 \u00b7 < sk\u22121 < sK , and computes the attention bias as\nb(i, j) = { rk sk \u2264 i\u2212 j < sk+1; k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1 rK i\u2212 j \u2265 sK . (5)\nIt\u2019s easy to see that the formulation in Sec. 2.2 is a special case of Eq. (5) by setting sk = k. In the official T5 implementation8, the buckets are defined based on \u201clog binning\". With K +1 buckets and a pre-defined distance L1, the attention bias is calculated as (assuming K + 1 is even)\nb(i, j) =  ri\u2212j 0 \u2264 i\u2212 j < K+12 rK+1 2 +\u230a K+1 2 log( 2(i\u2212j) K+1 )/ log( 2L1 K+1 )\u230b K+1 2 \u2264 i\u2212 j < L1\nrK i\u2212 j \u2265 L1 . (6)\nThis is also a special case of Eq. (5).\nIn the proof of Theorem 3.1, we will be working on the most general formulation (Eq. (5)), so that the proof works for any specific instances."
        },
        {
            "heading": "A.2 PROOF OF THEOREM 3.1",
            "text": "Proof. For each RPE variant (T5\u2019s RPE, Alibi, Kerple, and Sandwich), we provide constructions in which FIRE represent each of the target b0 for 0 < j \u2264 i < L0.\nT5\u2019s RPE. We consider the general T5\u2019s RPE formulation with bucketing in Eq. (5). The target positional encoding function can be rewritten as\nb0(i, j) = r0 + K\u2211 k=1 (rk \u2212 rk\u22121) \u00b7 1{i\u2212j\u2265sk}. (7)\nConsider a two-layer MLP with activation \u03c3(x) = 1{x\u22650} and K hidden neurons:\nf\u03b8(x) = v \u22a4 2 \u03c3(v1x+ b1) + b2. (8)\nLet v1 = L01 (where 1 denotes an all-one vector), b1 = [\u2212s1,\u2212s2, \u00b7 \u00b7 \u00b7 ,\u2212sK ]\u22a4,v2 = [r1\u2212r0, r2\u2212 r1, \u00b7 \u00b7 \u00b7 , rK \u2212 rK\u22121]\u22a4, and b2 = r0. In the positional encoding function of FIRE (Eq. (4)), we set the transform \u03c8 to be the identity mapping x 7\u2192 x and the threshold L to L0.\n8https://github.com/google-research/text-to-text-transfer-transformer.\nThen for any 0 < j \u2264 i \u2264 L0,\nbFIRE(i, j) =f\u03b8 ( i\u2212 j L0 ) (9)\n= [r1 \u2212 r0 r2 \u2212 r1 \u00b7 \u00b7 \u00b7 rK \u2212 rK\u22121]\u03c3   i\u2212 j \u2212 s1 i\u2212 j \u2212 s2\n... i\u2212 j \u2212 sK\n + r0 (10)\n= [r1 \u2212 r0 r2 \u2212 r1 \u00b7 \u00b7 \u00b7 rK \u2212 rK\u22121]  1{i\u2212j\u2265s1} 1{i\u2212j\u2265s2}\n... 1{i\u2212j\u2265sK} + r0 (11) =\nK\u2211 k=1 (rk \u2212 rk\u22121) \u00b7 1{i\u2212j\u2265sk} + r0. (12)\nThus, we have bFIRE(i, j) = b0(i, j) for any 0 < j \u2264 i \u2264 L0.\nAlibi. The target positional encoding function is b0(i, j) = \u2212r(i\u2212 j) (note that we focus on the setting where i \u2265 j). Consider a one-layer MLP with identity activation and no bias term (which degrades to a linear mapping) f\u03b8(x) = v1x, and let v1 = \u2212rL0. In the positional encoding function of FIRE (Eq. (4)), we set the transform \u03c8 to be the identity mapping x 7\u2192 x and the threshold L to L0. Then for any 0 < j \u2264 i \u2264 L0,\nbFIRE(i, j) = f\u03b8 ( i\u2212 j L0 ) = \u2212r(i\u2212 j) = b0(i, j), (13)\nwhich concludes the proof.\nKerple (logarithmic variant). The target positional encoding function is b0(i, j) = \u2212r1 log(1 + r2(i \u2212 j)) (note that we focus on the setting where i \u2265 j). Consider a one-layer MLP with identity activation and no bias term (which degrades to a linear mapping) f\u03b8(x) = v1x. and let v1 = \u2212r1 log(1 + r2L0). In the positional encoding function of FIRE (Eq. (4)), we set the transform \u03c8 to be the log transform x 7\u2192 log(r2x+1) and the threshold L to L0. Then for any 0 < j \u2264 i \u2264 L0,\nbFIRE(i, j) = f\u03b8 ( log(1 + r2(i\u2212 j)) log(1 + r2L0) ) = \u2212r1 log(1 + r2(i\u2212 j)) = b0(i, j), (14)\nwhich concludes the proof.\nKerple (power variant). The target positional encoding function is b0(i, j) = \u2212r1(i\u2212 j)r2 (note that we focus on the setting where i \u2265 j). Consider a two-layer MLP with activation \u03c3(x) = xr2 , one hidden neuron, and no bias term: f\u03b8(x) = v2(v1x)r2 . Let v1 = r2 \u221a r1L0 and v2 = \u22121. In the positional encoding function of FIRE (Eq. (4)), we set the transform \u03c8 to be the identity mapping x 7\u2192 x and the threshold L to L0. Then for any 0 < j \u2264 i \u2264 L0,\nbFIRE(i, j) = f\u03b8 ( i\u2212 j L0 ) = \u2212( r2 \u221a r1(i\u2212 j))r2 = \u2212r1(i\u2212 j)r2 = b0(i, j), (15)\nwhich concludes the proof.\nSandwich. The target positional encoding function is\np0(i, j) = c d\u2032\u2211 k=1 cos ( (i\u2212 j)/10000 kd\u2032 ) . (16)\nConsider a two-layer MLP with cos activation, d\u2032 hidden neurons, and no bias term:\nf\u03b8(x) = v \u22a4 2 cos(v1x). (17)\nLet v1 = [ L0/10000 1 d\u2032 , L0/10000 2 d\u2032 , \u00b7 \u00b7 \u00b7 , L0/100001 ]\u22a4 and v2 = c1. In the positional encoding\nfunction of FIRE (Eq. (4)), we set the transform \u03c8 to be the identity mapping x 7\u2192 x and the threshold L to L0. Then for any 0 < j \u2264 i \u2264 L0,\nbFIRE(i, j) =f\u03b8 ( i\u2212 j L0 ) (18)\n= [c c \u00b7 \u00b7 \u00b7 c]  cos ( (i\u2212 j)/10000 1d\u2032 ) cos ( (i\u2212 j)/10000 2d\u2032 ) ...\ncos ( (i\u2212 j)/100001 )\n (19)\n=c d\u2032\u2211 k=1 cos ( (i\u2212 j)/10000 kd\u2032 ) . (20)\nThus, we have bFIRE(i, j) = b0(i, j) for any 0 < j \u2264 i \u2264 L0."
        },
        {
            "heading": "B ABLATION STUDY",
            "text": "The positional encoding function of FIRE can be viewed as a composition of a position transformation and a function approximator b(i, j) = f\u03b8(g(i\u2212 j, i)). The position transformation g takes the relative distance i \u2212 j and the query position i as the input and produces a \u201cnormalized\u201d distance. For example, in Eq. (4), the position transformation g : (i\u2212 j, i) 7\u2192 \u03c8(i\u2212 j)/\u03c8(max{i, L}). Different choices of \u03c8 leads different position transformation g. The function approximator f\u03b8 should be in an expressive function class parametrized by \u03b8, which transforms the normalized distances into attention biases. For example, we use a two-hidden-layer MLP with 32 neurons in each hidden layer and ReLU activation by default, as discussed in Appendix C.1.\nIn this section we ablate our design choices for both the position transformation and the function approximator. We also conduct ablation experiments to test the length generalization performances on different training sequence lengths. All the ablation experiments are based on base-sized models."
        },
        {
            "heading": "B.1 THE LOG TRANSFORM AND THRESHOLDING IN POSITION TRANSFORMATIONS",
            "text": "In Sec. 3.2, we propose two modifications, the log transformation and thresholding operation, as additional transformations to the relative distance. We conduct experiments to ablate these design choices and demonstrate their effectiveness. We experiment with base-sized models and compare FIRE variants with or without the additional transformations in Sec. 3.2. Specifically, we consider three variants with the following positional encoding functions:\nWithout log transform/thresholding: b1(i, j) = f\u03b8 ( i\u2212 j i ) . (21)\nWith log transform but without thresholding: b2(i, j) = f\u03b8\n( log(c(i\u2212 j) + 1)\nlog(ci+ 1)\n) . (22)\nWith log transform and thresholding: b3(i, j) = f\u03b8\n( log(c(i\u2212 j) + 1)\nlog(cmax{L, i}+ 1)\n) . (23)\nFor all the three variants (Eq. (21-23)), f\u03b8 is parameterized as a two-hidden-layer MLP with 32 neurons in each hidden layer and ReLU activation to ensure a fair comparison. Eq. (23) is the standard FIRE positional encoding function used in Sec. 4. We experiment on C4 language modeling and GLUE/SuperGLUE benchmark using the settings and evaluation metrics described in Appendix C. The experimental results are shown in Table 3. From the language modeling results, we can see that both the log transformation and the thresholding operation improve the language modeling quality for all the lengths, and the standard FIRE positional encoding function in Eq. (23) is the best variant. In particular, the log transformation largely improve the performance on long sequences,\nindicating that amplifying the differences among local positions helps in the long sequence regimes. We further study the effectiveness of the thresholding operation on GLUE/SuperGLUE benchmark which contains relatively short sequences. The results show that the thresholding operation leads to 0.72 point performance gain on average GLUE/SuperGLUE accuracy, verifying its effectiveness on improving short sequence modeling."
        },
        {
            "heading": "Log transform Thresholding Formula 512 1024 2048 4096 8192",
            "text": "Additional discussions on the thresholding operation. We note that even FIRE without thresholding outperforms all the baselines (including RoPE, T5\u2019s RPE, etc) on all the sequence lengths on C4 language modeling. Detailed comparisons are in Table 4.\nIn all the experiments presented in the paper, the threshold L of FIRE in Eq. (23) is a learnable parameter. For the base-sized model pretrained on sequence length 2048, the learned parameter L is between 1200 to 1600 across different layers. Setting L to a fixed value is also a viable option. In our preliminary exploration, FIRE with either fixed or learnable L outperforms all the baselines, while the learnable variant leads to better performances. The fixed variant introduces one more hyper-parameter and may require more tuning. Thus, FIRE uses learnable threshold L as the default choice."
        },
        {
            "heading": "B.2 EFFECTS OF THE FUNCTION APPROXIMATOR CAPACITY ON THE PERFORMANCES",
            "text": "We experimentally study the impact of the function approximator (f\u03b8) capacity on the model performance. We compare a linear layer, a one-hidden-layer MLP, and a two-hidden-layer MLP. The MLPs both have 32 neurons in the hidden layers and use ReLU (Nair & Hinton, 2010) activation function. Two-hidden-layer MLP is the defualt choice for FIRE in Sec. 4. We experiment on C4 language\nmodeling and evaluate the models on varying sequence lengths using the settings and evaluation metrics described in Appendix C.1 and present the experiment result in 5. The result shows that a linear layer is not experssive enough and leads to suboptimal performance on C4 language modeling. Introducing non-linearty and parametrizing f\u03b8 as a one/two-hidden-layer MLP leads to much better results. In particular, using a one-hidden-layer MLP has largely improve the overall performances especially in the long sequence regimes. For example, it outperforms a linear f\u03b8 by 0.24 point log perplexity on sequence length 8192. Moreover, using an MLP with larger capacity (two hidden layers v.s. one hidden layer) can further brings performance gains. That being said, the MLP is still very tiny (with only 32 hidden neurons) and we believe it\u2019s the non-linearty that helps."
        },
        {
            "heading": "B.3 CHOICE OF THE MLP ACTIVATION FUNCTION",
            "text": "We study the impact of the MLP activation function on the model performance. We experiment on C4 language modeling and evaluate the models on varying sequence lengths using the settings and evaluation metrics described in Appendix C.1. We compare ReLU (Nair & Hinton, 2010) and GeLU (Hendrycks & Gimpel, 2016) activation functions and present the experiment result in 6. The result shows that the model performance is not sensitive to the choice of activation function in the length generalization setting, while ReLU works better on normal sequence lengths. Thus, we use ReLU as our default activation function."
        },
        {
            "heading": "B.4 CHOICE OF FINAL ACTIVATION OF MLP OUTPUT",
            "text": "In our main experiments, we focus on MLPs of the form f\u03b8(x) = v\u22a4\u2113 \u03c3(\u00b7 \u00b7 \u00b7\u03c3(v1x)) where \u03c3 is the activation function. In this implementation, the MLP ends with a linear layer and no activation function is applied to the MLP final output. A slightly different choice is to consider f\u0303\u03b8(x) = \u03c3(v\u22a4\u2113 \u03c3(\u00b7 \u00b7 \u00b7\u03c3(v1x))) where a final activation is applied to the MLP output. We compare these two choices by experimenting on C4 language modeling and evaluating the models on varying sequence lengths. We use one-hidden-layer MLP with 32 hidden neurons and the ReLU (Nair & Hinton, 2010) activation function in both model variants. The results are presented in Table 7. We find that MLP without final activation leads to better performances on long sequences and use it as our default choice."
        },
        {
            "heading": "B.5 FIRE IS STILL STRONG WHEN TRAINED ON SEQUENCE LENGTH 512",
            "text": "In most of our pretraining experiments, the training sequence length is set to 2048 (see Appendix C.1). In this experiment we train models with different positional encodings on C4 with training sequence length 512 to confirm that the overall performance trends are not sensitive to the pretraining\nsequence length. Other experimental settings are te same as those in Appendix C.1. We evaluate the models on varying sequence lengths and report the log perplexity in Fig. 6. It\u2019s clear that FIRE still achieves the strongest overall performance compared with all the other baselines. The results in Fig. 1 & 6 demonstrate that FIRE can robustly deliver higher modeling quality regardless of the training sequence lengths."
        },
        {
            "heading": "C EXPERIMENT SETTINGS & ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 LANGUAGE MODELING WITH LENGTH GENERALIZATION",
            "text": "Model configurations. In this experiment, we train decoder-only Transformer language models with different positional encoding variants while keeping all the other configurations the same. For T5\u2019s RPE, we follow Raffel et al. (2019) and use 64 position bucket for each attention head. For Alibi, we follow Raffel et al. (2019) to set the hyperparameters in the positional encoding function in each attention head. For our FIRE method, we use the positional encoding function defined in Eq. (4). In Eq. (4), we let \u03c8(x) = log(cx+ 1) where c is a learnable parameter; f\u03b8 is parametrized as a two-hidden-layer MLP with 32 neurons in each hidden layer and ReLU activation.\nWe experiment with two model size settings, base (125M parameters) and large (350M parameters). The model configurations follow (Brown et al., 2020) and are presented in Table 8.\nTraining recipe. Following Brown et al. (2020), we use the causal LM objective to pretrain decoder-only Transformers with different position encodings. We use the C4 dataset (Raffel et al., 2019) as the pretraining corpora. We set pretraining sequence lengths to 2048, and evaluate the zero-shot perplexity on sequence lengths {512, 1024, 2048, 4096, 8192}. We truncate documents with length greater than 2048 to multiple sequences of length 2048 during training; similar trucation is done to construct the validation sets of different sequence lengths. Our training recipe follows (Brown et al., 2020) and is presented in Table 9.\nAdditional results. We evaluate language modeling log perplexity with varying lengths on C4, arXiv, and Github datasets (Raffel et al., 2019; Gao et al., 2020) for both base and large models. The\nresults of base models on C4 are presented in Fig. 1. The results of large models on all the three datasets are presented in Fig. 2. In Fig. 7, we additionally present the results of base models on arXiv and Github. All the results show similar trends and FIRE consistently demonstrate strong length generalization behavior."
        },
        {
            "heading": "C.2 FINETUNING ON LONG TEXT BENCHMARK",
            "text": "Datasets and evaluation metrics. We use SCROLLS long text benchmark (Shaham et al., 2022) to further test the models\u2019 capability of learning and modeling long sequences. SCROLLS benchmark includes question-answering datasets - Qasper, NarrativeQA, and QuALITY ; natural language inference datasets - ContractNLI; and summarization datasets - QMSum, SummScreenFD, and GovReport. Following existing works Shaham et al. (2022); Ainslie et al. (2023), three different evaluation metrics are used for different datasets: Rgm score (the geometric mean of ROUGE-1,2,L) for GovReport, SummScreenFD, and QMSum, unigram overlap (F1) for Qasper and NarrativeQA, and exact match (EM) for ContractNLI and QuALITY. We also compute the average score across different datasets as done in the SCROLLS benchmark.\nModel and training configurations. We finetune the checkpoints pretrained on C4, so the model configurations are the same as those in Table 8. We use the same set of hyperparameters for all the models and all the tasks, and report the best results on the validation set. Table 10 presents our finetuning configurations."
        },
        {
            "heading": "C.3 ZERO-SHOT LENGTH GENERALIZATION ON NARRATIVEQA",
            "text": "Datasets and evaluation metrics. We use the NarrativeQA dataset (Koc\u030cisky\u0300 et al., 2018) with different input context lengths to test the model\u2019s ability to leverage long context in zero-shot learning settings. We use the base-sized model checkpoints pretrained on C4 (sequence length 2048) and finetuned on NarrativeQA (sequence length 8192). We evaluate the models on context lengths {512, 2048, 4096, 8192, 16384, 24576, 32768} and use unigram overlap (F1) as the evaluation metric.\nDetailed results. We provide detailed performances of all the tested models in Table 11. The result shows that FIRE is consistently outperforming all the baselines across all different context lengths."
        },
        {
            "heading": "C.4 FINETUNING ON GLUE/SUPERGLUE",
            "text": "Datasets, evaluation metrics, and configurations. GLUE and SuperGLUE are widely-used benchmarks to evaluation the natrual language understanding capability of neural language models (Wang et al., 2019b;a). We finetune the models on a mixture of the tasks in GLUE and SuperGLUE for simplicity. We evaluate the model on each task separately. We use the macro average accuracy/exact match across all the tasks as our main evaluation metric. Table 12 presents our finetuning configurations.\nDetailed results. For reference, we present detailed results for all the models on each individual dataset in Table 13. In general, FIRE achieves decent performances. Thus, FIRE\u2019s strong performances on long sequences does not come at the price of sacrificing model quality on short sequences and standard tasks.\nC.5 VISUALIZATION\nWe present another visualization of learned FIRE biases for query at position 8192 in Figure 8."
        },
        {
            "heading": "C.6 EFFICIENCY AND FIRE-SHARED",
            "text": "For FIRE-S (FIRE with layerwise sharing), we experiment with the base-sized model (125M parameters), and keep all the configurations and training recipes the same as those in previous subsections. The models are pretrained on C4 with sequence length 2048. The finetuning sequence lengths are 8192/1024 for SCROLLS and GLUE/SuperGLUE, respectively.\nFor the inference time evaluation, we test the forward time of base-sized model with different positional encodings on sequence length 2048. We measure the forward time on 4 TPUv2 chips for all the models, and report the average result over 10 runs."
        },
        {
            "heading": "D RELATED WORKS",
            "text": "In the main body of the paper, we cover the most relevant works to our paper (Sec. 2). In this section, we provide more discussions on related works.\nLength generalization. Many existing works show the length generalization failure of standard Transformer models (Press et al., 2022; Anil et al., 2022; Deletang et al., 2023; Liu et al., 2024). Recently, there have been growing interests in long-context applications such as multi-step reasoning (Wei et al., 2022; Dziri et al., 2023; Zhao et al., 2023) and document/book understanding (Koc\u030cisky\u0300 et al., 2018; Ke et al., 2022; Guo et al., 2022; Ainslie et al., 2023; Liu et al., 2023). Designing lengthgeneralizable Transformers is appealing for these applications. Dubois et al. (2020); Chowdhury & Caragea (2023) introduce location attention for length generalization on synthetic tasks. Bueno et al. (2022) show that generating step-by-step rationales and using marker tokens as positional guides helps length generalization. Studying positional encoding approaches for length generalization is a main direction in this line of research. Press et al. (2022); Chi et al. (2022; 2023) propose new relative positional encoding methods which emphasize recency bias and improve language modeling on longer sequences. Chu et al. (2023) propose Conditional Positional Encodings to enhance Vision Transformer length generalization. The most relevant to our work is a concurrent paper by Chen et al. (2023). It propose Position Interpolation (PI) for Rotary Positional Encoding (RoPE), which extends the context window of RoPE-based pretrained models given a downstream max sequence length. However, this requires additional finetuning on longer sequence data, albeit for much fewer steps than original training. By contrast, our proposed FIRE does not require a pre-defined max sequence length, and can be directly applied to length generalization setting without tuning. We provide extensive experimental comparisons in Sec. 4. More recently, Zhou et al. (2024) show that standard Transformers can generalize to a sequence length that is 2.5\u00d7 the training input length on integer addition using FIRE (and other techniques (Ruoss et al., 2023; Zhou et al., 2023)).\nPositional encoding in Transformers. Positional encoding is a critical component of Transformers. Vaswani et al. (2017) propose sinusoidal Absolute Positional Encoding (APE) to encode positional information in the sequential input. Shaw et al. (2018) are the first to propose Relative Positional Encoding (RPE) for Transformers, and many follow-up works explore different RPE strategies (Dai et al., 2019; Raffel et al., 2019). There are also many works that study positional encoding from different perspectives, including the disentanglement of positional and content information (Kitaev & Klein, 2018; Ke et al., 2021), the representational power of attention modules and Transformers (Cordonnier et al., 2019; Chen et al., 2021; Li et al., 2021; Luo et al., 2022), computational efficiency (Su et al., 2021; Liutkus et al., 2021; Luo et al., 2021; Choromanski et al., 2023), and length generalization (Press et al., 2022; Chi et al., 2022; 2023; Kazemnejad et al., 2023). Our work is based on a unified formulation of existing additive relative positional encoding approaches, and proposes new RPE variant aimed at improving length generalization.\nInterpolation techniques in deep learning. Interpolation techniques are successfully applied to many deep learning applications, especially in computer vision. Long et al. (2015) employ bilinear interpolation in up-sampling layers of convolutional neural networks for dense visual prediction. Dong et al. (2015); Johnson et al. (2016) employ bicubic interpolation for image super-resolution. Radford et al. (2015) probe generative models by interpolation in the latent space. Zhang et al. (2018); Han et al. (2022) use interpolating between pairs of examples and their labels as an data augmentation method. Recently, Dosovitskiy et al. (2021) propose to perform 2D interpolation of the pre-trained APE for Vision Transformer to apply the model to higher resolution images. In contrast, our interpretation is applied in the relative position encoding functions. Besides, we are focused on causal attention setting where \u201cglobal\u201d information such as the total sequence length is unknown, while Dosovitskiy et al. (2021) work on encoder-only Transformers with fixed input lengths.\nE IMPLEMENTATION\nIn this section, we present the implementation of our proposed FIRE module in PyTorch (Paszke et al., 2019).\n1 import torch 2 import torch.nn as nn 3 4 class FIRE(nn.Module): 5 def __init__(self, num_heads=12, mlp_width=32, init_c=0.1, 6 init_L=512., eps=1e-6): 7 \"\"\" 8 FIRE attention bias module. 9\n10 Args: 11 num_heads: number of attention heads. 12 mlp_width: Width of MLP. 13 init_c: initial value of log transformation parameter 14 init_L: initial value of thresholding parameter 15 eps: small constant for numerical stability 16 \"\"\" 17 super(FIRE, self).__init__() 18 19 # Define the MLP layers 20 self.mlp = nn.Sequential( 21 nn.Linear(1, mlp_width), 22 nn.ReLU(), 23 nn.Linear(mlp_width, num_heads) 24 ) 25 26 # Initialize c (log transformation parameter) 27 self.c = nn.Parameter(torch.tensor(init_c)) 28\n29 # Initialize L (threshold) 30 self.init_L = nn.Parameter(torch.tensor(init_L), 31 requires_grad=False) 32 # Learn a multiplier to L 33 self.L_multiplier = nn.Parameter(torch.tensor(1.0)) 34 35 self.eps = eps 36 37 def forward(self, x: torch.Tensor): 38 \"\"\" 39 Compute FIRE attention bias. 40 41 Args: 42 x: input sequence, 43 shape [bsz, num_heads, seq_len, hidden_dim] 44 45 Returns: 46 attention bias, 47 shape [1, num_heads, seq_len, seq_len] 48 \"\"\" 49 seq_length = x.size(2) 50 positions = torch.arange(seq_length, 51 dtype=torch.float, 52 device=x.device) 53 rel_distance = positions[:, None] - positions[None, :] 54 55 # Thresholding the normalizer 56 threshold = torch.abs(self.L_multiplier * self.init_L) 57 pos_normalizer = torch.max(positions, threshold) 58 pos_normalizer = pos_normalizer[:, None] 59 60 # Amplifying differences among local positions 61 # with log transform 62 rel_distance = torch.log( 63 torch.abs(self.c * rel_distance) + 1 64 ) 65 pos_normalizer = torch.log( 66 torch.abs(self.c * pos_normalizer) + 1 67 ) + self.eps 68 69 # Progressive interpolation 70 normalized_distance = rel_distance / pos_normalizer 71 fire_bias = self.mlp(normalized_distance.unsqueeze(-1)) 72 fire_bias = fire_bias.unsqueeze(0).permute(0, 3, 1, 2) 73 return fire_bias"
        }
    ],
    "title": "FUNCTIONAL INTERPOLATION FOR RELATIVE POSI-",
    "year": 2024
}