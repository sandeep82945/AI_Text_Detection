{
    "abstractText": "Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the results. This either produces high-frequency flickering, or texture-sticking artifacts that are not amenable to post-processing. With this in mind, we propose a novel method for preserving temporal correlations in a sequence of noise samples. This approach is materialized by a novel noise representation, dubbed \u222b -noise (integral noise), that reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses \u222b -noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed \u222b -noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation. See https://warpyournoise.github.io for video results.",
    "authors": [],
    "id": "SP:cd61fa3e5e95be9a2c4fed33236e95355b9d2de4",
    "references": [
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Qinsheng Zhang",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro",
                "Tero Karras",
                "Ming-Yu Liu"
            ],
            "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
            "year": 2022
        },
        {
            "authors": [
                "Ankan Kumar Bhunia",
                "Salman Khan",
                "Hisham Cholakkal",
                "Rao Muhammad Anwer",
                "Jorma Laaksonen",
                "Mubarak Shah",
                "Fahad Shahbaz Khan"
            ],
            "title": "Person Image Synthesis via Denoising Diffusion Model. nov 2022",
            "venue": "URL http://arxiv.org/abs/2211.12500",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Blattmann",
                "Robin Rombach",
                "Huan Ling",
                "Tim Dockhorn",
                "Seung Wook Kim",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "title": "Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2023
        },
        {
            "authors": [
                "Duygu Ceylan",
                "Chun-Hao Paul Huang",
                "Niloy J. Mitra"
            ],
            "title": "Pix2Video: Video Editing using Image Diffusion",
            "year": 2023
        },
        {
            "authors": [
                "Wenhao Chai",
                "Xun Guo",
                "Gaoang Wang",
                "Yan Lu"
            ],
            "title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing",
            "year": 2023
        },
        {
            "authors": [
                "Weifeng Chen",
                "Jie Wu",
                "Pan Xie",
                "Hefeng Wu",
                "Jiashi Li",
                "Xin Xia",
                "Xuefeng Xiao",
                "Liang Lin"
            ],
            "title": "Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models",
            "year": 2023
        },
        {
            "authors": [
                "Ernie Chu",
                "Tzuhsuan Huang",
                "Shuo-Yen Lin",
                "Jun-Cheng Chen"
            ],
            "title": "MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance",
            "year": 2023
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Byeongsu Sim",
                "Dohoon Ryu",
                "Jong Chul Ye"
            ],
            "title": "Improving Diffusion Models for Inverse Problems using Manifold Constraints. (NeurIPS):1\u201328, 2022",
            "venue": "ISSN 10495258",
            "year": 2022
        },
        {
            "authors": [
                "Paul Couairon",
                "Cl\u00e9ment Rambour",
                "Jean-Emmanuel Haugeard",
                "Nicolas Thome"
            ],
            "title": "VidEdit: ZeroShot and Spatially Aware Text-Driven Video Editing",
            "venue": "URL http://arxiv. org/abs/2306.08707",
            "year": 2023
        },
        {
            "authors": [
                "Keenan Crane"
            ],
            "title": "Digital geometry processing with discrete exterior calculus",
            "venue": "In ACM SIGGRAPH 2013 Courses,",
            "year": 2013
        },
        {
            "authors": [
                "ROBERT C Dalang",
                "Thomas S Mountford"
            ],
            "title": "Level sets, bubbles and excursions of a Brownian sheet",
            "venue": "Infinite Dimensional Stochastic Analysis (Amsterdam,",
            "year": 1999
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alex Nichol"
            ],
            "title": "Diffusion Models Beat GANs on Image Synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Songwei Ge",
                "Seungjun Nah",
                "Guilin Liu",
                "Tyler Poon",
                "Andrew Tao",
                "Bryan Catanzaro",
                "David Jacobs",
                "Jia-Bin Huang",
                "Ming-Yu Liu",
                "Yogesh Balaji"
            ],
            "title": "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models",
            "year": 2023
        },
        {
            "authors": [
                "Michal Geyer",
                "Omer Bar-Tal",
                "Shai Bagon",
                "Tali Dekel"
            ],
            "title": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing",
            "year": 2023
        },
        {
            "authors": [
                "R\u0131za Alp G\u00fcler",
                "Natalia Neverova",
                "Iasonas Kokkinos"
            ],
            "title": "Densepose: Dense human pose estimation in the wild",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yuwei Guo",
                "Ceyuan Yang",
                "Anyi Rao",
                "Yaohui Wang",
                "Yu Qiao",
                "Dahua Lin",
                "Bo Dai"
            ],
            "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
            "year": 2023
        },
        {
            "authors": [
                "Amir Hertz",
                "Ron Mokady",
                "Jay Tenenbaum",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
            "venue": "URL http: //arxiv.org/abs/2208.01626",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P. Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J. Fleet",
                "Tim Salimans"
            ],
            "title": "Imagen Video: High Definition Video Generation with Diffusion Models. oct 2022a. doi: 10.48550/arxiv.2210.02303. URL http://arxiv.org/abs/2210.02303",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J. Fleet. Video Diffusion Models. apr"
            ],
            "title": "ISSN 10495258",
            "venue": "doi: 10.48550/arxiv.2204.03458. URL http://arxiv.org/abs/2204.03458.",
            "year": 2022
        },
        {
            "authors": [
                "William Holt",
                "Duy Nguyen"
            ],
            "title": "Introduction to Bayesian Data Imputation",
            "venue": "SSRN Electronic Journal,",
            "year": 2023
        },
        {
            "authors": [
                "Inbar Huberman-Spiegelglas",
                "Vladimir Kulikov",
                "Tomer Michaeli"
            ],
            "title": "An Edit Friendly DDPM Noise Space: Inversion and Manipulations",
            "venue": "URL http://arxiv.org/abs/2304",
            "year": 2023
        },
        {
            "authors": [
                "Yoni Kasten",
                "Dolev Ofri",
                "Oliver Wang",
                "Tali Dekel"
            ],
            "title": "Layered neural atlases for consistent video editing",
            "venue": "ACM Transactions on Graphics,",
            "year": 2021
        },
        {
            "authors": [
                "Levon Khachatryan",
                "Andranik Movsisyan",
                "Vahram Tadevosyan",
                "Roberto Henschel",
                "Zhangyang Wang",
                "Shant Navasardyan",
                "Humphrey Shi"
            ],
            "title": "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators",
            "year": 2023
        },
        {
            "authors": [
                "Tuomas Kynk\u00e4\u00e4nniemi",
                "Tero Karras",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Improved precision and recall metric for assessing generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Wei-Sheng Lai",
                "Jia-Bin Huang",
                "Oliver Wang",
                "Eli Shechtman",
                "Ersin Yumer",
                "Ming-Hsuan Yang"
            ],
            "title": "Learning blind video temporal consistency",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Jun Hao Liew",
                "Hanshu Yan",
                "Jianfeng Zhang",
                "Zhongcong Xu",
                "Jiashi Feng"
            ],
            "title": "MagicEdit: HighFidelity and Temporally Coherent Video Editing",
            "venue": "pp. 1\u20138,",
            "year": 2023
        },
        {
            "authors": [
                "Guan-Horng Liu",
                "Arash Vahdat",
                "De-An Huang",
                "Evangelos A. Theodorou",
                "Weili Nie",
                "Anima Anandkumar"
            ],
            "title": "I2SB: Image-to-Image Schr\u00f6dinger Bridge. feb 2023a",
            "venue": "doi: 10.48550/arxiv.2302",
            "year": 2023
        },
        {
            "authors": [
                "Shaoteng Liu",
                "Yuechen Zhang",
                "Wenbo Li",
                "Zhe Lin",
                "Jiaya Jia"
            ],
            "title": "Video-P2P: Video Editing with Cross-attention Control",
            "venue": "2023b. URL http://arxiv.org/abs/2303.04761",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Shi Qiu",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic Models",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided Image Synthesis and Editing With Stochastic Differential Equations",
            "venue": "ICLR 2022 - 10th International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Null-text Inversion for Editing Real Images using Guided Diffusion Models. 2022",
            "venue": "URL http://arxiv.org/ abs/2211.09794",
            "year": 2022
        },
        {
            "authors": [
                "Haomiao Ni",
                "Changhao Shi",
                "Kai Li",
                "Sharon X. Huang",
                "Martin Renqiang Min"
            ],
            "title": "Conditional Image-to-Video Generation with Latent Flow Diffusion Models. mar 2023",
            "venue": "URL http:// arxiv.org/abs/2303.13744",
            "year": 2023
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
            "venue": "Proceedings of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Gaurav Parmar",
                "Krishna Kumar Singh",
                "Richard Zhang",
                "Yijun Li",
                "Jingwan Lu",
                "Jun Yan Zhu"
            ],
            "title": "Zero-shot Image-to-Image Translation",
            "venue": "Proceedings - SIGGRAPH 2023 Conference Papers,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical TextConditional Image Generation with CLIP Latents. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bjorn Ommer"
            ],
            "title": "HighResolution Image Synthesis with Latent Diffusion Models",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Ruta",
                "Gemma Canet Tarr\u00e9s",
                "Andrew Gilbert",
                "Eli Shechtman",
                "Nicholas Kolkin",
                "John Collomosse"
            ],
            "title": "DIFF-NST: Diffusion Interleaving For deFormable",
            "venue": "Neural Style Transfer",
            "year": 2023
        },
        {
            "authors": [
                "Chaehun Shin",
                "Heeseung Kim",
                "Che Hyun Lee",
                "Sang-gil Lee",
                "Sungroh Yoon"
            ],
            "title": "Edit-A-Video: Single Video Editing with Object-Aware Consistency",
            "year": 2023
        },
        {
            "authors": [
                "Uriel Singer",
                "Adam Polyak",
                "Thomas Hayes",
                "Xi Yin",
                "Jie An",
                "Songyang Zhang",
                "Qiyuan Hu",
                "Harry Yang",
                "Oron Ashual",
                "Oran Gafni",
                "Devi Parikh",
                "Sonal Gupta",
                "Yaniv Taigman"
            ],
            "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data. sep 2022",
            "venue": "doi: 10.48550/arxiv.2209.14792. URL http://arxiv.org/abs/2209.14792",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric A. Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-Based Generative Modeling Through Stochastic Differential Equations",
            "venue": "ICLR 2021 - 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jingwei Tang",
                "Vinicius C. Azevedo",
                "Guillaume Cordonnier",
                "I Barbara Solenthaler. Honey"
            ],
            "title": "Shrunk the Domain: Frequency-aware Force Field Reduction for Efficient Fluids Optimization",
            "venue": "Computer Graphics Forum,",
            "year": 2021
        },
        {
            "authors": [
                "Zachary Teed",
                "Jia Deng"
            ],
            "title": "Raft: Recurrent all-pairs field transforms for optical flow",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Narek Tumanyan",
                "Michal Geyer",
                "Shai Bagon",
                "Tali Dekel"
            ],
            "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation. 2022",
            "venue": "URL http://arxiv.org/abs/2211",
            "year": 2022
        },
        {
            "authors": [
                "Dani Valevski",
                "Matan Kalman",
                "Yossi Matias",
                "Yaniv Leviathan"
            ],
            "title": "UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image. oct 2022",
            "venue": "ISSN 15577368",
            "year": 2022
        },
        {
            "authors": [
                "Ruben Villegas",
                "Mohammad Babaeizadeh",
                "Pieter-Jan Kindermans",
                "Hernan Moraldo",
                "Han Zhang",
                "Mohammad Taghi Saffar",
                "Santiago Castro",
                "Julius Kunze",
                "Dumitru Erhan"
            ],
            "title": "Phenaki: Variable Length Video Generation From Open Domain Textual Description",
            "venue": "URL http: //arxiv.org/abs/2210.02399",
            "year": 2022
        },
        {
            "authors": [
                "Bram Wallace",
                "Akash Gokul",
                "Nikhil Naik"
            ],
            "title": "EDICT: Exact Diffusion Inversion via Coupled Transformations. nov 2022",
            "venue": "URL http://arxiv.org/abs/2211.12446",
            "year": 2022
        },
        {
            "authors": [
                "John B. Walsh"
            ],
            "title": "An introduction to stochastic partial differential equations",
            "venue": "In E\u0301cole d\u2019E\u0301te\u0301 de Probabilite\u0301s de Saint Flour XIV -",
            "year": 1984
        },
        {
            "authors": [
                "Chen Henry Wu",
                "Fernando De la Torre"
            ],
            "title": "Unifying Diffusion Models\u2019 Latent Space, with Applications to CycleDiffusion and Guidance. oct 2022",
            "venue": "URL http://arxiv.org/abs/2210",
            "year": 2022
        },
        {
            "authors": [
                "Jay Zhangjie Wu",
                "Yixiao Ge",
                "Xintao Wang",
                "Weixian Lei",
                "Yuchao Gu",
                "Yufei Shi",
                "Wynne Hsu",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou"
            ],
            "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation",
            "venue": "URL http://arxiv.org/abs/2212",
            "year": 2022
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the Generative Learning Trilemma With Denoising Diffusion Gans",
            "venue": "ICLR 2022 - 10th International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Shuai Yang",
                "Yifan Zhou",
                "Ziwei Liu",
                "Chen Change Loy"
            ],
            "title": "Rerender A Video: Zero-Shot TextGuided Video-to-Video Translation. jun 2023",
            "venue": "URL http://arxiv.org/abs/2306",
            "year": 2023
        },
        {
            "authors": [
                "Fisher Yu",
                "Ari Seff",
                "Yinda Zhang",
                "Shuran Song",
                "Thomas Funkhouser",
                "Jianxiong Xiao"
            ],
            "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop",
            "year": 2015
        },
        {
            "authors": [
                "Guoqiang Zhang",
                "J.P. Lewis",
                "W. Bastiaan Kleijn"
            ],
            "title": "Exact Diffusion Inversion via Bi-directional Integration Approximation",
            "venue": "pp. 1\u201313,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding Conditional Control to Text-to-Image Diffusion Models. feb 2023",
            "venue": "URL http://arxiv.org/abs/2302.05543",
            "year": 2023
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "year": 2018
        },
        {
            "authors": [
                "Yuxin Zhang",
                "Nisha Huang",
                "Fan Tang",
                "Haibin Huang",
                "Chongyang Ma",
                "Weiming Dong",
                "Changsheng Xu"
            ],
            "title": "Inversion-Based Style Transfer with Diffusion Models",
            "venue": "pp. 10146\u201310156,",
            "year": 2022
        },
        {
            "authors": [
                "Min Zhao",
                "Rongzhen Wang",
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing",
            "venue": "URL http://arxiv. org/abs/2305.17098",
            "year": 2023
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2023) or the interpolation methods are 3 orders of magnitude slower than the simple noise sampling methods. Finally, our warping method requires \u2248 1 second per frame to warp the noise. While being considerably less efficient than aforementioned methods, our approach is comparable to DDIM inversion. Additionally, our method trades off the noise warp accuracy with efficiency through two parameters",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "\u222b -noise (integral noise),\nthat reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses \u222b -noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed \u222b -noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation. See https://warpyournoise.github.io for video results."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Despite their notable ability to generate high-quality images (Rombach et al., 2022; Nichol et al., 2022) from simple text inputs, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have been applied with limited success when it comes to video processing and generation. While it is a common practice to build diffusion-based video pipelines upon pretrained text-to-image models (Ceylan et al., 2023; Geyer et al., 2023; Yang et al., 2023; Khachatryan et al., 2023), these approaches do not have a systematic way to preserve the natural correlations between subsequent frames from a video. One contributing factor to this problem is the indiscriminate use of noise to corrupt data during inference: all relevant motion information present in a video is lost in the diffusion process. Employing a different set of noises for each frame within video sequences yields results that manifest high-frequency hallucinations that are challenging to eliminate, and some approaches resort to finetuning neural networks with temporal attention layers (Liew et al., 2023; Liu et al., 2023b; Shin et al., 2023; Zhao et al., 2023; Wu et al., 2022).\nOther than using random sets of noise at inference, it is also common to employ fixed noise as a content-agnostic solution for artificially enforcing correlations. However, the problem of fixed noise signals manifests as texture-sticking artifacts that are oblivious to post-process techniques. Alternatively, one can also invert the set of noises that reconstruct a given image sequence under a conditional prompt (Ceylan et al., 2023; Geyer et al., 2023). The limitation of such an approach is that the set of inverted noises closely entangles the temporal information with the content of the frames, making such methods not suitable for injecting the learned temporal correlation into arbitrary noise samples. Lastly, techniques such as feature warping (Ni et al., 2023; Yang et al., 2023), or cross-frame attention (Ceylan et al., 2023; Geyer et al., 2023; Yang et al., 2023; Khachatryan et al., 2023) can also alleviate temporal coherency issues to a certain extent. However, these approaches are limited because the features are not able to represent high frequencies patterns of the fine resolution image. Given the importance of the role of the noise in inferring diffusion models, it\u2019s surprising how little research was devoted to exploring the impact of noise priors on temporal\ncoherency. Thus, we were motivated by the simple question: how can we create a noise prior that preserves correlations present in an image sequence?\nOur answer is to warp a noise signal, such that it generates a new noise sample that preserves the correlations induced by motion vectors or optical flow. However, implementing such approach is not so simple. The commonly employed Gaussian noise has special properties that are lost when warped/transported with standard methods. Transport relies on the sub-sampling of a noise signal in an undeformed space. These sampling operations create important cross-correlations that are detrimental to the preservation of the noise power spectral density and to the independence between pixels within a single sample. Our paper addresses the outlined limitations in transporting noise fields with three novel contributions. The first one is to reinterpret individual noise samples used in diffusion models as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite noise. We dub this new interpretation of a discrete noise sample as \u222b -noise (integral noise). The second contribution is the derivation of the noise transport equation, which allows accurate, distribution-preserving transport of a continuously defined noise signal. Lastly, we design a carefully tailored transport algorithm that discretizes the noise transport equation, maximizing the temporal correlation between samples while also preserving its original properties. Our results demonstrate that our noise warping method creates temporally-correlated noise fields, which can be used for a variety of tasks such as video restoration and editing, surrogate rendering, and conditional video generation.\n2 METHOD 2.1 THE \u222b -NOISE REPRESENTATION\n2\n1\n0\n\u221e\nA discrete 2D Gaussian noise of dimension D\u00d7D is represented by the function G : (i, j) \u2208 {1, . . . , D}2 \u2192 Xi,j that maps a pixel coordinate (i, j) to a random variable Xi,j . Diffusion models employ this discrete formulation, and random variables are assumed to be independently and identically distributed (i.i.d.) Gaussian samples Xi,j \u223c N (0, 1). At the core of our work is the reinterpretation of this discrete 2D Gaussian noise as the integral of an underlying infinite noise field.\nAn infinite-resolution noise field is represented by a 2D white Gaussian noise signal. To construct that, we start by endowing the domain E = [0, D]\u00d7[0, D] with the usual Borel \u03c3-algebra E = B(E) and the standard Lebesgue measure \u03bd. Then, the white Gaussian noise on the \u03c3-finite measure space (E, E , \u03bd) is defined as a function W : A \u2208 E \u2192 W (A) \u223c N (0, \u03bd(A)) that maps A \u2014 a subset of the domain E \u2014 to a Gaussian-distributed variable, with variance \u03bd(A) (Walsh, 2006). There are many valid ways to subdivide the domain representing the continuous noise. The standard discrete setting partitions the domain E into D\u00d7D regularly spaced non-overlapping square subsets. We denote this partition as A0 \u2286 E (level 0 in the inset image). Another way of partitioning the domain is to further refine E into a higher resolution set Ak \u2286 E (levels k = 1, 2, ...,\u221e in the inset image, where each k-th level below subdivides each pixel in level 0 into Nk = 2k \u00d7 2k sub-pixels.) Due to the properties of white Gaussian noise, integrating sub-pixels of the noise defined on Ak maintains the properties of noise defined on A0. If we assume there is only one single pixel sample in the domain (D = 1) A0 = [0, 1] \u00d7 [0, 1], with Ak = {Ak1 , . . . , AkNk} representing the Nk sub-pixels at a finer resolution k, the following holds:\nNk\u2211 i=1 W (Aki ) = W ( Nk\u22c3 i=1 Aki ) = W (A 0). (1)\nThe proposed \u222b\n-noise refers to the idea that instead of representing a noise value in a discrete point of domain, we rather represent the white noise integral over a pre-specified area. This property is represented in Equation (1) and a more thorough explanation is presented in Appendix B.1. Since we assume that each pixel on the coarsest level A0 has unit area, the noise variance \u03bdk = \u03bd(Aki ) at each level is implicitly scaled by the sub-pixel area as \u03bdk = 1/Nk. While it is impossible to sample the noise A\u221e in the infinite setting, we show in Section 2.2 that approximating it with a higher-resolution grid is sufficient for a temporally coherent noise transport.\nConditional white noise sampling. In practice, after obtaining an a priori noise, e.g. from noise inversion techniques in diffusion models, one important aspect is how to construct the \u222b -noise at Ak from samples defined at A0. This is fundamentally a conditional probability question: given the value of an entire pixel, what is the distribution of its sub-pixels values? Let W (Ak) =( W (Ak1), . . . ,W (A k Nk ) )\u22a4 \u223c N (0, \u03bdkI) be the Nk-dimensional Gaussian random variable rep-\nresenting sub-pixels of a single pixel. Then, the conditional distribution ( W (Ak)|W (A0) = x ) is\n( W (Ak)|W (A0) = x ) \u223c N ( \u00b5\u0304, \u03a3\u0304 ) , with \u00b5\u0304 = x\nNk u, \u03a3\u0304 =\n1\nNk\n( INk \u2212 1\nNk uu\u22a4\n) , (2)\nwhere u = (1, . . . , 1)\u22a4. By setting U = \u221a Nk\u03a3\u0304, the reparameterization trick gives us a simple way to sample W (Ak) as\n(W (Ak)|W (A0) = x) = \u00b5\u0304+UZ = x Nk u+ 1\u221a Nk (Z \u2212 \u27e8Z\u27e9u), with Z \u223c (0, I), (3)\nwhere \u27e8Z\u27e9 is the mean of Z. Intuitively, in order to conditionally sample the white noise under a pixel of value x at level k, one can 1) unconditionally sample a discrete Nk = 2k \u00d7 2k Gaussian sample, 2) remove its mean from it, and 3) add the pixel value x (up to a scaling factor). The full derivation of Equations (2) and (3) and the corresponding python code are included in Appendix B."
        },
        {
            "heading": "2.2 TEMPORALLY-CORRELATED DISTRIBUTION-PRESERVING NOISE TRANSPORT",
            "text": "In this section, we first introduce our proposed noise transport equation, which offers a theoretical way of warping a continuously defined white noise while preserving its characteristics. Then, we present our practical implementation and show that it still retains many theoretical guarantees from the infinite setting. Lastly, we provide a simple 1-D example to support the analysis of how the proposed \u222b -noise balances between satisfying seemingly opposite objectives such as preserving the correlation imposed by interpolation and maintaining the original noise distribution.\nWhite noise transport. We first assume that the noise is transported with a diffeomorphic deformation field T : E \u2192 E. This mapping could be represented by an optical flow field between two frames, or a deformation field for image editing. Our goal is to transport a continuous white noise W with T in a distribution-preserving manner. The resulting noise T (W ) can be expressed as an Ito\u0302 integral through our noise transport equation for any subset A \u2286 E as\nT (W )(A) = \u222b x\u2208A\n1\n|\u2207T (T \u22121(x)) | 12 W (T \u22121(x)) dx, (4)\nwhere |\u2207T | is the determinant of the Jacobian of T . Intuitively, Equation (4) warps a non-empty set of the domain with the deformation field T \u22121, fetching values from the original white noise at the warped domain coordinates. The determinant of the Jacobian is necessary to rescale the samples according to the amount of local stretching that the deformation induces, while also accounting for the variance change required by the white noise definition. A detailed derivation of Equation (4) can be found in the Appendix C.2. In practice, optical flow maps can be non-diffeomorphic due to discontinuities and disocclusions. Appendix C.4 explains our treatment of these cases.\nDiscrete Warping. The noise transport equation (Equation (4)) cannot be solved in practice due the infinite nature of the white noise. Thus, we first compute the higher-resolution discrete \u222b -noise W (Ak), possibly from an a priori sample (Equation (3)). Since the pixel area can undergo a nonlinear warping deformation, we subdivide its contour into s smaller segments which are mapped backwards via the reverse deformation field. The warped segments define a polygonal shape that is then triangulated and rasterized over the high-resolution domain Ak. Lastly, the sub-pixels covered by the warped polygon are summed together and normalized, which yields the discrete noise transport for the noise pixel at position p\nG(p) = 1\u221a |\u2126p| \u2211 Aki \u2208\u2126p Wk(A k i ) , (5)\nwhere Wk = \u221a Nk \u00b7W is the white noise scaled to unit variance at level k, and \u2126p \u2286 Ak contains all subpixels at level k that are covered by the warped pixel polygon, with |\u2126p| representing the cardinality of the set. A detailed algorithm is outlined in Appendix C.4, and illustrated in Figure 2a. Note that the discrete implementation will still preserve independence between neighboring pixels in the warped result. This is because the warped polygons still form a partition of the space, so each sub-pixel in Ak will only belong to a single warped polygon. More details on the discretization of Equation (4) into Equation (5) can be found in Appendix C.3.\nToy example in 1-D. We will demonstrate the properties of our \u222b\n-noise sampling in a simpler one dimensional setting. Consider a 1-D set of i.i.d. random variables indexed by I = {0, . . . , n} with values represented by {x0, x1, . . . , xn} \u223c N (0, 1), and a mapping function that translates the discrete locations by a constant T \u221211D (i) = i\u2212 \u03b1, where i \u2208 I and \u03b1 \u2208 [0, 1]. Using a simple linear interpolation to compute the transported values zi yields zi = \u03b1xi\u22121 + (1\u2212 \u03b1)xi, zi \u223c N (0, \u03c32z), with \u03c32z = \u03b12 + (1\u2212 \u03b1)2. The equation above means that the variance of zi is a quadratic function of \u03b1 such that \u03c32z = 1 for \u03b1 \u2208 {0, 1} and \u03c32z < 1 for \u03b1 \u2208 (0, 1). This shows that the linear interpolation does not preserve the original distribution of the input variables.\nHowever, if we obtain xi\u22121 and xi from the integral over an underlying high-resolution white noise, the original distribution can be preserved. By employing the \u222b -noise, the pixel\u2019s value that is sampled between xi\u22121 and xi is no longer deterministic. Figure 2b shows the value of z for different higher-resolution samples. Mathematically, the value of z is now a Brownian bridge between neighboring x values. In Appendix C.5, we show that the value of z is a conditional probability distribution given by\nzi|xi, xi\u22121 \u223c N (\u00b5\u221e, \u03c32\u221e), with {\n\u00b5\u221e = \u03b1xi\u22121 + (1\u2212 \u03b1)xi \u03c32\u221e = 1\u2212 (\u03b12 + (1\u2212 \u03b1)2) = 1\u2212 \u03c32z\n(6)\nThus, our continuous noise warping can be interpreted as performing a stochastic process centered around the result of a linear interpolation. This stochastic component precisely compensates the diminished variance induced by the linear interpolation, resulting in a unit variance for any \u03b1. This is the intuition behind why our \u222b -noise warping method is able to preserve the distribution of the noise sample after warping.\nLong-term temporal coherency. The method outlined by previous sections only explains how to consistently warp a noise sample with a single deformation field. To apply this idea to a full sequence of frames, there are two possible solutions. The first one is to warp the noise frame by frame, such that the noise Gn of the n-th frame is computed as Gn = T(n\u22121)\u2192n(Gn\u22121). However, the higher-resolution noises computed at each frame are no longer coherent with each other in this case. A better solution is to use the accumulated deformation field to warp back to the first sample, where the high resolution representation is always the same, i.e., Gn = T0\u2192n(G0). This produces noises that are more coherent over long periods of time.\n3 EXPERIMENTS AND RESULTS 3.1 VALIDATING \u222b -NOISE PRIOR\nOur proposed noise prior simultaneously achieves two seemingly competing objectives: maximizing the correct correlation between the warped and the original sample, and maintaining the indepen-\ndence of pixels within each sample. These two aspects can be visualized using cross-covariance and covariance matrices of two noise samples G0 and G1 respectively. As shown in Figure 3, our\u222b\n-noise prior achieves the same noise cross-correlation as the bilinear interpolation, while concurrently maintaining the same level of independence between pixels in the second noise sample G1 as methods such as resampling (\u201drandom\u201d) or reusing the previous noise (\u201dfixed\u201d)."
        },
        {
            "heading": "3.2 APPLICATIONS IN DIFFUSION-BASED TASKS",
            "text": "We validate our noise prior on four tasks: realistic appearance transfer with SDEdit (Meng et al., 2022); video restoration and super-resolution with I2SB (Liu et al., 2023a); pose-to-person video generation (Bhunia et al., 2022), and fluid simulation super-resolution. While we show qualitative examples of these applications in this paper, we urge the reader to check out the supplementary videos on our project webpage for better visual comparisons.\nBaselines. In many of our applications, we lift models trained on images to work on videos. Without any particular treatment, the default Random Noise prior uses different and independent noise samples for each frame. Alternatively, many approaches employ a Fixed Noise prior to reduce flickering artifacts, which reuses a same fixed set of noise samples for all frames. We compare the proposed\u222b\n-noise mainly against these two baselines, but more comparisons can be found in Appendix D.\nPhotorealistic appearance transfer for videos. We lift the stroke-based editing capabilities of SDEdit (Meng et al., 2022) to the temporal domain. We showcase results on the LSUN Bedroom dataset (Yu et al., 2015) by replacing the stroke paintings with simple renders of a synthetic bedroom scene to ensure that the inputs are temporally coherent. SDEdit then corrupts these frames and denoise them into realistic images. Figure 1a shows that our \u222b -noise prior significantly improves the temporal coherency of the denoised sequence by avoiding unnatural sticking artifacts. While our method also works without it, we use cross-frame attention in this specific application to better showcase the difference between the different noise priors.\nVideo restoration and super-resolution. In I2SB, Liu et al. (2023a) use Schro\u0308dinger bridges to diffuse between two distributions without going through the normal Gaussian distribution. The authors show examples for JPEG compression restoration and 4\u00d7 image super-resolution. When using their model on a per-frame basis for videos, the resulting images suffer from artifacts due to the choice of noise. By leveraging the motion information in the corrupted sequence to warp the\nnoise, we can noticeably reduce these temporal aberrations, as shown in the x-t plots of Figure 4. Quantitative comparisons in Table 1 substantiate these observations.\nPose-to-person video. Person Image Diffusion Model (PIDM) (Bhunia et al., 2022) is a non-latent diffusion-based method that takes an image of a person and a pose, and generates an image of the same person with the input pose. When applying the model in a frame-by-frame fashion on a sequence of poses, it either creates a substantial amount of unnatural flickering when using the Random Noise prior, or produces obvious cloth texture-sticking artifacts when combined with fixed noise (see blue squares in Figure 1b). We estimate a rough motion of the entire body from the pose sequence and show that this information can be utilized with our noise warping method to alleviate the texture sticking issue. Figure 1b shows how \u222b -noise translates the cloth textures along with the person\u2019s movements more naturally. Interestingly, this does not fully agree with the quantitative evaluation, and we further analyse this discrepancy in Appendix D.3.\nFluid simulation super-resolution. Finally, we apply our \u222b\n-noise prior to a challenging task of fluid super-resolution. Fluid simulations contains large motions and deformations which makes artifacts from standard noise priors much more visible. Large deformations also means that simpler warping methods like bilinear interpolation will fail miserably. We train a non-latent unconditional diffusion model from scratch on a 2D fluid simulation dataset generated with the fluid solver from Tang et al. (2021). The x-t plots in Figure 5 show that warping the noise along with the fluid density creates smoother transitions between frames.\nQuantitative evaluation. We quantitatively evaluate our noise prior against the baselines presented above, as well as some traditional interpolation methods in Table (1). We further add the noise priors from Ge et al. (2023) and Chen et al. (2023) to our evaluation. Following Geyer et al. (2023); Ceylan et al. (2023); Lai et al. (2018), we use the warping error as a metric for temporal coherency. Realism and visual quality of the results are assessed with FID Heusel et al. (2017), Improved Precision Kynka\u0308a\u0308nniemi et al. (2019) and/or LPIPS Zhang et al. (2018) whenever it makes sense. Additionally, we report the runtime of each method in Appendix C.7. Our method is computationally less efficient than the baselines in Table (1), but remains comparable to DDIM inversion."
        },
        {
            "heading": "4 ABLATION STUDIES AND DISCUSSIONS",
            "text": "Standard interpolation vs \u222b\n-noise. Figure 6 shows that bicubic and bilinear interpolation schemes for noise warping lead to blurry results, because the warped noise has less variance (histogram insets). While nearest neighbor interpolation preserves the distribution to a certain extent, it contains duplicating artifacts in regions of stretching (noise inset), which induces self-correlation between pixels. Our proposed method is able to retain high-frequency details in the result, and ensures complete independence between the pixels of the noise samples.\nUpsampling level \ud835\udc58\nRa tio\no f u\nnd ef\nin ed\np ix\nel s\n(% )The noise upsampling factor k. The main parameter of our noise warping algorithm is the choice of the upsample factor k. A smaller factor k means the warped polygons are rasterized on larger subpixels, increasing the chances that the polygon covers none of the sub-pixels. In this case, the resulting pixel value after warping is undefined. We test our method on the fluid example, where large deformations are present. The inset plot shows the ratio of undefined pixels in the warped noise for different values of k and different frames. The further the frame, the more undefined pixels appear due to the extreme warping the pixel polygons go through. Overall, sub-sampling the noise to level k = 1 already largely reduces the amount of unrasterized pixel polygons. We found setting k = 3 is sufficient for most cases. For very long sequences, we devise a resampling strategy detailed in Appendix C.4.\nNoise warping in latent diffusion models. Our experiments have shown that noise warping has limited impact on temporal coherency in latent diffusion models (LDM) (Rombach et al., 2022). This is because the noise used in latent diffusion models has a lower resolution, controlling mostly the compositional aspects and low frequency structures of the generated image. This not only limits the amount of motion that can be transferred to it but also offloads the responsibility of generating the final high-frequency details to the decoder part of the image autoencoder. In Appendix E, we provide an extensive set of tests that highlight the impact of using the warped noise in LDMs."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) represent the state the art for generative models, surpassing previous techniques in terms of sampling quality and mode coverage (Dhariwal & Nichol, 2021; Xiao et al., 2022). Diffusion models train a neural network to reverse a diffusion process, representing the data manifold through a Gaussian distribution. The set of sampled noises, along with integrator parameters, can be interpreted as the latent space of generative diffusion models (Huberman-Spiegelglas et al., 2023). Since their original conception, diffusion models have been employed in a plethora of tasks such as text-to-image (Rombach et al., 2022; Nichol et al., 2022; Ramesh et al., 2022; Balaji et al., 2022); text-to-video (Ho et al., 2022a;b; Singer et al., 2022; Villegas et al., 2022), image-to-image Parmar et al. (2023) and image-to-video Ni et al. (2023) generation; image (Meng et al., 2022; Valevski et al., 2022) and video (Yang et al., 2023; Chen et al., 2023; Ceylan et al., 2023) editing; and image (Saharia et al., 2022; Lugmayr et al., 2022; Chung et al., 2022) and video inpainting Liew et al. (2023).\nDiffusion-based image editing through noise inversion. It is common practice for image editing methods (Tumanyan et al., 2022; Ceylan et al., 2023; Mokady et al., 2022; Parmar et al., 2023;\nHuberman-Spiegelglas et al., 2023; Wu & De la Torre, 2022; Yang et al., 2023; Geyer et al., 2023) to invert the denoising process, recovering the noise that reconstructs an image under a textual condition. The inverted noise partially encodes the image structural composition, and through its recombination with a new textual prompt during the denoising process, edited images can be efficiently generated. This inversion process can either reconstruct a single (DDIM) (Hertz et al., 2022; Mokady et al., 2022; Song et al., 2021) or multiple (DDPM) (Huberman-Spiegelglas et al., 2023; Wu & De la Torre, 2022) noise samples per image. Inverting the noise can be inaccurate and exact inversion can be obtained by coupled transformations (Wallace et al., 2022) or by bi-direction integration approximation (Zhang et al., 2023). To further increase spatial fidelity to the original image when implementing prompt-to-prompt edits, some approaches advocate for injecting crossframe attention (Hertz et al., 2022; Parmar et al., 2023) and features maps (Tumanyan et al., 2022) from the source image during the denoising process. Noise inversion techniques were also used to perform image-to-image style transfer (Zhang et al., 2022; Ruta et al., 2023), combined with a noise regularization technique (Parmar et al., 2023) and improved by pivotal tuning and null-text optimization (Mokady et al., 2022).\nDiffusion-based video generation and editing. Similar to image editing, many video editing applications rely on a text-to-image models to invert the noise for each individual frame. Temporal coherency is a central challenge for these applications. Injecting cross-attention features (Ceylan et al., 2023; Geyer et al., 2023; Yang et al., 2023; Khachatryan et al., 2023) from an anchor frame is a possible solution to ensure consistent edits. Recent techniques rely on modifying the noise or the latent space of diffusion models to achieve better temporal coherency: Ni et al. (2023) warp the latent features from a diffusion model of a image-to-video pipeline; Blattmann et al. (2023) align the latent codes of a pre-trained image diffusion model during fine-tuning; Khachatryan et al. (2023) apply pre-specified translation vectors to the inverted noise from an anchor frame; Ge et al. (2023) construct correlated noise samples to train a neural network for generating videos from text; Chen et al. (2023) use a residual noise sampling that spatially preserve noise samples representing image pixels not moving in the video sequence. Alternatively, Layered Neural Atlases (Kasten et al., 2021) decomposes the video into a canonical editing space (Couairon et al., 2023; Chai et al., 2023).\nImage and video editing through fine-tuning. An alternative approach to deal with issues arising from image and video editing is to fine-tune neural networks on top of diffusion models. Control-net (Zhang & Agrawala, 2023) learns task-specific conditions (depth and normal maps, edges, poses) that are useful for preserving structures while editing images. For editing videos, several approaches fine-tune neural network with temporal-attention modules (Liew et al., 2023; Liu et al., 2023b; Shin et al., 2023; Zhao et al., 2023; Wu et al., 2022) to ensure temporally coherent outputs. Alternatively, embedding (Chu et al., 2023) or disentangling (Guo et al., 2023) motion and the content from videos is also an effective approach to control consistency."
        },
        {
            "heading": "6 LIMITATIONS & CONCLUSION",
            "text": "In this paper, we proposed a novel temporally-correlated noise prior. Our \u222b\n-noise representation reinterprets noise samples as the integral of a higher resolution white noise field, allowing variancepreserving sampling operations that are able to continuously interpolate a noise field. Moreover, we derived a novel noise transport equation, which accounts not only for the deformation of the underlying pixel shapes but also for the necessary variance rescaling. The proposed method creates temporally-correlated and distribution-preserving noise samples that are useful for a variety of tasks.\nOur method comes with some limitations. While more accurate, the proposed noise warping algorithm is computationally more inefficient than simpler techniques (Ge et al., 2023; Chen et al., 2023) for correlation preservation. Moreover, the underlying assumption we made is that a more temporally-correlated noise prior would result in better temporal coherency in video diffusion tasks. This assumption is not always guaranteed. The degree to which this holds depends on the training data and the pipeline of the diffusion model: while methods such as SDEdit give enough freedom to the noise for it to have a visible impact on the denoised result, more constrained pipelines can be much more oblivious to the choice of the noise prior. For future work, we believe that our noise warping algorithm can be further used to train neural networks that generate video inputs. Extending the noise prior to latent diffusion models can be another interesting direction. And that, dear reader, is how we warped your noise."
        },
        {
            "heading": "7 REPRODUCIBILITY STATEMENT",
            "text": "1. For applications using existing methods, we use the checkpoints provided by the authors of the methods.\n2. Proofs are provided in Appendix B and C. 3. Pseudocode are provided in Appendix B.3 and C.4. 4. Extra experimental results are provided in Appendix D. 5. Detailed analysis of our method in latent diffusion models is provided in Appendix E."
        },
        {
            "heading": "A LIST OF SYMBOLS",
            "text": ""
        },
        {
            "heading": "B CONDITIONAL WHITE NOISE SAMPLING",
            "text": "An important step in our pipeline is to lift a given initial discrete noise sample to a continuously defined white noise function. In Section 2.1, we introduced a method for conditionally sampling a white noise at any finer resolution given a discrete sample. In this section we outline additional important white noise properties; for a more rigorous white noise definition please refer to (Walsh, 2006; Dalang & Mountford, 1999). We also include a proof of Equation 2 alongside with the pseudocode for conditional sampling."
        },
        {
            "heading": "B.1 WHITE NOISE",
            "text": "Let (\u2126,F , P ) be a complete probability space and let (E, E , \u03bd) be a \u03c3-finite measure space. White noise on E based on \u03bd is a random set function\nA 7\u2192W (A), (7)\ndefined for A \u2208 E with \u03bd(A) <\u221e, with values in L2(\u2126,F , P ), such that:\n1. W (A) is a Gaussian random variable with mean 0 and variance \u03bd(A);\n2. if A and B are disjoint, then W (A) and W (B) are independent and\nW (A \u222aB) = W (A) +W (B).\nIn our setting, we usually consider the continuous white noise to be defined on E \u2286 R2. A useful fact about the covariance of W (A) and W (B) is that it can be computed from the definition:\nE(W (A)W (B)) = E((W (A\\B) +W (A \u2229B))(W (B\\A) +W (B \u2229A))) = E ( W (A \u2229B)2 ) = \u03bd(A \u2229B).\n(8)"
        },
        {
            "heading": "B.2 CONDITIONAL PROBABILITY DERIVATION",
            "text": "Setup. To simplify the derivation and without loss of generality, let us consider a single discrete pixel associated with a Gaussian multivariable X \u223c N (01, I1), where 01 and I1 represent the zero and identity square matrices of dimensionality 1. In our noise formulation, the pixel does not represent a point, but it rather defines a constant value in a unit square A = [0, 1]2. Our goal is to subdivide the pixel area A into N\u00d7N sub-pixels, and sample them conditionally given an a priori sample X = x. To do so, we start by partitioning A into N2 sub-patches {Bk,l = [k\u22121N , k N ]\u00d7 [ l\u22121 N , l N ]}(k,l)\u2208[1,N ]2 , such that \u03bd(Bk,l) = 1\nN2 , W (Bk,l) \u223c N (0,\n1\nN2 ), \u2200 k, l \u2208 [1, . . . , N ]2. (9)\nProof of Equation (2). Let Y = (W (B1,1), ...,W (BN,N )) \u22a4 be the random vector associated with all the sub-patches. Y is a multivariate Gaussian random vector thanks to the independence of {W (Bk,l)}k,l, and we have Y \u223c N (\u00b5Y = 0N2 ,\u03a3Y = 1N2 IN2). Now, consider the vector Z = (Y,X)\u22a4. From Equation (1), we have that X := W (A) = W (\u22c3 k,l Bk,l ) = \u2211\nk,l W (Bk,l), i.e. X is a linear combination of Y. As a result, any linear combination of elements in Z is also a linear combination of elements in Y, thus a Gaussian variable. Therefore, Z is a multivariate Gaussian variable, and:\nZ \u223c (\u00b5,\u03a3), with \u00b5 = [ \u00b5Y \u00b5X ] , \u03a3 = [ C(Y,Y) C(Y,X) C(X,Y) C(X,X) ] , (10)\nwhere C constructs the covariance matrix for two given variables. Given the white noise definitions in Appendix B.1, the covariance between two individual Gaussian random variables defined on A (pixel) and Bk,l (sub-pixel) is their intersected area \u03bd(Bk,l \u2229 A) = \u03bd(Bk,l) = 1N2 . This yields the following covariance matrices:\nC(Y,Y) = 1\nN2 IN2 , C(Y,X) =\n1\nN2 u, C(X,Y) = C(Y,X)\n\u22a4, C(X,X) = I1, (11)\nwhere u = (1, ..., 1)\u22a4 \u2208 RN2 . Since Z is a Gaussian vector, the conditional probability of Y|X = x is also a Gaussian, and its sampling is given by (Holt & Nguyen, 2023)\nY|X \u223c N ( \u00b5(Y|X),\u03a3(Y|X) ) , where\n{ \u00b5(Y|X) = \u00b5Y +C(Y,X) C(X,X)\n\u22121 (X\u2212 \u00b5X), \u03a3(Y|X) = C(Y,Y) \u2212C(Y,X) (C(X,X))\u22121 C(X,Y).\n(12)\nThe terms above further simplify using Equation (11) as\n\u00b5(Y|X) = \u00b5Y +C(Y,X) (C(X,X)) \u22121 (X\u2212 \u00b5X) = 0N2 +\n1\nN2 u(X\u2212 01) =\nx\nN2 u,\n\u03a3(Y|X) = C(Y,Y) \u2212C(Y,X) C(X,Y) = 1\nN2 IN2 \u2212\n1\nN2 u(\n1\nN2 u)\u22a4 =\n1\nN2 IN2 \u2212\n1\nN4 uu\u22a4.\n(13)\nThe last missing piece to derive Equation (2) comes by setting UU\u22a4 = \u03a3(Y|X), with U = 1 N ( IN2 \u2212 1N2uu \u22a4). These definitions can be verified using the following identity: (uu\u22a4)2 = uu\u22a4uu\u22a4 = (u\u22a4u)uu\u22a4 = N2uu\u22a4. (14)"
        },
        {
            "heading": "B.3 A PRIORI SAMPLING IN PYTHON",
            "text": "Unit variance scaling. Note that in the previous derivation, the upsampled noise represented by the new pixels (Bk,l) has a variance of 1/N2. Sometimes, it can be more practical to generate higherresolution discrete noise with unit variance. This is easily fixed by multiplying the Equation (3) by N , resulting in the upsampling function\nupsample\u221e(x,N) := x\nN u+ (Z \u2212 \u27e8Z\u27e9u), Z \u223c (0N2 , IN2), \u27e8Z\u27e9 =\n1\nN2 u\u22a4Z (15)\nAlgorithm (1) illustrates in Python a procedure that upsamples a H \u00d7W noise sample by a factor N scaling it to unit variance.\nAlgorithm 1 Conditional White Noise Sampling\ndef upsample_noise(X, N): b, c, h, w = X.shape Z = torch.randn(b, c, N*h, N*w) Z_mean = Z.unfold(2, N, N).unfold(3, N, N).mean((4, 5)) Z_mean = F.interpolate(Z_mean, scale_factor=N, mode='nearest') X = F.interpolate(X, scale_factor=N, mode='nearest') return X / N + Z - Z_mean"
        },
        {
            "heading": "C DISTRIBUTION-PRESERVING NOISE WARPING",
            "text": "The distribution-preserving noise warping method proposed in Section 2.2 involves warping the continuous white noise with a diffeomorphic deformation field. In this section we mathematically derive the noise transport equation (Equation 4)."
        },
        {
            "heading": "C.1 BROWNIAN MOTION AND ITO\u0302 INTEGRAL",
            "text": "White noise and Brownian motion. An alternative definition of white noise {W (x)}x\u2208E is through the distributional derivative of a Brownian motion {B(x)}x\u2208E (also called Brownian sheet for dimension \u2265 2) as W (x)dx = dB(x). Ito\u0302 integral. As the Ito\u0302 integral \u222b A \u03d5(x)dB(x) of a deterministic function \u03d5 in L2 is always Gaussian. The variance is given by:\nE (\u222b \u03d5(x)dB(x) )2 = \u222b \u03d5(x)2dx = \u2225\u03d5\u222522. (16)\nFrom Equation (16) we can relate back to the first definition of white noise by setting \u03d5 = 1. Indeed,\nW (A) = \u222b x\u2208A 1dB(x) (17)\nis a Gaussian variable of variance \u222b x\u2208A \u03d5(x) 2dx = \u222b x\u2208A 1dx = \u03bd(A)."
        },
        {
            "heading": "C.2 NOISE TRANSPORT EQUATION DERIVATION",
            "text": "Warping. The differential form of the transport equation for pointwise pixel values \u03c1(x) of an arbitrary image is given by\n\u2202\u03c1(x)\n\u2202t = \u2212\u2207 \u00b7 (\u03c1(x)v(x)) , (18)\nwhere v(x) is a velocity field. The mapping T is defined by integrating the set of positions x with the velocity field v(x). More specifically, T : E \u2192 E is a diffeomorphism from our measure space to itself: T is differentiable, structure-preserving, and its inverse T \u22121 is defined for all x. The transport equation can be solved by the method of characteristics mapping (Nabizadeh et al., 2022) to generate the warped field \u03c1\u0303(x) by\n\u03c1\u0303(x) = \u03c1(T \u22121(x)). (19)\nHowever, this method only considers pointwise pixel values, whereas our definition of white noise requires the transport in continuously defined pixel areas. In 2-D this amounts to regularizing pointwise values by the area that they represent, while simultaneously tracking the distortions resulting from the mapping application. This notion is also common in exterior calculus: 0-forms (points) to\n2-forms (faces) are connected through the dual operator (Crane, 2013). We employ a simpler finite volume method to represent \u03c1(\u2206x) as a cell-centered area-averaged integration\n\u03c1(\u2206x) = 1\n\u03bd(\u2206x) \u222b x\u2208\u2206x \u03c1(x) dx. (20)\nNote that the resulting \u03c1(\u2206x) can still be considered a pointwise quantity, since it is the integrated value over a pre-specified area regularized by the area size. Applying Equation (20) to T \u22121(\u2206x), the warped cell-centered area quantity \u03c1\u0303(\u2206x) is\n\u03c1\u0303(\u2206x) = \u03c1(T \u22121(\u2206x)) = 1 \u03bd(T \u22121(\u2206x)) \u222b x\u2208T \u22121(\u2206x) \u03c1(x) dx. (21)\nIf ones naively replaces \u03c1 by a white noise distribution W , the warped noise W\u0303 follows\nW\u0303 (\u2206x) = 1\n\u03bd(T \u22121(\u2206x)) \u222b x\u2208T \u22121(\u2206x) W (x) dx. (22)\nW\u0303 (\u2206x) is a Gaussian random variable, since it is an Ito\u0302 integral. Its variance is given by evaluating Equation (16) as\n\u03c32 ( W\u0303 (\u2206x) ) =\n1\n\u03bd(T \u22121(\u2206x))2 \u222b x\u2208T \u22121(\u2206x) 12 dx = 1 \u03bd(T \u22121(\u2206x)) . (23)\nThe variance of the integrated white noise above is not correct, as it should be proportional to the integrated area, in accordance with the definitions in Appendix B.1. This shows that a continuous Gaussian white noise loses its distribution when we transport it with standard transport equations. In Section 2.2, we introduced an alternative to the standard transport equations which accounts for variance preservation. We referred to it as the noise transport equation\nW\u0303 (A) = \u222b x\u2208A\n1\n|\u2207T (T \u22121(x)) | 12 W (T \u22121(x)) dx, (24)\ndefined for any A \u2286 E. This equation correctly accounts for a variance rescaling term, which ensures that the integrated white noise is in conformity with its defining properties. We prove that Equation (24) is the correct formulation that produces a continuously defined noise representation that further satisfies\n1. W\u0303 is a white noise as defined in Section B.1;\n2. W\u0303 is a warping of W by T , i.e. it satisfies a relation similar to Equation (19). Specifically,\nW\u0303 (\u2206x)\n\u03bd(\u2206x) 1 2 = W (T \u22121(\u2206x)) \u03bd(T \u22121(\u2206x)) 12\n(25)\nProof 1. To show that W\u0303 is indeed a white noise, we first show that for A s.t. \u03bd(A) < \u221e, we have W\u0303 (A) \u223c N (0, \u03bd(A)). We proceed to a change of variable y = T \u22121(x), i.e. |\u2207T (y)|dy = dx. Thus,\nW\u0303 (A) = \u222b y\u2208T \u22121(A)\n1\n|\u2207T (y)| 12 W (y)|\u2207T (y)|dy\n= \u222b y\u2208T \u22121(A) |\u2207T (y)| 12W (y)dy\n= \u222b y\u2208T \u22121(A) |\u2207T (y)| 12 dB(y).\n(26)\nAgain, this Ito\u0302 integral is a Gaussian variable and its variance is given by:\n\u03c32 = \u222b y\u2208T \u22121(A) ( |\u2207T (y)| 12 )2 dy\n= \u222b y\u2208T \u22121(A) |\u2207T (y)| dy\n= \u222b x\u2208A dx = \u03bd(A),\n(27)\nwhere we used the change of variable the other way round. It is not difficult to verify that W\u0303 also satisfies the second condition on disjoint sets, thanks to the linearity of the integral operator and the bijectivity of T . Proof 2. Next, we want to prove that this new noise corresponds to our intuitive definition of a warped noise. For this, let x \u2208 E and consider \u2206x \u2208 E a tiny subset around x. By setting A = T (\u2206x) in the definition of W\u0303 , we have:\nW\u0303 (T (\u2206x)) = \u222b y\u2208T (\u2206x)\n1\n|\u2207T (T \u22121(y)) | 12 W (T \u22121(y))dy (28)\nBy the same change of variable as before we get: W\u0303 (T (\u2206x)) = \u222b y\u2208\u2206x |\u2207T (y) | 12W (y)dy. (29)\nBy linearization, we assume that the determinant of the warping field\u2019s Jacobian is constant over \u2206x, replacing it by its mean yields\nW\u0303 (T (\u2206x)) \u2243 \u222b y\u2208\u2206x ( 1 \u03bd(\u2206x) \u222b u\u2208\u2206x |\u2207T (u) |du ) 1 2 W (y)dy\n=\n( 1\n\u03bd(\u2206x) \u222b u\u2208\u2206x |\u2207T (u) |du ) 1 2 (\u222b y\u2208\u2206x W (y)dy )\n=\n( 1\n\u03bd(\u2206x) \u222b v\u2208T (\u2206x) dv ) 1 2 (\u222b y\u2208\u2206x dB(y) )\n= ( \u03bd(T (\u2206x)) \u03bd(\u2206x) ) 1 2 W (\u2206x),\n(30)\nwhich concludes the proof."
        },
        {
            "heading": "C.3 FROM CONTINUOUS TO DISCRETE FORMULATION",
            "text": "In practice, it is infeasible to warp a continuous noise field with the noise transport equation. We will demonstrate that Equation (4) becomes Equation (5) once discretized. Our goal is to compute the pixel value G(p) in the warped noise for the given pixel coordinates p = (i, j). Using our\u222b\n-noise interpretation, G(p) corresponds to the integral of the underlying warped white noise, i.e. G(p) = T (W )(A), where A = [i\u2212 1, i]\u00d7 [j \u2212 1, j] is the subset of the domain E covered by the pixel at position p. Thus, substituting G(p) in Equation (4):\nG(p) = \u222b x\u2208A\n1\n|\u2207T (T \u22121(x)) | 12 W (T \u22121(x)) dx (31)\nAssuming that the warping field has a locally constant Jacobian over the entire pixel area around p, we can apply the approximation from Equation (25) with \u2206x = A:\nG(p) \u2243 ( \u03bd(A)\n\u03bd(T \u22121(A))\n) 1 2\nW (T \u22121(A))\n= 1\n\u03bd(T \u22121(A)) 12 W (T \u22121(A)) since \u03bd(A) = 1 by definition of A.\n(32)\nThe second approximation comes from rasterization. We approximate the warped pixel shape by its rasterized version at level k. This is formally written as\nT \u22121(A) \u2243 \u22c3\nAki \u2208\u2126p\nAki , (33)\nwhere \u2126p \u2286 Ak contains all subpixels at level k that are covered by the warped pixel polygon T \u22121(A) after rasterization. Substituting this into G(p) and using linearity of \u03bd and W , we get\nG(p) \u2243 1 \u03bd (\u22c3\n\u2126p Aki\n) 1 2 W \u22c3 \u2126p Aki  =\n1(\u2211 Aki \u2208\u2126p \u03bd(Aki ) ) 1 2 \u2211 Aki \u2208\u2126p W (Aki )\n= \u221a Nk |\u2126p| \u2211 Aki \u2208\u2126p W (Aki ) since \u03bd(A k i ) = 1/Nk, = 1\u221a |\u2126p| \u2211 Aki \u2208\u2126p Wk(A k i ) since Wk(A k i ) = \u221a NkW (A k i ),\n(34)\nwhich concludes the derivation.\nIt is easy to verify that the resulting G(p) remains a standard normal Gaussian variable, as it is the \u201cGaussian average\u201d of multiple standard normal variables. Furthermore, the independence between different pixels G(p) and G(q) is ensured with a proper implementation of rasterization, i.e. if rasterization of adjacent triangles are disjoint.\nLastly, note that all the approximations we made to go from the continuous setting to the discrete one only affect the accuracy of the deformation field (locally constant Jacobian, approximate warped region), but never the actual values of the noise samples. This is key to the distribution-preserving properties of our method.\nC.4 IMPLEMENTATION We include the pseudo-code of our \u222b\n-noise warping for a single pixel in Algorithm (2), and provide more details on the algorithm itself regarding specific aspects such as warping and the treatment of disocclusions.\nWarping. To warp the triangulation points V for the pixel contour, we follow the common practice in physics simulations. As the deformation map T is only defined at pixel centers, we use bicubic interpolation to obtain the deformation map values at the sub-pixel triangulation points.\nHandling disocclusions. When employing optical flow maps as the deformation mapping T between the anchor and current frame, some pixels can end up with an undefined value. There are two possible causes to this issue. The first one is that the pixel polygon was stretched in a way that it ended up covering zero sub-pixels after rasterization. This can happen when the deformation is large, or the sub-pixels are not fine enough (i.e. k is too low). The second one is that two pixel polygons are rasterized one on top of the other. As we only keep the values of the last one, no sub-pixel\nAlgorithm 2 Distribution-preserving noise warping (for a single pixel)\nInput: G: discrete noise at anchor frame (in size D \u00d7D) A: pixel area in current frame T : deformation mapping between the anchor and current frame k: noise upsampling factor s: polygon subdivision steps Output: pixel value x in current frame\nG\u221e \u2190 UPSAMPLE\u221e(G, k) (V, F )\u2190 TRIANGULATE AREA(A, s) V \u2190 WARP\u221e(V, T ) \u2126\u2190 RASTERIZE((V, F ), G\u221e) x\u2190 \u2211 (i,j)\u2208\u2126 G\u221e(i, j)/ \u221a SIZE(\u2126)\nis associated with the first polygon. This typically happens if there are disocclusions and the flow map is not diffeomorphic.\nTo solve this in practice, we use a two-stage process. First, we warp the noise from the initial frame and fill in the current frame\u2019s pixels. At this stage, some pixels might be undefined. At the second stage, we perform another warping, this time from the previous frame (up-sampled on the fly), and use it to fill the remaining missing values. If there are still missing values left, we replace them by randomly sampled noise. This allows the best temporal consistency for most of the pixels.\nC.5 1-D TOY EXAMPLE: PROOF In Section 2.2, we gave an intuition on why our \u222b\n-noise is able to preserve the distribution using a 1-D toy example. Here we aim at proving Equation 6 in a similar way of proving the conditional white noise sampling presented in Appendix B.2.\nFirst, let us consider a continuously defined noise signal W over the 1-D segment E = [0, 2]. We further define two independent random variables x0 = W ([0, 1]),x1 = W ([1, 2]) representing two neighboring \u201cpixels\u201d. A random variable z interpolating linearly between x0 and x1 in space can be defined as z = W ([\u03b1, 1 + \u03b1]), with \u03b1 \u2208 [0, 1]. Then, all three variables have a variance of 1 and z is correlated with x0 and x1 through\nCov(z,x0) = \u03bd([\u03b1, 1 + \u03b1] \u2229 [0, 1]) = \u03bd([\u03b1, 1]) = 1\u2212 \u03b1, Cov(z,x1) = \u03bd([\u03b1, 1 + \u03b1] \u2229 [1, 2]) = \u03bd([1, 1 + \u03b1]) = \u03b1.\n(35)\nThus, merging the variables into a vector Y = (z,x0,x1)\u22a4 yields\nY \u223c (\u00b5,\u03a3), with \u00b5 = 0, \u03a3 =  1 1\u2212 \u03b1 \u03b11\u2212 \u03b1 1 0 \u03b1 0 1  . (36) Applying the conditional probability rule for a Gaussian vector (as in Appendix B.2) gives:\nz|x0 = x0,x1 = x1 \u223c N ( \u00b5\u0304, \u03c3\u03042 ) , where { \u00b5\u0304 = (1\u2212 \u03b1, \u03b1) \u00b7 (x0, x1)\u22a4, \u03c3\u03042 = 1\u2212 (1\u2212 \u03b1, \u03b1) \u00b7 (1\u2212 \u03b1, \u03b1)\u22a4. (37)\nSimplifying the expressions yields\n\u00b5\u0304 = (1\u2212 \u03b1)x0 + \u03b1x1, and \u03c32 = 1\u2212 (\u03b12 + (1\u2212 \u03b1)2), (38)\nwhich concludes the proof."
        },
        {
            "heading": "C.6 COMPARISONS AGAINST OTHER WARPING APPROACHES",
            "text": "Interpolation baselines. Our method enables accurate Gaussian noise warping, even when the noise undergoes substantial deformations. Given a two dimensional warping field extracted from\nthe fluid simulation dataset (Figure 5), Figure 7 shows a comparative study between various warping methods. Three classical types of interpolation were tested: bilinear, bicubic and nearest neighbor. While bilinear and bicubic interpolations compromise high-frequency details, nearest neighbor creates artifacts in the form of duplicate samples, since it is oblivious to space deformations introduced by the warping field. We also experimented with a simple modification of the bilinear interpolation that is able to preserve the variance of the pixels by replacing the weighting coefficients with their square roots. Though being efficient, this approach still introduces spatial correlations that also eventually lead to artifacts. Our noise method outperforms all existing warping methods by transporting the noise perfectly while keeping its Gaussian properties.\nNoise prior baselines. We also compared the proposed \u222b\n-noise warping against previous noise priors such as the residual-based noise sampling of Control-A-Video (Chen et al., 2023), and the mixed and progressive noise model from PYoCo (Ge et al., 2023). The residual-based sampling keeps the noise signal fixed for image pixels that remain unchanged in a video, while the noise is resampled in locations in which temporal variations of RGB values go over a predefined threshold. PYoCo introduces two sampling schemes: mixed and progressive. Mixed noise sampling linearly combines a frame-dependent noise sample and a noise shared across all frames. Progressive noise sampling generates a sequence of noise samples in an autoregressive fashion: the noise at the current frame is generated by perturbing the noise from the previous frame. These approaches are not able to preserve the exact spatial correlations of the original sequence, since they are not based on warping. We show a comparison between different warping methods for the task of super-resolution\ns = 1 s = 2 s = 3 s = 4\nk = 0 21.6 21.7 23.0 26.2 k = 1 23.6 23.5 23.8 25.1 k = 2 30.5 29.3 29.6 30.8 k = 3 58.8 55.4 55.0 53.7 k = 4 143.8 137.5 132.4 128.2\ns = 1 s = 2 s = 3 s = 4\nk = 0 10.5 10.6 11.3 12.9 k = 1 10.5 10.6 10.8 11.3 k = 2 11.2 11.3 11.5 11.9 k = 3 15.5 15.3 15.5 15.6 k = 4 29.1 29.2 28.6 28.7\nTable 4: CPU time (top) and wall time (bottom) of \u222b -noise computation for different k and s parameters. The measurements are for a video sequence of 24 frames at resolution 256\u00d7 256, in seconds.\nin Figures 14 and 15. We refer to our supplementary website for additional comparisons between different warping techniques."
        },
        {
            "heading": "C.7 RUNTIME COMPARISONS",
            "text": "We estimate both wall time and CPU time for all methods on a video sequence at resolution 256\u00d7256 in Table 3. By evaluating the run time for different sub-sequence lengths, we can linearly regress the average per frame computation time of each method. This is evaluated on a GeForce RTX 3090 GPU and an Intel i9-12900K CPU.\nSimple noise sampling methods like PYoCo Ge et al. (2023), fixed or random noise can be executed efficiently on GPU. Methods that rely on information from the input sequence in a simple way such as Control-A-Video Chen et al. (2023) or the interpolation methods are 3 orders of magnitude slower than the simple noise sampling methods. Finally, our warping method requires \u2248 1 second per frame to warp the noise. While being considerably less efficient than aforementioned methods, our approach is comparable to DDIM inversion.\nAdditionally, our method trades off the noise warp accuracy with efficiency through two parameters: k, the noise upsampling factor, and s, the polygon subdivision steps. We report the CPU and wall clock time in seconds for noise warping in a video sequence of 24 frames at resolution 256 \u00d7 256 for different k and s in Table 4."
        },
        {
            "heading": "D ADDITIONAL RESULTS",
            "text": "In this section, we provide more results and evaluations for the applications shown in Section 3."
        },
        {
            "heading": "D.1 PHOTOREALISTIC APPEARANCE TRANSFER WITH SDEDIT",
            "text": "Figure 8 shows different variations of photorealistic appearance transfer from the same synthetic video sequence (top row). Contrary to DDIM inversion, our temporally-correlated warping scheme can be applied to arbitrary noise samples, allowing us to generate different edits from the same input sequence. In Figure 10, we compare our \u222b -noise warping against a fixed noise sample on a close-up scene. When using a fixed noise sample, the original fold in the corner of the bed stays at the same position in the image even after the bed rotates; our method, on the other hand, successfully transports the fold, following the bed movement. Other comparisons for the bedroom example are shown in Figure 9 and 12. In Table 5, we quantitatively evaluate our method on different bedroom scenes. Our noise warping method not only outperforms standard Gaussian noise priors in terms of temporal\ncoherency, it also manages to beat standard interpolation methods. The reason for this is visualized in Figure 11: standard interpolation methods are unable to account for space deformations, which leads to artifacts in the denoised results.\nWe evaluate the visual quality of the generated frames in Table 6 using FID (Heusel et al., 2017) and Improved Precision (Kynka\u0308a\u0308nniemi et al., 2019). FID (Heusel et al., 2017) is computed between the LSUN Bedroom dataset and the set containing all images from a given video example. It is important to note that FID simultaneously measures sample quality and diversity, while we are only interested in quality. Since we are computing FID over a sequence of frames from the same video, the diversity is naturally very low, which explains the overall high values we have in Table 6 (typical FID values are less than 50). Additionally, less temporally coherent methods like Random Noise will likely have more variations between frames, which improves diversity. This explains why random noise performs the best in terms of FID, and our method typically performs slightly worse. All in all, the main take-away from the FID comparison is that our method performs in the same range as other Gaussian noise priors (random, fixed, PYoCo, Control-A-Video) and much better than the other interpolation methods. Improved Precision (Kynka\u0308a\u0308nniemi et al., 2019) is a metric of sample quality only, as it measures the likelihood of the generated samples to belong to the ground truth dataset manifold. As the table shows, our method performs on par with other Gaussian noise priors in terms of visual quality.\nWe additionally compared our noise prior with DDIM inversion in Figure 13. As DDIM cannot be directly used with DDPM-based methods like SDEdit, we devise two reasonable ways of comparing\n-noise (top), fixed noise (center), and random noise (bottom) for a different\nbedroom example.\nwith DDIM noise in photorealistic appearance transfer. In the first experiment (Figure 13, top row), we apply DDIM inversion to intermediate step (60% of total steps) and then denoise it using forward DDIM. As expected, since neither the prompt or settings are changed, this approach mostly reconstructs the original synthetic video without adding any realistic appearance details. In the second experiment, DDIM inversion is employed to obtain the initial noise (Figure 13, middle row), we then add it to input frames and denoise using forward DDIM. Because the input video is far from the data distribution of the model (trained on realistic images of bedrooms), the DDIM-inverted noise is far from Gaussian. This makes it a poor candidate as a noise prior. As DDIM remains primarily an inversion method, the spatial and temporal information of the image are entangled inside the noise. On the contrary, our noise prior only contains temporal information, so the model can generate realistic details on top of the synthetic scene without being constrained to reconstruct the input sequence.\nD.2 VIDEO RESTORATION WITH I2SB\nWe evaluate our noise prior on two video restoration tasks based on I2SB, namely super-resolution and JPEG compression restoration. Table 7 and 8 respectively evaluate the temporal coherency and the image quality for different noise priors. Our \u222b -noise prior performs worse in JPEG restoration\nrelative to super-resolution. This might be due to the optical flow being harder to estimate from the corrupted sequence in the first case.\nIn Figure 14 and Figure 15, we visualize temporal slices of the denoised videos by concatenating a single column from each frame. In this visualization, sticking artifacts manifests as horizontal stripes, making it easy to evaluate temporal coherency qualities. The various sequences show that warping the noise consistently yields temporally-smoother and spatially-sharper results."
        },
        {
            "heading": "D.3 POSE-TO-PERSON VIDEO GENERATION WITH PIDM",
            "text": "When employing the Person Image Diffusion Model (PIDM) (Bhunia et al., 2022) for generating videos relative to an input posed person sequence, the pose sequence is not enough to estimate accurate motion vectors. So we run a few steps of diffusion with PIDM to get a blurry early prediction of the person, from which we then extract the full body pose with DensePose (Gu\u0308ler et al., 2018). The"
        },
        {
            "heading": "Train",
            "text": ""
        },
        {
            "heading": "Cows Goat",
            "text": "DensePose result is overlayed on top of the early prediction as the final prediction before computing the optical flow (Teed & Deng, 2020). The different stages of the pipeline are depicted in Figure 16.\nWe get test examples from the DeepFashion dataset (Liu et al., 2016). Table 9 gives a more detailed quantitative evaluation regarding temporal coherency. In particular, we notice that in the examples evaluated in Section 3, the pose sequences generally have little motion (as is often the case when fashion models pose). This could explain why fixing the seed has a better temporal coherency. We alternatively test on pose sequences where we simulate the camera moving, causing the poses to shift horizontally. The results are reported in the table under \u201cHigh-variance pose\u201d. Similarly, fixing the random seed gives fairly good temporal coherency. This is in contradiction with the visual results, which clearly shows sticking artifacts. A possible reason comes from the fact that we compute the temporal coherency on the entire image, and most of the background has zero motion, which may bias the mean warp error to favor the fixed seed experiments."
        },
        {
            "heading": "D.4 FLUID SUPER-RESOLUTION",
            "text": "Our last application is on the super-resolution of 2-D fluid simulations. We train an unconditional diffusion model on a large dataset of fluid simulations generated with the fluid solver from Tang et al. (2021) without special treatment on the noise. We then compare different noise priors, including the one from Chen et al. (2023). The results are hard to assess frame by frame. Thus, we encourage the reader to take a look at the supplementary videos."
        },
        {
            "heading": "E NOISE WARPING IN LATENT DIFFUSION MODELS",
            "text": ""
        },
        {
            "heading": "E.1 UNDERSTANDING THE LIMITATIONS",
            "text": "Noise warping does not improve temporal coherency in latent diffusion models as much as one would expect. We pinpoint this observation to three main reasons. First, temporally coherent images do not necessarily translate into temporally coherent autoencoder latent vectors. Thus, temporally consistent noise priors in the VAE latent space might be suboptimal. Second, the noise used in latent diffusion models does not contribute directly to the fine details of the results in image space, which is where warped noise priors excel. Lastly, the autoencoder almost always introduces a nonnegligible reconstruction error that affects the temporal coherency of the results, independent of the latent diffusion process.\nFrom a more theoretical perspective, the temporally-correlated infinite-resolution noise warping we propose is only possible because of the structural self-similarity of Gaussian noise, which allows us to interpret a discrete Gaussian noise sample as an aggregated view of more Gaussian samples at a smaller scale. The warping could thus be operated at the limit, in the continuous setting. This core assumption does not hold in latent diffusion models, because temporal coherency is no longer targeted in the latent space where the noise resides, but rather in the image space mapped by the decoder. Unfortunately, Gaussian noise decoded by the VAE no longer possesses the self-similarity property. This aspect can be visualized in the decoded noise shown by Figure 17a (a) (iii).\nNonetheless, the assumption that moving the noise helps with temporal coherency still holds in latent models to some extent, as some of our experiments below can show. One of our early observations was that the VAE decoder is translationally equivariant in a discrete way, i.e. translating the latent vector by an integer number of pixels leads to an almost perfectly shifted image when decoded. For an autoencoder with a compression factor f = 8 (e.g. Stable Diffusion), this means that a 1-pixel shift in the latent space would produce a 8-pixel shift in the final image."
        },
        {
            "heading": "E.2 EXPERIMENT SETUP",
            "text": "With this in mind, we generated a simple video consisting of two concentric gray discs translating by 8 pixels to the right every frame, in front of a white background (see Figure 17a (a) (i)). We add noise to each frame, and denoise it using Stable Diffusion with the following prompt:\n\u201cCommercial photography, square salami pizza, italian food, white lighting, top view, studio light, 8k octane rendering, high resolution photography, insanely detailed, fine details, on white isolated plain, 8k, commercial photography, stock photo, professional color grading\u201d\nWe manually simulate the warped Gaussian noise used in latent space by moving a disc of noise in front of a noisy background (Figure 17a (a) (ii)). The input frames are used as \u201dscribbles\u201d with ControlNet and the prompt provides the necessary information to generate the results. This sequence, as many of our other results, is better visualized in the included supplementary videos."
        },
        {
            "heading": "E.3 RESULTS AND ANALYSIS",
            "text": "We perform three sets of experiments, each time with increasing constraints on the model. All the results reveal empirically that our \u222b -noise prior is better, provided it is warped properly.\nBaseline: using the input frames with ControlNet and the given prompt, we generate results frame by frame (Figure 17b). While all the noise schemes produce temporally inconsistent results, flickering is especially visible when using random noise at each frame (top row). For fixed random seed (middle row), the insets show an overall structure is fixed in image space, even though the pizza is translating. This can be seen be paying attention to the regions covered by the basil leaves, which remains rougly the same. Finally, warping the noise with the input video improves the movement of the synthesized details (bottom row). In particular, the basil leaves progressively invades the inset, indicating proper translation.\nCross-frame attention: a common trick used to boost temporal-coherency in diffusion-based videoto-video translation tasks is to replace all self-attention layers by cross-attention with respect to an anchor keyframe (Hertz et al., 2022; Parmar et al., 2023). Figure 17c shows the results of additionally adding cross-frame attention with respect to the first frame for every frame. This effectively creates more constraints for the model, leading to similar content between frames. Nonetheless, fixing the noise (top row) still creates sticking artifacts, mainly on the position of the salami slices in image space. Another visible artifact can be seen in the position of the darker spots of the pizza crust, which stays in place. On the contrary, moving the noise with the input generates much more natural motion of the ingredients on the pizza (bottom row).\nFeature map injection: another tool for improving temporal coherency involves injecting feature maps (Tumanyan et al., 2022) from one image to the other. This is usually done in addition to employing cross-frame attention. It can be applied between frames, as long as feature maps are also warped. By injecting the feature maps from the first frame for all layers 4-11, our noise warping method produces pixel-perfect translation of the generated pizza (Figure 17d bottom row). Keeping the noise fixed, however, induces sticking artifacts of fine details (top row).\nAnalysis. As shown by the experiments, the impact of the noise scheme is negatively correlated with the amount of constraints given to the model. As most of the low-frequency structures gets dictated by the cross-frame attention and the feature maps, the noise becomes solely responsible for the fine high-frequency details. In any case, noise warping is a necessary treatment tor provide temporal coherency to diffusion models when they are employed on a frame by frame basis."
        }
    ],
    "year": 2023
}