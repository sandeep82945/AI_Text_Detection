{
    "abstractText": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",
    "authors": [],
    "id": "SP:f4bafd7f04d3e4b0d26675c3a7b697e1c187b1a5",
    "references": [
        {
            "authors": [
                "David Alvarez Melis",
                "Tommi Jaakkola"
            ],
            "title": "Towards robust interpretability with self-explaining neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Marco Ancona",
                "Enea Ceolini",
                "Cengiz \u00d6ztireli",
                "Markus Gross"
            ],
            "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
            "venue": "In International Conference on Learning Representations(ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian Bach",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Frederick Klauschen",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "venue": "PloS one,",
            "year": 2015
        },
        {
            "authors": [
                "Umang Bhatt",
                "Adrian Weller",
                "Jos\u00e9 M.F. Moura"
            ],
            "title": "Evaluating and aggregating feature-based model explanations",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Binder",
                "Leander Weber",
                "Sebastian Lapuschkin",
                "Gr\u00e9goire Montavon",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "Shortcomings of top-down randomization-based sanity checks for evaluations of deep neural network explanations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Prasad Chalasani",
                "Jiefeng Chen",
                "Amrita Roy Chowdhury",
                "Xi Wu",
                "Somesh Jha"
            ],
            "title": "Concise explanations of neural networks using adversarial training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Chattopadhay",
                "Anirban Sarkar",
                "Prantik Howlader",
                "Vineeth N Balasubramanian"
            ],
            "title": "Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks",
            "venue": "IEEE winter conference on applications of computer vision (WACV),",
            "year": 2018
        },
        {
            "authors": [
                "Ann-Kathrin Dombrowski",
                "Maximillian Alber",
                "Christopher Anders",
                "Marcel Ackermann",
                "KlausRobert M\u00fcller",
                "Pan Kessel"
            ],
            "title": "Explanations can be manipulated and geometry is to blame",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Finale Doshi-Velez",
                "Been Kim"
            ],
            "title": "Towards a rigorous science of interpretable machine learning",
            "venue": "arXiv preprint arXiv:1702.08608,",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Fel",
                "Agustin Picard",
                "Louis Bethune",
                "Thibaut Boissin",
                "David Vigouroux",
                "Julien Colin",
                "R\u00e9mi Cad\u00e8ne",
                "Thomas Serre"
            ],
            "title": "Craft: Concept recursive activation factorization for explainability",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ruigang Fu",
                "Qingyong Hu",
                "Xiaohu Dong",
                "Yulan Guo",
                "Yinghui Gao",
                "Biao Li"
            ],
            "title": "Axiom-based grad-cam: Towards accurate visualization and explanation of cnns",
            "venue": "In 31st British Machine Vision Conference 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Shanghua Gao",
                "Zhong-Yu Li",
                "Ming-Hsuan Yang",
                "Ming-Ming Cheng",
                "Junwei Han",
                "Philip Torr"
            ],
            "title": "Large-scale unsupervised semantic segmentation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "Abubakar Abid",
                "James Zou"
            ],
            "title": "Interpretation of neural networks is fragile",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "James Wexler",
                "James Y Zou",
                "Been Kim"
            ],
            "title": "Towards automatic concept-based explanations",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Anna Hedstr\u00f6m",
                "Leander Weber",
                "Daniel Krakowczyk",
                "Dilyara Bareeva",
                "Franz Motzkus",
                "Wojciech Samek",
                "Sebastian Lapuschkin",
                "Marina M-C H\u00f6hne"
            ],
            "title": "Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations and beyond",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Peng-Tao Jiang",
                "Chang-Bin Zhang",
                "Qibin Hou",
                "Ming-Ming Cheng",
                "Yunchao Wei"
            ],
            "title": "Layercam: Exploring hierarchical class activation maps for localization",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Been Kim",
                "Martin Wattenberg",
                "Justin Gilmer",
                "Carrie Cai",
                "James Wexler",
                "Fernanda Viegas"
            ],
            "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Maximilian Kohlbrenner",
                "Alexander Bauer",
                "Shinichi Nakajima",
                "Alexander Binder",
                "Wojciech Samek",
                "Sebastian Lapuschkin"
            ],
            "title": "Towards best practice in explaining neural network decisions with lrp",
            "venue": "In 2020 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2020
        },
        {
            "authors": [
                "Wenjie Luo",
                "Yujia Li",
                "Raquel Urtasun",
                "Richard Zemel"
            ],
            "title": "Understanding the effective receptive field in deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Gr\u00e9goire Montavon",
                "Sebastian Lapuschkin",
                "Alexander Binder",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
            "venue": "Pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Gr\u00e9goire Montavon",
                "Alexander Binder",
                "Sebastian Lapuschkin",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Layer-wise relevance propagation: an overview",
            "venue": "Explainable AI: interpreting, explaining and visualizing deep learning,",
            "year": 2019
        },
        {
            "authors": [
                "Harish Guruprasad Ramaswamy"
            ],
            "title": "Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization",
            "venue": "In proceedings of the IEEE/CVF winter conference on applications of computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations(ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "venue": "In Workshop at International Conference on Learning Representations(ICLR),",
            "year": 2014
        },
        {
            "authors": [
                "Daniel Smilkov",
                "Nikhil Thorat",
                "Been Kim",
                "Fernanda B. Vi\u00e9gas",
                "Martin Wattenberg"
            ],
            "title": "Smoothgrad: removing noise by adding noise",
            "year": 2017
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "Martin Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "venue": "In Workshop at International Conference on Learning Representations(ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Suraj Srinivas",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Full-gradient representation for neural network visualization",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Haofan Wang",
                "Zifan Wang",
                "Mengnan Du",
                "Fan Yang",
                "Zijian Zhang",
                "Sirui Ding",
                "Piotr Mardziel",
                "Xia Hu"
            ],
            "title": "Score-cam: Score-weighted visual explanations for convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Jianming Zhang",
                "Sarah Adel Bargal",
                "Zhe Lin",
                "Jonathan Brandt",
                "Xiaohui Shen",
                "Stan Sclaroff"
            ],
            "title": "Top-down neural attention by excitation backprop",
            "venue": "International Journal of Computer Vision,",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In light of the remarkable advancements in deep learning, the necessity for transparent and reliable decision-making has sparked significant interest in explainable AI (XAI) methods. In response to this imperative demand, XAI researchers have aimed to provide insightful and meaningful explanations that shed light on the decision-making process of complex deep learning models. However, the reliability of existing explanation methods in providing genuine insights into the decision-making process of complex AI models has been questioned.\nPrevious methods have not consistently adhered to the model but rather customized it to their respective preference. As a result, many of them are vulnerable to adversarial attacks, causing doubt on their reliability. To address this issue, we focus on faithfully representing the model\u2019s inference process, relying exclusively on model-generated information, and refraining from any form of correction. This approach supports the robustness of our explanations compared to other methods.\nMoreover, existing methods have traditionally analyzed models at the neuronal level, often overlooking the intricate nonlinear interaction between neurons1 to form a concept. This approach has been derived from the assumption that an individual scaler-valued channel (a filter or a neuron) carries a specific conceptual meaning. That is, the value of a single neuron directly determines the conceptual magnitude with the significance of a pixel being determined as a linear combination of each constituting neuron\u2019s conceptual magnitude. However, this assumption may oversimplify the complex nature of deep learning models, wherein multiple neurons nonlinearly collaborate to form a concept. Therefore, we analyze the models from a perspective of a vector, exploring the vector space to account for the interaction among neurons. Specifically, we introduce the pointwise feature vector (PFV), which is a vector along a channel axis of a hidden layer, amalgamating neurons that share the same receptive field.\n1A neuron outputs a scalar, an element in a tensor, by combining the information in its receptive field.\nIn addition, we alter the conventional way of calculating relevance based on post-activation values into the one based on pre-activation values. It is widely believed that in order to harmonize the concepts and differentiate classes at the final layer, image activations from the same class should be progressively merged along the shallow layers to deep layers (Fel et al., 2023). With this belief, previous methods have primarily focused on analyzing the value of the postactivation output, identifying the key contributor to the merged concept. However, we observe a fascinating phenomenon termed Activation-Pattern-Only Prediction (APOP), which shows that classification accuracies can be considerably maintained without receiving any input image, relying solely on the on/off activation pattern of the network (Refer to Tab. 1 for details). This observation highlights the significance of both the inactive and active neurons. However, after the nonlinear activation process, such as ReLU, the information about the contributors to the inactive neurons, which in turn contribute to the activation pattern, is lost. Therefore, we consider the contribution of the neurons in the prior layer to inactive neurons to fully comprehend the contribution of features.\nConsidering the aforementioned challenges, we present our novel method, Sharing Ratio Decomposition (SRD), which decomposes a PFV comprising preactivation neurons occupying the same spatial location of a layer into the shares of PFVs in its receptive field. Our approach is centered on faithfully adhering to the model, relying solely on model-generated information without any alterations, thus enhancing the robustness of our explanations. Furthermore, while conventional methods have predominantly examined models at the neuronal level, with linear assumptions about channel significance, we introduce a vector perspective, delving into the intricate nonlinear interactions between filters. Additionally, with our captivating observation of APOP, we redefine our relevance, focusing on contributions to the pre-activation feature map, where all pertinent information is encapsulated. Our approach goes beyond the limitations of traditional techniques in terms of both quality and robustness, by sincerely reflecting the inference process of the model.\nBy recursively decomposing a PFV into PFVs of any prior layer with our Sharing Ratio Decomposition (SRD), we could obtain a high-resolution Effective Receptive Field (ERF) at any layer, which further enables us to envision a comprehensive exploration spanning from local to global explanation. While the local explanation allows us to address where in terms of model behavior, the global explanation enables us to delve into what the model looks at. Furthermore, by decomposing the steps of our explanation, we could see a hint on how the model inferences (Appendix A)."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Backpropagation-based methods Saliency (Simonyan et al., 2014), Guided Backprop (Springenberg et al., 2015), GradInput (Ancona et al., 2018), InteGrad (Sundararajan et al., 2017), Smoothgrad (Smilkov et al., 2017), Fullgrad (Srinivas & Fleuret, 2019) generate attribution maps by analyzing a model\u2019s sensitivity to small changes through backpropagation. These methods are regarded as gradient-based methods, since they calculate the error through backpropagation for the input value to indicate the importance of each pixel. Gradient-based methods, however, generate noisy saliency maps due to the presence of noisy gradients. Moreover, given that the model does not have information about gradients during the inference process, the faithfulness about their explainability at inference is doubtful. In contrast, LRP (Bach et al., 2015) constructs saliency maps solely using the model\u2019s weights and activations, without using gradient information. By propagating relevance, LRP calculates the contribution of every neuron. From this perspective, our SRD can be interpreted as a vectorized version of LRP, which calculates the relevance of vectors instead of neurons.\nYet, different from LRP families (Bach et al., 2015; Montavon et al., 2017; 2019), which either ignores or assigns minor contribution to negatively contributing neurons for the active neuron, our SRD acknowledges the significance of every contribution in the model\u2019s inference process. Moreover, while LRP may not account for contributions to inactive neurons, which hold vital information for the inference, we elaborately handle contributions to both active and inactive neurons.\nActivation-based methods generate activation maps by using the linearly combined weights of activations from each convolutional layer of a model. The activation maps highlight regions within an input image that exert the most influence on the final prediction. Class Activation Mapping (CAM) (Zhou et al., 2016), for instance, replaces fully-connected layers with convolutional layers and global average pooling to derive class activation maps. Grad-CAM (Selvaraju et al., 2017) extends this approach by replacing CAM\u2019s fully connected layer weights with gradients of the predicted class with respect to the feature maps of the last convolutional layer, allowing general applicability to any architecture. Moreover, Grad-CAM++ (Chattopadhay et al., 2018) refines Grad-CAM by incorporating second-order derivatives in the computation of importance weights for feature maps, and further applying ReLU to the second-order derivatives for fine-grained details and improved localization of salient image regions.\nThese CAM-based approaches assume that each channel possesses distinct significance, and the linear combination of channel importance and layer activation can explain the regions where the model looks at importantly. However, due to nonlinear correlations between neurons, the CAM methods, except LayerCAM, struggle at lower layers, yielding saliency maps only with low-resolution. In contrast, LayerCAM (Jiang et al., 2021) inspects the importance of individual neurons, aggregating them in a channel-wise manner. It seems similar to our SRD as it calcalates the importance of a pixel (thus a vector). However, it also disregards negative contributions of each neuron and does not account for contribution of inactive neurons, as gradients do not flow through them.\nDesiderata of explanations While various methods have been proposed to generate compelling saliency maps, the absence of an objective \u2018ground truth\u2019 poses a challenge for comparison, as explainability is inherently subject to human interpretation (Doshi-Velez & Kim, 2017). This subjectivity introduces complexity in objectively evaluating different explanations. Despite the absence of a definitive ground truth for saliency maps, specific desiderata have been established for their assessment, including Localization, Complexity, Faithfulness, and Robustness (Binder et al., 2023). Localization requires saliency maps to accurately capture crucial regions during model inference. Complexity dictates that the saliency map should be sparse, as overly intricate and noisy maps are less interpretable. Faithfulness demands that if \u2018important\u2019 pixels indicated by the saliency map are removed, the model\u2019s certainty in its prediction should significantly decrease. Lastly, robustness requires the saliency map to remain consistent even under perturbation of the input image. The model should exhibit resilience not only to random perturbation, but also to targeted perturbations designed to fool explanations (Ghorbani et al., 2019a; Dombrowski et al., 2019). Our model, SRD, surpasses other state-of-art methods in meeting these desiderata without any modification of neuronal contributions during model inference."
        },
        {
            "heading": "3 METHOD: SHARING RATIO DECOMPOSITION (SRD)",
            "text": "Our method provides the versatility to perform both in forward (Fig. 1) and backward (Fig. 2) passes through the neural network, enabling a comprehensive analysis from different angles. A formal proof demonstrating this equivalence is provided in Appendix C."
        },
        {
            "heading": "3.1 FORWARD PASS",
            "text": "Pointwise Feature Vector The pointwise feature vector (PFV), our new analytical unit, comprises neurons in the hidden layer that share the same receptive field along the channel axis. Consequently, the PFV serves as a fundamental unit of representation for the hidden layer, as it is inherently a pure function of its receptive field. For linear layers, we compute the contributions of the previous PFVs to the current layer directly, leveraging the distributive law. However, for nonlinear layers, it is challenging to obtain the exact vector transformed by the layer, leading us to calculate relevance instead. The output or activation of layer l, denoted as Al \u2208 RC\u00d7HW , is composed of HW PFVs,\ni\u2192(5,7). Each v\n25\ni is labeled with its cor-\nresponding ERF. Bottom Left: The process of building ERF for v27(5,7). Bottom Right: The final saliency map is derived as a weighted sum of the ERFs at the encoder output layer.\nvlp \u2208 RC , where p \u2208 {1, \u00b7 \u00b7 \u00b7 , HW} \u225c [HW ] denotes the location of the vector in the feature map. Note that each vector belongs to the same C-dimensional vector space V l. Effective Receptive Field Each neuron can be considered as a function of its Receptive Field (RF), and likewise, other neurons situated at the same spatial location but within different channels are also functions of the same RF. Consequently, the PFV, which comprises neurons along the channel axis, serves as a collective representation of the RF, effectively encoding the characteristics of its corresponding RF. However, note that the contribution of individual pixels within the RF is not uniform. For example, pixels in the central area of the RF contribute more in the convolution operation compared to edge pixels. This influential region is known as the Effective Receptive Field (ERF), which corresponds to only a fraction of the theoretical receptive field (Luo et al., 2016). However, the method employed in (Luo et al., 2016) lacks consideration for the Activation-Pattern-Only Prediction (APOP) phenomenon and the instance-level of ERF. To address this limitation, we introduce the sharing ratio \u00b5 to reflect the different contributions of pixels and make a more faithful ERF for each PFV. With our ERFs, we can investigate the vector space of the PFV, leading to a global explanation of the model. For more details, refer to Appendix A.\nSharing Ratio Decomposition Assuming we have prior knowledge of the sharing ratio, denoted as \u00b5, between layers (which can be derived at any point, even during inference), where \u00b5 signifies the extent to which each PFV contributes to the PFV of the subsequent layer (Exact way to obtain \u00b5 is differed to Sec. 3.2). Given that we already possess information on the ERFs and sharing ratios of PFVs, we can construct the ERF of the next activation layer through a weighted sum of the constituent PFV\u2019s ERFs, expressed as follows (Fig. 1 Top, Bottom Left):\u2211\nl<k \u2211 i \u00b5l\u2192ki\u2192j \u00b7 ERFvli = ERFvkj , (1)\nwhere \u00b5l\u2192ki\u2192j is the sharing ratio of pixel i of layer l to pixel j of the subsequent layer k, and ERFvli is an ERF of PFV vli. Note that we can summate the ERFs of different layers which are parallelly connected to the k-th layer, e.g., residual connection. For the first layer, its ERF is defined as:\nERFv0i = E i, (2)\nwhere Ei is a unit matrix, where only the i-th element of the matrix is one, and all the others are zero. This means that the ERF for an input pixel is the pixel itself.\ni\u2192j is a sharing ratio of each v\nl\ni\u2192j to v\nk\nj . R\nl\ni\u2192j is the relevance share of i\nin the leading layer to j in the following layer. Right: RF kj is the receptive field of pixel j and Rkj is the relevance score of j to the output. Relevance Rli in the leading layer can be calculated recursively using the next layer\u2019s relevance Rkj \u2019s via R l i\u2192j\u2019s for j\u2019s whose receptive field includes pixel i.\nConsequently, we can sequentially construct the ERF for each layer until reaching the output of the encoder. The encoder\u2019s output consists of HW PFVs along with their ERFs. The final sailency map \u03d5c(x) is obtained through a weighted sum of the ERFs of the encoder\u2019s output PFVs (Fig. 1 Bottom Right): \u2211\ni\n\u00b5L\u2192Oi\u2192c \u00b7 ERFvLi = \u03d5c(x), (3)\nwhere \u00b5L\u2192Oi\u2192c is the sharing ratio of the pixel i of last layer L to the pixel c of the output (logit) O, which is the contribution of each PFV to output class c. As MLP classifier after encoder flattens the vectors to the scalars, there is no need to persist with our vector-based approach. Thus, for an MLP layer, we opt for the established backward methods such as Grad-CAM (Selvaraju et al., 2017).\nAdditionally, in order to ensure class-discriminative saliency, we subtract the mean of its saliency and disregard any negative contributions. Then, the modified sharing ratio2, \u00b5L\u2192Oi\u2192c , for the encoder output layer is calculated as follows:\n\u00b5L\u2192Oi\u2192c = max(\u03a6 c i \u2212\n1\nK \u2211 k\u2208[K] \u03a6ki , 0), \u03a6 c i = \u2211 k \u03b1ckA k i , \u03b1 c k = 1 HW \u2211 i\u2208[HW ] \u2202yc \u2202Aki , (4)\nwhere Aki is the k-th element of the PFV v last i and y c is the c-th element of the output logit y \u2208 RK for K classes. ReLU operation is ommited when calculating \u03a6ci since it is already applied after subtracting the mean."
        },
        {
            "heading": "3.2 BACKWARD PASS (FOR CALCULATING SHARING RATIO)",
            "text": "Suppose a PFV vkj positioned at j just prior to activation layer k. In a feed-forward network, v k j is entirely determined by the l-th activation layer\u2019s PFVs vli\u2019s within the receptive field of j, RF k j , i.e,\nvkj = f(V kl j ) = \u2211 i f li\u2192j(v l i) = \u2211 i v\u0302li\u2192j where V kl j = {vli|i \u2208 RF kj }, (5)\n2The summation of modified sharing ratios does not necessarily equal to 1.\nfor some affine function f(\u00b7) (See details in Appendix B). Note that PFV vkj can be decomposed into v\u0302li\u2192j which is a sole function of PFV v l i. In our approach, we initially define the relevance R k j of pixel j in layer k as the contribution of the pixel to the output, typically the logit. Then, we distribute the relevance, Rkj , to pixel i\u2019s in layer l by the sharing ratio \u00b5 l\u2192k i\u2192j , which is calculated as taking the inner product of v\u0302li\u2192j with v k j and normalizing both vectors by\n\u2225\u2225vkj \u2225\u2225 as follows (Fig. 2 Left): \u00b5l\u2192ki\u2192j = \u27e8\nv\u0302li\u2192j\u2225\u2225vkj \u2225\u2225 , v k j\u2225\u2225vkj \u2225\u2225 \u27e9 where v\u0302li\u2192j = f li\u2192j(vli), i.e, \u2211 i\u2208RFkj \u00b5l\u2192ki\u2192j = 1. (6)\nThen, according to the sharing ratio \u00b5l\u2192ki\u2192j , we decompose the relevance to the output:\nRli\u2192j = \u00b5 l\u2192k i\u2192jR k j , i.e, R k j = \u2211 i\u2208RFkj Rli\u2192j . (7)\nFinally, the relevance of i to the output can be calculated as Rli = \u2211\nj\u2208PF li\nRli\u2192j , PF l i = {j|i \u2208 RF kj }, (8)\nwhere PF li is the Projective Field of pixel i to the next nonlinear layer (Fig. 2 Right).\nThe initial relevance at the last layer L, RLi\u2192c, is given as\nRLi\u2192c = \u00b5 L\u2192O i\u2192c , (9)\nwhere \u00b5L\u2192Oi\u2192c is the modified sharing ratio described in Eq. 4, which represents the contribution of pixel i in the encoder output layer to class c."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": "In this section, we conducted a comprehensive comparative analysis involving our proposed method, SRD, and several state-of-the-art methods: Saliency (Simonyan et al., 2014), Guided Backprop (Springenberg et al., 2015), GradInput (Ancona et al., 2018), InteGrad (Sundararajan et al., 2017), LRPz+ (Montavon et al., 2017), Smoothgrad (Smilkov et al., 2017), Fullgrad (Srinivas & Fleuret, 2019), GradCAM (Selvaraju et al., 2017), GradCAM++ (Chattopadhay et al., 2018), ScoreCAM (Wang et al., 2020), AblationCAM (Ramaswamy et al., 2020), XGradCAM (Fu et al., 2020), and LayerCAM (Jiang et al., 2021).\nIn our experiments, we leveraged ResNet50 (He et al., 2016) and VGG16 (Simonyan & Zisserman, 2015) models3 For a balanced comparison, we conducted experiments with our method, SRD, targeting various layers to accommondate the varying resolutions of generated attribution maps. Since most CAM-based methods except for LayerCAM exhibit optimal performance when targeting higher layers, we generated low-resolution explanation maps for them.\nFor VGG16, the CAM-based methods with low resolution targeted the Conv5 3 layer, while the high-resolution LayerCAM focused on the Conv4 2 layer. In the case of ResNet50, the CAM-based methods targeted the avgpool layer, and LayerCAM was applied to the Conv3 4 layer. The dimensions of the resulting saliency maps were as follows: (7, 7) for low-resolution, (28, 28) for highresolution, and (224, 224) for input-scale. All saliency maps were normalized by dividing them by their maximum values, followed by bilinear interpolation to achieve a resolution of (224, 224)."
        },
        {
            "heading": "4.1 QUALITATIVE RESULTS",
            "text": "We visualize the counterfactual explanations of an original image with a cat and a dog. Fig. 3 shows that our explanations with SRD, are not only fine-grained but also counterfactual, while other methods do not capture the class-relevant areas and result in nearly identical maps. For more examples, refer to Appendix F.1.\n3In this paper, we restrict our discussion to Convolutional Neural Networks (CNNs); however, the extension to arbitrary network architecture is straightforward."
        },
        {
            "heading": "4.2 QUANTITATIVE RESULTS",
            "text": "Experimental setting We conducted a series of experiments to assess the performance of our method compared to existing explanation methods. All evaluations were carried out on the ImageNet-S50 dataset (Gao et al., 2022), which contains 752 samples along with object segmentation masks.\nMetric The metrics used in our experiments are as follows: To evaluate localization, Pointing Game (\u2191) (Zhang et al., 2018) measures whether maximum attribution point is on target, while Attribution Localization (\u2191) (Kohlbrenner et al., 2020) measures the ratio between attributions within the segmentation mask and total attributions. To evaluate complexity, Sparseness (\u2191) (Chalasani et al., 2020) measures how sparse the attribution map is, based on Gini index. For a faithfulness test, Fidelity (\u2191) (Bhatt et al., 2020) measures correlation between classification logit and attributions. To evaluate robustness, Stability (\u2193) (Alvarez Melis & Jaakkola, 2018) measures stability of explanation against noise perturbation, calculating the maximum distance between original attribution and perturbed attribution for finite samples. All of the metrics are calculated after clamping the attributions to [-1, 1], since all the attrubution methods are visualized after clamping. Also, the arrow inside the parentheses indicates whether a higher value of that metric is considered desirable. For more details of the metrics, refer to Appendix E.\nResults Overall, our method, SRD, demonstrated superior performance across the metrics (Table 2). Specifically, on VGG16, SRD-high achieved the highest scores in both Pointing game and Fidelity, and the second highest on Attribution Localization. In addition, SRD-input excelled in Sparseness and Stability, while maintaining comparably high scores across other metrics, especially in comparison to input-scale methods.\nSince many saliency map methods struggle to properly handle residual connections, some of the methods show a decline in performance even when the model performance itself improved. Remarkably, our method retains its competitive performance on ResNet50. On ResNet50, SRD-high achieved the highest scores in Attribution Localization and Fidelity with the second highest score at Pointing game. SRD-input achieved the best performance for Pointing game and Stability, achieving\nthe second highest scores in Attribution Localization and Sparseness. These results point out that our proposed method, SRD, can give functional, faithful, and robust explanation."
        },
        {
            "heading": "4.3 ADVERSARIAL ROBUSTNESS",
            "text": "An explanation can be easily manipulated by adding small perturbations to the input, while maintaining the model prediction almost unchanged. This means that there is a discrepancy between the actual cues the model relies on and those identified as crucial by explanation. While the Stability metric in Sec 4.2 assesses the explanation method\u2019s resilience to random perturbations, explanation manipulation (Dombrowski et al., 2019) evaluates the method\u2019s vulnerability to targeted adversarial attacks, while maintaining the logit output unchanged. The perturbation \u03b4 is optimized to minimize the loss below:\nL = \u03bb1 \u2225\u03d5(xadv)\u2212 \u03d5(xtarget)\u22252 + \u03bb2 \u2225F (xadv)\u2212 F (xorg)\u22252 , (10)\nwhere xadv = xorg + \u03b4, \u03d5(x) is the saliency map of image x, and F (x) is the logit output of model F given image x. We set \u03bb1 = 1e11 and \u03bb2 = 1e6.\nExperimental setting We conducted targeted manipulation on a set of 100 randomly selected ImageNet image pairs for the VGG16 model. Given that adversarial attacks can be taken only to gradient-trackable explanation methods, we selected Gradient, GradInput, Guided Backpropagation, Integrated Gradients, LRPz+ and our SRD for comparison. The learning rate was 0.001 for all methods. For more detail, refer to the work of Dombrowski et al. (2019). The attack was stopped once the Mean Squared Error (MSE) between x and xadv reached 0.001, while ensuring that the change in RGB values was bounded within 8 in a scale of 0-255 to let xadv be visually undistinguishable with x. After computing saliency maps, the absolute values were taken, as for the default setting in (Dombrowski et al., 2019). Since we obtained our \u00b5L\u2192Oi\u2192c by leveraging other methods, we set all \u00b5L\u2192Oi\u2192c to a constant value of 1 to eliminate the potential influence of other methods.\nMetric To quantitatively compare robustness of the explanation methods towards the adversarial attacks, we measured the similarity between the original explanation \u03d5(xorg) and the manipulated explanation \u03d5(xadv) using metrics such as the Structural Similarity Index Measure (SSIM) and Pearson Corelation Coefficient (PCC). High values of SSIM and PCC denote that \u03d5(xadv) maintained the original features of \u03d5(xorg), thereby demonstrating the robustness of the explanation method.\nResult In both PCC and SSIM results (Figure 4), SRD achieved the highest scores compared to other input-scale resolution saliency maps. Along with the result of Stability experiments on Table 2, we demonstrate that our proposed method, SRD, exhibits superior resistance against adversarial attacks, maintaining its explanatory power even in the face of perturbations."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose a novel method, Sharing Ratio Decomposition (SRD), which analyzes the model with Pointwise Feature Vectors and decomposes relevance with sharing ratios. By reflecting the model\u2019s inference process faithfully and relying exclusively on model-generated data, the explanations generated with our method fulfill the pressing need for robust and trustworthy explanations. The adoption of a vector perspective, accounting for nonlinear interactions between filters, marks a departure from conventional neuron-level analyses. Additionally, our identification of Activation-Pattern-Only Prediction (APOP) highlights the influence of inactive neurons in model behavior, which has been overlooked so far by conventional approaches.\nOur comparative and comprehensive analysis demonstrates that SRD outperforms other saliency map methods across various metrics including effectiveness, sophistication, and resilience. Especially, it showcases notable proficiency in robustness, withstanding both random noise perturbation and targeted adversarial attacks. We believe that this robustness is a consequence of our thorough reflection of the model\u2019s behavior, signaling a promising direction for local explanation methods.\nMoreover, through the recursive decomposition of Pointwise Feature Vectors (PFVs), we can derive high-resolution Effective Receptive Fields (ERFs) at any layer. With this, we would be able to generate a comprehensive exploration from local to global explanations in the future. Furthermore, we will go beyond answering where and what the model looks importantly to providing insights into how the model makes its decision."
        },
        {
            "heading": "A FUTURE WORKS: GLOBAL EXPLANATION WITH SRD",
            "text": "Local Explanation methods explain where the model regards important for classification, and global explanation methods (Kim et al., 2018; Ghorbani et al., 2019b; Fel et al., 2023) explain what it means. However, with our method, SRD, we even go further to explain how the model makes its decision.\nIn short, we can provide how the model predicted along with where the model saw and what it meant. Through empirical observation, by labeling the Pointwise Feature Vector (PFV) with Effective receptive field (ERF), we discerned that each PFV encodes a specific concept. While there are numerous sophisticated global explanation methods available, for clarity, we opted for a more straightforward approach: examining the nearest neighbors of a given PFV. By observing its closest neighbors, we can discern the meaning of the target PFV (Top of Figure 5).\nFurthermore, by analyzing the sharing ratio of a PFV, we gain insights into how each subconcept\u2014components of the target PFV\u2014shapes our target PFV (Bottom of Figure 5).\nThis recursive application allows SRD to thoroughly illuminate the model\u2019s decision-making process. Figure 5 shows an example of this attempt and gives a hint on how decision is made in a model. Detailed research on the global explanation of SRD will be dealt with in our next paper.\nB DETAIL DESCRIPTION OF AFFINE FUNCTION f li\u2192j\nIn this section, we describe how to calculate affine function f li\u2192j in Eq. 6.\nConvolutional layer Each PFV in a CNN is transformed linearly by the convolutional layer and then aggregated with a bias term. We regard a PFV of a convolutional layer as a linear combination of PFVs in the previous layers in addition to the contribution of the bias vector. For example, consider a convolutional layer with a kernel \u03c9 \u2208 RC\u2032\u00d7C\u00d7hw, where C \u2032and C are the number of output and input channels, and h and w are the height and width of the kernel, respectively. The affine function of one convolutional layer f li\u2192j is defined as:\nf li\u2192j(v l i) = \u03c9i\u2192jv l i + b \u2225\u2225vli\u2225\u2225\u2211 k\u2208RFj\n\u2225\u2225vlk\u2225\u2225 , (11) where \u03c9i\u2192j \u2208 RC\n\u2032\u00d7C , the size of RFj is h\u00d7 w. Pooling layer The average pooling layer computes the average of the PFVs within the receptive field. As a result, the contribution of each PFV is scaled down proportionally to the size of the receptive field. On the other hand, the max pooling layer performs a channel-wise selection process, whereby only a subset of channels from vl is carried forward to vl+1. This is achieved by clamping the non-selected vl\u2019s contribution to zero:\nf li\u2192j(v l i) = 1m(vl,j,i) \u2299 vli (12)\nHere, 1m \u2208 RC is the indicator function that outputs 1 only for the maximum index among the receptive field, RFj , which can be easily obtained during inference and \u2299 denotes the elementwise multiplication. Thus, given information from inference, we can consider max pooling as a linear function, f li\u2192j , whose coefficients are binary (0/1).\nBatch normalization layer Additionally, for batch normalization layer, we manipulate each PFV in a direct manner by scaling it and adding a batch-norm bias vector to it, without resorting to any intermediate representation.\nMultiple functions If there are multiple affine functions between vli and v l+1 j , we composite multiple affine function along possible paths. For example, if there are max pooling layer and convolutional layers together, the resulting f li\u2192j would be:\nf li\u2192j = \u2211 k gli\u2192k \u2299 hlk\u2192j , (13)\nwhere gli\u2192k is affine function of max pooling layer and h l k\u2192j is affine function of convolutional layer."
        },
        {
            "heading": "C PROOF OF EQUIVALENCE BETWEEN FORWARD AND BACKWARD PROCESSES",
            "text": "Forward process: Given that the saliency map for class c being \u03d5c(x) = \u2211 i \u00b5L\u2192Oi\u2192c \u00b7 ERFvLi , (14)\nand for each layer l we have\nERFvl+1i = \u2211 j \u00b5l\u2192l+1j\u2192i \u00b7 ERFvlj , (15)\nERFvLi can be broken down as follows: ERFvLi = \u2211\nj\u2208RFi\n\u00b5 (L\u22121)\u2192L j\u2192i \u00b7 ( \u2211 k\u2208RFj \u00b5 (L\u22122)\u2192(L\u22121) k\u2192j (\u00b7 \u00b7 \u00b7 ( \u2211 p\u2208RFq \u00b50\u21921p\u2192q \u00b7 ERFv0p))). (16)\nThis can be generalized as ERFvLi = \u2211\np\u2208[HW ] ( \u2211 \u03c4\u2208T \u220f l\u2208[L] \u00b5(l\u22121)\u2192lpl\u22121\u2192pl) \u00b7 E p, (17)\nwhere \u03c4 = (p0 = p, p1, \u00b7 \u00b7 \u00b7 , pL\u22121, pL = i) is a trajectory (path) from a pixel in an input image p to a pixel in the last layer L, and T denotes the set of all the trajectories. Note that H and W are the height and width of an input image and invalid trajectories have at least one zero sharing ratio on their path, i.e, \u00b5 = 0 for some layer.\nFrom Eq. 17, \u03d5c(x) becomes \u03d5c(x) = \u2211 p \u2211 i \u00b5L\u2192Oi\u2192c ( \u2211 \u03c4\u2208T \u220f l\u2208[L] \u00b5(l\u22121)\u2192lpl\u22121\u2192pl) \u00b7 E p. (18)\nBackward process: The saliency map \u03d5c(x) is defined as \u03d5c(x) = \u2211 p R0p \u00b7 Ep, (19)\nwhere Rl\u22121i = \u2211 j\u2208PFi \u00b5 (l\u22121)\u2192l i\u2192j R l j (20)\nThus, R0 becomes R0p = \u2211\nj\u2208PFp\n\u00b50\u21921p\u2192j( \u2211\nk\u2208PFj\n\u00b51\u21922k\u2192j(\u00b7 \u00b7 \u00b7 ( \u2211\ni\u2208PFq\n\u00b5 (L\u22121)\u2192L q\u2192i \u00b7R L i ))). (21)\nThis can be generalized as R0p = \u2211 i RLi ( \u2211 \u03c4\u2208T \u220f l\u2208[L] \u00b5(l\u22121)\u2192lpl\u22121\u2192pl). (22)\nSince RLi = \u00b5 L\u2192O i\u2192c and \u03d5c(x) = \u2211 p R 0 p \u00b7 Ep,\n\u03d5c(x) = \u2211 p \u2211 i \u00b5L\u2192Oi\u2192c ( \u2211 \u03c4\u2208T \u220f l\u2208[L] \u00b5(l\u22121)\u2192lpl\u22121\u2192pl) \u00b7 E p, (23)\nwhich is identical to Eq. 18."
        },
        {
            "heading": "D MORE RESULT OF APOP",
            "text": "We made an interesting observation during our experiments, which we term Activation-Pattern-Only Prediction (APOP). This phenomenon was discovered by conducting a series of experiments where a model made predictions with an image input. Subsequently, the model retained the binary on/off activation pattern along with its corresponding label (Algorithm 1). Following this, the model made a prediction once more, but this time with an entirely different input (i.e. zeros, ones) while keeping the activation pattern frozen.\nAll of our APOP experiments were conducted on the ImageNet validation dataset. We conducted experiments under three different input conditions: \u2018zeros\u2019, \u2018ones\u2019, and \u2018normal\u2019. The \u2019zeros\u2019 setting is the experiment introduced in the main paper (Table 1). In \u2019ones\u2019 setting, we predicted again with matrix with ones instead of empty matrix. In \u2019normal\u2019 setting, matrix filled with normal distribution N(0, 1) was used. As shown in Table 3, all settings achieved higher accuracy compared to random prediction baselines \u2013 0.001 for Top-1 accuracy and 0.005 for Top-5 accuracy. Especially, it is intriguing that it achieved almost the same accuracy with the original accuracy in APOP & ReLU setting, supporting our idea that activation pattern is a crucial component in explanation, complementing the actual values of the neurons.\nWe carried out an additional experiment: Particular Layer Activation Binarization as illustrated in Figure 6. Instead of entirely freezing the activation pattern, we replaced the activation value of a\nparticular layer into 1 or 0; if the activation value was greater than 0, then it was set to 1, otherwise, it was set to 0. Remarkably, even under this setting, the model predicted more accurately than random guessing. It happened even when this binarization occurred in the very first activation layer. This experiment reinforces our notion that the activation pattern holds comparable significance to the actual neuronal values.\nAlgorithm 1 APOP process in PyTorch pseudocode\nimport torch import torch.nn as nn import torch.nn.functional as F\nclass CustomReLU(nn.ReLU): def forward(self,x):\noutput = F.relu(x) self.mask = torch.sign(output) # make binary mask return output\ndef APOP_forward(self,x): output = x * self.mask # mask inactive neuron return output\nclass CustomMaxPool2d(nn.MaxPool2d): def forward(self,x):\noutput,self.mask_indices = F.max_pool2d(x,return_indices=True) return output\ndef APOP_forward(self,x): output = indice_pool(x,self.mask_indices) # mask inactive neuron\n# with saved mask_indices return output\ntotal_sample = 0 original_correct_predictions = 0 APOP_correct_predictions = 0 model = CustomModel(model) # replace ReLU and Maxpool into CustomReLU and CustomMaxPool2d empty_input = torch.zeros_like(data) for data,labels in data_loader:\noriginal_predictions = CustomModel(x) # predict original prediction and save masks APOP_predictions = CustomModel.APOP_forward(empty_input) # APOP with saved masks original_correct_predictions += compute_accuracy(original_predictions,labels) APOP_correct_predictions += compute_accuracy(APOP_predictions,labels) total_samples += labels.size(0)\noriginal_model_accuracy = original_correct_predictions / total_sample APOP_model_accuracy = APOP_correct_predictions / total_sample"
        },
        {
            "heading": "E DETAIL OF METRICS",
            "text": "Pointing Game (\u2191) (Zhang et al., 2018) evaluates the precision of attribution methods by assessing whether the highest attribution point is on the target. The groundtruth region is expanded for some margin of tolerance (15px) to insure fair comparison between low-resolution saliency map and highresolution saliency map. Intuitively, the strongest attribution should be confined inside the target object, making a higher value for a more accurate explanation method.\n\u00b5PG = Hits\nHits+Misses (24)\nAttribution Localization (\u2191) (Kohlbrenner et al., 2020) measures the accuracy of an attribution method by calculating the ratio , \u00b5AL, between attributions located within the segmentation mask and the total attributions. A high value indicates that the attribution method accurately explains the crucial features within the target object.\n\u00b5AL = Rin Rtot , (25)\nwhere \u00b5AL is an inside-total relevance ratio without consideration of the object size. Rin is the sum of positive relevance in the bounding box, Rtot is the total sum of positive relevance in the image.\nSparseness (\u2191) (Chalasani et al., 2020) evaluates the density of the attribution map using the Gini index. A low value indicates that the attribution is less sparse, which may be observed in lowresolution or noisy attribution maps.\n\u00b5Spa = 1\u2212 2 d\u2211\nk=1\nv(k) ||v||1 ( d\u2212 k + 0.5 d ), (26)\nwhere v is a flatten vector of the saliency map \u03d5(x)\nFidelity (\u2191) (Bhatt et al., 2020) measures the correlation between classification logit and attributions. Randomly selected 200 pixels are replaced to value of 0. The metric then measures the correlation\nbetween the drop in target logit and the sum of attributions for the selected pixels.\n\u00b5Fid = Corr S\u2208( [d]|S|) (\u2211 i\u2208S \u03d5(x)i, F (x)\u2212 F ( x[xs=x\u0304s] )) ,\nwhere F is the classifier, \u03d5(x) the saliency map given x\nStability (\u2193) (Alvarez Melis & Jaakkola, 2018) evaluates the stability of an explanation against noise perturbation. While measuring robustness against targeted perturbation (as discussed in Section 4.1) can be computationally intensive and complicated due to non-continuity of some attribution methods, a weaker robustness metric is introduced to assess stability against random small perturbations. This metric calculates the maximum distance between the original attribution and the perturbed attribution for finite samples. A low stability score is preferred, indicating a consistent explanation under perturbation.\n\u00b5Sta = max xj\u2208N\u03f5(xi) \u2225\u03d5(xi)\u2212 \u03d5(xj)\u22252 \u2225xi \u2212 xj\u22252 , (27)\nwhere N\u03f5(xi) is a gaussian noise with standard deviation 0.1. all of the metrics are measure after clamping the attributions to [-1,1], as all the attrubution methods are visualized after clamping."
        },
        {
            "heading": "F ADDITIONAL SALIENCY MAP COMPARISON",
            "text": "F.1 SALIENCY MAP COMPARISON\nFig. 7-16 are some examples that compare the saliency maps of different methods.\nF.2 EXPLANTION MANIPULATION COMPARISON\nFig. 17-21 are examples that compare explanation manipulation of different methods."
        }
    ],
    "title": "SHARING RATIO DECOMPOSITION",
    "year": 2023
}