{
    "abstractText": "This paper introduces a novel Transitional Dictionary Learning (TDL) framework that can implicitly learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. We propose a game-theoretic diffusion model to decompose the input into visual parts using the dictionaries learned by the Expectation Maximization (EM) algorithm, implemented as the online prototype clustering, based on the decomposition results. Additionally, two metrics, clustering information gain, and heuristic shape score are proposed to evaluate the model. Experiments are conducted on three abstract compositional visual object datasets, which require the model to utilize the compositionality of data instead of simply exploiting visual features. Then, three tasks on symbol grounding to predefined classes of parts and relations, as well as transfer learning to unseen classes, followed by a human evaluation, were carried out on these datasets. The results show that the proposed method discovers compositional patterns, which significantly outperforms the state-of-the-art unsupervised part segmentation methods that rely on visual features from pre-trained backbones. Furthermore, the proposed metrics are consistent with human evaluations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junyan Cheng"
        }
    ],
    "id": "SP:f1ad60f875edc2f6163db71d1ea80a2b16d2b61f",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Judy Fan",
                "Robert Hawkins",
                "Noah Goodman",
                "Leonidas J Guibas"
            ],
            "title": "Shapeglot: Learning language for shape differentiation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Rakesh Agrawal",
                "Tomasz Imieli\u0144ski",
                "Arun Swami"
            ],
            "title": "Mining association rules between sets of items in large databases",
            "venue": "In Proceedings of the 1993 ACM SIGMOD international conference on Management of data,",
            "year": 1993
        },
        {
            "authors": [
                "Shir Amir",
                "Yossi Gandelsman",
                "Shai Bagon",
                "Tali Dekel"
            ],
            "title": "Deep vit features as dense visual descriptors",
            "venue": "arXiv preprint arXiv:2112.05814,",
            "year": 2021
        },
        {
            "authors": [
                "Saeed Amizadeh",
                "Hamid Palangi",
                "Alex Polozov",
                "Yichen Huang",
                "Kazuhito Koishida"
            ],
            "title": "Neurosymbolic visual reasoning: Disentangling",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Bear",
                "Chaofei Fan",
                "Damian Mrowca",
                "Yunzhu Li",
                "Seth Alter",
                "Aran Nayebi",
                "Jeremy Schwartz",
                "Li F Fei-Fei",
                "Jiajun Wu",
                "Josh Tenenbaum"
            ],
            "title": "Learning physical graph representations from visual scenes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Aaron Courville",
                "Pascal Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Lukas Biewald"
            ],
            "title": "Experiment tracking with weights and biases, 2020",
            "venue": "URL https://www.wandb. com/. Software available from wandb.com",
            "year": 2020
        },
        {
            "authors": [
                "Kaidi Cao",
                "Maria Brbic",
                "Jure Leskovec"
            ],
            "title": "Concept learners for few-shot learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Mathilde Caron",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Matthijs Douze"
            ],
            "title": "Deep clustering for unsupervised learning of visual features",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Sihong Chen",
                "Kai Ma",
                "Yefeng Zheng"
            ],
            "title": "Med3d: Transfer learning for 3d medical image analysis",
            "venue": "arXiv preprint arXiv:1904.00625,",
            "year": 2019
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chen Liang",
                "Adams Wei Yu",
                "Denny Zhou",
                "Dawn Song",
                "Quoc V. Le"
            ],
            "title": "Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Junyan Cheng",
                "Iordanis Fostiropoulos",
                "Barry Boehm"
            ],
            "title": "Gn-transformer: Fusing sequence and graph representation for improved code summarization, 2021a",
            "year": 2021
        },
        {
            "authors": [
                "Junyan Cheng",
                "Iordanis Fostiropoulos",
                "Barry Boehm"
            ],
            "title": "Graph conditioned sparse-attention for improved source code understanding, 2021b",
            "year": 2021
        },
        {
            "authors": [
                "Junyan Cheng",
                "Iordanis Fostiropoulos",
                "Barry Boehm",
                "Mohammad Soleymani"
            ],
            "title": "Multimodal phased transformer for sentiment analysis",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Subhabrata Choudhury",
                "Iro Laina",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Unsupervised part discovery from contrastive reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Edo Collins",
                "Radhakrishna Achanta",
                "Sabine Susstrunk"
            ],
            "title": "Deep feature factorization for concept discovery",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Cristina Cornelio",
                "Jan Stuehmer",
                "Shell Xu Hu",
                "Timothy Hospedales"
            ],
            "title": "Learning where and when to reason in neuro-symbolic inference",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriella Csurka",
                "Christopher Dance",
                "Lixin Fan",
                "Jutta Willamowski",
                "C\u00e9dric Bray"
            ],
            "title": "Visual categorization with bags of keypoints",
            "venue": "In Workshop on statistical learning in computer vision, ECCV,",
            "year": 2004
        },
        {
            "authors": [
                "Wang-Zhou Dai",
                "Qiuling Xu",
                "Yang Yu",
                "Zhi-Hua Zhou"
            ],
            "title": "Bridging machine learning and logical reasoning by abductive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Honghua Dong",
                "Jiayuan Mao",
                "Tian Lin",
                "Chong Wang",
                "Lihong Li",
                "Denny Zhou"
            ],
            "title": "Neural logic machines",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yilun Du",
                "Shuang Li",
                "Igor Mordatch"
            ],
            "title": "Compositional visual generation with energy based models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yilun Du",
                "Shuang Li",
                "Yash Sharma",
                "Josh Tenenbaum",
                "Igor Mordatch"
            ],
            "title": "Unsupervised learning of compositional energy concepts",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Li Fei-Fei",
                "Pietro Perona"
            ],
            "title": "A bayesian hierarchical model for learning natural scene categories",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),",
            "year": 2005
        },
        {
            "authors": [
                "Qingzhe Gao",
                "Bin Wang",
                "Libin Liu",
                "Baoquan Chen"
            ],
            "title": "Unsupervised co-part segmentation through assembly",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Nicola Garau",
                "Niccol\u00f3 Bisagno",
                "Zeno Sambugaro",
                "Nicola Conci"
            ],
            "title": "Interpretable part-whole hierarchies and conceptual-semantic relationships in neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Anirudh Goyal",
                "Aniket Didolkar",
                "Nan Rosemary Ke",
                "Charles Blundell",
                "Philippe Beaudoin",
                "Nicolas Heess",
                "Michael C Mozer",
                "Yoshua Bengio"
            ],
            "title": "Neural production systems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tanmay Gupta",
                "Aniruddha Kembhavi"
            ],
            "title": "Visual programming: Compositional visual reasoning without training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Xingzhe He",
                "Bastian Wandt",
                "Helge Rhodin"
            ],
            "title": "Ganseg: Learning to segment by unsupervised hierarchical image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Geoffrey Hinton"
            ],
            "title": "How to represent part-whole hierarchies in a neural network, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Chih Hung",
                "Varun Jampani",
                "Sifei Liu",
                "Pavlo Molchanov",
                "Ming-Hsuan Yang",
                "Jan Kautz"
            ],
            "title": "Scops: Self-supervised co-part segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jeevana Priya Inala",
                "Yichen Yang",
                "James Paulos",
                "Yewen Pu",
                "Osbert Bastani",
                "Vijay Kumar",
                "Martin Rinard",
                "Armando Solar-Lezama"
            ],
            "title": "Neurosymbolic transformers for multi-agent communication",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wonjik Kim",
                "Asako Kanezaki",
                "Masayuki Tanaka"
            ],
            "title": "Unsupervised learning of image segmentation based on differentiable feature clustering",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Kipf",
                "Elise van der Pol",
                "Max Welling"
            ],
            "title": "Contrastive learning of structured world models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kenneth Kreutz-Delgado",
                "Joseph F. Murray",
                "Bhaskar D. Rao",
                "Kjersti Engan",
                "Te-Won Lee",
                "Terrence J. Sejnowski"
            ],
            "title": "Dictionary Learning Algorithms for Sparse Representation",
            "venue": "Neural Computation, 15(2):349\u2013396,",
            "year": 2003
        },
        {
            "authors": [
                "Taku Kudo"
            ],
            "title": "Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Brenden M Lake",
                "Ruslan Salakhutdinov",
                "Joshua B Tenenbaum"
            ],
            "title": "Human-level concept learning through probabilistic program induction",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "A path towards autonomous machine intelligence version 0.9",
            "venue": "Open Review,",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin Lefaudeux",
                "Francisco Massa",
                "Diana Liskovich",
                "Wenhan Xiong",
                "Vittorio Caggiano",
                "Sean Naren",
                "Min Xu",
                "Jieru Hu",
                "Marta Tintore",
                "Susan Zhang",
                "Patrick Labatut",
                "Daniel Haziza"
            ],
            "title": "xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Zhixuan Lin",
                "Yi-Fu Wu",
                "Skand Vishwanath Peri",
                "Weihao Sun",
                "Gautam Singh",
                "Fei Deng",
                "Jindong Jiang",
                "Sungjin Ahn"
            ],
            "title": "Space: Unsupervised object-oriented scene representation via spatial attention and decomposition",
            "year": 2001
        },
        {
            "authors": [
                "Chao Lou",
                "Wenjuan Han",
                "Yuhuan Lin",
                "Zilong Zheng"
            ],
            "title": "Unsupervised vision-language parsing: Seamlessly bridging visual scene graphs with language structures via dependency relationships",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jiayuan Mao",
                "Chuang Gan",
                "Pushmeet Kohli",
                "Joshua B. Tenenbaum",
                "Jiajun Wu"
            ],
            "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Christian Rupprecht",
                "Iro Laina",
                "Andrea Vedaldi"
            ],
            "title": "Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jorge A Mendez",
                "Harm van Seijen",
                "ERIC EATON"
            ],
            "title": "Modular lifelong reinforcement learning via neural composition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Fausto Milletari",
                "Nassir Navab",
                "Seyed-Ahmad Ahmadi"
            ],
            "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
            "venue": "In 2016 fourth international conference on 3D vision (3DV),",
            "year": 2016
        },
        {
            "authors": [
                "Allen Newell",
                "Herbert A. Simon"
            ],
            "title": "Computer science as empirical inquiry: Symbols and search",
            "venue": "Commun. ACM,",
            "year": 1976
        },
        {
            "authors": [
                "Fakir S. Nooruddin",
                "Greg Turk"
            ],
            "title": "Simplification and repair of polygonal models using volumetric techniques",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 2003
        },
        {
            "authors": [
                "Augustus Odena",
                "Vincent Dumoulin",
                "Chris Olah"
            ],
            "title": "Deconvolution and checkerboard artifacts. Distill, 2016",
            "venue": "doi: 10.23915/distill.00003. URL http://distill.pub/2016/ deconv-checkerboard",
            "year": 2016
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Marwin HS Segler",
                "Mike Preuss",
                "Mark P Waller"
            ],
            "title": "Planning chemical syntheses with deep neural networks and symbolic",
            "venue": "ai. Nature,",
            "year": 2018
        },
        {
            "authors": [
                "Murray Shanahan",
                "Kyriacos Nikiforou",
                "Antonia Creswell",
                "Christos Kaplanis",
                "David Barrett",
                "Marta Garnelo"
            ],
            "title": "An explicitly relational neural network architecture",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jiankai Sun",
                "Hao Sun",
                "Tian Han",
                "Bolei Zhou"
            ],
            "title": "Neuro-symbolic program search for autonomous driving decision module design",
            "venue": "In Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Tadahiro Taniguchi",
                "Emre Ugur",
                "Matej Hoffmann",
                "Lorenzo Jamone",
                "Takayuki Nagai",
                "Benjamin Rosman",
                "Toshihiko Matsuka",
                "Naoto Iwahashi",
                "Erhan Oztop",
                "Justus Piater"
            ],
            "title": "Symbol emergence in cognitive developmental systems: a survey",
            "venue": "IEEE transactions on Cognitive and Developmental Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Wouter Van Gansbeke",
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Luc Van Gool"
            ],
            "title": "Unsupervised semantic segmentation by contrasting object mask proposals",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Po-Wei Wang",
                "Priya Donti",
                "Bryan Wilder",
                "Zico Kolter"
            ],
            "title": "Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Tailin Wu",
                "Megan Tjandrasuwita",
                "Zhengxuan Wu",
                "Xuelin Yang",
                "Kevin Liu",
                "Rok Sosic",
                "Jure Leskovec"
            ],
            "title": "Zeroc: A neuro-symbolic model for zero-shot concept recognition and acquisition at inference time",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hong-Ming Yang",
                "Xu-Yao Zhang",
                "Fei Yin",
                "Cheng-Lin Liu"
            ],
            "title": "Robust classification with convolutional prototype learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Halley Young",
                "Osbert Bastani",
                "Mayur Naik"
            ],
            "title": "Learning neurosymbolic generative models via program synthesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Chang Yu",
                "Xiangyu Zhu",
                "Xiaomei Zhang",
                "Zidu Wang",
                "Zhaoxiang Zhang",
                "Zhen Lei"
            ],
            "title": "Hp-capsule: Unsupervised face part discovery by hierarchical parsing capsule network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Adrian Ziegler",
                "Yuki M Asano"
            ],
            "title": "Self-supervised learning of object parts for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "PPO (Schulman"
            ],
            "title": "2017) uses an Actor-Critic framework to train the agent, we apply a shared encoder actor and critic that the U-Net encoder in the diffusion model is shared, and we train two decoders for the actor and critic, respectively. The actor samples a move with a Bernoulli distribution, where the probability is given by the diffusion model output, on each pixel as a mask. A reward function rating on this mask by the loss and the shape score. And the critic is trained to predict",
            "year": 2017
        },
        {
            "authors": [
                "Achlioptas"
            ],
            "title": "2019) that they are composed of shared basic elements, from ShapeNetCore v2 (Chang et al., 2015), an updated version of the core ShapeNet",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, there has been a desire to incorporate the interpretability, compositionality, and logistics of symbolic systems into neural networks (NNs). Existing methods combine them in an ad hoc manner, Dong et al. (2019); Wang et al. (2019) converts symbolic programs into differentiable forms, Cornelio et al. (2023); Segler et al. (2018) introduce symbolic modules to assist NNs, Gupta & Kembhavi (2023) use neural and symbolic modules as building blocks for visual programs. These methods do not truly bring symbolic power to NNs, but simply allow them to work synergically. The essential disparity lies at a low level. NNs use distributed representations, while symbolic systems use symbolic representations. This motivates us to explore ways to bridge neural and symbolic representations, thus combining the two types of intelligence from the ground up.\nCognitive science studies (Taniguchi et al., 2018) have suggested that symbolic representation in the human brain does not appear out of nowhere; rather, there is a gradual transition from neural perception to preliminary symbols and eventually to symbolic languages over the course of human evolution, as people observe and interact with their environment. The transitional representation, preliminary symbols, is essential in connecting neural and symbolic representation. Taking this concept into account, we attempt to replicate the process of transitional representation arising from neural representation, through unsupervised learning on visual observations in this study, as an exploration of the potential path of unifying neural and symbolic thinking at the representation level.\nRepresentation explains how the input is made up of reusable components (Bengio et al., 2013). Taking visual observations as an example, distributed representation is explained by vectors, such as principal components, that illustrate the high-dimensional statistical features, and symbolic representation uses structural methods, such as logic sentences, that explain the visual parts and their connections. A transitional representation should be in between that (1) contains high-dimensional details of the input and (2) implies structural information about the semantics of the input.\nWe propose a novel Transitional Dictionary Learning (TDL) framework that implicitly learns symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. With a simple fine-tuning that aligns the output with human preference through reinforcement learning and a heuristic reward, the model can give a human-interpretable decomposition of the input; examples are shown in Figure 1. TDL uses an Expectation Maximization (EM) algorithm to iteratively update dictionaries that store hidden representations of symbolic knowledge, through an online prototype clustering on the visual parts decomposed from the inputs by a novel game-theoretic diffusion model using the current dictionaries.\nWe suggest two metrics to evaluate the learned representation. Clustering Information Gain, which assesses if the learned dictionary is parsimonious and representative, and a heuristic shape score that assesses if the decompositions are in line with human intuition. We conduct unsupervised learning experiments on three abstract compositional visual object datasets, which require the model to utilize the compositionality of data instead of simply visual features, and three tasks on symbol grounding to predefined classes of parts and relations, and transfer learning to unseen classes. The results show huge improvements compared to the state-of-the-art part segmentation baselines, which struggle to process abstract objects that lack distinct visual features. We also conduct human evaluations; the results demonstrate significantly improved interpretability of the proposed method and the proposed metrics are consistent with human assessments. Our contributions are concluded as follows.\n\u2022 We propose the unsupervised Transitional Dictionary Learning to learn symbolic features in representations with a novel game-theoretic diffusion model and online prototype clustering.\n\u2022 We introduce two metrics, the clustering information gain and the heuristic shape score, to evaluate the learned representation and give evaluation results agreed to human judgment.\n\u2022 We perform experiments, compare our method with state-of-the-art unsupervised part segmentation models, and conduct human evaluations for all models and proposed metrics.\nOur code and data are available at https://github.com/chengjunyan1/TDL."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Neural-Symbolic Learning. Some approaches incorporate a symbolic program to assist NNs. Segler et al. (2018) used a tree search in learning retrosynthetic routes, Amizadeh et al. (2020) applied first-order logic to answer visual questions, and Young et al. (2019) introduced a symbolic controller to generate repeating visual patterns. However, they are usually tailored to specific tasks. Program synthesis could be more flexible. Inala et al. (2020) synthesized multi-agent communication policies, Sun et al. (2021) searched for autonomous vehicle control programs, and Gupta & Kembhavi (2023) synthesized visual task scripts. Nevertheless, symbolic thinking was not truly incorporated. Thus, differentiable symbolic modules are proposed. Wang et al. (2019) relaxed an SAT solver as a NN layer, Riegel et al. (2020) made first-order logic propositions differentiable, Dong et al. (2019) used NNs as trainable logical functions, Dai et al. (2019) optimized NN with abductive logic, and Goyal et al. (2021) learned neural production systems of visual entities. However, they overlooked the fact that the disparity between neural and symbolic representations is the source of the problem.\nCompositional Representation. Fei-Fei & Perona (2005); Csurka et al. (2004) show early efforts to learn compositionality through the bag of words. Hinton (2021) proposed an architecture to learn\npart-whole hierarchies, which Garau et al. (2022) implemented using an attention model. Du et al. (2020) employed Energy-Based Models (EBMs) to learn compositional parts for image generation. Chen et al. (2020) learned to compose programs from basic blocks. Mendez et al. (2022) learned reusable compositions for lifelong learning. Lake et al. (2015) used Bayesian Program Learning to compose handwritten characters from parts and strokes. Shanahan et al. (2020) used attention to learn relations among objects. Mao et al. (2019) learned visual concept embeddings and Cao et al. (2021) learned them as prototypes. Kipf et al. (2020) used contrastive learning to embed concepts and implicit relations, and Wu et al. (2022) employed EBMs to embed concepts and relations for zero-shot inference. Du et al. (2021) also used EBM to represent concepts. LeCun (2022) introduced an EBM-based framework to learn hierarchical planning. In comparison, we aim to learn the seamless transition between neural and symbolic representations unsupervisedly.\nUnsupervised Segmentation. Segmentation extracts structural information from visual inputs. Lin et al. (2020) used spatial attention to learn scene parsing, Bear et al. (2020) utilized physical properties, Kim et al. (2020) clustered on feature maps, and Van Gansbeke et al. (2021) improved it by contrastive learning. Lou et al. (2022) parsed scene graphs by aligning the image with the dependency graph of a caption. Melas-Kyriazi et al. (2022) used deep spectral methods to segment. The parsed elements in these methods are not reusable; co-part segmentation attempts to address it. Collins et al. (2018) learned visual concepts by NMF, Hung et al. (2019); Choudhury et al. (2021) learned reusable parts by self-supervised learning, Amir et al. (2021) extracted concepts from a pre-trained vision Transformer, Gao et al. (2021) learned co-parts utilizing motions in videos, Ziegler & Asano (2022) uncovered parts with a Sinkhorn-Knopp clustering, Yu et al. (2022) utilized capsule networks to discover face parts and He et al. (2022) learned parts by hierarchical image generation. However, they rely on concrete visual features and learn visual patterns instead of discovering compositionality like us, thus not working on abstract input, as shown in experiments in Section 5."
        },
        {
            "heading": "3 TRANSITIONAL DICTIONARY LEARNING",
            "text": "We first introduce the transitional representation and its optimization target in Section 3.1, then optimize this target with our TDL framework based on an EM algorithm in Section 3.2. Finally, we propose the clustering information gain to evaluate the learned representation in Section 3.3. We use this convention if it is not specified separately: superscript \u00b7i denotes i-arity, superscript \u00b7(i) with brackets denotes i-th sample in a dataset, and subscript \u00b7i denotes i-th visual part in a sample."
        },
        {
            "heading": "3.1 TRANSITIONAL REPRESENTATION",
            "text": "Given a visual input x \u2208 RH\u00d7W\u00d7C , suppose 2D here for simplicity without loss of generality, we can compress it into a low-dimensional embedding r = f(x) \u2208 Rd using a NN or other machine learning models f that minimizes the reconstruction error by minr \u03f5(g(r), x) with a decoder g. As discussed above, such representations lack interpretability, compositionality, and structural information.\nAlternatively, we can employ a symbolic representation that explicitly identifies structural information. Predicate logic, the dominant and theoretically complete (Newell & Simon, 1976) symbolic representation, expresses the input x as a conjunction of logical statements \u2126 = \u03c111(\u00b7) \u2227 \u03c112(\u00b7) \u2227 ... \u2227 \u03c121(\u00b7, \u00b7) \u2227 \u03c122(\u00b7, \u00b7) \u2227 ... that minimizes semantic distance min\u2126 dS(\u2126, x), where \u03c1ki is the i-th logical sentence using a predicate of arity k, the number of arguments. Arguments \u00b7 can be logic variables, constants, or even logical sentences that form high-order sentences.\nTo simplify the analysis while keeping generality, we assume that an input x is linearly composed of visual parts xi by x = \u2211NP i=1 xi, where NP is the number of parts, although non-linear assumptions exist, such as viewing an image as a stack of layers or projection from a 3D space. To create a meaningful logical representation that is semantically close to x, we begin with the entity mappings xi \u2192 \u03c11j , grounding a 1-ary predicate \u03c11j \u2208 D1 of entities from the dictionary D1, such as Cat(xi), Tree(xi), Person(xi), in xi. Then, we construct relation mappings with higher-ary predicates, taking 2-ary as an example, (xi, xj) \u2192 \u03c12k, where \u03c12k \u2208 D2, such as left of(xi, xj), larger(xi, xj). Finally, we depict the attributes by predicates such as Red(xi), Length(xi, 5cm).\nThis process is an optimization problem argmax\u2126 P (\u2126|x,D) where D = {D1, D2, ...} is a dictionary collection of different ary predicates. There are two major drawbacks, which also led to\nthe downfall of symbolic AI. Firstly, symbol grounding that links predicates to visual components is non-trivial. Although it can be automated by supervised learning, annotation is expensive and inflexible. Secondly, designing attribute predicates that capture all the details is impossible.\nTherefore, we propose a transitional representation, the neural logic variables R = {r1, r2, ..., rNP } generated by a model f(x; \u03b8) with a hidden dictionary parameterized by \u03b8 as D. R is composed of entity vectors ri \u2208 Rd and follows the optimization target.\nmin \u03b8 N\u2211 i \u03f5(g(R(i); \u03b8), x(i)) + \u03b1ED\u0303(dS(gD\u0303(R (i); \u03b8), x(i))) where R(i) = f(x(i), \u03b8) (1)\nwhere R(i) = {r(i)j } NP j=1 are variables for x (i) in dataset X = {x(i)}Ni=1, we omit the parameters of decomposition model f and g that also need to be optimized in this target for simplicity. It minimizes both the reconstruction error of the first \u201cneural\u201d term, where g(R(i); \u03b8) = \u2211NP j=1 g\u0302(r (i) j ; \u03b8) and g\u0302(r (i) j ; \u03b8) is the decoder, and the expected semantic distance of all meaningful concrete dictionaries by the second \u201csymbolic\u201d term. The predicate head gD\u0303 can map R (i) to logic sentences \u2126(i) = gD\u0303(R (i)) that maximally preserve the semantics of the input given a concrete dictionary D\u0303. dS is an ideal metric that can accurately measure whether two representations express the same semantics. The expectation considers all meaningful dictionaries of an input (e.g., different fonts for the same character), while non-meaningful ones are not considered (e.g., the inputs are dogs, the dictionary is for cats). The coefficient \u03b1 adjusts the two goals. In practice, we can train an \u201caverage\u201d dictionary with transitional representations that have minimal possible distances from all concrete ones, and then align each by fine-tuning. Transitional representation tackles the second problem above by compressing attributes in embeddings and the first problem with unsupervised learning will be discussed in Section 3.2."
        },
        {
            "heading": "3.2 EXPECTATION-MAXIMIZATION FOR TRANSITIONAL REPRESENTATION",
            "text": "The first term in Equation 1 can be optimized by the following target (Kreutz-Delgado et al., 2003)\nargmin \u03b8 N\u2211 i=1 \u03f5(x(i), NP\u2211 j g\u0302(r (i) j ; \u03b8)) + \u03bb NP\u2211 j |rij | (2)\noptimizes the hidden dictionary \u03b8 by minimizing the reconstruction error from visual parts. However, the key challenge comes from the second term. We consider R = {ri}NPi=1, or its corresponding visual parts, as a bag of words for the image x that implicates hidden logical sentences \u2126R = g\u03b8(R), where the optimal \u03b8\u2217 = argmin \u03b8ED\u0303[dS(gD\u0303(R), g\u03b8(R))]. As we only need to consider meaningful dictionaries D\u0303 = argminD\u0303 dS(gD\u0303(R), x) that allows semantically equivalent representations of input x, an alternative target for Equation 1 is: min\u03b8 \u2211N i \u03f5(g(R (i); \u03b8), x(i)) + \u03b1dS(x (i), g\u03b8(R (i))).\nWe can reasonably assume that dS(x(i), g\u03b8(R(i))) \u221d \u2212P (\u2126R(i) |x(i), \u03b8) where meaningful logic variables and relations are more likely to appear in the dataset than non-meaningful ones, i.e., reusable and compositional. In other words, the optimal dictionary \u03b8\u2217 maximizes the likelihood of the dataset. By regarding x(i) as a visual sentence composed of visual words R(i) and dataset X as a visual corpus, we optimize the second term in the alternative target via EM algorithm inspired by the Unigram Language Model (ULM) (Kudo, 2018) which maximizes the likelihood of the dataset by iteratively updating the dictionaries given decomposed visual parts using current dictionary\nargmax \u03b8 L = N\u2211 i=1 logP (\u2126R(i) |x(i), \u03b8) (3)\nThe likelihood of the dataset is computed as the summation of the log-likelihood of the logic representations of all sample x(i) from 1 to NA arities by L = \u2211N i=1 \u2211NA j=1 logP (\u2126 j R(i)\n|x(i), \u03b8). For 1-ary, we follow ULM by logP (\u21261\nR(i) |x(i), \u03b8) = \u2211NP k=1 logP (r (i) k ), for 2-ary, the Markov\nassumption for sequential data is not suitable, thus we use a joint probability logP (\u21262 R(i) |x(i), \u03b8) =\u2211NP p=1 \u2211NP q=1 logP (r (i) p , r (i) q ), and the same applies for higher arities.\nThe optimization targets in Equations 2 and 3 give our Transitional Dictionary Learning (TDL) framework. Equation 3 can be optimized by clustering all decomposed visual parts, pairs of parts, etc. The complexity increases exponentially with the arity which is unacceptable despite low-ary is adequate to provide a graph-level representation power, we use techniques like online clustering and random sampling to improve the efficiency that are discussed later in Section 4.2. Further discussion of the limitations and the broader impacts of the TDL framework can be found in Appendix L."
        },
        {
            "heading": "3.3 CLUSTERING INFORMATION GAIN",
            "text": "We wish that the learned predicates are reusable and compositional, the decomposed parts and pairs of the test set should be clustered in as few centroids C as possible. Thus, we propose Clustering Information Gain (CIG) by comparing the Mean Clustering Error (MCE) of the decomposed parts in the test set, marked MCE = [ \u2211N i=1 \u2211NP j=1(minc\u2208C ||r (i) j \u2212 c||2)/NP ]/N with the random decomposition MCErand, which is a lower bound when the decomposed terms are randomly scattered, while the best case of MCE is 0 when the parts match perfectly learned predicates. CIG is given by CIG = 1\u2212MCEmodel/MCErand normalized between [0, 1]. See Appendix I for more details."
        },
        {
            "heading": "4 METHOD",
            "text": "To implement the TDL, we need an encoder f : x\u2192 R that decomposes the input x intoR = {ri}NPi=1 where each ri decoded by a decoder g : ri \u2192 mi into a visual part xi = mix, mi \u2208 [0, 1]H\u00d7W\u00d7C is a mask. Inspired by Wu et al. (2022), we adopt a U-Net-based diffusion model Song et al. (2021) to iteratively refine the generated mask, the encoder downsamples the x to the embedding ri which upsampled by the decoder as mi. The model iterates K steps. NP copies of the model sharing the same parameters, generate NP masks in parallel with each produces one mask. At each step, for model i, the mask mi(t) generated in the previous step (a random mask for the first step) and other feature maps, such as other models\u2019 output, are inputted, to produce an updated mask mi(t + 1). After K steps, the model outputs NP visual parts and the corresponding representation R.\nTo generate multiple meaningful visual parts at the same time, we propose a game-theoretic method in Section 4.1 inspired by Gemp et al. (2021) who model PCA as a competitive game of principal components. We regard the decomposition process as a cooperative game of visual parts that converge in K steps to cooperatively reconstruct the input while competing with each other to avoid repetition and so on. Each part is adjusted by a \u201cplayer\u201d, one of the NP copies of the model. With the visual parts generated, we use prototype clustering to implement the EM algorithm in Section 4.2. We also introduce a shape score to measure model performance and serve as a reward to tune the unsupervised learned model with reinforcement learning in Section 4.3. Figure 2 shows an overview of our method."
        },
        {
            "heading": "4.1 GAME-THEORETIC DECOMPOSITION",
            "text": "A player model adjusts the generated visual parts to maximize the utility modeled by a GT loss\nLGT = LRec + \u03b11Loverlap + \u03b12Lresources + \u03b13Lnorm (4)\nevaluates the equilibrium state composed of NP generated parts \u2212\u2192x = (x1, x2, ...xNP ), in detail:\nReconstruction Error. LRec evaluates the reconstructed input x\u0303 = \u2211NP\ni xi using a combination of focal loss (Lin et al., 2017) and dice loss (Milletari et al., 2016).\nOverlapping Penalty. Loverlap = \u2211\nH,W,C max(0, x\u0303\u2212x) introduces a competition between players that avoids overlap between parts by penalizing redundant parts.\nResources Penalty. Lresources = \u2211NP\ni max(0, qR \u2212 |mi|) where qR \u2208 R is the quota of a player, prevents one player from reconstructing everything while others output empty. The quota restricts one player from having enough resources to output the entire input, thus requiring cooperation.\nL2 Norm. Lnorm = \u2211NP\ni ||m\u0304i||22 where m\u0304i is the unactivated mask before input into the Sigmoid function, which can simplify the search space of the model to accelerate convergence.\nSee Appendix E for further details. We follow SMLD (Song et al., 2021) to train a scoring network \u2207mi(t)LGT = fS(si(t); \u03b8, \u03d5), where \u03d5 = {\u03d5i} NA i=1 are prototype dictionaries from 1 to NA arities that will be discussed in Section 4.2, to approximate the gradient of optimal move for each player i that maximizes utility \u2212LGT . The input state si(t) = (ei(t), x\u0304i(t)) includes the feature map x\u0304i(t) = concat(x;mi(t); \u2211 k \u0338=imk(t), ...) that contains the input, the current mask, and the moves of other players (i.e., \u201cbroadcast\u201d), and an embedding ei(t) = etimei (t) + e pid i + e pred i (t) covers a time step etimei (t) \u2208 Rdemb and player index e pid i \u2208 Rdemb from learned embedding tables, and a\npredicate embedding epredi (t) = \u2211 \u03c3\u2208\u03d51 P (\u03c3|xi(t))\u03c3 computed for every step. The move sampled by Langevin dynamics mi(t + 1) = mi(t) + \u03f5\u2207mi(t)LGT + \u221a 2\u03f5z(t), z(t) \u223c N(0, I), t = 0, 1, ...,K where \u03f5 is the step size. A loss term LSMLD from the SMLD paper can be used to minimize E[||\u2207mi(t)LGT \u2212 fS(si(t); \u03b8)||2] to train the scoring network to give good approximations. We apply this term as a regularization to form Decomposition Loss LDecomposition = LGT + \u03b2LSMLD in Figure 2. Further details can be found in the Appendix A."
        },
        {
            "heading": "4.2 ONLINE PROTOTYPE CLUSTERING",
            "text": "We cluster R to implement the EM algorithm in Equation 3. As discussed in Section 3, we learn multiple dictionaries for different arities. Each dictionary is composed of prototypes \u03d5i \u2208 RN\u03d5i\u00d7d\u03d5 where N\u03d5i is the dictionary size and d\u03d5 is the dimension of the prototype. We train a predicate head for each dictionary; for example, for 1-ary, \u00b51i = f 1 \u00b5(ri) maps a neural logic variable ri to a representation \u00b51i \u2208 Rd\u00b5 , for 2-ary, \u00b52k = f2\u00b5(ri, rj) maps a pair (ri, rj). In our work, mappers are implemented as convolution layers on visual parts and their combinations, e.g., \u00b52k = f 2 \u00b5(xi + xj).\nWe perform an online clustering during training by maintaining a FIFO memory bank M = {M i}NAi=1 where M i \u2208 RLiM\u00d7d\u00b5 that adds new terms \u2212\u2192 \u00b5i after each training step. Similar to Caron et al. (2018), we run K-Means for every one or a few training steps after the warm-up epochs, in a set \u2212\u2192 \u00b5i +M i for each dictionary, where, taking 1-ary as an example, \u2212\u2192 \u00b51 = (\u00b51(1), \u00b51(2), ..., \u00b51(B)) is the set of 1-ary representations in an input batch of length B. \u2212\u2192 \u00b5i has gradients while M i does not. We drop unwanted terms, such as empty ones, and randomly sample, for example, 30% pairs to increase efficiency.\nThe K-Means output assignments Ci for each term in \u2212\u2192 \u00b5i , we make pseudo-labels Y i for prototypes in the dictionary \u03d5i by assigning them to their nearest clustering centroids from Ci. Then we minimize the distance between prototypes and their assigned centroids in latent space by Cross-Entropy (CE) loss min\u03d5i,\u03b8 LiCE = CELoss(dist( \u2212\u2192 \u00b5i, \u03d5i), Y i) where dist is a distance metric (e.g. L2 distance).\nThe Clustering Loss is the sum of the CE loss for all dictionaries LClustering = \u03b3 \u2211NA i=1 L i CE . We optimize it with the decomposition loss as LTDL = LClustering + LDecomposition to implement the\nTDL framework. More details can be found in the Appendix A.2. We visualize the latent space of a model trained in LineWorld for all the decomposed parts of the test set in Appendix D by tSNE, where we can see that the predicates in the dictionary are learned as different clusters, while the baseline, UPD, does not provide predicate information and shows a less organized latent space."
        },
        {
            "heading": "4.3 REINFORCEMENT LEARNING AND SHAPE SCORE",
            "text": "We employ PPO (Schulman et al., 2017) to tune an unsupervised learning model by regarding the decomposition process as an episode. We use a heuristic shape score to evaluate the decomposed part by 3 factors. (1) Continuity Rcont: the shape is not segmented and is an integral whole. Rcont =\nmax(AC) sum(AC) where AC is the list of contoured areas for segments in a part. Rcont = 1 if the part is not segmented. We use findContours in OpenCV to obtain the segments for 2D data and DBSCAN for 3D after converting to point clouds. (2) Solidity Rsolid: no holes inside a part. Rsolid =\nAP sum(AC) where AP is the space or area of the part. Rsolid = 1 if there is no hole. (3) Smoothness Rsmooth: the surfaces or contours of the part are smooth. Rsmooth = \u03c1S\u03c1O , where \u03c1S is the perimeter of the smoothed largest contour and \u03c1O is for the original contour. We apply RDP to smooth 2D data and alpha shape for 3D. The shape scoreRS = Rcont\u00d7Rsolid\u00d7Rsmooth normalized between 0 and 1. It can also be used to measure model performance. See Appendix J for more details."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In Section 5.1, we present our experiment setup to assess whether models can learn meaningful components in abstract objects without supervision, results discussed in Section 5.2. We then evaluate the learned representations by pre-training the models in an unsupervised manner and fine-tuning them for downstream tasks in Section 5.3. Finally, we conduct a human study in Section 5.4."
        },
        {
            "heading": "5.1 EXPERIMENT SETTING",
            "text": "We use three abstract compositional visual object datasets as shown in Figure 1, where parts cannot be separated by edges, features, colors, etc. Such contiguous shapes can only be decomposed by knowledge of compositionality, thus excluding confoundings. LineWorld is generated by the babyARC engine (Wu et al., 2022), consisting of images with 1 to 3 non-overlapping shapes made up of parallel or perpendicular lines. OmniGlot (Lake et al., 2015) contains handwritten characters. ShapeNet5 is composed of 3D shapes in 5 categories (bed, chair, table, sofa, lamp) from ShapeNet (Chang et al., 2015) voxelized by binvox (Min, 2004 - 2019). We replace 2D conv layers with 3D when using this dataset. We create three downstream tasks based on them in Section 5.3.\nWe compare three state-of-the-art unsupervised part segmentation methods: DFF (Collins et al., 2018) clusters pixels by non-negative matrix factorization (NMF) on the activations of the last conv layer; SCOPS (Hung et al., 2019) and UPD (Choudhury et al., 2021) learn to produce a k channels heatmap of parts self-supervisedly. The baselines require pre-trained visual backbones. Following\n(Choudhury et al., 2021), we use VGG19 for 2D data and MedicalNet Chen et al. (2019), a ResNetbased high-resolution 3D medical voxel model, for 3D. We conducted hyperparameter searches for baselines to get their best results. Further details of the setting are provided in the Appendix B."
        },
        {
            "heading": "5.2 UNSUPERVISED LEARNING OF TRANSITIONAL REPRESENTATION",
            "text": "The results are presented in the first three columns of Table 1. We train Auto-Encoders as a reference to see if the generated parts match the input and whether the transitional representation preserves high-dimensional information. The LineWorld and ShapeNet5 inputs are binary, so we use IoU for a better intuitive. CIG is introduced in Section 3.3 and the shape score (SP) is discussed in Section 4.3.\nOur model significantly outperforms the baselines with 58.0, 68.5, 54.6 CIG, and 82.6, 70.6, 60.1 SP in the three datasets, respectively. Even without reinforcement learning, the advantages remain. The low reconstruction error of 94.3 IoU, 1.8 MAE, and 79.8 IoU indicates the preservation of high-dimensional information. This is because the baselines depend on concrete visual features such as edges, colors, textures, etc., enabled by pre-trained vision backbones, to identify the boundaries of parts, which are absent in our datasets. For instance, there is no explicit color or texture difference between strokes in a handwritten character, and seems like contiguous integrity, thus can only be distinguished by the knowledge of strokes, which is learned via discovering compositional patterns. Figure 3 shows a comparison in the OmniGlot test set. See more samples in the Appendix M."
        },
        {
            "heading": "5.3 ADAPT TO DOWNSTREAM TASKS",
            "text": "Symbol Grounding. We design two symbol grounding tasks: LW-G and OG-G. LW-G synthesized with babyARC while preserving the shape masks (e.g. lines) and the pair-wise relation annotations (e.g. perpendicular and parallel) from the engine as labels. The goal is to predict the shape masks and classify the pair-wise relations. We aligned the predicted mask with the ground truth by the assignment with minimal overall IoU before computing the metrics. We pre-train the models on LineWorld and add a relation prediction head on the top of baselines while our method directly adapts the 2-ary predicate head for relations classifying. OG-G is a subset of OmniGlot with the provided stroke masks as ground truth to predict. We align prediction and ground truth as LW-G. We pre-train models on OmniGlot without OG-G samples. We show examples of LW-G and OG-G in Appendices\nG and 9. As shown in Table 1 under LW-G and OG-G, we achieve 78.4 IoU, 74.8 Acc. in LW-G and 75.9 IoU in OG-G, outperforming baselines whose relation prediction did not converge due to incorrect segmentations. This demonstrates that the learned transitional representation enables smooth transfer to a concrete set of symbols, as hypothesized in Section 3.1.\nTransfer Learning. Following ShapeGlot (Achlioptas et al., 2019), we pre-train with shapes from \u201cchair\u201d and other 4 similar categories, then transfer to 230\u223c550 samples from unseen categories \u201cBed\u201d, \u201cLamp\u201d, \u201cSofa\u201d, \u201cTable\u201d. We compare our method with and without pre-training. The results in Table 2 demonstrate that the learned representations are reusable and effectively generalized to unseen classes. Without pre-training, the samples for each class were not sufficient to converge."
        },
        {
            "heading": "5.4 HUMAN EVALUATION",
            "text": "Human Interpretability. We conduct a human evaluation to evaluate our method and the baselines by humans. We randomly selected 500 samples from the OmniGlot test set and the decomposition results for each method. We use the Google Vertex AI (Google, 2021) data labeling service to evaluate the results as an image annotation task with three annotators. Annotators are given decomposed samples and asked to provide one of four opinions, examples of which can be found in Appendix K, as outlined in an instruction that must be read before the task begins. The 2000 samples are shuffled and then randomly assigned to the annotators. The results in Figure 4 left show a much better interpretability of our method, while \u223c 65% of the baseline results are not considered strokes.\nInterpretability vs. Metrics. We further train 6 more models, in addition to the 4 models in Section 5.2, to get near-even distributed SP and CIG by early stop. We then conduct human evaluations in the same way as above. We assign points for each sample by Non-stroke:0, Unnatural:1, Acceptable:3, Good:5, then average them as scores for each model. And compare the scores with the metrics of each model in Figure 4 right, which shows that SP and CIG are positively correlated with human interpretability, which can be used as reliable predictors of interpretability."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper presents the TDL framework, which uses an EM algorithm to learn a neural-symbolic transitional representation that incorporates structural information into representations. We introduce a game-theoretic diffusion model with online prototype clustering to implement TDL and assess by proposed metrics, clustering information gain, and shape score. We evaluate our method on three abstract compositional visual object datasets, using unsupervised learning, downstream task experiments, and human assessments. Our results demonstrate that our method largely outperforms existing unsupervised part segmentation methods, which rely on visual features instead of discovering compositionality. Furthermore, our proposed metrics are in agreement with human judgment. We believe that our work can help bridge the gap between neural and symbolic intelligence."
        },
        {
            "heading": "7 REPRODUCIBILITY STATEMENT",
            "text": "To guarantee the reproducibility and completeness of this paper, we provide the full details of our model architecture and implementation in the Appendix A. Appendix B contains information about the generation or preprocessing of samples for each dataset and the split used in each experiment. Appendix C contains our settings for hyperparameter search and our hardware platform information. The tricks we used to calculate the GT loss are included in Appendix E. We also make our code and data publicly available for readers to reproduce our work."
        },
        {
            "heading": "A ARCHITECTURE DETAILS",
            "text": "We present the details of our model here. The architecture is depicted in Figure 5, which includes two core components, a diffusion model, and a predicate head, as well as three optional optimizations: Latent diffusion model (LDM) to improve efficiency, an attention mechanism to incorporate context information such as the moves of other players, and PPO to tune the model. We assume a single 2D input by default in this section for simplicity, and it is easy to extend to batch cases.\nA.1 DIFFUSION MODEL\nAlgorithm 1 Decompose an input Require: an scoring model fS(\u00b7; \u03b8, \u03d5), decoder fg : Rd 7\u2192 RH\u00d7W\u00d7C map representation to visual\npart Require: an input x \u2208 RH\u00d7W\u00d7C , number of players NP , time steps K\n1: Randomly initialize variables of NP players R(0) \u2208 RN\u00d7d 2: Initialize player id embedding epid \u2208 RN\u00d7demb and time embedding etime \u2208 RK\u00d7demb 3: for t \u2208 {1, ...,K} do 4: \u2212\u2192x (t) = fg(R(t)), where \u2212\u2192x (t) \u2208 RN\u00d7H\u00d7W\u00d7C \u25b7 visual parts of N players 5: \u2200j \u2208 {1, ..., NP }, \u03c8j(t) = [x;xj(t); \u2211 k \u0338=j xk(t); \u2211 k \u0338=j,\u03c3\u2208\u03d52 P (xk(t), \u03c3|xj(t))xk(t)] 6: \u2200j \u2208 {1, ..., NP }, ej(t) = epidj + etimej (t) +GetPrediEmb(\u03c8j(t), \u03d5) \u25b7 see A.2.1 for GetPrediEmb\n7: R(t+ 1) = fS([\u03c8(t), e(t)]; \u03b8, \u03d5) 8: end for 9: return R(K)\nThe algorithm 1 illustrates how the model decomposes an input. Notice that there are a few differences between Section 4.1 where the scoring model also incorporates the prototype dictionaries. Our model is based on SMLD (Song et al., 2021). SMLD adds a dense layer between each convolution layer in U-Net, which introduces the time-step embedding etime \u2208 Rdemb by adding the output of this dense layer to the output of the convolutional layer. At each time step t, for a player j, the input state sj(t) composed of feature maps \u03c8j(t) \u2208 RH\u00d7W\u00d7Cinp and the embedding e(t) \u2208 Rdemb , to produce updated decomposed composition xj(t+ 1).\nA.2 PREDICATE HEAD\nThe predicate head embeds decomposed parts into a latent space and clusters them to learn dictionaries for predicates. The head f i\u00b5 for i-ary predicates implemented with a shared mapper fmapper for\nall arities implemented as convolution layers with global pooling, mapping part xj \u2208 RH\u00d7W\u00d7C to tensor \u03c4j \u2208 Rdmapper , and an arity-specific linear layer Qi \u2208 Rdmapper\u00d7d\u00b5 map the \u03c4j to embedding \u00b5ij . In our work, we apply the multi-prototype trick (Yang et al., 2018), which means that we apply K\u03d5i prototypes for each i-ary cluster, resulting in a multi-prototype dictionary \u03d5i \u2208 RK\u03d5i\u00d7N\u03d5i\u00d7d\u00b5 .\nAlgorithm 2 Compute clustering error for 1 and 2-ary predicates Require: L variables R \u2208 RL\u00d7d, prototypes \u03d51 \u2208 RN\u03d51\u00d7d\u00b5 , \u03d52 \u2208 RN\u03d52\u00d7d\u00b5 Require: memory banks M1 \u2208 RL1M\u00d7d\u00b5 , M2 \u2208 RL2M\u00d7d\u00b5 Require: mappers f1\u00b5, f2\u00b5 : Rd 7\u2192 Rd\u00b5 , decoder fg : Rd 7\u2192 RH\u00d7W\u00d7C Require: distance metric for two vector sequences of length n1 and n2: d : Rn1\u00d7d\u00b5 \u00d7 Rn2\u00d7d\u00b5 7\u2192\nRn1\u00d7n2 1: \u2212\u2192x = filter(fg(R)), where \u2212\u2192x \u2208 R\u00b5 \u2032\u00d7H\u00d7W\u00d7C \u25b7 filter out unwanted visual parts 2: \u2212\u2192 \u00b51 = sample(f1\u00b5( \u2212\u2192x ), N1\u00b5), where \u2212\u2192 \u00b51 \u2208 RN 1 \u00b5\u00d7d\u00b5 \u25b7 random sample N1\u00b5 items 3: \u2212\u2192p = filter({xq + xw,\u2200xq, xw \u2208 \u2212\u2192x }), where \u2212\u2192p \u2208 R\u00b5\u2032\u2032\u00d7H\u00d7W\u00d7C \u25b7 pair representations 4: \u2212\u2192\u00b52 = sample(f2\u00b5(\u2212\u2192p ), N2\u00b5), where \u2212\u2192 \u00b52 \u2208 RN 2 \u00b5\u00d7d\u00b5 \u25b7 random sample N2\u00b5 items 5: 6: function ASSIGN(Ci \u2032 ,\u03d5i)\n7: C\u0304 = { \u2211 0<j\u2264Ni\u00b5 \u03d5ij\u22ae(C i j==k)\u2211\n0<j\u2264Ni\u00b5 \u22ae(Cij==k)\n,\u22000 < k \u2264 N\u03d5i} \u25b7 Centroids for i-ary cluster\n8: dE = L1 Dist(C\u0304, IN\u03d5i ), dE \u2208 R N\u03d5i\u00d7N\u03d5i \u25b7 L1 Distance from centroids to permutations\nof assignments 9: while k < N\u03d5i do\n10: (row, col) = argmin(dE) 11: Y ik = col 12: dE [row, :] = +inf 13: dE [:, col] = +inf 14: end while 15: return Y i 16: end function 17: 18: function GETLOSS( \u2212\u2192 \u00b5i ,M i,\u03d5i) 19: Ci = K-Means([ \u2212\u2192 \u00b5i;M i]), Ci\n\u2032 = {Cij , j = {1, ..., N i\u00b5}}\n20: Y i = Assign(Ci \u2032 , \u03d5i), Y i \u2208 ZN i \u00b5\u00d7N\u03d5i 21: disti = d( \u2212\u2192 \u00b5i, \u03d5i), disti \u2208 RN i \u00b5\u00d7N\u03d5i 22: return CrossEntropyLoss(disti, Y i) 23: end function 24: return GetLoss( \u2212\u2192 \u00b51,M1, \u03d51) +GetLoss( \u2212\u2192 \u00b52,M2, \u03d52)\nIn each time step t, for player j, the predicate head accepts the current decomposition xj(t) as input and outputs the predicate embedding and cooperator map. After K time steps, given the representation R, the predicate head computes the clustering error by Algorithm 2. We further explore a Higher-Order Logic (HOL) predicate, an HOL representation is given by dot-product attention, as hij = \u2211 k \u0338=j P (r i j , r i k)r i k, and HOL pairs \u03b7 i q,w can be constructed with h\ni in the same way as 2-ary predicates by summing up the corresponding image parts.\nA.2.1 PREDICATE EMBEDDING For a player j, the predicate embedding is computed as epredj (t) = \u2211\n\u03c3\u2208\u03d51 P (\u03c3|xj(t))\u03c3 where P (\u03c3|xj(t)) \u221d \u2212dist(\u03c3, q1j (t)), dist is a distance metric, we use L2 distance by default in this work and q1j (t) = Q\n1fmapper(xj(t)). It represents the potential 1-ary predicate of the current decomposition result. If using a higher-order predicate, we sum epredj (t) with \u2211 \u03c3\u2208\u03d5h1 P (\u03c3|hj(t))\u03c3, where\nP (\u03c3|hj(t)) \u221d \u2212dist(\u03c3, qh1j (t)), qh1j = Qh1fmapper(fg(hj(t))) and hj(t) = \u2211\nk \u0338=j P (rj , rk)rk where P (rj , rk) \u221d rjrk, the dot-product distance, to model relations over grouped players.\nA.3 LATENT DIFFUSION MODEL\nLDM (Rombach et al., 2022) uses an Auto-Encoder to encode an input x \u2208 RH\u00d7W\u00d7C into a compressed input x\u2032 \u2208 RH\u2032\u00d7W \u2032\u00d7C\u2032 , where H \u2032 < H and W \u2032 < W , and input the compressed input x\u2032 instead of the original input x into the diffusion model, which outputs the composition xj\u2032 \u2208 RH\n\u2032\u00d7W \u2032\u00d7C\u2032 in latent space, then uses the decoder to decompress it into the original pixel space xj \u2208 RH\u00d7W\u00d7C . We use a U-Net-based Auto-Encoder in our work, and we keep the skip connection that inputs the downsampled middle results from the encoder to the decoder.\nA.4 ATTENTION LAYERS\nWe introduce optional transformer blocks (Vaswani et al., 2017) between the convolution layers in U-Net, similar to CLIP (Radford et al., 2021). There are two types of transformer blocks that have been used in our methods, self-attention, and cross-attention, self-attention is only used in the encoder part when using an LDM since there is no context, and all other places use the crossattention. For cross-attention, the context is the set of the mapped representation of the competitors {\u03c41, \u03c42, ..., \u03c4j\u22121, \u03c4j+1, ...\u03c4m} for player j mapped by fmapper introduced in Section A.2. We utilize memory-efficient attention in xFormers (Lefaudeux et al., 2022) in our implementation.\nAttention also gives a powerful tool for incorporating multimodal information (Cheng et al., 2021c) and inductive biases (Cheng et al., 2021a;b), the context may come from other modalities, similar to Rombach et al. (2022), and in a multi-agent case, the context could come from other agents.\nA.5 PPO AND ACTOR-CRITIC\nPPO (Schulman et al., 2017) uses an Actor-Critic framework to train the agent, we apply a shared encoder actor and critic that the U-Net encoder in the diffusion model is shared, and we train two decoders for the actor and critic, respectively. The actor samples a move with a Bernoulli distribution, where the probability is given by the diffusion model output, on each pixel as a mask. A reward function rating on this mask by the loss and the shape score. And the critic is trained to predict the reward given a state. We follow the PPO algorithm with the implementation of PPO2 (OpenAI, 2018-2021). The model is updated every few steps of sampling. A small buffer that saves actions, states, following states, and other useful information is maintained and retrieved iteratively when the model is updated."
        },
        {
            "heading": "B DATASET DETAILS",
            "text": "We present statistical information about the data we used, the division of the training, testing, and development sets, and the details of how we generated and pre-processed the datasets.\nLineWorld. We employ the babyARC engine (Wu et al., 2022) to generate the LineWorld dataset. This dataset consists of objects made up of lines, which are related to each other in terms of parallelism and perpendicularity. Each sample is an image containing between one and three \u201cLshape\u201d, \u201cTshape\u201d, \u201cEshape\u201d, \u201cRectangle\u201d, \u201cHshape\u201d, \u201cCshape\u201d, \u201cAshape\u201d, and \u201cFshape\u201d objects, each of which may have a different size and one of three randomly selected colors. The shapes are non-overlapping and placed randomly on a white background. In total, we synthesize 50000 samples, which are divided into 8:1:1 splits for training, development, and testing.\nLineWorld-Grounding (LW-G). We used the babyARC engine to synthesize the LW-G dataset. The objects in LW-G are composed of lines as a basic concept with four relations to each other:\nParallel. The two lines are parallel.\nVerticalMid. The two lines are vertical, with an endpoint of one line attached to the middle of another.\nVerticalEdge. The two lines are vertical, with an endpoint of one line attached to an endpoint of another.\nVerticalSepa. The two lines are vertical, but the endpoint of one line is not attached to another.\nEach object is composed of a pattern. Apart from the 4 relationships as the basic pattern that simply samples two random lines following the 4 relations, we design 7 more complex patterns:\nPatterns Sampling process F pattern Sample line in a random position, sample the second line that \u201cVerticalMid\u201d to the first line, then sample a third line \u201cVerticalEdge\u201d to the first E pattern Sample one more line that \u201cVerticalEdge\u201d to the first line in a \u201cFpattern\u201d with one endpoint attached to the unattached endpoint of the first line A pattern Sample one more line that is \u201cParallel\u201d to the first line in a \u201cFpattern\u201d, the line can be anywhere that does not overlap with the first line C pattern Sample one line in a random position, sample the second line that \u201cVerticalEdge\u201d\nto the first line, then sample a third line \u201cVerticalEdge\u201d to the first but attach to another endpoint\nH pattern Sample one line in a random position, sample the second line that \u201cVerticalMid\u201d to the first line, then sample a third line \u201cParallel\u201d to the first P pattern Sample one more line that \u201cVerticalEdge\u201d to the second line in a \u201cFpattern\u201d on the unattached endpoint Rect Sample one more line that \u201cVerticalEdge\u201d to the second line in a \u201cCpattern\u201d on the unattached endpoint\nThe length and direction of the lines are randomly determined and the color of each line is randomly selected from two available colors. This implies that an \u201cF pattern\u201d does not necessarily have to be in the form of an \u201cF\u201d - the two parallel lines may have different directions and lengths, and this is true for all other patterns as well. Each sample is composed of an image of one object, a list of concepts (i.e., lines) where each concept is represented by a mask that points out this concept in the image, and a list of relation tuples between the concepts (e.g. (line1, line2, parallel)). In total, we generated 7000 samples, with a 5:1:1 split for train, dev, and test sets.\nWe assess concept prediction in this manner: Let us assume that the model provides k potential concepts and k2 connections between them, forming a complete graph GC . We then determine whether G is included in GC . We assign c to the closest candidates with the least Intersection over Union (IoU) and calculate the mean IoU. Subsequently, we calculate the top-1 accuracy of the predicted relation based on the assignment of nodes.\nOur model reads the relation between two parts directly from the two-ary predicates. For baselines, we added a relation prediction head that takes two parts of the heatmap as input and predicts the relation between them. This prediction head has a similar structure to our composition mapper, which is used to embed the outputted composition from each player for clustering. The parts are mapped first, and then a classifier is used to predict the relation.\nOmniGlot. The OmniGlot dataset was collected by Lake et al. (2015) using Amazon\u2019s Mechanical Turk (Amazon, 2005). They recruited participants to draw 1623 characters from 50 different alphabets, each of which was drawn 20 times by different people. Each sample was composed of multiple stroke sequences of [x, y, t] coordinates that documented the strokes used to create the character. We employ the program from Lake et al. (2015) to transform the stroke sequences into images. We allocate 24000, 1500, and 1500 images for the training, testing, and development sets, respectively, and the remaining images are used for the OG-G dataset.\nOmniGlot-Grounding (OG-G). OG-G is composed of the remaining 5811 samples, apart from the image of the character, each sample also contains a set of images of the strokes that are converted from the sequence of each stroke that composed the character as the ground truth. We use 4311, 750, and 750 samples for train, test, and dev sets.\nShapeNet5. ShapeNet5 consisted of all 20938 shapes of 5 categories (bed, chair, table, sofa, lamp), suggested by Achlioptas et al. (2019) that they are composed of shared basic elements, from ShapeNetCore v2 (Chang et al., 2015), an updated version of the core ShapeNet dataset. There are\nmany ways to represent 3D data including point clouds, meshes, voxels, Signed Distance Fields (SDF), and octrees. In order to make 3D shapes directly applied to the same architecture with other datasets, we choose to voxelize the shapes; thus, we can handle them by simply replacing 2D convolutions with 3D. We use the binvox library (Min, 2004 - 2019; Nooruddin & Turk, 2003) to voxelize shapes into solid voxels (i.e., the interiors of the shapes are filled). We applied 15938, 2500, and 2500 samples for train, test, and dev sets.\nShapeGlot Transfer Learning. This transfer learning dataset consists of a pre-training set and fine-tuning sets. The pretraining set covers 11470 samples from 5 categories (chair, bench, cabinet, bookshelf, bathtub) that we regard as having similar basic compositions. Four fine-tuning sets correspond to four categories that share similar basic elements with the pretraining set: bed (233), lamp (532), sofa (550), and table (580), number of samples for each category is provided in brackets. We voxelize the samples the same way with ShapeNet5."
        },
        {
            "heading": "C HYPER-PARAMETER SEARCH",
            "text": "We use Weights & Biases Sweep (Biewald, 2020) to perform hyperparameter searches for our method. Table 3 shows the distribution of the empirically significant parameters in our hyper-param search. \u03b1overlap and \u03b1resources are \u03b11 and \u03b12 in Equation 4. U(a, b; s) is a discrete uniform distribution with a step size of s between a and b, while U(a, b) is a uniform distribution between a and b. We use a combination of random search and grid search to explore the search space, with random search used to identify good traces, and then, with the help of the visualization tool provided by Sweep, we get a smaller search space for a finer grid search. The final good range determined by the random search can vary for different experiments; however, the distribution given by Table 3 provides the common initial range that empirically likely covers the optimal sets to explore.\nWe conducted our experiments on our internal clusters, and a major workload has the following configuration: six Quadro RTX 5000 GPUs and one Quadro RTX 8000 GPU, along with an Intel (R) Xeon (R) Silver 4214R CPU @ 2.40GHz and 386 GB RAM. We employed PyTorch Lightning (Falcon & The PyTorch Lightning team, 2019) for parallel training."
        },
        {
            "heading": "D LATENT SPACE VISUALIZATION",
            "text": ""
        },
        {
            "heading": "E GT LOSS DETAILS",
            "text": "There are some important tricks to apply apart from only using the GT loss to avoid some unwanted outputs as shown in Figure 7. The weak-overlapping is a tricky way learned by the model to bypass the mechanisms. It means that each player produces a weak copy of the input xj = cx where 0 < c < 1, it fully meets the requirements of the GT loss. We solve it with a soft step function trick, which computing the overlapping loss by max(0, \u2211NP j Step(xj ; thS)\u2212Step(x; thS)), where Step is a point-wise step function that assigns 1 to a position if the value is above a threshold thS and 0 otherwise. A soft step function can be implemented with a Heaviside function. With a step function, the overlapping loss becomes more sensitive, and a small value in a position will be regarded as an occupation. And the threshold controls the sensitivity. Thus, the model cannot cheat with weak overlapping and be more careful when assigning values.\nAnother undesirable situation is chessboard overlapping, where players avoid overlap by outputting pixels with intervals. This is typically caused by using a deconvolutional upsampling (Odena et al., 2016) and can be solved by replacing it with bilinear interpolation. The non-interpretable shape occurs in a base model without a predicate head. By applying predicate clustering and other techniques that encourage interpretability, such cases can be largely reduced. In the instance separation case, the model learns an unwanted good case that gives a segmentation of instances. This case is caused by the given dictionary size being too large, thus the model can memorize all shapes."
        },
        {
            "heading": "F ABLATION STUDY ON MODEL ARCHITECTURE",
            "text": "We perform an ablation study of the proposed predicate clustering method, PPO tuning, and optimizations in LineWorld and OmniGlot datasets. The results are listed in Table 4.\n\u201cBase\u201d means the model trained with only decomposition loss. Without clustering, the model gives an arbitrary decomposition that reconstructs the input while meeting the game mechanisms, which generate parts with diverse near-random shapes, thus having low CIG and SP.\n\u201c+ Cluster\u201d means adding a clustering loss to the base model. With cluster loss, the model does dictionary learning, which significantly improves the CIG and SP, since the model learns to find common elements to represent the data. It also shows that learning an efficient dictionary itself also results in simpler and more natural shapes. Although not sufficient to learn the human interpretable shapes.\n\u201c+ HOL\u201d adds the higher-order predicate optimization to the model with clustering, and it shows marginal improvement, which may be due to the low complexity of the shapes in our datasets, which do not contain too complex relationships that need to be depicted with higher-order predicates. And a better way of representing higher-order predicates may also lead to better results.\n\u201c+ PPO\u201d adds a PPO tuning with heuristic reward to the model with clustering, and it clearly improves the SP in both datasets due to the introduction of an explicit bias that guides the model to learn more natural shapes, which shows the importance of feedback and environmental interactions.\n\u201c+ Attn.\u201d adds attention layers to the model with clustering; it also gives marginal improvement which may be due to the relatively limited complexity of our dataset. Moreover, as discussed earlier, a better case for cross-attention is multimodal learning. And we do not test self-attention, which we regard should be applied to larger datasets.\n\u201c+ LDM\u201d uses a latent diffusion in the model with clustering, the result shows that the use of LDM has only limited harm to the model performance; compared to efficiency improvement, the downside is quite acceptable.\nIn conclusion, the findings demonstrate that dictionary learning is a critical factor in learning transitional representation, while PPO can effectively enhance the representation. HOL and attention offer minor improvements in our datasets; however, they may be more beneficial in a more complex dataset and with better higher-order representation. LDM can improve model efficiency with minimal impact on performance, which is essential for scaling to larger inputs."
        },
        {
            "heading": "G HIERARCHICAL CONCEPTS AND RELATIONS",
            "text": "We give more insights on how relation clustering or 2-ary predicates work here. In Figure 8, we show examples from the three datasets, we query a pair of red and blue parts in the learned 2-ary predicate dictionary by comparing the distance of their representation obtained by the mapper and query layer and the 2-ary predicate prototypes, which gives a confidence distribution over each predicate, or a relative distance from them. One can also use an absolute distance with a threshold to obtain a better\nOut-of-Distribution (OOD) detection ability for not only relation prediction but also other dictionaries and also better robustness (Yang et al., 2018), for simplicity, we keep a relative distance in our work.\nOn the left side of the figure, we visualized the distribution in two LineWorld samples, where a more common pattern \u201cT\u201d is close to predicate 3, while another overlapping pattern that is not allowed in the dataset is remote to all prototypes. On the right side, we provide the confidence of the predicate with the largest confidence for each sample; we can see that the more common sample shows higher confidence than a more random one which is hard to be concluded as any categories.\nWhile the 1-ary predicates learn visual primitives, the 2-ary predicates implicitly learn common combinations, and the higher-ary and order predicates can be seen as learning subparts. It shows that our method can implicitly learn the hierarchy concepts (Lake et al., 2015)."
        },
        {
            "heading": "H SYMBOL GROUNDING",
            "text": "Here, we show how symbol grounding works that grounds an image to a set of predefined predicates. Figure 9 shows an example of grounding an LW-G sample to the predicates defined by the babyARC engine. The model first decomposes the image into parts, which are the lines in different positions with different lengths, as 1-ary predicates. Then predict the relationships between the pairs of the parts by the relation predictor which is implemented as 2-ary predicate prototypes.\nThis gives a complete graph GS with 1-ary concepts as nodes and 2-ary relationships as edges. Suppose the ground-truth graph is Ggt. Since the number of players is assumed to be larger than the number of ground-truth concepts, the model actually gives a complete graph with outputs from all players as nodes GP , and we require Ggt to be a sub-graph of GP and the nodes that are included in GP but not Ggt to be empty. In training time, we extract the best matching subgraph of GP to Ggt as GS and compute the loss. In inference time, the prediction is the complete graph of the non-empty nodes. To extend our method to a non-complete graph, we may simply introduce a threshold to relation prediction, or other OOD methods including using the absolute distance in prototype classifier as discussed in G, or simply introduce a category for empty relation."
        },
        {
            "heading": "I ANALYSIS OF CLUSTERING INFORMATION GAIN",
            "text": "To calculate the CIG of a model, we first decompose each sample x in the test set into parts \u2212\u2192x , resulting in the set \u2212\u2192 X of all parts. We then generate a set of parts \u2212\u2192 X rand randomly sampling parts from samples in the test set, using a mask with a normal distribution in each position. We then compute the MCE of both sets to obtain MCEmodel and MCErand, which allows us to calculate the CIG. To do this, we reduce the dimension of the parts and then run a K-Means clustering.\nThe influence of dimension reduction on fairness can be seen by comparing three typical dimensionreduction techniques: PCA, Auto-Encoder (AE), and a pre-trained CNN, VGG19. Table 5 provides a comparison of the three methods. The \u201cReference\u201d in LineWorld is composed of common elements\nsuch as lines and two vertical lines in the mid or edge (i.e. \u201cL\u201d and \u201cT\u201d shapes) with different lengths, colors, positions, and directions; in OmniGlot, the \u201cReference\u201d is the set of the ground-truth strokes. To test the stability of AE-based CIG, we trained AE ten times in each dataset and calculated the average results and variance. The results show that the AE-based CIG is stable with a variance of approximately 2.85% across different runs.\nWe utilized AE as the dimension reduction technique in our experiments because PCA was not satisfactory. Since the pre-trained VGG can only be used for image data and AE yields a similar outcome to VGG, we sought a method that could provide a reasonable differentiation while being able to be applied to all types of data. We train a shared AE when comparing different methods. For each dataset, we train an AE on that dataset and then use it to calculate and compare the CIG of different methods trained on the same dataset. We run K-Means with a fixed K. For OmniGlot and ShapeNet5, we set K to 32, and for LineWorld, we select 10, which is the number of components that we consider suitable for constructing the dataset."
        },
        {
            "heading": "J ANALYSIS OF SHAPE SCORE",
            "text": "We compare three ways to smooth the contour, minimal convex hull, Ramer\u2013Douglas\u2013Peucker (RDP) algorithm, and B-spline interpolation in LineWorld and OmniGlot dataset, the result is listed in Table 6, where the three methods are marked as \u201cHull\u201d, \u201cRDP\u201d and \u201cSpline\u201d, respectively, \u201cReference\u201d is the same as Table 5. \u201cRandom\u201d is obtained with the random sampling method to calculate the CIG.\nIn our experiments, we apply RDP as the default smooth method, since the convex hull is too strict and prefers straight or round shapes, B-Spline and RDP give similar results, but RDP gives slightly better differentiation. We further visualize the scores under different cases and different smooth methods in Figure 10, we can see that RDP gives a smoother shape that fits the original shape better."
        },
        {
            "heading": "K EXAMPLES OF HUMAN EVALUATION CRITERIA",
            "text": "As shown on the right. In each image, different strokes are marked with different colors, the same color means one stroke. There are four options for human evaluators to choose from, as follows, depending on whether they can redraw the character with given strokes.\n\u2022 Non-stroke: They are not strokes at all; it is impossible to draw the character with them.\n\u2022 Unnatural: Can draw with these strokes, but unnatural or uncomfortable\n\u2022 Acceptable: The strokes are not ideal enough, but not that unnatural.\n\u2022 Good: The strokes are close to those used by humans.\nDetailed instructions can be found in the Supplementary Material."
        },
        {
            "heading": "L LIMITATIONS AND BROADER IMPACTS",
            "text": "We discuss the limitations of the TDL framework in Section L.1 and its broader impacts, as well as future directions in Section L.2.\nL.1 LIMITATIONS\nData Insufficiency. TDL identifies compositional and reusable predicates through multi-ary clustering on the decomposed parts. Therefore, sufficient samples are required to form clusters, and TDL will not work when not provided with enough samples. For real-life data, more samples are needed to form robust clusters considering the noise. In contrast, humans are able to find compositionality in a few-shot manner and have a high tolerance to the noise, by extrapolating the prior knowledge. This is a mystery that TDL has yet to uncover.\nNon-linear Composition. We assume that the input x is linearly composed of parts. This simplifies the computation of the reconstruction error, as opposed to non-linear cases, where the reconstructed input needs to be obtained through a composition function that takes the set of compositions {xi}NPi=1 as input and produces the reconstructed input x\u0303. It also makes it easier to represent combinations of compositions, since they can be simply added together. However, this may limit the representation power of the model, as viewing an image as a linear combination of parts may not capture the true generation process of the input. For example, in the real world, an image is the projection of a 3D world, so the parts should be 3D, and should be reconstructed like a rendering process. Additionally, the linear assumption may not hold for other domains that also contain compositionality, such as language, audio, trajectories, etc.\nCommonsense and Reasoning. Some predicates can only be discovered or grounded when given a certain context. This context can be the environment, the cultural context, or common sense. Humans can also infer the missing context or deduce additional information from the input. For instance, when presented with a picture of a cat and an elephant, we can use our common sense of their sizes and the knowledge of perspective to compare their distance to us based on the size shown in the picture. This process requires a common sense of the size of the elephant and cat and reasoning ability using knowledge of perspective. However, as a representation framework, such predicates cannot be expected from TDL, as it has no common sense or a capacity to reason.\nL.2 BROADER IMPACTS AND FUTURE WORK\nGeneral Transitional Representation. The TDL can be extended beyond vision, as compositionality is present in many areas, not just vision. Vision is an intuitive case for us to gain a better understanding of compositionality in high-dimensional data. The TDL looks for compositional and reusable elements or combinations as predicates from the data by treating the sample as a bag of words and the dataset as a corpus. For instance, in robotics, a trajectory is composed of reusable actions, and certain combinations of actions are known as skills. A decomposition model can be trained to suggest potential decompositions of trajectories into actions using current predicate dictionaries, and then refined through clustering the decomposed actions and combinations. By defining the decomposition models, the TDL can be used to learn the transitional representation in different domains. Another potential future direction is to learn a general cross-domain transitional representation by clustering the embeddings of multi-modal data from different decomposition models.\nNeural-Symbolic Pre-training. As an unsupervised representation learning framework, it is promising for TDL to scale up for large-scale pertaining. The foundation models that learn neuralsymbolic transitional representations can provide better interpretability due to the embedded structural information as well as the prototype predicate dictionaries. Furthermore, the learned representation is a pre-digging of the compositional information of the data in the pre-training, which can be reused in unseen tasks, and thus theoretically perform better for downstream applications.\nHidden Logical Rules. With the logical sentences of entities and relations grounded on the predicates learned by TDL, the model should be able to reason when the rules are provided. Therefore, a significant future work is learning hidden logical rules and reasoning with them in an unsupervised manner. The domain related to unsupervised learning of rules is association rule learning (Agrawal et al., 1993), which discovers rules such asX \u21d2 Y (whereX and Y are sets of items) from the dataset. In the context of TDL, the items can be predicate prototypes. Instead of the joint probability P (X,Y ), a conditional probability P (Y X) can be used to model such rules in the likelihood computation in Equation 3."
        },
        {
            "heading": "M QUALITATIVE COMPARISONS ON OMNIGLOT",
            "text": "We randomly selected 260 characters from the OmniGlot test set for comparison and qualitative analysis between our model and the baseline models. Our model, which uses a dictionary learning paradigm, can learn concepts such as lines and curves that are similar to human strokes. Each sample is colored differently; however, the colors may blend together if a pixel is associated with different parts with varying levels of confidence. The less color mixing, the higher the confidence.\nM.1 OURS\nM.2 DFF\nM.3 SCOPS\nM.4 UPD"
        }
    ],
    "title": "TIONS WITH TRANSITIONAL DICTIONARY LEARNING",
    "year": 2024
}