{
    "abstractText": "Conformal prediction (CP) for regression can be challenging, especially when the output distribution is heteroscedastic, multimodal, or skewed. Some of the issues can be addressed by estimating a distribution over the output, but in reality, such approaches can be sensitive to estimation error and yield unstable intervals. Here, we circumvent the challenges by converting regression to a classification problem and then use CP for classification to obtain CP sets for regression. To preserve the ordering of the continuous-output space, we design a new loss function and present necessary modifications to the CP classification techniques. Empirical results on many benchmarks shows that this simple approach gives surprisingly good results on many practical problems.",
    "authors": [],
    "id": "SP:44a1c5b10d374867ee9d18a46f55f0a19891e207",
    "references": [
        {
            "authors": [
                "Victor Chernozhukov",
                "Kaspar W\u00fcthrich",
                "Yinchu Zhu"
            ],
            "title": "Distributional conformal prediction",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Joel W Cohen",
                "Steven B Cohen",
                "Jessica S Banthin"
            ],
            "title": "The medical expenditure panel survey: a national information resource to support healthcare cost research and inform policy and practice",
            "year": 2009
        },
        {
            "authors": [
                "Isidro Cort\u00e9s-Ciriano",
                "Andreas Bender"
            ],
            "title": "Concepts and applications of conformal prediction in computational drug discovery",
            "venue": "Artificial Intelligence in Drug Discovery,",
            "year": 2020
        },
        {
            "authors": [
                "Raul Diaz",
                "Amit Marathe"
            ],
            "title": "Soft labels for ordinal regression",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Edwin Fong",
                "Chris C Holmes"
            ],
            "title": "Conformal Bayesian computation",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Huan Fu",
                "Mingming Gong",
                "Chaohui Wang",
                "Kayhan Batmanghelich",
                "Dacheng Tao"
            ],
            "title": "Deep ordinal regression network for monocular depth estimation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Etash Kumar Guha",
                "Eugene Ndiaye",
                "Xiaoming Huo"
            ],
            "title": "Conformalization of sparse generalized linear models",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Antonis Lambrou",
                "Harris Papadopoulos",
                "Alex Gammerman"
            ],
            "title": "Evolutionary conformal prediction for breast cancer diagnosis",
            "venue": "In 2009 9th international conference on information technology and applications in biomedicine,",
            "year": 2009
        },
        {
            "authors": [
                "Antonis Lambrou",
                "Harris Papadopoulos",
                "Efthyvoulos Kyriacou",
                "Constantinos S Pattichis",
                "Marios S Pattichis",
                "Alexander Gammerman",
                "Andrew Nicolaides"
            ],
            "title": "Assessment of stroke risk based on morphological ultrasound image analysis with conformal prediction",
            "venue": "In Artificial Intelligence Applications and Innovations: 6th IFIP WG 12.5 International Conference,",
            "year": 2010
        },
        {
            "authors": [
                "Jing Lei",
                "Larry Wasserman"
            ],
            "title": "Distribution-free prediction bands for non-parametric regression",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2014
        },
        {
            "authors": [
                "Jing Lei",
                "James Robins",
                "Larry Wasserman"
            ],
            "title": "Distribution-free prediction sets",
            "venue": "Journal of the American Statistical Association,",
            "year": 2013
        },
        {
            "authors": [
                "Jing Lei",
                "Max G\u2019Sell",
                "Alessandro Rinaldo",
                "Ryan J Tibshirani",
                "Larry Wasserman"
            ],
            "title": "Distributionfree predictive inference for regression",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "Charles Lu",
                "Anastasios N Angelopoulos",
                "Stuart Pomerantz"
            ],
            "title": "Improving trustworthiness of ai disease severity rating in medical imaging with ordinal conformal prediction sets",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2022
        },
        {
            "authors": [
                "Harris Papadopoulos",
                "Kostas Proedrou",
                "Volodya Vovk",
                "Alex Gammerman"
            ],
            "title": "Inductive confidence machines for regression",
            "venue": "In Machine Learning: ECML 2002: 13th European Conference on Machine Learning Helsinki,",
            "year": 2002
        },
        {
            "authors": [
                "Y. Romano",
                "E. Patterson",
                "E.J. Candes"
            ],
            "title": "Conformalized quantile regression",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Rasmus Rothe",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Dex: Deep expectation of apparent age from a single image",
            "venue": "In IEEE International Conference on Computer Vision Workshops (ICCVW),",
            "year": 2015
        },
        {
            "authors": [
                "Jonas Rothfuss",
                "Fabio Ferreira",
                "Simon Walther",
                "Maxim Ulrich"
            ],
            "title": "Under review as a conference paper at ICLR",
            "year": 2024
        },
        {
            "authors": [
                "Martin J Wainwright",
                "Michael I Jordan"
            ],
            "title": "Algorithmic learning in a random world",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Quantifying and estimating the uncertainty of machine-learning models is an important task for many problems, especially mission-critical ones requiring trustworthy and reliable predictions. Conformal Prediction (CP) (Vovk et al., 2005) has recently gained popularity and has been used successfully in applications such as breast cancer detection (Lambrou et al., 2009), stroke risk prediction (Lambrou et al., 2010), and drug discovery (Corte\u0301s-Ciriano & Bender, 2020). Under mild conditions, CP techniques aim to construct a prediction set that, for given test inputs, is guaranteed to contain the true (unknown) output with high probability. The set is built using a conformity score, which, roughly speaking, indicates the similarity between a new test example and the training examples. The conformal set merely gathers examples that have large conformity scores. Despite its popularity, CP for regression can be challenging, especially when the output distribution is heteroscedastic, multimodal, or skewed (Lei & Wasserman, 2014). The main challenge lies in the design of the conformity score. It is common to use a simple choice for score functions such as distance to mean regressor, but such choices may ignore the subtle features of the shape of the output distribution. For instance, this could lead to symmetric intervals or ignoring the heteroscedasticity. In theory, it is better to estimate the (conditional) distribution over the output, for example, by using kernel density estimation and directly using it to build a confidence interval. However, such estimation approaches are also challenging, and estimates can be sensitive to the choice of kernel and hyperparameters, which can yield unstable results.\nWe circumvent the challenges by exploiting the existing CP techniques for classification. We proceed by first converting regression to a classification problem and then using CP techniques for classification to obtain a conformal set. Regression-as-classification approaches are popular for various applications in computer vision and have led to more accurate training than only-regression training (Stewart et al., 2023; Zhang et al., 2016; Fu et al., 2018; Rothe et al., 2015; Van Den Oord et al., 2016; Diaz & Marathe, 2019). We leverage them to construct a distribution-based conformal set that can flexibly capture the shape of the output distribution while preserving the simplicity and efficiency of CP for classification. First, we discretize the output space into bins, treating each bin as a distinct class. Second, to preserve the ordering of the continuous output space, we design an alternative loss function that penalizes the density on bins far from the true output value but also facilitates variability by using an entropy regularization. The loss design is similar in spirit to Weigend & Srivastava (1995); Diaz & Marathe (2019). The resulting method can adapt to heteroscedasticity, bimodality, or both in the label distribution. We verify this on synthetic and real datasets where we achieve the shortest intervals compared to other CP baselines. See examples in Figure 1."
        },
        {
            "heading": "2 BACKGROUND ON CONFORMAL PREDICTION",
            "text": "Given a new input xnew, CP techniques aim to construct a set that contains the true but unknown output ynew with high probability. Assuming that a pair of input-output variables (x, y) has a joint density p(x, y) and a conditional density p(y | x), oracle prediction sets (with joint and conditional coverage) for the output y can be constructed as\n{z \u2208 R : p(x, z) \u2265 \u03c4\u03b1} or {z \u2208 R : p(z | x) \u2265 \u03c4\u03b1,x}, (1) where the thresholds \u03c4\u03b1 and \u03c4\u03b1,x are selected to ensure that the corresponding sets have a probability mass that meets or exceeds prescribed confidence level 1 \u2212 \u03b1 \u2208 (0, 1). As the ground-truth distribution is unknown, we rely solely on estimating these uncertainty sets using the density estimators p\u0302(x, y) and p\u0302(y | x). The latter can be inaccurate due to numerous sources of errors such as model misspecification, small sample size, high optimization errors during training, and overfitting. Without a stronger distribution assumption, the finite-sample guarantee is typically not upheld.\nConformal Prediction has arisen as a method for yielding sets that do hold finite-sample guarantees. Given a partially observed instance (xnew, ynew) where ynew is unknown, Conformal Prediction (CP) (Vovk et al., 2005) constructs a set of values that contains ynew with high probability without knowing the underlying data distribution. Under conformal prediction, this property is guaranteed under the mild assumption that the data satisfies exchangeability. The set is called the conformal set and is built using a conformity score, denoted by \u03c3(x, y), which measures how appropriate an output value is for a given input example. There are many ways to build the conformity score, but they all involve splitting the data into a training setDtr and a calibration setDcal. Often, a prediction model \u00b5tr(x) is built using the training set, and then a conformity score is obtained using this model along with the calibration set (we will shortly give an example). The conformal set merely gathers the points with larger conformity scores:\n{z \u2208 R : \u03c3(xnew, z) \u2265 Q1\u2212\u03b1(Dcal)}, where Q1\u2212\u03b1(Dcal) is the (1 \u2212 \u03b1)-quantile of the conformity scores on the calibration data. This set provably contains ynew with probability larger than 1\u2212 \u03b1 for any finite sample size and without assumption on the ground-truth distribution.\nThere are many design choices for this conformity score. For example, one can choose a prediction model \u00b5tr(x) as an estimate of the conditional expectation and measure conformity as the absolute value of the residual, i.e., \u03c3(x, y) = \u2212|y \u2212 \u00b5tr(x)|. The corresponding conformal set is a single interval centered around the prediction \u00b5tr(x) and of constant length Q1\u2212\u03b1(Dcal) for any example xnew, without taking into account its variability. However, in situations where the underlying data distribution demonstrates skewness or heteroscedasticity, we may desire a more flexible conformity\nscore. We wish to generate an approach to approximate this density function that is versatile in the distributions it can represent and avoids the difficulties seen in prior methods. We explore established density estimation in Classification Conformal Prediction that are already performing effectively.\nRelated Works Since their introduction, a lot of work has been done to improve the set of conformal predictions. As simple score function, distance to conditional mean ie \u03c3(x, y) = |y \u2212 \u00b5(x)| where \u00b5(x) is an estimate of E(y | x) was prominently used (Papadopoulos et al., 2002; Lei et al., 2018; Guha et al., 2023). Instead, (Romano et al., 2019) suggests estimating a conditional quantile instead and a conformity score function based on the distance from a trained quantile regressor, i.e. \u03c3(x, y) = max(\u00b5\u03b1/2(x)\u2212 y, y\u2212\u00b51\u2212\u03b1/2(x)) where \u00b5\u03b1 are the \u03b1-th quantile regressors. Within the literature, our strategy is more closely related to the distribution-based methods (Lei et al., 2013). Following a similar line of work (Chernozhukov et al., 2021) argued that the conformal quantile regression score function might be less adaptive since the distance of the quantile behaves similarly to the distance of the mean estimate. Instead, they suggested estimating the cumulative (conditional) distribution function and directly outputting {y : F (x, y) \u2208 [\u03b1/2, 1\u2212\u03b1/2]}. However, their method cannot account for bimodality since it can only output a single interval by design. An equally interesting approach for distribution-based prediction sets is based on learning a Bayesian estimator, which, however, may require a well-specified prior and can be computationally expensive (Fong & Holmes, 2021). Variants based on estimating the conditional density of the response using histogram regressors (Sesia & Romano, 2021) could detect a possible asymmetry of the ground-truth distribution. However, our densities are estimated through classification techniques, whereas their densities are learned through a histogram of many regressors. Smoothness over the distribution is encoded in our loss function, whereas they use a post hoc subinterval finding algorithm through linear programming to prevent many disjoint intervals. Our techniques have some overlap with ordinal regression and ordinal classification. Xu et al. (2023) discusses various risk categories (similar to coverage) for ordinal classification, while our work considers different score functions where coverage serves as the loss function. Lu et al. (2022) discuss the adaptation of APS style score functions to accommodate the ordinal structure of classes, which was subsequently utilised in (Sesia & Romano, 2021)."
        },
        {
            "heading": "3 CP VIA REGRESSION-AS-CLASSIFICATION",
            "text": ""
        },
        {
            "heading": "3.1 CLASSIFICATION CONFORMAL PREDICTION",
            "text": "We aim to compute a conformity function that accurately predicts the appropriateness of a label for a specific data point. Given that the distribution function across labels can adopt diverse forms, such as being bimodal, heavy-tailed, or heteroscedastic, our approach must effectively factor in such shapes while upholding its coverage precision. A frequently employed technique involves using the conditional label density as a conformity function, leading to reliable results in the classification context. Typically, practitioners perform conformal prediction for classification with probability estimates from a Softmax neural network that covers K output logits using cross-entropy loss.\nLet us denote the parametrized density\nq\u03b8(\u00b7 | x) = softmax(f\u03b8(x)), where softmax(v)j = exp(vj)\u2211K\nk=1 exp(vk),\nas the outputted discrete probability distribution over the labels of the input x.\nTraditionally, we fit our neural network by minimizing the cross-entropy loss on the training set:\n\u03b8\u0302 \u2208 argmin \u03b8 n\u2211 i=1 KL(\u03b4yi || q\u03b8(\u00b7 | xi)).\nLet us assume that we have trained and acquired a \u03b8\u0302 that has minimized the traditional cross-entropy loss function on the training dataset. A natural conformity score is simply the probability of a label according to the learned conditional distribution, i.e., \u03c3(x, y) = q\u03b8\u0302(y | x). This approach is both straightforward and efficient. The neural network\u2019s flexibility allows it to learn numerous label distributions with adaptivity across examples with less explicit design of specific prior structure."
        },
        {
            "heading": "3.2 REGRESSION TO CLASSIFICATION APPROACH",
            "text": "Naturally, we strive to embody such practicality and effectiveness in regression settings. However, the distribution of labels in the regression scenario is continuous, and learning a continuous distribution directly using a neural network is challenging (Rothfuss et al., 2019). Often, Bayesian or kernel density estimators are employed to estimate this distribution. Other techniques acquire knowledge of this distribution by training numerous regressors and categorizing the regressors, for example, Conformal Histogram Regression (Sesia & Romano, 2021).\nHowever, these methods look different from the conformal prediction approaches for classification. It would be desirable to be able to use similar methods for both classification and regression conformal prediction. One method of unifying classification and regression problems outside the conformal prediction literature is known as Regression-as-Classification. We simply turn a regression problem into a classification problem by binning the range space. Specifically, we generate K bins with K equally spaced numbers covering the interval Y = [ymin, ymax], where ymin (or ymax) is the minimum (or maximum) value of the labels observed in the training set. More explicitly, we define our discretization of the label space as\nY\u0302 = {y\u03021, . . . , y\u0302K} where y\u0302k+1 = y\u0302k + y\u0302K \u2212 y\u03021 K \u2212 1 with y\u03021 = ymin and y\u0302K = ymax.\nThese values y\u0302 \u2208 Y\u0302 form the midpoints for each bin of our discretization. Naturally, the kth bin is all the labels in the range space Y closest to y\u0302k. Intuitively, we can treat each bin as a class. Thus, we have turned a regression problem into a classification problem. This method is simple but has yielded surprising results. Some work has suggested that this form of binning results in more stable training (Stewart et al., 2023) and gives significantly better results for learning conditional expectations. To unify classification and regression conformal prediction, a simple solution is to employ the Classification Conformal Prediction model with discrete labels y\u0303i = argminy\u0302\u2208Y\u0302 |yi\u2212y\u0302|. This will aid in training the neural network with modified labels through cross-entropy loss, resulting in a discrete distribution of q\u03b8(\u00b7 | x), as outlined in the previous section. To compute conformity scores for all labels, we employ linear interpolation from the discrete probability function q\u03b8(\u00b7 | x) to generate the continuous distribution q\u0304\u03b8(\u00b7 | x). Nevertheless, this approach encounters a critical issue when employed for regression."
        },
        {
            "heading": "3.3 DATA FITTING",
            "text": "A critical problem with employing CrossEntropy loss in the classification conformal prediction context is that any structural relationships between classes are disregarded. This is not surprising given that in the classification context, no structure exists between classes, and each class is independent. Therefore, the CrossEntropy loss does not need to differentiate between whether q\u03b8 allocates probable mass far away or close to the actual label. Instead, it only incentivizes the allocation of probable mass on the correct label. However, within the regression setting, despite a multitude of labels, the labels adhere to an ordinal structure. Thus, to enhance the accuracy of labeling, it is imperative to devise a loss function that incentivizes the allocation of probabilistic mass not only to the correct bin but also to the neighboring bins. Formally, given an input and output pair (x, y), our goal is to determine a density estimate q\u03b8(\u00b7 | x) that assigns low (resp. high) probability to points that are far (resp. close) to the true label y, i.e.,\nq\u03b8(y\u0302 | xi) high (resp. small) when the loss \u2113(y\u0302, yi) is small (resp. high).\nHence, a natural desideratum for learning the probability density function q\u03b8 is that their product \u2113(y, y\u0302)q(y\u0302 | x) is small in expectation. We propose to find a distribution q\u03b8 minimizing the loss\nEy\u0302\u223cq(\u00b7|x)[\u2113(y, y\u0302)] = K\u2211\nk=1\n\u2113(y, y\u0302k)q(y\u0302k | x)\nMinimizing this loss in the space of all possible distributions q(\u00b7 |x) is equivalent to minimizing the original loss \u2113(y, \u00b7), where the minimizing distribution is a Dirac delta \u03b4y\u0302\u22c6 at the minimizer y\u0302\u22c6 of \u2113(y, \u00b7). Therefore, we expect an unregularized version of this loss to share similarities with the typical empirical risk minimization on \u2113(y, \u00b7). However, a key difference is that when minimizing in\nAlgorithm 1 Regression to Classication Conformal Prediction (R2CCP). 1: Input:\n\u2022 Dataset Dn = {(x1, y1), . . . , (xn, yn)} and new input xn+1 \u2022 Desired confidence level \u03b1 \u2208 (0, 1)\n2: Hyperparameters: temperature \u03c4 > 0, p > 0, number of bins K > 1 3: Discretize the output space [ymin, ymax] into K equidistant bins with midpoints {y\u03021, . . . , y\u0302K} 4: Randomly split the dataset Dn in training Dtr and calibration Dcal 5: Find a distribution q\u03b8\u0302(\u00b7 |x) by (approximately) optimizing on the training set Dtr\n\u03b8\u0302 \u2208 arg min \u03b8\u2208Rd ntr\u2211 i=1 K\u2211 k=1 |yi \u2212 y\u0302k|pq\u03b8(y\u0302k | xi)\u2212 \u03c4H(q\u03b8(\u00b7 |xi))\nwhere q\u03b8(y\u0302k|x) = softmax(f\u03b8(x))k for a model (e.g., neural net) f\u03b8 : Rd \u2192 RK . 6: S \u2190 { q\u0304\u03b8\u0302(y | x) for (x, y) \u2208 Dcal } # q\u0304\u03b8\u0302(\u00b7 |x) is linear interpolation of softmax probabilities. 7: Q\u03b1(Dcal)\u2190 quantile(S, \u03b1) 8: return Conformal Set \u0393(\u03b1)(xn+1) = {z \u2208 R | q\u0304\u03b8\u0302(z | xn+1) \u2265 Q\u03b1(Dcal)}\na restricted family of distributions (for example, those representable by a neural network with a fixed architecture), the distributional output can represent multi-modal or heavy-tailed label distributions. Minimizing the original loss \u2113(y, \u00b7), one would always be confined to a point estimate."
        },
        {
            "heading": "3.4 ENTROPIC REGULARIZATION",
            "text": "Although the proposed loss function better encodes the connection between bins, it tends towards outputting Dirac distributions, as outlined in the previous paragraph. Overconfidence in our neural network is a commonly reported problem in the literature (Wei et al., 2022). Nevertheless, smoothness has been a traditional requirement in density learning. As such, we rely on a classical entropyregularization technique for learning density estimators (Wainwright & Jordan, 2008). Given the set of density estimators that match the training label distribution well and put high probability mass on the best bins, we prefer the probability distribution that maximizes the entropy since this intuitively takes fewer assumptions on the data distribution structure. Our choice is based on selecting density estimators that effectively match the distribution of the training labels and assign a higher probability to the best bins. Formally, we can calculate the entropy of our probability distribution by using the Shannon entropyH of the produced probability distribution q(\u00b7 |x) as a penalty term as follows:\nH(q\u03b8(\u00b7 |x)) = K\u2211\nk=1\nq\u03b8(y\u0302k|x) log q\u03b8(y\u0302k|x).\nSummary We learn a distribution by minimizing the following expected loss over a given training data Dtr = {(x1, y1), . . . , (xntr , yntr)} of size ntr:\nL(\u03b8) = ntr\u2211 i=1 K\u2211 k=1 \u2113(yi, y\u0302k) q\u03b8(y\u0302k |xi)\u2212 \u03c4H(q\u03b8(\u00b7 |xi)), (2)\nIn particular, we choose \u2113(yi, y\u0302k) as \u2113(yi, y\u0302k) = |yi \u2212 y\u0302k|p where p > 0 is a hyperparameter. This selection functions as a natural distance metric that meets all required objectives. As a result, we can employ a technique similar to the aforementioned Classification Conformal Prediction methods. Initially, we train a Softmax neural network f\u03b8 with K logits, grounded on the loss function Equation (2) on the training dataset. To calculate conformity scores for the calibration set, we utilize the linearly interpolated \u03c3(x, y) = q\u0304\u03b8(y | x) for a specific data point (x, y). Namely, for any y between y\u0302k and y\u0302k+1, q\u0304\u03b8(y | x) is defined as q\u0304\u03b8(y | x) = (y\u0302k+1\u2212y)q\u03b8(y\u0302k|x)y\u0302k+1\u2212y\u0302k + (y\u2212y\u0302k)q\u03b8(y\u0302k+1|x) y\u0302k+1\u2212y\u0302k . Complete details of this procedure are outlined in Algorithm 1."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We investigate the empirical behavior of our R2CCP (Regression-to-Classification Conformal Prediction) method, which we have explained in detail in Algorithm 1. We have three sets of experiments. The first one is described in Section 4.1 and presents empirical evidence of the algorithm\u2019s ability to produce narrower intervals by utilizing label density characteristics, including heteroscedasticity, bimodality, or a combination. Section 4.2 demonstrates the effectiveness of our algorithm on synthetic and real data by comparing it with various benchmarks from the Conformal Prediction literature in terms of length and coverage. Section 4.3 evaluates the effect of different loss functions on the final learned densities and their impact on the intervals produced. To note, all experiments were run over 5 different seeds at a coverage level of 90%, and the standard error over the experiments is reported in the subscript. We do not tune the hyperparameters and keep values of K = 50, p = .5, and \u03c4 = 0.2 constant across all experiments. For all experiments, we report length, meaning the length of all the sets predicted, and coverage, the percent of instances where the true label is contained in the predicted intervals."
        },
        {
            "heading": "4.1 SPECIFIC CHARACTERISTICS OF LABEL DISTRIBUTION",
            "text": "Heteroscedascity We generate a toy dataset where the input is one-dimensional. It contains samples from the following distribution: y \u223c N (0, |x|). The label distribution is heteroscedastic, meaning the variance of the labels changes as the input changes. In traditional Conformal Prediction literature, many existing algorithms fail to capture heteroscedasticity, resulting in wide intervals for inputs x where the label distribution of y has low variance. However, our learned algorithm can directly learn this relation and adjust the outputted probability distribution accordingly. Thus, we see that the lengths of the intervals will increase as the variance of the label distribution increases, which is desirable. We see this relation in Figure 1a. Moreover, we also use the dataset generated from (Lei & Wasserman, 2014) as discussed in Appendix A. This dataset exhibits heteroskedascity and bimodality as X passes \u22120.5. We see that our learned method can adjust intervals accordingly to maintain coverage and length for all X . We plot this in Figure 1b. We plot how the intervals (grey) change as the data distribution (black) changes. As the variance of the labels increases as x increases, the produced intervals adaptively get wider, taking advantage of the heteroscedasticity.\nBimodality We showcase our algorithm\u2019s capability to address labels with a bimodal distribution. Our bimodal dataset is generated by repeatedly (1) sampling two sets of random features that are close geometrically and (2) giving one set of features a label of 1 plus some Gaussian noise and giving the other a label of \u22121 plus some Gaussian noise. Therefore, our dataset is comprised of many similar data points with bimodally distributed labels. This bimodal distribution is particularly hard for many existing CP algorithms to solve since it requires outputting two disjoint conformal sets to achieve low length. CQR, for instance, cannot deal with this circumstance and will generate a conformal set that covers the entire range space. However, our method is flexible enough to assign a low probability to labels between \u22121 and 1, and our resulting conformal set will not include these intermediate labels. We see this in Figure 3d. In Figure 3d, our outputted probability distribution has two modes around labels \u22121 and 1 and assigns a low probability value to the valley between the modes."
        },
        {
            "heading": "4.2 COMPARISON TO OTHER CONFORMAL PREDICTION ALGORITHMS",
            "text": "The crucial criteria for assessing a Conformal Prediction algorithm consist of (1) coverage, representing the percentage of generated conformal sets that incorporate accurate labels, and (2) the length of the generated conformal sets. Our baseline techniques consist of the Kernel Density Estimator (KDE) as proposed by Lei & Wasserman (2014), alongside a conformity score shown by the estimated probability density. Furthermore, we take into consideration Fong & Holmes (2021) (CB) , which employs the likelihood of a posterior distribution in their conformity function, and\nSesia & Romano (2021) (CHR) , which uses quantile regressors on every bin of a histogram density estimator. We have included the Conformal Quantile Regression as described by Romano et al. (2019) (CQR) , which employs conformity based on the labels\u2019 distance from quantile regressors. Moreover, we had the Distributional Conformal Prediction which uses a Cumulative Distribution\nFunction to form its intervals from Chernozhukov et al. (2021) (DCP) . Additionally, we have used the Lasso Conformal Predictor with a distance to mean regressors, which is the most straightforward option (LASSO) . We use synthetic data exhibiting bimodally distributed and log-normally distributed to illustrate particular weaknesses of existing methods. We use real datasets also in Romano et al. (2019). Specifically, these are several datasets from the UCI Machine Learning repository (Bio, Blog, Concrete, Community, Energy, Forest, Stock, Cancer, Solar, Parkinsons, Pendulum) (Nottingham et al., 2023) and the medical expenditure panel survey number 19\u201321 (MEPS-19\u201321) (Cohen et al., 2009). These regression datasets are commonly used to benchmark regression models. Our approach yields tighter intervals on real datasets than some of the strongest baselines.\nResults We report lengths and coverages results in Table 1 respectively. We added figures depicting example probability distributions on these datasets in Figure 3 in Appendix D. The intervals produced by our method are the shortest over 10 of the 16 datasets. From Figure 3, we see that our method can learn many different shaped distributions well, which accounts for this significant improvement in intervals. Overall, the Kernel Density Estimation and our method can accurately predict the best intervals on datasets where the connection between the data and feature is simple, such as Bimodal or Log-Normal. While the Kernel Density Estimator can fail when the labels are complexly related to inputs, no such connection exists in these datasets since the labels are independent of the features. Thus, the Kernel Density Method\u2019s simplicity allows it to learn the label density quickly. Our method also seems to handle the case where there is no connection between the data and the labels, as seen in Figure 3d in Appendix D. Moreover, on datasets where the label density is smooth and close to the Laplace prior, such as on Figure 3c and Figure 3i, Conformal Bayes and our method can accurately learn this distribution since both methods can output smooth distributions. Moreover, on very sharp datasets such as on Concrete in Figure 3j and Energy in Figure 3h, both our method and CQR can capture the sharp and unnoisy distribution needed to achieve strong length. Moreover, on complex distributions such as in Pendulum in Figure 3e and Bio in Figure 3f, we see that both CHR and our method have the flexibility to portray complex distributions resulting in the most accurate intervals. Thus, the flexibility of our algorithm to smoothly learn sharp, wide, complex, and simple conditional label densities results in our method achieving the best length most consistently over the entire dataset."
        },
        {
            "heading": "4.3 ABLATION STUDIES",
            "text": "Our loss function consists of an error and entropy terms. The error term penalizes distributions that put weight far away from the true label, whereas the entropy term acts as a regularizing term in the probability distribution space. We will do ablations on both the error and entropy terms to illustrate the importance of each part. For the error term, there are several notable alternatives that a practitioner may use. An alternative is the log maximum likelihood or cross-entropy formulations\nof the error term, which we denote as\nLMLE(\u03b8) = \u2212 N\u2211 i=1 log(q\u03b8(y\u0303i | xi)),\nwhere we remind the reader that y\u0303i = argminy\u0302\u2208Y\u0302 |y\u0302 \u2212 yi|. We note that both MLE and CE are equivalent formulations in this setting. These two are standard error terms often used in practice. We will train our models with this loss function over all datasets to see how the change of error term affects the intervals\u2019 length and the learned density. We will show that our chosen error term is better for producing optimal intervals empirically. Moreover, we will also demonstrate the importance of our entropy term. We do this by retraining our models with the entropy part omitted. We also test by combining the MLE error term with entropy regularization. Therefore, we will perform 4 different ablation experiments on the loss function by retraining on different variations of the loss functions and reporting the final length and coverage generated. Overall, the four loss functions we will use are the original loss function L, the original loss function without entropy LNE, the MLE loss function LMLE, and the MLE loss function with entropy added LMLE + E. We will demonstrate that our chosen loss function delivers the best results across all the loss functions. We report our results in Table 2.\nResults We see that our chosen loss function achieves the best length across most loss functions in Table 2. The only datasets where our chosen loss function does not achieve the best length are Energy, Solar, Bimodal, and Log-Normal. We now discuss the individual differences between our loss function and each variant loss function. When comparing our loss L with the variant without\nentropy LNE, we see the lengths constantly increase except for the bimodal distribution. Without an entropy regularizing term, the outputted probability distributions are not smooth, placing much mass on a single data point. This overconfidence results in the \u03b1th quantile of the low probability of the true label as in Figure 2. When looking at the differences between our loss function when changing the error term to Maximum Log Likelihood, we see similar overconfidence. While MLE loss works on several datasets, such as Energy, Solar, Bimodal, and Log-Normal, it also performs poorly on other datasets. MLE works well for datasets with less simple label distributions but fails otherwise due to similar overconfidence. Even when adding entropy as a regularizer to the MLE loss as in LMLE + E, we see that the addition of entropy does not improve the length. Since MLE loss does not treat nearby bins differently than far away bins, regularization decreases the overconfidence but does so uniformly across all bins. The result is a roughly uniform distribution with a single spike. This does not utilize the structure of regression where bins near the best bin are preferable. Thus, prioritizing nearby bins is crucial for achieving strong length. Moreover, when adding entropy regularization to our error term, we do not see a uniform increase in probability across all bins but instead a smoothening of a direct. This connection between entropy regularization and our distancebased loss function appears to be a powerful synergy per the intervals produced. Thus, the error term utilizing the regression structure by prioritizing nearby bins and the regularizing entropy term preventing overconfidence seem crucial for producing good intervals.\nAnother important insight is that the datasets where other loss functions excel are the same ones where other CP methods perform better than ours. In particular, on the Bimodal, Log-Normal, Parkinson\u2019s, and Solar datasets, our CP method is not superior, and other loss functions outperform it. This indicates that the MLE\u2019s sharp distributions result in greater accuracy on these specific datasets. Hence, adjusting the weighting terms for entropy \u03c4 and smoothing p could enhance the smoothness of the learned distribution and, thereby, the performance of our algorithm across these datasets, leading to the best outcomes. Nevertheless, to maintain fairness, we avoid this approach."
        },
        {
            "heading": "5 LIMITATIONS",
            "text": "There are several limitations to our algorithm. The loss function we choose to optimize needs larger representational power due to increased complexity of the output. Therefore, the neural networks we used in practice to minimize this loss function are larger than the ones used for CQR. Moreover, to achieve strong training, we had to slightly vary the hyperparameters for a dataset depending on its size. For example, larger datasets needed models to be trained with smaller initial learning rate to avoid divergence. This was not needed for Quantie Regressors. If the network does not train well or achieve good training or validation loss, the intervals produced will be suboptimal, so finding a training setup that minimizes the loss effectively is crucial for our algorithm to produce meaningful intervals. Moreover, we do not directly learn the label distribution and our produced probability distributions do not directly mirror that of the ground truth distribution. One reason for this is the use of entropy regularization. While using entropy regularization introduces bias, we found it to be necessary to prevent the neural network from being overconfident on a simple bin in practice."
        },
        {
            "heading": "A DETAILS ON DATASET FROM LEI & WASSERMAN (2014)",
            "text": "For the dataset referenced in Figure 1b, we generate the dataset by sampling many (X,Y ) pairs from the following distribution.\nX \u223c Unif[\u22121.5, 1.5]\nY | X \u223c 1 2 N (f(X)\u2212 g(X), \u03c32(X)) + 1 2 N (f(X) + g(X), \u03c32(X))\nf(X) = (X \u2212 1)2(X + 1), g(X) = 4 \u221a (X + 1/2)I(X \u2265 \u22121/2), \u03c32(X) = 1/4 + |X|\nThis dataset demonstrates bimodality after x passes the threshold of\u22120.5. This dataset was similarly used by the Lei & Wasserman (2014)."
        },
        {
            "heading": "B COVERAGE DATA",
            "text": "We have presented the coverage data for the Conformal Prediction comparison as well as the ablation experiments in Table 3 and Table 4, respectively. Since all methods obey the classical Conformal Prediction framework, we expect coverage at the guaranteed level of 1\u2212\u03b1. This guarantee is indeed what we see across all experiments. This confirms that our coverage guarantee indeed holds in practice."
        },
        {
            "heading": "C MORE EXPERIMENTAL DETAILS",
            "text": "In order to maintain fairness across all baselines, we use the same size neural network for our method across all experiments. Specifically, we discretize the range space into K = 50 points, weight the entropy term by \u03c4 = 0.2, use a 1000 hidden dimension, use 4 layers, use weight decay of 1e \u2212 4, use p = .5, and use AdamW as an optimizer. For most of the experiments, we use learning rate 1e\u22124 and batch size 32. However, for certain datasets, namely the MEPS datasets, we used a larger batch size of 256 to improve training time and used a smaller learning rate to prevent training divergence. We did change any other parameter between all of our runs. For the baselines of CQR and CHR, we use the neural network configurations mentioned in the paper. We found that the parameterizations mentioned by the authors in the papers achieved the best performance and changing the parameterizations weakened their results."
        },
        {
            "heading": "D OUTPUT DISTRIBUTION",
            "text": "Here, we plot the distribution of lengths of all Conformal Prediction methods over all datasets. We also plot example density functions learned by out method, CHR, and KDE on all datasets. We note that KDE\u2019s learned densities seem to be relatively less informative. Moreover, the learned densities from CHR are noisy and not smooth. Moreover, we plot several examples of only our learned probability distribution. We use this when referencing the experiments to demonstrate the different shapes of label distributions from the data.\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0.5 1.0 1.5 2.0 Length\nC B\nWorstBest\n(a) Bimodal Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Length\nC B\nWorstBest\n(b) Bio Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0 1 2 3 4 5 Length\nC B\nWorstBest\n(a) Breast Cancer Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0 1 2 3 4 Length\nC B\nWorstBest\n(b) Community Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0.0 0.5 1.0 1.5 2.0 2.5 Length\nC B\nWorstBest\n(a) Concrete Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0.5 1.0 1.5 2.0 2.5 3.0 Length\nC B\nWorstBest\n(b) Diabetes Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0 1 2 3 4 Length\nC B\nWorstBest\n(a) Energy Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0 1 2 3 4 5 6 7 Length\nC B\nWorstBest\n(b) Forest Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Length\nC B\nWorstBest\n(a) Log Normal Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0 10 20 30 40 Length\nC B\nWorstBest\n(b) MEPS 19 Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0 10 20 30 40 50 60 Length\nC B\nWorstBest\n(a) MEPS 20 Length Distribution\nO ur s K D E\nLa ss o C Q R\nC H\nR\n0 10 20 30 40 50 60 Length\nC B\nWorstBest\n(b) MEPS 21 Length Distribution"
        }
    ],
    "year": 2023
}