{
    "abstractText": "We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the unknown SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE (Concomitant Linear DAG Estimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of noise variances in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seyed Saman Saboksayr"
        },
        {
            "affiliations": [],
            "name": "Gonzalo Mateos"
        },
        {
            "affiliations": [],
            "name": "Mariano Tepper"
        }
    ],
    "id": "SP:993f96e7ea9d37f9e36da7bc8fbb5cdacca82f36",
    "references": [
        {
            "authors": [
                "Raghavendra Addanki",
                "Shiva Kasiviswanathan",
                "Andrew McGregor",
                "Cameron Musco"
            ],
            "title": "Efficient intervention design for causal discovery with latents",
            "venue": "In Proc. Int. Conf. Mach. Learn.,",
            "year": 2020
        },
        {
            "authors": [
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si",
                "R\u00e9ka Albert"
            ],
            "title": "Emergence of scaling in random networks",
            "venue": "Science,",
            "year": 1999
        },
        {
            "authors": [
                "Kevin Bello",
                "Bryon Aragam",
                "Pradeep Ravikumar"
            ],
            "title": "DAGMA: Learning DAGs via M-matrices and a log-determinant acyclicity characterization",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandre Belloni",
                "Victor Chernozhukov",
                "Lie Wang"
            ],
            "title": "Square-root lasso: pivotal recovery of sparse signals via conic programming",
            "year": 2011
        },
        {
            "authors": [
                "Peter J Bickel",
                "Ya\u2019acov Ritov",
                "Alexandre B Tsybakov"
            ],
            "title": "Simultaneous analysis of Lasso and Dantzig selector",
            "venue": "Ann. Stat.,",
            "year": 2009
        },
        {
            "authors": [
                "Philippe Brouillard",
                "S\u00e9bastien Lachapelle",
                "Alexandre Lacoste",
                "Simon Lacoste-Julien",
                "Alexandre Drouin"
            ],
            "title": "Differentiable causal discovery from interventional data",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Peter B\u00fchlmann",
                "Jonas Peters",
                "Jan Ernest"
            ],
            "title": "CAM: Causal additive models, high-dimensional order search and penalized regression",
            "venue": "Ann. Stat.,",
            "year": 2014
        },
        {
            "authors": [
                "George Casella",
                "Roger L Berger"
            ],
            "title": "Statistical Inference",
            "venue": "Cengage Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bertrand Charpentier",
                "Simon Kibler",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Differentiable DAG sampling",
            "venue": "In Proc. Int. Conf. Learn. Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Eunice Yuh-Jie Chen",
                "Yujia Shen",
                "Arthur Choi",
                "Adnan Darwiche"
            ],
            "title": "Learning bayesian networks with ancestral constraints",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2016
        },
        {
            "authors": [
                "David Maxwell Chickering"
            ],
            "title": "Learning Bayesian networks is NP-complete. Learning from Data: Artificial Intelligence and Statistics V",
            "year": 1996
        },
        {
            "authors": [
                "David Maxwell Chickering"
            ],
            "title": "Optimal structure identification with greedy search",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2002
        },
        {
            "authors": [
                "David Maxwell Chickering",
                "David Heckerman",
                "Chris Meek"
            ],
            "title": "Large-sample learning of Bayesian networks is NP-hard",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2004
        },
        {
            "authors": [
                "Chris Cundy",
                "Aditya Grover",
                "Stefano Ermon"
            ],
            "title": "BCD Nets: Scalable variational approaches for Bayesian causal discovery",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2021
        },
        {
            "authors": [
                "Chang Deng",
                "Kevin Bello",
                "Bryon Aragam",
                "Pradeep Kumar Ravikumar"
            ],
            "title": "Optimizing NOTEARS objectives via topological swaps",
            "venue": "In Proc. Int. Conf. Mach. Learn.,",
            "year": 2023
        },
        {
            "authors": [
                "Chang Deng",
                "Kevin Bello",
                "Pradeep Kumar Ravikumar",
                "Bryon Aragam"
            ],
            "title": "Global optimality in bivariate gradient-based DAG learning",
            "venue": "ICML",
            "year": 2023
        },
        {
            "authors": [
                "AmirEmad Ghassami",
                "Alan Yang",
                "Negar Kiyavash",
                "Kun Zhang"
            ],
            "title": "Characterizing distribution equivalence and structure learning for cyclic and acyclic directed graphs",
            "venue": "In Proc. Int. Conf. Mach. Learn.,",
            "year": 2020
        },
        {
            "authors": [
                "Trevor Hastie",
                "Robert Tibshirani",
                "Jerome H Friedman"
            ],
            "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, volume",
            "year": 2009
        },
        {
            "authors": [
                "Peter J Huber"
            ],
            "title": "Robust Statistics",
            "year": 1981
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Proc. Int. Conf. Learn. Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Neville K Kitson",
                "Anthony C Constantinou",
                "Zhigao Guo",
                "Yang Liu",
                "Kiattikun Chobtham"
            ],
            "title": "A survey of Bayesian network structure learning",
            "venue": "Artif. Intell. Rev.,",
            "year": 2023
        },
        {
            "authors": [
                "Xinguo Li",
                "Haoming Jiang",
                "Jarvis Haupt",
                "Raman Arora",
                "Han Liu",
                "Mingyi Hong",
                "Tuo Zhao"
            ],
            "title": "On fast convergence of proximal algorithms for sqrt-lasso optimization: Don\u2019t worry about its nonsmooth loss function",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Phillip Lippe",
                "Taco Cohen",
                "Efstratios Gavves"
            ],
            "title": "Efficient neural causal discovery without acyclicity constraints",
            "venue": "In Proc. Int. Conf. Learn. Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Po-Ling Loh",
                "Peter B\u00fchlmann"
            ],
            "title": "High-dimensional learning of linear causal networks via inverse covariance estimation",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2014
        },
        {
            "authors": [
                "J Scott Long",
                "Laurie H Ervin"
            ],
            "title": "Using heteroscedasticity consistent standard errors in the linear regression model",
            "venue": "Am. Stat.,",
            "year": 2000
        },
        {
            "authors": [
                "Peter JF Lucas",
                "Linda C Van der Gaag",
                "Ameen Abu-Hanna"
            ],
            "title": "Bayesian networks in biomedicine and health-care",
            "venue": "Artif. Intell. Med.,",
            "year": 2004
        },
        {
            "authors": [
                "Julien Mairal",
                "Francis Bach",
                "Jean Ponce",
                "Guillermo Sapiro"
            ],
            "title": "Online learning for matrix factorization and sparse coding",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2010
        },
        {
            "authors": [
                "Mathurin Massias",
                "Olivier Fercoq",
                "Alexandre Gramfort",
                "Joseph Salmon"
            ],
            "title": "Generalized concomitant multi-task lasso for sparse multimodal regression",
            "venue": "In Proc. Int. Conf. Artif. Intell. Statist.,",
            "year": 2018
        },
        {
            "authors": [
                "Eugene Ndiaye",
                "Olivier Fercoq",
                "Alexandre Gramfort",
                "Vincent Lecl\u00e8re",
                "Joseph Salmon"
            ],
            "title": "Efficient smoothed concomitant lasso estimation for high dimensional regression",
            "venue": "In Journal of Physics: Conference Series,",
            "year": 2017
        },
        {
            "authors": [
                "Ignavier Ng",
                "AmirEmad Ghassami",
                "Kun Zhang"
            ],
            "title": "On the role of sparsity and DAG constraints for learning linear DAGs",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Siqi Nie",
                "Denis D Mau\u00e1",
                "Cassio P De Campos",
                "Qiang Ji"
            ],
            "title": "Advances in learning bayesian networks of bounded treewidth",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2014
        },
        {
            "authors": [
                "Art B Owen"
            ],
            "title": "A robust hybrid of lasso and ridge regression",
            "venue": "Contemp. Math.,",
            "year": 2007
        },
        {
            "authors": [
                "Young Woong Park",
                "Diego Klabjan"
            ],
            "title": "Bayesian network learning via topological order",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2017
        },
        {
            "authors": [
                "Jonas Peters",
                "Dominik Janzing",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Elements of Causal Inference: Foundations and Learning Algorithms",
            "year": 2017
        },
        {
            "authors": [
                "Olivier Pourret",
                "Patrick Na",
                "Bruce Marcot"
            ],
            "title": "Bayesian Networks: A Oractical Guide to Applications",
            "year": 2008
        },
        {
            "authors": [
                "Joseph Ramsey",
                "Madelyn Glymour",
                "Ruben Sanchez-Romero",
                "Clark Glymour"
            ],
            "title": "A million variables and more: The fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance",
            "venue": "images. Int. J. Data Sci. Anal.,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Reisach",
                "Christof Seiler",
                "Sebastian Weichwald"
            ],
            "title": "Beware of the simulated DAG! Causal discovery benchmarks may be easy to game",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2021
        },
        {
            "authors": [
                "Karen Sachs",
                "Omar Perez",
                "Dana Pe\u2019er",
                "Douglas A Lauffenburger",
                "Garry P Nolan"
            ],
            "title": "Causal protein-signaling networks derived from multiparameter single-cell data",
            "year": 2005
        },
        {
            "authors": [
                "Andrew D Sanford",
                "Imad A Moosa"
            ],
            "title": "A Bayesian network structure for operational risk modelling in structured finance operations",
            "venue": "J. Oper. Res. Soc.,",
            "year": 2012
        },
        {
            "authors": [
                "Chandler Squires",
                "Yuhao Wang",
                "Caroline Uhler"
            ],
            "title": "Permutation-based causal structure learning with unknown intervention targets",
            "venue": "In Conf. Uncertainty Artif. Intell.,",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas St\u00e4dler",
                "Peter B\u00fchlmann",
                "Sara Van De Geer"
            ],
            "title": "l1-penalization for mixture regression models",
            "venue": "Test, 19:209\u2013256,",
            "year": 2010
        },
        {
            "authors": [
                "Tingni Sun",
                "Cun-Hui Zhang"
            ],
            "title": "Scaled sparse linear regression",
            "venue": "Biometrika, 99(4):879\u2013898,",
            "year": 2012
        },
        {
            "authors": [
                "Robert Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the lasso",
            "venue": "J. R. Stat. Soc., B: Stat. Methodol.,",
            "year": 1996
        },
        {
            "authors": [
                "Jussi Viinikka",
                "Antti Hyttinen",
                "Johan Pensar",
                "Mikko Koivisto"
            ],
            "title": "Towards scalable Bayesian learning of causal DAGs",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Matthew J. Vowels",
                "Necati Cihan Camgoz",
                "Richard Bowden"
            ],
            "title": "D\u2019ya like DAGs? A survey on structure learning and causal discovery",
            "venue": "ACM Computing Surveys,",
            "year": 2022
        },
        {
            "authors": [
                "Dennis Wei",
                "Tian Gao",
                "Yue Yu"
            ],
            "title": "DAGs with no fears: A closer look at continuous optimization for learning Bayesian networks",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Albert Xue",
                "Jingyou Rao",
                "Sriram Sankararaman",
                "Harold Pimentel"
            ],
            "title": "dotears: Scalable, consistent DAG estimation using observational and interventional data",
            "year": 1921
        },
        {
            "authors": [
                "Yang Yang",
                "Marius Pesavento",
                "Zhi-Quan Luo",
                "Bj\u00f6rn Ottersten"
            ],
            "title": "Inexact block coordinate descent algorithms for nonsmooth nonconvex optimization",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2020
        },
        {
            "authors": [
                "Yue Yu",
                "Jie Chen",
                "Tian Gao",
                "Mo Yu"
            ],
            "title": "DAG-GNN: DAG structure learning with graph neural networks",
            "venue": "In Proc. Int. Conf. Mach. Learn.,",
            "year": 2019
        },
        {
            "authors": [
                "Valentina Zantedeschi",
                "Luca Franceschi",
                "Jean Kaddour",
                "Matt Kusner",
                "Vlad Niculae"
            ],
            "title": "DAG learning on the permutahedron",
            "venue": "In Proc. Int. Conf. Learn. Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Bin Zhang",
                "Chris Gaiteri",
                "Liviu-Gabriel Bodea",
                "Zhi Wang",
                "Joshua McElwee",
                "Alexei A Podtelezhnikov",
                "Chunsheng Zhang",
                "Tao Xie",
                "Linh Tran",
                "Radu Dobrin"
            ],
            "title": "Integrated systems approach identifies genetic nodes and networks in late-onset Alzheimer\u2019s disease",
            "year": 2013
        },
        {
            "authors": [
                "Xun Zheng",
                "Bryon Aragam",
                "Pradeep K Ravikumar",
                "Eric P Xing"
            ],
            "title": "DAGs with no tears: Continuous optimization for structure learning",
            "venue": "In Proc. Adv. Neural. Inf. Process. Syst.,",
            "year": 2018
        },
        {
            "authors": [
                "e.g",
                "(Peters"
            ],
            "title": "A subset of studies within this domain introduces modifications to the original problem by incorporating additional assumptions regarding the DAG or the number of parent nodes associated with each variable (Nie et",
            "venue": "Viinikka et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Ndiaye"
            ],
            "title": "2017) stands out as the most recent and efficient method suitable for high-dimensional settings. The approach in (Ndiaye et al., 2017) is to jointly estimate the regression coefficients \u03b8 and the noise level \u03c3 by solving the jointly convex problem",
            "year": 2017
        },
        {
            "authors": [],
            "title": "Inclusion of the constraint",
            "year": 2017
        },
        {
            "authors": [
                "Remark. Li"
            ],
            "title": "2020) noticed that the non-differentiability of the squared-root lasso is not",
            "year": 2020
        },
        {
            "authors": [
                "by Ghassami"
            ],
            "title": "2020), and we will not reproduce all definitions and technical details",
            "year": 2020
        },
        {
            "authors": [
                "Ng"
            ],
            "title": "The code for GOLEM is publicly available on GitHub at https://github.com/ignavier/ golem. Utilizing Tensorflow, GOLEM requires GPU support. We adopt their recommended hyperparameters for GOLEM",
            "venue": "For GOLEM-EV,",
            "year": 2020
        },
        {
            "authors": [
                "Ng"
            ],
            "title": "Closely following the guidelines in Ng et al. (2020), we initialize GOLEM-NV with the output of GOLEM-EV",
            "venue": "GOLEM-NV,",
            "year": 2022
        },
        {
            "authors": [
                "Ng"
            ],
            "title": "It is important to note that the Gaussian log-likelihood score function used by GOLEM incorporates a noise-dependent term",
            "year": 2020
        },
        {
            "authors": [
                "Ng"
            ],
            "title": "2020, Appendix C), which is profiled out during optimization. This noise-dependent term bears some similarities with our CoLiDE formulation. Indeed, after dropping the log (|det(I\u2212W)|) term in (42) that vanishes when W corresponds to a DAG, the respective score functions are given by: CoLiDE-NV: S(W,\u03a3",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Directed acyclic graphs (DAGs) have well-appreciated merits for encoding causal relationships within complex systems, as they employ directed edges to link causes and their immediate effects. This graphical modeling framework has gained prominence in various machine learning (ML) applications spanning domains such as biology (Sachs et al., 2005; Lucas et al., 2004), genetics (Zhang et al., 2013), finance (Sanford & Moosa, 2012), and economics Pourret et al. (2008), to name a few. However, since the causal structure underlying a group of variables is often unknown, there is a need to address the task of inferring DAGs from observational data. While additional interventional data are provably beneficial to the related problem of causal discovery (Lippe et al., 2021; Squires et al., 2020; Addanki et al., 2020; Brouillard et al., 2020), said interventions may be infeasible or ethically challenging to implement. Learning a DAG solely from observational data poses significant computational challenges, primarily due to the combinatorial acyclicity constraint which is notoriously difficult to enforce (Chickering, 1996; Chickering et al., 2004). Moreover, distinguishing between DAGs that generate the same observational data distribution is nontrivial. This identifiability challenge may arise when data are limited (especially in high-dimensional settings), or, when candidate graphs in the search space exhibit so-termed Markov equivalence; see e.g., (Peters et al., 2017).\nRecognizing that DAG learning from observational data is in general an NP-hard problem, recent efforts have advocated a continuous relaxation approach which offers an efficient means of exploring the space of DAGs (Zheng et al., 2018; Ng et al., 2020; Bello et al., 2022). In addition to breakthroughs in differentiable, nonconvex characterizations of acyclicity (see Section 3), the choice of an appropriate score function is paramount to guide continuous optimization techniques that search for faithful representations of the underlying DAG. Likelihood-based methods (Ng et al., 2020) enjoy desirable statistical properties, provided that the postulated probabilistic model aligns with the actual causal relationships (Hastie et al., 2009; Casella & Berger, 2021). On the other hand, regression-based methods (Zheng et al., 2018; Bello et al., 2022) exhibit computational efficiency,\nrobustness, and even consistency (Loh & Bu\u0308hlmann, 2014), especially in high-dimensional settings where both data scarcity and model uncertainty are prevalent (Ndiaye et al., 2017). Still, workhorse regression-based methods such as ordinary least squares (LS) rely on the assumption of homoscedasticity, meaning the variances of the exogenous noises are identical across variables. Deviations from this assumption can introduce biases in standard error estimates, hindering the accuracy of causal discovery (Long & Ervin, 2000; Loh & Bu\u0308hlmann, 2014). Fundamentally, for heteroscedastic linear Gaussian models the DAG structure is non-identifiable from observational data (Peters et al., 2017).\nScore functions often include a sparsity-promoting regularization, for instance an \u21131-norm penalty as in lasso regression (Tibshirani, 1996). This in turn necessitates careful fine-tuning of the penalty parameter that governs the trade-off between sparsity and data fidelity (Massias et al., 2018). Theoretical insights suggest an appropriately scaled regularization parameter proportional to the observation noise level (Bickel et al., 2009), but the latter is typically unknown. Accordingly, several linear regression studies (unrelated to DAG estimation) have proposed convex concomitant estimators based on scaled LS, which jointly estimate the noise level along with the regression coefficients (Owen, 2007; Sun & Zhang, 2012; Belloni et al., 2011; Ndiaye et al., 2017). A noteworthy representative is the smoothed concomitant lasso (Ndiaye et al., 2017), which not only addresses the aforementioned parameter fine-tuning predicament but also accommodates heteroscedastic linear regression scenarios where noises exhibit non-equal variances. While these challenges are not extraneous to DAG learning, the impact of concomitant estimators is so far largely unexplored in this timely field.\nContributions. In this work, we bring to bear ideas from concomitant scale estimation in (sparse) linear regression and propose a novel convex score function for regression-based inference of linear DAGs, demonstrating significant improvements relative to existing state-of-the-art methods.1 Our contributions can be summarized as follows:\n\u2022 We propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale parameters to enhance DAG topology inference using continuous first-order optimization. We augment this LS-based score function with a smooth, nonconvex acyclicity penalty term to arrive at CoLiDE (Concomitant Linear DAG Estimation), a simple regression-based criterion that facilitates efficient computation of gradients and estimation of exogenous noise levels via closed-form expressions. To the best of our knowledge, this is the first time that ideas from concomitant scale estimation permeate benefits to DAG learning.\n\u2022 In existing methods, the sparsity regularization parameter depends on the unknown exogenous noise levels, making the calibration challenging. CoLiDE effectively removes this coupling, leading to minimum (or no) recalibration effort across diverse problem instances. Our score function is fairly robust to deviations from Gaussianity, and relative to ordinary LS used in prior score-based DAG estimators, experiments show CoLiDE is more suitable for challenging heteroscedastic scenarios.\n\u2022 We demonstrate CoLiDE\u2019s effectiveness through comprehensive experiments with both simulated (linear SEM) and real-world datasets. When benchmarked against state-of-the-art DAG learning algorithms, CoLiDE consistently attains better recovery performance across graph ensembles and exogenous noise distributions, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in various domain-specific metrics, underscoring the robustness of our novel estimator."
        },
        {
            "heading": "2 PRELIMINARIES AND PROBLEM STATEMENT",
            "text": "Consider a directed graph G (V, E ,W), where V = {1, . . . , d} represents the set of vertices, and E \u2286 V \u00d7 V is the set of edges. The adjacency matrix W = [w1, . . . ,wd] \u2208 Rd\u00d7d collects the edge weights, with Wij \u0338= 0 indicating a direct link from node i to node j. We henceforth assume that G belongs to the space D of DAGs, and rely on the graph to represent conditional independencies among the variables in the random vector x = [x1, . . . , xd]\u22a4 \u2208 Rd. Indeed, if the joint distribution P(x) satisfies a Markov property with respect to G \u2208 D, each random variable xi depends solely on\n1While preparing the final version of this manuscript, we became aware of independent work in the unpublished preprint (Xue et al., 2023), which also advocates estimating (and accounting for) exogenous noise variances to improve DAG topology inference. However, dotears is a two-step marginal estimator requiring additional interventional data, while our concomitant framework leads to a new LS-based score function and, importantly, relies on observational data only.\nits parents PAi = {j \u2208 V : Wji \u0338= 0}. Here, we focus on linear structural equation models (SEMs) to generate said probability distribution, whereby the relationship between each random variable and its parents is given by xi = w\u22a4i x + zi, \u2200i \u2208 V , and z = [z1, . . . , zd]\u22a4 is a vector of mutually independent, exogenous noises; see e.g., (Peters et al., 2017). Noise variables zi can have different variances and need not be Gaussian distributed. For a dataset X \u2208 Rd\u00d7n of n i.i.d. samples drawn from P(x), the linear SEM equations can be written in compact matrix form as X = W\u22a4X+ Z.\nProblem statement. Given the data matrix X adhering to a linear SEM, our goal is to learn the latent DAG G \u2208 D by estimating its adjacency matrix W as the solution to the optimization problem\nmin W\nS(W) subject to G(W) \u2208 D, (1)\nwhere S(W) is a data-dependent score function to measure the quality of the candidate DAG. Irrespective of the criterion, the non-convexity comes from the combinatorial acyclicity constraint G(W) \u2208 D; see also Appendix A for a survey of combinatorial search approaches relevant to solving (1), but less related to the continuous relaxation methodology pursued here (see Section 3).\nA proper score function typically encompasses a loss or data fidelity term ensuring alignment with the SEM as well as regularizers to promote desired structural properties on the sought DAG. For a linear SEM, the ordinary LS loss 12n\u2225X\u2212W\n\u22a4X\u22252F is widely adopted, where \u2225\u00b7\u2225F is the Frobenius norm. When the exogenous noises are Gaussian, the data log-likelihood can be an effective alternative (Ng et al., 2020). Since sparsity is a cardinal property of most real-world graphs, it is prudent to augment the loss with an \u21131-norm regularizer to yield S(W) = 12n\u2225X \u2212 W\n\u22a4X\u22252F + \u03bb\u2225W\u22251, where \u03bb \u2265 0 is a tuning parameter that controls edge sparsity. The score function S(W) bears resemblance to the multi-task variant of lasso regression (Tibshirani, 1996), specifically when the response and design matrices coincide. Optimal rates for lasso hinge on selecting \u03bb \u224d \u03c3 \u221a log d/n (Bickel et al., 2009; Li et al., 2020). However, the exogenous noise variance \u03c32 is rarely known in practice. This challenge is compounded in heteroscedastic settings, where one should adopt a weighted LS score (see (Loh & Bu\u0308hlmann, 2014) for an exception unlike most DAG learning methods that stick to bias-inducing ordinary LS). Recognizing these limitations, in Section 4 we propose a novel LS-based score function to facilitate joint estimation of the DAG and the noise levels."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "We briefly review differentiable optimization approaches that differ in how they handle the acyclicity constraint, namely by using an explicit DAG parameterization or continuous relaxation techniques.\nContinuous relaxation. Noteworthy methods advocate an exact acyclicity characterization using nonconvex, smooth functions H : Rd\u00d7d 7\u2192 R of the adjacency matrix, whose zero level set is D. One can thus relax the combinatorial constraint G(W) \u2208 D by instead enforcing H(W) = 0, and tackle the DAG learning problem using standard continuous optimization algorithms (Zheng et al., 2018; Yu et al., 2019; Wei et al., 2020; Bello et al., 2022). The pioneering NOTEARS formulation introduced Hexpm(W) = Tr(eW\u25e6W) \u2212 d, where \u25e6 denotes the Hadamard (element-wise) product and Tr(\u00b7) is the matrix trace operator (Zheng et al., 2018). Diagonal entries of powers of W \u25e6 W encode information about cycles in G, hence the suitability of the chosen function. Follow-up work suggested a more computationally efficient acyclicity function Hpoly(W) = Tr((I+ 1dW\u25e6W)\nd)\u2212d, where I is the identity matrix (Yu et al., 2019); see also (Wei et al., 2020) that studies the general family H(W) = \u2211d k=1 ck Tr((W \u25e6 W)d), ck > 0. Recently, Bello et al. (2022) proposed the log-determinant acyclicity characterization Hldet(W, s) = d log(s)\u2212 log(det(sI\u2212W \u25e6W)), s \u2208 R; outperforming prior relaxation methods in terms of (nonlinear) DAG recovery and efficiency. Although global optimality results are so far elusive, progress is being made (Deng et al., 2023b).\nOrder-based methods. Other recent approaches exploit the neat equivalence G(W) \u2208 D \u21d4 W = \u03a0\u22a4U\u03a0, where \u03a0 \u2208 {0, 1}d\u00d7d is a permutation matrix (essentially encoding the variables\u2019 causal ordering) and U \u2208 Rd\u00d7d is an upper-triangular weight matrix. Consequently, one can search over exact DAGs by formulating an end-to-end differentiable optimization framework to minimize S(\u03a0\u22a4U\u03a0) jointly over \u03a0 and U, or in two steps. Even for nonlinear SEMs, the challenging process of determining the appropriate node ordering is typically accomplished through the Birkhoff polytope of permutation matrices, utilizing techniques like the Gumbel-Sinkhorn approximation (Cundy et al., 2021), or, the SoftSort operator (Charpentier et al., 2022). Limitations stemming from\nmisaligned forward and backward passes that respectively rely on hard and soft permutations are well documented (Zantedeschi et al., 2023). The DAGuerreotype approach in (Zantedeschi et al., 2023) instead searches over the Permutahedron of vector orderings and allows for non-smooth edge weight estimators. TOPO (Deng et al., 2023a) performs a bi-level optimization, relying on topological order swaps at the outer level. Still, optimization challenges towards accurately recovering the causal ordering remain, especially when data are limited and the noise level profile is heterogeneous.\nWhile this work is framed within the continuous constrained relaxation paradigm to linear DAG learning, preliminary experiments with TOPO (Deng et al., 2023a) show that CoLiDE also benefits order-based methods (see Appendix E.8). We leave a full exploration as future work."
        },
        {
            "heading": "4 CONCOMITANT LINEAR DAG ESTIMATION",
            "text": "Going back to our discussion on lasso-type score functions in Section 2, minimizing S(W) = 1 2n\u2225X \u2212 W\n\u22a4X\u22252F + \u03bb\u2225W\u22251 subject to a smooth acyclicity constraint H(W) = 0 (as in e.g., NoTears (Zheng et al., 2018)): (i) requires carefully retuning \u03bb when the unknown SEM noise variance changes across problem instances; and (ii) implicitly relies on limiting homoscedasticity assumptions due to the ordinary LS loss. To address issues (i)-(ii), here we propose a new convex score function for linear DAG estimation that incorporates concomitant estimation of scale. This way, we obtain a procedure that is robust (both in terms of DAG estimation performance and parameter finetuning) to possibly heteroscedastic exogenous noise profiles. We were inspired by the literature of concomitant scale estimation in sparse linear regression (Owen, 2007; Belloni et al., 2011; Sun & Zhang, 2012; Ndiaye et al., 2017); see Appendix A.1 for concomitant lasso background that informs the approach pursued here. Our method is dubbed CoLiDE (Concomitant Linear DAG Estimation).\nHomoscedastic setting. We start our exposition with a simple scenario whereby all exogenous variables z1, . . . , zd in the linear SEM have identical variance \u03c32. Building on the smoothed concomitant lasso (Ndiaye et al., 2017), we formulate the problem of jointly estimating the DAG adjacency matrix W and the exogenous noise scale \u03c3 as\nmin W,\u03c3\u2265\u03c30\n1\n2n\u03c3 \u2225X\u2212W\u22a4X\u22252F +\nd\u03c3\n2 + \u03bb\u2225W\u22251\ufe38 \ufe37\ufe37 \ufe38\n:=S(W,\u03c3)\nsubject to H(W) = 0, (2)\nwhere H : Rd\u00d7d 7\u2192 R is a nonconvex, smooth function, whose zero level set is D as discussed in Section 3. Notably, the weighted, regularized LS score function S(W, \u03c3) is now also a function of \u03c3, and it can be traced back to the robust linear regression work of Huber (1981). Due to the rescaled residuals, \u03bb in (2) decouples from \u03c3 as minimax optimality now requires \u03bb \u224d \u221a log d/n (Li et al., 2020; Belloni et al., 2011). A minor tweak to the argument in the proof of (Owen, 2007, Theorem 1) suffices to establish that S(W, \u03c3) is jointly convex in W and \u03c3. Of course, (2) is still a nonconvex optimization problem by virtue of the acyclicity constraint H(W) = 0. Huber (1981) included the term d\u03c3/2 so that the resulting squared scale estimator is consistent under Gaussianity. The additional constraint \u03c3 \u2265 \u03c30 safeguards against potential ill-posed scenarios where the estimate \u03c3\u0302 approaches zero. Following the guidelines in (Ndiaye et al., 2017), we set \u03c30 =\n\u2225X\u2225F\u221a dn \u00d7 10\u22122.\nWith regards to the choice of the acyclicity function, we select Hldet(W, s) = d log(s) \u2212 log(det(sI\u2212W \u25e6W)) based on its more favorable gradient properties in addition to several other compelling reasons outlined in (Bello et al., 2022, Section 3.2). Moreover, while LS-based linear DAG learning approaches are prone to introducing cycles in the estimated graph, it was noted that a log-determinant term arising with the Gaussian log-likelihood objective tends to mitigate this undesirable effect (Ng et al., 2020). Interestingly, the same holds true when Hldet is chosen as a regularizer, but this time without being tied to Gaussian assumptions. Before moving on to optimization issues, we emphasize that our approach is in principle flexible to accommodate other convex loss functions beyond LS (e.g., Huber\u2019s loss for robustness against heavy-tailed contamination), other acyclicity functions, and even nonlinear SEMs parameterized using e.g., neural networks. All these are interesting CoLiDE generalizations beyond the scope of this paper.\nOptimization considerations. Motivated by our choice of the acyclicity function, to solve the constrained optimization problem (2) we follow the DAGMA methodology by Bello et al. (2022). Therein, it is suggested to solve a sequence of unconstrained problems where Hldet is dualized and\nviewed as a regularizer. This technique has proven more effective in practice for our specific problem as well, when compared to, say, an augmented Lagrangian method. Given a decreasing sequence of values \u00b5k \u2192 0, at step k of the COLIDE-EV (equal variance) algorithm one solves\nmin W,\u03c3\u2265\u03c30 \u00b5k\n[ 1\n2n\u03c3 \u2225X\u2212W\u22a4X\u22252F +\nd\u03c3\n2 + \u03bb\u2225W\u22251\n] +Hldet(W, sk), (3)\nwhere the schedule of hyperparameters \u00b5k \u2265 0 and sk > 0 must be prescribed prior to implementation; see Section 4.1. Decreasing the value of \u00b5k enhances the influence of the acyclicity function Hldet(W, s) in the objective. Bello et al. (2022) point out that the sequential procedure (3) is reminiscent of the central path approach of barrier methods, and the limit of the central path \u00b5k \u2192 0 is guaranteed to yield a DAG. In theory, this means no additional post-processing (e.g., edge trimming) is needed. However, in practice we find some thresholding is required to reduce false positives.\nUnlike Bello et al. (2022), CoLiDE-EV jointly estimates the noise level \u03c3 and the adjacency matrix W for each \u00b5k. To this end, one could solve for \u03c3 in closed form [cf. (4)] and plug back the solution in (3), to obtain a DAG-regularized square-root lasso (Belloni et al., 2011) type of problem in W. We did not follow this path since the resulting loss \u2225X \u2212 W\u22a4X\u2225F is not decomposable across samples, challenging mini-batch based stochastic optimization if it were needed for scalability (in Appendix B, we show that our score function is fully decomposable and present corresponding preliminary experiments). A similar issue arises with GOLEM (Ng et al., 2020), where the derived Gaussian profile likelihood yields a non-separable log-sum loss. Alternatively, and similar to the smooth concomitant lasso algorithm Ndiaye et al. (2017), we rely on (inexact) block coordinate descent (BCD) iterations. This cyclic strategy involves fixing \u03c3 to its most up-to-date value and minimizing (3) inexactly w.r.t. W, subsequently updating \u03c3 in closed form given the latest W via\n\u03c3\u0302 = max (\u221a Tr ((I\u2212W)\u22a4 cov(X)(I\u2212W)) /d, \u03c30 ) , (4)\nwhere cov(X) := 1nXX \u22a4 is the precomputed sample covariance matrix. The mutually-reinforcing interplay between noise level and DAG estimation should be apparent. There are several ways to inexactly solve the W subproblem using first-order methods. Given the structure of (3), an elegant solution is to rely on the proximal linear approximation. This leads to an ISTA-type update for W, and overall a provably convergent BCD procedure in this nonsmooth, nonconvex setting (Yang et al., 2020, Theorem 1). Because the required line search can be computationally taxing, we opt for a simpler heuristic which is to run a single step of the ADAM optimizer (Kingma & Ba, 2015) to refine W. We observed that running multiple ADAM iterations yields marginal gains, since we are anyways continuously re-updating W in the BCD loop. This process is repeated until either convergence is attained, or, a prescribed maximum iteration count Tk is reached. Additional details on gradient computation of (3) w.r.t. W and the derivation of (4) can be found in Appendix B.\nHeteroscedastic setting. We also address the more challenging endeavor of learning DAGs in heteroscedastic scenarios, where noise variables have non-equal variances (NV) \u03c321 , . . . , \u03c3 2 d. Bringing to bear ideas from the generalized concomitant multi-task lasso (Massias et al., 2018) and mimicking the optimization approach for the EV case discussed earlier, we propose the CoLiDE-NV estimator\nmin W,\u03a3\u2265\u03a30 \u00b5k\n[ 1\n2n Tr ( (X\u2212W\u22a4X)\u22a4\u03a3\u22121(X\u2212W\u22a4X) ) + 1 2 Tr(\u03a3) + \u03bb\u2225W\u22251 ] +Hldet(W, sk).\n(5) Note that \u03a3 = diag(\u03c31, . . . , \u03c3d) is a diagonal matrix of exogenous noise standard deviations (hence not a covariance matrix). Once more, we set \u03a30 = \u221a diag (cov(X))\u00d7 10\u22122, where \u221a (\u00b7) is meant to be taken element-wise. A closed form solution for \u03a3 given W is also readily obtained,\n\u03a3\u0302 = max (\u221a diag ((I\u2212W)\u22a4 cov(X)(I\u2212W)),\u03a30 ) . (6)\nA summary of the overall computational procedure for both CoLiDE variants is tabulated under Algorithm 1; see Appendix B for detailed gradient expressions and a computational complexity discussion. CoLiDE\u2019s per iteration cost is O(d3), on par with state-of-the-art DAG learning methods. The first summand in (5) resembles the consistent weighted LS estimator studied in Loh & Bu\u0308hlmann (2014), but therein the assumption is that exogenous noise variances are known up to\nAlgorithm 1: CoLiDE optimization In: data X and hyperparameters \u03bb and H = {(\u00b5k, sk, Tk)}Kk=1. Out: DAG W and the noise estimate \u03c3 (EV) or \u03a3 (NV). Compute lower-bounds \u03c30 or \u03a30. Initialize W = 0, \u03c3 = \u03c30 \u00d7 102 or \u03a3 = \u03a30 \u00d7 102. foreach (\u00b5k, sk, Tk) \u2208 H do\nfor t = 1, . . . , Tk do Apply CoLiDE-EV or NV updates using \u00b5k and sk.\nFunction CoLiDE-EV update: Update W with one iteration of\na first-order method for (3) Compute \u03c3\u0302 using (4)\nFunction CoLiDE-NV update: Update W with one iteration of\na first-order method for (5) Compute \u03a3\u0302 using (6)\na constant factor. CoLiDE-NV removes this assumption by jointly estimating W and \u03a3, with marginal added complexity over finding the DAG structure alone. Like GOLEM (Ng et al., 2020) and for general (non-identifiable) linear Gaussian SEMs, as n \u2192 \u221e CoLiDE-NV probably yields a DAG that is quasi-equivalent to the ground truth graph (see Appendix C for futher details)."
        },
        {
            "heading": "4.1 FURTHER ALGORITHMIC DETAILS",
            "text": "Initialization. Following Bello et al. (2022), we initialize W = 0 as it always lies in the feasibility region of Hldet. We also initialize \u03c3 = \u03c30 \u00d7 102 in CoLiDE-EV or \u03a3 = \u03a30 \u00d7 102 for CoLiDE-NV. Hyperparameter selection and termination rule. To facilitate the comparison with the state-ofthe-art DAGMA and to better isolate the impact of our novel score function, we use the hyperparameters selected by Bello et al. (2022) for CoLiDE-EV and NV. Hence, our algorithm uses \u03bb = 0.05 and employs K = 4 decreasing values of \u00b5k \u2208 [1.0, 0.1, 0.01, 0.001], and the maximum number of BCD iterations is specified as Tk = [2 \u00d7 104, 2 \u00d7 104, 2 \u00d7 104, 7 \u00d7 104]. Furthermore, early stopping is incorporated, activated when the relative error between consecutive values of the objective function falls below 10\u22126. The learning rate for ADAM is 3\u00d710\u22124. We adopt sk \u2208 [1, 0.9, 0.8, 0.7]. Post-processing. Similar to several previous works (Zheng et al., 2018; Ng et al., 2020; Bello et al., 2022), we conduct a final thresholding step to reduce false discoveries. In this post-processing step, edges with absolute weights smaller than 0.3 are removed."
        },
        {
            "heading": "5 EXPERIMENTAL RESULTS",
            "text": "We now show that CoLiDE leads to state-of-the-art DAG estimation results by conducting a comprehensive evaluation against other state-of-the-art approaches: GES (Ramsey et al., 2017), GOLEM (Ng et al., 2020), DAGMA (Bello et al., 2022), SortNRegress (Reisach et al., 2021), and DAGuerreotype (Zantedeschi et al., 2023). We omit conceptually important methods like NoTears (Zheng et al., 2018), that have been outclassed in practice. To improve the visibility of the figures, we report in each experiment the results of the best performing methods, excluding those that perform particularly poorly. We use standard DAG recovery metrics: (normalized) Structural Hamming Distance (SHD), SHD over the Markov equivalence class (SHD-C), Structural Intervention Distance (SID), True Positive Rate (TPR), and False Discovery Rate (FDR). We also assess CoLiDE\u2019s ability to estimate the noise level(s) across different scenarios. Further details about the setup are in Appendix D.\nDAG generation. As standard (Zheng et al., 2018), we generate the ground truth DAGs utilizing the Erdo\u030bs-Re\u0301nyi or the Scale-Free random models, respectively denoted as ERk or SFk, where k is the average nodal degree. Edge weights for these DAGs are drawn uniformly from a range of feasible edge weights. We present results for k = 4, the most challenging setting (Bello et al., 2022).\nData generation. Employing the linear SEM, we simulate n = 1000 samples using the homo- or heteroscedastic noise models and drawing from diverse noise distributions, i.e., Gaussian, Exponential, and Laplace (see Appendix D). Unless indicated otherwise, we report the aggregated results of ten independent runs by repeating all experiments 10 times, each with a distinct DAG.\nAppendix E contains additional results (e.g., other graph types, different number of nodes, and k configurations). There, CoLiDE prevails with similar observations as in the cases discussed next.\nHomoscedastic setting. We begin by assuming equal noise variances across all nodes. We employ ER4 and SF4 graphs with 200 nodes and edge weights drawn uniformly from the range E \u2208 [\u22122,\u22120.5] \u222a [0.5, 2] (Zheng et al., 2018; Ng et al., 2020; Bello et al., 2022).\nIn Figure 1, we investigate the impact of noise levels varying from 0.5 to 10 for different noise distributions. CoLiDE-EV clearly outperforms its competitors, consistently reaching a lower SHD. Here, it is important to highlight that GOLEM, being based on the profile log-likelihood for the Gaussian case, intrinsically addresses noise estimation for that particular scenario. However, the more general CoLiDE formulation still exhibits superior performance even in GOLEM\u2019s specific scenario (left column). We posit that the logarithmic nonlinearity in GOLEM\u2019s data fidelity term hinders its ability to fit the data. CoLiDE\u2019s noise estimation provides a more precise correction, equivalent to a square-root nonlinearity [see (9) in Appendix A.1], giving more weight to the data fidelity term and consequently allowing to fit the data more accurately.\nIn Table 1, we deep-dive in two particular scenarios: (1) when the noise variance equals 1, as in prior studies (Zheng et al., 2018; Ng et al., 2020; Bello et al., 2022), and (2) when the noise level is increased to 5. Note that SortNRegress does not produce state-of-the-art results (SHD of 652.5 and 615 in each case, respectively), which relates to the non-triviality of the problem instance. These cases show that CoLiDE\u2019s advantage over its competitors is not restricted to SHD alone, but equally extends to all other relevant metrics. This behavior is accentuated when the noise variance is set to 5, as CoLiDE naturally adapts to different noise regimes without any manual tuning of its hyperparameters. Additionally, CoLiDE-EV consistently yields lower standard deviations than the alternatives across all metrics, underscoring its robustness.\nCoLiDE-NV, although overparametrized for homoscedastic problems, performs remarkably well, either being on par with CoLiDE-EV or the second-best alternative in Figure 1 and Table 1. This is of particular importance as the characteristics of the noise are usually unknown in practice, favoring more general and versatile formulations.\nHeteroscedastic setting. The heteroscedastic scenario, where nodes do not share the same noise variance, presents further challenges. Notably, this setting is known to be non-identifiable (Ng et al., 2020) from observational data. This issue is exacerbated as the number d of nodes grows as, whether we are estimating the variances explicitly or implicitly, the problem contains d additional unknowns, which renders its optimization harder from a practical perspective. We select the edge weights by uniformly drawing from [\u22121,\u22120.25] \u222a [0.25, 1] and the noise variance of each node from [0.5, 10].\nThis compressed interval, compared to the one used in the previous section, has a reduced signal-tonoise ratio (SNR) (Zheng et al., 2018; Reisach et al., 2021), which obfuscates the optimization.\nFigure 2 presents experiments varying noise distributions, graph types, and node numbers. CoLiDENV is the clear winner, outperforming the alternatives in virtually all variations. Remarkably, CoLiDE-EV performs very well and often is the second-best solution, outmatching GOLEM-NV, even considering that an EV formulation is clearly underspecifying the problem.\nThese trends are confirmed in Table 2, where both CoLiDE formulations show strong efficacy on additional metrics (we point out that SHD is often considered the more accurate among all of them) for the ER4 instance with Gaussian noise. In this particular instance, SortNRegress is competitive with CoLiDE-NV in SID, SHD-C, and FDR. Note that, CoLiDE-NV consistently maintains lower deviations than DAGMA and GOLEM, underscoring its robustness. All in all, CoLiDE-NV is leading the pack in terms of SHD and performs very strongly across all other metrics.\nIn Appendix E.5, we include additional experiments where we draw the edge weights uniformly from [\u22122,\u22120.5]\u222a [0.5, 2]. Here, CoLiDE-NV and CoLiDE-EV are evenly matched as the simplified problem complexity does not warrant the adoption of a more intricate formulation like CoLiDE-NV.\nBeyond continuous optimization. We also tested the CoLiDE objective in combination with TOPO Deng et al. (2023a), a recently proposed bi-level optimization framework. Results in Appendix E.8 show that CoLiDE yields improvements when using this new optimization method.\nNoise estimation. An accurate noise estimation is the crux of our work (Section 4), leading to a new formulation and algorithm for DAG learning. A method\u2019s ability to estimate noise variance reflects its proficiency in recovering accurate edge weights. Metrics such as SHD and TPR prioritize the detection of correct edges, irrespective of whether the edge weight closely matches the actual value. For methods that do not explicitly estimate the noise, we can evaluate them a posteriori by estimating the DAG first and subsequently computing the noise variances using the residual variance formula \u03c3\u03022 = 1dn\u2225X\u2212 W\u0302 \u22a4X\u22252F in the EV case, or, \u03c3\u0302i 2 = 1n\u2225xi \u2212 w\u0302i\n\u22a4x\u222522 in the NV case. We can now examine whether CoLiDE surpasses other state-of-the-art approaches in noise estimation. To this end, we generated 10 distinct ER4 DAGs with 200 nodes, employing the homo and heteroscedastic settings described earlier in this section. Assuming a Gaussian noise distribution, we\ngenerated different numbers of samples to assess CoLiDE\u2019s performance under limited data scenarios. We exclude GOLEM from the comparison due to its subpar performance compared to DAGMA. In the equal noise variance scenario, depicted in Figure 3 (left), CoLiDE-EV consistently outperforms DAGMA across varying sample sizes. CoLiDE-EV is also slightly better than CoLiDE-NV, due to its better modeling of the homoscedastic case. In Figure 3 (right), we examine the non-equal noise variance scenario, where CoLiDE-NV demonstrates superior performance over both CoLiDEEV and DAGMA across different sample sizes. Of particular interest is the fact that CoLiDE-NV provides a lower error even when using half as many samples as DAGMA.\nReal data. To conclude our analysis, we extend our investigation to a real-world example, the Sachs dataset (Sachs et al., 2005), which is used extensively throughout the probabilistic graphical models literature. The Sachs dataset encompasses cytometric measurements of protein and phospholipid constituents in human immune system cells (Sachs et al., 2005). This dataset comprises 11 nodes and 853 observation samples. The associated DAG is established through experimental methods as outlined by Sachs et al. (2005), and it enjoys validation from the biological research community. Notably, the ground truth DAG in this dataset consists of 17 edges. The outcomes of this experiment are consolidated in Table 3. Evidently, the results illustrate that CoLiDE-NV outperforms all other state-of-the-art methods, achieving a SHD of 12. To our knowledge, this represents the lowest achieved SHD among continuous optimization-based techniques applied to the Sachs problem."
        },
        {
            "heading": "6 CONCLUDING SUMMARY, LIMITATIONS, AND FUTURE WORK",
            "text": "In this paper we introduced CoLiDE, a framework for learning linear DAGs wherein we simultaneously estimate both the DAG structure and the exogenous noise levels. We present variants of CoLiDE to estimate homoscedastic and heteroscedastic noise across nodes. Additionally, estimating the noise eliminates the necessity for fine-tuning the model hyperparameters (e.g., the weight on the sparsity penalty) based on the unknown noise levels. Extensive experimental results have validated CoLiDE\u2019s superior performance when compared to other state-of-the-art methods in diverse synthetic and real-world settings, including the recovery of the DAG edges as well as their weights.\nThe scope of our DAG estimation framework is limited to observational data adhering to a linear SEM. In future work, we will extend it to encompass nonlinear and interventional settings. Therein, CoLiDE\u2019s formulation, amenable to first-order optimization, will facilitate a symbiosis with neural networks to parameterize SEM nonlinearities. Although CoLiDE\u2019s decomposability is a demonstrable property, further experimental results are needed to fully assert the practical value of stochastic optimization. Finally, we plan to introduce new optimization techniques to realize CoLiDE\u2019s full potential both in batch and online settings, with envisioned impact also to order-based methods."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research was partially conducted while the first author was an intern at Intel Labs. The authors would like to thanks the anonymous reviewers for their thoughtful feedback and valuable suggestions on the original version of this manuscript, which led to a markedly improved revised paper."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We have made our code publicly available at https://github.com/SAMiatto/colide. CoLiDE-related algorithmic details including hyperparameter selection are given in Section 4.1. Additional implementation details are in Appendix D."
        },
        {
            "heading": "A ADDITIONAL RELATED WORK",
            "text": "While the focus in the paper has been on continuous relaxation algorithms to tackle the linear DAG learning problem, numerous alternative combinatorial search approaches have been explored as well; see (Kitson et al., 2023) and (Vowels et al., 2022) for up-to-date tutorial expositions that also survey a host of approaches for nonlinear SEMs. For completeness, here we augment our Section 3 review of continuous relaxation and order-based methods to also account for discrete optimization alternatives in the literature.\nDiscrete optimization methods. A broad swath of approaches falls under the category of scorebased methods, where (e.g., BD, BIC, BDe, MDL) scoring functions are used to guide the search for DAGs in D, e.g., (Peters et al., 2017, Chapter 7.2.2). A subset of studies within this domain introduces modifications to the original problem by incorporating additional assumptions regarding the DAG or the number of parent nodes associated with each variable (Nie et al., 2014; Chen et al., 2016; Viinikka et al., 2020). Another category of discrete optimization methods is rooted in greedy search strategies or discrete optimization techniques applied to the determination of topological orders (Chickering, 2002; Park & Klabjan, 2017). GES (Ramsey et al., 2017) is a scalable greedy algorithm for discovering DAGs that we chose as one of our baselines. Constraint-based methods represent another broad category within discrete optimization approaches (Bu\u0308hlmann et al., 2014). These approaches navigate D by conducting independence tests among observed variables; see e.g., (Peters et al., 2017, Chapter 7.2.1). Overall, many of these combinatorial search methods exhibit scalability issues, particularly when confronted with high-dimensional settings arising with large DAGs. This challenge arises because the space of possible DAGs grows at a superexponential rate with the number of nodes, e.g., Chickering (2002)."
        },
        {
            "heading": "A.1 BACKGROUND ON SMOOTHED CONCOMITANT LASSO ESTIMATORS",
            "text": "Consider a linear regression setting y = H\u03b8+\u03f5 in which we have access to a response vector y \u2208 Rd and a design matrix H \u2208 Rd\u00d7p comprising p explanatory variables or features. To obtain a sparse vector of regression coefficients \u03b8 \u2208 Rp, one can utilize the convex lasso estimator (Tibshirani, 1996)\n\u03b8\u0302 \u2208 argmin \u03b8\n1\n2d \u2225y \u2212H\u03b8\u222522 + \u03bb\u2225\u03b8\u22251 (7)\nwhich facilitates continuous estimation and variable selection. Statistical guarantees for lasso hinge on selecting the scalar parameter \u03bb > 0 to be proportional to the noise level. In particular, under the assumption that \u03f5 \u223c N (0, \u03c32Id), solving (7) with \u03bb\u2217 \u224d \u03c3 \u221a log p d yields the minimax optimal solution for parameter estimation in high dimensions (Bickel et al., 2009; Li et al., 2020). However, in most cases having knowledge of the noise variance is a luxury we may not possess.\nTo address the aforementioned challenge, a promising solution involves the simultaneous estimation of both sparse regression coefficients and the noise level. Over the years, various formulations have been proposed to tackle this problem, ranging from penalized maximum likelihood approaches (Sta\u0308dler et al., 2010) to frameworks inspired by robust theory (Huber, 1981; Owen, 2007). Several of these methods are closely related; in particular, the concomitant lasso approach in (Owen, 2007) has been shown to be equivalent to the so-termed square-root lasso (Belloni et al., 2011), and rediscovered as the scaled lasso estimator (Sun & Zhang, 2012). Notably, the smoothed concomitant lasso (Ndiaye et al., 2017) stands out as the most recent and efficient method suitable for high-dimensional settings. The approach in (Ndiaye et al., 2017) is to jointly estimate the regression coefficients \u03b8 and the noise level \u03c3 by solving the jointly convex problem\nmin \u03b8,\u03c3\n1\n2d\u03c3 \u2225y \u2212H\u03b8\u222522 +\n\u03c3 2 + \u03bb\u2225\u03b8\u22251 + I {\u03c3 \u2265 \u03c30} , (8)\nwhere \u03c30 is a predetermined lower bound based on either prior information or proportional to the initial noise standard deviation, e.g., \u03c30 =\n\u2225y\u22252\u221a d \u00d7 10\u22122. Inclusion of the constraint \u03c3 \u2265 \u03c30 is motivated in (Ndiaye et al., 2017) to prevent ill-conditioning as the solution \u03c3\u0302 approaches zero. To solve (8), a BCD algorithm is adopted wherein one iteratively (and cyclically) solves (8) for \u03b8 with fixed \u03c3, and subsequently updates the value of \u03c3 using a closed-form solution given the most up-to-date value of \u03b8; see (Ndiaye et al., 2017) for further details.\nInterestingly, disregarding the constraint \u03c3 \u2265 \u03c30 and plugging the noise estimator \u03c3\u0302 = \u2225y \u2212 H\u03b8\u22252/ \u221a d in (8), yields the squared-root lasso problem (Belloni et al., 2011), i.e.,\nmin \u03b8 1\u221a d \u2225y \u2212H\u03b8\u22252 + \u03bb\u2225\u03b8\u22251. (9)\nThis problem is convex but with the added difficulty of being non-smooth when y = H\u03b8. It has been proven that the solutions to problems (8) and (9) are equivalent (Ndiaye et al., 2017). Moreover, one can show that if \u03f5 \u223c N (0, \u03c32Id) and \u03bb\u2217 \u224d \u221a log p d , the solution of (9) is minimax optimal, and notably, it is independent of the noise scale \u03c3. Remark. Li et al. (2020) noticed that the non-differentiability of the squared-root lasso is not an issue, in the sense that a subgradient can be used safely, if one is guaranteed to avoid the singularity. For DAG estimation, due to the exogenous noise in the linear SEM, we are exactly in this situation. However, we point out that this alternative makes the objective function not separable across samples, precluding stochastic optimization that could be desirable for scalability. Nonetheless, we leave the exploration of this alternative as future work.\nA limitation of the smoothed concomitant lasso in (8) is its inability to handle multi-task settings where response data Y \u2208 Rd\u00d7n is collected from d diverse sources with varying noise levels. Statistical and algorithmic issues pertaining to this heteroscedastic scenario have been successfully addressed in (Massias et al., 2018), by generalizing the smoothed concomitant lasso formulation to perform joint estimation of the coefficient matrix \u0398 \u2208 Rp\u00d7n and the square-root of the noise covariance matrix \u03a3 = diag(\u03c31, . . . , \u03c3d) \u2208 Rd\u00d7d. Accordingly, the so-termed generalized concomitant multi-task lasso is given by\nmin \u0398,\u03a3\n1\n2nd \u2225Y \u2212H\u0398\u22252\u03a3\u22121 + tr(\u03a3) 2d + \u03bb\u2225\u0398\u22252,1 + I {\u03a3 \u2265 \u03a30} . (10)\nSimilar to the smoothed concomitant lasso, the matrix \u03a30 is either assumed to be known a priori, or, it is estimated from the data, e.g., via \u03a30 =\n\u2225Y\u2225F\u221a nd \u00d7 10\u22122 as suggested in (Massias et al., 2018)."
        },
        {
            "heading": "B ALGORITHMIC DERIVATIONS",
            "text": "In this section, we compute the gradients of the proposed score functions with respect to W, derive closed-form solutions for updating \u03c3 and \u03a3, and discuss the associated computational complexity."
        },
        {
            "heading": "B.1 EQUAL NOISE VARIANCE",
            "text": "Gradients. We introduced the score function S(W, \u03c3) = 12n\u03c3\u2225X \u2212W \u22a4X\u22252F + d\u03c32 + \u03bb\u2225W\u22251 in (2). Defining f(W, \u03c3) := 12n\u03c3\u2225X \u2212 W \u22a4X\u22252F + d\u03c32 , we calculate the gradient of f(W, \u03c3) w.r.t. W, for fixed \u03c3 = \u03c3\u0302.\nThe smooth terms in the score function can be rewritten as\nf(W, \u03c3\u0302) = 1\n2n\u03c3\u0302 \u2225X\u2212W\u22a4X\u22252F +\nd\u03c3\u0302\n2 (11)\n= 1 2n\u03c3\u0302 Tr ( (X\u2212W\u22a4X)\u22a4(X\u2212W\u22a4X) ) + d\u03c3\u0302 2 (12)\n= 1 2n\u03c3\u0302 Tr ( X\u22a4(I\u2212W)(I\u2212W\u22a4)X ) + d\u03c3\u0302 2 (13)\n= 1 2\u03c3\u0302 Tr ( (I\u2212W)\u22a4 cov(X)(I\u2212W) ) + d\u03c3\u0302 2 , (14)\nwhere cov(X) = 1nXX \u22a4 is the sample covariance matrix. Accordingly, the gradient of f(W, \u03c3) is\n\u2207Wf(W, \u03c3\u0302) = \u2212 (XX\u22a4)\nn\u03c3\u0302 +\n(XX\u22a4)W\nn\u03c3\u0302 (15)\n= \u2212 1 \u03c3\u0302 cov(X)[I\u2212W]. (16)\nClosed-form solution for \u03c3\u0302 in (4). We update \u03c3 by fixing W to W\u0302 and computing the minimizer in closed form. The first-order optimality condition yields\n\u2202f(W\u0302, \u03c3)\n\u2202\u03c3 = \u22121 2n\u03c32 \u2225X\u2212 W\u0302\u22a4X\u22252F + d 2 = 0. (17)\nHence,\n\u03c3\u03022 = 1\nnd \u2225X\u2212 W\u0302\u22a4X\u22252F (18)\n= 1 nd Tr ( (X\u2212 W\u0302\u22a4X)\u22a4(X\u2212 W\u0302\u22a4X) ) (19) = 1 nd Tr ( X\u22a4(I\u2212 W\u0302)(I\u2212 W\u0302\u22a4)X ) (20) = 1 nd Tr ( (I\u2212 W\u0302\u22a4)XX\u22a4(I\u2212 W\u0302) ) (21) = 1 d Tr ( (I\u2212 W\u0302)\u22a4 cov(X)(I\u2212 W\u0302) ) . (22)\nBecause of the constraint \u03c3 \u2265 \u03c30, the minimizer is given by\n\u03c3\u0302 = max (\u221a Tr ((I\u2212W)\u22a4 cov(X)(I\u2212W)) /d, \u03c30 ) . (23)\nComplexity. The gradient with respect to W involves subtracting two matrices of size d\u00d7d, resulting in a computational complexity of O(d2). Additionally, three matrix multiplications contribute to a complexity of O(nd2 + d3). The subsequent element-wise division adds O(d2). Therefore, the main computational complexity of gradient computation is O(d3), on par with state-of-the-art continuous optimization methods for DAG learning. Similarly, the closed-form solution for updating \u03c3 has a computational complexity of O(d3), involving two matrix subtractions (O(2d2)), four matrix multiplications (O(nd2 + 2d3)), and matrix trace operations (O(d)). Online updates. The closed-form solution for \u03c3\u0302 in (4) can be used in a batch setting when all samples are jointly available. The proposed score function allows to design a solution for this scenario, as the updates naturally decompose in time (i.e., across samples). In the mini-batch setting, at each iteration t, we have access to a randomly selected subset of data Xt \u2208 Rd\u00d7nb with nb < n as the size of the mini-batch. Consequently, we could utilize mini-batch stochastic gradient descent as the optimization method within our framework. Given an initial solution W\u03020, it is possible to conceptualize an online algorithm with the following iterations for t = 1, 2, . . .\n1. Compute the sample covariance matrix cov(Xt) = 1t ( (t\u2212 1) cov(Xt\u22121) + 1nbXtX \u22a4 t ) .\n2. Compute W\u0302t using cov(Xt) and a first-order update as in Algorithm 1. 3. Compute the noise estimate \u03c3\u0302t using cov(Xt) and (4).\nAlternatively, we could keep sufficient statistics for the noise estimate. In this case, the update would proceed as follows. We first compute the residual \u03f5t = 1nbd\u2225Xt\u2212W\u0302 \u22a4 t\u22121Xt\u222522 and update the\nsufficient statistic et = et\u22121 + \u03f5t. Finally, we compute the noise estimate \u03c3\u0302t = max (\u221a 1 t et, \u03c30 ) . The use of this type of sufficient statistics in online algorithms has a long-standing tradition in the adaptive signal processing and online learning literature (e.g., Mairal et al., 2010). Although the above iterations form the conceptual sketch for an online algorithm, many important details need to be addressed to achieve a full online solution and we leave this work for the future. Our goal here was to illustrate that resulting DAG structure and scale estimators are decomposable across samples.\nTo empirically explore the mentioned sufficient statistics, we conducted an experiment using minibatches of data. The performance of these optimization methods is tested on an ER4 graph of size d = 50, where we generate n = 1000 i.i.d samples using a linear SEM. We assume the noise distribution is Gaussian, and the noise variances are equal. We explore two distinct batch sizes, namely 100 and 500, corresponding to using 10% and 50% of the data at each iteration, respectively. To monitor how well these algorithms track the output of the CoLiDE-EV algorithm, denoted as W\u22c6 and \u03c3\u22c6, we calculate the relative error \u2225Wt\u2212W \u22c6\u2225F\n\u2225W\u22c6\u2225F at each iteration t. Similarly, the relative error\nfor the noise scale is computed as |\u03c3t\u2212\u03c3 \u22c6|\n|\u03c3\u22c6| . As outlined in Section 4, we address CoLiDE-EV for sequences of decreasing \u00b5k. To enhance visual clarity, we focus on Wt and \u03c3t for the smallest \u00b5k.\nThe experimental results, shown in Figure 4, demonstrate that mini-batch stochastic gradient descent with varying batch sizes adeptly follows the output of CoLiDE-EV for both the DAG adjacency matrix (left) and the noise level (right). While these findings showcase promising results, we acknowledge the need for further evaluation to conclusively assert the decomposability benefits of our method. This experiment serves as an initial exploration, providing valuable insights for a subsequent analysis of the online updates."
        },
        {
            "heading": "B.2 NON-EQUAL NOISE VARIANCE",
            "text": "Gradients. We introduced the score function S(W,\u03a3) in (5). Upon defining\ng(W,\u03a3) = 1 2n Tr ( (X\u2212W\u22a4X)\u22a4\u03a3\u22121(X\u2212W\u22a4X) ) + 1 2 Tr(\u03a3), (24)\nwe derive the gradient of g(W,\u03a3) w.r.t. W, while maintaining \u03a3 = \u03a3\u0302 fixed. To this end, note that g(W, \u03a3\u0302) can be rewritten as\ng(W, \u03a3\u0302) = 1 2n Tr ( (X\u2212W\u22a4X)\u22a4\u03a3\u0302 \u22121 (X\u2212W\u22a4X) ) + 1 2 Tr(\u03a3\u0302) (25)\n= 1 2n Tr ( \u03a3\u0302 \u22121 (X\u2212W\u22a4X)(X\u2212W\u22a4X)\u22a4 ) + 1 2 Tr(\u03a3\u0302) (26)\n= 1\n2 Tr\n( \u03a3\u0302 \u22121 (I\u2212W\u22a4)XX \u22a4\nn (I\u2212W)\n) + 1\n2 Tr(\u03a3\u0302) (27)\n= 1 2 Tr ( \u03a3\u0302 \u22121 (I\u2212W)\u22a4 cov(X)(I\u2212W) ) + 1 2 Tr(\u03a3\u0302). (28)\nAccordingly, the gradient of g(W,\u03a3) is\n\u2207Wg(W, \u03a3\u0302) = 1\n2n\n[ \u2212XX\u22a4\u03a3\u0302 \u2212\u22a4 \u2212XX\u22a4\u03a3\u0302 \u22121 +XX\u22a4W\u03a3\u0302 \u2212\u22a4 +XX\u22a4W\u03a3\u0302 \u22121] (29)\n= \u2212XX\u22a4\u03a3\u0302\n\u22121\nn +\nXX\u22a4W\u03a3\u0302 \u22121\nn (30)\n= \u2212 cov(X) [I\u2212W] \u03a3\u0302 \u22121 . (31)\nClosed-form solution for \u03a3\u0302 in (6). We update \u03a3 by fixing W to W\u0302 and computing the minimizer in closed form. The first-order optimality condition yields\n\u2207\u03a3g(W\u0302,\u03a3) = \u2212\u03a3\u2212\u22a4\n2n (X\u2212 W\u0302\u22a4X)(X\u2212 W\u0302\u22a4X)\u22a4\u03a3\u2212\u22a4 + 1 2 I = 0. (32)\nHence,\n\u03a3\u0302 2 =\n(X\u2212 W\u0302\u22a4X)(X\u2212 W\u0302\u22a4X)\u22a4\nn (33)\n= (I\u2212 W\u0302)\u22a4 cov(X)(I\u2212 W\u0302). (34)\nNote that we can ascertain \u03a3\u0302 is a diagonal matrix, based on SEM assumptions of exogenous noises being mutually independent. Because of the constraint \u03a3 \u2265 \u03a30, the minimizer is given by\n\u03a3\u0302 = max (\u221a diag ((I\u2212W)\u22a4 cov(X)(I\u2212W)),\u03a30 ) , (35)\nwhere \u221a\n(\u00b7) and max(\u00b7) indicates an element-wise operation, while the operator diag(\u00b7) extracts the diagonal elements of a matrix.\nComplexity. The gradient with respect to W entails the subtraction of two d \u00d7 d matrices, resulting in O(d2) complexity. The inverse of a diagonal matrix, involved in the calculation, also incurs a complexity of O(d2). Additionally, four matrix multiplications contribute to a complexity of O(nd2 + 2d3). The subsequent element-wise division introduces an additional O(d2). Consequently, the principal computational complexity of gradient computation is O(d3), aligning with the computational complexity of state-of-the-art methods. The closed-form solution for updating \u03a3 has a computational complexity of O(d3), involving two matrix subtractions (O(2d2)), four matrix multiplications (O(nd2 + 2d3)), and two element-wise operations on the diagonal elements of a d\u00d7 d matrix (O(2d)). Online updates. As in the homoscedastic case, we can devise an online algorithm based on CoLiDE-NV. The main difference is that we now need a collection of d sufficient statistics, one for each node in the graph. For j = 1, . . . , d and all t = 1, 2, . . ., we now have\n\u03f5j,t = ( (xt)j \u2212 (W\u0302\u22a4t\u22121xt)j )2 , (36)\nej,t = ej,t\u22121 + \u03f5j,t, (37)\n\u03c3\u0302j,t = max\n(\u221a 1\nt ej,t, \u03c30\n) , (38)\nwhere xt = [(xt)1, . . . , (xt)d] and ej,0 = 0 for all j. Using xt, the noise estimates {\u03c3\u03021,t, . . . , \u03c3\u0302d,t}, and W\u0302t\u22121, we can compute W\u0302t, e.g., using a first-order update as in Algorithm 1. Although these elements provide the foundation for an online algorithm, we leave the completion of this new technique as future work. In any case, this shows the updates decompose across samples."
        },
        {
            "heading": "B.3 GRADIENT OF THE LOG-DETERMINANT ACYCLICITY FUNCTION",
            "text": "We adopt Hldet(W, s) = d log(s)\u2212 log(det(sI\u2212W \u25e6W)) as the acyclicity function in CoLiDE\u2019s formulation. As reported in Bello et al. (2022), the gradient of Hldet(W, s) is given by\n\u2207Hldet(W) = 2 [sI\u2212W \u25e6W]\u2212\u22a4 \u25e6W. (39)\nThe computational complexity incurred in each gradient evaluation is O(d3). This includes a matrix subtraction (O(d2)), four element-wise operations (O(4d2)), and a matrix inversion (O(d3)). Complexity summary. All in all, both CoLiDE variants, CoLiDE-EV and CoLiDE-NV, incur a per iteration computational complexity of O(d3). While CoLiDE concurrently estimates the unknown noise levels along with the DAG topology, this comes with no significant computational complexity overhead relative to state-of-the-art continuous relaxation methods for DAG learning."
        },
        {
            "heading": "C GUARANTEES FOR (HETEROSCEDASTIC) LINEAR GAUSSIAN SEMS",
            "text": "Consider a general linear Gaussian SEM, whose exogenous noises can have non-equal variances (i.e., the heteroscedastic case). This is a non-identifiable model (Peters et al., 2017), meaning that the ground-truth DAG cannot be uniquely recovered from observational data alone. Still, we argue that the interesting theoretical analysis framework put forth in (Ghassami et al., 2020) and (Ng et al., 2020) \u2013 as well as its conclusions \u2013 carry over to CoLiDE. The upshot is that just like GOLEM, the solution of the CoLiDE-NV problem with \u00b5k \u2192 \u221e asymptotically (in n) will be a DAG equivalent to the ground truth. The precise notion of (quasi)-equivalence among directed graphs was introduced by Ghassami et al. (2020), and we will not reproduce all definitions and technical details here. The interested reader is referred to (Ng et al., 2020, Section 3.1).\nSpecifically, it follows that under the same assumptions in (Ng et al., 2020, Section 3.1), Theorems 1 and 2 therein hold for CoLiDE-NV when \u00b5k \u2192 \u221e. Moreover, (Ng et al., 2020, Corollary 1) holds as well. This corollary motivates augmenting the score function in (5) with the DAG penalty Hldet(W, sk), to obtain a DAG solution quasi-equivalent to the ground truth DAG in lieu of the sotermed \u2018Triangle assumption\u2019. The proofs of these results are essentially identical to the ones in (Ng et al., 2020, Appendix B), so in the sequel we only highlight the minor differences.\nLet G\u22c6 and \u0398 be the ground truth DAG and the precision matrix of the random vector x \u2208 Rd, so that the generated distribution is x \u223c N (0,\u0398\u22121). Let W and \u03a32 be the weighted adjacency matrix and the diagonal matrix containing exogenous noise variances \u03c32i , respectively. As n \u2192 \u221e, the law of large numbers asserts that the sample covariance matrix cov(X) \u2192 \u0398\u22121, almost surely. Then, if we drop the penalty term, \u00b5k \u2192 \u221e, and considering \u03bb such that both trace terms in the score function in (5) dominate asymptotically, CoLiDE-NV\u2019s optimality condition implies [cf. (34)]\n\u03a3\u0302 2 = (I\u2212 W\u0302)\u22a4\u0398\u22121(I\u2212 W\u0302). (40)\nNote that when \u00b5k \u2192 \u221e, CoLiDE-NV is convex and we are thus ensured to attain global optimality. This means that we will find a pair (W\u0302, \u03a3\u0302) of estimates such that (I \u2212 W\u0302)\u03a3\u0302 \u22122 (I \u2212 W\u0302)\u22a4 = \u0398,\nand denote the directed graph corresponding to W\u0302 by G\u0302(W\u0302). One can readily show that under the mild assumptions in (Ng et al., 2020), G\u0302(W\u0302) is quasi equivalent to G\u22c6. The remaining steps of the proofs of both Theorems are identical to those laid out by Ng et al. (2020, Appendix B).\nD IMPLEMENTATION DETAILS\nIn this section, we provide a comprehensive description of the implementation details for the experiments conducted to evaluate and benchmark the proposed DAG learning algorithms."
        },
        {
            "heading": "D.1 COMPUTING INFRASTRUCTURE",
            "text": "All experiments were executed on a 2-core Intel Xeon processor E5-2695v2 with a clock speed of 2.40 GHz and 32GB of RAM. For models like GOLEM and DAGuerreotype that necessitate GPU processing, we utilized either the NVIDIA A100, Tesla V100, or Tesla T4 GPUs."
        },
        {
            "heading": "D.2 GRAPH MODELS",
            "text": "In our experiments, each simulation involves sampling a graph from two prominent random graph models:\n\u2022 Erdo\u030bs-Re\u0301nyi (ER): These random graphs have independently added edges with equal probability. The chosen probability is determined by the desired nodal degree. Since ER graphs are undirected, we randomly generate a permutation vector for node ordering and orient the edges accordingly.\n\u2022 Scale-Free (SF): These random graphs are generated using the preferential attachment process (Baraba\u0301si & Albert, 1999). The number of edges preferentially attached is based on the desired nodal degree. The edges are oriented each time a new node is attached, resulting in a sampled directed acyclic graph (DAG)."
        },
        {
            "heading": "D.3 METRICS",
            "text": "We employ the following standard metrics commonly used in the context of DAG learning:\n\u2022 Structural Hamming distance (SHD): Quantifies the total count of edge additions, deletions, and reversals required to transform the estimated graph into the true graph. Normalized SHD is obtained by dividing this count by the number of nodes.\n\u2022 SHD-C: Initially, we map both the estimated graph and the ground truth DAG to their corresponding Completed Partially Directed Acyclic Graphs (CPDAGs). A CPDAG represents a Markov equivalence class encompassing all DAGs that encode identical conditional independencies. Subsequently, we calculate the SHD between the two CPDAGs. The normalized version of SHD-C is obtained by dividing the original metric by the number of nodes.\n\u2022 Structural Intervention Distance (SID): Counts the number of causal paths that are disrupted in the predicted DAG. When divided by the number of nodes, we obtain the normalized SID.\n\u2022 True Positive Rate (TPR): Measures the proportion of correctly identified edges relative to the total number of edges in the ground-truth DAG.\n\u2022 False Discovery Rate (FDR): Represents the ratio of incorrectly identified edges to the total number of detected edges.\nFor all metrics except TPR, a lower value indicates better performance."
        },
        {
            "heading": "D.4 NOISE DISTRIBUTIONS",
            "text": "We generate data by sampling from a set of linear SEMs, considering three distinct noise distributions:\n\u2022 Gaussian: zi \u223c N (0, \u03c32i ), i = 1, . . . , d, where the noise variance of each node is \u03c32i \u2022 Exponential: zi \u223c Exp(\u03bbi), i = 1, . . . , d, where the noise variance of each node is \u03bb\u22122i . \u2022 Laplace: zi \u223c Laplace(0, bi), i = 1, . . . , d, where the noise variance of each node is 2b2i ."
        },
        {
            "heading": "D.5 BASELINE METHODS",
            "text": "To assess the performance of our proposed approach, we benchmark it against several state-of-theart methods commonly recognized as baselines in this domain. All the methods are implemented using Python.\n\u2022 GOLEM: This likelihood-based method was introduced by Ng et al. (2020). The code for GOLEM is publicly available on GitHub at https://github.com/ignavier/ golem. Utilizing Tensorflow, GOLEM requires GPU support. We adopt their recommended hyperparameters for GOLEM. For GOLEM-EV, we set \u03bb1 = 2\u00d710\u22122 and \u03bb2 = 5, and for GOLEM-NV, \u03bb1 = 2\u00d7 10\u22123 and \u03bb2 = 5; refer to Appendix F of Ng et al. (2020) for details. Closely following the guidelines in Ng et al. (2020), we initialize GOLEM-NV with the output of GOLEM-EV.\n\u2022 DAGMA: We employ linear DAGMA as introduced in Bello et al. (2022). The code is available at https://github.com/kevinsbello/dagma. We adhere to their recommended hyperparameters: T = 4, s = {1, .9, .8, .7}, \u03b21 = 0.05, \u03b1 = 0.1, and \u00b5(0) = 1; consult Appendix C of Bello et al. (2022) for specifics.\n\u2022 GES: This method utilizes a fast greedy approach for learning DAGs, outlined in Ramsey et al. (2017). The implementation leverages the py-causal Python package and can be accessed at https://github.com/bd2kccd/py-causal. We configure the hyperparameters as follows: scoreId = \u2019cg-bic-score\u2019, maxDegree = 5, dataType = \u2019continuous\u2019, and faithfulnessAssumed = False.\n\u2022 SortNRegress: This method adopts a two-step framework involving node ordering based on increasing variance and parent selection using the Least Angle Regressor (Reisach et al., 2021). The code is publicly available at https://github.com/Scriddie/ Varsortability.\n\u2022 DAGuerreotype: This recent approach employs a two-step framework to learn node orderings through permutation matrices and edge representations, either jointly or in a bilevel fashion (Zantedeschi et al., 2023). The implementation is accessible at https: //github.com/vzantedeschi/DAGuerreotype and can utilize GPU processing. We employ the linear version of DAGuerreotype with sparseMAP as the operator for node ordering learning. Remaining parameters are set to defaults as per their paper. For real datasets, we use bi-level optimization with the following hyperparameters: K = 100, pruning reg=0.01, lr theta=0.1, and standardize=True. For synthetic simulations, we use joint optimization with K = 10."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENTS",
            "text": "Here, we present supplementary experimental results that were excluded from the main body of the paper due to page limitations."
        },
        {
            "heading": "E.1 ADDITIONAL METRICS FOR THE HOMOSCEDASTIC EXPERIMENTS",
            "text": "For the homoscedastic experiments in Section 5, we exclusively presented the normalized SHD for 200-node graphs at various noise levels. To offer a comprehensive analysis, we now introduce additional metrics\u2014SID and FDR\u2014in Figure 5. We have also incorporated normalized SHD-C, as illustrated in Figure 6. The results in Figures 5 and 6 demonstrate the consistent superiority of CoLiDE-EV over other methods in both metrics, across all noise variances."
        },
        {
            "heading": "E.2 SMALLER GRAPHS WITH HOMOSCEDASTIC ASSUMPTION",
            "text": "For the homoscedastic experiments in Section 5, we exclusively examined graphs comprising 200 nodes. Here, for a more comprehensive analysis, we extend our investigation to encompass smaller\ngraphs, specifically those with 100 and 50 nodes, each with a nodal degree of 4. Utilizing the Linear SEM model, as detailed in Section 5, and assuming equal noise variances across nodes, we generated the respective data. The resulting normalized SHD and FDR are depicted in Figure 7, where the first three rows present results for 100-node graphs, and the subsequent rows showcase findings for 50-node graphs. It is notable that as the number of nodes decreases, GOLEM\u2019s performance improves, benefitting from a greater number of data samples per variable. However, even in scenarios with fewer nodes, CoLiDE-EV consistently outperforms other state-of-the-art methods, including GOLEM."
        },
        {
            "heading": "E.3 ADDITIONAL METRICS FOR THE HETEROSCEDASTIC EXPERIMENTS",
            "text": "For the heteroscedastic experiments in Section 5, we exclusively displayed normalized SHD for various node counts and noise distributions. To ensure a comprehensive view, we now include normalized SID and FDR in Figure 8 and normalized SHD-C in Figure 9, representing the same graphs analyzed in Section 5. Figures 8 and 9 demonstrate the consistent superiority of CoLiDENV over other state-of-the-art methods across multiple metrics."
        },
        {
            "heading": "E.4 DATA STANDARDIZATION IN HETEROSCEDASTIC SETTINGS",
            "text": "In simulated SEMs with additive noise, a phenomenon highlighted in (Reisach et al., 2021) indicates that the data generation model might inadvertently compromise the underlying structure, thereby unexpectedly influencing structure learning algorithms. Reisach et al. (2021) argue that, for generically sampled additive noise models, the marginal variance tends to increase along the causal order. In response to this observation, they propose a straightforward DAG learning algorithm named SortNRegress (see Appendix D.5), which we have integrated into our experiments to demonstrate the challenging nature of the task at hand. To mitigate the information leakage caused by such inadvertent model effects, Reisach et al. (2021) suggest standardizing the data of each node to have zero mean and unit variance.\nWe note that such standardization constitutes a non-linear transformation that can markedly impact performance, particularly for those methods developed for linear SEMs. For our experiments here, we utilized the same graphs presented in the heteroscedastic settings section of the paper but standardized the data. The recovery performance of the algorithms tests is evaluated using the normalized SHD and SHD-C metrics, as illustrated in Figure 10. Despite the overall degradation in per-\nformance resulting from such standardization, CoLiDE-NV continues to outperform state-of-the-art algorithms in this challenging task.\nWe also conducted a similar standardization on 10 small and sparse ER1 graphs, each comprising d = 20 nodes, with a substantial dataset of n = 100, 000 i.i.d. observations. This time, we considered the greedy algorithm GES (see Appendix D.5) for comparisons. Prior to standardization, the performance of the algorithms in terms of SHD was as follows: CoLiDE-EV: 12.8; CoLiDE-NV: 14.9; and GES: 14.9. Post-standardization, the SHD values changed to CoLiDE-EV: 18.8; CoLiDENV: 19.5; and GES: 14.9. Notably, the performance of GES remained unaffected by standardization. While CoLiDE exhibited superior performance compared to others based on continuous relaxation (see Figure 10), it displayed some sensitivity to standardization, indicating that it is not as robust in this aspect as greedy algorithms. We duly acknowledge this as a limitation of our approach."
        },
        {
            "heading": "E.5 HIGH SNR SETTING WITH HETEROSCEDASTICITY ASSUMPTION",
            "text": "For the heteroscedastic experiments in Section 5, we discussed how altering edge weights can impact the SNR, consequently influencing the problem\u2019s complexity. In this section, we explore a less challenging scenario by setting edge weights to be drawn from [\u22122,\u22120.5] \u222a [0.5, 5], creating an easier counterpart to the heteroscedastic experiments in Section 5. We maintain the assumption of varying noise variances across nodes. The normalized SHD and FDR for these experiments are presented in Figure 11. As illustrated in Figure 11, CoLiDE variants consistently outperform other state-of-the-art methods in this experiment. Interestingly, CoLiDE-EV performs on par with\nCoLiDE-NV in certain cases and even outperforms CoLiDE-NV in others, despite the mismatch in noise variance assumption. We attribute this to the problem\u2019s complexity not warranting the need for a more intricate framework like CoLiDE-NV in some instances."
        },
        {
            "heading": "E.6 LARGER AND SPARSER GRAPHS",
            "text": "It is a standard practice to evaluate the recovery performance of proposed DAG learning algorithms on large sparse graphs, as demonstrated in recent studies (Ng et al., 2020; Bello et al., 2022). Therefore, we conducted a similar analysis, considering both Homoscedastic and Heteroscedastic settings, with an underlying graph nodal degree of 2.\nHomoscedastic setting. We generated 10 distinct ER and SF DAGs, each consisting of 500 nodes and 1000 edges. The data generation process aligns with the setting described in Section 5, assuming equal noise variance for each node. We replicated this analysis across various noise levels and distributions. The performance results, measured in terms of Normalized SHD and FDR, are presented in Figure 12. CoLiDE-EV consistently demonstrates superior performance in most scenarios. Notably, GOLEM-EV performs better on sparser graphs and, in ER2 with an Exponential noise distribution and high variance, it marginally outperforms CoLiDE-EV. However, CoLiDEEV maintains a consistent level of performance across different noise distributions and variances in homoscedastic settings.\nHeteroscedastic setting. Similarly, we generated sparse DAGs ranging from 200 nodes to 500 nodes, assuming a nodal degree of 2. However, this time, we generated data under the heteroscedasticity assumption and followed the settings described in Section 5. The DAG recovery performance is summarized in Figure 13, where we report Normalized SHD and FDR across different noise distributions, graph models, and numbers of nodes. In most cases, CoLiDE-NV outperforms other stateof-the-art algorithms. In a few cases where CoLiDE-NV is the second-best, CoLiDE-EV emerges as the best-performing method, showcasing the superiority of CoLiDE\u2019s variants in different noise scenarios."
        },
        {
            "heading": "E.7 DENSE GRAPHS",
            "text": "To conclude our analysis, we extended our consideration to denser DAGs with a nodal degree of 6.\nHomoscedastic setting. We generated 10 distinct ER and SF DAGs, each comprising 100 and 200 nodes, assuming a nodal degree of 6. The data generation process was in line with the approach detailed in Section 5, assuming equal noise variance for each node. This analysis was replicated across varying noise levels and distributions. The performance results, assessed in terms of normalized SHD and FDR, are presented in Figure 14. CoLiDE-EV consistently demonstrates superior\nperformance across all scenarios for DAGs with 100 nodes (Figure 14 - bottom three rows), and in most cases for DAGs with 200 nodes (Figure 14 - top three rows). Interestingly, in the case of 200-node DAGs, we observed that GOLEM-EV outperforms CoLiDE-EV in terms of normalized SHD in certain scenarios. However, a closer inspection through FDR revealed that GOLEM-EV fails to detect many of the true edges, resulting in a higher number of false positives. Consequently, the SHD metric tends to converge to the performance of CoLiDE-EV due to the fewer detected edges overall. This suggests that CoLiDE-EV excels in edge detection. It appears that CoLiDE-EV encounters challenges in handling high-noise scenarios in denser graphs compared to sparser DAGs like ER2 or ER4. Nonetheless, it still outperforms other state-of-the-art methods\nHeteroscedastic setting. In this analysis, we also incorporate the heteroscedasticity assumption while generating data as outlined in Section 5. Dense DAGs, ranging from 50 nodes to 200 nodes and assuming a nodal degree of 6, were created. The DAG recovery performance is summarized in Figure 15, presenting normalized SHD (first two columns) and FDR across varying noise distributions, graph models, and node numbers. CoLiDE-NV consistently exhibits superior performance compared to other state-of-the-art algorithms in the majority of cases. In a few instances where CoLiDE-NV is the second-best, GOLEM-EV emerges as the best-performing method by a small margin. These results reinforce the robustness of CoLiDE-NV across a spectrum of scenarios, spanning sparse to dense graphs.\nE.8 INTEGRATING COLIDE WITH OTHER OPTIMIZATION METHODS\nCoLiDE introduces novel convex score functions for the homoscedastic and heteroscedastic settings in (2) and (5), respectively. Although we propose an inexaxt BCD optimization algorithm based on a continuous relaxation of the NP-hard problem, our convex score functions can be readily optimized using other techniques. For instance, TOPO (Deng et al., 2023a) advocates a bi-level optimization algorithm. In the outer level, the algorithm performs a discrete optimization over topological orders by iteratively swapping pairs of nodes within the topological order of a DAG. At the inner level, given a topological order, the algorithm optimizes a score function. When paired with a convex score function, TOPO is guaranteed to find a local minimum and yields solutions with lower scores. The integration of TOPO and CoLiDE combines the former\u2019s appealing convergence properties with the latter\u2019s attractive features discussed at length in the paper.\nUsing the available code for TOPO in https://github.com/Duntrain/TOPO, here we provide an initial assessment of this novel combination, denoted as CoLiDE-TOPO. We generate 10 different ER graphs with 50 nodes from a linear Gaussian SEM, where the noise variance of each node was randomly selected from the range [0.5, 10]. For the parameters of TOPO, we adhered to the values (s0 = 10, ssmall = 100, and slarge = 1000) recommended by Deng et al. (2023a). The results in Table 4 show that utilizing TOPO as an optimizer enables CoLiDE-NV to attain better performance, with up to 30% improvements in terms of SHD and for this particular setting.\nGranted, the selection of the solver depends on the problem at hand. While discrete optimizers excel over smaller graphs, continuous optimization tends to be preferred for larger DAGs (with hundreds of nodes). Our experiments show that CoLiDE offers advantages in both of these settings."
        },
        {
            "heading": "E.9 EXAMINATION OF THE ROLE OF SCORE FUNCTIONS",
            "text": "Here we conduct an empirical investigation on the role of score functions in recovering the underlying DAG, by decoupling their effect from the need of a (soft or hard) DAG constraint. Given a DAG G(W), the corresponding weighted adjacency matrix W can be expressed as W = \u03a0\u22a4U\u03a0, where \u03a0 \u2208 {0, 1}d\u00d7d is a permutation matrix (effectively encoding the causal ordering of variables), and U \u2208 Rd\u00d7d is an upper-triangular weight matrix. Assuming we know the ground truth \u03a0 and parameterizing the DAG as W = \u03a0\u22a4U\u03a0 so that an acyclicity constraint is no longer needed, the DAG learning problem boils down to minimizing the score function S(W) solely with respect to U\nmin U\nS ( \u03a0\u22a4U\u03a0 ) . (41)\nNotice that (41) is a convex optimization problem if the score function is convex. Subsequently, we reconstruct the DAG structure using the true permutation matrix and the estimated upper-triangular weight matrix W\u0302 = \u03a0\u22a4U\u0302\u03a0.\nIn this setting, we evaluate the CoLiDE-EV and CoLiDE-NV score functions in (2) and (5). In comparison, we assess the profile log-likelihood score function used by GOLEM (Ng et al., 2020). As a reminder, the GOLEM-NV score function tailored for heteroscedastic settings, is given by\n\u22121 2 d\u2211 i=1 log (\u2225\u2225xi \u2212w\u22a4i X\u2225\u222522)+ log (|det(I\u2212W)|) + \u03bb\u2225W\u22251, (42)\nwhere xi \u2208 Rn represents node i\u2019s data, and W = [w1, . . . ,wd] \u2208 Rd\u00d7d. The GOLEM-EV score function, a simplified version of (42), is given by\n\u2212d 2 log (\u2225\u2225X\u2212W\u22a4X\u2225\u22252 F ) + log (|det(I\u2212W)|) + \u03bb\u2225W\u22251. (43)\nIn all cases we select \u03bb = 0.002 by following the guidelines in Ng et al. (2020).\nIt is important to note that the Gaussian log-likelihood score function used by GOLEM incorporates a noise-dependent term \u2212 12 \u2211d i=1 log \u03c3 2 i , as discussed in Ng et al. (2020, Appendix C), which is profiled out during optimization. This noise-dependent term bears some similarities with our CoLiDE formulation. Indeed, after dropping the log (|det(I\u2212W)|) term in (42) that vanishes when W corresponds to a DAG, the respective score functions are given by:\nCoLiDE-NV: S(W,\u03a3) = 1 2n\nTr ( (X\u2212W\u22a4X)\u22a4\u03a3\u22121(X\u2212W\u22a4X) ) + 1\n2 d\u2211 i=1 \u03c3i + \u03bb\u2225W\u22251\nGOLEM-NV: S(W,\u03a3) = 1 2n\nTr ( (X\u2212W\u22a4X)\u22a4\u03a3\u22122(X\u2212W\u22a4X) ) + 1\n2 d\u2211 i=1 log \u03c32i + \u03bb\u2225W\u22251.\nWhile the similarities can be striking, notice that in CoLiDE the squared linear SEM residuals are scaled by the noise standard deviations \u03c3i. On the other hand, the negative Gaussian log-likelihood uses the variances \u03c32i for scaling as in standard weighted LS squares. This observation notwithstanding, the key distinction lies in the fact that the negative Gaussian log-likelihood lacks convexity in the \u03c3i, making it non-jointly convex. In contrast, CoLiDE is jointly convex in W and \u03a3 due to Huber\u2019s concomitant estimation techniques (see Appendix A.1), effectively convexifying the Gaussian log-likelihood; refer to Owen (2007, Section 5) for a more detailed discussion in the context of linear regression.\nGoing back to the experiment, we generate 10 different ER4 DAGs along with their permutation matrices. These graphs have number of nodes ranging from 10 to 50. Data is generated using a linear SEM under the heteroscedasticity assumption, akin to the experiments conducted in the main body of the paper. The normalized SHD and SHD-C of the reconstructed DAGs are presented in Figure 16. Empirical results suggest that the profile log-likelihood score function outperforms CoLiDE in smaller graphs with 10 or 20 nodes. However, as the number of nodes increases, CoLiDE (especially the CoLiDE-EV score function), outperforms the likelihood-based score functions."
        }
    ],
    "title": "COLIDE: CONCOMITANT LINEAR DAG ESTIMATION",
    "year": 2024
}