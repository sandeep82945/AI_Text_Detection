{
    "abstractText": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
    "authors": [],
    "id": "SP:e5e71a36f4b6e8ff13db5c2f0c88c8351585765b",
    "references": [
        {
            "authors": [
                "David Bau",
                "Steven Liu",
                "Tongzhou Wang",
                "Jun-Yan Zhu",
                "Antonio Torralba"
            ],
            "title": "Rewriting a deep generative model",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Belinkov"
            ],
            "title": "Probing classifiers: Promises, shortcomings, and advances",
            "venue": "Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xi Chen",
                "Yan Duan",
                "Rein Houthooft",
                "John Schulman",
                "Ilya Sutskever",
                "Pieter Abbeel"
            ],
            "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Zhi Chen",
                "Yijie Bei",
                "Cynthia Rudin"
            ],
            "title": "Concept whitening for interpretable image recognition",
            "venue": "Nature Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Nicola De Cao",
                "Thomas Kipf"
            ],
            "title": "Molgan: An implicit generative model for small molecular graphs",
            "venue": "arXiv preprint arXiv:1805.11973,",
            "year": 1973
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mateo Espinosa Zarlenga",
                "Pietro Barbiero",
                "Gabriele Ciravegna",
                "Giuseppe Marra",
                "Francesco Giannini",
                "Michelangelo Diligenti",
                "Zohreh Shams",
                "Frederic Precioso",
                "Stefano Melacci",
                "Adrian Weller"
            ],
            "title": "Concept embedding models: Beyond the accuracy-explainability trade-off",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow"
            ],
            "title": "Nips 2016 tutorial: Generative adversarial networks",
            "venue": "arXiv preprint arXiv:1701.00160,",
            "year": 2016
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Erik H\u00e4rk\u00f6nen",
                "Aaron Hertzmann",
                "Jaakko Lehtinen",
                "Sylvain Paris"
            ],
            "title": "Ganspace: Discovering interpretable gan controls",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Peter Hase",
                "Mohit Bansal",
                "Been Kim",
                "Asma Ghandeharioun"
            ],
            "title": "Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models",
            "venue": "arXiv preprint arXiv:2301.04213,",
            "year": 2023
        },
        {
            "authors": [
                "Marton Havasi",
                "Sonali Parbhoo",
                "Finale Doshi-Velez"
            ],
            "title": "Addressing leakage in concept bottleneck models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Irina Higgins",
                "Loic Matthey",
                "Arka Pal",
                "Christopher Burgess",
                "Xavier Glorot",
                "Matthew Botvinick",
                "Shakir Mohamed",
                "Alexander Lerchner"
            ],
            "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
            "venue": "In International conference on learning representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems, 33:6840\u20136851,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.02303,",
            "year": 2022
        },
        {
            "authors": [
                "John Ingraham",
                "Max Baranov",
                "Zak Costello",
                "Vincent Frappier",
                "Ahmed Ismail",
                "Shan Tie",
                "Wujie Wang",
                "Vincent Xue",
                "Fritz Obermeyer",
                "Andrew Beam"
            ],
            "title": "Illuminating protein space with a programmable generative model",
            "year": 2022
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "arXiv preprint arXiv:1611.01144,",
            "year": 2016
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Samuli Laine",
                "Erik H\u00e4rk\u00f6nen",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Alias-free generative adversarial networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Been Kim",
                "Martin Wattenberg",
                "Justin Gilmer",
                "Carrie Cai",
                "James Wexler",
                "Fernanda Viegas"
            ],
            "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "An introduction to variational autoencoders",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Thao Nguyen",
                "Yew Siang Tang",
                "Stephen Mussmann",
                "Emma Pierson",
                "Been Kim",
                "Percy Liang"
            ],
            "title": "Concept bottleneck models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Anders Boesen Lindbo Larsen",
                "S\u00f8ren Kaae S\u00f8nderby",
                "Hugo Larochelle",
                "Ole Winther"
            ],
            "title": "Autoencoding beyond pixels using a learned similarity metric",
            "venue": "In International conference on machine learning. PMLR,",
            "year": 2016
        },
        {
            "authors": [
                "Phillip Lippe"
            ],
            "title": "Normalizing Flows for image modeling",
            "venue": "https://uvadlc-notebooks.readthedocs. io/en/latest/tutorial_notebooks/tutorial11/NF_image_modeling.html,",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Francesco Locatello",
                "Stefan Bauer",
                "Mario Lucic",
                "Gunnar Raetsch",
                "Sylvain Gelly",
                "Bernhard Sch\u00f6lkopf",
                "Olivier Bachem"
            ],
            "title": "Challenging common assumptions in the unsupervised learning of disentangled representations",
            "venue": "In international conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Max Losch",
                "Mario Fritz",
                "Bernt Schiele"
            ],
            "title": "Interpretability beyond classification output: Semantic bottleneck networks",
            "venue": "arXiv preprint arXiv:1907.10882,",
            "year": 1907
        },
        {
            "authors": [
                "Charles Lovering",
                "Ellie Pavlick"
            ],
            "title": "Unit testing for concepts in neural networks",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Anita Mahinpei",
                "Justin Clark",
                "Isaac Lage",
                "Finale Doshi-Velez",
                "Weiwei Pan"
            ],
            "title": "Promises and pitfalls of black-box concept learning models",
            "venue": "arXiv preprint arXiv:2106.13314,",
            "year": 2021
        },
        {
            "authors": [
                "Emanuele Marconato",
                "Andrea Passerini",
                "Stefano Teso"
            ],
            "title": "Glancenets: Interpretable, leak-proof concept-based models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Emanuele Marconato",
                "Andrea Passerini",
                "Stefano Teso"
            ],
            "title": "Interpretability is in the mind of the beholder: A causal framework for human-interpretable representation learning",
            "venue": "arXiv preprint arXiv:2309.07742,",
            "year": 2023
        },
        {
            "authors": [
                "Andrei Margeloiu",
                "Matthew Ashman",
                "Umang Bhatt",
                "Yanzhi Chen",
                "Mateja Jamnik",
                "Adrian Weller"
            ],
            "title": "Do concept bottleneck models learn as intended",
            "venue": "arXiv preprint arXiv:2105.04289,",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in gpt",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Cristian Meo",
                "Anirudh Goyal",
                "Justin Dauwels"
            ],
            "title": "Tc-vae: Uncovering out-of-distribution data generative factors",
            "venue": "arXiv preprint arXiv:2304.04103,",
            "year": 2023
        },
        {
            "authors": [
                "Mehdi Mirza",
                "Simon Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "arXiv preprint arXiv:1411.1784,",
            "year": 2014
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D Manning"
            ],
            "title": "Fast model editing at scale",
            "venue": "arXiv preprint arXiv:2110.11309,",
            "year": 2021
        },
        {
            "authors": [
                "Augustus Odena",
                "Christopher Olah",
                "Jonathon Shlens"
            ],
            "title": "Conditional image synthesis with auxiliary classifier gans",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "John Platt",
                "Alan Barr"
            ],
            "title": "Constrained differential optimization",
            "venue": "In Neural Information Processing Systems,",
            "year": 1987
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Kanchana Ranasinghe",
                "Muzammal Naseer",
                "Munawar Hayat",
                "Salman Khan",
                "Fahad Shahbaz Khan"
            ],
            "title": "Orthogonal projection loss",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Vikas Raunak",
                "Arul Menezes"
            ],
            "title": "Rank-one editing of encoder-decoder models",
            "venue": "arXiv preprint arXiv:2211.13317,",
            "year": 2022
        },
        {
            "authors": [
                "Niels Rogge",
                "Kashif Rasul. The annotated diffusion model. June"
            ],
            "title": "URL https://huggingface",
            "venue": "co/blog/annotated-diffusion. 3, 16",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Mahalaxmi Elango",
                "David Bau",
                "Antonio Torralba",
                "Aleksander Madry"
            ],
            "title": "Editing a classifier by rewriting its prediction rules",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Maximilian Seitzer"
            ],
            "title": "pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/ pytorch-fid, August 2020",
            "venue": "Version 0.3.0",
            "year": 2020
        },
        {
            "authors": [
                "Alon Shoshan",
                "Nadav Bhonker",
                "Igor Kviatkovsky",
                "Gerard Medioni"
            ],
            "title": "Gan-control: Explicitly controllable gans",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Shengbang Tong",
                "Erik Jones",
                "Jacob Steinhardt"
            ],
            "title": "Mass-producing failures of multimodal systems with language models",
            "venue": "arXiv preprint arXiv:2306.12105,",
            "year": 2023
        },
        {
            "authors": [
                "Luan Tran",
                "Xi Yin",
                "Xiaoming Liu"
            ],
            "title": "Disentangled representation learning gan for pose-invariant face recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Ruben Villegas",
                "Mohammad Babaeizadeh",
                "Pieter-Jan Kindermans",
                "Hernan Moraldo",
                "Han Zhang",
                "Mohammad Taghi Saffar",
                "Santiago Castro",
                "Julius Kunze",
                "Dumitru Erhan"
            ],
            "title": "Phenaki: Variable length video generation from open domain textual description",
            "venue": "arXiv preprint arXiv:2210.02399,",
            "year": 2022
        },
        {
            "authors": [
                "Catherine Wah",
                "Steve Branson",
                "Peter Welinder",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "The caltech-ucsd birds-2002011 dataset",
            "year": 2011
        },
        {
            "authors": [
                "Sheng-Yu Wang",
                "David Bau",
                "Jun-Yan Zhu"
            ],
            "title": "Sketch your own gan",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Sheng-Yu Wang",
                "David Bau",
                "Jun-Yan Zhu"
            ],
            "title": "Rewriting geometric rules of a gan",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Lilian Weng"
            ],
            "title": "What are diffusion models? lilianweng.github.io, Jul 2021",
            "venue": "URL https://lilianweng. github.io/posts/2021-07-11-diffusion-models/",
            "year": 2021
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Maggie Wang",
                "James Zou"
            ],
            "title": "Post-hoc concept bottleneck models",
            "venue": "arXiv preprint arXiv:2205.15480,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Zhang",
                "Xingbo Du",
                "Junchi Yan",
                "Shihua Zhang"
            ],
            "title": "Decoupling concept bottleneck model. https://openreview.net/forum?id=vVbUB9oWUup\u00e5, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "2020) architectures and flow-based models. B.1.1 CONCEPT BOTTLENECK STYLEGAN Figure 7 shows how a CB layer can be inserted into StyleGAN Karras et al. (2019); we found that the best location for the CB layer is after the mapping network so that all layers in the synthesis",
            "year": 2019
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "Celeb-A 64x64 and CUB 128x128. FID is computed between 40k generated images for each dataset is reported in Table 4. Samples of images generated from CB-StyleGAN2 are given in Figure 8. We find that adding a CB layer does not degrade the quality of the generated images",
            "year": 2020
        },
        {
            "authors": [
                "Ho"
            ],
            "title": "The implementation follows a conditional U-NET architecture. Dataset: We started with the entire collection of the LAION Aesthetics dataset. We then follow the Baio (2022) filtering process to characters, celebrites, and artistic style with at least 2500 samples, which resulted in 155 concepts",
            "venue": "DDPM",
            "year": 2020
        },
        {
            "authors": [
                "Tong"
            ],
            "title": "2023), to demonstrate how concept bottleneck layers make it easier to debug generative models. This is in comparison to standard text-toimage models that provide no way to determine the features that a model is basing its output on",
            "venue": "Overview and Experimental Design. Tong et al",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Improvements in generative modeling have led to these models being applied to produce photo realistic images (Saharia et al., 2022), video (Ho et al., 2022; Villegas et al., 2022), protein sequences (Ingraham et al., 2022), small molecules (De Cao and Kipf, 2018), and coherent text (Brown et al., 2020). However, current generative models admit little-to-no-room for interpretation, which limits the ability to fix these models when they make mistakes. Consider a model trained to generate protein sequences; a domain expert might be interested in determining whether the model has captured desirable features like thermostability and toxicity. An important goal is to use these features, as knobs, to steer the model to generate sequences that satisfy desired ranges of thermostability and toxicity. In this work, we develop generative models with intrinsically interpretable components that can be used to simulataneously: interpret, debug, and steer the output of the model.\nChallenges with interpreting and steering generative model representations. Current approaches for interpreting generative models cannot reliably indicate that a model\u2019s representations map to human-understandable features that the model relies on for its output. One approach (Belinkov, 2022) for interpreting a generative model\u2019s representations uses a low-complexity model to predict a human understandable feature from the model\u2019s representations. However, high predictive performance of the low-complexity model does not indicate that the model\u2019s output relies on that feature (Lovering and Pavlick, 2022). Another approach constrains the model to learn disentangled (Higgins et al., 2017; Tran et al., 2017; Meo et al., 2023) representations\u2014that is, representations that can be decomposed into independent factors. Nevertheless, it is not possible to guarantee that a disentangled representation is human interpretable (Locatello et al., 2019). Other approaches project model representations into lower dimensions, and then search for directions correlated with human interpretable features (H\u00e4rk\u00f6nen et al., 2020); yet, since the model was not constrained to learn such features, it is possible that its representations do not encode these desired features.\nConcept bottleneck generative models (CBGMs). To address challenges with interpreting and steering current generative models, we present concept bottleneck generative models, a generative model where one of its internal layers\u2014a concept bottleneck (CB) layer\u2014is constrained to encode human-understandable features. We insert the CB layer into the generative model\u2019s architecture to give 3 parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. The pre-concept bottleneck portion maps from the input to activations, which are then mapped into human understandable features by the CB layer. The pre-defined concepts alone can be incomplete,\nso we allow additional representational capacity for unknown concepts that are constrained to be orthogonal to the pre-defined features. Lastly, the post-concept bottleneck layer maps both the output of the CB layer and unknown concepts to the generated output. To train CBGMs, we complement the traditional loss functions for training generative models with two terms: a concept loss, and an orthogonality loss. The CB layer is model agnostic, which we demonstrate by applying them to 3 different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. Using the CB layer, we are able to demonstrate the following capabilities:\n\u2022 Steering Generative Models: By intervening on the output of the CB layer, we can modulate the level of a particular concept present in the output of a generative model. We use this capability to control single concepts independently, and multiple concepts simultaneously. Figure 1, shows an example of intervening on denoising diffusion probabilistic models with a CB layer. We can change the concept of the image while preserving the image quality. Across several datasets and 3 types of generative models, steering a generative model with the CB layer outperforms several input conditioning baselines. We find that when the number of concepts increases the performance of conditioning models dramatically degrades while the performance of CBGMs remains almost unchanged; in such cases, we can achieve up to 10x improvement in steerability compared to traditional approaches.\n\u2022 Understanding and Debugging Generative Models: The CB layer can also be used to debug a generative model during and post training. In Section 4.3, we show that the CB layer can help identify the important concepts that are responsible for a model\u2019s generated output. Similarly, the output of the concept bottleneck (CB) layer can be used to distinguish a model that has learned the pre-defined human-understandable features from a model that has not."
        },
        {
            "heading": "2 SETUP & BACKGROUND",
            "text": "We now give an overview of concept bottleneck models, and the types of generative models that we consider. We assume that all training samples come with pre-defined human understandable features as such: {(xi, ci)}ni=1, where an example xi \u2208 Rd and an associated concept ci \u2208 Rk with k << d. Generative models consider the task of modeling the probability distribution of an observed random variable x \u223c P(x). We consider three generative model families: variational autoencoders (VAEs) (Kingma and Welling, 2013), generative adversarial networks (GANs) (Goodfellow, 2016), and diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2020; Ho et al., 2020).\nConcept Bottleneck Models (CBMs) & Concept Embedding Models (CEMs). CBMs (Koh et al., 2020) inserts an \u2018interpretable\u2019 layer into a deepnet. Specifically, CBMs map samples xi to labels yi by first mapping xi to an intermediate representation ci = h(xi), where ci are understandable human concepts (e.g., hair color). An \u2018interpretable\u2019 label predictor, f , then maps the predicted concepts to labels: y = f (h (x)). Consequently, the goal is to learn the following relationship\nxi h\u2212\u2192 ci f\u2212\u2192 yi. The functions h and f can be learned jointly, sequentially or independently. To improve the expressiveness of CBMs, (Espinosa Zarlenga et al., 2022) proposed CEMs, which map the input to a high-dimensional representation for each concept.\nVariational Autoencoder. VAEs (Kingma and Welling, 2013; Kingma et al., 2019) consists of two components: an encoder and a decoder. The encoder, q(zi|xi), maps an input, xi, to a distribution over the latent variable, zi. The encoder parameterizes a Gaussian density, from which we sample a latent vector zdi . VAEs regularizes this latent distribution to be similar to the prior distribution p(z) which is typically z \u223c N (0, I). The vector zdi is passed to the decoder, p(xi|zdi ), a function that maps the latent vector to a distribution over the input. The task loss for the VAE is the negative log-likelihood:\nL(xi)task,vae = DKL ( q (zi|xi) ||p (zi) ) \u2212 E [ log p(xi|zdi ) ] , (1)\nwhere DKL is the Kullback-Leibler divergence.\nGenerative Adversarial Networks. GANs (Goodfellow et al., 2014) consist of: a generator G that captures the data distribution and a discriminator D that estimates the probability that a sample came from either the training data or G. The generator maps a noise vector zi to an output x\u0302i = G(zi). Conditional GANs (Mirza and Osindero, 2014; Chen et al., 2016; Odena et al., 2017) augment the input to the generator with the concepts and also learn G(zi|ci). They however do not learn ci = h(zi). To learn ci = h(zi), we first encode xi into a latent vector q(zi|xi), since p(ci|xi) is known, i.e., concepts for a given sample are known, we now can learn ci = h(zi). Similar to VAEs, we then sample a new latent vector zdi and use this for generation G(z d i ). This approach has been widely employed (Larsen et al., 2016; Isola et al., 2017; Wang et al., 2018) for image generation. For training, we use the loss introduced by VAE-GANs (Larsen et al., 2016), which combines the VAE encoder regularization prior loss with a GAN loss.\nL(xi)task,gan = DKL ( q (zi|xi) ||p (zi) ) + E [ logD (xi) ] + E [ 1\u2212 logD ( G ( zdi ))] . (2)\nDiffusion models. Diffusion models can be interpreted as latent variable models with two stages (Sohl-Dickstein et al., 2015; Ho et al., 2020). Given input data xi, the first stage is the forward diffusion process, which involves incrementally adding Gaussian noise to the input and can be described as: q ( xti|x t\u22121 i ) = N ( xti; \u221a 1\u2212 \u03b2txti, \u03b2tI ) , where \u03b2t is determined according to\na pre-specified schedule. The second stage of the process learns a denoising model, p ( xt\u22121i |xti ) that reverses the forward process. The model is trained to maximize a lower bound to the marginal likelihood, which can be relaxed into a mean-squared error loss as:\nL(xti)task,df = T\u2211\ni=1\nE \u2225\u2225\u00b5(xti, t)\u2212 \u00b5\u0302 (xti, xt=0i ) \u2225\u22252. (3)\nWe refer to (Ho et al., 2020; Weng, 2021; Rogge and Rasul, 2022) for a more detailed overview of diffusion models."
        },
        {
            "heading": "3 CONCEPT BOTTLENECK GENERATIVE MODELS",
            "text": "We propose to insert a concept embedding layer\u2014which we term a concept bottleneck (CB) layer\u2014 into a generative model. Our overall framework, shown in Figure 2, consists of 3 parts: the portion of the generative model before the CB layer (the pre-concept bottleneck network); the CB layer, and the portion of the generative model after the CB layer (the post-concept bottleneck network). The pre-concept bottleneck and post-concept bottleneck networks are specific to the type of model (GANs, diffusion, & VAE) used for generation, while the CB layer is common across all generative model families. We now discuss the architecture of the CB layer, its loss functions, and an intervention procedure for steering the generative model."
        },
        {
            "heading": "3.1 ARCHITECTURE",
            "text": "We adapt the CEM layer of Espinosa Zarlenga et al. (2022) to the generative model setting. However, unlike in supervised learning tasks where the concept set is assumed to be near complete (Koh et al., 2020; Espinosa Zarlenga et al., 2022; Yuksekgonul et al., 2022), it is unrealistic to expect the pre-defined human understandable features\u2014concepts\u2014to be complete in the generative setting. For example, consider the task of generating human face; finding a comprehensive set of concepts that can control every generation aspect is challenging. However, one might have available concepts such as hair color, eye color, and skin tone. We extend the CEM layer of Espinosa Zarlenga et al. (2022) to encode for unknown concepts that might also be required for generation. The proposed modification of the concept bottleneck layer is shown in Figure 2, and consists of k concept networks and an extra unknown-concept network.\nConcept Embeddings. Each concept is represented with two embeddings: w+i , w \u2212 i \u2208 Rm representing the active and inactive concept states, respectively. The output of the pre-concept bottleneck portion, an embedding vector h, is fed into a context network, \u03d5, that maps h into two embeddings per concept. This context network can be viewed as two separate functions: w+i = \u03d5 +(h) and w\u2212i = \u03d5 \u2212(h). Embeddings w+i and w \u2212 i are encouraged to be aligned with ground-truth concept ci by the function \u03a8i trained to predict the probability of concept ci being active from the joint embedding space, c\u0302i = \u03a8i([w+i , w \u2212 i ]\nT ) \u2208 [0, 1] (i.e, c\u0302i is the predicted probability of concept ci). The final context embedding, wi, is a weighted mixture of the two embedding wi = ( c\u0302iw + i + (1\u2212 c\u0302i)w \u2212 i ) . All k concept embeddings are concatenated together, along with the non-concept embeddings, w = [w1, w2, . . . , wk+1], resulting in a bottleneck f(h) = w with size m(k + 1), which is fed into the post-concept network.\nAdding the CB layer. Figure 3 shows where to add the CB layer for each type of generative model. For the VAE Figure 3-a, the CB layer is the final layer of the encoder, meaning the output latent vector of the encoder is directly associated with pre-defined concepts. Similarly, in GANs Figure 3-b, the input to the generator contains the pre-defined concepts. Diffusion models typically follow a U-Net structure, and the CB layer is inserted after the middle block of the U-Net as shown in Figure 3-c.\nIntervention on Concept Probabilities. CBGMs support test-time interventions, which allows the user to steer the output of the generative model. To intervene on concept ci, one simply replaces the probability of the concept being active, c\u0302i, with the desired probability c\u0304i. The CB layers\u2019 context embedding, wi = ( c\u0304iw + i + (1\u2212 c\u0304i)w \u2212 i ) , is combined as a convex combination of the positive and negative context vectors. The new context embedding will then be passed to the post-concept bottleneck model to generate the new output with the desired concept."
        },
        {
            "heading": "3.2 LOSS FUNCTIONS & TRAINING",
            "text": "We train CBGMs in an end-to-end fashion by jointly minimizing the following loss function:\nLtotal = Ltask + \u03b1Lcon + \u03b2Lorth (4)\nwhere Ltask is the task loss, Lcon is the concept loss, and Lorth is the concept orthogonality loss. The hyperparameters \u03b1, and \u03b2 control the relative importance of the concept, and orthogonality losses. While the task loss is specific to each generative model family, Lcon, and Lorth are common across different models.\n\u2022 Task Loss: the task loss is the traditional objective function for each generative model family as defined in Equations 1, 2, and 3, respectively.\n\u2022 Concept Loss: across all generative model classes, the concept loss, Lcon, is the binary crossentropy loss on the output of each probability network of the CB layer.\n\u2022 Concept Orthogonality Loss: An undesirable situation occurs if the unknown concepts are transformations of the known concepts, which hampers the ability to control the model\u2019s output. To prevent this, we encourage the unknown concepts to be orthogonal to the outputs of each concept network using an orthogonality constraint (Ranasinghe et al., 2021) that minimizes the cosine similarity between the concept context embedding and the unknown context embedding as follows:\nLorth = \u2211 j\u2208B \u2211i=k i=1 \u2223\u2223\u27e8wi , wk+1\u27e9\u2223\u2223\u2211i=k i=1 1\n(5)\nwhere \u27e8\u00b7 , \u00b7\u27e9 is the cosine similarity applied to two embedding, | \u00b7 | is the absolute value, and B denotes mini-batch size and j denotes each sample in the mini-batch. The cosine similarity in the above equation involves the normalization of features such that \u27e8xi , xj\u27e9 = xi\u00b7xj\u2225xi\u22252\u00b7\u2225xj\u22252 , where \u2225 \u00b7 \u22252 is l2 norm.\nHyperparameters. The combined loss function that we propose consists of a weighted combination of losses, which can be difficult to optimize in practice. A standard approach for this setting is the modified differential method of multipliers (Platt and Barr, 1987) (See Appendix Section B.2). In practice, we find that a standard grid search over pre-specified ranges [0.01\u22120.1] was most effective."
        },
        {
            "heading": "4 EXPERIMENTS & RESULTS",
            "text": "In this section, we answer the following:\n1. Steerability: How can the CB layer control the output of the generative model? How do CBGMs compare to current prevailing approaches? 2. Interpretability: How can the CB layer be used to help interpret and debug a generative model?\n3. Generation quality and Ablations Does the insertion of the CB layer hurt generation quality? Are the proposed CBGMs components important?"
        },
        {
            "heading": "4.1 SETUP",
            "text": "Datasets. We consider the following datasets: (a) The 64 \u00d7 64 version of CelebFaces Attributes (Celeb-A), (Liu et al., 2015) that is annotated with 40 attributes (e.g., male, smiling etc.); (b) a curated subset of \u2018aesthetic\u2019 subset of the LAION (Schuhmann et al., 2022) dataset that is annotated with concepts corresponding to one of three categories: artistic style, presence of a famous person, and fictional characters giving a total of 155 concepts; (c) The Caltech-UCSD birds species (CUB) (Wah et al., 2011) that comes annotated with 312 concepts; and (d) the Color-MNIST dataset (Deng, 2012), where we take each label category and color as concepts, giving a total of 12 concepts.\nGenerative Model Baselines. We consider three types of generative models: VAE, GANs, and diffusion models. We compare our method with each family\u2019s most commonly used conditional generation approach. For GANs, we consider CGAN (Mirza and Osindero, 2014), ACGAN (Odena et al., 2017), and InfoGAN (Chen et al., 2016). For diffusion models, we compare with classifier (Dhariwal and Nichol, 2021) and classifier-free (CF) guidance (Ho and Salimans, 2022). For VAEs, we benchmark against conditional VAEs (CVAE) (Sohn et al., 2015)."
        },
        {
            "heading": "4.2 STEERING CB GENERATIVE MODELS",
            "text": "We assess the ability of the CB layer to effectively steer the output of the generative model. As discussed in Section 3, to \u2018turn on\u2019 a concept, we intervene on the concept probability, ci. We compare concept intervention via the CB layer to other prominent conditional generation strategies.\nConcept steerability quantitative experiments We train concept classifiers to detect the presence of a concept in an input on the real data; we ensure that the minimum accuracy of any concept classifier is at least 98% on a held-out test set from the real data. We sample noise latent vectors and then use them to generate images from the model; for each concept, we pass the generated image to the concept classifier; we save the latent vector if the classifier predicts the concept to be absent (i.e., the probability of the concept present is less than 0.5) we continue this process until we have 1000 samples. For each intervening approach (including the CB layer), we intervene to \u2018turn-on\u2019 the concept and generate new images. We then pass the newly generated images to the classifier again and measure the fraction of inputs for which it predicts the concept to be present; we refer to this as steerability accuracy. We examine the performance of models in two regimes: (a) Small balanced concepts regime: We extract a subset of concepts with balanced labels across the dataset; for Celeb-A we extract the 8 most balanced concepts, and for CUB, we extract the 10 most balanced concepts. We then train models on the balanced subset. (b) Large unbalanced concepts regime: We train on all concepts regardless of their distribution in the dataset, here we considered all 40 concepts for Celeb-A. We repeat each experiment 3 times and report the mean and variance in Table 1 (per concept metrics is available in the appendix).\nResults Small balanced concepts regime: we find that steering via the CB layer is much more effective across different datasets and generative models when compared to different standard\nconditioning approaches. In some cases up to 10x more effective, as shown on the CUB dataset for diffusion and VAEs. Large unbalanced concepts regime: we find that the performance of conditioning models dramatically decreases with the increase in the number of concepts for example InfoGan accuracy drops from 12.9% to 1.2% when the model is required to learn 40 concepts instead of 8 concepts however using a CB layer the performance degradation is minimal (at most 2.5%).\nScaling the Concept Bottleneck. To understand the limits of the concept bottleneck layer in the generative setting, we train a CB diffusion model on a curated subset of the LAION dataset where each image is annotated with 155 concepts. In Figure 1, we demonstrate the effect of steering the artistic style. The first column shows generated inputs without any concept intervention, while subsequent columns hold object attribute fixed, and intervene on a various artistic styles. These results suggest that concept bottleneck generative models can scale to contemporary settings."
        },
        {
            "heading": "4.3 INTERPRETABILITY",
            "text": "We now show how the concept-bottleneck layer can be used to: (a) interpret the output of a generative model, and (b) debug the model during and after training."
        },
        {
            "heading": "4.3.1 INTERPRETING THE OUTPUT OF A CB GENERATIVE MODEL",
            "text": "Contemporary generative models are largely inscrutable and provide no way to identify the key concepts upon which they are reliant. In supervised learning, the CB layer partitions the output into a concept basis to aid interpretability. We translate this insight to the generative model setting."
        },
        {
            "heading": "CB-DDPMCB-VAE",
            "text": "Inspecting Sample Concept Probabilities. The CB layer partitions the pre-concept bottleneck representations into known and unknown concepts, which both determine the generated output. Similar to the supervised learning setting, the concept probability for the known concepts corresponds directly to the importance of a concept\u2019s embedding for a generated sample. This means that the most important concepts are those with the highest concept probability scores.\nIn Figure 4, we show samples from a CB-VAE and a CB diffusion model trained on Celeb-A and Color-MNIST, respectively. We plot the three concepts with the highest concept probability scores along with each sample. We observe a correspondence between the properties of the generated samples and the top-ranked concept probability scores. This suggests that the vector of concept probability scores helps identify the key factors the generative model relies on most."
        },
        {
            "heading": "4.3.2 MODEL DEBUGGING",
            "text": "Given a trained generative model, a challenging model debugging task seeks to determine whether the model captures a concept of interest. For example, we might be interested in determining whether the representation of a model trained on Celeb-A captures hair color.\nDebugging by tracking the concept loss. To determine whether a CB generative model captures a desired concept, we track the concept loss (or accuracy) on a validation set during training. The trajectory of the estimate indicates the level to which the model can capture the concept of interest. Post-training, the concept probability score can also reveal the \u2018quality\u2019 of a model. A test-time debugging strategy inspects the per-concept probability histogram of randomly generated samples. The assumption is that a high-quality model should capture training-like concept distributions, while a low-quality model might ignore these concepts altogether.\nExperimental setup. We train two types of models: (a) Model-a is trained on the ground truth set of concepts, and (b) Model-b is trained on a modified dataset where the concept labels are randomized. We instantiate this experiment on CB-GAN and CB-DDPM trained on Color-MNIST and Celeb-A, respectively.\nTraining-time debugging. In Figure 5, we show the training trajectory of the concept validation accuracy. The models trained on random concept labels fail to achieve above-random accuracy. Inspecting the performance curves allows us to differentiate these two models.\nTest-time debugging. We train two CB-GAN models on Color-MNIST. Model-a is trained on the ground truth concepts for both \u2018green\u2019 and \u2018red\u2019. Model-b is trained on the corrupted \u2018red\u2019 concept and the ground truth \u2018green\u2019 concept. We plot the concept probability distribution as shown in Figure 6, The defective model is unable to capture the red distribution, but the model trained on normal, non-randomized, data does. Again, we find that such inspection helps differentiate these two classes of models.\n4.4 GENERATION QUALITY & ABLATIONS\nThe effect of adding a CB layer on generation quality: To test that a CB generative model does not affect generation quality, we compare the quality of the images generated by a CB generative model to an unconstrained model on Celeb-A (64\u00d764 version). We keep the input resolution, data pre-processing steps, and architecture fixed across all settings. From each method, we generate 40K random samples; we calculate Frechet inception distance (FID) (Heusel et al., 2017; Seitzer, 2020) between the synthetic images and the training dataset. Table 2 shows FID scores across generative model classes. We observe that the concept bottleneck versions of the generative models have FID scores that is comparable to unconstrained generative models.\nAblations on CBGMs losses and components: We performed ablation experiments for the proposed loss functions to determine the relative importance of each loss term; we also added an ablation\nexperiment to measure the effect of removing the unknown concept embedding. Ablations were done on CB-GANs. Results are shown in Table 3. Loss ablations we find that, as expected, the concept loss is most important, and without it steerability degrades by about 15%. We observe a decline in steerability for the orthogonality loss as well, by 5.7 %. Unknown concept embedding ablation We find that removing the unknown concept embedding negatively impacts steerability, decreasing steerability 9.1 %; in addition, we observed a large degradation in image quality when removing the unknown embedding FID score increased to 44.1 (compared to 18.4) this indicates how important it is to have unknown concepts embedding since it is unrealistic to expect the pre-defined concepts to be complete (i.e, encode every aspect of generation) in the generative setting."
        },
        {
            "heading": "5 DISCUSSION & CONCLUSION",
            "text": "Related Work. Concept-based interpretability approaches and concept bottleneck methods have been extensively investigated in supervised learning settings either post hoc (Kim et al., 2018; Yuksekgonul et al., 2022) or as an interpretable-by-design unit(Koh et al., 2020; Chen et al., 2020; Losch et al., 2019). Despite their benefits, CB layers are susceptible to feature leakage (Mahinpei et al., 2021; Margeloiu et al., 2021), which hampers steerability. Recent work has proposed alternatives to address leakage (Havasi et al., 2022; Zhang et al., 2022). Here, we incorporate these ideas into and adapt the concept embedding layer of Espinosa Zarlenga et al. (2022) into generative models.\nA closely related work is the GlanceNets of Marconato et al. (2022), which incorporates a CBM into a VAE architecture. GlanceNets enforce disentanglement among concepts, and avoid feature leakage by ascertaining the out-of-domain inputs. Our formulation does not require disentanglement among concepts, but only between the group of known and unknown concepts. Beyond a VAE, we adapt the CEM to three families of generative models, and scale them to more realistic settings. Overall, our proposal complements theirs, and the limitations of our proposal\u2014concept leakage\u2014can be addressed with their novel disentanglement and alignment formulation (Marconato et al., 2023).\nCurrent text-to-image models (e.g DALLE, etc) (Radford et al., 2021; Ramesh et al., 2022; Saharia et al., 2022) show remarkable conditional generation capabilities. However, our primary goal is not solely conditional generation; the CB layer enables capabilities that current text-to-image models do not enable: Model debugging, i.e., during training, we can identify which concepts the model can encode; Interpretability, i.e., why did my model generate this output? Instead of an outright replacement to current large-scale foundational models, this work is a stepping stone towards augmenting these models with intrinsically interpretable components that will make them easier to debug.\nSteering via the CB layer can be seen as a form of model editing\u2014a task that has taken on a renewed significance. Recent work has demonstrated intriguing control of classifiers (Santurkar et al., 2021), GANs (Bau et al., 2020; Wang et al., 2021; 2022), and large language models (Raunak and Menezes, 2022; Mitchell et al., 2021; Meng et al., 2022). The main paradigm for editing first searches the model\u2019s latent space to localize human-interpretable features, which are then manipulated for control. However, large-scale models can learn distributed representations, which makes effective localization challenging (Hase et al., 2023). Even if the localization strategy is effective, the latent representations of the generative model might not encode the interpretable feature. We circumvent these challenges by directly mapping to interpretable features, which obviates the need for a search.\nLimitations. Our proposed CB generative model does come with certain challenges: it requires that the entire training set be annotated with pre-defined concepts that can potentially be a laborious requirement in practice. Even though the CB layer can be applied broadly, we have only tested it for image tasks. Moving to text poses further challenges about what the nature of the concepts should be.\nConclusion. Due to unprecedented improvements, there has been an increased use of generative models across various settings. However, these models are mostly inscrutable and difficult to steer. In this paper, we present concept bottleneck generative models, a type of generative model where one of its internal layers\u2014a concept bottleneck (CB) layer\u2014is constrained to map from input representations to human-understandable features. The CB layer can be used as a simple plug-in module across different types of generative models. We show that inserting the CB layer does not hurt generation quality but helps to better steer and debug the models during and post-training. Overall, we see this work as a stepping stone for new kinds of generative models that are easier to understand and debug."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "A Additional details on Concept Bottleneck Layer 14",
            "text": ""
        },
        {
            "heading": "B Additional Experiments and Details 14",
            "text": "B.1 CBM in Different Generative Models . . . . . . . . . . . . . . . . . . . . . . . 14 B.2 Hyper-parameter Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 16 B.3 Steering CB Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.4 Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "C Debugging VLMs vs Concept Bottleneck Models. 20",
            "text": ""
        },
        {
            "heading": "A ADDITIONAL DETAILS ON CONCEPT BOTTLENECK LAYER",
            "text": "Continuous Variables. Our discussion in the main draft mostly addressed binary variables. Here is discuss a simple extension of the current framework that also extends to continuous variables as well. First, we represent each continuous variable with a single context vector instead of the positive and negative context vectors. Second, we transform each continuous variable so that it is normalized to [0, 1]. The output of the probability vector is then the normalized continuous variable. Instead of the binary cross-entropy loss function as before, we use the mean-squared error loss function. With these changes, we can now account for continuous variables.\nJoint Categorical Variables. This can be extended to categorical concepts by having j context vector where j is the number of categories, i.e., each concept is represented as w1i , . . . , w j i ; \u03a8i now predicts a probability for each class via a Softmax function [c\u03021i . . . c\u0302 j i ] = \u03a8i([w 1 i , . . . , w j i ]\nT ) the final context vector is constructed as the weighted mixture of all classes. In practice, we found that forcing sparse concept probabilities by adding temperature to the Softmax or using Gumbel-Softmax Jang et al. (2016) improves performance since this makes the probabilities more aligned with the ground truth concept distribution. For continuous variables, the context network \u03a8 would generate a single context vector, which would then be concatenated directly into the final context embedding w; for interventions, an additional encoder is needed to encode the continuous variable to its context vector as done by Shoshan et al. (2021)."
        },
        {
            "heading": "B ADDITIONAL EXPERIMENTS AND DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 CBM IN DIFFERENT GENERATIVE MODELS",
            "text": "CB layer is model agnostic; however, the location of a CB layer in a particular architecture is a design choice. Below, we show how CBM can be inserted into some special architectures like StyleGAN Karras et al. (2019; 2020) architectures and flow-based models."
        },
        {
            "heading": "B.1.1 CONCEPT BOTTLENECK STYLEGAN",
            "text": "Figure 7 shows how a CB layer can be inserted into StyleGAN Karras et al. (2019); we found that the best location for the CB layer is after the mapping network so that all layers in the synthesis network take the context embedding as an input this allows high-quality image generation while ensuring that the model learns different concepts. Note that a similar location can be used for StyleGAN2 Karras et al. (2020) and StyleGAN3 Karras et al. (2021); one would change the synthesis network accordingly.\nExperiments We trained CB-StyleGAN2 Karras et al. (2020) on Celeb-A 64x64 and CUB 128x128. FID is computed between 40k generated images for each dataset is reported in Table 4. Samples of images generated from CB-StyleGAN2 are given in Figure 8. We find that adding a CB layer does not degrade the quality of the generated images."
        },
        {
            "heading": "B.1.2 CONCEPT BOTTLENECK NORMALIZING FLOWS",
            "text": "Here we discuss an initial proposal on adding a concept bottleneck layer to a normalizing flow generative model. The discussion here will follow Tutorial 11 of the University of Amsterdam deep learning notebooks (Lippe, 2023). We adapt their tutorial to the color MNIST setup described in the main manuscript. We then insert a concept bottleneck (CB) layer into the model.\nIn normalization flows, an input is mapped using a \u2018flow\u2019 network into a latent vector, then the latent vector is mapped through an inverse flow network to obtain a reconstructed input. Different from a VAE, a normalization flow parametrizes its flow mappings to be invertible functions.\nHere we present a simple proposal for adding a CB layer into a normalization flow architecture. We insert the concept bottleneck layer to be part of the layer in the flow function that maps into the latent vector. Concretely, Flow-based generative models minimize the negative log-likelihood function of a data given a invertible flow function. The flow function maps from an input xi to a latent vector zi.\nChoice of CB Layer insertion. We insert the concept bottleneck after a series of coupling layers. The output of the concept bottleneck layer is passed to the rest of the flow function. Interesting, the coupling layer of the normalizing flow is parametrized so that the output of the layer is split into two parts\u2014one of which remains unchanged by the flow.\nPerformance. We measure the performance of the flow model with the bits per dimension (bpd) metric. For the flow model without a concept bottleneck layer, we obtain a test bpd of 4.87, and for a CB normalizing flow, we obtain a test bpd of 5.1. These values are within 4 percent of one another which suggests that the insertion of the concept bottleneck does not result in any degradation."
        },
        {
            "heading": "B.1.3 CONCEPT BOTTLENECK DDPM",
            "text": "We plan to release all of the code and data to replicate our anaylsis. However, in the meantime, we discuss the key details of the training DDPM on LAION here.\nArchitecture: In this work, we consider the Denoising Diffusion Probabilistic Models (DDPMs) as the diffusion model of choice. We follow the opensource implementation of Rogge and Rasul (2022), which adapts the original DDPM implementation of Ho et al. (2020). The implementation follows a conditional U-NET architecture.\nDataset: We started with the entire collection of the LAION Aesthetics dataset. We then follow the Baio (2022) filtering process to characters, celebrites, and artistic style with at least 2500 samples, which resulted in 155 concepts across the entire dataset. Next, we follow Brooks et al. (2023)\u2019s data preprocessing recipe but with the curated 155 concepts. Overall, we sample 100 samples for each additional concept for each of the 155 concepts to arrive at 15500 samples in the second data processing step.\nTraining Strategy: We first train a CB-Diffusion model on the curated dataset following Baio (2022)\u2019s strategy. In the second stage, we simply finetune the trained model on the collection of samples obtained via Brooks et al. (2023)\u2019s training process. We found this two-stage strategy to be most effective for generating high-quality samples. Overall, training required 240 V100s gpus hours."
        },
        {
            "heading": "B.2 HYPER-PARAMETER OPTIMIZATION",
            "text": "We tested two different hyper-parameter optimization method. We found a simple grid search over parameters to be most effective. Here we discuss the details of the Platt and Barr approach mentioned in the text.\nOn the VAE model we compared Platt and Barr to a standard grid search/sweep over a hyper-parameter set. We didn\u2019t observe a substantial improvement in model FID scores for the subset of models trained. We used an open-source Pytorch implementation1 of the Platt and Barr approach, which does not require any modification for the setting considered here. The approach minimizes a loss function that is subject to bound constraints with arbitrary parameters that weight the importance of these functions. We provide an overview implementation of how to use the package in the example code listing below.\n# Platt and Barr using open source implementation. import mdmm import torch;\nmdmm_module = mdmm.MDMM(constraints) opt = mdmm_module.make_optimizer(model.parameters(), lr=args.lr)\n1See: https://github.com/crowsonkb/mdmm\nloss = crit(outputs, targets) mdmm_return = mdmm_module(loss) mdmm_return.value.backward() opt.step()"
        },
        {
            "heading": "B.3 STEERING CB GENERATIVE MODELS",
            "text": ""
        },
        {
            "heading": "B.3.1 SINGLE-CONCEPT STEERABILITY",
            "text": "Figure 11 and Figure 12 show images generated by CB-GAN and CB-StyleGAN2 before and after single concept intervention. We find that by changing the concept probability vector we can control the generated output.\nSmall balanced concepts regime experiments: We report the per-concept steerability metric for the small balanced concepts regime experiments in Table 5. At a high level, for each class of generative model and across most features considered, we find that controlling the presence of a concept in the generated output is more effective with concept-bottleneck generative models than current approaches. In GANs, for prominent concepts like gender, steering the model output with the CB layer can be five times as effective as compared to the closest baseline (ACGAN). In the diffusion model setting, the concept-bottleneck diffusion model outperforms classifier-free diffusion control across all 5 attributes considered. Similar results were also found in VAEs.\nLarge unbalanced concepts regime experiments: Figure 13 shows the per concept steerability accuracy when models were trained on 40 Celeb-A concepts. Across all concepts CB-GAN outperforms other baselines."
        },
        {
            "heading": "B.3.2 MULTI-CONCEPT STEERABILITY",
            "text": "Figure 14 shows images generated by CB-GAN, we can add and remove multiple concepts from the generated images using the CB layer.\nWe report the steerability metric across five attributes in Table 6. For each setting of K, we intervene on the concept vector for one thousand inputs during generation and measure the induced change in attributes with attribute-independent classifiers. We find that CB-GAN has average concept accuracy around 30%, across all experiments, which indicates that CB-GAN steerability performance does not deteriorate with the addition of concepts. CB-GAN all outperforms the baselines for up till K = 3. In the diffusion model and VAEs setting, adding a CB layer outperforms the baseline across all values of K."
        },
        {
            "heading": "B.3.3 CONCEPT ANNOTATIONS ABLATIONS",
            "text": "One of the main drawbacks of CBGMs is that it requires concept annotations for the entire training set. However, this requirement can be easily relaxed. CB layer can be trained with a small subset of data with concepts annotations. For samples with concepts annotations, we train the model with loss in Equation 4; for samples without annotations, we remove the concept loss Lcon from Ltotal so for such samples new loss is given as below.\nLtotal = Ltask + \u03b2Lorth (6) We repeated the experiment in section 4.2 for CB-GAN model trained on Celeb-A 64 by 64, where we only use annotations for a subset of the training samples (note that here the models sees 100% of training samples but only a subset of samples is annotated), results are shown in Figure 15. We find that with about 20% of annotations we can directly match steerability metrics of a fully annotated training set. This result is not surprising; in the interpretability literature, concept bottleneck models for classification papers Kim et al. (2018) and Yuksekgonul et al. (2022) have shown that it is possible to learn a high-performing concept classifier with as few as 200 samples.\nB.4 INTERPRETABILITY\nB.4.1 INTERPRETING THE GENERATED OUTPUT\nFigure 16 shows concept probabilities for different samples generated from CB-VAE and CB-Diffuision on Celeb-A and color MNIST. By looking at each bar chart, one can understand which concepts were most effective in generating the output."
        },
        {
            "heading": "B.4.2 CORRESPONDENCE EXPERIMENT",
            "text": "Overview & Experimental Details: We conduct a correspondence experiment to measure the alignment between the generated output and the concept probabilities. This experiment measures the ability of the concept probability to communicate the feature importance of a concept-bottleneck generative model to a end-user seeking to better understand a model.\nWe generate 50 random samples and intervene on a subset of 2 concepts for these samples. For the Celeb-A dataset, we intervene on the male and smiling concepts. We chose these two concepts because they are the ones we could reliably judge by simply inspection. Some of the other concepts, like attractiveness and high cheekbones, are more subjective. For the Color-MNIST setting, we select 1 color concept (out of two) at random and a digit concept (out of ten) at random to intervene on. We then inspect these 50 samples and assess whether\nthe intervened samples are reflected in the generated output. This assessment communicates the extent to which the probability histogram can be used as a reliable interpretation.\nResults: On the CB-VAE trained on Celeb-A 64 by 64, we find an 87 percent correspondence, and 96 percent for a CB-DDPM trained on Color-MNIST. The consistent high correspondence scores suggest that the concept probability scores are an effective medium for communicating the generative model\u2019s sample dependent feature importance. By design, we constrain generative models to learn human understandable features precisely because we seek to enable feature importance communication between the model and a domain expert interested in debugging the model."
        },
        {
            "heading": "B.4.3 MODEL DEBUGGING",
            "text": "In the main paper, we showed that we can use a validation dataset to check if the CB-layer is actually learning concepts during training. This can also be done by examining the concept training loss define in main paper Section 3.2. Figure 17-a shows the training loss for different models in the debugging experiment described in the main paper Section 4.4.2. Figure 17-a shows that both models were able to learn the green concept, i.e., training loss decreases for both models. Figure 17-b shows that only Model-a\u2019s training loss decreased for the red concept, while the training loss for Model-b remained high."
        },
        {
            "heading": "C DEBUGGING VLMS VS CONCEPT BOTTLENECK MODELS.",
            "text": "In this section, we designed a synthetic debugging task, inspired by Tong et al. (2023), to demonstrate how concept bottleneck layers make it easier to debug generative models. This is in comparison to standard text-toimage models that provide no way to determine the features that a model is basing its output on.\nOverview and Experimental Design. Tong et al. (2023) found that most text-to-image models find it difficult to generate images of an empty cup, and would typically generate a glass half full instead. To replicate this finding, we probe stable diffusion XL for 1k images of \u2019empty cup\u2019. We manually inspect these images to narrow down to a set of examples where the model makes a mistake and generates a cup with water instead. With the remaining corpus, we fine-tune the model on this corpus, which results in the model reliably generating images of cups that contain water when prompted for an empty cup.\nIn the second phase of the experimental design, we modify the LAION dataset to inject two bugs: we switch the labels for two artistic style concepts: Thomas Kinkade and Van Gogh. Secondly, we added all 1k images of the non-empty cup that we obtained from the stable diffusion model to the LAION training set. For this task, we only consider 4 known concepts: 1) Empty, 2) Cup, 3) Thomas Kinkade, and 4) Van Gogh. We then train the same CB-DDPM model discussed in the main text on this task. The model makes two mistakes:\n\u2022 When we intervene on the Thomas Kinkade concept, it generates images with Van Gogh artistic styles, and vice versa;\n\u2022 When we intervene on the concept \u2018empty\u2019, it generates images of cups that are not empty since all the images tagged as empty in the training set are not empty.\nResult We show in Figure 18, the output from a fine-tuned Stable Diffusion XL for the prompt: \"Empty Cup\". Here we observe that the model indeed generates an output that is not empty. However, there is no way to understand why the model is making that mistake and to try to correct it.\nDifferent from the standard text-to-image models that donot enable interpretation, we can inspect the probability histogram for an input where we the model generates an empty cup, and we have intervened on the Thomas Kinkade concept. We observe that the model generates a non-empty cup, and a Van Gogh style image. This immediately indicates that there is likely an error with the Van Gogh and empty concepts in the dataset, and they are the sources of the errors. We can then inspect the training data to confirm these errors."
        }
    ],
    "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
    "year": 2023
}