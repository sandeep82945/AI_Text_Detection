{
    "abstractText": "Monte Carlo (MC) integration has been employed as the standard approximation method for the Sliced Wasserstein (SW) distance, whose analytical expression involves an intractable expectation. However, MC integration is not optimal in terms of absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on QuasiMonte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically evaluate various methods to construct QMC point sets on the 3D unit-hypersphere, including the Gaussian-based and equal area mappings, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimator for stochastic optimization, we extend QSW to Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness in the discussed point sets. Theoretically, we prove the asymptotic convergence of QSW and the unbiasedness of RQSW. Finally, we conduct experiments on various 3D tasks, such as point-cloud comparison, point-cloud interpolation, image style transfer, and training deep point-cloud autoencoders, to demonstrate the favorable performance of the proposed QSW and RQSW variants1.",
    "authors": [
        {
            "affiliations": [],
            "name": "SLICED WASSERSTEIN"
        },
        {
            "affiliations": [],
            "name": "Khai Nguyen"
        }
    ],
    "id": "SP:f32e52fe22afeef441b76856ec3a2b58fa768902",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Olga Diamanti",
                "Ioannis Mitliagkas",
                "Leonidas Guibas"
            ],
            "title": "Learning representations and generative models for 3d point clouds",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Christoph Aistleitner",
                "Johann S Brauchart",
                "Josef Dick"
            ],
            "title": "Point sets on the sphere with small spherical cap discrepancy",
            "venue": "Discrete & Computational Geometry,",
            "year": 2012
        },
        {
            "authors": [
                "Brandon Amos",
                "Samuel Cohen",
                "Giulia Luise",
                "Ievgen Redko"
            ],
            "title": "Meta optimal transport",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Kinjal Basu"
            ],
            "title": "Quasi-Monte Carlo Methods in Non-Cubical Spaces",
            "venue": "Stanford University,",
            "year": 2016
        },
        {
            "authors": [
                "Cl\u00e9ment Bonet",
                "Nicolas Courty",
                "Fran\u00e7ois Septier",
                "Lucas Drumetz"
            ],
            "title": "Efficient gradient flows in sliced-Wasserstein",
            "venue": "space. Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Bonneel",
                "Julien Rabin",
                "Gabriel Peyr\u00e9",
                "Hanspeter Pfister"
            ],
            "title": "Sliced and Radon Wasserstein barycenters of measures",
            "venue": "Journal of Mathematical Imaging and Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Johann Brauchart",
                "E Saff",
                "I Sloan",
                "R Womersley"
            ],
            "title": "Qmc designs: optimal order quasi monte carlo integration schemes on the sphere",
            "venue": "Mathematics of computation,",
            "year": 2014
        },
        {
            "authors": [
                "Johann S Brauchart",
                "Josef Dick"
            ],
            "title": "Quasi\u2013monte carlo rules for numerical integration over the unit sphere",
            "venue": "Numerische Mathematik,",
            "year": 2012
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Nicolas Courty",
                "R\u00e9mi Flamary",
                "Amaury Habrard",
                "Alain Rakotomamonjy"
            ],
            "title": "Joint distribution optimal transportation for domain adaptation",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Roy Cranley",
                "Thomas NL Patterson"
            ],
            "title": "Randomization of number theoretic methods for multiple integration",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 1976
        },
        {
            "authors": [
                "Henri Faure"
            ],
            "title": "Discr\u00e9pance de suites associ\u00e9es \u00e0 un syst\u00e8me de num\u00e9ration (en dimension s)",
            "venue": "Acta arithmetica,",
            "year": 1982
        },
        {
            "authors": [
                "Jean Feydy",
                "Benjamin Charlier",
                "Fran\u00e7ois-Xavier Vialard",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Optimal transport for diffeomorphic registration",
            "venue": "20th International Conference,",
            "year": 2017
        },
        {
            "authors": [
                "M G\u00f6tz"
            ],
            "title": "On the distribution of weighted extremal points on a surface in",
            "venue": "Potential Analysis,",
            "year": 2000
        },
        {
            "authors": [
                "J Halton",
                "G Smith"
            ],
            "title": "Radical inverse quasi-random point sequence, algorithm 247",
            "venue": "Commun. ACM,",
            "year": 1964
        },
        {
            "authors": [
                "John Hammersley"
            ],
            "title": "Monte carlo methods",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Doug P Hardin",
                "TJ Michaels",
                "Edward B Saff"
            ],
            "title": "A comparison of popular point configurations on S2",
            "venue": "arXiv preprint arXiv:1607.04590,",
            "year": 2016
        },
        {
            "authors": [
                "Holger Heitsch",
                "Ren\u00e9 Henrion"
            ],
            "title": "An enumerative formula for the spherical cap discrepancy",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 2021
        },
        {
            "authors": [
                "Edmund Hlawka"
            ],
            "title": "Funktionen von beschr\u00e4nkter variatiou in der theorie der gleichverteilung",
            "venue": "Annali di Matematica Pura ed Applicata,",
            "year": 1961
        },
        {
            "authors": [
                "Nhat Ho",
                "XuanLong Nguyen",
                "Mikhail Yurochkin",
                "Hung Hai Bui",
                "Viet Huynh",
                "Dinh Phung"
            ],
            "title": "Multilevel clustering via Wasserstein means",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Tianxin Huang",
                "Zhonggan Ding",
                "Jiangning Zhang",
                "Ying Tai",
                "Zhenyu Zhang",
                "Mingang Chen",
                "Chengjie Wang",
                "Yong Liu"
            ],
            "title": "Learning to measure the point cloud reconstruction loss in a representation space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Joe",
                "Frances Y Kuo"
            ],
            "title": "Remark on algorithm 659: Implementing Sobol\u2019s quasirandom sequence generator",
            "venue": "ACM Transactions on Mathematical Software (TOMS),",
            "year": 2003
        },
        {
            "authors": [
                "Alexander Keller"
            ],
            "title": "A quasi-monte carlo algorithm for the global illumination problem in the radiosity setting. In Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing: Proceedings of a conference at the University of Nevada",
            "year": 1994
        },
        {
            "authors": [
                "Hyeongju Kim",
                "Hyeonseung Lee",
                "Woo Hyun Kang",
                "Joun Yeop Lee",
                "Nam Soo Kim"
            ],
            "title": "Softflow: Probabilistic framework for normalizing flow on manifolds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "JF Koksma"
            ],
            "title": "Een algemeene stelling uit de theorie der gelijkmatige verdeeling modulo 1",
            "venue": "Mathematica B (Zutphen),",
            "year": 1942
        },
        {
            "authors": [
                "Soheil Kolouri",
                "Gustavo K Rohde",
                "Heiko Hoffmann"
            ],
            "title": "Sliced Wasserstein distance for learning Gaussian mixture models",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Rongjie Lai",
                "Hongkai Zhao"
            ],
            "title": "Multiscale nonrigid point cloud registration using rotation-invariant sliced-Wasserstein distance via laplace\u2013beltrami eigenmap",
            "venue": "SIAM Journal on Imaging Sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Thanh Tung Le",
                "Khai Nguyen",
                "shanlin sun",
                "Kun Han",
                "Nhat Ho",
                "Xiaohui Xie"
            ],
            "title": "Diffeomorphic mesh deformation via efficient optimal transport for cortical surface reconstruction",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Chen-Yu Lee",
                "Tanmay Batra",
                "Mohammad Haris Baig",
                "Daniel Ulbricht"
            ],
            "title": "Sliced Wasserstein discrepancy for unsupervised domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Tao Li",
                "Cheng Meng",
                "Jun Yu",
                "Hongteng Xu"
            ],
            "title": "Hilbert curve projection distance for distribution comparison",
            "venue": "arXiv preprint arXiv:2205.15059,",
            "year": 2022
        },
        {
            "authors": [
                "Han Lin",
                "Haoxian Chen",
                "Krzysztof M Choromanski",
                "Tianyi Zhang",
                "Clement Laroche"
            ],
            "title": "Demystifying orthogonal monte carlo and beyond",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Pertti Mattila"
            ],
            "title": "Geometry of sets and measures in Euclidean spaces: fractals and rectifiability",
            "venue": "Number 44. Cambridge university press,",
            "year": 1999
        },
        {
            "authors": [
                "Robb J Muirhead"
            ],
            "title": "Aspects of multivariate statistical theory",
            "year": 2009
        },
        {
            "authors": [
                "Kimia Nadjahi",
                "Alain Durmus",
                "L\u00e9na\u0131\u0308c Chizat",
                "Soheil Kolouri",
                "Shahin Shahrampour",
                "Umut Simsekli"
            ],
            "title": "Statistical and topological properties of sliced probability divergences",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Khai Nguyen",
                "Nhat Ho"
            ],
            "title": "Energy-based sliced Wasserstein distance",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Khai Nguyen",
                "Nhat Ho"
            ],
            "title": "Sliced Wasserstein estimation with control variates",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Khai Nguyen",
                "Nhat Ho",
                "Tung Pham",
                "Hung Bui"
            ],
            "title": "Distributional sliced-Wasserstein and applications to generative modeling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Khai Nguyen",
                "Dang Nguyen",
                "Nhat Ho"
            ],
            "title": "Self-attention amortized distributional projection optimization for sliced Wasserstein point-cloud reconstruction",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Khai Nguyen",
                "Shujian Zhang",
                "Tam Le",
                "Nhat Ho"
            ],
            "title": "Sliced Wasserstein with random-path projecting directions",
            "venue": "arXiv preprint arXiv:2401.15889,",
            "year": 2024
        },
        {
            "authors": [
                "Harald Niederreiter"
            ],
            "title": "Random number generation and quasi-Monte Carlo methods",
            "year": 1992
        },
        {
            "authors": [
                "Art B Owen"
            ],
            "title": "Randomly permuted (t, m, s)-nets and (t, s)-sequences. In Monte Carlo and QuasiMonte Carlo Methods in Scientific Computing: Proceedings of a conference at the University of Nevada",
            "year": 1994
        },
        {
            "authors": [
                "Art B Owen"
            ],
            "title": "Monte carlo theory, methods and examples",
            "year": 2013
        },
        {
            "authors": [
                "Lois Paulin",
                "Nicolas Bonneel",
                "David Coeurjolly",
                "Jean-Claude Iehl",
                "Antoine Webanck",
                "Mathieu Desbrun",
                "Victor Ostromoukhov"
            ],
            "title": "Sliced optimal transport sampling",
            "venue": "ACM Trans. Graph.,",
            "year": 2020
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Evguenii A Rakhmanov",
                "Edward B Saff",
                "YM1306011 Zhou"
            ],
            "title": "Minimal discrete energy on the sphere",
            "venue": "Mathematical Research Letters,",
            "year": 1994
        },
        {
            "authors": [
                "Tim Salimans",
                "Han Zhang",
                "Alec Radford",
                "Dimitris Metaxas"
            ],
            "title": "Improving GANs using optimal transport",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Zhengyang Shen",
                "Jean Feydy",
                "Peirong Liu",
                "Ariel H Curiale",
                "Ruben San Jose Estepar",
                "Raul San Jose Estepar",
                "Marc Niethammer"
            ],
            "title": "Accurate point cloud registration with robust optimal transport",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "I Sobol"
            ],
            "title": "The distribution of points in a cube and the accurate evaluation of integrals (in russian) zh",
            "venue": "Vychisl. Mat. i Mater. Phys,",
            "year": 1967
        },
        {
            "authors": [
                "Eloi Tanguy"
            ],
            "title": "Convergence of sgd for training neural networks with sliced Wasserstein losses",
            "venue": "arXiv preprint arXiv:2307.11714,",
            "year": 2023
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: Old and New",
            "year": 2008
        },
        {
            "authors": [
                "Zhirong Wu",
                "Shuran Song",
                "Aditya Khosla",
                "Fisher Yu",
                "Linguang Zhang",
                "Xiaoou Tang",
                "Jianxiong Xiao"
            ],
            "title": "3d shapenets: A deep representation for volumetric shapes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Mingxuan Yi",
                "Song Liu"
            ],
            "title": "Sliced Wasserstein variational inference",
            "venue": "In Fourth Symposium on Advances in Approximate Bayesian Inference,",
            "year": 2021
        },
        {
            "authors": [
                "Aistleitner"
            ],
            "title": "From the low-discrepancy sequence property of Sobol sequences (Sobol, 1967), we have that XL converges to X \u223c U(0, 1) in distribution as L \u2192 \u221e. Since our function f(x) is continuous on [0, 1], using the continuous mapping theorem, we have that \u03b8L = f(XL) converges to f(X) \u223c U(Sd\u22121) in distribution as L \u2192 \u221e",
            "year": 2012
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The Wasserstein (or Earth Mover\u2019s) distance (Peyre\u0301 & Cuturi, 2020) has been widely recognized as a geometrically meaningful metric for comparing probability measures. For instance, it has been successfully employed in various applications such as generative modeling (Salimans et al., 2018), domain adaptation (Courty et al., 2017), clustering (Ho et al., 2017), and so on. Specifically, the Wasserstein distance serves as the standard metric for applications involving 3D data, such as pointcloud reconstruction (Achlioptas et al., 2018), point-cloud registration (Shen et al., 2021), point-cloud completion (Huang et al., 2023), point-cloud generation (Kim et al., 2020), mesh deformation (Feydy et al., 2017), image style transfer (Amos et al., 2023), and various other tasks.\nDespite its appealing features, the Wasserstein distance exhibits high computational complexity. When using conventional linear programming solvers, evaluating the Wasserstein distance carries a O(n3 log n) time complexity (Peyre\u0301 & Cuturi, 2020), particularly when dealing with discrete probability measures supported on at most n atoms. Furthermore, computing the Wasserstein distance has at least O(n2) space complexity, which is related to storing the pairwise transportation cost matrix. The Sliced Wasserstein (SW) distance (Bonneel et al., 2015) stands as a rapid alternative metric to the plain Wasserstein distance. Since the SW distance is defined as a sliced probability metric based on the Wasserstein distance, it is equivalent to the latter while enjoying appealing properties (Nadjahi et al., 2020). More importantly, the time complexity and space complexity of the SW metric are only O(n log n) and O(n), respectively. As a result, the SW distance has been successfully adopted in various applications, including domain adaptation (Lee et al., 2019), generative models (Nguyen & Ho, 2024; Nguyen et al., 2024), clustering (Kolouri et al., 2018), gradient flows (Bonet et al., 2022), Bayesian inference (Yi & Liu, 2021), and more. In the context of 3D data analysis, the SW distance is employed in numerous applications such as point-cloud registration (Lai & Zhao, 2017),\n1Code for the paper is published at https://github.com/khainb/Quasi-SW.\nreconstruction, and generation (Nguyen et al., 2023), mesh deformation (Le et al., 2024), image style transfer (Li et al., 2022), along with various other tasks.\nFormally, the SW distance is defined as the expectation of the Wasserstein distance between two one-dimensional projected measures under the uniform distribution over projecting directions, i.e., the unit hypersphere. Exact computation of the SW distance is well-known to be intractable; hence, in practice, it is estimated empirically through Monte Carlo (MC) integration. Specifically, (pseudo)random samples are drawn from the uniform distribution over the unit hypersphere to approximate the analytical integral. However, the approximation error of MC integration is suboptimal because (pseudo-)uniform random samples may not exhibit sufficient \u201cuniformity\u201d over the space (Owen, 2013). Quasi-Monte Carlo (QMC) methods (Keller, 1995) address this issue by building deterministic point sets, known as \u201clow-discrepancy sequences\u201d, on which to evaluate the integrand. Low discrepancy implies that the points are more \u201cuniform\u201d and provide a superior approximation of the uniform expectation over the domain, compared to randomly drawn points.\nConventional QMC methods primarily focus on integration over the unit hypercube [0, 1]d (for d \u2265 1). To assess the uniformity of a point set on [0, 1]d, a widely employed metric is the \u201cstardiscrepancy\u201d (Koksma, 1942). A lower star-discrepancy value typically results in reduced approximation error, as per the Koksma\u2013Hlawka inequality (Koksma, 1942). When a point set exhibits a sufficiently small star-discrepancy, it is referred to as a \u201clow-discrepancy sequence\u201d. For the unit cube, several options exist, such as the Halton sequence (Halton & Smith, 1964), the Hammersley point set (Hammersley, 2013), the Faure sequence (Faure, 1982), the Niederreiter sequence (Niederreiter, 1992), and the widely used Sobol sequence (Sobol, 1967). QMC integration is renowned for its efficiency and effectiveness, especially in low (e.g., 3) dimensions.\nContribution. In short, we integrate QMC methodologies into the framework for SW distance computation. Specifically, our contributions are three-fold:\n1. As the SW distance involves integration over the unit hypersphere of dimension d\u2212 1, rather than the well-studied (for QMC purposes) hypercube, we provide an overview of practical methods for constructing point sets on the unit hypersphere, which can serve as candidates for low-discrepancy sequences (referred to as QMC point sets). Specifically, our exploration encompasses the following techniques: (i) mapping a low-discrepancy sequence from the 3D unit cube to the unit sphere using the normalized inverse Gaussian CDF, (ii) transforming a low-discrepancy sequence from the 2D unit grid to the unit sphere via the Lambert equal-area mapping, (iii) using generalized spiral points, (iv) maximizing pairwise absolute discrepancy, (v) minimizing the Coulomb energy. Notably, we believe that our work is the first to make use of the recent numerical formulation of spherical cap discrepancy (Heitsch & Henrion, 2021) to assess the uniformity of the aforementioned point sets.\n2. We introduce the family of Quasi-Sliced Wasserstein (QSW) deterministic approximations to the SW distance, based on QMC point sets. Furthermore, we establish the asymptotic convergence of QSW to the SW distance, as the size of the point set grows to infinity, for nearly all constructions of QMC point sets. For stochastic optimization, we present Randomized Quasi-Monte Carlo (RMQC) methods applied to the unit sphere, resulting in Randomized Quasi-Sliced Wasserstein (RQSW) estimations. In particular, we explore two approaches for generating random point sets on Sd\u22121: transforming randomized point sets from the unit cube and random rotation. We prove that nearly all variants of RQSW provide unbiased estimates of the SW distance.\n3. We empirically demonstrate that QSW and RQSW offer better approximations of the SW distance in 3D applications. Specifically, we first establish that QSW provides a superior approximation to the population SW distance compared to conventional Monte Carlo (MC) approximations when comparing 3D empirical measures over point clouds. Then, we conduct experiments involving pointcloud interpolation, image style transfer, and training deep point-cloud autoencoders to showcase the superior performance of various QSW and RQSW variants.\nOrganization. The remainder of the paper is organized as follows. We first provide some background on the SW distance, MC estimation, and QMC methods in Section 2. Then, we discuss how to construct QMC point sets on Sd\u22121, define QSW and RQSW approximations, and discuss some of their theoretical properties in Section 3. Section 4 contains experiments on point-cloud autoencoders, image style transfer, and deep point-cloud reconstruction. We conclude the paper in Section 5. Finally, we defer the proofs of key results, related work, and additional material to the Appendices.\nNotation. For any d \u2265 2, we define the unit hypersphere Sd\u22121 := {\u03b8 \u2208 Rd | ||\u03b8||22 = 1}, and denote the uniform distribution on it as U(Sd\u22121). For p \u2265 1, Pp(X ) represents the set of all probability measures on the set X that have finite p-moments. We denote \u03b8\u266f\u00b5 as the push-forward measure \u00b5 \u25e6 f\u22121\u03b8 of \u00b5 through the function f\u03b8 : Rd \u2192 R defined as f\u03b8(x) = \u03b8\u22a4x. For a vector X = (x1, . . . , xm) \u2208 Rm, PX represents the empirical measure 1m \u2211m i=1 \u03b4xi ."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "In Section 2.1, we define the SW distance and review the standard MC approach to estimate it. After that, in Section 2.2, we delve into QMC methods for approximating integrals over the unit hypercube."
        },
        {
            "heading": "2.1 SLICED WASSERSTEIN DISTANCE AND MONTE CARLO ESTIMATION",
            "text": "Definitions. Given p \u2265 1, the Sliced Wasserstein (SW) distance of order p (Bonneel et al., 2015) between two probability measures \u00b5, \u03bd \u2208 Pp(Rd) (i.e., with finite pth moment) is defined as\nSWpp(\u00b5, \u03bd) := E\u03b8\u223cU(Sd\u22121)[Wpp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd)], (1)\nwhere Wp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) is the one-dimensional Wasserstein between the projections of \u00b5 and \u03bd along direction \u03b8. As mentioned, one has the closed-form Wpp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) = \u222b 1 0 |F\u22121\u03b8\u266f\u00b5(z) \u2212 F \u22121 \u03b8\u266f\u03bd(z)|pdz, where F\u22121\u03b8\u266f\u00b5(\u00b7) and F \u22121 \u03b8\u266f\u03bd(\u00b7) are the inverse cumulative distribution functions of \u03b8\u266f\u00b5 and \u03b8\u266f\u03bd.\nMonte Carlo estimation. To approximate the intractable expectation in the SW distance formula, MC samples are generated and give rise to the following estimate:\nS\u0302W p p(\u00b5, \u03bd;L) = 1\nL L\u2211 l=1 Wpp(\u03b8l\u266f\u00b5, \u03b8l\u266f\u03bd), (2)\nwhere random samples \u03b81, . . . , \u03b8L (referred to as projecting directions) are drawn i.i.d. from U(Sd\u22121). When \u00b5 and \u03bd are discrete probability measures that have at most n supports, the time complexity of to compute S\u0302Wp is O(Ln log n+ Ldn), while the corresponding space complexity is O(Ld+ Ln). We refer to Algorithm 1 in Appendix B for more details on the computation of (2).\nMonte Carlo error. Similar to other usages of MC, the approximation error of the SW decreases at O(L\u22121/2) rate. In greater detail, a general upper-bound (Nadjahi et al., 2020) is:\nE \u03b81,...,\u03b8L iid\u223cU(Sd\u22121)\n[ |S\u0302W p p(\u00b5, \u03bd;L)\u2212 SWpp(\u00b5, \u03bd)| ] \u2264 1\u221a\nL Var\u03b8\u223cU(Sd\u22121)\n[ Wpp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) ]1/2 ."
        },
        {
            "heading": "2.2 QUASI-MONTE CARLO METHODS",
            "text": "Problem. Conventional Quasi-Monte Carlo (QMC) methods focus on approximating an integral I = \u222b [0,1]d\nf(x)dx = Ex\u223cU([0,1]d)[f(x)] on the unit hypercube [0, 1]d, with U([0, 1]d) denoting the corresponding uniform distribution. Similarly to MC methods, QMC integration also approximates the expectation with an equal weight average I\u0302(L) = 1L \u2211L l=1 f(xl). However, the point set \u03b81, . . . , \u03b8L is constructed differently.\nLow-discrepancy sequences. QMC requires a point set x1, . . . , xL such that I\u0302(L) \u2192 I as L \u2192 \u221e, and aims to obtain high uniformity. To measure the latter, the star discrepancy (Owen, 2013) has been used: D\u2217(x1, . . . , xL) = supx\u2208[0,1)d |FL(x|x1, . . . , xL) \u2212 FU([0,1]d)(x)|, where FL(x|x1, . . . , xL) = 1L \u2211L l=1 1xl\u2264x (the empirical CDF) and FU([0,1]d)(x) = Vol([0, x]) is the CDF of the uniform distribution over the unit hypercube. Since the star discrepancy is the sup-norm between the empirical CDF and the CDF of the uniform distribution, the points x1, . . . , xL are asymptotically uniformly distributed if D\u2217(x1, . . . , xL) \u2192 0. Moreover, there is a connection between the star discrepancy and the approximation error (Hlawka, 1961) via the Koksma-Hlawka inequality. In particular, we have:\n|I\u0302(L)\u2212 I| \u2264 D\u2217(x1, . . . , xL)VarHK(f), (3)\nwhere VarHK(f) is the total variation of f in the sense of Hardy and Krause (Niederreiter, 1992). Formally, x1, . . . , xL is called a low-discrepancy sequence if D\u2217(x1, . . . , xL) \u2208 O(L\u22121 log(L)d). Therefore, QMC integration can achieve better approximation than its MC counterpart if L \u2265 2d, since the error rate of MC is O(L\u22121/2). In relatively low dimensions, e.g., three dimensions, QMC gives a better approximation than MC. Several such sequences have been proposed, e.g., the Halton sequence (Halton & Smith, 1964), the Hammersley point set (Hammersley, 2013), the Faure sequence (Faure, 1982), the Niederreiter sequence Niederreiter (1992), and the Sobol sequence (Sobol, 1967). We refer the reader to Appendix B for the construction of the Sobol sequence."
        },
        {
            "heading": "3 QUASI-MONTE CARLO FOR 3D SLICED WASSERSTEIN",
            "text": "In Section 3.1, we explore the construction of candidate point sets as low-discrepancy sequences on the unit hypersphere. Subsequently, we introduce Quasi-Sliced Wasserstein (QSW), Randomized Quasi-Sliced Wasserstein (RQSW) distance, and discuss their properties in Section 3.2-3.3."
        },
        {
            "heading": "3.1 LOW-DISCREPANCY SEQUENCES ON THE UNIT-HYPERSPHERE",
            "text": "Spherical cap discrepancy. The most used discrepancy to measure the uniformity of a point set \u03b81, . . . , \u03b8L \u2208 Sd\u22121 is the spherical cap discrepancy (Brauchart & Dick, 2012):\nD\u2217Sd\u22121(\u03b81, . . . , \u03b8L) = sup w\u2208Sd\u22121,t\u2208[\u22121,1] \u2223\u2223\u2223\u2223\u2223 1L L\u2211 l=1 1\u03b8L\u2208C(w,t) \u2212 \u03c30(C(w, t)) \u2223\u2223\u2223\u2223\u2223 , (4) where C(w, t) = {x \u2208 Sd\u22121|\u27e8w, x\u27e9 \u2264 t} is a spherical cap, and \u03c30 is the law of U(Sd\u22121). It is proven that \u03b81, . . . , \u03b8L are asymptotically uniformly distributed if D\u2217Sd\u22121(\u03b81, . . . , \u03b8L) \u2192 0 (Brauchart & Dick, 2012). A point set \u03b81, . . . , \u03b8L is called a low-discrepancy sequence on S2 if D\u2217S2(\u03b81, . . . , \u03b8L) \u2208 O(L\u22123/4 \u221a log(L)). For some functions belonging to suitable Sobolev spaces, a lower spherical cap discrepancy leads to a better worse-case error (Brauchart & Dick, 2012; Brauchart et al., 2014).\nQMC point sets on Sd\u22121. We explore various methods to construct potentially low-discrepancy sequences on the unit hypersphere. Some of these constructions are applicable to any dimension, while others are specifically designed for the 2-dimensional sphere S2 \u2282 R3. Gaussian-based mapping. Utilizing the connection between Gaussian distribution and the uniform distribution over the unit hypersphere, i.e., x \u223c N (0, Id) then x/\u2225x\u22252 \u223c U(Sd\u22121), we can map a low-discrepancy sequence x1, . . . , xL on [0, 1]d to a potentially low-discrepancy sequence \u03b81, . . . , \u03b8L on Sd\u22121 through the mapping \u03b8 = f(x) = \u03a6\u22121(x)/\u2225\u03a6\u22121(x)\u22252, where \u03a6\u22121 is the inverse CDF of N (0, 1) (entry-wise). This technique is mentioned in (Basu, 2016) and can be used in any dimension. Equal area mapping. Following the same idea of transforming a low-discrepancy sequence on the unit grid, we can utilize an equal area mapping (projection) to map from [0, 1]2 to S2. For instance, we use the Lambert cylindrical mapping f(x, y) = (2 \u221a y \u2212 y2 cos(2\u03c0x), 2 \u221a y \u2212 y2 sin(2\u03c0x), 1\u2212 2y). This approach generates an asymptotically uniform sequence which is empirically shown to be low-discrepancy on S2 (Aistleitner et al., 2012).\nGeneralized Spiral. We can explicitly construct a set of L points that are equally distributed on S2 with spherical coordinates (\u03d51, \u03d52) (Rakhmanov et al., 1994): zi = 1 \u2212 2i\u22121L , \u03d5i1 = cos \u22121(zi), \u03d5i2 =\n1.8 \u221a L\u03d51i mod 2\u03c0 for i = 1, . . . , L. We can then retrieve Euclidean coordinates through the mapping (\u03d51, \u03d52) 7\u2192 (sin(\u03d51) cos(\u03d52), sin(\u03d51) sin(\u03d52), cos(\u03d51)). This construction outputs an asymptotically uniform sequence (Hardin et al., 2016) which is empirically shown to achieve optimal worst-case integration error Brauchart et al. (2014) for properly defined Sobolev integrands.\nMaximizing Distance and minimizing Coulomb energy. Previous work (Brauchart et al., 2014; Hardin et al., 2016) suggests that choosing a point set \u03b81, . . . , \u03b8L which maximizes the distance\u2211L i=1 \u2211L j=1 |\u03b8i\u2212\u03b8j | or minimizes the Coulomb energy \u2211L i=1 \u2211L j=1 1 |\u03b8i\u2212\u03b8j | could create a potentially low-discrepancy sequence. Such sequences are also shown to achieve optimal worst-case error by Brauchart et al. (2014), though they might suffer from sub-optimal optimization in practice. Also, minimizing the Coulomb energy is proven to create an asymptotically uniform sequence (Go\u0308tz, 2000). In this work, we use generalized spiral points as initialization points for optimization.\nEmpirical comparison. We adopt a recent numerical approximation for the spherical cap discrepancy (Heitsch & Henrion, 2021) to compare the discussed L-point sets. We visualize these sets and the corresponding discrepancies for L = 10, 50, 100 in Figure 6 in Appendix D.1. Overall, generalized spiral points and optimization-based points yield the lowest discrepancies, followed by equal area mapping construction. The Gaussian-based mapping construction performs worst among QMC methods; however, it still yields much lower spherical cap discrepancies than conventional random points. Qualitatively, we observe that the spherical cap discrepancy is consistent with the uniformity of point sets. We also include a comparison with the theoretical line CL\u22123/4 \u221a log(L) for some constant C, in Figure 7 in Appendix D.1. In this case, we observe that the equal area mapping sequences, generalized spiral sequences, and optimization-based sequences seem to attain low-discrepancy, as per definition. For convenience, we refer to these sequences as QMC point sets."
        },
        {
            "heading": "3.2 QUASI-SLICED WASSERSTEIN",
            "text": "Quasi-Monte Carlo methods for SW distances. Based on the aforementioned QMC point sets in Section 3.1, we can define the the QMC approximation of the SW distance as follows.\nDefinition 1. Given p \u2265 1, d \u2265 2, two probability measures \u00b5, \u03bd \u2208 Pp(Rd), and a QMC point set \u03b81, . . . , \u03b8L \u2208 Sd\u22121, Quasi-Sliced Wasserstein (QSW) approximation of order p between \u00b5 and \u03bd is:\nQ\u0302SW p p(\u00b5, \u03bd; \u03b81, . . . , \u03b8L) = 1\nL L\u2211 l=1 Wpp(\u03b8l\u266f\u00b5, \u03b8l\u266f\u03bd). (5)\nWe refer to Algorithm 2 in Appendix B for the computational algorithm of the QSW distance.\nQuasi-Sliced Wasserstein variants. We refer to (i) QSW with Gaussian-based mapping QMC point set as GQSW, (ii) QSW with equal area mapping QMC point set as EQSW, (iii) QSW with QMC generalized spiral points as SQSW, (iv) QSW with maximizing distance QMC point sets as DQSW, and (v) QSW with minimizing Coulomb energy sequence as CQSW.\nProposition 1. With point sets constructed through the Gaussian-based mapping, the equal area mapping, the generalized spiral points, and minimizing Coulomb energy, we have Q\u0302SW p\np(\u00b5, \u03bd; \u03b81, . . . , \u03b8L) \u2192 SWpp(\u00b5, \u03bd) as L \u2192 \u221e.\nThe proof of Proposition 1 is in Appendix A.1. We now discuss some properties of QSW variants.\nComputational complexities. QSW variants are deterministic, which means that the construction of QMC point sets, which can be reused multiple times, carries a one-time cost. Therefore, the computation of QSW variants has the same properties as for the SW distance, i.e., the time and space complexities are O(Ln log n+ Ldn) and O(Ld+ Ln), respectively. Since the QSW distance does not require resampling the set of projecting directions at each evaluation time, it is faster to compute than the SW distance if QMC point sets have been constructed in advance.\nGradient Approximation. When dealing with parametric probability measures, e.g., \u03bd\u03d5, we might be interested in computing the gradient \u2207\u03d5SWpp(\u00b5, \u03bd\u03d5) for optimization purposes. When using QMC integration, we obtain the corresponding deterministic approximation \u2207\u03d5Q\u0302SW p\np(\u00b5, \u03bd\u03d5; \u03b81, . . . , \u03b8L) = 1 L \u2211L l=1 \u2207\u03d5W p p(\u03b8l\u266f\u00b5, \u03b8l\u266f\u03bd\u03d5) for a QMC point set \u03b81, . . . , \u03b8L. For a more detailed definition of the gradient of the SW distance, please refer to Tanguy (2023). Since a deterministic gradient approximation may not lead to good convergence of optimization algorithms for relatively small L, we develop an unbiased estimation from QMC point sets in the next Section.\nRelated works. The SW distance is used as an optimization objective to construct a QMC point set on the unit cube and the unit ball in Paulin et al. (2020). However, a QMC point set on the unit-hypersphere is not discussed, and the SW distance is still approximated by conventional Monte Carlo integration. In contrast to the mentioned work, our focus is on using QMC point sets on the unit-hypersphere to approximate SW. The usage of heuristic scaled mapping with Halton sequence for SW distance approximation is briefly mentioned for the comparison between two Gaussians in (Lin et al., 2020). In this work, we consider a broader class of QMC point sets, assess their quality with the spherical cap discrepancy, discuss some randomized versions, and compare them in real applications. For further discussion on related work, please refer to Appendix C."
        },
        {
            "heading": "3.3 RANDOMIZED QUASI-SLICED WASSERSTEIN",
            "text": "While QSW approximations could improve approximation error, they are all deterministic. Furthermore, the gradient estimator based on QSW is deterministic, which may not be well-suited for convergence in optimization with the SW loss function. Moreover, QSW cannot yield any confidence interval about the SW value. Consequently, we propose Randomized Quasi-Sliced Wasserstein estimations by introducing randomness into QMC point sets.\nRandomized Quasi-Monte Carlo methods. The idea behind the Randomized Quasi-Monte Carlo (RQMC) approach is to inject randomness into a given QMC point set. For the unit cube, we can achieve a random QMC point set x1, . . . , xL by shifting (Cranley & Patterson, 1976) i.e., y1 = (xi +U) mod 1 for all i = 1, . . . , L and U \u223c U([0, 1]d). In practice, scrambling (Owen, 1995) is preferable since it gives a uniformly distributed random vector when applied to x \u2208 [0, 1]d. In greater detail, x is rewritten into x = \u2211\u221e k=1 b\n\u2212kak for base b digits and ak \u2208 {0, 1, . . . , b\u22121}. After that, we permute a1, . . . , ak randomly to obtain the scrambled version of x. Scrambling is applied to all points in a QMC point set to obtain a randomized QMC point set.\nRandomized QMC point sets on Sd\u22121. To the best of our knowledge, there is no prior work of randomized QMC point sets on the unit-hypersphere. Therefore, we discuss two practical ways to obtain random QMC point sets i.e., pushfoward QMC point sets and random rotation.\nPushfoward QMC point sets. Given a randomized QMC point set x\u20321, . . . , x \u2032 L on the unit-cube (unit-grid), we can use the Gaussian-based mapping (or the equal area mapping) to create a random QMC point set on the unit hypersphere \u03b8\u20321, . . . , \u03b8 \u2032 L. As long as the randomized sequence x \u2032 1, . . . , x \u2032 L is low-discrepancy on the mapping domain (e.g., as it happens when using scrambling), the spherical point set \u03b8\u20321, . . . , \u03b8 \u2032 L will have the same uniformity as the non-randomized construction.\nRandom rotation. Given a QMC point set \u03b81, . . . , \u03b8L on the unit-hypersphere Sd\u22121, we can apply uniform random rotation to achieve a random QMC point set. In particular, we first sample U \u223c U(Vd(Rd)) where Vd(Rd) = {U \u2208 Rd\u00d7d|U\u22a4U = Id} is the Stiefel manifold. After that, we form the new sequence \u03b8\u20321, . . . , \u03b8 \u2032 L with \u03b8 \u2032 i = U\u03b8i for all i = 1, . . . , L. Since rotation does not change the norm of vectors, the randomized QMC point set can be still a low-discrepancy sequence of the original QMC point set is low-discrepancy. Moreover, sampling uniformly from the Stiefel manifold is equivalent to applying the Gram-Smith orthogonalization process to z1, . . . , zl\niid\u223c N (0, Id) by the Bartlett decomposition theorem (Muirhead, 2009).\nDefinition 2. Given p \u2265 1, d \u2265 2, two measures \u00b5, \u03bd \u2208 Pp(Rd), and a randomized QMC point set \u03b8\u20321, . . . , \u03b8 \u2032 L \u2208 Sd\u22121, Randomized Quasi-Sliced Wasserstein estimation of order p between \u00b5 and \u03bd is:\nR\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) =\n1\nL L\u2211 l=1 Wpp(\u03b8 \u2032 l\u266f\u00b5, \u03b8 \u2032 l\u266f\u03bd). (6)\nWe refer to Algorithms 3 and 4 for more details on the computation of the RQSW approximation.\nRandomized Quasi-Sliced Wasserstein variants. For pushfoward QMC point sets, we refer to (i) RQSW with Gaussian-based mapping as RGQSW, (ii) RQSW with equal area mapping as REQSW. For random rotation QMC point sets, we refer to (iii) RQSW with Gaussian-based mapping as RRGQSW, (iv) RQSW with equal area mapping as RREQSW (v) RQSW with generalized spiral\npoints as RSQSW, (vi) RQSW with maximizing distance QMC point set as RDQSW, and (vii) RQSW with minimizing Coulomb energy sequence as RCQSW. Proposition 2. Gaussian-based mapping and random rotation randomized Quasi-Monte Carlo point sets are uniformly distributed, and the corresponding estimators RQSWpp(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) are unbiased estimations of SWpp(\u00b5, \u03bd) i.e., E[R\u0302QSW p p(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L)] = SW p p(\u00b5, \u03bd).\nThe proof of Proposition 2 is in Appendix A.2. We now discuss some properties of RQSW variants.\nComputational complexities. Compared to QSW, RQSW requires additional computation for randomization. For the push-forward approach, scrambling and shifting carry a O(Ld) time complexity. In addition, mapping the randomized sequence from the unit-cube (unit-grid) to the unit-hypersphere has time complexity O(Ld). For the random rotation approach, sampling a random rotation matrix costs O(d3). After that, multiplying the sampled rotation matrix with the precomputed QMC point set costs O(Ld2) in time complexity and O(Ld) in space complexity. Overall, in the 3D setting where d = 3 and n >> L > d, the additional computation for RQSW approximations is negligible compared to the O(n log n) cost from computing one-dimensional Wasserstein distances. Gradient estimation. In contrast to QSW, RQSW is random and is an unbiased estimation when combined with the proposed construction of randomized QMC point sets from Proposition 2. Therefore, it follows directly that E[\u2207\u03d5R\u0302QSW p p(\u00b5, \u03bd\u03d5; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L)] = \u2207\u03d5SW p p(\u00b5, \u03bd\u03d5) due to the Leibniz rule of differentiation. Therefore, this estimation can lead to better convergence for optimization."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We first demonstrate that QSW variants outperform the conventional Monte Carlo approximation (referred to as SW) in Section 4.1. We then showcase the advantages of RQSW variants in point-cloud interpolation and image style transfer, comparing them to both QSW variants and the conventional SW approximation in Section 4.2 and Section 4.3, respectively. Finally, we present the favorable performance of QSW and RQSW variants in training a deep point-cloud autoencoder."
        },
        {
            "heading": "4.1 APPROXIMATION ERROR",
            "text": "Setting. We select randomly four point-clouds (1, 2, 3, and 4 with 3 dimensions, 2048 points) from ShapeNet Core-55 dataset (Chang et al., 2015) as shown in Figure 1. After that, we use MC estimation with L = 100000 to approximate SW 22 between empirical distributions over pointclouds 1-2, 1-3, 2-3, and 3-4, then treat them as the population value. Next, we vary L in the set {10, 100, 500, 1000, 2000, 5000, 10000} and compute the corresponding absolute error of the estimation from MC (SW), and QMC (QSWs).\nResults. We illustrate the approximation errors in Figure 1. From the plot, it is evident that QSW approximations yield lower errors compared to the conventional SW approximation. Among the QSW approximations, CQSW and DQSW perform the best, followed by SQSW. In this simulation, the quality of GQSW and EQSW is not comparable to the previously mentioned approximations. Nevertheless, their errors are at least comparable to SW and are considerably better most of the time."
        },
        {
            "heading": "4.2 POINT-CLOUD INTERPOLATION",
            "text": "Setting. To interpolate between two point-clouds X and Y , we define the curve Z\u0307(t) = \u2212n\u2207Z(t) [ SW2 ( PZ(t), PY )] where PX and PY are empirical distributions over X and Y in turn. Here, the curve starts from Z(0) = X and ends at Y . In this experiment, we set X as point-cloud 1\nand Y as point-cloud 3 in Figure 1. After that, we use different gradient approximations from the conventional SW, QSW variants, and RQSW variants to perform the Euler scheme with 500 iterations, step size 0.01. To verify which approximation gives the shortest curve in length, we compute the Wasserstein-2 distance (POT library, Flamary et al. (2021)) between PZ(t) and PY .\nResults. We report Wasserstein-2 distances (from three different runs) between PZ(t) and PY at time step 100, 200, 300, 400, 500 in Table 1 with L = 100. From the table, we observe that QSW variants do not perform well in this application due to the deterministic approximation of the gradient with a fixed set of projecting directions. In particular, although EQSW and CQSW perform the best at time steps 100 and 200, QSW variants cannot make the curves terminate. As expected, RQSW variants can solve the issue by injecting randomness to create new random projecting directions. Compared to SW, RQSW variants are all better except RRGQSW. We visualize the interpolation for SW, CQSW, and RCQSW in Figure 2. The full visualization from all approximations is given in Figure 8 in Appendix D.2. From the figures, we observe that the qualitative comparison is consistent with the quantitative comparison in Table 1. In Appendix D.2, we also provide the result for L = 10 in Table 3, and the result for a different pair of point-clouds in Table 4-5 and Figure 9. We refer the reader to Appendix D.2 for a more detailed discussion."
        },
        {
            "heading": "4.3 IMAGE STYLE TRANSFER",
            "text": "Setting. Given a source image and a target image, we denote the associated color palettes as X and Y , which are matrices of size n\u00d7 3 (n is the number of pixels). Similar to point-cloud interpolation, we iterate along the curve between PX and PY . However, since the value of the color palette (RGB) is in the set {0, . . . , 255}, we need to perform an additional rounding step at the final Euler iterations. Moreover, we use more iterations i.e., 1000, and a bigger step size i.e., 1.\nResults. For L = 100, we report the Wasserstein-2 distances at the final time step and the corresponding transferred images from SW, CQSW, and RCQSW in Figure 3. The full results for all approximations are given in Figure 10 in Appendix D.3. In addition, we provide results for L = 10 in Figure 11 in Appendix D.3. Overall, QSW variants and RQSW perform better than SW in terms\nof both Wasserstein distance and visualization (brighter transferred images). Comparing QSW and RQSW, the latter yields considerably lower Wasserstein distances. In this task, RQSW variants display quite similar performance. We refer the reader to Appendix D.3 for more detail."
        },
        {
            "heading": "4.4 DEEP POINT-CLOUD AUTOENCODER",
            "text": "Setting. We follow the experimental setting in (Nguyen et al., 2023) to train deep point-cloud autoencoders with the SW distance on the ShapeNet Core-55 dataset Chang et al. (2015). We aim to optimize the following objective min\u03d5,\u03b3 EX\u223c\u00b5(X)[SWp(PX , Pg\u03b3(f\u03d5(X)))], where \u00b5(X) is our data distribution, f\u03d5 and g\u03c8 are a deep encoder and a deep decoder with Point-Net Qi et al. (2017) architecture. To optimize the objective, we use conventional MC estimation, QSW, and RQSW to approximate the gradient \u2207\u03d5 and \u2207\u03c8. We then utilize the standard SGD optimizer to train the autoencoder (with an embedding size of 256) for 400 epochs with a learning rate of 1e-3, a batch size of 128, a momentum of 0.9, and a weight decay of 5e-4. To evaluate the quality of trained autoencoders, we compute the average reconstruction losses, which are the W2 and SW2 distances (estimated with 10000 MC samples), on a different dataset i.e., ModelNet40 dataset (Wu et al., 2015).\nResults. We report the reconstruction losses with L = 100 in Table 2 (from three different training times). Interestingly, CQSW performs the best among all approximations i.e., SW, QSW variants, and RQSW variants at the last epoch. We have an explanation for this phenomenon. In contrast to point-cloud interpolation which considers only one pair of point-clouds, we estimate an autoencoder from an entire dataset of point-clouds. Therefore, model misspecification might happen here i.e., the family of Point-Net autoencoders may not contain the true data-generating distribution. Hence, L = 100 might be large enough to approximate well with QSW. When we reduce L to 10 in Table 6 in Appendix 4.4, CQSW and other QSW variants become considerably worse. In this application, we also observe that GQSW suffers from some numerical issues which leads to a very poor performance. As a solution, RQSW performs consistently well compared to SW especially random rotation variants. We present some reconstructed point-clouds from SW, CQSW, and RCQSW in Figure 4 and full visualization in Figure 12- 13. Overall, we recommend RCQSW for this task as a safe choice. We refer the reader to Appendix D.4 for more detail."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We presented Quasi-Sliced Wasserstein (QSW) approximation methods, which give rise to a better class of numerical estimates for the Sliced Wasserstein (SW) distance based on Quasi-Monte Carlo (QMC) methods. We discussed various ways to construct QMC point sets on the unit hypersphere, including the Gaussian-based mapping, the equal area mapping, generalized spiral points, maximizing distance points, and minimizing Coulomb energy points. Moreover, we proposed Randomized QuasiSliced Wasserstein (RQSW) approximations, which is a family of unbiased estimators of the SW distance based on injecting randomness into deterministic QMC point sets. We showed that QSW methods can reduce approximation error in comparing 3D point clouds. In addition, we showed that QSW variants and RQSW variants provide better gradient approximation for point-cloud interpolation, image-style transfer, and training point-cloud autoencoders. Overall, we recommend RQSW with random rotation of QMC point sets minimizing Coulomb energy, since it gives consistent and stable behavior across tested applications. In the future, we plan on extending QSW and RQSW approximations to higher dimensions d > 3, and apply QMC to other variants of the SW distance."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We would like to thank Peter Mu\u0308ller for his insightful discussion during the course of this project. NH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning. NB acknowledges the financial support by the Bank of Italy\u2019s \u201cG. Mortara\u201d scholarship."
        },
        {
            "heading": "A PROOFS",
            "text": "A.1 PROOF OF PROPOSITION 1\nWe first discuss the asymptotic uniformity of the mentioned QMC point set.\nFor the Gaussian-based mapping construction, From the construction, we have the function \u03b8 = f(x) = \u03a6\n\u22121(x) ||\u03a6\u22121(x)||2 . Given a Sobol sequence x1, . . . , xL, the corresponding spherical vectors are \u03b81, . . . , \u03b8L with \u03b8l = f(xl) for all l = 1, . . . , L. Let XL \u223c 1L \u2211L l=1 \u03b4xl . From the low-discrepancy sequence property of Sobol sequences (Sobol, 1967), we have that XL converges to X \u223c U(0, 1) in distribution as L \u2192 \u221e. Since our function f(x) is continuous on [0, 1]d, using the continuous mapping theorem, we have that \u03b8L = f(XL) converges to f(X) \u223c U(Sd\u22121) in distribution as L \u2192 \u221e. For the equal area mapping construction, we refer the reader to Aistleitner et al. (2012) for the proof of uniformity. For the generalized spiral points construction, we refer the reader to Hardin et al. (2016) for the proof of uniformity of this construction. Minimizing Coulomb energy is proven to create an asymptotic uniform sequence in Go\u0308tz (2000).\nNow denote \u03b3L = 1L \u2211L i=1 \u03b4\u03b8i and \u03b8 \u223c U(Sd\u22121). Given an asymptotically uniform point set \u03b81, . . . , \u03b8L, we have \u03b3L w\u2192 U(Sd\u22121) as L \u2192 \u221e, where w\u2192 denotes weak convergence of probability measures. That is, E\u03b8\u223c\u03b3L [g(\u03b8)] \u2192 E\u03b8\u223cU(Sd\u22121)[g(\u03b8)] for all bounded continuous functions g. Thus, by the definition of the SW distance and its QSW approximation, one is left to show that \u03b8 7\u2192 Wpp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) is bounded and continuous for any two measures \u00b5, \u03bd with finite p\nth moment. We show these properties for Wp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) and then invoke continuity of the real function x 7\u2192 x1/p.\nAs for boundedness, one can use the Cauchy-Schwartz inequality on Rd to get Wp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) = (\ninf \u03c0\u2208\u03a0(\u03bd,\u00b5) \u222b Rd |\u03b8\u22a4x\u2212 \u03b8\u22a4y|p\u03c0(dx, dy) )1/p\n\u2264 (\ninf \u03c0\u2208\u03a0(\u03bd,\u00b5) \u222b Rd \u2225x\u2212 y\u2225p\u03c0(dx, dy) )1/p\n= Wp(\u00b5, \u03bd) < \u221e for all \u03b8 \u2208 Sd\u22121. As for continuity, let (\u03b8t)t\u22651 be a sequence on Sd\u22121 converging to \u03b8 \u2208 Sd\u22121. Then\n|Wp(\u03b8t\u266f\u00b5, \u03b8t\u266f\u03bd)\u2212 Wp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd)| \u2264 |Wp(\u03b8t\u266f\u00b5, \u03b8t\u266f\u03bd)\u2212 Wp(\u03b8\u266f\u00b5, \u03b8t\u266f\u03bd)| + |Wp(\u03b8\u266f\u00b5, \u03b8t\u266f\u03bd)\u2212 Wp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd)| \u2264 Wp(\u03b8\u266f\u00b5, \u03b8t\u266f\u00b5) + Wp(\u03b8\u266f\u03bd, \u03b8t\u266f\u03bd),\nwhere the last inequality is a straightforward consequence of the triangle inequality applied to the metric Wp. Let \u03bb \u2208 {\u00b5, \u03bd}. Then, using again the Cauchy-Schwartz inequality, we obtain\nWp(\u03b8\u266f\u03bb, \u03b8t\u266f\u03bb) = (\ninf \u03c0\u2208\u03a0(\u03bb,\u03bb) \u222b Rd |\u03b8\u22a4t x\u2212 \u03b8\u22a4y|p\u03c0(dx, dy) )1/p\n\u2264 (\u222b\nRd |\u03b8\u22a4t x\u2212 \u03b8\u22a4x|p\u03bb(dx) )1/p \u2264\n(\u222b Rd \u2225x\u2225p\u03bb(dx) )1/p\n\ufe38 \ufe37\ufe37 \ufe38 <\u221e\n\u2225\u03b8t \u2212 \u03b8\u2225 \u2192 0 as t \u2192 \u221e.\nThis implies Wp(\u03b8t\u266f\u00b5, \u03b8t\u266f\u03bd) \u2192 Wp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) as t \u2192 \u221e, proving continuity.\nA.2 PROOF OF PROPOSITION 2\nFor the Gaussian-based mapping construction, given a Sobol sequence x1, . . . , xL \u2208 [0, 1]d, applying scrambling returns x\u20321, . . . , x \u2032 L \u2208 U([0, 1]d) (Owen, 1995). Since f(x) = \u03a6\u22121(x) ||\u03a6\u22121(x)||2 is the normalized inverse Gaussian CDF, \u03b8\u2032l = f(x \u2032 l) \u223c U(Sd\u22121) for all l = 1, . . . , L.\nFor the random rotation construction, given a fixed vector \u03b8 \u2208 Sd\u22121 and U = (u1, . . . , ud) \u223c U(Vd(Rd)), we now prove that U\u03b8 \u223c U(Sd\u22121). For any U1 \u2208 Vd(Rd), we have U1U = U2 with U2 \u223c U(Vd(Rd)). Therefore, we have that U\u03b8 has the same distribution as U1U\u03b8. Since there is only one distribution on Sd\u22121 is invariant to rotation (Theorem 3.7 in (Mattila, 1999)) which is the uniform distribution, U\u03b8 \u223c U(Sd\u22121). Therefore, we obtain that \u03b8\u20321, . . . , \u03b8\u2032L, generated by uniform random rotation of a point set \u03b81, . . . , \u03b8L, are uniformly distributed.\nNow, given \u03b8\u20321, . . . , \u03b8 \u2032 L \u223c U(Sd\u22121), we have\nE[R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L)] = E\n[ 1\nL L\u2211 l=1 Wpp(\u03b8 \u2032 l\u266f\u00b5, \u03b8 \u2032 l\u266f\u03bd)\n]\n= 1\nL L\u2211 l=1 E[Wpp(\u03b8\u2032l\u266f\u00b5, \u03b8\u2032l\u266f\u03bd)]\n= 1\nL L\u2211 l=1 SWpp(\u00b5, \u03bd) = SW p p(\u00b5, \u03bd),\nwhich completes the proof."
        },
        {
            "heading": "B ADDITIONAL BACKGROUND",
            "text": "Wasserstein distance. Given two probability measures \u00b5 \u2208 Pp(Rd) and \u03bd \u2208 Pp(Rd), and p \u2265 1, the Wasserstein distance (Villani, 2008; Peyre\u0301 & Cuturi, 2019) between \u00b5 and \u03bd is\nWp(\u00b5, \u03bd) = (\ninf \u03c0\u2208\u03a0(\u00b5,\u03bd) \u222b Rd\u00d7Rd \u2225x\u2212 y\u2225ppd\u03c0(x, y) )1/p , (7)\nwhere \u03a0(\u00b5, \u03bd) is the set of all couplings whose marginals are \u00b5 and \u03bd. Considering the discrete case, namely, \u00b5 = \u2211n i=1 \u03b1i\u03b4xi and \u03bd = \u2211n j=1 \u03b2j\u03b4yj with \u2211n i=1 \u03b1i = \u2211n j=1 \u03b2j , one obtains:\nWpp(\u00b5, \u03bd) = min \u03c0\u2208\u03a0(\u03b1,\u03b2) n\u2211 i=1 n\u2211 j=1 \u03c0ij ||xi \u2212 yj ||pp, (8)\nwhere \u03a0(\u00b5, \u03bd) = {\u03c0 \u2208 Rn\u00d7n+ |\u03c01 = \u03b1, \u03c0\u22a41 = \u03b2}. Using linear programming, the computational complexity and memory complexity of the Wasserstein distance are O(n3 log n) and O(n2). Algorithms. We first introduce the computational algorithm for Monte Carlo estimation of SW in Algorithm 1. Next, we provide the algorithm for QMC approximation of SW in Algorithm 2. Finally, we present the algorithms for Randomized QMC estimation of the SW distance with scrambling and random rotation in Algorithms 3 and 4, respectively.\nGeneration of Sobol sequence. From (Joe & Kuo, 2003), for generating a Sobol point set x1, . . . , xL \u2208 [0, 1]d, we need to follow the following procedure. For the j-th point, we need to choose a primitive polynomial of some degree sl in the field Z2 (set of integer of module 2), that is:\nzsj + a1,jz sj\u22121 + . . .+ asj\u22121,jz + 1,\nwhere the coefficients a1,j , . . . , asj\u22121,j are either 0 or 1. We then use a1,j , . . . , asj\u22121,j to define a sequence m1,j ,m2,j , . . .msj ,j such that:\nmk,j = 2a1,jmk\u22121,j \u2295 22a2,jmk\u22122,j \u2295 . . .\u2295 2sj\u22121asj\u22121,jmk\u2212sj+1,j \u2295 2sjmk\u2212sj ,1 \u2295mk\u2212sj ,j ,\nAlgorithm 1 Monte Carlo estimation of the Sliced Wasserstein distance. Input: Probability measures \u00b5 and \u03bd, p > 1, and the number of projections L. Set S\u0302W p\np(\u00b5, \u03bd;L) = 0 for l = 1 to L do\nSample \u03b8l \u223c U(Sd\u22121) Compute S\u0302W p\np(\u00b5, \u03bd;L) = 1 L \u2211L l=1 \u222b 1 0 |F\u22121\u03b8l\u266f\u00b5(z)\u2212 F \u22121 \u03b8l\u266f\u03bd\n(z)|pdz end for Return: S\u0302W p\np(\u00b5, \u03bd;L)\nAlgorithm 2 Quasi-Monte Carlo approximation of the sliced Wasserstein distance.\nInput: Probability measures \u00b5 and \u03bd, p > 1, QMC point set \u03b81, . . . , \u03b8L \u2208 Sd\u22121. Set Q\u0302SW p\np(\u00b5, \u03bd; \u03b81, . . . , \u03b8L) = 0 for l = 1 to L do\nCompute Q\u0302SW p p(\u00b5, \u03bd; \u03b81, . . . , \u03b8L) = Q\u0302SW p p(\u00b5, \u03bd; \u03b81, . . . , \u03b8L)+ 1 L \u222b 1 0 |F\u22121\u03b8l\u266f\u00b5(z)\u2212F \u22121 \u03b8l\u266f\u03bd\n(z)|pdz end for Return: Q\u0302SW p\np(\u00b5, \u03bd; \u03b81, . . . , \u03b8L)\nAlgorithm 3 Randomized Quasi-Monte Carlo estimation of the Sliced Wasserstein distance with scrambling.\nInput: Probability measures \u00b5 and \u03bd, p > 1, QMC point set x1, . . . , xL \u2208 [0, 1]d. Scramble x1, . . . , xL to obtain x\u20321, . . . , x \u2032 L Compute \u03b8\u20321, . . . , \u03b8 \u2032 L = f(x \u2032 1), . . . , f(x \u2032 L) for f the Gaussian-based mapping or the equal area mapping. Set R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) = 0\nfor l = 1 to L do Compute R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) = R\u0302QSW\np p(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) + 1 L \u222b 1 0 |F\u22121\u03b8\u2032l\u266f\u00b5(z) \u2212\nF\u22121\u03b8\u2032l\u266f\u03bd (z)|pdz\nend for Return: R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L)\nAlgorithm 4 The Randomized Quasi-Monte Carlo estimation of sliced Wasserstein distance with random rotation.\nInput: Probability measures \u00b5 and \u03bd, p \u2265 1, QMC point set \u03b81, . . . , \u03b8L \u2208 Sd\u22121. Sample U \u223c U(Vd(Rd)) Compute \u03b8\u20321, . . . , \u03b8 \u2032 L = U\u03b81, . . . , U\u03b8L Set R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) = 0\nfor l = 1 to L do Compute R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) = R\u0302QSW\np p(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L) + 1 L \u222b 1 0 |F\u22121\u03b8\u2032l\u266f\u00b5(z) \u2212\nF\u22121\u03b8\u2032l\u266f\u03bd (z)|pdz\nend for Return: R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1, . . . , \u03b8 \u2032 L)\nfor k > sj + 1 and \u2295 is the bit-by-bit exclusive-OR operator. The initial values of m1,j ,m2,j , . . .msj ,j are chosen freely such that mk,j , 1 \u2264 k \u2264 sj is odd and less than 2k. After that, direction numbers v1,j , v2,j , . . . vsj ,j are defined as:\nvk,j = mk,j 2k .\nFinally, we have:\nxl,j = b1v1,j \u2295 b2v2,j \u2295 . . . ,\u2295bsjvsj ,j ,\nwhere bi is the i-th bit from the right when l is written in binary ,i.e, , (. . . b2b1)2 is the binary representation of l. For greater detail, we refer the reader to (Joe & Kuo, 2003) for more detailed and practical algorithms.\nConfidence Intervals. Using the discussed methodology, one can obtain M i.i.d RQSW estimates, i.e., R\u0302QSW p\np(\u00b5, \u03bd; \u03b8 \u2032 1m, . . . , \u03b8 \u2032 Lm) for m = 1, . . . ,M . Since RQSW is an unbiased estimate of the\npopulation SW, the central limit theorem ensures the following:\n\u00b5\u0302M \u2212 SWpp(\u00b5, \u03bd) s\u0302M/ \u221a M d\u2192 N (0, 1)\nas M \u2192 \u221e, where \u00b5\u0302M and s\u0302M are the sample mean and standard deviation based on the generated M -size sample. Therefore, a 1 \u2212 \u03b1 size asymptotic confidence interval for SWpp(\u00b5, \u03bd) is readily obtained as\n\u00b5\u0302M \u00b1 z\u03b1/2s\u0302M/ \u221a M.\nwith z\u03b1/2 denoting the \u03b1/2 quantile of a standard normal random variable. Alternatively, by sampling with replacement from the M generated RQSW estimates, one can obtain B bootstrap replications of \u00b5\u0302M , say \u00b5\u0302 (b) M for b = 1, . . . , B, and construct a 1\u2212 \u03b1 bootstrap confidence interval for SW p p(\u00b5, \u03bd) as [q\u0302\u03b1/2, q\u03021\u2212\u03b1/2], where q\u0302\u03c9 denotes the \u03c9 sample quantile of {\u00b5\u0302 (b) M : b = 1, . . . , B}."
        },
        {
            "heading": "C RELATED WORKS",
            "text": "Beyond the uniform slicing distribution. Recent works have explored non-uniform slicing distributions (Nguyen et al., 2021; Nguyen & Ho, 2023). Nevertheless, the uniform distribution remains foundational in constructing the pushforward slicing distribution (Nguyen et al., 2021) and the proposal distribution (Nguyen & Ho, 2023). Consequently, Quasi-Monte Carlo methods can also enhance the approximation of the uniform distribution.\nBeyond 3D. It is worth noting that the Gaussian-based construction, maximizing distance, and minimizing Coulomb energy can be applied directly in higher dimensions, i.e., d > 3. Similarly, their randomized versions could also be used directly in higher dimensions. However, the quality of QMC point sets in high dimensions and their approximation errors are still open questions and require a detailed investigation. Therefore, we will leave this exploration to future work\nScaled Mapping. Quasi-Monte Carlo is briefly used for SW in (Lin et al., 2020). In particular, the authors utilize the Halton sequence in the three-dimensional unit cube, then map them to the unit sphere via the scaled mapping f(x) = x\u2225x\u22252 . However, this construction is heuristic and lacks meaningful properties. We visualize point sets of sizes 10, 50, 100 in Figure 5. From the figure, it is evident that this construction does not exhibit low-discrepancy behavior, as all points are concentrated in one region of the sphere.\nNear Orthogonal Monte Carlo. Motivated by orthogonal Monte Carlo, the authors in (Lin et al., 2020) propose near-orthogonal Monte Carlo, aiming to make the angles between any two samples close to orthogonal. We utilized the published code for optimization-based approaches available at https://github.com/HL-hanlin/OMC to generate point sets of size L in three dimensions, where L is chosen from the set 10, 50, 100. For our experiments, we generated only one batch of L points, avoiding the need to specify the second hyperparameter related to the number of batches. We visualize the resulting point sets and their corresponding spherical cap discrepancies in Figure 5. From the figure, it is evident that NOMC yields better spherical cap discrepancies compared to conventional Monte Carlo methods. However, it is important to note that NOMC does not achieve the same level of performance as the QMC point sets we discuss in this work. In this study, our primary focus is on QMC methods, and as such, we leave a detailed investigation of the application of OMC methods for SW to future research."
        },
        {
            "heading": "D DETAILED EXPERIMENTS",
            "text": "D.1 SPHERICAL CAP DISCREPANCY\nWe plotted the spherical cap discrepancies of the discussed QMC point sets and added hypothetical lines of CL\u22123/4 \u221a log(L) for C = 1.6, C = 1, C = 0.95 in Figure 7. From the figure, it is evident that QMC point sets derived from generalized spiral points, maximizing distance, and minimizing Coulomb energy exhibit a faster convergence rate than O(L\u22123/4 \u221a log(L)). Consequently, they can be classified as low-discrepancy sequences. Regarding the equal-area mapping construction, it demonstrates approximately the same convergence rate as O(L\u22123/4 \u221a log(L)), suggesting its potential as a low-discrepancy sequence. However, Gaussian-based mapping QMC point sets and random (MC) point sets do not exhibit low-discrepancy behavior. In summary, we recommend using generalized spiral points, maximizing distance, and minimizing Coulomb energy point sets for approximating SW when distance values are a critical factor in the application.\nD.2 POINT-CLOUD INTERPOLATION\nApproximate Euler methods. We want to iterate through the curve Z\u0307(t) = \u2212n\u2207Z(t) [SW2 (PZ(t), PY )]. For each iteration with t = 1, . . . , T , we first construct a point set \u03b81, . . . , \u03b8L based on the discussed approaches using MC, QMC methods, and randomized QMC methods. After that, with a step size \u03b7 > 0, we update:\nZ(t) = Z(t\u2212 1)\u2212 n\u03b7\u2207Z(t\u22121)\n[ 1\nL L\u2211 l=1 W22 ( \u03b8l\u266fPZ(t\u22121), \u03b8l\u266fPY\n)]1/2 .\nVisualization for L = 100. In addition to the partial visualization in the main text, we provide a full visualization of point-cloud interpolation from all QSW and RQSW variants in Figure 8. We observe that QSW variants cannot produce smooth point clouds at the final time step since they use the same QMC point sets across all time steps. In contrast, RQSW variants expedite the process of achieving a smooth point cloud that closely resembles the target. When compared to RQSW variants, the point cloud at the final time step from SW (the conventional MC) still contains some points that deviate significantly from the main shape.\nResults for L = 10. We repeated the same experiments with L = 10. We have reported the Wasserstein-2 distances for intermediate point-clouds (relative to the target point-cloud) in Table 3. We observed a similar phenomenon as with L = 100, namely, RQSW outperforms QSW significantly and also performs better than SW. Compared to L = 100, all approximations from L = 10 yield higher Wasserstein-2 distances. However, the gaps between QSW variants are wider. Therefore, RQSW variants are more robust to the choice of L than QSW.\nResults for a different pair of point-clouds. We conduct the same experiments with a different pair of point-clouds, namely, 2 and 3 in Figure 1. We present a summary of Wasserstein-2 distances in Table 4 for L = 100 and Table 5 for L = 10. We observe the same phenomena as in the previous experiments. Firstly, RQSW variants produce shorter curves than QSW variants. Secondly, a larger\nnumber of projections is better, and RQSW variants are more robust to changes in L than QSW variants. Additionally, we provide visualizations for L = 100 in Figure 9. From the figure, we can see consistent qualitative comparisons with the Wasserstein-2 distances reported in the tables.\nRecommended variants. Overall, we recommend RSQSW, RDQSW, and RCQSW for the pointcloud interpolation application since they give consistent performance for L = 100 and L = 10 for both tried pairs of point-clouds.\nD.3 IMAGE STYLE TRANSFER\nDetailed settings. We first reduce the number of colors in the images to 3000 using K-means clustering. Similar to the point-cloud interpolation, we iterate through the curve between the empirical distribution of colors in the source image and the empirical distribution of colors in the target image using the approximate Euler method.\nFull results for L = 100. We present style-transferred images and their corresponding Wasserstein-2 distances to the target image in terms of color palettes at the last iteration (1000) in Figure 10. From the figure, it is evident that QSW variants facilitate faster color transfer compared to SW. To elaborate, SW exhibits a Wasserstein-2 distance of 458.29, while the highest Wasserstein-2 distance among QSW variants is 158.6, achieved by GQSW. The use of RQSW can further enhance quality; for instance, the highest Wasserstein-2 distance among RQSW variants is 1.45, achieved by RGQSW. The best-performing variant in this application is RSQSW; however, other RQSW variants are also comparable.\nFull results for L = 10. We repeat the experiment with L = 10. In all approximations, decreasing L to 10 results in a higher Wasserstein-2 distance, which is understandable based on the approximation error analysis. In this scenario, the performance of some QSW variants (GQSW, EBQSW, SQSW) degrades to the point of being even worse than SW. In contrast, the degradation of RQSW variants is negligible, particularly for RCQSW.\nRecommended variants. Overall, we recommend RCQSW for this application since it performs consistently for both setting of L = 100 and L = 10.\nD.4 DEEP POINT-CLOUD AUTOENCODER\nFull visualization for L = 100. We first visualize reconstructed point-clouds from all approximations, including SW, QSW variants, and RQSW variants in Figure 12. Overall, we observe that the sharpness of the reconstructed point-clouds aligns with the reconstruction losses presented in Table 2. However, the point-clouds generated by GQSW lack meaningful structure, likely due to numerical issues encountered during training. These issues may stem from the numerical computation of the inverse CDF for specific projecting directions at certain iterations. Randomized versions of GQSW could potentially mitigate such problems, as stochastic gradient training may help avoid undesirable configurations in neural networks.\nResults for L = 10. We reduce the number of projections L to 10 and subsequently report the reconstruction losses in Table 6. Similar to other applications, reducing L results in increased reconstruction losses, particularly for QSW variants. In this specific application, RQSW variants demonstrate their robustness to the choice of L; the reconstruction losses for L = 10 are comparable to those for L = 100, as shown in Table 2. Additionally, we provide visualizations of the reconstructed point-clouds for L = 10 in Figure 13. It is evident from the figure that reconstructed point-clouds from QSW variants exhibit significant noise.\nRecommended variants. Overall, we recommend RCQSW for this application since it performs well in both settings of L = 100 and L = 10 in terms of both reconstruction losses and qualitative comparison."
        },
        {
            "heading": "E COMPUTATIONAL INFRASTRUCTURE",
            "text": "We use a single NVIDIA V100 GPU to conduct experiments on training deep point-cloud autoencoder. Other applications are done on a desktop with an Intel core I5 CPU chip."
        }
    ],
    "year": 2024
}