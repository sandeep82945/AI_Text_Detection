{
    "abstractText": "Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modeling. In this work, we generate a big dataset of structure-property relationships for strut-based lattices. The dataset is made available to the community which can fuel the development of methods anchored in physical principles for the fitting of fourth-order tensors. In addition, we present a higher-order GNN model trained on this dataset. The key features of the model are (i) SE(3) equivariance, and (ii) consistency with the thermodynamic law of conservation of energy. We compare the model to non-equivariant models based on a number of error metrics and demonstrate its benefits in terms of predictive performance and reduced training requirements. Finally, we demonstrate an example application of the model to an architected material design task. The methods which we developed are applicable to fourth-order tensors beyond elasticity such as piezo-optical tensor etc.",
    "authors": [],
    "id": "SP:3d0d544c1b72c726c009625e69c777c7a2bd720d",
    "references": [
        {
            "authors": [
                "Brandon Anderson",
                "Truong Son Hy",
                "Risi Kondor"
            ],
            "title": "Cormorant: Covariant molecular neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Peter J. Basser",
                "Sinisa Pajevic"
            ],
            "title": "Spectral decomposition of a 4th-order covariance tensor: Applications to diffusion tensor MRI",
            "venue": "Signal Processing,",
            "year": 2007
        },
        {
            "authors": [
                "Jan-Hendrik Bastek",
                "Siddhant Kumar",
                "Bastian Telgen",
                "Rapha\u00ebl N. Glaesener",
                "Dennis M. Kochmann"
            ],
            "title": "Inverting the structure\u2013property map of truss metamaterials by deep learning",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Ilyes Batatia",
                "David P Kovacs",
                "Gregor Simm",
                "Christoph Ortner",
                "Gabor Csanyi"
            ],
            "title": "Mace: Higher order equivariant message passing neural networks for fast and accurate force fields",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Simon Batzner",
                "Albert Musaelian",
                "Lixin Sun",
                "Mario Geiger",
                "Jonathan P. Mailoa",
                "Mordechai Kornbluth",
                "Nicola Molinari",
                "Tess E. Smidt",
                "Boris"
            ],
            "title": "Kozinsky. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Brandstetter",
                "Rob Hesselink",
                "Elise van der Pol",
                "Erik J Bekkers",
                "Max Welling"
            ],
            "title": "Geometric and physical quantities improve e(3) equivariant message passing",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "N.A. Fleck",
                "V.S. Deshpande",
                "M.F. Ashby"
            ],
            "title": "Micro-architectured materials: past, present and future",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,",
            "year": 2010
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S. Schoenholz",
                "Patrick F. Riley",
                "Oriol Vinyals",
                "George E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,",
            "year": 2017
        },
        {
            "authors": [
                "Peter Helnwein"
            ],
            "title": "Some remarks on the compressed matrix representation of symmetric second-order and fourth-order tensors",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2001
        },
        {
            "authors": [
                "Padmeya Prashant Indurkar",
                "Sri Karlapati",
                "Angkur Jyoti Dipanka Shaikeea",
                "Vikram S. Deshpande"
            ],
            "title": "Predicting deformation mechanisms in architected metamaterials using gnn, 2022",
            "venue": "URL https://doi.org/10.48550/arXiv.2202.09427",
            "year": 2022
        },
        {
            "authors": [
                "Charles F. Jekel",
                "Kenneth E. Swartz",
                "Daniel A. White",
                "Daniel A. Tortorelli",
                "Seth E. Watts"
            ],
            "title": "Neural network layers for prediction of positive definite elastic stiffness tensors, 2022",
            "venue": "URL https://doi.org/10.48550/arXiv.2203.13938",
            "year": 2022
        },
        {
            "authors": [
                "Chaitanya K. Joshi",
                "Cristian Bodnar",
                "Simon V. Mathis",
                "Taco Cohen",
                "Pietro Li\u00f2"
            ],
            "title": "On the expressive power of geometric graph neural networks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Konstantinos Karapiperis",
                "Dennis M. Kochmann"
            ],
            "title": "Prediction and control of fracture paths in disordered architected materials using graph neural networks",
            "venue": "Communications Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "George Em Karniadakis",
                "Ioannis G. Kevrekidis",
                "Lu Lu",
                "Paris Perdikaris",
                "Sifan Wang",
                "Liu Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nature Reviews Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Risi Kondor",
                "Zhen Lin",
                "Shubhendu Trivedi"
            ],
            "title": "Clebsch\u2013gordan nets: a fully fourier space spherical convolutional neural network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Siddhant Kumar",
                "Stephanie Tan",
                "Li Zheng",
                "Dennis M. Kochmann"
            ],
            "title": "Inverse-designed spinodoid metamaterials",
            "venue": "npj Computational Materials,",
            "year": 2020
        },
        {
            "authors": [
                "Yi-Lun Liao",
                "Tess Smidt"
            ],
            "title": "Equiformer: Equivariant graph attention transformer for 3d atomistic graphs",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas S. Lumpe",
                "Tino Stankovic"
            ],
            "title": "Exploring the property space of periodic cellular structures based on crystal networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Maurizi",
                "Chao Gao",
                "Filippo Berto"
            ],
            "title": "Predicting stress, strain and deformation fields in materials and structures with graph neural networks",
            "venue": "Scientific Reports,",
            "year": 2022
        },
        {
            "authors": [
                "Paul P. Meyer",
                "Colin Bonatti",
                "Thomas Tancogne-Dejean",
                "Dirk Mohr"
            ],
            "title": "Graph-based metamaterials: Deep learning of structure-property relations",
            "venue": "Materials & Design,",
            "year": 2022
        },
        {
            "authors": [
                "Tom\u00e1\u0161 M\u00e1nik"
            ],
            "title": "A natural vector/matrix notation applied in an efficient and robust return-mapping algorithm for advanced yield functions",
            "venue": "European Journal of Mechanics - A/Solids,",
            "year": 2021
        },
        {
            "authors": [
                "Michael O\u2019Keeffe",
                "Maxim A. Peskov",
                "Stuart J. Ramsden",
                "Omar M. Yaghi"
            ],
            "title": "The reticular chemistry structure resource (RCSR) database of, and symbols for, crystal nets",
            "venue": "Accounts of Chemical Research,",
            "year": 2008
        },
        {
            "authors": [
                "S.J. Ramsden",
                "V. Robins",
                "S.T. Hyde"
            ],
            "title": "Three-dimensional euclidean nets from two-dimensional hyperbolic tilings: kaleidoscopic examples",
            "venue": "Acta Crystallographica Section A Foundations of Crystallography,",
            "year": 2009
        },
        {
            "authors": [
                "Elissa Ross",
                "Daniel Hambleton"
            ],
            "title": "Using graph neural networks to approximate mechanical response on 3d lattice structures",
            "venue": "URL https://thinkshell.fr/ wp-content/uploads/2019/10/AAG2020_24_Ross.pdf",
            "year": 2020
        },
        {
            "authors": [
                "V\u0131\u0301ctor Garcia Satorras",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "E(n) equivariant graph neural networks",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Nathaniel Thomas",
                "Tess Smidt",
                "Steven Kearnes",
                "Lusann Yang",
                "Li Li",
                "Kai Kohlhoff",
                "Patrick Riley"
            ],
            "title": "Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Sikko Van \u2019t Sant",
                "Prakash Thakolkaran",
                "Jon\u00e0s Mart\u0131\u0301nez",
                "Siddhant Kumar"
            ],
            "title": "Inverse-designed growth-based cellular metamaterials",
            "venue": "Mechanics of Materials,",
            "year": 2023
        },
        {
            "authors": [
                "Maurice Weiler",
                "Mario Geiger",
                "Max Welling",
                "Wouter Boomsma",
                "Taco S Cohen"
            ],
            "title": "3d steerable cnns: Learning rotationally equivariant features in volumetric data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Tian Xie",
                "Jeffrey C. Grossman"
            ],
            "title": "Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties",
            "venue": "Phys. Rev. Lett.,",
            "year": 2018
        },
        {
            "authors": [
                "Kailai Xu",
                "Daniel Z. Huang",
                "Eric Darve"
            ],
            "title": "Learning constitutive relations using symmetric positive definite neural networks",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Tianju Xue",
                "Sigrid Adriaenssens",
                "Sheng Mao"
            ],
            "title": "Learning the nonlinear dynamics of mechanical metamaterials with graph networks",
            "venue": "International Journal of Mechanical Sciences,",
            "year": 2023
        },
        {
            "authors": [
                "Li Zheng",
                "Siddhant Kumar",
                "Dennis M. Kochmann"
            ],
            "title": "Unifying the design space of truss metamaterials by generative modeling, 2023",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modeling. In this work, we generate a big dataset of structure-property relationships for strut-based lattices. The dataset is made available to the community which can fuel the development of methods anchored in physical principles for the fitting of fourth-order tensors. In addition, we present a higher-order GNN model trained on this dataset. The key features of the model are (i) SE(3) equivariance, and (ii) consistency with the thermodynamic law of conservation of energy. We compare the model to non-equivariant models based on a number of error metrics and demonstrate its benefits in terms of predictive performance and reduced training requirements. Finally, we demonstrate an example application of the model to an architected material design task. The methods which we developed are applicable to fourth-order tensors beyond elasticity such as piezo-optical tensor etc."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "A relatively new class of materials, architected (meta-)materials, emerged in the last century. (Fleck et al., 2010) These materials draw inspiration from nature, where many materials are light, yet strong, because of their porosity and microscopic architecture. As a subclass of architected materials, lattices are a collection of struts (edges) which are connected at nodes. See Figure 1a below and Figure 5 in the Appendix. Lattices are especially mechanically efficient, offering a very high specific stiffness (stiffness divided by density). For instance, it is possible to make materials with the density of water and the strength of steel.\nThe established tool for computational analysis of lattices is the finite element (FE) method, which is also the industry standard for other structures from buildings to cars and airplanes. There are a number of principles which the FE solution satisfies (subject to a suitable PDE and constitutive model). First, force equilibrium is satisfied at all nodes and the computed displacements are compatible. Second, the strain energy under any deformation is nonnegative as required by energy conservation. Third, results are equivariant to rigid body transformations: rotating the lattice does not change its fundamental properties; they rotate accordingly.\nAlthough the FE method is robust and physically grounded, its high computational cost can be prohibitive: for example, if each unit cell of size 1cm is discretized into 100 elements, a wing structure of \u223c 20m length would require n \u223c 109 elements. Machine learning methods have been used to overcome the computational cost of FE methods. Indurkar et al. (2022) employed message-passing GNN to classify lattices based on their mechanical response. Karapiperis & Kochmann (2023) used GNN to predict the crack path in disordered lattices. Xue et al. (2023) build a GNN to learn the non-linear dynamics of mechanical metamaterials. Maurizi et al. (2022) use GNN to predict the mechanical response of composites and lattices. Meyer et al. (2022) have presented a GNN framework to predict the stiffness tensor of shell-based lattices. Machine learning methods have also been used to do inverse design of materials. Kumar et al. (2020) couple inverse and forward models to design spinodoid materials with orthotropic symmetry. Bastek et al. (2022) use a similar models for strut-based lattices with fully tailorable 3D anisotropic stiffness. Zheng et al. (2023) build a VAE model for generation of lattices with up to 27 nodes and cubic symmetry.\nWhile these machine learning models offer a much higher speed than FE, they lack grounding in physical principles. This might not be an issue when the application is restricted to a particular lattice symmetry class and when the model is deployed for data that are close to the training distribution. However, models without encoded equivariance and energy conservation principles could fail dramatically if deployed to out-of-distribution lattice topologies \u2013 the predictions for the same lattice at different orientations might not be self-consistent, and negative deformation energy could be predicted, implying the ability to extract energy from passive material.\nIn this work, we rely on the equivariant methods which have been introduced by the computational chemistry community. (Thomas et al., 2018; Batzner et al., 2022; Batatia et al., 2022) As key contributions:\n\u2022 We introduce a new task into the ML community and provide a real-world dataset which can be used by researchers in the future to improve higher order physics-focused models.\n\u2022 We present one such model \u2013 the first equivariant model trained for prediction of the fourthorder elasticity tensor whose predictions are always energy conserving (consistent with the laws of physics).\n\u2022 We benchmark the model against non-equivariant models and show the benefits of key model components and training strategies."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 EQUIVARIANCE",
            "text": "In the domain of physical sciences, invariance or equivariance under some physical transformations is an important property. For example, in chemistry, the energy of a molecule needs to be the same regardless of the coordinate system chosen to represent the coordinates of the atoms. In our modeling of lattices, model predictions need to satisfy similar rules. Let L represent a lattice that has attributes of the following types: scalars (e.g. edge lengths L), vectors (e.g. edge directions v = vi), tensors (e.g. 4th order stiffness tensor C = Cijkl).\nIn the discussion of equivariance, we focus on two main actions: rigid body rotation and translation. Let Q(. ;R) represent the rotation given by the rotation matrix R applied on the object, which is the first argument of the function. The following analytical transformation rules apply for scalars,\nvectors, and higher order tensors:\nL\u0302 = Q(l;R) = L\nv\u0302i = Q(v;R) = Rijvj\nK\u0302ijk... = Q(K;R) = RiaRjbRkc...Kabc... All these objects are invariant to rigid body translation.\nThe notation Q(L;R) represents the rotation of the lattice L, which implies the rotation of all attributes of the lattice according to the aforementioned transformation rules. Translation of the lattice T (L; t) simply means displacing the nodal positions by the vector t: x\u2190 x+ t. Our task is to predict the stiffness tensor C for the lattice L. The prediction of modelM isM(L). The equivariance requirement is then\nM(T (L; t)) =M(L) \u2200t Q(M(L);R) =M(Q(L;R)) \u2200R"
        },
        {
            "heading": "2.2 EUCLIDEAN EQUIVARIANT MESSAGE PASSING NEURAL NETWORKS",
            "text": "Message Passing Neural Networks Euclidean Equivariant Message Passing Neural Networks (MPNNs) (Liao & Smidt, 2023; Thomas et al., 2018; Weiler et al., 2018; Kondor et al., 2018; Batzner et al., 2022; Brandstetter et al., 2022; Batatia et al., 2022; Satorras et al., 2021) are graph neural networks that are equivariant to rotations and translations. MPNNs map a graph G with labels called states \u03c3i on each node i, to a target y. At each layer t, MPNNs operate in four successive steps, the edge embedding, the pooling, the update and the readout,\nm (t) i = \u2295 j\u2208N (i) Mt ( \u03c3 (t) i , \u03c3 (t) j ) , h (t+1) i = Ut ( \u03c3 (t) i ,m (t) i ) , y = Rt ( {\u03c3(t)i } i,t ) (1)\nwhere Mt is the edge embedding function, \u2295\nj\u2208N (i) is the pooling operation (usually just a sum) over the neighborhood of the node i, N (i). Ut is the update function. These steps are repeated T times. Finally, the readoutRt maps the states to the target quantity.\nEquivariant MPNNs Most Euclidean MPNNs expand their internal features in a spherical basis. Node features carry an index lm specifying the order of the basis expansion.\nh (t) i,lm(R \u00b7 (r1, . . . , rN )) = \u2211 m\u2032 Dlm\u2032,m(R)h (t) i,lm\u2032(r1, . . . , rN ) (2)\nwith Dlm\u2032,m(R) the Wigner-D matrices corresponding to the action of the rotation group on the spherical basis. Therefore, this lm index is carried over to all internal features of the model.\nHigher order MPNNs In full generality, the message can be a simultaneous function of all neighboring atoms of the central atoms i. Therefore, one can expand the message in a many-body expansion of the states,\nm (t) i = \u2211 j u1 ( \u03c3 (t) i ;\u03c3 (t) j ) + \u2211 j1,j2 u2 ( \u03c3 (t) i ;\u03c3 (t) j1 , \u03c3 (t) j2 ) + \u00b7 \u00b7 \u00b7+ \u2211 j1,...,j\u03bd u\u03bd ( \u03c3 (t) i ;\u03c3 (t) j1 , . . . , \u03c3 (t) j\u03bd ) (3)\nThe number of simultaneous dependency is called the body-order. MPNN potentials were shown to increase the body order of messages by stacking layers. An alternative route is to include higherorder terms in the message construction. The MACE (Batatia et al., 2022) architecture, on which we will be building in this work, introduced a systematic way to efficiently approximate equivariant messages of an ordered arbitrary body."
        },
        {
            "heading": "2.3 SOLID MECHANICS",
            "text": "We consider lattices as infinite periodic tessellations of a unit cell. The resulting metamaterial can be characterized by macroscopic (homogenized) properties. The key variables in solid mechanics\nunder the assumption of small deformations are stress, \u03c3 = \u03c3ij , which is a measure of force, and strain \u03f5 = \u03f5kl, which is a measure of deformation. Both stress and strain are symmetric 3\u00d73 second order tensors (\u03c3ij = \u03c3ji, \u03f5kl = \u03f5lk).\nA third key component in solving a solid mechanics problem is the constitutive law, which related stress and strain. Linear elasticity postulates that \u03c3ij = Cijkl\u03f5kl where C = Cijkl is the fourth-order stiffness tensor.\nDeformation energy \u03c8 under deformation \u03f5 is given by the following tensor contraction. Importantly, thermodynamic laws prescribe non-negative deformation energy \u03c8 \u2265 0 for any admissible deformation \u03f5: \u03c8 = \u03c3ij\u03f5ij = \u03f5ijCijkl\u03f5kl \u2265 0 \u2200\u03f5 (4) Since this has to be true for all strains \u03f5, all eigenvalues of the stiffness tensor C must be nonnegative. The stiffness tensor must be positive semi-definite.\nThe 4th order stiffness tensor Cijkl is a 3\u00d73\u00d73\u00d73 tensor. While a tensor with such dimensionality could have up to 81 components, it can be easily shown that the tensor has only 21 independent components, because it possesses both minor and major symmetries (Section A.1):\nCijkl = Cjikl = Cijlk = Cklij"
        },
        {
            "heading": "3 RELATED WORK",
            "text": "Finite element (FE) modelling The gold standard in computational methods in mechanics has been finite element modelling. In the FE framework, the properties of constituent material (e.g. steel) are known, and FE is used to calculate the structural response. The structure has degrees of freedom uj , and it is loaded by external forces fi. The first step to solving a FE problem is to assemble stiffness matrix Kij , which relates displacements and forces: fi = Kijuj . The second step is to solve this matrix equation for u (usually by LU factorization). If properties of the material are known, FE provides very accurate predictions of the overall structural response. However, the computational complexity of the matrix inversion (or LU factorization) can be very high.\nDataset Lumpe & Stankovic (2021) explored the property space of a large dataset of mechanical lattices. The dataset which they used and made available comes from two crystallographic databases (Ramsden et al., 2009; O\u2019Keeffe et al., 2008). The assembled dataset includes nodal positions, edge connectivity, crystal constants, and some elastic properties (Young\u2019s moduli, shear moduli and Poisson\u2019s ratios in the three principal directions).\nCrystal Graph Convolutions (CGC) and modified CGC (mCGC) To our knowledge, the only existing GNN model used to predict the stiffness tensor of architected materials is due to Meyer et al. (2022). Instead of beam-based lattices, the authors fitted the stiffness tensor of shell-based lattices. Their model is not equivariant. They use a form of data augmentation whereby each lattice is rotated 90\u25e6 around the x\u2212, y\u2212 and z\u2212 axis, and mirrored about the x\u2212y, y\u2212z, and x\u2212z planes. This increased the size of the training dataset 7-fold. The loss used in training is component-wise smooth L1 on the 21 independent components of the stiffness tensor.\nNNConv for 3d lattices Ross & Hambleton (2020) use GNN to model cubic lattices with 48 rotational and reflectional symmetries. Their model is based on the NNConv layer, which was introduced by Gilmer et al. (2017). In the NNConv model, messages between nodes are formed as a matrix-vector product, where the entries of the matrix are not constant but rather depend on the features of the edge connecting the two nodes.\nE(3)-Equivariant Message Passing Neural Networks Equivariant Message Passing Neural Networks (MPNNs) Anderson et al. (2019); Thomas et al. (2018); Brandstetter et al. (2022); Batzner et al. (2022); Batatia et al. (2022) are a class of GNNs that respect Euclidean symmetries (rotations, reflections, and translations). Messages are usually expanded in a spherical basis, and depending on the order of expansion, not only vectors but also higher-order features such as tensors can be passed between layers. The have emerged as a powerful architecture for learning on geometric point clouds.\nMethods for ensuring positive (semi-)definiteness Jekel et al. (2022) review a number of methods that can be used to ensure that the output of a model is positive semi-definite. These include a methods based on Cholesky factorization by (Xu et al., 2021; Van \u2019t Sant et al., 2023) and a methods based on eigenvalue decomposition. Note that the eigenvalue decomposition can often have unstable gradients and assembling the matrix by Cholesky factorization is not SO(3) equivariant."
        },
        {
            "heading": "4 METHODS",
            "text": ""
        },
        {
            "heading": "4.1 DATASET",
            "text": "We adapted dataset from Lumpe & Stankovic (2021). The dataset contains 17 222 lattices with nodal positions, lattice constants, bar connectivities, and elastic properties: 3 Young\u2019s moduli, 3 shear moduli, and 12 Poisson\u2019s ratios at the given orientation.\nWe first process the dataset to fix or avoid problematic lattices which reduces the dataset size to 8296 base lattices. We augment the dataset by introducing nodal perturbations: for perturbation level 0.1, each node of a lattice is displaced from the original position by distance 0.1 in a random direction. After the new perturbed lattice is obtained, its elastic properties have to be computed using FE analysis.\nWe augmented the dataset by introducing nodal perturbations at levels 0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.1. At each level, we formed 10 distinct realizations of nodal perturbations. (Perturbations could only be applied to lattices which have at least 2 fundamental nodes.) This enlarged the entire database to 354 000 distinct geometries. For each geometry, FE analysis was run at 3 relative densities (strut thicknesses).\nWe split the dataset based on base lattices into: (i) 7000 training base names, and (ii) 1296 validation/test base names. This split ensures that we do not have similars perturbations of the same lattice in both training and test sets.\nSee the appendix for the various compositions of the training dataset. Validation and test sets are fixed for all training runs. Validation set consists of the 1296 lattices without any perturbations. Test set consists of 3 realizations of nodal perturbations at level 0.1 for the 1296 lattices. Thus, testing is done on OOD data."
        },
        {
            "heading": "4.2 ARCHITECTURE",
            "text": "The diagram in Figure 1b depicts the architecture of the model. Further details are explained in the Appendix. We highlight the main components here. The model relies on the MACE architecture for message passing which was adapted with minor changes. In particular, we used Gaussian embedding of edge scalars and all node features were initialised as ones and expanded using a linear layer. We modified the nonlinear readout to enable the processing of higher order tensors.\nThe significant contribution of this work is the positive semi-definite (PSD) stack. As a first step, the fourth-order tensor is transformed to Cartesian basis and then represented in Mandel notation as a matrix. Subsequently, a suitable PSD function is applied to the matrix, which enforces its positive semi-definiteness. This ensures energy conservation which was the key requirement of this work. The following section provides further details about the PSD stack."
        },
        {
            "heading": "4.3 MANDEL REPRESENTATION AND PSD LAYER",
            "text": "A fourth-order Cartesian tensor with major and minor symmetries Cijkl = Cijlk = Cjikl = Cklij has 21 independent components. Suppose this fourth-order tensor is a map between second-order stress and strain:\n\u03c3ij = Cijkl\u03f5kl\nIn Mandel notation, the second-order tensors can be written as 6-component vectors, and the fourthorder tensor A can be represented as a 6\u00d7 6 symmetric matrix:\n\u03c3(M) = [ \u03c311, \u03c322, \u03c333, \u221a 2\u03c323, \u221a 2\u03c313, \u221a 2\u03c312 ]T \u03f5(M) = [ \u03f511, \u03f522, \u03f533, \u221a 2\u03f523, \u221a 2\u03f513, \u221a 2\u03f512 ]T\nC(M) =  C1111 C1122 C1133 \u221a 2C1123 \u221a 2C1113 \u221a 2C1112 C2211 C2222 C2233 \u221a 2C2223 \u221a 2C2213 \u221a 2C2212 C3311 C3322 C3333 \u221a 2C3323 \u221a 2C3313 \u221a 2C3312\u221a 2C2311 \u221a 2C2322 \u221a 2C2333 2C2323 2C2313 2C2312\u221a\n2C1311 \u221a 2C1322 \u221a 2C1333 2C1323 2C1312 2C1312\u221a\n2C1211 \u221a 2C1222 \u221a 2C1233 2C1223 2C1213 2C1212  The energy conservation requirement can be rewritten as\n\u03c8 = \u03c3(M),T \u03f5(M) = \u03f5(M),TC(M)\u03f5(M) \u2200\u03f5(M). Therefore, positive-definite fourth-order C is equivalent to positive-definite matrix C(M).\nWe can apply various methods to enforce the positive definiteness of matrix C(M). These include taking even powers of the matrix, A2 and A4, matrix exponential, and its truncated versions, eA, (I +A/2)2, (I +A/4)4.\nWe prove in the Appendix that the PSD stack maintains equivariance of the framework."
        },
        {
            "heading": "4.4 TRAINING AND EVALUATION DETAILS",
            "text": "Base CGC and mCGC models are trained according to the procedure described in ref Meyer et al. (2022). Model MACE is a plain version of MACE that is trained without data augmentation. Where \u201d+tr\u201d is added to the model name, it denotes that the model was trained using our training method as outlined below. The suffix \u201d+ve\u201d denotes a model which includes the positive semi-definite layer and was trained using our training method. In our training method, we use dynamic data augmentation, whereby every time a lattice is accessed from the dataset, it is returned at a different random orientation (and target stiffness is transformed accordingly). Further details about training including the equations for the various types of loss (Lcomp, Ldir, Ldir,rel, Lequiv, \u03bb\u2212%) are in the Appendix."
        },
        {
            "heading": "5 RESULTS",
            "text": "In this section we compare the performance of our model with other models and identify the key components of both the model and training procedures. We also show an example of a downstream application of the GNN model in a design task."
        },
        {
            "heading": "5.1 EQUIVARIANT MODELS OUTPERFORM NON-EQUIVARIANT MODELS",
            "text": "In Table 1, we show the performance of three main classes of models: CGC, NNConv, and MACE for dataset 1imp (find dataset details in the Appendix).1 Crystal graph convolution (CGC) is based on works by Xie & Grossman (2018) and Meyer et al. (2022). In CGC, a constant learnt matrix multiplies node and edge features to create messages. Models NNConv and MACE are similar in the nature of their message passing: the matrix which multiplies node features to obtain messages is a function of edge features. Details of all the models are explained in the appendix. Comparing the errors, it is evident that the equivariant MACE model class achieves lowest errors by all metrics.\nWe observe the following. (i) By adding data augmentation to CGC and NNConv models, models CGC+tr and NNConv+tr achieve substantially reduced stiffness-based errors (Lcomp, Ldir, Ldir,rel), as well as equivariance loss, Lequiv. However, these models are more prone to predict negative eigenvalues (\u03bb\u2212%). (ii) Adding data augmentation to equivariant MACE model leads to a much smaller improvement. The proportion of predicted negative eigenvalues, \u03bb\u2212%, is not significantly affected. (iii) Models CGC+ve, NNConv+ve, and MACE+ve with encoded positive semi-definite output suffer in terms of increased stiffness-based errors. (iv) CGC-based models outperform NNConv-based models. NNConv models will therefore not be considered in the following sections.\n1The choice of lattice representation. Meyer et al. (2022) combined message passing on primal and dual graph in model mCGC. We do not observe any performance gain from using dual graph representation on our data, therefore this model is omitted from the main discussion. We report more details in the Appendix (Table 8)."
        },
        {
            "heading": "5.2 INDUCTIVE BIASES SUPERIOR TO OBSERVATION AND LEARNING BIASES",
            "text": "As outlined by Karniadakis et al. (2021), there are three conceptual pathways to embedding physics knowledge into machine learning models: observation bias, learning bias and inductive bias. Here we evaluate the efficacy of these biases from the viewpoint of equivariance and energy conservation.\nEquivariance learning We achieve observation bias for equivariance by rotating the data that the model is trained on. As explained in section 4, models which end in \u201c+tr\u201d suffix were trained using data augmentation by rotation. Table 2 shows that the equivariance error reduces dramatically when we incorporate rotation augmentation of data. During training of model CGC in Table 2, all lattices were processed at a single orientation. Note that this is different from model CGC in Table 1 which was trained with 7-fold data augmentation as presented by Meyer et al. (2022). Figure 2b shows that the equivariance error reduces during training for both CGC and CGC+tr models. Not only is the final equivariance loss for model CGC+tr lower, but also the rate at which Lequiv reduces is faster. It is instructive to note further that while validation loss keeps reducing during training (Figure 2a), the equivariance loss is not a monotonously decreasing function. Model MACE is equivariant by design, therefore equivariance loss, Lequiv, is zero both in Table 2 and Figure 2. Furthermore, the equivariant MACE model also has a lower component loss, Lcomp.\nEnergy conservation learning A second physical principle that our model should be aligned with is the positive semi-definiteness of stiffness. We evaluate this for the MACE model in Table 2 and Figure 2c. The base MACE model trained on the data predicts negative eigenvalues for 30% of lattices. We attempt to introduce learning bias in model MACE+lb as follows. Loss is modified to include a penalty which is calculated from directional projections cq the of predicted stiffness tensor, C\u0303, into 250 random directions dq: cq = C\u0303ijkldqidqjdqkdql. The penalty is then computed as k \u00d7 relu(\u2212cq) where k is a suitably chosen multiplier. This penalty is added to loss during training.\nTable 2 shows that this learning bias is not very effective in guiding the model towards positive semi-definite predictions. Figure 2c shows that the learning bias can have a positive effect during the dynamics of learning, but the final values of \u03bb\u2212%are similar whether or not learning bias is used.\nScaling with dataset size Using more training data is effectively an observation bias which should lead to better results for all models. In Figure 3a we plot component loss, Lcomp, for base CGC model, CGC with data augmentation and positive semi-definite layer (CGC+ve) and MACE model with positive semi-definite layer (MACE+ve). The composition of training datasets is explained in the Appendix. At any dataset size, the equivariant MACE+ve model outperforms the CGC-based models. The CGC model with data augmentation outperforms the base CGC model. The scaling slope was calculated as linear fit on log-log axes and is displayed on the graph. It is evident that the MACE+ve model has the most favourable scaling."
        },
        {
            "heading": "5.3 CHOICE OF POSITIVE (SEMI-)DEFINITE LAYER AND MACE-SPECIFIC PARAMETERS",
            "text": "We evaluate a number of methods for making the output positive (semi-)definite for MACE model class. 2 The results are displayed in Table 3. We empirically observe that the matrix square method, A2, achieves most favourable results. Moreover, this method also has the lowest associated computational cost. For these reasons, we use the matrix square method throughout the paper whenever \u201c+ve\u201d suffix is used, unless stated otherwise.\nSpherical frequency Lmax and degeneracy One of the most important hyperparameters of the MACE model is the maximum frequency of expansion in spherical basis, Lmax. In Figure 3b we show the sensitivity of model accuracy to the degree of expansion Lmax. Empirically, we observe that degree Lmax = 4 is required to achieve good accuracy. This is in line with the spherical form of the fourth-order stiffness tensor, which contains L = 4 components. Moreover, it has been remarked by Joshi et al. (2023) that certain types of highly symmetric graphs require high order of tensors L. More specifically, to identify the orientation of neighbourhood with L -fold symmetry, at least Lorder tensors are required. In the Appendix, we show how model which is internally truncated to Lmax = 2 or 3 is unable to predict anisotropic behaviour of a simple cubic lattice (Section A.14, Figure 7).\n2Positive definite vs semi-definite The physical principle of energy conservation requires non-negative deformation energy \u03c8 = \u03f5ijCijkl\u03f5kl \u2265 0 \u2200\u03f5. The case of zero eigenvalue of C does not violate energy conservation. A structure whose stiffness tensor has zero eigenvalue is a mechanism \u2013 it is possible to deform it without exerting any work. However, it is a feature of our dataset that all eigenvalues are positive. Therefore, we have the freedom to try both positive definite and positive semi-definite layers.\nCorrelation order \u03bd In Figure 3c we show the sensitivity of test error to the body-order of messages. Model with \u03bd = 1 does not contain the equivariant product layer and it equivalent to Tensor Field Network with 2-body messages. We observe that increasing the order of messages to threeand four-body (\u03bd = 2 and 3) significantly reduces error over the test dataset."
        },
        {
            "heading": "5.4 SPEEDUP USING MACHINE LEARNING METHODS",
            "text": "Table 4 shows a comparison of inference time for the three classes of investigated GNN models as well as for finite-element calculation. Time is reported for computation of stiffness tensor for 5000 lattices. The tests were run on desktop computer with Intel i7-11700 CPU, 96GB RAM and Nvidia RTX3070 GPU. While the equivariant MACE-based architecture is slower than the more standard CGC- and NNConv-based models, all these models are 3 orders of magnitude faster than FE calculation."
        },
        {
            "heading": "5.5 EXAMPLE APPLICATION: DESIGN OF AN ARCHITECTED MATERIAL",
            "text": "An important application of architected solids is to achieve complex anisotropic stiffness tensor that cannot be found in existing materials. In Figure 4 we show an example application of our GNN model in a gradient-based optimization scheme for the design of specific stiffness tensor. The starting unit cell, as correctly predicted by the GNN model, has the same stiffness in x\u2212 and y\u2212directions. The task is to perturb nodal positions to break the x\u2212y symmetry and reduce stiffness in the y\u2212direction. We use gradients returned from backpropagation and execute 50 steps of gradient descent algorithm. The optimization scheme produced the desired result with great accuracy as verified post-optimization using FE baseline. Error between the desired output and FE-verified ground truth is Lcomp = 1.84. Based on the speedup of the GNN model compared to FE, we estimate the GNN-based optimisation to also be 3 orders of magnitude faster than the FE baseline."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we present the application of Euclidean equivariant GNNs to the prediction of the 4th order stiffness tensor of architected lattice metamaterials. In addition to the intrinsic equivariance to rigid body rotations and translations, we designed the model to also preserve positive semi-definiteness of the predicted stiffness, in line with energy conservation. We benchmark the model against other architectures that were previously used for property prediction of lattice materials and demonstrate superior performance by all the metrics studied. Finally, we demonstrate a possible downstream use of the model in ML-based structural optimization. Fast and accurate property prediction models, such as the one we are presenting, achieve a significant improvement over the high computational cost of traditional FE methods and they are applicable to tensors beyond the stiffness tensor such as piezo-optical, elasto-optical and the flexoelectric tensors."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "To ensure reproducibility and completeness, we include detailed descriptions of the models used, hyperparameters, and data sources in the Appendix. The code and datasets will be available to the reviewers during the review process and they will be made available publicly after paper acceptance."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 SYMMETRIES OF THE STIFFNESS TENSOR",
            "text": "From equation 4, stiffness tensor can be expressed as the derivative of strain energy with respect to strain:\nC = Cijkl = \u22022\u03c8\n\u2202\u03f5ij\u2202\u03f5kl\nThe order of \u03f5ij and \u03f5kl is interchangeable, which results in the major symmetry for the stiffness tensor: Cijkl = Cklij .\nFurthermore, strain is defined as the symmetric gradient of displacement, u:\n\u03f5ij = 1\n2 ( \u2202ui \u2202xj + \u2202uj \u2202xi ) Therefore, \u03f5ij = \u03f5ji, which gives rise to the minor symmetry of the stiffness tensor.\nAll in all, the stiffness tensor C has both minor and major symmetries:\nCijkl = Cjikl = Cijlk = Cklij\nAs a result, 21 of the 3\u00d7 3\u00d7 3\u00d7 3 = 81 components of the stiffness tensor are independent."
        },
        {
            "heading": "A.2 METHODS FOR ENFORCING POSITIVE (SEMI-)DEFINITENESS",
            "text": "Here we outline methods which can be used to enforce positive (semi-)definiteness for n\u00d7nmatrices Rn \u2192 Rn. Section A.4 explains how the 4th order stiffness tensor can be efficiently represented in a matrix form which justifies the use of these methods."
        },
        {
            "heading": "A.2.1 CHOLESKY-BASED METHOD",
            "text": "Cholesky decomposition is defined for a Hermitian positive-definite matrix A : Rn \u2192 Rn as: A = LL\u2217\nwhere L is a lower diagonal matrix and L\u2217 is its conjugate transpose. The diagonal entries of L are positive.\nMachine learning methods (Xu et al., 2021; Jekel et al., 2022; Van \u2019t Sant et al., 2023) can use Cholesky factorization as follows. Suppose we require n\u00d7n positive definite matrix A : Rn \u2192 Rn. A neural network outputs k = n(n+1)/2 entries: a0, ..., ak. They are arranged into lower diagonal matrix L with the diagonal elements passed through a suitable function \u03c1 : R \u2192 R>0 (such as \u03c1(x) = exp(x)):\nL =  \u03c1(a0) 0 0 . . . a1 \u03c1(a2) 0 . . . a3 a4 \u03c1(a5) . . . ... ... ... . . .  Matrix product LL\u2217 then guarantees a symmetric (Hermitian) positive-definite matrix. Note that if zero eigenvalues are admissible, a different function \u03c1 : R\u2192 R\u22650 can be used (e.g. relu). Such construction, while simple, will not produce equivariant output because components of the matrix a0, ...ak are treated independent scalars."
        },
        {
            "heading": "A.2.2 EIGENVALUE-BASED METHOD",
            "text": "Symmetric matrix A is positive definite iff all its eigenvalues are positive. The eigenvalue-based methods operate on this premise (Jekel et al., 2022).\nSuppose we require n \u00d7 n positive definite matrix A : Rn \u2192 Rn. A neural network again outputs k = n(n+ 1)/2 entries: a0, ..., ak. They are arranged into a symmetric matrix M :\nM =  a0 a1 a3 . . . a1 a2 a4 . . . a3 a4 a5 . . . ... ... ... . . .  Eigenvalue decomposition is performed on this matrix: M = U\u039bUT . Next, a suitable function \u03c1 : R\u2192 R>0 is applied to the eigenvalue matrix \u039b:\n\u039b+ = \u03c1(\u03bb1) 0 . . .0 \u03c1(\u03bb2) . . .... ... . . . \nand positive definite matrix A is assembled as A = U\u039b+UT . Similarly, for positive semidefiniteness, function \u03c1 : R\u2192 R\u22650 should be used. The advantage of this method, as opposed to the Cholesky-based method, is that the geometric representation of eigenvectors is maintained \u2013 in other words, if the overall model had been equivariant with respect to vectors in U , it will remain equivariant after eigenvalues are made positive. The significant disadvantage of this method is that eigenvalue decomposition is not a stable operation with respect to gradients, which is also noted in the official PyTorch documentation."
        },
        {
            "heading": "A.2.3 MATRIX POWER AND MATRIX EXPONENTIAL",
            "text": "To avoid the computational complexity and gradient instability of eigenvalue decomposition, we can look for methods which will provide the same result \u2013 matrix with positive eigenvalues \u2013 without explicitly computing the eigenvalue decomposition. We have experimented with a number of methods which are based on taking even powers of matrix and calculating matrix exponential.\nMatrix exponential The action of matrix exponential on square symmetric n\u00d7 n matrix M is A = matrix exp(M) = Ue\u039bUT\ni.e. eigenvalues of A are exponentiated eigenvalues of M .\nThe method is usually implemented as an iterative algorithm in which the explicit calculation of eigenvectors is not required. While it is stable with respect to gradients, its execution takes 1.5 times longer than computing eigenvalue decomposition (comparing PyTorch linalg.matrix exp(M) and linalg.eigh(M)). The key difference between matrix exponential and matrix powers is that it produces strictly positive eigenvalues (and hence positive definite matrix).\nMatrix power Even powers of a symmetric n\u00d7 n matrix ensure non-negative eigenvalues: A = Mn = U\u039bnUT\nTherefore, it has the same effect as carrying out the eigenvalue decomposition and raising the eigenvalues to power n. However, it has a lower complexity and could be up to 80 times faster (comparing PyTorch linalg.matrix power(M , 2) and linalg.eigh(M)).\nWe evaluate the performance of 2nd and 4th power in Section 5.3.\nTruncated matrix exponential One of the ways to write matrix exponential is\neM = lim k\u2192\u221e\n( I + A\nk )k We evaluate the performance of a positive semi-definite layer for k = 2 and 4 in Section 5.3.\nAn important advantage of these methods as opposed to Cholesky-based methods is that they can be made equivariant. However, that is predicated on using the Mandel notation as opposed to the more traditional Voigt notation, as discussed in the following section."
        },
        {
            "heading": "A.3 SPECTRUM OF THE 4TH ORDER TENSOR",
            "text": "As outlined by Lord Kelvin (Thomson, 1856), there are 6 principal strains \u03f5 = E(i) such that stress is parallel to strain under that deformation:\n\u03c3 ( \u03f5 = E(i) ) = C : E(i) = \u03bb(i)E(i)\nwhere \u03bb(i) is a scalar eigenvalue of the stiffness tensor and E(i), i = 1, ..., 6 are the six 2nd order eigentensors. See also a more recent text by Basser & Pajevic (2007)\nThe equivalence between tensor notation using C, \u03f5ij , \u03c3ij and vector/matrix notation using C(M), \u03f5(M), \u03c3(M) provides a way to calculate the eigenvalues and eigentensors of the 4th order tensor C. The eigenvalues \u03bb(i) for tensor C are the eigenvalues of matrix C(M), and\nthe eigentensors E(i) are obtained by rearranging the eigenvectors of C(M). Suppose x =[ \u03f511, \u03f522, \u03f533, \u221a 2\u03f523, \u221a 2\u03f513, \u221a 2\u03f512 ] is an eigenvector of C(M). The corresponding eigentensor for C is\nE = [ \u03f511 \u03f512 \u03f513 \u03f512 \u03f522 \u03f523 \u03f513 \u03f523 \u03f533 ]"
        },
        {
            "heading": "A.4 MANDEL/KELVIN NOTATION",
            "text": "Stress \u03c3ij and strain \u03f5ij are 2nd order symmetric tensors:\n\u03c3 = [ \u03c311 \u03c312 \u03c313 \u03c312 \u03c322 \u03c323 \u03c313 \u03c323 \u03c333 ] ; \u03f5 = [ \u03f511 \u03f512 \u03f513 \u03f512 \u03f522 \u03f523 \u03f513 \u03f523 \u03f533 ]\nThey have 6 independent components: three direct components (indexed by 11,22,33), and three shear components (indexed by 12,13,23). They are often arranged in vector form using Voigt notation.\n\u03c3(V ) =  \u03c311 \u03c322 \u03c333 \u03c323 \u03c313 \u03c312  ; \u03f5(V ) =  \u03f511 \u03f522 \u03f533 2\u03f523 2\u03f513 2\u03f512  The factor of 2 in front of shear components of strain is to preserve the dot product equivalence for strain energy: in 2nd order notation, strain energy can be written as the contraction of stress and strain:\n\u03c8 = 1\n2 \u03c3 : \u03f5 =\n1 2 \u03c3(V ) \u00b7 \u03f5(V )\nFollowing this notation, the 4th order stiffness tensor can be represented as 6\u00d7 6 matrix C(V ) such that \u03c3(V ) = C(V )\u03f5(V ):\nC(V ) =  C1111 C1122 C1133 C1123 C1113 C1112 C2211 C2222 C2233 C2223 C2213 C2212 C3311 C3322 C3333 C3323 C3313 C3312 C2311 C2322 C2333 C2323 C2313 C2312 C1311 C1322 C1333 C1323 C1312 C1312 C1211 C1222 C1233 C1223 C1213 C1212  The disadvantage of this approach is that stress and strain are expressed in contravariant and covariant bases, respectively, which do not coincide (Helnwein, 2001; Ma\u0301nik, 2021). This makes the Voigt notation unsuitable for our GNN model. In particular, if the equivariant GNN model outputs 4th order tensor C which are arranged into 6\u00d7 6 matrix C(V ) using the Voigt notation, and we then apply a positive definite layer (e.g. matrix square), the output matrix loses equivariance property (as further explained in the following section).\nThis issue can be resolved using the Mandel/Kelvin notation. In Mandel notation, the second order stress and strain tensors are also written as 6-dimensional vectors, but they take the following form:\n\u03c3(M) =  \u03c311 \u03c322 \u03c333\u221a 2\u03c323\u221a 2\u03c313\u221a 2\u03c312  \u03f5(M) =  \u03f511 \u03f522 \u03f533\u221a 2\u03f523\u221a 2\u03f513\u221a 2\u03f512  Strain energy can still be expressed as contraction\n\u03c8 = 1\n2 \u03c3 : \u03f5 =\n1 2 \u03c3(M) \u00b7 \u03f5(M)\nMoreover, the norm of stress and strain is preserved under this notation\n||\u03f5|| = \u03f5 : \u03f5 = \u03f5(M) \u00b7 \u03f5(M); ||\u03c3|| = \u03c3 : \u03c3 = \u03c3(M) \u00b7 \u03c3(M)\nThe corresponding stiffness tensor C(M) can be written such that \u03c3(M) = C(M)\u03f5(M):\nC(M) =  C1111 C1122 C1133\n\u221a 2C1123 \u221a 2C1113 \u221a 2C1112\nC2211 C2222 C2233 \u221a 2C2223 \u221a 2C2213 \u221a 2C2212\nC3311 C3322 C3333 \u221a 2C3323 \u221a 2C3313 \u221a 2C3312\u221a\n2C2311 \u221a 2C2322 \u221a 2C2333 2C2323 2C2313 2C2312\u221a\n2C1311 \u221a 2C1322 \u221a 2C1333 2C1323 2C1312 2C1312\u221a\n2C1211 \u221a 2C1222 \u221a 2C1233 2C1223 2C1213 2C1212  Using this notation, both stress and strain are expressed in the same orthonormal basis. Contrary to using Voigt notation, we can use the Mandel notation in an equivariant framework. If equivariant GNN model outputs 4th order tensor C, we can arrange the components into 6\u00d7 6 stiffness matrix, and apply a positive definite layer to this matrix. Importantly, this pipeline will satisfy equivariance as shown below."
        },
        {
            "heading": "A.5 PROOF OF EQUIVARIANCE OF PSD LAYER IN MANDEL NOTATION",
            "text": "Let M be the output (arranged in Mandel notation) of equivariant MACE backboneM for lattice L M(L) = M\nand let Q(. ;R) represent the rotation given by the rotation matrix R applied on the object, which is the first argument of the function. We postulate that the representation of the rotation group in Mandel basis can be written in terms of matrix R(M) such that the rotated output M\u0302 is given by\nM\u0302 =M(Q(L);R)) = R(M)MR(M),T\nAfter pass through the PSD layer (for instance using matrix square), the output and its rotated version are given by\nA = M2 (5)\nA\u0302 = M\u03022 = R(M)MR(M),TR(M)MR(M),T (6)\nTherefore, the PSD layer constructed in this way is equivariant iff matrix R(M) is orthonormal: R(M),TR(M) = I . In this section, we prove both the postulate that the rotation of stiffness tensor in Mandel basis can be expressed using matrix R(M) and the statement that this matrix is orthonormal.\nWe first consider the basis of representation of stress in Mandel notation. For stress \u03c3(M) with components [u1, ..., u6], the basis is formed by the following second order tensors\n\u03c3(M) =  u1 u2 u3 u4 u5 u6  = u1e (1) \u2297 e(1)+ +u2e (2) \u2297 e(2)+ +u3e (3) \u2297 e(3)+ + u4\u221a 2 ( e(2) \u2297 e(3) + e(3) \u2297 e(2) ) + + u5\u221a 2 ( e(1) \u2297 e(3) + e(3) \u2297 e(1) ) +\n+ u6\u221a 2\n( e(1) \u2297 e(2) + e(2) \u2297 e(1)\n) (7)\nWe now proceed to derive the representation of SO(3) rotation group in Mandel notation. Without loss of generality, we assume that the basis vectors e(1), e(2), e(3) are originally aligned with the Cartesian axes. Therefore, the i-th component of vector e(j) is equivalent to Kronecker delta: e (j) i = \u03b4ij . The effect of rotation on the basis vectors e\n(1), e(2), e(3) can be expressed by matrix multiplication with conventional rotation matrix Rij as\ne\u0302 (p) i = Rije (p) j\nwhere e\u0302(p) is the basis vector e(p) expressed in the rotated basis and we use standard Einstein summation convention for repeated indices. The elements of the rotation matrix are therefore\nRij = e\u0302 (i) \u00b7 e(j)\nWe now express stress \u03c3 in the rotated frame \u03c3\u0302 in terms of the components of Mandel representation u1, ..., u6:\n\u03c3\u0302ij = RipRjp ( u1e (1) p e (1) q + ...+\nu4\u221a 2\n( e(2)p e (3) q + e (3) p e (2) q ) + ... ) = RipRjp ( u1\u03b41p\u03b41q + ...+\nu4\u221a 2 (\u03b42p\u03b43q + \u03b43p\u03b42q) + ... ) This can be written as a matrix-vector product in Mandel representation \u03c3\u0302(M) = R(M)\u03c3(M)\n=  R211 R 2 12 R 2 13\n\u221a 2R12R13\n\u221a 2R11R13\n\u221a 2R11R12\nR221 R 2 22 R 2 23\n\u221a 2R22R23\n\u221a 2R21R23\n\u221a 2R21R22\nR231 R 2 32 R 2 33\n\u221a 2R32R33\n\u221a 2R31R33 \u221a 2R31R32\u221a\n2R21R31 \u221a 2R22R32 \u221a 2R23R33 R22R33 +R23R32 R21R33 +R23R31 R21R32 +R22R31\u221a\n2R11R31 \u221a 2R12R32 \u221a 2R13R33 R12R33 +R13R32 R11R33 +R13R31 R11R32 +R12R31\u221a\n2R11R21 \u221a 2R12R22 \u221a 2R13R23 R12R23 +R13R22 R11R23 +R13R21 R11R22 +R12R21\n  u1 u2 u3 u4 u5 u6  Matrix R(M) is the representation of SO(3) rotation in Mandel notation. We can now proceed to derive the corresponding rotation rule for the stiffness matrix C(M). In the original frame,\n\u03c3(M) = C(M)\u03f5(M) (8) while in the rotated frame:\n\u03c3\u0302(M) = C\u0302(M)\u03f5\u0302(M)\nR(M)\u03c3(M) = C\u0302(M)R(M)\u03f5(M)\n\u03c3(M) = R(M),\u22121C\u0302(M)R(M)\u03f5(M) (9) comparing equations equation 8 and equation 9, we obtain the rotation rule for stiffness matrix C(M):\nC\u0302(M) = R(M)C(M)R(M),\u22121 (10)\nWe now proceed to show that matrix R(M) is orthonormal. This can be done by expanding the product R(M),TR(M) and showing that R(M)pi R (M) pj = \u03b4ij , but we choose an alternative route: by considering the double contraction of stress as dot product in Mandel basis.\nFrom equation equation 7, it is straightforward to show that\n\u03c3ij\u03c3ij = \u03c3 (M) i \u03c3 (M) i \u2200\u03c3ij\nWe then consider this contraction in rotated basis: \u03c3\u0302 (M) i \u03c3\u0302 (M) i = R (M) ip \u03c3 (M) p R (M) iq \u03c3 (M) q = \u03c3 (M),TR(M),TR(M)\u03c3(M)\nTherefore, to show that matrix R(M) is orthonormal, it suffices to show that \u03c3\u0302(M)i \u03c3\u0302 (M) i = \u03c3 (M) i \u03c3 (M) i .\n\u03c3\u0302 (M) i \u03c3\u0302 (M) i = RipRjqRiaRjb ( u1\u03b41p\u03b41q + ...+\nu4\u221a 2 (\u03b42p\u03b43q + \u03b43p\u03b42q) + ...\n)( u1\u03b41a\u03b41b + ...+\nu4\u221a 2 (\u03b42a\u03b43b + \u03b43a\u03b42b) + ... ) = \u03b4pa\u03b4qb ( u1\u03b41p\u03b41q + ...+\nu4\u221a 2 (\u03b42p\u03b43q + \u03b43p\u03b42q) + ...\n)( u1\u03b41a\u03b41b + ...+\nu4\u221a 2 (\u03b42a\u03b43b + \u03b43a\u03b42b) + ... ) = u21 + u 2 2 + u 2 3 + u 2 4 + u 2 5 + u 2 6 = \u03c3 (M) i \u03c3 (M) i\nwhere we used the fact that the conventional 3\u00d7 3 rotation matrix R is orthonormal RipRia = \u03b4pa. Therefore\nR(M),\u22121 = R(M),T\nWe can now reformulate the rotation rule for stiffness in Mandel notation from equation equation 10:\nC\u0302(M) = R(M)C(M)R(M),\u22121\nwhich validates the postulate of this proof. This combined with equation equation 6 proves that our PSD layer is equivariant."
        },
        {
            "heading": "A.6 PROOF OF NON-EQUIVARIANCE OF PSD LAYER IN VOIGT NOTATION",
            "text": "Analogous analysis can be performed for the stress/strain in Voigt basis. However, in Voigt notation, the bases for strain and stress are different:\n\u03c3(V ) =  u1 u2 u3 u4 u5 u6  = u1e (1) \u2297 e(1)+ +u2e (2) \u2297 e(2)+ +u3e (3) \u2297 e(3)+ +u4 ( e(2) \u2297 e(3) + e(3) \u2297 e(2) ) + +u5 ( e(1) \u2297 e(3) + e(3) \u2297 e(1) ) + +u6 ( e(1) \u2297 e(2) + e(2) \u2297 e(1) )\n\u03f5(V ) =  u1 u2 u3 u4 u5 u6  = u1e (1) \u2297 e(1)+ +u2e (2) \u2297 e(2)+ +u3e (3) \u2297 e(3)+ +u42 ( e(2) \u2297 e(3) + e(3) \u2297 e(2) ) + +u52 ( e(1) \u2297 e(3) + e(3) \u2297 e(1) ) + +u62 ( e(1) \u2297 e(2) + e(2) \u2297 e(1)\n) It can be shown through analogous process that the corresponding representations of the rotation group are matrices R(V,\u03c3) and R(V,\u03f5):\n\u03c3\u0302(V ) = R(V,\u03c3)\u03c3(V )\n=  R211 R 2 12 R 2 13 2R12R13 2R11R13 2R11R12 R221 R 2 22 R 2 23 2R22R23 2R21R23 2R21R22 R231 R 2 32 R 2 33 2R32R33 2R31R33 2R31R32\nR21R31 R22R32 R23R33 R22R33 +R23R32 R21R33 +R23R31 R21R32 +R22R31 R11R31 R12R32 R13R33 R12R33 +R13R32 R11R33 +R13R31 R11R32 +R12R31 R11R21 R12R22 R13R23 R12R23 +R13R22 R11R23 +R13R21 R11R22 +R12R21\n  u1 u2 u3 u4 u5 u6  \u03f5\u0302(V ) = R(V,\u03f5)\u03f5(V )\n=  R211 R 2 12 R 2 13 R12R13 R11R13 R11R12 R221 R 2 22 R 2 23 R22R23 R21R23 R21R22 R231 R 2 32 R 2 33 R32R33 R31R33 R31R32\n2R21R31 2R22R32 2R23R33 R22R33 +R23R32 R21R33 +R23R31 R21R32 +R22R31 2R11R31 2R12R32 2R13R33 R12R33 +R13R32 R11R33 +R13R31 R11R32 +R12R31 2R11R21 2R12R22 2R13R23 R12R23 +R13R22 R11R23 +R13R21 R11R22 +R12R21\n  u1 u2 u3 u4 u5 u6  and it can be shown that\nR(V,\u03c3),\u22121 = R(V,\u03f5),T .\nWe can now derive the rotation rule for stiffness in Voigt notation as\n\u03c3(V ) = C(V )\u03f5(V )\n\u03c3\u0302(V ) = C\u0302(V )\u03f5\u0302(V )\nR(V,\u03c3)\u03c3(M) = C\u0302(V )R(V,\u03f5)\u03f5(V )\n\u03c3(V ) = R(V,\u03c3),\u22121C\u0302(V )R(V,\u03f5)\u03f5(V )\nC\u0302(V ) = R(V,\u03c3)C(V )R(V,\u03f5),\u22121 = R(V,\u03c3)C(V )R(V,\u03c3),T . Importantly, matrices R(V,\u03c3) and R(V,\u03f5) are not orthonormal ( R(V,\u03c3),TR(V,\u03c3) \u0338= I ) which implies that using Voigt representation in PSD layer breaks equivariance."
        },
        {
            "heading": "A.7 LATTICE METAMATERIALS, GRAPH REPRESENTATION OF LATTICE UNIT CELLS AND FE",
            "text": "Figure 5 outlines the concept of metamaterials. We consider periodic lattices which are constructed by repeating a predefined building block in three dimensions. This building block is called unit cell. When the unit cell is repeated over a distance much longer than its size, there will be millions of unit cells in the sample of interest and its overall response can be characterized by effective material properties.\nIn this work, we model strut-based lattices. There is a clean analogy between such lattice and the mathematical concept of a graph. The struts in a lattice can be thought of as edges, while their intersections are nodes. We wish to model large samples of metamaterials, which comprise millions of unit cells. Rather than converting such sample into a graph with billions of nodes and edges, we use the concept of periodicity: the lattice can be fully defined by its unit cell and we wish to predict the stiffness of the material from the geometry of the unit cell.\nAll unit cells can be represented in a reduced coordinate system, where the unit cell is a unit cube (0 \u2264 xi \u2264 1). The real positions of nodes, transformed coordinates x\u0304, can be expressed as an affine transformation of the reduced coordinates: x\u0304 = Ax, where A is a suitable matrix. Unit cells that we study in this work range from very simple geometries with just a few nodes to very complex unit cells with hundreds of nodes. We define four node types based on the following conditions:\n1. inner nodes, where 0 < xi < 1 \u2200i, 2. face nodes, where xi \u2208 {0, 1} for only one index i, 3. edge nodes, where xi \u2208 {0, 1} for two indices i, 4. corner nodes, where xi \u2208 {0, 1} for all three indices i.\nFigure 6 illustrates the 4 node types. Further note that face, edge and corner nodes are shared by 2, 4 and 8 neighboring unit cells, respectively.\nThe unit cells are representation of the infinite periodic lattice. Therefore, it is possible to shift the window of observation to perceive a different unit cell of the same lattice. This is illustrated in Figure 6a-c. The simple cubic lattice is typically represented as a square with four edges on the boundaries and four edge nodes.3 Displacing the unit cell window, we obtain a view with 4 face nodes and 1 inner node.\n[detail] Finite element simulations are run on the windowed representation of lattices, because it makes the handling of periodicity much simpler. In particular, when macroscopic strain \u03f5ij is applied to the material, the following equations are prescribed in FE setup:\nuBi \u2212 uAi = \u2211 j \u03f5ij ( xBj \u2212 xAj ) \u03b8Bi \u2212 \u03b8Ai = 0\nThe fundamental representation of a lattice is such where only the inner nodes are kept and edges are connected across periodic boundaries. In Figure 6d, we show the fundamental representation of the simple cubic lattice. The lattice has 1 inner node (N0) and 2 fundamental edges (E0, E1). The\n3note that intuitively, we might call these nodes corner nodes, but to adhere to definitions above, in 2d xi \u2208 {0, 1} for two indices i\nedges are defined by edge adjacency, and edge shifts:\nadjacency shift E0 : N0\u2192 N0; [1, 0] E1 : N0\u2192 N0; [0, 1]\nIf edge has adjacencyNi \u2192 Nj and shift t(ij), then the edge vector will be v(ij) = x(j)\u2212x(i)+t(ij)\nGraph representation is obtained when graph is connected according to the fundamental edge adjacency and edge shifts are stored with the graph (Fig. 6e)."
        },
        {
            "heading": "A.8 DATASET",
            "text": "The dataset contains 8296 base lattices. We split the dataset into: (i) 7000 training base names, and (ii) 1296 validation/test base names.\nTraining sets of various sizes are formed as follows:\n# graphs Description 0imp quarter 1750 1750 lattices with no perturbations 0imp half 3500 3500 lattices with no perturbations 0imp 7000 7000 lattices with no perturbations 1imp 27847 7000 lattices with 1 realization at 0.0,0.02,0.04,0.07 levels 2imp 48681 7000 lattices with 1 realization at 0.0 and 2 realizations at 0.02,0.04,0.07 levels 4imp 90336 7000 lattices with 1 realization at 0.0 and 4 realizations at 0.02,0.04,0.07 levels\nNote that three relative densities (strut radii) of each distinct geometry are used."
        },
        {
            "heading": "A.9 GRAPH ATTRIBUTES",
            "text": "Vanilla CGC In the base CGC model, we use the same node, edge and graph features as Meyer et al. (2022) with the addition of strut radius. Node features of the graph are nodal positions:\nh = [x1, x2, x3]\nEdge features are unit vector, length, and radius:\ne = [u1, u2, u3, L, r]\nAugmented GCG-based models We wish to have a model which is invariant to rigid body translation, therefore we drop nodal positions from node features. The following input features are used.\nh = [1]\ne = [u1, u2, u3, L, r]"
        },
        {
            "heading": "A.10 ARCHITECTURE",
            "text": "We consider lattices as geometric graphs, with node positions, xi \u2208 R3, edge adjacency, {{i, j}}, edge shifts4, ui \u2208 R3, and edge thickness, rij \u2208 R+. Note that edge adjacency is a multiset as there can be multiple edges between the same set of nodes. The role of edge shifts is to account for periodic connections. Further detail can be found in the Appendix. The model we develop acts in three steps, first the embedding, then S layers of MACE, and finally the readout.\nEmbeddings The length and thickness of the edges are encoded using Gaussian embeddings with 6 bases: GL, Gr : R\u2192 R6. They are then concatenated and used as edge attributes\nzij = {e\u2212\u03b3c(\u2225xi\u2212xj\u22252\u2212\u00b5c) 2}c \u2295 {e\u2212\u03b3c(rij\u2212\u00b5c) 2}c (11) where \u2295 denotes concatenation and (\u03b3c, \u00b5c) are a collection of fixed parameters of the Gaussians and they depend on the number of bases. The edge vectors are expanded in a spherical basis up to Lmax = 4. Unlike atoms of various elements in chemistry, all our nodes are of the same type. Therefore, the node features are initialized as ones and are expanded to the desired number of dimensions using a linear layer: h(0)i = wi.\nMACE layer At each layer s of MACE, edge embedding \u03d5(s)ij are formed by taking the tensor product between the node features hj and the edge vectors expanded in a spherical basis. This tensor product is weighted with a non-linear function of the edge attributes zij ,\n\u03d5 (s) ij,k\u03b71l3m3\n= \u2211\nl1l2m1m2\nCl3m3\u03b71,l1m1l2m2R (s) k\u03b71l1l2l3 (zij)Y m1 l1 (x\u0302ij)h (s) j,kl2m2\n(12)\nWhere k indexes feature channels, l,m index angular momenta, Cl3m3\u03b71,l1m1l2m2 are the ClebschGordan coefficients that enforce the equivariance5, and \u03b7 indexes combinations of lmwhich preserve equivariance. The Atomic Basis A(s)i of the node i at layer s is constructed by summing edge embeddings over the edges of i:\nA (s) i,kl3m3 = \u2211 k\u0303,\u03b71 W (s) kk\u0303\u03b71l3 \u2211 j\u2208N (i) \u03d5 (s) ij,k\u0303\u03b71l3m3 (13)\nA tensor product is applied to the Atomic Basis \u03bd times to increase the body order of the feature, and the resulting features are symmetrized using a set generalized Clebsch-Gordan coefficients CLM\u03b7\u03bd lm.\nB (s),\u03bd i,\u03b7\u03bdkLM = \u2211 lm CLM\u03b7\u03bd lm \u03bd\u220f \u03be=1 A (s) i,kl\u03bem\u03be\n(14)\nwhere B(s),\u03bdi,\u03b7\u03bdkLM are called sketched product basis. The B\u2212features are then linearly mixed to form a many-body message,\nm (s) i,kLM = \u2211 \u03bd \u2211 \u03b7\u03bd W (t),\u03bd zi\u03b7\u03bdkL B (s),\u03bd i,\u03b7\u03bdkLM\n(15)\nFinally the message is used to update the next node features using an update function.\n4As defined in Section A.7 of the Appendix 5For further details, see the original MACE paper by Batatia et al. (2022)\nReadout After S layers of MACE, we use an equivariant non-linear readout followed by global graph pooling. Invariance to tessellation is maintained by mean pooling operation which ensures that the predicted stiffness will not grow if nodes with identical neighborhoods are added to the graph. Finally, another linear layer outputs two scalars, two l = 1 vectors and one l = 4 vector, corresponding to the spherical component of a 4th order tensor with the correct permutation symmetry. This is converted to Cartesian basis and assembled into Mandel notation to form the final matrix output A (see the Appendix for details).\nPositive Semi-Definite Layer The positive semi-definite layer is the key step to ensure that the final output is positive semi-definite, in line wih the law of energy conservation. We evaluate a number of methods to make the stiffness tensor positive semi-definite. These include taking even powers of the matrix, A2 and A4, matrix exponential, and its truncated versions, eA, (I +A/2)2, (I +A/4)4."
        },
        {
            "heading": "A.11 TRAINING AND EVALUATION DETAILS",
            "text": "Definitions of loss metrics If Cp = Cpij and C\u0303p = C\u0303pij are the predicted and target stiffnesses for lattice p (in Mandel notation), respectively, and Cp = Cpijkl and C\u0303p = C\u0303pijkl are the predicted and target stiffnesses for lattice p (in 4th order notation), respectively, the loss used during training is calculated as:\n\u03b3p = 1\n36 \u2211 ij C\u0303pijC\u0303pij Lcomp,p = \u2211 ij ( Cpij \u2212 C\u0303pij )2 Ltrain = 1 B \u2211 p Lcomp,p \u03b3p\n(16)\nwhere B denotes the total number of lattices p, \u03b3p the mean stiffness, Lcomp,p the component loss and Ltrain the overall loss. For testing purposes, in addition to loss Lcomp, we define directional loss, Ldir, and relative directional loss, Ldir,rel,p which are calculated using N = 250 random directions on the unit sphere (dq, q = 1...N ):\nLdir,p = 1\nN \u2211 q \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 ijkl ( Cpijkl \u2212 C\u0303pijkl ) dqidqjdqkdql \u2223\u2223\u2223\u2223\u2223\u2223 Ldir,rel,p = Ldir,p/\u221a\u03b3p (17) withLdir,p the directional loss andLdir,p the relative directional loss. We further report the proportion of lattices with negative eigenvalues, \u03bb\u2212%, and equivariance loss,Lequiv which is calculated as follows. We choose S random orientations (here S = 10) parameterized by corresponding rotation matrices\nR(s) = R (s) ij , (s = 1...S). The predicted stiffness tensor in the original orientation is C\u0302\n(p) . For\neach lattice in the test dataset L(p), we calculate the predictions for each of the 10 rotations: C(p,s)ijkl . Equivariance loss is defined as\nLequiv = 1 SNB \u2211 pqs \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 ijkl ( Q ( C\u0302 (p) ;R(s) ) ijkl \u2212 C(p,s)ijkl ) dqidqjdqkdql \u2223\u2223\u2223\u2223\u2223\u2223 Note that the equivariance loss is calculated purely from predictions, disregarding the mismatch from the ground truth.\nAll models were trained on a single NVIDIA A100 GPU with 80GB of memory. The training routines were handled by Pytorch Lightning. Specifics vary between models and are outlined below."
        },
        {
            "heading": "A.11.1 CGC AND MCGC",
            "text": "Hyperparameters were searched on a grid (Table 5). Every experiment was run with constant learning rate for up to 100 000 steps. Optimizer AdamW was used with settings (\u03b21, \u03b22) = (0.9, 0.999), \u03f5 = 1 \u00d7 10\u22128, weight decay=1 \u00d7 10\u22128. Validation loss was checked every 100 steps and training was stopped by early stopping callback with patience 50 if validation loss has not reduced.\nThe setup of the vanilla CGC and mCGC followed as closely as possible the methods used by Meyer et al. (2022). Key differences between our training routines for CGC-based models and the vanilla versions are as follows.\n\u2022 Static 7-fold data augmentation was used for training set. Each lattice was rotated by 90 deg around the x\u2212, y\u2212 and z\u2212axes, and mirrored about the x\u2212y, x\u2212z, and y\u2212z planes. This added six more versions of the lattice into the dataset.\n\u2022 The 21 components of stiffness tensor were independently normalized to lie between 0 and 1.\n\u2022 Optimizer RAdam was used\n\u2022 Loss smooth l1 was used"
        },
        {
            "heading": "A.12 NNCONV",
            "text": "In model NNConv, linear learnable layers were used as node and edge feature embeddings to increase latent dimensionality. The message passing NNConv layer was composed of 3-layer neural network with ReLU nonlinearity. \u201cSum\u201d aggregation was used to aggregate messages from neighbors, after which ReLU layer was applied. The layers of message passing were not shared, but independent. A residual connection between layers was used. Hyperparameters were searched on a grid (Table 6). Every experiment was run with constant learning rate for up to 100 000 steps. Optimizer AdamW was used with settings (\u03b21, \u03b22) = (0.9, 0.999), \u03f5 = 1 \u00d7 10\u22128, weight decay=1 \u00d7 10\u22128. Validation loss was checked every 100 steps and training was stopped by early stopping callback with patience 50 if validation loss has not reduced.\nA.12.1 MACE\nHyperparameters for MACE-based models were searched on a grid in Table 7. Every experiment was run with a constant learning rate for up to 30 000 steps. Optimizer AdamW was used with settings (\u03b21, \u03b22) = (0.9, 0.999), \u03f5 = 1 \u00d7 10\u22128, weight decay=1 \u00d7 10\u22128. A smaller batch size of 64 was used because of the higher memory requirements of the MACE model. To maintain consistency with the batch size of 256 from CGC models, gradient accumulation over 4 batches was used. Validation loss was checked every 100 steps and training was stopped by early stopping callback with patience 50 if validation loss has not reduced. Value-based gradient clipping was used with cutoff 10.0."
        },
        {
            "heading": "A.13 PRIMAL, DUAL AND COMBINED GRAPHS",
            "text": "The choice of graph over which message passing is run is important. For instance, some studies in the mechanics community use the dual graph where the centres of lattice cells are converted to nodes, and the neighbouring cells are connected by graph edges.(Karapiperis & Kochmann, 2023)\nMeyer et al. (2022) combine the primal graph, with a line graph. The original (primal) graph can be converted to a line graph as follows. The nodes of line graph are the edges of primal graph. Two nodes in the line graph are connected if the two corresponding edges of the primal graph meet at a node.\nHere we investigate the performance of GNN based on the choice of graph over which message passing is done. Table 8 shows the results for various models and training strategies. Primal and combined correspond to models CGCNN and mCGCNN from Meyer et al. (2022). Dual is message passing done purely on the line graph. Static augmentation corresponds to the training routine from Meyer et al. (2022) as described in Section A.11.1. Dynamic augmentation corresponds to our training routine whereby each time a lattice is retrieved from the dataset, it is obtained at a different orientation.\nIn summary, we empirically do not see any benefit of incorporating the line graph into our model. Therefore, we do not consider these models in the main text.\nA.14 TRAINING WITH Lmax < 4 AND DEGENERACY OF HIGHLY-SYMMETRIC LATTICES\nIn Figure 7 we show the unit cell of the simple cubic lattice. The lattice has a high degree of symmetry which has profound consequences for message passing. If the maximum degree of spherical expansion, Lmax, inside the model is lower than 4, the model is restricted to fitting an isotropic stiffness tensor for this lattice. When the Lmax \u2265 4, the anisotropy can be captured. Note that even when the model is trained with Lmax < 4, it still needs to output a fourth-order tensor whose spherical form includes L = 4 component: 2 \u00d7 0e + 2 \u00d7 2e + 1 \u00d7 4e. 6 We enable this by incorporating a tensor product expansion layer. Suppose the message passing is done up to Lmax = 2. After message passing and graph pooling, each graph has features of the form N \u00d7 0e + N \u00d7 1o + N \u00d7 2e, where N is the number of channels (N chosen even). We split the channels into two sets of size m = N/2 and do a tensor product between them:\n[m\u00d70e+m\u00d71o+m\u00d72e]\u2297 [m\u00d70e+m\u00d71o+m\u00d72e]\u2192 [(3m2)\u00d70e+(4m2)\u00d72e+(m2)\u00d74e] The output is passed through a linear layer with learnable weights which reduces it to the correct dimensionality 2\u00d7 0e+ 2\u00d7 2e+ 1\u00d7 4e.\n6The notation used here is specific to the software implementation of e3nn (Geiger et al., 2022)."
        }
    ],
    "title": "TICITY OF LATTICE ARCHITECTED METAMATERIALS",
    "year": 2023
}