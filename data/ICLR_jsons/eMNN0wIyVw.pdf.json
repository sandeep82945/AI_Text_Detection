{
    "abstractText": "In the realm of reinforcement learning (RL), off-policy evaluation (OPE) holds a pivotal position, especially in high-stake human-centric scenarios such as elearning and healthcare. Applying OPE to these domains is often challenging with scarce and underrepresentative offline training trajectories. Data augmentation has been a successful technique to enrich training data. However, directly employing existing data augmentation methods to OPE may not be feasible, due to the Markovian nature within the offline trajectories and the desire for generalizability across diverse target policies. In this work, we propose an offline trajectory augmentation approach, named OAT, to specifically facilitate OPE in human-involved scenarios. We propose sub-trajectory mining to extract potentially valuable sub-trajectories from offline data, and diversify the behaviors within those sub-trajectories by varying coverage of the state-action space. Our work was empirically evaluated in a wide array of environments, encompassing both simulated scenarios and realworld domains like robotic control, healthcare, and e-learning, where the training trajectories include varying levels of coverage of the state-action space. By enhancing the performance of a variety of OPE methods, our work offers a promising path forward for tackling OPE challenges in situations where human-centric data may be limited or underrepresentative.",
    "authors": [],
    "id": "SP:e048f36c1a20282f7018e0e058688ea94624fa56",
    "references": [
        {
            "authors": [
                "Antreas Antoniou",
                "Amos Storkey",
                "Harrison Edwards"
            ],
            "title": "Data augmentation generative adversarial networks",
            "venue": "arXiv preprint arXiv:1711.04340,",
            "year": 2017
        },
        {
            "authors": [
                "Hamoon Azizsoltani",
                "Yeo Jin"
            ],
            "title": "Unobserved is not equal to non-existent: Using gaussian processes to infer immediate rewards across contexts",
            "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Sasan Barak",
                "Ehsan Mirafzali",
                "Mohammad Joshaghani"
            ],
            "title": "Improving deep learning forecast using variational autoencoders",
            "venue": "Available at SSRN,",
            "year": 2022
        },
        {
            "authors": [
                "Roger C Bone",
                "Robert A Balk",
                "Frank B Cerra",
                "R Phillip Dellinger",
                "Alan M Fein",
                "William A Knaus",
                "Roland MH Schein",
                "William J Sibbald"
            ],
            "title": "Definitions for sepsis and organ failure and guidelines for the use of innovative therapies in sepsis",
            "venue": "Chest, 101(6):1644\u20131655,",
            "year": 1992
        },
        {
            "authors": [
                "Pedro G Campos",
                "Fernando D\u0131\u0301ez",
                "Iv\u00e1n Cantador"
            ],
            "title": "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols",
            "venue": "User Modeling and User-Adapted Interaction,",
            "year": 2014
        },
        {
            "authors": [
                "Jonathan Chang",
                "Masatoshi Uehara",
                "Dhruv Sreenivas",
                "Rahul Kidambi",
                "Wen Sun"
            ],
            "title": "Mitigating covariate shift in imitation learning via offline data with partial coverage",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Min Chi",
                "Kurt VanLehn",
                "Diane Litman",
                "Pamela Jordan"
            ],
            "title": "Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies",
            "venue": "User Modeling and User-Adapted Interaction,",
            "year": 2011
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Dandelion Mane",
                "Vijay Vasudevan",
                "Quoc V Le"
            ],
            "title": "Autoaugment: Learning augmentation strategies from data",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Matthew J Delano",
                "Peter A Ward"
            ],
            "title": "The immune system\u2019s role in sepsis progression, resolution, and long-term outcome",
            "venue": "Immunological reviews,",
            "year": 2016
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Chris Donahue",
                "Julian McAuley",
                "Miller Puckette"
            ],
            "title": "Adversarial audio synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Shayan Doroudi",
                "Philip S Thomas",
                "Emma Brunskill"
            ],
            "title": "Importance sampling for fair policy selection",
            "venue": "Grantee Submission,",
            "year": 2017
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4rl: Datasets for deep data-driven reinforcement learning",
            "venue": "arXiv preprint arXiv:2004.07219,",
            "year": 2020
        },
        {
            "authors": [
                "Justin Fu",
                "Mohammad Norouzi",
                "Ofir Nachum",
                "George Tucker",
                "Ziyu Wang",
                "Alexander Novikov",
                "Mengjiao Yang",
                "Michael R Zhang",
                "Yutian Chen",
                "Aviral Kumar"
            ],
            "title": "Benchmarks for deep offpolicy evaluation",
            "venue": "arXiv preprint arXiv:2103.16596,",
            "year": 2021
        },
        {
            "authors": [
                "Ge Gao",
                "Qitong Gao",
                "Xi Yang",
                "Miroslav Pajic",
                "Min Chi"
            ],
            "title": "A reinforcement learning-informed pattern mining framework for multivariate time series classification",
            "venue": "In Proceedings of the ThirtyFirst International Joint Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Ge Gao",
                "Song Ju",
                "Markel Sanz Ausin",
                "Min Chi"
            ],
            "title": "Hope: Human-centric off-policy evaluation for e-learning and healthcare",
            "venue": "In AAMAS,",
            "year": 2023
        },
        {
            "authors": [
                "Anirudh Goyal",
                "Abram Friesen",
                "Andrea Banino",
                "Theophane Weber",
                "Nan Rosemary Ke",
                "Adria Puigdomenech Badia",
                "Arthur Guez",
                "Mehdi Mirza",
                "Peter C Humphreys",
                "Ksenia Konyushova"
            ],
            "title": "Retrieval-augmented reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Joy Paul Guilford"
            ],
            "title": "Fundamental statistics in psychology and education",
            "year": 1950
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Jimmy Ba",
                "Mohammad Norouzi"
            ],
            "title": "Dream to control: Learning behaviors by latent imagination",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "David Hallac",
                "Sagar Vare",
                "Stephen Boyd",
                "Jure Leskovec"
            ],
            "title": "Toeplitz inverse covariance-based clustering of multivariate time series data",
            "venue": "In ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining,",
            "year": 2017
        },
        {
            "authors": [
                "Josiah Hanna",
                "Scott Niekum",
                "Peter Stone"
            ],
            "title": "Importance sampling policy evaluation with an estimated behavior policy",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Botao Hao",
                "Xiang Ji",
                "Yaqi Duan",
                "Hao Lu",
                "Csaba Szepesv\u00e1ri",
                "Mengdi Wang"
            ],
            "title": "Bootstrapping statistical inference for off-policy evaluation",
            "venue": "arXiv preprint arXiv:2102.03607,",
            "year": 2021
        },
        {
            "authors": [
                "Brian Kenji Iwana",
                "Seiichi Uchida"
            ],
            "title": "An empirical survey of data augmentation for time series classification with neural networks",
            "venue": "Plos one,",
            "year": 2021
        },
        {
            "authors": [
                "Brian Kenji Iwana",
                "Seiichi Uchida"
            ],
            "title": "Time series data augmentation for neural networks by time warping with a discriminative teacher",
            "venue": "In 2020 25th International Conference on Pattern Recognition (ICPR),",
            "year": 2021
        },
        {
            "authors": [
                "Nan Jiang",
                "Lihong Li"
            ],
            "title": "Doubly robust off-policy value evaluation for reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ho-Taek Joo",
                "In-Chang Baek",
                "Kyung-Joong Kim"
            ],
            "title": "A swapping target q-value technique for data augmentation in offline reinforcement learning",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Song Ju",
                "Shitian Shen",
                "Hamoon Azizsoltani",
                "Tiffany Barnes",
                "Min Chi"
            ],
            "title": "Importance sampling to identify empirically valid policies and their critical decisions",
            "venue": "In EDM (Workshops),",
            "year": 2019
        },
        {
            "authors": [
                "Song Ju",
                "Yeo Jin Kim",
                "Markel Sanz Ausin",
                "Maria E Mayorga",
                "Min Chi"
            ],
            "title": "To reduce healthcare workload: Identify critical sepsis progression moments through deep reinforcement learning",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2021
        },
        {
            "authors": [
                "Krzysztof Kamycki",
                "Tomasz Kapuscinski",
                "Mariusz Oszust"
            ],
            "title": "Data augmentation with suboptimal warping for time-series classification",
            "year": 2019
        },
        {
            "authors": [
                "Yeo-Jin Kim",
                "Min Chi"
            ],
            "title": "Temporal belief memory: Imputing missing data during rnn training",
            "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Matthieu Komorowski",
                "Leo A Celi",
                "Omar Badawi",
                "Anthony C Gordon",
                "A Aldo Faisal"
            ],
            "title": "The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care",
            "venue": "Nature medicine,",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Denis Yarats",
                "Rob Fergus"
            ],
            "title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
            "venue": "arXiv preprint arXiv:2004.13649,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Laskin",
                "Aravind Srinivas",
                "Pieter Abbeel"
            ],
            "title": "Curl: Contrastive unsupervised representations for reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Misha Laskin",
                "Kimin Lee",
                "Adam Stooke",
                "Lerrel Pinto",
                "Pieter Abbeel",
                "Aravind Srinivas"
            ],
            "title": "Reinforcement learning with augmented data",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hoang Le",
                "Cameron Voloshin",
                "Yisong Yue"
            ],
            "title": "Batch policy learning under constraints",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Arthur Le Guennec",
                "Simon Malinowski",
                "Romain Tavenard"
            ],
            "title": "Data augmentation for time series classification using convolutional neural networks",
            "venue": "In ECML/PKDD workshop on advanced analytics and learning on temporal data,",
            "year": 2016
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Alex X Lee",
                "Anusha Nagabandi",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Jonathan J Hunt",
                "Alexander Pritzel",
                "Nicolas Heess",
                "Tom Erez",
                "Yuval Tassa",
                "David Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971,",
            "year": 2015
        },
        {
            "authors": [
                "Zachary C Lipton",
                "David Kale",
                "Randall Wetzel"
            ],
            "title": "Directly modeling missing data in sequences with rnns: Improved classification of clinical time series",
            "venue": "In Machine learning for healthcare conference,",
            "year": 2016
        },
        {
            "authors": [
                "Bo Liu",
                "Zhenguo Zhang",
                "Rongyi Cui"
            ],
            "title": "Efficient time series augmentation methods",
            "year": 2020
        },
        {
            "authors": [
                "Jinxin Liu",
                "Zhang Hongyin",
                "Donglin Wang"
            ],
            "title": "Dara: Dynamics-aware reward augmentation in offline reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Liu",
                "Lihong Li",
                "Ziyang Tang",
                "Dengyong Zhou"
            ],
            "title": "Breaking the curse of horizon: Infinitehorizon off-policy estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Vincent Liu",
                "Gabriel J Escobar",
                "John D Greene",
                "Jay Soule",
                "Alan Whippy",
                "Derek C Angus",
                "Theodore J Iwashyna"
            ],
            "title": "Hospital deaths in patients with sepsis from 2 independent cohorts",
            "venue": "Jama, 312(1):90\u201392,",
            "year": 2014
        },
        {
            "authors": [
                "Travis Mandel",
                "Yun-En Liu",
                "Sergey Levine",
                "Emma Brunskill",
                "Zoran Popovic"
            ],
            "title": "Offline policy evaluation across representations with applications to educational games",
            "venue": "In AAMAS,",
            "year": 2014
        },
        {
            "authors": [
                "G.S Martin",
                "D.M Mannino"
            ],
            "title": "The epidemiology of sepsis in the united states from 1979",
            "venue": "New England Journal of Medicine,",
            "year": 2000
        },
        {
            "authors": [
                "Iris Miliaraki",
                "Klaus Berberich",
                "Rainer Gemulla",
                "Spyros Zoupanos"
            ],
            "title": "Mind the gap: Large-scale frequent sequence mining",
            "venue": "In Proceedings of the 2013 ACM SIGMOD international conference on management of data,",
            "year": 2013
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "R\u00e9mi Munos",
                "Tom Stepleton",
                "Anna Harutyunyan",
                "Marc Bellemare"
            ],
            "title": "Safe and efficient off-policy reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ofir Nachum",
                "Yinlam Chow",
                "Bo Dai",
                "Lihong Li"
            ],
            "title": "Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Hongseok Namkoong",
                "Ramtin Keramati",
                "Steve Yadlowsky",
                "Emma Brunskill"
            ],
            "title": "Off-policy policy evaluation for sequential decisions under unobserved confounding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Allen Nie",
                "Yannis Flet-Berliac",
                "Deon Richmond Jordan",
                "William Steenbergen",
                "Emma Brunskill"
            ],
            "title": "Data-efficient pipeline for offline reinforcement learning with limited data",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Cosmin Paduraru"
            ],
            "title": "Off-policy evaluation in markov decision processes",
            "year": 2013
        },
        {
            "authors": [
                "Jongjin Park",
                "Younggyo Seo",
                "Jinwoo Shin",
                "Honglak Lee",
                "Pieter Abbeel",
                "Kimin Lee"
            ],
            "title": "Surf: Semi-supervised reward learning with data augmentation for feedback-efficient preference-based reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Doina Precup"
            ],
            "title": "Eligibility traces for off-policy policy evaluation",
            "venue": "Computer Science Department Faculty Publication Series, pp",
            "year": 2000
        },
        {
            "authors": [
                "Aniruddh Raghu",
                "Matthieu Komorowski",
                "Imran Ahmed",
                "Leo Celi",
                "Peter Szolovits",
                "Marzyeh Ghassemi"
            ],
            "title": "Deep reinforcement learning for sepsis treatment",
            "venue": "arXiv preprint arXiv:1711.09602,",
            "year": 2017
        },
        {
            "authors": [
                "Roberta Raileanu",
                "Maxwell Goldstein",
                "Denis Yarats",
                "Ilya Kostrikov",
                "Rob Fergus"
            ],
            "title": "Automatic data augmentation for generalization in reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Abhishek Gupta",
                "Giulia Vezzani",
                "John Schulman",
                "Emanuel Todorov",
                "Sergey Levine"
            ],
            "title": "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations",
            "venue": "Robotics: Science and Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Judy Robertson",
                "Maurits Kaptein"
            ],
            "title": "Modern statistical methods for HCI",
            "year": 2016
        },
        {
            "authors": [
                "Havard Rue",
                "Leonhard Held"
            ],
            "title": "Gaussian Markov random fields: theory and applications",
            "venue": "CRC press,",
            "year": 2005
        },
        {
            "authors": [
                "Oleh Rybkin",
                "Chuning Zhu",
                "Anusha Nagabandi",
                "Kostas Daniilidis",
                "Igor Mordatch",
                "Sergey Levine"
            ],
            "title": "Model-based reinforcement learning via latent-space collocation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kajetan Schweighofer",
                "Markus Hofmarcher",
                "Marius-Constantin Dinu",
                "Philipp Renz",
                "Angela BittoNemling",
                "Vihang Prakash Patil",
                "Sepp Hochreiter"
            ],
            "title": "Understanding the effects of dataset characteristics on offline reinforcement learning",
            "venue": "In Deep RL Workshop NeurIPS 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Mervyn Singer",
                "Clifford S Deutschman",
                "Christopher Warren Seymour",
                "Manu Shankar-Hari",
                "Djillali Annane",
                "Michael Bauer",
                "Rinaldo Bellomo",
                "Gordon R Bernard",
                "Jean-Daniel Chiche",
                "Craig M Coopersmith"
            ],
            "title": "The third international consensus definitions for sepsis and septic shock",
            "venue": "(sepsis-3). Jama,",
            "year": 2016
        },
        {
            "authors": [
                "Charles Spearman"
            ],
            "title": "The proof and measurement of association between two things",
            "venue": "The American journal of psychology,",
            "year": 1987
        },
        {
            "authors": [
                "Philip Thomas",
                "Emma Brunskill"
            ],
            "title": "Data-efficient off-policy policy evaluation for reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Masatoshi Uehara",
                "Jiawei Huang",
                "Nan Jiang"
            ],
            "title": "Minimax weight and q-function learning for offpolicy evaluation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Cameron Voloshin",
                "Nan Jiang",
                "Yisong Yue"
            ],
            "title": "Minimax model learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Cameron Voloshin",
                "Hoang Minh Le",
                "Nan Jiang",
                "Yisong Yue"
            ],
            "title": "Empirical study of off-policy policy evaluation for reinforcement learning",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Lu Wang",
                "Ruiming Tang",
                "Xiaofeng He",
                "Xiuqiang He"
            ],
            "title": "Hierarchical imitation learning via subgoal representation learning for dynamic treatment recommendation",
            "venue": "In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Ruosong Wang",
                "Yifan Wu",
                "Ruslan Salakhutdinov",
                "Sham Kakade"
            ],
            "title": "Instabilities of offline rl with pre-trained neural representation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jingda Wu",
                "Zhiyu Huang",
                "Wenhui Huang",
                "Chen Lv"
            ],
            "title": "Prioritized experience-based reinforcement learning with human guidance for autonomous driving",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le"
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tengyang Xie",
                "Yifei Ma",
                "Yu-Xiang Wang"
            ],
            "title": "Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yaniv Yacoby",
                "Weiwei Pan",
                "Finale Doshi-Velez"
            ],
            "title": "Failure modes of variational autoencoders and their effects on downstream tasks",
            "venue": "arXiv preprint arXiv:2007.07124,",
            "year": 2020
        },
        {
            "authors": [
                "Mengjiao Yang",
                "Ofir Nachum",
                "Bo Dai",
                "Lihong Li",
                "Dale Schuurmans"
            ],
            "title": "Off-policy evaluation via the regularized lagrangian",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mengjiao Yang",
                "Bo Dai",
                "Ofir Nachum",
                "George Tucker",
                "Dale Schuurmans"
            ],
            "title": "Offline policy selection under uncertainty",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Xi Yang",
                "Guojing Zhou",
                "Michelle Taub",
                "Roger Azevedo",
                "Min Chi"
            ],
            "title": "Student subtyping via eminverse reinforcement learning. International Educational Data Mining Society, 2020b",
            "year": 2020
        },
        {
            "authors": [
                "Xi Yang",
                "Yuan Zhang",
                "Min Chi"
            ],
            "title": "Multi-series time-aware sequence partitioning for disease progression modeling",
            "venue": "In IJCAI,",
            "year": 2021
        },
        {
            "authors": [
                "Jinsung Yoon",
                "Daniel Jarrett",
                "Mihaela Van der Schaar"
            ],
            "title": "Time-series generative adversarial networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Michael R Zhang",
                "Thomas Paine",
                "Ofir Nachum",
                "Cosmin Paduraru",
                "George Tucker",
                "Mohammad Norouzi"
            ],
            "title": "Autoregressive dynamics models for offline policy evaluation and optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Michael R Zhang",
                "Tom Le Paine",
                "Ofir Nachum",
                "Cosmin Paduraru",
                "George Tucker",
                "Ziyu Wang",
                "Mohammad Norouzi"
            ],
            "title": "Autoregressive dynamics models for offline policy evaluation and optimization",
            "venue": "arXiv preprint arXiv:2104.13877,",
            "year": 2021
        },
        {
            "authors": [
                "Shangtong Zhang",
                "Bo Liu",
                "Shimon Whiteson"
            ],
            "title": "Gradientdice: Rethinking generalized offline estimation of stationary values",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chenyu Zheng",
                "Guoqiang Wu",
                "Chongxuan Li"
            ],
            "title": "Toward understanding generative data augmentation",
            "venue": "Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Guojing Zhou",
                "Hamoon Azizsoltani",
                "Markel Sanz Ausin",
                "Tiffany Barnes",
                "Min Chi"
            ],
            "title": "Hierarchical reinforcement learning for pedagogical policy induction",
            "venue": "In International conference on artificial intelligence in education,",
            "year": 2019
        },
        {
            "authors": [
                "Guojing Zhou",
                "Hamoon Azizsoltani",
                "Markel Sanz Ausin",
                "Tiffany Barnes",
                "Min Chi"
            ],
            "title": "Leveraging granularity: Hierarchical reinforcement learning for pedagogical policy induction",
            "venue": "International journal of artificial intelligence in education,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Kaixiang Lin",
                "Anil K Jain",
                "Jiayu Zhou"
            ],
            "title": "Transfer learning in deep reinforcement learning: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "\u2208 Mc"
            ],
            "title": "penalizes the adjacent events that are not assigned to the same cluster and \u03b5 is a constant balancing off the scale of the two terms. This optimization problem can be solved using the expectation-maximization family of algorithms by updating \u03a3\u22121 and M alternatively",
            "venue": "DETAILED FORMULATION OF THE VAE-MDP",
            "year": 2017
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "2021) that train dynamics and reward models on transitions from the offline data; value-based estimators (Munos et al., 2016; Le et al., 2019) such as Fitted Q Evaluation (FQE) which is a policy evaluation counterpart to batch Q learning; minimax estimators (Liu et al., 2018; Zhang et al., 2020b; Voloshin et al., 2021a) such as DualDICE that estimates the discounted stationary distribution ratios (Yang et al., 2020a). (iii) Hybrid methods",
            "venue": "(Paduraru,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Off-policy evaluation (OPE) has been recognized as an important part of reinforcement learning (RL), especially for human-involved RLs (Zhu et al., 2023; Wu et al., 2022), in which evaluations of online policies can have high stakes (Levine et al., 2020). The objective of OPE is to evaluate target policies based on offline trajectories collected from behavioral policies different from the target ones. One major barrier often lies in the fact that the offline trajectories in human-involved tasks often only provide limited coverage of the entire state-action space (Chang et al., 2021; Schweighofer et al., 2021). This can be caused by homogeneous behavioral policies; for example, during clinical procedures, physicians need to follow certain standardized guidelines. However, a sub-optimal autonomous control agent (e.g., surgical robots under training) may deviate from such guidelines, and thus result in trajectories where the state-action space may not be fully covered by the offline trajectories collected, which introduces great challenges for OPE, as illustrated in Figure 1. Therefore, to improve the OPE performance, it is essential to enrich the offline trajectories.\nData augmentation is a powerful tool for data enrichment by artificially generating new data points from existing data. It has shown effectiveness in facilitating learning more robust supervised and unsupervised models (Iwana & Uchida, 2021a; Xie et al., 2020). Specifically, generative methods such as variational autoencoder (VAE) have achieved superior performance in time-series augmentation (Yoon et al., 2019; Barak et al., 2022). However, an important characteristic of OPE training data is the Markovian nature, as the environments are usually formulated as a Markov decision process (MDP) (Thomas & Brunskill, 2016; Fu et al., 2021). As a result, prior works on time-series augmentation may not be directly applicable to MDP trajectory augmentation. Recently, though data augmentation methods have been extended to facilitate RL policy optimization, most existing works focus on enriching the state space, such as adding noise to input images to generate sufficient data and improve the generality of agents (Laskin et al., 2020a; Raileanu et al., 2021), but overlook the coverage of the joint state-action distribution over time. More importantly, the goal of data augmentation towards OPE is different from RL policy optimization. Data augmentation in RL gen-\nerally aims to quickly facilitate identifying and learning from high-reward regions of the state-action space (Liu et al., 2021; Park et al., 2022). In contrast, the evaluation policies considered by OPE can be heterogeneous and lead to varied performance, i.e., the policies to be evaluated by OPE do not necessarily perform well; therefore, it is equally important to allow the agent learning from trajectories resulted from high- and low-reward regions. As a result, OPE methods prefer training data that provides comprehensive coverage of the state-action space, including the trajectories resulting from low-performing and sub-optimal policies. To the best of our knowledge, there does not exist a method that augments historical trajectories specific to OPE.\nIn this paper, we propose a framework to facilitate OPE using Augmented Trajectories (OAT). Specifically, motivated by the intrinsic nature that human-involved systems (HIS) are often provided limited coverage of the stateaction space, while human may behave diversely when following different policies (Yang et al., 2020b; Wang et al., 2022), we propose potential sub-trajectories (PSTs) mining to identify sub-trajectories of historical trajectories whose state-action space is less covered but have great potential to enrich the space. Then a generative modeling framework is used to capture the dynamic underlying the PSTs and induce augmented sub-trajectories. Based on that, we design the fuse process by simultaneously taking the augmented sub-trajectories while maintaining the part of the states and actions associated with\nnon-PSTs. The key contributions of this work are summarized as follows: (i) To the best of our knowledge, OAT is the first method augmenting historical trajectories to facilitate OPE in HIS. (ii) We conduct extensive experiments to validate OAT in a variety of simulation and real-world environments, including robotics, healthcare, and e-learning. (iii) The experimental results present that OAT can significantly facilitate OPE performance and outperform all data augmentation baselines."
        },
        {
            "heading": "2 OPE WITH AUGMENTED TRAJECTORIES (OAT)",
            "text": "We propose a framework to facilitate OPE with augmented trajectories (OAT). Specifically, we first introduce offline trajectories and OPE. Then we propose a sub-trajectory mining method that identifies the sub-trajectories of trajectories that have great potential to increase the offline trajectories\u2019 coverage over the state-action space, i.e., potential subtrajectories (PSTs). A generative modeling framework is used to capture the dynamics underlying the selected PSTs, followed by a fuse process that generates augmented trajectories with augmented subtrajectories which will be used to train the OPE methods.\nOffline Trajectories. We consider framing an agent\u2019s interaction with the environment over a sequence of decision-making steps as a Markov decision process (MDP), which is formulated as a 6- tuple (S,A,P,S0, r, \u03b3). S is the state space. A is the action space. P defines transition dynamics from the current state and action to the next state. S0 defines the initial state distribution. r is the reward function. \u03b3 \u2208 (0, 1] is discount factor. Episodes are\nof finite horizon T . At each time-step t, the agent observes the state st \u2208 S of the environment, then chooses an action at \u2208 A following a policy \u03c0. The environment accordingly provides a reward rt = r(st, at), and the agent observes the next state st+1 determined by P . \u03c4 (i) is defined as a trajectory where \u03c4 (i) = [..., (st, at, rt, s\u2032t), ...] T t=1.\nOffline Policy Evaluation (OPE). The goal of OPE is to estimate the expected total return over the evaluation (target) policy \u03c0, V \u03c0 = E[ \u2211T t=1 \u03b3\nt\u22121rt|at \u223c \u03c0], using set of historical trajectories D collected over a behavioral policy \u03b2 \u0338= \u03c0. The historical trajectories D = {..., \u03c4 (i), ...}Ni=1 consist of a set of N trajectories."
        },
        {
            "heading": "2.1 MINING OF POTENTIAL SUB-TRAJECTORIES (PSTS)",
            "text": "The historical trajectories D collected from HIS are often provided with limited coverage of the state-action space, due to the intrinsic nature that human may follow homogeneous behavioral policies or specific guidelines when performing their professions (Yang et al., 2020b; Wang et al., 2022). For example, a surgeon could perform appendectomy in various ways across patients depending on each patient\u2019s specific condition; however, they may strictly follow similar steps at the beginning (e.g., disinfection) and the end of surgeries (e.g., stitching). Therefore, the resulting trajectories may lead to limited coverage for part of the state-action space representing similar scenarios. However, a sub-optimal autonomous agent, subject to be evaluated by OPE, may visit states unseen from the trajectories collected from the surgeon, e.g., towards the beginning/end of the surgery. As a result, we consider augmenting the part of trajectories, i.e., the PSTs, that are more likely to be insufficiently covered by the historical trajectories D. Moreover, the downstream generative models, such as VAEs, do not necessarily need to reconstruct entire trajectories for long horizons and over limited samples which are the common limitations of data collected from HIS (Yacoby et al., 2020).\nTo identify the PSTs that are subject to be augmented, we introduce a three-step approach, i.e., (i) discrete representation mapping, followed by (ii) determining support from discrete representations, where the support is used in step (iii) to identify PSTs to be augmented.\nStep (i) \u2013 Discrete Representation Mapping. Trajectories collected from HIS can be complex, due to the unobservable underlying human mindset and high-dimensional state space (Mandel et al., 2014). Discrete representation mapping has been recognized as effectively providing abstractions from complex original data and helping capture homogeneous behaviors shared across trajectories (Yang et al., 2021). We assume that states st \u2208 S can be mapped into C clusters, where each st is associated with a cluster from the set K = {K1, . . . ,KC}. After mapping, each state si,t on trajectory \u03c4 (i) is mapped to Ki,t \u2208 K. Step (ii) \u2013 Determine Support from Discrete Representations. We assume that each trajectory \u03c4 (i) can be mapped to a corresponding temporal discrete sequence K(i) = [Ki,1, . . . ,Ki,T ] \u2282 ZT , based on the state mapping, where T is the horizon of the environment and Z is the set of integers. We also define H = {...,K(i), ...}Ni=1 which is the set of all temporal discrete sequences mapped from the set of original trajectories D. We define \u03b4(i)\u03b6,\u03b6+W\u22121 = [Ki,\u03b6 , ...,Ki,\u03b6+W\u22121] as a temporal discrete sub-sequence (TDSS) with length W \u2208 [1, T ] of K(i), where \u03b6 \u2208 [1, T \u2212 W + 1], denoted as \u03b4(i)\u03b6,\u03b6+W\u22121 \u2291 K(i). Note that C is generally greatly smaller than T \u00d7 N as considered in discrete representation mapping in general (Hallac et al., 2017; Yang et al., 2021). Therefore, it is possible that a temporal discrete sub-sequence \u03b4(i)\u03b6i,\u03b6i+W\u22121 is \u201cequal\u201d to another temporal discrete sub-sequence \u03b4(j)\u03b6j ,\u03b6j+W\u22121, such that \u03b4 (i) \u03b6i,\u03b6i+W\u22121 = \u03b4 (j) \u03b6j ,\u03b6j+W\u22121 if every Ki,\u03b6i = Kj,\u03b6j given Ki,\u03b6i ,Kj,\u03b6j \u2208 Z. Though \u03b6i does not necessarily equals to \u03b6j , we omit the superscript for concise expression. Then, the support (or frequency) of any TDSS \u03b4(i)\u03b6,\u03b6+W\u22121 appears in H is the number of K(i) in H containing the TDSS, i.e.,\nsupportH(\u03b4 (i) \u03b6,\u03b6+W\u22121) = N\u2211 j=1 [ 1(\u03b4 (j) \u03b6,\u03b6+W\u22121 \u2291 K (j))\u00d7 1(\u03b4(j)\u03b6,\u03b6+W\u22121 = \u03b4 (i) \u03b6,\u03b6+W\u22121) ] , (1)\nwhere 1(\u00b7) is the indicator function.\nStep (iii) \u2013 Identify PSTs. We denote \u03c6(i)\u03b6,\u03b6+W\u22121 = [..., (s\u0302 (i) t , a\u0302 (i) t , r\u0302 (i) t , s\u0302 \u2032(i) t ), ...] \u03b6+W\u22121 t=\u03b6 as a subtrajectory with length W of \u03c4 (i). Given the mapping from trajectory \u03c4 (i) to temporal discrete sequence K(i) (introduced in the step above), we define that each sub-trajectory \u03c6(i)\u03b6,\u03b6+W\u22121 can be mapped to a corresponding TDSS \u03b4(i)\u03b6,\u03b6+W\u22121. Now we can identify the PSTs that will be used to train the generative model for reconstructing new sub-trajectories (i.e., augmentation) in Section 2.2, following the definition below.\nDefinition 2.1 (Potential Sub-Trajectory (PST)). Given historical trajectories D and a threshold \u03be, a sub-trajectory \u03c6(i)\u03b6,\u03b6+W\u22121 is considered as a potential sub-trajectory if the support of its mapped temporal discrete sub-sequence \u03b4(i)\u03b6,\u03b6+W\u22121 satisfies supportH(\u03b4 (i) \u03b6,\u03b6+W\u22121) \u2265 \u03be.\nAn intuitive way to determine the threshold \u03be is ranking the TDSSs by their supports in descending order and picking the top k ones. In this study, we iteratively select the top k distinct TDSSs until the support of the set of G selected TDSSs {\u03b4g\u03b6,\u03b6+W\u22121}1, g \u2208 [1, G], i.e., the number of K(i) in H containing any TDSS \u03b4 \u2208 {\u03b4g\u03b6,\u03b6+W\u22121}, is greater than or equal to .99N , or we early stop at k = 5.\nFollowing the step above, a set of PSTs is determined for historical trajectories D, from which we can obtain a set of G distinct corresponding TDSSs {\u03b4g\u03b6,\u03b6+W\u22121} mapped from the PSTs. Then we can obtain G sets of PSTs, such that each set T g = {\u03c6(i)\u03b6,\u03b6+W\u22121}, where all \u03c6 (i) \u03b6,\u03b6+W\u22121 \u2208 T g satisfy that their corresponding \u03b4(i)\u03b6,\u03b6+W\u22121 = \u03b4 g \u03b6,\u03b6+W\u22121. Each set of PSTs may contain unique information captured from the original historical trajectories D, as previous works have found that the PSTs in the same set, T g , are in general associated with similar temporal and cross-attributes correlations (Gao et al., 2022).\n2.2 AUGMENTING THE PSTS\nPotential Sub-Trajectory (PST)\nIn this section, we introduce how to adapt VAE to capture the MDP transitions, i.e., VAE-MDP, underlying each set of PSTs, T g , as well as reconstruct new PST samples that will be fused back with the original historical trajectories D for OPE methods to estimate the returns of evaluation (target) policies. The adaptation mainly consists of three parts: the latent prior, variational encoder, and generative decoder. Given a set of PSTs, T g = {\u03b4\u03b6,\u03b6+W\u22121}2, the formulation of VAEMDP consists of three major components, i.e., (i) the latent prior p(z\u03b6) \u223c N (0, I) representing the distribution of the initial latent states (at the beginning of each PST in the set T g), where I is the identity covariance matrix. (ii) the encoder q\u03c9(zt|st\u22121, at\u22121, st) that encodes the MDP transitions into the latent space, and (iii) the decoders p\u03b7(zt|zt\u22121, at\u22121), p\u03b7(st|zt), p\u03b7(rt\u22121|zt) that reconstructs new PST samples. The detailed setup can be found in Appendix A.4, and the overall encoding and decoding processes are illustrated in Figure 3.\nThe training objective for VAE-MDP is to maximize the evidence lower bound (ELBO), which consists of the log-likelihood of reconstructing the states and rewards, and regularization of the approximated posterior, i.e.,\nL = \u2212ELBO(\u03c9, \u03b7) = \u2212Eq\u03c9 [\u2211\u03b6+W\u22121\nt=\u03b6 log p\u03b7(st|zt) + \u2211\u03b6+W\u22121 t=\u03b6+1 log p\u03b7(rt\u22121|zt)\u2212KL ( q\u03c9(z\u03b6 |s\u03b6)||p(z\u03b6) ) \u2212 \u2211\u03b6+W\u22121 t=\u03b6+1 KL ( q\u03c9(zt|zt\u22121, at\u22121, st)||p\u03b7(zt|zt\u22121, at\u22121) )] .\n(2)\nThe proof of Equation 2 are provided in Appendix A.5. Consequently, given a set of PSTs, T g , a VAE-MDP to the set can be trained to reconstruct a set of new PST samples, denoted as T\u0302 g = {\u03c6\u0302v\u03b6,\u03b6+W\u22121}, v \u2208 [1, V ], where \u03c6\u0302v\u03b6,\u03b6+W\u22121 = [..., (s\u0302vt , a\u0302vt , r\u0302vt , s\u0302\u2032vt ), ...] \u03b6+W\u22121 t=\u03b6 is a augmented PST and V is the total number of augmented PST samples, generated from VAE-MDP, for the set T g .\n1From now we use superscript g to replace (i) for \u03b4\u2019s, since there may exist multiple TDSSs that are equivalent.\n2From now on we omit the superscripts of \u03b4 for conciseness."
        },
        {
            "heading": "2.3 FUSE AUGMENTED PSTS BACK TO THEIR ORIGINS",
            "text": "With new augmented sub-trajectories rolled out by the VAE-MDP, we fuse them back to the original historical trajectories D for the OPE methods to leverage. This fusing process is designed to (i) provide enhanced coverage over the state-action space where the corresponding PSTs do not explicitly capture homogeneous behaviors, and still (ii) maintain the part of the covered state-action distribution associated with non-PSTs, since those may indicate object-specific information that is not shared across all trajectories, e.g., the part of the surgical procedure specific to each patient, following from the surgery analogy above. Below we introduce how to fuse T\u0302 g with the original trajectories from D. A graphical illustration of this step can be found in Figure 2.\nGiven a trajectory \u03c4 (i) \u2208 D, the G sets of PSTs {T 1, ..., T G} mined from D following Section 2.1, and G sets of augmented sub-trajectories {T\u0302 1, ..., T\u0302 G} generated from G corresponding VAEMDPs following Section 2.2, an augmented trajectory \u03c4\u0302 (i) corresponding to \u03c4 (i) can be obtained\nby \u03c4\u0302 (i) = [ 1(t \u2208 [\u03b6, \u03b6 + W \u2212 1])(s\u0302vt , a\u0302vt , r\u0302vt , s\u0302\u2032vt ) \u2228 1(t /\u2208 [\u03b6, \u03b6 + W \u2212 1])(s (i) t , a (i) t , r (i) t , s \u2032(i) t ) ]T t=1 ; (s\u0302vt , a\u0302 v t , r\u0302 v t , s\u0302 \u2032v t ) \u2208 \u03c6\u0302v\u03b6,\u03b6+W\u22121, \u03c6\u0302v\u03b6,\u03b6+W\u22121 \u2208 T\u0302 g . In this study, the \u03c6\u0302v\u03b6,\u03b6+W\u22121 is selected as the one whose state and action are the closest to the original state and action at step \u03b6+W \u22121. More details are provided in Appendix A.6."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "In this section, we first introduce augmentation baselines and OPE methods used for experiments, and environments. Then, results are presented and discussed."
        },
        {
            "heading": "3.1 SETUP",
            "text": "Baselines. We investigate a variety of augmentation methods as baselines, including (i) RL-oriented methods: TDA (Park et al., 2022) which originally incorporates with rewards learning by randomly extracting sub-trajectories from trajectories, we replace PST mining by TDA in OAT so that TDA can be used for OPE with augmentation; permutation, Gaussian jittering, and scaling have been broadly employed in RL (Laskin et al., 2020b; Liu et al., 2020; Raileanu et al., 2021); (ii) generative methods: TimeGAN (Yoon et al., 2019) and VAE (Barak et al., 2022); (iii) time-series methods: SPAWNER (Kamycki et al., 2019) and DGW (Iwana & Uchida, 2021b) that consider similarities across time series. We implement RL-augmentation methods strictly following original algorithms, and use open-sourced code provided by the authors for the generative and time-series augmentation methods. Since generative and time-series methods are not proposed towards trajectories, we treat trajectories as multivariate time series as their input.\nAblations. One ablation of our approach is to apply VAE-MDP to reconstruct entire trajectories as augmentations, i.e., without PST mining (Section 2.1) and fusing back (Section 2.3). Moreover, TDA (Park et al., 2022) and VAE (Barak et al., 2022) can be considered as two ablations as well, since TDA isolates our PST mining from OAT and VAE augments entire trajectories following the vanilla VAE (Kingma & Welling, 2013), i.e., without being adapted to the Markovian setting.\nOPE methods considered. Outputs from all augmentation methods are fed into five OPE methods to compare the performance achieved with versus without augmentations. The OPE methods we consider include importance sampling (IS) (Precup, 2000), fitted Q-evaluation (FQE) (Le et al., 2019), distribution correction estimation (DICE) (Yang et al., 2020a), doubly robust (DR) (Thomas & Brunskill, 2016), and model-based (MB) (Zhang et al., 2020a). We use the open-sourced implementations provided by the Deep OPE (DOPE) benchmark (Fu et al., 2021).\nStandard validation metrics. To validate OPE\u2019s performance (for both with and without augmentations), we use standard OPE metrics as introduced in the DOPE benchmark, which include absolute error, Spearman\u2019s rank correlation coefficient (Spearman, 1987), regret@1, and regret@5. Definitions of the metrics are described in Appendix B.3."
        },
        {
            "heading": "3.2 ENVIRONMENTS",
            "text": "To evaluate our method, OAT, as well as the existing augmentation approaches for OPE, we use both simulated and real-world environments, spanning the domains of robotics, healthcare, and elearning. The environments are human-involved which is generally challenging with highly limited\nquantity of demonstrations containing underrepresented state space, due to homogeneous interventions when collecting the historical trajectories.\nAdroit. Adriot (Rajeswaran et al., 2018) is a simulation environment with four synthetic real-world robotics tasks, where a simulated Shadow Hand robot is asked to hammer a nail (hammer), open a door (door), twirl a pen (pen), or pick up and move a ball (relocate). Each task contains three training datasets with different levels of human-involvements, including full demonstration data from human (human), induced data from a fine-tuned RL policy (expert), and mixing data with a 50-50 ratio of demonstration and induced data (cloned). We follow the experimental settings provided in Deep OPE benchmark, with 11 DAPG-based evaluation policies ranging from random to expert performance (Fu et al., 2021).\nReal-World Sepsis Treatment. We investigate a challenging task in healthcare, sepsis treatments, which has raised broad attention in OPE (Namkoong et al., 2020; Nie et al., 2022). Specifically, the trajectories are taken from electronic health records containing 221,700 patient visits collected from a hospital over two years. The state space is constituted by 15 continuous sepsis-related clinical attributes that represent patients\u2019 health status, including heart rate, creatinine, etc. The cardinality of the action space is 4, i.e., two binary treatment options over {antibiotic administration, oxygen assistance}. Given the four stages of sepsis defined by the clinicians (Delano & Ward, 2016), the rewards are set for each stage: infection (\u00b15), inflammation (\u00b110), organ failure (\u00b120), and septic shock (\u00b150). Negative rewards are given when a patient enters a worse stage, and positive rewards are given when the patient recovers to a better stage. The environment considers discrete time steps, with the horizon being 1160 steps. Five evaluation (target) policies are obtained by training Deep Q Networks (DQNs) (Mnih et al., 2015) respectively over different hyper-parameters. More details are provided in Appendix D.\nReal-World Intelligent Tutor. Another important human-involved task for OPE is intelligent tutoring, where students interact with intelligent tutors, with the goal of improving students\u2019 engagements and learning outcomes. Such topics have been investigated in prior OPE works (Mandel et al., 2014; Nie et al., 2022). Specifically, we collect trajectories recorded from 1,307 students\u2019 interaction logs with an intelligent tutor, over seven semesters of an undergraduate course at an university. Since students\u2019 underlying learning states are unobservable (Mandel et al., 2014), we consult with domain experts who help defines the state space which is constituted by 142 attributes that could possibly capture students\u2019 learning status from their logs. The cardinality of the action space is 3, i.e., on each problem, the tutor need to decide whether the student should solve the next problem by themselves, study a solution provided by the tutor, or work together with the tutor to solve on the problem. During the tutoring, each student is required to solve 12 problems, thus the horizon of the environment is considered as 12 discrete steps. Sparse rewards are obtained at the end of the tutoring, which are defined as students\u2019 normalized learning gains (Chi et al., 2011). We use the trajectories collected from six semesters as the training data, where the behavior policy follows an expert policy commonly used in e-learning (Zhou et al., 2019), and test on the upcoming semester. There are 4 evaluation policies, including three obtained by training DQNs over different hyperparameters respectively, in addition to one expert policy. More details are provided in Appendix E.\n3.3 RESULTS\nThe need of PSTs mining. To better understand the need of PSTs mining (Section 2.1) conceptually, we visualize the set of augmented trajectories produced by our method, against the original set of historical trajectories D, over the Maze2D-umaze environment which is a toy navigation task requiring an agent to reach a fixed goal location (Fu et al., 2020). We uniformly down-sample a limited number (i.e., 250) of trajectories from the original dataset provided by D4RL (overall 12k trajectories), and use our method to augment this subset such that the total number of trajectories becomes ten times (\u00d710) larger. The visualization is shown in Figure 4. It can be observed\nthat there exist 3 sets of PSTs (as circled in the figure) that have significantly increased state space visitation after augmentation, benefiting from the PSTs mining methodology introduced in Section 2.1.\n3.3.1 RESULTS OVER ADROIT\nFigure 5 summarizes the averaged improvements across five OPE methods, over all four tasks (i.e., hammer, door, pen, relocate) in Adroit human, quantified by the percentage increases over the four validation metrics achieved by the OPE methods evaluated over the augmented against the original datasets. Overall, our method significantly improves OPE methods in terms of all standard validation metrics, and achieves the best performance compared to all augmentation baselines. This illustrates the effectiveness and robustness of our proposed methods across environments and tasks. There is no clear winner among baselines, where VAE, TimeGAN, and scaling in general perform better in terms of MAE, DGW and scaling performs better in terms of rank correlation, permutation and jittering perform better in terms of regrest@5. More specifically, besides the fact that all methods can in general improve MAE, most baselines lead to negative effects in terms of the other three metrics. Detailed results are presented in Appendix C.1.\nMore importantly, it can be observed that the ablation baseline VAE-MDP is significantly outperformed by OAT across all metrics, which further justifies the importance of augmenting over the PSTs\ninstead of the entire horizon. It can be also observed that VAE-MDP in general outperforms the vanilla VAE without adaptation to the Markovian setting, illustrating the importance of the adaptation step introduced in Section 2.2. We also find that generative models achieve the best performance among the baselines over environments that have relatively shorter horizons (e.g., pen), while their performance is diminished when horizons increased. That further indicates the advantage of PSTs mining that provides much shorter and representative sub-trajectories for generative learning."
        },
        {
            "heading": "3.3.2 RESULTS OVER REAL-WORLD HEALTHCARE AND E-LEARNING",
            "text": "Figure 6 presents the average MAE improvements across all OPE methods in e-learning (left), and improved rank correlation in healthcare (right). Complete results for empirical study and all validation metrics are provided in Appendix E. Regret@5 is not applicable to both environments, since the total number of evaluation policies are less than or equal to five.\nOverall, our method can significantly improve OPE performance in terms of MAE, rank correlation, and regret@1 in both real-world environments. In both e-learning and healthcare, most augmentation baselines lead to neutral to negative\npercentage improvements over the metrics considered, while OAT significantly improves OPE\u2019s performance over all baselines, with the ablation VAE-MDP attains the 2nd best performance. A reason for baselines perform worse in real-world environments than in simulations can be that real-world HIS are considered sophisticated, as the human mental states impact their behaviors implicitly. This further indicates the importance of extracting underlying information from historical trajectories D, as did in OAT and VAE-MDP, as well as effectively enriching the coverage of state-action space to provide more comprehensive coverage for OPE methods to leverage, powered by the methodologies introduced in Section 2."
        },
        {
            "heading": "3.3.3 MORE DISCUSSIONS",
            "text": "We explore the following two major questions that are commonly involved in analyses over HIS.\nWill the level of human involvements affect trajectory augmentations for OPE? As presented in Figure 7, we evaluate augmentation methods across the four tasks in Adroit environment with three different levels of human involvements (LoHI) sorted from the most to least, i.e., human, cloned, and expert. The results show that our method achieves the best performance in terms of all validation metrics when humans are involved in data collection (i.e., human, cloned). The performance of our method is slightly attenuated (but still effective) when the LoHI decreased, while our ablation VAE-MDP leads MAE when the LoHI is 0% (i.e., expert). Though TDA is effective under the case when the LoHI is 0%, it still performs worse than OAT and consistently worse at other levels. Such a finding further confirms the effectiveness of PST mining. Moreover, most baselines are ineffective when the LoHI is below 50%. The reason is that the trajectories obtained from demonstrations often provide limited and/or biased coverage over the state-action space (Fu et al.,\n2021; Chang et al., 2021), thus any augmentation methods that can potentially increase the coverage might be able to improve OPE\u2019s performance. In contrast, the historical trajectories induced from simulations using fine-tuned policies tend to result in better coverage over the state-action space in general (Fu et al., 2020), and the augmentation methods that do not consider the Markovian setting generate trajectories that could be less meaningful (i.e., providing limited or even unrealistic information) to the OPE methods, making them less effective (e.g., negative effects on OPE when LoHI is 0%). For example, in the realm of surgery, permutation can result in a trajectory with stitching happened before incision, which is unrealistic.\nTable 1: Statistical significance test at the level of \u03c1 < 0.05 with bootstrapping on three RL-induced policy \u03c01, \u03c02, \u03c03 compared to expert policy \u03c0expert from real-world intelligent tutoring. The results that show significance are in bold.\nIS result \u03c01 tp \u03c02 tp \u03c03 tp No Aug. 7.24.00 7.07.00 -4.48.00\nOAT 3.10.00 1.33.19 2.11.06 TDA 10.14.00 5.82.00 -13.58.00 Perm. -1.78.08 -1.77.08 1.77.08\nJittering -1.89.06 -1.89.06 1.90.06 Scaling -1.33.06 -1.33.06 1.33.06\nVAE -1.94.06 -1.92.06 1.54.13 TimeGAN 1.90.06 -2.25.03 -2.25.03 SPAWNER -1.00.32 -1.00.32 1.00.32\nDGW -1.43.06 -1.43.16 1.43.16 Empirical result 2.01.04 0.61.54 0.20.84\nCan trajectory augmentation facilitate OPE in terms of significance test? OPE validation metrics generally focus on standard error metrics as proposed in (Fu et al., 2021), while domain experts emphasis statistical significance test for real-world HIS (Robertson & Kaptein, 2016; Zhou et al., 2022). For example, rank correlation summarizes the performance of relative rankings of a set of policies using averaged returns; in contrast, statistical significance tests can examine if the relationships being found are due to randomness. Moreover, they can be easier conveyed to and interpreted by domain experts (Guilford, 1950; Ju et al., 2019).\nOne key measurement for RL-induced policies is whether they significantly outperform the expert policy in HIS (Zhou et al., 2019; 2022). We conduct t-test over OPE estimations (with and without augmentations) obtained from bootstrapping as introduced in (Hao et al., 2021), and measure whether there is a significant difference between the mean value of OPE estimation for each RL-induced policy against the expert policy. Interestingly, the results show that IS performs the best among all 5 OPE methods we considered, in terms of all standard validation metrics in e-learning experiments, with and without augmentations using each augmentation method. This can be caused by the fact that the behavioral policies are intrinsically similar, where 3 out of the 4 policies (i.e., \u03c02, \u03c03, \u03c0expert) lead to similar returns (as shown in Appendix E.1) and the horizon (i.e., T=12) is not much long, the unbiased nature of IS estimators could dominate it\u2019s high variance downside. Such characteristics of IS made it broadly used in short-horizon settings (Mandel et al.,\n2014; Xie et al., 2019). The statistical significance results are summarized in Table 1. It can be observed that, without augmentation, IS estimates that all RL-induced policies performs significantly different from the expert policy. However, in empirical study, only \u03c01 performs significantly better than expert policy, while the other two, i.e., \u03c02 and \u03c03 not. And our proposed method is the only one that improves the IS estimation to be aligned with empirical results across all three policies, while the baselines improve estimation at most one policy. Therefore, the results indicate the effectiveness of our proposed method in terms of both standard OPE validation metrics and significance test."
        },
        {
            "heading": "4 RELATED WORKS",
            "text": "OPE A variety of contemporary OPE methods has been proposed, which can be mainly divided into three categories (Voloshin et al., 2021b): (i) Inverse propensity scoring (Precup, 2000; Doroudi et al., 2017), such as Importance Sampling (IS) (Doroudi et al., 2017). (ii) Direct methods that directly estimate the value functions of the evaluation policy (Nachum et al., 2019; Uehara et al., 2020; Xie et al., 2019; Zhang et al., 2021; Yang et al., 2022), including but not limited to model-based estimators (MB) (Paduraru, 2013; Zhang et al., 2021), value-based estimators (Munos et al., 2016; Le et al., 2019) such as Fitted Q Evaluation (FQE), and minimax estimators (Liu et al., 2018; Zhang et al., 2020b; Voloshin et al., 2021a) such as DualDICE (Yang et al., 2020a). (iii) Hybrid methods combine aspects of both inverse propensity scoring and direct methods (Jiang & Li, 2016; Thomas & Brunskill, 2016), such as DR (Jiang & Li, 2016). However, a major challenge of applying OPE to real-world is that many methods can perform unpleasant when human-collected data is highly limited as demonstrated in (Fu et al., 2020; Gao et al., 2023). Therefore, augmentation can be an important way to facilitate OPE performance.\nData Augmentation for RL In RL, data augmentation has been recognized as effective to improve generalizability of agents over various tasks (Laskin et al., 2020b;a; Kostrikov et al., 2020; Liu et al., 2021; Raileanu et al., 2021; Joo et al., 2022; Goyal et al., 2022). For instance, automatic augmentation selection frameworks are proposed for actor-critic algorithms by regularizing the policy and value functions (Raileanu et al., 2021). However, most of the prior work only consider image input which may not capture temporal dependencies in trajectories. More importantly, the prior work is proposed towards RL policy optimization by learning from high-reward regions of state-action space, while OPE aims to generalize over evaluation policies that can be heterogeneous and lead to varied performance. To the best of our knowledge, no prior work has extensively investigated various prior augmentation methods in OPE, nor proposed augmentation towards offline trajectories to scaffold OPE in real-world domains. More comprehensive review of related works on OPE and data augmentations in general can be found in Appendix G."
        },
        {
            "heading": "5 CONCLUSION, LIMITATION AND SOCIAL IMPACTS",
            "text": "We have proposed OAT, which can capture the dynamics underlying human-involved environments from historical trajectories that provide limited coverage of the state-action space and induce effective augmented trajectories to facilitate OPE. This is achieved by mining potential sub-trajectories, as well as extending a generative modeling framework to capture dynamics under the potential sub-trajectories. We have validated OAT in both simulation and real-world environments, and the results have shown that OAT can generally improve OPE performance and outperform a variety of data augmentation methods. Latent-model-based models such as VAE have been commonly used for augmentation in offline RL, while they generally rarely come with theoretical error bounds provided (Hafner et al., 2020; Lee et al., 2020; Rybkin et al., 2021). Such a challenge also remains for many data augmentation methods (Zheng et al., 2023). However, once the trajectories are augmented, one can choose to use the downstream OPE methods which come with guarantees, such as DR and DICE. Moreover, prior works found that distribution shift could be a challenge for OPE (Wang et al., 2021; Fu et al., 2021). Though this is beyond the scope of this work, given we use existing OPE methods as backbones to process the augmented trajectories, a potential future work is coming up with new OPE methods that can resolve the distribution shift. We conduct extensive experiments to examine the proposed augmentation method, and the results demonstrating its effectiveness. OAT can be stand-alone to generate trajectories without any assumptions over target policies. And it can be utilized by built-on-top works such as policy optimization and representation learning. More discussions regarding social impacts are provided in Appendix H."
        },
        {
            "heading": "LIST OF APPENDICES",
            "text": "Appendix16"
        },
        {
            "heading": "A More Details on Methodology 16",
            "text": "A.1 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nA.1.1 Training Time with VAE-MDP . . . . . . . . . . . . . . . . . . . . . . . . 16 A.2 More Analysis on Augmented Trajectories from OAT . . . . . . . . . . . . . . . . 17 A.2.1 Scatter Plots on Returns . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.3 Details of Discrete Representation Mapping . . . . . . . . . . . . . . . . . . . . . 18 A.3.1 TICC Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.4 Detailed Formulation of the VAE-MDP . . . . . . . . . . . . . . . . . . . . . . . 19 A.5 Proof of Equation 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.6 More Details of Fusing Back Step . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.7 OAT with Non-temporally-aligned Trajectories . . . . . . . . . . . . . . . . . . . 20"
        },
        {
            "heading": "B Experimental Setup 21",
            "text": "B.1 Training Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2 Implementation Details & Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . 21 B.3 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "C Adroit 22",
            "text": "C.1 Detailed Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.2 Start and End Time of PSTs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36"
        },
        {
            "heading": "D Real-World Sepsis Treatment 38",
            "text": "D.1 Task Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38"
        },
        {
            "heading": "E Real-World Intelligent Tutoring 39",
            "text": "E.1 Task Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39"
        },
        {
            "heading": "F Additional Experiments and Results: Gym-Mujoco 41",
            "text": "F.1 Details of Gym-Mujoco . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 F.2 Results on Gym-Mujoco and Discussions . . . . . . . . . . . . . . . . . . . . . . 41"
        },
        {
            "heading": "G More Related Works 42",
            "text": "H Social Impacts 43"
        },
        {
            "heading": "A MORE DETAILS ON METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A.1 COMPLEXITY ANALYSIS",
            "text": "OAT consists of three main steps, including PSTs mining, PSTs augmentation, and fuse back; thus, the overall complexity depends on the specific techniques used in each step. In PSTs mining, the discrete representation mapping uses TICC, which takes O(TNC) time to assign TN states (T is the horizon, N is the number of trajectories) into C clusters (Hallac et al., 2017). To identify PSTs, we use MG-FSM (Miliaraki et al., 2013) for sub-trajectory mining which has O(T ) time complexity. We found that providing complexity for PSTs augmentation could be challenging given that training VAE requires stochastic gradient descent algorithms with parameter tuning (e.g., for step size and network architecture). In fusing back, the time complexity is O(N) if the same amount of augmented trajectories is added, and we are positive this can be shortened with parallel processing. Overall, as shown in Table 2, on average, OAT takes less training time of timeGAN and VAE with the same neural network architecture, respectively, which indicates OAT benefits more in augmenting on much shorter PSTs than the mining process.\nFor the cost of reaching effectiveness, the total number of TDSSs from which PSTs are selected is correlated to the length of the TDSSs which are capped at the horizon, i.e., the total number of TDSSs is capped at CT . However, in Adroit-human, we noticed that the longest length of the PSTs found is 10 when \u03be is 10 (less than N/2), so T seems like a generous upper bound. For example, in Adroit door-human, PSTs mining takes 224s over 6.7k data points."
        },
        {
            "heading": "A.1.1 TRAINING TIME WITH VAE-MDP",
            "text": "We plot the training time of VAE-MDP vs length of PSTs across all 12 Adroit datasets in Figure 8, obtained from the same architecture of LSTMs and training settings as provided in Appendix B.2. From the Figure 8, we can observe that the training time of VAE-MDP is increased almost linearly with the length of PSTs. In experiments, the lengths of PSTs mined from most of the datasets (i.e., 9 out of 12 datasets) are within 11, which only require less than 1 minute to train the VAE-MDP."
        },
        {
            "heading": "A.2 MORE ANALYSIS ON AUGMENTED TRAJECTORIES FROM OAT",
            "text": ""
        },
        {
            "heading": "A.2.1 SCATTER PLOTS ON RETURNS",
            "text": "We present scatter plots plotting the true returns of each policy against the estimated returns. Each point on the plot represents one target policy."
        },
        {
            "heading": "A.3 DETAILS OF DISCRETE REPRESENTATION MAPPING",
            "text": "In this work, we leverage Toeplitz inverse covariance-based clustering (TICC) (Hallac et al., 2017) to map states st \u2208 S into C clusters, where each st is associated with a cluster from the set K = {K1, . . . ,KC}. The states mapped to the same cluster can be considered sharing graphical connectivity structure of both temporal and cross-attributes information captured by TICC. There are variations of TICC targeting specific characteristics of data. Specifically, we used MT-TICC (Yang et al., 2021) which is proposed towards time-awareness and multi-trajectories. The hyperparameter C can be determined by calculating silhouette score (Hallac et al., 2017)."
        },
        {
            "heading": "A.3.1 TICC PROBLEM",
            "text": "Each cluster c \u2208 [1, C] is defined as a Markov random field Rue & Held (2005), or correlation network, captured by its Gaussian inverse covariance matrix \u03a3\u22121c \u2208 Rm\u00d7m, where m is the dimension of state space. We also define the set of clusters M = {M1, . . . ,MC} \u2282 R as well as the set of\ninverse covariance matrices \u03a3\u22121 = {\u03a3\u221211 , . . . ,\u03a3 \u22121 C }. Then the objective is set to be\nmax \u03a3\u22121,M C\u2211 c=1 [ \u2211 o (i) t \u2208Mc ( L(o(i)t ; \u03a3\u22121c )\u2212 \u03f51{o (i) t\u22121 /\u2208 Mc} )] , (3)\nwhere the first term defines the log-likelihood of o(i)t coming from Mc as L(o (i) t ; \u03a3 \u22121 c ) = \u2212 12 (o (i) t \u2212 \u00b5ck) T\u03a3\u22121c (o (i) t \u2212 \u00b5c) + 12 log det\u03a3 \u22121 c \u2212 n2 log(2\u03c0) with \u00b5c being the empirical mean of cluster Mc, the second term 1{o(i)t\u22121 /\u2208 Mc} penalizes the adjacent events that are not assigned to the same cluster and \u03f5 is a constant balancing off the scale of the two terms. This optimization problem can be solved using the expectation-maximization family of algorithms by updating \u03a3\u22121 and M alternatively Hallac et al. (2017)."
        },
        {
            "heading": "A.4 DETAILED FORMULATION OF THE VAE-MDP",
            "text": "The latent prior p(z\u03b6) \u223c N (0, I) representing the distribution of the initial latent states (at the beginning of each PST in the set T g), where I is the identity covariance matrix. The encoder q\u03c9(zt|st\u22121, at\u22121, st) is used to approximate the posterior distribution p\u03b7(zt|st\u22121, at\u22121, st) = p\u03b7(zt\u22121,at\u22121,zt,st)\u222b\nzt\u2208Z p(zt\u22121,at\u22121,zt,st)dzt\n, where Z \u2282 Rm and m is the dimension. Given that q\u03c9(z\u03b6:\u03b6+W\u22121|s\u03b6:\u03b6+W\u22121, a\u03b6:\u03b6+W\u22122) = q\u03c9(z\u03b6 |s\u03b6) \u220f\u03b6+W\u22121\nt=\u03b6+1 q\u03c9(zt|zt\u22121, at\u22121, st), both distributions q\u03c9(z\u03b6 |s\u03b6) and q\u03c9(zt|zt\u22121, at\u22121, st) follow diagonal Gaussian, where mean and diagonal covariance are determined by multi-layer perceptrons (MLPs) and long short-term memory (LSTM), with neural network weights \u03c9. Thus, one can infer z\u03c9\u03b6 \u223c q\u03c9(z\u03b6 |s\u03b6), z\u03c9t \u223c q\u03c9(zt|h\u03c9t ), with h\u03c9t = f\u03c9(h \u03c9 t\u22121, z \u03c9 t\u22121, at\u22121, st) where f\u03c9 represents LSTM layer and h \u03c9 t represents LSTM recurrent hidden state.\nThe decoder p\u03b7(zt, st, rt\u22121|zt\u22121, at\u22121) is used to sample new trajectories. Given p\u03b7(z\u03b6+1:\u03b6+W\u22121, s\u03b6:\u03b6+W\u22121, r\u03b6:\u03b6+W\u22122|z\u03b6 , \u03b2) =\u220f\u03b6+W\u22121\nt=\u03b6 p\u03b7(st|zt) \u220fT\nt=\u03b6+1 p\u03b7(zt|zt\u22121, at\u22121)p\u03b7(rt\u22121|zt), where at\u2019s are determined following the behavioral policy \u03b2, distributions p\u03b7(st|zt) and p\u03b7(rt\u22121|zt) follow diagonal Gaussian with mean and covariance determined by MLPs and p\u03b7(zt|zt\u22121, at\u22121) follows diagonal Gaussian with mean and covariance determined by LSTM.\nThus, the generative process can be formulated as, i.e., at initialization, z\u03b7\u03b6 \u223c p(z\u03b6), s \u03b7 \u03b6 \u223c p\u03b7(s\u03b6 |z \u03b7 \u03b6 ), a\u03b6 \u223c \u03b2(a\u03b6 |s\u03b7\u03b6 ); followed by z \u03b7 t \u223c p\u03b7(h\u0303 \u03b7 t ), r \u03b7 t\u22121 \u223c p\u03b7(rt\u22121|z \u03b7 t ), s \u03b7 t \u223c p\u03b7(st|z \u03b7 t ), at \u223c \u03b2(at|s \u03b7 t ), with h\u0303\u03b7t = g\u03b7[f\u03b7(h \u03b7 t\u22121, z \u03b7 t\u22121, at\u22121)] where g\u03b7 represents an MLP."
        },
        {
            "heading": "A.5 PROOF OF EQUATION 2",
            "text": "The derivation of the evidence lower bound (ELBO) for the joint log-likelihood distribution can be found below.\nlog p\u03b7(s\u03b6:\u03b6+W\u22121, r\u03b6:\u03b6+W\u22122) (4)\n= log \u222b z\u03b6+1:\u03b6+W\u22121\u2208Z p\u03b7(s\u03b6:\u03b6+W\u22121, z\u03b6+1:\u03b6+W\u22121, r\u03b6:\u03b6+W\u22122)dz (5)\n= log \u222b z\u03b6+1:\u03b6+W\u22121\u2208Z p\u03b7(s\u03b6:\u03b6+W\u22121, z\u03b6+1:\u03b6+W\u22121, r\u03b6:\u03b6+W\u22122) q\u03c9(z\u03b6:\u03b6+W\u22121|s\u03b6:\u03b6+W\u22121, a\u03b6:\u03b6+W\u22122) q\u03c9(z\u03b6:\u03b6+W\u22121|s\u03b6:\u03b6+W\u22121, a\u03b6:\u03b6+W\u22122)dz\n(6) Jensen\u2032s inequality\n\u2265 Eq\u03c9 [log p(z\u03b6) + log p\u03b7(s\u03b6:\u03b6+W\u22121, z\u03b6+1:\u03b6+W\u22121, r\u03b6:\u03b6+W\u22122|z\u03b6)\u2212 log q\u03c9(z\u03b6:\u03b6+W\u22121|s\u03b6:\u03b6+W\u22121, a\u03b6:\u03b6+W\u22122)] (7)\n=Eq\u03c9 [ log p(z\u03b6) + log p\u03b7(s\u03b6 |z\u03b6) + \u2211\u03b6+W\u22121 t=\u03b6 log p\u03b7(st, zt, rt\u22121|zt\u22121, at\u22121)\n\u2212 log q\u03c9(z\u03b6 |s\u03b6)\u2212 \u2211\u03b6+W\u22121\nt=\u03b6+1 log q\u03c9(zt|zt\u22121, at\u22121, st)\n] (8)\n=Eq\u03c9 [ log p(z\u03b6)\u2212 log q\u03c9(z\u03b6 |s\u03b6) + log p\u03b7(s\u03b6 |z\u03b6) + \u2211\u03b6+W\u22121 t=\u03b6+1 log ( p\u03b7(st|zt)p\u03b7(rt\u22121|zt)p\u03b7(zt|zt\u22121, at\u22121) ) \u2212 \u2211\u03b6+W\u22121\nt=\u03b6+1 log q\u03c9(zt|zt\u22121, at\u22121, st)\n] (9)\n=Eq\u03c9 [\u2211\u03b6+W\u22121\nt=\u03b6 log p\u03b7(st|zt) + \u2211\u03b6+W\u22121 t=\u03b6+1 log p\u03b7(rt\u22121|zt)\n\u2212KL ( q\u03c9(z\u03b6 |s\u03b6)||p(z\u03b6) ) \u2212 \u2211\u03b6+W\u22121 t=\u03b6+1 KL ( q\u03c9(zt|zt\u22121, at\u22121, st)||p\u03b7(zt|zt\u22121, at\u22121) )] .\n(10)"
        },
        {
            "heading": "A.6 MORE DETAILS OF FUSING BACK STEP",
            "text": "We have ensured that the transition from the original trajectory to the beginning of the augmented PSTs are smoothed, by letting the generation of s\u0302\u03b6 , the initial state in the PST (e.g., s\u03023 in Figure 3), to be conditioned on s\u03b6\u22121 which is the last state in the original trajectory (e.g., s\u03022 in Figure 3), which equivalently set s\u03b6\u22121 as the underlying initial state for the generated PST. To smooth the end of PSTs, we select the augmented PSTs with the states and actions that have the least distance to the original states and actions, at step \u03b6 +W \u2212 1. In experiments, we use the Euclidean distance as the measure."
        },
        {
            "heading": "A.7 OAT WITH NON-TEMPORALLY-ALIGNED TRAJECTORIES",
            "text": "In the main context, for notation conciseness, we use horizon T to denote the length of all trajectories for notational simplicity, since our work is the first one introducing sub-trajectory mining and augmentation to OPE. Note that OAT can work with non-temporally-aligned trajectories, i.e., trajectories with different lengths and varied start and end times. In the PSTs mining step, the length W is flexible and the TDSSs can be generated with a small length (e.g., 2) and expanded recursively until reaching a maximum length (e.g., T ). Since our goal is to extract PSTs whose TDSSs are shared across trajectories with large supports, a threshold \u03be is used to bound the supports of PSTs so that the PSTs with the greatest supports can be always extracted. Moreover, in our experiment, the Adroit-human environments contain temporally non-aligned trajectories (e.g., length of trajectories from door and relocate, varied from 223 to 300, and 297 to 527, respectively). More environmental details are provided in Appendix C. And Appendix C.2 shows the the length of trajectories, and start and end time of PSTs found on each trajectory can be varied."
        },
        {
            "heading": "B EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "B.1 TRAINING RESOURCES",
            "text": "We implement the proposed method in Python. Training of our method and baselines are supported by four NVIDIA TITAN Xp 12GB, three NVIDIA Quadro RTX 6000 24GB, and four NVIDIA RTX A5000 24GB GPUs.\nB.2 IMPLEMENTATION DETAILS & HYPER-PARAMETERS\nThe cluster number for discrete representation mapping can be determined by silhouette score using training data following (Hallac et al., 2017), we perform search among [10, 20] for C in all datasets and the one with the highest silhouette score is selected. In our experiments, C = 18, 10, 19, 16 for {pen, door, relocate, hammer}-human, respectively; C = 20, 10, 10, 10 for {pen, door, relocate, hammer}-cloned, respectively; C = 11, 10, 10, 10 for {pen, door, relocate, hammer}-expert, respectively; C = 14, 17 for e-learning and healthcare, respectively.\nThe experimental results are obtained with selecting the PSTs using the threshold \u03be \u2208 [2, N ] at the top 1, i.e., we use the PST with the highest support of its corresponding TDSS, for easier investigation of the PSTs mining and comparison to other augmentation baselines such as TDA, and present straightforward and general effects of our method. The percentage supports of the selected PSTs, i.e., support(\u00b7)/N , are all \u2265 82% across all datasets and all experimental environments, especially can cover 100% trajectories in all Adroit human tasks, which may further indicates the effectiveness of PSTs mining. We choose the neural network architectures as follows.\nFor the components involving LSTMs, which include q\u03c9(zt|zt\u22121, at\u22121, st) and p\u03b7(zt|zt\u22121, at\u22121), their architecture include one LSTM layer with 64 nodes, followed by a dense layer with 64 nodes. All other components do not have LSTM layers involved, so they are constituted by a neural network with 2 dense layers, with 128 and 64 nodes respectively. The output layers that determine the mean and diagonal covariance of diagonal Gaussian distributions use linear and softplus activations, respectively. The ones that determine the mean of Bernoulli distributions (e.g., for capturing early termination of episodes) are configured to use sigmoid activations. For training OAT and its ablation VAE-MDP, maximum number of iteration is set to 100 and minibatch size set to 4 (given the small numbers of trajectories, i.e., 25 for each task) in Adroit, and 1,000 and 64 for real-world healthcare and e-learning, respectively. Adam optimizer is used to perform gradient descent. To determine the learning rate, we perform grid search among {1e\u2212 4, 3e\u2212 3, 3e\u2212 4, 5e\u2212 4, 7e\u2212 4}. Exponential decay is applied to the learning rate, which decays the learning rate by 0.997 every iteration. For OPE, the model-based methods are evaluated by directly interacting with each target policy for 50 episodes, and the mean of discounted total returns (\u03b3 = 0.995 for Adroit, \u03b3 = 0.99 for Healthcare, \u03b3 = 0.9 for e-learning) over all episodes is used as estimated performance for the policy."
        },
        {
            "heading": "B.3 EVALUATION METRICS",
            "text": "Absolute error The absolute error is defined as the difference between the actual value and estimated value of a policy:\nAE = |V \u03c0 \u2212 V\u0302 \u03c0| (11)\nwhere V \u03c0 represents the actual value of the policy \u03c0, and V\u0302 \u03c0 represents the estimated value of \u03c0.\nRegret@1 Regret@1 is the (normalized) difference between the value of the actual best policy, and the actual value of the best policy chosen by estimated values. It can be defined as:\nR1 = (max i\u22081:P V \u03c0i \u2212 max j\u2208best(1:P ) V \u03c0j )/ max i\u22081:P V \u03c0i (12)\nwhere best(1 : P ) denotes the index of the best policy over the set of P policies as measured by estimated values V\u0302 \u03c0 .\nRank correlation Rank correlation measures the Spearman\u2019s rank correlation coefficient between the ordinal rankings of the estimated values and actual values across policies:\n\u03c1 = Cov(rank(V \u03c01:P ), rank(V\u0302 \u03c0 1:P ))\n\u03c3(rank(V \u03c01:P ))\u03c3(rank(V\u0302 \u03c0 1:P ))\n(13)\nwhere rank(V \u03c01:P ) denotes the ordinal rankings of the actual values across policies, and rank(V\u0302 \u03c0 1:P ) denotes the ordinal rankings of the estimated values across policies.\nC ADROIT\nAs shown in Figure 13, Adriot (Rajeswaran et al., 2018) is a simulation environment with four synthetic real-world robotics tasks, where a 24-DoF simulated Shadow Hand robot is asked to hammer a nail (hammer), open a door (door), twirl a pen (pen), or pick up and move a ball (relocate). Each task contains three training datasets with different levels of human-involvements, including full demonstration data from human (human), induced data from a fine-tuned RL policy (expert), and mixing data with a 50-50 ratio of demonstration and induced data (cloned). Task properties are provided in Table 3.\nC.1 DETAILED RESULTS"
        },
        {
            "heading": "C.2 START AND END TIME OF PSTS.",
            "text": "We present that the length of trajectories, and start and end time of found PSTs on each trajectory in Adroit human can be varied, as shown in Tables 17, 18, 19, & 20. It can be observed that PST can start at different time steps across trajectories, which resolves the non-temporally-aligned cases."
        },
        {
            "heading": "D REAL-WORLD SEPSIS TREATMENT",
            "text": "Sepsis, which is defined as life-threatening organ dysfunction in response to infection, is the leading cause of mortality and the most expensive condition associated with in-hospital stay (Liu et al., 2014). In particular, septic shock, which is the most advanced complication of sepsis due to severe abnormalities of circulation and/or cellular metabolism (Bone et al., 1992), reaches a mortality rate as high as 50% (Martin et al., 2003). It is critical to find an effective policy that can be followed to prevent septic shock and recover from sepsis."
        },
        {
            "heading": "D.1 TASK DETAILS",
            "text": "Labels. The hospital provided the EHRs over two years, including 221,700 visits with 35 static variables such as gender, age, and past medical condition, and 43 temporal variables including vital signs, lab analytes, and treatments. Our study population is patients with a suspected infection which was identified by the administration of any type of antibiotic, antiviral, antibacterial, antiparasitic, or antifungal, or a positive test result of PCR (Point of Care Rapid). On the basis of the Third International Consensus Definitions for Sepsis and Septic Shock (Singer et al., 2016), our medical experts identified septic shock as any of the following conditions are met:\n\u2022 Persistent hypertension as shown through two consecutive readings (\u2264 30 minutes apart). Systolic Blood Pressure (SBP) < 90 mmHg Mean Arterial Pressure (MAP) < 65 mmHg Decrease in SBP \u2265 40 mmHg with an 8-hour period\n\u2022 Any vasopressor administration.\nFrom the EHRs, 3,499 septic shock positive and 81,398 negative visits were identified based on the intersection of the expert sepsis diagnostic rules and International Codes for Disease 9th division (ICD-9); the 36,122 visits with mismatched labels between the expert rule and the ICD-9 were excluded in our study. 2,205 shock visits were obtained by excluding the visits admitted with septic shock and the long-stay visits and then we did the stratified random sampling from non-shock visits, keeping the same distribution of age, gender, ethnicity, and length of hospital stay. The final data constituted 4,410 visits with an equal ratio of shock and non-shock visits.\nStates. To approximate patient observations, 15 sepsis-related attributes were selected based on the sepsis diagnostic rules. In our data, the average missing rate across the 15 sepsis-related attributes was 78.6%. We avoided deleting sparse attributes or resampling with a regular time interval because the attributes suggested by medical experts are critical to decision making for sepsis treatment, and the temporal missing patterns of EHRs also provide the information of patient observations. The missing values were imputed using Temporal Belief Memory (Kim & Chi, 2018) combined with missing indicators (Lipton et al., 2016).\nActions. For actions, we considered two medical treatments: antibiotic administration and oxygen assistance. Note that the two treatments can be applied simultaneously, which results in a total of four actions. Generally, the treatments are mixed in discrete and continuous action spaces according to their granularity. For example, a decision of whether a certain drug is administrated is discrete, while the dosage of drug is continuous. Continuous action space has been mainly handled by policybased RL models such as actor-critic models (Lillicrap et al., 2015), and it is generally only available for online RL. Since we cannot search continuous action spaces while online interacting with actual patients, we focus on discrete actions. Moreover, in this work, the RL agent aims to let the physicians know when and which treatment should be given to a patient, rather than suggests an optimal amount of drugs or duration of oxygen control that requires more complex consideration.\nRewards. Two leading clinicians, both with over 20-year experience on the subject of sepsis, guided to define the reward function based on the severity of septic stages. The rewards were defined as follows: infection [-5], inflammation [-10], organ failures [-20], and septic shock [-50]. Whenever a patient was recovered from any stage of them, the positive reward for the stage was gained back.\nThe data was divided into 80% (the earlier 80% according to the time of the first event recorded in patients\u2019 visits) for training and (the later) 20% for test, following the common practice while splitting up time-series for training and testing (Campos et al., 2014).\nPolicies We assume that the clinical care team is well-trained with sufficient medical knowledge and follows standard protocols in sepsis treatments, thus we consider the behavioral policy, parameterized through behavior cloning (Azizsoltani & Jin, 2019), that generates the trajectories above as an expert policy. We estimate the behavior policy with behavior cloning as in (Fu et al., 2021; Hanna et al., 2019). The evaluation policies were trained using off-policy DQN algorithm with different hyper-parameter settings, where DQN was trained using default setting (learning rate 1e\u2212 3, \u03b3 = 0.99), learning rate 1e\u2212 4, learning rate 1e\u2212 5, a different random seed, \u03b3 = 0.9, respectively. Evaluate Performance of Target Policies Since the RL agent cannot directly interact with patients, it only depends on offline data for both policy induction and evaluation. In similar fashion to prior studies (Komorowski et al., 2018; Azizsoltani & Jin, 2019; Raghu et al., 2017), the induced policies were evaluated using the septic shock rate. And the OPE validation metric, rank correlation, can be calculated by comparing the ranking by OPE estimations versus the rankings of septic shock rate of target policies. The assumption behind that is (Raghu et al., 2017): when a septic shock prevention policy is indeed effective, the more the real treatments in a patient trajectory agree with the induced policy, the lower the chance the patient would get into septic shock; vice versa, the less the real treatments in a patient trajectory agree with the induced policy (more dissimilar), the higher the chance the patient would get into septic shock. Specifically, we follow the recent design by (Ju et al., 2021): We measured agreement rate with the agent policy, which is the number of actions of a target policy agreed with the agent policy among the total number of actions in a trajectory. Then we sort the trajectories by their similarity rate in ascending order and calculate the septic shock rate for the top 10% of trajectories with the highest similarity rate. If the agent policies are indeed effective, the more the actually executed treatments agree with the agent policy, the less likely the patient is going to have septic shock."
        },
        {
            "heading": "E REAL-WORLD INTELLIGENT TUTORING",
            "text": ""
        },
        {
            "heading": "E.1 TASK DETAILS",
            "text": "Our data contains a total of 1,307 students\u2019 interaction logs with a web-based ITS collected over seven semesters\u2019 classroom studies. The ITS is used in an undergraduate STEM course at a college, which has been extensively used by over 2, 000 students with \u223c800k recorded interaction logs through eight academic years. The ITS is designed to teach entry-level undergraduate students with ten major probability principles, including complement theorem, Bayes\u2019 rule, etc. The GUI of the ITS is provided in Figure 14.\nStates. During tutoring, there are many factors that might determine or indicate students\u2019 learning state, but many of them are not well understood by educators. Thus, to be conservative, we extract varieties of attributes that might determine or indicate student learning observations from student-system interaction logs. In sum, 142 attributes with both discrete and continuous values are extracted, which can be categorized into the following five groups:\n(i) Autonomy (10 features): the amount of work done by the student, such as the number of times the student restarted a problem;\n(ii) Temporal Situation (29 features): the time-related information about the work process, such as average time per step;\n(iii) Problem-Solving (35 features): information about the current problem-solving context, such as problem difficulty;\n(iv) Performance (57 features): information about the student\u2019s performance during problemsolving, such as percentage of correct entries;\n(v) Hints (11 features): information about the student\u2019s hint usage, such as the total number of hints requested.\nActions. For each problem, the ITS agent will decide whether the student should solve the next problem, study a solution provided by the tutor or work together with the tutor to solve on the problem. For each problem, the agent makes two levels of granularity: problem first and then step. For problem level, it first decides whether the next problem should be a worked example (WE), problem solving (PS), or a collaborative problem solving worked example (CPS). In WEs, students observe how the tutor solves a problem; in PSs, students solve the problem themselves; in CPSs, the students and the tutor co-construct the solution. If a CPS is selected, the tutor will then make step-level decisions on whether to elicit the next step from the student or to tell the solution step to the student directly.\nRewards. There was no immediate reward but the empirical evaluation matrix (i.e., delayed reward), which was the students\u2019 Normalized Learning Gain (NLG). NLG measured students\u2019 learning gain irrespective of their incoming competence. NLG is defined as: NLG = scoreposttest\u2212scorepretest\u221a\n1\u2212scorepretest ,\nwhere 1 denotes the maximum score for both pre- and post-test that were taken before and after usage of the ITS, respectively.\nPolicies. The study were conducted across seven semesters, where the first six semesters\u2019 data were collected over expert policy and the seventh semester\u2019s data were collected over four different policies (three policies were RL-induced policies and one was the expert policy). The expert policy randomly picked actions. The three RL-induced policies were trained using off-policy DQN algorithm with different learning rates lr = {1e\u2212 3, 1e\u2212 4, 1e\u2212 5}. Evaluate Performance of Target Policies. Target policies are randomly assigned to 140 students who take the Probability course in one semester. During the studies, all students used the same tutor, followed the same general procedure, studied the same training materials, and worked through the same training problems. All students went through the same four phases: 1) reading textbook, 2) pre-test, 3) working on the ITS, and 4) post-test. During reading textbook, students read a general description of each principle, reviewed examples, and solved some training problems to get familiar with the ITS. Then the students took a pre-test which contained a total of 14 single- and multipleprinciple problems. Students were not given feedback on their answers, nor were they allowed to go back to earlier questions (so as the post-test). Next, students worked on the ITS, where they received the 12 problems on ITS in the same order. After that, students took the 20-problem post-test, where 14 of the problems were isomorphic to the pre-test and the remainders were non-isomorphic multiple-principle problems. Tests were auto-graded following the same grading criteria. Test scores were normalized to the range of [0, 1]."
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTS AND RESULTS: GYM-MUJOCO",
            "text": ""
        },
        {
            "heading": "F.1 DETAILS OF GYM-MUJOCO",
            "text": "A total of 4 environments are provided by Gym-Mujoco, and we follow the guidelines from DOPE benchmark to validate our work and baselines (Fu et al., 2020). Moreover, each environment is provided with 5 training datasets collected using different behavioral policies, resulting in a total of 20 sets of tasks. DOPE also provides 11 target policies for each environment, whose performance are to be evaluated by the OPE methods. Though the Gym-Mujoco environments may not fit our major interests, i.e., human-involved tasks, we provide additional validation of our work on them, given they are popular testbeds and may be interested by readers. Table 21 shows summary of the Gym-Mujoco environments and datasets."
        },
        {
            "heading": "F.2 RESULTS ON GYM-MUJOCO AND DISCUSSIONS",
            "text": "The Table 22 presents summary results on Gym-Mujoco environments using OAT and augmentation baselines. The results are obtained by calculating the improved percentage (for MAE), or distance (for Rank Correlation, Regret@1, and Regret@5) after augmentation compared to the original OPE\nresults without augmentation. OAT can outperforms the baselines in terms of all four evaluation metrics (as bold in Table 22)."
        },
        {
            "heading": "G MORE RELATED WORKS",
            "text": "OPE In real-world, deploying and evaluating RL policies online are high stakes in such domains, as a poor policy can be fatal to humans. It\u2019s thus crucial to propose effective OPE methods. OPE is used to evaluate the performance of a target policy given historical data drawn from (alternative) behavior policies. A variety of contemporary OPE methods has been proposed, which can be mainly divided into three categories (Voloshin et al., 2021b): (i) Inverse propensity scoring (Precup, 2000; Doroudi et al., 2017), such as Importance Sampling (IS) (Doroudi et al., 2017), to reweigh the rewards in historical data using the importance ratio between \u03b2 and \u03c0. (ii) Direct methods directly estimate the value functions of the evaluation policy (Nachum et al., 2019; Uehara et al., 2020; Xie et al., 2019; Zhang et al., 2021; Yang et al., 2022), including but not limited to model-based estimators (MB) (Paduraru, 2013; Zhang et al., 2021) that train dynamics and reward models on transitions from the offline data; value-based estimators (Munos et al., 2016; Le et al., 2019) such as Fitted Q Evaluation (FQE) which is a policy evaluation counterpart to batch Q learning; minimax estimators (Liu et al., 2018; Zhang et al., 2020b; Voloshin et al., 2021a) such as DualDICE that estimates the discounted stationary distribution ratios (Yang et al., 2020a). (iii) Hybrid methods combine aspects of both inverse propensity scoring and direct methods (Jiang & Li, 2016; Thomas & Brunskill, 2016). For example, DR (Jiang & Li, 2016) leverages a direct method to decrease the variance of the unbiased estimates produced by IS. However, a major challenge of applying OPE to real world is many methods can perform unpleasant when human-collected data is highly limited as in (Fu et al., 2020; Gao et al., 2023), augmentation can be an important way to facilitate OPE performance.\nData Augmentation Data augmentation has been widely investigated in various domains, including computer vision, time series, and RL. In computer vision, images are the major target and augmentation have improved downstream models\u2019 performance (LeCun et al., 1998; Deng et al., 2009; Cubuk et al., 2019; Xie et al., 2020). However, many image-targeted methods, such as crop and rotate images, will discard important information in trajectories. In time series, a variety of data augmentation has been proposed to capture temporal and multivariate dependencies (Le Guennec et al., 2016; Kamycki et al., 2019; Yoon et al., 2019; Iwana & Uchida, 2021a). For instance, SPAWNER (Kamycki et al., 2019) and DGW (Iwana & Uchida, 2021b) augment time series by capturing group-level similarities to facilitate supervised learning. Generative models such as GAN\nand VAE have achieved state-of-the-art performance in time series augmentation for both supervised and unsupervised learning (Antoniou et al., 2017; Donahue et al., 2018; Yoon et al., 2019; Barak et al., 2022). However, those approaches for images and time-series do not consider the Markovian nature in OPE training data, and may not be directly applicable to MDP trajectory augmentation."
        },
        {
            "heading": "H SOCIAL IMPACTS",
            "text": "All educational and healthcare data employed in this paper were obtained anonymously through an exempt IRB-approved protocol and were scored using established rubrics. No demographic data or class grades were collected. All data were shared within the research group under IRB, and were de-identified and automatically processed for labeling. This research seeks to remove societal harms that come from lower engagement and retention of students who need more personalized interventions and developing more robust medical interventions for patients."
        }
    ],
    "year": 2023
}