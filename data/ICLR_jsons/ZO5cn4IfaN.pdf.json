{
    "abstractText": "The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces localupdating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",
    "authors": [],
    "id": "SP:9161f036bdc41d516ed7f9c110c1507351325831",
    "references": [
        {
            "authors": [
                "Dan Alistarh",
                "Demjan Grubic",
                "Jerry Li",
                "Ryota Tomioka",
                "Milan Vojnovic"
            ],
            "title": "Qsgd: Communicationefficient sgd via gradient quantization and encoding",
            "venue": "arXiv preprint arXiv:1610.02132,",
            "year": 2016
        },
        {
            "authors": [
                "Mahmoud Assran",
                "Nicolas Loizou",
                "Nicolas Ballas",
                "Mike Rabbat"
            ],
            "title": "Stochastic gradient push for distributed deep learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Saar Barkai",
                "Ido Hakimi",
                "Assaf Schuster"
            ],
            "title": "Gap aware mitigation of gradient staleness",
            "venue": "arXiv preprint arXiv:1909.10802,",
            "year": 2019
        },
        {
            "authors": [
                "Leighton Pate Barnes",
                "Huseyin A. Inan",
                "Berivan Isik",
                "Ayfer Ozgur"
            ],
            "title": "rtop-k: A statistical estimation approach to distributed sgd",
            "venue": "arXiv preprint arXiv:2005.10761,",
            "year": 2020
        },
        {
            "authors": [
                "Heinz H Bauschke",
                "J\u00e9r\u00f4me Bolte",
                "Marc Teboulle"
            ],
            "title": "A descent lemma beyond lipschitz gradient continuity: first-order methods revisited and applications",
            "venue": "Mathematics of Operations Research,",
            "year": 2017
        },
        {
            "authors": [
                "Jianmin Chen",
                "Xinghao Pan",
                "Rajat Monga",
                "Samy Bengio",
                "Rafal Jozefowicz"
            ],
            "title": "Revisiting distributed synchronous sgd",
            "venue": "arXiv preprint arXiv:1604.00981,",
            "year": 2016
        },
        {
            "authors": [
                "Alon Cohen",
                "Amit Daniely",
                "Yoel Drori",
                "Tomer Koren",
                "Mariano Schain"
            ],
            "title": "Asynchronous stochastic optimization robust to arbitrary delays",
            "venue": "arXiv preprint arXiv:2106.11879,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Sam Shleifer",
                "Luke Zettlemoyer"
            ],
            "title": "8-bit optimizers via block-wise quantization",
            "venue": "arXiv preprint arXiv:2110.02861,",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Sanghamitra Dutta",
                "Jianyu Wang",
                "Gauri Joshi"
            ],
            "title": "Slow and stale gradients can win the race",
            "venue": "IEEE Journal on Selected Areas in Information Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Priya Goyal",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Pieter Noordhuis",
                "Lukasz Wesolowski",
                "Aapo Kyrola",
                "Andrew Tulloch",
                "Yangqing Jia",
                "Kaiming He"
            ],
            "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
            "venue": "arXiv preprint arXiv:1706.02677,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Qirong Ho",
                "James Cipar",
                "Henggang Cui",
                "Seunghak Lee",
                "Jin Kyu Kim",
                "Phillip B Gibbons",
                "Garth A Gibson",
                "Greg Ganger",
                "Eric P Xing"
            ],
            "title": "More effective distributed ml via a stale synchronous parallel parameter server",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Sebastian U. Stich",
                "Martin Jaggi"
            ],
            "title": "Sharper convergence guarantees for asynchronous sgd for distributed and federated learning",
            "venue": "arXiv preprint arXiv:2206.08307,",
            "year": 2022
        },
        {
            "authors": [
                "Conglong Li",
                "Ammar Ahmad Awan",
                "Hanlin Tang",
                "Samyam Rajbhandari",
                "Yuxiong He"
            ],
            "title": "1-bit lamb: Communication efficient large-scale large-batch training with lamb\u2019s convergence speed",
            "venue": "arXiv preprint arXiv:2104.06069,",
            "year": 2021
        },
        {
            "authors": [
                "Mu Li",
                "David G Andersen",
                "Jun Woo Park",
                "Alexander J Smola",
                "Amr Ahmed",
                "Vanja Josifovski",
                "James Long",
                "Eugene J Shekita",
                "Bor-Yiing Su"
            ],
            "title": "Scaling distributed machine learning with the parameter server",
            "venue": "In 11th USENIX Symposium on operating systems design and implementation (OSDI",
            "year": 2014
        },
        {
            "authors": [
                "Shen Li",
                "Yanli Zhao",
                "Rohan Varma",
                "Omkar Salpekar",
                "Pieter Noordhuis",
                "Teng Li",
                "Adam Paszke",
                "Jeff Smith",
                "Brian Vaughan",
                "Pritam Damania",
                "Soumith Chintala"
            ],
            "title": "Pytorch distributed: Experiences on accelerating data parallel training",
            "venue": "arXiv preprint arXiv:2006.15704,",
            "year": 2020
        },
        {
            "authors": [
                "Shidi Li",
                "Christian Walder",
                "Alexander Soen",
                "Lexing Xie",
                "Miaomiao Liu"
            ],
            "title": "Sampled transformer for point sets",
            "venue": "arXiv preprint arXiv:2302.14346,",
            "year": 2023
        },
        {
            "authors": [
                "Shigang Li",
                "Torsten Hoefler"
            ],
            "title": "Near-optimal sparse allreduce for distributed deep learning",
            "venue": "arXiv preprint arXiv:2201.07598,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Lin",
                "Sebastian U. Stich",
                "Kumar Kshitij Patel",
                "Martin Jaggi"
            ],
            "title": "Don\u2019t use large mini-batches, use local sgd",
            "venue": "arXiv preprint arXiv:1808.07217,",
            "year": 2018
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "arXiv preprint arXiv:1612.03144,",
            "year": 2016
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov. Roberta"
            ],
            "title": "A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692,",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Yucheng Lu",
                "Conglong Li",
                "Minjia Zhang",
                "Christopher De Sa",
                "Yuxiong He"
            ],
            "title": "Maximizing communication efficiency for large-scale training via 0/1 adam",
            "venue": "arXiv preprint arXiv:2202.06009,",
            "year": 2022
        },
        {
            "authors": [
                "Konstantin Mishchenko",
                "Francis Bach",
                "Mathieu Even",
                "Blake Woodworth"
            ],
            "title": "Asynchronous sgd beats minibatch sgd under arbitrary delays",
            "venue": "arXiv preprint arXiv:2206.07638,",
            "year": 2022
        },
        {
            "authors": [
                "Yatian Pang",
                "Wenxiao Wang",
                "Francis EH Tay",
                "Wei Liu",
                "Yonghong Tian",
                "Li Yuan"
            ],
            "title": "Masked autoencoders for point cloud self-supervised learning",
            "venue": "In European conference on computer vision,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Samyam Rajbhandari",
                "Jeff Rasley",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "Zero: Memory optimizations toward training trillion parameter models",
            "venue": "In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Shaohuai Shi",
                "Xiaowen Chu",
                "Ka Chun Cheung",
                "Simon See"
            ],
            "title": "Understanding top-k sparsification in distributed deep learning",
            "venue": "arXiv preprint arXiv:1911.08772,",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian U Stich"
            ],
            "title": "Local sgd converges fast and communicates little",
            "venue": "arXiv preprint arXiv:1805.09767,",
            "year": 2018
        },
        {
            "authors": [
                "Wenbo Su",
                "Yuanxing Zhang",
                "Yufeng Cai",
                "Kaixu Ren",
                "Pengjie Wang",
                "Huimin Yi",
                "Yue Song",
                "Jing Chen",
                "Hongbo Deng",
                "Jian Xu",
                "Lin Qu",
                "Bo zheng"
            ],
            "title": "Gba: A tuning-free approach to switch between synchronous and asynchronous training for recommendation model",
            "venue": "arXiv preprint arXiv:2205.11048,",
            "year": 2022
        },
        {
            "authors": [
                "Weixuan Sun",
                "Zhen Qin",
                "Hui Deng",
                "Jianyuan Wang",
                "Yi Zhang",
                "Kaihao Zhang",
                "Nick Barnes",
                "Stan Birchfield",
                "Lingpeng Kong",
                "Yiran Zhong"
            ],
            "title": "Vicinity vision transformer",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Hanlin Tang",
                "Shaoduo Gan",
                "Ammar Ahmad Awan",
                "Samyam Rajbhandari",
                "Conglong Li",
                "Xiangru Lian",
                "Ji Liu",
                "Ce Zhang",
                "Yuxiong He"
            ],
            "title": "1-bit adam: Communication efficient large-scale training with adam\u2019s convergence speed",
            "venue": "arXiv preprint arXiv:2102.02888,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenheng Tang",
                "Shaohuai Shi",
                "Xiaowen Chu",
                "Wei Wang",
                "Bo Li"
            ],
            "title": "Communication-efficient distributed deep learning: A comprehensive survey",
            "venue": "arXiv preprint arXiv:2003.06307,",
            "year": 2020
        },
        {
            "authors": [
                "Jianyu Wang",
                "Gauri Joshi"
            ],
            "title": "Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Jianyu Wang",
                "Vinayak Tantia",
                "Nicolas Ballas",
                "Michael Rabbat"
            ],
            "title": "Slowmo: Improving communication-efficient distributed sgd with slow momentum",
            "venue": "arXiv preprint arXiv:1910.00643,",
            "year": 2019
        },
        {
            "authors": [
                "Jianyu Wang",
                "Hao Liang",
                "Gauri Joshi"
            ],
            "title": "Overlap local-SGD: An algorithmic approach to hide communication delays in distributed SGD",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Wettig",
                "Tianyu Gao",
                "Zexuan Zhong",
                "Danqi Chen"
            ],
            "title": "Should you mask 15arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Zhirong Wu",
                "Shuran Song",
                "Aditya Khosla",
                "Fisher Yu",
                "Linguang Zhang",
                "Xiaoou Tang",
                "Jianxiong Xiao"
            ],
            "title": "3d shapenets: A deep representation for volumetric shapes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Zijie Yan"
            ],
            "title": "Gradient sparification for asynchronous distributed training",
            "venue": "arXiv preprint arXiv:1910.10929,",
            "year": 2019
        },
        {
            "authors": [
                "Jian Zhang",
                "Christopher De Sa",
                "Ioannis Mitliagkas",
                "Christopher R\u00e9"
            ],
            "title": "Parallel sgd: When does averaging help",
            "venue": "arXiv preprint arXiv:1606.07365,",
            "year": 2016
        },
        {
            "authors": [
                "Wei Zhang",
                "Suyog Gupta",
                "Xiangru Lian",
                "Ji Liu"
            ],
            "title": "Staleness-aware async-sgd for distributed deep learning",
            "venue": "arXiv preprint arXiv:1511.05950,",
            "year": 2015
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler"
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "In The IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Distributed optimization is crucial for the efficient training of large-scale deep neural networks. Mini-batch parallel optimization methods (Goyal et al., 2017; Li et al., 2014) like stochastic gradient decent (SGD) with distributed data parallel (DDP) paradigm are commonly used, but communication overhead can pose significant challenges when scaling out to larger GPU clusters. Existing techniques leverage gradient bucketing to partially overlap communication with backward computation to enhance training efficiency, but residual overhead remains a challenge in scenarios with large model sizes and limited inter-node communication bandwidth.\nVarious strategies have been proposed to address the communication-related issues. These strategies can be classified into three following categories: 1) Communication Compression in Single Iteration, which comprises techniques such as gradient sparsification (Shi et al., 2019; Li & Hoefler, 2022; Yan, 2019; Barnes et al., 2020) and gradient quantization (Alistarh et al., 2016; Tang et al., 2021; Li et al., 2021; Dettmers et al., 2021). These methods accelerate distributed training by minimizing the volume of communication traffic during each iteration. 2) Communication Frequency Reduction is represented by work such as (Lin et al., 2018; Wang et al., 2019; 2020; Wang & Joshi, 2021), which maintains the communication volume per iteration but lessens the frequency of communication events. 3) Communication and Computation Overlapping aims to overlap communication with computation, either in a single step or across multiple steps. For instance, the distributed module in PyTorch (Li et al., 2020) leverages gradient bucketing to partially overlap gradient all-reducing with the backward pass. Additionally, asynchronous distributed training methods (Dutta et al., 2021; Cohen et al., 2021; Mishchenko et al., 2022; Su et al., 2022; Koloskova et al., 2022) synchronize the transmission of stale gradients with the latest local computation on each worker.\nThese communication-efficient approaches improve convergence speed but often yield sub-optimal final optimization outcomes. None of these methods have achieved complete overlap of communication with computation, i.e., 100% scalability throughout the entirety of the training process on a large cluster, especially under varying communication conditions while preserving the performance.\nIn this paper, we propose CO2 which enables complete overlap of Comunication with Computation. This is made possible through the use of the local updating strategy and asynchronous communication, where each worker node independently optimizes its model parameters without requiring synchronization with other peers at the local updating stage. As shown in Fig. 1, to guarantee parameter consistency across all workers, model parameter synchronization is performed asynchronously after completing every \u03c4 local updates. By selecting appropriate local updating step \u03c4 in accordance with the communication environment, we can achieve full overlap of model parameter synchronization with multiple local computation steps, thereby attaining 100% scalability.\nMaintaining training stability as well as performance with asynchronous parameter updates is crucial for the success of our method. We introduce two additional techniques: staleness gap penalty and outer momentum clipping. The staleness gap penalty mechanism effectively quantifies and penalizes the discrepancy between different parameter versions during the update of outer momentum. This approach significantly contributes to the overall training stability and performance by addressing the inconsistencies in parameter updates. Similarly, outer momentum clipping serves as an essential tool to mitigate the emergence of anomalous values in the outer momentum. This helps prevent extreme values, thus further bolsters the training stability. In addition to our empirical findings, we establish robust theoretical convergence bounds. These bounds provide compelling evidence that our proposed framework is capable of achieving a convergence rate that is on par with that of baseline optimizers. It is worth noting that CO2 can seamlessly integrate with widely adopted ZeRO-series optimizers (Rajbhandari et al., 2020), which are well-established for their capacity to reduce memory consumption within data-parallel distributed training scenarios.\nWe evaluate CO2 by testing it on various tasks such as image classification, semantic segmentation, point cloud processing, autoregressive language modeling, and bidirectional language modeling. Our thorough assessment demonstrates that CO2 performs equally well in terms of convergence speed and generalization capabilities when compared to other optimization methods.\nOur primary contributions can be summarized as follows:\n\u2022 Outstanding scalability. CO2 enables outstanding scalability by fully overlapping communication with computation. This approach excels in large-scale multi-node clusters even with limited communication bandwidth. \u2022 Good convergence and generalization performance. We empirically demonstrate that CO2 achieves convergence and generalization performance that is close to well-established baseline optimizers, such as SGD and Adamw. Our results underscore the robustness of CO2 across a diverse range of tasks and datasets. \u2022 Theoretical convergence analysis. We provide a rigorous theoretical analysis that guarantees the convergence of CO2. This theoretical foundation enhances our understanding of the practical effectiveness of our approach. \u2022 Compatibility with ZeRO-series optimizers. CO2 can integrate with ZeRO-series optimizers to reduce memory usage for large-scale model training."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Local Updating. To our known, local updating in distributed deep learning can date back to Zhang et al. (2016). Subsequently, extensive research has delved into local updating methods. LocalSGD (Stich, 2018) provided rigorous theoretical convergence proof for local updating methods. Post-Local-SGD (Lin et al., 2018) and Overlap-Local-SGD (Wang et al., 2020) reduced communication costs but raised concerns about convergence. SlowMo (Wang et al., 2019) improved convergence stability and Cooperative SGD (Wang & Joshi, 2021) presented a unified framework of communication-efficient SGD algorithms. However, communication overhead remains a challenge. Our proposed CO2 introduces asynchronism to overlap parameter synchronization and local computation, achieving 100% scalability with good convergence performance.\nAsynchronism. There is a longstanding history of asynchronous distributed training techniques (Chen et al., 2016). Recently, Zhang et al. (2015); Barkai et al. (2019) have introduced methods that penalize gradient staleness in asynchronous SGD by employing well-defined gap metrics. Dutta et al. (2021) have delved into the trade-offs between staleness and convergence within the context of asynchronous training. Moreover, work like Cohen et al. (2021); Mishchenko et al. (2022), through rigorous theoretical proofs and empirical experiments, have established that asynchronous SGD can achieve convergence comparable to that of mini-batch SGD, even in the presence of arbitrary gradient delays. In a related vein, Su et al. (2022) proposed GBA as an approach to seamlessly transition between synchronous and asynchronous training, for recommendation models. Koloskova et al. (2022) provided enhanced convergence guarantees for asynchronous SGD in the context of distributed federated learning. These contributions collectively advance our understanding of asynchronous training techniques in distributed settings. Our method, as a comparison, only introduces one-step asynchronous staleness in the outer loop, thus obtaining more robust convergence and performance.\nEfficient Communication. To improve communication efficiency in synchronizing gradients in distributed training, previous methods often utilized quantization techniques to reduce the communication overhead, such as 1-bit Adam (Tang et al., 2021), 1-bit LAMB (Li et al., 2021), and 0/1 Adam (Lu et al., 2022). In a distinct vein, Assran et al. (2019) proposed stochastic gradient push to perform approximate distributed averaging at each iteration, which offers an alternative dimension for accelerating communication. These methods significantly enhance communication efficiency by mitigating communication overhead within a single iteration, which is orthogonal to our proposed method. More substantial benefits can be realized when integrating with these methods."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we start with an overview of our proposed CO2 and share two novel techniques that enhance its convergence and training stability. Additionally, we provide a mathematical proof of convergence, which proves that CO2 is capable of achieving a convergence rate that is comparable to those of previously widely used optimizers."
        },
        {
            "heading": "3.1 THE OVERVIEW",
            "text": "For distributed data-parallel training of a large-scale deep neural network, the target can be formulated as:\nmin x\n1\nG G\u2211 i=1 E\u03b6i\u223cDiL (i)(x; \u03b6i), (1)\nwhere the objective is to minimize a function with respect to the parameters x \u2208 Rn. The optimization problem involves a summation over G workers, each indexed by i. For each worker i, there are loss function L(i) and data samples \u03b6i drawn from the distribution Di.\nIn the routine DDP training, model parameters are replicated across all workers within the communication group. Each worker i is then provided with distinct batches of data samples and independently performs forward and backward passes to compute distinct losses L(i) and gradients g(i). To ensure uniformity of model parameters across all workers within the same DDP communication group, the gradients on all workers are exactly synchronized through an all-reduce operation, resulting in the aggregated gradient g, i.e., g = 1G \u2211G i=1 g\n(i). This aggregated gradient is then utilized to update the model parameters across all workers, ensuring consistency of parameter values across each worker.\nAlgorithm 1: CO2 Algorithm 1 Input: Data samples \u03b6(i) on worker i; Inner learning\nrate \u03b3t; Inner loop steps \u03c4 ; Outer learning rate \u03b1; Outer momentum factor \u03b2; Outer loop steps T ; Initial outer momentum m0 = 0.\n2 for t \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , T \u2212 1} do 3 for k \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , \u03c4 \u2212 1} on worker i do 4 FP & BP: g(i)t,k = \u2207L(i)(x (i) t,k; \u03b6 (i) t,k) 5 LU: x(i)t,k+1 = x (i) t,k \u2212 \u03b3tg (i) t,k 6 end 7 Launch AAR for x(i)t,\u03c4 : AAR(x (i) t,\u03c4 ) 8 if is_completed(AAR(x(i)t\u22121,\u03c4 )) is not True then 9 xt\u22121,\u03c4 = wait(AAR(x (i) t\u22121,\u03c4 ))\n10 end 11 Update staleness gap:\n\u039bt = \u2225xt,0\u2212xt\u22121,0\u2225\n\u03c4\u2225xt\u22121,1\u2212xt\u22121,0\u2225 + 1 n\n12 Update one-step stale outer momentum: mt = \u03b2mt\u22121 +\n1 \u039bt \u00b7 (xt\u22121,0 \u2212 xt\u22121,\u03c4 ) 13 Update outer iterates: xt+1,0 = xt,0 \u2212 \u03b1 \u00b7 Clip(mt, \u03d5) 14 end\nLocal-updating methods, such as Local-SGD or other variants, structure the overall training process into two discernible phases: the inner loop and the outer loop. The inner loop unfolds within each worker and encompasses localized updates. Within this phase, every worker maintains its dedicated set of model parameters, facilitating autonomous updates without the need for gradient synchronization at every step. Upon the completion of \u03c4 local updates, the training process transitions to the outer loop. In this phase, an all-reduce operation on model parameters is executed to exactly synchronize the model parameters across all computational workers, i.e., x = 1G \u2211G i=1 x (i) t,\u03c4 , ensuring convergence alignment. Recent work (Wang et al., 2019) introduces additional outer iterates to enhance the convergence of communicationefficient base optimizers, which has been proven effective.\nCO2 introduces asynchronous allreduce on model parameters within the outer loop to achieve full overlap between communication and computation throughout the entire training procedure. As delineated in Algorithm 1, during the outer iteration t, all workers independently perform local updates involving the FP, BP, and LU on their respective worker nodes. The inner loop iterates \u03c4 steps, from k = 0 to k = \u03c4 \u2212 1. Notably, during this process, no gradient synchronization takes place among the workers, resulting in distinct model parameters x(i)t,\u03c4 on each worker i at the conclusion of the inner loop. This necessitates a synchronization operation to harmonize the model parameters across all workers.\nConsequently, upon the completion of inner loop, CO2 launches an all-reduce operation on x(i)t,\u03c4 for each worker i to synchronize them, i.e., AAR(x(i)t,\u03c4 ). In contrast to conventional methods, this allreduce is asynchronous with subsequent computations, which means the subsequent computations are not dependent upon the results obtained by AAR(x(i)t,\u03c4 ). Instead, it leverages the outdated model parameters x(i)t\u22121,\u03c4 , which are subjected to a previous AAR operation in the preceding outer iteration t \u2212 1. It is worth noting that, at this moment, AAR(x(i)t\u22121,\u03c4 ) may not have finished. Therefore, a waiting operation wait(\u00b7) should be conducted if it is checked not completed, this guarantees the execution of AAR(x(i)t\u22121,\u03c4 ). The resulting xt\u22121,\u03c4 is subsequently employed for updating the stale outer momentum, which is then applied to perform an outer iterate to obtain the initial parameters xt+1,0 for next outer loop. For each respective outer loop, the stale outer momentum lags by only one step compared to using the new version of xt,\u03c4 . This strategy introduces minimal one-step staleness in the outer loop and has a subtle impact on network convergence.\nThe AAR operation runs in parallel with the subsequent outer updates as well as local computations in the next outer loop. Given the practical considerations of cluster sizes and variable communication conditions, configuring an appropriate number of local steps \u03c4 enables the seamless overlap of the entire asynchronous communication with computations."
        },
        {
            "heading": "3.2 STALENESS GAP PENALTY",
            "text": "CO2 achieves full overlap between communication and computation through one-step asynchronous communication, enhancing scalability up to 100% with an appropriate number of local steps. However,\nthe asynchronous nature adds noise to the optimization process, potentially causing discrepancies in training loss and generalization accuracy. To compensate, we propose a staleness gap penalty mechanism, which requires an accurate metric to quantify the staleness gap of outer momentum.\nRecall the formulation in (1), where fi(x) = E\u03b6i\u223cDiL(i)(x; \u03b6i) is the local objective function at worker i. Before constructing the staleness gap, we first assume that each fi(x) is L-Lipschitz as stated in Assumption 3.1.\nAssumption 3.1 For some existed L > 0, there is \u2225\u2207fi(x)\u2212\u2207fi(y)\u2225 \u2264 L\u2225x\u2212 y\u2225, for all inputs x, y \u2208 Rn and i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , G}.\nThe Lipschitz assumption stated above is intuitive, as it suggests that a viable alternative metric for precisely quantifying gradient differences is to consider the distinctions in model parameters. This insight serves as motivation for us to employ the model parameters from different versions as indicators of the staleness gap for outer momentum.\nTo quantify the staleness gap in outer momentum, a straightforward approach involves computing the difference in outer momentum between different iterations, such as iterations t + 1 and t, and normalizing this difference to assess its magnitude. Considering mt+1 = \u03b2mt + (xt,0 \u2212 xt,\u03c4 ) and mt = \u03b2mt\u22121 + (xt\u22121,0 \u2212 xt\u22121,\u03c4 ), the staleness gap of mt in comparison to mt+1 can be straightforwardly represented as:\n\u2225mt+1 \u2212mt\u2225 = \u2225\u03b2(mt \u2212mt\u22121) + (xt,0 \u2212 xt,\u03c4 )\u2212 (xt\u22121,0 \u2212 xt\u22121,\u03c4 )\u2225 \u2264 \u03b2\u2225(mt \u2212mt\u22121)\u2225+ \u2225xt,0 \u2212 xt\u22121,0\u2225+ \u2225xt,\u03c4 \u2212 xt\u22121,\u03c4\u2225. (2)\nIt is essential to emphasize that at the initiation of iteration t, the ongoing asynchronous all-reduce operation involving xt,\u03c4 is generally in progress and has may not reached completion. This situation implies that a globally averaged value for xt,\u03c4 , accessible for all workers, is unavailable. Considering the outcomes presented in (2), a suitable and readily accessible metric for measuring the staleness gap, denoted as \u039b, can be defined as:\nDefinition 3.1 The staleness gap of the outer momentum \u039bt \u2208 Rn at step t is quantified as the ratio of the model displacement within the outer loop to the maximum distance the parameters can traverse within the inner loop, in iteration t\u2212 1. This can be formally articulated as follows:\n\u039bt = \u2225xt,0 \u2212 xt\u22121,0\u2225\n\u03c4\u2225xt\u22121,1 \u2212 xt\u22121,0\u2225 + 1n. (3)\nIn (3), the term \u2225xt,0 \u2212 xt\u22121,0\u2225 denotes the magnitude of the model parameter changes within the outer loop at iteration t \u2212 1, which occurred with the presence of stale outer momentum. \u03c4\u2225xt\u22121,1 \u2212 xt\u22121,0\u2225 represents the maximum distance that the model parameters can traverse in inner steps without utilizing stale outer momentum. Typically, during the training process, both the gradient and the learning rate within the inner loop experience a decay, leading to a reduction in the model parameter distance within a single inner step. This behavior elucidates why \u2225xt\u22121,1 \u2212 xt\u22121,0\u2225 signifies the maximum distance achievable in a single inner step. The notation 1n signifies the computation of \u039b is parameter-wise, where n corresponds to the number of parameters. All computations within (3) are conducted element-wise. At each iteration, we recalculate the staleness gap by performing (3), and then penalize it when updating the outer momentum via: mt = \u03b2mt\u22121+ 1\u039bt \u00b7(xt\u22121,0\u2212xt\u22121,\u03c4 )."
        },
        {
            "heading": "3.3 OUTER MOMENTUM CLIPPING",
            "text": "Ensuring training stability is of paramount importance, particularly for large language models. We empirically find that the asynchronism introduced by CO2 can occasionally have adverse effects on training stability. To mitigate the potential issues related to unusual values in outer momentum and enhance overall training stability, we have implemented a coordinate-wise outer momentum clipping as Clip(\u03c7, \u03d5) = max{\u2212\u03d5,min{\u03c7, \u03d5}}, which is used to update the outer iterates as xt+1,0 = xt,0 \u2212 \u03b1 \u00b7 Clip(mt, \u03d5), where \u03d5 denotes the clipping threshold, which satisfies \u03d5 > 0."
        },
        {
            "heading": "3.4 CONVERGENCE ANALYSIS",
            "text": "Considering the staleness introduced by our approach, it is imperative to establish its convergence guarantee from a theoretical standpoint. To do so, we begin by presenting the following assumptions, which align with standard practices in this domain.\nAssumption 3.2 For all i \u2208 {1, 2, . . . , G}, there exists a finite positive constant \u03c32 such that E\u03b6\u223cDi \u2225\u2225\u2225\u2207L(i)(x; \u03b6)\u2212\u2207fi(x)\u2225\u2225\u22252 \u2264 \u03c32, i.e., the variance of fi(x) is bounded. Assumption 3.3 There exists a finite positive constant V such that E \u2225gt,k \u2212 E [gt,k]\u22252 \u2264 V .\nUnder these assumptions, our convergence results are given as below, where the detailed proof can be found in Appendix A.4.\nTheorem 1 If we take \u03bb = 1/\u039bt, \u03b3t = \u03b3 and ignore the clip operation, such that \u03bb\u0304 = \u03b1\u03bb, \u03bb\u0304\u03b31\u2212\u03b2 =\u221a G T\u03c4 and T\u03c4 \u2265 GL2 ( 1 + \u221a 3max { 3\u03c4(1\u2212\u03b2\u2212\u03b1) \u03b1 , 4\u03c4\u03b2 1\u2212\u03b2 , 1 }) , then under Assumptions 3.1, 3.2, 3.3 and\n1 G \u2211G i=1 \u2225\u2207f(x)\u2212\u2207fi(x)\u2225 2 \u2264 \u03b42, where \u03b4 is a positive finite constant, we have:\n1 T\u03c4 \u2211T\u22121 t=0 \u2211\u03c4\u22121 k=0 E \u2225\u2207f (xt,k)\u2225 2 = O ( 1\u221a GT\u03c4 ) +O ( G\u03c4 T ) , (4)\nwhere f = 1G \u2211G\ni=1 fi(x). The theorem indicates that, when the total steps T\u03c4 is sufficiently large, i.e, T \u226b G3\u03c43, the RHS is dominated by O ( 1\u221a GT\u03c4 ) . So, when the number of workers is G times more, we only need G times less total steps to achieve the same error."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We conducted a thorough assessment of CO2 in a range of deep learning applications spanning computer vision (CV) and natural language processing (NLP). Our testing encompassed an array of model architectures, including convolution networks, transformer networks, and linear transformer networks. For more specific information regarding the tasks, models, their categorizations, parameter counts, and datasets used, please refer to Table 5 in Appendix A. It is worth noting that the TransNormer-LLM (7B) experiment mainly aims to evaluate CO2\u2019s scalability, so we only pre-train it on a relatively small dataset WikiText-103 to assess the convergence."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "We compared CO2 against other state-of-the-art optimization techniques, including SGD, Adamw, Local-SGD/Adamw, Overlap-Local-SGD/Adamw, and SlowMo. Hyperparameters for each approach were meticulously tuned for maximum performance. For CO2, we conducted an extensive hyperparameter search for \u03c4 , ranging from {1, 3, 6, 12, 24, 48, 96, 192} to find the optimal balance between efficiency and performance. All experiments were executed five times with distinct random seeds to ensure robust results. For hardware, software, and hyperparameter details, see Appendix A.1 and A.2.\nImage Classification (IC). We train ResNet-50 (He et al., 2016), ViT (Dosovitskiy et al., 2020), and VVT (Sun et al., 2023) on ImageNet-1K dataset, using eight A100 nodes with 64 GPUs. For the training of ResNet-50, we use a total mini-batch size of 8192 and train for 90 epochs with a cosine learning rate schedule. For the training of ViT and VVT, they closely adhere to analogous hyperparameters. We use an uniform total batch size of 2048 for both and train for 300 epochs.\nSemantic Segmentation (SS). We leverage the pre-trained VVT on ImageNet-1K as the backbone model, adopt semantic feature pyramid network (Semantic FPN) (Lin et al., 2016) as the decoder and finetune on the ADE20K dataset, using four 3090 nodes with 32 GPUs.\n3D Point Cloud (PC) Reconstruction. We pre-train the Point-MAE (Pang et al., 2022; Li et al., 2023) via reconstruction task using ShapeNet dataset (Chang et al., 2015). We use a point number of 1024, a total batch size of 256, epoch number of 300, train on four 3090 nodes with 32 GPUs."
        },
        {
            "heading": "ViT",
            "text": "Autoregressive Language Modeling (ALM). For ALM tasks, we train autoregressive GPT-2 models with 125M (Small), 355M (Medium), and 770M (Large) parameters on the OpenWebText dataset (Radford et al., 2019). All experiments use a context length of 1024 and a total batch size of 480, training on eight A100 nodes with 64 GPUs. Additionally, we train the TransNormer-LLM (7B) model on WikiText-103 for 100K iterations on sixteen A100 nodes with 128 GPUs, in order to verify the performance of CO2 on large language models. The ZeRO-2 optimizer is integrated with CO2 to lessen memory redundancy during training TransNormer-LLM (7B).\nBidirectional Language Modeling (BLM). For BLM tasks, we train a RoBERTa (Large) (Liu et al., 2019) model on a combined dataset consisting of Wikipedia and BookCorpus (Wettig et al., 2022) for approximately 3 epochs, using a total batch size of 4096 and sequence length of 128, train on four 3090 nodes with 32 GPUs."
        },
        {
            "heading": "4.2 CONVERGENCE AND GENERALIZATION",
            "text": "Computer vision results. Table 1 shows the results of convergence and generalization in computer vision tasks. CO2 optimizer consistently outperforms other efficient optimization techniques in terms of Top-1 validation accuracy for ResNet-50, ViT, and VVT architectures. Although CO2 has a slightly lower accuracy than the Adamw for both ViT and VVT, the reduction is marginal and falls within the expected range of random fluctuations. In addition to its good performance, CO2 achieves more efficient communication, including the potential for zero communication delay compared to the baseline methods. A similar trend is observed for VVT in semantic segmentation tasks. CO2 demonstrates a notable performance advantage over (Overlap-)Local-SGD and SlowMo, with only a slight performance gap of 0.02% when compared to Adamw. For 3D point cloud reconstruction, our proposed CO2 outperforms all other counterparts, exceeds the baseline Adamw by 0.33%.\nWe also present validation curves that track the training progress of these models on image classification. As shown in Fig. 2, CO2 features a more stable validation curve for ResNet-50 and maintains consistently higher accuracy curves compared to (Overlap-)Local-Adamw and SlowMo for ViT and VVT.\nNatural language processing results. Table 2 and Fig. 3 presents the convergence and generalization outcomes for natural language processing tasks. The table shows that CO2 consistently outperforms alternative methods across GPT-2 variants (Small, Medium, and Large) for ALM tasks, resulting in lower validation perplexity values as the model size scales. Furthermore, CO2 achieves the best training perplexity on the large TransNormer-LLM (7B) model, when trained on a relatively small"
        },
        {
            "heading": "GPT-2 (Medium)",
            "text": "dataset. We also give validation curves with respect to relative time (in seconds) for GPT-2 models of varying sizes as illustrated in Fig. 7, Appendix A.3.\nFor BLM tasks, CO2 demonstrates comparable performance to the standard Adamw and SlowMo algorithms, surpassing (Overlap-)Local-Adamw by a significant margin. These results emphasize the capacity of CO2 to optimize encoder-only transformer models without compromising accuracy."
        },
        {
            "heading": "4.3 SCALABILITY",
            "text": "We tested our proposed CO2 using up to 16 DGX-A100 servers to train TransNormer-LLM with 7B parameters. Each server has 8 A100 GPUs interconnected via NVSwitch for an inter-GPU bandwidth of 600GBps. We evaluated CO2\u2019s scalability under different cluster communication conditions using two kinds of inter-node network configurations. The first uses RoCE solution with 8 RDMA adapters per server for inter-server communication at 800Gbps. The second uses TCP/IP Ethernet with significantly lower bandwidth at around 80Gbps. Note that, to mitigate fluctuations, the presented throughput results are averaged values extracted from iterations 100 to 200 for each experiment.\nWe evaluated two methods, Adamw and CO2, on RoCE RDMA and TCP/IP networks. On the RoCE RDMA network, CO2\u2019s communication advantage became prominent in larger clusters (more than 64 GPUs) resulting in higher throughput than Adamw. On the TCP/IP network, CO2 performed similarly to its performance on the RoCE RDMA network when \u03c4 was set to 48. However, Adamw performed poorly due to pronounced communication latency. We also test the influence of \u03c4 on communication efficiency, with varying levels of overlap. Results are shown in Fig. 4(a).\nUsing CO2 with TCP/IP and \u03c4 = 12 caused a moderate reduction in speed and scalability, but it still outperformed Adamw on TCP/IP due to overlap between communication and computation. Transitioning from 8 GPUs to 16 GPUs caused a notable drop in throughput values for both CO2 and Adamw due to the diminished inter-node connectivity.\nTable 3 shows throughput and scalability of CO2 and Adamw. Scalability ratios were calculated for different communication scenarios, specifically when transitioning from 16 GPUs to 128 GPUs. CO2 exhibits scalability ratios exceeding 1, mainly due to inherent measurement fluctuations."
        },
        {
            "heading": "4.4 ABLATION STUDY",
            "text": "We conducted ablation studies on CO2 using a small-scale GPT-2 (Small) model across four servers, each with eight 3090 GPUs. The experiments lasted for 100K steps.\nPerformance and scalability w.r.t. \u03c4 . We conducted experiments to examine time per iteration and validation perplexity across different values of \u03c4 . Increasing \u03c4 generally leads to faster training but higher perplexity. However, at \u03c4 = 12, computation and communication overlap, resulting in a significant acceleration. Larger \u03c4 values benefit from reduced communication frequency. \u03c4 = 12 also yields a slightly lower perplexity, which can be attributed to the regularization effect.\nStaleness gap penalty and outer momentum clipping. Table 4 shows that incorporating the staleness gap penalty has a more significant positive impact than outer momentum clipping. The latter mainly targets training stability rather than performance improvement."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We proposed CO2, a method for large-scale distributed training on clusters with low-cost and limitedspeed interconnections. CO2 enables exceptional scalability through local-updating and one-step asynchronous communication. Mathematical analysis is provided on its convergence rate. Our experiments in computer vision and natural language processing demonstrate CO2\u2019s ability to achieve 100% scalability even on clusters with very limited communication bandwidth. We have enhanced CO2\u2019s convergence and stability through the introduction of two techniques: the staleness gap penalty and outer momentum clipping. CO2 is highly compatible with established optimizers and can be integrated with ZeRO-series optimizers to reduce memory consumption for large model training."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "Given the widespread adoption of large language models trained on extensive corpora, the significance of distributed training across expansive clusters has grown substantially. Inefficient training practices result in substantial computational resource wastage, leading to elevated economic costs and an increased emission of CO2, a significant contributor to the global greenhouse effect. In the context of distributed training for neural networks on GPU clusters, efficient communication plays a pivotal role. However, existing methods still grapple with low communication efficiency, particularly on modern, large-scale GPU clusters characterized by limited inter-node connections. Our proposed approach addresses this challenge by advocating for the full overlap of communication and computation, thereby substantially expediting model training even in scenarios with severely restricted communication capabilities. This not only reduces training time but also translates into cost savings and environmental benefits, as it contributes to mitigating the impact on the environment.\nIn addition, our proposed method is relatively general and not limited to specific models or tasks. It can be well extended to other tasks not included in this paper, such as stable diffusion for image generation, object detection for autonomous driving, etc."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "For the sake of facilitating reproducibility in our experiments, we provide a comprehensive listing of the hardware and software components employed in our study. This information is thoughtfully presented in Appendix A and encompasses details such as the specifications of GPUs, inter-node communication configurations, and the versions of critical software components including PyTorch, CUDA, cuDNN, and NCCL. Additionally, we give links to the open-source code repositories that were instrumental in the execution of experiments across a spectrum of tasks in the paper. These concerted efforts reinforce the transparency and replicability of our research."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A EXPERIMENT DETAILS",
            "text": "We show specific experiment plans regarding the tasks, models, their categorizations, parameter counts, and datasets used in Table .5."
        },
        {
            "heading": "A.1 HARDWARE AND SOFTWARE",
            "text": "Hardware. Our experimental setup comprises up to 16 DGX-A100 servers, with each server featuring 8 A100 GPUs. These GPUs are interconnected via NVSwitch, providing an inter-GPU bandwidth of 600GBps. Two distinct inter-node network configurations are used. The first configuration leverages RoCE (RDMA over Converged Ethernet) solution, employing 8 RoCE RDMA adapters in each server to facilitate inter-server communication with a bandwidth capacity of 800Gbps. The second configuration employs the TCP/IP protocol which operates at a substantially reduced bandwidth compared to RoCE. Specifically, it offers approximately 1/10th the bandwidth, totaling around 80Gbps. Part of experiments are conducted on a 3090 GPU cluser with total 10 servers. Each server is equipmented by eight 3090 GPUs.\nSoftware. Experiments are implemented in PyTorch 1.13.0 with CUDA 11.7, cuDNN 8.0, and NCCL 2.14.3. Our algorithm is developed upon FairScale 0.4.13. The image classification experiments build on https://github.com/huggingface/pytorch-image-models and https://github.com/ OpenNLPLab/Vicinity-Vision-Transformer. The semantic segmentation experiments build on https://github.com/whai362/PVT. The 3D point cloud reconstruction experiments build on https: //github.com/Pang-Yatian/Point-MAE. The autoregressive language modeling experiments build on https://github.com/karpathy/nanoGPT and https://github.com/facebookresearch/metaseq. The bidirectional language modeling experiments build on https://github.com/princeton-nlp/DinkyTrain."
        },
        {
            "heading": "A.2 HYPERPARAMETER DETAILS",
            "text": "We detail hyperparameter settings for all tasks in the following:\nImage Classification. The image classification experiments are conducted on ImageNet1K dataset, which contains 1.28M training images and 50K validation images from 1, 000 categories. We adopt ResNet-50 (He et al., 2016), ViT (Dosovitskiy et al., 2020), and VVT (Sun et al., 2023), which respectively correspond to convolutional network, transformer network, and linear transformer network. For the training of ResNet-50, the total mini-batch size is 8192, and the training runs 90 epochs. We sourced hyperparameters from DeepLearningExamples 1. The momentum value is set to\n1https://github.com/NVIDIA/DeepLearningExamples\n0.875 and the learning rate is initialized to 0.256 for a batch size of 256, the value is linearly scaled for batch sizes distinct from this reference. The learning rate schedule adheres to a cosine schedule, while a linear warm-up mechanism for the learning rate is introduced for the initial 5 epochs. We set the weight decay to 1/32768 and we do not apply weight decay on batch normalization trainable parameters. A label smoothing coefficient of 0.1 is adopted to enhance model robustness.\nFor the training of ViT and VVT, they closely adhere to analogous hyperparameters. A uniform total batch size of 2048 is employed, sustained across 300 epochs. A cosine learning rate schedule supplemented by a linear warm-up during the initial 5 epochs is incorporated. A label smoothing coefficient of 0.1 is applied, while weight decay is set at 0.05. We use an initial learning rate of 5\u00d710\u22124, this value diminishes with a cosine schedule, with 5 epochs dedicated to the warm-up phase. The augmentation follows previous literature, which includes practices such as random cropping and random horizontal flipping. Throughout the training, all models undergo 300 epochs of training on the training set, adopting a crop size of 224 \u00d7 224. The evaluation metric is the top-1 accuracy.\nSemantic Segmentation. For the semantic segmentation task on the ADE20K dataset, the pretrained ViT and VVT models undergo a finetuning process. The ADE20K dataset is composed of 150 distinct classes, distributed across 20210, 2000, and 3352 images for training, validation, and testing, respectively. We leverage the ViT and VVT models as the foundational backbone, already pre-trained on ImageNet1K. we adopt the Semantic Feature Pyramid Network (Semantic FPN) (Lin et al., 2016) as the decoder. Hyperparameters governing the training of both ViT and VVT models are determined with reference to VVT.\n3D Point Cloud Reconstruction. We trained the Point-MAE (Pang et al., 2022; Li et al., 2023) via the reconstruction task using ShapeNet dataset (Chang et al., 2015). We further adopt the Chamfer distance as reconstruction loss, as well as a point number of 1024, a batch size of 256, epoch number of 300, cosine learning rate schedule, Adamw optimizer (Loshchilov & Hutter, 2018) with a learning rate of 0.0005, and weight decay of 0.05. The distributed training performance is evaluated not only by the training loss but also by the accuracy in the validation dataset of ModelNet40 (Wu et al., 2015). Detailed reconstruction pre-training and validation settings may be found in Pang et al. (2022); Li et al. (2023).\nAutoregressive Language Modeling. For ALM tasks, we trained autoregressive GPT-2 models (Radford et al., 2019) on OpenWebText dataset (Radford et al., 2019). Three GPT-2 models with 125M (Small), 355M (Medium), and 770M (Large) parameters are tested to prove the scalability of CO2 . All experiments use the context length of 1024 and use a total batch size of 480. In addition, to further validate the scalability and efficiency of CO2 , we conducted experiments a large efficient language model, namely TransNormer-LLM (7B) for 100k steps on small dataset WikiText-103. ZeRO-series optimizers are integrated with CO2 to lessen memory redundancy during training TransNormer-LLM (7B).\nBidirectional Language Modeling. For BLM tasks, we trained a RoBERTa (Large) model on a combined dataset with Wikipedia 2 and BookCorpus (Zhu et al., 2015) for approximately 3 epochs. We utilize a total batch size of 4096 and sequence length of 128. A polynomial learning rate decay scheduler was employed, with a peak learning rate of 2e-3 after 6% of total updates as warm-up."
        },
        {
            "heading": "A.3 ADDITIONAL EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "A.3.1 TRAINING CONVERGENCE CURVES",
            "text": "In main paper, we show validation accuracy curves on classification task of three models, i.e., ResNet50, ViT and VVT, in Fig. 2, we show their respective training loss curves in Fig. 5. Similarly, Fig.3 demonstrates the validation perplexity curves of GPT-2 with different sizes (Small, Medium and Large) on autoregressive language tasks. We show their respective curves of training perplexity in Fig. 6. These curves further validate the effectiveness of the proposed CO2.\n2https://dumps.wikimedia.org"
        },
        {
            "heading": "A.3.2 TIME TO ACCURACY CURVES",
            "text": "In order to highlight the benefits of CO2 in accelerating the training process, we give validation curves with respect to relative time (in seconds) for GPT-2 models of varying sizes as illustrated in Fig. 7. The outcomes demonstrate that CO2 excels not only in achieving superior generalization performance but also in minimizing the duration of the training process, thereby affirming its practical utility in distributed training."
        },
        {
            "heading": "A.3.3 CONVERGENCE RESULTS WITH MORE DETAILS",
            "text": "To show advantages of CO2 on the trade-off of training efficiency and convergence performance, we list the corresponding throughput and settings of \u03c4 for both CV and NLP tasks in Table 6 and Table 7. The presented throughput results represent averaged values extracted from iterations 100 to 200 for each experiment. Given the extensive nature of our experiments, convergence outcomes in the paper originate from two distinct clusters. Specifically, the IC and ALM tasks were executed on\nan A100 cluster equipped with RoCE RDMA high-speed inter-node connections, featuring 8 nodes and a total of 64 GPUs. Conversely, the SS, PC, and BLM tasks were conducted on a 3090 cluster with a standard TCP/IP Ethernet inter-node connection, comprising 4 nodes and 32 GPUs. Upon observing tasks trained on the A100 platform, it is evident that the throughput of CO2 surpasses other counterparts, although with a slight advantage. This advantage becomes more pronounced on tasks trained on the 3090 platform. It is reasonable to infer that this advantage will be amplified on larger clusters, especially those with slower inter-node connections.\nPrior to commencing large-scale convergence experiments, we meticulously tune the values of \u03c4 for each task using a simple grid search strategy within the range of {1, 3, 6, 12, 24, 48, 96, 192} to reach a balance of accuracy and throughput. Hyperparameter tuning experiments are conducted on smaller versions of models when the model to be trained is large, owing to the associated high time and resource costs. We start the tuning of \u03c4 from 1 to larger candidate values given the consideration of high-speed communication on the corresponding platform. It is worth noting that, in addition to CO2, other methods such as Local-SGD/Adamw and SlowMo also require tuning of \u03c4 . In practice, we exclusively tune \u03c4 for CO2 and employ the same tuned \u03c4 values for Local-SGD/Adamw and SlowMo. This is because of the similar role played by \u03c4 in these local-updating methods. Employing identical \u03c4 values for different methods on the same task ensures a fair comparison of their convergence and training throughput."
        },
        {
            "heading": "A.3.4 COMMUNICATION-COMPUTATION OVERLAP RATIO",
            "text": "We conducted quantitative measurements on the time allocated to local computations and asynchronous communication. This assessment specifically targeted CO2 with different \u03c4 configurations on an A100 cluster equipped with 128 GPUs, employing a slower TCP/IP inter-node network. Adhering to the settings outlined in the paper, utilizing TransNormer-LLM (7B), we maintained consistency in our experimental conditions. The measured duration for a single-step local computation was approximately 0.109s, while an all-reduce communication incurred a cost of 1.566s. Subsequently, we computed and calculated the communication/computation overlap ratio for various \u03c4 values, presenting the results in the Table 8."
        },
        {
            "heading": "A.3.5 SCALABILITY RESULTS ON GPT-3 (13B)",
            "text": "To further effectively demonstrate the scalability advantages of CO2 on large models, we conducted experiments specifically on GPT-3 (13B), a well-known language model with autoregressive transformer architecture. The results are shown in the Table 9, includes throughput comparisons with SlowMo. The results reveal that on GPT-3 (13B), CO2 consistently achieves higher throughput on platforms with different communication conditions."
        },
        {
            "heading": "A.3.6 PENALTY PERFORMANCE ON NON-DECAY LEARNING RATE",
            "text": "To evaluate the performance of staleness gap penalty on non-decay learning rate, we conducted a comparative experiment on GPT-2 (Small) using the CyclicLR schedule provided in PyTorch with the triangular2 policy 3, training for 100K steps. The test results and their comparisons with CosineAnnealingLR are summarized in the table 10.\nThe experimental outcomes consistently affirm the favorable impact of the staleness gap penalty technique on training performance, regardless of the choice between CosineAnnealingLR and CyclicLR. While the enhancements in PPL may not be substantial, they exhibit a reliable positive trend. It is noteworthy that PPL values using CyclicLR surpass those using CosineAnnealingLR, potentially owing to sub-optimal learning rate tuning. Despite the cyclic nature of the learning rate in CyclicLR, the staleness gap penalty continues to exert its positive influence. This could be attributed to the periodic decrease in the learning rate during CyclicLR\u2019s training iterations, facilitating the efficacy of the\n3https://github.com/bckenstler/CLR\nstaleness gap penalty. It is pertinent to acknowledge that both tested schedules incorporate a learning rate warm-up during the initial 4000 iterations, which also represents a form of increasing learning rate. From a technical standpoint, we posit that there may exist specific learning rate schedules where the staleness gap penalty technique may not exhibit optimal performance. In such instances, we advocate exploring alternative decaying learning rate schedules or considering the option to disable the staleness gap penalty."
        },
        {
            "heading": "A.4 CONVERGENCE PROOF",
            "text": "The convergence proof is conducted for a constant learning rate, i.e., \u03b3t = \u03b3 and constant staleness gap, i.e., \u03bb = 1/\u039bt, and we ignore the outer momentum clipping operation to make the analysis easy to follow. Besides, we follows the standard assumptions (Wang et al., 2019) as below:\nAssumption A.1 For some existed L > 0, there is \u2225\u2207fi(x)\u2212\u2207fi(y)\u2225 \u2264 L\u2225x\u2212 y\u2225, for all inputs x, y \u2208 Rn and i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , G}.\nAssumption A.2 For all i \u2208 {1, 2, . . . , G}, there exists a finite positive constant \u03c32 such that E\u03b6\u223cDi \u2225\u2225\u2207L(i)(x; \u03b6)\u2212\u2207fi(x)\u2225\u22252 \u2264 \u03c32, i.e., the variance of fi(x) is bounded. Assumption A.3 There exists a finite positive constant V such that E \u2225gt,k \u2212 E [gt,k]\u22252 \u2264 V .\nUnder these assumptions, the momentum update rule in Algorithm 1 becomes:\nmt = \u03b2mt\u22121 + \u03bb(xt\u22121,0 \u2212 xt\u22121,\u03c4 ). (5) Let us define:\ngt,k = 1\nG G\u2211 i=1 g (i) t,k. (6)\nNote that the local update rule is:\nx (i) t,k+1 = x (i) t,k \u2212 \u03b3g (i) t,k. (7)\nSo we have:\nxt,k+1 = 1\nG G\u2211 i=1 x (i) t,k+1\n= 1\nG (\u2211 i x (i) t,k \u2212 \u2211 i \u03b3g (i) t,k ) = xt,k \u2212 \u03b3gt,k = xt,k\u22121 \u2212 \u03b3(gt,k\u22121 + gt,k) = xt,0 \u2212 \u03b3 k\u2211\nj=0\ngt,j .\n(8)\nThe momentum update rules then becomes:\nmt = \u03b2mt\u22121 + \u03bb(xt\u22121,0 \u2212 xt\u22121,\u03c4 ) = \u03b2mt\u22121 + \u03bb\u03b3 \u03c4\u22121\u2211 k=0 gt\u22121,k. (9)\nThe outer update rule becomes:\nxt+1,0 = xt,0 \u2212 \u03b1mt\n= xt,0 \u2212 \u03b1\u03b2mt\u22121 \u2212 \u03b1\u03bb\u03b3 \u03c4\u22121\u2211 k=0 gt\u22121,k\n= xt,0 \u2212 \u03b1\u03bb\u03b3 \u03c4\u22121\u2211 k=0 gt\u22121,k + \u03b2(xt,0 \u2212 xt\u22121,0).\n(10)\nWe define:\nyt,0 = xt,0 + \u03b2\n1\u2212 \u03b2 (xt,0 \u2212 xt\u22121,0) . (11)\nSo we have:\nyt+1,0 \u2212 yt,0 = xt+1,0 \u2212 xt,0 + \u03b2\n1\u2212 \u03b2 (xt+1,0 \u2212 xt,0)\u2212\n\u03b2\n1\u2212 \u03b2 (xt,0 \u2212 xt\u22121,0)\n= 1\n1\u2212 \u03b2 (xt+1,0 \u2212 xt,0 \u2212 \u03b2 (xt,0 \u2212 xt\u22121,0))\n= \u2212 \u03b1\u03bb\u03b3 1\u2212 \u03b2 \u03c4\u22121\u2211 k=0 gt\u22121,k.\n(12)\nWe then define yt,\u03c4 = yt+1,0 and\nyt,k+1 \u2212 yt,k = \u2212 \u03b1\u03bb\u03b3\n1\u2212 \u03b2 gt\u22121,k \u225c \u2212\n\u03bb\u0304\u03b3\n1\u2212 \u03b2 gt\u22121,k. (13)\nSince fi is L-Smooth, f = 1G \u2211G\ni=1 fi is also L-Smooth. then according to L-Smooth of f and the Descent Lemma in Bauschke et al. (2017), we have\nf(yt,k+1) = f ( yt,k \u2212 \u03bb\u0304\u03b3\n1\u2212 \u03b2 gt\u22121,k ) \u2264 f(yt,k)\u2212 \u03bb\u0304\u03b3\n1\u2212 \u03b2 \u27e8\u2207f (yt,k) , gt\u22121,k\u27e9+\n\u03bb\u03042\u03b32L\n2(1\u2212 \u03b2)2 \u2225gt\u22121,k\u22252.\n(14)\nTaking expectation, then we have\nE[f(yt,k+1)] \u2264 f(yt,k)\u2212 \u03bb\u03b3\n1\u2212 \u03b2 \u27e8\u2207f (yt,k) ,E[gt\u22121,k]\u27e9+\n\u03bb\u03042\u03b32L 2(1\u2212 \u03b2)2 E [ \u2225gt\u22121,k\u22252 ] (15)\nAccording to (15), and we define finf = infx f(x) (Wang et al., 2019). We have:\n1\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0 E \u2225\u2207f (xt,k)\u22252\n\u2264 2 (f (x0,0)\u2212 finf ) +mV L\u221a GT\u03c4 + 1 T\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0 E \u2225\u2207f (xt,k)\u2212 Et,k [gt\u22121,k]\u22252\n+ 4GV L2(\u03c4 \u2212 1)\nT\u03c4\n( 1\u2212 \u03b2 \u03bb\u0304 \u2212 1 )2 + 8GV L2\u03c4 T\u03c4\n\u03b22\n(1\u2212 \u03b22)\n\u2264 2 (f (x0,0)\u2212 finf ) +mV L\u221a GT\u03c4\n+ 1\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0 E \u2225\u2207f (xt,k)\u2212 Et,k [gt,k] + Et,k [gt,k]\u2212 Et,k [gt\u22121,k]\u22252\n+ 4GV L2(\u03c4 \u2212 1)\nT\u03c4\n( 1\u2212 \u03b2 \u03bb\u0304 \u2212 1 )2 + 8GV L2\u03c4 T\u03c4\n\u03b22\n(1\u2212 \u03b22)\n\u2264 2 (f (x0,0)\u2212 finf ) +mV L\u221a GT\u03c4\n+ 1\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0\nE \u2225\u2207f (xt,k)\u2212 Et,k [gt,k]\u22252\ufe38 \ufe37\ufe37 \ufe38 Effect of Local SGD\n+ 2\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0 E [ (\u2207f (xt,k)\u2212 Et,k [gt,k])\u22a4 (Et,k [gt,k]\u2212 Et,k [gt\u22121,k]) ] \ufe38 \ufe37\ufe37 \ufe38\nEffect of cross term\n+ 1\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0\nE \u2225Et,k [gt,k]\u2212 Et,k [gt\u22121,k]\u22252\ufe38 \ufe37\ufe37 \ufe38 Effect of asynchronous updates\n+ 4GV L2(\u03c4 \u2212 1)\nT\u03c4\n( 1\u2212 \u03b2 \u03bb\u0304 \u2212 1 )2 + 8GV L2\u03c4 T\u03c4\n\u03b22\n(1\u2212 \u03b22)\ufe38 \ufe37\ufe37 \ufe38 Effect of outer momentum .\n(16)\nNote that Et,k [gt,k] = Et,k [gt\u22121,k], so the cross-term and asynchronous-term eliminate, which results:\n1\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0 E \u2225\u2207f (xt,k)\u22252 \u2264 2 (f (x0,0)\u2212 finf ) +mV L\u221a GT\u03c4\n+ 1\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0\nE \u2225\u2207f (xt,k)\u2212 Et,k [dt,k]\u22252\ufe38 \ufe37\ufe37 \ufe38 Effect of Local SGD\n+ 4GV L2(\u03c4 \u2212 1)\nT\u03c4\n( 1\u2212 \u03b2 \u03bb\u0304 \u2212 1 )2 + 8GV L2\u03c4 T\u03c4\n\u03b22\n(1\u2212 \u03b22)\ufe38 \ufe37\ufe37 \ufe38 Effect of outer momentum\n(17)\nThen, we finally have:\n1\nT\u03c4 T\u22121\u2211 t=0 \u03c4\u22121\u2211 k=0 E \u2225\u2207f (xt,k)\u22252 = O ( 1\u221a GT\u03c4 ) +O ( G\u03c4 T ) . (18)"
        },
        {
            "heading": "B DISCUSSIONS",
            "text": ""
        },
        {
            "heading": "B.1 DISTINCTIONS WITH EXISTED METHODS",
            "text": "The stale-synchronous parallel (SSP) framework (Ho et al., 2013; Tang et al., 2020) aims to mitigate the straggler problem with relaxed synchronization. Specifically, SSP allows faster workers to perform more updates than slower ones, reducing the waiting time of faster workers. However, to maintain model consistency and ensure convergence, SSP imposes a staleness bounded barrier that limits the iteration gap between the fastest and slowest workers. Comparing with CO2, their differences mainly lie in these three aspects: 1) The SSP framework is built upon the Parameter-Server architecture. While CO2 as well as the Local-SGD and SlowMo algorithms investigated in the paper all based on the All-Reduce architecture. 2) The SSP-like algorithms indeed introduces asynchronization into its training procedure, however, it does not allow the communication to be overlapped with computations. And its staleness is not punished, which will leads unsatisfactory convergence performances. As comparison, the asynchronous communication in CO2 makes the communication can be overlapped with multiple-step local updates, which hugely improves the training efficiency. And the staleness gap penalty technique allows to mitigate the negative effects on convergence results. These imply the scaling efficiency and convergence performance of SSP-like algorithms will be worse than CO2 in practice. 3) The convergence analysis of SSP framework indicates that the convergence rate of SSP in normal cases is O( 1\u221a\nT ) . Comparing with CO2, when the total steps T\u03c4 is sufficiently large, i.e, T \u226b G3\u03c43, the convergence rate of CO2 is O (\n1\u221a GT\u03c4\n) . So theoretically, the convergence rates of\nthese two algorithms are similar.\nThe Cooperative SGD (Wang & Joshi, 2021) aims to unify a variety of local-update SGD algorithms, such as local SGD, elastic averaging SGD, and decentralized parallel SGD, under the proposed unified Cooperative SGD framework. It also provides a unified convergence analysis for the methods under this framework. As one of the by-products, it proposes to add an auxiliary variable z on the elastic averaging SGD, which serves as a local copy of the model. It can be seen as a preliminary approach that enables the asynchronous communication of model averaging to overlap with computation. However, our method leverages more sophisticated asynchronous communication design to improve the convergence. The differences between our method and the Cooperative SGD are threefold: 1) The CO2 algorithm is built upon SlowMo, which involves slow momentum and outer updates to improve the convergence; 2) CO2 leverages more sophisticated asynchronous communication design together with the outer momentum and outer updates to hide more communications; 3) CO2 also introduces staleness gap penalty and outer momentum clipping to improve the convergence as well as training stability.\nThe Overlap-Local SGD (Wang et al., 2020) method is also able to overlap communication by local computations via adding an anchor model on each node. The distinctions between CO2 and Overlap-Local-SGD are threefold: 1) Overlap-Local-SGD achieves communication-computation overlap on the top of naive Local-SGD, whose convergence performance is normally not good enough. On the other hand, the CO2 is built on SlowMo, which involves outer loop includes outer momentum and outer updates to improve the convergence of native Local-SGD. 2) As mentioned by the reviewer, CO2 introduced staleness gap penalty and outer momentum clipping to prevent divergence and improve training stability, which has been verified effective via extensive experiments. 3) The Overlap-Local-SGD is only tested on the CIFAR-10 image classification task using ResNet18, while CO2 has been widely verified on multiple tasks on both CV and NLP fields, including image classification, semantic segmentation, 3D point cloud reconstruction, autoregressive language modeling and bidirectional language modeling."
        },
        {
            "heading": "B.2 MEMORY OVERHEADS OF CO2",
            "text": "As an asynchronous method, CO2 leverages the advantages of a high overlap ratio facilitated by asynchronous communication, while contending with an augmented memory footprint due to the one-step asynchronous delay. Here we present a preliminary comparative analysis of the memory footprint of the methods discussed in the paper, using the widely adopted Adamw optimizer as the baseline and excluding considerations of mixed precision. Local-Adamw, a variant of Adamw, conducts local forward and backward passes and updates parameters without synchronizing gradients. It shares the same memory footprint as Adamw but exhibits lower communication frequency and\ndiminished accuracy performance. SlowMo, can be seen as a variant of Local-Adamw, introduces slow momentum calculation and outer model updates to enhance accuracy. Despite maintaining the same communication frequency as Local-Adamw, SlowMo incurs an additional memory footprint (twice that of the model itself) due to the introduced slow momentum and outer updates. In comparison, CO2 introduces a one-step-delay outer momentum and outer model updates to overlap communication with multiple steps of local-updating computation, enhancing communication-computation overlap. This improvement comes at the cost of an additional memory footprint (twice that of the model itself) compared to SlowMo, attributed to asynchronous operations. In summary, SlowMo requires additional memory footprint which corresponds to 2 model copies than Local-Adamw. Then on the top of SlowMo, CO2 involves another additional memory footprint which corresponds to 2 model copies, so totally, CO2 needs additional memory overheads of 4 model copies comparing with the vanilla Adamw or Local-Adamw.\nDespite the heightened memory overhead introduced by CO2, it enables full overlap (in the best case) of communication and computation, particularly advantageous for large clusters with limited inter-node connections. Techniques such as ZeRO-1, ZeRO-2, and ZeRO-3 have been incorporated to alleviate redundant memory footprint issues associated with optimizer states, gradients, and model parameters within the inner loop training of CO2. These measures could mitigate the adverse effects of CO2\u2019s augmented memory overheads."
        },
        {
            "heading": "B.3 COMPATIBILITY WITH PARALLELISM TECHNOLOGIES",
            "text": "Given the additional memory footprints associated with asynchronous updates, the compatibility of CO2 with various parallelism techniques becomes an indispensable consideration, especially in the context of large-scale model training. During the implementation of the algorithm, we systematically investigated its compatibility with ZeRO-series optimizers. Currently, ZeRO-1, ZeRO-2, and ZeRO-3 are integrated into the inner loop training of CO2, with the exclusion of the outer loop. We conducted thorough experiments to validate the effectiveness and efficiency of these integrations. In particular, with respect to ZeRO-3, which entails more sophisticated communication during both forward and backward passes, we had considered its compatibility with CO2. Given CO2\u2019s local-updating behavior in the inner loop, continuous gradient synchronization is deemed unnecessary at each step. As a result, communication operations for CO2 with ZeRO-3 involve only two all-gather collective communication primitives at the beginning of the forward and backward passes, respectively, which differs from the original implementation of ZeRO-3.\nConcerning tensor parallelism, sequence parallelism, and their integration with CO2, while we have not implemented them at this moment, their mechanisms appear non-conflicting with CO2\u2019s inner loop training. Tensor parallelism divides large matrix multiplications across multiple devices, and sequence parallelism divides the input sequence into chunks, processing them simultaneously on different devices for efficient training of long sequences. We will try to integrate them with CO2 in the future work."
        }
    ],
    "year": 2023
}