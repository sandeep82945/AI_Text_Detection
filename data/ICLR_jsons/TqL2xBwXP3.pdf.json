{
    "abstractText": "The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson\u2019s r) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
    "authors": [
        {
            "affiliations": [],
            "name": "Rohin Manvi"
        },
        {
            "affiliations": [],
            "name": "Samar Khanna"
        },
        {
            "affiliations": [],
            "name": "Gengchen Mai"
        },
        {
            "affiliations": [],
            "name": "Marshall Burke"
        },
        {
            "affiliations": [],
            "name": "David Lobell"
        },
        {
            "affiliations": [],
            "name": "Stefano Ermon"
        }
    ],
    "id": "SP:ac957908ad6f5c695239a9e43a3d26cdd807ba7a",
    "references": [
        {
            "authors": [
                "Wala Draidi Areed",
                "Aiden Price",
                "Kathryn D Arnett",
                "Kerrie Lee Mengersen"
            ],
            "title": "Spatial statistical machine learning models to assess the relationship between development vulnerabilities and educational factors in children in queensland, australia",
            "venue": "BMC Public Health,",
            "year": 2022
        },
        {
            "authors": [
                "John E. Ball",
                "Derek T. Anderson",
                "Chee Seng Chan"
            ],
            "title": "Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community",
            "venue": "Journal of Applied Remote Sensing,",
            "year": 2017
        },
        {
            "authors": [
                "Ning Bian",
                "Xianpei Han",
                "Le Sun",
                "Hongyu Lin",
                "Yaojie Lu",
                "Ben He"
            ],
            "title": "Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models",
            "venue": "ArXiv, abs/2303.16421,",
            "year": 2023
        },
        {
            "authors": [
                "Joshua Evan Blumenstock",
                "Gabriel Cadamuro",
                "Robert On"
            ],
            "title": "Predicting poverty and wealth from mobile phone metadata",
            "venue": "URL https://api",
            "year": 2015
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "dlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "ArXiv, abs/2005.14165,",
            "year": 2020
        },
        {
            "authors": [
                "M. Burke",
                "Anne Driscoll",
                "D. Lobell",
                "Stefano Ermon"
            ],
            "title": "Using satellite imagery to understand and promote sustainable development",
            "venue": "Science, 371,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chang",
                "Yingjie Hu",
                "Dane Taylor",
                "Brian M Quigley"
            ],
            "title": "The role of alcohol outlet visits derived from mobile phone location data in enhancing domestic violence prediction at the neighborhood level",
            "venue": "Health & Place,",
            "year": 2022
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Guanghua Chi",
                "Han Fang",
                "Sourav Chatterjee",
                "Joshua Evan Blumenstock"
            ],
            "title": "Microestimates of wealth for all low- and middle-income countries",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America,",
            "year": 2021
        },
        {
            "authors": [
                "Gr\u2019egoire Del\u2019etang",
                "Anian Ruoss",
                "Paul-Ambroise Duquenne",
                "Elliot Catt",
                "Tim Genewein",
                "Christopher Mattern",
                "Jordi Grau-Moya",
                "Wenliang Kevin Li",
                "Matthew Aitchison",
                "Laurent Orseau",
                "Marcus Hutter",
                "Joel Veness"
            ],
            "title": "Language modeling is compression",
            "year": 2023
        },
        {
            "authors": [
                "Cheng Deng",
                "Tianhang Zhang",
                "Zhongmou He",
                "Qiyuan Chen",
                "Yuanyuan Shi",
                "Le Zhou",
                "Luoyi Fu",
                "Weinan Zhang",
                "Xinbing Wang",
                "Chenghu Zhou"
            ],
            "title": "Learning a foundation language model for geoscience knowledge understanding and utilization",
            "venue": "arXiv preprint arXiv:2306.05064,",
            "year": 2023
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer"
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "ArXiv, abs/2305.14314,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805,",
            "year": 2019
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Jeremy R. Cole",
                "Julian Martin Eisenschlos",
                "Daniel Gillick",
                "Jacob Eisenstein",
                "William W. Cohen"
            ],
            "title": "Time-aware language models as temporal knowledge bases",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "M. Zhou",
                "Hsiao-Wuen Hon"
            ],
            "title": "Unified language model pre-training for natural language understanding and generation",
            "venue": "In Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Joshua Feldman",
                "Joe Davison",
                "Alexander M. Rush"
            ],
            "title": "Commonsense knowledge mining from pretrained models",
            "venue": "ArXiv, abs/1909.00505,",
            "year": 2019
        },
        {
            "authors": [
                "Leandra Fichtel",
                "Jan-Christoph Kalo",
                "Wolf-Tilo Balke"
            ],
            "title": "Prompt tuning or fine-tuning - investigating relational knowledge in pre-trained language models",
            "venue": "In 3rd Conference on Automated Knowledge Base Construction,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "X. Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Andrew Head",
                "M\u00e9lanie Manguin",
                "Nhat Tran",
                "Joshua Evan Blumenstock"
            ],
            "title": "Can human development be measured with satellite imagery",
            "venue": "Proceedings of the Ninth International Conference on Information and Communication Technologies and Development,",
            "year": 2017
        },
        {
            "authors": [
                "J. Edward Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "ArXiv, abs/2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Yingjie Hu",
                "Xinyue Ye",
                "Shih-Lung Shaw"
            ],
            "title": "Extracting and analyzing semantic relatedness between cities using news articles",
            "venue": "International Journal of Geographical Information Science,",
            "year": 2017
        },
        {
            "authors": [
                "Neal Jean",
                "M. Burke",
                "Sang Michael Xie",
                "W. Matthew Davis",
                "D. Lobell",
                "Stefano Ermon"
            ],
            "title": "Combining satellite imagery and machine learning to predict poverty",
            "venue": "URL https://api.semanticscholar.org/CorpusID:16154009",
            "year": 2016
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "J. Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? Transactions of the Association for",
            "venue": "Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Tomas Mikolov"
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "arXiv preprint arXiv:1607.01759,",
            "year": 2016
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "T.J. Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeff Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "venue": "ArXiv, abs/2001.08361,",
            "year": 2020
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Seyeon Lee",
                "Rahul Khanna",
                "Xiang Ren"
            ],
            "title": "Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models",
            "venue": "In Conference on Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Leo Z. Liu",
                "Yizhong Wang",
                "Jungo Kasai",
                "Hannaneh Hajishirzi",
                "Noah A. Smith"
            ],
            "title": "Probing across time: What does roberta know and when",
            "venue": "In Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692,",
            "year": 2019
        },
        {
            "authors": [
                "Gengchen Mai",
                "Weiming Huang",
                "Jin Sun",
                "Suhang Song",
                "Deepak Mishra",
                "Ninghao Liu",
                "Song Gao",
                "Tianming Liu",
                "Gao Cong",
                "Yingjie Hu"
            ],
            "title": "On the opportunities and challenges of foundation models for geospatial artificial intelligence",
            "venue": "arXiv preprint arXiv:2304.06798,",
            "year": 2023
        },
        {
            "authors": [
                "Gengchen Mai",
                "Ni Lao",
                "Yutong He",
                "Jiaming Song",
                "Stefano Ermon"
            ],
            "title": "Csp: Self-supervised contrastive spatial pre-training for geospatial-visual representations",
            "venue": "In the Fortieth International Conference on Machine Learning (ICML 2023),",
            "year": 2023
        },
        {
            "authors": [
                "Gengchen Mai",
                "Yao Xuan",
                "Wenyun Zuo",
                "Yutong He",
                "Jiaming Song",
                "Stefano Ermon",
                "Krzysztof Janowicz",
                "Ni Lao"
            ],
            "title": "Sphere2vec: A general-purpose location representation learning over a spherical surface for large-scale geospatial predictions",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing,",
            "year": 2023
        },
        {
            "authors": [
                "Catherine Nakalembe"
            ],
            "title": "Characterizing agricultural drought in the karamoja subregion of uganda with meteorological and satellite-based indices",
            "venue": "Natural Hazards,",
            "year": 2018
        },
        {
            "authors": [
                "Benjamin Newman",
                "Prafulla Kumar Choubey",
                "Nazneen Rajani"
            ],
            "title": "P-adapters: Robustly extracting factual information from language models with diverse prompts",
            "venue": "ArXiv, abs/2110.07280,",
            "year": 2021
        },
        {
            "authors": [
                "Kristine Nilsen",
                "Natalia Tejedor-Garavito",
                "Douglas R. Leasure",
                "Chigozie Edson Utazi",
                "Corrine Warren Ruktanonchai",
                "Adelle Wigley",
                "Claire A. Dooley",
                "Zoe Matthews",
                "Andrew J. Tatem"
            ],
            "title": "A review of geospatial methods for population estimation and their use in constructing reproductive, maternal, newborn, child and adolescent health service indicators",
            "venue": "BMC Health Services Research,",
            "year": 2021
        },
        {
            "authors": [
                "Roland Olbricht"
            ],
            "title": "Overpass api, 2023. URL https://wiki.openstreetmap.org/wiki/ Overpass_API",
            "year": 2023
        },
        {
            "authors": [
                "Michael J. Paul",
                "Mark Dredze"
            ],
            "title": "You are what you tweet: Analyzing twitter for public health",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media,",
            "year": 2011
        },
        {
            "authors": [
                "Anthony Perez",
                "Christopher Yeh",
                "George Azzari",
                "M. Burke",
                "D. Lobell",
                "Stefano Ermon"
            ],
            "title": "Poverty prediction with public landsat 7 satellite imagery and machine",
            "venue": "learning. ArXiv,",
            "year": 2017
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H. Miller",
                "Sebastian Riedel"
            ],
            "title": "Language models as knowledge",
            "venue": "bases? ArXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jas\u2019 Eisner"
            ],
            "title": "Learning how to ask: Querying lms with mixtures of soft prompts",
            "venue": "ArXiv, abs/2104.06599,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan"
            ],
            "title": "Improving language understanding by generative pretraining. 2018",
            "venue": "URL https://api.semanticscholar.org/CorpusID:49313245",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever. Language models are unsupervised multitask learners."
            ],
            "title": "URL https://api",
            "venue": "semanticscholar.org/CorpusID:160025533.",
            "year": 2019
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam M. Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Conference on Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Caleb Robinson",
                "Fred Hohman",
                "Bistra N. Dilkina"
            ],
            "title": "A deep learning approach for population estimation from satellite imagery",
            "venue": "Proceedings of the 1st ACM SIGSPATIAL Workshop on Geospatial Humanities,",
            "year": 2017
        },
        {
            "authors": [
                "Evan Sheehan",
                "Chenlin Meng",
                "Matthew Sze Piao. Tan",
                "Burak Uzkent",
                "Neal Jean",
                "D. Lobell",
                "M. Burke",
                "Stefano Ermon"
            ],
            "title": "Predicting economic development using geolocated wikipedia articles",
            "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2019
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L Logan IV",
                "Eric Wallace",
                "Sameer Singh"
            ],
            "title": "Eliciting knowledge from language models using automatically generated prompts",
            "venue": "In Conference on Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Alessio Signorini",
                "Alberto Maria Segre",
                "Philip M. Polgreen"
            ],
            "title": "The use of twitter to track levels of disease activity and public concern in the u.s. during the influenza a h1n1 pandemic",
            "venue": "PLoS ONE,",
            "year": 2011
        },
        {
            "authors": [
                "Helen R. Sofaer",
                "Catherine S. Jarnevich",
                "Ian S. Pearse",
                "Regan L. Smyth",
                "Stephanie Auer",
                "Gericke Cook",
                "Thomas C. Edwards",
                "Gerald F. Guala",
                "Timothy G. Howard",
                "Jeffrey T. Morisette",
                "Healy Hamilton"
            ],
            "title": "Development and delivery of species distribution models to inform decisionmaking",
            "venue": "BioScience,",
            "year": 2019
        },
        {
            "authors": [
                "Mujeen Sung",
                "Jinhyuk Lee",
                "Sean S. Yi",
                "Minji Jeon",
                "Sungdong Kim",
                "Jaewoo Kang"
            ],
            "title": "Can language models be biomedical knowledge",
            "venue": "bases? ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew J. Tatem"
            ],
            "title": "Worldpop, open data for spatial demography",
            "venue": "Scientific Data,",
            "year": 2017
        },
        {
            "authors": [
                "Yifan Zhang",
                "Cheng Wei",
                "Shangyou Wu",
                "Zhengting He",
                "Wenhao Yu"
            ],
            "title": "Geogpt: Understanding",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Geospatial predictions with ML are widely used across various domains, including poverty estimation (Jean et al., 2016; Chi et al., 2021), public health (Nilsen et al., 2021; Areed et al., 2022; Chang et al., 2022), food security (Nakalembe, 2018), biodiversity preservation (Mai et al., 2023c;b), and environmental conservation (Sofaer et al., 2019). The covariates used in these predictions include geographical coordinates, remote sensing data, satellite imagery, human mobility data (Chang et al., 2022), and phone metadata (Blumenstock et al., 2015; Burke et al., 2019). While having access to quality covariates is essential, it can be challenging due to limited spatiotemporal coverage, high costs, and accessibility barriers (Ball et al., 2017). For example, researchers often use publicly available satellite images to estimate socio-economic indicators like asset wealth, population density, and infrastructure access, because they are free and globally available (Yeh et al., 2020; Robinson et al., 2017; Head et al., 2017). However, model predictive power can be limited due to the fact that important features may not be visible from space.\nLarge language models (LLMs) have proven to be highly effective foundation models that can be fine-tuned or prompted to perform tasks in various domains including healthcare, education, law,\n\u2217Corresponding author, rohinm@cs.stanford.edu \u2020Stanford University \u2021University of Georgia\nfinance, and scientific research (Bommasani et al., 2021; Zhao et al., 2023). This is because LLMs have compressed the knowledge contained in their training corpus, which includes billions or trillions of tokens of data from the internet (Del\u2019etang et al., 2023). Here, we seek to understand whether LLMs possess geospatial knowledge and explore methods to extract this knowledge to offer a novel set of geospatial covariates that can enhance various geospatial ML tasks.\nAs shown in fig. 1a, a substantial amount of geospatial knowledge in LLMs can be revealed simply by querying them to describe an address. However, extracting this knowledge from LLMs is not trivial. While the most natural interface involves using geographic coordinates like latitude and longitude to retrieve specific location information, this often yields poor results as shown in fig. 1b. The inherent challenge lies in the LLMs\u2019 capability to understand and interpret these numeric coordinates in relation to real-world locations, especially when the location is remote or lesser-known.\nIn this paper, we introduce GeoLLM, a novel method that can efficiently extract the vast geospatial knowledge contained in LLMs by fine-tuning them on prompts constructed with auxiliary map data from OpenStreetMap OpenStreetMap contributors (2017). With our prompting strategy shown in fig. 1b, we can pinpoint a location and provide the LLM with enough spatial context information, thereby enabling it to access and leverage its vast geospatial knowledge. We find that including information from nearby locations in the prompt can improve GPT-3.5\u2019s (OpenAI, 2023b) performance by 3.3x, measured in Pearson\u2019s r2, over simply providing the target location\u2019s coordinates.\nWe find that popular LLM models such as GPT-3.5 and Llama 2 (Touvron et al., 2023) can be fine-tuned to achieve state-of-the-art performance on a variety of large-scale geospatial datasets for tasks including assessing population density, asset wealth, mean income, women\u2019s education and more. With GeoLLM, GPT-3.5, Llama 2, and RoBERTa (Liu et al., 2019), show a 70%, 43%, and 13% improvement in Pearson\u2019s r2 respectively over baselines that use nearest neighbors and use information directly from the prompt, suggesting that the models\u2019 geospatial knowledge scales with their size and the size of their pretraining dataset. Our experiments demonstrate LLMs\u2019 remarkable sample efficiency and consistency worldwide. Importantly, GeoLLM could mitigate the limitations of existing geospatial covariates and complement them well.\nWe present the following main research contributions:\n1. Language models possess substantial geospatial knowledge and our method unlocks this knowledge across a range of models and tasks. 2. Constructing the right prompt is key to extracting geospatial knowledge. Our ablations find that prompts constructed from map data allow the models to more efficiently access their knowledge."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Identifying Knowledge Pre-training instills large amounts of knowledge into language models. Since the introduction of pre-trained language models such as BERT (Devlin et al., 2019), many works try to quantify the amount of knowledge they can encode (Roberts et al., 2020; Jiang et al., 2019) and see if they can be used as knowledge bases (Petroni et al., 2019). Other works have looked for more specific types of knowledge such as commonsense (Feldman et al., 2019), temporal (Dhingra et al., 2021; Liu et al., 2021), biomedical (Sung et al., 2021), numerical (Lin et al., 2020), scale (Zhang et al., 2020), and many others. With the recent introduction of instruction-finetuned LLMs, directly querying knowledge with prompts is a potentially simpler method. In this paper, we focus on showing the quantity and quality of geospatial knowledge in pre-trained language models.\nKnowledge Extraction Many works aim to improve knowledge extraction from language models by fine-tuning or prompt tuning. Works on prompt tuning explore mining prompts from the web (Jiang et al., 2019), optimizing prompts in the discrete space of words and tokens (Shin et al., 2020), optimizing prompts in the continuous embedding space (Zhong et al., 2021), and using adapters (Newman et al., 2021). However, prompts are often difficult to craft (Qin & Eisner, 2021; Jiang et al., 2019), and are sensitive to small changes. Fine-tuning language models for downstream tasks has shown to be an effective and simple method to tune and elicit specific knowledge for evaluation (Radford & Narasimhan, 2018; Dong et al., 2019). It is also shown that fine-tuning may result in higher performance gains compared to prompt tuning (Fichtel et al., 2021). Additionally, LoRA and quantization are shown to greatly reduce the computational cost of fine-tuning (Hu et al., 2021; Dettmers et al., 2023). For these reasons, we choose to extract geospatial knowledge with finetuning.\nNLP for Geospatial Tasks Natural language processing (NLP) has been used for many geospatial tasks with various data sources. Hu et al. (2017) extracted semantics relations among cities by leveraging news articles. Sheehan et al. (2019) utilized Wikipedia to predict poverty. Xu et al. (2016) accessed traffic by using OpenStreetMap data. Signorini et al. (2011); Paul & Dredze (2011) utilized tweets to predict track-levels disease activities. Recently, researchers started to explore LLMs\u2019 capability on various geospatial tasks. Mai et al. (2023a) demonstrated the usability of large language models on various geospatial applications including fine-grained address recognition, time-series dementia record forecasting, urban function prediction, and so on. GeoGPT (Zhang et al., 2023) has been proposed as a GPT-3.5-based autonomous AI tool that can conduct geospatial data collection, processing, and analysis in an autonomous manner with the instruction of only natural language. However, both works mainly relied on the pre-trained LLMs and did not explore the possibility of fine-tuning LLMs for a geo-specific foundation model. Deng et al. (2023) developed an LLM in geoscience, so-called K2, by fine-tuning on a geoscience text corpus which has shown remarkable performances on various NLP tasks in the geoscience domain. However, the K2 geoscience language foundation model is still limited to the common NLP tasks such as question answering, summarization, text classification, etc. In contrast, in this work, we conduct a deep investigation into the possibility of extracting geospatial knowledge from LLMs and whether we can use it for various real-world geospatial tasks like predicting population density. By fine-tuning various LLMs, we demonstrate the quantity, quality, and scalable nature of geospatial knowledge contained in LLMs."
        },
        {
            "heading": "3 METHOD",
            "text": "Abstractly, we want to map geographic coordinates (latitude, longitude) and additional features to a response variable, such as population density or asset wealth. A naive approach would be to spatially interpolate using the coordinates. However, this is far from ideal, especially for small sample sizes. Features or covariates can help, but such data can be difficult to obtain. Given this challenge, we aim to determine how much LLMs know about these coordinates and how we can use that knowledge to\nbetter predict response variables of interest. In this section, we describe our approach of extracting this knowledge with the fine-tuning of language models with prompts generated using map data."
        },
        {
            "heading": "3.1 MINIMUM VIABLE GEOSPATIAL PROMPT",
            "text": "A minimum viable prompt needs to specify the location and the prediction task. Our basic prompt consists of geographic coordinates and a description of the task. The use of geographic coordinates is essential to extracting high-resolution data as they are a universal and precise interface for knowledge extraction. The description comprises the task\u2019s name and the scale indicating the range of possible labels. The ground truth label, is also added when training an LLM with unsupervised learning. We present an example of this simple structure in the top part of fig. 1b.\nWhile we find that adding the name of the task helps, more specific information, such as the name of the dataset, generally does not help. This suggests that basic context sufficiently assists LLMs in efficiently accessing the knowledge in its weights.\nSince the label has to be specified through text for LLMs, we use classification instead of regression. However, we find that a classification task closer to regression (e.g., 0.0 to 9.9 as opposed to 1 to 5) is beneficial, despite the fact that LLMs use softmax. If the distribution in the original dataset already has an approximate uniform distribution, we simply scale the values to the range from 0 to 9.9 and round to one decimal place. If the dataset is not inherently uniform, we distribute the values evenly among 100 bins to maintain a uniform distribution. Each bin is then associated with the range of values from 0.0 to 9.9."
        },
        {
            "heading": "3.2 PROMPT WITH MAP DATA",
            "text": "The prompt described above, however, has a major limitation. Through zero-shot prompting, one can easily find that LLMs struggle to identify the locations of coordinates. Not knowing where the coordinates are located would be detrimental for geospatial tasks. This is why we focus on making sure the prompt contains additional context that helps the model understand where the coordinates are located. We also design our prompt to be descriptive and useful for a range of geographical scopes such as ZIP code areas or a single square kilometer.\nOur prompt, shown in the lower part of fig. 1b, consists of two additional components constructed using map data:\n1. Address: A detailed reverse-geocoded description, encompassing place names from the neighborhood level up to the country itself.\n2. Nearby Places: A list of the closest 10 locations in a 100 kilometer radius with their respective names, distances, and directions.\nWe use OpenStreetMap (OpenStreetMap contributors, 2017) for our map data as it can be queried for free using various APIs, although any map data can be used. In our testing, map data from Google is of higher quality, but was prohibitively expensive. We generate addresses using reverse geocoding from Nominatim (Hoffmann, 2023) and the names and locations of nearby places from Overpass API (Olbricht, 2023). A comprehensive prompt template and additional prompt example is provided in appendix A.2"
        },
        {
            "heading": "3.3 FINE-TUNING AND INFERENCE WITH LANGUAGE MODELS",
            "text": "RoBERTa RoBERTaBASE is a 125 million parameter encoder-only transformer model trained on 160 GB worth of text (Liu et al., 2019). We choose this as it and other encoder-only models like BERT (Devlin et al., 2019) are often used for classification and regression tasks (Bommasani et al., 2021). We fine-tune the model with all of its parameters being trainable. It takes in the entire prompt and outputs a continuous value which we round to the nearest bin.\nLlama 2 Llama 2 7B is a 7 billion parameter decoder-only transformer model trained on 2 trillion tokens (Touvron et al., 2023). Despite being unsuited for regression tasks, it has the potential to perform well as it has been trained on significantly more data. Similar to how the model was pretrained, we fine-tune Llama 2 using unsupervised learning on the prompts and labels concatenated. We find\nthat this unsupervised learning works better than using supervised finetuning. This means that the model is learning to generate the prompt as well as the label. One could imagine that it is beneficial to learn to generate the address given the coordinates or the nearby places using the coordinates and address. During inference time, we provide it with the prompt and let it generate the three tokens required for the prediction (e.g., \u201d9\u201d, \u201d.\u201d, and \u201d9\u201d). QLoRA (Dettmers et al., 2023) allows us to greatly reduce the computational cost and effectively fine-tune with just 33 million trainable parameters. However, since it also quantizes the frozen model to 4-bit, this comes with the potential cost of diluting the knowledge contained in the weights.\nGPT-2 GPT-2 is a 124 million parameter decoder-only transformer model trained on 40 GB worth of text (Radford et al., 2019). We use the same finetuning and inference process as Llama 2. We use this model as an LLM baseline and as a way to gain insight into the effect of the size of the model and its dataset on performance.\nGPT-3.5 GPT-3.5 (gpt-3.5-turbo-0613) (OpenAI, 2023b) is a chat based LLM. It is the successor of GPT-3 which is a decoder-only transformer model with 175 billion parameters (Brown et al., 2020). GPT-3.5 shows promise in containing a substantial amount of knowledge (Bian et al., 2023). We fine-tune GPT-3.5 through OpenAI\u2019s fine-tuning API (OpenAI, 2023a). Similar to Llama 2, we provide it the prompt and let it generate the three tokens required for the completion.\nThe details of the finetuning of these models and their hyperparameters are specified in appendix A.3. We also demonstrate the importance of fine-tuning in appendix A.4 by testing the few-shot performance of GPT-3.5 and GPT-4."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We construct a comprehensive benchmark that encompasses a wide range of geospatial prediction tasks. This benchmark includes various domains and geographies, ranging from women\u2019s education levels in developing countries to housing prices in the US. We develop a robust set of baselines to demonstrate the performance attainable using the information directly in the prompt. Finally, we present our results and an ablation study."
        },
        {
            "heading": "4.1 TASKS AND SOURCES",
            "text": "WorldPop WorldPop (Tatem, 2017) is a research group focused on providing spatial demographic data for the use of health and development applications, including addressing the Sustainable Development Goals. We use their population counts dataset that covers the whole world for the population density task. More specifically, we use their unconstrained global mosaic for 2020 that has a res-\nolution of 30 arc (approximately 1km at the equator) (WorldPop & CIESIN, 2018). To ensure a comprehensive representation of populations, we employed importance sampling, weighted by population. This allows us to capture a wide range of populations; without it, our samples would largely consist of sparsely populated areas. To construct a learning curve for this task, we use training sample sizes of 100, 1,000, and 10,000.\nDHS SustainBench (Yeh et al., 2021) includes tasks derived from survey data from the Demographic and Health Surveys (DHS) program (DHS, 2023). These surveys constitute nationally representative household-level data on assets, housing conditions, and education levels, among other attributes across 48 countries. While DHS surveys provide household-level data, SustainBench summarizes the survey data into \u201ccluster-level\u201d labels, where a \u201ccluster\u201d roughly corresponds to a village or local community. The geocoordinates are \u201cjittered\u201d randomly up to 2km for urban clusters and 10km for rural clusters to protect survey participant privacy. From their datasets, we use asset wealth index, women educational attainment, sanitation index, and women BMI. Since their datasets cover a wide scope, we use training sample sizes of 1,000 and 10,000.\nUSCB The United States Census Bureau (USCB) (US, 2023) is responsible for producing data about the American people and economy. In addition to the decennial census, the Census Bureau continually conducts annual demographics surveys through the American Community Survey (ACS) program. From these two sources, we get tasks for population, mean income, and Hispanic to NonHispanic ratio by what they call ZIP Code Tabulation Areas. Since their data only covers the US, we use training sample sizes of 100 and 1,000.\nZillow Zillow provides the Zillow Home Value Index (ZHVI) (ZR, 2023): A measure of the typical home value and market changes across a given region and housing type. It reflects the typical value for homes in the 35th to 65th percentile range. We use the smoothed, seasonally adjusted measure that they provide. Like with the USCB data, we also use this data on a ZIP code level. This serves as our home value task. Since this dataset also only covers the US, we use training sample sizes of 100 and 1,000.\nWhen explicit coordinates are not given in the datasets, we approximate them by determining the center of relevant areas. For instance, we approximate the center of each of the ZIP code area so that we can generate prompts for the tasks from USCB or Zillow. We evaluate performance using the squared Pearson correlation coefficient r2 between the predictions and labels so that comparisons can be made with previous literature (Perez et al., 2017; Jean et al., 2016; Yeh et al., 2020; 2021). We use 2,000 samples for our test and validation sets across all tasks. We split the data into training, test, and validation partitions early in the process before sampling different sizes. We also explicitly check again for any overlap between the train and test samples before fine-tuning or training."
        },
        {
            "heading": "4.2 BASELINES",
            "text": "The purpose of the first three baselines is to demonstrate the performance attainable using the information in the prompt. Pre-trained language models can surpass these baselines only if they effectively use the knowledge embedded in their weights. This helps us understand the extent of knowledge encapsulated within pre-trained language models. The next baseline has a pretrained component but provides further insight into performance attainable using sentence embeddings. The last baseline allows for comparison with a well-established satellite-based method.\nk-NN Our simplest baseline is k-NN. It takes the mean of the 5 closest neighbors. This is a reasonable baseline as physical distance is very relevant for geospatial tasks.\nXGBoost Our second baseline uses XGBoost (Chen & Guestrin, 2016). This baseline incorporates all numerical and categorical data from the prompt, including coordinates, distances, and directions. It could outperform k-NN since it leverages not just the coordinates, but also the distance and direction data. We established this baseline believing that the direction and distance information could be covariates for some of the tasks we use.\nXGBoost-FT Our third baseline, XGBoost-FT, leverages both XGBoost and fastText (Joulin et al., 2016). This baseline uses all information contained in the prompt. In addition to using the coordinates, distances, and directions, it also uses embeddings generated from the address and the names of the nearby places. We use fastText to learn sentence embeddings on any given dataset using strings created by concatenating the address and the names of the nearby places.\nMLP-BERT Our fourth baseline, MLP-BERT, also uses coordinates, distances, directions, and an embedding of the address and the names of the nearby places. Unlike XGBoost-FT, it uses sentence embeddings from the 110 million parameter pre-trained BERT (Devlin et al., 2019). To accommodate for the larger embedding size, we use a 2-layer MLP.\nNightlight Our final baseline uses publicly available satellite images of luminosity at night (\u201cnightlights\u201d) from VIIRS which comes at a resolution of 500 meters (Elvidge et al., 2017). Nightlight images are commonly used as they are a proxy for economic development (Yeh et al., 2020; Sheehan et al., 2019; Jean et al., 2016). We find that images of sizes 16 km by 16 km work best for our diverse set of tasks. We use GBTs as they have been shown to be effective for single-band nightlight imagery (Perez et al., 2017). We find that ResNet (He et al., 2015) models, as used in (Yeh et al., 2020), require large numbers of samples to outperform GBTs.\nAll baselines except GPT-2, treat the tasks as a regression problem. Their output is rounded to the nearest bin (0.0 to 9.9) for classification. We get the average of the values in a 25 square kilometer area. We then use those values to get the ground truth ranking that is used in the experiments."
        },
        {
            "heading": "4.3 PERFORMANCE ON TASKS",
            "text": "From the results presented in table 1 and table 4, it is evident that LLMs are the best-performing models. Among them, GPT-3.5 stands out as the best-performing LLM. Its Pearson\u2019s r2 ranges from 0.46 to 0.74, 0.55 to 0.87, and 0.63 to 0.78 for sample sizes of 100, 1,000, and 10,000, respectively. While these variations indicate that the tasks differ in difficulty, LLMs generally outperform all prompt-based baselines and even the nightlight baseline. This suggests that LLMs possess a substantial amount of geospatial knowledge.\nNot only does GPT-3.5 outperform all other models on every test, but its performance is also relatively consistent across tasks and sample sizes. Llama 2 does better than all baselines for 17 out of the 19 total tests and consistently does better than RoBERTa. RoBERTa consistently does better than all prompt-based baselines at 10,000 training samples, but struggles at lower sample sizes. Furthermore, GPT-3.5 and Llama 2 retain a much more acceptable level of performance at a sample size of 100, emphasizing their sample efficiency.\nGPT-3.5 and Llama 2 do especially well with the population tasks from WorldPop and the USCB compared to the baselines. GPT-3.5 is also especially impressive with the home value task from Zillow with a Pearon\u2019s r2 of up to 0.87. However, the difference in performance between the models is less pronounced for the tasks from the DHS. This might be due to the noise that is added when the coordinates for these tasks are \u201cjittered\u201d up to 5 kilometers. With the added noise, it is potentially more difficult to achieve a high Pearson\u2019s r2.\nAs shown in fig. 3 GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, indicating that the method scales well. Fig. 4 again shows that the sample efficiencies of Llama 2 and GPT-3.5 are exceptional, especially when making predictions on a global scale. Note that with larger sample sizes the gaps in performance will decrease as the physical distances between the training coordinates and test coordinates become smaller.\nIt is visually clear in fig. 2 that GPT-3.5 does substantially better than the best prompt-based baselines trained from scratch for each task. It also appears that GPT-3.5 generally only struggles where the baselines struggle, suggesting that those areas might just be difficult to estimate rather than it being a specific failing of the model. Notably, GPT-3.5 shows has impressive geographic consistency."
        },
        {
            "heading": "4.4 ABLATIONS ON THE PROMPT",
            "text": "The ablation study shown in table 2 demonstrates the importance of using a highly descriptive prompt that helps the model understand where the coordinates are located and efficiently access the knowledge contained in its weights. It also suggests that all components of the prompt contribute to the overall performance of the models. In addition to performing better than the other language models, GPT-3.5 is more resilient to the removal of the map data. This is evident from the fact that prompt without map data is completely ineffective for Llama 2 and RoBERTa. This suggests that GPT-3.5 can more effectively access its knowledge."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "LLMs not only match but also exceed the performance of methods that use satellite imagery. Satellite-based methods achieve Pearson\u2019s r2 of 0.56 (Jean et al., 2016), 0.71 (Perez et al., 2017), and 0.67 (Yeh et al., 2020) in predicting asset wealth in Africa. With 10,000 training samples (approximately 3,700 of which are from Africa), a fraction of the over 87,000 data points available from the DHS, GPT-3.5 achieves an r2 of 0.72 when evaluated in Africa.\nThe performance of the LLMs is particularly notable considering an inherent disadvantage they have relative to the baselines and RoBERTa. Generating multiple tokens with softmax is far less intuitive than regression which the other models use. Despite this, there are substantial improvements in performance and sample efficiency with the LLMs.\nIt also appears that the significant increase in performance is not simply due to the number of parameters being trained during fine-tuning. With QLoRA, Llama 2 has 33 million trainable parameters as opposed to RoBERTa\u2019s 125 million trainable parameters. This suggests that the performance increase is due to the knowledge contained in Llama 2\u2019s frozen weights.\nThis method\u2019s performance appears to scale with both the size of the model\u2019s pretraining dataset and its capacity for data compression, a capability influenced by the model\u2019s size. This suggests our method likely scales with the general capability of the model as well (Kaplan et al., 2020). For example, GPT-4 (OpenAI, 2023c) would likely outperform GPT-3.5.\nOne could potentially use our method to better understand the biases of LLMs that they inherit from their training corpora. Surprisingly, the performance across countries and continents appears very consistent, as evident from the maps of absolute errors shown in fig. 2. However, preliminary evidence of biases towards sparsely populated or underdeveloped areas is shown in appendix A.6.\nThis work lays the foundation for the future use of LLMs for geospatial tasks. Additional dimensions could extend the applicability of our method. For instance, one could augment prompts with names of amenities or roads to enrich the geospatial context and allow for even higher-resolution knowledge extraction. Temporal data such as dates could be integrated into the prompts for making forecasts. Multimodal LLMs could potentially also leverage satellite imagery or street view images. Overall, there are likely many ways to extend GeoLLM."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we introduced GeoLLM, a novel method that can efficiently extract the vast geospatial knowledge contained in LLMs by fine-tuning them on prompts constructed with auxiliary map data from OpenStreetMap. We found that including information from nearby locations in the prompt greatly improves LLMs\u2019 performance. We demonstrated the utility of our approach across various tasks from multiple large-scale real-world datasets. Our method demonstrated a 70% improvement in Pearson\u2019s r2 over traditional baselines including k-NN and XGBoost, exceeding the performance of satellite-based methods. With GeoLLM, we observed that GPT-3.5 outperforms Llama 2 by 19% and RoBERTa by 51%, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our simple method revealed that LLMs are sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in substantially mitigating the limitations of traditional geospatial covariates."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 ADDITIONAL VISUALIZATIONS OF PERFORMANCE\nA.2 PROMPT TEMPLATE\nTo aid the reproducibility of our results, we provide the prompt template, shown in fig. 5a, that is used to construct all prompts across all tasks. In addition to the example shown in the lower part of fig. 1b we provide an example used for the asset wealth task shown in fig. 5b that further clarifies how the prompt template is used.\nA.3 LANGUAGE MODEL FINETUNING DETAILS\nRoBERTa We fine-tune the model for regression with all of its parameters being trainable. With a single V100 GPU, finetuning never takes more than 2 hours. We use a learning rate of 1 \u00d7 10\u22125 with AdamW optimizer, training for 8 epochs with a batch size of 16, warmup ratio of 0.1, weight decay of 0.1, and a cosine learning rate scheduler.\nLlama 2 Similar to how the model was pretrained, we fine-tune Llama 2 using unsupervised learning with the prompts and labels concatenated. With QLoRA, finetuning never takes more than 2 hours for 10,000 samples with a single A100 GPU. For LoRA, we use rank of 64, alpha of 16, and a dropout probability of 0.1. We also use 4-bit quantization with 4-bit NormalFloat. We train for 4 epochs with bfloat16 mixed precision training, a batch size of 8, gradient accumulation steps of 2, gradient checkpoints enabled, a maximum gradient norm of 0.3, an initial learning rate of 1.5 \u00d7 10\u22123 for the AdamW optimizer, a weight decay of 0.001, a cosine learning rate scheduler, and a warmup ratio of 0.03. Interestingly, the model\u2019s performance still improves even when the loss on the validation set increases by the fourth epoch. This suggests that the model is less prone to overfitting on the prediction tasks compared to the prompts.\nGPT-2 We use the same finetuning process for GPT-2 as Llama 2. The only difference is that we train the model for 8 epochs instead of 4. Other changes in hyperparameters do not seem to increase performance significantly.\nGPT-3.5 We find that 4, 3, and 2 epochs work well for 100, 1,000, and 10,000 training samples respectively. This costs roughly $0.67, $5, and $33 for those sample sizes. It is worth noting that the specific fine-tuning procedures and internal architecture of GPT-3.5 are not publicly disclosed by OpenAI.\nA.4 FEW-SHOT\nWe demonstrate GPT-3.5 and GPT-4 few-shot performance by providing them with a series of the 10 closest (by physical distance) training samples for each test example. We use system messages \u201cYou are a detailed and knowledgeable geographer\u201d and \u201cYou complete sequences of data with predictions/estimates\u201d before providing the 10 examples.\nThe test set consists of 200 samples. The size reduction of the test set is required as 10-shot is quite expensive, especially with GPT-4. We also reevaluate the performance of the finetuned GPT-3.5 model with this smaller test set to be able to make direct comparisons.\nThe results in table 3 demonstrate that while few-shot does work, it performs significantly worse than the respective finetuned model as can be seen with the performance difference between the GPT-3.5 models. Interestingly, GPT-4 does far better than GPT-3.5 in the few-shot setting, suggesting that a fine-tuned version of this model could potentially perform significantly better.\nA.5 MEAN ABSOLUTE ERROR\nAs a potentially more interpretable metric, we present the mean absolute error (MAE) for all models across all tasks and training sample sizes in table 4. One can observe that the same conclusions that are made with the Pearson\u2019s r2 can be made when comparing the MAE of the various models. In particular, the relative ranking in performance of the models within tasks is consistent with Pearson\u2019s r2.\nA.6 PRELIMINARY INVESTIGATION ON PERFORMANCE BIAS\nWhile there does not appear to be any obvious signs of performance bias across countries or continents as seen in fig. 2, the existence of biases in performance is inevitable as it is very likely that the internet training corpora of LLMs are inherently biased towards developed and densely populated areas. We find preliminary signs of this as the increase in MAE shown in table 5 indicates moderate performance bias towards densely populated and developed areas. However, further research is needed for a comprehensive analysis of the geospatial biases in LLMs and their training corpora. As we have demonstrated here, GeoLLM has the potential to be used as a tool to reveal LLM biases on a geographical scale."
        }
    ],
    "title": "GEOLLM: EXTRACTING GEOSPATIAL KNOWLEDGE FROM LARGE LANGUAGE MODELS",
    "year": 2024
}