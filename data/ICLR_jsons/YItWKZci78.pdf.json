{
    "abstractText": "In this paper, we extend mean-field Langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. We propose mean-field Langevin averaged gradient (MFL-AG), a single-loop algorithm that implements gradient descent ascent in the distribution spaces with a novel weighted averaging, and establish average-iterate convergence to the mixed Nash equilibrium. We also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. Furthermore, we propose mean-field Langevin anchored best response (MFL-ABR), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. Finally, we study applications to zero-sum Markov games and conduct simulations demonstrating long-term optimality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Juno Kim"
        },
        {
            "affiliations": [],
            "name": "Kakei Yamamoto"
        },
        {
            "affiliations": [],
            "name": "Kazusato Oko"
        },
        {
            "affiliations": [],
            "name": "Zhuoran Yang"
        },
        {
            "affiliations": [],
            "name": "Taiji Suzuki"
        }
    ],
    "id": "SP:1665b07881bbadeb7d3a9d0279969436776fe316",
    "references": [
        {
            "authors": [
                "M. Arjovsky",
                "S. Chintala",
                "L. Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "M.D. Bakry"
            ],
            "title": "\u00c9mery. Diffusions hypercontractives",
            "venue": "Se\u0301minaire de probabilite\u0301s de Strasbourg,",
            "year": 1985
        },
        {
            "authors": [
                "J.-B. Bardet",
                "N. Gozlan",
                "F. Malrieu",
                "P.-A. Zitt"
            ],
            "title": "Functional inequalities for Gaussian convolutions of compactly supported measures: explicit bounds and dimension dependence",
            "year": 2018
        },
        {
            "authors": [
                "S. Bobkov",
                "M. Ledoux"
            ],
            "title": "One-dimensional empirical measures, order statistics, and Kantorovich transport distances",
            "venue": "Memoirs of the American Mathematical Society,",
            "year": 2016
        },
        {
            "authors": [
                "G. Brown"
            ],
            "title": "Iterative solution of games by fictitious play",
            "venue": "Activity Analysis of Production and Allocation,",
            "year": 1951
        },
        {
            "authors": [
                "P. Cattiaux",
                "A. Guillin"
            ],
            "title": "Functional inequalities for perturbed measures with applications to logconcave measures and to some Bayesian problems",
            "year": 2022
        },
        {
            "authors": [
                "S. Cen",
                "Y. Wei",
                "Y. Chi"
            ],
            "title": "Fast policy extragradient methods for competitive games with entropy regularization",
            "venue": "In 35th Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "S. Cen",
                "Y. Chi",
                "S.S. Du",
                "L. Xiao"
            ],
            "title": "Faster last-iterate convergence of policy optimization in zerosum Markov games",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "F. Chen",
                "Z. Ren",
                "S. Wang"
            ],
            "title": "Uniform-in-time propagation of chaos for mean field Langevin dynamics",
            "venue": "arXiv preprint arXiv:2212.03050v2,",
            "year": 2022
        },
        {
            "authors": [
                "F. Chen",
                "Z. Ren",
                "S. Wang"
            ],
            "title": "Entropic fictitious play for mean field optimization problem",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "L. Chizat"
            ],
            "title": "Mean-Field Langevin dynamics : exponential convergence and annealing",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "L. Chizat",
                "F. Bach"
            ],
            "title": "On the global convergence of gradient descent for over-parameterized models using optimal transport",
            "venue": "In 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "G. Conforti",
                "A. Kazeykina",
                "Z. Ren"
            ],
            "title": "Game on random environment, mean-field Langevin system and neural networks",
            "venue": "Math. Oper. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "C. Daskalakis",
                "I. Panageas"
            ],
            "title": "Last-iterate convergence: zero-sum games and constrained min-max optimization",
            "venue": "In Innovations in Theoretical Computer Science,",
            "year": 2019
        },
        {
            "authors": [
                "V. De Bortoli",
                "A. Durmus",
                "X. Fontaine",
                "U. Simsekli"
            ],
            "title": "Quantitative propagation of chaos for SGD in wide neural networks",
            "venue": "In 34th Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "R.L. Dobrushin"
            ],
            "title": "Prescribing a system of random variables by conditional distributions",
            "venue": "Theory of Probability and Its Applications,",
            "year": 1970
        },
        {
            "authors": [
                "C. Domingo-Enrich",
                "S. Jelassi",
                "A. Mensch",
                "G. Rotskoff",
                "J. Bruna"
            ],
            "title": "A mean-field analysis of twoplayer zero-sum games",
            "venue": "In 34th Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "N. Fournier",
                "A. Guillin"
            ],
            "title": "On the rate of convergence in Wasserstein distance of the empirical measure",
            "venue": "Probability Theory and Related Fields,",
            "year": 2015
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Guo",
                "Z. Wu",
                "Y. Yan",
                "X. Wang",
                "T. Yang"
            ],
            "title": "Revisiting SGD with increasingly weighted averaging: optimization and generalization perspectives",
            "venue": "arXiv preprint arXiv:2003.04339,",
            "year": 2020
        },
        {
            "authors": [
                "J. Hiriart-Urruty",
                "C. Lemar\u00e9chal"
            ],
            "title": "Fundamentals of convex analysis",
            "venue": "Grundlehren Text Editions. Springer,",
            "year": 2004
        },
        {
            "authors": [
                "R. Holley",
                "D.W. Stroock"
            ],
            "title": "Logarithmic Sobolev inequalities and stochastic Ising models",
            "venue": "Journal of Statistical Physics,",
            "year": 1987
        },
        {
            "authors": [
                "Y.-P. Hsieh",
                "C. Liu",
                "V. Cevher"
            ],
            "title": "Finding mixed Nash qquilibria of generative adversarial networks",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "K. Hu",
                "Z. Ren",
                "D. \u0160i\u0161ka"
            ],
            "title": "Szpruch. Mean-field Langevin dynamics and energy landscape of neural networks",
            "venue": "Annales de l\u2019Institut Henri Poincare\u0301, Probabilite\u0301s et Statistiques,",
            "year": 2043
        },
        {
            "authors": [
                "N. Lanzetti",
                "S. Bolognani",
                "F. D\u00f6rfler"
            ],
            "title": "First-order conditions for optimization in the Wasserstein space",
            "venue": "arXiv preprint arXiv:2209.12197,",
            "year": 2022
        },
        {
            "authors": [
                "R.-A. Lascu",
                "M.B. Majka",
                "L. Szpruch"
            ],
            "title": "Entropic mean-field min-max problems via best response and Fisher-Rao flows",
            "venue": "arXiv preprint arXiv:2306.03033,",
            "year": 2023
        },
        {
            "authors": [
                "M. Ledoux"
            ],
            "title": "Concentration of measure and logarithmic Sobolev inequalities",
            "venue": "Se\u0301minaire de Probabilite\u0301s XXXIII,",
            "year": 1999
        },
        {
            "authors": [
                "Y. Lu"
            ],
            "title": "Two-scale gradient descent ascent dynamics finds mixed Nash equilibria of continuous games: A mean-field perspective",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "C. Ma",
                "L. Ying"
            ],
            "title": "Provably convergent quasistatic dynamics for mean-field two-player zero-sum games",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "A. Madry",
                "A. Makelov",
                "L. Schmidt",
                "D. Tsipras",
                "A. Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In The Sixth International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "R.D. McKelvey",
                "T.R. Palfrey"
            ],
            "title": "Quantal response equilibria for normal form games",
            "venue": "Games and Economic Behavior,",
            "year": 1995
        },
        {
            "authors": [
                "S. Mei",
                "A. Montanari",
                "P.-M. Nguyen"
            ],
            "title": "A mean field view of the landscape of two-layer neural networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Nesterov"
            ],
            "title": "Primal-dual subgradient methods for convex problems",
            "venue": "Mathematical Programming,",
            "year": 2009
        },
        {
            "authors": [
                "A. Nitanda",
                "D. Wu",
                "T. Suzuki"
            ],
            "title": "Convex analysis of the mean field Langevin dynamics",
            "venue": "In 25th International Conference on Artificial Intelligence and Statistics. PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "A. Nitanda",
                "D. Wu",
                "T. Suzuki"
            ],
            "title": "Particle dual averaging: optimization of mean field neural network with global convergence rate analysis",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2022
        },
        {
            "authors": [
                "F. Otto",
                "C. Villani"
            ],
            "title": "Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality",
            "venue": "Journal of Functional Analysis,",
            "year": 2000
        },
        {
            "authors": [
                "L.S. Shapley"
            ],
            "title": "Stochastic games",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1953
        },
        {
            "authors": [
                "A. Sinha",
                "H. Namkoong",
                "J.C. Duchi"
            ],
            "title": "Certifying some distributional robustness with principled adversarial training",
            "venue": "In The Sixth International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "S. Sokota",
                "R. D\u2019Orazio",
                "J. Kolter",
                "N. Loizou",
                "M. Lanctot",
                "I. Mitliagkas",
                "N. Brown",
                "C. Kroer"
            ],
            "title": "A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "year": 2018
        },
        {
            "authors": [
                "T. Suzuki",
                "D. Wu",
                "A. Nitanda"
            ],
            "title": "Convergence of mean-field Langevin dynamics: Time and space discretization, stochastic gradient, and variance reduction",
            "venue": "arXiv preprint arXiv:2306.07221,",
            "year": 2023
        },
        {
            "authors": [
                "A.-S. Sznitman"
            ],
            "title": "Topics in propagation of chaos",
            "venue": "E\u0301cole d\u2019E\u0301te\u0301 de Probabilite\u0301s de Saint-Flour XIX1989,",
            "year": 1991
        },
        {
            "authors": [
                "W. Tao",
                "W. Li",
                "Z. Pan",
                "Q. Tao"
            ],
            "title": "Gradient descent averaging and primal-dual averaging for strongly convex optimization",
            "venue": "In The Thirty-Fifth AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Optimal transport: old and new",
            "venue": "Grundlehren der mathematischen Wissenschaften. Springer Berlin,",
            "year": 2009
        },
        {
            "authors": [
                "C. Wei",
                "C. Lee",
                "M. Zhang",
                "H. Luo"
            ],
            "title": "Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive Markov games",
            "venue": "In 34th Annual Conference on Learning Theory, Proceedings of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "R. Wong"
            ],
            "title": "Asymptotic Approximations of Integrals",
            "year": 1989
        },
        {
            "authors": [
                "L. Xiao"
            ],
            "title": "Dual averaging method for regularized stochastic learning and online optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "S. Zeng",
                "T.T. Doan",
                "J. Romberg"
            ],
            "title": "Regularized gradient descent ascent for two-player zero-sum Markov games",
            "venue": "In 36th Conference on Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Suzuki"
            ],
            "title": "Bk factor and obtain tight short-term error bounds. The leave-one-out error of the modified process can also be characterized as follows. We remark that the arguments in Lemmas C.2 and C.4",
            "year": 2023
        },
        {
            "authors": [
                "Lascu"
            ],
            "title": "6C\u03bd + C\u03bc\u03bd + C\u03bd\u03bd)(t",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The mean-field Langevin dynamics (MFLD) provides powerful theoretical tools to analyze optimization on the space of probability measures such as the training of two-layer neural networks (Mei et al., 2018; Chizat & Bach, 2018). The McKean-Vlasov stochastic process corresponds to the Wasserstein gradient flow minimizing an entropy-regularized convex functional, where the Gaussian noise encourages exploration and ensures global convergence (Hu et al., 2021; Chizat, 2022; Nitanda et al., 2022a). Langevin-based methods are especially attractive as they capture nonlinear aspects of learning as well as admit efficient particle discretizations. However, it remains unclear how to extend beyond single-objective optimization problems in a principled manner.\nIn this work, we develop MFLD for distributional minimax optimization problems. Denote by P2(X ),P2(Y) the spaces of probability measures of finite variance on X ,Y with fixed base measures \u03c1\u00b5, \u03c1\u03bd . We consider the entropy-regularized saddle point problem for a convex-concave functional L : P2(X )\u00d7 P2(Y)\u2192 R with regularization strength or temperature \u03bb > 0,1\nmin \u00b5\u2208P2(X ) max \u03bd\u2208P2(Y)\nL\u03bb(\u00b5, \u03bd), L\u03bb(\u00b5, \u03bd) := L(\u00b5, \u03bd) + \u03bbKL(\u00b5\u2225\u03c1\u00b5)\u2212 \u03bbKL(\u03bd\u2225\u03c1\u03bd). (1) This formulation encompasses all objectives of the form L(\u00b5, \u03bd) = \u222b\u222b\nQ(x, y)\u00b5(dx)\u03bd(dy) for generic nonconvex-nonconcave potentials Q. Such problems naturally arise for example in training generative adversarial networks (Goodfellow et al., 2020; Arjovsky et al., 2017; Hsieh et al., 2019), robust learning (Madry et al., 2018; Sinha et al., 2018) or solving zero-sum games in reinforcement learning (Daskalakis & Panageas, 2019; Domingo-Enrich et al., 2020; Zeng et al., 2022).\nOne is immediately led to consider mean-field Langevin descent ascent (MFL-DA) dynamics, the coupled distribution-dependent stochastic processes which seek to simultaneously minimize L over \u00b5 and maximize over \u03bd (see Appendix A.2 for definitions of functional derivative and convexity):\ndXt = ( \u2212\u2207x \u03b4L\u03b4\u00b5 (\u00b5t, \u03bdt)(Xt) + \u03bb\u2207x log \u03c1 \u00b5(Xt) ) dt+ \u221a 2\u03bb dW\u00b5t , \u00b5t = Law(Xt),\ndYt = ( \u2207y \u03b4L\u03b4\u03bd (\u00b5t, \u03bdt)(Yt) + \u03bb\u2207y log \u03c1 \u03bd(Yt) ) dt+ \u221a 2\u03bb dW \u03bdt , \u03bdt = Law(Yt),\n1Throughout the paper, sub/superscripts such as \u03c1\u00b5, \u03c1\u03bd differentiate quantities related to the min and max variables, and do not indicate dependency on the distributions \u00b5, \u03bd. Our results are easily extended to different temperatures for each variable. We will also present many results for \u00b5 and omit the analogous statement for \u03bd.\nwhere W\u00b5t ,W \u03bd t are independent Brownian motions. Descent ascent methods are more challenging to analyze compared to their single optimization counterparts; it is known that simultaneous updates may display cyclic or divergent behavior even for the simplest matrix games (Daskalakis & Panageas, 2019). For finite strategy spaces, a vigorous line of research has established convergence guarantees by employing optimistic or extragradient update rules; see Cen et al. (2023); Zeng et al. (2022) for an overview of recent literature and applications to Markov games.\nUnfortunately, the convergence of MFL-DA is to the best of our knowledge still an open problem, and mean-field minimax dynamics remains largely unexplored. Existing results fail to establish convergence guarantees (Domingo-Enrich et al., 2020) or only give proofs for near-static flows where one strategy updates extremely or even infinitely quickly compared to the other (Ma & Ying, 2021; Lu, 2022). These works also impose the unrealistic assumption that X ,Y are both compact Riemannian manifolds without boundary. In contrast, we allow X ,Y to be Euclidean spaces. Another fundamental consideration when implementing mean-field dynamics is to account for the errors arising from time discretization and particle approximation in a non-asymptotic manner, the latter referred to as propagation of chaos (Sznitman, 1991). Prior works generally give error bounds that blow up exponentially as training progresses (Mei et al., 2018; De Bortoli et al., 2020); uniformin-time results were proven in the single optimization case only recently by Chen et al. (2022); Suzuki et al. (2023). Hence we are faced with the following research question:\nCan we develop symmetric MFLD algorithms for distributional minimax problems with global convergence guarantees, and further provide uniform-in-time control over discretization errors?"
        },
        {
            "heading": "1.1 SUMMARY OF CONTRIBUTIONS",
            "text": "We address the above problem by proposing mean-field Langevin averaged gradient, a symmetric single-loop algorithm which takes inspiration from dual averaging methods and replaces the MFLDA drift with the historical weighted average. We prove average-iterate convergence to the mixed Nash equilibrium. We also study both time and particle discretization and establish a new uniformin-time propagation of chaos result. The analysis is greatly complicated by the dependence of the interactions on all previous distributions and the techniques developed are of independent interest.\nIn addition, we propose a symmetric double-loop algorithm, mean-field Langevin anchored best response, which realizes the best-response flow suggested in Lascu et al. (2023) via an inner loop running Langevin dynamics. We show that the outer loop updates enjoy last-iterate linear convergence to the mixed Nash equilibrium. Furthermore, we apply our theory to zero-sum Markov games and propose a two-step iterative scheme that finds the regularized Markov perfect equilibrium. Finally, we numerically demonstrate the superior optimality of both algorithms compared to MFL-DA."
        },
        {
            "heading": "2 PROBLEM SETTING AND ASSUMPTIONS",
            "text": "Denote by P2(Rd) the space of probability measures on Rd equipped with the Borel \u03c3-algebra with finite second moment. Let X = RdX ,Y = RdY and L : P2(X )\u00d7P2(Y)\u2192 R be a weakly convexconcave functional. Our objective is to find the mixed Nash equilibrium (MNE) solving (1). Entropic regularization is frequently adopted in minimax optimization to account for imperfect information and ensure good convergence properties (McKelvey & Palfrey, 1995; Sokota et al., 2023).\nWe proceed to state our assumptions which are standard in the MFLD literature (Suzuki et al., 2023). Assumption 1 (Regularity of \u03c1\u00b5, \u03c1\u03bd). We assume that \u03c1\u00b5 = exp(\u2212U\u00b5) and \u03c1\u03bd = exp(\u2212U\u03bd) for r\u00b5- and r\u03bd-strongly convex potentials U\u00b5 : X \u2192 R and U\u03bd : Y \u2192 R, respectively. Furthermore, \u2207xU\u00b5 and\u2207yU\u03bd are R\u00b5- and R\u03bd-Lipschitz, repsectively, and\u2207xU\u00b5(0) = \u2207yU\u03bd(0) = 0. Assumption 2 (Regularity of L for MFL-AG). We assume L is convex-concave and admits C1 functional derivatives \u03b4L\u03b4\u00b5 , \u03b4L \u03b4\u03bd at any (\u00b5, \u03bd), whose gradients are uniformly bounded, and Lipschitz continuous with respect to the input and \u00b5, \u03bd. That is, there exist constants K\u00b5, L\u00b5,M\u00b5 > 0 such that \u2225\u2207x \u03b4L\u03b4\u00b5 (\u00b5, \u03bd)(x)\u2225 \u2264M\u00b5 and\u2225\u2225\u2225\u2207x \u03b4L\u03b4\u00b5 (\u00b5, \u03bd)(x)\u2212\u2207x \u03b4L\u03b4\u00b5 (\u00b5\u2032, \u03bd\u2032)(x\u2032)\u2225\u2225\u2225 \u2264 K\u00b5 \u2225x\u2212 x\u2032\u2225+ L\u00b5(W1(\u00b5, \u00b5\u2032) +W1(\u03bd, \u03bd\u2032)) (2) for all x, x\u2032, \u00b5, and \u03bd. The same properties hold for \u2207y \u03b4L\u03b4\u03bd with K\u03bd , L\u03bd ,M\u03bd > 0.\nAssumption 2 implies in particular that \u03b4L\u03b4\u00b5 is M\u00b5-Lipschitz and \u00b5 7\u2192 L(\u00b5, \u03bd) is M\u00b5-Lipschitz in W1. To present our results at full generality, we do not require boundedness of the functional derivatives \u03b4L\u03b4\u00b5 and \u03b4L \u03b4\u03bd , which would nevertheless simplify some arguments and improve for instance the log-Sobolev constants via the Holley-Stroock argument (Proposition A.4).\nThe KL regularization is enough to assure existence and uniqueness of the MNE via an application of the Kakutani fixed-point theorem (Conforti et al., 2020); see Appendix A.2 for the proof. Proposition 2.1 (Existence and uniqueness of MNE). Under Assumptions 1 and 2, the solution (\u00b5\u2217, \u03bd\u2217) to (1) uniquely exists and satisfies the first-order equations\n\u00b5\u2217 \u221d \u03c1\u00b5 exp ( \u2212 1\u03bb \u03b4L \u03b4\u00b5 (\u00b5 \u2217, \u03bd\u2217) ) , \u03bd\u2217 \u221d \u03c1\u03bd exp ( 1 \u03bb \u03b4L \u03b4\u03bd (\u00b5 \u2217, \u03bd\u2217) ) . (3)\nThe suboptimality of any given pair (\u00b5, \u03bd) is quantified via the Nikaido\u0302-Isoda (NI) error (Nikaido\u0302 & Isoda, 1955),\nNI(\u00b5, \u03bd) := max \u03bd\u2032\u2208P2(Y) L\u03bb(\u00b5, \u03bd\u2032)\u2212 min \u00b5\u2032\u2208P2(X ) L\u03bb(\u00b5\u2032, \u03bd).\nFrom the discussion in the proof of Proposition 2.1, it follows that NI(\u00b5, \u03bd) \u2265 0 and NI(\u00b5, \u03bd) = 0 if and only if \u00b5 = \u00b5\u2217, \u03bd = \u03bd\u2217. A pair (\u00b5, \u03bd) satisfying NI(\u00b5, \u03bd) \u2264 \u03f5 is called an \u03f5-MNE. As is usual in both discrete (Cen et al., 2021; Wei et al., 2021) and continuous (Lu, 2022; Lascu et al., 2023) minimax settings, our main goal is to prove convergence of the NI error along the proposed algorithms, which also implies convergence to the MNE in relative entropy (Lemma 3.5).\nCen et al. (2023) also point out that the MNE serves to approximate the MNE of the unregularized objective L as \u03bb\u2192 0. However, Lmay not possess an MNE at all, e.g. for some bilinear objectives. Domingo-Enrich et al. (2020); Lu (2022) bypass this issue by assumingX ,Y are compact manifolds without boundary, in which case existence is guaranteed by Glicksberg\u2019s theorem. Alternatively, we may restrict the initialization and solution space to KL(\u00b5\u2225\u03c1\u00b5) \u2264 R, KL(\u03bd\u2225\u03c1\u03bd) \u2264 R for some large radius R. Furthermore, if L does possess an MNE, it is possible to adopt the \u03bbt = \u0398(1/ log t) cooling schedule studied in Lu (2022) for which our results can be modified to ensure O(1/ log t) convergence to the unregularized MNE. Nonetheless, our focus is on the regularized problem L\u03bb."
        },
        {
            "heading": "3 MEAN-FIELD LANGEVIN AVERAGED GRADIENT",
            "text": ""
        },
        {
            "heading": "3.1 PROPOSED METHOD",
            "text": "The main obstruction to proving convergence of MFL-DA is the complicated dependency of the proximal Gibbs distribution \u00b5\u0302 for \u00b5 on the opposing policy \u03bd. Motivated by dual averaging methods (Nesterov, 2009; Xiao, 2009; Nitanda et al., 2022b), our idea is simply to take the average of the drift over time so that the slowdown of the rolling average will ensure convergence of the KL gap.\nWe propose the mean-field Langevin averaged gradient (MFL-AG) flow with a weighting scheme (\u03b2t)t\u22650 and temperature \u03bb > 0 as the coupled pair of history-dependent McKean\u2013Vlasov processes\ndXt = \u2212 ( 1\nBt \u222b t 0 \u03b2s\u2207x \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(Xt) ds+ \u03bb\u2207xU\u00b5(Xt) ) dt+ \u221a 2\u03bb dW\u00b5t ,\ndYt =\n( 1\nBt \u222b t 0 \u03b2s\u2207y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(Yt) ds\u2212 \u03bb\u2207yU\u03bd(Yt) ) dt+ \u221a 2\u03bb dW \u03bdt ,\n(4)\nwhere \u00b5t = Law(Xt), \u03bdt = Law(Yt) and W \u00b5 t , W \u03bd t are independent Brownian motions on X and Y , respectively. The corresponding particle algorithm is studied in Section 3.3. By weighting scheme we mean any integrable function \u03b2 = (\u03b2t) : R\u22650 \u2192 R>0 where the normalizing weight Bt = \u222b t 0 \u03b2s ds satisfies Bt \u2192 \u221e and \u03b2t/Bt \u2192 0 as t \u2192 \u221e. These conditions are roughly equivalent to \u2126\u0303(1/t) \u2264 \u03b2t < O\u0303(et) and ensure that the most recent update continues to influence the rolling average, but at an ever-decreasing rate. We will often substitute \u03b2t = tr for a fixed exponent r to obtain explicit convergence rates.\nThe dependence on previous distributions (\u00b5s, \u03bds)s\u2264t serves as a major point of departure from most existing works on mean-field dynamics. Nevertheless, existence and uniqueness of the flow (4) can be verified by extending the classical contraction argument of Sznitman (1991). The proof can be found in Appendix B.1.\nProposition 3.1 (Well-definedness of MFL-AG flow). Under Assumptions 1 and 2, the MFL-AG flow (Xt, Yt) (4) with continuous sample paths uniquely exists for all t \u2208 [0,\u221e) for any initial distribution \u00b50 \u2208 P2(X ), \u03bd0 \u2208 P2(Y).\nThe Fokker-Planck equations corresponding to the system (4) can be formulated as \u2202t\u00b5t = \u2207x \u00b7 ( \u00b5t Bt \u222b t 0 \u03b2s\u2207x \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds+ \u03bb\u00b5t\u2207xU\u00b5 ) + \u03bb\u2206x\u00b5t = \u03bb\u2207x \u00b7 ( \u00b5t\u2207x log \u00b5t \u00b5\u0302t ) ,\n\u2202t\u03bdt = \u2212\u2207y \u00b7 ( \u03bdt Bt \u222b t 0 \u03b2s\u2207y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds) ds\u2212 \u03bb\u03bdt\u2207yU\u03bd ) + \u03bb\u2206y\u03bdt = \u03bb\u2207y \u00b7 ( \u03bdt\u2207y log \u03bdt \u03bd\u0302t ) ,\nwhere \u00b5\u0302t, \u03bd\u0302t are the MFL-AG proximal distributions given as \u00b5\u0302t \u221d \u03c1\u00b5 exp ( \u2212 1 \u03bbBt \u222b t 0 \u03b2s \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds ) , \u03bd\u0302t \u221d \u03c1\u03bd exp ( 1 \u03bbBt \u222b t 0 \u03b2s \u03b4L \u03b4\u03bd (\u00b5s, \u03bds) ds ) (5) which are well-defined due to the strong convexity of U\u00b5, U\u03bd and Assumption 2.\nMFL-AG is similar in spirit to fictitious play methods (Brown, 1951) in the two-player zero-sum game setting with \u03b2t \u2261 1, where each player assumes their opponent has a stationary strategy and optimizes based on the average behavior of the opponent; the ideal fictitious play algorithm would perform the update \u00b5t+1 = \u00b5\u0302t. If \u03b2t is increasing, the algorithm can be considered to undervalue older information which is more suitable for non-stationary environments. However, such methods require exact computation of the optimal response at every step which is generally unfeasible. In contrast, the MFL-AG policies continuously flow towards their response policies at any given time.\nAs usual, \u00b5\u0302t, \u03bd\u0302t satisfy a log-Sobolev inequality which is crucial to controlling the mean-field flows. The mild dependency \u03b1\u00b5 = \u2126(1/dX ) is the only manifestation of dimensional dependence in our results, and can be avoided in cases where the Holley-Stroock argument applies. See Appendix A.1 for details. Proposition 3.2. Let the probability measure \u00b5 \u221d \u03c1\u00b5 exp(\u2212\u03bb\u22121h) \u2208 P2(X ) with \u2225h\u2225Lip \u2264 M\u00b5. Then under Assumption 1, \u00b5 satisfies the log-Sobolev and Talagrand\u2019s inequalities with constant\n\u03b1\u00b5 \u2265 r\u00b5 2 e \u2212\n4M2\u00b5 r\u00b5\u03bb2 \u221a 2dX \u03c0 \u2228\n( 4\nr\u00b5 + ( M\u00b5 r\u00b5\u03bb + \u221a 2 r\u00b5 )2( 2 + dX 2 log e2R\u00b5 r\u00b5 + 4M2\u00b5 r\u00b5\u03bb2 ) e M2\u00b5 2r\u00b5\u03bb2 )\u22121 ."
        },
        {
            "heading": "3.2 CONTINUOUS-TIME CONVERGENCE",
            "text": "We begin by studying the properties of the flow (4). At each time t the policies evolve towards the proximal distributions, and the deceleration of the rolling average allows the flow to catch up with \u00b5\u0302t, \u03bd\u0302t; this observation plays a key part in further analyses. Note that we state many results for only the min policy \u00b5 and omit the analogous statement for \u03bd. Proposition 3.3 (Proximal convergence of MFL-AG flow). Under Assumptions 1 and 2, for the weighting scheme \u03b2t = tr with a fixed exponent r > \u22121 the proximal KL gap is bounded as\nKL(\u00b5t\u2225 \u00b5\u0302t) \u2264 2(r + 1)2M2\u00b5 \u03b13\u00b5\u03bb 4t2 +O(t\u22123).\nSee Appendix B.2 for the proof. It is then clear that if MFL-AG converges, it must converge to the MNE (3) by setting \u00b5\u221e = \u00b5\u0302\u221e, \u03bd\u221e = \u03bd\u0302\u221e.\nFor ordinary MFLD, KL gap convergence of the above type is generally enough to show absolute convergence through entropy sandwich inequalities, see e.g. Nitanda et al. (2022a); Lu (2022). In our case, however, the relative entropy no longer quantifies the optimality gap at (\u00b5t, \u03bdt) since the proximal distributions are no longer \u2018state functions\u2019 and depend on the entire history in (5). Nevertheless, we are able to obtain our first main result, average-iterate convergence of MFL-AG. Our approach, detailed in Appendix B.3, extends conjugate function arguments from dual averaging to the minimax setting and also leverages the preceding O(1/t2) KL gap convergence. Theorem 3.4 (Average-iterate convergence of MFL-AG flow). Denote the weighted average of the MFL-AG distributions up to time t as \u00b5\u0304t = 1Bt \u222b t 0 \u03b2s\u00b5s ds, \u03bd\u0304t = 1Bt \u222b t 0 \u03b2s\u03bds ds. Then under\nAlgorithm 1 Mean-field Langevin Averaged Gradient Require: temperature \u03bb, max epochs K, learning rate \u03b7, number of particles N , exponent r Initialization: X K ,Y K \u2190 \u2205, X1, Y1\nfor k = 1, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do For all particles i = 1, \u00b7 \u00b7 \u00b7 , N sample \u03be\u00b5,ik \u223c N (0, IdX ), \u03be \u03bd,i k \u223c N (0, IdY ) and update\nXik+1 \u2190 Xik \u2212 \u03b7 Bk \u2211k j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(X i k)\u2212 \u03bb\u03b7\u2207xU\u00b5(Xik) + \u221a 2\u03bb\u03b7\u03be\u00b5,ik Y ik+1 \u2190 Y ik + \u03b7 Bk \u2211k j=1 \u03b2j\u2207y \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )(Y i k )\u2212 \u03bb\u03b7\u2207yU\u03bd(Y ik ) + \u221a 2\u03bb\u03b7\u03be\u03bd,ik\nend for for k = 1, \u00b7 \u00b7 \u00b7 ,K do\nSample \u230a\u03b2kN/BK\u230b particles from Xk,Yk and concatenate with X K ,Y K , resp. end for return X K , Y K\nAssumptions 1 and 2, for the weighting scheme \u03b2t = tr with fixed exponent r > 0, the NI error of the averaged pair \u00b5\u0304t, \u03bd\u0304t converges with rate\nNI(\u00b5\u0304t, \u03bd\u0304t) \u2264 ( M2\u00b5 \u03b12\u00b5 + M2\u03bd \u03b12\u03bd ) 4(r + 1)2 r\u03bb2t +O(t\u22122),\nand the leading term is optimized when \u03b2t = t. For the unweighted averaging scheme \u03b2t \u2261 1, NI(\u00b5\u0304t, \u03bd\u0304t) \u2264 ( M2\u00b5 \u03b12\u00b5 + M2\u03bd \u03b12\u03bd ) 4 log t \u03bb2t +O(t\u22121). In light of Lemma 3.5 (proved in Appendix A.2), Theorem 3.4 immediately implies convergence with the same rate of (\u00b5\u0304t, \u03bd\u0304t) in relative entropy to the MNE. Lemma 3.5 (Entropy sandwich lower bound). For any \u00b5 \u2208 P2(X ) and \u03bd \u2208 P2(Y) it holds that KL(\u00b5\u2225\u00b5\u2217) + KL(\u03bd\u2225\u03bd\u2217) \u2264 \u03bb\u22121 NI(\u00b5, \u03bd). The weighting exponent r can be thought of as a hyperparameter controlling the following tradeoff. A larger r tends to give more weight to recent information, which leads to a faster-moving average and slower convergence of the proximal gap (Proposition 3.3). However, it also allows for faster convergence of the weighted average to the MNE. The rate is optimized when r = 1, which is in agreement with works such as Tao et al. (2021) on dual averaging and Guo et al. (2020) on stochastic gradient descent which incorporate averaging with increasing weights \u03b2t \u221d t to obtain improved rates (\u223c 1/t) compared to the unweighted averages (\u223c log t/t)."
        },
        {
            "heading": "3.3 TIME AND SPACE DISCRETIZATION",
            "text": "We now summarize our discretization analysis of MFL-AG developed throughout Appendix C. Our study incorporates both a discrete time step \u03b7 for the Langevin flow and particle approximations for the laws \u00b5, \u03bd. Denote ordered sets of N particles by X = (Xi)Ni=1 \u2208 X N , Y = (Y i)Ni=1 \u2208 Y N\nand the corresponding empirical distributions by \u00b5X = 1N \u2211N i=1 \u03b4Xi , \u03bdY = 1 N \u2211N i=1 \u03b4Y i . The update Xk+1,Yk+1 will depend on the full history (X1:k,Y1:k), where X1 and Y1 are sampled independently from initial distributions \u00b5\u25e6 \u2208 P2(X ) and \u03bd\u25e6 \u2208 P2(Y), respectively. In order to implement gradient averaging, the integral in (4) must be replaced by the discrete-time average with respect to a sequence of weights (\u03b2k)k\u2208N; the cumulative weights are denoted as Bk = \u2211k j=1 \u03b2j . Moreover, the final average of \u00b5X1 , \u00b7 \u00b7 \u00b7 , \u00b5XK may be computed by randomly sampling \u03b2kN/BK particles from each set Xk and concatenating. See Algorithm 1 for details.\nThe propagation of chaos framework recently developed in Chen et al. (2022); Suzuki et al. (2023) relies on a lifted proximal distribution \u00b5\u0302(N) on the configuration space XN . By integrating out the conditioning on the previous step in the continuity equation, this is used to elegantly control the evolution of the joint distribution \u00b5(N) of the N particles. In our case, however, the dependency on the full history (X1:k,Y1:k) cannot be integrated out consistently and must be retained:\n\u00b5\u0302 (N) k (X ) \u221d \u03c1 \u00b5\u2297N (X ) exp ( \u2212 N \u03bbBk \u222b X k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )\u00b5X (dx) ) .\nThis renders the KL gap argument with \u00b5(N) inaccessible and we must work step-by-step with the atomic measures \u00b5Xk , \u03bdYk , which further complicates matters as we cannot directly utilize metrics involving \u00b5Xk in order to avoid the curse of dimensionality. Instead, we prove and exploit the following uniform law of large numbers (Appendix C.3).\nProposition 3.6. Let F : P2(X )\u00d7 P2(Y)\u00d7 X \u2192 R, (\u00b5, \u03bd, x) 7\u2192 F (\u00b5, \u03bd)(x) be a functional such that F (\u00b5, \u03bd) is M\u00b5-Lipschitz on X and further satisfies\n\u2225F (\u00b5, \u03bd)\u2212 F (\u00b5\u2032, \u03bd\u2032)\u2225Lip \u2264 L\u00b5(W1(\u00b5, \u00b5\u2032) +W1(\u03bd, \u03bd\u2032)).\nIf \u03b7 \u2264 \u03b7\u0304 := r\u00b5\u03bb2(L\u00b5+\u03bbR\u00b5)2 \u2227 r\u00b5 4\u03bbR2\u00b5 \u2227 r\u03bd\u03bb2(L\u03bd+\u03bbR\u03bd)2 \u2227 r\u03bd 4\u03bbR2\u03bd and the weight sequence \u03b2k = kr for r \u2265 0, then for all integers k,N it holds that\nEX1:k,Y1:k [\u222b\nX F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u22121)(dx)\n] \u2264 r + 1\nk C1(\u03b7) + C2\n\u221a \u03b7 + C3\u221a N . (6)\nThe same bound also holds for the max policy \u03bd. The constants C2, C3 only depend on problem quantities (including the LSI constants) with at most polynomial order, while the function C1 depends on problem quantities and \u03b7.\nHere, \u03a0 denotes the average of the N pushforward operators along the coordinate projection maps X 7\u2192 Xi. The main idea of the proof is to look backwards in time: close enough so that the dynamics is nearly particle-independent due to the slowdown of the averaged gradient, but far enough to assure exponential convergence to the approximate stationary distribution. Furthermore, the W1Lipschitz leave-one-out argument in Step 3 shows that the O(1/ \u221a N) rate is optimal.\nWe finally present our main discretization error bound; the proof is presented in Appendix C.5.\nTheorem 3.7 (Convergence of discretized MFL-AG). Denote the averaged empirical distributions as \u00b5X k = 1 Bk \u2211k j=1 \u03b2j\u00b5Xj , \u03bdY k = 1 Bk \u2211k j=1 \u03b2j\u03bdYj . If \u03b7 \u2264 \u03b7\u0304 and \u03b2k = kr with r > 0, the MFL-AG discrete update satisfies for all K,N ,\nW 21 (E[\u00b5X K ], \u00b5 \u2217) +W 21 (E[\u03bdY K ], \u03bd\n\u2217) \u2264 (r + 1) 2\nrK C\u03031(\u03b7) + C\u03032\n\u221a \u03b7 + C\u03033\u221a N\nwith similar constants as in Proposition 3.6. If r = 0, the first term is replaced by O(logK/K).\nHence the errors arising from time and particle discretization are separately bounded as O( \u221a \u03b7) and O(1/ \u221a N). An unfortunate byproduct of the perturbation analysis is a roughly \u03b7\u22121/\u03b1\u00b5 order dependency in the constant C1(\u03b7); nonetheless, the convergence in time is O(1/K) for any fixed \u03b7. In particular, for any specified error \u03f5 > 0 we can take \u03b7 = O(\u03f52) and N = O(\u03f5\u22122) as well as K = O(\u03f5\u22121/\u03b1\u00b5\u2227\u03b1\u03bd ) so that W 21 (E[\u00b5X K ], \u00b5 \u2217) +W 21 (E[\u03bdY K ], \u03bd \u2217) < \u03f5. We remark that the squared Wasserstein distance is a natural measure of optimality consistent with the continuous-time rate obtained in Theorem 3.4 in view of Lemma 3.5. Note that Theorem 3.7 quantifies the bias of the MFL-AG outputs, but does not tell us anything about the variance. In Appendix C.6, we give a bound for the expected distance E[W1(\u00b5X k , \u00b5 \u2217)+W1(\u03bdY k , \u03bd \u2217)] and also discuss why the curse of dimensionality is unavoidable in this setting."
        },
        {
            "heading": "4 MEAN-FIELD LANGEVIN ANCHORED BEST RESPONSE",
            "text": ""
        },
        {
            "heading": "4.1 PROPOSED METHOD",
            "text": "Our second proposal builds upon the mean-field best response (MF-BR) flow recently proposed in Lascu et al. (2023). There, the authors prove that the strategies (\u00b5t, \u03bdt)t\u22650 given by the linear flow\nd\u00b5t(x) = \u03b2(\u00b5\u0302t(x)\u2212 \u00b5t(x)) dt, d\u03bdt(x) = \u03b2(\u03bd\u0302t(x)\u2212 \u03bdt(x)) dt, with speed \u03b2 > 0 converge exponentially to the unique MNE, where \u00b5\u0302t \u221d \u03c1\u00b5 exp ( \u2212 1\u03bb \u03b4L \u03b4\u00b5 (\u00b5t, \u03bdt) ) ,\n\u03bd\u0302t \u221d \u03c1\u03bd exp ( 1 \u03bb \u03b4L \u03b4\u03bd (\u03bdt, \u03bdt) ) are the best response proximal distributions, so called because they are the optimal responses against the current policies of all players (rather than the historical average\nin MFL-AG). However, a major weakness of MF-BR is that the flow is not directly realizable by a particle algorithm.\nWe therefore propose the mean-field Langevin anchored best response (MFL-ABR) process by incorporating an inner loop running Langevin dynamics, decoupled by anchoring the gradient at the output (\u00b5k, \u03bdk) of the previous outer loop:\nX\u20200 \u223c \u03c1\u00b5, dX \u2020 t = \u2212 ( \u2207x\n\u03b4L \u03b4\u00b5 (\u00b5k, \u03bdk)(X \u2020 t ) + \u03bb\u2207xU\u00b5(X \u2020 t )\n) dt+ \u221a 2\u03bb dW\u00b5t , 0 \u2264 t \u2264 \u03c4,\nand similarly for Y \u2020t . The outputs at time \u03c4 , denoted by \u00b5 \u2020 k,\u03c4 = Law(X \u2020 \u03c4 ), \u03bd \u2020 k,\u03c4 = Law(Y \u2020 \u03c4 ) serve as approximations of the best response proximal distributions (replacing time t with the discrete index k). The outer loop then performs the discretized MF-BR update,\n\u00b5k+1 = (1\u2212 \u03b2)\u00b5k + \u03b2\u00b5\u2020k,\u03c4 , \u03bdk+1 = (1\u2212 \u03b2)\u03bdk + \u03b2\u03bd \u2020 k,\u03c4 ,\nwhere \u00b50 = \u03c1\u00b5, \u03bd0 = \u03c1\u03bd . The flow can be immediately realized by a simple particle algorithm; see Algorithm 2 in the appendix. A similar method for single convex optimization was also recently implemented in Chen et al. (2023) but without any theoretical guarantees."
        },
        {
            "heading": "4.2 CONTINUOUS-TIME CONVERGENCE",
            "text": "To analyze the convergence of MFL-ABR, we require the following alternative assumptions for L which are taken from Lascu et al. (2023). Assumption 3 (Regularity of L for MFL-ABR). We assume that L is convex-concave and admits C1 functional derivatives which are uniformly bounded as \u2225 \u03b4L\u03b4\u00b5 (\u00b5, \u03bd)\u2225\u221e \u2264 C\u00b5, \u2225 \u03b4L \u03b4\u03bd (\u00b5, \u03bd)\u2225\u221e \u2264 C\u03bd for constants C\u00b5, C\u03bd > 0. Furthermore, L admits second order functional derivatives which are uniformly bounded as \u2225 \u03b4\n2L \u03b4\u00b52 \u2225\u221e \u2264 C\u00b5\u00b5, \u2225 \u03b42L \u03b4\u00b5\u03b4\u03bd \u2225\u221e \u2264 C\u00b5\u03bd , \u2225 \u03b42L \u03b4\u03bd2 \u2225\u221e \u2264 C\u03bd\u03bd and symmetric in the\nsense that \u03b4 2L \u03b4\u00b5\u03b4\u03bd (\u00b5, \u03bd, x, y) = \u03b42L \u03b4\u03bd\u03b4\u00b5 (\u00b5, \u03bd, y, x) for all \u00b5, \u03bd and x \u2208 X , y \u2208 Y .\nExistence and uniqueness of the MNE still hold under this assumption as proved in Lascu et al. (2023). Also, \u00b5\u0302t, \u03bd\u0302t both satisfy the LSI with constant \u03b1 = r\u00b5 exp ( \u2212 4C\u00b5\u03bb ) \u2227 r\u03bd exp ( \u2212 4C\u03bd\u03bb ) by the Holley-Stroock argument; we take the minimum since it dominates the overall convergence rate.\nWe now present the overall convergence result for MFL-ABR. The proof, given in Appendix D.2, is a combination of a time-discretized version of the argument in Lascu et al. (2023) for the outer loop and a TV distance perturbation analysis for the inner loop developed in Appendix D.1. Theorem 4.1 (Convergence of MFL-ABR). The NI error of the MFL-ABR outer loop output after k steps is bounded for a constant C as\nNI(\u00b5k, \u03bdk) \u2264 2(C\u00b5 + C\u03bd) exp(\u2212\u03b2k) + 12\u03bb\u2212 1 2 (C 3 2 \u00b5 + C 3 2 \u03bd ) exp(\u2212\u03b1\u03bb\u03c4) + C\u03b2.\nHence we achieve linear convergence in the outer loop iteration, with a uniform-in-time inner loop error linearly converging in \u03c4 and time discretization error proportional to \u03b2. It follows that an \u03f5-MNE may be obtained in k = O( 1\u03f5 log 1 \u03f5 ) outer loop iterations with \u03b2 = O(\u03f5) and \u03c4 = O(log 1 \u03f5 ).\nWe do not give a discrete-particle analysis of MFL-ABR and instead remark that discretization of the fixed-drift inner loop is trivial, while Theorem 4.1 already covers the outer-loop error due to finite \u03c4 and nonzero \u03b2. The remaining element is particle discretization analysis of the outer loop momentum sampling which we feel strays from the scope of this work."
        },
        {
            "heading": "5 APPLICATIONS TO ZERO-SUM MARKOV GAMES",
            "text": ""
        },
        {
            "heading": "5.1 BILINEAR PROBLEMS",
            "text": "We briefly discuss the case when L is bilinear, that is L(\u00b5, \u03bd) = \u222b\u222b\nQ(x, y)\u00b5(dx)\u03bd(dy) for a C1 reward Q : X \u00d7Y \u2192 R. Assumption 2 is easily verified under the conditions \u2225\u2207xQ\u2225\u221e \u2264 Qx and \u2207xQ is Lix-Lipschitz in each coordinate i = 1, \u00b7 \u00b7 \u00b7 , dX by taking M\u00b5 = Qx, K\u00b5 = L\u00b5 = \u2225Lx\u22252, while Assumption 3 holds if Q is uniformly bounded. The averaged gradient in (4) is then equal to\n1 Bt \u222b t 0 \u03b2s\u2207x \u03b4L\u03b4\u00b5 (\u00b5s, \u03bds)(Xt) ds+ \u03bb\u2207xU \u00b5(Xt) = \u222b Y \u2207xQ(Xt, y)\u03bd\u0304t(dy) + \u03bb\u2207xU \u00b5(Xt);\nthe drift only depends on the history through the average distributions \u00b5\u0304t, \u03bd\u0304t. Therefore, instead of storing and iterating over all previous states which could be computationally prohibitive, we only require the rolling averages to be stored and updated alongside the primary dynamics. In the discrete case, this means that we store the length N arrays X ,Y alongside Xk,Yk and perform\nXik+1 \u2190 Xik \u2212 \u03b7 N \u2211N m=1\u2207xQ(Xik, Y m )\u2212 \u03bb\u03b7\u2207xU\u00b5(Xik) + \u221a 2\u03bb\u03b7\u03be\u00b5,ik , Y ik+1 \u2190 Y ik + \u03b7 N \u2211N m=1\u2207yQ(X m , Y ik )\u2212 \u03bb\u03b7\u2207yU\u03bd(Y ik ) + \u221a 2\u03bb\u03b7\u03be\u03bd,ik .\nWe then discard \u230a\u03b2k+1N/Bk+1\u230b particles from X ,Y and replace with random samples drawn from Xk+1,Yk+1, respectively. After K steps, the arrays X ,Y are returned.\nThus, both algorithms only require 4 arrays to be stored and updated (the inner and outer states for MFL-ABR), incurring no significant computational cost compared to MFL-DA (2 arrays)."
        },
        {
            "heading": "5.2 ZERO-SUM MARKOV GAMES",
            "text": "In this section we outline an application to policy optimization in Markov games. We consider the two-player zero-sum discounted Markov game defined by the tuple M = (S,X ,Y, P, r, \u03b3) with continuous action spaces X = RdX ,Y = RdY , finite state space S, reward r : S \u00d7 X \u00d7Y \u2192 R, transition kernel P : S \u00d7 X \u00d7Y \u2192 P(S) and discount factor \u03b3 \u2208 [0, 1). The strategies of the min and max players are represented by \u00b5 = \u00b5(s) = \u00b5(\u00b7|s) : S \u2192 P2(X ) and \u03bd : S \u2192 P2(Y). The regularized value and Q-functions are defined for all s \u2208 S as\nV \u00b5,\u03bd\u03bb (s) = E [ \u221e\u2211 t=0 \u03b3t ( r(st, xt, yt) + \u03bb log \u00b5(xt|st) \u03c1\u00b5(xt) \u2212 \u03bb log \u03bd(yt|st) \u03c1\u03bd(yt) ) \u2223\u2223\u2223\u2223s0 = s ] , Q\u00b5,\u03bd\u03bb (x, y|s) = r(s, x, y) + \u03b3Es\u2032\u223cP (\u00b7|s,x,y)[V \u00b5,\u03bd \u03bb (s \u2032)],\nwhere the expectation is taken over all trajectories s0, x0, y0, s1, \u00b7 \u00b7 \u00b7 generated by xk \u223c \u00b5(\u00b7|sk), yk \u223c \u03bd(\u00b7|sk) and sk+1 \u223c P (\u00b7|sk, xk, yk). Our goal is to find the MNE which solves the distributional minimax problem min\u00b5:S\u2192P2(X ) max\u03bd:S\u2192P2(Y) V \u00b5,\u03bd \u03bb (s) for all states simultaneously; a detailed introduction to the topic can be found in e.g. Sutton & Barto (2018); Cen et al. (2023). For zero-sum Markov games, the MNE is also called the regularized Markov perfect equilibrium.\nTo this end, we propose the following two-step iterative scheme. For simplicity, we only consider the continuous-time MFLD and assume full knowledge of game quantities as well as the existence and uniqueness of the MNE (\u00b5\u2217, \u03bd\u2217) which is known for finite Markov games (Shapley, 1953).\nStep 1 (Minimax dynamics). Given Q(k), run MFL-AG or MFL-ABR for each state s \u2208 S for sufficient time to obtain an \u03f5L-MNE (\u00b5(k)(s), \u03bd(k)(s)) for the regularized minimax problem\nL\u03bb(\u00b5, \u03bd;Q(k)(s)) := \u222b\u222b X \u00d7Y Q (k)(x, y|s)\u00b5(dx)\u03bd(dy) + \u03bbKL(\u00b5\u2225\u03c1\u00b5)\u2212 \u03bbKL(\u03bd\u2225\u03c1\u03bd).\nStep 2 (Approximate value iteration). For each s, set V (k+1)(s) = L\u03bb(\u00b5(k)(s), \u03bd(k)(s);Q(k)(s)) and update the Q-function by letting Q(k+1) = Q(\u00b7, \u00b7|s) satisfying\u2223\u2223Q(x, y|s)\u2212 r(s, x, y)\u2212 \u03b3Es\u2032\u223cP (\u00b7|s,x,y)[V (k+1)(s\u2032)]\u2223\u2223 \u2264 \u03f5Q, where \u03f5Q > 0 quantifies a model error. In practice, Q(k+1) can be obtained by any offline RL algorithm with function approximation, e.g. a deep neural network, as long as the sup norm of Bellman error to the update is bounded. Moreover, we assume the gradients\u2207xQ,\u2207yQ are bounded and Lipschitz and can be queried freely.\nWith this scheme, we are guaranteed convergence to the MNE. The proof is identical to the discrete strategy case (Cen et al., 2021, Theorem 3) and is included in Appendix A.3 for completeness. Proposition 5.1. The above scheme linearly converges to the optimal value function as\n\u2225V (k) \u2212 V \u2217\u2225\u221e \u2264 \u03b3k\u2225V (0) \u2212 V \u2217\u2225\u221e + \u03f5L + \u03f5Q 1\u2212 \u03b3 .\nThis proposition shows that our two-step algorithm finds the Markov perfect equilibrium at a linear rate of convergence up to a sum of the optimization error for learning the MNE of the inner problem, and the Bellman error for estimating the Q-functions."
        },
        {
            "heading": "6 NUMERICAL EXPERIMENTS",
            "text": "We test our proposed algorithms and compare against ordinary descent ascent dynamics in a simulated setting. We consider dX = dY = 1 and optimize the bilinear objective\nL(\u00b5, \u03bd) = \u222b\u222b Q(x, y)\u00b5(dx)\u03bd(dy), Q(x, y) = (1 + e\u2212(x\u2212y) 2 )\u22121.\nThe sigmoid nonlinearity introduces nontrivial interactions between the min and max policies. We also take regularizers \u03c1\u00b5 = \u03c1\u03bd = N (0, 1) and \u03bb = 0.01. Both MFL-AG with r = 1 and MFL-DA are run with 1,000 particles for 1,000 epochs with learning rate \u03b7 = 0.3. MFL-ABR is run with 1,000 particles for 50 outer loop iterations with 20 inner iterations per loop and \u03b7 = 0.3, \u03b2 = 0.15. We implement the rolling average update for MFL-AG in Section 5.1 and a \u2018warm start\u2019 scheme for MFL-ABR where the inner loop is not re-initialized for stability. We report the results in Figure 1.\nFigure 1(a)-(c) show kernel density plots of the evolving min and max policies \u00b5Xk , \u03bdYk for each algorithm per every 100 epochs. MFL-AG and MFL-ABR converge to similar solutions while MFLDA converges to a different distribution much more rapidly. Figure 1(d) plots convergence speed by computing the sum of the empirical Wasserstein distances W1(\u00b5Xk , \u00b5Xk+1) +W1(\u03bdYk , \u03bdYk+1).\nTo compare the optimality of the outputs (X i,Y i) (i = 0, 1, 2) of the three algorithms, we use the 3-point NI error NIi := maxj L\u03bb(\u00b5X i , \u03bdY j ) \u2212 minj L\u03bb(\u00b5X j , \u03bdY i) which measures relative optimality analogous to a 3 \u00d7 3 payoff matrix. The values are reported in Figure 1(e). While the MFL-DA output is initially the desirable strategy due to its rapid convergence, MFL-AG gradually optimizes and soon dominates MFL-DA with zero error, which is later followed by MFL-ABR. We therefore conclude MFL-AG and MFL-ABR can substantially outperform ordinary descent ascent despite the slower convergence rates."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we developed the first symmetric mean-field Langevin dynamics for entropyregularized minimax problems with global convergence guarantees. We proposed the single-loop MFL-AG algorithm and proved average-iterate convergence to the MNE. We also established a new uniform-in-time analysis of propagation of chaos that accounts for dependence on history using novel perturbative techniques. Furthermore, we proposed the double-loop MFL-ABR algorithm and proved time-discretized linear convergence of the outer loop.\nOur work represents early steps towards an understanding of mean-field dynamics for multiple learning agents and opens up further avenues of investigation. Some interesting directions are developing a single-loop symmetric algorithm with last-iterate convergence, studying nonconvex-nonconcave parametrizations or applications to multi-agent reinforcement learning."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "JK was partially supported by JST CREST (JPMJCR2015) and Toshiba Corporation. KO was partially supported by JST ACT-X (JPMJAX23C4). TS was partially supported by JSPS KAKENHI (20H00576) and JST CREST (JPMJCR2115)."
        },
        {
            "heading": "TABLE OF CONTENTS",
            "text": ""
        },
        {
            "heading": "1 Introduction 1",
            "text": "1.1 Summary of Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2"
        },
        {
            "heading": "2 Problem Setting and Assumptions 2",
            "text": ""
        },
        {
            "heading": "3 Mean-field Langevin Averaged Gradient 3",
            "text": "3.1 Proposed Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3.2 Continuous-Time Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.3 Time and Space Discretization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"
        },
        {
            "heading": "4 Mean-field Langevin Anchored Best Response 6",
            "text": "4.1 Proposed Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.2 Continuous-Time Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"
        },
        {
            "heading": "5 Applications to Zero-Sum Markov Games 7",
            "text": "5.1 Bilinear Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.2 Zero-Sum Markov Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"
        },
        {
            "heading": "6 Numerical Experiments 9",
            "text": ""
        },
        {
            "heading": "7 Conclusion 9",
            "text": ""
        },
        {
            "heading": "A Preliminaries 14",
            "text": "A.1 Optimal Transport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nA.2 Mixed Nash Equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nA.3 Proof of Proposition 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"
        },
        {
            "heading": "B Convergence Analysis of MFL-AG 16",
            "text": "B.1 Proof of Proposition 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nB.2 Proof of Proposition 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nB.3 Proof of Theorem 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "C Time and Space Discretization 22",
            "text": "C.1 Gradient Stopped Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nC.2 Proximal Pushforward Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.3 Proof of Proposition 3.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nC.4 Properties of Conjugate Functionals . . . . . . . . . . . . . . . . . . . . . . . . . 31\nC.5 Proof of Theorem 3.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 C.6 Expected Wasserstein Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37"
        },
        {
            "heading": "D Convergence Analysis of MFL-ABR 40",
            "text": "D.1 Inner Loop Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nD.2 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nAlgorithm 2 Mean-field Langevin Anchored Best Response Require: temperature \u03bb, outer loop iteration K, inner loop iteration L, learning rate \u03b7, number of\nparticles N , exponent r Initialization: X0 \u223c \u03c1\u00b5, Y0 \u223c \u03c1\u03bd\nfor k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do Sample X \u20200 \u223c \u03c1\u00b5, Y \u2020 0 \u223c \u03c1\u03bd\nfor \u2113 = 0, \u00b7 \u00b7 \u00b7 , L\u2212 1 do For all particles i = 1, \u00b7 \u00b7 \u00b7 , N sample \u03be\u00b5,i\u2113 \u223c N (0, IdX ), \u03be \u03bd,i \u2113 \u223c N (0, IdY ) and update\nX\u2020i\u2113+1 \u2190 X \u2020i \u2113 \u2212 \u03b7\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xk , \u03bdYk)(X \u2020i \u2113 )\u2212 \u03bb\u03b7\u2207xU\u00b5(X \u2020i \u2113 ) + \u221a 2\u03bb\u03b7\u03be\u00b5,i\u2113 Y \u2020i\u2113+1 \u2190 Y \u2020i \u2113 + \u03b7\u2207y \u03b4L \u03b4\u03bd (\u00b5Xk , \u03bdYk)(Y \u2020i \u2113 )\u2212 \u03bb\u03b7\u2207yU\u03bd(Y \u2020i \u2113 ) + \u221a 2\u03bb\u03b7\u03be\u03bd,i\u2113\nend for Discard \u230a\u03b2N\u230b particles from Xk,Yk and replace with random samples from X \u2020L ,Y \u2020 L , resp.\nend for return XK ,YK"
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "A.1 OPTIMAL TRANSPORT",
            "text": "We begin by introducing basic concepts and inequalities from optimal transport theory that will be useful in analyzing the behavior of Langevin dynamics.\nDefinition A.1 (p-Wasserstein metric). For p \u2208 [1,\u221e), let Pp(Rd) be the space of probability measures on Rd with finite pth moment. The p-Wasserstein distance between \u00b5, \u03bd \u2208 Pp(Rd) is defined as\nWp(\u00b5, \u03bd) =\n( inf\n\u03b3\u2208\u03a0(\u00b5,\u03bd) \u222b Rd \u2225x\u2212 y\u2225p d\u03b3(x, y) ) 1 p\nwhere \u03a0(\u00b5, \u03bd) denotes the set of joint distributions on Rd\u00d7Rd with marginal laws \u00b5 and \u03bd on the first and second factors, respectively. By Kantorovich-Rubinstein duality, the metric W1 can also be written as\nW1(\u00b5, \u03bd) = sup \u2225f\u2225Lip\u22641 \u222b Rd f d\u00b5\u2212 \u222b Rd f d\u03bd.\nDefinition A.2 (Log-Sobolev inequality). A probability measure \u03bd \u2208 P2(Rd) is said to satisfy the logarithmic Sobolev inequality (LSI) with constant \u03b1 > 0 if for any smooth function f : Rd \u2192 R,\nEnt\u03bd(f 2) := E\u03bd [f2 log f2]\u2212 E\u03bd [f2] logE\u03bd [f2] \u2264\n2 \u03b1 E\u03bd [\u2225\u2207f\u222522].\nFor any measure \u00b5 \u2208 P2(Rd) absolutely continuous with respect to \u03bd, the LSI implies that KL divergence is upper bounded by the relative Fisher information,\nKL(\u00b5\u2225\u03bd) \u2264 1 2\u03b1 E\u00b5 [\u2225\u2225\u2225\u2225\u2207 log d\u00b5d\u03bd \u2225\u2225\u2225\u22252 2 ] .\nProposition A.3 (Bakry & E\u0301mery, 1985). If f : Rd \u2192 R is a function such that \u22072f \u2ab0 \u03b1Id, the probability density p \u221d exp(\u2212f) satisfies the LSI with constant \u03b1. Proposition A.4 (Holley & Stroock, 1987). Let p be a density on Rd satisfying the LSI with constant \u03b1. For a bounded function B : Rd \u2192 R, the perturbed distribution\npB(x)dx = exp(B(x))p(x)\nEp[exp(B(x))] dx\nalso satisfies the LSI with constant \u03b1/ exp(4 \u2225B\u2225\u221e).\nDefinition A.5 (Poincare\u0301 and Talagrand\u2019s inequalities). A probability measure \u03bd \u2208 P2(Rd) is said to satisfy the Poincare\u0301 inequality with constant \u03b1 > 0 if for any smooth function f : Rd \u2192 R,\nVar\u03bd(f) := E\u03bd [f2]\u2212 (E\u03bd [f ])2 \u2264 1\n\u03b1 E\u03bd [\u2225\u2207f\u222522].\nMoreover, \u03bd is said to satisfy Talagrand\u2019s inequality with constant \u03b1 > 0 if for any measure \u00b5 \u2208 P2(Rd) absolutely continuous with respect to \u03bd, the 2-Wasserstein distance is upper bounded as\n\u03b1 2 W 22 (\u00b5, \u03bd) \u2264 KL(\u00b5\u2225\u03bd).\nIf \u03bd satisfies the LSI with constant \u03b1, then it satisfies the Poincare\u0301 inequality with the same constant. We also have the following implication.\nTheorem A.6 (Otto & Villani, 2000). If a probability measure \u03bd \u2208 P2(Rd) satisfies the LSI with constant \u03b1, then it satisfies Talagrand\u2019s inequality with the same constant.\nProof of Proposition 3.2. We take the stronger of the two bounds in Lemma 2.1 of Bardet et al. (2018) and Theorem 2.7 of Cattiaux & Guillin (2022); the latter removes the exponential dependency on dX in exchange for more complicated polynomial terms. See Lemma 6 of Suzuki et al. (2023) for more details."
        },
        {
            "heading": "A.2 MIXED NASH EQUILIBRIUM",
            "text": "Definition A.7 (functional derivative). Let F be a functional on P2(Rd). The functional derivative \u03b4F \u03b4\u00b5 at \u00b5 \u2208 P2(R d) is defined as a functional P2(Rd)\u00d7 Rd \u2192 R satisfying for all \u03bd \u2208 P2(Rd),\nd\nd\u03f5 F (\u00b5+ \u03f5(\u03bd \u2212 \u00b5)) \u2223\u2223\u2223\u2223 \u03f5=0 = \u222b Rd \u03b4F \u03b4\u00b5 (\u00b5)(x)(\u03bd \u2212 \u00b5)(dx).\nAs the functional derivative is defined up to additive constants, we impose the additional condition\u222b Rd \u03b4F \u03b4\u00b5 (\u00b5) d\u00b5 = 0. Furthermore, F is defined to be convex if its satisfies the convexity condition for all \u03bd \u2208 P2(Rd):\nF (\u03bd) \u2265 F (\u00b5) + \u222b Rd \u03b4F \u03b4\u00b5 (\u00b5)(x)(\u03bd \u2212 \u00b5)(dx).\nFinally, F is defined to be concave if \u2212F is convex.\nProof of Proposition 2.1. Recall that the 2-Wasserstein distance is finite and metrizes weak convergence on P2(Rd) (Villani, 2009, Theorem 6.9). Also, the divergence \u00b5 7\u2192 KL(\u00b5\u2225\u03c1\u00b5) is proper and lower semi-continuous with respect to the weak topology (Lanzetti et al., 2022). Furthermore, \u03c1\u00b5 satisfies Talagrand\u2019s inequality with constant r\u00b5 by Theorem A.6 so that the map \u00b5 7\u2192 L\u03bb(\u00b5, \u03bd) is strongly convex. Hence the minimizer of \u00b5 7\u2192 L\u03bb(\u00b5, \u03bd) is unique, and similarly the maximizer of \u03bd 7\u2192 L\u03bb(\u00b5, \u03bd) is unique. Existence of the MNE is now guaranteed by Theorem 3.6 in Conforti et al. (2020) by verifying Assumption 2.1 and conditions (i)-(iii).\nFor uniqueness, suppose to the contrary that (\u00b5\u2217, \u03bd\u2217), (\u00b5\u0303\u2217, \u03bd\u0303\u2217) are two distinct solutions of (1). The optimality conditions yield the chain of strict inequalities\nL\u03bb(\u00b5\u2217, \u03bd\u2217) > L\u03bb(\u00b5\u2217, \u03bd\u0303\u2217) > L\u03bb(\u00b5\u0303\u2217, \u03bd\u0303\u2217) > L\u03bb(\u00b5\u0303\u2217, \u03bd\u2217) > L\u03bb(\u00b5\u2217, \u03bd\u2217),\na contradiction. Finally, the first-order conditions follow from Corollary 3.3 in Conforti et al. (2020), adjusting the base measures as to be different for \u00b5, \u03bd.\nProof of Lemma 3.5. By convex-concavity of L and the first-order condition (3),\nNI(\u00b5, \u03bd) \u2265 L\u03bb(\u00b5, \u03bd\u2217)\u2212 L\u03bb(\u00b5\u2217, \u03bd) \u2265 \u222b X \u03b4L \u03b4\u00b5 (\u00b5\u2217, \u03bd\u2217)(\u00b5\u2212 \u00b5\u2217)(dx) + \u03bbKL(\u00b5\u2225\u03c1\u00b5)\u2212 \u03bbKL(\u03bd\u2217\u2225\u03c1\u03bd)\n\u2212 \u222b Y \u03b4L \u03b4\u03bd (\u00b5\u2217, \u03bd\u2217)(\u03bd \u2212 \u03bd\u2217)(dy)\u2212 \u03bbKL(\u00b5\u2217\u2225\u03c1\u00b5) + \u03bbKL(\u03bd\u2225\u03c1\u03bd)\n= \u2212 \u222b X \u03bb log \u00b5\u2217 \u03c1\u00b5 (\u00b5\u2212 \u00b5\u2217)(dx) + \u03bbKL(\u00b5\u2225\u03c1\u00b5)\u2212 \u03bbKL(\u03bd\u2217\u2225\u03c1\u03bd)\n\u2212 \u222b Y \u03bb log \u03bd\u2217 \u03c1\u03bd (\u03bd \u2212 \u03bd\u2217)(dy)\u2212 \u03bbKL(\u00b5\u2217\u2225\u03c1\u00b5) + \u03bbKL(\u03bd\u2225\u03c1\u03bd)\n= \u03bbKL(\u00b5\u2225\u00b5\u2217) + \u03bbKL(\u03bd\u2225\u03bd\u2217)."
        },
        {
            "heading": "A.3 PROOF OF PROPOSITION 5.1",
            "text": "We use the bound | L\u03bb(\u00b5, \u03bd)\u2212L\u03bb(\u00b5\u2217, \u03bd\u2217)| \u2264 NI(\u00b5, \u03bd) which can be shown by the following string of inequalities,\nL\u03bb(\u00b5, \u03bd)\u2212 L\u03bb(\u00b5\u2217, \u03bd\u2217) \u2264 max \u03bd\u2032 L\u03bb(\u00b5, \u03bd\u2032)\u2212 L\u03bb(\u00b5\u2217, \u03bd) \u2264 max \u03bd\u2032 L\u03bb(\u00b5, \u03bd\u2032)\u2212min \u00b5\u2032 L\u03bb(\u00b5\u2032, \u03bd), L\u03bb(\u00b5, \u03bd)\u2212 L\u03bb(\u00b5\u2217, \u03bd\u2217) \u2265 min \u00b5\u2032 L\u03bb(\u00b5\u2032, \u03bd)\u2212 L\u03bb(\u00b5, \u03bd\u2217) \u2265 min \u00b5\u2032 L\u03bb(\u00b5\u2032, \u03bd)\u2212max \u03bd\u2032 L\u03bb(\u00b5, \u03bd\u2032).\nDenoting the ideal minimax update in Step 1 as\nV\u0303 (k+1)(s) = min \u00b5\u2208P2(X ) max \u03bd\u2208P2(Y) L\u03bb(\u00b5, \u03bd;Q(k)(s)),\nthis implies |V (k+1)(s)\u2212 V\u0303 (k+1)(s)| \u2264 NI(\u00b5(k)(s), \u03bd(k)(s)) \u2264 \u03f5L. Now denote the ideal value iteration in Step 2 as\nQ\u0303(k)(s) = r(s, x, y) + \u03b3Es\u2032\u223cP (\u00b7|s,x,y)[V (k)(s)]\nand note that the optimal value and Q-functions V \u2217 = V \u00b5 \u2217,\u03bd\u2217\n\u03bb , Q \u2217 = Q\u00b5\n\u2217,\u03bd\u2217\n\u03bb satisfy the Bellman equation Q\u2217(x, y|s) = r(s, x, y) + \u03b3Es\u2032\u223cP (\u00b7|s,x,y)[V \u2217(s\u2032)]. Hence we bound\n\u2225V (k+1) \u2212 V \u2217\u2225\u221e \u2264 \u03f5L + \u2225V\u0303 (k+1) \u2212 V \u2217\u2225\u221e \u2264 \u03f5L + sup\n\u00b5,\u03bd,s \u2223\u2223L\u03bb(\u00b5, \u03bd;Q(k)(s))\u2212 L\u03bb(\u00b5, \u03bd;Q\u2217(s))\u2223\u2223 \u2264 \u03f5L + \u2225Q(k) \u2212Q\u2217\u2225\u221e \u2264 \u03f5L + \u2225Q(k) \u2212 Q\u0303(k)\u2225\u221e + \u2225Q\u0303(k) \u2212Q\u2217\u2225\u221e \u2264 \u03f5L + \u03f5Q + \u03b3\u2225V (k) \u2212 V \u2217\u2225\u221e.\nTherefore by Gronwall\u2019s lemma we conclude that\n\u2225V (k) \u2212 V \u2217\u2225\u221e \u2264 \u03b3k\u2225V (0) \u2212 V \u2217\u2225\u221e + \u03f5L + \u03f5Q 1\u2212 \u03b3 ."
        },
        {
            "heading": "B CONVERGENCE ANALYSIS OF MFL-AG",
            "text": ""
        },
        {
            "heading": "B.1 PROOF OF PROPOSITION 3.1",
            "text": "Some definitions are in order. Denote by CX ,T = C([0, T ],X ) the space of continuous sample paths on X and byM(CX ,T ) the space of probability measures on CX ,T . We define two versions of the lifted 1-Wasserstein distance onM(CX ,T ) as\nW\u03031,T (\u00b5, \u00b5 \u2032) = inf\n\u03b3 \u222b sup t\u2264T \u2225\u03c9(t)\u2212 \u03c9\u2032(t)\u2225 d\u03b3(\u03c9, \u03c9\u2032) \u2227 1,\nW1,T (\u00b5, \u00b5 \u2032) = inf\n\u03b3 \u222b sup t\u2264T \u2225\u03c9(t)\u2212 \u03c9\u2032(t)\u2225 \u2227 1 d\u03b3(\u03c9, \u03c9\u2032)\nwhere the infimum runs over all couplings \u03b3 \u2208 M(CX ,T \u00d7 CX ,T ) with marginal laws \u00b5, \u00b5\u2032. The inner truncated metric W1,T is complete, nondecreasing in T and metrizes the weak topology on M(CX ,T ) (Dobrushin, 1970); the outer truncation W\u03031,T serves to upper bound W1,T . We repeat the construction for Y and extend W1,T , W\u03031,T to the product spaceM(CX ,T )\u00d7M(CY,T ) as\nW1,T ((\u00b5, \u03bd), (\u00b5 \u2032, \u03bd\u2032)) =W1,T (\u00b5, \u00b5 \u2032) +W1,T (\u03bd, \u03bd \u2032),\netc. Now define \u03a6 :M(CX ,T )\u00d7M(CY,T )\u2192M(CX ,T )\u00d7M(CY,T ) as the map which associates to the pair (\u00b5, \u03bd) the laws of the stochastic processes (Xt)t\u2264T , (Yt)t\u2264T ,\nXt = X0 \u2212 \u222b t 0 1 Bs \u222b s 0 \u03b2r\u2207x \u03b4L \u03b4\u00b5 (\u00b5r, \u03bdr)(Xs) dr + \u03bb\u2207xU\u00b5(Xs) ds+ \u221a 2\u03bbW\u00b5t ,\nYt = Y0 + \u222b t 0 1 Bs \u222b s 0 \u03b2r\u2207y \u03b4L \u03b4\u03bd (\u00b5r, \u03bdr)(Ys) dr \u2212 \u03bb\u2207yU\u03bd(Ys) ds+ \u221a 2\u03bbW \u03bdt\nfor 0 \u2264 t \u2264 T , where \u00b5t, \u03bdt denote the marginal distributions of \u00b5, \u03bd at time t and in particular \u00b50, \u03bd0 are the prescribed initial distributions. A solution to (4) then corresponds precisely to a fixed point of \u03a6.\nLemma B.1. There exists a constant CT > 0 so that for any 0 \u2264 t \u2264 T ,\nW\u03031,t(\u03a6(\u00b5, \u03bd),\u03a6(\u00b5 \u2032, \u03bd\u2032)) \u2264 CT \u222b t 0 W\u03031,s((\u00b5, \u03bd), (\u00b5 \u2032, \u03bd\u2032)) ds.\nProof. First note that for any 0 \u2264 s \u2264 t \u2264 T ,\nW\u03031,t(\u00b5, \u00b5 \u2032) \u2265 inf\n\u03b3\n\u222b \u2225\u03c9(s)\u2212 \u03c9\u2032(s)\u2225 d\u03b3(\u03c9, \u03c9\u2032) \u2227 1 \u2265W1(\u00b5s, \u00b5\u2032s) \u2227 1.\nLet (X \u2032t)t\u2264T , (Y \u2032 t )t\u2264T denote the synchronous processes\nX \u2032t = X0 \u2212 \u222b t 0 1 Bs \u222b s 0 \u03b2r\u2207x \u03b4L \u03b4\u00b5 (\u00b5\u2032r, \u03bd \u2032 r)(X \u2032 s) dr + \u03bb\u2207xU\u00b5(X \u2032s) ds+ \u221a 2\u03bbW\u00b5t ,\nY \u2032t = Y0 + \u222b t 0 1 Bs \u222b s 0 \u03b2r\u2207y \u03b4L \u03b4\u03bd (\u00b5\u2032r, \u03bd \u2032 r)(Y \u2032 s ) dr \u2212 \u03bb\u2207yU\u03bd(Y \u2032s ) ds+ \u221a 2\u03bbW \u03bdt\ncorresponding to another pair of distributions (\u00b5\u2032, \u03bd\u2032). Then by Assumption 2,\nsup s\u2264t \u2225Xs \u2212X \u2032s\u2225\n\u2264 \u222b t 0 sup r\u2264s \u2225\u2225\u2225\u2225\u2207x \u03b4L\u03b4\u00b5 (\u00b5r, \u03bdr)(Xs)\u2212\u2207x \u03b4L\u03b4\u00b5 (\u00b5\u2032r, \u03bd\u2032r)(X \u2032s) \u2225\u2225\u2225\u2225+ \u03bb \u2225\u2207xU\u00b5(Xs)\u2212\u2207xU\u00b5(X \u2032s)\u2225 ds\n\u2264 \u222b t 0 (K\u00b5 + \u03bbR\u00b5) \u2225Xs \u2212X \u2032s\u2225+ sup r\u2264s L\u00b5(W1(\u00b5r, \u00b5 \u2032 r) +W1(\u03bdr, \u03bd \u2032 r)) \u2227 2M\u00b5 ds\n\u2264 (K\u00b5 + \u03bbR\u00b5) \u222b t 0 \u2225Xs \u2212X \u2032s\u2225 ds+ (L\u00b5 \u2228 2M\u00b5) \u222b t 0 sup r\u2264s W1(\u00b5r, \u00b5 \u2032 r) \u2227 1 +W1(\u03bdr, \u03bd\u2032r) \u2227 1 ds.\nThus by Gronwall\u2019s lemma we obtain\nsup s\u2264t \u2225Xs \u2212X \u2032s\u2225 \u2264 (L\u00b5 \u2228 2M\u00b5)e(K\u00b5+\u03bbR\u00b5)T \u222b t 0 sup r\u2264s W1(\u00b5r, \u00b5 \u2032 r) \u2227 1 +W1(\u03bdr, \u03bd\u2032r) \u2227 1 ds."
        },
        {
            "heading": "Then defining the constant CT = (L\u00b5 \u2228 2M\u00b5)e(K\u00b5+\u03bbR\u00b5)T + (L\u03bd \u2228 2M\u03bd)e(K\u03bd+\u03bbR\u03bd)T , by taking",
            "text": "the joint distribution coupling of (Xt)t\u2264T and (X \u2032t)t\u2264T we have\nW\u03031,t(\u03a6(\u00b5, \u03bd),\u03a6(\u00b5 \u2032, \u03bd\u2032)) \u2264 CT \u222b t 0 sup r\u2264s W1(\u00b5r, \u00b5 \u2032 r) \u2227 1 +W1(\u03bdr, \u03bd\u2032r) \u2227 1 ds,\nwhich proves the lemma.\nWe now use the contraction property to prove Proposition 3.1. Starting at any (\u00b5, \u03bd) and recursively applying Lemma B.1, we have\nW\u03031,T (\u03a6 k+1(\u00b5, \u03bd),\u03a6k(\u00b5, \u03bd)) \u2264 CkT \u222b T 0 \u222b t1 0 \u00b7 \u00b7 \u00b7 \u222b tk\u22121 0 W\u03031,tk(\u03a6(\u00b5, \u03bd), (\u00b5, \u03bd)) dtk \u00b7 \u00b7 \u00b7 dt2 dt1\n\u2264 C k TT k\nk! W\u03031,T (\u03a6(\u00b5, \u03bd), (\u00b5, \u03bd)),\nso that W\u03031,T (\u03a6k+1(\u00b5, \u03bd),\u03a6k(\u00b5, \u03bd))\u2192 0 as k \u2192\u221e. Since W\u03031,T upper boundsW1,T , the sequence (\u03a6k(\u00b5, \u03bd))k\u22650 is Cauchy and therefore converges to a fixed point of \u03a6 due to the completeness of M(CX ,T ) \u00d7M(CY,T ) with respect to W1,T . Similarly, recursively applying Lemma B.1 to two fixed points (\u00b5, \u03bd), (\u00b5\u2032, \u03bd\u2032) yields\nW1,T ((\u00b5, \u03bd), (\u00b5 \u2032, \u03bd\u2032)) \u2264 W\u03031,T ((\u00b5, \u03bd), (\u00b5\u2032, \u03bd\u2032)) \u2264\nCkTT k\nk! W\u03031,T ((\u00b5, \u03bd), (\u00b5\n\u2032, \u03bd\u2032))\u2192 0,\nhence the fixed point is unique. Finally, truncating the obtained flows ((\u00b5t)t\u2264T , (\u03bdt)t\u2264T ) at time T \u2032 < T must again yield the fixed point inM(CX ,T \u2032) \u00d7M(CY,T \u2032) so that we may consistently extend the flows to all time t \u2208 [0,\u221e)."
        },
        {
            "heading": "B.2 PROOF OF PROPOSITION 3.3",
            "text": "Write the normalization factor for \u00b5\u0302t as\nZ\u00b5t = \u222b X exp ( \u2212 1 \u03bbBt \u222b t 0 \u03b2s \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds ) \u03c1\u00b5(dx).\nWe first compute the time derivative of the proximal distribution,\n\u2202t log \u00b5\u0302t = \u2212\u2202t logZ \u00b5 t \u2212 \u03b2t \u03bbBt \u03b4L \u03b4\u00b5 (\u00b5t, \u03bdt) + \u03b2t \u03bbB2t \u222b t 0 \u03b2s \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds\n= \u222b X ( \u03b2t \u03bbBt \u03b4L \u03b4\u00b5 (\u00b5t, \u03bdt)\u2212 \u03b2t \u03bbB2t \u222b t 0 \u03b2s \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds ) \u00b5\u0302t(dx\u0303)\n\u2212 \u03b2t \u03bbBt \u03b4L \u03b4\u00b5 (\u00b5t, \u03bdt) + \u03b2t \u03bbB2t \u222b t 0 \u03b2s \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds.\nRoughly speaking, the proximal evolution speed is O(\u03b2t/Bt) which converges to zero as new information is continually downscaled. However, the maximum total displacement is O(logBt) \u2192 \u221e, ensuring that the algorithm does not prematurely stop before reaching equilibrium.\nThe time derivative of the KL gap can then be controlled by translating back into KL distance as \u2202tKL(\u00b5t\u2225 \u00b5\u0302t) = \u222b X ( log \u00b5t \u00b5\u0302t ) \u2202t\u00b5t(dx)\u2212 \u222b X (\u2202t log \u00b5\u0302t)\u00b5t(dx)\n= \u2212\u03bb \u222b X \u2225\u2225\u2225\u2225\u2207x log \u00b5t\u00b5\u0302t \u2225\u2225\u2225\u22252 2 \u00b5t(dx)\n+ \u03b2t \u03bbBt \u222b X ( \u03b4L \u03b4\u00b5 (\u00b5t, \u03bdt)\u2212 1 Bt \u222b t 0 \u03b2s \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds ) (\u00b5t \u2212 \u00b5\u0302t)(dx)\n\u2264 \u22122\u03b1\u03bb \u00b7KL(\u00b5t\u2225 \u00b5\u0302t) + 2M\u00b5\u03b2t \u03bbBt W1(\u00b5t, \u00b5\u0302t)\nby Proposition 3.2. The Wasserstein term is further bounded via Talagrand\u2019s inequality as W1(\u00b5t, \u00b5\u0302t) \u2264W2(\u00b5t, \u00b5\u0302t) \u2264 \u221a 2\n\u03b1\u00b5 KL(\u00b5t, \u00b5\u0302t).\nHence\n\u2202t \u221a KL(\u00b5t\u2225 \u00b5\u0302t) \u2264 \u2212\u03b1\u00b5\u03bb \u221a KL(\u00b5t\u2225 \u00b5\u0302t) +\nM\u00b5\u03b2t \u03bbBt\n\u221a 2\n\u03b1\u00b5\nand using an integrating factor, we conclude (starting from an arbitrary small but positive time t0 to avoid potential singularities at t = 0)\nexp(\u03b1\u00b5\u03bbt) \u221a\nKL(\u00b5t\u2225 \u00b5\u0302t) \u2264 M\u00b5 \u03bb\n\u221a 2\n\u03b1\u00b5 \u222b t t0 \u03b2s Bs exp(\u03b1\u00b5\u03bbs) ds+ exp(\u03b1\u00b5\u03bbt0) \u221a KL(\u00b5t0\u2225 \u00b5\u0302t0).\nIn particular, for the weight scheme \u03b2t = tr with r > \u22121, by employing the asymptotic expansion of the exponential integral (Wong, 1989, Section I.4)\nEi(z) = \u222b z \u2212\u221e exp(t) t dt = exp(z) z ( n\u2211 k=0 k! zk +O(|z|\u2212(n+1)) ) we conclude that\nKL(\u00b5t\u2225 \u00b5\u0302t) \u2264 exp(\u22122\u03b1\u00b5\u03bbt)\n( (r + 1)M\u00b5\n\u03bb\n\u221a 2\n\u03b1\u00b5 Ei(\u03b1\u00b5\u03bbt) + const.\n)2\n\u2264 2(r + 1)2M2\u00b5 \u03b13\u00b5\u03bb 4t2 +O(t\u22123).\nWe also show a boundedness result which guarantees that the flow is in a sense well-behaved.\nLemma B.2. The MFL-AG flow (\u00b5t, \u03bdt) satisfies for all t \u2265 0,\nKL(\u00b5t\u2225\u03c1\u00b5) \u2264 KL(\u00b50\u2225\u03c1\u00b5) \u2228 M2\u00b5 2r\u00b5\u03bb2 and KL(\u03bdt\u2225\u03c1\u03bd) \u2264 KL(\u03bd0\u2225\u03c1\u03bd) \u2228 M2\u03bd 2r\u03bd\u03bb2 .\nProof. The density \u03c1\u00b5 satisfies the LSI with constant r\u00b5 by Proposition A.3 so that we may derive \u2202tKL(\u00b5t\u2225\u03c1\u00b5) = \u222b X ( log \u00b5t \u03c1\u00b5 ) \u2202t\u00b5t(dx)\n= \u2212\u03bb \u222b X \u2207x log \u00b5t \u03c1\u00b5 \u00b7 \u2207x log \u00b5t \u00b5\u0302t \u00b5t(dx)\n= \u2212\u03bb \u222b X \u2225\u2225\u2225\u2225\u2207x log \u00b5t\u03c1\u00b5 \u2225\u2225\u2225\u22252 2 \u00b5t(dx) + \u03bb \u222b X \u2207x log \u00b5t \u03c1\u00b5 \u00b7 \u2207x log \u00b5\u0302t \u03c1\u00b5 \u00b5t(dx)\n\u2264 \u2212\u03bb 2 \u222b X \u2225\u2225\u2225\u2225\u2207x log \u00b5t\u03c1\u00b5 \u2225\u2225\u2225\u22252 2 \u00b5t(dx) + \u03bb 2 \u222b X \u2225\u2225\u2225\u2225\u2207x log \u00b5\u0302t\u03c1\u00b5 \u2225\u2225\u2225\u22252 2 \u00b5t(dx) \u2264 \u2212r\u00b5\u03bb \u00b7KL(\u00b5t\u2225\u03c1\u00b5) + M2\u00b5 2\u03bb .\nThe assertion is then proved by Gronwall\u2019s inequality."
        },
        {
            "heading": "B.3 PROOF OF THEOREM 3.4",
            "text": "We first introduce two conjugate-type auxiliary functionals and state some properties.\nLemma B.3. Given Lipschitz functions \u03b6\u00b5 : X \u2192 R, \u03b6\u03bd : Y \u2192 R, for the pair of probability measures \u00b5 \u2208 P2(X ), \u03bd \u2208 P2(Y) define the time-dependent functional\nJt(\u00b5, \u03bd|\u03b6\u00b5, \u03b6\u03bd) = \u2212 \u222b X \u03b6\u00b5(\u00b5\u2212 \u03c1\u00b5)(dx) + \u222b Y \u03b6\u03bd(\u03bd \u2212 \u03c1\u03bd)(dy)\u2212 \u03bbBt(KL(\u00b5\u2225\u03c1\u00b5) + KL(\u03bd\u2225\u03c1\u03bd))."
        },
        {
            "heading": "Then the maximum",
            "text": "J\u0302t(\u03b6\n\u00b5, \u03b6\u03bd) = max \u00b5\u2208P2(X ) max \u03bd\u2208P2(Y) Jt(\u00b5, \u03bd|\u03b6\u00b5, \u03b6\u03bd)\nexists for all t > 0 and is uniquely attained by the pair of probability distributions defined as \u00b5\u0302t(\u03b6 \u00b5) \u221d exp(\u2212(\u03bbBt)\u22121\u03b6\u00b5 \u2212 U\u00b5) and \u03bd\u0302t(\u03b6\u03bd) \u221d exp((\u03bbBt)\u22121\u03b6\u03bd \u2212 U\u03bd).\nProof. Since Jt(\u00b5, \u03bd|\u03b6\u00b5, \u03b6\u03bd) decomposes into terms depending only on \u00b5 and \u03bd, respectively, the proof is similar to that of Proposition 2.1. That is, \u00b5 7\u2192 KL(\u00b5\u2225\u03c1\u00b5) is lower semi-continuous and strongly convex with respect to the 2-Wasserstein metric by Talagrand\u2019s inequality for \u03c1\u00b5 so that combined with any linear functional,\nargmax \u00b5\u2208P2(X )\n\u03b6\u00b5(\u00b5\u2212 \u03c1\u00b5)(dx)\u2212 \u03bbBt \u00b7KL(\u00b5\u2225\u03c1\u00b5)\nhas a unique maximizer \u00b5\u0302t(\u03b6 \u00b5) which moreover is given by the stated first-order condition.\nThe following properties are direct extensions of standard conjugacy results in convex analysis, see e.g. Hiriart-Urruty & Lemare\u0301chal (2004), Section E.\nLemma B.4. The functional J\u0302t(\u03b6\u00b5, \u03b6\u03bd) satisfies the following properties.\n(i) J\u0302t is nonnegative and convex in both arguments.\n(ii) J\u0302t admits functional derivatives at any (\u03b6\u00b5, \u03b6\u03bd) which are given as\n\u03b4J\u0302t \u03b4\u03b6\u00b5 (\u03b6\u00b5, \u03b6\u03bd) = \u2212 \u00b5\u0302t(\u03b6\u00b5) + \u03c1\u00b5, \u03b4J\u0302t \u03b4\u03b6\u03bd (\u03b6\u00b5, \u03b6\u03bd) = \u03bd\u0302t(\u03b6 \u03bd)\u2212 \u03c1\u03bd .\n(iii) The derivative with respect to time is bounded as\n\u2202tJ\u0302t(\u03b6 \u00b5, \u03b6\u03bd) \u2264 \u2212\u03bb\u03b2t(KL(\u00b5\u0302t(\u03b6\u00b5)\u2225\u03c1\u00b5) + KL(\u03bd\u0302t(\u03b6\u03bd)\u2225\u03c1\u03bd)).\nProof. (i) Note that J\u0302t \u2265 0 by taking \u00b5 = \u03c1\u00b5, \u03bd = \u03c1\u03bd , and J\u0302t is convex in both \u03b6\u00b5, \u03b6\u03bd as it is a pointwise maximum of affine functionals.\n(ii) Due to the explicit dependency of \u00b5\u0302t(\u03b6 \u00b5) on \u03b6\u00b5, J\u0302t(\u03b6\u00b5, \u03b6\u03bd) = Jt(\u00b5\u0302t(\u03b6 \u00b5), \u00b5\u0302t(\u03b6 \u03bd)|\u03b6\u00b5, \u03b6\u03bd) admits a functional derivative with respect to \u03b6\u00b5 and\n\u03b4J\u0302t \u03b4\u03b6\u00b5 (\u03b6\u00b5, \u03b6\u03bd) = \u2212 \u00b5\u0302t(\u03b6\u00b5) + \u03c1\u00b5 \u2212 \u222b X ( \u03b6\u00b5 + \u03bbBt log \u00b5\u0302(\u03b6\u00b5) \u03c1\u00b5 ) \u03b4 \u03b4\u03b6\u00b5 \u00b5\u0302t(\u03b6 \u00b5)(dx) = \u2212 \u00b5\u0302t(\u03b6\u00b5) + \u03c1\u00b5.\n(iii) The time derivative of J\u0302t exists due to the differentiability of (Bt)t\u22650. For any t\u2032 > t,\nJ\u0302t\u2032(\u03b6 \u00b5, \u03b6\u03bd) = Jt\u2032(\u00b5\u0302t\u2032(\u03b6 \u00b5), \u03bd\u0302t\u2032(\u03b6 \u03bd)|\u03b6\u00b5, \u03b6\u03bd)\n= Jt(\u00b5\u0302t\u2032(\u03b6 \u00b5), \u03bd\u0302t\u2032(\u03b6 \u03bd)|\u03b6\u00b5, \u03b6\u03bd)\u2212 \u03bb(Bt\u2032 \u2212Bt)(KL(\u00b5\u0302t\u2032(\u03b6\u00b5)\u2225\u03c1\u00b5) + KL(\u03bd\u0302t\u2032(\u03b6\u03bd)\u2225\u03c1\u03bd)) \u2264 J\u0302t(\u03b6\u00b5, \u03b6\u03bd)\u2212 \u03bb(Bt\u2032 \u2212Bt)(KL(\u00b5\u0302t\u2032(\u03b6\u00b5)\u2225\u03c1\u00b5) + KL(\u03bd\u0302t\u2032(\u03b6\u03bd)\u2225\u03c1\u03bd))\nby the maximality of J\u0302t, thus taking the limit t\u2032 \u2193 t yields the stated inequality.\nWe proceed to the proof of Theorem 3.4. Denote the unnormalized aggregate derivatives as\n\u03b4\u00b5t = \u222b t 0 \u03b2s \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds) ds, \u03b4 \u03bd t = \u222b t 0 \u03b2s \u03b4L \u03b4\u03bd (\u00b5s, \u03bds) ds\nwhich are Lipschitz due to Assumption 2. Then by Lemma B.4,\nd dt J\u0302t(\u03b4 \u00b5 t , \u03b4 \u03bd t )\n= \u222b X \u2202t\u03b4 \u00b5 t \u03b4J\u0302t \u03b4\u03b6\u00b5 (\u03b4\u00b5t , \u03b4 \u03bd t )(dx) + \u222b Y \u2202t\u03b4 \u03bd t \u03b4J\u0302t \u03b4\u03b6\u03bd (\u03b4\u00b5t , \u03b4 \u03bd t )(dy) + (\u2202tJ\u0302t)(\u03b4 \u00b5 t , \u03b4 \u03bd t )\n\u2264 \u03b2t \u222b X \u03b4L \u03b4\u00b5 (\u00b5t, \u03bdt)(\u2212 \u00b5\u0302t(\u03b4 \u00b5 t ) + \u03c1 \u00b5)(dx) + \u03b2t \u222b Y \u03b4L \u03b4\u03bd (\u00b5t, \u03bdt)(\u03bd\u0302t(\u03b4 \u03bd t )\u2212 \u03c1\u03bd)(dy)\n\u2212 \u03bb\u03b2t(KL(\u00b5\u0302t(\u03b4 \u00b5 t )\u2225\u03c1\u00b5) + KL(\u03bd\u0302t(\u03b4\u03bdt )\u2225\u03c1\u03bd)).\nThe NI error of the averaged distributions can now be bounded,\nNI(\u00b5\u0304t, \u03bd\u0304t) = max \u00b5,\u03bd L\u03bb(\u00b5\u0304t, \u03bd)\u2212 L\u03bb(\u00b5, \u03bd\u0304t)\n\u2264 max \u00b5,\u03bd\n1\nBt \u222b t 0 \u03b2s(L\u03bb(\u00b5s, \u03bd)\u2212 L\u03bb(\u00b5, \u03bds)) ds\n\u2264 max \u00b5,\u03bd\n1\nBt \u222b t 0 \u03b2s (\u222b Y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(\u03bd \u2212 \u03bds)(dy)\u2212 \u222b X \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(\u00b5\u2212 \u00b5s)(dx)\n+ \u03bb(KL(\u00b5s\u2225\u03c1\u00b5)\u2212KL(\u03bd\u2225\u03c1\u03bd)\u2212KL(\u00b5\u2225\u03c1\u00b5) + KL(\u03bds\u2225\u03c1\u03bd)) ) ds\n= 1\nBt max \u00b5,\u03bd (\u222b Y \u03b4\u03bdt (\u03bd \u2212 \u03c1\u03bd)(dy)\u2212 \u222b X \u03b4\u00b5t (\u00b5\u2212 \u03c1\u00b5)(dx)\u2212 \u03bbBt(KL(\u00b5\u2225\u03c1\u00b5) + KL(\u03bd\u2225\u03c1\u03bd)) ) + 1\nBt \u222b t 0 \u03b2s (\u222b Y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(\u03c1 \u03bd \u2212 \u03bds)(dy)\u2212 \u222b X \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(\u03c1 \u00b5 \u2212 \u00b5s)(dx)\n+ \u03bb(KL(\u00b5s\u2225\u03c1\u00b5) + KL(\u03bds\u2225\u03c1\u03bd)) ) ds,\nwhere we have used the convex-concavity of L\u03bb and L in succession. By extracting the terms corresponding to the auxiliary functional J\u0302t, we are able to apply Lemma B.4(iii) and obtain that\n1\nBt\n[ J\u0302t(\u03b4 \u00b5 t , \u03b4 \u03bd t ) + \u222b t 0 \u03b2s (\u222b Y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(\u03c1 \u03bd \u2212 \u03bds)(dy)\u2212 \u222b X \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(\u03c1 \u00b5 \u2212 \u00b5s)(dx)\n+ \u03bb(KL(\u00b5s\u2225\u03c1\u00b5) + KL(\u03bds\u2225\u03c1\u03bd)) ) ds ] \u2264 1 Bt [ \u222b t 0 ( \u2212 \u03bb\u03b2s(KL(\u00b5\u0302s(\u03b4 \u00b5 t )\u2225\u03c1\u00b5) + KL(\u03bd\u0302s(\u03b4\u03bdt )\u2225\u03c1\u03bd))\n+ \u03b2s \u222b X \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(\u2212 \u00b5\u0302s(\u03b4\u00b5s ) + \u03c1\u00b5)(dx) + \u03b2s \u222b Y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(\u03bd\u0302s(\u03b4 \u03bd s )\u2212 \u03c1\u03bd)(dy) ) ds\n+ \u222b t 0 \u03b2s (\u222b Y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(\u03c1 \u03bd \u2212 \u03bds)(dy)\u2212 \u222b X \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(\u03c1 \u00b5 \u2212 \u00b5s)(dx)\n+ \u03bb(KL(\u00b5s\u2225\u03c1\u00b5) + KL(\u03bds\u2225\u03c1\u03bd)) ) ds ] = 1\nBt \u222b t 0 \u03b2s ( \u03bb(KL(\u00b5s\u2225\u03c1\u00b5)\u2212KL(\u00b5\u0302s\u2225\u03c1\u00b5) + KL(\u03bds\u2225\u03c1\u00b5)\u2212KL(\u03bd\u0302s\u2225\u03c1\u03bd))\n+ \u222b X \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(\u00b5s \u2212 \u00b5\u0302s)(dx)\u2212 \u222b Y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(\u03bds \u2212 \u03bd\u0302s)(dy) ) ds\n= 1\nBt \u222b t 0 \u03b2s ( \u03bb \u222b X log \u00b5\u0302s \u03c1\u00b5 (\u00b5s \u2212 \u00b5\u0302s)(dx) + \u03bb \u222b X log \u00b5s \u00b5\u0302s \u00b5s(dx)\n+ \u03bb \u222b Y log \u03bd\u0302s \u03c1\u03bd (\u03bds \u2212 \u03bd\u0302s)(dy) + \u03bb \u222b Y log \u03bds \u03bd\u0302s \u03bds(dy)\n+ \u222b X \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)(\u00b5s \u2212 \u00b5\u0302s)(dx)\u2212 \u222b Y \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)(\u03bds \u2212 \u03bd\u0302s)(dy) ) ds\n= 1\nBt \u222b t 0 \u03b2s [ \u222b X ( \u03b4L \u03b4\u00b5 (\u00b5s, \u03bds)\u2212 1 Bs \u222b s 0 \u03b2r \u03b4L \u03b4\u00b5 (\u00b5r, \u03bdr) dr ) (\u00b5s \u2212 \u00b5\u0302s)(dx)\n\u2212 \u222b Y ( \u03b4L \u03b4\u03bd (\u00b5s, \u03bds)\u2212 1 Bs \u222b s 0 \u03b2r \u03b4L \u03b4\u00b5 (\u00b5r, \u03bdr) dr ) (\u03bds \u2212 \u03bd\u0302s)(dy)\n+ \u03bb \u222b X log \u00b5s \u00b5\u0302s \u00b5s(dx) + \u03bb \u222b Y log \u03bds \u03bd\u0302s \u03bds(dy) ] ds.\nBy Proposition 3.3 and Talagrand\u2019s inequality, we can therefore bound\nNI(\u00b5\u0304t, \u03bd\u0304t)\n\u2264 1 Bt \u222b t 0 \u03b2s(2M\u00b5W1(\u00b5s, \u00b5\u0302s) + 2M\u03bdW1(\u03bds, \u03bd\u0302s) + \u03bbKL(\u00b5s\u2225 \u00b5\u0302s) + \u03bbKL(\u03bds\u2225 \u03bd\u0302s)) ds\n\u2264 2 Bt \u222b t 0 \u03b2s ( M\u00b5 \u221a 2 \u03b1\u00b5 KL(\u00b5s\u2225 \u00b5\u0302s) +M\u03bd \u221a 2 \u03b1\u00b5 KL(\u03bds\u2225 \u03bd\u0302s) ) ds\n+ \u03bb\nBt \u222b t 0 \u03b2s(KL(\u00b5s\u2225 \u00b5\u0302s) + KL(\u03bds\u2225 \u03bd\u0302s)) ds\n\u2264 ( M2\u00b5 \u03b12\u00b5 + M2\u03bd \u03b12\u03bd ) 4(r + 1) \u03bb2Bt \u222b t t0 \u03b2s s ( 1 +O(s\u22121) ) ds.\nIn particular, for \u03b2t = tr with r > 0, we obtain the convergence rate NI(\u00b5\u0304t, \u03bd\u0304t) \u2264 ( M2\u00b5 \u03b12\u00b5 + M2\u03bd \u03b12\u03bd ) 4(r + 1)2 r\u03bb2t +O(t\u22122)\nwhose leading term is optimized when r = 1. For \u03b2t = 1, we obtain the slightly slower rate NI(\u00b5\u0304t, \u03bd\u0304t) \u2264 ( M2\u00b5 \u03b12\u00b5 + M2\u03bd \u03b12\u03bd ) 4 log t \u03bb2t +O(t\u22121).\nWe remark that for decreasing \u03b2t, the integral tends to converge so that the normalizing B\u22121t term dominates, leading to significantly slower convergence. For example, if \u03b2t \u223c tr for\u22121 < r < 0 the rate is O(t\u22121\u2212r); if \u03b2t \u223c t\u22121, the rate is O( 1log t )."
        },
        {
            "heading": "C TIME AND SPACE DISCRETIZATION",
            "text": ""
        },
        {
            "heading": "C.1 GRADIENT STOPPED PROCESS",
            "text": "Denote Xk = (Xik) N i=1,Yk = (Y i k ) N i=1 and \u00b5Xk = 1 N \u2211N i=1 \u03b4Xik , \u03bdYk = 1 N \u2211N i=1 \u03b4Y ik . That is, the subscript k denotes the number of steps while superscript i denotes the ith particle. We also write (X ,Y )1:k := (X1:k,Y1:k) for notational simplicity.\nWe analyze the following MFL-AG N -particle update for all i = 1, \u00b7 \u00b7 \u00b7 , N ,\nXik+1 = X i k \u2212\n\u03b7\nBk k\u2211 j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(X i k)\u2212 \u03bb\u03b7\u2207xU\u00b5(Xik) + \u221a 2\u03bb\u03b7\u03be\u00b5,ik ,\nY ik+1 = Y i k +\n\u03b7\nBk k\u2211 j=1 \u03b2j\u2207y \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )(Y i k )\u2212 \u03bb\u03b7\u2207yU\u03bd(Y ik ) + \u221a 2\u03bb\u03b7\u03be\u03bd,ik ,\n(7)\nwhere \u03be\u00b5,ik , \u03be \u03bd,i k are i.i.d. standard Gaussian and the initial values X1, Y1 are sampled from initial distributions \u00b50 \u2208 P2(X ), \u03bd0 \u2208 P2(Y). We write the history-dependent averaged drift function as\nb\u00b5k = b \u00b5 k(\u00b7|(X ,Y )1:k) = \u2212\n1\nBk k\u2211 j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )\u2212 \u03bb\u2207xU\u00b5\nand similarly for b\u03bdk. The history-dependent N -particle proximal distributions are defined on the configuration spaces XN ,YN as the product distributions\n\u00b5\u0302 (N) k (X ) \u221d \u03c1 \u00b5\u2297N (X ) exp ( \u2212 N \u03bbBk \u222b X k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )\u00b5X (dx) ) ,\n\u03bd\u0302 (N) k (Y ) \u221d \u03c1 \u03bd\u2297N (Y ) exp\n( N\n\u03bbBk \u222b Y k\u2211 j=1 \u03b2j \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )\u03bdY (dy) ) .\nWe substitute \u03b2k = kr with r \u2208 R\u22650 whenever necessary to simplify the calculations, although similar results may be derived for any well-behaved sequence of weights.\nThe following lemma quantifies the sequential evolution of the averaged drift.\nLemma C.1. For any pair of integers k > \u2113 we have \u2225b\u00b5k \u2212 b \u00b5 \u2113 \u2225\u221e \u2264 2 ( 1\u2212 B\u2113Bk ) M\u00b5.\nProof. For any x \u2208 X ,\n\u2225b\u00b5k(x)\u2212 b \u00b5 \u2113 (x)\u2225 = \u2225\u2225\u2225\u2225\u2212 1Bk k\u2211 j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(x) + 1 B\u2113 \u2113\u2211 j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(x) \u2225\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225Bk \u2212B\u2113B\u2113Bk \u2113\u2211 j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(x)\u2212 1 Bk k\u2211 j=\u2113+1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(x) \u2225\u2225\u2225\u2225 \u2264 2 ( 1\u2212 B\u2113\nBk\n) M\u00b5,\nyielding the assertion.\nThe gradient-stopped process. For given integers k > \u2113, consider the following synchronous modification of the MFL-AG update with the drift stopped at time k \u2212 \u2113,\nX\u0303ij+1 = X\u0303 i j + \u03b7 b \u00b5 j\u2227(k\u2212\u2113)(X\u0303 i j) + \u221a 2\u03bb\u03b7\u03be\u00b5,ij , Y\u0303 i j+1 = Y\u0303 i j + \u03b7 b \u03bd j\u2227(k\u2212\u2113)(Y\u0303 i j ) + \u221a 2\u03bb\u03b7\u03be\u03bd,ij .\nThe initializations X\u03031, Y\u03031 and the random vectors \u03be \u00b5,i j , \u03be \u03bd,i j are to be shared with the original process so that (X\u0303 , Y\u0303 )1:k\u2212\u2113+1 = (X ,Y )1:k\u2212\u2113+1. We will study this process alongside the original in order to facilitate short-term perturbation analyses.\nLemma C.2. If \u03b7 \u2264 r\u00b54\u03bbR2\u00b5 , the second moments of the particles X i k and X\u0303 i k are uniformly bounded for all k \u2265 1 as\nE[\u2225Xik\u22252], E[\u2225X\u0303ik\u22252] \u2264 E[\u2225Xi1\u22252] + s\u00b5, s\u00b5 := 2\nr\u00b5 ( M2\u00b5 r\u00b5\u03bb2 + \u03bb\u03b7M2\u00b5 + dX ) .\nProof. From the update rule (7),\nE[\u2225Xik+1\u22252] = E[\u2225Xik\u22252]\u2212 2\u03b7 \u2329 Xik, 1\nBk k\u2211 j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(X i k) + \u03bb\u2207xU\u00b5(Xik)\n\u232a\n+ \u03b72 \u2225\u2225\u2225\u2225 1Bk k\u2211 j=1 \u03b2j\u2207x \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(X i k) + \u03bb\u2207xU\u00b5(Xik) \u2225\u2225\u2225\u22252 + 2\u03bb\u03b7dX \u2264 E[\u2225Xik\u22252] + 2\u03b7M\u00b5E[\u2225Xik\u2225]\u2212 2\u03bb\u03b7r\u00b5E[\u2225Xik\u22252]\n+ 2\u03bb2\u03b72M2\u00b5 + 2\u03bb 2\u03b72R2\u00b5E[\u2225Xik\u22252] + 2\u03bb\u03b7dX\n\u2264 (1\u2212 \u03bb\u03b7r\u00b5)E[\u2225Xik\u22252] + 2\u03b7M2\u00b5 r\u00b5\u03bb + 2\u03bb2\u03b72M2\u00b5 + 2\u03bb\u03b7dX ,\nwhere we have used E[\u2225Xik\u2225] \u2264 r\u00b5\u03bb 4M\u00b5 E[\u2225Xik\u22252] + M\u00b5 r\u00b5\u03bb and \u03b7 \u2264 r\u00b54\u03bbR2\u00b5 . The statement now follows from induction. The same logic can be applied to E[\u2225X\u0303ik\u22252].\nLemma C.3. If \u03b7 \u2264 r\u00b5\u03bb2(L\u00b5+\u03bbR\u00b5)2 , the Wasserstein error between the original and gradient-stopped process at time k > \u2113 is bounded as\nW2(\u00b5Xk , \u00b5X\u0303k) \u2264 r + 1 k \u2212 \u2113+ 1 w\u00b5\u2113 ,\nwhere\n(w\u00b5\u2113 ) 2 :=\n( 2\u03b7 + 1\nr\u00b5\u03bb \u2228 1 2L\u00b5\n) M2\u00b5(1 + 2\u03b7L\u00b5) 2((1 + 2\u03b7L\u00b5) \u2113 \u2212 1)\n\u03b72L3\u00b5 .\nProof. Decomposing the difference at each step j > k \u2212 \u2113 as\nXij+1 \u2212 X\u0303ij+1 = Xij \u2212 X\u0303ij + \u03b7(b \u00b5 j (X i j)\u2212 b \u00b5 j (X\u0303 i j)) + \u03b7(b \u00b5 j (X\u0303 i j)\u2212 b \u00b5 k\u2212\u2113(X\u0303 i j)),\nwe expand to obtain\n\u2225Xij+1 \u2212 X\u0303ij+1\u22252\n\u2264 \u2225Xij \u2212 X\u0303ij\u22252 + 2\u03b7\u27e8Xij \u2212 X\u0303ij , b \u00b5 j (X i j)\u2212 b \u00b5 j (X\u0303 i j)\u27e9+ 2\u03b7\u2225Xij \u2212 X\u0303ij\u2225 \u00b7 \u2225b \u00b5 j \u2212 b \u00b5 k\u2212\u2113\u2225\u221e\n+ 2\u03b72\u2225b\u00b5j (X i j)\u2212 b \u00b5 j (X\u0303 i j)\u22252 + 2\u03b72\u2225b \u00b5 j \u2212 b \u00b5 k\u2212\u2113\u2225 2 \u221e \u2264 \u2225Xij \u2212 X\u0303ij\u22252 + 2\u03b7(L\u00b5 \u2212 \u03bbr\u00b5)\u2225Xij \u2212 X\u0303ij\u22252 + 4\u03b7 ( 1\u2212 Bk\u2212\u2113\nBj\n) M\u00b5\u2225Xij \u2212 X\u0303ij\u2225\n+ 2\u03b72(L\u00b5 + \u03bbR\u00b5) 2\u2225Xij \u2212 X\u0303ij\u22252 + 8\u03b72\n( 1\u2212 Bk\u2212\u2113\nBj\n)2 M2\u00b5\n\u2264 (1 + 2\u03b7L\u00b5)\u2225Xij \u2212 X\u0303ij\u22252 + ( 4\u03b7M2\u00b5 r\u00b5\u03bb + 8\u03b72M2\u00b5 )( 1\u2212 Bk\u2212\u2113 Bj )2 .\nStarting from Xik\u2212\u2113 \u2212 X\u0303ik\u2212\u2113 = 0 and iterating,\n\u2225Xik\u2212 X\u0303ik\u22252 \u2264 ( 4\u03b7M2\u00b5 r\u00b5\u03bb +8\u03b72M2\u00b5 ) k\u22121\u2211 j=k\u2212\u2113+1 (1+2\u03b7L\u00b5) k\u2212j\u22121 ( 1\u2212 Bk\u2212\u2113 Bj )2 , k \u2265 \u2113+2. (8)\nNow noting that with \u03b2j = jr\n1\u2212 Bk\u2212\u2113 Bj\n\u2264 (j \u2212 k + \u2113)j r\u222b j\n0 zr dz\n= (r + 1) ( 1\u2212 k \u2212 \u2113\nj\n) \u2264 (r + 1)(j \u2212 k + \u2113)\nk \u2212 \u2113+ 1 ,\nsetting \u03b8 = (1 + 2\u03b7L\u00b5)\u22121 we can explicitly compute k\u22121\u2211\nj=k\u2212\u2113+1 (j \u2212 k + \u2113)2(1 + 2\u03b7L\u00b5)k\u2212j\u22121 = \u03b81\u2212\u2113 \u2113\u22121\u2211 j=1 j2\u03b8j = \u03b82\u2212\u2113\n(1\u2212 \u03b8)3 (\u2212(\u2113\u2212 1)2\u03b8\u2113+1 + (2\u21132 \u2212 2\u2113\u2212 1)\u03b8\u2113 \u2212 \u21132\u03b8\u2113\u22121 + 3\u2212 \u03b8)\n\u2264 \u03b8 (1\u2212 \u03b8)3 ( 3\u2212 \u03b8 \u03b8\u2113\u22121 \u2212 2 ) \u2264 2\u03b8 (1\u2212 \u03b8)3 (\u03b8\u2212\u2113 \u2212 1).\nPlugging back into (8) gives \u2225Xik \u2212 X\u0303ik\u22252 \u2264 ( 4M2\u00b5 r\u00b5\u03bb + 8\u03b7M2\u00b5 ) (r + 1)2(1 + 2\u03b7L\u00b5) 2 4\u03b72L3\u00b5 (1 + 2\u03b7L\u00b5) \u2113 \u2212 1 (k \u2212 \u2113+ 1)2\n\u2264 (r + 1)2M2\u00b5 ( 2\u03b7 + 1\nr\u00b5\u03bb \u2228 1 2L\u00b5\n) (1 + 2\u03b7L\u00b5) 2\n\u03b72L3\u00b5\n(1 + 2\u03b7L\u00b5) \u2113 \u2212 1\n(k \u2212 \u2113+ 1)2\nuniformly for all i \u2208 [N ]. Note that the (2L\u00b5)\u22121 term is added to simplify later analyses and is generally vacuous. Finally, taking W 22 (\u00b5Xk , \u00b5X\u0303k) \u2264 1 N \u2225Xk\u2212X\u0303k\u2225 2 yields the desired bound.\nThe calculations for the two above two results are similar but the bounds are fundamentally different. In Lemma C.2 we rely on the long-distance dissipative nature of b\u00b5k to prove a uniform-in-time guarantee, while in Lemma C.3 we forego the contraction to isolate the 1 \u2212 B\u2113Bk factor and obtain tight short-term error bounds.\nThe leave-one-out error of the modified process can also be characterized as follows. We remark that the arguments in Lemmas C.2 and C.4 are identical to that in Suzuki et al. (2023).\nLemma C.4. Denote the set of N \u2212 1 particles (X\u03031k , \u00b7 \u00b7 \u00b7 , X\u0303 i\u22121 k , X\u0303 i+1 k , \u00b7 \u00b7 \u00b7 , X\u0303Nk ) as X \u2212i k . If \u03b7 \u2264 r\u00b54\u03bbR2\u00b5 , the W2 distance between \u00b5X\u0303k and \u00b5X\u0303 \u2212ik at time k > \u2113 can be bounded on average as\nE X\u0303k|(X ,Y )1:k\u2212\u2113 [ W 22 (\u00b5X\u0303k , \u00b5X\u0303 \u2212ik ) ] \u2264 4s \u00b5 N +\n2 N(N \u2212 1) \u2211 j \u0338=i \u2225Xjk\u2212\u2113\u2225 2 + 2 N \u2225Xjk\u2212\u2113\u2225 2.\nProof. Similarly to Lemma C.2 but starting from time k \u2212 \u2113, it can be shown that\nE X\u0303k|(X ,Y )1:k\u2212\u2113 [\u2225X\u0303 j k\u2225 2] \u2264 \u2225Xjk\u2212\u2113\u2225 2 \u2228 s\u00b5, j \u2208 [N ],\nwhich will be useful in the sequel. Then taking the coupling \u2211 j \u0338=i\n1 N \u03b4(X\u0303jk,X\u0303 j k) + 1N(N\u22121)\u03b4(X\u0303ik,X\u0303 j k)\nfor \u00b5 X\u0303k , \u00b5 X\u0303 \u2212ik gives\nE X\u0303k|(X ,Y )1:k\u2212\u2113 [ W 22 (\u00b5X\u0303k , \u00b5X\u0303 \u2212ik ) ] \u2264 E X\u0303k|(X ,Y )1:k\u2212\u2113  1 N(N \u2212 1) \u2211 j \u0338=i \u2225X\u0303jk \u2212 X\u0303 i k\u22252 \n\u2264 E X\u0303k|(X ,Y )1:k\u2212\u2113  2 N(N \u2212 1) \u2211 j \u0338=i \u2225X\u0303jk\u2225 2 + 2 N \u2225X\u0303ik\u22252  \u2264 4s \u00b5\nN +\n2 N(N \u2212 1) \u2211 j \u0338=i \u2225Xjk\u2212\u2113\u2225 2 + 2 N \u2225Xjk\u2212\u2113\u2225 2.\nThe same bound holds for the original process."
        },
        {
            "heading": "C.2 PROXIMAL PUSHFORWARD BOUNDS",
            "text": "For a measure \u00b5(N) on X (N), denote by \u03a0 the average of the pushforward operators \u03a0i\u266f along the projections X 7\u2192 Xi with the defining property\u222b\nX f(x)\u03a0\u00b5(N)(dx) = \u222b XN \u03a0\u2217f(X )\u00b5(N)(dX ) = \u222b XN 1 N N\u2211 i=1 f(Xi)\u00b5(N)(dX )\nfor any integrable function f : X \u2192 R. We immediately see that\n\u03a0 \u00b5\u0302 (N) k = \u03c1 \u00b5 exp ( \u2212 1 \u03bbBk \u222b X k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj ) )\nis the stationary distribution of the continuous-time Ito\u0302 diffusion dZt = b \u00b5 k(Zt) dt + \u221a 2\u03bb dW\u00b5t , which entails the following uniform moment bound.\nLemma C.5. The unnormalized second moment \u222b X \u2225x\u2225 2 \u03a0 \u00b5\u0302 (N) k (dx) is bounded above for any integer k by q\u00b5 := r\u22122\u00b5 \u03bb \u22122M2\u00b5 + 2r \u22121 \u00b5 dX . We also denote p\u00b5 := 1N \u2211N i=1 E[\u2225Xi1\u22252] <\u221e.\nProof. We may compute for the initialization Z0 = 0,\nd dt E[\u2225Zt\u22252] = 2E [\u27e8Zt, b\u00b5k(Zt)\u27e9] + 2\u03bbdX\n\u2264 2M\u00b5E[\u2225Zt\u2225]\u2212 2r\u00b5\u03bbE[\u2225Zt\u22252] + 2\u03bbdX \u2264 \u2212r\u00b5\u03bbE[\u2225Zt\u22252] + M2\u00b5 r\u00b5\u03bb + 2\u03bbdX ,\nwhich yields the bound in the infinite-time limit by Gronwall\u2019s lemma.\nIn particular, \u03a0 \u00b5\u0302(N)k\u2212\u2113 is the approximate stationary distribution of each independent particle of the gradient stopped process after time k\u2212 \u2113 and enjoys an exponential convergence guarantee up to an O(\u03b7) discretization error term.\nProposition C.6. Assuming \u03b7 \u2264 r\u00b54\u03bbR2\u00b5 , the KL gap from \u00b5\u0303 i k = Law(X\u0303 i k|(X ,Y )1:k\u2212\u2113) to \u03a0 \u00b5\u0302 (N) k\u2212\u2113 of the gradient stopped process satisfies\nKL(\u00b5\u0303ik\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) \u2264\n( 1 +\n3 exp(\u2212(\u2113\u2212 1)\u03b1\u00b5\u03bb\u03b7) 2\u03b72(L\u00b5 + \u03bbR\u00b5)2\n) (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5),\nwhere\nK\u00b5 := \u03b72R2\u00b5(L\u00b5 + \u03bbR\u00b5) 2\n\u03b1\u00b5 , L\u00b5 :=\n\u03b7(L\u00b5 + \u03bbR\u00b5) 2 \u03b1\u00b5\u03bb2 ( \u03b7M2\u00b5 + \u03bb 2\u03b7R2\u00b5s \u00b5 + \u03bbdX ) are both of order O(\u03b7).\nHence, choosing\n\u2113 = \u2113\u00b5 := 1\n\u03b1\u00b5\u03bb\u03b7\n\u2308 log\n3\n2\u03b72(L\u00b5 + \u03bbR\u00b5)2\n\u2309 + 1 (9)\nguarantees that\nW2(\u00b5\u0303 i k\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) \u2264\n\u221a 4\n\u03b1\u00b5 (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5)\nfor any integer k > \u2113.\nProof. We emulate the one-step analysis in Nitanda et al. (2022a) whilst keeping the history (X ,Y )1:k\u2212\u2113 fixed; this dependence is omitted here for notational clarity. For j \u2265 k\u2212 \u2113, denote by \u00b5\u2020t the law of the process\ndX\u2020t = b \u00b5 k\u2212\u2113(X\u0303 i j) dt+ \u221a 2\u03bb dW \u2020t , 0 \u2264 t \u2264 \u03b7\nwith X\u20200 = X\u0303 i j so that X \u2020 \u03b7 d = X\u0303ij+1. We overload notation and denote both conditional and joint distributions involving X\u2020t by \u00b5 \u2020 t . The evolution of \u00b5 \u2020 t is governed by the conditional Fokker-Planck equation \u2202t\u00b5 \u2020 t(X \u2020 t |X\u0303ij) = \u2212\u2207x \u00b7 ( \u00b5\u2020t(X \u2020 t |X\u0303ij) b \u00b5 k\u2212\u2113(X\u0303 i j) ) + \u03bb\u2206x\u00b5 \u2020 t(X \u2020 t |X\u0303ij).\nIntegrating out X\u0303ij ,\n\u2202t\u00b5 \u2020 t(X \u2020 t ) = \u222b X \u2212\u2207x \u00b7 ( \u00b5\u2020t(X \u2020 t , X\u0303 i j) b \u00b5 k\u2212\u2113(X\u0303 i j) ) (dX\u0303ij) + \u03bb\u2206x\u00b5 \u2020 t(X \u2020 t )\n= \u2207x \u00b7 ( \u00b5\u2020t(X \u2020 t ) ( \u2212EX\u0303ij |X\u2020t [ b\u00b5k\u2212\u2113(X\u0303 i j) ] + \u03bb\u2207x log\u00b5\u2020t(X \u2020 t ) ))\n= \u03bb\u2207x \u00b7 ( \u00b5\u2020t(X \u2020 t )\u2207x log \u00b5\u2020t\n\u03a0 \u00b5\u0302 (N) k\u2212\u2113\n(X\u2020t ) ) +\u2207x \u00b7 ( \u00b5\u2020t(X \u2020 t ) ( b\u00b5k\u2212\u2113(X \u2020 t )\u2212 EX\u0303ij |X\u2020t [ b\u00b5k\u2212\u2113(X\u0303 i j) ])) .\nHence the proximal KL gap from \u00b5\u2020t to \u03a0 \u00b5\u0302 (N) k\u2212\u2113 satisfies\n\u2202tKL(\u00b5 \u2020 t\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) = \u222b X log \u00b5\u2020t\n\u03a0 \u00b5\u0302 (N) k\u2212\u2113\n(\u2202t\u00b5 \u2020 t)(dX \u2020 t )\n= \u2212\u03bb \u222b X \u2225\u2225\u2225\u2225\u2207x log \u00b5\u2020t \u03a0 \u00b5\u0302\n(N) k\u2212\u2113 \u2225\u2225\u2225\u22252\u00b5\u2020t(dX\u2020t ) \u2212 \u222b\u222b\nX \u00d7X log\n\u00b5\u2020t\n\u03a0 \u00b5\u0302 (N) k\u2212\u2113\n\u00b7 ( b\u00b5k\u2212\u2113(X \u2020 t )\u2212 b \u00b5 k\u2212\u2113(X\u0303 i j) ) \u00b5\u2020t(dX \u2020 t dX\u0303 i j)\n\u2264 \u2212\u03bb 2 \u222b X \u2225\u2225\u2225\u2225\u2207x log \u00b5\u2020t \u03a0 \u00b5\u0302\n(N) k\u2212\u2113\n\u2225\u2225\u2225\u22252\u00b5\u2020t(dX\u2020t ) + (L\u00b5 + \u03bbR\u00b5)22\u03bb \u222b\u222b X \u00d7X \u2225X\u2020t \u2212 X\u0303ij\u22252\u00b5 \u2020 t(dX \u2020 t dX\u0303 i j)\n\u2264 \u2212\u03b1\u00b5\u03bb \u00b7KL(\u00b5\u2020t\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) +\n(L\u00b5 + \u03bbR\u00b5) 2\n2\u03bb\n\u222b X E\u03be\u2020 [\u2225\u2225\u2225b\u00b5k\u2212\u2113(X\u0303ij)t+\u221a2\u03bbt\u03be\u2020\u2225\u2225\u22252] \u00b5\u0303ij(dX\u0303ij) where \u03be\u2020 \u223c N (0, IdX ) and we have used the LSI for \u03a0 \u00b5\u0302 (N) k\u2212\u2113. The second term is further bounded as\nE\u03be\u2020 [\u2225\u2225\u2225b\u00b5k\u2212\u2113(X\u0303ij)t+\u221a2\u03bbt\u03be\u2020\u2225\u2225\u22252] \u2264 \u03b72 EX\u0303ij |X1:k\u2212\u2113 [\u2225\u2225\u2225b\u00b5k\u2212\u2113(X\u0303ij)\u2225\u2225\u22252]+ 2\u03bb\u03b7dX \u2264 2\u03b72M2\u00b5 + 2\u03bb2\u03b72R2\u00b5 E[\u2225X\u0303ij\u22252] + 2\u03bb\u03b7dX \u2264 2\u03b72M2\u00b5 + 2\u03bb2\u03b72R2\u00b5 ( \u2225Xik\u2212\u2113\u22252 \u2228 s\u00b5 ) + 2\u03bb\u03b7dX\nby the proof of Lemma C.4. Gronwall\u2019s lemma now leads to\nKL(\u00b5\u0303ij+1\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113)\u2212 (K \u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5) \u2264 e\u2212\u03b1\u00b5\u03bb\u03b7 ( KL(\u00b5\u0303ij\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113)\u2212 (K \u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5) ) .\nThus, iterating the bound for k \u2212 \u2113 < j < k gives\nKL(\u00b5\u0303ik\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) \u2264 exp(\u2212(\u2113\u2212 1)\u03b1\u00b5\u03bb\u03b7)KL(\u00b5\u0303 i k\u2212\u2113+1\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) + K \u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5,\nwhere we have stopped at time k \u2212 \u2113 + 1 because the initial distribution \u00b5\u0303ik\u2212\u2113 = \u03b4Xik\u2212\u2113 is atomic. Instead, the relative entropy after the first step can be directly bounded; since X\u2020t is a rescaled Brownian motion with constant drift, the first iteration of \u03b4Xik\u2212\u2113 is distributed as\n\u00b5\u0303ik\u2212\u2113+1 d = N (Xik\u2212\u2113 + \u03b7 b \u00b5 k\u2212\u2113(X i k\u2212\u2113), 2\u03bb\u03b7 IdX ).\nThe LSI then gives that\nKL(\u00b5\u0303ik\u2212\u2113+1\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) \u2264\n1\n2\u03b1\u00b5 E\u00b5\u0303ik\u2212\u2113+1 [\u2225\u2225\u2225\u2225\u2207x log \u00b5\u0303ik\u2212\u2113+1 \u03a0 \u00b5\u0302\n(N) k\u2212\u2113\n\u2225\u2225\u2225\u22252 ]\n\u2264 3 2\u03b1\u00b5 ( dX 2\u03bb\u03b7 + M2\u00b5 \u03bb2 +R2\u00b5 EXik\u2212\u2113+1|(X ,Y )1:k\u2212\u2113 [\u2225X i k\u2212\u2113+1\u22252] )\n\u2264 3 2\u03b1\u00b5 ( dX 2\u03bb\u03b7 + M2\u00b5 \u03bb2 +R2\u00b5 ( \u2225Xik\u2212\u2113\u22252 \u2228 s\u00b5 ))\n< 3\n2\u03b72(L\u00b5 + \u03bbR\u00b5)2 (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5).\nHence we arrive at the desired statement,\nKL(\u00b5\u0303ik\u2225\u03a0 \u00b5\u0302 (N) k\u2212\u2113) \u2264\n( 1 +\n3 exp(\u2212(\u2113\u2212 1)\u03b1\u00b5\u03bb\u03b7) 2\u03b72(L\u00b5 + \u03bbR\u00b5)2\n) (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5).\nThe subsequent lemmas provide control over the Wasserstein distance between pushforward distibutions. In particular, Lemma C.8 is the discrete analogue of the O(\u03b2t/Bt) time derivative bound obtained in the proof of Proposition 3.3.\nLemma C.7. For any two measures \u00b5(N), \u00b5\u0303(N) \u2208 P2(XN ) it holds that\nW2(\u03a0\u00b5 (N),\u03a0\u00b5\u0303(N)) \u2264 1\u221a\nN W2(\u00b5\n(N), \u00b5\u0303(N)).\nProof. Recall the dual formulation of W2,\nW 22 (\u00b5, \u00b5\u0303) = sup \u03d5,\u03c8\n{\u222b \u03d5 d\u00b5\u2212 \u222b \u03c8 d\u00b5\u0303 \u2223\u2223\u2223\u2223 \u03d5, \u03c8 : X \u2192 R, \u03d5(x)\u2212 \u03c8(y) \u2264 \u2225x\u2212 y\u22252}.\nThen for any pair of functions \u03d5, \u03c8 such that \u03d5(x) \u2212 \u03c8(y) \u2264 \u2225x\u2212 y\u22252, the pullback functions \u03a0\u2217\u03d5,\u03a0\u2217\u03c8 on XN satisfy\n\u03a0\u2217\u03d5(X )\u2212\u03a0\u2217\u03c8(Y ) = 1 N N\u2211 i=1 \u03d5(Xi)\u2212 \u03c8(Y i) \u2264 1 N N\u2211 i=1 \u2225Xi \u2212 Y i\u22252 = 1 N \u2225X \u2212 Y \u22252L2(XN ) .\nTherefore,\u222b X \u03d5(x)\u03a0\u00b5(N)(dx)\u2212 \u222b X \u03c8(x)\u03a0\u00b5\u0303(N)(dx)\n= \u222b XN \u03a0\u2217\u03d5(X )\u00b5(N)(dX )\u2212 \u222b XN \u03a0\u2217\u03c8(X )\u00b5\u0303(N)(dX ) \u2264 1 N W 22 (\u00b5 (N), \u00b5\u0303(N)),\nwhich yields the assertion by taking the supremum over all permissible \u03d5, \u03c8.\nLemma C.8. The projected 2-Wasserstein distance between \u00b5\u0302(N)k , \u00b5\u0302 (N) k\u22121 is bounded as\nW2(\u03a0 \u00b5\u0302 (N) k ,\u03a0 \u00b5\u0302 (N) k\u22121) \u2264 2M\u00b5\u03b2k \u03b1\u00b5\u03bbBk .\nProof. The proof is deferred to Section C.4."
        },
        {
            "heading": "C.3 PROOF OF PROPOSITION 3.6",
            "text": "We take \u2113 = \u2113\u00b5 = O(\u03b7\u22121 log \u03b7\u22121) as defined in (9) throughout the proof and only consider the case k \u2265 2\u2113 in Steps 1 through 4.\nStep 1. We first look \u2113\u22121 steps back to the past and control the displacement of the proximal \u03a0 \u00b5\u0302(N)k\u22121 from the stationary state \u03a0 \u00b5\u0302(N)k\u2212\u2113 of the modified process via Lemma C.8, conditioning on the earlier history (X ,Y )1:k\u2212\u2113.\nE(X ,Y )k\u2212\u2113+1:k|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u22121)(dx) ] \u2264 E(X ,Y )k|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u2212\u2113)(dx)\n] +M\u00b5\n\u2113\u22121\u2211 j=1 E(X ,Y )k\u2212\u2113+1:k\u2212j |(X ,Y )1:k\u2212\u2113 [ W1(\u03a0 \u00b5\u0302 (N) k\u2212j\u22121,\u03a0 \u00b5\u0302 (N) k\u2212j) ]\n\u2264 E(X ,Y )k|(X ,Y )1:k\u2212\u2113 [\u222b\nX F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u2212\u2113)(dx)\n] +\n2M2\u00b5 \u03b1\u00b5\u03bb \u2113\u22121\u2211 j=1 \u03b2k\u2212j Bk\u2212j .\nIt is simple to further verify that\n2M2\u00b5 \u03b1\u00b5\u03bb \u2113\u22121\u2211 j=1 \u03b2k\u2212j Bk\u2212j \u2264 2M2\u00b5 \u03b1\u00b5\u03bb (r + 1)(\u2113\u2212 1) k \u2212 \u2113+ 1 .\nStep 2. Next, we look back to the future and convert the expectation with respect to \u00b5Xk to the corresponding expectation for the modified process. The incurred error can be bounded by utilizing Lemmas C.2, C.3 and C.4 as\nE(X ,Y )k|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u2212\u2113)(dx) ] \u2212 E\n(X\u0303 ,Y\u0303 )k|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5 X\u0303k , \u03bdY\u0303k)(\u00b5X\u0303k \u2212\u03a0 \u00b5\u0302 (N) k\u2212\u2113)(dx) ] = E\n(X ,X\u0303 ,Y ,Y\u0303 )k|(X ,Y )1:k\u2212\u2113 [ \u222b X F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212 \u00b5X\u0303k)(dx)\n+ \u222b X ( F (\u00b5Xk , \u03bdYk)\u2212 F (\u00b5X\u0303k , \u03bdY\u0303k) ) (\u00b5 X\u0303k \u2212\u03a0 \u00b5\u0302(N)k\u2212\u2113)(dx) ] \u2264 E\n(X ,X\u0303 ,Y ,Y\u0303 )k|(X ,Y )1:k\u2212\u2113 [ M\u00b5W1(\u00b5Xk , \u00b5X\u0303k)\n+ 1\nN N\u2211 i=1 \u2225\u2225\u2225F (\u00b5Xk , \u03bdYk)\u2212 F (\u00b5X\u0303k , \u03bdY\u0303k)\u2225\u2225\u2225LipW1(\u03b4X\u0303ik ,\u03a0 \u00b5\u0302(N)k\u2212\u2113) ]\n\u2264 (r + 1)M\u00b5 k \u2212 \u2113+ 1 w\u00b5\u2113\n+ (r + 1)L\u00b5 k \u2212 \u2113+ 1 (w\u00b5\u2113 +w \u03bd \u2113 )E(X\u0303ik|(X ,Y )1:k\u2212\u2113\n[( 2\nN N\u2211 i=1 \u222b X \u2225X\u0303ik \u2212 x\u22252 \u03a0 \u00b5\u0302 (N) k\u2212\u2113(dx) ) 1 2 ]\n\u2264 (r + 1)M\u00b5 k \u2212 \u2113+ 1 w\u00b5\u2113 + (r + 1)L\u00b5 k \u2212 \u2113+ 1 (w\u00b5\u2113 +w \u03bd \u2113 )\n( 2\nN N\u2211 i=1 \u2225Xik\u2212\u2113\u22252 + q\u00b5 + 2s\u00b5 ) 1 2 .\nStep 3. For the modified process, we apply a leave-one-out argument and consider the expectation with respect to each particle X\u0303ik which is independent of X\u0303 \u2212i k , Y\u0303k when conditioned on the stopped history (X ,Y )1:k\u2212\u2113. That is,\nE (X\u0303 ,Y\u0303 )k|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5 X\u0303k , \u03bdY\u0303k)(\u00b5X\u0303k \u2212\u03a0 \u00b5\u0302 (N) k\u2212\u2113)(dx) ] = 1\nN N\u2211 i=1 E X\u0303 \u2212ik ,Y\u0303k|(X ,Y )1:k\u2212\u2113 EX\u0303ik|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5 X\u0303k , \u03bdY\u0303k)(\u03b4X\u0303ik \u2212\u03a0 \u00b5\u0302(N)k\u2212\u2113)(dx) ]\n\u2264 1 N N\u2211 i=1 E X\u0303 \u2212ik ,Y\u0303k|(X ,Y )1:k\u2212\u2113 EX\u0303ik|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5 X\u0303 \u2212ik , \u03bdY\u0303k)(\u03b4X\u0303ik \u2212\u03a0 \u00b5\u0302(N)k\u2212\u2113)(dx) ]\n+ 1\nN N\u2211 i=1 E X\u0303k|(X ,Y )1:k\u2212\u2113 [\u2225\u2225\u2225F (\u00b5X\u0303k , \u03bdY\u0303k)\u2212 F (\u00b5X\u0303 \u2212ik , \u03bdY\u0303k)\u2225\u2225\u2225LipW1(\u03b4X\u0303ik ,\u03a0 \u00b5\u0302(N)k\u2212\u2113) ]\n\u2264 1 N N\u2211 i=1 E X\u0303 \u2212ik ,Y\u0303k|(X ,Y )1:k\u2212\u2113 EX\u0303ik|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5 X\u0303 \u2212ik , \u03bdY\u0303k)(\u03b4X\u0303ik \u2212\u03a0 \u00b5\u0302(N)k\u2212\u2113)(dx) ]\n+ L\u00b5 N N\u2211 i=1 E X\u0303k|(X ,Y )1:k\u2212\u2113 [ W1(\u00b5X\u0303k , \u00b5X\u0303 \u2212ik )W1(\u03b4X\u0303ik ,\u03a0 \u00b5\u0302 (N) k\u2212\u2113) ] = 1\nN N\u2211 i=1 E X\u0303 \u2212ik ,Y\u0303k|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5 X\u0303 \u2212ik , \u03bdY\u0303k)(\u00b5 i k(X\u0303 i k)\u2212\u03a0 \u00b5\u0302 (N) k\u2212\u2113)(dx) ]\n+ L\u00b5 N N\u2211 i=1 E X\u0303k|(X ,Y )1:k\u2212\u2113 [ W1(\u00b5X\u0303k , \u00b5X\u0303 \u2212ik )W1(\u03b4X\u0303ik ,\u03a0 \u00b5\u0302 (N) k\u2212\u2113) ] \u2264 M\u00b5\nN N\u2211 i=1 W1(\u00b5 i k,\u03a0 \u00b5\u0302 (N) k\u2212\u2113)\n+ L\u00b5 N N\u2211 i=1 ( E X\u0303k|(X ,Y )1:k\u2212\u2113 [ W 22 (\u00b5X\u0303k , \u00b5X\u0303 \u2212ik ) ] E X\u0303k|(X ,Y )1:k\u2212\u2113 [ W 22 (\u03b4X\u0303ik ,\u03a0 \u00b5\u0302 (N) k\u2212\u2113) ]) 1 2\n\u2264 2M\u00b5 N N\u2211 i=1 \u221a \u03b1\u22121\u00b5 (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5)\n+ 2L\u00b5 N N\u2211 i=1\n( 2s\u00b5\nN +\n1 N(N \u2212 1) \u2211 j \u0338=i \u2225Xjk\u2212\u2113\u2225 2 + 1 N \u2225Xik\u2212\u2113\u22252 ) 1 2 ( \u2225Xik\u2212\u2113\u22252 + q\u00b5 + s\u00b5 ) 1 2\nby applying Lemma C.2, Lemma C.4 and Proposition C.6.\nStep 4. Putting things together, we obtain the conditional bound\nE(X ,Y )k\u2212\u2113+1:k|(X ,Y )1:k\u2212\u2113 [\u222b X F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u22121)(dx) ] \u2264\n2M2\u00b5 \u03b1\u00b5\u03bb (r + 1)(\u2113\u2212 1) k \u2212 \u2113+ 1\n+ (r + 1)M\u00b5 k \u2212 \u2113+ 1 w\u00b5\u2113 + (r + 1)L\u00b5 k \u2212 \u2113+ 1 (w\u00b5\u2113 +w \u03bd \u2113 )\n( 2\nN N\u2211 i=1 \u2225Xik\u2212\u2113\u22252 + q\u00b5 + 2s\u00b5 ) 1 2\n+ 2M\u00b5 N N\u2211 i=1 \u221a \u03b1\u22121\u00b5 (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5)\n+ 2L\u00b5 N N\u2211 i=1\n( 2s\u00b5\nN +\n1 N(N \u2212 1) \u2211 j \u0338=i \u2225Xjk\u2212\u2113\u2225 2 + 1 N \u2225Xik\u2212\u2113\u22252 ) 1 2 ( \u2225Xik\u2212\u2113\u22252 + q\u00b5 + s\u00b5 ) 1 2 .\nRecalling E[\u2225Xik\u2212\u2113\u22252] \u2264 E[\u2225Xi1\u22252] + s\u00b5 from Lemma C.2, taking the expectation with respect to the history (X ,Y )1:k\u2212\u2113 finally gives\nE(X ,Y )1:k [\u222b X F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u22121)(dx) ] \u2264 r + 1 k \u2212 \u2113+ 1 ( 2M2\u00b5 \u03b1\u00b5\u03bb (\u2113\u2212 1) +M\u00b5w\u00b5\u2113 + L\u00b5 (w \u00b5 \u2113 +w \u03bd \u2113 ) (2p \u00b5 + q\u00b5 + 4s\u00b5) 1 2 )\n+ 2M\u00b5E(X ,Y )1:k\n[ 1\n\u03b1\u00b5N N\u2211 i=1 (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5)\n] 1 2\n+ L\u00b5\nN 3 2 N\u2211 i=1 E(X ,Y )1:k\n[ 1\nN \u2212 1 N\u2211 j=1 \u2225Xjk\u2212\u2113\u2225 2 + 2N \u2212 3 N \u2212 1 \u2225Xik\u2212\u2113\u22252 + q\u00b5 + 3s\u00b5 ]\n\u2264 r + 1 k \u2212 \u2113+ 1 ( 2M2\u00b5 \u03b1\u00b5\u03bb (\u2113\u2212 1) +M\u00b5w\u00b5\u2113 + L\u00b5 (w \u00b5 \u2113 +w \u03bd \u2113 ) (2p \u00b5 + q\u00b5 + 4s\u00b5) 1 2 )\n+ 2M\u00b5\n( K\u00b5p\u00b5 + L\u00b5\n\u03b1\u00b5\n) 1 2\n+ 2L\u00b5\u221a N (3p\u00b5 + q\u00b5 + 6s\u00b5)\n\u2264 r + 1 k C1(\u03b7) + C2 \u221a \u03b7 + C3\u221a N ,\nwhere the last bound holds if k \u2265 2\u2113\u00b5. To be explicit,\nC1(\u03b7) = 2 ( 2M2\u00b5 \u03b1\u00b5\u03bb (\u2113\u2212 1) +M\u00b5w\u00b5\u2113 + L\u00b5 (w \u00b5 \u2113 +w \u03bd \u2113 ) (2p \u00b5 + q\u00b5 + 4s\u00b5) 1 2 ) ,\nC2 = 2M\u00b5\n( \u03b7\u0304R2\u00b5(L\u00b5 + \u03bbR\u00b5) 2p\u00b5\n\u03b12\u00b5 +\n(L\u00b5 + \u03bbR\u00b5) 2\n\u03b12\u00b5\u03bb 2\n( \u03b7\u0304M2\u00b5 + \u03bb 2\u03b7\u0304R2\u00b5s \u00b5 + \u03bbdX )) 12 ,\nC3 = 2L\u00b5 (3p \u00b5 + q\u00b5 + 6s\u00b5) .\nThe constantsC2, C3 can be taken to be polynomial and independent of \u03b7 by substituting in the upper bound \u03b7\u0304 = r\u00b5\u03bb2(L\u00b5+\u03bbR\u00b5)2 \u2227 r\u00b5 4\u03bbR2\u00b5\nin the expressions for s\u00b5,K\u00b5/\u03b7,L\u00b5/\u03b7, while \u2113\u00b5 = O(\u03b7\u22121 log \u03b7\u22121). However, C1(\u03b7) contains the dependency\nO(w\u00b5\u2113 ) = O ( \u03b7\u22121 exp(\u2113L\u00b5\u03b7) ) = O\n( 1\n\u03b7\n( 3\n2\u03b72(L2\u00b5 + \u03bbR 2 \u00b5) 2\n) L\u00b5 \u03b1\u00b5\u03bb ) ,\nwhich is a consequence of uniformly bounding the perturbation from the gradient stopped process over a time period of \u2113.\nStep 5. For k < 2\u2113, proceeding similarly without converting to the modified process gives\nE(X ,Y )1:k [\u222b X F (\u00b5Xk , \u03bdYk)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k\u22121)(dx) ] \u2264 M\u00b5\nN N\u2211 i=1 E(X ,Y )1:k [ W1(\u03b4Xik ,\u03a0 \u00b5\u0302 (N) k\u22121) ] + L\u00b5 N N\u2211 i=1 ( E(X ,Y )1:k [ W 22 (\u00b5X\u0303k , \u00b5X\u0303 \u2212ik ) ] E(X ,Y )1:k [ W 22 (\u03b4X\u0303ik ,\u03a0 \u00b5\u0302 (N) k\u22121) ]) 1 2\n\u2264 M\u00b5 N N\u2211 i=1 ( E[\u2225Xi1\u22252] + q\u00b5 + s\u00b5 ) 1 2\n+ 2L\u00b5 N N\u2211 i=1\n( 2s\u00b5\nN +\n1 N(N \u2212 1) \u2211 k \u0338=i E[\u2225Xk1 \u22252] + 1 N E[\u2225Xi1\u22252] ) 1 2 ( E[\u2225Xi1\u22252] + q\u00b5 + s\u00b5 ) 1 2\n\u2264M\u00b5 \u221a p\u00b5 + q\u00b5 + s\u00b5 + 2L\u00b5(3p \u00b5 + q\u00b5 + 3s\u00b5)\u221a\nN\n< C1(\u03b7)\n2\u2113 + C3\u221a N ,\nwhere the final bound follows by noting \u03b7 < r\u00b54L\u00b5R\u00b5 \u2264 1 4L\u00b5 , hence by expanding (1 + 2\u03b7L\u00b5)\u2113\n(w\u00b5\u2113 ) 2 >\n1 2L\u00b5 \u00b7 M2\u00b5 \u03b72L3\u00b5\n( 2\u03b7L\u00b5\u2113+ 2\u03b7 2L2\u00b5\u2113(\u2113\u2212 1) ) > M2\u00b5\u2113 2\nL2\u00b5\nand so C1(\u03b7) > 2L\u00b5w \u00b5 \u2113 (2p \u00b5 + q\u00b5 + 4s\u00b5) 1 2 > 2M\u00b5\u2113 (p \u00b5 + q\u00b5 + s\u00b5) 1 2 .\nThus the bound holds for all integers k. We conclude the proof by taking the maximum with the corresponding quantities for \u03bd."
        },
        {
            "heading": "C.4 PROPERTIES OF CONJUGATE FUNCTIONALS",
            "text": "We proceed to develop the N -particle lifted analogues J (N)k , J\u0302 (N) k of the conjugate functionals in the proof of Theorem 3.4. In order to deal with time and particle discretization, we will need a more precise characterization of their perturbative properties. Many of the subsequent results do not follow from standard methods and requires a careful synthesis of the discussion thus far. Lemma C.9. Given Lipschitz functions \u03b6\u00b5 : X \u2192 R, \u03b6\u03bd : Y \u2192 R and a pair of N -particle probability measures \u00b5(N) \u2208 P2(XN ), \u03bd(N) \u2208 P2(YN ) define the functional\nJ (N) k (\u00b5 (N), \u03bd(N)|\u03b6\u00b5, \u03b6\u03bd) = \u2212 \u222b XN \u222b X \u03b6\u00b5(\u00b5X \u2212 \u03c1\u00b5)(dx)\u00b5(N)(dX ) + \u222b YN \u222b Y \u03b6\u03bd(\u03bdY \u2212 \u03c1\u03bd)(dy)\u03bd(N)(dY )\n\u2212 \u03bbBk N\n( KL(\u00b5(N)\u2225\u03c1\u00b5\u2297N ) + KL(\u03bd(N)\u2225\u03c1\u03bd\u2297N ) ) ."
        },
        {
            "heading": "Then the maximum",
            "text": "J\u0302 (N) k (\u03b6 \u00b5, \u03b6\u03bd) = max \u00b5(N)\u2208P2(XN ) max \u03bd(N)\u2208P2(YN ) J (N) k (\u00b5 (N), \u03bd(N)|\u03b6\u00b5, \u03b6\u03bd)\nexists for all k \u2208 N and is uniquely attained by the pair of distributions\n\u00b5\u0302 (N) k (\u03b6 \u00b5) \u221d \u03c1\u00b5\u2297N exp ( \u2212 N \u03bbBk \u222b X \u03b6\u00b5\u00b5X (dx) ) , \u03bd\u0302 (N) k (\u03b6 \u03bd) \u221d \u03c1\u03bd\u2297N exp ( N \u03bbBk \u222b Y \u03b6\u03bd\u03bdY (dy) ) .\nProof. The proof is similar to Lemma B.3; we only check the first-order condition by setting\n\u03b4J (N) k \u03b4\u00b5(N) (\u00b5(N))(X ) = \u2212 \u222b X \u03b6\u00b5(\u00b5X \u2212 \u03c1\u00b5)(dx)\u2212 \u03bbBk N log \u00b5(N)(X ) \u03c1\u00b5\u2297N (X ) = const.\nThe N -particle proximal distributions \u00b5\u0302(N)k (\u03b6 \u00b5), \u03bd\u0302 (N) k (\u03b6 \u03bd), despite being defined over the configuration spaces XN ,YN also satisfy the log-Sobolev inequality with the same constant as before due to the tensorization property of entropy. Lemma C.10 (product log-Sobolev inequality). Suppose that \u03b6\u00b5/Bk, \u03b6\u03bd/Bk are M\u00b5,M\u03bdLipschitz, respectively. Then \u00b5\u0302(N)k (\u03b6 \u00b5), \u03bd\u0302 (N) k (\u03b6\n\u03bd) satisfy the LSI on XN ,YN , with the same constants \u03b1\u00b5, \u03b1\u03bd as in Proposition 3.2.\nProof. We can write \u00b5(N) = \u00b5\u0302(N)k (\u03b6 \u00b5) as the symmetric product distribution\n\u00b5(N)(X ) = N\u220f i=1 \u00b5i(Xi), \u00b5i(Xi) = \u03c1\u00b5(Xi) exp ( \u2212\u03b6 \u00b5(Xi) \u03bbBk ) , 1 \u2264 i \u2264 N,\nwhere the marginals \u00b5i(Xi) each satisfy the LSI with constant \u03b1\u00b5 by Proposition 3.2. Also write \u00b5\u2212i(X\u2212i) = \u220f j \u0338=i \u00b5\ni(Xi). For an appropriately integrable function f on XN , denote by f i for the functions f i(Xi) = f(X1, \u00b7 \u00b7 \u00b7 , Xi, \u00b7 \u00b7 \u00b7 , XN ). Then by Proposition 2.2 of Ledoux (1999),\nEnt\u00b5(N)(f 2) \u2264 N\u2211 i=1 E\u00b5\u2212i [Ent\u00b5i((f i)2)] \u2264 N\u2211 i=1 2 \u03b1\u00b5 E\u00b5\u2212iE\u00b5i [\u2225\u2207f i\u22252] = 2 \u03b1\u00b5 E\u00b5(N) [\u2225\u2207f\u2225 2 ].\nLemma C.11. The functional J\u0302 (N)k is convex in both arguments, and admits functional derivatives at any (\u03b6\u00b5, \u03b6\u03bd) which are given as\n\u03b4J\u0302 (N) k\n\u03b4\u03b6\u00b5 (\u03b6\u00b5, \u03b6\u03bd) = \u2212\u03a0 \u00b5\u0302(N)k (\u03b6\n\u00b5) + \u03c1\u00b5, \u03b4J\u0302\n(N) k\n\u03b4\u03b6\u03bd (\u03b6\u00b5, \u03b6\u03bd) = \u03a0 \u03bd\u0302 (N) k (\u03b6 \u03bd)\u2212 \u03c1\u03bd .\nProof. Substituting J\u0302 (N)k (\u03b6 \u00b5, \u03b6\u03bd) = J (N) k (\u00b5\u0302 (N) k (\u03b6 \u00b5), \u00b5\u0302 (N) k (\u03b6 \u03bd)|\u03b6\u00b5, \u03b6\u03bd),\n\u03b4J\u0302 (N) k\n\u03b4\u03b6\u00b5 (\u03b6\u00b5, \u03b6\u03bd) = \u2212 \u03b4 \u03b4\u03b6\u00b5 \u222b XN \u222b X \u03b6\u00b5(\u00b5X \u2212 \u03c1\u00b5)(dx)\u00b5(N)(dX ) \u2223\u2223\u2223\u2223 \u00b5(N)=\u00b5\u0302\n(N) k (\u03b6 \u00b5) \u2212 \u222b XN \u222b X \u03b6\u00b5(\u00b5X \u2212 \u03c1\u00b5)(dx) \u03b4 \u00b5\u0302 (N) k \u03b4\u03b6\u00b5 (\u03b6\u00b5)(dX )\u2212 \u03bbBk N \u222b XN ( log \u00b5\u0302 (N) k (\u03b6 \u00b5) \u03c1\u00b5\u2297N ) \u03b4 \u00b5\u0302 (N) k \u03b4\u03b6\u00b5 (\u03b6\u00b5)(dX )\n= \u03b4\n\u03b4\u03b6\u00b5 ( \u2212 \u222b XN 1 N N\u2211 i=1 \u03b6\u00b5(Xi)\u00b5(N)(dX ) + \u222b X \u03b6\u00b5\u03c1\u00b5(dx) )\u2223\u2223\u2223\u2223\u2223 \u00b5(N)=\u00b5\u0302\n(N) k (\u03b6 \u00b5)\n= \u2212\u03a0 \u00b5\u0302(N)k (\u03b6 \u00b5) + \u03c1\u00b5.\nThe integral over the configuration space measure \u00b5\u0302(N)k therefore lifts the expectation with respect to the discrete measure \u00b5X to a differentiable functional of \u03b6\u00b5, which in turn pushes forward \u00b5\u0302 (N) k onto the space X .\nThe following proposition is crucial to controlling the evolution of the conjugate functional as well as the proximal distributions over time.\nProposition C.12. Suppose \u03b6\u00b5/Bk, \u03b6\u0303\u00b5/Bk areM\u00b5-Lipschitz functions such that the difference \u03b6\u00b5\u2212 \u03b6\u0303\u00b5 is m\u00b5-Lipschitz for some m\u00b5 > 0. Then the projected proximal distributions satisfy\nW2(\u03a0 \u00b5\u0302 (N) k (\u03b6 \u00b5),\u03a0 \u00b5\u0302 (N) k (\u03b6\u0303 \u00b5)) \u2264 m\u00b5 \u03b1\u00b5\u03bbBk .\nProof. Taking the first-order conditions\n\u2212 \u222b X \u03b6\u00b5(\u00b5X \u2212 \u03c1\u00b5)(dx)\u2212 \u03bbBk N log \u00b5\u0302 (N) k (\u03b6 \u00b5) \u03c1\u00b5\u2297N = const.,\n\u2212 \u222b X \u03b6\u0303\u00b5(\u00b5X \u2212 \u03c1\u00b5)(dx)\u2212 \u03bbBk N log \u00b5\u0302 (N) k (\u03b6\u0303 \u00b5) \u03c1\u00b5\u2297N = const.\nSubtracting both sides and integrating over the difference \u00b5\u0302(N)k (\u03b6 \u00b5)\u2212 \u00b5\u0302(N)k (\u03b6\u0303\u00b5), we obtain \u2212 \u222b XN \u222b X (\u03b6\u00b5 \u2212 \u03b6\u0303\u00b5)\u00b5X (dx)(\u00b5\u0302(N)k (\u03b6 \u00b5)\u2212 \u00b5\u0302(N)k (\u03b6\u0303 \u00b5))(dX )\n= \u03bbBk N \u222b XN log \u00b5\u0302 (N) k (\u03b6 \u00b5)\n\u00b5\u0302 (N) k (\u03b6\u0303\n\u00b5) (\u00b5\u0302\n(N) k (\u03b6 \u00b5)\u2212 \u00b5\u0302(N)k (\u03b6\u0303 \u00b5))(dX ).\n(10)\nNow the left-hand side of (10) can be bounded from above by \u2212 \u222b XN \u222b X (\u03b6\u00b5 \u2212 \u03b6\u0303\u00b5)\u00b5X (dx)(\u00b5\u0302(N)k (\u03b6 \u00b5)\u2212 \u00b5\u0302(N)k (\u03b6\u0303 \u00b5))(dX )\n= \u2212 \u222b X (\u03b6\u00b5 \u2212 \u03b6\u0303\u00b5)(\u03a0 \u00b5\u0302(N)k (\u03b6 \u00b5)\u2212\u03a0 \u00b5\u0302(N)k (\u03b6\u0303 \u00b5))(dx) \u2264 m\u00b5W1(\u03a0 \u00b5\u0302(N)k (\u03b6 \u00b5),\u03a0 \u00b5\u0302 (N) k (\u03b6\u0303 \u00b5)) \u2264 m\u00b5W2(\u03a0 \u00b5\u0302(N)k (\u03b6 \u00b5),\u03a0 \u00b5\u0302 (N) k (\u03b6\u0303 \u00b5)),\nwhile the right-hand side of (10) is bounded from below by\n\u03bbBk N\n( KL(\u00b5\u0302\n(N) k (\u03b6 \u00b5)\u2225 \u00b5\u0302(N)k (\u03b6\u0303 \u00b5)) + KL(\u00b5\u0302 (N) k (\u03b6\u0303 \u00b5)\u2225 \u00b5\u0302(N)k (\u03b6 \u00b5)) )\n\u2265 \u03b1\u00b5\u03bbBk N W 22 (\u00b5\u0302 (N) k (\u03b6 \u00b5), \u00b5\u0302 (N) k (\u03b6\u0303 \u00b5)) \u2265 \u03b1\u00b5\u03bbBkW 22 (\u03a0 \u00b5\u0302 (N) k (\u03b6 \u00b5),\u03a0 \u00b5\u0302 (N) k (\u03b6\u0303 \u00b5)),\nwhere we have used Talagrand\u2019s inequality from Lemma C.10 and the W2 pushforward bound from Lemma C.7. Combining the two results yields the desired statement.\nDenote the unnormalized aggregate derivatives as\n\u03b4\u00b5k = k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj ), \u03b4 \u03bd k = k\u2211 j=1 \u03b2j \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )\nso that \u00b5\u0302(N)k = \u00b5\u0302 (N) k (\u03b4 \u00b5 k ), \u03bd\u0302 (N) k = \u03bd\u0302 (N) k (\u03b4 \u03bd k). The functions \u03b4 \u00b5 k/Bk and \u03b4 \u03bd k/Bk are M\u00b5- and M\u03bdLipschitz, respectively, due to Assumption 2. Lemma C.11 and Proposition C.12 then allow us to quantify the change in J\u0302 (N)k (\u03b4 \u00b5 k , \u03b4 \u03bd k) as time progresses. Lemma C.13. We have the following one-step relation for J\u0302 (N)k , k \u2265 2:\nJ\u0302 (N) k (\u03b4 \u00b5 k , \u03b4 \u03bd k)\u2212 J\u0302 (N) k\u22121(\u03b4 \u00b5 k\u22121, \u03b4 \u03bd k\u22121) \u2264 \u03b2k \u222b X \u03b4L \u03b4\u00b5 (\u00b5Xk , \u03bdYk)(\u2212\u03a0 \u00b5\u0302 (N) k\u22121 +\u03c1 \u00b5)(dx) + \u03b2k \u222b Y \u03b4L \u03b4\u03bd (\u00b5Xk , \u03bdYk)(\u03a0 \u03bd\u0302 (N) k\u22121\u2212\u03c1 \u03bd)(dy)\n\u2212 \u03bb\u03b2k N\n( KL(\u00b5\u0302\n(N) k \u2225\u03c1 \u00b5\u2297N ) + KL(\u03bd\u0302 (N) k \u2225\u03c1\n\u03bd\u2297N ) ) + ( M2\u00b5 \u03b1\u00b5 + M2\u03bd \u03b1\u03bd ) \u03b22k 2\u03bbBk\u22121 .\nProof. By the maximality of J\u0302 (N)k ,\nJ\u0302 (N) k (\u03b4 \u00b5 k , \u03b4 \u03bd k) = J (N) k (\u00b5\u0302 (N) k (\u03b4 \u00b5 k ), \u00b5\u0302 (N) k (\u03b4 \u03bd k)|\u03b4 \u00b5 k , \u03b4 \u03bd k)\n= J (N) k\u22121(\u00b5\u0302 (N) k (\u03b4 \u00b5 k ), \u00b5\u0302 (N) k (\u03b4 \u03bd k)|\u03b4 \u00b5 k , \u03b4 \u03bd k)\u2212 \u03bb\u03b2k N\n( KL(\u00b5\u0302\n(N) k (\u03b4 \u00b5 k )\u2225\u03c1 \u00b5\u2297N ) + KL(\u03bd\u0302 (N) k (\u03b4 \u03bd k)\u2225\u03c1\u03bd\u2297N )\n)\n\u2264 J\u0302 (N)k\u22121(\u03b4 \u00b5 k , \u03b4 \u03bd k)\u2212 \u03bb\u03b2k N\n( KL(\u00b5\u0302\n(N) k \u2225\u03c1 \u00b5\u2297N ) + KL(\u03bd\u0302 (N) k \u2225\u03c1\n\u03bd\u2297N ) ) .\nFurther defining the interpolations\n\u03b4\u00b5k (s) = \u03b4 \u00b5 k\u22121 + s(\u03b4 \u00b5 k \u2212 \u03b4 \u00b5 k\u22121) = k\u22121\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj ) + s\u03b2k \u03b4L \u03b4\u00b5 (\u00b5Xk , \u03bdYk), 0 \u2264 s \u2264 1\nand similarly for \u03b4\u03bdk(s), we have\nJ\u0302 (N) k\u22121(\u03b4 \u00b5 k , \u03b4 \u03bd k)\u2212 J\u0302 (N) k\u22121(\u03b4 \u00b5 k\u22121, \u03b4 \u03bd k\u22121) = \u222b 1 0 d ds J\u0302 (N) k\u22121(\u03b4 \u00b5 k (s), \u03b4 \u03bd k(s)) ds\n= \u222b 1 0 \u222b X (\u03b4\u00b5k \u2212 \u03b4 \u00b5 k\u22121) \u03b4J\u0302 (N) k\u22121 \u03b4\u03b6\u00b5 (\u03b4\u00b5k (s), \u03b4 \u03bd k(s))(dx) + \u222b Y (\u03b4\u03bdk \u2212 \u03b4\u03bdk\u22121) \u03b4J\u0302 (N) k\u22121 \u03b4\u03b6\u03bd (\u03b4\u00b5k (s), \u03b4 \u03bd k(s))(dy) ds\n= \u222b 1 0 \u222b X \u2212(\u03b4\u00b5k \u2212 \u03b4 \u00b5 k\u22121)\u03a0 \u00b5\u0302 (N) k\u22121(\u03b4 \u00b5 k (s))(dx) + \u222b Y (\u03b4\u03bdk \u2212 \u03b4\u03bdk\u22121)\u03a0 \u03bd\u0302 (N) k\u22121(\u03b4 \u03bd k(s))(dy) ds\n+ \u222b X (\u03b4\u00b5k \u2212 \u03b4 \u00b5 k\u22121)\u03c1 \u00b5(dx)\u2212 \u222b Y (\u03b4\u03bdk \u2212 \u03b4\u03bdk\u22121)\u03c1\u03bd(dy)\n\u2264 \u222b 1 0 \u222b X \u2212(\u03b4\u00b5k \u2212 \u03b4 \u00b5 k\u22121)\u03a0 \u00b5\u0302 (N) k\u22121(\u03b4 \u00b5 k\u22121)(dx) + \u222b Y (\u03b4\u03bdk \u2212 \u03b4\u03bdk\u22121)\u03a0 \u03bd\u0302 (N) k\u22121(\u03b4 \u03bd k\u22121)(dy) ds\n+ \u222b X (\u03b4\u00b5k \u2212 \u03b4 \u00b5 k\u22121)\u03c1 \u00b5(dx)\u2212 \u222b Y (\u03b4\u03bdk \u2212 \u03b4\u03bdk\u22121)\u03c1\u03bd(dy)\n+ \u222b 1 0 M\u00b5\u03b2kW1(\u03a0 \u00b5\u0302 (N) k\u22121(\u03b4 \u00b5 k (s)),\u03a0 \u00b5\u0302 (N) k\u22121(\u03b4 \u00b5 k\u22121)) ds\n+ \u222b 1 0 M\u03bd\u03b2kW1(\u03a0 \u03bd\u0302 (N) k\u22121(\u03b4 \u03bd k(s)),\u03a0 \u03bd\u0302 (N) k\u22121(\u03b4 \u03bd k\u22121)) ds\n\u2264 \u03b2k \u222b X \u2212\u03b4L \u03b4\u00b5 (\u00b5Xk , \u03bdYk)\u03a0 \u00b5\u0302 (N) k\u22121(dx) + \u03b2k \u222b Y \u03b4L \u03b4\u03bd (\u00b5Xk , \u03bdYk)\u03a0 \u03bd\u0302 (N) k\u22121(dy)\n+ \u03b2k \u222b X \u03b4L \u03b4\u00b5 (\u00b5Xk , \u03bdYk)\u03c1 \u00b5(dx)\u2212 \u03b2k \u222b Y \u03b4L \u03b4\u03bd (\u00b5Xk , \u03bdYk)\u03c1 \u03bd(dy) + ( M2\u00b5 \u03b1\u00b5 + M2\u03bd \u03b1\u03bd ) \u03b22k 2\u03bbBk\u22121 ,\nwhere for the first inequality we used the fact that \u03b4\u00b5k \u2212 \u03b4 \u00b5 k\u22121 is M\u00b5\u03b2k-Lipschitz, and for the second we applied Proposition C.12 with m\u00b5 = sM\u00b5\u03b2k.\nWe now give the promised proof of the pushforward evolution bound.\nProof of Lemma C.8. Note that \u00b5\u0302(N)k\u22121 = \u00b5\u0302 (N) k\u22121(\u03b4 \u00b5 k\u22121) may also be written as\n\u00b5\u0302 (N) k\u22121 = \u00b5\u0302 (N) k ( Bk Bk\u22121 k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj ) ) = \u00b5\u0302 (N) k ( Bk Bk\u22121 \u03b4\u00b5k\u22121 ) .\nSince \u03b4\u00b5k\u22121/Bk\u22121 is M\u00b5-Lipschitz and\n\u03b4\u00b5k \u2212 Bk Bk\u22121 \u03b4\u00b5k\u22121 = \u2212 \u03b2k Bk\u22121 k\u22121\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj ) + \u03b2k \u03b4L \u03b4\u00b5 (\u00b5Xk , \u03bdYk)\nis 2M\u00b5\u03b2k-Lipschitz, by Proposition C.12 we obtain the bound\nW2(\u03a0 \u00b5\u0302 (N) k ,\u03a0 \u00b5\u0302 (N) k\u22121) \u2264 2M\u00b5\u03b2k \u03b1\u00b5\u03bbBk ."
        },
        {
            "heading": "C.5 PROOF OF THEOREM 3.7",
            "text": "Step 1. We first prove a convergent upper bound of the following surrogate N(\u00b5X k , \u03bdY k) for the NI error of the average distributions. Note that the defining maximum is lifted to the configuration space and the discrete empirical distributions have been replaced with their proximal counterparts for measuring relative entropy. While N is not exactly the desired quantity, it arises naturally from the discrete conjugate argument and helps to bound the expected error.\nN(\u00b5X k , \u03bdY k)\n:= max \u00b5(N),\u03bd(N) \u2212 1 Bk k\u2211 j=1 \u03b2jL(\u03a0\u00b5(N), \u03bdYj )\u2212 \u03bb N KL(\u00b5(N)\u2225\u03c1\u00b5\u2297N ) + \u03bb NBk k\u2211 j=1 \u03b2j KL(\u03bd\u0302 (N) j \u2225\u03c1 \u03bd\u2297N )\n+ 1\nBk k\u2211 j=1 \u03b2jL(\u00b5Xj ,\u03a0\u03bd(N))\u2212 \u03bb N KL(\u03bd(N)\u2225\u03c1\u03bd\u2297N ) + \u03bb NBk k\u2211 j=1 \u03b2j KL(\u00b5\u0302 (N) j \u2225\u03c1 \u00b5\u2297N )\n\u2264 max \u00b5(N),\u03bd(N) \u2212 \u222b XN \u222b X 1 Bk k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(\u00b5X \u2212 \u00b5Xj )(dx)\u00b5(N)(dX )\n+ \u222b YN \u222b Y 1 Bk k\u2211 j=1 \u03b2j \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )(\u03bdY \u2212 \u03bdYj )(dy)\u03bd(N)(dY )\n\u2212 \u03bb N\n( KL(\u00b5(N)\u2225\u03c1\u00b5\u2297N ) + KL(\u03bd(N)\u2225\u03c1\u03bd\u2297N ) ) + \u03bb\nNBk k\u2211 j=1 \u03b2j ( KL(\u00b5\u0302 (N) j \u2225\u03c1 \u00b5\u2297N ) + KL(\u03bd\u0302 (N) j \u2225\u03c1 \u03bd\u2297N ) )\n= 1\nBk\n[ J\u0302 (N) k (\u03b4 \u03bd k , \u03b4 \u03bd k) + \u03bb\nN k\u2211 j=1 \u03b2j ( KL(\u00b5\u0302 (N) j \u2225\u03c1 \u00b5\u2297N ) + KL(\u03bd\u0302 (N) j \u2225\u03c1 \u03bd\u2297N ) )\n+ \u222b X k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(\u00b5Xj \u2212 \u03c1\u00b5)(dx)\u2212 \u222b Y k\u2211 j=1 \u03b2j \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )(\u03bdYj \u2212 \u03c1\u03bd)(dy) ] ,\ndue to the convex-concavity of L. Recursively applying Lemma C.13 then yields N(\u00b5X k , \u03bdY k)\n\u2264 1 Bk [ k\u2211 j=2 ( J\u0302 (N) j (\u03b4 \u03bd j , \u03b4 \u03bd j )\u2212 J\u0302 (N) j\u22121(\u03b4 \u03bd j\u22121, \u03b4 \u03bd j\u22121) ) + 1 Bk J\u0302 (N) 1 (\u03b4 \u03bd 1 , \u03b4 \u03bd 1 )\n+ \u222b X k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(\u00b5Xj \u2212 \u03c1\u00b5)(dx)\u2212 \u222b Y k\u2211 j=1 \u03b2j \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )(\u03bdYj \u2212 \u03c1\u03bd)(dy)\n+ \u03bb\nN k\u2211 j=1 \u03b2j ( KL(\u00b5\u0302 (N) j \u2225\u03c1 \u00b5\u2297N ) + KL(\u03bd\u0302 (N) j \u2225\u03c1 \u03bd\u2297N ) )]\n\u2264 1 Bk [ k\u2211 j=1 \u03b2j \u222b X \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(\u00b5Xj \u2212\u03a0 \u00b5\u0302 (N) j\u22121)(dx)\n\u2212 k\u2211 j=1 \u03b2j \u222b Y \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )(\u03bdYj \u2212\u03a0 \u03bd\u0302 (N) j\u22121)(dy) + 1 2\u03bb ( M2\u00b5 \u03b1\u00b5 + M2\u03bd \u03b1\u03bd ) k\u2211 j=2 \u03b22j Bj\u22121 ] ,\nwhere the initial term is substituted as J\u0302 (N)1 (\u03b4 \u03bd 1 , \u03b4 \u03bd 1 ) = J (N) 1 (\u00b5\u0302 (N) 1 , \u03bd\u0302 (N) 1 |\u03b4\u03bd1 , \u03b4\u03bd1 ) with the convention that \u00b5\u0302(N)0 = \u00b5\u0302 (N) 1 , \u03bd\u0302 (N) 0 = \u03bd\u0302 (N) 1 . Now taking the expectation over the full history and applying Proposition 3.6, we arrive at\nE(X ,Y )1:k [ N(\u00b5X k , \u03bdY k) ]\n\u2264 1 Bk E(X ,Y )1:k [ k\u2211 j=1 \u03b2j \u222b X \u03b4L \u03b4\u00b5 (\u00b5Xj , \u03bdYj )(\u00b5Xj \u2212\u03a0 \u00b5\u0302 (N) j\u22121)(dx)\n\u2212 k\u2211 j=1 \u03b2j \u222b Y \u03b4L \u03b4\u03bd (\u00b5Xj , \u03bdYj )(\u03bdYj \u2212\u03a0 \u03bd\u0302 (N) j\u22121)(dy) + 1 2\u03bb ( M2\u00b5 \u03b1\u00b5 + M2\u03bd \u03b1\u03bd ) k\u2211 j=2 \u03b22j Bj\u22121 ]\n\u2264 1 Bk\n[ 2\nk\u2211 j=1 \u03b2j ( r + 1 j C1(\u03b7) + C2 \u221a \u03b7 + C3\u221a N ) + 1 2\u03bb ( M2\u00b5 \u03b1\u00b5 + M2\u03bd \u03b1\u03bd ) k\u2211 j=2 \u03b22j Bj\u22121 ]\n\u2264 ( (r + 1)2\nrk +O(k\u22122)\n)( 2C1(\u03b7) + 1\n2\u03bb ( M2\u00b5 \u03b1\u00b5 + M2\u03bd \u03b1\u03bd )) + 2C2 \u221a \u03b7 + 2C3\u221a N\n\u2264 ( (r + 1)2\nrk +O(k\u22122) ) \u00b7 9 4 C1(\u03b7) + 2C2 \u221a \u03b7 + 2C3\u221a N\nby simply using \u2113 > 1. For r = 0, the last expression is replaced by the exact bound\n1 + log k k \u00b7 9 4 C1(\u03b7) + 2C2 \u221a \u03b7 + 2C3\u221a N .\nStep 2. We now control the NI error of the averaged pushforward proximal distributions using N. In the defining maximum over \u00b5(N) \u2208 P2(XN ), \u03bd(N) \u2208 P2(YN ), we may restrict to product distributions \u00b5(N) = \u00b5\u2297N , \u03bd(N) = \u03bd\u2297N so that\nE(X ,Y )1:k [ N(\u00b5X k , \u03bdY k) ] \u2265 E(X ,Y )1:k [ max \u00b5,\u03bd \u2212 1 Bk k\u2211 j=1 \u03b2jL(\u00b5, \u03bdYj )\u2212 \u03bbKL(\u00b5\u2225\u03c1\u00b5) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u03bd\u0302 (N) j \u2225\u03c1 \u03bd)\n+ 1\nBk k\u2211 j=1 \u03b2jL(\u00b5Xj , \u03bd)\u2212 \u03bbKL(\u03bd\u2225\u03c1\u03bd) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u00b5\u0302 (N) j \u2225\u03c1 \u00b5)\n]\n\u2265 max \u00b5,\u03bd E(X ,Y )1:k [ \u2212 1 Bk k\u2211 j=1 \u03b2jL(\u00b5, \u03bdYj )\u2212 \u03bbKL(\u00b5\u2225\u03c1\u00b5) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u03bd\u0302 (N) j \u2225\u03c1 \u03bd)\n+ 1\nBk k\u2211 j=1 \u03b2jL(\u00b5Xj , \u03bd)\u2212 \u03bbKL(\u03bd\u2225\u03c1\u03bd) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u00b5\u0302 (N) j \u2225\u03c1 \u00b5) ] \u2265 max\n\u00b5,\u03bd \u2212L(\u00b5,E[\u03bdY k ])\u2212 \u03bbKL(\u00b5\u2225\u03c1 \u00b5) + \u03bbKL(E[\u03a0 \u03bd\u0302k]\u2225\u03c1\u03bd)\n+ L(E[\u00b5X k ], \u03bd)\u2212 \u03bbKL(\u03bd\u2225\u03c1 \u03bd) + \u03bbKL(E[\u03a0 \u00b5\u0302k]\u2225\u03c1\u00b5)\nby convex-concavity of L as well as convexity of KL divergence, where we have written\n\u03a0 \u00b5\u0302k := 1\nBk k\u2211 j=1 \u03b2j\u03a0 \u00b5\u0302 (N) j , \u03a0 \u03bd\u0302k := 1 Bk k\u2211 j=1 \u03b2j\u03a0 \u03bd\u0302 (N) j .\nAgain by Proposition 3.6, this is further bounded as E(X ,Y )1:k [ N(\u00b5X k , \u03bdY k) ] \u2265 max\n\u00b5,\u03bd \u2212L\u03bb(\u00b5,E[\u03a0 \u03bd\u0302k])\u2212 E(X ,Y )1:k\n[ 1\nBk \u222b Y k\u2211 j=1 \u03b2j \u03b4L \u03b4\u03bd (\u00b5,E[\u03a0 \u03bd\u0302k])(\u03bdYj \u2212\u03a0 \u03bd\u0302 (N) j )(dy) ]\n+ L\u03bb(E[\u03a0 \u00b5\u0302k], \u03bd) + E(X ,Y )1:k\n[ 1\nBk \u222b X k\u2211 j=1 \u03b2j \u03b4L \u03b4\u00b5 (E[\u03a0 \u00b5\u0302k], \u03bd)(\u00b5Xj \u2212\u03a0 \u00b5\u0302 (N) j )(dx) ]\n\u2265 NI(E[\u03a0 \u00b5\u0302k],E[\u03a0 \u03bd\u0302k])\u2212 ( (r + 1)2\nrk +O(k\u22122)\n) \u00b7 2C1(\u03b7)\u2212 2C2 \u221a \u03b7 \u2212 2C3\u221a\nN ,\nwith the appropriate modification for r = 0.\nStep 3. Finally, we convert the above pushforward proximal bounds back to a Wasserstein distance bound for the expected empirical measures. By Lemma 3.5 and Talagrand\u2019s inequality for the MNE (\u00b5\u2217, \u03bd\u2217),\nW 22 (E[\u03a0 \u00b5\u0302k], \u00b5\u2217) +W 22 (E[\u03a0 \u03bd\u0302k], \u03bd\u2217)\n\u2264 2 \u03b1\u00b5 \u2228 2 \u03b1\u03bd\n( KL(E[\u03a0 \u00b5\u0302k]\u2225\u00b5\u2217) + KL(E[\u03a0 \u03bd\u0302k]\u2225\u03bd\u2217) ) \u2264 2 \u03b1\u00b5\u03bb \u2228 2 \u03b1\u03bd\u03bb NI(E[\u03a0 \u00b5\u0302k],E[\u03a0 \u03bd\u0302k])\n\u2264 2 \u03b1\u00b5\u03bb \u2228 2 \u03b1\u03bd\u03bb\n[( (r + 1)2\nrk +O(k\u22122) ) \u00b7 17 4 C1(\u03b7) + 4C2 \u221a \u03b7 + 4C3\u221a N ] .\nNote also by Proposition 3.6 and Lemma C.8 that\nM\u00b5W1(E[\u00b5X k ],E[\u03a0 \u00b5\u0302k])\n= sup \u2225F\u2225Lip\u2264M\u00b5 E(X ,Y )1:k  1 Bk k\u2211 j=1 \u03b2j \u222b X F (\u00b5Xj \u2212\u03a0 \u00b5\u0302 (N) j )(dx)  \u2264 1 Bk k\u2211 j=1 \u03b2j ( r + 1 j C1(\u03b7) + C2 \u221a \u03b7 + C3\u221a N ) + 1 Bk k\u2211 j=2 \u03b2jW1(\u03a0 \u00b5\u0302 (N) j ,\u03a0 \u00b5\u0302 (N) j\u22121)\n\u2264 1 Bk k\u2211 j=1 \u03b2j ( r + 1 j C1(\u03b7) + C2 \u221a \u03b7 + C3\u221a N ) + 2M\u00b5 \u03b1\u00b5\u03bbBk k\u2211 j=2 \u03b22j Bj\n\u2264 ( (r + 1)2\nrk +O(k\u22122) ) \u00b7 3 2 C1(\u03b7) + C2 \u221a \u03b7 + C3\u221a N ,\nso the square of this term can be ignored. Hence we can conclude that\nW 21 (E[\u00b5X k ], \u00b5 \u2217) +W 21 (E[\u03bdY k ], \u03bd\n\u2217) \u2264 (r + 1) 2\nrk C\u03031(\u03b7) + C\u03032\n\u221a \u03b7 + C\u03033\u221a N ,\nagain with the 1 + log k modification when r = 0."
        },
        {
            "heading": "C.6 EXPECTED WASSERSTEIN DISTANCE",
            "text": "Theorem 3.7 gives error bounds for the expected distributions E[\u00b5X k ] and E[\u03bdY k ]. This quantifies a sort of bias of the MFL-AG outputs, but does not tell us anything about the variance. Can we similarly bound the expected distance E[W1(\u00b5X k , \u00b5 \u2217) +W1(\u03bdY k , \u03bd \u2217)] of the empirical distributions to the MNE? The following fundamental fact about Wasserstein distance tells us that this is impossible: Theorem C.14 (Rate of convergence of the empirical measure, adapted from Fournier & Guillin (2015), Theorem 1). Let Xi be independent samples drawn from \u00b5i \u2208 P2(Rd) for each i \u2208 [N ]. If d \u2265 3, the 1-Wasserstein distance between the empirical measure \u00b5X = 1N \u2211N i=1 \u03b4Xi and the\nunderlying averaged measure \u00b5 = 1N \u2211N i=1 \u00b5 i is bounded in expectation as\nE[W1(\u00b5X , \u00b5)] \u2264 CW \u221a m2(\u00b5) \u00b7N\u22121/d,\nwhere m2(\u00b5) is the raw second moment of \u00b5 and CW is a universal constant. If d = 2, the rate is O(N\u22121/2(logN)2); if d = 1, the rate is O(N\u22121/2). Furthermore, this rate is tight up to constants.\nProof. The original theorem only considers i.i.d. samples \u00b51 = \u00b7 \u00b7 \u00b7 = \u00b5N = \u00b5 and omits the W1 case for simplicity, so we present the necessary modifications.\nFor a Borel subset A \u2282 Rd, the quantity N\u00b5X (A) is not distributed as Binomial(N,\u00b5(A)) but as a sum of independent Bernoulli(\u00b5i(A)) random variables. Nonetheless, we obtain the same bound\nE[|\u00b5X (A)\u2212 \u00b5(A)|] \u2264 (E[\u00b5X (A)] + \u00b5(A)) \u2227 \u221a Var\u00b5X (A)\n\u2264 2\u00b5(A) \u2227 \u221a \u00b5(A)/N.\nWe now repeat the same arguments and substitute p = 1, q = 2 to arrive at the following inequality,\nE[W1(\u00b5X , \u00b5)] \u2264 C \u221a m2(\u00b5) \u00b7 \u221e\u2211 n=0 \u221e\u2211 m=0 2\u2212m(2\u2212n \u2227 (2dm/N)1/2)\nfrom which point we give explicit computations. Defining\nmN =\n\u2308 log2N\nd\n\u2309 , nm = \u2308 log2N \u2212 dm\n2\n\u2309 ,\nwe have for d \u2265 3 that \u221e\u2211 n=0 \u221e\u2211 m=0 2\u2212m(2\u2212n \u2227 (2dm/N)1/2)\n= mN\u22121\u2211 m=0 2\u2212mnm(2 dm/N)1/2 + mN\u22121\u2211 m=0 \u221e\u2211 n=nm 2\u2212m\u2212n + \u221e\u2211 m=mN \u221e\u2211 n=0 2\u2212m\u2212n\n\u2264 1 2 \u221a N mN\u22121\u2211 m=0 (dmN \u2212 dm+ 2)2(d/2\u22121)m + mN\u22121\u2211 m=0 21\u2212m\u2212nm + 22\u2212mN \u2264 (2 + d)2 (d/2\u22121)(mN+1)\n(2d/2 \u2212 2) \u221a N\n+ 22+(d/2\u22121)mN\n(2d/2 \u2212 2) \u221a N + 22\u2212mN\n= O(N\u22121/d).\nWhen d = 2, the rate is easily checked to be N\u22121/2(logN)2. The tight rate in one dimension is derived using different techniques in Bobkov & Ledoux (2016), Section 3.\nThat is, even in the ideal case where chaos does not propagate and the particles are somehow i.i.d. sampled directly from the true distribution, the expected Wasserstein distance will always be of order N\u22121/dX\u2228dY , automatically incurring the curse of dimensionality. We emphasize that the uniform law of large numbers and short-term perturbation methods developed throughout Section C as well as the presentation of Theorem 3.7 have been carefully designed to bypass this technicality.\nNevertheless, it is still possible to bound the expected Wasserstein distance in a similar manner save for the unavoidableN\u22121/dX\u2228dY dependency.2 We first present a more direct bound for the proximal gap. Proposition C.15. The following inequality holds for all k,\nE [ W1(\u00b5Xk ,\u03a0 \u00b5\u0302 (N) k ) ] \u2264 r + 1\nk C \u20321(\u03b7) + C \u2032 2\n\u221a \u03b7 + C \u20323N \u22121/dX .\nProof. The derivations are similar and more straightforward compared to the proof of Proposition 3.6. We only look at k \u2265 2\u2113 and directly compare \u00b5Xk to \u00b5X\u0303k using Lemma C.3, \u00b5X\u0303k to the expected modified distribution using Theorem C.14 (recall that the modified particle trajectories X\u0303ik are independent when conditioned on (X ,Y )1:k\u2212\u2113), the expected modified distribution to the stationary distribution \u03a0 \u00b5\u0302(N)k\u2212\u2113 using Proposition C.6, and \u03a0 \u00b5\u0302 (N) k\u2212\u2113 back to \u03a0 \u00b5\u0302 (N) k using Lemma C.8.\nE[W1(\u00b5Xk ,\u03a0 \u00b5\u0302 (N) k )]\n\u2264 E[W2(\u00b5Xk , \u00b5X\u0303k)] + E[W1(\u00b5X\u0303k ,E[\u00b5X\u0303k ])]\n2Of course, we may also simply run the algorithm multiple (M ) times and take the average of the outputs, which would also bypass the issue and yield the standard 1/ \u221a M convergence.\n+ 1\nN N\u2211 i=1 E[W2(\u00b5ik,\u03a0 \u00b5\u0302 (N) k\u2212\u2113)] + \u2113\u22121\u2211 j=0 E[W2(\u03a0 \u00b5\u0302(N)k\u2212j\u22121,\u03a0 \u00b5\u0302 (N) k\u2212j)]\n\u2264 r + 1 k \u2212 \u2113+ 1 w\u00b5\u2113 + CW \u221a E[m2(\u00b5X\u0303k\u2212\u2113)] \u00b7N \u22121/dX\n+ 1\nN N\u2211 i=1\n\u221a 4\n\u03b1\u00b5 (K\u00b5\u2225Xik\u2212\u2113\u22252 + L\u00b5) + 2M2\u00b5 \u03b1\u00b5\u03bb \u2113\u22121\u2211 j=0 \u03b2k\u2212j Bk\u2212j\n\u2264 r + 1 k \u2212 \u2113+ 1 w\u00b5\u2113 + CW \u221a p\u00b5 + s\u00b5 \u00b7N\u22121/dX +\n\u221a 4\n\u03b1\u00b5 (K\u00b5(p\u00b5 + s\u00b5) + L\u00b5) + 2M\u00b5 \u03b1\u00b5\u03bb (r + 1)\u2113 k \u2212 \u2113+ 1\n= r + 1\nk C \u20321(\u03b7) + C \u2032 2\n\u221a \u03b7 + C \u20323N \u22121/dX .\nWe now give the desired bound for the expected Wasserstein distance to the MNE. Note the effect of dimensionality compared to Theorem 3.7.\nTheorem C.16 (Variance of discretized MFL-AG). If \u03b7 \u2264 \u03b7\u0304 and \u03b2k = kr with r > 0, the MFL-AG discrete update satisfies for all K,N ,\nE[W1(\u00b5X K , \u00b5 \u2217)]2 + E[W1(\u03bdY K , \u03bd\n\u2217)]2 \u2264 (r + 1) 2\nrK C\u03031(\u03b7) + C\u03032\n\u221a \u03b7 + C\u03033N \u22121/dX\u2228dY\nwith similar constants as in Proposition 3.6. When r = 0, the first term is replaced by O(logK/K). If dX \u2228 dY = 2, the third term is replaced by O(N\u22121/2(logN)2); if dX = dY = 1, by O(N\u22121/2).\nProof. Note that by convexity,\nL(\u00b5Xk , \u03bd)\u2212 L(\u03a0 \u00b5\u0302 (N) k , \u03bd) \u2265 \u222b X \u03b4L \u03b4\u00b5 (\u03a0 \u00b5\u0302 (N) k , \u03bd)(\u00b5Xk \u2212\u03a0 \u00b5\u0302 (N) k )(dx) \u2265 \u2212M\u00b5W1(\u00b5Xk ,\u03a0 \u00b5\u0302 (N) k ).\nWe can modify Step 2 of Section C.5 using Proposition C.15 as follows. E(X ,Y )1:k [ N(\u00b5X k , \u03bdY k) ] \u2265 E(X ,Y )1:k [ max \u00b5,\u03bd \u2212 1 Bk k\u2211 j=1 \u03b2jL(\u00b5, \u03bdYj )\u2212 \u03bbKL(\u00b5\u2225\u03c1\u00b5) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u03bd\u0302 (N) j \u2225\u03c1 \u03bd)\n+ 1\nBk k\u2211 j=1 \u03b2jL(\u00b5Xj , \u03bd)\u2212 \u03bbKL(\u03bd\u2225\u03c1\u03bd) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u00b5\u0302 (N) j \u2225\u03c1 \u00b5)\n]\n\u2265 E(X ,Y )1:k [ max \u00b5,\u03bd \u2212 1 Bk k\u2211 j=1 \u03b2jL(\u00b5,\u03a0 \u03bd\u0302(N)j )\u2212 \u03bbKL(\u00b5\u2225\u03c1 \u00b5) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u03bd\u0302 (N) j \u2225\u03c1 \u03bd)\n+ 1\nBk k\u2211 j=1 \u03b2jL(\u03a0 \u00b5\u0302(N)j , \u03bd)\u2212 \u03bbKL(\u03bd\u2225\u03c1 \u03bd) + \u03bb Bk k\u2211 j=1 \u03b2j KL(\u03a0 \u00b5\u0302 (N) j \u2225\u03c1 \u00b5)\n\u2212 M\u00b5 Bk k\u2211 j=1 \u03b2jW1(\u00b5Xk ,\u03a0 \u00b5\u0302 (N) k )\u2212 M\u03bd Bk k\u2211 j=1 \u03b2jW1(\u03bdYk ,\u03a0 \u03bd\u0302 (N) k )\n]\n\u2265 E(X ,Y )1:k [ max \u00b5,\u03bd \u2212L(\u00b5,\u03a0 \u00b5\u0302k)\u2212 \u03bbKL(\u00b5\u2225\u03c1\u00b5) + \u03bbKL(\u03a0 \u03bd\u0302k\u2225\u03c1\u03bd)\n+ L(\u03a0 \u00b5\u0302k, \u03bd)\u2212 \u03bbKL(\u03bd\u2225\u03c1\u03bd) + \u03bbKL(\u03a0 \u00b5\u0302k\u2225\u03c1\u00b5) ]\n\u2212 M\u00b5 Bk k\u2211 j=1 \u03b2jE(X ,Y )1:k [W1(\u00b5Xk ,\u03a0 \u00b5\u0302 (N) k )]\u2212 M\u03bd Bk k\u2211 j=1 \u03b2jE(X ,Y )1:k [W1(\u03bdYk ,\u03a0 \u03bd\u0302 (N) k )]\n\u2265 E(X ,Y )1:k [ NI(\u03a0 \u00b5\u0302k,\u03a0 \u03bd\u0302k) ] \u2212 M\u00b5 Bk k\u2211 j=1 \u03b2j ( r + 1 j C \u20321(\u03b7) + C \u2032 2 \u221a \u03b7 + C \u20323N \u22121/dX\u2228dY ) .\nCombining with Step 1 and Lemma 3.5 gives that\nE [ KL(\u03a0 \u00b5\u0302k\u2225\u00b5\u2217) + KL(\u03a0 \u03bd\u0302k\u2225\u03bd\u2217) ] \u2264 (r + 1) 2\nrk C \u2032\u20321 (\u03b7) + C \u2032\u2032 2\n\u221a \u03b7 + C \u2032\u20323N \u22121/dX\u2228dY .\nFinally, we convert back to a Wasserstein distance bound by invoking Talagrand\u2019s inequality and Proposition C.15 again:\nE[W1(\u00b5X k , \u00b5 \u2217)]2 \u2264 2\n( 1\nBk k\u2211 j=1 \u03b2jE[W1(\u00b5Xj ,\u03a0 \u00b5\u0302 (N) j )] )2 + 4 \u03b1\u00b5 E[KL(\u03a0 \u00b5\u0302k\u2225\u00b5\u2217)].\nThis concludes the proof.\nRemark. If we assume a higher degree of regularity so that all relevant distributions have finite fourth moments, say, then Theorem C.14 actually holds for the 2-Wasserstein metric. Theorem C.16 can then be stated in terms of the 2-Wasserstein distance to the MNE, guaranteeing us slightly better control over the error compared to Proposition 3.6 which only allows a W1 formulation."
        },
        {
            "heading": "D CONVERGENCE ANALYSIS OF MFL-ABR",
            "text": "D.1 INNER LOOP CONVERGENCE\nThe convergence of the decoupled inner loop is a simple consequence of the convex analysis for single optimization (Nitanda et al., 2022a); we reproduce the proof here for completeness. Proposition D.1 (Convergence of MFL-ABR inner loop). Under Assumptions 1 and 3,\nKL(\u00b5\u2020k,\u03c4\u2225 \u00b5\u0302k) \u2264 2C\u00b5 \u03bb exp(\u22122\u03b1\u03bb\u03c4), KL(\u03bd\u2020k,\u03c4\u2225 \u03bd\u0302k) \u2264 2C\u03bd \u03bb exp(\u22122\u03b1\u03bb\u03c4).\nProof. For any 0 \u2264 t \u2264 \u03c4 , the KL gap converges as\nd dt KL(\u00b5\u2020k,t\u2225 \u00b5\u0302k) = \u222b X log \u00b5\u2020k,t \u00b5\u0302k \u2202t\u00b5 \u2020 k,t(dx) = \u03bb \u222b X log \u00b5\u2020k,t \u00b5\u0302k \u2207x \u00b7 ( \u00b5\u2020k,t\u2207x log \u00b5\u2020k,t \u00b5\u0302k ) (dx)\n= \u2212\u03bb \u222b X \u2225\u2225\u2225\u2225\u2207x log \u00b5\u2020k,t\u00b5\u0302k \u2225\u2225\u2225\u22252\u00b5\u2020k,t(dx) \u2264 \u22122\u03b1\u03bb \u00b7KL(\u00b5\u2020k,t\u2225 \u00b5\u0302k)\nby substituting the Fokker-Planck equation for \u00b5\u2020k,t and applying the LSI for \u00b5\u0302k via Theorem A.6. Invoking Gronwall\u2019s lemma and Lemma D.2 below for \u00b5\u0302k concludes the proof.\nThe following result gives uniform bounds to control the magnitude of perturbations. Lemma D.2. For any w > 0, define the class\nF\u00b5w := { \u00b5 \u2208 P2(X ) : \u2225\u2225\u2225\u2225log \u00b5\u03c1\u00b5 \u2225\u2225\u2225\u2225 \u221e \u2264 wC\u00b5 \u03bb } .\nThen under Assumption 3, the distribution \u00b5\u0302k \u2208 F \u00b5 2 and \u00b5k, \u00b5 \u2020 k,\u03c4 \u2208 F \u00b5 4 .\nProof. For \u00b5\u0302k, the exponential term and the normalizing integral\nexp ( \u2212 1 \u03bb \u03b4L \u03b4\u00b5 (\u00b5k, \u03bdk) ) , Z\u00b5k = \u222b X \u03c1\u00b5 exp ( \u2212 1 \u03bb \u03b4L \u03b4\u00b5 (\u00b5k, \u03bdk) ) dx\nare both bounded by T\u00b5/\u03bb, proving the assertion. For \u00b5\u2020k,\u03c4 , define the density ratio ht = \u00b5 \u2020 k,t/ \u00b5\u0302k. The Fokker-Planck equation for \u00b5\u2020k,t reads\n\u2202t\u00b5 \u2020 k,t = \u2207x \u00b7 ( \u00b5\u2020k,t\u2207x ( \u03b4L \u03b4\u00b5 (\u00b5k, \u03bdk) + \u03bb\u2207xU\u00b5 )) + \u03bb\u2206x\u00b5 \u2020 k,t = \u03bb\u2207x \u00b7 ( \u00b5\u2020k,t\u2207x log \u00b5\u2020k,t \u00b5\u0302k ) ,\nso that the parabolic partial differential equation satisfied by ht is derived as\n\u2202tht = \u00b5\u0302 \u22121 k \u2202t\u00b5 \u2020 k,t\n= \u03bb \u00b5\u0302\u22121k \u2207x \u00b7 (\u00b5\u0302k ht\u2207x log ht) = \u03bb\u2207x log \u00b5\u0302k \u00b7\u2207xht + \u03bb\u2206ht\n= \u2212\u2207x ( \u03b4L \u03b4\u00b5 (\u00b5k, \u03bdk) + \u03bb\u2207xU\u00b5 ) \u00b7 \u2207xht + \u03bb\u2206ht = L\u2020ht,\nwhere L\u2020 is the infinitesimal generator for the stochastic process X\u2020t . Hence by the Feynman-Kac formula, we may write for any t \u2208 [0, \u03c4 ]\nht(x) = Ex[h0(X\u2020t )] = Ex [ \u03c1\u00b5\n\u00b5\u0302k (Xt)\n] .\nSince \u2225log(\u00b5\u0302k /\u03c1\u00b5)\u2225\u221e \u2264 2C\u00b5/\u03bb as discussed above, we infer that \u2225h\u03c4\u2225 \u2264 2C\u00b5/\u03bb and therefore\u2225\u2225\u2225\u2225 log \u00b5\u2020k,\u03c4\u03c1\u00b5 \u2225\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225\u2225 log \u00b5\u2020k,\u03c4\u00b5\u0302k \u2225\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225\u2225 log \u00b5\u0302k\u03c1\u00b5 \u2225\u2225\u2225\u2225 \u221e \u2264 4C\u00b5 \u03bb , i.e. \u00b5\u2020k,\u03c4 \u2208 F\u00b5\u221e for all k. Finally, since F\u00b5\u221e is closed under linear combinations in P2(X ) we conclude that\n\u00b5k = \u03b2\u00b5 \u2020 k,\u03c4 + \u03b2(1\u2212 \u03b2)\u00b5 \u2020 k\u22121,\u03c4 + \u00b7 \u00b7 \u00b7\u03b2(1\u2212 \u03b2) k\u00b5\u20200,\u03c4 \u2208 F\u00b5\u221e."
        },
        {
            "heading": "D.2 PROOF OF THEOREM 4.1",
            "text": "We perform one-step analysis of the outer loop by setting for 0 \u2264 s \u2264 1\n\u00b5(s) = (1\u2212 \u03b2s)\u00b5k + \u03b2s\u00b5\u2020k,\u03c4 , \u03bd(s) = (1\u2212 \u03b2s)\u03bdk + \u03b2s\u03bd \u2020 k,\u03c4 ,\nso that \u00b5(0) = \u00b5k, \u00b5(1) = \u00b5k+1 and \u03bd(0) = \u03bdk, \u03bd(1) = \u03bdk+1. We track the KL divergence to the interpolated proximal distributions defined as\n\u00b5\u0302(s) = \u03c1\u00b5\nZ\u00b5(s) exp ( \u2212 1 \u03bb \u03b4L \u03b4\u00b5 (\u00b5(s), \u03bd(s)) ) , \u03bd\u0302(s) = \u03c1\u03bd Z\u03bd(s) exp ( 1 \u03bb \u03b4L \u03b4\u03bd (\u00b5(s), \u03bd(s)) ) .\nNote that the second order bounds in Assumption 3 immediately imply the following Lipschitz property in TV distance,\u2225\u2225\u2225\u2225\u03b4L\u03b4\u00b5 (\u00b5, \u03bd)\u2212 \u03b4L\u03b4\u00b5 (\u00b5\u2032, \u03bd\u2032) \u2225\u2225\u2225\u2225 \u221e \u2264 2C\u00b5\u00b5 TV(\u00b5, \u00b5\u2032) + 2C\u00b5\u03bd TV(\u03bd, \u03bd\u2032).\nSimilarly to Lascu et al. (2023), Lemma A.2 we can then prove that\nTV(\u00b5\u0302k, \u00b5\u0302(s))\n\u2264 1 2\u03bb\n( exp ( C\u00b5 \u03bb ) + exp ( 2C\u00b5 \u03bb )) (2C\u00b5\u00b5 TV(\u00b5k, \u00b5(s)) + 2C\u00b5\u03bd TV(\u03bdk, \u03bd(s)))\n\u2264 \u03b2st\u00b5,\nwhere we have written\nt\u00b5 := C\u00b5\u00b5 + C\u00b5\u03bd\n\u03bb\n( exp ( C\u00b5 \u03bb ) + exp ( 2C\u00b5 \u03bb )) .\nAlso, \u00b5\u0302(s) \u2208 F\u00b52 and \u00b5(s) \u2208 F \u00b5 4 by Lemma D.2 which implies \u2225log(\u00b5(s)/ \u00b5\u0302(s))\u2225\u221e \u2264 6C\u00b5/\u03bb. Now the derivative of the KL gap of the max policy for any 0 \u2264 s \u2264 1 is d\nds KL(\u00b5(s)\u2225 \u00b5\u0302(s)) = \u222b X log \u00b5(s) \u00b5\u0302(s) \u2202s\u00b5(s)(dx)\u2212 \u222b X \u2202s log \u00b5\u0302(s)\u00b5(s)(dx).\nThe first term can be decomposed as\u222b X log \u00b5(s) \u00b5\u0302(s) \u2202s\u00b5(s)(dx)\n= \u03b2 \u222b X log \u00b5(s) \u00b5\u0302(s) (\u00b5\u2020k,\u03c4 \u2212 \u00b5k)(dx)\n= \u03b2 \u222b X log \u00b5(s) \u00b5\u0302(s) (\u00b5\u0302(s)\u2212 \u00b5(s) + \u00b5(s)\u2212 \u00b5k + \u00b5\u2020k,\u03c4 \u2212 \u00b5\u0302k + \u00b5\u0302k \u2212 \u00b5\u0302(s))(dx)\n\u2264 \u2212\u03b2 (KL(\u00b5(s)\u2225 \u00b5\u0302(s)) + KL(\u00b5(s)\u2225 \u00b5\u0302(s))) + 2\u03b22s \u2225\u2225\u2225\u2225log \u00b5(s)\u00b5\u0302(s) \u2225\u2225\u2225\u2225 \u221e TV(\u00b5\u2020k,\u03c4 , \u00b5k)\n+ \u03b2 \u2225\u2225\u2225\u2225log \u00b5(s)\u00b5\u0302(s) \u2225\u2225\u2225\u2225 \u221e \u221a 2KL(\u00b5\u2020k,\u03c4\u2225 \u00b5\u0302k) + 2\u03b2 \u2225\u2225\u2225\u2225log \u00b5(s)\u00b5\u0302(s) \u2225\u2225\u2225\u2225 \u221e TV(\u00b5\u0302k, \u00b5\u0302(s))\n\u2264 \u2212\u03b2KL(\u00b5(s)\u2225 \u00b5\u0302(s)) + 12\u03b2C\u00b5 \u03bb\n( 2\u03b2s(t\u00b5 + 1) + \u221a C\u00b5 \u03bb exp(\u2212\u03b1\u03bb\u03c4) ) .\nby Proposition D.1. For the second term, we may follow the derivations presented in Section 3 of Lascu et al. (2023) with minimal modifications to obtain\n\u2212 \u222b X \u2202s log \u00b5\u0302(s)\u00b5(s)(dx) = \u2212\u03b2 \u03bb \u222b\u222b X \u00d7X \u03b42L \u03b4\u00b52 (\u00b5(s), \u03bd(s), x, z)(\u00b5\u0302(s)\u2212 \u00b5(s))(dx)(\u00b5\u0302k \u2212\u00b5k)(dz)\n+ \u03b2\n\u03bb \u222b\u222b X \u00d7Y \u03b42L \u03b4\u00b5\u03b4\u03bd (\u00b5(s), \u03bd(s), x, w)(\u00b5\u0302(s)\u2212 \u00b5(s))(dx)(\u03bd\u0302k \u2212\u03bdk)(dw).\nWhen s = 0, the first integral is nonpositive due to convexity while the second integral cancels out when adding with the corresponding term for the KL gap of the max policy, which completes the argument in Lascu et al. (2023). Hence the remaining error we must control is\n\u2212 \u03b2 \u03bb \u222b\u222b X \u00d7X \u03b42L \u03b4\u00b52 (\u00b5(s), \u03bd(s), x, z)(\u00b5\u0302(s)\u2212 \u00b5\u0302k +\u00b5k \u2212 \u00b5(s))(dx)(\u00b5\u0302k \u2212\u00b5k)(dz)\n+ \u03b2\n\u03bb \u222b\u222b X \u00d7Y \u03b42L \u03b4\u00b5\u03b4\u03bd (\u00b5(s), \u03bd(s), x, w)(\u00b5\u0302(s)\u2212 \u00b5\u0302k +\u00b5k \u2212 \u00b5(s))(dx)(\u03bd\u0302k \u2212\u03bdk)(dw)\n\u2264 4\u03b2 \u03bb (C\u00b5\u00b5 + C\u00b5\u03bd)(TV(\u00b5\u0302(s), \u00b5\u0302k) + TV(\u00b5k, \u00b5(s))) \u2264 4\u03b2 2s\n\u03bb (C\u00b5\u00b5 + C\u00b5\u03bd)(t\n\u00b5 + 1).\nAdding everything up, we obtain\nd\nds (KL(\u00b5(s)\u2225 \u00b5\u0302(s)) + KL(\u03bd(s)\u2225 \u03bd\u0302(s))) \u2264 \u2212\u03b2 (KL(\u00b5(s)\u2225 \u00b5\u0302(s)) + KL(\u03bd(s)\u2225 \u03bd\u0302(s)))\n+ 12\u03b2C\u00b5 \u03bb\n( 2\u03b2(t\u00b5 + 1) + \u221a C\u00b5 \u03bb exp(\u2212\u03b1\u03bb\u03c4) ) + 4\u03b22 \u03bb (C\u00b5\u00b5 + C\u00b5\u03bd)(t \u00b5 + 1)\n+ 12\u03b2C\u03bd \u03bb\n( 2\u03b2(t\u03bd + 1) + \u221a C\u03bd \u03bb exp(\u2212\u03b1\u03bb\u03c4) ) + 4\u03b22 \u03bb (C\u00b5\u03bd + C\u03bd\u03bd)(t \u03bd + 1).\nBy applying Gronwall\u2019s lemma over s \u2208 [0, 1] and iterating over k, we conclude that\nKL(\u00b5k\u2225 \u00b5\u0302k) + KL(\u03bdk\u2225 \u03bd\u0302k) \u2264 2(C\u00b5 + C\u03bd) \u03bb exp(\u2212\u03b2k) + 12 \u03bb 3 2 ( C 3 2 \u00b5 + C 3 2 \u03bd ) exp(\u2212\u03b1\u03bb\u03c4)\n+ 4\u03b2\n\u03bb ((6C\u00b5 + C\u00b5\u00b5 + C\u00b5\u03bd)(t\n\u00b5 + 1) + (6C\u03bd + C\u00b5\u03bd + C\u03bd\u03bd)(t \u03bd + 1)) .\nFinally, applying Lemma 3.4 of Lascu et al. (2023) yields the suboptimality bound\nNI(\u00b5k, \u03bdk) \u2264 2(C\u00b5 + C\u03bd) exp(\u2212\u03b2k) + 12\u221a \u03bb\n( C 3 2 \u00b5 + C 3 2 \u03bd ) exp(\u2212\u03b1\u03bb\u03c4) + C\u03b2.\nHence an \u03f5-MNE may be obtained in k = O( 1\u03f5 log 1 \u03f5 ) outer loop iterations by taking \u03b2 = O(\u03f5) and \u03c4 = O(log 1\u03f5 )."
        }
    ],
    "title": "DISTRIBUTIONAL MINIMAX PROBLEMS",
    "year": 2024
}