{
    "abstractText": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhenwen Dai"
        },
        {
            "affiliations": [],
            "name": "Federico Tomasi"
        },
        {
            "affiliations": [],
            "name": "Sina Ghiassian"
        }
    ],
    "id": "SP:130f832e55cd3d6202df0f025caaf41bd13d6c0b",
    "references": [
        {
            "authors": [
                "Jacob Abernethy",
                "Alekh Agarwal",
                "Teodor V Marinov",
                "Manfred K Warmuth"
            ],
            "title": "A mechanism for sample-efficient in-context learning for sparse retrieval tasks",
            "venue": "arXiv preprint arXiv:2305.17040,",
            "year": 2023
        },
        {
            "authors": [
                "Marcin Andrychowicz",
                "Misha Denil",
                "Sergio Gomez",
                "Matthew W Hoffman",
                "David Pfau",
                "Tom Schaul",
                "Brendan Shillingford",
                "Nando De Freitas"
            ],
            "title": "Learning to learn by gradient descent by gradient descent",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Maximilian Balandat",
                "Brian Karrer",
                "Daniel R. Jiang",
                "Samuel Daulton",
                "Benjamin Letham",
                "Andrew Gordon Wilson",
                "Eytan Bakshy"
            ],
            "title": "BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Misha Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ron Dorfman",
                "Idan Shenfeld",
                "Aviv Tamar"
            ],
            "title": "Offline meta reinforcement learning \u2013 identifiability challenges and effective data collection strategies",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yan Duan",
                "Marcin Andrychowicz",
                "Bradly Stadie",
                "OpenAI Jonathan Ho",
                "Jonas Schneider",
                "Ilya Sutskever",
                "Pieter Abbeel",
                "Wojciech Zaremba"
            ],
            "title": "One-shot imitation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Damien Ernst",
                "Pierre Geurts",
                "Louis Wehenkel"
            ],
            "title": "Tree-based batch mode reinforcement learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Flennerhag",
                "Yannick Schroecker",
                "Tom Zahavy",
                "Hado van Hasselt",
                "David Silver",
                "Satinder Singh"
            ],
            "title": "Bootstrapped meta-learning",
            "venue": "arXiv preprint arXiv:2109.04504,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "Off-policy deep reinforcement learning without exploration",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Caglar Gulcehre",
                "Ziyu Wang",
                "Alexander Novikov",
                "Thomas Paine",
                "Sergio G\u00f3mez",
                "Konrad Zolna",
                "Rishabh Agarwal",
                "Josh S Merel",
                "Daniel J Mankowitz",
                "Cosmin Paduraru"
            ],
            "title": "Rl unplugged: A suite of benchmarks for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Abhishek Gupta",
                "Benjamin Eysenbach",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Unsupervised metalearning for reinforcement learning",
            "venue": "arXiv preprint arXiv:1806.04640,",
            "year": 2018
        },
        {
            "authors": [
                "Timothy Hospedales",
                "Antreas Antoniou",
                "Paul Micaelli",
                "Amos Storkey"
            ],
            "title": "Meta-learning in neural networks: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Kyle Hsu",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Unsupervised learning via meta-learning",
            "venue": "arXiv preprint arXiv:1810.02334,",
            "year": 2018
        },
        {
            "authors": [
                "Michael Janner",
                "Qiyang Li",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning as one big sequence modeling problem",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jungtaek Kim",
                "Seungjin Choi"
            ],
            "title": "BayesO: A Bayesian optimization framework in Python",
            "venue": "https: //bayeso.org,",
            "year": 2017
        },
        {
            "authors": [
                "Sascha Lange",
                "Thomas Gabel",
                "Martin Riedmiller"
            ],
            "title": "Batch reinforcement learning",
            "venue": "In Reinforcement learning: State-of-the-art,",
            "year": 2012
        },
        {
            "authors": [
                "Michael Laskin",
                "Luyu Wang",
                "Junhyuk Oh",
                "Emilio Parisotto",
                "Stephen Spencer",
                "Richie Steigerwald",
                "DJ Strouse",
                "Steven Stenberg Hansen",
                "Angelos Filos",
                "Ethan Brooks",
                "maxime gazeau",
                "Himanshu Sahni",
                "Satinder Singh",
                "Volodymyr Mnih"
            ],
            "title": "In-context reinforcement learning with algorithm distillation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Kuang-Huei Lee",
                "Ofir Nachum",
                "Mengjiao (Sherry) Yang",
                "Lisa Lee",
                "Daniel Freeman",
                "Sergio Guadarrama",
                "Ian Fischer",
                "Winnie Xu",
                "Eric Jang",
                "Henryk Michalewski",
                "Igor Mordatch"
            ],
            "title": "Multi-game decision transformers",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Ke Li",
                "Jitendra Malik"
            ],
            "title": "Learning to optimize",
            "venue": "arXiv preprint arXiv:1606.01885,",
            "year": 2016
        },
        {
            "authors": [
                "Long-Ji Lin"
            ],
            "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
            "venue": "Machine learning,",
            "year": 1992
        },
        {
            "authors": [
                "Kevin Lu",
                "Aditya Grover",
                "Pieter Abbeel",
                "Igor Mordatch"
            ],
            "title": "Pretrained transformers as universal computation engines",
            "venue": "arXiv preprint arXiv:2103.05247,",
            "year": 2021
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "arXiv preprint arXiv:2202.12837,",
            "year": 2022
        },
        {
            "authors": [
                "Nikhil Mishra",
                "Mostafa Rohaninejad",
                "Xi Chen",
                "Pieter Abbeel"
            ],
            "title": "A simple neural attentive metalearner",
            "venue": "arXiv preprint arXiv:1707.03141,",
            "year": 2017
        },
        {
            "authors": [
                "Eric Mitchell",
                "Rafael Rafailov",
                "Xue Bin Peng",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Offline metareinforcement learning with advantage weighting",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "Ashvin Nair",
                "Abhishek Gupta",
                "Murtaza Dalal",
                "Sergey Levine"
            ],
            "title": "Awac: Accelerating online reinforcement learning with offline datasets",
            "venue": "arXiv preprint arXiv:2006.09359,",
            "year": 2020
        },
        {
            "authors": [
                "Vitchyr H Pong",
                "Ashvin V Nair",
                "Laura M Smith",
                "Catherine Huang",
                "Sergey Levine"
            ],
            "title": "Offline metareinforcement learning with online self-supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sachin Ravi",
                "Hugo Larochelle"
            ],
            "title": "Optimization as a model for few-shot learning",
            "venue": "In International conference on learning representations,",
            "year": 2016
        },
        {
            "authors": [
                "Scott Reed",
                "Konrad Zolna",
                "Emilio Parisotto",
                "Sergio Gomez Colmenarejo",
                "Alexander Novikov",
                "Gabriel Barth-Maron",
                "Mai Gimenez",
                "Yury Sulsky",
                "Jackie Kay",
                "Jost Tobias Springenberg",
                "Tom Eccles",
                "Jake Bruce",
                "Ali Razavi",
                "Ashley Edwards",
                "Nicolas Heess",
                "Yutian Chen",
                "Raia Hadsell",
                "Oriol Vinyals",
                "Mahyar Bordbar",
                "Nando de Freitas"
            ],
            "title": "A generalist agent. May 2022a",
            "year": 2022
        },
        {
            "authors": [
                "Scott Reed",
                "Konrad Zolna",
                "Emilio Parisotto",
                "Sergio Gomez Colmenarejo",
                "Alexander Novikov",
                "Gabriel Barth-Maron",
                "Mai Gimenez",
                "Yury Sulsky",
                "Jackie Kay",
                "Jost Tobias Springenberg"
            ],
            "title": "A generalist agent",
            "venue": "arXiv preprint arXiv:2205.06175,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Riedmiller"
            ],
            "title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method",
            "venue": "In Machine Learning: ECML 2005: 16th European Conference on Machine Learning,",
            "year": 2005
        },
        {
            "authors": [
                "Juergen Schmidhuber",
                "Jieyu Zhao",
                "MA Wiering"
            ],
            "title": "Simple principles of metalearning",
            "venue": "Technical report IDSIA,",
            "year": 1996
        },
        {
            "authors": [
                "Noah Y Siegel",
                "Jost Tobias Springenberg",
                "Felix Berkenkamp",
                "Abbas Abdolmaleki",
                "Michael Neunert",
                "Thomas Lampe",
                "Roland Hafner",
                "Nicolas Heess",
                "Martin Riedmiller"
            ],
            "title": "Keep doing what worked: Behavioral modelling priors for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2002.08396,",
            "year": 2020
        },
        {
            "authors": [
                "Bradly C Stadie",
                "Ge Yang",
                "Rein Houthooft",
                "Xi Chen",
                "Yan Duan",
                "Yuhuai Wu",
                "Pieter Abbeel",
                "Ilya Sutskever"
            ],
            "title": "Some considerations on learning to explore via meta-reinforcement learning",
            "venue": "arXiv preprint arXiv:1803.01118,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton"
            ],
            "title": "Learning to predict by the methods of temporal differences",
            "venue": "Machine learning,",
            "year": 1988
        },
        {
            "authors": [
                "Richard S Sutton"
            ],
            "title": "A history of meta-gradient: Gradient methods for meta-learning",
            "venue": "arXiv preprint arXiv:2202.09701,",
            "year": 2022
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "David McAllester",
                "Satinder Singh",
                "Yishay Mansour"
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "Advances in neural information processing systems,",
            "year": 1999
        },
        {
            "authors": [
                "Adaptive Agent Team",
                "Jakob Bauer",
                "Kate Baumli",
                "Satinder Baveja",
                "Feryal Behbahani",
                "Avishkar Bhoopchand",
                "Nathalie Bradley-Schmieg",
                "Michael Chang",
                "Natalie Clay",
                "Adrian Collister"
            ],
            "title": "Human-timescale adaptation in an open-ended task space",
            "venue": "arXiv preprint arXiv:2301.07608,",
            "year": 2023
        },
        {
            "authors": [
                "Sebastian Thrun",
                "Lorien Pratt"
            ],
            "title": "Learning to learn",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Vivek Veeriah",
                "Matteo Hessel",
                "Zhongwen Xu",
                "Janarthanan Rajendran",
                "Richard L Lewis",
                "Junhyuk Oh",
                "Hado P van Hasselt",
                "David Silver",
                "Satinder Singh"
            ],
            "title": "Discovery of useful questions as auxiliary tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jane X Wang",
                "Zeb Kurth-Nelson",
                "Dhruva Tirumala",
                "Hubert Soyer",
                "Joel Z Leibo",
                "Remi Munos",
                "Charles Blundell",
                "Dharshan Kumaran",
                "Matt Botvinick"
            ],
            "title": "Learning to reinforcement learn",
            "venue": "arXiv preprint arXiv:1611.05763,",
            "year": 2016
        },
        {
            "authors": [
                "Olga Wichrowska",
                "Niru Maheswaranathan",
                "Matthew W Hoffman",
                "Sergio Gomez Colmenarejo",
                "Misha Denil",
                "Nando Freitas",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Learned optimizers that scale and generalize",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Zhongwen Xu",
                "Hado P van Hasselt",
                "David Silver"
            ],
            "title": "Meta-gradient reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Zhongwen Xu",
                "Hado P van Hasselt",
                "Matteo Hessel",
                "Junhyuk Oh",
                "Satinder Singh",
                "David Silver"
            ],
            "title": "Meta-gradient reinforcement learning with an objective discovered online",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhangyue Yin",
                "Qiushi Sun",
                "Qipeng Guo",
                "Jiawen Wu",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Do large language models know what they don\u2019t know? In Findings of the Association for Computational Linguistics: ACL",
            "year": 2023
        },
        {
            "authors": [
                "Tom Zahavy",
                "Zhongwen Xu",
                "Vivek Veeriah",
                "Matteo Hessel",
                "Junhyuk Oh",
                "Hado P van Hasselt",
                "David Silver",
                "Satinder Singh"
            ],
            "title": "A self-tuning actor-critic algorithm. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Qinqing Zheng",
                "Amy Zhang",
                "Aditya Grover"
            ],
            "title": "Online decision transformer",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning, Proceedings of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Zeyu Zheng",
                "Junhyuk Oh",
                "Satinder Singh"
            ],
            "title": "On learning intrinsic rewards for policy gradient methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Luisa Zintgraf",
                "Sebastian Schulze",
                "Cong Lu",
                "Leo Feng",
                "Maximilian Igl",
                "Kyriacos Shiarlis",
                "Yarin Gal",
                "Katja Hofmann",
                "Shimon Whiteson"
            ],
            "title": "Varibad: Variational bayes-adaptive deep rl via meta-learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformers represent a highly effective approach to sequence modelling, with applications extending across multiple domains like text, image, and audio. In the field of reinforcement learning (RL), Chen et al. (2021) and Janner et al. (2021) have suggested the concept of addressing offline RL as a sequential prediction problem using Transformer. This method has proven successful in tackling a range of tasks using solely offline training when combined with large-scale sequence modelling techniques (Lee et al., 2022; Reed et al., 2022a). A notable shortcoming lies with the policy\u2019s inability to self-improve when employed in online environments. To overcome this, fine-tuning methods have been introduced, like (Zheng et al., 2022), which enable continued policy improvement. However, these methods often rely on slow, computationally expensive gradient-based optimization.\nConversely, in-context learning, a characteristic observed in large language models (LLMs), can handle new tasks by providing the task details through language prompts, effectively eliminating the need for fine-tuning. Laskin et al. (2023) suggest an in-context learning algorithm for RL, which uses a sequence model to distill a policy learning algorithm from RL training trajectories. The resulting model is capable of conducting policy learning at inference-time through an iterative process of action sampling and prompt augmentation. This method incurs significant computational costs in collecting extensive training trajectory sets and training large Transformer models that are needed to model a substantial part of a training trajectory. The key reason for this high computational cost is the lengthy RL training trajectories resulting from the slow trial-and-error process of RL policy learning algorithms.\nThis paper aims to improve the efficiency of in-context policy learning by eliminating the need to learn from policy learning trajectories. In an ideal scenario, efficient policy learning could be achieved through an efficient trial-and-error process. For simplified RL problems such as multiarmed bandits (MAB), efficient trial-and-error process such as Thompson sampling and upper confidence bounds are proven to exist. This process, often referred to as the exploration-exploitation (EE) trade-off, relies heavily on the epistemic uncertainty derived from Bayesian belief. However, it is intractable to infer the exact epistemic uncertainty of sequential RL problems using conventional Bayesian methods. In light of recent studies on uncertainty estimation of LLMs (Yin et al., 2023),\nwe examine predictive distributions of sequence models, demonstrating that, by training with purely supervised learning on offline data, a sequence model can capture epistemic uncertainty in sequence prediction. This suggests the potential for implementing EE in offline RL.\nBased on this observation, we develop an in-context exploration-exploitation (ICEE) algorithm for policy learning. ICEE takes as inputs a sequence of multiple episodes of the same task and predicts the corresponding action at every step conditioned on some hindsight information. This offline RL design resembles the Decision Transformer (DT, Chen et al., 2021), but ICEE tackles in-context policy learning by modeling multiple episodes of a task while DT only models a single episode. Furthermore, these episodes do not need to originate from a training trajectory, thereby averting the high computational costs associated with generating and consuming learning trajectories. The action distribution learned in DT is biased towards the data collection policy, which may not be ideal when it is sub-optimal. To address this bias, we introduce an unbiased objective and develop a specific form of hindsight information for efficient EE across episodes.\nWith experiments, we demonstrate that the EE behavior emerges in ICEE during inference thanks to epistemic uncertainty in action prediction. This is particularly evident when applying ICEE to Bayesian optimization (BO), as the performance of ICEE is on par with a Gaussian process based method on discrete BO tasks. We also illustrate that ICEE can successfully improve the policy for a new task with trials-and-errors from scratch for sequential RL problems. To the best of our knowledge, ICEE is the first method that successfully incorporates in-context exploration-exploitation into RL through offline sequential modelling."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Meta Learning. Interest in meta learning or learning to learn algorithms has recently increased. While learner is an agent that learns to solve a task using observed data, a learning to learn algorithm includes a meta-learner that continually improves the learning process of the learner (Schmidhuber et al., 1996; Thrun & Pratt, 2012; Hospedales et al., 2021; Sutton, 2022). Lots of work has been done within the space of meta learning. For example, Finn et al. (2017) proposed a general model-agnostic meta-learning algorithm that trains the model\u2019s initial parameters such that the model has maximal performance on a new task after the model parameters have been updated through a few gradient steps computed with a small amount of data from the new task. Other works on meta learning include improving optimizers (Andrychowicz et al., 2016; Li & Malik, 2016; Ravi & Larochelle, 2016; Wichrowska et al., 2017), improving few-shot learning (Mishra et al., 2017; Duan et al., 2017), learning to explore (Stadie et al., 2018), and unsupervised learning (Hsu et al., 2018).\nIn the space of deep meta RL (Wang et al., 2016), a few works focused on a special form of metalearning called meta-gradients. In meta-gradients the meta-learner is trained by gradients by measuring the effect of meta-parameters on a learner that itself is also trained using a gradient algorithm (Xu et al., 2018). In other work, Zheng et al. (2018) used meta-gradients to learn rewards. Gupta et al. (2018) focused on automating the process of task design in RL, to free the expert from the burden of manual design of meta-learning tasks. Similarly, Veeriah et al. (2019) presented a method for a reinforcement learning agent to discover questions formulated as general value functions through the use of non-myopic meta-gradients. More recently, meta gradient reinforcement learning has seen substantial breakthroughs from performance gains on popular benchmarks to hybrid offline-online algorithms for meta RL (Xu et al., 2020; Zahavy et al., 2020; Flennerhag et al., 2021; Mitchell et al., 2021; Yin et al., 2023; Pong et al., 2022). The role of uncertainty in meta RL has been studied by Zintgraf et al. (2021), which result in an efficient online meta RL method. This work is then extended by Dorfman et al. (2021) to the off-policy setting.\nOffline Reinforcement Learning. In general, reinforcement learning was proposed as a fundamentally online paradigm (Sutton, 1988; Sutton et al., 1999; Sutton & Barto, 2018). This online learning nature comes with some limitations such as making it difficult to adopt it to many applications for which it is impossible to gather online data and learn at the same time, such as autonomous driving and is also sometimes not as data efficient as it could bem since it might choose to learn from a sample and then dispose the sample and move on to the next (Levine et al., 2020). One idea for getting more out of the collected experience is to use replay buffers. When using buffers, part of the samples are kept in a memory and then are re-used multiple times so that the agent can learn more from them (Lin, 1992; Mnih et al., 2015). A variant of reinforcement learning, referred to as offline RL studies\nRL algorithms that can learn fully offline, from a fixed dataset of previously gathered data without gathering new data at the time of learning (Ernst et al., 2005; Riedmiller, 2005; Lange et al., 2012; Fujimoto et al., 2019; Siegel et al., 2020; Gulcehre et al., 2020; Nair et al., 2020). Recent literature on decision transformers also focuses on offline RL (Chen et al., 2021) because it needs to calculate the return to go at training time, which in turn necessitates previously gathered data.\nIn-context learning. In-context RL algorithms are the ones that improve their policy entirely incontext without updating the network parameters or without any fine-tuning of the model(Lu et al., 2021). There has been some work studying the phenomenon of in-context learning trying to explain how learning in-context might be possible Abernethy et al. (2023); Min et al. (2022). The \u201cGato\u201d agent developed by Reed et al. (2022b) works as a multi-model, multi-task, multi-embodiment generalist agent, meaning that the same trained agent can play Atari, caption images, chat, stack blocks with a real robot arm only based on its context. By training an RL agent at large scale, Team et al. (2023) showed that an in-context agent can adapt to new and open ended 3D environments. Of special interest to us is Algorithm Distillation (AD), which is a meta-RL method (Laskin et al., 2023). More specifically, AD is an in-context offline meta-RL method. Basically, AD is gradient-free\u2014it adapts to downstream tasks without updating its network parameters."
        },
        {
            "heading": "3 EPISTEMIC UNCERTAINTY IN SEQUENCE MODEL PREDICTION",
            "text": "DT, also known as Upside-down RL, treats the offline policy learning problem as a sequence modelling problem. In this section, we consider a generic sequencing model and analyze its predictive uncertainty.\nLetX1:T = (x1, . . . ,xT ) be a sequence of inputs of length T and Y1:T = (y1, . . . ,yT ) be the corresponding sequence of outputs. Assume that the output sequence is generated following a step-wise probabilistic distribution parameterized by \u03b8, yt \u223c p(yt|xt,X1:t\u22121,Y1:t\u22121,\u03b8). Each sequence is generated with a different parameter sampled from its prior distribution, \u03b8 \u223c p(\u03b8). These define a generative distribution of a sequence:\np(Y1:T ,\u03b8|X1:T ) = p(\u03b8)p(y1|x1) T\u220f\nt=2\np(yt|xt,X1:t\u22121,Y1:t\u22121,\u03b8). (1)\nA sequence modeling task is often defined as training an autoregressive model parameterized by \u03c8, p\u03c8(yt|xt,X1:t\u22121,Y1:t\u22121), given a dataset of sequences D = {X(i),Y (i)}i generated from the above unknown generative distribution. At the limit of infinite data, the objective of maximum likelihood learning of the above sequence model can be formulated as \u03c8\u2217 = argmax\u03c8 L\u03c8 ,\nL\u03c8 =\u2212 \u2211\nt\n\u222b p(Y1:t\u22121|X1:t\u22121)\nDKL (p(yt|xt,X1:t\u22121,Y1:t\u22121)||p\u03c8(yt|xt,X1:t\u22121,Y1:t\u22121)) dY1:t\u22121 + C, (2)\nwhere DKL(\u00b7||\u00b7) denotes the Kullback Leibler divergence and C is a constant with respect to \u03c8. The left hand side distribution in the cross entropy term p(yt|xt,X1:t\u22121,Y1:t\u22121) is the true predictive distribution of yt|xt conditioned on the observed history Y1:t\u22121 and X1:t\u22121, which can be written as\np(yt|xt,X1:t\u22121,Y1:t\u22121) = \u222b p(yt|xt,X1:t\u22121,Y1:t\u22121,\u03b8)p(\u03b8|X1:t\u22121,Y1:t\u22121)d\u03b8, (3)\nwhere\np(\u03b8|X1:t\u22121,Y1:t\u22121) = p(\u03b8)p(Y1:t\u22121|X1:t\u22121,\u03b8)\u222b\np(\u03b8\u2032)p(Y1:t\u22121|X1:t\u22121,\u03b8\u2032)d\u03b8\u2032 . (4)\nAs shown above, the true predictive distribution of yt|xt contains both aleatoric uncertainty and epistemic uncertainty, in which the epistemic uncertainty is contributed by p(\u03b8|X1:t\u22121,Y1:t\u22121). With sufficient data and model capacity, the generative distribution in the sequence model p\u03c8(yt|xt,X1:t\u22121,Y1:t\u22121) will be trained to match the true predictive distribution. As a result, we can expect epistemic uncertainty to be contained in the predictive distribution of a sequence model. Note that the predictive distribution can only capture the epistemic uncertainty with respect to the parameters of sequences \u03b8, but does not include the epistemic uncertainty regarding the hyper-parameters (if exist)."
        },
        {
            "heading": "4 IN-CONTEXT POLICY LEARNING",
            "text": "Epistemic uncertainty is the essential ingredient of EE. With the observation that the predictive distribution sequence model contains epistemic uncertainty, we design an in-context policy learning algorithm with EE.\nConsider the problem of solving a family of RL games based on offline data. From each game, a set of trajectories are collected from a number of policies, where \u03c4 (i) k = (o (i) k,1,a (i) k,1, r (i) k,1, . . . ,o (i) k,Tk ,a (i) k,Tk , r (i) k,Tk ) is the trajectory of the k-th episode of the i-th game and o, a, r denote the observed state, action and reward respectively. The policy used to collect \u03c4 (i)k is denoted as \u03c0(i)k (a (i) k,t|o (i) k,t). We concatenate all the episodes of the i-th game into a single sequence \u03c4 (i) = (\u03c4 (i)1 , . . . , \u03c4 (i) K ). For convenience, the superscript\n(i) will be omitted in the following text unless the i-th game is explicitly referred to.\nWe propose a sequence model that is trained to step-wisely predict p\u03c8(ak,t|Rk,t,ok,t,Hk,t), where Rk,t is the return-to-go at the k-th episode and the t time step and Hk,t = (\u03c4k,1:t\u22121, \u03c41:k\u22121) is the history until the time step t including the past episodes1. The above model formulation is similar to DT but a sequence in DT only contains one episode. Note that, different from AD, the concatenated trajectories do not need to be from a RL learning algorithm.\nAs shown in the previous section, by doing maximum likelihood learning on the collected trajectories, the predictive distribution will be trained to match the true posterior distribution of action of the data collection policy,\np(ak,t|Rk,t,ok,t,Hk,t) = p(Rk,t|ak,t,ok,t,Hk,t)\u03c0k(ak,t|ok,t)\u222b\np(Rk,t|a\u2032k,t,ok,t,Hk,t)\u03c0k(a\u2032k,t|ok,t)da\u2032k,t , (5)\nwhere p(Rk,t|ak,t,ok,t,Hk,t) is the distribution of return after the time step t following \u03c0k. As shown in (5), the posterior distribution of action is biased towards the data collection policy. Following such an action distribution allows us to reproduce the trajectories generated by the data collection policy but will lead to recreation of suboptimal trajectories if the data collection policy is not optimal. A more desirable action distribution is the action distribution that corresponds to the specified return without the influence of data collection policy, i.e.\np\u0302(ak,t|Rk,t,ok,t,Hk,t) = p(Rk,t|ak,t,ok,t,Hk,t)U(ak,t)\u222b\np(Rk,t|a\u2032k,t,ok,t,Hk,t)U(a\u2032k,t)da\u2032k,t , (6)\nwhere U(ak,t) is the uniform random policy, which gives all the actions equal probabilities. To let the sequence model learn the unbiased action distribution, the maximum likelihood objective needs to be defined as\nL\u03c8 = \u2211\nk,t\n\u222b p\u0302(Rk,t,ak,t|ok,t,Hk,t) log p\u03c8(ak,t|Rk,t,ok,t,Hk,t)dRk,tdak,t. (7)\nAfter applying the importance sampling trick, the Monte Carlo approximation of the above objective can be derived as\nL\u03c8 \u2248 \u2211\nk,t\nU(ak,t) \u03c0k(ak,t|ok,t) log p\u03c8(ak,t|Rk,t,ok,t,Hk,t), (8)\nwhere ak,t \u223c \u03c0k(ak,t|ok,t) and Rk,t \u223c p(Rk,t|ak,t,ok,t,Hk,t), i.e., ak,t and Rk,t are sampled from the data collection policy \u03c0k."
        },
        {
            "heading": "5 DESIGN OF RETURN-TO-GO",
            "text": "Return-to-go is a crucial component of DT for solving RL tasks at inference using a trained sequence model. Its return-to-go scheme is designed to calculate the expected return signal from a single episode. In order to achieve in-context policy learning, we design the return-to-go across episodes.\n1Although the learned action distribution does not need to condition on the return-to-go of the past actions, for the convenience of a causal Transformer implementation, Hk,t contains the return-to-go of the past actions.\nAlgorithm 1: In-context Exploration-Exploitation (ICEE) Action Inference Input: A trained ICEE model p\u03c8 H1,1 = {} ; for each episode k = 1, . . . ,K do\nReset the environment; Select the episode return-to-go c\u0303k ; for each step t = 1, . . . , Tk do\nGet an observation ok,t ; Sample the step return-to-go ck,t \u223c q(ck,t) ; Sample an action ak,t \u223c p\u03c8(ak,t|Rk,t,ok,t,Hk,t) ; Take the sampled action ak,t and collect the reward rk,t; Expand the historyHk,t+1 =Hk,t \u222a {Rk,t,ok,t,ak,t, rk,t};\nend Compute the true return-to-go based on the whole episode {R\u0302k,t}Tkt=1; Update the historyHk,Tk with the true return-to-go {R\u0302k,t}Tkt=1;\nend\nThe return-to-go of ICEE consists of two components: one for the individual steps within an episode and the other for the cross-episode behavior, Rk,t = (ck,t, c\u0303k). The in-episode return-to-go ck,t follows the design used in (Chen et al., 2021), which is defined as the cumulative rewards starting from the current step ck,t = \u2211 t\u2032>t rk,t\u2032 . This design borrows the concept of the cumulative reward of RL and has the benefit of encapsulating the information of the future rewards following the policy. This is very useful when the outcomes of future steps strongly depend on the state and action of the current step. It allows the action that leads to a good future outcome to be differentiated from the action that leads to a bad future outcome in the sequence model. The downside is that with a nonexpert data collection policy, the optimal return-to-go at each state is often unobserved. This will limits the ability of the sequence model to achieve better performance than the data collection policy at inference time.\nIn the cross-episode return-to-go design, the situation is different. The initial states of individual episodes are independent of each other. What determines the cumulative rewards of individual episodes are the sequence of actions. If we consider the whole policy space as the action space for each episode, the cross-episode decision making is closer to MAB, where the policy is the action and the return of an episode is the MAB reward. Driven by this observation, we define the return-to-go based on the improvement of the return of current episode compared to all the previous episodes. Specifically, we define the cross-episode return-to-go as\nc\u0303k = { 1 r\u0304k > max1\u2264j\u2264k\u22121 r\u0304j , 0 otherwise.\n(9)\nwhere r\u0304k = \u2211\nt rk,t is the cumulative reward of the k-th episode. Intuitively, at inference time, by conditioning on c\u0303k = 1, we take actions from a policy that is \u201csampled\u201d according to the probability of being better than all the previous episodes. This encourages the sequence model to deliver better performance after collecting more and more episodes. This design avoids the limitation of the need of observing the optimal policy learning trajectories.\nAction Inference. After training the sequence model, the model can be used to perform policy learning from scratch. At each step, we sample an action from the sequence model conditioned on the trajectory so far and a return-to-go for the step. The return-to-go for action sampling is defined as follows. The cross-episode return-to-go c\u0303k is always set to one to encourage policy improvements. For the in-episode return-to-go, we follow the action inference proposed by Lee et al. (2022). During training of ICEE, a separate sequence model is trained to predict the discretized return from the trajectories, p\u03d5(ck,t|c\u0303k,ok,t,Hk,t). At inference time, an in-episode return-to-go for each step is sampled from an augmented distribution\nq(ck,t) \u221d p\u03d5(ck,t|c\u0303k,ok,t,Hk,t)( ck,t \u2212 cmin cmax \u2212 cmin )\u03ba. (10)\nThis augmentation biases the return-to-go distribution towards higher values, which encourages the agent to take actions that leads to better returns. The return prediction is not combined into the main\nsequence model as in (Lee et al., 2022), because in the main sequence model the returns are also fed into the inputs. In this way, the model quickly figures out ck,t can be predicted from ck,t\u22121. This is problematic because at inference time the true return cannot be observed until the end of an episode.\nAfter ck,t is sampled, an action is sampled conditioned on the combined return-to-go Rk,t. The resulting state and reward are concatenated to the trajectory for the next step action prediction. At the end of an episode, we will recalculate the true ck,t and c\u0303k based on the rewards from the whole episode and update the return-to-go in the trajectory with the recalculated ones. This makes the trajectory at inference time be as close to the training trajectories as possible. The description of the action inference algorithm can be found in Algorithm 1."
        },
        {
            "heading": "6 BAYESIAN OPTIMIZATION EXPERIMENTS",
            "text": "Bayesian optimization (BO) is a very successful application of exploration-exploitation (EE). It is able to search for the optimum of a function with the minimum number of function evaluations. The Gaussian process (GP) based BO methods have been widely used in various domains like hyperparameter tuning, drug discovery, aerodynamic optimization. To evaluate the EE performance of ICEE, we apply it to BO and compare it with a GP-based approach using one of most widely used acquisition functions, expected improvement (EI).\nWe consider a discrete BO problem. The task is to find the location from a fixed set of points that has the lowest function value as few number of function evaluations as possible. BO can be viewed as a special type of multi-armed bandit (MAB), where each action is associated with a location in a bounded space. To solve BO with ICEE, we encode the iterative search trajectory of a function as a single sequence, where at is the location of which a function value is collected at the step t, rt is the corresponding function value and Rt is the return-to-go. The observations {ot} can be used to encode any side information that is known about the function and the minimum. As no such information is available for generic BO, we do not use {ot} in our experiment. As the function value can seen as the available immediate reward for, we treat each action as a different episode and use only the episode return-to-go as the Rt here. As each action is associated with a location, we embed actions by learning a linear projection between the location space and the embedding space. When decoding a Transformer output for an action, the logit of an action is generated using a MLP that takes as input the Transformer output together with the embedding associated with the action. The design is to tackle the challenge that the set of locations of each function may be different.\nTo train ICEE to solve the discrete BO problem, we need to generate training data that consists of input-output pairs of randomly sampled functions. During training, the input-output pairs at random locations are generated on-the-fly. We use a GP with the Mate\u0301rn 5/2 kernel to sample 1024 points for each function. The locations of these points are sampled from a uniform distribution on [0, 1]. The length scales of the kernel are sampled from a uniform distribution on [0.05, 0.3].\nIdeally, for each sampled function, we can use a BO algorithm to solve the search problem and take the sequence of actions as the training data. This is very computationally expensive due to\nthe high computational cost of BO algorithms. Thanks to the EE property of ICEE, the training data do not need to come from an expert inference algorithm. This is very different from behavior cloning / algorithm distillation. We use a cheap data collection algorithm that randomly picks actions according to a probability distribution that is computed based on the function values of actions. The probability distribution is defined as follows: The actions are sorted in an ascending order according to their function values and the action at the i-th position gets the probability p(a = ai) \u221d exp( 1024\u2212i1023 \u03b3), \u03b3 is sampled from a uniform distribution between 0 and 10 for each function. Intuitively, the locations with lower function values are more likely to be picked.\nAfter training ICEE, we evaluate its performance on a set of 2D benchmark functions. We use 16 2D functions implemented in (Kim & Choi, 2017). The input space of every function is normalized to be between 0 and 1. We randomly sample a different set of 1024 points from each function and normalize the resulting function values to be zero-mean and unit-variance. Each function was given five trials with different initial designs. At each step of search, we compute the difference of the function value between the current best estimate and the true minimum. The average performance of all the evaluated functions is shown in Fig. 1. The performance of ICEE is compared with the GP-based BO method using the EI acquisition function and the random baseline that picks locations according to a uniform random probability. \u201cICEE-biased\u201d denotes a variant of ICEE that does not use the action bias correction objective as shown in (8). The search efficiency of ICEE is on par with the GP-based BO method with EI. Both of them are significantly better than random and are noticeably better than ICEE-biased. The performance gap between ICEE and ICEE-biased shows the loss of efficiency due to the biased learned action distribution. Being able to perform on par with a state-of-the-art BO method demonstrates that ICEE is able to perform state-of-the-art EE with in-context inference.\nA clear advantage of ICEE is that the whole search is done through model inference without need of any gradient optimization. In contrast, GP-based BO methods need to fit a GP surrogate function at each step, which results into a significant speed difference. Fig. 1b shows the same search results with the x-axis being the elapsed time. All the methods run on a single A100 GPU. Thanks to the in-context inference, ICEE is magnitude faster than conventional BO methods. More details of the BO experiments can be found in Appendix D."
        },
        {
            "heading": "7 RL EXPERIMENTS",
            "text": "We investigate the in-context policy learning capability of ICEE on sequential RL problems. To demonstrate the capability of in-context learning, we focus on the families of environments that cannot be solved through zero-shot generalization of a pre-trained model, so in-context policy learning is necessary for solving the tasks. This implies that some information that are important to the success of a task is missing from the state representation and will need to be discovered by the agent. We use the two grid world environments in (Lee et al., 2022): dark room and dark key-to-door. The details of them are as follows.\nDark Room. The experiment takes place in a 2D discrete POMDP, where an agent is placed inside a room to locate a goal spot. The agent has access to its own location coordinates (x, y), but is unaware of the goal\u2019s placement requiring it to deduce it from the rewards received. The room\u2019s dimensions are 9x9 with the possible actions by the agent including moving one step either left, right, up or down, or staying idle, all within an episode length of 20. Upon completion, the agent gets placed back at the mid-point of the map. Two environment variants are considered for this experiment: The Dark Room case where the agent obtains a reward (r=1) each time the goal is achieved, and the Dark Room Hard case where the rewards are sparse (r=1 only once for attaining the goal). Whenever the reward value is not 1, it will be considered as 0. Different from (Lee et al., 2022), we keep the room size of the hard case to be 9 x 9.\nDark Key-to-Door. This setting is similar to the Dark Room, but with added challenging features. The task of the agent is to locate an invisible key to receive a one-time reward of r=1, and subsequently, identify an invisible door to gain another one-time reward of r=1. Otherwise, the reward remains at r=0. The agent\u2019s initial location in each episode resets randomly. The room size is still 9 x 9 but the episode length increases to 50 steps.\nTo collect data for offline training, we sample a set of new games for each mini-batch. We collect K episodes from each game. Thanks to the EE capability of ICEE, the training data do not need to be from a real RL learning algorithm like Deep Q-Network (DQN), which is expensive to run. Instead, we let a cheap data collection policy act for K episodes independently and concatenate the resulting episodes into a single sequence. We use an \u03f5-greedy version of the \u201ccheating\u201d optimal policy. The policy knows the goal location that is unknown to the agent and will move straightly towards the goal with 1\u2212 \u03f5 probability and with \u03f5 probability it will take an action that does not make the agent closer to the goal. For each episode, \u03f5 is sampled from a uniform distribution between 0 and 1. Intuitively, this policy has some chance to solve a game efficiently when \u03f5 is small but on average it does not deliver a good performance. For Dark Room experiments, each sequence consists of 50 episodes and for Dark Key-To-Door, it consists of 20 episodes.\nDark Room (Biased). To demonstrate the benefits of EE for sequential RL problem when the data collection policy cannot be optimal, we create a variant of the dark room environment. At each step, the data collection policy takes the \u201cleft\u201d action with the 2/3 probability and with the 1/3 probability acts as described above. At training time, the goal can be at any where in the room and, at evaluation time, the goal will only appear on the right hand side where x > 5.\nFor sequential RL problems, ICEE consists of two sequence models: one for action prediction and the other for the in-episode return-to-go prediction. The return-to-go sequence model takes as inputs the sequence of state, action, reward triplets and predicts the in-episode return-to-go. The action prediction model takes as inputs the sequence of the triplets and the two return-to-go Rk,t and predicts the action sequence. The two models are trained together with the same gradient optimizer. To encourage ICEE to solve the games quickly, when calculating the in-episode return-to-go, a negative reward, \u22121/T , is given to each step that does not receive a reward, where T is the episode length. Both c\u0303k and ck,t are discrete and tokenized."
        },
        {
            "heading": "7.1 BASELINE METHODS",
            "text": "Source. We use the data collection policy as a baseline for comparison. As the data collection policy solves each episode independently, we calculate the average return across multiple episodes.\nAlgorithm Distillation (AD, Laskin et al., 2023). The in-context learning algorithm that distills a RL algorithms from RL training trajectories. AD predicts action based only on the current states and the history of state, action and reward triplets. We replicate the implementation of AD using the Transformer architecture as ICEE. We apply AD to the same training data as ICEE uses (ignoring the return-to-go signals), despite they are generated from RL learning trajectories.\nAD-sorted. AD is designed to be trained on RL learning trajectories. An important property of RL learning trajectories is that the performance of the agent gradually increases throughout training. To mimic such trajectories using our data, we sort the episodes in a sequence according to the sampled \u03f5 of the data collection policy in the descent order. \u03f5 determines how close the data collection policy is to the optimal policy. In this order, the episodes in a latter position of sequence tend to have a higher return. We train AD using this sorted sequences instead of the original ones.\nMulti-game Decision Transformer (MGDT, Lee et al., 2022). MGDT is not an in-context learning algorithm. We train MGDT using only one episode from each sampled game. The performance of MGDT shows that what the performance of the agent is when there is no in-context policy learning."
        },
        {
            "heading": "7.2 EVALUATION & RESULTS",
            "text": "After training, ICEE will be evaluated on solving a set of sampled games. The inference algorithm is described in Alg. 1. No online model update is performed by ICEE and all the baseline methods at evaluation time. For each sampled game, ICEE and two variants of AD will act for K episodes consecutively. In each episode, the trajectories from the past episodes are used as in the history representation. Ideally, a good performing agent identifies the missing information with as few number of episodes as possible and then maximizes the return in the following episodes. For each problem, we sample 100 games and K is 50 for Dark Room and 20 for Key-To-Door.\nThe experiment results are shown in Fig. 2. ICEE is able to solve the sampled games efficiently compared to the baseline methods. The EE capability allows ICEE to search for the missing infor-\nmation efficiently and then acts with confidence once the missing information is found. An indicator of such behavior is the continuous decrease of the action entropies as the agent experiences more episodes (see Fig. 2e).\nAs expected, the original AD learns to mimic the data collection policy, which results in an average performance slightly below the data collection policy. MGDT fails to solve most of the games due to the missing information. Interestingly, despite that the training data is not generated from a RL learning algorithm, AD-sorted is able to clone the behavior of the data collection policy with different \u03f5 at different stages, which allows it to solve the games at the end of the sequence.\nICEE-biased is not shown in Fig. 2a, Fig. 2b and Fig. 2c as it achieves similar performance as ICEE does. The reason is that there is no clear bias in the action distribution of the data collection policy. However, as shown in Fig. 2d, for the Dark Room (Biased) environment, ICEE clearly outperforms ICEE-biased, as it can overcome the bias in the data collection policy and keep sufficient uncertainty in action distribution to explore the right hand side of the room. AD-sorted fails the task because it clones the data collection policy, which is unlikely to solve the tasks due to the action bias."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "In this paper, we analyze the predictive distribution of sequence models and show that the predictive distribution can contain epistemic uncertainty, which inspires the creation of an EE algorithm. We present an in-context EE algorithm by extending the DT formulation to in-context policy learning and deriving an unbiased training objective. Through the experiments on BO and discrete RL problems, we demonstrate that: (i) ICEE can perform EE in in-context learning without the need of explicit Bayesian inference; (ii) The performance of ICEE is on par with state-of-the-art BO methods without the need of gradient optimization, which leads to significant speed-up; (iii) New RL tasks can be solved within tens of episodes."
        }
    ],
    "year": 2024
}