{
    "abstractText": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
    "authors": [
        {
            "affiliations": [],
            "name": "ERY IN"
        },
        {
            "affiliations": [],
            "name": "DYNAMIC RADIANCE FIELD"
        },
        {
            "affiliations": [],
            "name": "Kaizhi Yang"
        },
        {
            "affiliations": [],
            "name": "Xiaoshuai Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhiao Huang"
        },
        {
            "affiliations": [],
            "name": "Xuejin Chen"
        },
        {
            "affiliations": [],
            "name": "Zexiang Xu"
        },
        {
            "affiliations": [],
            "name": "Hao Su"
        }
    ],
    "id": "SP:dadf01836bfc92437f92aa16695e8957f2efdfd2",
    "references": [
        {
            "authors": [
                "Antonio Agudo",
                "Francesc Moreno-Noguer"
            ],
            "title": "Robust spatio-temporal clustering and reconstruction of multiple deformable bodies",
            "venue": "IEEE TPAMI,",
            "year": 2019
        },
        {
            "authors": [
                "Christoph Bregler",
                "Aaron Hertzmann",
                "Henning Biermann"
            ],
            "title": "Recovering non-rigid 3d shape from image streams",
            "venue": "In CVPR,",
            "year": 2000
        },
        {
            "authors": [
                "Ang Cao",
                "Justin Johnson"
            ],
            "title": "Hexplane: A fast representation for dynamic scenes",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Anpei Chen",
                "Zexiang Xu",
                "Andreas Geiger",
                "Jingyi Yu",
                "Hao Su"
            ],
            "title": "Tensorf: Tensorial radiance fields",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Laura Downs",
                "Anthony Francis",
                "Nate Koenig",
                "Brandon Kinman",
                "Ryan Hickman",
                "Krista Reymann",
                "Thomas B. McHugh",
                "Vincent Vanhoucke"
            ],
            "title": "Google scanned objects: A high-quality dataset of 3d scanned household",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Du",
                "Yinan Zhang",
                "Hong-Xing Yu",
                "Joshua B. Tenenbaum",
                "Jiajun Wu"
            ],
            "title": "Neural radiance flow for 4d view synthesis and video processing",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Bernhard Egger",
                "William A.P. Smith",
                "Ayush Tewari",
                "Stefanie Wuhrer",
                "Michael Zollhoefer",
                "Thabo Beeler",
                "Florian Bernard",
                "Timo Bolkart",
                "Adam Kortylewski",
                "Sami Romdhani",
                "Christian Theobalt",
                "Volker Blanz",
                "Thomas Vetter"
            ],
            "title": "3d morphable face models\u2014past, present, and future",
            "year": 2021
        },
        {
            "authors": [
                "Jiemin Fang",
                "Taoran Yi",
                "Xinggang Wang",
                "Lingxi Xie",
                "Xiaopeng Zhang",
                "Wenyu Liu",
                "Matthias Nie\u00dfner",
                "Qi Tian"
            ],
            "title": "Fast dynamic radiance fields with time-aware neural voxels",
            "year": 2022
        },
        {
            "authors": [
                "Ronald Fedkiw",
                "Jos Stam",
                "Henrik Wann Jensen"
            ],
            "title": "Visual simulation of smoke",
            "venue": "In Siggraph,",
            "year": 2001
        },
        {
            "authors": [
                "Sara Fridovich-Keil",
                "Giacomo Meanti",
                "Frederik Rahb\u00e6k Warburg",
                "Benjamin Recht",
                "Angjoo Kanazawa"
            ],
            "title": "K-planes: Explicit radiance fields in space, time, and appearance",
            "year": 2023
        },
        {
            "authors": [
                "Chen Gao",
                "Ayush Saraf",
                "Johannes Kopf",
                "Jia-Bin Huang"
            ],
            "title": "Dynamic view synthesis from dynamic monocular video",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Paulo FU Gotardo",
                "Aleix M Martinez"
            ],
            "title": "Non-rigid structure from motion with complementary rank-3 spaces",
            "venue": "In CVPR,",
            "year": 2011
        },
        {
            "authors": [
                "Tianhao Wu",
                "Kwang Moo Yi",
                "Fangcheng Zhong",
                "Andrea Tagliasacchi"
            ],
            "title": "Kubric: a scalable dataset generator",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Guo",
                "Guanying Chen",
                "Yuchao Dai",
                "Xiaoqing Ye",
                "Jiadai Sun",
                "Xiao Tan",
                "Errui Ding"
            ],
            "title": "Neural deformable voxel grid for fast optimization of dynamic view synthesis",
            "year": 2022
        },
        {
            "authors": [
                "Marc Habermann",
                "Weipeng Xu",
                "Michael Zollhoefer",
                "Gerard Pons-Moll",
                "Christian Theobalt"
            ],
            "title": "Livecap: Real-time human performance capture from monocular video",
            "venue": "In ACM TOG,",
            "year": 2019
        },
        {
            "authors": [
                "Zhang Jiakai",
                "Liu Xinhang",
                "Ye Xinyi",
                "Zhao Fuqiang",
                "Zhang Yanshun",
                "Wu Minye",
                "Zhang Yingliang",
                "Xu Lan",
                "Yu Jingyi"
            ],
            "title": "Editable free-viewpoint video using a layered neural representation",
            "year": 2021
        },
        {
            "authors": [
                "Yuki Kawana",
                "YUSUKE Mukuta",
                "Tatsuya Harada"
            ],
            "title": "Unsupervised pose-aware part decomposition for 3d articulated objects",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Margret Keuper",
                "Bjoern Andres",
                "Thomas Brox"
            ],
            "title": "Motion trajectory segmentation via minimum cost multicuts",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Muhammed Kocabas",
                "Nikos Athanasiou",
                "Michael J. Black"
            ],
            "title": "Vibe: Video inference for human body pose and shape estimation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Chen Kong",
                "Simon Lucey"
            ],
            "title": "Deep non-rigid structure from motion",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Tianye Li",
                "Mira Slavcheva",
                "Michael Zollh\u00f6fer",
                "Simon Green",
                "Christoph Lassner",
                "Changil Kim",
                "Tanner Schmidt",
                "Steven Lovegrove",
                "Michael Goesele",
                "Richard Newcombe",
                "Zhaoyang Lv"
            ],
            "title": "Neural 3d video synthesis from multi-view video",
            "year": 2022
        },
        {
            "authors": [
                "Zhengqi Li",
                "Simon Niklaus",
                "Noah Snavely",
                "Oliver Wang"
            ],
            "title": "Neural scene flow fields for space-time view synthesis of dynamic scenes",
            "year": 2021
        },
        {
            "authors": [
                "Jia-Wei Liu",
                "Yan-Pei Cao",
                "Weijia Mao",
                "Wenqiao Zhang",
                "David Junhao Zhang",
                "Jussi Keppo",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou"
            ],
            "title": "Devrf: Fast deformable voxel radiance fields for dynamic scenes",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Miles Macklin",
                "Matthias M\u00fcller",
                "Nuttapong Chentanez",
                "Tae-Yong Kim"
            ],
            "title": "Unified particle physics for real-time applications",
            "venue": "In ACM TOG,",
            "year": 2014
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Tongzhou Mu",
                "Zhan Ling",
                "Fanbo Xiang",
                "Derek Cathera Yang",
                "Xuanlin Li",
                "Stone Tao",
                "Zhiao Huang",
                "Zhiwei Jia",
                "Hao Su"
            ],
            "title": "Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada"
            ],
            "title": "Neural articulated radiance field",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Umar Iqbal",
                "Jonathan Tremblay",
                "Tatsuya Harada",
                "Orazio Gallo"
            ],
            "title": "Watch it move: Unsupervised discovery of 3D joints for re-posing of articulated objects",
            "year": 2022
        },
        {
            "authors": [
                "Martin Ralf Oswald",
                "Jan St\u00fchmer",
                "Daniel Cremers"
            ],
            "title": "Generalized connectivity constraints for spatio-temporal 3d reconstruction",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Anestis Papazoglou",
                "Vittorio Ferrari"
            ],
            "title": "Fast object segmentation in unconstrained video",
            "venue": "In ICCV,",
            "year": 2013
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Jonathan T. Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Steven M. Seitz",
                "Ricardo Martin-Brualla"
            ],
            "title": "Nerfies: Deformable neural radiance fields",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Peter Hedman",
                "Jonathan T. Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Ricardo Martin-Brualla",
                "Steven M. Seitz"
            ],
            "title": "Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields",
            "venue": "ACM TOG,",
            "year": 2021
        },
        {
            "authors": [
                "Sida Peng",
                "Chen Geng",
                "Yuanqing Zhang",
                "Yinghao Xu",
                "Qianqian Wang",
                "Qing Shuai",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "title": "Implicit neural representations with structured latent codes for human body modeling",
            "venue": "IEEE TPAMI,",
            "year": 2023
        },
        {
            "authors": [
                "Erik Learned-Miller Pia Bideau"
            ],
            "title": "It\u2019s moving! a probabilistic model for causal motion segmentation in moving camera videos",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Albert Pumarola",
                "Enric Corona",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "D-nerf: Neural radiance fields for dynamic scenes",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Zhang Richard",
                "Isola Phillip",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "year": 2018
        },
        {
            "authors": [
                "Yahao Shi",
                "Xinyu Cao",
                "Bin Zhou"
            ],
            "title": "Self-supervised learning of part mobility from point cloud sequence",
            "venue": "In Computer Graphics forum,",
            "year": 2021
        },
        {
            "authors": [
                "Vikramjit Sidhu",
                "Edgar Tretschk",
                "Vladislav Golyanik",
                "Antonio Agudo",
                "Christian Theobalt"
            ],
            "title": "Neural dense non-rigid structure from motion with latent space constraints",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Cheng Sun",
                "Min Sun",
                "Hwann-Tzong Chen"
            ],
            "title": "Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction",
            "year": 2022
        },
        {
            "authors": [
                "Edgar Tretschk",
                "Ayush Tewari",
                "Vladislav Golyanik",
                "Michael Zollh\u00f6fer",
                "Christoph Lassner",
                "Christian Theobalt"
            ],
            "title": "Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Vadim Tschernezki",
                "Diane Larlus",
                "Andrea Vedaldi"
            ],
            "title": "NeuralDiff: Segmenting 3D objects that move in egocentric videos",
            "venue": "In 3DV,",
            "year": 2021
        },
        {
            "authors": [
                "Tony Tung",
                "Shohei Nobuhara",
                "Takashi Matsuyama"
            ],
            "title": "Complete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo",
            "venue": "In ICCV,",
            "year": 2009
        },
        {
            "authors": [
                "Fangyin Wei",
                "Rohan Chabra",
                "Lingni Ma",
                "Christoph Lassner",
                "Michael Zollhoefer",
                "Szymon Rusinkiewicz",
                "Chris Sweeney",
                "Richard Newcombe",
                "Mira Slavcheva"
            ],
            "title": "Self-supervised neural articulated shape and appearance models",
            "year": 2022
        },
        {
            "authors": [
                "Chung-Yi Weng",
                "Brian Curless",
                "Pratul P. Srinivasan",
                "Jonathan T. Barron",
                "Ira KemelmacherShlizerman"
            ],
            "title": "HumanNeRF: Free-viewpoint rendering of moving people from monocular video",
            "year": 2022
        },
        {
            "authors": [
                "Tianhao Walter Wu",
                "Fangcheng Zhong",
                "Andrea Tagliasacchi",
                "Forrester Cole",
                "Cengiz Oztireli"
            ],
            "title": "D\u02c62neRF: Self-supervised decoupling of dynamic and static objects from a monocular video",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Wenqi Xian",
                "Jia-Bin Huang",
                "Johannes Kopf",
                "Changil Kim"
            ],
            "title": "Space-time neural irradiance fields for free-viewpoint video",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Xie",
                "Yu Xiang",
                "Zaid Harchaoui",
                "Dieter Fox"
            ],
            "title": "Object discovery in videos as foreground motion clustering",
            "year": 2019
        },
        {
            "authors": [
                "Jiarui Xu",
                "Shalini De Mello",
                "Sifei Liu",
                "Wonmin Byeon",
                "Thomas Breuel",
                "Jan Kautz",
                "Xiaolong Wang"
            ],
            "title": "Groupvit: Semantic segmentation emerges from text supervision",
            "year": 2022
        },
        {
            "authors": [
                "Charig Yang",
                "Hala Lamdouar",
                "Erika Lu",
                "Andrew Zisserman",
                "Weidi Xie"
            ],
            "title": "Self-supervised video object segmentation by motion grouping",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Yu",
                "Sara Fridovich-Keil",
                "Matthew Tancik",
                "Qinhong Chen",
                "Benjamin Recht",
                "Angjoo Kanazawa"
            ],
            "title": "Plenoxels: Radiance fields without neural networks",
            "year": 2022
        },
        {
            "authors": [
                "Wentao Yuan",
                "Zhaoyang Lv",
                "Tanner Schmidt",
                "Steven Lovegrove"
            ],
            "title": "Star: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering",
            "year": 2021
        },
        {
            "authors": [
                "Li Zhang",
                "Brian Curless",
                "Steven M. Seitz"
            ],
            "title": "Spacetime stereo: Shape recovery for dynamic scenes",
            "venue": "In CVPR,",
            "year": 2003
        },
        {
            "authors": [
                "Wang Zhou",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE TIP,",
            "year": 2004
        },
        {
            "authors": [
                "Yi Zhou",
                "Connelly Barnes",
                "Jingwan Lu",
                "Jimei Yang",
                "Hao Li"
            ],
            "title": "On the continuity of rotation representations in neural networks",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\n3D scene reconstruction and understanding is one of the central problems in computer vision and graphics, with a wide range of applications in mixed reality, robotics, movie production, etc. While many works focus on static scenes, real-world physical scenes are usually dynamic and entangled with illumination changes, object motion, and shape deformation. The reconstruction of dynamic scenes is known to be highly challenging. Nonrigid structure from motion methods (Bregler et al. (2000); Gotardo & Martinez (2011); Kong & Lucey (2019); Sidhu et al. (2020)) could recover nonrigid shapes but are limited to sparse feature tracking. To reduce the ambiguity between shape and motion, some other methods introduce multi-view capture (Zhang et al. (2003); Oswald et al. (2014); Tung et al. (2009)) or category-specific priors (Egger et al. (2021); Habermann et al. (2019); Kocabas et al. (2020)). Recently, neural radiance representa-\ntion NeRF (Mildenhall et al. (2020)) has been applied to this field and achieved promising dynamic capture performance on general scenes using only monocular input (Pumarola et al. (2020); Park et al. (2021a); Li et al. (2022)). However, most NeRF-based dynamic scene modeling methods only focus on scene reconstruction without considering scene understanding, thus lacking the ability to directly support downstream applications that need tracking, shape editing, re-animation, etc.\n\u2217denotes equal advisory and \u2020 denotes the corresponding author.\nOur goal is to enable practical dynamic scene capture with both high-quality reconstruction and meaningful scene understanding from monocular input. To this end, we propose MovingParts, a novel NeRF-based approach that can achieve not only fast dynamic scene reconstruction but also automatic rigid part discovery. Our key insight is that motion (while complicating the reconstruction) is an effective cue for identifying object parts because the scene content belonging to one rigid part has to share the same rigid transformation. Therefore, we design novel modules to explain the motions in dynamic neural fields, enabling unsupervised object part discovery via motion grouping. Since the rigid motion patterns are used as the evidence of part discovery, we make explicit the assumption of our input here, which is the general scene with piece-wise rigid motion.\nOur approach is inspired by the literature on fluid simulation. We note that a family of previous dynamic NeRF methods model motion using a 3D field that encodes scene flow (Li et al. (2021)) or deformation (Pumarola et al. (2020); Park et al. (2021a)). Specifically, at time t, for each location x, the 3D field encodes which particle xc in the canonical frame has been deformed to x, which actually backward deforms the particles from world space to static canonical space. As shown in Figure 1, this is essentially the Eulerian view in fluid simulation (Fedkiw et al. (2001)) \u2013 motion information is denoted as a function \u03a8E(x, t) at each specific location x in the world coordinate frame. It is known that, while the entire scene motion can be modeled under the Eulerian view, specific object/part motion at different temporal moments is actually intractable and hard to analyze. On the other hand, the Lagrangian view (Macklin et al. (2014)), as the duality of the Eulerian view, uses the particlebased representation to track the motion of each particle belonging to the object, which forward deforms the particles from canonical space to world space. The constructed particle trajectory from the Lagrangian view can be an important clue for scene analysis. A relevant Lagrangian-based work is Watch-It-Move (Noguchi et al. (2022)) which composes objects into several ellipsoid-like parts by rendering supervision. However, the multi-view requirement and the ellipsoidal geometric prior highly limit its application. In contrast, we mainly focus on monocular input.\nTo achieve meaningful scene understanding by motion analysis, we propose a hybrid approach that learns motion under both the Eulerian and the Lagrangian views. In particular, our neural dynamic scene model consists of three modules: (1) a canonical module that models the scene geometry and appearance as a radiance field in a static canonical space, (2) an Eulerian module \u03a8E(x, t) that records which particle xc in the canonical space passes through each specific location x in the world coordinate frame at every time step, and (3) a Lagrangian module \u03a8L(xc, t) that records the trajectory of all particles xc in the canonical space. Note that the motions modeled by the Eulerian and Lagrangian modules are inherently reciprocal, we, therefore, apply a cycle-consistency loss during reconstruction to enforce the consistency between the two modules, constraining them to model the same underlying motion in the scene.\nThe construction of the Lagrangian view makes it convenient to discover parts by factorizing \u03a8L(xc, t). As the particles in a rigid part share a common rigid transformation pattern, we propose a novel motion grouping module as part of our Lagrangian module. By projecting the particle motion features into a few groups, we divide the scene into meaningful parts. Once reconstructed, our Lagrangian module could offer part-level representation and allow for direct downstream applications such as part tracking, object control, and scene editing. Since the number of rigid parts generally differs across scenes, we introduce an additional post-processing merging module that can adaptively merge the over-segmented groups into a reasonable number of rigid parts.\nWe jointly train all modules with only rendering supervision. We demonstrate that our approach achieves high-quality dynamic scene reconstruction and realistic rendering results on par with stateof-the-art methods. More importantly, compared with previous monocular NeRF methods, ours is the only one that simultaneously achieves part discovery, allowing for many more downstream applications. Finally, inspired by recent fast NeRF reconstruction methods (Sun et al. (2022); Chen et al. (2022); Yu et al. (2022)), we construct our system with feature volumes and light-weight multilayer perceptrons (MLPs), leading to a fast reconstruction speed comparable to other concurrent methods that are specifically focused on speeding up dynamic NeRF.\nIn summary, our key contributions are: \u2022 We propose a novel NeRF-based method for simultaneous dynamic scene reconstruction\nand rigid part discovery from monocular image sequences; \u2022 The hybrid representation of feature volume and neural network allows us to achieve both\nhigh-quality reconstruction and reasonable part discovery within 30 minutes;\n\u2022 The extracted part-level representation can be directly applied to downstream applications like part tracking, object control, scene editing, etc."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Dynamic Neural Radiance Fields. Recently, the emergence of Neural Radiance Fields (NeRF) (Mildenhall et al. (2020)) has facilitated the tasks of scene reconstruction and image synthesis. Due to the dynamic properties of the physical world, an important branch of NeRF research is to extend it to dynamic scenes (Pumarola et al. (2020); Li et al. (2021); Park et al. (2021b); Fridovich-Keil et al. (2023); Cao & Johnson (2023)). In particular, some methods directly extend the 5D radiance field function to 6D by adding additional time-dependent input to the network (Li et al. (2022); Xian et al. (2021)). Other works enhance the temporal consistency in the 6D dynamic radiance field by explicitly modeling dynamic scene flows (Li et al. (2021); Du et al. (2021); Gao et al. (2021)), leading to promising results from only monocular input. Meanwhile, deformation modules have also been adopted in NeRF-based methods (Pumarola et al. (2020); Tretschk et al. (2021); Yuan et al. (2021); Park et al. (2021a;b); Liu et al. (2022)), offering strong regularization for temporal consistency. Note that these various NeRF-based methods all explain motions (modeled as flows or deformation fields) from the location-based Eulerian view and do not support part discovery. We instead propose a hybrid model that models motions with both location-based Eulerian and particlebased Lagrangian views, enabling high-quality dynamic scene reconstruction with automatic part discovery based on particle motion. In addition to these general methods, some NeRF methods have been devised for particular domains, such as humans (Jiakai et al. (2021); Noguchi et al. (2021); Weng et al. (2022); Peng et al. (2023)), and articulated objects within specific categories Wei et al. (2022). While capable of achieving dynamic scene rendering and part segmentation, these methods often incorporate category priors into the pipeline and cannot be directly applied to general objects.\nPart Discovery from Motion. At the image level, most motion-based object discovery methods (Keuper et al. (2015); Pia Bideau (2016); Yang et al. (2021); Xie et al. (2019); Papazoglou & Ferrari (2013)) employ the clustering of 2D pixels based on features related to optical flow. We share a common underlying logic with these 2D methods that discover parts (or objects) by constructing and grouping motion trajectories. However, in contrast to these approaches, our method establishes a motion group module on canonical 3D particles and relies on predicted 3D rigid motion, which ensures arbitrary viewpoints consistency and temporal consistency of the grouping results. In the 3D domain, some methods (Shi et al. (2021); Kawana et al. (2022)) reason about object parts by constructing point-wise correspondence at different object states and clustering their trajectories. Without 3D input, (Agudo & Moreno-Noguer (2019)) adopts non-rigid structure from motion to reconstruct the 3D shape and applies spatio-temporal clustering to the 3D points to reason about segmentation. However, only the geometry of sparse feature points could be achieved. Recently, NeRF-based dynamic scene decoupling methods (Yuan et al. (2021); Tschernezki et al. (2021); Wu et al. (2022)) have been proposed. Although they achieve dynamic scene decomposition with highquality reconstruction, they can only divide the scene into static/dynamic parts and are unable to identify motion patterns. A relevant recent work is Watch-It-Move (Noguchi et al. (2022)), which achieves high-quality part-level reconstruction from image sequences. However, it requires dense multi-view input and imposes ellipsoid-like priors to the part geometry, which may completely fail on challenging monocular data with complex scene geometry. In contrast, our NeRF-based method does not require any shape priors of dynamic objects in complex scenes and can achieve dynamic reconstruction and part discovery from monocular input."
        },
        {
            "heading": "3 PRELIMINARIES: NERF AND D-NERF",
            "text": "By incorporating implicit function and volume rendering, Neural Radiance Field (NeRF) (Mildenhall et al. (2020)) allows for scene reconstruction and novel view synthesis via optimizing scene representation directly. In general, NeRF interprets static scenario as a continuous implicit function F\u03b8. By querying spatial coordinates (x) and view direction (d), F\u03b8 outputs the corresponding density (\u03c3) and observed color (c) as (c, \u03c3) = F\u03b8(x,d). Through classical volume rendering in graphics, the 3D scene representation F\u03b8 can be rendered into a 2D image. Specifically, given a ray r emitted from the optical center to a specific pixel in the image, the rendered color of that pixel is an integral of all the colors on the ray with near and far bounds hn and hf :\nC(r) = \u222b hf hn T (h)\u03c3 ( r(h) ) c ( r(h),d ) dh, where T (h) = exp ( \u2212 \u222b h hn \u03c3(r(s))ds ) . (1)\nT (h) can be interpreted as the transparency accumulated from hn to h. Because of the inherent differentiability of Eq. 1, it only requires a set of images with camera poses to optimize F\u03b8 directly.\nD-NeRF (Pumarola et al. (2020)) extends NeRF to capture dynamic scenes, assuming that there is a static canonical space and includes all objects. It divides the dynamic scene reconstruction in world space into two sub-problems: the NeRF representation learning of canonical space and the learning of the mapping from the world space to the canonical space (scene flow prediction) as:\n(c, \u03c3) = F\u03b8(\u03a8(x, t),d) (2)\nwhere \u03a8(x, t) predict the canonical space position from x at time t into its canonical configuration."
        },
        {
            "heading": "4 OUR METHOD",
            "text": "From the perspective of fluid simulation, the scene motion is composed of particle motions. \u03a8(x, t) in D-NeRF can be interpreted as recording the motion of particles passing through a given coordinate x at time t, corresponding to the Eulerian perspective. Following D-NeRF, we also assume a canonical space that is static and includes all objects. Besides the Eulerian perspective, we also describe the dynamic scene from the Lagrangian perspective. Accordingly, we construct three modules, as Figure 2 shows, that include an Eulerian module \u03a8E(x, t) which maps a position x at any time t in the world space to the canonical space, a Lagrangian module \u03a8L(xc, t) which tracks the trajectory of a particle corresponding to xc in the canonical space, and a canonical module which encodes the appearance and geometry in the canonical scene. Under the assumption of finite rigid bodies, we exploit the learned motion by the Lagrangian module and design a motion grouping module to discover moving parts. The particles in the same group share a common rigid transformation and should belong to the same part. Next, we will describe these modules and loss functions in detail."
        },
        {
            "heading": "4.1 CANONICAL MODULE",
            "text": "Same as NeRF, the canonical module is formulated as an implicit function F\u03b8(xc,d) \u2192 (c, \u03c3) which encodes the geometry and appearance in a canonical space. To accelerate convergence, a hybrid representation of feature volume and neural network is adopted. The queried canonical coordinate xc is first used to interpolate the corresponding features within a 3D feature volume Vc \u2208 RNx\u00d7Ny\u00d7Nz\u00d7C , where the Nx \u00d7Ny \u00d7Nz denotes the spatial resolution and C is the feature dimension. To alleviate the local gradient artifact of grid representation, we adopt multi-distance interpolation and concatenate the features in different resolutions as (Fang et al. (2022)):\nfc = Tri-Interp(xc,V)\u2295 ...\u2295 Tri-Interp(xc,V[:: sM ]). (3)\nAfter positional encoding, the queried feature with d is fed into MLPs to predict \u03c3 and c."
        },
        {
            "heading": "4.2 EULERIAN MODULE",
            "text": "The Eulerian module \u03a8E(x, t) records which particle xc in the canonical space goes through a specific location x at the query time t. Assuming that the scene is piece-wise rigid, we formulate this mapping as a rigid transformation in SE(3) similar to Park et al. (2021a), which ensures that all the points on the same rigid body can be transformed using the same set of parameters. Specifically, our Eulerian module contains three components. First, the 3D feature volume VE stores the information about the particles that pass through each position during the entire observation period. Second, a motion extractor EE decodes the motion feature from the interpolated feature in VE at query time t. Third, different from Park et al. (2021a) that uses a screw axis as an intermediate representation, our rigid transformation decoder DE directly maps the motion feature to rotation and translation parameters. The overall process can be formulated as:\n(RE , tE) = DE(fEm), where fEm = EE (Tri-Interp (x,VE) , t) (4)\nHere we employ the continuous 6D intermediate representation (Zhou et al. (2019)) for 3D rotation RE . The Eulerian mapping from the world space at each temporal frame to the canonical space can be calculated by:\nxc = RE(x\u2212 tE) (5)\n4.3 LAGRANGIAN MODULE\nAs the inverse of the Eulerian module, the Lagrangian module \u03a8L(xc, t) tracks the trajectories of specific object particles over time. We use the same manner to construct VL, EL and DL. Different from the Eulerian perspective, the trajectories of each particle in the Lagrangian perspective can be an important cue for rigid part discovery. All particles belonging to the same rigid part share the same rigid body transformation, which means that their motion can be represented by a single feature vector. So we add an additional motion grouping network G (see Figure 3) after VL to restrict that particle trajectories are only subject to a finite number of rigid motion patterns.\nSimilar to (Xu et al. (2022)), we use the attention module with the straight-through estimator trick to achieve the hard grouping of La-\ngrangian features. To encourage the spatial coherence of points in the same group, the coordinate of each point xic is concatenated to the corresponding Lagrangian feature f i L. Specifically, we first compute the similarity map A between the feature {f iLx = f iL \u2295 xic} and learnable slots {Sl} by Gumbel-softmax:\nAil = exp(Wqf iLx \u00b7WkSl + \u03b3i)\u2211L 1 exp(Wqf i Lx \u00b7WkSl + \u03b3i) , (6)\nwhere Wq and Wk are linear mappings and \u03b3 is a sample drawn from Gumbel(0, 1). Then the straight-through estimator trick is used to convert the soft similarity map to one-hot formulation:\nA\u0302 = one-hot(Aargmax) +A\u2212 detach(A) (7) where the detach operation cuts off the corresponding gradient. Despite the hard conversion, Equation 7 can still keep the gradient the same as A. The hard similarity matrix A\u0302 distributes all the Lagrangian features into several groups, where each group represents the particles with the same motion pattern. Instead of directly assigning the learned slot as the updated Lagrangian feature, we calculate the average of all the Lagrangian features in the same group to update the original Lagrangian features. In this way, each updated Lagrangian feature will be directly related to the Lagrangian grid VL, allowing for more efficient optimization. This procedure can be formulated as:\nf\u0302 iL = \u2211I 1 A\u0302il \u00b7 f iL\u2211I\n1 A\u0302il (8)\nThen the updated Lagrangian features f\u0302 iL with query time t is fed into EL and DL sequentially to decode the motion feature fLm and the rigid transformation RL, tL. As mentioned in Section 4.4, to efficiently implement the cycle consistency between the Eulerian and Lagrangian modules, we expect RL = RE and tL = tE . So the Lagrangian mapping from the canonical space to the world space at each temporal frame is calculated by:\nx = R\u22121L (xc + tL) (9)"
        },
        {
            "heading": "4.4 LOSS FUNCTIONS",
            "text": "As our main optimization goal, we adopt the Mean Squared Error (MSE) between the rendered pixel color and the ground truth pixel color as our reconstruction loss:\nLphoto = 1 |R| \u2211 r\u2208R \u2225C\u0302(r)\u2212C(r)\u222522. (10)\nWe also use a total variation loss Ltv to smooth the motion volumes and encourage motion similarity of spatial neighbors. Following Sun et al. (2022), the per-point color loss Lper pt and background entropy loss Lentropy are used to directly supervise the sampled point color and encourage the background probability to concentrate around 0 or 1.\nIn addition, a cyclic consistency loss is designed to encourage the reciprocity of the Lagrangian module and the Eulerian module. Instead of measuring the displacement of the transformations between these two views like Liu et al. (2022), we found that accounting for the difference between low-level motion features fLm and fEm leads to more robust optimization and better part discovery. Our cycle loss is defined as:\nLcycle = 1 |Pobj | \u2211\nx\u2208Pobj\n\u2225fxLm \u2212 fxEm\u222522. (11)\nPlease refer to Appendix A.1 for a more detailed discussion of these two implementations of the cyclic consistency loss. Since the deformation of free space does not satisfy the assumption of finite rigid motions, we filter out free space according to density value and only calculate Lcycle at sampled points on objects {x \u2208 Pobj |\u03c3x > \u03f5}. In our experiments, \u03f5 = 10\u22124. The overall loss function is:\nL = Lphoto + wcycleLcycle + wper ptLper pt + wentropyLentropy + wtvLtv. (12)\n4.5 GROUP MERGING MODULE\nIt is not reasonable to use the same number of groups for a variety of scenarios. We generally set a large number of groups as an upper bound on the number of rigid bodies in the scene, which may cause over-segmented results (Figure 5). This is because we provide an excessive number of groups, and also the same rigid transformations could be easily represented by very different high-level motion features. To address this problem, we design an efficient heuristic algorithm for group merging based on motion differences. This algorithm is used as post-processing after training only and does not affect the parameters of the model. We summa-\nrize this group merging algorithm in Figure 4. 1) We sample points uniformly in canonical space and filter the free space points with density lower than the threshold \u03f5. 2) These remained points are fed into the Lagrangian module to get the updated feature f\u0302 iL, which is the high-level representation of each motion group. 3) We evaluate the rigid transformation similarity between each pair of groups: The rigid transformation sequences are generated by decoding the updated slots into rotation and translation with uniformly sampled times between 0 and 1. 4) We use the Absolute Pose Error (APE) to measure the difference between each sequence pair:\nAPEi,j = \u2211 t \u2225(Pti)\u22121Ptj \u2212 I4\u00d74\u2225, (13)\nwhere Pti is the transformation matrix of group i at time t. 5) We recursively find the two groups with the smallest APE at the current step and record their merge APE cost until all the groups are merged into a single one. In the early stages, the groups with similar motion patterns are merged, which keeps the merging cost growth slow. Once groups representing different motions are merged, the cost will jump, indicating that the merging process should terminate. In practice, we simply find the termination step with the largest cost increase to the subsequent step as our final result."
        },
        {
            "heading": "5 EXPERIMENTS AND RESULTS",
            "text": "Our method not only enables high-quality dynamic scene reconstruction but also allows for the discovery of reasonable rigid parts. In this section, we first evaluate the reconstruction and part discovery performance of our method on the D-NeRF 360\u25e6 synthetic dataset. Then, we construct a synthetic dataset with ground-truth motion masks to quantitatively evaluate our motion grouping results. Finally, we provide direct applications for structural scene modeling and editing."
        },
        {
            "heading": "5.1 IMPLEMENTATION",
            "text": "We use 50\u00d7 50\u00d7 50 voxels for the Eulerian and Lagrangian volume and a 160\u00d7 160\u00d7 160 voxel for the canonical volume. Following Fang et al. (2022), we employ the progressive upsample the resolution for acceleration. We use two separated linear layers to predict the 6D rotation and 3D translation with biases as (1, 0, 0, 0, 1, 0) and (0, 0, 0), respectively, so that the initial deformation is an identity. We use the Adam optimizer for a total of 20k iterations, by sampling 4096 rays from a randomly sampled image in each iteration. All the experiments were conducted on a single NVIDIA RTX3090 GPU. More details can be found in the appendix."
        },
        {
            "heading": "5.2 EVALUATION ON D-NERF DATASET",
            "text": "We adopt the 360\u25e6 Synthetic dataset provided by D-NeRF (Pumarola et al. (2020)) to evaluate our method quantitatively and qualitatively. The dataset contains eight synthetic dynamic scenes with different motion patterns, and only one view is captured at each time step. We compare our method with the state-of-the-art dynamic NeRF methods: Non-deformation-based methods T-NeRF (Pumarola et al. (2020)), K-Planes (Fridovich-Keil et al. (2023)), HexPlane (Cao & Johnson (2023)),\nEulerian-based method D-NeRF (Pumarola et al. (2020)), TiNeuVox (Fang et al. (2022)), NDVG (Guo et al. (2022)) and a Lagrangian-view method WIM (Noguchi et al. (2022)). For TiNeuVox, we use their base version with a canonical grid in 1603 resolution and hidden layers of 256 channels. Following these previous works, we train each scene with images at 400 \u00d7 400 resolution and use two metrics for evaluation: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) (Zhou et al. (2004)) and Learned Perceptual Image Patch Similarity (LPIPS) (Richard et al. (2018)).\nAs shown in Table 1, while keeping the training time within 30 minutes on one GPU, our method not only achieves high rendering quality but also supports part discovery. Compared to the previous methods, we achieved the best in SSIM and LPIPS, and second best in PSNR. Compared to TiNeuVox, our method has a slight PSNR drop. The main reason is that TiNeuVox employs a temporal enhancement module in the canonical space to improve quality, which also leads to a time-varying canonical space. After removing this enhancement module in TiNeuVox, its average PSNR drops to 31.47. In our paper, to achieve better disentanglement of geometry and motion, we expect the geometric evolution only comes from the scene motion. Therefore we did not adopt a similar enhancement strategy to form a time-invariant canonical space. For WIM, due to the nonexistence of canonical space, the significant motion ambiguity under the single view setting causes the failure.\nWe show our visualization results in Figure 5. It can be seen that our method enables high-quality appearance and geometry reconstruction. We also assign each query point the corresponding group color and render it to 2D images. As discussed in Section 4.5, over-segmentation occurs because similar motion could be represented by different high-level features (see the third row in Figure 5). Through our group merging algorithm, we only retain the highly distinguishable motion modes and obtain concise part segmentation. Thanks to the motion-based grouping mechanism, our method is capable of overlooking motion-irrelevant characteristics in geometry and appearance and producing clean part discovery results on these realistic complex scenes."
        },
        {
            "heading": "5.3 MOTION GROUPING EVALUATION",
            "text": "In this section, we provide a quantitative evaluation of our motion grouping results. We created a synthetic dataset with ground truth image-segmentation pairs using Kubric toolkit (Greff et al. (2022)). Each created scene contains 1 to 5 realistic real-world objects from the GSO dataset (Downs et al. (2022)) with different initial velocities and motion directions. We followed the same sampling and rendering process as D-NeRF (Pumarola et al. (2020)) to generate a 120-frame monocular image sequence with 256\u00d7256 resolution for each scene. To begin our evaluation, we first establish the correlation pairs between the ground truth label and our predicted groups. For each group, we assign the ground truth label with the highest number of pixels corresponding to it in the first 10 frames. More details are included in the appendix. We calculate the mean Intersection over Union (IOU) for the assigned label mask with its corresponding ground truth mask over the entire image sequence. It is noted that achieving a high mIOU score over the entire sequence requires more than just the ability to accurately distinguish each individual part. It also necessitates the capacity to consistently track each part throughout the sequence.\nWe present 10 examples in Figure 6, showcasing both quantitative mIOU and qualitative visualization and comparisons. Despite the variation in the scene configurations, our method achieves an mIOU score of over 85% on most scenes, clearly demonstrating its robustness over the dataset. Moreover, the high mIOU score indicates that our method can generate accurate part segmentation results and continuously track specific parts throughout the sequences, see the learned trajectories in Figure 6, ensuring both temporal and multi-view consistency of the discovered parts. Furthermore, our method is capable of dealing with complex geometry and topology. Holes (cable in example 5) and geometry details are nicely revealed by our method. By utilizing our motion-based grouping approach, our method can accurately segment objects even if they are spatially separated\u2013 see the gloves (example 1) and 3-car (example 7) in Figure 6."
        },
        {
            "heading": "5.4 APPLICATION: STRUCTURED SCENE MODELING BY ROBOTIC MANIPULATION",
            "text": "Observation and interaction are crucial for human beings to learn from the real world. In this section, we show that our method can identify objects and understand the functionality of their parts by observing physical interaction procedures. To demonstrate this, we capture a set of robotic manipulation sequences with a similar monocular camera setting as (Pumarola et al. (2020)). As shown in Figure 7 above, by observing the robot\u2019s work process, like picking up a toy or inserting a peg, our method can accurately identify the manipulated object, as well as the links and joints of the robot. Note that since the robotic arms\u2019 trajectories are different in the two sequences, the joints discovered by motion are also different. The discovered 3D parts with their Lagrangian motion could provide a strong prior for downstream functionality reasoning and robotic reinforcement learning tasks."
        },
        {
            "heading": "5.5 APPLICATION: SCENE EDITING",
            "text": "In addition to scene understanding, with the learned structural representation of dynamic scenes, our method can also edit scenes and generate new renderings from the scene. Figure 7 below presents a few scene-editing applications supported by our approach in HyperNeRF realworld sequence (Park et al. (2021b)). Since our method conducts grouping in the 3D canonical space, the consistency can be maintained not only across multiple views but also across time steps. We show the removal or modification of specific objects in these two real scenes and\ndemonstrate the scalability of our method to real-world applications."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we present MovingParts, a novel method for 3D dynamic scene reconstruction and part discovery. Inspired by fluid simulation, we observe the motion in the scene from both the Eulerian view and the Lagrangian view. In the particle-based Lagrangian view, we constrain the motion pattern of the particles to be a few rigid transformations, so that we successfully perform part discovery. To ensure fast convergence during training, we utilize a hybrid feature volume and neural network representation, for both views which are efficiently supervised by a cycle-consistency loss. What is more, the learned part representation could directly be applied to downstream tasks, e.g., object tracking, structured scene modeling, editing, etc.\nLimitations. Motion modeling at a specific location can be considered as a sequence decoding task. In this paper, we explicitly store the motion features in low-dimensional vectors, which makes it challenging to model motion on very long sequences. Although we can circumvent the issue by manually splitting long sequences into shorter ones, a unified long sequence encoding-decoding scheme will still be a more elegant and efficient solution. We defer the exploration of this challenging setting to future work."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Thanks for the real-scenario robotic data built by Litian Liang (UCSD) and Liuyu Bian (THU). This work was supported by the National Natural Science Foundation of China (NSFC) under Grants 62076230 and the Fundamental Research Funds for the Central Universities under Grant WK3490000008."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 ABLATION STUDY In this section, we conduct ablation experiments on the Lagrangian module and the loss functions to showcase their effectiveness. These ablations were conducted on the D-NeRF synthetic dataset and we report their averaged metric values (PSNR, SSIM and LPIPS) with their corresponding model settings (A1 \u2013 7) in Table 2. Moreover, we show the rendered results of these experiments on the Bouncing Balls scene in Fig. 9 to illustrate the influence of individual modules on rendering and part discovery. We present our full model in Experiment A1.\nLagrangian module. Our Lagrangian module is mainly designed for automatic part discovery, which is the main focus of the this work. Without this module, the model cannot achieve part discovery at all while dynamic reconstruction and novel view rendering can still be done. We evaluate how the Lagrangian module is affecting the final rendering with the ablated model setting A2, where we set the weight of the cycle consistency loss to 0, which essentially disables the Lagrangian module. As shown in Table 2, the absence of rigid part motion constraints in the Lagrangian module leads to only a slightly higher PSNR (less than 0.2db difference). In general, our Lagrangian module enables automatic part discovery while retaining high rendering quality.\nCycle Consistency loss. To validate the effectiveness of the motion feature-based cycle consistency loss, we compare it with the displacement-based cycle loss in A3. We enforce that the displacement modeled by the Eulerian and Lagrangian modules remains consistent, akin to the Deformation Cycle Consistency in Liu et al. (2022):\nLcycle-disp = 1 |Pobj | \u2211\nx\u2208Pobj\n\u2225x\u2212R\u22121L (RE(x\u2212 tE) + tL)\u2225 2 2. (14)\nAs demonstrated in Table 2, there is a decline in PSNR for displacement-based cycle consistency (A3) when compared to the feature-level consistency used in our paper (A1). More importantly, the discovery of parts is contingent upon grouping Lagrangian motion features. The inherent ambiguity\nbetween motion features and displacement, where multiple motion features may correspond to the same displacement, poses a challenge in attaining precise part discovery results. As a result, the A3 model often fails to distinguish different parts in the scene (see A3 in Figure 9).\nTotal variation loss. The total variation loss imposes constraints on the feature similarity among neighboring grids in the motion volume, effectively ensuring that adjacent particles in space exhibit similar motion patterns. Experiment A4 demonstrates the critical role of this regularization in motion modeling. In its absence, there is a notable decline in image rendering quality. Furthermore, due to the unrestrained movement of near-neighbor particles, the discovered parts lack the characteristic localized nature in 3D space, as illustrated in A4 of Figure 9.\nAdditional losses. The removal of either the cross-entropy loss (A5) or the per-point RGB loss (A6) lead to a decline in rendering quality. Notably, the cross-entropy loss plays a crucial role in regulating foreground-background probability, and its omission may lead to ghosting in the background, as evident in A6 of Figure 9. It is noteworthy that even in the absence of these additional regularization losses, our method can still produce reasonable part discovery results.\nA.2 MORE RECONSTRUCTION AND GROUP RESULTS\nIn this subsection, we report the per-scene results of the D-NeRF synthetic dataset in Table 3. We also show more dynamic scene reconstruction and part discovery results in Figure 8. It can be seen that our method can achieve high-quality dynamic reconstruction with different motion patterns and also shows the ability that reasonably segment the moving regions and obtain meaningful parts, like the legs and arms. Note that our approach does not explicitly introduce any category and geometric priors. Therefore, it is not necessary to separate parts that are perceptually/semantically distinguishable but have no relative motion with other parts in the video capture, like the head and body.\nA.3 VISUALIZATION OF GROUP MERGING\nDue to the excessive number of groups and nonlinear neural networks, the same rigid transformation trajectory could be easily represented by different high-level motion features. Therefore, we introduce a group merging module to reduce the number of the group to a plausible level. In this subsection, we visualize two examples of all steps of group merging, as shown in Figure 10 At each merge step, the pair of groups with the current most similar motion patterns are aggregated; the entire process actually builds a binary tree of the original groups. The whole process stops when all groups are combined into a single one, i.e. the step with a single color. Finally, the most reasonable step is selected by\nthe merging threshold discussed in our paper, shown as the rectangles in the figure. Note that the same color in different steps does not denote the same group.\nA.4 MOTION GROUPING EVALUATION DETAILS. We utilized Kubric (Greff et al. (2022)), a data generation pipeline for multi-object videos with annotation, to create our evaluation dataset. The MOVi-C pipeline from Kubric was employed to generate scenes, utilizing realistic, textured objects from the GSO dataset (Downs et al. (2022)). To create a diverse set of scenes, a random HDRI was used to generate the background. The number of objects in each scene varied randomly between one to five, and object shadows were not considered due to their highly non-rigid nature. Objects were placed randomly in space with a randomized initial velocity. To capture each scene, the camera was positioned above the objects with a camera movement setting similar to Pumarola et al. (2020). Finally, each scene was simulated and rendered into a 120-frame image sequence, with ground truth instance masks. To evaluate the mean Intersection over Union (mIOU) between our grouping result and the ground truth mask, it is necessary to establish correspondence between them. To achieve this, we utilized the first 10 frames in the sequence to determine the correspondence, which then was extended to the whole sequence. For each group, we tallied the number of pixels belonging to each label in the first ten frames and identified the ground truth label with the highest number of pixels to associate it with the group. This operation enabled us to map each group to a label and convert the group map into a label map. Subsequently, we calculated the per-frame IOU between the converted label map and the ground truth mask. To obtain the final mIOU, we averaged the IOUs of all labels over the entire image sequence.\nA.5 TRAINING DETAILS AND HYPER-PARAMETER SETTINGS We use 50\u00d750\u00d750 voxels to construct the Eulerian volume VE and the Lagrangian volume VL with a feature dimension of 20. The canonical volume Vc is constructed with a 160\u00d7 160\u00d7 160 voxel. Following Fang et al. (2022), the feature dimension of Vc is set as 6. To alleviate the optimization difficulty and speed up training, we set the initial resolution of the canonical volume to 403 and upsample it at the 4k, 6k, and 8k iterations. For the MLPs in our framework, we set the channel number of all hidden layers to 128. We use two-layer MLPs for the motion extractors EE and EL. The parameters of the rigid transformation decoders, DE and DL, are shared. This sharing ensures that the decoded motion parameters exhibit consistency when the motion features are consistent. Two separated linear layers are used for the decoders to predict the 6D rotation and 3D translation with biases as (1, 0, 0, 0, 1, 0) and (0, 0, 0), respectively so that the initial deformation is an identity. For motion grouping, we set the slot number to 12 and initialize the slots from a standard normal distribution. We use the Adam optimizer for a total of 20k iterations, by sampling 4096 rays from a randomly sampled image in each iteration. To reduce the learning difficulty, we add images into the training set progressively at the early training stage. We set the learning rate as 0.08 for the Eulerian and Lagrangian volumes, 0.01 for the canonical volume, 6\u00d7 10\u22124 for E and D, 8\u00d7 10\u22124 for other networks. For the overall loss function, we described it as:\nL = Lphoto + wcycleLcycle + wper ptLper pt + wentropyLentropy + wtvLtv. (15)\nWe use Lper pt and Lentropy to supervise the color of sampled points and regularize the background probability, respectively. We set wper pt and wentropy to 0.01 and 0.001. To encourage reciprocity of motion between the Eulerian and Lagrangian views, we use the cycle consistency loss Lcycle with a weight parameter wcycle of 0.1. Additionally, we apply the total variation loss to smooth the features of the motion volumes VE and VL:\nLtv = 1\nN\n\u2211 ( \u221a \u25b32VL + wE \u221a \u25b32VE) (16)\nWhere N denotes the number of parameters of each motion volume, \u25b32 represents the square difference between neighboring values in the motion volume, wE denotes the additional weight between VE and VL. We set the wtv = 0.01 and wE = 1 for the D-NeRF synthetic dataset. For motion grouping evaluation, we decrease these two weights due to the more complex object geometry and motion patterns, we set the wtv = 0.001 and wE = 0.1.\nA.6 MORE RESULTS OF APPLICATIONS ON SYNTHETIC DATA.\nExcept for the real data application, we also conducted structural scene modeling and editing on robotic synthetic data. We utilized a subset of the robotic manipulation environments in ManiSkill (Mu et al. (2021)) to simulate robot-object interactions with a similar monocular setting as (Pumarola et al. (2020)). As shown in Figure 11, We conducted experiments using two different setups: articulated object manipulation and rigid-body manipulation. In the first setup, the robot performs operations on a particular movable part of an articulated object, as shown in Figure 11 above. In the second setup, the robot is tasked with grasping a specific object and moving it to a target position, as illustrated in Figure 11 below. In both setups, through observation, our method can accurately identify the manipulated object or object part, such as the drawer (orange) and the cabinet door (yellow), as well as the links and joints of the robot. What is more, benefit from the learned structural scene representation, we could apply direct high-level scene editing. In Figure 11, we show the editing operations like duplication, removal, scaling of a specific object in the scene, and the control of an articulated robot arm. Note that these operations are all performed in 3D space and rendered into 2D images for visualization.\nA.7 IMPLEMENTATION DETAILS OF SCENE EDITING\nBased on our part-level representation, we could directly edit the 3D dynamic scenes. In this paper, we present four scene-editing applications: Removal, Duplication, Rescaling, and Re-posing. Here, we will provide more details about how to implement these applications.\nAs shown in Figure 12, these application implementations could be divided into 2 ways. For Removal, after ray marching, the sampled points are fed into the Eulerian module to be transformed into canonical space. Then these canonical coordinates are grouped by our motion grouping network. To delete\nthe specified group, we simply assign the density of the points in this group to zero to remove it from the canonical space. After rendering, the specific group will be deleted from the image.\nDuplication, rescaling, and re-posing could be implemented in the same way. We do this by processing the group to be edited and the background (all the other groups) respectively. Specifically, the sampled points by ray marching are fed into the background branch and the edit branch. In the background branch, we leave the background group fixed and delete the group to be edited, which is the same as the Removal application. In the edit branch, we perform transformations for the target group, such as rotation, translation, scaling, etc. We achieve this by first performing a manually specified transformation on the sampled points. Then these transformed points are fed into the Eulerian module and the motion grouping network obtains the canonical coordinates and corresponding group index. After that, we set the density of the points in the background groups to 0 so that the transformation only acts on the target group. Finally, we integrate the corresponding points of the two branches to render them into the 2D image, the integration formula is as follows:\n\u03c3 = \u03c3bg + \u03c3edit\nc = 1\n\u03c3 (\u03c3bg \u00b7 cbg + \u03c3edit \u00b7 cedit)\n(17)\nNote that by introducing additional edit branches, we could make different edits to different groups simultaneously."
        }
    ],
    "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
    "year": 2024
}