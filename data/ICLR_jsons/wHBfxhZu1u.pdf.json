{
    "abstractText": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bowen Peng"
        },
        {
            "affiliations": [],
            "name": "Jeffrey Quesnelle"
        },
        {
            "affiliations": [],
            "name": "Honglu Fan"
        },
        {
            "affiliations": [],
            "name": "Enrico Shippole"
        }
    ],
    "id": "SP:f82b802b5b4662ee0147d401f1c4906cde8bc9f5",
    "references": [
        {
            "authors": [
                "Z. Azerbayev",
                "E. Ayers",
                "B. Piotrowski"
            ],
            "title": "URL https://github.com/ zhangir-azerbayev/proof-pile",
            "venue": "Proof-pile,",
            "year": 2022
        },
        {
            "authors": [
                "S. Black",
                "S. Biderman",
                "E. Hallahan",
                "Q. Anthony",
                "L. Gao",
                "L. Golding",
                "H. He",
                "C. Leahy",
                "K. McDonell",
                "J. Phang",
                "M. Pieler",
                "U.S. Prashanth",
                "S. Purohit",
                "L. Reynolds",
                "J. Tow",
                "B. Wang",
                "S. Weinbach"
            ],
            "title": "GPT-NeoX-20B: An open-source autoregressive language",
            "year": 2022
        },
        {
            "authors": [
                "C. Chen"
            ],
            "title": "Transformer Inference Arithmetic, 2022",
            "venue": "URL https://kipp.ly/blog/ transformer-inference-arithmetic/",
            "year": 2022
        },
        {
            "authors": [
                "S. Chen",
                "S. Wong",
                "L. Chen",
                "Y. Tian"
            ],
            "title": "Extending context window of large language models via positional interpolation",
            "year": 2023
        },
        {
            "authors": [
                "A. Spiridonov",
                "R. Sepassi",
                "D. Dohan",
                "S. Agrawal",
                "M. Omernick",
                "A.M. Dai",
                "T.S. Pillai",
                "M. Pellat",
                "A. Lewkowycz",
                "E. Moreira",
                "R. Child",
                "O. Polozov",
                "K. Lee",
                "Z. Zhou",
                "X. Wang",
                "B. Saeta",
                "M. Diaz",
                "O. Firat",
                "M. Catasta",
                "J. Wei",
                "K. Meier-Hellstern",
                "D. Eck",
                "J. Dean",
                "S. Petrov",
                "N. Fiedel"
            ],
            "title": "PaLM: Scaling language modeling with pathways, 2022",
            "year": 2022
        },
        {
            "authors": [
                "P. Clark",
                "I. Cowhey",
                "O. Etzioni",
                "T. Khot",
                "A. Sabharwal",
                "C. Schoenick",
                "O. Tafjord"
            ],
            "title": "Think you have solved question answering? try ARC, the AI2",
            "venue": "Reasoning Challenge,",
            "year": 2018
        },
        {
            "authors": [
                "T. Computer"
            ],
            "title": "Redpajama: An open source recipe to reproduce llama training dataset, 2023",
            "venue": "URL https://github.com/togethercomputer/RedPajama-Data",
            "year": 2023
        },
        {
            "authors": [
                "T. Dao"
            ],
            "title": "Flashattention-2: Faster attention with better parallelism and work partitioning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "J. Gehring",
                "M. Auli",
                "D. Grangier",
                "D. Yarats",
                "Y.N. Dauphin"
            ],
            "title": "Convolutional sequence to sequence learning, 2017",
            "year": 2017
        },
        {
            "authors": [
                "C. Han",
                "Q. Wang",
                "W. Xiong",
                "Y. Chen",
                "H. Ji",
                "S. Wang"
            ],
            "title": "LM-Infinite: Simple on-the-fly length generalization for large language",
            "year": 2023
        },
        {
            "authors": [
                "D. Hendrycks",
                "C. Burns",
                "S. Basart",
                "A. Zou",
                "M. Mazeika",
                "D. Song",
                "J. Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "L. Huang",
                "S. Cao",
                "N. Parulian",
                "H. Ji",
                "L. Wang"
            ],
            "title": "Efficient attentions for long document summarization",
            "venue": "Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "A. Kazemnejad",
                "I. Padhi",
                "K.N. Ramamurthy",
                "P. Das",
                "S. Reddy"
            ],
            "title": "The impact of positional encoding on length generalization in transformers",
            "year": 2023
        },
        {
            "authors": [
                "S. Lin",
                "J. Hilton",
                "O. Evans"
            ],
            "title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "A. Mohtashami",
                "M. Jaggi"
            ],
            "title": "Landmark attention: Random-access infinite context length for transformers",
            "year": 2023
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. K\u00f6pf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "PyTorch: An imperative style, high-performance deep learning library",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "O. Press",
                "N. Smith",
                "M. Lewis"
            ],
            "title": "Train Short, Test Long: Attention with linear biases enables input length extrapolation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "J. Quesnelle",
                "E. Shippole",
                "\"Kaiokendev"
            ],
            "title": "Llongma: Scaling rotary embeddings through linear positional interpolation",
            "venue": "https://huggingface.co/conceptofmind/ LLongMA-2-7b/,",
            "year": 2023
        },
        {
            "authors": [
                "J.W. Rae",
                "A. Potapenko",
                "S.M. Jayakumar",
                "C. Hillier",
                "T.P. Lillicrap"
            ],
            "title": "Compressive transformers for long-range sequence modelling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "A. Roberts",
                "C. Raffel",
                "K. Lee",
                "M. Matena",
                "N. Shazeer",
                "P.J. Liu",
                "S. Narang",
                "W. Li",
                "Y. Zhou"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Technical report,",
            "year": 2019
        },
        {
            "authors": [
                "B. Rozi\u00e8re",
                "J. Gehring",
                "F. Gloeckle",
                "S. Sootla",
                "I. Gat",
                "X.E. Tan",
                "Y. Adi",
                "J. Liu",
                "T. Remez",
                "J. Rapin",
                "A. Kozhevnikov",
                "I. Evtimov",
                "J. Bitton",
                "M. Bhatt",
                "C.C. Ferrer",
                "A. Grattafiori",
                "W. Xiong",
                "A. D\u00e9fossez",
                "J. Copet",
                "F. Azhar",
                "H. Touvron",
                "L. Martin",
                "N. Usunier",
                "T. Scialom",
                "G. Synnaeve"
            ],
            "title": "Code Llama: Open foundation models for code, 2023",
            "year": 2023
        },
        {
            "authors": [
                "P. Shaw",
                "J. Uszkoreit",
                "A. Vaswani"
            ],
            "title": "Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
            "year": 2018
        },
        {
            "authors": [
                "J. Su"
            ],
            "title": "Rectified rotary position embeddings",
            "venue": "https://github.com/bojone/rerope,",
            "year": 2023
        },
        {
            "authors": [
                "J. Su",
                "Y. Lu",
                "S. Pan",
                "A. Murtadha",
                "B. Wen",
                "Y. Liu"
            ],
            "title": "RoFormer: Enhanced transformer with rotary position",
            "year": 2022
        },
        {
            "authors": [
                "M. Tancik",
                "P.P. Srinivasan",
                "B. Mildenhall",
                "S. Fridovich-Keil",
                "N. Raghavan",
                "U. Singhal",
                "R. Ramamoorthi",
                "J.T. Barron",
                "R. Ng"
            ],
            "title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "H. Touvron",
                "T. Lavril",
                "G. Izacard",
                "X. Martinet",
                "M.-A. Lachaux",
                "T. Lacroix",
                "B. Rozi\u00e8re",
                "N. Goyal",
                "E. Hambro",
                "F. Azhar",
                "A. Rodriguez",
                "A. Joulin",
                "E. Grave",
                "G. Lample"
            ],
            "title": "LLaMA: Open and efficient foundation language models, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "I. Molybog",
                "Y. Nie",
                "A. Poulton",
                "J. Reizenstein",
                "R. Rungta",
                "K. Saladi",
                "A. Schelten",
                "R. Silva",
                "E.M. Smith",
                "R. Subramanian",
                "X.E. Tan",
                "B. Tang",
                "R. Taylor",
                "A. Williams",
                "J.X. Kuan",
                "P. Xu",
                "Z. Yan",
                "I. Zarov",
                "Y. Zhang",
                "A. Fan",
                "M. Kambadur",
                "S. Narang",
                "A. Rodriguez",
                "R. Stojnic",
                "S. Edunov",
                "T. Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "W. Xiong",
                "J. Liu",
                "I. Molybog",
                "H. Zhang",
                "P. Bhargava",
                "R. Hou",
                "L. Martin",
                "R. Rungta",
                "K.A. Sankararaman",
                "B. Oguz",
                "M. Khabsa",
                "H. Fang",
                "Y. Mehdad",
                "S. Narang",
                "K. Malik",
                "A. Fan",
                "S. Bhosale",
                "S. Edunov",
                "M. Lewis",
                "S. Wang",
                "H. Ma"
            ],
            "title": "Effective long-context scaling of foundation models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "R. Zellers",
                "A. Holtzman",
                "Y. Bisk",
                "A. Farhadi",
                "Y. Choi"
            ],
            "title": "HellaSwag: Can a machine really finish your sentence",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [],
            "title": "L is a new context window beyond the pre-trained limit. With the original pre-trained model plus the modified RoPE formula, they fine-tuned the language model further on several orders of magnitude fewer tokens (a few billion in Chen et al. (2023)) and successfully acheived context window extension",
            "year": 2023
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "TRAINING EFFICIENCY OF YARN Table 6 shows a side-by-side comparison of the Llama 2 7B model extended from 4096 to 8192 context length via PI (LLongMA-2 7B6) and YaRN. Note that the PI model was trained using the methodology",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformer-based Large Language Models(Vaswani et al., 2017) (LLMs) have become the nearubiquitous choice for many natural language processing (NLP) tasks where long-range abilities such as in-context learning (ICL) has been crucial. In performing the NLP tasks, the maximal length of the sequences (the context window) determined by its training processes has been one of the major limits of a pretrained LLM. Being able to dynamically extend the context window via a small amount of fine-tuning (or without fine-tuning) has become more and more desirable. To this end, the position encodings of transformers are the center of the discussions.\nThe original Transformer architecture used an absolute sinusoidal position encoding, which was later improved to a learnable absolute position encoding (Gehring et al., 2017). Since then, relative positional encoding schemes (Shaw et al., 2018) have further increased the performance of Transformers. Currently, the most popular relative positional encodings are T5 Relative Bias (Roberts et al., 2019), RoPE (Su et al., 2022), XPos (Sun et al., 2022), and ALiBi (Press et al., 2022).\nOne reoccurring limitation with positional encodings is the inability to generalize past the context window seen during training. While some methods such as ALiBi are able to do limited generalization, none are able to generalize to sequences significantly longer than their pre-trained length (Kazemnejad et al., 2023).\nSome works have been done to overcome such limitation. (Chen et al., 2023) and concurrently (kaiokendev, 2023) proposed to extend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning on a small amount of data. As an alternative, (bloc97, 2023a) proposed the \"NTK-aware\" interpolation by taking the loss of high frequency into account. Since then, two improvements of the \"NTK-aware\" interpolation have been proposed, with different emphasis:\n\u2022 the \"Dynamic NTK\" interpolation method (emozilla, 2023) for pre-trained models without fine-tuning.\n\u2022 the \"NTK-by-parts\" interpolation method (bloc97, 2023b) which performs the best when fine-tuned on a small amount of longer-context data.\nThe \"NTK-aware\" interpolation and the \"Dynamic NTK\" interpolation have already seen their presence in the open-source models such as Code Llama (Rozi\u00e8re et al., 2023) (using \"NTK-aware\" interpolation) and Qwen 7B (qwe) (using \"Dynamic NTK\").\nIn this paper, in addition to making a complete account of the previous unpublished works on the \"NTK-aware\", the \"Dynamic NTK\" and the \"NTK-by-parts\" interpolations, we present YaRN (Yet another RoPE extensioN method), an improved method to efficiently extend the context window of models trained with Rotary Position Embeddings (RoPE) including the LLaMA (Touvron et al., 2023a), the GPT-NeoX (Black et al., 2022), and the PaLM (Chowdhery et al., 2022) families of models.\nThe relationship between different methods and how they evolve into YaRN can be summarized into the following diagram:\nYaRN reaches state-of-the-art performances in context window extensions after fine-tuning on less than \u223c0.1% of the original pre-training data. In the meantime, by combining with the inference-time technique called Dynamic Scaling, the Dynamic-YaRN allows for more than 2x context window extension without any fine-tuning."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 ROTARY POSITION EMBEDDINGS",
            "text": "The basis of our work is the Rotary Position Embedding (RoPE) introduced in (Su et al., 2022). We work on a hidden layer where the set of hidden neurons are denoted by D. Given a sequence of vectors x1, \u00b7 \u00b7 \u00b7 ,xL \u2208 R|D|, following the notation of (Su et al., 2022), the attention layer first converts the vectors into the query vectors and the key vectors:\nqm = fq(xm,m) \u2208 R|D|, kn = fk(xn, n) \u2208 R|D|. (1) Next, the attention weights are calculated as\nsoftmax( qTmkn\u221a\n|D| ), (2)\nwhere qm,kn are considered as column vectors so that qTmkn is simply the Euclidean inner product. In RoPE, we first assume that |D| is even and identify the embedding space and the hidden states as complex vector spaces:\nR|D| \u223c= C|D|/2\nwhere the inner product qTk becomes the real part of the standard Hermitian inner product Re(q\u2217k). More specifically, the isomorphisms interleave the real part and the complex part(\n(xm)1, \u00b7 \u00b7 \u00b7 , (xm)|D| ) 7\u2192 ( (xm)1 + i(xm)2, \u00b7 \u00b7 \u00b7 , ((xm)|D|\u22121 + i(xm)|D|) ) , (3)(\n(qm)1, \u00b7 \u00b7 \u00b7 , (qm)|D| ) 7\u2192 ( (qm)1 + i(qm)2, \u00b7 \u00b7 \u00b7 , ((qm)|D|\u22121 + i(qm)|D|) ) . (4)\nTo convert embeddings xm,xn into query and key vectors, we are first given R-linear operators\nWq,Wk : R|D| \u2192 R|D|. Let \u03b8 = diag(\u03b81, \u00b7 \u00b7 \u00b7 , \u03b8|D|/2). In complex coordinates, we define\nfW (xm,m,\u03b8) = e im\u03b8Wxm, (5)\nfor any linear operator W . The functions fq, fk in RoPE are given by\nfq = fWq , fk = fWk . (6)\nwhere \u03b8d = b\u22122d/|D| and b = 10000. This way, RoPE associates each (complex-valued) hidden neuron with a separate frequency \u03b8d. The benefit of doing so is that the dot product between the query vector and the key vector only depends on the relative distance m\u2212 n. In later discussions, a context length interpolation usually aims to modify the equation Eq. 5. To set up a uniform convention for these discussions, note that a modification f \u2032W can take the following form:\nf \u2032W (xm,m,\u03b8) = fW (xm, g(m),h(\u03b8)), (7)\nwhere g(m) is a map between real numbers and h(\u03b8) acts on the entries of the diagonal matrix \u03b8 uniformly by diag(h(\u03b81), \u00b7 \u00b7 \u00b7 , h(\u03b8|D|/2)) according to a function h. g and h are method-dependent functions.\nIn the subsequent sections, when we introduce a new interpolation method of the form Eq. 7, we only specify the functions g(m) and h(\u03b8d)."
        },
        {
            "heading": "2.2 ADDITIONAL NOTATIONS",
            "text": "Given the pretrained maximal context length L, our goal is to extend it to L\u2032 > L either with or without finetuning. We introduce the notion of scale factor s defined by s = L \u2032\nL .\nFor the convenience of some discussions, we also introduce wavelength \u03bbd associated with the d-th hidden dimension of RoPE as follows:\n\u03bbd = 2\u03c0\n\u03b8d = 2\u03c0b\n2d |D| . (8)\nThe wavelength describes the length of tokens needed in order for the rotary position embedding at dimension d to perform a full rotation (2\u03c0)."
        },
        {
            "heading": "2.3 RELATED WORK",
            "text": "Position Interpolation (PI) is one of the earlier works extending context lengths of RoPE proposed by Chen et al. (2023), and concurrently kaiokendev (2023). Under the notation of Eq. 7, it is setting\ng(m) = s \u00b7m, h(\u03b8) = \u03b8, (9)\nwhere s is the scale factor L \u2032\nL . We include some details in Appendix A.1.\nReRoPE (Su, 2023) also aims to extend the context size of existing models pre-trained with RoPE, and claims \"infinite\" context length without needing any fine-tuning. This claim is backed by a monotonically decreasing loss with increasing context length up to 16k on the Llama 2 13B model. It achieves context extension by modifying the attention mechanism and thus is not purely an embedding interpolation method. Since it is currently not compatible with Flash Attention 2 (Dao, 2023) and requires two attention passes during inference, we do not consider it for comparison.\nConcurrently with our work, LM-Infinite (Han et al., 2023) proposes similar ideas to YaRN, but focuses on \"on-the-fly\" length generalization for non-fine-tuned models. Since they also modify the attention mechanism of the models, it is not an embedding interpolation method and is not immediately compatible with Flash Attention 2."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Whereas PI stretches all RoPE dimensions equally, we find that the theoretical interpolation bound described by PI (Chen et al., 2023) is insufficient at predicting the complex dynamics between RoPE and the LLM\u2019s internal embeddings. In the following subsections, we describe the main issues with PI we have individually identified and solved, so as to give the readers the context, origin and justifications of each method which we use in concert to obtain the full YaRN method."
        },
        {
            "heading": "3.1 LOSS OF HIGH FREQUENCY INFORMATION - \"NTK-AWARE\" INTERPOLATION",
            "text": "If we look at rotary position embeddings (RoPE) only from an information encoding perspective, it was shown in (Tancik et al., 2020), using Neural Tangent Kernel (NTK) theory, that deep neural networks have trouble learning high frequency information if the input dimension is low and the corresponding embeddings lack high frequency components. Here we can see the similarities: a token\u2019s positional information is one-dimensional, and RoPE expands it to an n-dimensional complex vector embedding. RoPE closely resembles Fourier Features (Tancik et al., 2020) in many aspects, as it is possible to define RoPE as a special 1D case of a Fourier Feature.\nIn the case of Positional Interpolation (PI), as we strech all dimensions equally by a factor s, it removes the high frequency components of RoPE. This degradation is worsened as the scaling factor s grows, and at some point, the network will not be able to recover. Previous fine-tunes (kaiokendev, 2023) (Chen et al., 2023) (Together.ai, 2023) (Quesnelle et al., 2023) using PI were only able to achieve a scaling factor of roughly s = 8 before the LLM\u2019s outputs starts to degrade, even after fine-tuning.\nIn order to alleviate this issue, the \"NTK-aware\" interpolation was developed in (bloc97, 2023a). Instead of scaling every dimension of RoPE equally by a factor s, we spread out the interpolation pressure across multiple dimensions by scaling high frequencies less and low frequencies more. One can obtain such a transformation in many ways, but the simplest would be to perform a base change on the value of \u03b8. The details are described in the Appendix A.2 and the method has seen some open-source adoptions1.\nOne main issue of this \"NTK-aware\" scaling is that it is very difficult to determine what optimal base should be used for an intended context extension by s times. The best base to use for \"NTK-aware\" interpolation usually has to be found empirically, which significantly increases the difficulty and cost of obtaining a successful fine-tuned model. Despite its limitations, the observations from the NTK theory is valid and the following idea is still maintained and executed in a different way in the \"NTK-by-parts\" interpolation introduced in the next section."
        },
        {
            "heading": "3.2 LOSS OF RELATIVE LOCAL DISTANCES - \"NTK-BY-PARTS\" INTERPOLATION",
            "text": "To understand why \"NTK-aware\" interpolation works better than PI and to eliminate its disadvantages, we have to take a closer look at RoPE. In this section, we think heavily in terms of the wavelengths \u03bbd defined in Eq. 8 in the formula of RoPE. For simplicity, we omit the subscript d in \u03bbd and the reader is encouraged to think about \u03bb as the wavelength of an arbitrary periodic function.\nIn theory, as RoPE is a relative position embedding, it should be quite surprising that it fails to generalize to unseen longer context sizes. However, we can show that in practice, RoPE does not only encode relative position. One observation we can make is that given a context size L, there are some dimensions d where the wavelength is longer than the maximum context length seen during pretraining (\u03bb > L), this suggests that some dimensions\u2019 rotary embeddings might not be distributed evenly in the rotational domain (i.e. does not perform a full rotation for the entire training context size). In such cases, we presume having unique position pairs2 implies that the absolute positional information remains intact in those dimensions. On the contrary, when the wavelength is short, only relative positional information is accessible to the network.\nGiven these observations, we can see that it is important to not touch the dimensions that only encode relative positional information, as they are crucial for the network to distinguish the relative order of nearby tokens. Meanwhile, dimensions that only encode absolute positional information should always be interpolated, as larger distances will be out of distribution. Instead of arbitrarily changing the base in \"NTK-aware\" interpolation (which basically does something similar to what is described here), we can formulate an explicit and targeted interpolation method that takes in account all of the above.\n1We note that shortly before the release of this article, Code Llama (Rozi\u00e8re et al., 2023) was released and uses \"NTK-aware\" scaling by manually scaling the base b to 1M, in which they call this method as RoPE \"adjusted base frequency\" (ABF).\n2Since the dimension never rotates fully at least once during pre-training, if we pick the first token as the anchor, every other token during pre-training has an unique distance to it, which the neural network can use to determine its absolute position.\nIn other words,\n\u2022 if the wavelength \u03bb is much smaller than the context size L, we do not interpolate; \u2022 if the wavelength \u03bb is equal to or bigger than the context size L, we want to only interpolate\nand avoid any extrapolation (unlike the previous \"NTK-aware\" method); \u2022 dimensions in-between can have a bit of both, similar to the \"NTK-aware\" interpolation.\nAs a result, it is more convenient to introduce the ratio r = L\u03bb between the original context size L and the wavelength \u03bb. This ratio represents the number of rotations a certain RoPE dimension makes given a fixed pretrained context length L. In the d-th hidden state, the ratio r depends on d in the following way:\nr(d) = L\n\u03bbd =\nL\n2\u03c0b 2d |D|\n. (10)\nIn order to define the boundary of the different interpolation strategies as above, we introduce two extra parameters \u03b1, \u03b2. All hidden dimensions d where r(d) < \u03b1 are those where we linearly interpolate by a scale s (exactly like PI, avoiding any extrapolation), and the d where r(d) > \u03b2 are those where we do not interpolate at all. Define the ramp function \u03b3 to be\n\u03b3(r) =  0, if r < \u03b1 1, if r > \u03b2 r \u2212 \u03b1 \u03b2 \u2212 \u03b1 , otherwise. (11)\nWith the help of the ramp function, the \"NTK-by-parts\" method can be described as follows.\nDefinition 1 The \"NTK-by-parts\" interpolation is a modification of RoPE using Eq. 7 with the following functions3.\ng(m) = m (12) h(\u03b8d) = ( 1\u2212 \u03b3 ( r(d) ))\u03b8d s + \u03b3 ( r(d) ) \u03b8d. (13)\nThe values of \u03b1 and \u03b2 should be tuned on a case-by-case basis. For example, we have found experimentally that for the Llama family of models, good values for \u03b1 and \u03b2 are \u03b1 = 1 and \u03b2 = 32.\nUsing the techniques described in this section, a variant of the resulting method was released under the name \"NTK-by-parts\" interpolation (bloc97, 2023b). This improved method performs better than the previous PI (Chen et al., 2023) and \"NTK-aware\" 3.1 interpolation methods, both with non-fine-tuned models and with fine-tuned models, as shown in (bloc97, 2023b) and Section 4.2."
        },
        {
            "heading": "3.3 YARN",
            "text": "In addition to the previous interpolation techniques, we also observe that introducing a temperature t on the logits before the attention softmax has a uniform impact on perplexity regardless of the data sample and the token position over the extended context window (See Appendix A.3). More precisely, instead of Eq. 2, we modify the computation of attention weights into\nsoftmax\n( qTmkn\nt \u221a |D|\n) . (14)\nThe reparametrization of RoPE as a set of 2D matrices has a clear benefit on the implementation of this attention scaling: we can instead use a \"length scaling\" trick which scales both qm and kn by a constant factor \u221a 1/t by simply scaling the complex rotary position embeddings by the same amount. With this, YaRN can effectively alter the attention mechanism without modifying its code. Furthermore, it has zero overhead during both inference and training, as rotary position embeddings are generated in advance and are reused for all forward passes. Combining it with the \"NTK-by-parts\" interpolation, we have the YaRN method.\n3The interpolation by linear ramp on h may have alternatives, such as a harmonic mean over \u03b8d/s and \u03b8d converted from a linear interpolation on wavelengths. The choice of h here was for the simplicity of implementation, but both would work.\nDefinition 2 By the \"YaRN method\", we refer to a combination of the attention scaling in Eq. 14 and the \"NTK-by-parts\" interpolation introduced in Section 3.2.\nFor LLaMA and Llama 2 models, we recommend the following values:\n\u221a 1\nt = 0.1 ln(s) + 1. (15)\nThe equation above is found by fitting \u221a\n1/t at the lowest perplexity against the scale extension by various factors s using the \"NTK-by-parts\" method (Section 3.2) on LLaMA 7b, 13b, 33b and 65b models without fine-tuning. We note that the same values of t also apply fairly well to Llama 2 models (7b, 13b and 70b). It suggests that the property of increased entropy and the temperature constant t may have certain degree of \"universality\" and may be generalizable across some models and training data.\nThe YaRN method combines all our findings and surpasses all previous methods in both fine-tuned and non-fine-tuned scenarios. Thanks to its low footprint, YaRN allows for direct compatibility with libraries that modify the attention mechanism such as Flash Attention 2 (Dao, 2023)."
        },
        {
            "heading": "3.4 DYNAMIC SCALING - \"DYNAMIC NTK\" INTERPOLATION",
            "text": "In a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to the maximal context size. A typical example is the autoregressive generation where the sequence lengths increment by 1 after each step. There are two ways of applying an interpolation method that uses a scale factor s (including PI, \"NTK-aware\", \"NTK-by-parts\" and YaRN):\n1. Throughout the whole inference cycle, the embedding layer is fixed including the scale factor s = L\u2032/L where L\u2032 is the fixed number of extended context size.\n2. In each forward-pass, the position embedding updates the scale factor s = max(1, l\u2032/L) where l\u2032 is the sequence length of the current sequence.\nThe problem of (1) is that the model may experience a performance discount at a length less than L and an abrupt degradation when the sequence length is longer than L\u2032. But by doing Dynamic Scaling as (2), it allows the model to gracefully degrade instead of immediately breaking when hitting the trained context limit L\u2032. We call this inference-time method the Dynamic Scaling method. When it is combined with \"NTK-aware\" interpolation, we call it \"Dynamic NTK\" interpolation. It first appeared in public as a reddit post in (emozilla, 2023).\nOne notable fact is that the \"Dynamic NTK\" interpolation works exceptionally well on models pretrained on L without any finetuning (L\u2032 = L). This is supported by the experiment in Appendix B.7.\nOften in the repeated forward-passes, the kv-caching (Chen, 2022) is applied so that we can reuse the previous key-value vectors and improve the overall efficiency. We point out that in some implementations when the rotary position embeddings are cached, some care has to be taken in order to modify it for Dynamic Scaling with kv-caching. The correct implementation should cache the kv-embeddings before applying rotary position embeddings, as the RoPE of every token changes when s changes."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 TRAINING",
            "text": "We broadly followed the training and evaluation procedures as outlined in Chen et al. (2023).\nFor training the 128k context window size models, we extended the Llama 2 (Touvron et al., 2023b) 7B and 13B parameter models. No changes were made to the LLaMA model architecture other than the calculation of the embedding frequencies as described in Section 3.3 with s = 16 and s = 32.\nWe used a learning rate of 2 \u00d7 10\u22125 with no weight decay and a linear warmup of 20 steps along with AdamW (Loshchilov and Hutter, 2019) \u03b21 = 0.9 and \u03b22 = 0.95. For the s = 16 model, we\nfine-tuned for 400 steps with global batch size 64 using PyTorch (Paszke et al., 2019) Fully Sharded Data Parallelism (Zhao et al., 2023) and Flash Attention 2 (Dao, 2023) on the PG19 dataset (Rae et al., 2020) chunked into 64k segments bookended with the BOS and EOS token. For s = 32 we followed the same procedure, but due to compute constraints, we started from the finished s = 16 checkpoint and trained for only an additional 200 steps. Note that the s = 32 model is also trained with 64k context data, but we show that it is able to extrapolate to a context size of 128k in Section 4.2.\nFor the ablation studies, we used the LLaMA 7B model. It has the same architecture as the newer Llama 2 models except for a shorter pretrained context window size4, which reduces compute requirements and allows for faster training and evaluations. The training procedure is similar to the 128k models, but we chunk the PG19 dataset into 32k segments instead, and train using s = 16 for 400 steps. As shown in Figure 2, YaRN converges faster compared to other interpolation techniques during training and consistently has lower loss."
        },
        {
            "heading": "4.2 LONG SEQUENCE LANGUAGE MODELING",
            "text": "To evaluate the long sequence language modeling performances, we use the GovReport (Huang et al., 2021) and Proof-pile (Azerbayev et al., 2022) datasets both of which contain many long sequence samples. For all evaluations, the test splits of both datasets were used exclusively. All perplexity evaluations were calculated using the sliding window method from Press et al. (2022) with S = 256, which takes in account the entire documents\u2019 perplexity contribution, even if the context window of the model is shorter.\nFirst, we select 10 random samples from Proof-pile with at least 128k tokens each and evaluate the perplexity of each of these samples when truncated at 2k steps from a sequence length of 2k tokens through 128k tokens. Table 1 shows the long sequence performance of fine-tuned Llama 2 s = 16 and s = 32 models. We demonstrate that YaRN is able to generalize and extrapolate to unseen context lengths and benefit from transfer learning, since the s = 32 model was only further trained for 200 steps using the s = 16 checkpoint with 64k data and is able to extrapolate to 128k context.\nIn order to further confirm the effectiveness of YaRN, we compare all four interpolation methods in Figure 3 on the left and Table 5 from Appendix B.1 as an ablation study. YaRN consistently outperforms (has lower perplexity than) other methods in both non fine-tuned and fine-tuned scenarios when using the same number of training steps. We also demonstrate that YaRN has better training efficiency compared to PI in Appendix B.2. More comparisons against open models can be found in Appendix B.3."
        },
        {
            "heading": "4.3 PASSKEY RETRIEVAL",
            "text": "The passkey retrieval task as defined in Mohtashami and Jaggi (2023) measures a model\u2019s ability to retrieve a simple passkey (i.e., a five-digit number) from amongst a large amount of otherwise meaningless text. For our evaluation of the fine-tuned 32k LLaMA 7B models, we performed\n4LLaMA models have a pretrained context size of 2k tokens, while Llama 2 models have 4k.\n50 iterations of the passkey retrieval task with the passkey placed at a random location uniformly distributed across the evaluation context window on different prompt lengths ranging from 2k to 32k. YaRN achieves higher scores compared to other interpolation methods when given similar training budget, as seen in Figure 3 on the right. More results and comparisons for Llama 2 models are shown in Appendix B.5."
        },
        {
            "heading": "4.4 STANDARDIZED BENCHMARKS",
            "text": "The Hugging Face Open LLM Leaderboard (Hugging Face, 2023) compares a multitude of LLMs across a standardized set of four public benchmarks. Specifically, we use 25-shot ARCChallenge (Clark et al., 2018), 10-shot HellaSwag (Zellers et al., 2019), 5-shot MMLU (Hendrycks et al., 2021), and 0-shot TruthfulQA (Lin et al., 2022).\nTo test the degradation of models\u2019 short context performance under context extension, we evaluated our Llama 2 and 32k LLaMA 7B models using this suite and compared it to established scores for the baselines. The results are summarized in Table 10 and Table 3. More results for Llama 2 models are shown in Appendix B.6.\nWe observe that there is minimal performance degradation between the YaRN models and their respective Llama 2 baselines. Some variance is to be expected as the PG19 dataset (Rae et al., 2020) we used for fine-tuning is very different from the original pre-training datased used for LLaMA and Llama 2 models. We also observe that there was on average a 0.49% drop in scores between the YaRN s = 16 and s = 32 models and can conclude that the the iterative extension from 64k to 128k results in negligible performance loss."
        },
        {
            "heading": "4.5 COMPUTATIONAL EFFICIENCY",
            "text": "Given that rotary position embeddings are cached during training and inference when the context window size is fixed to a preset length L, modifying the interpolation on rotary position embeddings incurs no additional computational or memory cost compared to previous context extension methods, which is the case for all four interpolation methods outlined in this work. YaRN converges the fastest during training compared to other methods, thus is the most computationally efficient, as shown in Table 4."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In conclusion, we have shown that YaRN improves upon all existing RoPE interpolation methods and can act as a drop-in replacement to PI, with no downsides and minimal implementation effort. The fine-tuned models preserve their original abilities on multiple benchmarks while being able to attend to a very large context size. Furthermore, YaRN allows efficient extrapolation with finetuning on shorter datasets and can take advantage of transfer learning for faster convergence, both of which are crucial under compute-constrained scenarios. Finally, we have shown the effectiveness of extrapolation with YaRN where it is able to \"train short, and test long\"."
        },
        {
            "heading": "6 REPRODUCIBILITY",
            "text": "To aid in reproducibility, we provide, as supplementary material, the entirety of of the code used to train the YaRN models in Table 7, as well as the evaluation code that produced Figure 7 and Tables 6, 7, 10, 8, and 9. The code also contains implementations of various extension methods referenced throughout the paper. For training YaRN, we used the publicly available PG19 dataset (Rae et al., 2020) tokenized to contiguous chunks of 64k tokens."
        },
        {
            "heading": "A ADDITIONAL DETAILS ON INTERPOLATION METHODS",
            "text": ""
        },
        {
            "heading": "A.1 POSITION INTERPOLATION",
            "text": "As mentioned in Section 2.2, PI is one of the earlier works extending context lengths of RoPE. We include some extra details here:\nWhile a direct extrapolation does not perform well on sequences w1, \u00b7 \u00b7 \u00b7 , wL with L larger than the pre-trained limit, they discovered that interpolating the position indicies within the pre-trained limit works well with the help of a small amount of fine-tuning. Specifically, given a pre-trained language model with RoPE, they modify the RoPE by\nf \u2032W (xm,m,\u03b8) = fW ( xm, mL\nL\u2032 ,\u03b8\n) , (16)\nwhere L\u2032 > L is a new context window beyond the pre-trained limit. With the original pre-trained model plus the modified RoPE formula, they fine-tuned the language model further on several orders of magnitude fewer tokens (a few billion in Chen et al. (2023)) and successfully acheived context window extension."
        },
        {
            "heading": "A.2 DETAILS OF \"NTK-AWARE\" INTERPOLATION",
            "text": "In Section 3.1, we introduce a change of basis from b to b\u2032 in the definition of \"NTK-aware\" interpolation method.\nPrecisely, following the notations set out in Section 2.1 Eq. 7, we define the \"NTK-aware\" interpolation scheme as follows:\nDefinition 3 The \"NTK-aware\" interpolation is a modification of RoPE using Eq. 7 with the following functions, given s as the scale factor.\ng(m) = m (17)\nh(\u03b8d) = b \u2032\u22122d/|D|, (18)\nwhere\nb\u2032 = b \u00b7 s |D| |D|\u22122 (19)\ns = L\u2032\nL . (20)\nGiven the results from (bloc97, 2023a), this method performs much better at extending the context size of non-fine-tuned models compared to PI (Chen et al., 2023). However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation (bloc97, 2023a) yields inferior results to PI (Chen et al., 2023). Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.\nThe mathematical derivation of the base change is the following:\nRecall that our goal is to spread out the interpolation pressure across the hidden dimensions using a base-change instead of scaling the frequencies by a fixed factor s. The property we want to guarantee is that: The lowest frequency needs to be scaled as much as linear positional scaling and the highest frequency to stay constant.\nWe introduce a new base b\u2032 such that the last dimension matches the wavelength of linear interpolation with a scale factor s. Since the original RoPE method skips odd dimensions in order to concatenate both cos( 2\u03c0x\u03bb ) and sin( 2\u03c0x \u03bb ) components into a single embedding, the last dimension d \u2208 D is |D| \u2212 2. The new base b\u2032 can be chosen so that\nb\u2032 |D|\u22122 |D| = s \u00b7 b |D|\u22122 |D| . (21)\nSolving for b\u2032 yields\nb\u2032 = b \u00b7 s |D| |D|\u22122 . (22)"
        },
        {
            "heading": "A.3 THE IMPACT OF PRE-SOFTMAX SCALING OF YARN ON PERPLEXITY",
            "text": "In Section 3.3, we mention the impact of the factor t inside the softmax computation of attention weights. Here we fix 896 16k-token documents from RedPajama (Computer, 2023)5, and calculate their perplexity scores with different scaling 1/ \u221a t. The result is in Figure 4. For comparison, recall that our recommended factor in this case (s = 8) is given by the following.\n\u221a 1\nt = 0.1 ln(s) + 1 \u2248 1.208. (23)\nTo show the impact of the factor 1/ \u221a t on different token positions, we cut each 16k-token document into chunks of 2048 tokens, and further plot the mean perplexity change comparing to t = 1 in percentages\nppl(t)\u2212 ppl(t = 1) ppl(t = 1)\n(24)\nof each chunk. The plot is shown in Figure 5.\n5We choose RedPajama because it is the open-source dataset closest to the training dataset of LLaMA as far as we are aware of.\nTo further demonstrate the best values of t across all samples over different token positions, we plot the sample counts with minimal perplexity at a given 1/ \u221a t for each of the 8 position segments over the 16k-token range in Figure 6.\nWe observe that:\n\u2022 for a suitable t, a sample may obtain better perplexity scores across the extended context window;\n\u2022 the best value of t is mostly consistent across different samples and different positions.\nWe remark that this finding is consistent for different values of s and the best value of t follows our recommended formula (Eq. 15) closely."
        },
        {
            "heading": "B.1 ABLATION STUDY",
            "text": ""
        },
        {
            "heading": "B ADDITIONAL TABLES AND CHARTS",
            "text": ""
        },
        {
            "heading": "B.2 TRAINING EFFICIENCY OF YARN",
            "text": "Table 6 shows a side-by-side comparison of the Llama 2 7B model extended from 4096 to 8192 context length via PI (LLongMA-2 7B6) and YaRN. Note that the PI model was trained using the methodology in Chen et al. (2023), while YaRN used the same methodology but 2.5x less training steps and data, as described in Section 4. Even if YaRN was only fine-tuned for 400 steps compared to PI\u2019s 1000 steps, we obtain similar results to PI."
        },
        {
            "heading": "B.3 COMPARING THE PERPLEXITY OF VARIOUS METHODS OVER A SLIDING WINDOW",
            "text": "We further evaluated the Llama 2 models fine-tuned using YaRN at the scale factor s = 16, 32 and compared them against a few long-context open-source models fine-tuned from Llama-2 such as Together.ai (Together.ai, 2023) and \"NTK-aware\" Code Llama (Rozi\u00e8re et al., 2023). The results are summarized in Table 7 (with a more detailed plot in Figure 7).\nWe observe that the model exhibits strong performance across the entire targeted context size, with YaRN interpolation being the first method to successfully extend the effective context size of Llama 2 to 128k.\nFurthermore, in Appendix B.4, we show the results of the average perplexity on 50 untruncated GovReport documents with at least 16k tokens per sample evaluated on the setting of 32k maximal context window without Dynamic Scaling in Table 8. Similar to the Proof-pile results, the GovReport results show that fine-tuning with YaRN achieves good performance on long sequences.\nTable 7 summarizes the results and a visualized and more detailed view is presented in Figure 7 here.\n6LLongMA-2 7B (Quesnelle et al., 2023) is fine-tuned from Llama 2 7B, trained at 8k context length with PI using the RedPajama dataset (Computer, 2023)."
        },
        {
            "heading": "B.4 GOVREPORT EVALUATIONS",
            "text": "In Section 4.2, we mention the evaluation on GovReport documents. The evaluation results are detailed in Table 8 below."
        },
        {
            "heading": "B.5 PASSKEY RETRIEVAL",
            "text": "For our evaluation of the 64k and 128k models, we performed 10 iterations of the passkey retrieval task with the passkey placed at a random location uniformly distributed across the evaluation context window on different context window sizes ranging from 8k to 128k. Both 7b and 13b models fine-tuned using YaRN at 128k context size passes the passkey retrieval task with very high accuracy (> 99%) within the entire context window size.\nHere we can observe that the lowest perplexity point alone does not provide a comprehensive depiction on the \"effective context size\" that an LLM can attend to. While the Code Llama 13b model exhibits increasing perplexity above 100k context lengths, it was still able to accurately retrieve the passkey at a context length of 128k. This suggest that while the output of Code Llama might start to degrade in quality above 100k context size, it is still able to maintain strong retrieval capabilities.\nIn addition, as YaRN with s = 32 was trained for 200 more steps than YaRN with s = 16 while having a higher passkey accuracy with similar perplexity, we hypothesize that perplexity may not be a great indicator of whether an LLM is able to attend to all tokens and does not exhaustively determine long context performance. This also suggests that the YaRN models with s = 16 might be relatively undertrained for the passkey retrieval task."
        },
        {
            "heading": "B.6 STANDARDIZED BENCHMARKS",
            "text": "To test the degradation of model performance under context extension, we evaluated our models using this suite and compared it to established scores for the Llama 2 baselines as well as publicly available PI and \"NTK-aware\" models."
        },
        {
            "heading": "B.7 DYNAMIC SCALING ON MODELS WITHOUT ANY FINE-TUNING",
            "text": "We first recall from Section 3.4 that the Dynamic Scaling technique is an inference-time technique that dynamically update the factor s in interpolation methods such as PI, \"NTK-by-parts\" and YaRN. We choose the original Llama 2, fix a sample in GovReport and calculate its perplexity on a sliding window of 256 tokens using RoPE, Dynamic-PI and Dynamic-YaRN.\nSince the original maximal context length of Llama 2 is 4096, we observe that Dynamic Scaling effectively extend the inference length and Dynamic-YaRN achieves better performance than Dynamic-PI. The resulting chart is in Figure 8.\nWe see that\n\u2022 Dynamic Scaling effectively prevents the blow-up of perplexity score beyond pretrained context window;\n\u2022 Dynamic-YaRN outperforms Dynamic-PI in terms of long-range perplexity on pretrained Llama-2 without any finetuning."
        }
    ],
    "title": "YARN: EFFICIENT CONTEXT WINDOW EXTENSION OF LARGE LANGUAGE MODELS",
    "year": 2024
}