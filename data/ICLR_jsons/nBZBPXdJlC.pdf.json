{
    "abstractText": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is the first multimodal large language model that focuses on general audio (rather than just speech) understanding.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuan Gong1B"
        },
        {
            "affiliations": [],
            "name": "Hongyin Luo"
        },
        {
            "affiliations": [],
            "name": "Alexander H. Liu"
        },
        {
            "affiliations": [],
            "name": "Leonid Karlinsky"
        },
        {
            "affiliations": [],
            "name": "James Glass"
        }
    ],
    "id": "SP:9c6b7c8e537a2231d9be412e290728f2b561e4f6",
    "references": [
        {
            "authors": [
                "Jerome Abdelnour",
                "Giampiero Salvi",
                "Jean Rouat"
            ],
            "title": "Clear: A dataset for compositional language and elementary acoustic reasoning",
            "venue": "arXiv preprint arXiv:1811.10561,",
            "year": 2018
        },
        {
            "authors": [
                "Jerome Abdelnour",
                "Jean Rouat",
                "Giampiero Salvi"
            ],
            "title": "Naaqa: A neural architecture for acoustic question answering",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Anderson",
                "Basura Fernando",
                "Mark Johnson",
                "Stephen Gould"
            ],
            "title": "Spice: Semantic propositional image caption evaluation",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Wei-Cheng Tseng",
                "Shang-Wen Li",
                "Hung-yi Lee"
            ],
            "title": "An exploration of prompt tuning on generative spoken language model for speech processing tasks",
            "venue": "In INTERSPEECH,",
            "year": 2022
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Yu-Kai Wang",
                "Hua Shen",
                "Iu-thing Kang",
                "Wei-Cheng Tseng",
                "Shang-Wen Li",
                "Hung-yi Lee"
            ],
            "title": "Speechprompt v2: Prompt tuning for speech classification",
            "venue": "tasks. arXiv preprint arXiv:2303.00733,",
            "year": 2023
        },
        {
            "authors": [
                "Honglie Chen",
                "Weidi Xie",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Vggsound: A large-scale audiovisual dataset",
            "venue": "In ICASSP, pp",
            "year": 2020
        },
        {
            "authors": [
                "Ke Chen",
                "Xingjian Du",
                "Bilei Zhu",
                "Zejun Ma",
                "Taylor Berg-Kirkpatrick",
                "Shlomo Dubnov"
            ],
            "title": "Htsat: A hierarchical token-semantic audio transformer for sound classification and detection",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Kun Chen",
                "Yusong Wu",
                "Ziyue Wang",
                "Xuan Zhang",
                "Fudong Nian",
                "Shengchen Li",
                "Xi Shao"
            ],
            "title": "Audio captioning based on transformer and pre-trained cnn",
            "venue": "In DCASE,",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023",
            "venue": "URL https: //lmsys.org/blog/2023-03-30-vicuna/",
            "year": 2023
        },
        {
            "authors": [
                "Jason Cramer",
                "Ho-Hsiang Wu",
                "Justin Salamon",
                "Juan Pablo Bello"
            ],
            "title": "Look, listen, and learn more: Design choices for deep audio embeddings",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Soham Deshmukh",
                "Benjamin Elizalde",
                "Rita Singh",
                "Huaming Wang"
            ],
            "title": "Pengi: An audio language model for audio tasks",
            "venue": "arXiv preprint arXiv:2305.11834,",
            "year": 2023
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Konstantinos Drossos",
                "Samuel Lipping",
                "Tuomas Virtanen"
            ],
            "title": "Clotho: An audio captioning dataset",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin Elizalde",
                "Soham Deshmukh",
                "Mahmoud Al Ismail",
                "Huaming Wang"
            ],
            "title": "Clap learning audio concepts from natural language supervision",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin"
            ],
            "title": "Hierarchical neural story generation",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Eduardo Fonseca",
                "Xavier Favory",
                "Jordi Pons",
                "Frederic Font",
                "Xavier Serra"
            ],
            "title": "Fsd50k: an open dataset of human-labeled sound events",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Frederic Font",
                "Gerard Roma",
                "Xavier Serra"
            ],
            "title": "Freesound technical demo",
            "venue": "In Proceedings of the 21st ACM international conference on Multimedia,",
            "year": 2013
        },
        {
            "authors": [
                "Logan Ford",
                "Hao Tang",
                "Fran\u00e7ois Grondin",
                "James R Glass"
            ],
            "title": "A deep residual network for largescale acoustic scene analysis",
            "venue": "In INTERSPEECH,",
            "year": 2019
        },
        {
            "authors": [
                "Heting Gao",
                "Junrui Ni",
                "Kaizhi Qian",
                "Yang Zhang",
                "Shiyu Chang",
                "Mark Hasegawa-Johnson"
            ],
            "title": "Wavprompt: Towards few-shot spoken language understanding with frozen language models",
            "venue": "arXiv preprint arXiv:2203.15863,",
            "year": 2022
        },
        {
            "authors": [
                "Jort F Gemmeke",
                "Daniel PW Ellis",
                "Dylan Freedman",
                "Aren Jansen",
                "Wade Lawrence",
                "R Channing Moore",
                "Manoj Plakal",
                "Marvin Ritter"
            ],
            "title": "Audio set: An ontology and human-labeled dataset for audio events",
            "venue": "In ICASSP,",
            "year": 2017
        },
        {
            "authors": [
                "Yuan Gong",
                "Yu-An Chung",
                "James Glass"
            ],
            "title": "Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Gong",
                "Yu-An Chung",
                "James Glass"
            ],
            "title": "AST: Audio Spectrogram Transformer",
            "venue": "In Proc. Interspeech",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Gong",
                "Andrew Rouditchenko",
                "Alexander H Liu",
                "David Harwath",
                "Leonid Karlinsky",
                "Hilde Kuehne",
                "James R Glass"
            ],
            "title": "Contrastive audio-visual masked autoencoder",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Gong",
                "Jin Yu",
                "James Glass"
            ],
            "title": "Vocalsound: A dataset for improving human vocal sounds recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Mehdi Mirza",
                "Da Xiao",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
            "venue": "arXiv preprint arXiv:1312.6211,",
            "year": 2013
        },
        {
            "authors": [
                "Zheng Gu",
                "Xinzhou Xu",
                "Shuo Liu",
                "Bj\u00f6rn Schuller"
            ],
            "title": "Zero-shot audio classification using synthesised classifiers and pre-trained models",
            "venue": "In 2022 15th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI),",
            "year": 2022
        },
        {
            "authors": [
                "Andrey Guzhov",
                "Federico Raue",
                "J\u00f6rn Hees",
                "Andreas Dengel"
            ],
            "title": "Audioclip: Extending clip to image, text and audio",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Shawn Hershey",
                "Daniel PW Ellis",
                "Eduardo Fonseca",
                "Aren Jansen",
                "Caroline Liu",
                "R Channing Moore",
                "Manoj Plakal"
            ],
            "title": "The benefit of temporally-strong labels in audio event classification",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Po-Yao Huang",
                "Hu Xu",
                "Juncheng Li",
                "Alexei Baevski",
                "Michael Auli",
                "Wojciech Galuba",
                "Florian Metze",
                "Christoph Feichtenhofer"
            ],
            "title": "Masked autoencoders that listen",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rongjie Huang",
                "Mingze Li",
                "Dongchao Yang",
                "Jiatong Shi",
                "Xuankai Chang",
                "Zhenhui Ye",
                "Yuning Wu",
                "Zhiqing Hong",
                "Jiawei Huang",
                "Jinglin Liu"
            ],
            "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
            "venue": "arXiv preprint arXiv:2304.12995,",
            "year": 2023
        },
        {
            "authors": [
                "Shaohan Huang",
                "Li Dong",
                "Wenhui Wang",
                "Yaru Hao",
                "Saksham Singhal",
                "Shuming Ma",
                "Tengchao Lv",
                "Lei Cui",
                "Owais Khan Mohammed",
                "Qiang Liu"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "venue": "arXiv preprint arXiv:2302.14045,",
            "year": 2023
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Lav R Varshney",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Ctrl: A conditional transformer language model for controllable generation",
            "year": 1909
        },
        {
            "authors": [
                "Chris Dongjoo Kim",
                "Byeongchang Kim",
                "Hyunmin Lee",
                "Gunhee Kim"
            ],
            "title": "Audiocaps: Generating captions for audios in the wild",
            "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),",
            "year": 2019
        },
        {
            "authors": [
                "Eungbeom Kim",
                "Jinhee Kim",
                "Yoori Oh",
                "Kyungsu Kim",
                "Minju Park",
                "Jaeheon Sim",
                "Jinwoo Lee",
                "Kyogu Lee"
            ],
            "title": "Improving audio-language learning with mixgen and multi-level test-time augmentation",
            "venue": "arXiv preprint arXiv:2210.17143,",
            "year": 2022
        },
        {
            "authors": [
                "Yuma Koizumi",
                "Yasunori Ohishi",
                "Daisuke Niizumi",
                "Daiki Takeuchi",
                "Masahiro Yasuda"
            ],
            "title": "Audio captioning using pre-trained large-scale language model guided by audio-based similar caption retrieval",
            "year": 2012
        },
        {
            "authors": [
                "Qiuqiang Kong",
                "Yong Xu",
                "Wenwu Wang",
                "Mark D Plumbley"
            ],
            "title": "Audio set classification with attention model: A probabilistic perspective",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Qiuqiang Kong",
                "Changsong Yu",
                "Yong Xu",
                "Turab Iqbal",
                "Wenwu Wang",
                "Mark D Plumbley"
            ],
            "title": "Weakly labelled audioset tagging with attention neural networks",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Qiuqiang Kong",
                "Yong Xu",
                "Wenwu Wang",
                "Mark D Plumbley"
            ],
            "title": "Sound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Anurag Kumar",
                "Maksim Khadkevich",
                "Christian F\u00fcgen"
            ],
            "title": "Knowledge transfer from weakly labeled audio using convolutional neural network for sound events and scenes",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Kushal Lakhotia",
                "Eugene Kharitonov",
                "Wei-Ning Hsu",
                "Yossi Adi",
                "Adam Polyak",
                "Benjamin Bolte",
                "Tu-Anh Nguyen",
                "Jade Copet",
                "Alexei Baevski",
                "Abdelrahman Mohamed"
            ],
            "title": "On generative spoken language modeling from raw audio",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Seungjun Lee",
                "Haesang Yang",
                "Hwiyong Choi",
                "Woojae Seong"
            ],
            "title": "Zero-shot single-microphone sound classification and localization in a building via the synthesis of unseen features",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Lipping",
                "Konstantinos Drossos",
                "Tuoams Virtanen"
            ],
            "title": "Crowdsourcing a dataset of audio captions",
            "venue": "In Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE),",
            "year": 2019
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Xinhao Mei",
                "Xubo Liu",
                "Qiushi Huang",
                "Mark D Plumbley",
                "Wenwu Wang"
            ],
            "title": "Audio captioning transformer",
            "venue": "arXiv preprint arXiv:2107.09817,",
            "year": 2021
        },
        {
            "authors": [
                "Xinhao Mei",
                "Xubo Liu",
                "Haohe Liu",
                "Jianyuan Sun",
                "Mark D Plumbley",
                "Wenwu Wang"
            ],
            "title": "Automated audio captioning with keywords guidance",
            "venue": "Technical report, Tech. rep., DCASE2022 Challenge,",
            "year": 2022
        },
        {
            "authors": [
                "Xinhao Mei",
                "Chutong Meng",
                "Haohe Liu",
                "Qiuqiang Kong",
                "Tom Ko",
                "Chengqi Zhao",
                "Mark D Plumbley",
                "Yuexian Zou",
                "Wenwu Wang"
            ],
            "title": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
            "venue": "arXiv preprint arXiv:2303.17395,",
            "year": 2023
        },
        {
            "authors": [
                "Annamaria Mesaros",
                "Toni Heittola",
                "Tuomas Virtanen"
            ],
            "title": "Tut database for acoustic scene classification and sound event detection",
            "venue": "In 2016 24th European Signal Processing Conference (EUSIPCO),",
            "year": 2016
        },
        {
            "authors": [
                "Annamaria Mesaros",
                "Toni Heittola",
                "Aleksandr Diment",
                "Benjamin Elizalde",
                "Ankit Shah",
                "Emmanuel Vincent",
                "Bhiksha Raj",
                "Tuomas Virtanen"
            ],
            "title": "Dcase 2017 challenge setup: Tasks, datasets and baseline system",
            "venue": "In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events,",
            "year": 2017
        },
        {
            "authors": [
                "R Channing Moore",
                "Daniel PW Ellis",
                "Eduardo Fonseca",
                "Shawn Hershey",
                "Aren Jansen",
                "Manoj Plakal"
            ],
            "title": "Dataset balancing can hurt model performance",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao"
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277,",
            "year": 2023
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Wenhui Wang",
                "Li Dong",
                "Yaru Hao",
                "Shaohan Huang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "title": "Kosmos-2: Grounding multimodal large language models to the world",
            "venue": "arXiv preprint arXiv:2306.14824,",
            "year": 2023
        },
        {
            "authors": [
                "Karol J Piczak"
            ],
            "title": "Esc: Dataset for environmental sound classification",
            "venue": "In Proceedings of the 23rd ACM international conference on Multimedia,",
            "year": 2015
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "Alpaca: A strong, replicable instructionfollowing model",
            "venue": "Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html,",
            "year": 2023
        },
        {
            "authors": [
                "Mi Tian",
                "Ajay Srinivasamurthy",
                "Mark Sandler",
                "Xavier Serra"
            ],
            "title": "A study of instrument-wise onset detection in beijing opera percussion ensembles",
            "venue": "In 2014 ieee international conference on acoustics, speech and signal processing (icassp),",
            "year": 2014
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yun Wang",
                "Juncheng Li",
                "Florian Metze"
            ],
            "title": "A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Jian Wu",
                "Yashesh Gaur",
                "Zhuo Chen",
                "Long Zhou",
                "Yimeng Zhu",
                "Tianrui Wang",
                "Jinyu Li",
                "Shujie Liu",
                "Bo Ren",
                "Linquan Liu"
            ],
            "title": "On decoder-only architecture for speech-to-text and large language model integration",
            "venue": "arXiv preprint arXiv:2307.03917,",
            "year": 2023
        },
        {
            "authors": [
                "Yusong Wu",
                "Ke Chen",
                "Tianyu Zhang",
                "Yuchen Hui",
                "Taylor Berg-Kirkpatrick",
                "Shlomo Dubnov"
            ],
            "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
            "venue": "In ICASSP,",
            "year": 2023
        },
        {
            "authors": [
                "Yuzhong Wu",
                "Tan Lee"
            ],
            "title": "Reducing model complexity for dnn based large-scale audio classification",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Huang Xie",
                "Tuomas Virtanen"
            ],
            "title": "Zero-shot audio classification based on class label embeddings",
            "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),",
            "year": 2019
        },
        {
            "authors": [
                "Huang Xie",
                "Tuomas Virtanen"
            ],
            "title": "Zero-shot audio classification via semantic embeddings",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Huang Xie",
                "Okko R\u00e4s\u00e4nen",
                "Tuomas Virtanen"
            ],
            "title": "Zero-shot audio classification with factored linear and nonlinear acoustic-semantic projections",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Huang Xie",
                "Okko R\u00e4s\u00e4nen",
                "Konstantinos Drossos",
                "Tuomas Virtanen"
            ],
            "title": "Unsupervised audiocaption aligning learns correspondences between individual sound events and textual phrases",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Xuenan Xu",
                "Heinrich Dinkel",
                "Mengyue Wu",
                "Kai Yu"
            ],
            "title": "A crnn-gru based reinforcement learning approach to audio captioning",
            "venue": "In DCASE,",
            "year": 2020
        },
        {
            "authors": [
                "Xuenan Xu",
                "Heinrich Dinkel",
                "Mengyue Wu",
                "Zeyu Xie",
                "Kai Yu"
            ],
            "title": "Investigating local and global information for automated audio captioning with transfer learning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Changsong Yu",
                "Karim Said Barsim",
                "Qiuqiang Kong",
                "Bin Yang"
            ],
            "title": "Multi-level attention model for weakly supervised audio classification",
            "venue": "In DCASE Workshop,",
            "year": 2018
        },
        {
            "authors": [
                "Lipping"
            ],
            "title": "2019), and Sound Bible (soundbible.com, 2006) as our training data. For all these datasets, we only include data marked as training and validation samples and exclude any data marked as test or evaluation. A total of 845K unique audio clips are used. In the main manuscript, we discuss the relabeling method (the novel method",
            "year": 2006
        },
        {
            "authors": [
                "Kong"
            ],
            "title": "Due to the space limitation, we only show the results of the best supervised and specialized models in Table 4. These models are Chen et al",
            "venue": "Elizalde et al",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "As we go about our daily lives, we are surrounded by complex mixtures of audio signals. Our cognitive abilities enable us not only to perceive and identify these sounds but also to comprehend their implicit meaning. For instance, upon hearing a clock chime six times, we probably assume it\u2019s 6 o\u2019clock; when hearing a train whistle we might infer the arrival or departure of a train; when encountering unfamiliar animal sounds in the wilderness, we possess the ability to assess potential danger by discerning certain audio characteristics. Additionally, we can also grasp the emotional atmosphere conveyed by audio cues. We anticipate that future artificial intelligence systems will have similar abilities to perceive and comprehend an auditory environment.\nThanks to the release of the large-scale audio corpus AudioSet (Gemmeke et al., 2017), significant advances have been achieved in general audio event recognition over the past five years. Notably, the mean Average Precision (mAP) for audio tagging has improved significantly, increasing from 31.4 (Gemmeke et al., 2017) to 47.3 (Huang et al., 2022), demonstrating the increasingly strong capabilities of deep learning models to perceive sounds. However, models trained with discrete sound label sets (e.g., the 527-category AudioSet), while being strong in perception, possess very limited reasoning and understanding capabilities. For example, the model may recognize a clock chime 6 times, but not know that it indicates a time of 6 o\u2019clock. On the other hand, we find that modern large language models (LLMs), such as GPT (Brown et al., 2020; OpenAI, 2023) and LLaMA (Touvron et al., 2023) possess strong audio knowledge and reasoning abilities without any in-domain audio training or fine-tuning, e.g., ChatGPT outputs \u201cdog bark is short, repetitive, and sharp woof or bark sound\u201d when we ask it the question \u201cwhat does a dog bark sound like?\u201d However, even the most advanced LLMs are not yet able to perceive general audio events.\nCode, dataset, and pretrained models are available at https://github.com/yuangongnd/ltu.\n...\n...\nP oo\nlin g\nThe potential synergy between conventional audio models and LLMs in sound perception and reasoning motivates us to integrate them into a single model that is able to Listen to, Think about, and Understand the sound environment; we call this new model LTU. Specifically, the LTU framework effectively integrates a high-performant audio perception model AST (Gong et al., 2021b) with an open-source LLM LLaMA (Touvron et al., 2023). Since the data required to train LTU did not exist, we created a new dataset called OpenAQA-5M that combines 8 mainstream audio datasets, and formats all data as (audio, question, answer) tuples. Notably, OpenAQA-5M includes 3.7 million free-form open-ended and very diverse audio question-answer (AQA) pairs generated by a new GPT-assisted method called Audio Instruction Generation (AIG).\nPerformance-wise, on audio classification tasks, LTU outperforms the conventional audio-text CLAP model (Elizalde et al., 2023) on all 8 benchmarks with an average relative improvement of 23.6%. In addition, unlike CLAP, LTU does not require a predefined label set during inference. On captioning tasks, LTU is comparable with SOTA specialized models. Of greater interest, LTU exhibits emerging audio reasoning and comprehension abilities (shown in Figure 1 and Table 6) that are absent in existing audio models. It can answer open-ended questions about audio with an instruction following and factual correctness rate of 82.9%. With these capabilities, LTU can serve as a foundation audio reasoning model. To the best of our knowledge, LTU is the first multimodal LLMs that focuses on general audio (rather than just speech) understanding, and our contributed OpenAQA-5M is the largest and most diverse audio question-answering dataset to date."
        },
        {
            "heading": "2 LTU MODEL ARCHITECTURE",
            "text": "The architecture of the LTU model is depicted in Figure 1.\nAudio Encoder. We use an Audio Spectrogram Transformer (AST) (Gong et al., 2021b) pretrained with the CAV-MAE objective (Gong et al., 2022a) and finetuned on AudioSet-2M as our audio encoder. Each 10-second audio waveform is first converted to a sequence of 128-dimensional log Mel filterbank (fbank) features computed with a 25ms Hanning window every 10ms. This results in a 1024 (time) \u00d7 128 (frequency) spectrogram. We then split the spectrogram into 512 (64 (time) \u00d7 8 (frequency)) square patches of shape 16 \u00d7 16 and input them to AST. The output of AST is a sequence of 512 audio embeddings of 768-dimensions. We reshape it back to 64 (time) \u00d7 8 (frequency) and apply a frequency mean pooling and 2\u00d7 temporal downsampling to produce a sequence of 32 audio embedding in temporal order (3.2Hz). Finally, we project the 768-dimensional audio embedding to 4096-dimensional to match the embedding dimension of LLaMA. The 32 audio embeddings are then concatenated with text embeddings as the input to the large language model.\nLLaMA Large Language Model. In this paper, we use the LLaMA-7B large language model (Touvron et al., 2023) with Vicuna (Chiang et al., 2023) instruction following training. LLaMA is pretrained on a combination of natural and programming language corpora in a self-supervised manner. Based on LLaMA, Vicuna is further trained on instruction-following language prompts generated by GPT models (Taori et al., 2023; Peng et al., 2023a), and thus is more effective on a number of reasoning and generation tasks.\nLow-rank Adapters. During LTU training, we adopt Low-rank Adaptation (Hu et al., 2021) (LoRA) rather than end-to-end fine-tuning of LLaMA parameters to 1) mitigate catastrophic forgetting (Goodfellow et al., 2013) and overfitting, and 2) improve the training efficiency. LoRA introduces a small set of auxiliary learnable weights, named LoRA adapters, on top of the pre-trained LLM, leaving all parameters in the latter unchanged. Each LoRA adapter is attached to a specific model layer, modifying its frozen parameter by the simple addition of a low-rank learnable matrix of the same size. In LTU, we inject LoRA adapters (rank=8 and \u03b1=16) to the projection layers for key and query in all self-attention layers (Vaswani et al., 2017) of the LLaMA model, introducing only 4.2 million learnable parameters for fine-tuning.\nTraining Objective. LTU is trained on the next token prediction task conditioning on the past tokens and the reference audio, i.e., maximizing\nP (xt | x1:t\u22121, A) (1) through cross-entropy for all 1 < t \u2264 T given text sequence x1:T and the reference audio A. Generation Setting. Throughout this paper, we use a plain generation setting of Temperature=0.1, Top K=500, and Top P=0.95 with a repetition penalty of 1.1 (Fan et al., 2018; Keskar et al., 2019). We observe that this setting generally works well for all tasks; tuning generation parameters might benefit a specific task but is less practical in real applications."
        },
        {
            "heading": "3 THE OPENAQA DATASET",
            "text": "Since LTU is a generative AI system and is anticipated to possess a wide range of capabilities for diverse audio tasks, existing datasets are not sufficient to train LTU. Therefore, we created a new audio dataset called OpenAQA-5M. Specifically, we formulate all tasks as audio question answering, i.e., each sample in the dataset is a (audio, question, answer) tuple, where audio and question are the model inputs, and answer is the label. This allows us to unify nearly all audio tasks into a single dataset and also maps the labels to a semantic space.\nAs there are already many high-quality audio datasets, we do not collect new audio data, but instead relabel existing public datasets including AudioSet (including a 500K subset of the original 2M weakly-labeled release (Gemmeke et al., 2017) and the 100K subset with temporally-strong labels (Hershey et al., 2021)), VGGSound (Chen et al., 2020a), FSD50K (Fonseca et al., 2021),\nAudioCaps (Kim et al., 2019), Freesound (Font et al., 2013), Clotho v2 (Lipping et al., 2019), and Sound Bible (soundbible.com, 2006) as our training data. For all datasets, we only include data marked as training and validation samples and exclude any data marked as test or evaluation. A total of 845K unique audio clips are used.\nThe OpenAQA dataset consists of two subsets: a 1.9M closed-ended question subset, and a 3.7M open-ended question subset, both are crucial to the success of LTU training. Although the LLaMA7B large language model in LTU already has general knowledge and reasoning ability about the sound, in data generation, we use a larger and more powerful GPT-3.5-Turbo (Brown et al., 2020) LLM to improve the data quality. Table 1 presents the statistics of OpenAQA-5M."
        },
        {
            "heading": "3.1 CLOSE-ENDED AUDIO QUESTION-ANSWER GENERATION",
            "text": "We include four closed-ended audio tasks. For each task, we paraphrase the question with GPT-3.5Turbo assistance to generate a diverse question set, but the answers are generated with a rule-based algorithm, and thus have a fixed format. The upper section of Table 2 presents samples of closedended question-answer pairs. As we will discuss in Section 4, such closed-ended tasks are crucial for guiding LTU when being conditioned on audio, and can greatly mitigate the hallucination issue.\nClassification: The question for this task is \u201cclassify the sound events in the audio clip\u201d and its GPT-assisted paraphrases; The answer is a list of sound class names that the audio clip contains.\nAcoustic Features: This task aims to train the model to recognize low-level features for better generalization. Original audio datasets do not have this information. We thus generate the acoustic feature description using GPT-3.5-Turbo with the prompt \u201cdescribe the acoustic characteristic of {sound class name} sound precisely with a sentence less than 10 words\u201d. We generate 10 different descriptions for each sound class. The question for this task is \u201cclassify the sound events in the audio clip based on acoustic features\u201d and its GPT-assisted paraphrases. The answer is a list of GPT-3.5-Turbo-generated acoustic features and the sound class names.\nCaptioning: The question is \u201cwrite an audio caption describing the sound\u201d, and the answer is the caption. For AudioCaps (Kim et al., 2019) and Clotho v2 (Drossos et al., 2020), we use the original manually-crafted captions. For Freesound (Font et al., 2013) and SoundBible (soundbible.com, 2006), we use the captions generated with GPT assistance in the WavCaps project (Mei et al., 2023).\nTemporal Analysis: We use the time stamp information in the temporally strongly-labeled AudioSet (Hershey et al., 2021) to craft questions including listing the time stamps of all sound events, asking the time stamps of a specific sound, and the order of sounds with a rule-based algorithm."
        },
        {
            "heading": "3.2 OPEN-ENDED AUDIO QUESTION-ANSWER GENERATION",
            "text": "Compared to closed-ended QA, generating large-scale, diverse open-ended QA is more challenging. On the one hand, generating millions of QA pairs presents an impractical task for humans, even with crowd-sourcing; on the other hand, rule-based automatic generation methods could result in significant limitations in terms of output diversity (Abdelnour et al., 2022). For pure language tasks, it was found that using GPT large language models to automatically generate question-answer pairs as the training data for instruction tuning (Taori et al., 2023; Peng et al., 2023a) is a plausible solution.\nNonetheless, even the most advanced large language models (e.g., GPT-4) have not yet supported non-speech audio input (which is one of the main motivations of this work). How can we therefore use large language models to automatically generate audio QA? In this paper, we present Audio Instruction Generation (AIG). Specifically, we first extract the meta information of the audio (e.g., audio events, acoustic features, caption, and temporal information) and input the audio meta information to the GPT-3.5-Turbo model in the form of pure text, and then use the prompt shown in Table 2 to let the GPT-3.5-Turbo model generate audio QA pairs. In this process, GPT-3.5-Turbo only sees audio meta information instead of listening to the audio sample. We generate multiple QAs for each audio clip. With more meta-information provided, the generated QAs are more diverse, thus we generate more questions for samples with richer meta-information (e.g., strongly labeled AudioSet). We also use all 8 audio datasets to increase audio diversity.\nA total of 3.7 million QA pairs are generated, which is about two orders of magnitude greater than the instruction tuning datasets used for pure language tasks (Taori et al., 2023). We observe consistent performance improvement with the increase of training data, indicating the necessity of having a larger dataset for audio since the LLaMA model has not been pretrained with any audio data. The generated open-ended QA pairs are very diverse, among the 3.7M QA pairs, 78% questions and 95% answers are unique, i.e., most questions and answers only appear once in the dataset. In addition, the generated QAs cover a wide spectrum from low-level tasks (e.g., identifying the audio events) to high-level understanding tasks (e.g., identifying the atmosphere of the audio). It is also worth mentioning that approximately 6.5% of the questions generated by GPT-3.5-Turbo do not have an answer based on the provided audio information, and the corresponding generated answers are \u201cit cannot be determined from the audio that ...\u201d. These questions are equally valuable and important since they teach the model to know what it does not know and reduces the hallucination issue.\nNote that only the raw audio and generated QA pairs are input to LTU in training, the audio meta information is only used in QA generation and is not input to LTU. Thus, the LTU model is forced to learn directly from the raw audio that contains richer and more fine-grained information rather than from the extracted meta-information. In inference, LTU solely use raw audio to answer the question."
        },
        {
            "heading": "4 LTU TRAINING RECIPE",
            "text": "We find an appropriate curriculum is crucial to successfully train LTU. As we mentioned in Section 2, we freeze the LLaMA model to mitigate catastrophic forgetting, and only train the AST audio encoder, the audio projection layer, and the LoRA adapters. Since the audio projection layer is randomly ini-\ntialized, we first freeze AST and LoRA adapters and only train the audio projection layer with the closed-ended classification and acoustic feature description tasks in stage 1. Then in stages 2-4, we set all parameters (excluding LLaMA) as trainable, but gradually increase the openness of the\ntraining task, from only the classification task and acoustic feature description tasks (stage 2), to all closed-ended tasks (stage 3), and finally to all closed-ended and open-ended tasks (stage 4).\nThe reason for this design is that we find open-ended tasks to be too difficult for the model at the beginning of training. The model tends to use its language capability to answer the free-form openended question instead of conditioning on the audio input (i.e., hallucination). Thus, it is crucial to guide the model to attend to audio using closed-ended tasks with objective answers (e.g., sound class labels) in earlier stages, where the model gets a high penalty for wrong predictions. In other words, this curriculum teaches the model first to perceive, and then understand the sound. We refer to this training curriculum as a perception-to-understanding curriculum. As we will show in Section 5, the model performance significantly drops without such a training curriculum.\nIn all training stages, we use a batch size of 256 and linear learning rate decay with warmup. We set the text token cutoff length to 108. The model is trained on 4\u00d7 RTX A6000 GPUs for about 3 days."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 CLOSED-ENDED AUDIO TASK EXPERIMENTS",
            "text": "Correct perception is the basis of advanced reasoning. If the model cannot correctly recognize the sounds, its answer to open-ended questions would be nothing but just hallucination. Therefore, before we discuss the exciting open-ended task results, we first rigorously evaluate LTU on classic closed-ended tasks including 8 audio classification benchmarks and 2 audio captioning benchmarks.\nAudio Classification: Since LTU directly outputs audio label names or descriptions in text form instead of label index, to benchmark and compare it with existing models, we encode the LTU output and the evaluation dataset label names using a text encoder (gpt-text-embedding-ada), and compute the cosine similarity between the text embedding of LTU output and each label. For singlelabel classification tasks, we use the label that has the highest similarity score as the prediction and report accuracy or F1-score; for multi-label classification tasks, we use the similarity score as the prediction score and report the mAP. Note that LTU learns to only output prominent sound classes, which aligns with human perception. Conventional audio tagging models (e.g., AST) trained with binary cross entropy losses output a score for each class, to output prominent classes, a threshold needs to be calibrated for each class. However, this nice feature of LTU leads to a lower mAP score because the likelihood of non-prominent sound classes is not predicted. We tested two prompts \u201cclassify the sound events in the audio clip\u201d and \u201cwrite an audio caption describing the sound\u201d, while both led to good results in our subjective evaluation, the latter led to better text embedding for the automatic evaluation framework and is used for benchmarking. As shown in Table 4, LTU outperforms contrastive audio-text model CLAP (Elizalde et al., 2023) on all eight classification tasks with an average relative improvement of 23.6% and is comparable to a concurrent work (Wu et al., 2023b). Note that LTU is trained and evaluated with audios sampled at 16kHz while both CLAP and (Wu et al., 2023b) use 44.1KHz sampling rate, a higher sampling rate is known to lead to better audio classification performance but is also more computationally expensive. In addition, unlike CLAP and (Wu et al., 2023b), LTU does not need a pre-defined label set for classification.\nAudio Captioning: We use the prompt \u201cwrite an audio caption describing the sound\u201d and take the model output as the prediction. We use SPICE (Anderson et al., 2016) as the evaluation metric and evaluate LTU on the test set of two major audio captioning benchmarks Clotho v2 (Drossos et al., 2020) and AudioCaps (Kim et al., 2019) with the standard evaluation protocol. As shown in Table 4, LTU achieves 17.0 and 11.9 SPICE scores on AudioCaps and Clotho, respectively, which is comparable with SOTA models. Note that compared with specialized models trained and evaluated on the same dataset, LTU is trained with more diverse data and thus may not fit the data-specific vocabulary or style, so the SPICE score may underestimate the performance of LTU as it does not count synonyms as correct, e.g., LTU\u2019s prediction of \u201ca drill is running and vibrating\u201d gets a 0 SPICE score though it is semantically close to the ground truth caption \u201ca tool buzzing\u201d.\nAblation Studies (Table 5): First, we train LTU models with full finetuning (i.e., trainable LLaMA) and various LoRA adapter settings. When the learning rate is set to an appropriate value (2e-5), LTU performs slightly better on audio tasks but dramatically loses its original language reasoning ability, i.e., catastrophic forgetting, which justifies the need to freeze the LLaMA model. LoRA hyperparameters do not have a large impact on model performance, but removing LoRA adapters will lead to noticeable lower performance, which justifies the need to have adapters for audio tasks.\nSecond, we train a model without the perception-to-understanding curriculum. To make a fair comparison, we train this model with the same number of total iterations as the LTU model. The audio task performance significantly declines, which justifies the necessity of the training curriculum. We also tested various training epochs and learning rates and found that the default setting is the best.\nThird, we evaluate the performance of the model trained with only closed-ended tasks (i.e., stage 1-3) and find its performance on closed-ended tasks is worse than LTU trained with both closedended and open-ended tasks, indicating that open-ended task training also benefits closed-ended task performance, i.e., learning to understand improves the perception ability.\nSummary: As an open-ended AQA model, LTU also exhibits a strong generalization ability across closed-ended datasets and tasks. Notably, LTU directly outputs its prediction in text, automatically filters out non-prominent sounds in the prediction, and requires no pre-defined label set for inference. These distinguish LTU from existing audio-text models and make it a potentially more attractive system for real-world applications."
        },
        {
            "heading": "5.2 OPEN-ENDED AUDIO TASKS EXPERIMENT",
            "text": ""
        },
        {
            "heading": "5.2.1 EMERGING AUDIO REASONING CAPABILITIES IN OPEN-ENDED TASKS",
            "text": "The main advantage of LTU is its capability to address open-ended audio tasks where LTU exhibits emerging reasoning capabilities that are not present in conventional models. In Table 6, we show LTU\u2019s answers to questions asking about realistic test audios. Key findings are as follows:\nLTU can answer follow-up questions about the details. Conventional audio tagging models map audio to a set of pre-defined sound labels, but they cannot elaborate. In contrast, LTU can describe the details of the sound if the user asks a follow-up question. For example, in Table 6, Sample 1, LTU can tell us the bell rings three times, and has a feature of bright and high-pitch.\nLTU can explain its predictions. Conventional models produce predictions without explanation. In contrast, LTU can explain its prediction if the user asks. For example, in Table 6, Sample 2, LTU tells the reason why it thinks the atmosphere is dangerous. More interestingly, in multi-turn conversation, it can explain why it made a wrong prediction when the user points out a mistake.\nLTU can think step by step. Conventional models produce predictions without intermediate steps. In contrast, LTU can do step-by-step reasoning if the user asks. For example, in Figure 1, in the Right Sample, LTU does advanced step-by-step reasoning to understand the audio clip.\nLTU can understand the scene and can connect sounds to actions and usages. In Table 6, Sample 2, LTU understands it is a dangerous scene and suggests seeking shelter when a gunshot is heard; in Sample 3, LTU suggests the owl calling sounds can be used for dubbing a horror movie.\nLTU knows what it does not know. Though we would like LTU to answer all our questions, we do not want it to hallucinate what it does not hear clearly or answer questions that are not answerable based on the audio. As shown in Table 6, Sample 1, LTU refuses to give a definite answer about the bell type but instead lists a few possibilities. We find the \u223c6.5% unanswerable QA pairs in the training data improve LTU\u2019s ability to refuse unanswerable questions. If we remove them from the training set, LTU has a lower refusal rate to GPT-generated unanswerable questions (69.3%\u219251.9%), i.e., a higher risk of hallucination. Also, the close-ended performance slightly drops (50.3\u219250.0). Note that GPT-generated unanswerable questions are not answerable by GPT based on the audio meta information, but they may be answerable by LTU based on the audio file."
        },
        {
            "heading": "5.2.2 QUANTITATIVE EVALUATION",
            "text": "In addition to the compelling qualitative analysis presented in Section 5.2.1, we further conducted a series of evaluations to quantitatively assess the overall quality of LTU predictions.\nWe evaluated the ability of LTU to directly answer any question raised by the user (i.e., instruction following) and if the answer is factually correct rather than a hallucination. Owing to the absence of an automated evaluation protocol for open-ended audio question answering, we conducted a twostage human subjective evaluation through Amazon Mechanical Turk (AMT) with a total of 476 unique human evaluators independent of us. In Stage 1, we use open-ended questions generated by GPT-4 to evaluate LTU (note that in generating our training data we only used GPT 3.5). We play the audio and show the human evaluators the question and the LTU answer and ask them to respond to each of the following questions: 1) if the answer directly addresses the question (instruction following); 2) if LTU answer is factually correct; 3) compare between LTU and GPT-4 answers, which is better? Finally, we ask the human evaluators to ask another question and provide an answer\nbased on the audio, which are used in Stage 2 evaluation. Stage 2 is the same as Stage 1 except we use human-generated QA pairs, i.e., LTU answers questions posed by humans and not by GPT-4, and other human evaluators compare its predictions with human-generated responses.\nTable 7 presents the results of the evaluation. The instruction following & factual correct rate of LTU is 82.9% to humangenerated audio questions, and 75.9% to GPT-4 generated audio questions, respectively. In addition, 74.9% of human evaluators rate LTU answers are better than human-generated answers, and 69.1% of human evaluators rate LTU answers are better than GPT-4 generated answers. As a supplement to the human subjective evaluation, we follow (Peng et al., 2023a) to automatically evaluate the instruction fol-\nlowing rate with GPT-4 assistance with the prompt \u201cBelow is a pair of question and response. Identify if the response answers the question\u201d. Results show that LTU has an instruction following rate of 96.9%. In contrast, the model trained with only closed-ended tasks cannot follow instructions well (22.5%). These results suggest LTU indeed does consistently well for open-ended tasks."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "In the past year, modern large language models (OpenAI, 2023; Chiang et al., 2023; Zhang et al., 2023b) have demonstrated powerful reasoning and understanding abilities. To further enable multi-modal capabilities for LLMs, Flamingo (Alayrac et al., 2022), GPT-4 (OpenAI, 2023), Kosmos (Huang et al., 2023b), BLIP (Li et al., 2023), LLaMA-Adapter (Zhang et al., 2023b), PaLME (Driess et al., 2023), and LLaVA (Liu et al., 2023) each proposes a different integration method. Nonetheless, they all focus on the visual modality. Among these, LLaVA (Liu et al., 2023) (concurrent) is the closest work with us as it also proposes an instruction tuning method and a 2-stage training. However, the main motivation and implementations are different due to the very different focusing modalities. In the sound domain, WavPrompt (Gao et al., 2022) introduces a framework that integrates GPT-2 and a speech representation model to address speech understanding in the classification form. SpeechPrompt (Chang et al., 2022; 2023) studies speech understanding in a text-less style through prompt tuning spoken language model (Lakhotia et al., 2021). More recently, SpeechGPT (Zhang et al., 2023a) (concurrent) and Speech-LLaMA (Wu et al., 2023a) (concurrent) apply LLMs to speech recognition and translation, respectively. However, all these efforts mainly focus on speech and have limited ability for open-ended question answering. AudioGPT (Huang et al., 2023a) (concurrent) introduced a system using Chat-GPT as an interface for a broad collection of audio and speech applications and thus relies on external audio systems. Pengi (Deshmukh et al., 2023) (concurrent) also proposes an audio language model, but is less focused on open-ended tasks. LTU differs from these prior works in that 1) LTU focuses on general audio understanding instead of pure speech; 2) LTU can answer both closed-ended and open-ended questions, and 3) LTU is a standalone model that does not rely on external systems. To the best of our knowledge, LTU is the first model bridging audio perception and advanced reasoning."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This paper aims to build an artificial intelligence system named LTU that can listen to, think about, and understand the surrounding audio environment. With extensive experiments, we find a dataset consisting of both closed-ended and open-ended questions and a perception-to-understanding training curriculum are two key components to successfully train LTU. The closed-ended tasks are used to train the model at the beginning stage to force the model conditioned on audio, while the openended tasks enable the model to have advanced reasoning and understanding ability. By formulating all tasks as audio question-answering, we can train LTUwith many diverse tasks. Consequently, LTU not only outperforms the SOTA audio-text model CLAP with an average relative improvement of 23.6% on closed-ended tasks, but also exhibits emerging reasoning capability in open-ended tasks.\nLimitations: In this work, we mainly focus on general audio understanding and LTU thus has limited ability to understand speech content, i.e., it is not an automatic speech recognition model."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research is supported by the MIT-IBM Watson AI Lab. We thank Samuel Thomas, Hilde Kuehne, Rogerio Feris, and Brian Kingsbury for their helpful discussions."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "The audio data used in this paper are publicly available online, we do not use private audio data to train the model. The proposed audio large language model has the potential to benefit individuals with disabilities by providing hearing aids. However, it is imperative to acknowledge the potential applications of such models in security-related domains. While these models inherently do not specialize in speech recognition, their capabilities can, nonetheless, be repurposed or adapted for tasks that might have security implications. That said, given their primary design not being for speech recognition, the associated risk is comparatively mitigated. It is important for researchers, developers, and users who employ this technology responsibly to ensure its application aligns with ethical considerations and avoids potential misuse."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We document all implementation details in Section 2, 3, 4 and Appendix. Code, dataset, and pretrained models are available at https://github.com/yuangongnd/ltu."
        },
        {
            "heading": "A EXTENDED LITERATURE REVIEW",
            "text": "Due to the space limitation, in the main manuscript, we focus on discussing works closest to LTU (mainly LLM-related research). In this section, we further extend the discussion and focus more on general audio processing research.\nIn the past decade, audio tagging and classification research has moved from small and constrained datasets consisting of tens of sound classes to much larger datasets with a greater variety and range of real-world audio events. A significant milestone is the release of the AudioSet corpus (Gemmeke et al., 2017) containing over 2 million audio clips labeled with a set of 527 event labels. Based on AudioSet, a series of efforts have been made to improve the audio tagging performance in terms of mAP (Gong et al., 2021a;b; Ford et al., 2019; Wang et al., 2019; Yu et al., 2018; Kong et al., 2018; 2019; Kumar et al., 2018; Wu & Lee, 2018; Huang et al., 2022). Nonetheless, these models can only predict sound class in the 527-class AudioSet label set, and cannot generalize to unseen sound classes. To overcome this limitation, zero-shot audio classification has been proposed in multiple recent work (Xie & Virtanen, 2019; 2021; Xie et al., 2021; Gu et al., 2022; Lee et al., 2021; Elizalde et al., 2023; Guzhov et al., 2022). Specifically, zero-shot learning uses auxiliary information about the sound classes such as their textual descriptions and maps the audio to the auxiliary information instead of label indexes. In the inference stage, the model can generalize to new classes as long as its auxiliary information is provided. However, a predefined label set is still needed. LTU differs with the above efforts in that 1) it is not constrained with a fixed label set and can generalize to unseen classes; and 2) it directly outputs the label names in text form and does not require a predefined label set in the inference stage. These make LTU a more practical system for real-world applications.\nOn the other hand, a sound label (usually a word or a phrase) is not informative enough for many applications. To better describe the sound, automated audio captioning (AAC), the task to generate audio content description using free text, is getting increasingly more attention. Large-scale audio captioning datasets including AudioCaps (Kim et al., 2019) and Clotho (Drossos et al., 2020) established a solid basis for AAC research and many new approaches have been proposed including (Xu et al., 2020; Xie et al., 2022; Xu et al., 2021; Chen et al., 2020b; Koizumi et al., 2020; Mei et al., 2021; 2022). The main limitation of audio captioning is that the description is usually short (less than 20 words) and cannot cover all information of the audio. Therefore, the caption may miss the information that the user is interested in. To overcome this issue, a new task audio question answering has been proposed (Abdelnour et al., 2018; 2022). While it is an important step towards audio understanding and reasoning, both (Abdelnour et al., 2018; 2022) have a limited number of closed-ended question types and support a limited number of sound events. LTU differs from the above efforts in that LTU can not only generate audio caption and description, but also answer any open-ended questions about the details and explanation. With the LLaMA large language model, LTU exhibits advanced reasoning and understanding ability, e.g., it can connect the audio with actions. Such reasoning and understanding ability are not yet present in existing audio models."
        },
        {
            "heading": "B EXTENDED DISCUSSION OF LIMITATIONS",
            "text": "First, the LLaMA-7B model used in LTU is the smallest model in the LLaMA model family, LLaMA-13B, and larger models are more commonly used and may have a stronger reasoning and understanding ability. Particularly, larger LLM may exhibit emerging properties that the smaller LLM model does not have. While we show LTU exhibits stronger audio reasoning and understanding abilities than existing models, it could have been even stronger if a larger LLM was used. The choice of using LLaMA-7B is mainly due to the limit of our computational resources.\nSecond, for the same computational efficiency consideration, we applied a 2\u00d7 temporal downsampling to make the audio token frequency at 3.2Hz. This may limit the fine-grained temporal reasoning ability of the model. Increasing the temporal resolution of the audio input may further improve LTU.\nThird, while we used 8 audio datasets to construct the OpenAQA-5M dataset, only a small portion of them are with temporal-level annotation (i.e., the onset and offset of the sound event), which is important information for GPT-3.5-Turbo to generate time-related questions. Therefore, the capability of the model to temporally localize the sound event might be weaker than its capability to recognize the sound events.\nFinally, while we find closed-ended training and the unanswerable training questions greatly mitigate the hallucination issue (the model will answer \u201cI do not know\u201d to unanswerable questions), hallucination and bias may not be completely eliminated and may occur in some cases."
        },
        {
            "heading": "C EXTENDED DISCUSSION ABOUT THE CHOICE OF AUDIO ENCODER",
            "text": "In this work, we use an Audio Spectrogram Transformer (AST) pretrained (with the CAV-MAE objective) and finetuned on AudioSet-2M proposed in (Gong et al., 2022a) as our audio encoder. CAVMAE is a self-supervised pretraining framework that combines contrastive learning and masked data modeling. In the CAV-MAE pretraining stage, AST and a visual branch are pretrained jointly with both audio and visual data of AudioSet, but in the finetuning stage, only the audio branch (i.e., AST) is finetuned and only audio data of AudioSet is used. Therefore, our AST audio encoder only takes audio as input. The main reasons why we use AST in CAV-MAE instead of vanilla AST (Gong et al., 2021b) are performance and efficiency. Performance-wise, the AST in CAV-MAE leads to an mAP of 46.6 on the AudioSet evaluation set, which is better than vanilla AST (45.9 mAP); Efficiencywise, the AST in CAV-MAE splits the audio spectrogram into patches without overlapping and leads to a patch sequence length of 512 for each 10-second audio clip, which is much shorter than that of vanilla AST (1212). Due to the quadratic complexity of the Transformer, the AST in CAV-MAE is noticeably more efficient. In addition, due to the multi-modal CAV-MAE pretraining, we expect the representation of AST in CAV-MAE to be more semantic than vanilla AST. Note that while AST might be an optimal choice, any audio model can serve as the audio encoder of LTU as long as it can map audio input to a continuous embedding space and can be trained with backpropagation."
        },
        {
            "heading": "D EXTENDED DISCUSSION ON THE IMPACT OF UNANSWERABLE QUESTIONS",
            "text": "GPT naturally generates about 6.5% questions that do not have an answer based on the given audio meta information with the proposed AIG method. By default, we include them in the training set. As shown in Table 8, if we remove them from the training set, we find the close-ended performance would slightly drop, more importantly, it has a lower refusal rate to GPT-generated unanswerable questions, i.e., a higher risk of hallucination. Note that questions not answerable by GPT based on the audio meta information may be answerable by LTU based on the audio file, so this test is an estimation. This also shows an interesting phenomenon that even when there are almost no unanswerable QAs in the training data, LTU is still able to refuse to answer questions that it is not sure about, but with a lower refusal rate, i.e., this behavior seems to be partly learned in the LLM training stage."
        },
        {
            "heading": "E SCALING UP THE LARGE LANGUAGE MODEL",
            "text": "Throughout the paper, we use the LLaMA model consisting of 7 billion parameters (LLaMA-7B) as the LTU language model. In this section, we conduct experiments on LTU based on LLaMA with 13 billion parameters (LLaMA-13B). As shown in Table 9, we find the overall performance on audio classification slightly drops with the increased size of LLaMA while the captioning performance is slightly stronger. There is no obvious difference in the instruction following rate for pure language and audio open-ended questions. However, we find that the 13B model tends to output more rational answers due to its better knowledge base, as demonstrated in the following sample.\nQuestion: \u201cQuestion: How would the audio clip be different if the power tool was being used in a larger or open space?\u201d LTU-7B: \u201cLTU-7B: The sound of the power tool would likely be louder and more noticeable in an open space.\u201d (incorrect) LTU-13B: \u201cLTU-13B: If the power tool was being used in a larger or open space, the sound of the power tool would likely be more diffuse and less intense than it is in this clip.\u201d (correct)"
        },
        {
            "heading": "F DISCUSSION ON THE AUDIO AND TEXT EMBEDDING SPACE ALIGNMENT",
            "text": "Throughout the paper, we utilize the AST with CAV-MAE pretraining and fine-tuning as our audio encoder. Prior to LTU training, this audio encoder has not been trained for any audio-text alignment tasks; that is, its output embedding is not in the same space as text embeddings. The AST\u2019s output is used as soft prompts for the large language model. According to Lester et al. (2021); Driess et al. (2023), these (multimodal) soft prompts do not necessarily need to be in the text embedding space. However, it remains unclear whether audio-text alignment pretraining prior to LTU training could enhance performance. Therefore, in this section, we conduct further experiments to investigate the impact of audio-text alignment pretraining.\nSpecifically, we add an additional audio-text alignment training stage before LTU training. we test two types of alignment modules: one is the linear layer used in the original LTU, and the other is a Transformer layer, which allows for more model capacity in the alignment. We train the alignment module on 1.6 million audio-text pairs (comprising audio labels or captions) for 10 epochs, using a batch size of 1536 distributed over 4 GPUs, with 384 samples on each GPU. We start with an initial learning rate of 1e-3 and halve it after each epoch. During this stage, we keep the audio and LLaMA text encoders frozen and only train the alignment module. We use the following loss:\nL = Lc + \u03bb \u00b7 Lmse. (2)\nLmse = 1\nN N\u2211 i=1 (eai \u2212 eti)2 (3)\nLc = \u2212 1\nN N\u2211 i=1 log\n[ exp(si,i/\u03c4)\u2211\nk \u0338=i exp(si,k/\u03c4) + exp(si,i/\u03c4)\n] (4)\nwhere \u03bb = 10 is used to balance the scale between MSE loss and contrastive loss, et and ea are the text and audio embeddings, respectively, N = 384 is the batch size, si,j is the similarity score calculated as the dot product of normalized text embedding eti and audio embedding e a j , and \u03c4 = 0.05 is the temperature. This loss aligns both the scale and direction of the audio and text embeddings.\nWe present the results in Table 10 and observe that for the same linear projection layer, additional audio-text alignment training does not significantly impact model performance. Furthermore, scaling up the alignment module to a Transformer layer even worsens the performance. We hypothesize that this is attributable to several reasons:\n1. In the original LTU training, the model is trained with 5.6 million samples over multiple epochs, and the audio encoder is set as trainable for three stages of the training. Consequently, during the original LTU training process, the audio encoder is sufficiently trained to bridge the gap with the LLM, and additional alignment training is unnecessary.\n2. Increasing the capacity of the alignment module would introduce a larger number of randomly initialized parameters, complicating the training process. Considering that the AST audio encoder is a 12-layer Transformer and was set as trainable in the original LTU training, an increase in the capacity of the alignment module is not necessary.\n3. The audio embeddings serve as soft prompts (Lester et al., 2021; Driess et al., 2023) for LLMs and do not necessarily need to reside in the text embedding space. Therefore, additional audio-text training does not provide a better initialization for LTU training."
        },
        {
            "heading": "G COMPARE WITH PENGI",
            "text": "In this section, we compare LTU with Pengi (Deshmukh et al., 2023), a concurrent work on audio language models."
        },
        {
            "heading": "G.1 TECHNICAL SIMILARITY AND DIFFERENCE",
            "text": ""
        },
        {
            "heading": "Similarity",
            "text": "1. Architecture-wise, both Pengi and LTU connect an audio encoder with an autoregressive language model. Both models can generate text from the given audio and text prompt.\n2. Performance-wise, both Pengi and LTU can do zero-shot predictions on unseen datasets. On closed-ended tasks, Pengi and LTU perform similarly."
        },
        {
            "heading": "Difference",
            "text": "1. Motivation: Pengi focuses on transfer learning, i.e., using a single model for 8 audio perception tasks, while LTU focuses on unifying audio perception with understanding. Unlike LTU, Pengi is not designed for audio understanding and answering free-form open-ended questions from users.\n2. Language Model Scale: Pengi employs GPT2-Base (124M parameters) as its language model, whereas LTU uses LLaMA (7B parameters), which is over 50 times larger than GPT2-Base. Additionally, LLaMA is trained with significantly more data than GPT2. Scaling up the language model substantially enhances LTU\u2019s understanding capabilities."
        },
        {
            "heading": "G.2 CLOSED-ENDED TASK PERFORMANCE COMPARISON",
            "text": "ESC50 FSD50K DCASE TUT Beijing Opera Vocal Sound Average\nPengi 91.9 46.7 33.8 35.2 62.3 60.3 55.1 LTU (Default) 83.1 46.3 45.9 32.5 69.9 55.6 55.6 LTU (Full FT) 85.9 45.7 47.2 33.0 59.8 69.0 56.7\nWe summarize the performance of LTU and Pengi on closed-ended audio classification tasks in Table 11. Pengi and LTU each win on three out of the six tasks. In general, LTU with the default LoRA setting performs similarly to Pengi (average score 55.6 vs 55.1), while the fully finetuned version of LTU performs slightly better, achieving an average score of 56.7."
        },
        {
            "heading": "G.3 OPEN-ENDED TASK PERFORMANCE COMPARISON",
            "text": "We use GPT-4 (with the prompt \u201cBelow is a pair of question and response. Identify if the response answers the question. Return yes or no.\u201d) to evaluate the instruction following rate of LTU and Pengi in response to open-ended questions and present the results in Table 12. With a stronger language model and training with the OpenAQA dataset, LTU dramatically outperforms Pengi. However, it is important to note that Pengi is neither designed nor trained for open-ended question answering and audio understanding, making this task not fair to Pengi. We conduct this experiment just to highlight the differences between LTU and Pengi."
        },
        {
            "heading": "G.4 SUMMARY",
            "text": "To summarize, while LTU and Pengi share some similarities in architecture, they have different motivations and designs. They perform similarly in closed-ended audio classification tasks, but LTU is more capable of answering open-ended questions and demonstrating audio understanding."
        },
        {
            "heading": "H LTU TEMPORAL ANALYSIS EXPERIMENTS",
            "text": "In Section 5.1, we show the LTU model performance on closed-ended tasks of classification and captioning. In this section, we further evaluate LTU\u2019s capacity in temporal analysis. Specifically, we conduct experiments on the audio event order recognition task (seen in the closed-ended training stage) and audio event counting task (unseen in the closed-ended training stage) with ESC50 (Piczak, 2015), a dataset not being used in training. For both experiments, we only include the cases in which LTU follows the instruction."
        },
        {
            "heading": "H.1 RECOGNIZING THE ORDER OF SOUND EVENTS",
            "text": "We create a test set where each test sample consists of two audio samples from different categories of the ESC50 dataset, with no temporal overlapping. We then ask LTU to predict the order of the sound events with the prompt \u201cWhich sound begins and ends first?\u201d. We use regular expressions and cosine similarity (based on gpt-text-embedding-ada) to interpret the raw LTU output. For example, for the LTU raw output \u201cthe applause starts first, while the footsteps follow in a rhythmic pattern afterward.\u201d, we first extract \u201capplause\u201d and \u201cfootsteps follow in a rhythmic pattern\u201d using regular expressions. Then we compare the cosine similarity between the text embedding of the ground truth \u201cclapping\u201d with the text embeddings of \u201capplause\u201d and \u201cfootsteps follow in a rhythmic pattern\u201d, respectively. Since the cosine similarity between the text embedding of the ground truth \u201cclapping\u201d and the LTU prediction \u201capplause\u201d is higher, we count this sample as correct.\nWe evaluate LTU on 550 such samples, obtaining 390 correct and 160 incorrect answers, achieving an accuracy of 70.9%. This shows that LTU can recognize the order of sound events with a reasonable accuracy."
        },
        {
            "heading": "H.2 COUNTING THE NUMBER OF APPEARANCES OF SOUND EVENTS",
            "text": "In Table 6, the first sample shows a demo where LTU recognizes the bell rings three times. In this section, we further quantify LTU\u2019s capacity in counting the number of appearances of sound events. Note that this task is not in our closed-ended training. One problem in evaluating this task is that the definition of the number of appearances is not clear for all sound classes, e.g., it is hard to define what the appearance of a train sound is as it could consist of multiple click-clack sounds. Thus, we create a test set where each test sample consists of one or two (repeated) audio samples from the\nESC50 dataset, with no temporal overlapping. We then ask LTU to count the sounds in the audio clips using the prompt, \u201cIn how many instances is the sound of {:s} heard?\u201d. Since one ESC50 audio sample may contain sounds multiple times (e.g., three dog barking sounds), we calculate the Pearson Correlation Coefficient (PCC) between the LTU output count and the number of ESC50 audio clips in the test sample. As shown in Table 13, we find a high correlation for many sound classes. Additionally, we calculate the ratio between LTU output counts for test samples consisting of two ESC50 samples and test samples consisting of one ESC50 sample. Ideally, this value should be 2 because the number of appearances of sounds in two ESC50 samples should be twice the number of appearances of sounds in one ESC50 sample. We find it is indeed close to 2 for many sound classes. However, there are some sound classes that LTU cannot count well, potentially due to an undefined number of sound appearances for these classes. This demonstrates the sound event counting capability of LTU, although it has not been explicitly trained for this task in the closedended training stage."
        },
        {
            "heading": "H.3 SUMMARY",
            "text": "To summarize, LTU demonstrates reasonable ability in temporal analysis. However, compared with classification and captioning tasks, its temporal analysis ability is weak. We hypothesize that this is due to two reasons: First, we aggressively pool the audio tokens to save computation, which may hurt fine-grained tasks; Second, LTU has not been trained with a sufficient amount of data for temporal analysis, as reported in Peng et al. (2023b), such training is crucial for fine-grained tasks."
        },
        {
            "heading": "I OPEN-ENDED EVALUATION DETAILS",
            "text": ""
        },
        {
            "heading": "I.1 HUMAN SUBJECTIVE EVALUATION",
            "text": "As we mentioned in the main manuscript, we conducted a two-stage human evaluation through Amazon Mechanical Turk (AMT) with a total of 476 unique human evaluators independent of us. In Stage 1, we evaluate LTU with GPT-4 generated open-ended questions, we play the audio and show the human evaluators the question and LTU answer and ask them to rate 1) if the answer directly addresses the question (instruction following); 2) if LTU answer is factually correct; 3) between LTU and GPT-4 answers, which is better? Finally, we ask the human evaluators to ask another question and provide the answer based on the audio, which are used in Stage 2. Stage 2 is the same as Stage 1 except we use human-generated QAs. The actual questionnaire is shown in Figure 2.\nTo better control the quality of human subjective evaluation, we 1) only allow AMT workers that have a pass rate over 90% to participate in our human evaluation, which means the workers have a good record in AMT tasks; 2) pay the workers with a higher rate than similar tasks; 3) set verifications to make sure the human-generated question and answers are longer than 5 words, and filtering out bad question/answers that are clear copy/paste.\nThe evaluation result shows human evaluators think LTU answers are better than human-generated answers (crafted by another independent human subject after listening to the audio), which may be quite surprising. In our observation, LTU answers are usually more complete and detailed than human answers, which is the reason why they are rated better. We want to highlight that consistency and endurance are advantages of LTU as a machine. Below is an example:\nQuestion: \u201cWhat type of sound is it?\u201d LTU Answer: \u201cThe audio clip contains the sounds of a train, clickety-clack, and steam whistle blowing at a high pitch.\u201d Human Answer: \u201cIt was train moving sound.\u201d"
        },
        {
            "heading": "I.2 GPT-ASSISTED EVALUATION",
            "text": "As a supplement to human subjective evaluation, to quantitatively evaluate how well LTU follows the instruction, we follow (Peng et al., 2023a) to evaluate it with GPT assistance. Specifically, we first use GPT-3.5-Turbo and GPT-4 (GPT-4 is not used in generating the training data) to generate 1,000 new questions based on unseen audios sampled from the evaluation split of the stronglylabeled AudioSet using AIG as described in Section 3.2, respectively. We then input these questions\nand corresponding audios to LTU and let it produce the answer. Finally, we use GPT-4 to evaluate if the LTU output answers the question by the prompt \u201cBelow is a pair of question and response. Identify if the response answers the question. Return yes or no.\u201d As shown in Table 14, LTU has an instruction following rate over 95% for questions generated by both GPT-3.5-Turbo and GPT-4. In contrast, the model trained with only closed-ended tasks cannot follow instructions well (22.5%)."
        },
        {
            "heading": "J EXISTING CAPTIONING BENCHMARK ISSUE IN MEASURING LTU",
            "text": "Existing audio captioning metrics are not optimal for models trained on diverse datasets with large vocabulary like LTU as they do not count synonyms as correct. We show two sample LTU outputs that are semantically correct but get 0 SPICE scores due to not hitting the keywords in the 5 ground truths of each audio. For this reason, we mainly use classification benchmarks in the ablation studies."
        },
        {
            "heading": "K ACOUSTIC FEATURE LEARNING AND DECOMPOSITION",
            "text": "Among our closed-ended tasks, one task is asking the model about the acoustic features. This task aims to train the model to recognize low-level features for better generalization. Original audio datasets do not have this information. We thus generate the acoustic feature description using GPT-3.5-Turbo with the prompt \u201cdescribe the acoustic characteristic of {sound class name} sound precisely with a sentence less than 10 words\u201d. We generate 10 different descriptions for each sound class. The question for this task is \u201cclassify the sound events in the audio clip based on acoustic features\u201d and its GPT-assisted paraphrases. The answer is a list of GPT-3.5-Turbo-generated acoustic features and the sound class names.\nAs shown in Table 16, most generated acoustic features are indeed low-level descriptors. More importantly, we find acoustic concepts usually appear in pairs, e.g., \u201chigh pitched\u201d appears in the descriptions of 243 audio classes, while its opposite concept \u201clow pitched\u201d is in the descriptions of 77 classes. This allows LTU to learn better about the features.\nTo further check if LTU indeed learns acoustic concepts rather than just simply associates the acoustic concept with a sound class, we conduct a probe test. In our training data, samples of the \u201cambulance siren\u201d class are always described as \u201chigh pitched\u201d. To check if LTU can disentangle the concept of a high-pitch and sound class of ambulance sirens. We manually lower the pitch (librosa.pitch shift) of 53 evaluation audios of the \u201cambulance siren\u201d class and check LTU\u2019s output to the question \u201cWhat is the pitch?\u201d on these audios. As shown in Figure 2, LTU\u2019s prediction aligns well with the actual pitch, indicating it indeed learns the concept of pitch rather than just associating it with a specific sound class."
        },
        {
            "heading": "K.1 SAMPLE ACOUSTIC FEATURES",
            "text": "We present GPT-3.5-Turbo generated acoustic descriptions for 10 majority sound classes and 10 minority sound classes in AudioSet in Table 17. In general, these feature descriptions are factually correct and informative for both common and rare sounds."
        },
        {
            "heading": "L TRAINING DATA DIVERSITY",
            "text": "The proposed OpenAQA is very diverse. In Figure 3 we show the histogram of question and answer occurrences in the 3.7M open-ended OpenAQA dataset. Over 95% of questions and answers appear only once in the dataset, demonstrating the diversity of the dataset."
        },
        {
            "heading": "M FULL GPT-3.5-TURBO PROMPT AND SAMPLE OUTPUTS",
            "text": "In Table 2, we show the prompt for GPT-3.5-Turbo for generating AQAs. Due to space limitations, we remove some formatting instructions. Below is the full prompt we have used:\n\u201cBased on the following audio clip, generate 10 different types of complex open-ended questions that require step-by-step thinking, and corresponding step-by-step answers. The following information is provided: the sound events appear in the audio clip, together with its acoustic features, and corresponding onset and offset time stamps. A description of the content of the audio clip is also provided. Questions should be about the audio, e.g., which sound event is recognized and why (e.g., based on its acoustic feature), what can be inferred based on the combination of sound events; the temporal relationship between the sound events and what can be inferred from that; the potential scenario that such an audio clip could happen, if the audio clip is special (e.g., urgent, funny, interesting, abnormal, unique, etc) and why, what mood or atmosphere this audio clip conveys, etc. The more complex and diverse the question, the better. Format each QA pair in a single line as a JSON dictionary (key \u201cq\u201d for question, and \u201ca\u201d for answer, wrapped with { and }). Do not include any other explanation.\u201d\nThe output of GPT-3.5-Turbo is in the form of a JSON dictionary, as shown in the following example:\n[{ \"q\": \"How would you describe the tone of the sound of the\naccelerating engine?\", \"a\": \"The tone of the sound of the accelerating engine is\nhigh-pitched, short and intense.\" }, ... {\n\"q\": \"What is the acoustic feature that distinguishes the sound of the ambulance siren from the generic impact sounds?\", \"a\": \"The acoustic feature that distinguishes the sound of the ambulance siren from the sound of generic impact sounds is that the former is high-pitched and wailing, while the latter is loud and sharp.\"\n}]"
        },
        {
            "heading": "N LOSS CURVE",
            "text": "We show the training loss curve of various training settings of Training Stage 3 (the last training) in Figure 4. While full-finetuned models (i.e., set LLaMA trainable) achieve noticeably lower losses compared to LoRA models, as shown in Table 5, they only perform slightly better on audio tasks. In addition, they dramatically lose the original language reasoning ability."
        },
        {
            "heading": "O MORE LTU DEMO SAMPLES",
            "text": "Due to space limitations, we are only able to present a limited number of demos. In Table 18, we show additional demos of LTU."
        },
        {
            "heading": "P DATASET DETAILS",
            "text": ""
        },
        {
            "heading": "P.1 TRAINING DATASETS",
            "text": "As we mentioned in the main manuscript, we relabel existing public datasets including AudioSet (including a 500K subset of the original 2M weakly-labeled release (Gemmeke et al., 2017) and the 100K subset with temporally-strong labels (Hershey et al., 2021)), VGGSound (Chen et al., 2020a), FSD50K (Fonseca et al., 2021), AudioCaps (Kim et al., 2019), Freesound (Font et al., 2013), Clotho v2 (Lipping et al., 2019), and Sound Bible (soundbible.com, 2006) as our training data. For all these datasets, we only include data marked as training and validation samples and exclude any data marked as test or evaluation. A total of 845K unique audio clips are used. In the main manuscript, we discuss the relabeling method (the novel method) in detail but just briefly mention the source datasets, below we introduce them and our preprocessing method in detail.\nAudioSet-2M (Gemmeke et al., 2017)\nAudioSet-2M is the original release of AudioSet, it is a collection of 2 million 10-second audio clips excised from YouTube videos and labeled with the sounds that the clip contains from a set of 527 labels. AudioSet-2M is a weakly labeled and multi-label dataset, i.e., labels are given to a clip with no indication of the onset and offset timestamps, and every clip may have multiple labels.\nWhile we would like to include more audio, from the audio source diversity consideration, we do not want audio from a single source to dominate the OpenAQA dataset. Therefore, we sample a 500K subset from AudioSet-2M. Specifically, we sample the audio using the following algorithm. First, we calculate the weight of each sound class k as wk = 1/number of samples labeled as the class, i.e., uncommon sound classes have a higher weight. Then, we calculate the weight of each audio\nsample a as wa = \u2211\nk\u2208K wk, where K is the labels associated with audio a, i.e., audio samples containing more and uncommon sound events have a higher weight. Finally, we select the top 500K audio samples that have higher weights. Due to the constant change in Youtube video availability (e.g., videos being removed, or taken down), there is a natural shrinkage from the original dataset, so the 500K samples are selected from the 1,772,023 audio clips we were able to download.\nTemporally-Strongly Labeled AudioSet (Hershey et al., 2021)\nTemporally-Strongly Labeled AudioSet is a subset of the original AudioSet-2M with additional annotations. Specifically, the start and end times of each event are also annotated. It consists of 934,821 sound events across the 103,463 10-second audio clips. The label set of temporallystrongly labeled AudioSet is slightly different from the original AudioSet label set (the label set of temporally-strongly labeled AudioSet contains 447 labels, of which 376 are shared with the original 527-class label set).\nSince temporally-strongly labeled AudioSet contains rich annotations, we include the entire training set in OpenAQA. Similar to the original AudioSet, there is a natural shrinkage due to some of the Youtube videos being unavailable. We use 101,791 audio clips that we were able to download (among them, 91,075 clips are captioned by the WavCaps project (Mei et al., 2023)).\nVGGSound (Chen et al., 2020a) VGGSound is a collection of 200K 10-second audio clips excised from YouTube videos annotated with 309 classes. Different from AudioSet, each VGGSound audio sample contains only one audio event. We use 183,727 training audio clips that we were able to download.\nFSD50K (Fonseca et al., 2021) FSD50K contains 37,134 audio clips for training and 4,170 audio clips for validation from the Freesound project (Font et al., 2013). The audio clips are unequally distributed in 200 sound classes drawn from the AudioSet ontology. We use both the training and validation set, a total of 41,304 audio samples to train LTU.\nAudioCaps (Kim et al., 2019) AudioCaps is a subset of AudioSet with additional human-written audio caption annotations collected via crowdsourcing. The training and validation set contains 49,838 and 495 audio clips, respectively. Each training audio clip is annotated with 1 caption and each validation audio clip is annotated with 5 captions. We use both the training and validation set, a total of 46,462 audio clips and 48,298 captions that we were able to download.\nFreesound(Font et al., 2013) Freesound is an online collaborative sound-sharing project launched in 2005 with more than 560K audio clips. In this paper, we use 91,434 audio clips that are shorter than 10 seconds and captioned by the WavCaps project (Mei et al., 2023).\nClotho V2 (Lipping et al., 2019) Clotho is an audio caption dataset with audio sampled from Freesound and captions collected by Amazon Mechanical Turk. Each audio is annotated with 5 captions. We use 3,741 audio clips from the development set and 1045 audio clips from the validation set to train LTU.\nSound Bible (soundbible.com, 2006) SoundBible is a website sharing sound effects and audio clips. We use 1,225 audio clips that are captioned by the WavCaps project (Mei et al., 2023)."
        },
        {
            "heading": "P.2 EVALUATION DATASETS AND PROTOCOL",
            "text": ""
        },
        {
            "heading": "P.2.1 BEST SUPERVISED AND SPECIALIZED MODELS IN TABLE 4",
            "text": "Due to the space limitation, we only show the results of the best supervised and specialized models in Table 4. These models are Chen et al. (2022) (ESC-50), Kong et al. (2020) (DCASE), Elizalde et al. (2023) (VocalSound and TUT), Cramer et al. (2019) (Beijing Opera), Gong et al. (2022a) (VGGSound), Gong et al. (2021a) (FSD50K), Huang et al. (2022) (AudioSet), Kim et al. (2022) (AudioCaps), and Mei et al. (2022) (Clotho V2)."
        },
        {
            "heading": "P.2.2 ZERO-SHOT EVALUATION",
            "text": "VocalSound (Gong et al., 2022b) VocalSound is a dataset consisting of 21,024 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. We evaluate LTU on the VocalSound evaluation set consisting of 3,594 audio clips and report the 6-class\nclassification top-1 accuracy. Note that VocalSound is completely independent of the LTU training data, therefore it is a zero-shot evaluation.\nTUT 2017 (Mesaros et al., 2016) TUT 2017 is a dataset consisting of 10-second audio segments from 15 acoustic scenes recorded from distinct locations. In the evaluation set, each of the 15 acoustic scenes has 108 segments and the total number of evaluation samples is 1,620. We evaluate LTU on the evaluation set of TUT 2017 and report the top-1 accuracy. Note that TUT 2017 is completely independent of the LTU training data, therefore it is a zero-shot evaluation.\nBeijing Opera (Tian et al., 2014) Beijing Opera is an instrument classification dataset comprising 4 percussion instruments: \u201cbangu\u201d: \u201cclapper-drum\u201d, \u201cnaobo\u201d: \u201ccymbals\u201d, \u201cdaluo\u201d: \u201clarge gong\u201d, and \u201cxiaoluo\u201d: \u201csmall gong\u201d. We use \u201cclapper-drum\u201d, \u201ccymbals\u201d, \u201clarge gong\u201d, and \u201csmall gong\u201d as the name of labels. We evaluate LTU on the entire 236 Beijing Opera audio samples and report the top-1 accuracy. Note that Beijing Opera is completely independent of the LTU training data, therefore it is a zero-shot evaluation."
        },
        {
            "heading": "P.2.3 WEAK ZERO-SHOT EVALUATION",
            "text": "ESC-50 (Piczak, 2015) The ESC-50 dataset consists of 2,000 5-second environmental audio recordings organized into 50 classes. The standard evaluation protocol is 5-fold cross-validation, i.e., using 1,600 samples to train the model and using the rest 400 samples to evaluate the model. We do not do any training on ESC-50, instead, we directly evaluate LTU on the entire 2,000 audio clips and report the top-1 accuracy. Note that though we do not mix ESC-50 in our training set, ESC-50 is sampled from the Freesound project, which is also the source of part of our training data, we, therefore, call this a weak zero-shot setting.\nDCASE2017 Task 4 (Mesaros et al., 2017) DCASE 2017 Task 4 consists of audio clips of 17 sound events divided into two categories: \u201cWarning\u201d and \u201cVehicle\u201d. We evaluate LTU on the evaluation set of DCASE 2017 Task 4 consisting of 1,350 audio samples and report the micro F1-score. Note that though we do not mix DCASE 2017 Task 4 in our training set, it is sampled from the AudioSet project, which is also the source of part of our training data, we, therefore, call this a weak zero-shot setting.\nP.2.4 IN-DOMAIN EVALUATION\nEven for a dataset whose training split has been used to train LTU, due to the multi-dataset training setting and the free-form output nature of LTU, the prediction search space of LTU is still much larger than conventional models trained solely on a single dataset. In addition, LTU does not use any dataset-specific training tricks (Gong et al., 2021a; Moore et al., 2023). Therefore, it is not exactly fair to compare LTU with dataset-specialized models.\nVGGSound (Chen et al., 2020a) The training split of VGGSound is part of the LTU training data. We evaluate LTU on the VGGSound evaluation set consisting of 15,446 audio samples and report the top-1 accuracy.\nFSD50K (Fonseca et al., 2021) The training and validation split of FSD50K is part of the LTU training data. We evaluate LTU on the FSD50K evaluation set consisting of 10,231 audio samples and report the mean average precision (mAP).\nAudioSet (Gemmeke et al., 2017) The training split of AudioSet is part of the LTU training data. We evaluate LTU on the AudioSet evaluation set consisting of 17,249 audio samples and report the mean average precision (mAP).\nAudioCaps (Kim et al., 2019) The training and validation split of AudioCaps is part of the LTU training data. We evaluate LTU on the AudioCaps evaluation set consisting of 901 audio clips and 4,505 captions and report the SPICE score. It is worth noting that though AudioCaps are annotated by crowd workers (humans), the AudioSet labels are used as \u201cword hints\u201d to the worker in the annotation process, so the vocabulary of the captions may be relatively limited. This means a model trained solely or mainly on the dataset may fit better with its vocabulary and get a higher score on the evaluation set while a general model that has a larger vocabulary may have a lower score because the evaluation metrics like SPICE and CIDER view synonyms as a wrong prediction.\nClotho V2 (Drossos et al., 2020) The development and validation split of Clotho is part of the LTU training data. We evaluate LTU on the Clotho evaluation set consisting of 1,045 audio clips and 5,225 captions and report the SPICE score. Compared with AudioCaps (Kim et al., 2019), Clotho does not provide ground truth labels as \u201cword hints\u201d to the annotators, and more rigorous screening and sanitation are conducted. For example, text format is manually corrected for consistency (e.g., replacing \u201cit\u2019s\u201d with \u201cit is\u201d), unique words are rephrased to make sure they appear in both training and evaluation sets, the length of the caption is controlled to be between 8 to 20, etc. Thus model trained solely or mainly on the dataset may fit better with the vocabulary and style and get a higher score on the evaluation set while a general model that has a larger vocabulary and more free-form output may have a lower score because the evaluation metrics like SPICE and CIDER view synonyms as wrong prediction, and its output may be too brief (less than 8 words) or too specific (over 20 words) compared with the ground truth caption."
        }
    ],
    "year": 2024
}