{
    "abstractText": "We propose a novel random walk-based algorithm for unbiased estimation of arbitrary functions of a weighted adjacency matrix, coined general graph random features (g-GRFs). This includes many of the most popular examples of kernels defined on the nodes of a graph. Our algorithm enjoys subquadratic time complexity with respect to the number of nodes, overcoming the notoriously prohibitive cubic scaling of exact graph kernel evaluation. It can also be trivially distributed across machines, permitting learning on much larger networks. At the heart of the algorithm is a modulation function which upweights or downweights the contribution from di erent random walks depending on their lengths. We show that by parameterising it with a neural network we can obtain g-GRFs that give higher-quality kernel estimates or perform e cient, scalable kernel learning. We provide robust theoretical analysis and support our findings with experiments including pointwise estimation of fixed graph kernels, solving non-homogeneous graph ordinary di erential equations, node clustering and kernel regression on triangular meshes.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Isaac Reid1\u00fa"
        },
        {
            "affiliations": [],
            "name": "Krzysztof Choromanski"
        },
        {
            "affiliations": [],
            "name": "3\u00fa"
        },
        {
            "affiliations": [],
            "name": "Eli Berger4\u00fa"
        },
        {
            "affiliations": [],
            "name": "Adrian Weller"
        }
    ],
    "id": "SP:3767835505529c3801b0a1c2cb9bd86b57f936be",
    "references": [
        {
            "authors": [
                "Zhaojun Bai",
                "James Demmel",
                "Jack Dongarra",
                "Axel Ruhe",
                "Henk van der Vorst"
            ],
            "title": "Templates for the solution of algebraic eigenvalue problems: a practical guide",
            "year": 2000
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Shahar Mendelson"
            ],
            "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
            "venue": "Journal of Machine Learning Research,",
            "year": 2002
        },
        {
            "authors": [
                "Karsten M Borgwardt",
                "Cheng Soon Ong",
                "Stefan Sch\u00f6nauer",
                "SVN Vishwanathan",
                "Alex J Smola",
                "Hans-Peter Kriegel"
            ],
            "title": "Protein function prediction via graph kernels",
            "venue": "Bioinformatics, 21(suppl_1):i47\u2013i56,",
            "year": 2005
        },
        {
            "authors": [
                "Colin Campbell"
            ],
            "title": "Kernel methods: a survey of current techniques",
            "venue": "doi: 10.1016/S0925-2312(01)00643-9. URL https://doi.org/10.1016/",
            "year": 2002
        },
        {
            "authors": [
                "St\u00e9phane Canu",
                "Alexander J. Smola"
            ],
            "title": "Kernel methods and the exponential family",
            "venue": "Neurocomputing, 69(7-9):714\u2013720,",
            "year": 2006
        },
        {
            "authors": [
                "Olivier Chapelle",
                "Jason Weston",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Cluster kernels for semi-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2002
        },
        {
            "authors": [
                "Krzysztof Choromanski",
                "Valerii Likhosherstov",
                "David Dohan",
                "Xingyou Song",
                "Andreea Gane",
                "Tamas Sarlos",
                "Peter Hawkins",
                "Jared Davis",
                "Afroz Mohiuddin",
                "Lukasz Kaiser"
            ],
            "title": "Rethinking attention with performers",
            "venue": "arXiv preprint arXiv:2009.14794,",
            "year": 2020
        },
        {
            "authors": [
                "Krzysztof M Choromanski",
                "Mark Rowland",
                "Adrian Weller"
            ],
            "title": "The unreasonable e ectiveness of structured random orthogonal embeddings",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Krzysztof Marcin Choromanski"
            ],
            "title": "Taming graph kernels with random features",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Fan R.K. Chung",
                "Shing-Tung Yau"
            ],
            "title": "Coverings, heat kernels and spanning trees",
            "venue": "Electron. J. Comb.,",
            "year": 1999
        },
        {
            "authors": [
                "Corinna Cortes",
                "Mehryar Mohri",
                "Afshin Rostamizadeh"
            ],
            "title": "Generalization bounds for learning kernels",
            "venue": "In Proceedings of the 27th Annual International Conference on Machine Learning",
            "year": 2010
        },
        {
            "authors": [
                "Keenan Crane",
                "Clarisse Weischedel",
                "Max Wardetzky"
            ],
            "title": "The heat method for distance computation",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Anirban Dasgupta",
                "Ravi Kumar",
                "Tam\u00e1s Sarl\u00f3s"
            ],
            "title": "A sparse johnson: Lindenstrauss transform",
            "venue": "Proceedings of the 42nd ACM Symposium on Theory of Computing,",
            "year": 2010
        },
        {
            "authors": [
                "Michael Dawson-Haggerty"
            ],
            "title": "URL https://github.com/mikedh/ trimesh",
            "venue": "Trimesh repository,",
            "year": 2023
        },
        {
            "authors": [
                "Inderjit S Dhillon",
                "Yuqiang Guan",
                "Brian Kulis"
            ],
            "title": "Kernel k-means: spectral clustering and normalized cuts",
            "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2004
        },
        {
            "authors": [
                "Michel X. Goemans",
                "David P. Williamson"
            ],
            "title": "Approximation algorithms for max-3-cut and other problems via complex semidefinite programming",
            "venue": "J. Comput. Syst. Sci.,",
            "year": 2004
        },
        {
            "authors": [
                "Vladimir Ivashkin"
            ],
            "title": "Community graphs repository, 2023",
            "venue": "URL https://github.com/ vlivashkin/community-graphs",
            "year": 2023
        },
        {
            "authors": [
                "William B Johnson"
            ],
            "title": "Extensions of lipschitz mappings into a hilbert space",
            "venue": "Contemp. Math.,",
            "year": 1984
        },
        {
            "authors": [
                "Kyle Kloster",
                "David F Gleich"
            ],
            "title": "Heat kernel based community detection",
            "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2014
        },
        {
            "authors": [
                "Vladimir Koltchinskii",
                "Dmitry Panchenko"
            ],
            "title": "Empirical margin distributions and bounding the generalization error of combined classifiers",
            "venue": "The Annals of Statistics,",
            "year": 2002
        },
        {
            "authors": [
                "Risi Kondor",
                "John D. La erty"
            ],
            "title": "Di usion kernels on graphs and other discrete input spaces",
            "venue": "Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002),",
            "year": 2002
        },
        {
            "authors": [
                "Leonid Kontorovich",
                "Corinna Cortes",
                "Mehryar Mohri"
            ],
            "title": "Kernel methods for learning languages",
            "venue": "Theor. Comput. Sci.,",
            "year": 2008
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Isaac Reid",
                "Krzysztof Choromanski",
                "Adrian Weller"
            ],
            "title": "Quasi-monte carlo graph random features",
            "venue": "arXiv preprint arXiv:2305.12470,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander J Smola",
                "Risi Kondor"
            ],
            "title": "Kernels and regularization on graphs",
            "venue": "In Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop,",
            "year": 2003
        },
        {
            "authors": [
                "Alexander J. Smola",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Bayesian kernel methods",
            "venue": "Advanced Lectures on Machine Learning, Machine Learning Summer School",
            "year": 2002
        },
        {
            "authors": [
                "Yasutoshi Yajima"
            ],
            "title": "One-class support vector machines for recommendation tasks. In PacificAsia Conference on Knowledge Discovery and Data Mining, pages 230\u2013239",
            "year": 2006
        },
        {
            "authors": [
                "Felix Xinnan X Yu",
                "Ananda Theertha Suresh",
                "Krzysztof M Choromanski",
                "Daniel N Holtmann-Rice",
                "Sanjiv Kumar"
            ],
            "title": "Orthogonal random features",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Yufan Zhou",
                "Changyou Chen",
                "Jinhui Xu"
            ],
            "title": "Learning manifold implicitly via explicit heatkernel learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction and related work",
            "text": "The kernel trick is a powerful technique to perform nonlinear inference using linear learning algorithms (Campbell, 2002; Kontorovich et al., 2008; Canu and Smola, 2006; Smola and Sch\u00f6lkopf, 2002). Supposing we have a set of N datapoints X = {xi}Ni=1, it replaces Euclidean dot products x\u20ac\ni xj with evaluations of a kernel function K : X \u25ca X \u00e6 R, cap-\nturing the \u2018similarity\u2019 of the datapoints by instead taking an inner product between implicit (possibly infinite-dimensional) feature vectors in some Hilbert space HK . An object of key importance is the Gram matrix K \u0153 RN\u25caN whose entries enumerate the pairwise kernel evaluations, K := [K(xi,xj)]Ni,j=1. Despite the theoretical rigour and empirical success enjoyed by kernel-based learning algorithms, the requirement to manifest and invert this matrix leads to notoriously poor O(N3) time-complexity scaling. This has spurred research dedicated to e ciently approximating K, the chief example of which is random features (Rahimi and Recht, 2007): a Monte-Carlo approach which gives explicitly manifested, finite dimensional vectors \u201e(xi) \u0153 Rm whose Euclidean dot product is equal to the kernel evaluation in expectation,\nKij = E # \u201e(xi)\u20ac\u201e(xj) $ . (1)\nThis allows one to construct a low-rank decomposition of K which provides much better scalability. Testament to its utility, a rich taxonomy of random features exists to approximate many di erent Euclidean kernels including the Gaussian, softmax, and angular and linear kernels (Johnson, 1984; Dasgupta et al., 2010; Goemans and Williamson, 2004; Choromanski et al., 2020). Kernels defined on discrete input spaces, e.g. K : N \u25ca N \u00e6 R with N the set of nodes of a graph G (Smola and Kondor, 2003; Kondor and La erty, 2002; Chung and Yau, 1999),\n\u00fa Equal contribution. 1 Code is available at https://github.com/isaac-reid/general_graph_random_features.\nenjoy widespread applications including in bioinformatics (Borgwardt et al., 2005), community detection (Kloster and Gleich, 2014) and recommender systems (Yajima, 2006). More recently, they have been used in applications as diverse as manifold learning for deep generative modelling (Zhou et al., 2020) and for solving single- and multiple-source shortest path problems (Crane et al., 2017). However, for these graph-based kernel methods the problem of poor scalability is particularly acute. This is because even computing the corresponding Gram matrix K is typically of at least cubic time complexity in the number of nodes N , requiring e.g. the inversion of an N \u25ca N matrix or computation of multiple matrix-matrix products. Despite the presence of this computational bottleneck, random feature methods for graph kernels have proved elusive. Indeed, only recently was a viable graph random feature (GRF) mechanism proposed by Choromanski (2023). Their algorithm uses an ensemble of random walkers which deposit a \u2018load\u2019 at every vertex they pass through that depends on i) the product of weights of edges traversed by the walker and ii) the marginal probability of the subwalk. Using this scheme, it is possible to construct random features {\u201e(i)}N\ni=1 \u00b5 R N\nsuch that \u201e(i)\u20ac\u201e(j) gives an unbiased approximation to the ij-th matrix element of the 2-regularised Laplacian kernel. Multiple independent approximations can be combined to estimate the d-regularised Laplacian kernel with d \u201d= 2 or the di usion kernel (although the latter is only asymptotically unbiased). The GRFs algorithm enjoys both subquadratic time complexity and strong empirical performance on tasks like k-means node clustering, and it is trivial to distribute across machines when working with massive graphs. However, a key limitation of GRFs is that they only address a limited family of graph kernels which may not be suitable for the task at hand. Our central contribution is a simple modification which generalises the algorithm to arbitrary functions of a weighted adjacency matrix, allowing e cient and unbiased approximation a much broader class of graph node kernels. We achieve this by introducing an extra modulation function f that controls each walker\u2019s load as it traverses the graph. As well as empowering practitioners to approximate many more fixed kernels, we demonstrate that f can also be parameterised by a neural network and learned. We use this powerful approach to optimise g-GRFs for higher-quality approximation of fixed kernels and for scalable implicit kernel learning. The remainder of the manuscript is organised as follows. In Sec. 2 we introduce general graph random features (g-GRFs) and prove that they enable scalable approximation of arbitrary functions of a weighted adjacency matrix, including many of the most popular examples of kernels defined on the nodes of a graph. We also extend the core algorithm with neural modulation functions, replacing one component of the g-GRF mechanism with a neural network, and derive generalisation bounds for the corresponding class of learnable graph kernels (Sec. 2.1). In Sec. 3 we run extensive experiments, including: pointwise estimation of a variety of popular graph kernels (Sec. 3.1); simulation of time evolution under non-homogeneous graph ordinary di erential equations (Sec. 3.2); kernelised k-means node clustering including on large graphs (Sec. 3.3); training a neural modulation function to suppress the mean square error of fixed kernel estimates (Sec 3.4); and training a neural modulation function to learn a kernel for node attribute prediction on triangular mesh graphs (Sec. 3.5)."
        },
        {
            "heading": "2 General graph random features",
            "text": "Consider a directed weighted graph G(N , E , W := [wij ]i,j\u0153N ) where: i) N := {1, ..., N} is the set of nodes; ii) E is the set of edges, with (i, j) \u0153 E if there is a directed edge from i to j in G; and iii) W is the weighted adjacency matrix, with wij the weight of the directed edge from i to j (equal to 0 if no such edge exists). Note that an undirected graph can be described as directed with the symmetric weighted adjacency matrix W. Now consider the matrices K\u21b5(W) \u0153 RN\u25caN , where \u21b5 = (\u2013k)\u0152k=0 and \u2013k \u0153 R:\nK\u21b5(W) = \u0152\u00ff\nk=0 \u2013kWk. (2)\nWe assume that the sum above converges for all W under consideration, which can be ensured with a regulariser W \u00e6 \u2021W, \u2021 \u0153 R+. Without loss of generality, we also assume\nthat \u21b5 is normalised such that \u20130 = 1. The matrix K\u21b5(W) can be associated with a graph function KG\u21b5 : N \u25ca N \u00e6 R mapping from a pair of graph nodes to a real number. Note that if G is an undirected graph then K\u21b5(W) automatically inherits the symmetry of W. In this case, it follows from Weyl\u2019s perturbation inequality (Bai et al., 2000) that K\u21b5(W) is positive semidefinite for any given \u21b5 provided the spectral radius fl(W) := max\u2044\u0153 (W) (|\u2044|) is su ciently small (with (W) the set of eigenvalues of W). This can again be ensured by multiplying the weight matrix W by a regulariser \u2021 \u0153 R+. It then follows that K\u21b5(W) can be considered the Gram matrix of a graph kernel function KG\u21b5. With suitably chosen \u21b5 = (\u2013k)\u0152k=0, the class described by Eq. 2 includes many popular examples of graph node kernels in the literature (Smola and Kondor, 2003; Chapelle et al., 2002). They measure connectivity between nodes and are typically functions of the graph Laplacian matrix, defined by L := I\u2260 \u00caW with \u00caW := [wij/  didj ]Ni,j=1. Here, di := q j\nwij is the weighted degree of node i such that \u00caW is the normalised weighted adjacency matrix. For reference, Table 1 gives the kernel definitions and normalised coe cients \u2013k (corresponding to powers of \u00caW) to be considered later in the manuscript. In practice, factors in \u2013k equal to a quantity raised to the power of k are absorbed into the normalisation of \u00caW.\nThe chief goal of this work is to construct a random feature map \u201e(i) : N \u00e6 Rl with l \u0153 N that provides unbiased approximation of K\u21b5(W) as in Eq. 1. To do so, we consider the following algorithm.\nAlgorithm 1 Constructing a random feature vector \u201ef (i) \u0153 RN to approximate K\u21b5(W) Input: weighted adjacency matrix W \u0153 RN\u25caN , vector of unweighted node degrees (no. neighbours) d \u0153 RN , modulation function f : (N fi {0}) \u00e6 R, termination probability phalt \u0153 (0, 1), node i \u0153 N , number of random walks to sample m \u0153 N. Output: random feature vector \u201ef (i) \u0153 RN 1: initialise: \u201ef (i) \u03a9 0 2: for w = 1, ..., m do 3: initialise: load \u03a9 1 4: initialise: current_node \u03a9 i 5: initialise: terminated \u03a9 False 6: initialise: walk_length \u03a9 0 7: while terminated = False do 8: \u201ef (i)[current_node] \u03a9 \u201ef (i)[current_node]+load\u25caf (walk_length ) 9: walk_length \u03a9 walk_length+1 10: new_node \u03a9 Unif [N (current_node )] \u00db assign to one of neighbours 11: load \u03a9 load\u25ca d[current_node]1\u2260phalt \u25ca W [current_node,new_node ] \u00db update load 12: current_node \u03a9 new_node 13: terminated \u03a9 (t \u2265 Unif(0, 1) < phalt) \u00db draw RV t to decide on termination 14: end while 15: end for 16: normalise: \u201ef (i) \u03a9 \u201ef (i)/m\nThis is identical to the algorithm presented by Choromanski (2023) for constructing features to approximate the 2-regularised Laplacian kernel, apart from the presence of the extra modulation function f : (Nfi{0}) \u00e6 R in line 8 that upweights or downweights contributions from walks depending on their length (see Fig. 1). We refer to \u201ef as general graph random features (g-GRFs), where the subscript f identifies the modulation function. Crucially, the time complexity of Alg. 1 is subquadratic in the number of nodes N , in contrast to exact methods which are O(N3).2\nWe now state the following central result, proved in App. A.2. Theorem 2.1 (Unbiased approximation of K\u21b5 via convolutions). For two modulation functions: f1, f2 : (N fi {0}) \u00e6 R, g-GRFs ! \u201ef1(i))Ni=1, (\u201ef2(i) \"N i=1 constructed according to Alg. 1 give unbiased approximation of K\u21b5, [K\u21b5]ij = E # \u201ef1(i)\u20ac\u201ef2(j) $ , (3)\nfor kernels with an arbitrary Taylor expansion \u21b5 = (\u2013k)\u0152k=0 provided that \u21b5 = f1 \u00faf2. Here, \u00fa is the discrete convolution of the modulation functions f1, f2; that is, for all k \u0153 (Nfi{0}),\nk\u00ff p=0 f1(k \u2260 p)f2(p) = \u2013k. (4)\nClearly the class of pairs of modulation functions f1, f2 that satisfy Eq. 4 is highly degenerate. Indeed, it is possible to solve for f1 given any f2 and \u21b5 provided f2(0) \u201d= 0. For instance, a trivial solution is given by: f1(i) = \u2013i, f2(i) = I(i = 0) with I(\u00b7) the indicator function. In this case, the walkers corresponding to f2 are \u2018lazy\u2019, depositing all their load at the node at which they begin. Contributions to the estimator \u201ef1(i)\u20ac\u201ef2(j) only come from walkers initialised at i traversing all the way to j rather than two walkers both passing through an intermediate node. Also of great interest is the case of symmetric modulation functions f1 = f2, where now intersections do contribute. In this case, the following is true (proved in App. A.3). Theorem 2.2 (Computing symmetric modulation functions). Supposing f1 = f2 = f , Eq. 4 is solved by a function f which is unique (up to a sign) and is given by\nf(i) = \u00b1 i\u00ff\nn=0\n3 1 2 n 4 \u00ff\nk1+2k2+3k3...=i k1+k2+k3+...=n\n3 n\nk1k2k3...\n4 1 \u2013\nk1 1 \u2013 k2 2 \u2013 k3 3 ...\n2 . (5)\nMoreover, f(i) can be e ciently computed with the iterative formula Y ]\n[ f(0) = \u00b1\u00d4\u20130 = \u00b11\nf(i + 1) = \u2013i+1\u2260\nqi\u22601 p=0\nf(i\u2260p)f(p+1) 2f(0) for i \u00d8 0.\n(6)\n2 Concretely, Alg. 1 yields a pair of matrices 1,2 := (\u201e(i)) N i=1 \u0153 R N\u25caN such that K = E( 1 \u20ac2 ) in subquadratic time. Of course, explicitly multiplying the matrices to evaluate every element of \u201aK would be O(N3), but we avoid this since in applications we just evaluate 1( \u20ac 2 v) where v \u0153 R N is some vector and the brackets give the order of computation. This is O(N2).\nFor symmetric modulation functions, the random features \u201ef1(i) and \u201ef2(i) are identical apart from the particular sample of random walks used to construct them. They cannot share the same sample or estimates of diagonal kernel elements [K\u21b5]ii will be biased. Computational cost: Note that when running Alg. 1 one only needs to evaluate the modulation functions f1,2(i) up to the length of the longest walk one samples. A batch of size b, (f1,2(i))bi=1, can be pre-computed in time O(b2) and reused for random features corresponding to di erent nodes and even di erent graphs. Further values of f can be computed at runtime if b is too small and also reused for later computations. Moreover, the minimum length b required to ensure that all m walks are shorter than b with high probability (Pr(fim\ni=1len(\u00cai) \u00c6 b) > 1\u2260\u201d, \u201d \u03c0 1) scales only logarithmically with m (see App. A.1). This means that, despite estimating a much more general family of graph functions, g-GRFs are essentially no more expensive than the original GRF algorithm. Moreover, any techniques used for dimensionality reduction of regular GRFs (e.g. applying the Johnson-Lindenstrauss transform (Dasgupta et al., 2010) or using \u2018anchor points\u2019 (Choromanski, 2023)) can also be used with g-GRFs, providing further e ciency gains.\nGenerating functions: Inserting the constraint for unbiasedness in Eq. 4 back into the definition of K\u21b5(W), we immediately have that\nK\u21b5(W) = Kf1(W)Kf2(W) (7) where Kf1(W) := q\u0152 i=0 f1(i)Wi is the generating function corresponding to the sequence (f1(i))\u0152i=0. This is natural because the (discrete) Fourier transform of a (discrete) convolution returns the product of the (discrete) Fourier transforms of the respective functions. In the symmetric case f1 = f2, it follows that\nKf (W) = \u00b1 (K\u21b5(W)) 1 2 . (8)\nIf the RHS has a simple Taylor expansion (e.g. K\u21b5(W) = exp(W) so Kf (W) = exp( W2 )), this enables us obtain f without recourse to the conditional sum in Eq. 5 or the iterative\nName f(i)\nd-regularised Laplacian (d\u22602+2i)!!(2i)!!(d\u22602)!! p-step random walk ! p\n2 i\n\"\nDi usion 1\n2ii!\nexpression in Eq. 6. This is the case for many popular graph kernels; we provide some prominent examples in the table left. A notable exception is the inverse cosine kernel. As an interesting corollary, by considering the di usion kernel we have also proved thatq\nk p=0 1 2pp! 1 2k\u2260p(k\u2260p)! = 1 k! ."
        },
        {
            "heading": "2.1 Neural modulation functions, kernel learning and generalisation",
            "text": "Instead of using a fixed modulation function f to estimate a fixed kernel, it is possible to parameterise it more flexibly. For example, we can define a neural modulation function f\n(N) : (N fi {0}) \u00e6 R by a neural network (with a restricted domain) whose input and output dimensionalities are equal to 1. During training, we can choose the loss function to target particular desiderata of g-GRFs: for example, to suppress the mean square error of estimates of some particular fixed kernel (Sec. 3.4), or to learn a kernel which performs better in a downstream task (Sec. 3.5). Implicitly learning K\u21b5 via f (N) is more scalable than learning K\u21b5 directly because it obviates the need to repeatedly compute the exact kernel, which is typically of O(N3) time complexity. Since any modulation function f maps to a unique \u21b5 by Eq. 4, it is also always straightforward to recover the exact kernel which the g-GRFs estimate, e.g. once the training is complete. Supposing we have (implicitly) learned \u21b5, how can the learned kernel KG\u21b5 be expected to generalise? Let K\u2013 : x \u00e6 HK\u2013 denote the feature mapping from the input space to the reproducing kernel Hilbert space HK\u2013 induced by the kernel KG\u21b5. Define the hypothesis set\nH = {x \u00e6 w\u20ac K\u2013(x) : |\u2013i| \u00c6 \u2013 (M) i\n, \u00cew\u00ce2 \u00c6 1}, (9) where we restricted our family of kernels so that the absolute value of each Taylor coe cient \u2013i is smaller than some maximum value \u2013(M)i . Following very similar arguments to Cortes et al. (2010), the following is true.\nTheorem 2.3 (Empirical Rademacher complexity bound). For a fixed sample S = (xi)mi=1, the empirical Rademacher complexity \u201aR(H) is bounded by\n\u201aR(H) \u00c6 \u0131\u0302\u0131\u00d9 1\nm\n\u0152\u00ff\ni=0 \u2013\n(M) i fl(W)i, (10)\nwhere fl(W) is the spectral radius of the weighted adjacency matrix W.\nNaturally, the bound on \u201aR(H) increases monotonically with fl(W). Following standard arguments in the literature, this immediately yields generalisation bounds for learning kernels K\nG \u21b5. We discuss this in detail, including proving Theorem 2.3, in App. A.4."
        },
        {
            "heading": "3 Experiments",
            "text": "Here we test the empirical performance of g-GRFs, both for approximating fixed kernels (Secs 3.1-3.3) and with learnable neural modulation functions (Secs 3.4-3.5)."
        },
        {
            "heading": "3.1 Unbiased pointwise estimation of fixed kernels",
            "text": "We begin by confirming that g-GRFs do indeed give unbiased estimates of the graph kernels listed in Table 1, taking regularisers \u2021 = 0.25 and \u2013 = 20 with phalt = 0.1. We use symmetric modulation functions f , computed with the closed forms where available and using the iterative scheme in Eq. 6 if not. Fig. 2 plots the relative Frobenius norm error between the true kernels K and their approximations with g-GRFs \u201aK (that is, \u00ceK \u2260 \u201aK\u00ceF /\u00ceK\u00ceF ) against the number of random walkers m. We consider 8 di erent graphs: a small random Erd s-R\u00e9nyi graph, a larger Erd s-R\u00e9nyi graph, a binary tree, a d-regular graph and 4 real world examples (karate, dolphins, football and eurosis) (Ivashkin, 2023). They vary substantially in size. For every graph and for all kernels, the quality of the estimate improves as m grows and becomes very small with even a modest number of walkers."
        },
        {
            "heading": "3.2 Solving differential equations on graphs",
            "text": "An intriguing application of g-GRFs for fixed kernels is e ciently computing approximate solutions of time-invariant non-homogeneous ordinary di erential equations (ODEs) on graphs. Consider the following ODE defined on the nodes N of the graph G:\ndx(t) dt = Wx(t) + y(t), (11)\nwhere x(t) \u0153 RN is the state of the graph at time t, W \u0153 RN\u25caN is a weighted adjacency matrix and y(t) is a (known) driving term. Assuming the null initial condition x(0) = 0, Eq. 11 is solved by the convolution\nx(t) = \u2044 t\n0 exp(W(t \u2260 \u00b7))y(\u00b7)d\u00b7 = E\u00b7\u0153P\n5 1 p(\u00b7) exp(W(t \u2260 \u00b7))y(\u00b7) 6\n(12)\nwhere P is a probability distribution on the interval [0, t], equipped with a (preferably e cient) sampling mechanism and probability density function p(\u00b7). Taking n \u0153 N Monte Carlo samples (\u00b7i)ni=1 i.i.d. \u2265 P, we can construct the unbiased estimator:\n\u201ax(t) := 1 n\nn\u00ff\nj=1\n1 p(\u00b7j) exp(W(t \u2260 \u00b7j))y(\u00b7j). (13)\nNote that exp(W(t \u2260 \u00b7j)) is nothing other than the di usion kernel, which is expensive to compute exactly for large N but can be e ciently approximated with g-GRFs. Take exp(W(t \u2260 \u00b7j)) \u0192 j \u20acj with j := (\u201e(i))Ni=1 an N \u25ca N matrix whose rows are g-GRFs constructed to approximate the kernel at a particular \u00b7j . Then we have that\n\u201ax(t) := 1 n\nn\u00ff\nj=1\n1 p(\u00b7j) j \u20acj y(\u00b7j) (14)\nwhich can be computed in quadratic time (c.f. cubic if the heat kernel is computed exactly). Further speed gains are possible if dimensionality reduction techniques are applied to the g-GRFs (Choromanski, 2023; Dasgupta et al., 2010).\nAs an example, we consider di usion on three real-world graphs with a fixed source at one node, taking W = L (the graph Laplacian) and y(t) = y = (1, 0, 0, ...)\u20ac. The steady state is x(\u0152) = W\u22601(\u2260y). We simulate evolution under the ODE for t = 1 with n = 10 discretisation timesteps and P uniform, approximating exp(W(t \u2260 \u00b7j)) with di erent numbers of walkers m. As m grows, the quality of approximation improves and the (normalised) error on the final state \u00ce\u201ax(1) \u2260x(1)\u00ce2/\u00cex(1)\u00ce2 drops for every graph. We take 100 repeats for statistics and plot the results in Fig. 3. One standard deviation of the mean error is shaded. phalt = 0.1 and the regulariser is given by \u2021 = 1."
        },
        {
            "heading": "3.3 Efficient kernelised graph node clustering",
            "text": "As a further demonstration of the utility of our new mechanism, we show how estimates of the kernel K = exp(\u20212W) can be used to assign nodes to k = 3 clusters. Here, we choose W to be the (unweighted) adjacency matrix and the regulariser is \u20212 = 0.2. We follow the algorithm proposed by Dhillon et al. (2004), comparing the clusters when we use exact and g-GRF-approximated kernels. For all graphs, m \u00c6 80. Table 2 reports the clustering error, defined by\nEc := no. wrong pairs\nN(N \u2260 1)/2 . (15)\nThis is the number of misclassified pairs of nodes (assigned to the same cluster when the converse is true or vice versa) divided by the total number of pairs. The error is small even with a modest number of walkers and on large graphs; kernel estimates e ciently constructed using g-GRFs can be readily deployed on downstream tasks where exact methods are slow.\n3.4 Learning f (N) for better kernel approximation\nFollowing the discussion in Sec. 2.1 we now replaced fixed f with a neural modulation function f (N) parameterised by a simple neural network with 1 hidden layer of size 1:\nf (N)(x) = \u2021softplus (w2\u2021ReLU(w1x + b1) + b2) , (16)\nwhere w1, b1, w2, b2 \u0153 R and \u2021ReLU and \u2021softplus are the ReLU and softplus activation functions, respectively. Bigger, more expressive architectures (including allowing f (N) to take negative values) can be used but this is found to be su cient for our purposes. We define our loss function to be the Frobenius norm error between a target Gram matrix and our g-GRF-approximated Gram matrix on the small Erd s-R\u00e9nyi graph (N = 20) with m = 16 walks. For the target, we choose the 2-regularised Laplacian kernel. We train symmetric f (N)1 = f (N) 2 but provide a brief discussion of the asymmetric case (including its respective strengths and weaknesses) in App. A.5. On this graph, we minimise the loss with the Adam optimiser and a decaying learning rate (LR = 0.01, \u201c = 0.975, 1000 epochs). We make the following striking observation: f (N) does not generically converge to the unique unbiased (symmetric) modulation function implied by \u21b5, but instead to some di erent f that though biased gives a smaller mean squared error (MSE). This is possible because by downweighting long walks the learned f (N) gives estimators with a smaller variance, which is su cient to suppress the MSE on the kernel matrix elements even though it no longer gives the target value in expectation. We then fix f (N) and use it for kernel approximation on the remaining graphs. The learned, biased f (N) still provides better kernel estimates, including for graphs with very di erent topologies and a much greater number of nodes: eurosis is bigger by a factor of over 60. See Table 3 for the results. phalt = 0.5 and \u2021 = 0.8. Naturally, the learned f (N) is dependent on the the number of random walks m; as m grows, the variance on the kernel approximation drops so it is intuitive that the learned f (N) will approach the unbiased f . Fig. 4 empirically confirms this is the case, showing the learned f\n(N) for di erent numbers of walkers. The line labelled \u0152 is the unbiased modulation function, which for the 2-regularised Laplacian kernel is constant.\nTable 3: Kernel approximation error with m = 16 walks and unbiased or learned modulation functions. Lower is better. Brackets give one standard deviation of the last digit.\nGraph N Frob. norm error on \u201aK Unbiased Learned\nSmall ER 20 0.0488(9) 0.0437(9) Larger ER 100 0.0503(4) 0.0448(4) Binary tree 127 0.0453(4) 0.0410(4) d-regular 100 0.0490(2) 0.0434(2) karate 34 0.0492(6) 0.0439(6) dolphins 62 0.0505(5) 0.0449(5) football 115 0.0520(2) 0.0459(2) eurosis 1272 0.0551(2) 0.0484(2)\nFigure 4: Learned modulation function with di erent numbers of random walkers m. It approaches the unbiased f (N) as\nm \u00e6 \u0152\nThese learned modulation functions might guide the more principled construction of biased, low-MSE GRFs in the future. An analogue in Euclidean space is provided by structured orthogonal random features (SORFs), which replace a random Gaussian matrix with a HDproduct to estimate the Gaussian kernel (Choromanski et al., 2017; Yu et al., 2016). This likewise improves kernel approximation quality despite breaking estimator unbiasedness."
        },
        {
            "heading": "3.5 Implicit kernel learning for node attribute prediction",
            "text": "As suggested in Sec. 2.1, it is also possible to train the neural modulation function f (N) directly using performance on a downstream task, performing implicit kernel learning. We have argued that this is much more scalable than optimising K\u21b5 directly.\nIn this spirit, we now address the problem of kernel regression on triangular mesh graphs, as previously considered by Reid et al. (2023). For graphs in this dataset (DawsonHaggerty, 2023), every node is associated with a normal vector v(i) \u0153 R3 equal to the mean of the normal vectors of its 3 surrounding faces. The task is to predict the directions of missing vectors (a random 5% split) from the remainder. Our (unnormalised) predictions are given by \u201av(i) := q j\n\u201aK(N)(i, j)v(j), where \u201aK(N) is a kernel estimate constructed using g-GRFs with a neural modulation function f (N) (see Eq. 3). The angular prediction error is 1 \u2260 cos \u25cai with \u25cai the angle between the true v(i) and and\napproximate \u201av(i) normals, averaged over the missing vectors. We train a symmetric pair f (N) using this angular prediction error on the small cylinder graph (N = 210) as the loss function. Then we freeze f (N) and compute the angular prediction error for other larger graphs. Fig. 5 shows the learned f (N) as well as some other modulation functions corresponding to popular fixed kernels. Note also that learning f (N) already includes (but is not limited to) optimising the lengthscale of a given kernel: taking \u00caW \u00e6 \u2014 \u00caW is identical to f(i) \u00e6 f(i)\u2014i \u2019 i. The prediction errors are highly correlated between the di erent modulation functions for a\ngiven random draw of walks; ensembles which \u2018explore\u2019 the graph poorly, terminating quickly or repeating edges, will give worse g-GRF estimators for every f . For this reason, we compute the prediction errors as the average normalised di erence compared to the learned kernel result. Table 4 reports the results. Crucially, this di erence is found to be positive for every graph and every fixed kernel, meaning the learned kernel always performs best.\nIt is remarkable that the learned f (N) gives the smallest error for all the graphs even though it was just trained on cylinder, the smallest one. We have implicitly learned a good kernel for this task which generalises well across topologies. It is also intriguing that the di usion kernel performs only slightly worse. This is to be expected because their modulation functions are similar (see Fig. 5) so they encode very similar kernels, but this will not always be the case depending on the task at hand."
        },
        {
            "heading": "4 Conclusion",
            "text": "We have introduced \u2018general graph random features\u2019 (g-GRFs), a novel random walk-based algorithm for time-e cient estimation of arbitrary functions of a weighted adjacency matrix. The mechanism is conceptually simple and trivially distributed across machines, unlocking kernel-based machine learning on very large graphs. By parameterising one component of the random features with a simple neural network, we can futher suppress the mean square error of estimators and perform scalable implicit kernel learning."
        },
        {
            "heading": "5 Ethics and reproducibility",
            "text": "Ethics: Our work is foundational. There are no direct ethical concerns that we can see, though of course increases in scalability a orded by graph random features might amplify risks of graph-based machine learning, from bad actors or as unintended consequences. Reproducibility: To foster reproducibility, we clearly state the central algorithm in Alg. 1. Source code is available at https://github.com/isaac-reid/general_graph_random_features. All theoretical results are accompanied by proofs in Appendices A.1-A.4, where any assumptions are made clear. The datasets we use correspond to standard graphs and are freely available online. We link suitable repositories in every instance. Except where prohibitively computationally expensive, results are reported with uncertainties to help comparison."
        },
        {
            "heading": "6 Relative contributions and acknowledgements",
            "text": "EB initially proposed using a modulation function to generalise GRFs to estimate the diffusion kernel and derived its mathematical expression. IR and KC then developed the full g-GRFs algorithm for general functions of a weighted adjacency matrix, proving all theoretical results, running the experiments and preparing the manuscript. AW provided helpful discussions and advice. IR acknowledges support from a Trinity College External Studentship. AW acknowledges support from a Turing AI fellowship under grant EP/V025279/1 and the Leverhulme Trust via CFI. We thank Kenza Tazi and Austin Trip for their careful readings of the manuscript. Richard Turner provided valuable suggestions and support throughout the project."
        }
    ],
    "title": "General Graph Random Features",
    "year": 2024
}