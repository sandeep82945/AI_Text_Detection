{
    "abstractText": "Pre-trained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs. To address these challenges, we propose the Federated Prompt Tuning Paradigm for multilingual scenarios, which utilizes parameter-efficient fine-tuning while adhering to data sharing restrictions. We have designed a comprehensive set of experiments and analyzed them using a novel notion of language distance to underscore the strengths of this paradigm: Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local cross-lingual transfer tuning methods, our approach achieves 6.9% higher accuracy, reduces the training parameters by over 99%, and demonstrates better stability and generalization. Such findings underscore the potential of our approach to promote social equality and champion linguistic diversity, so that no language will be left behind.",
    "authors": [
        {
            "affiliations": [],
            "name": "LOW-RESOURCE LANGUAGES"
        }
    ],
    "id": "SP:56b81d145c549ec718ce1dbc3f42bc006f95f963",
    "references": [
        {
            "authors": [
                "Ife Adebara",
                "Muhammad Abdul-Mageed"
            ],
            "title": "Towards afrocentric NLP for African languages: Where we are and where we can go. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "David Ifeoluwa Adelani",
                "Jade Abbott",
                "Graham Neubig",
                "Daniel D\u2019souza",
                "Julia Kreutzer",
                "Constantine Lignos",
                "Chester Palen-Michel",
                "Happy Buzaaba",
                "Shruti Rijhwani",
                "Sebastian Ruder"
            ],
            "title": "Masakhaner: Named entity recognition for african",
            "venue": "languages. Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Awosan",
                "Tadesse Kebede Guge",
                "Sakayo Toadoum Sari",
                "Pamela Nyatsine",
                "Freedmore Sidume",
                "Oreen Yousuf",
                "Mardiyyah Oduwole",
                "Ussen Abre Kimanuka",
                "Kanda Patrick Tshinu",
                "Thina Diko",
                "Siyanda Nxakama",
                "Abdulmejid Tuni Johar",
                "Sinodos Gebre",
                "Muhidin Mohamed",
                "S.A. Mohamed",
                "Fuad Mire Hassan",
                "Moges Ahmed Mehamed",
                "Evrard Ngabire",
                "Pontus Stenetorp"
            ],
            "title": "Masakhanews: News topic classification for african",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Anil",
                "Andrew M Dai",
                "Orhan Firat",
                "Melvin Johnson",
                "Dmitry Lepikhin",
                "Alexandre Passos",
                "Siamak Shakeri",
                "Emanuel Taropa",
                "Paige Bailey",
                "Zhifeng Chen"
            ],
            "title": "Palm 2 technical report",
            "venue": "arXiv preprint arXiv:2305.10403,",
            "year": 2023
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Ponti",
                "Anna Korhonen",
                "Ivan Vuli\u0107"
            ],
            "title": "Composable sparse fine-tuning for crosslingual transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama"
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4623\u20134637,",
            "year": 2020
        },
        {
            "authors": [
                "Syreen Banabilah",
                "Moayad Aloqaily",
                "Eitaa Alsayed",
                "Nida Malik",
                "Yaser Jararweh"
            ],
            "title": "Federated learning review: Fundamentals, enabling technologies, and future applications",
            "venue": "Information Processing & Management,",
            "year": 2022
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Yoav Goldberg",
                "Shauli Ravfogel"
            ],
            "title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Daniel J. Beutel",
                "Taner Topal",
                "Akhil Mathur",
                "Xinchi Qiu",
                "Javier Fernandez-Marques",
                "Yan Gao",
                "Lorenzo Sani",
                "Kwing Hei Li",
                "Titouan Parcollet",
                "Pedro Porto Buarque de Gusm\u00e3o",
                "Nicholas D. Lane"
            ],
            "title": "Flower: A friendly federated learning research",
            "venue": "URL https://arxiv",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ethan C. Chau",
                "Lucy H. Lin",
                "Noah A. Smith"
            ],
            "title": "Parsing with multilingual BERT, a small corpus, and a small treebank",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Yihong Chen",
                "Kelly Marchisio",
                "Roberta Raileanu",
                "David Ifeoluwa Adelani",
                "Pontus Stenetor",
                "Sebastian Riedel",
                "Mikel Artetx"
            ],
            "title": "Improving language plasticity via pretraining with active forgetting",
            "venue": "NeurIPS 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah"
            ],
            "title": "Fiedel. Palm: Scaling language modeling with pathways, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Dan Garrette",
                "Kiat Chuan Tan",
                "Jason Riesa"
            ],
            "title": "Improving multilingual models with language-clustered vocabularies",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4536\u20134546,",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample"
            ],
            "title": "Cross-Lingual Language Model Pretraining",
            "venue": "Curran Associates Inc., Red Hook, NY,",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample",
                "Ruty Rinott",
                "Adina Williams",
                "Samuel R. Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov"
            ],
            "title": "Xnli: Evaluating cross-lingual sentence representations",
            "year": 2018
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "CoRR, abs/2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson"
            ],
            "title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization",
            "year": 2003
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "T. Garipov",
                "Dmitry P. Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u1ef3",
                "H Brendan McMahan",
                "Felix X Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated learning: Strategies for improving communication efficiency",
            "venue": "arXiv preprint arXiv:1610.05492,",
            "year": 2016
        },
        {
            "authors": [
                "Anne Lauscher",
                "Vinit Ravishankar",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161"
            ],
            "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt",
            "venue": "tuning. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Lewis",
                "Barlas O\u011fuz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk"
            ],
            "title": "Mlqa: Evaluating cross-lingual extractive question answering",
            "venue": "arXiv preprint arXiv:1910.07475,",
            "year": 2019
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582\u20134597,",
            "year": 2021
        },
        {
            "authors": [
                "Yaobo Liang",
                "Nan Duan",
                "Yeyun Gong",
                "Ning Wu",
                "Fenfei Guo",
                "Weizhen Qi",
                "Ming Gong",
                "Linjun Shou",
                "Daxin Jiang",
                "Guihong Cao",
                "Xiaodong Fan",
                "Ruofei Zhang",
                "Rahul Agrawal",
                "Edward Cui",
                "Sining Wei",
                "Taroon Bharti",
                "Ying Qiao",
                "Jiun-Hung Chen",
                "Winnie Wu",
                "Shuguang Liu",
                "Fan Yang",
                "Daniel Campos",
                "Rangan Majumder",
                "Ming Zhou"
            ],
            "title": "Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation",
            "year": 2004
        },
        {
            "authors": [
                "Wei Yang Bryan Lim",
                "Nguyen Cong Luong",
                "Dinh Thai Hoang",
                "Yutao Jiao",
                "Ying-Chang Liang",
                "Qiang Yang",
                "Dusit Niyato",
                "Chunyan Miao"
            ],
            "title": "Federated learning in mobile edge networks: A comprehensive survey",
            "venue": "IEEE Communications Surveys & Tutorials,",
            "year": 2031
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Chaoyang He",
                "Zihang Ze",
                "Hulin Wang",
                "Yufen Hua",
                "Christophe Dupuy",
                "Rahul Gupta",
                "Mahdi Soltanolkotabi",
                "Xiang Ren",
                "Salman Avestimehr"
            ],
            "title": "FedNLP: Benchmarking federated learning methods for natural language processing tasks. In Findings of the Association for Computational Linguistics: NAACL 2022",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Littell",
                "David R Mortensen",
                "Ke Lin",
                "Katherine Kairis",
                "Carlisle Turner",
                "Lori Levin"
            ],
            "title": "Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
            "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers,",
            "year": 2017
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig"
            ],
            "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "CoRR, abs/2107.13586,",
            "year": 2021
        },
        {
            "authors": [
                "Yu-An Chung"
            ],
            "title": "Seamlessm4t\u2014massively multilingual & multimodal machine translation",
            "venue": "ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Manuel Mager",
                "Arturo Oncevay",
                "Abteen Ebrahimi",
                "John Ortega",
                "Annette Rios Gonzales",
                "Angela Fan",
                "Ximena Gutierrez-Vasques",
                "Luis Chiruzzo",
                "Gustavo Gim\u00e9nez-Lugo",
                "Ricardo Ramos"
            ],
            "title": "Findings of the americasnlp 2021 shared task on open machine translation for indigenous languages of the americas",
            "venue": "In Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,",
            "year": 2021
        },
        {
            "authors": [
                "Chaitanya Malaviya",
                "Graham Neubig",
                "Patrick Littell"
            ],
            "title": "Learning language representations for typology prediction",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2017
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul"
            ],
            "title": "Peft: Stateof-the-art parameter-efficient fine-tuning methods",
            "venue": "https://github.com/huggingface/ peft,",
            "year": 2022
        },
        {
            "authors": [
                "Kelly Marchisio",
                "Patrick Lewis",
                "Yihong Chen",
                "Mikel Artetxe"
            ],
            "title": "Mini-model adaptation: Efficiently extending pretrained models to new languages via aligned shallow training. ACL 2023",
            "venue": "Findings of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics",
            "year": 2017
        },
        {
            "authors": [
                "Shamsuddeen Hassan Muhammad",
                "David Ifeoluwa Adelani",
                "Sebastian Ruder",
                "Ibrahim Said Ahmad",
                "Idris Abdulmumin",
                "Bello Shehu Bello",
                "Monojit Choudhury",
                "Chris Chinenye Emezue",
                "Saheed Salahudeen Abdullahi",
                "Anuoluwapo Aremu"
            ],
            "title": "Naijasenti: A nigerian twitter sentiment corpus for multilingual sentiment analysis",
            "venue": "arXiv preprint arXiv:2201.08277,",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder"
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7654\u20137673,",
            "year": 2020
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette"
            ],
            "title": "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4996\u20135001, Florence, Italy, July 2019a",
            "venue": "Association for Computational Linguistics. doi: 10.18653/v1/P19-1493. URL https://aclanthology.org/P19-1493",
            "year": 2019
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette"
            ],
            "title": "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4996\u20135001, Florence, Italy, July 2019b",
            "venue": "Association for Computational Linguistics. doi: 10.18653/v1/P19-1493. URL https://aclanthology.org/P19-1493",
            "year": 2019
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Goran Glava\u0161",
                "Olga Majewska",
                "Qianchu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen"
            ],
            "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2362\u20132376,",
            "year": 2020
        },
        {
            "authors": [
                "Yasar Abbas Ur Rehman",
                "Yan Gao",
                "Jiajun Shen",
                "Pedro Porto Buarque de Gusm\u00e3o",
                "Nicholas Lane"
            ],
            "title": "Federated self-supervised learning for video understanding",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze"
            ],
            "title": "True few-shot learning with Prompts\u2014A real-world perspective",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh"
            ],
            "title": "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
            "venue": "In Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Kushal Tirumala",
                "Aram Markosyan",
                "Luke Zettlemoyer",
                "Armen Aghajanyan"
            ],
            "title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Haoyu Wang",
                "Handong Zhao",
                "Yaqing Wang",
                "Tong Yu",
                "Jiuxiang Gu",
                "Jing Gao"
            ],
            "title": "Fedkc: Federated knowledge composition for multilingual natural language understanding",
            "venue": "In Proceedings of the ACM Web Conference 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Orion Weller",
                "Marc Marone",
                "Vladimir Braverman",
                "Dawn J Lawrie",
                "Benjamin Van Durme"
            ],
            "title": "Pretrained models for multilingual federated learning",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "Remi Louf",
                "Morgan Funtowicz",
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Lei Shu",
                "Philip Yu",
                "Bing Liu"
            ],
            "title": "Understanding pre-trained BERT for aspect-based sentiment",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Pre-trained large language models (LLMs) have been driving the recent progress in natural language processing (Brown et al., 2020; Chowdhery et al., 2022; Anil et al., 2023; Touvron et al., 2023a;b). These large models, built on extensive corpora, offer valuable insights and impressive results across a range of applications. At the meantime, in order to provide universally accessible knowledge with LLMs, extending them to multiple languages has become a particularly relevant research target (Conneau & Lample, 2019; Conneau et al., 2020; Artetxe et al., 2020; Pfeiffer et al., 2020).\nHowever, fine-tuning and deploying multilingual LLMs in practical downstream tasks are not as easy as its monolingual counterpart. First of all, sharing data across different regions can be difficult or even impossible. Regulations like the General Data Protection Regulation (GDPR) (Lim et al., 2020) limit cross-region data-sharing. Moreover, languages in various regions can be radically different, e.g. Sino-Tibetan and Indo-European, posing a Non-Independent and Identically Distributed (non-IID) challenge when learning a global multilingual model. This situation accentuates privacy concerns, and highlights the need for effective privacy-preserving techniques when using multilingual LLMs. To this end, recent works attempt to address privacy-constrained fine-tuning for multilingual tasks and explore how different languages impact the federated process (Weller et al., 2022). However, they primarily target high-resources languages; research on low-resource languages remains largely unexplored.\nAddressing low-resource languages is essential to promoting technological fairness and protecting the linguistic diversity. Unlike their high-resource counterparts, low-resources languages pose intriguing research challenges: i) Limited computational resources. Regions of low-resources languages are often economically developing areas, with little access to huge computational resources required to\neither train language models from scratch or fully fine-tune pre-trained large language models (Mager et al., 2021; Adebara & Abdul-Mageed, 2022). ii) Limited data in the target language. Due to a small speaking population or the spoken nature of the language, data is often scarce (Adelani et al., 2021; Muhammad et al., 2022; Ebrahimi et al., 2022). As depicted in Figure 1, the pre-training data for LLMs is predominantly in English, with little coverage of low-resource languages. Under such circumstances, the performance of low-resources languages is often unsatisfactory during fine-tuning because of their under-representation. iii) Memorization risk. Recent studies find that as pre-trained models scale up, their ability to memorize training data increases (Tirumala et al., 2022). This implies that, when fine-tuning these models with limited data, the risk of overfitting and potential privacy issues arises.\nRecognizing the challenges above, federated learning (FL) has emerged as a technology to address these concerns, where the model training is done across multiple decentralized devices or servers while the data is always kept localized (McMahan et al., 2017; Li et al., 2020; Yang et al., 2019). In a multilingual setting, FL becomes particularly natural, as data from diverse linguistic backgrounds can be sourced without compromising user privacy, and due to the geographical spread and inherent linguistic diversity of devices, data on each node is likely to exhibit non-IID distribution.\nIn this paper, in order to break the geographic border and the linguistic border between different language speaking countries, we propose a new paradigm grounded in FL, Multilingual Federated Prompt Tuning, focusing on parameter-efficient fine-tuning for multilingual tasks across various regions or devices. Specifically, by having each country fine-tune the model locally and then pass the parameters to a server for aggregation, we achieve a global model, which leverages collective knowledge and exposing models to a wider range of linguistic patterns. Considering the different linguistic patterns in various countries, our prompt encoders help generalize and adapt to the languages of different countries. This strategy enables the fine-tuning of a minimal amount of parameters, indirectly applying knowledge from other languages to downstream tasks in one\u2019s own language. This strategy requires minimal computational resources and significantly improves performance, particularly for low-resource languages.\nWe demonstrate the effectiveness of our paradigm on standard NLP tasks including New Classification and XNLI. The performance of our paradigm achieves 6.9% accuracy improvement while navigating privacy regulations that restrict cross-country data sharing. Compared with local monolingual finetuning, our paradigm reduces computational cost and communication cost by more than 99%. Our approach paves the way for fine-tuning multilingual large language models on resource-constraint devices across various regions, and holds the potential to promote social equality, privacy, and linguistic diversity in the research community. Our contributions are as follows:\n\u2022 We demonstrate that federated prompt tuning can serve as a new paradigm for addressing the linguistic and physical challenges of multilingual fine-tuning across regions or devices.\n\u2022 Compared to traditional local monolingual fine-tuning paradigm, federated prompt tuning is not only data and parameter efficient, suitable for situations with limited computational power, but also shows better generalization and stability, performing well in downstream tasks of low-resource languages with huge language distance from the pre-trained language.\n\u2022 Federated prompt tuning also helps to alleviating data leakage by reducing both the data transmission amount and data memorization, which opens up new avenues for exploration and has the potential to inform future research in this area."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Multilingual Language Models. Multilingual Pre-trained Language Models (PLMs) such as mBERT (Pires et al., 2019a), XLM-R (Conneau et al., 2020), and SeamlessM4T (Loic Barrault, 2023) have emerged as a viable option for bringing the power of pre-training to a large number of languages (?). Many studies analyzed mBERT\u2019s and XLM-R\u2019s capabilities and limitations, finding that the multilingual models work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel or comparable data, and translation dictionaries (Pires et al., 2019b; Wu & Dredze, 2019; Artetxe et al., 2020; Xu et al., 2020).\nHowever, these multilingual PLMs are not without limitations. Particularly, Conneau et al. (2020) observed the curse of multilinguality phenomenon: given a fixed model capacity, adding more languages does not necessarily improve the multilingual performance but can deteriorate the performance after a certain point, especially for underrepresented languages (Wu & Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020) Prior work tried to address this issue by increasing the model capacity (Artetxe et al., 2020; Pfeiffer et al., 2020; Chau et al., 2020) or through additional training for particular language pairs (Pfeiffer et al., 2020; Ponti et al., 2020) or by clustering and merging the vocabularies of similar languages, before defining a joint vocabulary across all languages (Chung et al., 2020). Despite these efforts, the multilingual PLMs still struggle with balancing their capacity across many languages in an sample-efficient and parameter-efficient way (Ansell et al., 2022; Marchisio et al., 2022; Chen et al., 2023).\nPrompt Learning and Parameter-Efficient fine-tuning. The size of pre-trained language models has been increasing significantly (Brown et al., 2020), presenting challenges to traditional task transfer based on full-parameter fine-tuning. Recent research has shifted its attention to Parameter-Efficient fine-tuning techniques, such as prompt tuning (Lester et al., 2021; Li & Liang, 2021; Liu et al., 2021b), adapters (Houlsby et al., 2019a), as well as combined approaches including LoRA (Hu et al., 2021) and BitFit (Ben Zaken et al., 2022). These methods utilize a minimal number of tuning parameters, yet they offer transfer performance that is comparable with traditional fine-tuning.\nPrompt learning involves training a small set of prompt tokens while keeping the original pretrained model parameters frozen, thus allowing for model personalization with minimal parameter updates. (Liu et al., 2021a). This paradigm shows promise in effectively leveraging large pre-trained models in a data-efficient manner by reducing the need for extensive labeled datasets (Schick & Sch\u00fctze, 2022). Additionally, prompt learning has exhibited a remarkable ability to generalize across a variety of tasks, suggesting a step towards more flexible and adaptable machine learning systems (Shin et al., 2020). Another most widely used Parameter-Efficient fine-tuning technique is LoRA, or Low-Rank Adaptation, which involves freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture, thereby achieving fine-tuning without incurring any additional inference latency (Hu et al., 2021).\nFederated Learning. Federated Learning has garnered significant attention in the academic realm. It bypasses the conventional model training process by sharing models instead of raw data. With Federated Averaging (FedAvg) (McMahan et al., 2017), participating clients train models using their respective private datasets locally, and the updated model parameters are aggregated. This preserves the privacy of the underlying data while collectively benefiting from the knowledge gained during the training process (Konec\u030cny\u0300 et al., 2016). Despite abundant research made on problems at hospitals, legal firms, and financial institutions, extending language models for multilingual usages effectively and efficiently, especially for low-resource languages remains under-explored.\nIn the general NLP domain, FL has been instrumental in tasks such as language modeling, sentiment analysis, and machine translation, showcasing its potential to revolutionize the way models are trained and deployed (Banabilah et al., 2022). Lin et al. (2022) introduces a benchmarking framework for evaluating various FL methods across NLP tasks, providing a universal interface between Transformerbased models and FL methods. Wang et al. (2022)is a federated approach designed for multilingual Natural Language Understanding (NLU) that integrates knowledge from multiple data sources through federated learning techniques to enhance the efficacy and accuracy of multilingual text processing. However, considerations regarding computational and communication efficiency in resource-constrained environments have not been adequately addressed."
        },
        {
            "heading": "3 A NEW PARADIGM FOR MULTILINGUALITY: FEDERATED PROMPT TUNING",
            "text": ""
        },
        {
            "heading": "3.1 NOTATION AND PRELIMINARIES",
            "text": "In our federated learning setup, we have K clients. Each client k has a private dataset, either monolingual or multilingual, defined as:\nDk = {(xk,i, yk,i) | i = 1, . . . , nk}\nwhere xk,i denotes the textual content, and yk,i is its corresponding label. The server sets up and maintains a global prompt encoder. We denote the parameters of the global prompt encoder as hg , and each client k has its own prompt encoder, denoted its parameters by hk, tuned based on its dataset."
        },
        {
            "heading": "3.2 VIRTUAL PROMPT ENCODER",
            "text": "Instead of selecting discrete text prompts in a manual or automated fashion, in our Multilingual Federated Prompt Tuning paradigm, we utilize virtual prompt embeddings that can be optimized via gradient descent. Specifically, each prompt encoder, whether global or local, takes a series of virtual tokens, which are updated during tuning to better aid the model.\nFigure 3 shows the how our prompt tuning works on both clients and server. Specifically, on each client k, a textual prompt tailored for a specific task and input text x are passed to the model. Then task specific virtual tokens are retrieved based on the textual prompt.\nThe primary objective of each prompt encoder is to generate an effective prompt embedding v0, v1, v2, . . . for each client based on task specific virtual tokens, to guide the PLM in producing the desired outputs. With the input text tokenized, the discrete word token embeddings x1, x2, x3, . . . are retrieved. Then virtual token embeddings are inserted among discrete token embeddings and passed together into the PLM. During the fine-tuning phase, based on a task-specific loss, the parameters of the prompt encoder, hk, are often tuned:\nL(x, y;hk) = Loss(D(v \u2295 x), y) (1)\nWhere D can be a decoder that maps the internal representation to task outputs, and Loss is an appropriate loss function, such as Cross-Entropy Loss. During the fine-tuning, the PLM\u2019s parameters keep frozen, whereas the prompt encoder\u2019s parameters hk are updated in accordance with the loss."
        },
        {
            "heading": "3.3 FEDERATED PROMPT AVERAGING",
            "text": "The federated prompt averaging process is as follows (also shown in Algorithm 1):\nIn every communication round t, Federated Prompt Averaging includes the following steps.\nInitialization: The server initializes the global prompt encoder htg. Each client initializes its local prompt encoder ht0, h t 1, . . . h t k.\nClient Selection: We select a fraction C of the total K clients for training. This subset size is m = max(C \u00d7K, 1). The subset we choose is denoted as S. Local Encoder Tuning: Each client k in S fetches the current global prompt encoder htg, and assembles it with the PLM. During the local training on the local data Dk, The PLM\u2019s parameters stay fixed while local prompt encoder parameters htk are tuned.\nAggregation: The server aggregates updates from all clients using weighted average. The global prompt encoder ht+1g is updated based on the received parameters h t k from clients for the next round of federated prompt tuning:\nht+1g = K\u2211 k=1 |Dk|\u2211K k=1 |Dk| htk (2)"
        },
        {
            "heading": "4 EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "4.1 TASKS AND DATASETS",
            "text": "We evaluate our model using the popular XGLUE benchmark (Liang et al., 2020), a cross-lingual evaluation benchmark for our multilingual evaluation. We conduct our experiments on classification tasks of News Classification (NC), XNLI (Conneau et al., 2018) and MasakhaNEWS (Adelani et al., 2023). Accuracy (ACC) of the multi-class classification is used as the metric for both of the tasks.\nNews Classification (NC) is a classification problem with 10 classes across 5 languages: English, Spanish, French, German, and Russian. This task aims to predict the category given a news article. Since only 10k annotated examples are available for each language (excluding the official test set), we sample 8k instances for training and 1k for evaluation sets.\nCross-lingual Natural Language Inference (XNLI) is a cross-lingual sentence understanding problem which covers 15 languages, including high-resource languages (English, French, Spanish, German, Russian and Chinese), medium-resource languages (Arabic, Turkish, Vietnamese and Bulgarian), and low-resource languages (Greek, Thai, Hindi, Swahili and Urdu). The task involves determining the relationship between a premise and a hypothesis sentence, and this relationship can be categorized into one of three classes: entailment, contradiction, or neutral. We sample 2k instances\nfor training and 250 for evaluation sets for each language. NLI serves as an effective benchmark for assessing cross-lingual sentence representations, and better approaches for XNLI will lead to better general Cross-Lingual Understanding (XLU) techniques.\nMasakhaNEWS is a benchmark dataset for news topic classification covering 16 languages widely spoken in Africa, where African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. The task involves categorizing news articles into different categories like sports, business, entertainment, and politics.We choose English, Hausa, Kiswahili, French and Yor\u00f9b\u00e1 in our experiments. We sample 1433 instances for training and 411 for evaluation sets for each language.\nOur base model for both tasks is the XLM-RoBERTa base-sized model (270M parameters), shown to perform well across many languages (Conneau et al., 2020)."
        },
        {
            "heading": "4.2 MULTILINGUAL FINE-TUNING PARADIGMS",
            "text": "1) Local Monolingual fine-tuning: Traditional fine-tuning where a separate model is fine-tuned using the corresponding dataset for each single language locally.\n2) Centralized fine-tuning: Standard fine-tuning using a combined dataset of all languages centralized in the cloud.\n3) Federated Full fine-tuning: Directly fine-tuning the whole pre-trained language model in a federated manner, with the full pre-trained model on the server or each client.\n4) Federated Prompt Tuning: Only training the prompt encoder in a federated manner, with the prompt encoder on the Server or each client.\n5) Federated LoRA (Low-Rank Adaptation): Only training over-parameterized models with a low intrinsic rank in a federated manner, with the trainable rank decomposition matrices on the server or each client.\nIn our tables, we use PE to denote the methods with parameter-efficient techniques."
        },
        {
            "heading": "5 EVALUATION AND ANALYSIS: HOW FEDERATED PROMPT TUNING HELPS MULTILINGUALITY",
            "text": ""
        },
        {
            "heading": "5.1 MAIN RESULTS",
            "text": "Table 3 presents the outcomes of experiments focused on news classification. When employing Prompt Tuning in comparison to Full fine-tuning, there is an acceptable decline in accuracy. Despite this decrease, the overall performance remains consistent and stable. A significant gain in accuracy is observed when adopting the FL approach. It is worth noting that the fine-tuning time is considerably reduced when employing the Prompt Tuning method as opposed to without it. For a comprehensive analysis of this, refer to the section 5.4.\nTable 2 summarises the results of our FL experiments on the XNLI task. To accentuate the potency of our Federated Prompt Tuning approach, a juxtaposition was made with traditional monolingual training. As the data portrays, our Federated Prompt Tuning, particularly on Non-IID dataset, consistently outperformed the monolingual method across all languages. Remarkably, this superior performance was maintained even for languages with limited available data. The average score further substantiates the prowess of Federated Prompt Tuning, marking a noticeable improvement from 32.94% in the monolingual approach to 39.83% with Non-IID Federated Prompt Tuning.\nFrom our results in section 5.1, we observe that some languages demonstrate superior accuracy with the FL method compared to the centralized approach. This enhanced performance might be attributed to the Federated Prompt Averaging in FL, which could introduce similar implicit regularization effects (Izmailov et al., 2018; Rehman et al., 2022). Additionally, the prompt encoder serves as a parameter-efficient alternative. By freezing the core language model parameters, we prevent the model from altering its foundational understanding of language. As a result, the model\u2019s propensity to overfit to a dataset is reduced, minimizing the risk of memorizing specific lexical cues and spurious correlations."
        },
        {
            "heading": "5.2 ABLATION STUDY I: DATA EFFICIENCY",
            "text": "As previous sections mentioned, one characteristic of low-resource languages is their limited available data. Hence, enhancing data sample efficiency is crucial when fine-tuning pre-trained models for downstream tasks. To better validate and simulate the advantages of our approach in real-world scenarios, we reduced the data volume for one language and observed the performance under traditional local fine-tuning as well as our Federated prompt fine-tuning method. We conducted experiments on German News Classification. German was chosen because it represents the language with the fewest resources among the five languages included in this task.\nAs shown in the Figure 4, our Federated Prompt Tuning method consistently outperforms the traditional monolingual approach. As we reduce the dataset size from 8,000 to near-zero, the accuracy of the traditional method drops significantly. On the other hand, the Federated Prompt Tuning method retains its performance, demonstrating its robustness even with limited data. This clearly indicates that our Federated Prompt Tuning approach is better suited for scenarios with limited data availability."
        },
        {
            "heading": "5.3 ABLATION STUDY II: LANGUAGE DISTANCE",
            "text": "As previously mentioned, another characteristic of low-resource languages is that their linguistic features differ from those of high-resource languages, particularly in aspects including syntax, phonology, and inventory. Consequently, direct fine-tuning on models pre-trained with highly dissimilar languages often yields unsatisfying results. Therefore, we conducted an ablation study to examine the impact of language similarity on performance, comparing our Federated Prompt fine-tuning method to the traditional local fine-tuning approach."
        },
        {
            "heading": "5.3.1 MULTILINGUAL DISTANCE MEASUREMENT",
            "text": "We define the pre-trained language as a representative composite language formed by blending each language in the multilingual corpus used for pre-training, in proportion to their amount. This is a formal representation for the mixed dataset composition. We define distance for a specific language in the downstream tasks, in terms of the negative logarithm of its similarity to the pre-trained language.\n(a) fine-tuning accuracy across different languages on the NC task. (b) fine-tuning accuracy across languages with varying similarity to the pre-trained language on the NC task.\n(c) fine-tuning accuracy across different languages on the XNLI task. (d) fine-tuning accuracy across languages with varying similarity to the pre-trained language on the XNLI task.\nFigure 5: Comparative performance of traditional local fine-tuning and our Federated Prompt Tuning method across languages with varying similarity to the pre-trained language for XNLI and NC.\nWe leverage the database from Littell et al. (2017); Malaviya et al. (2017) to extract feature vectors for each language. These vectors are then weighted according to the token count of each language in the pre-trained corpus to calculate the feature vector of the pre-trained language. Given the feature vector Vi for the i-th language, token count Ti, and total tokens Ttotal, the weight wi is given by wi =\nTi Ttotal\nand the feature vector Vp for the pre-trained model is computed as Vp = \u2211n i=1 wi \u00b7 Vi.\nWe define distance for a specific language in the downstream tasks, in terms of the negative logarithm of its cosine similarity to the pre-trained language. Let v represent the feature vector of a specific language in the downstream task. The diversity measure \u03d5 between this language and the average language of the pre-trained model is defined as \u03d5(vi) = \u2212 log(cos(vi, Vp))."
        },
        {
            "heading": "5.3.2 FINE-TUNING LANGUAGES DISTANCE FROM PRE-TRAINED LANGUAGE",
            "text": "Leveraging the distance metric, we compared model performance of languages with varying degrees of distance to the pre-trained language. We present our results from two key experiments on the NC and XNLI tasks. From Figure 5, a conspicuous trend is observed: As the language similarity to the pre-trained language decreases, the model\u2019s accuracy tends to drop. However, when we apply our Federated Prompt method, this decline is notably less steep. This means that even when we are dealing with languages that are quite different from the pre-trained one, our method manages to retain a decent level of accuracy. The difference between our method and the traditional local fine-tuning becomes even more obvious for languages with less data, indicating that our Federated Prompt Tuning method offers significant advantages, particularly in low-resource scenarios."
        },
        {
            "heading": "5.4 ABLATION STUDY III: PARAMETER EFFICIENCY",
            "text": ""
        },
        {
            "heading": "5.4.1 COMPUTATIONAL COST",
            "text": "From the perspective of trainable parameters, this significant reduction in parameters demonstrates exceptional parameter efficiency. In both of the tasks, despite the total number of parameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting for less than 0.5% of the total. Such a design can substantially reduce training time and computational resources, while also mitigating the risk of overfitting. In the context of LLMs, this high parameter efficiency offers potential for model deployment in resource-constrained environments."
        },
        {
            "heading": "5.4.2 COMMUNICATION COST",
            "text": "XLM-Roberta-Base\u2019s data transmission in FL with 5 clients and 10 communication rounds was 108 GB. After our optimization, using a prompt encoder with a 2\u00d7 768 structure, the transmission size reduced to 478.93 MB, a 99% reduction shown in Table 4. This optimization enhances efficiency in federated prompt tuning and expands its applicability to bandwidth-constrained environments including edge devices and mobile networks."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Addressing the complexities of multilingual LLMs, especially for low-resource languages, requires innovative approaches that can balance efficiency, privacy concerns, and performance. Our Multilingual Federated Prompt Tuning paradigm provides a solution to these challenges. By aggregating lightweight multilingual prompts, this approach offers enhanced fine-tuning capabilities with minimal computational demand. The robustness of our method is especially pronounced for low-resource languages with sparse data and rare linguistic features. Overall, this approach promises to advance privacy and linguistic diversity in the realm of NLP. Future work will focus on exploring the impact on the Multilingual Federated Tuning method based on prompt learning as the model scale increases."
        },
        {
            "heading": "A CHALLENGES AND OPPORTUNITIES OF MULTILINGUAL NLP",
            "text": "As natural language processing technologies advance, not all languages have been treated equally by developers and researchers. There are around 7,000 languages spoken in the world, and approximately 400 languages have more than 1 million speakers. However, there is scarce coverage of multilingual datasets. This is especially true for low-resource languages, where data scarcity is a major bottleneck. Furthermore, the under-indexing of certain languages is also driven by access to compute resources. Mobile data, compute, and other computational resources may often be expensive or unavailable in regions that are home to under-represented languages. Unless we address this disproportionate representation head-on, we risk perpetuating this divide and further widening the gap in language access to new technologies. One pressing example is biomedical data. Due to its global scale, this digital content is accessible in a variety of languages, yet most existing NLP tools remain Englishcentric. This situation highlights the need for effective strategies: how can we exploit abundant labeled data from resource-rich languages to make predictions in resource-lean languages?\nAlso, the problem is very timely compared to other application scenarios. It was not even considered a year ago. Previously, due to the smaller size of language models, the demand for data was not as high, and different kinds and sources of data were treated equally. Currently, the progress of LLMs, their usability, the amount of attention they receive, and the increased regulation on data, compound and lead to the urgency of this problem, where we are among the first batch to attempt to break both lingual and physical barriers."
        },
        {
            "heading": "B PROMPT CONSTRUCTION AND INITIALIZATION",
            "text": "When we use Prompt Tuning to optimize the parameter efficiency, the prompt tuning initialization text is Predict the category given the following news article for all the News Classification tasks. By providing the string of words, we initialize virtual token embeddings from existing embedding weights. This string is tokenized and tiled or truncated to match the number of virtual tokens."
        },
        {
            "heading": "C HYPERPARAMETERS AND IMPLEMENTATION",
            "text": "For all of the experiments, we report results using the 1e-3 learning rate, and we use early stopping (5 epochs of no improvement). For a fair comparison with the setup in Houlsby et al. (2019b), we restrict the model sequence length to 512 and used a fixed batch size for all tasks. For FL experiments, we adjust the parameter \u03b1 that controls the mixture of languages in the dataset. An \u03b1 value of 1.0 signifies a uniform mixture of all languages, while values closer to 0 indicate a dominant representation of individual languages or a more separated mixture. When we use Prompt Tuning to optimize the parameter efficiency, the number of virtual tokens is 1, and the prompt tuning init text is Predict the category given the following news article for all the News Classification tasks. When we use LoRA to optimize the parameter efficiency, We use two different ranks (1 and 8), LoRA \u03b1 is 16 and LoRA dropout is 0.1.\nWe use Hugging Face\u2019s transformers library (Wolf et al., 2020) and PEFT library (Mangrulkar et al., 2022) for loading pre-trained models and prompt tuning configurations. For our federated training and evaluation, we use the Flower framework (Beutel et al., 2020) and PyTorch as the underlying auto-differentiation framework (Paszke et al., 2019). We use the AdamW optimizer (Loshchilov & Hutter, 2019; Kingma & Ba, 2015) for all experiments. All experiments are conducted using NVIDIA A40."
        },
        {
            "heading": "D DATASETS FOR GENERATIVE TASKS",
            "text": "MultiLingual Question Answering (MLQA) (Lewis et al., 2019) is a benchmark dataset for cross-lingual question answering performance, covering English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese.\nUN Corpus (Ziemski et al., 2016) is a Machine Translation dataset of official records from the UN proceedings over the years 1990 to 2014, covering six languages: English, French, Spanish, Russian, Chinese, and Arabic. we sample 10k in each direction for training and 5k each for evaluation sets. We cover three machine translation directions: En \u2192 Fr, Ar \u2192 Es, Ru \u2192 Zh, and sample 10k in each direction for training and 5k each for evaluation sets."
        },
        {
            "heading": "E FEDERATED PROMPT AVERAGING ALGORITHM",
            "text": "Algorithm 1 Federated Prompt Averaging\n1: Server executes: 2: Initialize hg 3: for each round t do 4: Select subset S of m clients 5: for each client k in S do 6: Send hg to client k 7: end for 8: Aggregate client updates: 9: ht+1g = \u2211K k=1 |Dk|\u2211K k=1|Dk|\nhtk 10: end for\n1: Client k executes: 2: Retrieve current hg 3: Assemble full model using hk and PLM parameters 4: Train model on Dk 5: Update local prompt encoder hk 6: Send updated hk to server"
        }
    ],
    "year": 2023
}