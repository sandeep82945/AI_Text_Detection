{
    "abstractText": "Learning feature representations of geographical space is vital for any machine 1 learning model that integrates geolocated data, spanning application domains such 2 as remote sensing, ecology, or epidemiology. Recent work mostly embeds coor3 dinates using sine and cosine projections based on Double Fourier Sphere (DFS) 4 features \u2013 these embeddings assume a rectangular data domain even on global 5 data, which can lead to artifacts, especially at the poles. At the same time, rela6 tively little attention has been paid to the exact design of the neural network ar7 chitectures these functional embeddings are combined with. This work proposes 8 a novel location encoder for globally distributed geographic data that combines 9 spherical harmonic basis functions, natively defined on spherical surfaces, with 10 sinusoidal representation networks (SirenNets) that can be interpreted as learned 11 Double Fourier Sphere embedding. We systematically evaluate the cross-product 12 of positional embeddings and neural network architectures across various classifi13 cation and regression benchmarks and synthetic evaluation datasets. In contrast to 14 previous approaches that require the combination of both positional encoding and 15 neural networks to learn meaningful representations, we show that both spherical 16 harmonics and sinusoidal representation networks are competitive on their own 17 but set state-of-the-art performances across tasks when combined. 18",
    "authors": [],
    "id": "SP:ed746f4a53eeefe4205c6595b9fb9c61dcbfe714",
    "references": [
        {
            "authors": [
                "Nico Lang",
                "Walter Jetz",
                "Konrad Schindler",
                "Jan Dirk Wegner"
            ],
            "title": "A high-resolution canopy height",
            "year": 2024
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien Martel",
                "Alexander Bergman",
                "David Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Under review as a conference paper at ICLR",
            "year": 2024
        }
    ],
    "sections": [
        {
            "text": "Learning feature representations of geographical space is vital for any machine1 learning model that integrates geolocated data, spanning application domains such2 as remote sensing, ecology, or epidemiology. Recent work mostly embeds coor-3 dinates using sine and cosine projections based on Double Fourier Sphere (DFS)4 features \u2013 these embeddings assume a rectangular data domain even on global5 data, which can lead to artifacts, especially at the poles. At the same time, rela-6 tively little attention has been paid to the exact design of the neural network ar-7 chitectures these functional embeddings are combined with. This work proposes8 a novel location encoder for globally distributed geographic data that combines9 spherical harmonic basis functions, natively defined on spherical surfaces, with10 sinusoidal representation networks (SirenNets) that can be interpreted as learned11 Double Fourier Sphere embedding. We systematically evaluate the cross-product12 of positional embeddings and neural network architectures across various classifi-13 cation and regression benchmarks and synthetic evaluation datasets. In contrast to14 previous approaches that require the combination of both positional encoding and15 neural networks to learn meaningful representations, we show that both spherical16 harmonics and sinusoidal representation networks are competitive on their own17 but set state-of-the-art performances across tasks when combined.18\n1 INTRODUCTION19\nLocation information is informative meta-data in many geo-spatial applications, ranging from20 species distribution modeling (Mac Aodha et al., 2019; Cole et al., 2023) and wildlife monitoring21 (Jeantet & Dufourq, 2023) to solar irradiance forecasting (Boussif et al., 2023), global tree height22 prediction (Lang et al., 2022) or the estimation of housing prices (Klemmer et al., 2023). Coor-23 dinates also serve as a task-descriptive feature for meta-learning across geographically distributed24 tasks (Tseng et al., 2022) or as input for geographic question answering (Mai et al., 2020a). Lo-25 cation encoders alone can be used to learn an implicit neural representation of a dataset (Stanley,26 2007), for example, to compress weather data (Huang & Hoefler, 2023) or to learn an implicit27 neural representation of species distribution maps (Cole et al., 2023). Recent work here has been28 focused on learning functions on general manifolds (Koestler et al., 2022) and graphs (Grattarola &29 Vandergheynst, 2022) in non-euclidean geometries. This paper focuses on spheres, a special case30 of non-euclidean geometries, where we utilize spherical harmonic basis functions, which are the31 eigenfunctions of the Laplace-Beltrami operator on the sphere (Koestler et al., 2022)32\nMethodologically, predictive modeling with spatial coordinates as inputs has a long history and33 is classically done with non-parametric approaches, like Gaussian Processes (GP), i.e., Kriging,34 (Oliver & Webster, 1990; Matheron, 1969) of kernel methods in general. For instance, Berg35 et al. (2014) proposed adaptive kernels for bird species recognition, where a class-specific kernel36 is learned for each species separately. Relying on spatially close training points is intuitive and37 highly effective for interpolation tasks and small datasets but becomes computationally expensive38 when kernel similarities have to be computed between many test points and large training sets. And39 while research in scaling kernel methods to higher-dimensional data has progressed (Gardner et al.,40 2018), recent focus on scalable methods has shifted to using neural network weights to approximate41 the training points (Tang et al., 2015; Mac Aodha et al., 2019; Chu et al., 2019; Huang & Hoefler,42 2023). Inspired by Double Fourier Sphere (DFS) (Orszag, 1974) and the success of positional en-43\ncodings in transformers, a series of works developed and compared different sine-cosine embedding44 functions at different spatial scales (frequencies) where the number of scales controls the smoothness45 of the interpolation (Mai et al., 2020b; 2023b). Even though these DFS-inspired approaches have46 shown to be effective, they still assume a rectangular domain of longitude and latitude coordinates,47 which does not accurately reflect our planet\u2019s spherical geometry.48\nIn this work, we instead propose to use spherical harmonic basis functions as positional embeddings49 that are well-defined across the entire globe, including the poles. In particular, the scientific use50 of location encoders requires good global performance, with poles being especially important for51 climate science.52\nConcretely, we53\n1. propose Spherical Harmonic (SH) coordinate embeddings that work well with all tested54 neural networks, and especially well when paired with Sinusoidal Representation Networks55 (SirenNet) (Sitzmann et al., 2020), which was untested for location encoding previously;56\n2. show by analytic comparison that SirenNets can be seen as learned Double Fourier Sphere57 embedding, which explains its good performance even with direct latitude/longitude inputs;58\n3. provide a comprehensive comparative study using the cross-product of positional embed-59 ding strategies and neural networks across diverse synthetic and real-world prediction tasks,60 including interpolating ERA5 climate data and species distribution modeling.61\nThroughout the tasks, we observe improved performance of our approach and set a new state-of-62 the-art for geographic location encoding with neural networks, especially in polar regions where63 previous approaches struggle.64\nPE (\n, )\nLi n\nR eL\nU\nLi n\nR eL U D ro po ut Li n R eL U\n4\u21e5ResLayer\nLi ne\nar y\n(a) FCNET PE\n( , )\nLi n\nD ro\np\nSi n\nN\u21e5\nLi ne\nar\ny\n(b) SIRENNET\nGRID\nSPHEREM\nSPHEREC\nDFS\nWRAP\n0 0 1 . . .\n0\n0\n1\n...\n(c) Interaction terms (sin and cos) of the different DFS-based embeddings between longitudes and latitudes at different scales ( s, s). The 0-axis corresponds to single terms without interactions, and non-subscripted , correspond to the original coordinates.\nto appendix A.2. We then describe our approach for a novel global positional encoder, linked with86 SIRENNET (fig. 1b), a network which is broadly used for learning implicit neural representations of87 images (Mildenhall et al., 2021) but has not been tested in this context of location encoding.88\nDFS-based embeddings. In their foundational study, Mac Aodha et al. (2019) use a simple WRAP89 positional encoding PE( , ) = [cos , sin , cos , sin ], which removed the discontinuity at the90\ndateline ( = \u21e1). The original wrap positional encoding has since been identified as a special case91 of Double Fourier Sphere (DFS) (Orszag, 1974) coefficients by Mai et al. (2023b):92\nDFS( , ) = S 1[\nn=0\n[sin n, cos n] [ S 1[\nm=0\n[sin m, cos m][\nS 1[\nn=0\nS 1[\nm=0\n[cos n cos m, cos n sin m, sin n cos m, sin m sin m]\n(2)\nwhere the coordinates , are introduced across multiple frequencies s = \u21b5s and s = \u21b5s by a93\nscale-dependent factor \u21b5s = rmin \u00b7( rmaxrmin ) s\nS 1 that allows users to specify the minimum and maximum94 radii rmin, rmax that can be resolved at multiple scales up to S. The union operator S indicates the95 concatenation of these functions into one large vector.96\nThe full DFS embedding was found to be computationally impractical and quantitatively less ac-97 curate than the special cases SPHEREM and SPHEREC identified by Mai et al. (2023b). Based on98 experimental results, they proposed SPHEREM+ and SPHEREC+, which is a combination with the99 GRID embedding function of Mai et al. (2020b). In the same paper, Mai et al. (2020b) proposed to100 use hexagonal basis functions rather than rectangular grid cells, which is the THEORY embedding101 function that we will compare to in the results section. Adapted from (Mai et al., 2023b), we show102 a visual representation of these eq. (2)-based embeddings in fig. 1c.103\nBaseline embeddings. DIRECT, i.e., using the identity function to encode ( , ), or CARTE-104 SIAN3D, which correponds to encoding ( , ) into (x, y, z) coordinates, are also common baselines105 still used in recent work (Tseng et al., 2022).106\nOur Approach. We depart from DFS and instead propose to use SPHERICAL HARMONIC (SH)107 basis functions (detailed in section 2.2), which have a long tradition in the Earth sciences for rep-108 resenting the gravity field (Pail et al., 2011) or the magnetosphere of planets (Smith et al., 1980).109 The SH basis functions can be multiplied with learnable coefficient weights. Examples of SH basis110 functions and their corresponding learnable triangle weight matrix is shown in fig. 2a. The weight111 matrix can be expressed as a linear layer, i.e., a very simple \u201cneural network\u201d following the location112 encoding terminology of eq. (1). It can also be combined with more complex neural networks; we113 propose to use Sinusoidal Representation Networks (SIRENNET) (Sitzmann et al., 2020)\u2013shown in114 fig. 1b\u2013which are architecturally simpler than the FCNET (fig. 1a) used in related works. We com-115 pare SIRENNET with DFS encodings used in previous location encoding approaches in section 2.3.116\n2.2 WEIGHTED SPHERICAL HARMONICS117\nAny function on a sphere\nf( , ) = 1X\nl=0\nlX\nm= l w\nm l Y m l ( , ) (3)\nwith surface points defined by (lat, lon) = ( , ) can be expressed by the wm l weighted sum of orthogonal spherical harmonic basis functions Y m\nl of increasingly higher-frequency degrees l and\norders m. In practice, we choose a maximum number of Legendre polynomials L (instead of 1 in eq. (3)) to approximate a spherical function up to the highest-frequency harmonic. To illustrate, we draw examples of harmonics up to L = 3 in fig. 2a (bottom left) alongside their coefficients (top left), which combined approximate the landmass outline of our planet (right) with only nine trainable weights. Each individual harmonic\nY m l ( , ) =\ns 2l + 1\n4\u21e1 (l |m|)! (l + |m|)!P m l (cos )eim (4)\ncan be calculated with an associated Legendre polynomial Pm l . This polynomial can be analytically118 constructed by repeated differentiation Pm\nl (x) = ( 1)m(1 x2)m2 d\nm\ndxm (Pl(x)) of their zero-order119 (m = 0) Legendre polynomials Pl = P 0l . Zero-order polynomials of a certain degree l are similarly120 obtained by repeated differentiation Pl(x) = 12ll! d l dxl \u21e5 (x2 1)l \u21e4 . To obtain associated polynomials121\nof negative order (m < 0), symmetries P m l (x) = ( 1)m (l m)!(l+m)!P m l (x) can be used.122\nWe are interested in the real form of the original complex eq. (4), which can be implemented as\nYl,m( , ) =\n8 ><\n>:\n( 1)m p 2P\u0304 |m|\nl (cos ) sin(|m| ), if m < 0\nP\u0304 m l (cos ), if m = 0\n( 1)m p 2P\u0304m\nl (cos ) cos(m ), if m > 0\n(5)\nwith cosine and sine functions, where P\u0304m l (cos ) = q 2l+1 4\u21e1 (l |m|)! (l+|m|)!P m l (cos ) denotes the normal-123 ized associated Legendre polynomial. The polynomials can be computed iteratively in closed-form124 P m\nl (x) = ( 1)m \u00b7 2l \u00b7 (1 x1)m2 \u00b7\nP l\nk=m k! (k m!) \u00b7 x k m \u00b7 N k\n\u00b7 l+k 1\n2 l\nfollowing Green (2003).125\nHowever, pre-computing the polynomials analytically is more computationally efficient than using126 the closed-form, as we show empirically later in the results section. We use this analytically pre-127 computed implementation in experiments throughout the paper.128\n2.3 SINUSOIDAL REPRESENTATION NETWORKS AS LEARNED DFS EMBEDDINGS129\nBesides SPHERICAL HARMONICS, we propose to use Sinusoidal Representation Networks130 (SIRENNETS) (Sitzmann et al., 2020) as a preferred neural network for location encoding. We now131 show that SIRENNETS can be seen as a form of learned Double Fourier Sphere (DFS) positional132 embedding.133\nWe use the GRID embedding function by Mai et al. (2023b) as a simple example, but the following comparison can also be made for other DFS embeddings. GRID is defined as: GRID( , ) = S\nS 1 s=0 [cos s, sin s, cos s, sin s] = S S 1 s=0 h cos( \u21b5s ), sin( \u21b5s ), cos( \u21b5s ), sin( \u21b5s ) i , where \u21b5s is a scaling factor that induces increasingly high frequencies through scales s from 0 to S 1 and S indicates concatenation. With the 90-degree phase shift between sine and cosine, i.e., cos(\u2713) = sin(\u2713 + \u21e12 ), we can rewrite GRID solely using sine functions\nGRID( , ) = S 1[\ns=0\n sin(\n\u21b5s +\n\u21e1 2 ), sin( \u21b5s ), sin( \u21b5s + \u21e1 2 ), sin( \u21b5s ) , (6)\nwhere every block of four feature dimensions (indexed by s) is scaled by the same constant factor134 1 \u21b5s and every odd feature embedding is phase shifted (+\u21e12 ).135\nSimilarly, a simple 1-layer SIRENNET can be written out as SIRENNET( , ) = sin(W[ , ]T + b) = sin(w +w +b) with W = [w ,w ] 2 R[H,2], b 2 RH trainable weights and H hidden output dimensions. Writing out the vector-matrix multiplication as concatenated scalars results in SIRENNET( , ) = S H\nh=1[sin(w h + w h + b)]. If we now set every w h = w h+1 = 0 and every w\nh+2 = w h+3 = 0, we similarly alternate between longitudinal and latitudinal scaling for each feature. If we now also set bh+1 = bh+3 = 0, we arrive at a GRID-like SIREN variant\nGRIDSIREN( , ) = H 1[\nh=0,4,...\n[sin(w h + bh), sin(w h+1 ), sin(w h+2 + bh+2), sin(w h+3 )], (7)\nwhich corresponds to eq. (6) if bh = bh+2 = \u21e12 and all w h = w h+1 = w h+2 = w h+3 = 1 \u21b5s .136 Hence, a 1-layer SIRENNET with these hard-coded weights equals the GRID DFS encoding, where137 the longitudes and latitudes are manually scaled to produce lower or higher frequencies in each138 embedding dimension. Crucially, these scaling factors are learned in GRIDSIREN through gradient139 descent, while they are fixed for every block of four in the GRID embedding in eq. (6). This ob-140 servation highlights that SIRENNET blurs the distinction between Positional Embedding (PE) and141 Neural Network (NN) from eq. (1) and analytically explains the good performance of SIRENNET142 even with a simple DIRECT embedding functions, as we will show later.143\n3 DATASETS AND EXPERIMENTS144\nThe experiments in this work span from controllable synthetic problems to real-world data. Across145 all datasets, the location encoder model is fitted on training data to learn an underlying signal de-146 fined by the training label (probability map or regression value). How well the signal has been147 approximated in the network weights is measured on spatially different test points.148\n3.1 CHECKERBOARD CLASSIFICATION: A CONTROLLABLE SYNTHETIC TOY DATASET149\nA Fibonacci lattice is a mathematical idealization of natural patterns on a sphere with optimal pack-150 ing, where the surface area represented by each point is almost identical (Gonza\u0301lez, 2010) within an151 error of 2% (Swinbank & James Purser, 2006). It is calculated for 2N points (i ranges from N to152 N ) as i = arcsin 2i2N+1 and i = 2\u21e1i , with the Golden Ratio = 1 2 (1+ p 5) \u21e1 1.618. To make153 this a classification problem, we choose 100 points according to the Fibonacci-lattice and assign154 each point one of 16 classes in a regular order (i modulo 16). In later experiments, we increase155 or decrease the number of grid cells to vary the spatial scale and measure the maximum resolvable156 resolution of location encoders. For the test dataset (fig. 3a), we sample a second Fibonacci-lattice157 grid with 10 000 points and assign each point the class of the closest label point using the spheri-158 cal Haversine distance. For training and validation sets, we uniformly sample 10 000 points on the159 sphere and similarly assign the label of the closest labeled point.160\n3.2 LAND-OCEAN CLASSIFICATION: IMPLICIT NEURAL REPRESENTATION OF LAND161\nWe also design a more realistic land-ocean classification dataset shown in fig. 3b. This dataset162 tests the encoder\u2019s ability to learn patterns across different scales and resolutions, while providing163 training and evaluation points across the entire globe. For training and validation data, we sample164 5000 points uniformly on the sphere\u2019s surface and assign a positive label for land and a negative label165 for water depending on whether they are within landmasses of the \u201cNatural Earth Low Resolution\u201d166 shapefile1. For the test dataset, we generate 5000-equally spaced points using a Fibonacci-lattice.167\n3.3 MULTI-VARIABLE ERA5 INTERPOLATION168\nNext, we use real-world climate data. We downloaded globally distributed climate variables at169 Jan. 1, 2018, at 23:00 UTC from the fifth-generation atmospheric reanalysis of the global cli-170 mate (ERA5) product of the European Centre for Medium-Range Weather Forecasts. This dataset171 contains 6 483 600 observations corresponding to regularly gridded locations on the surface of our172\n1https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/physical/ne 50m land.zip\nplanet. At each location, we obtain eight continuous climate variables: the u and v components173 of near-surface wind speed, near-surface air temperature, surface air pressure, near-surface specific174 humidity, surface shortwave and longwave radiation, and total precipitation (rainfall and snowfall175 flux). An illustration of near surface air temperature (centered on the North pole) is shown in fig. 3c.176 We design a multi-task regression in which the model learns all eight variables jointly from a small177 (1%) randomly sampled training set. Another 5% of the data is reserved for the validation set.178 Through this dataset, we approximate the continuous signal underlying the gridded climate data into179 the model weights of a neural network and measure its ability to reproduce the signal continuously180 at any point without explicit interpolation of neighboring grid cells.181\n3.4 INATURALIST 2018: LOCATION-INFORMED IMAGE CLASSIFICATION182\nFinally, we test the location encoders\u2019 ability to encode location as auxiliary information for fine-183 grained species classification with georeferenced images. Here, we use the iNaturalist (iNat2018)184 (Van Horn et al., 2018) dataset, which contains georeferenced images of plant and animal species. It185 is a real-world dataset with several data-inherent challenges: it presents a fine-grained classification186 problem with 8142 individual categories in a long-tailed class distribution. It has a strong sampling187 bias toward the US and Europe due to crowd-sourcing. Adding location information to an image188 classifier has been proven beneficial, as, e.g., similarly-looking species from different continents189 can be ruled out. Hence, iNaturalist 2018 has been used extensively to benchmark location encoders190 (Mac Aodha et al., 2019; Mai et al., 2020b; 2023a). Following these works, we used the public191 training split of 436 063 georeferenced images and partitioned it randomly into training and valida-192 tion datasets in an 80:20 ratio. The public validation split of 24 343 georeferenced images was used193 as a test dataset. We combine the probabilities of the location encoder P (y| , ) and image clas-194 sifier P (y|I) by multiplication, as shown schematically in fig. 3d. A fixed pre-trained InceptionV3195 network from Mac Aodha et al. (2019) serves as the image classifier. The results section reports the196 relative accuracy increase that the location encoder adds to the base image classifier. Through this197 dataset, we learn a continuous species distribution map in the model weights from point observations198 of training points. At test time, the model predicts the probability of species presence at new test199 locations, which informs a separate image classifier and increases the overall prediction accuracy.200\n4 RESULTS201 The implementation details and hyperparameter tuning protocol can be found in appendix A.1.202\n4.1 COMPARATIVE PERFORMANCE203 Checkerboard and Land-Ocean Classification. tables 1a and 1b show averaged classification ac-204 curacies and standard deviations over five runs of the checkerboard and land-ocean classification205 datasets, respectively. We observe that the SPHERICAL HARMONICS (SH) embedding works well206\n(c) Inaturalist 2018: Improved classification accuracy with location. Standard dev. from 5 runs; higher is better. PE # NN ! LINEAR FCNET SIRENNET DIRECT 5.9\u00b1 0.1 +9.3\u00b1 0.3 +12.1\u00b1 0.1 CARTESIAN3D +0.8\u00b1 0.2 +11.8\u00b1 0.1 +12.0\u00b1 0.1 WRAP 0.1\u00b1 0.1 +12.1\u00b1 0.1 +12.1\u00b1 0.1 GRID +11.2\u00b1 0.1 +11.8\u00b1 0.2 +11.6\u00b1 0.4 THEORY +11.5\u00b1 0.0 +10.8\u00b1 0.0 +11.4\u00b1 0.1 SPHEREC +11.2\u00b1 0.1 +12.0\u00b1 0.2 +12.3\u00b1 0.1 SPHEREC+ +11.1\u00b1 0.2 +11.5\u00b1 0.3 +10.3\u00b1 0.4 SPHEREM +7.2\u00b1 0.2 +11.3\u00b1 0.2 +10.6\u00b1 0.6 SPHEREM+ +11.6\u00b1 0.1 +12.0\u00b1 0.1 +10.7\u00b1 0.2 SH (ours) +10.5\u00b1 0.1 +12.0\u00b1 0.0 +12.3\u00b1 0.2\nimage-only: 59.2% top-1 accuracy with encoder NN(PE) \"\nPE # NN ! LINEAR FCNET SIRENNET DIRECT 4.7\u00b1 0.1 +7.1\u00b1 0.2 +8.8\u00b1 0.1 CARTESIAN3D +0.8\u00b1 0.1 +8.6\u00b1 0.1 +8.7\u00b1 0.1 WRAP 0.1\u00b1 0.1 +8.6\u00b1 0.2 +8.7\u00b1 0.1 GRID +8.1\u00b1 0.1 +8.3\u00b1 0.1 +8.5\u00b1 0.1 THEORY +8.2\u00b1 0.1 +8.0\u00b1 0.1 +8.3\u00b1 0.2 SPHEREC +7.9\u00b1 0.1 +8.3\u00b1 0.1 +8.8\u00b1 0.1 SPHEREC+ +7.8\u00b1 0.0 +8.3\u00b1 0.1 +7.8\u00b1 0.2 SPHEREM +4.9\u00b1 0.1 +7.5\u00b1 0.2 +7.8\u00b1 0.3 SPHEREM+ +8.2\u00b1 0.1 +8.4\u00b1 0.1 +8.0\u00b1 0.1 SH (ours) +7.5\u00b1 0.1 +8.4\u00b1 0.1 +9.0\u00b1 0.1 image-only: 77.0% top-3 accuracy with encoder NN(PE) \"\nfor all neural networks, including the minimalist LINEAR layer. Similarly, SIRENNET (Sitzmann207 et al., 2020) achieves high accuracies consistently across all positional embeddings, while the FC-208 NET (Mac Aodha et al., 2019) shows larger variability, especially with simpler embedding functions,209 such as CARTESIAN3D, DIRECT, or WRAP.210\nERA5. Table 2 shows the mean squared211 error averaged over the eight climate vari-212 ables in the ERA5 dataset. Means and213 standard deviations are obtained from 10214 runs with different random initializations.215 This multi-task regression is especially216 difficult as it requires the location encoder217 to learn the spatial patterns of each channel218 separately and their interactions. SPHERI-219 CAL HARMONIC embeddings consistently220 achieve the lowest error for all neural net-221 works, while the combination with FC-222 NET was best with 0.58 \u00b1 0.02. This223 constitutes a substantial performance gain224\nover the next best existing competitor (SPHEREC+ and FCNET) with 1.38\u00b1 0.03.225 iNaturalist. Table 1c shows the relative top-1 and top-3 accuracy improvements that location in-226 formation through P (y|I, , ) adds to an image-only classifier P (y|I). The underlying image-227 only classification top-1 and top-3 accuracies are 59.2% and 77.0%, respectively. All location en-228 coders improve the top-1 accuracy substantially between +9.3% (FCNET(DIRECT)) and +12.2%229 (SIRENNET(SH)) except for the trivial LINEAR(DIRECT) and too simple LINEAR(WRAP) encod-230 ings. At top-1 accuracy, the SPHERICAL HARMONIC embedding used with the SIRENNET network231 is slightly more accurate than the other approaches but still roughly within the standard deviations232 (between 0.1% and 0.4%) over five evaluation runs. However, for other neural networks, other233 positional encoders also achieve the best column score (underlined). At top-3 accuracy, we see234 lower variances in general between standard deviations of 0.1 percentage. Also here, SIREN(SH) is235 slightly better with +9.0 \u00b1 0.1 % compared to +8.8 \u00b1 0.1% of SIREN(DIRECT) and significantly236 better than SPHEREM+ (Mai et al., 2023b) with +8.4%.237\nTakeaways. The SPHERICAL HARMONIC (SH) embedding function achieved the best results across238 all four datasets and consistently across different neural networks for three of them. For iNaturalist,239 SPHERICAL HARMONICS with SIRENNET was still best (bolded), but several other positional em-240 beddings were better in combination with FCNET and LINEAR (underlined). We generally connect241 this to the location bias in iNaturalist, where only a few species observations are recorded at higher242 latitudes, where good representations of the spherical geometry are most advantageous. Comparing243 neural networks: the commonly used FCNET (Mac Aodha et al., 2019) achieved the best results244 in predicting ERA5 interpolation and Land-Ocean classification, while the SIRENNET network was245 best in the iNaturalist and checkerboard problems. Overall, SIRENNET performed well across all246 positional embedding functions, including the trivial DIRECT embedding. Similarly, SPHERICAL247 HARMONICS performed reasonably well, even with a simple LINEAR layer on all datasets.248\n4.2 PROPERTIES AND CHARACTERISTICS249\nNext, we investigate characteristics of SPHERICAL HARMONICS with SIRENNET with respect to250 key properties of location encoders: resolvable resolution, accuracy across latitudes, and computa-251 tional efficiency.252\nResolvable Resolution. In fig. 4a, we focus on resolution limitations of SPHERICAL HARMON-253 ICS (SH) of different orders L. On the checkerboard dataset, we increase the number of grid cells254 in the Fibonacci lattice, which produces denser and increasingly fragmented patterns. We report255 the average distance between points on the x-Axis (in \u00b0 degree) as an indication of resolution. We256 test two SPHERICAL HARMONIC positional embeddings with 10 and 20 Legendre polynomials257 (SHL=10, SHL=20). We primarily use a LINEAR layer as a \u201cneural network\u201d to highlight the reso-258 lution limitations of spherical harmonics alone, as it represents eq. (3) where a single trainable pa-259 rameter weights one harmonic. In this case, we can analytically calculate the maximum frequency260 f\nmax = 1802L that a linear spherical harmonic embedding should be able to resolve. In the plot, we261 observe a decrease in accuracy at resolutions higher than 9- and 18-degree resolutions, respectively.262 Hence, more Legendre polynomials L allow the SH(LINEAR) model to resolve higher-frequency263\ncomponents. Replacing the LINEAR layer with a more complex SIRENNET network (dotted lines)264 further improves the model\u2019s resolution, indicated by a higher accuracy at smaller distances be-265 tween centers. Both SIRENNET(SHL=20) and SIRENNET(SHL=20) remain accurate up to grid cells266 spaced 6\u00b0 degrees apart with a slight edge for SIRENNET(SHL=20) thanks to the larger number of267 spherical harmonics. Hence, both increasing the number of Legendre polynomials and the complex-268 ity of the neural network (i.e., SIRENNET instead of LINEAR) helps resolve high-frequency signals269 on the sphere. This property can also be seen qualitatively in fig. 2b on the example of land-ocean270 classification.271\nLatitudinal Accuracy. In fig. 4b, we compare the checkerboard classification accuracies of SPHER-272 ICAL HARMONICS (SH), GRID (Mai et al., 2020b) and SPHEREC+ (Mai et al., 2023b) positional273 embeddings, combined with the FCNET (left) and SIRENNET (right) networks separated by 20\u00b0274 north-south spanning latitude bands. The DFS-based encodings (GRID and SPHEREC+) assume a275 rectangular domain of latitude and longitude. This assumption is violated at the poles (90\u00b0 North276 and 90\u00b0 South) where all meridians converge to a single point. This leads to polar prediction arti-277 facts in the checkerboard pattern, as shown in fig. 4c (left), and consequently to lower accuracies at278 higher latitudes. The SIRENNET network helps reduce these artifacts at the 50\u00b0 to 70\u00b0 bands N &279 S, but a performance degradation is still clearly observable close to the poles (70\u00b0-90\u00b0). In contrast,280 no substantial degradation at higher latitudes is visible on the location encoder with the spherical281 harmonic embedding function with both FCNET (left) and SIRENNET (right) networks, as spherical282 harmonics are well-defined on the entire sphere. Such artifacts also occur on real-world datasets like283 iNaturalist, as shown as vertical lines along the North-South meridians on the learned Arctic Fox284 prediction map from iNaturalist shown in fig. 4c (right) with the FCNET(GRID) location encoder.285\nComputational Efficiency of Positional Encoders.286 Figure 5 shows the computational impact measured in287 inference runtime of 10 000 test points of the checker-288 board dataset with an increasing number of Legen-289 dre polynomials (or scales for SPHEREC+) on the290 checkerboard dataset. We first compare a spherical291 harmonics implementation that uses the closed form292 implementation (SH-closed-form) following (Green,293 2003) with the SPHEREC+ embedding. The closed-294 form implementation (SH-closed-form) scales poorly295 with large orders, as the inference time reaches 10 sec-296 onds with L = 40 compared to 0.3s of SPHEREC+297 (Mai et al., 2023b). To overcome this limitation, we298 calculate the individual spherical harmonics analyti-299\ncally for each harmonic with eq. (5) and simplify the equation with SymPy (Meurer et al., 2017).300 This leads to a substantial performance improvement at 1 second for L = 40 over the closed-form301 implementation. All other experimental results in the paper use this analytic calculation, which is302 faster regarding runtime than the provided SPHEREC+ implementation up to L > 20 polynomials.303\n5 CONCLUSION AND OUTLOOK304\nEncoding geographical location effectively and in a usable way is key to meet many real-world305 modeling needs and requires taking our planet\u2019s spherical geometry into account. To this end, we306 propose to use orthogonal SPHERICAL HARMONIC basis functions paired with sinusoidal represen-307 tation networks (SIRENNETs) to learn representations of geographic location. Even though these308 spherical harmonics are not as trivial to implement as classical sine-cosine functions, they are inher-309 ently better suited to represent functions on the sphere and can be applied at a competitive compu-310 tational runtime to existing sine-cosine encodings. Regarding neural networks, we have shown that311 SIRENNET can be seen as a learned Double Fourier Sphere positional embedding, which provides312 an intuition to explain its good performance even with a simple DIRECT positional embedding. This313 blurs the established division of positional embedding and neural networks established in previous314 work (Mai et al., 2022) and opens new research questions towards learning more parameter-efficient315 positioned embeddings. We hope to have provided insight into best practices for location encoding316 of large-scale geographic data and can concretely recommend SIRENNET for any problem involving317 geographic coordinates and SPHERICAL HARMONIC embeddings for problems that involve data at318 a global scale or in polar regions, which are not represented well by existing approaches.319\nREFERENCES320 Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:321 A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM322 SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2623\u20132631,323 2019.324\nThomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N325 Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In Proceedings of326 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2011\u20132018, 2014.327\nOussama Boussif, Ghait Boukachab, Dan Assouline, Stefano Massaroli, Tianle Yuan, Loubna Ben-328 abbou, and Yoshua Bengio. What if we enrich day-ahead solar irradiance time series forecasting329 with spatio-temporal context? arXiv preprint arXiv:2306.01112, 2023.330\nGrace Chu, Brian Potetz, Weijun Wang, Andrew Howard, Yang Song, Fernando Brucher, Thomas331 Leung, and Hartwig Adam. Geo-aware networks for fine-grained recognition. In Proceedings of332 the IEEE/CVF International Conference on Computer Vision Workshops, pp. 0\u20130, 2019.333\nElijah Cole, Grant Van Horn, Christian Lange, Alexander Shepard, Patrick Leary, Pietro Perona,334 Scott Loarie, and Oisin Mac Aodha. Spatial implicit neural representations for global-scale335 species mapping. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,336 Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on337 Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 6320\u20136342.338 PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/cole23a.339 html.340\nWilliam Falcon and The PyTorch Lightning team. PyTorch Lightning, March 2019. URL https:341 //github.com/Lightning-AI/lightning.342\nJacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wil-343 son. GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration.344 Advances in Neural Information Processing Systems, 2018.345\nA\u0301lvaro Gonza\u0301lez. Measurement of areas on a sphere using fibonacci and latitude\u2013longitude lattices.346 Mathematical Geosciences, 42:49\u201364, 2010.347\nDaniele Grattarola and Pierre Vandergheynst. Generalised implicit neural representations. Advances348 in Neural Information Processing Systems, 35:30446\u201330458, 2022.349\nRobin Green. Spherical harmonic lighting: The gritty details. In Archives of the Game Developers350 Conference, volume 56, pp. 4, 2003.351\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-352 nition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,353 pp. 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.354\nLangwen Huang and Torsten Hoefler. Compressing multidimensional weather and climate data into355 neural networks. International Conference on Learning Representations, 2023.356\nLore\u0300ne Jeantet and Emmanuel Dufourq. Improving deep learning acoustic classifiers with contextual357 information for wildlife monitoring. Ecological Informatics, 77:102256, 2023. ISSN 1574-9541.358 doi: https://doi.org/10.1016/j.ecoinf.2023.102256. URL https://www.sciencedirect.359 com/science/article/pii/S1574954123002856.360\nKonstantin Klemmer, Nathan S Safir, and Daniel B Neill. Positional encoder graph neural networks361 for geographic data. In International Conference on Artificial Intelligence and Statistics, pp.362 1379\u20131389. PMLR, 2023.363\nLukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, and Zorah La\u0308hner. Intrinsic364 neural fields: Learning functions on manifolds. In European Conference on Computer Vision, pp.365 622\u2013639. Springer, 2022.366\nNico Lang, Walter Jetz, Konrad Schindler, and Jan Dirk Wegner. A high-resolution canopy height367 model of the Earth. arXiv preprint arXiv:2204.08322, 2022.368\nOisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-only geographical priors for fine-369 grained image classification. In Proceedings of the IEEE/CVF International Conference on Com-370 puter Vision, pp. 9596\u20139606, 2019.371\nGengchen Mai, Krzysztof Janowicz, Ling Cai, Rui Zhu, Blake Regalia, Bo Yan, Meilin Shi, and372 Ni Lao. Se-kge: A location-aware knowledge graph embedding model for geographic question373 answering and spatial semantic lifting. Transactions in GIS, 24(3):623\u2013655, 2020a.374\nGengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, and Ni Lao. Multi-scale repre-375 sentation learning for spatial feature distributions using grid cells. International Conference on376 Learning Representations, 2020b.377\nGengchen Mai, Krzysztof Janowicz, Yingjie Hu, Song Gao, Bo Yan, Rui Zhu, Ling Cai, and Ni Lao.378 A review of location encoding for GeoAI: methods and applications. International Journal of379 Geographical Information Science, 36(4):639\u2013673, 2022.380\nGengchen Mai, Ni Lao, Yutong He, Jiaming Song, and Stefano Ermon. Csp: Self-supervised con-381 trastive spatial pre-training for geospatial-visual representations. 2023a.382\nGengchen Mai, Yao Xuan, Wenyun Zuo, Yutong He, Stefano Ermon, Jiaming Song, Krzysztof383 Janowicz, and Ni Lao. Sphere2vec: Self-supervised location representation learning on spherical384 surfaces. ISPRS Photogrammetry and Remote Sensing Journal., 2023b.385\nJulieta Martinez, Rayat Hossain, Javier Romero, and James J Little. A simple yet effective baseline386 for 3d human pose estimation. In Proceedings of the IEEE international conference on computer387 vision, pp. 2640\u20132649, 2017.388\nGeorges Matheron. Le krigeage universel (Universal kriging). Cahiers du Centre de Morphologie389 Mathematique, 1:83, 1969.390\nPhilip E. Merilees. The pseudospectral approximation applied to the shallow water equations on a391 sphere. Atmosphere, 11(1):13\u201320, 1973. doi: 10.1080/00046973.1973.9648342. URL https:392 //doi.org/10.1080/00046973.1973.9648342.393\nAaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondr\u030cej C\u030cert\u0131\u0301k, Sergey B. Kirpichev,394 Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rath-395 nayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam396 Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, S\u030cte\u030cpa\u0301n Rouc\u030cka,397 Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Scopatz. Sympy:398 symbolic computing in python. PeerJ Computer Science, 3:e103, January 2017. ISSN 2376-5992.399 doi: 10.7717/peerj-cs.103. URL https://doi.org/10.7717/peerj-cs.103.400\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and401 Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications402 of the ACM, 65(1):99\u2013106, 2021.403\nMargaret A Oliver and Richard Webster. Kriging: A method of interpolation for geographical404 information systems. International Journal of Geographical Information System, 4(3):313\u2013332,405 1990.406\nSteven A Orszag. Fourier series on spheres. Monthly Weather Review, 102(1):56\u201375, 1974.407\nRoland Pail, Sean Bruinsma, Federica Migliaccio, Christoph Fo\u0308rste, Helmut Goiginger, Wolf-Dieter408 Schuh, Eduard Ho\u0308ck, Mirko Reguzzoni, Jan Martin Brockmann, Oleg Abrikosov, Martin Ve-409 icherts, Thomas Fecher, Reinhard Mayrhofer, Ina Krasbutter, Fernando Sanso\u0300, and Carl Chris-410 tian Tscherning. First goce gravity field models derived by three different approaches. Jour-411 nal of Geodesy, 85(11):819\u2013843, 2011. doi: 10.1007/s00190-011-0467-x. URL https:412 //doi.org/10.1007/s00190-011-0467-x.413\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-414 plicit neural representations with periodic activation functions. Advances in Neural Information415 Processing Systems, 33:7462\u20137473, 2020.416\nEJ Smith, L Davis Jr, DE Jones, P Jo Coleman Jr, DS Colburn, P Dyal, and CP Sonett. Saturn\u2019s417 magnetic field and magnetosphere. Science, 207(4429):407\u2013410, 1980.418\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.419 Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine420 Learning Research, 15(56):1929\u20131958, 2014. URL http://jmlr.org/papers/v15/421 srivastava14a.html.422\nKenneth O Stanley. Compositional pattern producing networks: A novel abstraction of development.423 Genetic programming and evolvable machines, 8:131\u2013162, 2007.424\nRichard Swinbank and R James Purser. Fibonacci grids: A novel approach to global modelling.425 Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences,426 applied meteorology and physical oceanography, 132(619):1769\u20131793, 2006.427\nKevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, and Lubomir Bourdev. Improving image428 classification with location context. In Proceedings of the IEEE International Conference on429 Computer Vision, pp. 1008\u20131016, 2015.430\nGabriel Tseng, Hannah Kerner, and David Rolnick. TIML: Task-informed meta-learning for agri-431 culture. arXiv preprint arXiv:2202.02124, 2022.432\nGabriel Tseng, Ivan Zvonkov, Mirali Purohit, David Rolnick, and Hannah Kerner. Lightweight,433 pre-trained transformers for remote sensing timeseries. arXiv preprint arXiv:2304.14065, 2023.434\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam,435 Pietro Perona, and Serge Belongie. The iNaturalist species classification and detection dataset.436 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.437 8769\u20138778, 2018.438"
        }
    ],
    "year": 2023
}