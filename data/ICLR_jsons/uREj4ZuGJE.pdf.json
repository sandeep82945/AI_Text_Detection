{
    "abstractText": "We propose the In-context Autoencoder (ICAE), leveraging the power of a large language models (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context; Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing fewer than 1% additional parameters, effectively achieves 4\u00d7 context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE\u2019s significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and model will be released. As ar&ficial intelligence becomes an increasingly powerful force, some of the world\u2019s biggest companies are worrying about how the technology will be used ethically, and how the public will perceive its spread. To combat these problems (among others), five tech companies \u2014 Google, Amazon, MicrosoD, Facebook, and IBM \u2014 set up a research group called the Partnership on AI. ... AI taking white collar jobs, eroding trust in public media, becoming embedded in public ins&tu&ons like the courts and hospitals: these are the sorts of problems facing the industry in the future. Long context m\u03031 ... Memory Slots\t\u0303 mk m\u03031 m\u03032 ... m\u0303k Context: Prompt: List the companies that set up Partnership on AI research group LLM Response: Google, Amazon, MicrosoD, Facebook and IBM. m\u03031 m\u03032 ... m\u0303k Context: Prompt: What are poten6al challenges the AI industry might face in the future? LLM Response: AI taking white collar jobs, eroding trust in public media, and becoming embedded in public ins&tu&ons. m\u03032 In-ntext autncoding Figure 1: Compressing a long context into a short span of memory slots. The memory slots can be conditioned on by the target LLM on behalf of the original context to respond to various prompts.",
    "authors": [],
    "id": "SP:c9c0778c00935174f654a9354d8b676c939e3a4c",
    "references": [
        {
            "authors": [
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Dawn Drain",
                "Deep Ganguli",
                "Tom Henighan",
                "Andy Jones",
                "Nicholas Joseph",
                "Ben Mann",
                "Nova DasSarma"
            ],
            "title": "A general language assistant as a laboratory for alignment",
            "venue": "arXiv preprint arXiv:2112.00861,",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Bavarian",
                "Heewoo Jun",
                "Nikolas Tezak",
                "John Schulman",
                "Christine McLeavey",
                "Jerry Tworek",
                "Mark Chen"
            ],
            "title": "Efficient training of language models to fill in the middle",
            "venue": "arXiv preprint arXiv:2207.14255,",
            "year": 2022
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150,",
            "year": 2020
        },
        {
            "authors": [
                "Aydar Bulatov",
                "Yury Kuratov",
                "Mikhail Burtsev"
            ],
            "title": "Recurrent memory transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Aydar Bulatov",
                "Yuri Kuratov",
                "Mikhail S Burtsev"
            ],
            "title": "Scaling transformer to 1m tokens and beyond with rmt",
            "venue": "arXiv preprint arXiv:2304.11062,",
            "year": 2023
        },
        {
            "authors": [
                "Alexis Chevalier",
                "Alexander Wettig",
                "Anirudh Ajith",
                "Danqi Chen"
            ],
            "title": "Adapting language models to compress contexts",
            "venue": "arXiv preprint arXiv:2305.14788,",
            "year": 2023
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509,",
            "year": 2019
        },
        {
            "authors": [
                "Krzysztof Choromanski",
                "Valerii Likhosherstov",
                "David Dohan",
                "Xingyou Song",
                "Andreea Gane",
                "Tam\u00e1s Sarl\u00f3s",
                "Peter Hawkins",
                "Jared Davis",
                "Afroz Mohiuddin",
                "Lukasz Kaiser",
                "David Belanger",
                "Lucy J. Colwell",
                "Adrian Weller"
            ],
            "title": "Rethinking attention with performers",
            "venue": "ArXiv, abs/2009.14794,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Gabriel Synnaeve",
                "Yossi Adi"
            ],
            "title": "High fidelity neural audio compression",
            "venue": "arXiv preprint arXiv:2210.13438,",
            "year": 2022
        },
        {
            "authors": [
                "Gr\u00e9goire Del\u00e9tang",
                "Anian Ruoss",
                "Paul-Ambroise Duquenne",
                "Elliot Catt",
                "Tim Genewein",
                "Christopher Mattern",
                "Jordi Grau-Moya",
                "Li Kevin Wenliang",
                "Matthew Aitchison",
                "Laurent Orseau"
            ],
            "title": "Language modeling is compression",
            "venue": "arXiv preprint arXiv:2309.10668,",
            "year": 2023
        },
        {
            "authors": [
                "Jiayu Ding",
                "Shuming Ma",
                "Li Dong",
                "Xingxing Zhang",
                "Shaohan Huang",
                "Wenhui Wang",
                "Furu Wei"
            ],
            "title": "Longnet: Scaling transformers to 1,000,000,000 tokens",
            "venue": "arXiv preprint arXiv:2307.02486,",
            "year": 2023
        },
        {
            "authors": [
                "Randall W Engle",
                "Stephen W Tuholski",
                "James E Laughlin",
                "Andrew RA Conway"
            ],
            "title": "Working memory, short-term memory, and general fluid intelligence: a latent-variable approach",
            "venue": "Journal of experimental psychology: General,",
            "year": 1999
        },
        {
            "authors": [
                "K Anders Ericsson",
                "William G Chase",
                "Steve Faloon"
            ],
            "title": "Acquisition of a memory",
            "venue": "skill. Science,",
            "year": 1980
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The Pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiying Jiang",
                "Matthew Yang",
                "Mikhail Tsirlin",
                "Raphael Tang",
                "Yiqin Dai",
                "Jimmy Lin"
            ],
            "title": "lowresource\u201d text classification: A parameter-free classification method with compressors",
            "venue": "In Findings of the Association for Computational Linguistics: ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia. Colbert"
            ],
            "title": "Efficient and effective passage search via contextualized late interaction over bert",
            "venue": "In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Mark A. Kramer"
            ],
            "title": "Nonlinear principal component analysis using autoassociative neural networks",
            "venue": "Aiche Journal,",
            "year": 1991
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Kevin Lin",
                "John Hewitt",
                "Ashwin Paranjape",
                "Michele Bevilacqua",
                "Fabio Petroni",
                "Percy Liang"
            ],
            "title": "Lost in the middle: How language models use long contexts, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Eleanor A Maguire",
                "Elizabeth R Valentine",
                "John M Wilding",
                "Narinder Kapur"
            ],
            "title": "Routes to remembering: the brains behind superior memory",
            "venue": "Nature neuroscience,",
            "year": 2003
        },
        {
            "authors": [
                "Jesse Mu",
                "Xiang Lisa Li",
                "Noah Goodman"
            ],
            "title": "Learning to compress prompts with gist tokens",
            "venue": "arXiv preprint arXiv:2304.08467,",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "year": 2002
        },
        {
            "authors": [
                "Guangyue Peng",
                "Tao Ge",
                "Si-Qing Chen",
                "Furu Wei",
                "Houfeng Wang"
            ],
            "title": "Semiparametric language models are scalable continual learners",
            "venue": "arXiv preprint arXiv:2303.01421,",
            "year": 2023
        },
        {
            "authors": [
                "Guanghui Qin",
                "Benjamin Van Durme"
            ],
            "title": "Nugget: Neural agglomerative embeddings of text",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jack W Rae",
                "Anna Potapenko",
                "Siddhant M Jayakumar",
                "Timothy P Lillicrap"
            ],
            "title": "Compressive transformers for long-range sequence modelling",
            "year": 1911
        },
        {
            "authors": [
                "Charlie Snell",
                "Dan Klein",
                "Ruiqi Zhong"
            ],
            "title": "Learning by distilling context",
            "venue": "arXiv preprint arXiv:2209.15189,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652,",
            "year": 2021
        },
        {
            "authors": [
                "David Wingate",
                "Mohammad Shoeybi",
                "Taylor Sorensen"
            ],
            "title": "Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models",
            "venue": "arXiv preprint arXiv:2210.03162,",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zheng",
                "Chong Wang",
                "Lingpeng Kong"
            ],
            "title": "Linear complexity randomized self-attention mechanism",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Mu"
            ],
            "title": "2023), we formulate an evaluation prompt to be used with the GPT-4 API. The prompt, as illustrated in Listing 2, consists of a task description along with three specific examples. We supply GPT-4 with a text, a prompt, and two distinct model-generated responses. The task for GPT-4 is to determine the superior answer or recognize a tie. The chosen examples encompass scenarios where Assistant A performs better, Assistant B performs better, and when a tie occurs",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Long context modeling is a fundamental challenge for Transformer-based (Vaswani et al., 2017) LLMs due to their inherent self-attention mechanism. Much previous research (Child et al., 2019; Beltagy et al., 2020; Rae et al., 2019; Choromanski et al., 2020; Bulatov et al., 2022; Zheng et al., 2022; Wu et al., 2022; Bulatov et al., 2023; Ding et al., 2023) attempts to tackle the long context issue through architectural innovations of an LLM. While they approach long context with a significant reduction in computation and memory complexity, they often struggle to overcome the notable decline in performance on long contexts, as highlighted by (Liu et al., 2023). In contrast to these efforts, we approach the long context problem from a novel angle \u2013 context compression.\nContext compression is motivated by that a text can be represented in different lengths in an LLM while conveying the same information. As shown in Figure 2, if we use characters to represent the text, it will have a length of 2,572; if we represent it using (sub-)words, we only need a context length of 512 without affecting the response accuracy. So, is there a more compact representation allowing us to achieve the same goal with a shorter context?\nWe explore this problem and propose the ICAE which leverages the power of an LLM to achieve high compression of contexts. The ICAE consists of 2 modules: a learnable encoder adapted from the LLM with LoRA (Hu et al., 2021) for encoding a long context into a small number of memory slots, and a fixed decoder, which is the LLM itself where the memory slots representing the original context are conditioned on to interact with prompts to accomplish various goals, as illustrated in Figure 1.\nWe first pretrain the ICAE using both autoencoding (AE) and language modeling (LM) objectives so that it can learn to generate memory slots from which the decoder (i.e., the LLM) can recover the original context or perform continuation. The pretraining with massive text data enables the ICAE to be well generalized, allowing the resulting memory slots to represent the original context more accurately and comprehensively. Then, we fine-tune the pretrained ICAE on instruction data for practical scenarios by enhancing its generated memory slots\u2019 interaction with various prompts. We show the ICAE (based on Llama) learned with our pretraining and fine-tuning method can effectively produce memory slots with 4\u00d7 context compression. We highlight our contributions as follows:\n\u2022 We propose In-context Autoencoder (ICAE) \u2013 a novel approach to context compression by leveraging the power of an LLM. The ICAE either enables an LLM to express more information with the same context length or allows it to represent the same content with a shorter context, thereby enhancing the model\u2019s ability to handle long contexts with improved latency and memory cost during inference. Its promising results and its scalability may suggest further research efforts in context management for an LLM, which is orthogonal to other long context modeling studies and can be combined with them to further improve the handling of long contexts in an LLM.\n\u2022 In addition to context compression, ICAE provides an access to probe how an LLM performs memorization. We observe that extensive self-supervised learning in the pretraining phase is very helpful to enhance the ICAE\u2019s capability to encode the original context into compressed memory slots. This pretraining process may share some analogies with humans enhancing their memory capacity through extensive memory training, which improves the brain\u2019s memory encoding capabilities (Ericsson et al., 1980; Engle et al., 1999; Maguire et al., 2003). We also show that an LLM\u2019s memorization pattern is highly similar to humans (see Table 2 and Table 3). All these results imply a novel perspective on the connection between working memory in cognitive science (Baddeley, 1992) and representation learning in LLMs (i.e., context window)."
        },
        {
            "heading": "2 IN-CONTEXT AUTOENCODER",
            "text": ""
        },
        {
            "heading": "2.1 MODEL ARCHITECTURE",
            "text": "Like a typical autoencoder (Kramer, 1991), ICAE consists of an encoder and a decoder. Similar to the design of Gisting (Mu et al., 2023) and AutoCompressor (Chevalier et al., 2023), the ICAE performs both the encoding and decoding processes in an in-context manner, as illustrated in Figure 3.\nGiven the intuition, we propose to use a LoRA-adapted LLM as the encoder of the ICAE, as illustrated in Figure 3. When encoding a context c = (w1, . . . , wL) with the length L, we first append k (k << L) memory tokens (m1, . . . ,mk) to the context c to obtain their outputs (m\u03031, . . . , m\u0303k) as the memory slots for the context c. Therefore, the ICAE encoder is very lightweight \u2013 it only adds a LoRA adapter and an embedding lookup for memory tokens compared with the target LLM.\nAs introduced above, we expect the memory slots (m\u03031, . . . , m\u0303k) to be conditioned on by the target LLM on behalf of the original context c. Therefore, we use the untouched target LLM as the decoder of the ICAE to ensure the compatibility of memory slots within the target LLM."
        },
        {
            "heading": "2.2 PRETRAINING",
            "text": ""
        },
        {
            "heading": "2.2.1 AUTOENCODING",
            "text": "As a typical autoencoder, one of the ICAE\u2019s pretraining objectives is to restore the original input text c of the length L from its produced memory slots (m\u03031, . . . , m\u0303k) of the length k:\nLAE = max m\u03031,...,m\u0303k P (c|m\u03031, . . . , m\u0303k; \u0398LLM ) = max \u0398LoRA,em P (c|m1 . . .mk; \u0398LLM ,\u0398LoRA, em)\nTo indicate the autoencoding task, we append a special token \u201c[AE]\u201d to (m\u03031, . . . , m\u0303k) in the decoder, as Figure 3 shows. As this pretraining objective does not need any extra annotation, we can use massive text data to train the In-context Autoencoder."
        },
        {
            "heading": "2.2.2 TEXT CONTINUATION",
            "text": "While autoencoding pretraining offers a straightforward learning objective to encode a context, its inherent simplicity and exclusive focus on the single objective may lead to suboptimal generalization. To address this issue, we incorporate an additional objective during the pretraining phase: text continuation, as illustrated in Figure 7 in Appendix A. This self-supervised task is widely acknowledged to facilitate the learning of more generalizable representations in language models:\nLLM = max m\u03031,...,m\u0303k P (o|m\u03031, . . . , m\u0303k; \u0398LLM ) = max \u0398LoRA,em P (o|m1 . . .mk; \u0398LLM ,\u0398LoRA, em)\nwhere o = (wL+1, . . . , wL+N ) denotes the continuation of context c. This objective helps improve generalization and circumvent excessive reliance on, and overfitting to, the autoencoding task."
        },
        {
            "heading": "2.3 INSTRUCTION FINE-TUNING",
            "text": "After pretraining, the generated memory slots produced by the pretrained ICAE are expected to represent the original context. However, for LLMs, the purpose of providing a context extends beyond rote memorization or continuation; instead, the more common use scenario is using the provided context as a basis for accurately and appropriately responding to various prompts, ultimately accomplishing the tasks we want it to perform (Wei et al., 2021; Ouyang et al., 2022).\nTo enhance the interaction of memory slots produced by the ICAE with diverse prompts, we further fine-tune the ICAE with the PWC dataset (Prompt-with-Context) \u2013 a dataset1 introduced in this paper, which consists of thousands of (context, prompt, response) samples (as shown in Figure 1).\nFormally, the ICAE is fine-tuned for learning to encode the context into the memory slots based on which the decoder (i.e., the target LLM) can produce a desirable response r1 . . . rn according to a given prompt p1 . . . pm, as shown in Figure 8 in Appendix A:\nLFT = max m\u03031...m\u0303k P (r1 . . . rn|m\u03031 . . . m\u0303k, p1 . . . pm; \u0398LLM )\n= max \u0398LoRA,em\nP (r1 . . . rn|m1 . . .mk, p1 . . . pm; \u0398LLM ,\u0398LoRA, em)"
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "3.1 EXPERIMENTAL SETTING",
            "text": "Data We pretrain the ICAE with the Pile (Gao et al., 2020). For instruction fine-tuning, we use the PWC dataset, as introduced in Section 2.3, which contains 240k (context, prompt, response) samples for training and 18k samples for testing. The context length distribution of test samples is shown in Figure 10. By default, the maximal token length (excluding memory slots) we set during training is 512 in both the ICAE\u2019s encoder and decoder in our experiments.\nModel Configuration We use the LlaMa (Touvron et al., 2023a;b) as the target LLM to test the ICAE\u2019s performance in context compression. For the encoder of the ICAE, LoRA is applied to the query and value projections of the LLM\u2019s multi-head attention. In our default setting, the memory slot length k is set to 128, and the LoRA rank r is set to 128 unless otherwise specified. The resulting ICAE only adds about 1% learnable parameters on top of the target LLM.\nDue to space limitations, more training details are introduced in Table 9 in Appendix A."
        },
        {
            "heading": "3.2 RESULTS",
            "text": ""
        },
        {
            "heading": "3.2.1 PRETRAINED ICAE",
            "text": "We first evaluate the autoencoding performance of the pretrained ICAE (without instruction finetuning) using the following three metrics to understand how well it restores the original context from its produced memory slots: BLEU (Papineni et al., 2002), Exact-Match (EM)2 and cross entropy loss.\nFigure 4 presents the autoencoding results of the ICAE based on the Llama-7b. The ICAE demonstrates a very low overall loss, below 0.05, indicating that the produced memory slots retain almost all the information of the original context. When the context length is within 300, the ICAE can almost perfectly reconstruct the original context, achieving nearly 100% BLEU and EM scores. As the context length increases beyond 400, both BLEU and EM scores start to decline, indicating insufficient capacity of the 128-length memory slots. However, even at a context length of 500, the median BLEU remains over 0.98, and the median EM approaches 0.6 (e.g., perfectly reconstructing about the first 300 words of a 512-token context), showing remarkable performance of ICAE.\n1Despite some (prompt, response) datasets such as Self-Instruct (Wang et al., 2022), most of their samples either have no context or very short contexts, which are not suitable for our setting. Therefore, we establish the PWC dataset with the help of the GPT-4 (OpenAI, 2023). We include the details in Appendix C.\n2EM denotes the proportion of the exact matching prefix length to the total length. For a context of 512 tokens, if its first 256 tokens are perfectly restored but its 257th token is not, the EM score is 256/512 = 0.5.\nWe then analyze the effect of the memory size k on the result. According to Figure 5, as the memory slot length k decreases, the ICAE\u2019s ability to memorize longer samples significantly deteriorates. Compared to k = 128 where the BLEU score can still reach over 95% at a context length of 500, the BLEU scores become much less satisfactory for k values of 64 and 32, indicating an inability to losslessly retain the original context. This observation is also evident from the loss curve, suggesting that achieving over 4\u00d7 compression is rather challenging."
        },
        {
            "heading": "128\u2192128 (1\u00d7) 9.99 10.15 +0.16",
            "text": ""
        },
        {
            "heading": "256\u2192128 (2\u00d7) 9.45 9.77 +0.32",
            "text": ""
        },
        {
            "heading": "512\u2192128 (4\u00d7) 9.01 9.50 +0.49",
            "text": "Similarly, the text continuation evaluation presented in Table 1 also illustrates that a higher compression ratio tends to result in more pronounced losses in language modeling.\nTable 2 presents 1 specific example of the ICAE performing text restoration, demonstrating an interesting behavior: \u201clarge pretrained language model\u201d is restored as \u201clarge pretrained model\u201d and \u201cThe results prove\u201d is restored as \u201cThe experimental evidence proves\u201d. These restoration errors resemble mistakes humans would make when memorizing the same text. This suggests that, like\nhumans, the model selectively emphasizes or neglects certain parts of the information during the memorization based on its own understanding. It is also consistent with Peng et al. (2023): the stronger the LLM, the fewer it needs to memorize, and thus the smaller the memorization effort. This is similar to human learning: knowledgeable individuals tend to learn more effortlessly, while those with limited knowledge often rely on rote memorization to acquire new information.\nTo further look into the memorization insight, we test restoration performance for different types of 512-token texts with 128 memory slots produced by ICAE to investigate whether its memorization capability is consistent across different content types. According to Table 3, in contrast to compressing normal texts which can be well restored, compressing and restoring less common texts (i.e., random texts) becomes very challenging, reflected by much worse loss and BLEU scores. All these results strongly support our intuition that an LLM\u2019s memorization pattern is highly similar to humans.\nBased on this intuition, it is very likely that a more powerful LLM may support a higher compression ratio without significant forgetting. We will discuss it in Section 3.3.1.\nIt is also intuitive that ICAE can be used for document representation and retrieval (reranking). Following the setting adopted by Qin & Van Durme (2023), we employ the WikiText-103 corpus, randomly sampling 1024 passages as queries. For each query, its positive (target) passage is either a randomly sampled passage or its adjacent passage in the same article, while its negative examples are 19 passages from other articles with the BM25 retrieval. Table 4 shows that ICAE significantly outperforms the ColBERT baseline (Khattab & Zaharia, 2020) with a single vector that unavoidably loses much information. Interestingly, when retrieving adjacent passages, ICAE shows more superior performance than ColBERT, possibly because it is partially learned with the LM objective."
        },
        {
            "heading": "3.2.2 FINE-TUNED ICAE",
            "text": "In order to evaluate the fine-tuned ICAE\u2019s performance, we evaluate on the PWC test set. We use the GPT-4 to compare the outputs of the two systems to determine which one performs better or if they are on par with each other, following3 Mu et al. (2023). Table 5 shows the comparison of results of the LLMs conditioned on memory slots and original contexts. For Llama-7b (fine-tuned ICAE), we compare with Alpaca and StableLM-tuned-alpha-7b since there is no official instruction-tuned Llama-1 model. The Llama-7b (ICAE) conditioned on 128 memory slots largely outperforms both Alpaca and StableLM which can access original contexts (\u223c512 tokens), with a win rate of 56.7% and 74.1% respectively and a win+tie rate of 73%\u223c81%. However, when compared to the GPT-4 (we regard it as the gold standard), there is still a significant gap, with around 70% of the cases underperforming the GPT-4\u2019s results, and a win+tie ratio of about only 30%.\nWhen we switch the base model to Llama-2-chat, we observe ICAE\u2019s performance becomes much better than its counterpart based on Llama-1: when k = 128, its win+tie rate can reach around 75% againt the GPT-4 although it still lags behind its counterpart conditioning on the original context as the compression is lossy. As k increases, the win+tie rate further improves while the compression rate decreases. We perform the same comparative studies on Llama-2-13b-chat and observe better results of ICAE, supporting our assumption in Section 3.2.1 that the ICAE can benefit more on larger LLMs.\n3See Appendix D for details.\nWe investigate the impact of memory length on results. Table 6 shows pairwise comparisons between ICAE models with varying memory slot lengths. A higher compression ratio makes it harder to ensure response quality, but a larger ratio doesn\u2019t always lead to worse performance. Table 6 highlights that a pretrained ICAE with 8\u00d7 compression (k=64) can match a non-pretrained ICAE with 4\u00d7 compression (k=128). Under the same ratio, the pretrained ICAE performs much better than its non-pretrained counterpart, emphasizing the importance of pretraining. By comparing the outputs generated via the pretrained and non-pretrained ICAE, we find the pretrained ICAE suffers less from hallucination than the non-pretrained counterpart (see the examples in Table 10 in Appendix D). We assume the pretraining of ICAE improves the LLM\u2019s working memory as it shares some analogies with humans enhancing their memory capacity via extensive memory training which improves the brain\u2019s memory encoding capabilities. We also examine pretraining objectives and find combining4 AE and LM yields better results than using AE or LM individually (the 4th row in Table 6).\nThe last row of Table 6 compares ICAE\u2019s 128-length memory slots with a summary5 within 128 tokens (\u223c100 words). Memory slots significantly outperform summaries under the same context length, with \u223c2\u00d7 win/lose ratio, proving to be more compact and informative than natural language."
        },
        {
            "heading": "3.3 ANALYSIS",
            "text": ""
        },
        {
            "heading": "3.3.1 SCALABILITY",
            "text": "As discussed above, ICAE should achieve better compression performance with a more powerful target LLM. To verify this assumption, we compare the ICAE\u2019s performance on three target LLMs: Llama-7b, Llama-2-7b and Llama-2-13b in Table 7, which align well with our expectations \u2013 a more potent target LLM can achieve a superior compression ratio of contexts.\nIn addition to the target LLM, ICAE\u2019s effectiveness is scalable in terms of context length, as we show in Table 11 in Appendix E.1, indicating its potential for application to very long contexts.\n4Lpretrain = \u03bbLAE + (1\u2212 \u03bb)LLM. We find \u03bb = 0.4 \u223c 0.6 leads to the best result; we therefore set \u03bb = 0.5. 5Produced by the GPT-4. The specific prompt text is presented in Appendix D."
        },
        {
            "heading": "8*2048 LLM - 24.0 24.0LLM+ICAE 3.4 3.9 7.3 (3.3\u00d7)",
            "text": ""
        },
        {
            "heading": "32*512 LLM - 24.3 24.3LLM+ICAE 2.6 4.2 6.8 (3.6\u00d7)",
            "text": ""
        },
        {
            "heading": "3.3.2 LATENCY",
            "text": "We conducted an empirical test to evaluate the impact of ICAE\u2019s 4\u00d7 context compression on inference efficiency. For this efficiency test, we fix the context (i.e., input) length to either 512 or 2048 and the generation length to 128. Table 8 shows that context compression by ICAE is helpful to improve LLM (i.e., Llama-7b) inference efficiency, achieving over 2\u00d7 speedup. Its acceleration becomes even more significant \u2013 around 3.5\u00d7 \u2013 in compute-intensive scenarios (e.g., 8\u00d72048 and 32\u00d7512). Given that the compressed memory slots can be cached in advance (for frequently used texts like textbooks, government reports or articles of law), ICAE may introduce over 7\u00d7 inference speedup in these cases. Details of the profiling are presented in Appendix B."
        },
        {
            "heading": "3.3.3 MULTIPLE SPANS OF MEMORY SLOTS",
            "text": "Thus far, we have mainly discussed a single span of memory slots. In this section, we shall discuss multiple spans of memory slots. As illustrated in Figure 6(Left), we can segment a long context into N chunks, compress them individually, and then concatenate them to represent the original long context. However, this did not work initially, because the model had never seen multiple span concatenation patterns during training. Fortunately, we can incorporate a small number of multiple span concatenation samples during training, enabling the model to work with concatenated spans of memory slots, as OpenAI\u2019s work (Bavarian et al., 2022) on introducing the \u201cfill in the middle\u201d ability for the GPT. The results in Table 6(Right) indicate that, using an equivalent length context, ICAE\u2019s memory achieves better performance \u2013 because memory can represent 4\u00d7 the original context length. The ability of ICAE demonstrates great promise to handle long contexts, as it can save a significant amount of GPU memory when addressing long contexts without touching the existing LLM. As illustrated in Figure 6(Right), 2048-length memory slots can perform on par with 4096-token contexts. This means that conditioning on 2048 memory slots instead of the original 4096 context tokens can save about 20GB of GPU memory6 with minimal quality degradation."
        },
        {
            "heading": "3.3.4 MISCELLANEOUS",
            "text": "The ICAE\u2019s memory slots also have many other advantages. For example, they can conditioned on for chain-of-thought reasoning and can be specially trained for a specific task with better compression performance. Due to space limitations, we introduce them in Appendix E.2 and E.3."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Prompt compression and context distillation (Askell et al., 2021; Snell et al., 2022) are closely related areas to this work: Wingate et al. (2022) proposed a method to learn compact soft prompts to simulate the original natural language prompt by optimizing the KL divergence. However, this approach has a very high computational cost, as it requires performing back-propagation for each new\n6Llama-7b (fp16) requires 24GB GPU memory for 2048 context tokens and 44GB for 4096 during inference.\nincoming prompt to learn and obtain the compressed prompt, which severely limits its application. Qin & Van Durme (2023) proposes Neural Agglomerative Embeddings named NUGGET, which encodes language into a compact representation for an encoder-decoder model. GIST (Mu et al., 2023) achieves prompt compression by fine-tuning a LLM in a similar way to ours. The resulting model can produce a number of gist tokens as the compression of a prompt, which are similar to our memory slots. Nonetheless, this approach is limited to compressing short prompts7 and thus does not address the real issue of long contexts. Also, this method requires fine-tuning the LLM, and the obtained gist tokens also need to be used within the specially tuned LLM (for gist tokens) and seem not compatible with the untouched LLM. Another contemporary study related to ours is Chevalier et al. (2023), which proposed AutoCompressors for recursively compressing long text into summary vectors. Like Mu et al. (2023), the LLM must be tuned to work with generated summary vectors and its training is sophisticated as it involves recursive compression. In contrast, we propose a very simple, straightforward and scalable approach to generating memory slots that can be used in the target LLM with different prompts for various purposes. Moreover, our approach is much more parameter-efficient \u2013 only adding 1% parameters (i.e., LoRA) for tuning on top of the existing LLM.\nIn addition, Jiang et al. (2023) studies kNN-based prediction with general-purpose compressors (e.g., gzip) and Del\u00e9tang et al. (2023) comprehensively investigates LLMs\u2019 compression abilities and reveals their potential as versatile predictors and offering insights into scaling laws and tokenization recently. We believe the above research and this work share a connection, which potentially approaches a novel representation learning paradigm in the new Artificial General Intelligence (AGI) era."
        },
        {
            "heading": "5 CONCLUSION AND FUTURE WORK",
            "text": "We propose the In-context Autoencoder (ICAE) to leverage the power of an LLM to highly compress contexts. By generating compact and informative memory slots to represent the original context, the ICAE enables an LLM to acquire more information with the same context length or represent the same content with a shorter context, thereby enhancing the model\u2019s capability to handle long contexts as well as reducing computation and memory overheads for inference. Moreover, ICAE provides insight into how an LLM performs memorization, offering a novel perspective on the connection between the memory of LLMs and humans, and suggesting future research in LLM context management.\nDue to computational limitations, our experiments were conducted on Llama models up to 13 billion parameters. As discussed in the paper, ICAE is expected to benefit even more from more powerful LLMs, where it should be able to achieve more significant compression ratios. In the future, we hope to have sufficient computational resources to validate the effectiveness of ICAE on larger and stronger LLMs. In addition, we plan to explore the application of ICAE in multimodal LLMs (as the context length for images, videos, and audio is often much longer and has greater compression potential), and investigate discrete memory slots as a future research direction, similar to techniques like Vector Quantized Variational Autoencoders (VQ-VAE) (Van Den Oord et al., 2017) and Codecs (D\u00e9fossez et al., 2022), for helping unify compact representation across modalities in the era of LLM/AGI.\n7Prompts in Mu et al. (2023) refer to task instructions before input texts, so they are usually short."
        },
        {
            "heading": "A MODEL TRAINING CONFIGURATION",
            "text": "We show how to perform pretraining with the text continuation objective and instruction fine-tuning in Figure 7 and 8.\nWe train the ICAE on 8 Nvidia A100 GPUs (80GB). The hyperparameters for pretraining and fine-tuning ICAE are presented in Table 9. We by default train the ICAE with bf16."
        },
        {
            "heading": "B PROFILING SETUP",
            "text": "We test the latency (Section 3.3.2) on 1 Nvidia A100 GPU (80GB). The test machine has the CPU of AMD EPYC\u2122 7413 with 24 cores and 216GB RAM. The runtime configuration is python=3.9, pytorch=2.0.1, cuda=11.7, cudnn=8.5."
        },
        {
            "heading": "C PROMPT-WITH-CONTEXT DATASET",
            "text": "We introduce the PROMPT-WITH-CONTEXT (PWC) dataset where each sample entry is a triple (text, prompt, answer), as depicted in Figure 9. To construct this dataset, we first sample 20k texts from the Pile dataset. Then, for each text, we employ the GPT-4 to provide 15 prompts (10 specific prompts and 5 general prompts) about the text and give the corresponding answers. The prompt instructing the GPT-4 is outlined in Listing 1.\nThe dataset is composed of 240k examples for training purposes, with an additional 18k examples for testing. The context length distribution of test samples is presented in Table 10.\nListing 1: Prompt used by GPT4 API to generate the PWC dataset. Design 10 prompts specified to the above text to test understanding of the above text. These prompts should be diverse and cover as many aspects (e.g., topic, genre, structure, style, polarity, key information and details) of the text as possible. The first half of these prompts should be like an instruction, the other should be like a question. In addition to the prompts specified to the above text, please also design 5 general prompts like \"rephrase the above text\", \"summarize the above text\", \"write a title for the above text\", \"extract a few keywords for the above text\" and \"write a paragraph (i.e., continuation) that follows the above text\". Each prompt should be outputted in the following format: [{\"prompt\": your generated prompt, \"answer\": the answer to the prompt}]"
        },
        {
            "heading": "D GPT-4 EVALUATION",
            "text": "According to Mu et al. (2023), we formulate an evaluation prompt to be used with the GPT-4 API. The prompt, as illustrated in Listing 2, consists of a task description along with three specific examples. We supply GPT-4 with a text, a prompt, and two distinct model-generated responses. The task for GPT-4 is to determine the superior answer or recognize a tie. The chosen examples encompass scenarios where Assistant A performs better, Assistant B performs better, and when a tie occurs. This methodology enables us to effectively assess the model\u2019s quality. Specially, the orders where the model responses are presented to the GPT-4 are swapped randomly to alleviate bias, as Touvron et al. (2023b) did.\nListing 2: Prompt for the GPT-4 evaluation. This prompt consists of a description of the task and three specific examples. Given a piece of text, an instruction for this text, and two AI assistant answers, your task is to choose the better answer and provide reasons. Evaluate the answers holistically, paying special attention to whether the response (1) follows the given instruction and (2) is correct. If both answers correctly respond to the prompt, you should judge it as a tie.\nExample 1: \u2018\u2018\u2018 Text: We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4\u2019s performance based on models trained with no more than 1/1,000th the compute of GPT-4. Prompt: What is GPT4? Assistant A: GPT4 is a large-scale language-trained transformer-based model. Assistant B: GPT4 can produce outputs. \u2018\u2018\u2018\nYour output should be: \u2018\u2018\u2018 {\"reason\": \"The instruction asks what GPT4 is, and from the original text, we know that GPT4 is a multimodal, large-scale model that can generate text. Therefore, Assistant A is the closer answer, while Assistant B did not follow the instruction well in providing a response.\", \"choice\": \"A\"} \u2018\u2018\u2018\nExample 2: \u2018\u2018\u2018 Text: Making language models bigger does not inherently make them better at following a user\u2019s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent. Prompt: Write a title for the above text. Assistant A: Improving Fine-Tuning for Language Models: A GPT-3-inspired Approach Assistant B: Training language models to follow instructions with human feedback \u2018\u2018\u2018\nYour output should be: \u2018\u2018\u2018 {\"reason\": \"This text discusses how to make large language models follow user instructions better, and Assistant B\u2019s response is more in line with the meaning of the text.\", \"choice\": \"B\"} \u2018\u2018\u2018\nExample 3: \u2018\u2018\u2018 Text: Though technology companies announced massive layoffs last year, 2023 has been much worse. Layoffs have far outpaced last year\u2019s cuts, as tech giants including Amazon, Facebook parent company Meta, Microsoft, Google, IBM, SAP, and Salesforce - as well as many smaller companies - announce sweeping job cuts. The problem: Big Tech went on a hiring binge during the pandemic when lockdowns sparked a tech buying spree to support remote work and an uptick in e-commerce, and now they face revenue declines. Although global IT spending is forecast to rise in 2023, with enterprise software and IT services experiencing the greatest growth, the overall increase is expected to be modest, with data center systems and communications services growing by less than 1%, according to market research firm Gartner. Meanwhile hardware sales are forecast to decline. Continuing supply chain issues, inflation, and the war in Ukraine are also having an impact on both business and consumer spending, leading to fears of recession. According to data compiled by Layoffs.fyi, the online tracker keeping tabs on job losses in the technology sector, 784 tech companies have laid off about 208,181 staff so far this year, compared to 164,411 layoffs last year.\nPrompt: How many job losses have occurred in the technology sector so far in 2023? Assistant A: 208,181 job losses have occurred in the technology sector so far in 2023. Assistant B: 208,181. \u2018\u2018\u2018\nYour output should be: \u2018\u2018\u2018 {\"reason\": \"Both answers are acceptable and correct. They should be a tie.\", \"choice\": \"Tie\"} \u2018\u2018\u2018 Your response should only be in the JSON format above; THERE SHOULD BE NO OTHER CONTENT INCLUDED IN YOUR RESPONSE. Write the \"reason\" key before writing the \"choice\" key, so that you think step-by-step before making your decision. KEEP YOUR REASONING BRIEF. Again, don\u2019t favor either A or B if they are both acceptable and correct -- judge a tie instead.\nThe prompt that the GPT-4 uses to generate 128-token summary is as follows:\n\u201cWrite a summary for the above text. Your summary should not exceed 100 words but should include as much information of the original text as possible.\u201d\nWe show examples of the GPT-4 evaluation on a pretrained and a non-pretrained ICAE in Table 10."
        },
        {
            "heading": "E SUPPLEMENTARY RESULTS",
            "text": "E.1 CONTEXT LENGTH SCALABILITY\nWe test the context length scalability of ICAE and show results in Table 11. ICAE performs consistently well from 512 to 2048, demonstrating it is scalable in terms of context lengths.\nExamples of the system outputs (pretrained and non-pretrained ICAE) and the GPT-4 evaluation is presented in Table 10.\nE.2 CHAIN-OF-THOUGHT REASONING\nTo evaluate the LLM\u2019s reasoning capabilities utilizing the memory slots generated by ICAE, we conduct experiments on the Logic Grid Puzzle dataset8.\nTable 12 shows the results of reasoning with chain-of-thought on memory slots produced by ICAE. As natural language contexts, memory slots can also benefit from chain-of-thought for reasoning, demonstrating its general effectiveness.\nE.3 SPECIALIZED ICAE\nTable 13 shows the results of specialized ICAE trained on the text summarization task. In contrast to the performance drop in the PWC test where various prompts (i.e., tasks) are involved, we do not observe performance degradation; instead, an improvement in the specialized ICAE. This is because the specialized ICAE can specifically target a particular task for compression, allowing it to discard a significant amount of irrelevant information related to the task and thus leading to a higher compression rate without performance degradation. Also, as it is specially trained with the summarization task, it achieves a better result in the benchmark test set.\n8We refrain from testing on the GSM8K dataset, as Llama-2 inherently understands the necessity of employing chain-of-thought reasoning when solving mathematical problems. Therefore, it is difficult for us to test its performance using standard prompts in the GSM8K."
        }
    ],
    "year": 2023
}