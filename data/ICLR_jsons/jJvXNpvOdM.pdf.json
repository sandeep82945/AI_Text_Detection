{
    "abstractText": "This paper presents a novel hierarchical task planner under partial observability that empowers an embodied agent to use visual input to efficiently plan a sequence of actions for simultaneous object search and rearrangement in an untidy room, to achieve a desired tidy state. The paper introduces (i) a novel Search Network that utilizes commonsense knowledge from large language models to find unseen objects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novel graph-based state representation to produce a scalable and effective planner that interleaves object search and rearrangement to minimize the number of steps taken and overall traversal of the agent, as well as to resolve blocked goal and swap cases, and (iv) a sample-efficient cluster-biased sampling for simultaneous training of the proxy reward network along with the Deep RL network. Furthermore, the paper presents new metrics and a benchmark dataset RoPOR, to measure the effectiveness of rearrangement planning. Experimental results show that our method significantly outperforms the state-of-the-art rearrangement methodsWeihs et al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",
    "authors": [],
    "id": "SP:9382b785eeb3490cdfe53e385b23b9cff02a2f16",
    "references": [
        {
            "authors": [
                "Dario Amodei",
                "Chris Olah",
                "Jacob Steinhardt",
                "Paul Christiano",
                "John Schulman",
                "Dan Man\u00e9"
            ],
            "title": "Concrete problems in ai safety, 2016",
            "venue": "URL https://arxiv.org/abs/1606.06565",
            "year": 2016
        },
        {
            "authors": [
                "Dhruv Batra",
                "Angel X Chang",
                "Sonia Chernova",
                "Andrew J Davison",
                "Jia Deng",
                "Vladlen Koltun",
                "Sergey Levine",
                "Jitendra Malik",
                "Igor Mordatch",
                "Roozbeh Mottaghi"
            ],
            "title": "Rearrangement: A challenge for embodied ai",
            "venue": "arXiv preprint arXiv:2011.01975,",
            "year": 2020
        },
        {
            "authors": [
                "Craig J Bester",
                "Steven D James",
                "George D Konidaris"
            ],
            "title": "Multi-pass q-networks for deep reinforcement learning with parameterised action spaces",
            "year": 1905
        },
        {
            "authors": [
                "D S Chaplot",
                "D Gandhi",
                "S Gupta",
                "A Gupta",
                "Ruslan S"
            ],
            "title": "Learning to explore using active neural slam, 2020",
            "venue": "URL https://arxiv.org/abs/2004.05155",
            "year": 2004
        },
        {
            "authors": [
                "Dan Dewey"
            ],
            "title": "Reinforcement learning and the reward engineering principle",
            "venue": "In AAAI Spring Symposia,",
            "year": 2014
        },
        {
            "authors": [
                "S Gadre",
                "K Ehsani",
                "S Song",
                "R Mottaghi"
            ],
            "title": "Continuous scene representations for embodied ai",
            "year": 2022
        },
        {
            "authors": [
                "Clement Gehring",
                "Masataro Asai",
                "Rohan Chitnis",
                "Tom Silver",
                "Leslie Pack Kaelbling",
                "Shirin Sohrabi",
                "Michael Katz"
            ],
            "title": "Reinforcement learning for classical planning: Viewing heuristics as dense reward generators",
            "year": 2022
        },
        {
            "authors": [
                "Sourav Ghosh",
                "Dipanjan Das",
                "Abhishek Chakraborty",
                "Marichi Agarwal",
                "Brojeshwar Bhowmick"
            ],
            "title": "Planning large-scale object rearrangement using deep reinforcement learning",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Dmitry Kalashnikov",
                "Alex Irpan",
                "Peter Pastor",
                "Julian Ibarz",
                "Alexander Herzog",
                "Eric Jang",
                "Deirdre Quillen",
                "Ethan Holly",
                "Mrinal Kalakrishnan",
                "Vincent Vanhoucke"
            ],
            "title": "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation",
            "venue": "arXiv preprint arXiv:1806.10293,",
            "year": 2018
        },
        {
            "authors": [
                "Y Kant",
                "A Ramachandran",
                "S Yenamandra",
                "I Gilitschenski",
                "D Batra",
                "A Szot",
                "H Agrawal"
            ],
            "title": "Housekeep: Tidying virtual households using commonsense reasoning, 2022a. URL https: //arxiv.org/abs/2205.10712",
            "year": 2022
        },
        {
            "authors": [
                "Yash Kant",
                "Arun Ramachandran",
                "Sriram Yenamandra",
                "Igor Gilitschenski",
                "Dhruv Batra",
                "Andrew Szot",
                "Harsh Agrawal"
            ],
            "title": "Housekeep: Tidying virtual households using commonsense reasoning",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Kolve",
                "Roozbeh Mottaghi",
                "Winson Han",
                "Eli VanderBilt",
                "Luca Weihs",
                "Alvaro Herrasti",
                "Daniel Gordon",
                "Yuke Zhu",
                "Abhinav Gupta",
                "Ali Farhadi"
            ],
            "title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
            "year": 2017
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Jonathan J Hunt",
                "Alexander Pritzel",
                "Nicolas Heess",
                "Tom Erez",
                "Yuval Tassa",
                "David Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971,",
            "year": 2015
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "URL http://arxiv.org/abs/1907.11692",
            "year": 1907
        },
        {
            "authors": [
                "Zhizhou Ren",
                "Ruihan Guo",
                "Yuan Zhou",
                "Jian Peng"
            ],
            "title": "Learning long-term reward redistribution via randomized return decomposition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "G Sarch",
                "Z Fang",
                "A.W. Harley",
                "P. Schydlo",
                "M.J. Tarr",
                "S. Gupta",
                "K. Fragkiadaki"
            ],
            "title": "Tidee: Tidying up novel rooms using visuo-semantic commonsense priors",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Luca Weihs",
                "Matt Deitke",
                "Aniruddha Kembhavi",
                "Roozbeh Mottaghi"
            ],
            "title": "Visual room rearrangement",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Kalashnikov"
            ],
            "title": "2018) for our replay buffer. We use PyTorch to train our models. I QUALITATIVE RESULTS We show the results of our method to solve the room-rearrangement problem under partial observability and swap cases with 5, and 10 objects in the supplementary video",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Tidying a disordered room based on user specifications is a challenging task as it involves addressing issues related to perception, planning, navigation, and manipulation Batra et al. (2020). An agent performing an embodied room rearrangement must use the sensor observations and a prior knowledge to produce a long horizon plan for generating a sequence of object movements to achieve the tidy goal state. This goal state is specified through geometry, images, language, etc. Batra et al. (2020).\nMajority of the existing research on room rearrangement emphasizes on perception and commonsense reasoning while assuming navigation and manipulation abilities, without incorporating efficient planning. Based on the goal state definition, they broadly fall into two categories; (i) Commonsense based reasoning without a predefined goal state: The methods Kant et al. (2022b); Sarch et al. (2022) in this category utilize image or language-based commonsense reasoning to identify if an object is misplaced from the correct receptacles in their egoview followed by rearranging them using a sub-optimal heuristic planner. Moreover, utilizing text or semantic relation-based anomaly detectors to identify misplaced objects does not resolve blocked goal or swap cases, where an object\u2019s goal position is obstructed by another misplaced object or vice versa. (ii) User-specific room rearrangement with a pre-defined tidy goal state: In this setting, the rearrangement is done based on explicit user specification. Methods like Weihs et al. (2021a); Gadre et al. (2022) focus on egocentric perception and use image or image feature-based scene representation to identify misplaced objects and a greedy planner to sequence actions for rearrangement. Sarch et al. Sarch et al. (2022) also performs a user-specific room rearrangement by using semantic relations to identify misplaced objects in agent\u2019s egoview, and then rearrange them as they appear without planning. These methods Kant et al. (2022b); Sarch et al. (2022); Gadre et al. (2022) explicitly explore the room to find objects that are initially outside the agent\u2019s egoview, since it only provides a partial information about the room. However, these approaches incur a significant traversal cost due to exploration. Additionally, these methods employ non-optimal planning that does not optimize the number of steps or overall traversal. In contrast, efficient planning makes rearrangement more effective by optimizing the sequence of actions and minimizing the time and effort required to achieve the goal state. Ghosh et al. Ghosh et al. (2022), addresses the rearrangement task planning problem by assuming the complete visibility of the room, through the bird\u2019s eye view. Their method addresses some planning problems, such as the combinatorial expansion of rearrangement sequencing, and blocked goal and swap cases without\nexplicit buffer. However, the approach does not minimize overall agent traversal during the planning, and its state representation is not scalable to large numbers of objects. Moreover, their reliance on the ground truth object positions in both the current and goal states is impractical in real-life. Our aim is directed towards a novel and more practical aspect of the room rearrangement problem through efficient task planning under partial observability of a room using agent\u2019s egocentric camera view.\nThe major challenges associated with efficient task planning for room rearrangement under partial observability, as shown in Fig. 1, are (i) uncertainty over the location of unseen objects due to partial observability (objects outside the agent\u2019s field of view presently which are visible from a different perspective, or objects placed within a closed receptacle e.g. spoon in drawer), (ii) scalability to a large number of objects, (iii) combinatorial expansion of sequencing due to simultaneous object search (for unseen objects) and rearrangement, (iv) minimizing the overall traversal during simultaneous object search and rearrangement, (v) blocked goal and swap cases without explicit buffer.\nIn this paper, we propose a novel hierarchical method for a task planner to address the aforementioned challenges. At the beginning, our agent captures the goal state by exploring the room to record the semantic and the geometric configuration Batra et al. (2020) of objects and receptacles through egocentric perception. Once the goal state is captured, the objects in the room are shuffled. In the untidy current state, our hierarchical method partitions the task planning problem into two parts; object search and planning, with the aim of minimizing the overall agent traversal during simultaneous object search and rearrangement. First, we propose a novel commonsense knowledge based Search Network using large language models (LLMs) Liu et al. (2019); Kant et al. (2022a) that leverages the object-receptacle semantics to predict the most probable receptacle for an unseen object in the egoview. Second, we use a Deep RL network with hybrid action space Ghosh et al. (2022) to plan our action sequence for simultaneous object search and rearrangement by resolving blocked goal and the swap cases. To this extent, we define the Deep RL state space with a novel graph-based state representation for the current and the goal state that incorporates geometric information about objects. This representation compactly encodes the scene geometry that aids in rearrangement planning and makes the Deep RL state space scalable to a large number of objects and scene invariant. In addition, we present a novel, sample-efficient cluster-biased sampling for simultaneous training of the proxy reward network Ren et al. (2022) and Deep RL to get a better estimate of the problem\u2019s true objective from the episodic reward than the dense reward in Ghosh et al. (2022). The judicious combination of all the aforementioned components effectively tackle the challenging combinatorial optimization problem in rearrangement as detailed in Sec. 3.6.\nThe major contributions of this paper are :\n1. To the best of our knowledge, this is the first end-to-end method to address the task planning problem for room-rearrangement from an egocentric view under partial observability, using a user-defined goal state. 2. A novel Search Network that leverages object-receptacle semantics using the commonsense knowledge from LLMs to predict the most probable receptacle for an unseen object. 3. Use of Deep RL based planner trained with proxy reward to overcome combinatorial expansion in rearrangement sequencing and, to optimize the overall traversal and the number of steps taken. 4. A new Graph-based state representation for the current and goal state to include geometric information about objects, making the Deep RL state space scalable to large numbers of objects and scene-invariant. 5. Introduction of a novel, sample-efficient cluster-biased sampling for simultaneous training of the proxy reward network and the Deep RL network. 6. We introduce a new set of metrics in Sec. 3.4 to obtain a thorough assessment of the rearrangement planner\u2019s effectiveness by not only evaluating the success of the rearrangement, but also taking into account the number of steps taken and the overall agent traversal. 7. To address the inadequacies in existing benchmarks Weihs et al. (2021a) for evaluating task planning under partial observability, we introduce the RoPOR - Benchmark Dataset. We plan to openly release the dataset to contribute to the research fraternity."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": "In our room-rearrangement setup, the agent explores the room to capture the tidy user-specified goal state. During this exploration, the agent creates a 2D occupancy map M2D for the agent\u2019s navigation while, 3D map M3D is utilized to augment the detection of 3D object and receptacle centroids to a fixed global reference frame (R3). Additionally, we generate an object list O = {[Wi,Pi], i = 1, 2, .., N} and a receptacle list R = {[WRi ,PRi ], i = 1, 2, .., NR}. Here, N , W and P \u2208 R3 are the total numbers of objects, their semantic labels, and 3D object centroids, respectively. While NR, WR and PR \u2208 R3 are the total numbers of receptacles, their semantic labels including the room name from Ai2Thor Kolve et al. (2017), and the 3D receptacle centroids respectively. Then, we randomly shuffle a few objects from the goal state to make the room untidy and fork the agent at a random location in the room. In this untidy current state, the agent\u2019s knowledge is limited to the visible part of the room in its egocentric view. In the agent\u2019s egocentric perception, only a set of objects OV = {[WVi ,PVi ], i = 1, 2, .., NV } are visible. NV , WV and PV \u2208 R3 are the number of visible objects, their semantic labels, and their 3D object centroids respectively in the current state. Comparing O in the goal state with OV in the current state allows for determining only the semantics of unseen objects OV\u0302 = {WV\u0302i , i = 1, 2, .., NV\u0302 }, where NV\u0302 is the number of unseen objects and WV\u0302 their semantic labels. To plan efficiently and achieve the goal state, the agent must know the positions of all objects in the current state. This involves optimizing the search for unseen objects\nbased on the object-receptacle semantics and simultaneously rearranging visible objects based on their positions in the current and goal state. To this end, we present a hierarchical method for task planner, as shown in Fig. 2, with : (i) Search network, (ii) Graph-based state representation, (iii) Deep RL network trained with proxy reward. The objective of our task planner is to minimize the number of steps and the agent\u2019s overall traversal by simultaneously sequencing high-level actions to either pick-place misplaced objects or search for unseen objects at predicted receptacles."
        },
        {
            "heading": "2.1 BACKGROUND",
            "text": "The agent maps the room in the goal state using an exploration strategy Sarch et al. (2022) and receives RGB-D images and egomotion information at each step from Ai2Thor Kolve et al. (2017). The agent constructs M2D and M3D of the environment using the RGB-D input and egomotion. A d-DETR Zhu et al. (2021) detector is used on the RGB images to obtain 2D bounding boxes and semantic labels for objects and receptacles, and the corresponding 3D centroids are obtained using depth input, camera intrinsic and extrinsic. Finally, the agent has O, R, M2D, and M3D from the goal state. In the current state, the agent uses d-DETR detector Zhu et al. (2021) detetector along with M3D to obtain OV . The agent uses the Djikstra path planner on M2D to navigate and execute high-level actions by assuming perfect motion and manipulation capabilities."
        },
        {
            "heading": "2.2 SEARCH NETWORK",
            "text": "We present a novel LLM-based Search Network to reliably predict the receptacles for OV\u0302 . In case the predicted receptacle is articulated, the agent opens it and looks for the object. The agent uses the predicted receptacle\u2019s position from the goal state to be the probable location for OV\u0302 in the current state, since receptacles are static in the room. To this end, we finetune the RoBERTa embeddings to exploit the commonsense knowledge in LLM and learn the semantic relationship between OV\u0302 and R. Fine-tuning LLM embeddings is essential because LLMs, being trained on large data corpus, may not necessarily produce human-commonsense compliant predictions for untidy scenes (see the Appendix for more details). Our Search Network (SN) consists of two parts: the Sorting Network(SRTN) and the Scoring Network(SCN). We use RoBERTa-Large model Liu et al. (2019) to generate pairwise embeddings (EV\u0302 R) for {WV\u0302i }i=1,2,..,NV\u0302 and {W R i }i=1,2,..,NR in the current state. Therefore, there are NE = NV\u0302 \u00d7NR number of embeddings for all the object-room-receptacle (ORR) pairs. Each ORR embedding is classified into one of the 3 classes, based on the probability {pi}i=1,2,3 from the Sorting Network. The ground truth class labels {Yi}i=1,2,3 for each ORR in the dataset (Sec. 3.1) is based on the probability to find an object at that room-receptacle, where {i = 1 : Most Probable Class, 2 : Less Probable Class , 3 : Implausible Class}. SRTN filters out the room-receptacles, where there is a negligible chance of finding the misplaced object. For instance, even in an untidy room, it is nearly impossible to find a cup in the bathtub of a bathroom. This sorting step reduces the scoring network\u2019s computation and minimizes the chances of erroneous scoring of an implausible ORR. We train a fully connected MLP in SRTN using the Cross-Entropy Loss (LCE) as shown in Eq. (1). The Scoring Network estimates probability scores {\u03c7\u0302i}i=1,2,...NSR for embeddings of higher probability classes, with NSR representing the total number of such embeddings. SCN provides a probability score metric, to choose the most probable receptacle for OV\u0302 . For training the fully connected MLP in SCN, we calculate the MSE Loss (LMSE) of probability scores, as in Eq. (2), with respect to the ground truth probability scores {\u03c7i}i=1,..NSR . Finally, we get the position ({PV\u0302 Ri }i=1,..NV\u0302 ) of the unseen objects as the position of their most probable receptacle.\nLCE = \u2212 1\nNE NE\u2211 3\u2211 i=1 Yi log pi (1)\nLMSE = 1\nNSR NSR\u2211 i=1 (\u03c7\u0302i \u2212 \u03c7i)2 (2)\nTo prevent fruitless searches, we implement simple strategies. If the agent cannot find the unseen object at the predicted receptacle, the Search Network identifies the next most probable roomreceptacle, and the prior prediction is discarded before re-planning a new sequence. Additionally, if the agent encounters a receptacle on its path that does not contain any unseen objects, it is removed from future searches. The agent updates OV\u0302 whenever it detects an unseen object in its egoview. If the agent locates the unseen object it is searching for before arriving at the predicted receptacle, it\nupdates OV\u0302 and re-plans a new sequence. Refer appendix for more details on the re-planning strategy."
        },
        {
            "heading": "2.3 GRAPH-BASED STATE REPRESENTATION",
            "text": "For our task planning algorithm, we create a spatial graph (G = {V,E}) representation of the current and the goal state namely Gc = {Vc, Ec} and Gg = {Vg, Eg} respectively. The nodes Vc = {OV } and Vg = {O}. The fully connected edges of the graph contain the path length as edge features, where Ec = {D(PVi , PVj )i \u0338=j} and Eg = {D(Pi, Pj)i\u0338=j}. The path length D(Ai, Aj)i \u0338=j is the length of the shortest collision free path, computed using Djikstra, between the 2D projections of Ai, Aj \u2208 R3 on M2D. For unseen objects in the current state, the object nodes and edges in Gc are augmented with P V\u0302 R from the search network as Vc = Vc\u222a{OV\u0302 , P V\u0302 R} and Ec = {D(P i, P j)i\u0338=j}, where P = PV \u222a P V\u0302 R. This graph representation helps the Deep RL state space to understand the semantic and geometric information of the current and the goal state. We use a novel Graph Representation Network (GRN) with an encoder-decoder to generate meaningful embeddings from Gc and Gg for Deep RL state space to incorporate the residual relative path length notion between every pair of current and goal state nodes. GRN consists of two major blocks, the Graph Siamese Encoder Network (GSEN) and the Residual Geodesic Distance Network (RGDN) . GSEN uses a Graph Convolution Network Xiang Gao (2020) to encode the graphs Gc and Gg and produce the graph embeddings Zc and Zg respectively. These graph embeddings are concatenated to get the final embeddings Zp = Zc \u222a Zg . RGDN acts as a decoder and predicts the residual relative path length \u03c4p between the two graphs. This network is trained in a supervised way as in Eq. (3), using the Graph Dataset (Sec. 3.1), which contains the ground truth relative path length (\u03c4 ) between the two graphs. This graph embedding makes the Deep RL state space invariant to a large number of objects and the scene. This compact representation concisely encodes the pairwise distance between the source and target nodes which aids in the reduction of the combinatorial expansion of rearrangement sequencing.\n\u03c4p = GRN(Gc, Gg) LGRN = ||\u03c4 \u2212 \u03c4p||2 (3)"
        },
        {
            "heading": "2.4 DEEP RL BASED PLANNER",
            "text": "Our task planner needs to select the objects or the probable receptacles for the unseen objects in an efficient manner, to minimize the overall traversal of the agent to simultaneously search the unseen objects and rearrange the visible ones. Moreover, the planner needs to identify free locations, when selecting objects with swap cases."
        },
        {
            "heading": "2.4.1 PARAMETERIZED DEEP-Q NETWORK",
            "text": "In order to achieve the aforementioned goals, we implement a Parameterized Deep-Q Network with hybrid action space, similar to Ghosh et al. (2022). We define a binary Collision vector (CN\u00d71), that signifies the objects with a blocked goal or swap case. The Deep RL state space defined as s = Zp\u222aC. Each action {ai = (k, pk)} in our sequence of actions {ai}i=1,2,..,K of length K is made up of a discrete action k, denoting the index of the selected object or the probable receptacle, followed by a continuous parameter pk which signifies the location for object placement or receptacle search. We use a Paramater network (\u03a6P ) and the Q-network (\u03a6Q) to generate a continuous parameter pk and a discrete action k respectively, similar to Ghosh et al.. According to a Markov Decision Process (MDP), our method receives a reward r(s, a) at each time step t, for choosing an action a, that advances the agent from the current state s to the next state s\u0304. Inspired by the work in Ghosh et al. (2022); Bester et al. (2019), we define the Q-values as a function of the joint continuous action parameter p = [pk]k=1,2,.,K instead of updating the Q-values with its corresponding continuous parameter sample pk. The modified Bellman equation is shown in Eq. (4). This prevents our method from producing degenerate solutions by incorporating the effect of other parameters for updating the Q-values.\nQ(s, k, p) = E r,s\u0304 [r + \u03b3max k\u0304\u2208K Q(s\u0304, k\u0304,\u03a6p(s\u0304))|s, k, p] (4)\nThe loss function Lp(\u03a6p) and LQ(\u03a6Q) for the parameter network(\u03a6p) and the Q network(\u03a6Q), is given by Eq. (5)\nLp(\u03a6p) = \u2212 RB\u2211 K\u2211\nk=1\nQ(s, k,\u03a6p(s); \u03a6Q)\nLQ(\u03a6Q) = E (s,k,p,r,s\u0304)\u2190RB\n[ 1\n2 (y \u2212Q(s, k, p; \u03a6Q))2]\n(5)\nHere, y = r + \u03b3max k\u2208K Q(s\u0304, k\u0304, p(s\u0304; \u03a6p); \u03a6Q) is the updated target from Eq. (4) and RB is the replay buffer. Lp(\u03a6p) indicates how the p must be updated to increase the Q-values. Here \u03a6Q works as critic to \u03a6p.\nFor Long Horizon planning, the sparse reward is not sampling efficient for training the Deep RL Gehring et al. (2022). Hence, we use step-wise environmental feedback based on the hierarchical dense reward similar to Ghosh et al.. The detailed reward structure is explained in the Appendix. This reward structure provides per-step feedback, but we need episodic reward-based feedback to improve RL policy generalization Amodei et al. (2016); Dewey (2014). Thus, for every episode (\u039b), we calculate the episodic reward (Rep) using the step-wise hierarchical dense reward (r) and overall episodic path length (L) as in Eq. (6), and save the reward and each step (s, a, s\u0304) of the episode into the replay buffer (RB). As this episodic reward is sparse, we use a proxy reward network to generate per-step dense Markovian reward with an episodic notion."
        },
        {
            "heading": "2.4.2 PROXY REWARD NETWORK",
            "text": "Our proxy reward network is trained on the sampled experience data from the replay buffer, to give our agent a notion of the overall objective of the episode. The random return decomposition (RRD) method used in Ren et al. (2022), trains a proxy reward network by randomly sampling steps from an episode. This training method is not sample efficient because it uniformly samples the steps without considering the reward distribution in the episode.\nAv er\nag e R\net ur\nn\nN um\nbe r O f St ep s\nTo this end, we propose a novel clusterbiased return reward decomposition (CBRD) to train our proxy reward network. We cluster the per-step reward for the episode into 3 clusters each of size Tj , where j \u2208 {1, 2, 3}, using the c-means clustering. These clusters represent the reward distribution in an episode. This information helps us to efficiently sample Ns number of steps from the episode. We randomly sample Uj = {(sij , aij , sij)} Nj i=1 from each cluster j, such that Nj = Ns \u00d7 Tj/Nep. Using {Uj}j=1,2,3, we estimate the learned episodic reward (Rep,\u03b8) from the proxy reward network (r\u03b8(s, a, s\u0304)), where \u03b8 is the learned weight.\nRep = Nep L Nep\u2211 i=1 ri (6)\nRep,\u03b8 = 3\u2211 j=1 pj Tj Nj Nj\u2211 i=1 r\u03b8(si,j , ai,j , si,j)\n(7)\nLCBRD = 1\nM M\u2211 i=1 [ (Repi \u2212Rep,\u03b8i) 2 ]\n(8)\nHere, M is the number of episodes sampled, Nep is the number of steps in an episode and pj = Tj/Nep is the uniform probability of choosing a sample from the episode that belongs to cluster j. We simultaneously train our Deep RL using Eq. (5) and proxy reward network using Eq. (8) as shown in Line 1. Fig. 3 shows that CB-RD provides effective feedback to our Deep RL method to achieve a higher average return in a lesser number of steps during training. Hence, CB-RD makes our Deep RL method more sample efficient compared to RRD, hierarchical dense reward and sparse reward.\nWe use an off-policy method with a replay buffer to train our Deep RL method with a diverse set of rearrangement configurations, similar to the work proposed by Kalashnikov et al. (2018). We use the \u03f5 greedy method Kalashnikov et al. (2018) to strike a balance between exploration and exploitation. We stabilize our Deep RL training using target networks for \u03a6Q and \u03a6p, and update the weights of target networks using polyak Lillicrap et al. (2015) averaging similar to Bester et al. (2019); Ghosh et al. (2022). Our ablation study in Appendix, shows that the selection of \u03f5 has a significant impact on the solution.\n3 EXPERIMENTS Algorithm 1: Training Proxy Reward Network 1 InitializeRB \u2190 { }; 2 for i\u2190 0, 1, 2, ... do 3 Using \u03f5-greedy , we rollout episode\u039bi ; 4 Calculate episodic rewardRepi usingEq. (6);\nRB \u2190 RB \u222a {\u039bi, Repi}; 5 for l\u2190 0, 1, 2, ... do 6 Sample M episodes {\u039bl \u2208 RB}Ml=1; 7 Group steps from each episode into j \u2208 {1, 2, 3} clusters based on r ; 8 Cluster Biased Sampling to get Ulj from\u039bl; 9 ComputeRep,\u03b8 usingEq. (7);\n10 CalculateLCBRD as inEq. (8); 11 \u03b8 \u2190 \u03b8 \u2212 \u03b1\u25bd\u03b8LCBRD, with learning rate\u03b1; 12 end 13 Optimise the policy\u03a6Q using r\u03b8(s, a) and Eq. (5) ; 14 end\nIn this section, we describe the datasets, metrics, and detailed results of our proposed method and its modules, in addressing the roomrearrangement problem."
        },
        {
            "heading": "3.1 DATASET",
            "text": "Graph Dataset : We generate this dataset to train GRN using Ai2Thor Kolve et al. (2017), by randomly placing objects for two types of rearrangement scenarios: (i) 40% without goal occupied rearrangement: by placing the objects in free spaces and (ii) goal occupied rearrangement: by placing the object in another object\u2019s target.\nSearch Network Dataset : The AMT dataset in Kant et al. (2022b) contains 268 object categories in 12 different rooms and 32 receptacle types. Each object-roomreceptacle (ORR) pair is ranked by 10 annotators in 3 classes: correct (positively ranked), misplaced (negatively ranked), and implausible (not ranked). For our problem statement, the misplaced class is of utmost importance. Hence, we rename the classes as (i) misplaced class\u2192 most probable class, (ii) correct class\u2192 less probable class, and (iii) implausible class remains the same. We find the ground truth score values for each ORR as the mean inverse of the ranks."
        },
        {
            "heading": "3.2 BENCHMARK DATASET FOR TESTING",
            "text": "The existing benchmark dataset, RoomR Weihs et al. (2021a), has limitations as it only allows up to 5 objects, no object placement within another receptacle, and no blocked goal or swap cases. Thus, it cannot fully evaluate planning aspects such as the number of steps taken, agent traversal, blocked goal, or swap cases. To address this, we introduce RoPOR, a new benchmark dataset for testing task planners in Ai2Thor. It includes a diverse range of rooms (120) and object-receptacle pairs (118), allowing for a wide variety of rearrangement scenarios with up to 20 objects and random partial observability cases, object placement within receptacles in the current state, and blocked goal and swap cases. Moreover, object placement configurations in RoPOR affect sub-optimal planning policies in terms of agent traversal. The mean room dimensions along x-axis and y-axis are 3.12m and 5.80m, respectively. Refer Appendix for details on the distribution of objects, rooms and receptacles."
        },
        {
            "heading": "3.3 TRAINING",
            "text": "The training details of our Search network, Graph-based state Representation Network, Deep RL planner, and proxy reward network are available in the Appendix."
        },
        {
            "heading": "3.4 METRICS",
            "text": "Metrics in Weihs et al. (2021b) do not highlight the efficacy of a task planner to judge efficient sequencing to reduce the number of steps taken or the agent traversal during rearrangement. For a fair evaluation of our method, and comparison against the existing methods and ablations, we define new metrics : \u2022 SNS : Success measured by the inverse Number of Steps uses a binary success rate (S) to evaluate\nthe successful completion of a rearrangement episode along with the number of steps (NT ) taken by an agent to rearrange a given number of objects (N ). S is 1 if all object positions in the current and goal state are approximately equal. Higher the SNS implies a lower NT for a given N , indicating more efficient and successful rearrangement episode. (SNS = S \u00d7N/NT ) \u2022 ENR: Efficiency in Number of Re-plans during object search by taking the ratio of the number of unseen objects initially (NV\u0302 ) with respect to the number of attempts to search (NSV\u0302 ). A higher ENR shows a lower NSV\u0302 for a given NV\u0302 indicating a more efficient search to find unseen objects. (ENR = NV\u0302 /NSV\u0302 )\n\u2022 Absolute Traversal Cost(ATC): The metric shows the overall distance traversed by the agent during the successful completion of a rearrangement episode. In an identical test configuration, a lower ATC indicates a more efficient rearrangement sequencing ."
        },
        {
            "heading": "3.5 ABLATION",
            "text": "We ablate our task planner against ground-truth perception, various methods for object search and a dense reward structure. To study the effect of erroneous perception on our task planner, we assume the availability of Ground-Truth object detection labelling and 3D centroid localisation from Ai2Thor (Ours-GT). To understand the importance of our Search Network in planning, we replace it by a (i) Random Search policy (Ours-RS), which predicts probable receptacles for unseen objects with uniform probability and a (ii) Greedy Exploration strategy (Ours-GE) Chaplot et al. (2020) that optimizes for map coverage to discover all the unseen objects. To highlight the generalisation of proxy reward network to the overall objective of the rearrangement episode, we replace it with a hierarchical Dense Reward structure Ghosh et al. (2022) (Ours-DR) .Please refer to the appendix to find the results for the ablations, along with the analysis for the choice of hyper-parameters for each of our learning based modules."
        },
        {
            "heading": "3.6 QUANTITATIVE RESULTS",
            "text": "We evaluate our approach along with the existing methods on RoPOR - Benchmark Datset in Ai2Thor. Tab. 1 indicates that our method is scalable to large number of objects, as demonstrated by the consistent value of SNS despite the increasing number of objects across complete visibility, partial observability, and swap cases without an explicit buffer. The gradual increase in ENR with the increase in number of objects can be attributed to the fact that rearrangement of visible objects and the search for some unseen objects, indirectly aids in finding other unseen objects.\nComparing our method against Housekeep Kant et al. (2022b) would be unfair because it does not perform a user-specific room-rearrangement with a pre-defined goal state. Instead, we have compared our method to previous works such as Weihs et al. Weihs et al. (2021a), Gadre et al. Gadre et al. (2022), Sarch et al. Sarch et al. (2022) and Ghosh et al. Ghosh et al. (2022), all of which have demonstrated results for a user-specific room-rearrangement. For a fair comparison with Weihs et al., we have used their best performing model - RN18+ANM, PPO+IL Weihs et al. (2021a). Since, Ghosh et al., uses groundtruth object positions in the current and the goal state, we compare it with our ablation method Ours-GT. Without erroneous perception, Ours-GT demonstrates efficient planning, by performing significantly better than all the existing methods Weihs et al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022), including Ours, in terms of SNR, ENR and ATC.\nUnder complete visibility, ours significantly outperforms Weihs et al., Gadre et al. and Sarch et al. in terms of SNS and ATC. Similarly, Ours-GT significantly outperforms Ghosh et al. in terms of ATC. The improvement over Weihs et al., Gadre et al. and Sarch et al. shows their heuristic planner is neither scalable nor does it optimize the overall agent traversal or the number of rearrangement steps. In contrast, our method leverages compact graph-based scene geometry capable of addressing large numbers of objects, and robust Deep RL makes our planner efficient in reducing the redundant traversal of the agent. Our method uses path length cost and proxy reward with the episodic notion,\nwhich helps to improve the overall traversal of the agent to produce lower ATC. In comparison, Ghosh et al. uses greedy Euclidean distance based reward without having an episodic notion, thus failing to optimize overall traversal. Moreover, Ghosh et al. shows a drop in performance on the RoPOR dataset as compared to their results evaluated on RoomR Weihs et al. (2021b), due to the variations in the testing scenarios in RoPOR that significantly impact agent traversal for sub-optimal rearrangement policies.\nUnder partial observability, there are two cases - (i) OOF: Objects located outside the field of view initially which are visible from a different perspective and (ii) OPR: Objects placed inside closed receptacles. In the case of OOF, our method substantially outperforms Weihs et al., Gadre et al. and Sarch et al. in terms of SNS, ENR and ATC. All these above methods use greedy sub-optimal planners and employ explicit scene exploration to find objects outside the field of view, incurring huge traversal cost as indicated by their ATC. To gauge the performance of the exploration strategy for object search in terms of ENR, we consider each newly generated location or a set of navigational steps from the exploration policy as a search attempt. Our approach\u2019s significantly higher ENR shows that the Search Network outperforms the exploration policies of Weihs et al. (2021a); Gadre et al. (2022); Sarch et al. (2022) in terms of the number of attempts to find unseen objects. Ghosh et al. does not address any case of partial observability. While Weihs et al., Gadre et al. and Sarch et al. do not solve the case of OPR, which involves object placement inside receptacles (SNS = 0). However, our approach performs equally well in both cases of partial observability due to our search network\u2019s ability to comprehend a commonsense based semantic relationship between an object and any type of receptacle - rigid or articulated.\nSwap cases without an explicit buffer are not handled by Weihs et al., Gadre et al. and Sarch et al., which is evident from SNS = 0. Ours, Ours-GT and Ghosh et al. can effectively resolve an increasing number of swap cases without an explicit buffer using the hybrid action space Ghosh et al. (2022) in the Deep RL network. However, Ours-GT performs better than Ghosh et al. in terms of ATC due to a novel collsion resolution reward that optimizes the agent\u2019s traversal.\nTo ground the values of our RoPOR dataset, we show the results for Ours, the ablation methods and the SOTA in the test set of RoomR in the Appendix. Moreover, additional results for individual methods in our pipeline can be found in the Appendix."
        },
        {
            "heading": "3.7 QUALITATIVE RESULTS",
            "text": "To show the results of our method in room-rearrangement, we have created videos in a number of test scenarios to highlight the robustness of our method. We also test our method in a new environment - Habitat, as demonstrated in our supplementary video. This transfer does not require any additional training for our Search Network, Graph-based State Representation or Deep RL planner. This shows the capability of our method for seamless sim-to-sim transfer, further emphasizing its suitability for real-world deployment. Please refer the supplementary video."
        },
        {
            "heading": "4 LIMITATIONS",
            "text": "Our approach is not capable of identifying unseen objects that are occluded due to clutter on receptacles (for e.g. a spoon may become occluded, if bread, box, lettuce etc. is placed before it). Our method also assumes the availability of perfect motion planning and manipulation capabilities."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we present a novel task planner to tidy up a room under partial observability. Our method is generalizable to different scenarios and produces a sequence of actions to minimize the overall traversal of the agent and the number of steps during simultaneous object search and rearrangement. We attribute these improvements due to Search Network followed by the Deep RL based planner using graph based state representation and episodic proxy reward. Our benchmark dataset RoPOR helps researchers to do further study in this field in the domain of Embodied AI based rearrangement. In the future, we would like to study the deployment of our method in the real-world."
        },
        {
            "heading": "A APPENDIX",
            "text": "This supplementary document first shows our results on RoomR Weihs et al. (2021a) dataset. Next, discusses our RoPOR dataset - statistics and configurations. Detailed information about our reward structure and rearrangement algorithm. Further, we discuss our network details and training schedule. Finally, we show our ablation study and our failure cases."
        },
        {
            "heading": "B RESULTS ON ROOMR DATASET (WEIHS ET AL. (2021A))",
            "text": "Tab. 2 shows the results for Ours, the ablation methods and the SOTA in the test set of RoomR. The similarity in the results of our ablation methods in RoomR reflects the redundancy in the dataset and its metrics to gauge the planning efficacy. Both Ghosh et al. (2022) and Ours-GT use ground-truth perception, hence comparing them with other methods would be unfair. Ours outperforms all the existing methods in RoomR, showing the generalizability and robustness of our method."
        },
        {
            "heading": "C ROPOR BENCHMARK DATASET",
            "text": "The existing datasets and benchmarks for room rearrangement, such as RoomR Weihs et al. (2021b), contain at most five objects in a scene. Further, this dataset and benchmark does not explicitly mention the inclusion of different configurations required to test the efficacy of rearrangement sequencing. To this end, we propose our benchmark dataset RoPOR, which contains various configurations for swap and object placements. Moreover, to cater to the needs of object rearrangement under partial observability, we take inspiration from Housekeep Kant et al. (2022b) to define the distribution of objects, rooms, and receptacles based on the ground-truth human annotations from AMT Kant et al. (2022b)."
        },
        {
            "heading": "C.1 DISTRIBUTION OF OBJECT-ROOM-RECEPTACLES",
            "text": "In each room of the RoPOR dataset, we ensure to keep a higher number of objects with a misplaced class of room-receptacles than with a correct or implausible class of room-receptacles. We use the ground truth classification in the AMT dataset to achieve this. Also, we ensure a random distribution of object placements within each of the classes mentioned above because rarely is anything welldefined in an untidy room. In Fig. 4, we show the average percentage of objects placed in each room-receptacle class for any given room with a fixed number of receptacles in the RoPOR dataset. As the graph shows, the percentage of correct objects decreases as the room-receptacles increase because there is only a small set of room-receptacles for a given object which can be classified as correct.\nMoreover, it is logical to have things in more disordered configurations for an untidy room, hence a higher representation of object-room receptacles from the misplaced and implausible class. The graph in Fig. 5 shows the distribution of the high-level object categories in the misplaced and the correct class of room-receptacles for the RoPOR dataset. The high-level categories in the dataset are shown in Tab. 6. It is evident from Fig. 5 that the number of misplaced room-receptacle is greater than the number of correct room-receptacles for most of the high-level object categories in the RoPOR dataset. As we have borrowed the ground truth classes from AMT, our results show a similar distribution to AMT in Housekeep."
        },
        {
            "heading": "C.2 OBJECT CONFIGURATIONS",
            "text": "C.2.1 INCLUSION OF SUFFICIENT SWAP CASES\nIn every testing scenario, we have two types of swap configuration : (i) With a distance of more than 1m, in which minimising the traversal distance is more important than minimising the number of pick-place actions and (ii) With a distance of less than 1m, in which minimising the traversal distance is insignificant compared to minimising the number of pick-place actions. This testing scenario is depicted in Fig. 9.\nC.2.2 IMPORTANCE OF PATH LENGTH DISTANCE To emphasize the importance of path length distance during rearrangement, we ensure to have sufficient representation of object configurations in each room so that the objects are closer in terms of euclidean distance than the path length distance and vice versa. An example of this configuration can be seen in Fig. 7, where lettuce (violet) is farther from the goal position (blue) of tomato (red) than the kettle (pink) in terms of path length distance due to the table. Also, this ensures that the lettuce is closer than the kettle to the goal position of the tomato in terms of Euclidean distance.\nC.2.3 IMPORTANCE OF NON-GREEDY POLICY In order to account for greedy policies during rearrangement, we have included scenarios in the RoPOR dataset which lead to poor performance in terms of agent traversal for policies that choose only the nearest object. For instance, see Fig. 6, where the policy which chooses the immediate nearest object after every move leads to a longer traversal path for the agent. After a similar step 1, the greedy policy chooses to pick-place lettuce (violet) which is closest in terms of the Euclidean distance. In contrast, an optimal policy chooses the kettle (pink), which is farther in terms of Euclidean distance. The optimal policy-based method chooses a step taking into account the whole rearrangement episode, which leads to a shorter traversal path overall. After taking greedy step 2, the greedy policy leads to a much longer path traversed.\nFinally, we have shown a sample configuration for 5 and 10 objects, including all the scenarios mentioned above in Fig. 8."
        },
        {
            "heading": "D REWARD STRUCTURE",
            "text": "We use a dense reward structure to compute the per-step reward and store it in the replay buffer. The computed rewards train our proxy network, and the step-wise proxy reward from the proxy reward network is used to train our Deep RL method. The proxy reward imparts episodic awareness and improves the sampling efficiency during our Deep RL training. Inspired by the work in Ghosh et al. (2022), we use a similar reward structure so that our Deep RL method minimizes the number of moves and reduces the overall traversal of the agent. We take special care for swap instances to strike a balance between (i) optimizing the number of moves and (ii) minimizing the overall traversal, unlike\nGhosh et al., which focuses only on (i). Further, we describe in detail the dense reward structure in Fig. 10, which enables our RL to produce an effective plan to move the correct objects and efficiently handle the blocked goals, specifically swap instances.\n\u2022 Infeasible action reward R1: The objective of this reward is to restrain the agent from producing infeasible actions that can not be realized in the environment. In Fig. 10, R1 block shows this reward structure. \u2022 Collision resolution reward R2 : This reward tries to strike a balance between (i) optimizing the number of moves and (ii) reducing the overall traversal of the agent. Fig. 9 shows an example scenario where objects A (pink) and B (blue)in the right image are far apart (more than 1m in the path length distance), whereas object D (pink) and C (blue) in the left image are close to each other. This reward ensures that the traversal is optimized for the case of A and B by slightly compromising the number of moves (one extra pick-place step). In contrast, in the case of D and E, we slightly comprise on the traversal cost to optimize the number of moves. It prioritizes the goal-occupied objects (objects which occupy the goal location of other objects) to move first instead of the goal-blocked objects. For example, moving B and C first helps free the space for the goal-blocked objects (A, D) before they move to resolve the collision. It also ensures that the free location of goal occupied object will be nearest to the goal-occupied object\u2019s target location instead of placing it in any random location. Instead of moving A to B\u2019s source location, this reward moves A to its nearby location and frees up B\u2019s goal location. This enforces that the traversal of the agent is minimized. In Fig. 10, R2 block describes this reward. \u2022 Nearest neighbour reward R3 : We use this reward (defined in R3 block of Fig. 10) to ensure that the agent\u2019s traversal should be minimal by first arranging the nearest objects from the previously placed object rather than arranging the objects randomly. \u2022 Goal reaching reward for object and receptacle R4: We use this reward (depicted in R4 block of Fig. 10) to eliminate erroneous redundant actions. This reward ensures that our method can place the object in its goal location in a single move whose goal position is free. It penalizes negative residual Euclidean distance if it fails to place the object in its goal location. \u2022 Episodic Traversal Cost : We use a novel episodic reward to impart the notion of path length traversed along with the number of steps taken in an episode as shown in Eq. (6)."
        },
        {
            "heading": "E ALGORITHM FOR OUR METHOD",
            "text": "We present the pseudo-code of our method in Algorithm 2. From the goal state room G, we capture the object list O = {[Wi,Pi]}i=1,2,..,N for all the detected objects as shown in Line 1. Moreover, we also get the room-receptacle list R = {[WRi ,PRi ], i = 1, 2, .., NR}. Also, we generate the 2D\noccupancy grid map M2D and 3D map M3D from the room in goal state. Using this information, we create a goal state graph (Gg) using O as described in Line 2 and Line 3. From the image It in the current state at time t, we get the semantic labels and positions of the visible objects as OV = {[WVi ,PVi ], i = 1, 2, .., NV }. Comparing O in the goal state with O\nV in the current state allows for determining only the semantics of unseen objects OV\u0302 = {WV\u0302i , i = 1, 2, .., NV\u0302 }. Our goal is to solve the rearrangement sequence for the visible objects (OV ) and search the unseen ones (OV\u0302 ). The Search network takes the input as the pairwise concatenated names of W V\u0302 and WR to generate the position of probable receptacles (P V\u0302 R). We generate the current state graph as shown in Line 10 and Line 11, using OV for visible objects and, OV\u0302 and P V\u0302 R for the unseen ones. The Graph Siamese Encoder Network(GSEN) is used to calculate the embedding Zp from the graph representation Gc and Gg. We find the binary collision vector (C) by computing the similarity between PV and P . Using the state space s, as computed in Line 14, in the Deep RL network, we generate a sequence of actions(a) such that a = {ai = (ki, pki)}i=1,2,..,K . This rearrangement continues unless the a becomes empty (\u03d5) or the maximum number of steps are exhausted (TM ).\nEach action in the sequence a consists of individual action ai = (ki, pki), where ki is the discrete action which specifies the objects that needs to be moved or searched and pki is the continuous parameter to specify the location of the pick-place or the search. If ki points towards an unseen object, say oV\u0302 \u2208 OV\u0302 as shown in Line 21, then a search action is performed on the most probable\nreceptacle whose position is pr \u2208 PR and name is wr \u2208 WR. The agent looks for unseen object oV\u0302 at pr and along the path from the agent\u2019s current position to pr, unless the maximum number of attempts are exhausted, as shown in Line 24. In case the predicted receptacle is articulated, the agent opens it and looks for the object. After the search loop ends, we remove the searched receptacle (wr) from WR as it no longer contains any information about other unseen objects which is depicted in Line 31. If ki points towards a visible object whose position Pi in the goal state is known, then we perform a pick-place action to place ki as close as possible to its target Pi as in Line 34."
        },
        {
            "heading": "E.1 RE-PLANNING STRATEGY",
            "text": "The agent needs to re-plan a new sequence of actions if, (i) the agent fails to find the unseen object at the predicted receptacle as in Line 31. In this case, the agent discards the predicted receptacle (WR \\ wr) from further search attempts and also predicts a new receptacle for the unseen object from the Search Network. (ii) The agent finds the unseen object, it is searching for, before arriving at the predicted receptacle as shown in Line 27, then the agent updates OV and OV\u0302 . These updates are reflected in Gc and using the state representation embeddings from the graph, the Deep RL understands the objects which need to be searched for or rearranged and accordingly generates a new sequence."
        },
        {
            "heading": "F EXPLORATION AND PATH PLANNING",
            "text": "Our method uses a similar exploration strategy as in Sarch et al.Sarch et al. (2022). We explore the scene using a classical mapping method. We take the initial pose of the agent to be the fixed coordinate frame in the map. We rotate the agent in-place and use the observations to instantiate an initial map. Second, the agent incrementally completes the maps by randomly sampling an unexplored, traversible location based on the 2D occupancy map built so far and then navigates to the sampled location, accumulating the new information into the maps at each time step. The number of observations collected at each point in the 2D occupancy map is thresholded to determine whether a given map location is explored or not. Unexplored positions are sampled until the environment has been fully explored, meaning that the number of unexplored points is fewer than a predefined threshold.\nTo navigate to a goal location, we compute the shortest path length to the goal from the current position using the Djikstra algorithm, given the 2D occupancy map M2D.\nAlgorithm 2: Task planner Input: It, TM Data: RobertaModel, RobertaTokenizer Result: Sequence of actions a = {a0, .., aK}\n1 O = {[Wi,Pi]}Ni=1, R = {[W R i ,P R i ]} NR i=1,M 2D,M 3D \u2190 G; 2 V (Gg)\u2190 {O}; 3 E(Gg)\u2190 D(Pi, Pj)i \u0338=j on M 2D; 4 t1 \u2190 0; 5 while t1 \u2264 TM and (W \\WV \u0338= \u03d5 or PV \u0338= P ) do 6 OV = {[WVi ,PVi ]} NV i=1 \u2190 It; 7 OV\u0302 \u2190 O \\OV ; 8 P V\u0302 R \u2190 SearchNetwork(W V\u0302 \u00d7WR); 9 P \u2190 PV \u222a P V\u0302 R;\n10 Nodes(Gc)\u2190 {OV , OV\u0302 \u222a P V\u0302 R}; 11 Edges(Gc)\u2190 D(P i, P j)i \u0338=j on M2D; 12 Zp \u2190 GSEN(Gc) \u222aGSEN(Gg); 13 C \u2190 CollisionDetection(PV \u2229 P ); 14 s\u2190 Zp \u222a C; 15 a = {ai = (ki, pki)}i=1,..K \u2190 DeepRL(s); 16 O\nV \u2190 OV ; 17 W\nR \u2190WR; 18 t2 \u2190 0; 19 while t2 \u2264 TM&(a \u0338= \u03d5&O V == OV &W R == WR) do 20 (ki, pki)\u2190 ai; 21 if ki \u2208 OV\u0302 then 22 {wr, oV\u0302 , pr} \u2190 ki; 23 t3 \u2190 0; 24 while t3 \u2264 TM&(not found oV\u0302 in Search (wr, pr)) do 25 Update{OV , OV\u0302 } \u2190 It+1; 26 if oV\u0302 \u2208 OV then 27 break; 28 end 29 t3 \u2190 t3 + 1; 30 end 31 WR \u2190WR \\ wr; 32 else 33 t4 \u2190 0; 34 while t4 \u2264 TM&PickP lace(ki) \u0338= Pi do 35 Update {OV , OV\u0302 } \u2190 It+1; 36 t4 \u2190 t4 + 1; 37 end 38 end 39 ai \u2190 ai+1; 40 t2 \u2190 t2 + 1; 41 end 42 t1 \u2190 t1 + 1; 43 end"
        },
        {
            "heading": "G NETWORKS",
            "text": ""
        },
        {
            "heading": "G.1 SEARCH NETWORK",
            "text": "\u2022 Generate RoBERTa embedding : Each object and room-receptacle label is concatenated together and encoded into an embedding(EV\u0302 R) of size 1024 using the average of the output from the second last hidden layer of pre-trained RoBERTa large model.\n\u2022 Sorting Network(SRTN): The combined embedding(EV\u0302 R) of the object-room-receptacle is passed through SRTN consisting of 4 fully connected(FC) layers followed by ReLU and softmax, which outputs the softmax probability of an object being at that room receptacle represented by the three classes: 1) Most probable class, 2) Less probable class and 3) Implausible class. These FC layers contain a dropout of 0.2, i.e. FC(EV\u0302 R, 512) \u2192 ReLU \u2192 FC(512, 256) \u2192 ReLU \u2192 FC(256, 64)\u2192 ReLU \u2192 FC(64, 3)\u2192 softmax. \u2022 Scoring Network(SCN): The embeddings of the most probable class(E0 V\u0302 R\n) and the less probable class(E1\nV\u0302 R ) is passed through the SCN consisting of 3 fully connected(FC) layers followed by\nReLU, which outputs a score(\u03c7\u0302) denoting the probability of finding an object at that room-receptacle. These FC layers contain a dropout of 0.2, i.e. FC(E0,1\nV\u0302 R , 256) \u2192 ReLU \u2192 FC(256, 64) \u2192\nReLU \u2192 FC(64, 1). The argmax of the probabilities decides the class to which the combined embedding belongs. For the objects in the implausible class the \u03c7\u0302 = 0 is used."
        },
        {
            "heading": "G.2 GRN",
            "text": "\u2022 GSEN : It takes Gc and Gg as input and uses a Siamese network to encode Gc and Gg . Each encoder of the Siamese network consists of 2 fully connected GraphConv followed by RELU. i.e. GraphConv(Gc, 256)\u2192 RELU \u2192 GraphConv(256, 128) and GraphConv(Gg, 256)\u2192 RELU \u2192 GraphConv(256, 128). \u2022 RGDN : It takes the combined embedding of Gc and Gg as Zp which is passed through 2 fully connected (FC) layers followed by RELU and produces \u03c4p. These FC layers contain a dropout of 0.25. i.e. FC(Zp, 64)\u2192 RELU \u2192 Dropout(0.25)\u2192 FC(64, \u03c4p)."
        },
        {
            "heading": "G.3 PROXY REWARD NETWORK",
            "text": "It takes input as Zp \u222a C \u222a a \u222a Z\u0304p \u222a C\u0304, where Z\u0304p \u222a C\u0304 is the graph embedding and collision vector to represent the state space of the next state. The network consists of 4 fully connected networks followed by RELU. These FC layers contain a dropout of 0.25, i.e. FC(Zp\u222aC\u222aa\u222a Z\u0304p\u222a C\u0304, 512)\u2192 RELU \u2192 Dropout(0.25) \u2192 FC(512, 128) \u2192 RELU \u2192 Dropout(0.25) \u2192 FC(128, 64) \u2192 RELU \u2192 Dropout(0.25)\u2192 FC(64, 1)."
        },
        {
            "heading": "G.4 DEEP RL",
            "text": "\u2022 (\u03a6Q) : It takes input state as Zp \u222a C and all action parameters p, and uses 3 fully connected (FC) layers followed by RELU, which outputs discrete actions K. These FC layers contain a dropout of 0.5, i.e. FC(ZP \u222a C \u222a p, 512)\u2192 RELU \u2192 Dropout(0.5)\u2192 FC(512, 128)\u2192 RELU \u2192 Dropout(0.25)\u2192 FC(128,K).\n\u2022 (\u03a6P ): It takes input as Zp \u222a C, and uses 3 fully connected (FC) layers followed by RELU, which outputs all action parameter p. These FC layers contain a dropout of 0.5, i.e. FC(Zp \u222a C, 512)\u2192 RELU \u2192 Dropout(0.5)\u2192 FC(512, 128)\u2192 RELU \u2192 Dropout(0.25)\u2192 FC(128, p)."
        },
        {
            "heading": "H TRAINING METHOD",
            "text": ""
        },
        {
            "heading": "H.1 SEARCH NETWORK",
            "text": "For training the Search network, we use the Search Network dataset, which contains the modified classes of the AMT dataset with the ground truth class labels and the probability score calculated from the mean inverse of Human annotated ranks. The entire dataset is split into train, val, and test sets with a ratio of 55 : 15 : 30.\n\u2022 Training SRTN: To train SRTN, we use an Adam optimizer with a learning rate(\u03b1SR = 0.001) and a weight decay rate(\u03bbSR = 0.0001). Further, we use a randomized weighted data sampler to equalize the classes in each batch size of 512. The argmax of softmax probabilities for each ORR in the three classes decides the class to which the combined embedding belongs. This is used to calculate the cross entropy loss(Eq.(1)) using the ground truth class labels in the Search Network dataset. This loss is backpropagated during each epoch to train the network. \u2022 Training SCN: To train SCN, we use the ORR belonging to the Most probable and the Less probable class in the Search Network dataset. Further, we use an Adam optimizer with a learning rate(\u03b1SC = 0.001) and a weight decay rate(\u03bbSC = 0.01). Further, we use a randomized weighted data sampler to equalise the classes in each batch size of 512. The output is used to calculate the mean square error loss(Eq.(2)) using the ground truth probability score in the Search Network dataset. This loss is backpropagated during each epoch to train the network."
        },
        {
            "heading": "H.2 GRN",
            "text": "The two encoders of GSEN use share weights to produce embeddings Zc and Zg for GC and Gg respectively . To train our GRN, we use an Adam optimizer with a learning rate(\u03b1GRN = 0.01). Further, we use a randomized data sampler with a batch size of 512. The output is used to calculate the mean square error loss (Eq.(3)) using the ground truth \u03c4 ."
        },
        {
            "heading": "H.3 DEEP RL AND PROXY",
            "text": "Fig. 14 shows our overall training method to train our Deep RL and Proxy Network. We train the Proxy Network using the loss shown in Eq. (8). We use off-policy method to train our RL. Our training method consists of the following three steps\n\u2022 Step 1: We use \u03f5-greedy exploration to generate trajectories {s, a, r, s\u0304} and store into replay buffer. Here the size of the replay buffer is 1000000000. \u2022 Step 2: We sample batch {s, a, s\u0304} from replay buffer and query the proxy network to produce rpredicted. We use {s, a, rpredicted, s\u0304} to update the Q-value by minimizing the Bellman error. We set the value of the learning rate for \u03a6Q and \u03a6p as 0.0001 and 0.000001 respectively. We use Adam optimizer for both networks.\nRB Store S\np\nQ arg max\na\nCurrent Weights Weights of target\nnetwork of ,\nProxy Reward Network\nStep 3: Update target (Polyak averaging)\nUpdate ,\nPolicy (Deep RL, \u03f5 - greedy)\n,\nTraining Proxy network\nFor training Deep RL\nStep 1: roll out trajectories\nStep 2: Update ,\nC-means Clustering\nSampled For Training Proxy Network\n[\n]j[\n]i\n\u2022 Step 3: We use polyak averaging to update the target networks of \u03a6Q and\u03a6p. The value of the rate of averaging for target networks of \u03a6Q and \u03a6P is 0.0075 and 0.00085 respectively. We use an eviction policy similar to Kalashnikov et al. (2018) for our replay buffer.\nWe use PyTorch to train our models."
        },
        {
            "heading": "I QUALITATIVE RESULTS",
            "text": "We show the results of our method to solve the room-rearrangement problem under partial observability and swap cases with 5, and 10 objects in the supplementary video."
        },
        {
            "heading": "J ABLATION",
            "text": "We ablate our task planner against ground-truth perception (Ours-GT), various methods for object search (Ours-GE and Ours-RS) and a dense reward structure (Ours-DR). To study the effect of erroneous perception on our task planner, we assume the availability of Ground-Truth object detection labelling and 3D centroid localization from Ai2Thor Kolve et al. (2017). To understand the importance of Search Network in planning, we replace it by a (i) Random Search policy, which predicts probable receptacles for unseen objects with uniform probability and a (ii) Greedy Exploration strategy Chaplot et al. (2020) that optimizes for map coverage to discover all the unseen objects. To highlight the generalization of proxy reward network to the overall objective of the rearrangement episode, we replace it with a hierarchical Dense Reward structure as in Ghosh et al.. Tab. 3 shows the results of the aforementioned ablation methods.\nWithout erroneous perception, Ours-GT demonstrates efficient planning, by performing significantly better than all the ablation methods in terms of SNR, ENR and ATC.\nUnder complete visibility and swap cases - Ours, Ours - RS and Ours - GE show similar results, since no partial observability cases exist. However, Ours-DR has a higher traversal (ATC) cost due to its greedy action selection based on the nearest neighbour reward. In contrast, Ours uses the episodic notion based proxy reward that considers the overall notion of the episode to train the Deep RL, which minimizes the agent\u2019s traversal.\nUnder partial observability, Ours performs significantly better than Ours-GE, Ours-RS and OursDR in terms SNS, ENR and ATC. This is due to the efficacy of the Search Network and the efficient planning of the Deep RL for simultaneous object search and rearrangement, which is trained with episodic notion based proxy reward. Whereas, Ours-GE incurs a high traversal cost in terms of ATC\nbecause it explicitly explores the entire room to find the OOF objects. Moreover, Ours-GE fails to address the OPR cases (SNS = 0) because the greedy exploration policy Chaplot et al. (2020) in terms of map coverage does not include opening and closing receptacles to find OPR. However, Ours-RS randomly visits receptacles to discover OOF or OPR cases, which again increases ATC. More number of attempts to search in Ours-GE and Ours-RS leads to a lower ENR as well as SNS. We observe that Ours-RS performs slightly better than Ours-GE in terms of ENR and ATC for OOF, because Ours-RS interleaves object search and rearrangement, rather than doing an explicit exploration strategy for finding objects. This is due to the fact that rearrangement of visible objects and search for some unseen objects, aids in the discovery of other unseen objects. Ours-DR shows a slightly higher ATC compared to Ours for both OOF and OPR cases in partial observability due to the greedy planning policy based on the nearest neighbour reward.\nFurther ablation study highlights the efficacy of different modules and our choice of hyper-parameters."
        },
        {
            "heading": "J.1 SEARCH NETWORK",
            "text": ""
        },
        {
            "heading": "J.1.1 REASON FOR ROBERTA FINETUNING",
            "text": "Large Language Models (LLMs) such as RoBERTa Liu et al. (2019) are trained on a huge corpus of available online text, enabling them to comprehend and make predictions based on general language understanding. Moreover, their large training data corpus enhances their ability to generalize to a diverse range of input data. However, it does not necessarily translate to optimal performance for specific tasks, thereby emphasizing the importance of fine-tuning. Fine-tuning serves as the key to harness full potential of these models, allowing it to proficiently grasp domain-specific terminology and generate tailored predictions. To illustrate, consider the task of predicting probable room-receptacles for unseen objects. In this context, possessing knowledge of the specific names of objects and room-receptacles within the environment becomes essential. Fine-tuning empowers the model to incorporate this domain-specific knowledge, allowing it to make more precise predictions and enhance its suitability for practical applications.\nSuppose we need to find spoon, an unseen object, in an untidy living-room. For a pre-trained RoBERTa, consider the following masked prompts : (i) In an untidy living-room, a spoon is usually placed in the <mask> and (ii) In an untidy living-room, a spoon is usually placed on the <mask>. The output obtained by unmasking these prompts in decreasing order of score is : middle, floor, table, corner, center, counter, carpet. Whereas, the output from our Search Network module based on fine-tuned RoBERTa embeddings in decreasing order of score is : table, carpet, coffee-table, console-table, bottom-cabinet, stool, sofa-chair. As evident, the output produced by the pre-trained RoBERTa model is linguistically sound but fails to effectively address the specific problem at hand and lacks the domain knowledge of room receptacles within a household. In our evaluation on the test set of Search Network Dataset (Sec 3.1), the Search Network module based on fine-tuned RoBERTa\nembeddings exhibits approximately a 40% improvement in performance compared to using just the pre-trained RoBERTa model."
        },
        {
            "heading": "J.1.2 DIFFERENT HIDDEN-LAYER ROBERTA EMBEDDINGS",
            "text": "We present quantitative results obtained by utilizing different embeddings from the RoBERTalarge model for the Filter and Ranking networks in Tab 4. These findings indicate that leveraging embeddings from the second-to-last hidden layer offers improved performance in capturing the underlying embedded commonsense within our Search Network framework."
        },
        {
            "heading": "J.2 GRAPH REPRESENTATION NETWORK",
            "text": "We try to understand the effect of pre-trained GRN features on our state space. Fig. 13 shows that our pretrained GRN produces good features which capture the relative pairwise distance between the source and goal graph well. Further, we also show the effect of using the edge features of pairwise Euclidean distance v/s pairwise path length distance between the objects. The results in Tab. 5 show that the performance of the task planner is impacted significantly by using the edge feature as path length distance over the Euclidean one because the path length metric better captures the real scene geometry compared to the Euclidean."
        },
        {
            "heading": "J.3 DEEP RL",
            "text": "We use the ablation study to decide the effective value of \u03f5. Fig. 11 shows that our Deep RL method works best for \u03f5 = 0.025. Moreover, we have shown the effect of number of clusters for Cluster Biased Return Decomposition (CB-RD) in terms of sampling efficiency. Fig. 12 shows that the cluster size of three gives the best performance in terms of sampling efficiency."
        },
        {
            "heading": "J.4 REWARD",
            "text": "We show the ablations for all the reward components mentioned in Appendix D in our supplementary video. The study highlights the significance of each reward component and also the improvement in the distance traversed due to the change in the Collision Resolution reward."
        },
        {
            "heading": "K FAILURE CASE",
            "text": "As depicted in Fig. 15, there are instances where an object at the receptacle is hidden from the agent\u2019s egocentric view of the current state due to occlusion by another object. Our method can not resolve the search in such scenarios. To deal with these cases, we need a method that generates a set of manipulation actions such as pick-place and moves the objects which are causing the occlusion as shown in Fig. 16."
        }
    ],
    "year": 2023
}