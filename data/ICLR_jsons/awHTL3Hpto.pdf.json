{
    "abstractText": "Convex relaxations are a key component of training and certifying provably safe neural networks. However, despite substantial progress, a wide and poorly understood accuracy gap to standard networks remains, raising the question of whether this is due to fundamental limitations of convex relaxations. Initial work focused on the simple and widely used IBP relaxation. It revealed that some univariate, convex, continuous piecewise linear (CPWL) functions cannot be encoded by any ReLU network such that its IBP-analysis is precise. To explore whether this limitation is shared by more advanced convex relaxations, we conduct the first indepth study on the expressive power of ReLU networks across all commonly used convex relaxations. We show that: (i) more advanced relaxations allow a larger class of univariate functions to be expressed as precisely analyzable ReLU networks, (ii) more precise relaxations can allow exponentially larger solution spaces of ReLU networks encoding the same functions, and (iii) even using the most precise single-neuron relaxations, it is impossible to construct precisely analyzable ReLU networks that express multivariate, convex, monotone CPWL functions.",
    "authors": [
        {
            "affiliations": [],
            "name": "CONVEX RELAXATIONS"
        },
        {
            "affiliations": [],
            "name": "Maximilian Baader"
        },
        {
            "affiliations": [],
            "name": "Mark Niklas M\u00fcller"
        },
        {
            "affiliations": [],
            "name": "Yuhao Mao"
        },
        {
            "affiliations": [],
            "name": "Martin Vechev"
        }
    ],
    "id": "SP:873998a234951b1b8c74383f2efab5c733b2717d",
    "references": [
        {
            "authors": [
                "Maximilian Baader",
                "Matthew Mirman",
                "Martin T. Vechev"
            ],
            "title": "Universal approximation with certified networks",
            "venue": "In Proc. of ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Mislav Balunovi\u0107",
                "Martin T. Vechev"
            ],
            "title": "Adversarial training and provable defenses: Bridging the gap",
            "venue": "In Proc. of ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Battista Biggio",
                "Igino Corona",
                "Davide Maiorca",
                "Blaine Nelson",
                "Nedim Srndic",
                "Pavel Laskov",
                "Giorgio Giacinto",
                "Fabio Roli"
            ],
            "title": "Evasion attacks against machine learning at test time",
            "venue": "In Proc of ECML PKDD,",
            "year": 2013
        },
        {
            "authors": [
                "Christopher Brix",
                "Mark Niklas M\u00fcller",
                "Stanley Bak",
                "Taylor T. Johnson",
                "Changliu Liu"
            ],
            "title": "First three years of the international verification of neural networks competition (VNN-COMP)",
            "venue": "CoRR, abs/2301.05815,",
            "year": 2023
        },
        {
            "authors": [
                "Rudy Bunel",
                "Jingyue Lu",
                "Ilker Turkaslan",
                "Philip H.S. Torr",
                "Pushmeet Kohli",
                "M. Pawan Kumar"
            ],
            "title": "Branch and bound for piecewise linear neural network verification",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Krishnamurthy Dvijotham",
                "Alexey Kurakin",
                "Aditi Raghunathan",
                "Jonathan Uesato",
                "Rudy Bunel",
                "Shreya Shankar",
                "Jacob Steinhardt",
                "Ian J. Goodfellow",
                "Percy Liang",
                "Pushmeet Kohli"
            ],
            "title": "Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming",
            "venue": "In Proc. of NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Souradeep Dutta",
                "Susmit Jha",
                "Sriram Sankaranarayanan",
                "Ashish Tiwari"
            ],
            "title": "Output range analysis for deep feedforward neural networks. In NASA Formal Methods - 10th International Symposium, NFM 2018",
            "venue": "Newport News, VA,",
            "year": 2018
        },
        {
            "authors": [
                "Krishnamurthy Dvijotham",
                "Robert Stanforth",
                "Sven Gowal",
                "Timothy A. Mann",
                "Pushmeet Kohli"
            ],
            "title": "A dual approach to scalable verification of deep networks",
            "venue": "In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "R\u00fcdiger Ehlers"
            ],
            "title": "Formal verification of piece-wise linear feed-forward neural networks. In Automated Technology for Verification and Analysis - 15th International Symposium, ATVA 2017",
            "venue": "Pune, India, October",
            "year": 2017
        },
        {
            "authors": [
                "Claudio Ferrari",
                "Mark Niklas M\u00fcller",
                "Nikola Jovanovi\u0107",
                "Martin T. Vechev"
            ],
            "title": "Complete verification via multi-neuron relaxation guided branch-and-bound",
            "venue": "In Proc. of ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Timon Gehr",
                "Matthew Mirman",
                "Dana Drachsler-Cohen",
                "Petar Tsankov",
                "Swarat Chaudhuri",
                "Martin T. Vechev"
            ],
            "title": "AI2: safety and robustness certification of neural networks with abstract interpretation",
            "venue": "IEEE Symposium on Security and Privacy, SP 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Sven Gowal",
                "Krishnamurthy Dvijotham",
                "Robert Stanforth",
                "Rudy Bunel",
                "Chongli Qin",
                "Jonathan Uesato",
                "Relja Arandjelovic",
                "Timothy A. Mann",
                "Pushmeet Kohli"
            ],
            "title": "On the effectiveness of interval bound propagation for training verifiably robust models",
            "venue": "ArXiv preprint,",
            "year": 2018
        },
        {
            "authors": [
                "Juncai He",
                "Lin Li",
                "Jinchao Xu",
                "Chunyue Zheng"
            ],
            "title": "Relu deep neural networks and linear finite elements",
            "venue": "Journal of Computational Mathematics,",
            "year": 2020
        },
        {
            "authors": [
                "Nikola Jovanovi\u0107",
                "Mislav Balunovi\u0107",
                "Maximilian Baader",
                "Martin T. Vechev"
            ],
            "title": "On the paradox of certified training",
            "venue": "Trans. Mach. Learn. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "Guy Katz",
                "Clark W. Barrett",
                "David L. Dill",
                "Kyle Julian",
                "Mykel J. Kochenderfer"
            ],
            "title": "Reluplex: An efficient SMT solver for verifying deep neural networks",
            "venue": "ArXiv preprint,",
            "year": 2017
        },
        {
            "authors": [
                "Yuhao Mao",
                "Mark Niklas M\u00fcller",
                "Marc Fischer",
                "Martin T. Vechev"
            ],
            "title": "TAPS: connecting certified and adversarial training",
            "venue": "CoRR, abs/2305.04574,",
            "year": 2023
        },
        {
            "authors": [
                "Yuhao Mao",
                "Mark Niklas M\u00fcller",
                "Marc Fischer",
                "Martin T. Vechev"
            ],
            "title": "Understanding certified training with interval bound propagation",
            "venue": "In Proc. of",
            "year": 2024
        },
        {
            "authors": [
                "Matthew Mirman",
                "Timon Gehr",
                "Martin T. Vechev"
            ],
            "title": "Differentiable abstract interpretation for provably robust neural networks",
            "venue": "In Proc. of ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew Mirman",
                "Maximilian Baader",
                "Martin T. Vechev"
            ],
            "title": "The fundamental limits of neural networks for interval certified robustness",
            "venue": "Trans. Mach. Learn. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Niklas M\u00fcller",
                "Gleb Makarchuk",
                "Gagandeep Singh",
                "Markus P\u00fcschel",
                "Martin T. Vechev"
            ],
            "title": "PRIMA: general and precise neural network certification via scalable convex hull approximations",
            "venue": "Proc. ACM Program. Lang.,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Niklas M\u00fcller",
                "Franziska Eckert",
                "Marc Fischer",
                "Martin T. Vechev"
            ],
            "title": "Certified training: Small boxes are all you need",
            "venue": "In Proc. of ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Harkirat S. Behl",
                "Rudy Bunel",
                "Philip H.S. Torr",
                "M. Pawan Kumar"
            ],
            "title": "Scaling the convex barrier with active sets",
            "venue": "In Proc. of ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Rudy Bunel",
                "Krishnamurthy Dvijotham",
                "M. Pawan Kumar",
                "Robert Stanforth",
                "Alessio Lomuscio"
            ],
            "title": "Expressive losses for verified robustness via convex combinations",
            "venue": "CoRR, abs/2305.13991,",
            "year": 2023
        },
        {
            "authors": [
                "Chongli Qin",
                "Krishnamurthy (Dj) Dvijotham",
                "Brendan O\u2019Donoghue",
                "Rudy Bunel",
                "Robert Stanforth",
                "Sven Gowal",
                "Jonathan Uesato",
                "Grzegorz Swirszcz",
                "Pushmeet Kohli"
            ],
            "title": "Verification of non-linear specifications for neural networks",
            "venue": "In Proc. of ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Aditi Raghunathan",
                "Jacob Steinhardt",
                "Percy Liang"
            ],
            "title": "Semidefinite relaxations for certifying robustness to adversarial examples",
            "venue": "In Proc. of NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Hadi Salman",
                "Greg Yang",
                "Huan Zhang",
                "Cho-Jui Hsieh",
                "Pengchuan Zhang"
            ],
            "title": "A convex relaxation barrier to tight robustness verification of neural networks",
            "venue": "In Proc. of NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Timon Gehr",
                "Matthew Mirman",
                "Markus P\u00fcschel",
                "Martin T. Vechev"
            ],
            "title": "Fast and effective robustness certification",
            "venue": "In Proc. of NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Rupanshu Ganvir",
                "Markus P\u00fcschel",
                "Martin T. Vechev"
            ],
            "title": "Beyond the single neuron convex barrier for neural network certification",
            "venue": "In Proc. of NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Timon Gehr",
                "Markus P\u00fcschel",
                "Martin T. Vechev"
            ],
            "title": "An abstract domain for certifying neural networks",
            "venue": "Proc. ACM Program. Lang.,",
            "year": 2019
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian J. Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In Proc. of ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Tjandraatmadja",
                "Ross Anderson",
                "Joey Huchette",
                "Will Ma",
                "Krunal Patel",
                "Juan Pablo Vielma"
            ],
            "title": "The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification",
            "venue": "In Proc. of NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Vincent Tjeng",
                "Kai Y. Xiao",
                "Russ Tedrake"
            ],
            "title": "Evaluating robustness of neural networks with mixed integer programming",
            "venue": "In Proc. of ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Shiqi Wang",
                "Huan Zhang",
                "Kaidi Xu",
                "Xue Lin",
                "Suman Jana",
                "Cho-Jui Hsieh",
                "J. Zico Kolter"
            ],
            "title": "Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification",
            "venue": "In Proc. of NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Zi Wang",
                "Aws Albarghouthi",
                "Gautam Prakriya",
                "Somesh Jha"
            ],
            "title": "Interval universal approximation for neural networks",
            "venue": "Proc. ACM Program. Lang.,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Wong",
                "J. Zico Kolter"
            ],
            "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
            "venue": "In Proc. of ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Eric Wong",
                "Frank R. Schmidt",
                "Jan Hendrik Metzen",
                "J. Zico Kolter"
            ],
            "title": "Scaling provable adversarial defenses",
            "venue": "In Proc. of NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Kaidi Xu",
                "Huan Zhang",
                "Shiqi Wang",
                "Yihan Wang",
                "Suman Jana",
                "Xue Lin",
                "Cho-Jui Hsieh"
            ],
            "title": "Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers",
            "venue": "In Proc. of ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Huan Zhang",
                "Tsui-Wei Weng",
                "Pin-Yu Chen",
                "Cho-Jui Hsieh",
                "Luca Daniel"
            ],
            "title": "Efficient neural network robustness certification with general activation functions",
            "venue": "In Proc. of NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Chaowei Xiao",
                "Sven Gowal",
                "Robert Stanforth",
                "Bo Li",
                "Duane S. Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Towards stable and efficient training of verifiably robust neural networks",
            "venue": "In Proc. of ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Huan Zhang",
                "Shiqi Wang",
                "Kaidi Xu",
                "Linyi Li",
                "Bo Li",
                "Suman Jana",
                "Cho-Jui Hsieh",
                "J. Zico Kolter"
            ],
            "title": "General cutting planes for bound-propagation-based neural network verification",
            "venue": "NeurIPS,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nWith the increased deployment of neural networks in mission-critical applications, formal robustness guarantees against adversarial examples (Biggio et al., 2013; Szegedy et al., 2014) have become an important and active field of research. Many popular certification methods (Zhang et al., 2018; Singh et al., 2018; 2019a;b) provide such safety guarantees by using convex relaxations to\ncompute over-approximations of a network\u2019s reachable set w.r.t. an adversary specification. However, despite significant progress, a wide and poorly understood accuracy gap between robust and conventional networks remains. This raises the fundamental question:\nIs the expressivity of ReLU-networks under convex relaxations fundamentally limited?\nInvestigating this question, Mirman et al. (2022) prove that, the class of convex, continuouspiecewise-linear (CPWL) functions cannot be encoded as ReLU-networks such that their analysis with the simple IBP-relaxation (Gehr et al., 2018; Gowal et al., 2018), is precise.\nThis Work: Expressivity of Common Relaxations To investigate whether this limitation of IBP is fundamental to all single-neuron convex relaxations, we conduct the first in-depth study on the expressive power of ReLU networks under all commonly used relaxations. To this end, we consider CPWL functions, naturally represented by ReLU networks, and two common restrictions, convexity and monotonicity. We illustrate key findings in Table 1, showing novel results as red \u2717 and green \u2713.\n\u2217Equal contribution.\nKey Results on Univariate Functions In this work, we prove the following key results:\n\u2022 The most precise single-neuron relaxation, \u2206 (Wong & Kolter (2018)), and the popular DEEPPOLY-relaxation (Singh et al., 2019b; Zhang et al., 2018) do not share IBP\u2019s limitation and can express univariate, convex, CPWL functions precisely.\n\u2022 All considered relaxations, including IBP, can express univariate, monotone, CPWL functions precisely.\n\u2022 The \u2206-relaxation permits an exponentially larger network solution space for convex CPWL functions compared to the less precise DEEPPOLY-relaxation.\n\u2022 Multi-neuron relaxations (Singh et al., 2019a; M\u00fcller et al., 2022) can express all univariate, CPWL functions precisely using a single layer.\nHaving thus shown that, for univariate functions, the expressivity of ReLU networks under convex relaxations is not fundamentally limited, we turn our analysis to multivariate functions.\nKey Results on Multivariate Functions In this setting, we prove the following result:\n\u2022 No single-neuron convex relaxation can precisely express even the heavily restricted class of multivariate, convex, monotone, CPWL functions.\nInterestingly, the exact analysis of such monotone functions on box input regions is trivial, making the failure of convex relaxations even more surprising. In fact, CPWL functions as simple as f(x, y) = max(x, y) = y + ReLU(x \u2212 y) cannot be encoded by any finite ReLU network such that its \u2206-analysis is precise. We thus conclude that, for multivariate functions, the expressivity of ReLU networks under single-neuron convex relaxations is fundamentally limited.\nImplications of our Results for Certified Training While we believe our results to be of general interest, they have particularly interesting implications for certified training. In this area, all stateof-the-art methods (M\u00fcller et al., 2023; Mao et al., 2023; Palma et al., 2023) are based on the simple IBP-relaxation even though it induces strong regularisation which severely reduces accuracy. While Jovanovic\u0301 et al. (2022) show that more precise relaxations induce significantly harder optimization problems, it remains an open question whether solving these would actually yield networks with better performance. Our results represent a major step towards answering this question.\nSpecifically in the univariate setting, we show that more precise relaxations increase expressivity (see Table 1) and lead to larger network solution spaces (compare Theorems 11 and 15). Thus, we hypothesize that using them during training yields a larger effective hypothesis space for the same network architecture. Importantly, this implies that networks with higher performance could indeed be obtained if we can overcome the optimization issues described by Jovanovic\u0301 et al. (2022).\nHowever, in the multivariate setting, perhaps surprisingly, we show that even the most precise singleneuron relaxations severely limit expressivity (see Corollary 21). This highlights the need for further study of more precise analysis methods such as multi-neuron or non-convex relaxations."
        },
        {
            "heading": "2 BACKGROUND ON CONVEX RELAXATIONS",
            "text": "Notation We denote vectors with bold lower-case letters a \u2208 Rn, matrices with bold upper-case letters A \u2208 Rn\u00d7d, and sets with upper-case calligraphic letters A \u2282 R. Inequalities a \u2265 b between vectors are elementwise. We refer to a hyperrectangle B \u2282 Rn as a box. Further, we consider general (finite) ReLU networks h with arbitrary skip connections, including CNNs and ResNets."
        },
        {
            "heading": "2.1 CONVEX RELAXATIONS IN NEURAL NETWORK CERTIFICATION",
            "text": "We call a classifier H : X \u2192 Y locally robust around an input x \u2208 X if it predicts the same, correct class y \u2208 Y for all similar inputs B\u03f5p(x) := {x\u2032 \u2208 X | \u2225x \u2212 x\u2032\u2225p \u2264 \u03f5}. To prove that a classifier is locally robust, we thus have to show that H(x\u2032) = H(x) = y,\u2200x\u2032 \u2208 B. For a neural network predicting H(x) := argmaxi h(x)i, this is equivalent to showing that the logit of the target class is always greater than that of all other classes, i.e., 0 < minx\u2032\u2208B,i\u0338=y h(x\u2032)y \u2212 h(x\u2032)i. As solving this non-convex optimization problem exactly is generally NP-complete (Katz et al., 2017), state-of-theart neural network verifiers (Brix et al., 2023) relax it to an efficiently solvable convex optimization\nproblem. To this end, we replace the non-linear activation functions with convex relaxations in their input-output space, allowing us to compute linear bounds on the output h(x):\n{Alix+ bli}i\u2208L \u2264 h(x) \u2264 {Aujx+ buj}j\u2208U , for some input region B\u03f5p(x), with index sets L and U . These bounds can in-turn be bounded by ly = minx\u2208B maxi\u2208L(Alix+ bli) \u2208 R and uy analogously. Hence, we have ly \u2264 h(x) \u2264 uy .\nIBP Interval bound propagation (Mirman et al., 2018; Gehr et al., 2018; Gowal et al., 2018) only considers elementwise, constant bounds of the form l \u2264 v \u2264 u. Affine layers y = Wv+ b are thus also relaxed as W (l+u)\u2212|W |(u\u2212l)\n2 + b \u2264Wv + b \u2264 W (l+u)+|W |(u\u2212l) 2 + b,\nwhere | \u00b7 | the elementwise absolute value. ReLU functions are relaxed by their concrete lower and upper bounds ReLU(l) \u2264 ReLU(v) \u2264 ReLU(u), illustrated in Fig. 1.\nDeepPoly (DP) DeepPoly, introduced by Singh et al. (2019b), is mathematically identical to CROWN (Zhang et al., 2018) and based on recursively deriving linear bounds of the form\nAlx+ al \u2264 v \u2264 Aux+ au on the outputs of every layer. While this allows affine layers to be handled exactly, ReLU layers y = ReLU(v) are relaxed neuron-wise, using one of the two relaxations illustrated in Fig. 2\n\u03bbv \u2264 ReLU(v) \u2264 (v \u2212 l) u u\u2212 l ,\nwhere product and division are elementwise. Typically, the lower-bound slope \u03bb \u2208 {0, 1} is chosen depending on the input bounds l and u. In this work, however, we analyze the relax-\nations obtained by always choosing the same lower-bound, which we denote with DEEPPOLY-0 (DP-0, green in Fig. 2) and DEEPPOLY-1 (DP-1, blue).\nTriangle-Relaxation (\u2206) In contrast to the above convex relaxations, the \u2206-relaxation (Wong & Kolter, 2018; Dvijotham et al., 2018; Salman et al., 2019; Qin et al., 2019) maintains multiple linear upper- and lower-bounds on every network activation v. We write\nAl1x+ al1 ,\n... Alnx+ aln ,\n \u2264 v \u2264  Au1x+ au1 ,\n... Aunx+ aun .\nUnstable ReLU activation y = ReLU(v) with l < 0 < u are relaxed with their convex hull as illustrated in Fig. 3\n0\nv\n} \u2264 ReLU(v) \u2264 (v \u2212 l) u\nu\u2212 l ,\nwhere again, product and division are elementwise. Note that this can lead to an exponential growth (with the depth of the network) in the number of constraints for any given activation.\nMulti-Neuron Relaxation (MN) All methods introduced so far relax activation functions neuron-wise and are thus limited in precision by the (single neuron) convex relaxation barrier (Salman et al., 2019), i.e., the activation function\u2019s convex hull in their input-output space.\nMulti-neuron relaxations, in contrast, compute the convex hull in the joint input-output space of multiple neurons in the same layer (Singh et al., 2019a; M\u00fcller et al., 2022), or consider multiple inputs jointly (Tjandraatmadja et al., 2020). We illustrate the increase in tightness in Fig. 4 for a group of just k = 2 neurons."
        },
        {
            "heading": "2.2 DEFINITIONS",
            "text": "We now define the most important concepts for this work.\nDefinition 1 (CPWL). We denote the set of continuous piecewise linear functions f : X \u2192 Y by CPWL(X ,Y). Further, if X is some interval I \u2282 R, then we enumerate the points where f changes slope and call them xi, where 0 \u2264 i \u2264 n, i < j implies xi < xj , and X = [x0, xn].\nAll CPWL functions f : I \u2192 R satisfy f(x) = f(xi) + (x \u2212 xi) f(xi+1)\u2212f(xi)xi+1\u2212xi for x \u2208 [xi, xi+1]. We denote by M-CPWL, C-CPWL, and MC-CPWL the class of monotone (M), convex (C), and monotone & convex (MC) CPWL functions, respectively. We say a network h encodes a function f if and only if they are equal on all inputs x \u2208 X : Definition 2 (Encoding). A neural network h : X \u2192 Y encodes a function f : X \u2192 Y if and only if for all x \u2208 X we have h(x) = f(x).\nIn the following, D denotes a convex relaxation and can be IBP, DEEPPOLY-0 (DP-0), DEEPPOLY1 (DP-1), \u2206, or Multi-Neuron (MN). We now call the over-approximation of a network\u2019s graph (set of input-output tuples) obtained with domain D its D-analysis:\nDefinition 3 (Analysis). Let h : X \u2192Y be a network, D a convex relaxation, and B \u2282 X an input box. We denote by hD(B) the polytope in h\u2019s input-output space containing the graph {(x, h(x)) | x \u2208 B}\u2286hD(B)\u2286X \u00d7Y of h on B, as obtained with D and refer to it as the D-analysis of h on B.\nFor Y \u2286 R, we denote the interval bounds of f on B by [f(B), f(B)] := [minx\u2208B f(x),maxx\u2208B f(x)] and, similarly, the interval bounds implied by hD(B) as [hD(B), hD(B)] := [min(x,y)\u2208hD(B) y,max(x,y)\u2208hD(B) y].\nAs any D-analysis of h captures the set of all possible outputs h(x),x \u2208 B, it is of key interest to us to investigate when the analysis does not lose precision. Specifically, whether the linear output bounds hD(B) do not exceed the interval bounds of f on B anywhere on B: Definition 4 (Precise). Let h be a network encoding f : X \u2192 Y and D a convex relaxation. We say that the D-analysis is precise for h if it yields precise lower and upper bounds, that is for all boxes B \u2282 X we have that [hD(B), hD(B)] = [f(B), f(B)].\nIn this work, we investigate the expressivity of ReLU-networks, that is, which function class they can encode such that their D-analysis is precise. Specifically:\nDefinition 5 (Expressivity). Let D be a convex relaxation, F a set of functions, and N a set of networks. We say that N can D-express F precisely, if and only if, for all f \u2208 F , there exists a network h \u2208 N , such that h encodes f and its D-analysis is precise.\nWe can often replace (sub-)networks to encode the same function but yield a (strictly) more precise analysis in terms of the obtained input-output polytope:\nDefinition 6 (Replacement). Let h and h\u2032 be ReLU networks, B a box, and D some convex relaxation. We say h\u2032 can replace h with respect to D, if h\u2032D(B) \u2286 hD(B) for all B and write h D\u21dd h\u2032."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "Below, we give a brief overview of the most relevant related work.\nExpressing CPWL Functions He et al. (2020) show that ReLU networks require at least 2 layers to encode CPWL functions in Rd (for d \u2265 2) with \u2308log2(d+ 1)\u2309 layers always being sufficient.\nExpressivity with IBP Baader et al. (2020) show that for any continuous function f : \u0393 \u2282 Rn \u2192 R over a compact domain \u0393 and \u03f5 > 0, there exists a finite ReLU network h, such that its IBPanalysis for any input box B \u2282 \u0393, denoted by hIBP(B), is precise up to an \u03f5-error:\n[f(B) + \u03f5, f(B)\u2212 \u03f5] \u2286 hIBP(B) \u2286 [f(B)\u2212 \u03f5, f(B) + \u03f5].\nAn equivalent result immediately follows for all strictly more precise domains such as DP-0, \u2206, and MN. Wang et al. (2022) propose a more efficient construction, generalize this result to squashable activation functions, and provide first results on the hardness of constructing such networks. However, as these results require network widths going to\u221e for approximation errors \u03f5 \u2192 0 and IBP-based methods fail empirically for realistic networks, the study of exact encodings is crucial.\nInvestigating what class of functions allows for an exact IBP-analysis, Mirman et al. (2022) show that for any function with points of non-invertibility, i.e., x = 0 for f(x) = |x|, there does not exist a ReLU network IBP-expressing this function.\nCertified Training Certified training methods typically optimize an upper bound on the worstcase loss over some adversary specification computed via convex relaxations. Surprisingly, using the imprecise IBP-relaxation (Mirman et al., 2018; Gowal et al., 2018) consistently yields better performance than tighter relaxations (Wong et al., 2018; Zhang et al., 2020; Balunovic\u0301 & Vechev, 2020). Jovanovic\u0301 et al. (2022) investigate this paradox and identify two key properties of the worstcase loss approximation, continuity and sensitivity, required for effective optimization, with only IBP possessing both. However, the heavy regularization that makes IBP trained networks amenable to certification also severely reduces their standard accuracy (Mao et al., 2024).\nNeural Network Certification We distinguish complete certification methods, which, given sufficient time, can decide any property, i.e., always compute precise bounds, and incomplete methods, which sacrifice precision for speed. Salman et al. (2019) unify a range of incomplete certification methods including IBP, DEEPPOLY, and \u2206, and show that their precision is limited by that of the \u2206-relaxation. They observe that for a wide range of networks and even when using the \u2206-relaxation, a substantial certification gap between the upper- and lower-bounds on robust accuracy remains. Semidefinite programming based methods (Dathathri et al., 2020; Raghunathan et al., 2018) increase tightness at the cost of computational efficiency.\nEarly, complete certification methods directly leveraged off-the-shelf SMT (Katz et al., 2017; Ehlers, 2017) or MILP solvers (Dutta et al., 2018; Tjeng et al., 2019), limiting their applicability to small networks. To improve scalability, Bunel et al. (2020) formulate a branch-and-bound (BaB) framework, that recursively splits the certification problem into easier subproblems until they can be decided by cheap incomplete methods. This concept has been widely adopted and improved using more efficient solvers (Xu et al., 2021; Wang et al., 2021) and tighter constraints (Palma et al., 2021; Ferrari et al., 2022; Zhang et al., 2022)."
        },
        {
            "heading": "4 CONVEX RELAXATIONS FOR UNIVARIATE FUNCTIONS",
            "text": "In this section, we differentiate all convex relaxations that are commonly used for neural network certification (IBP, DP-0, DP-1, \u2206, and MN) in terms of their expressivity, i.e., with respect to the function classes they can analyze precisely when encoded by a ReLU network.\nWe first show that finite-depth ReLU networks can IBP-express M-CPWL functions precisely (Theorem 9). This construction can be applied directly to the strictly more precise DP-0 and \u2206 relaxation and with slight modification also to DP-1. We, then, show that while finite ReLU networks can both DP-0- and \u2206-express M-CPWL and C-CPWL functions, the solution space is exponentially larger when using the more precise \u2206-analysis. Finally, we show that single-layer ReLU networks can MN-express arbitrary CPWL functions. We defer all proofs and supplementary lemmata to App. B.\n4.1 BOX\nx1\u2212x0 ReLU(x\u2212 x0)).\nTo show that M-CPWL functions can be IBP-expressed, we begin by constructing a step function, illustrated in (Fig. 5), as a two-layer ReLU network that can be IBPexpressed:\nLemma 7 (Step Function). Let \u03b2 \u2208 R\u22650 and f \u2208 CPWL(I,R) s.t. f(x) = 0 for x < x0, f(x) = \u03b2 for x > x1 and linear in between. Then, \u03d5x0,x1,\u03b2(x) = \u03b2 \u2212 ReLU(\u03b2 \u2212 \u03b2x1\u2212x0 ReLU(x\u2212 x0)) encodes f .\nLemma 8 (Precise Step). The IBP-analysis of \u03d5x0,x1,\u03b2 is precise.\nIntuitively, the key to this construction is to leverage that while the IBP-relaxation of the ReLU function does not capture any relational information, it recovers the exact output interval. By using two sequential ReLUs, we allow the inner one to cut away the output-half-space f(x) < 0 and the outer one to cut away the half-space f(x) > \u03b2, thus obtaining a precise analysis.\nWe can now construct arbitrary M-CPWL functions from these step functions, allowing us to show that they too can be IBP-expressed:\nTheorem 9 (Precise Monotone). Finite ReLU networks can IBP-express the set of monotone CPWL(I,R) functions precisely.\n4.2 DEEPPOLY-0\nWe show constructively that finite ReLU networks can DP-0express C-CPWL functions, by first encoding any such function as a single-layer ReLU network. We note that the below results equivalently apply to concave functions:\nLemma 10 (Convex encoding). Let f \u2208 CPWL(I,R) be convex. Then f is encoded by\nh(x) = b+ cx+ n\u22121\u2211 i=1 \u03b3i ReLU(\u00b1i(x\u2212 xi)), (1)\nfor any choice \u00b1i \u2208 {\u22121, 1}, if b and c are set appropriately, where \u03b1i =\nf(xi+1)\u2212f(xi) xi+1\u2212xi is the slope between points xi and xi+1, and\n\u03b3i = \u03b1i \u2212 \u03b1i\u22121 > 0 the slope change at xi+1.\nIntuitively, we encode the C-CPWL function f by starting with a linear function h0 = b + cx, coinciding with one of the linear segments of f . We then pick one of the points xi where f changes slope that are adjacent to this segment and add ReLU(\u00b1i(x\u2212 xi)) changing its activation state at this point. Regardless of \u00b1i, we now scale this ReLU with \u03b3i = \u03b1i \u2212 \u03b1i\u22121 to introduce the local change of slope, and update the linear term c\u2190 c\u2212 \u03b3i if the newly added ReLU affects the segment that the linear function matched originally. We repeat this process until h encodes f .\nWe illustrate this in Fig. 6 (top), where we start our construction with the left-most linear segment. We continue by adding a ReLU, first at the green and then the red point, and show the DP-0 relaxation of the added ReLUs as a shaded area of the same color. We\nillustrate the resulting overall DP-0-relaxation, obtained as their point-wise sum, striped grey. Observe that this always recovers the original linear term as the lower bound (see Fig. 6 top). This leads to an imprecise output range unless its slope c is 0. If f includes such a constant section with zero-slope, we can directly apply the above construction, always changing \u00b1i such that the ReLUs open \"outward\", i.e., in a direction that does not affect the constant segment. If f does not include such a constant section but a unique minimum, as in our example, we place two ReLUs at this point, treating it as a constant section with 0-width and recovering a precise lower bound (see Fig. 6 middle). Thus finite ReLU networks can DP-0-express C-CPWL functions but do not allow \u00b1i to be chosen freely. Note that the upper bound is still precise regardless of the choice of \u00b1i. Theorem 11 (DP-0 Convex). For any convex CPWL function f : I \u2192 R, there exists exactly one network of the form h(x) = b+ \u2211 i\u2208I \u03b3i ReLU(\u00b1i(x\u2212 xi)) encoding f , with |I| = n\u2212 1 if f has slope zero on some segment and otherwise |I| = n, such that its DP-0-analysis is precise, where \u03b3i > 0 for all i."
        },
        {
            "heading": "4.3 DEEPPOLY-1",
            "text": "To show that DP-1 has the same expressivity as DP-0, we encode a ReLU function as h(x) = x+ ReLU(\u2212x) which under DP-1-analysis yields the same linear bounds as h\u2032(x) = ReLU(x) under DP-0-analysis. The reverse also holds. Thus, the expressivity of DP-0 and DP-1 is equivalent.\nCorollary 12 (DP-1 ReLU). The ReLU network h(x) = x + ReLU(\u2212x) encodes the function f(x) = ReLU(x) and, the DP-1-analysis of h(x) is identical to the DP-0-analysis of ReLU. Further, the DP-0-analysis of h(x) is identical to the DP-1-analysis of ReLU.\nIt follows directly that any function that can be DP-0-expressed by a finite ReLU network can be DP-1-expressed by the same ReLU network after substituting every ReLU(x) with x+ReLU(\u2212x): Corollary 13 (DP-1 Approximation). Finite ReLU networks can DP-1- and DP-0-express the same function class precisely. In particular, they can DP-1-express the set of convex functions f \u2208 CPWL(I,R) and monotone functions f \u2208 CPWL(I,R) precisely."
        },
        {
            "heading": "4.4 TRIANGLE",
            "text": "To show that finite ReLU networks can \u2206-express C-CPWL functions, we reuse the construction from Lemma 10. However, as the \u2206-relaxation yields the exact convex hull for ReLU functions, we first show that the convex hull of a sum of convex functions (such as Eq. (1)) is recovered by the pointwise sum of their convex hulls:\nLemma 14 (Convex Hull Sum). Given two convex functions f, g : R\u2192 R and the box [l, u]. Then, the pointwise sum of the convex hullsHf +Hg is identical to the convex hull of the sum of the two functionsHf+g = Hf +Hg .\nThis follows directly from the definition and implies that the \u2206-analysis is precise for arbitrary choices of \u00b1i, illustrated in the bottom of Fig. 6: Theorem 15 (\u2206 Convex). Let f \u2208 CPWL(I,R) be convex. Then, for any network h encoding f as in Lemma 10, we have that its \u2206-analysis is precise. In particular, \u00b1i can be chosen freely."
        },
        {
            "heading": "4.5 MULTI-NEURON-RELAXATIONS",
            "text": "As multi-neuron relaxations yield the exact convex hull of the considered group of neurons (all within the same layer), it is sufficient to show that we can express arbitrary CPWL functions with a single-layer network to see that they MN-express CPWL functions. To this end, we use a similar construction as in Lemma 10, where the lack of convexity removes the positivity constraint on \u03b3i.\nTheorem 16 (Multi-Neuron Precision). For every f \u2208 CPWL(I,R), there exists a single layer ReLU network h encoding f , such that its MN-analysis (considering all ReLUs jointly) is precise.\n5 CONVEX RELAXATIONS FOR MULTIVARIATE FUNCTIONS\nAfter having shown in the previous section that, for univariate functions, the expressivity of ReLU networks under convex relaxations is not fundamentally limited, we now turn our attention to multivariate functions. There, we prove that no finite ReLU network can \u2206-express the maximum function max: R2 \u2192 R precisely (Theorem 20). This directly implies that no single-neuron relaxation can express the class of multivariate, monotone, and convex CPWL functions precisely. Note, this negative result generalizes to all relaxations less precise than \u2206, including IBP and DEEPPOLY.\nIntuitively, we will argue along the following lines. We first observe that for any finite ReLU-Network h that encodes the maximum function, we can find a point (x, y = x) \u2208 R2 with neighborhood U , such that on U , all ReLUs in h either switch their activation state for x = y or not at all (see Fig. 7). Then,\nwe show that for such a neighborhood, we can \u2206-replace the finite ReLU network h with a single layer consisting of just 2 neurons and a linear term (Theorems 17 and 18). Finally, we show that no such single-layer network can \u2206-express max precisely (Theorem 19), before putting everything together in Theorem 20. All proofs and support lemmata are again deferred to App. A.\nLet us begin by showing that we can express any finite ReLU network using the functional form of Eq. (2). That is, every i-layer network hi can be written as the sum of an (i \u2212 1)-layer network hi\u22121L and a linear function of a ReLU applied to another (i \u2212 1)-layer network Wi ReLU(h i\u22121 R ). Further, if, for a given input region U , all ReLUs in the original network switch activation state on the hyperplane w\u22a4x = 0 or not at all, then, we can ensure that every ReLU in both (i \u2212 1)-layer networks change activation state exactly for z := w\u22a4x = 0.\nTheorem 17 (Network Form Coverage). Given a neighborhood U and a finite k-layer ReLU network h such that on U and under \u2206-analysis all its ReLUs are either stably active (ReLU(v) = v), stably inactive (ReLU(v) = 0), or switch activation state for z := w\u22a4x = 0 with w \u2208 Rd, then h can be represented using the functional form\nhi{R,L} = h i\u22121 L +Wi ReLU(h i\u22121 R ), h 0 {R,L} = b+W0x, (2)\nfor i = k and such that all ReLUs switch their activation state at {x \u2208 X | w\u22a4x = 0}. Here, L and R are labels, used to distinguish the possibly different (i-1)-layer networks hi\u22121 from each other.\nWe can now leverage the fact that all ReLUs change activation state at the same point to simplify the sum of ReLUs to a linear term plus a single ReLU: \u2211 i ai ReLU(wiz) \u2206 \u21dd \u03b3z + \u03b1ReLU(z) for some \u03b3, \u03b1 \u2208 R (Lemma 22). This allows us to further simplify a ReLU applied to such a sum of ReLUs: ReLU(\u03b3+\u03b1ReLU(z)) \u2206\u21dd \u03b3\u2032z+\u03b1\u2032 ReLU(z) (Lemma 23). These two replacements allow us to recursively reduce the depth of networks in the form of Eq. (2) until just a single layer is left:\nTheorem 18 (Network Simplification). Let hk be a network as in Theorem 17 such that all ReLUs change activation state at z := w\u22a4x = 0 with w \u2208 Rd. We have\nhk = hk\u22121L +W ReLU(h k\u22121 R ) \u2206 \u21dd h(x) = b+Wx+ \u03b1ReLU(z),\nwhere h0(x) = b0 +W0x and all ReLU change state exactly at {x \u2208 X | w\u22a4x = 0}.\nNote that, hk, hk\u22121L and h map to R, while h k\u22121 R maps to the space of some hidden layer Rn. Next, we show directly via contradiction that single-layer ReLU networks of this form cannot \u2206-express the maximum function, illustrating the resulting (imprecise) bounds in Fig. 8:\nTheorem 19 (Triangle max). ReLU-networks of the form h(x, y) = b+wxx+wyy+\u03b1ReLU(x\u2212 y) can not \u2206-express the function max: R2 \u2192 R. Proof. We consider the input region B = [0, 1]2 and constrain our parameters by considering the following: For x = y = 0, we have f(0, 0) = 0 = b = h(0, 0). For x < y, we have f(x, y) = y = wxx + wyy = h(x, y) and thus wx = 0, wy = 1. Finally for x > y, we have f(x, y) = x = y + \u03b1(x\u2212 y) = h(x, y) and thus \u03b1 = 1. Hence we have h(x, y) = y +ReLU(x\u2212 y).\n0\nx\u2212 y\n} \u2264 ReLU(x\u2212 y) \u2264 12 (x\u2212 y + 1) =\u21d2\ny\nx\n} \u2264 h(x, y) \u2264 12 (x+ y + 1).\nThe maximum of the upper bound is attained at x = y = 1, where we get h\u2206(B) = 32 which is larger than max(B) = 1 (see Fig. 8). \u25a1\nTo show that no ReLU network can \u2206-express max it remains to argue how we can find a U such that all ReLUs switch their activation state at {x \u2208 X | w\u22a4x = 0} or not at all: Theorem 20 (\u2206 Impossibility max). Finite ReLU networks can not \u2206-express the function max. Proof. We will prove this theorem via contradiction in four steps. Assume there exists a finite ReLU network h that \u2206-expresses max precisely.\nFirst \u2013 Locality We argue this point in three steps: 1. There exists a point (x, y = x) with an \u03f5-neighborhood U \u2032 such that one of the following\nholds for any ReLU(v) with input v = hv(x, y) of the network h:\n\u2022 the ReLU is always active, i.e., \u2200(x, y) \u2208 U,ReLU(v) = v, \u2022 the ReLU is never active, i.e., \u2200(x, y) \u2208 U,ReLU(v) = 0, or \u2022 the ReLU changes activation state for x = y, i.e., \u2203v\u2032 \u2208 R, s.t.,ReLU(v) = ReLU(v\u2032(x\u2212 y)).\nThis follows directly from the fact that finite ReLU networks divide their input space into finitely many linear regions (see the illustration in Fig. 7).\n2. Further, there exists an \u03f5-neighborhood U of (x, y) such that the above holds under IBPanalysis, as it depends continuously on the input bounds and becomes exact for any network when the input bounds describe a point.\n3. Via rescaling and translation, we can assume that the point (x, y) is at 0 and that the neighborhood U covers [\u22121, 1]2.\nSecond \u2013 Network Form On the neighborhood U , any finite ReLU-network h can, w.r.t. \u2206, be represented by hk = hk\u22121 + W ReLU(hk\u22121), h0(v) = b + Wv with biases b \u2208 Rdk , weight matrices W \u2208 Rdk\u00d7dk\u22121 , where all ReLUs change activation state exactly for x = y (Theorem 17). Third \u2013 Network Replacements We can replace hk w.r.t. \u2206 with the single layer network h\u2032(x, y) = b+W (x, y)\u22a4 + \u03b1R(x\u2212 y) (Theorem 18). Fourth \u2013 Conclusion There exists no network of this form encoding the max-function such that its \u2206-analysis is precise on the interval [0, 1]2 (Theorem 19).\nThis concludes the proof. \u25a1\nAs max belongs to the class of multivariate, convex, monotone, CPWL functions, it follows directly from Theorem 20 that no finite ReLU network can \u2206-express this class precisely:\nCorollary 21 (\u2206 Impossibility). Finite ReLU networks can not \u2206-express the set of convex, monotone, CPWL functions mapping from some box I \u2282 R2 to R."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We conduct the first in-depth study on the expressivity of ReLU networks under all commonly used convex relaxations and find that: (i) more precise relaxations (\u2206, DP-0 or DP-1) allow a larger class of univariate functions (C-CPWL and M-CPWL) to be expressed precisely than the simple IBP-relaxation (M-CPWL), (ii) for the same function class (C-CPWL), a more precise relaxation (\u2206 vs DP-0 or DP-1), can allow an exponentially larger solution space of ReLU networks, (iii) MNrelaxations allow single-layer networks to express all univariate CPWL functions, (iv) even the most precise single-neuron relaxation (\u2206) is too imprecise to express multivariate, convex, monotone CPWL functions precisely with finite ReLU networks, despite their exact analysis being trivial.\nWhile more precise domains improve expressivity for univariate functions, all single-neuron convexrelaxations are fundamentally limited in the multivariate setting. Surprisingly, even simple functions that can be encoded with a single neuron h = y+ReLU(x\u2212y) = max(x, y), can not be \u2206-expressed precisely using any finite ReLU network. This highlights not only the importance of recent, more precise multi-neuron- and BaB-based neural network certification methods but also suggests more precise methods might be needed for training."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We would like to thank our anonymous reviewers for their constructive comments and insightful questions.\nThis work has been done as part of the EU grant ELSA (European Lighthouse on Secure and Safe AI, grant agreement no. 101070617). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission. Neither the European Union nor the European Commission can be held responsible for them.\nThe work has received funding from the Swiss State Secretariat for Education, Research and Innovation (SERI)."
        },
        {
            "heading": "A DEFERRED PROOFS ON MULTIVARIATE FUNCTIONS",
            "text": "Theorem 17 (Network Form Coverage). Given a neighborhood U and a finite k-layer ReLU network h such that on U and under \u2206-analysis all its ReLUs are either stably active (ReLU(v) = v), stably inactive (ReLU(v) = 0), or switch activation state for z := w\u22a4x = 0 with w \u2208 Rd, then h can be represented using the functional form\nhi{R,L} = h i\u22121 L +Wi ReLU(h i\u22121 R ), h 0 {R,L} = b+W0x, (2)\nfor i = k and such that all ReLUs switch their activation state at {x \u2208 X | w\u22a4x = 0}. Here, L and R are labels, used to distinguish the possibly different (i-1)-layer networks hi\u22121 from each other.\nProof. Given Wi ReLU(hi\u22121R ), we partition the columns of the weight matrix into Wi = (W+i |W \u2212 i |W \u00b1 i ), depending on whether the associated ReLU is stably active, stably inactive, or unstable, respectively. We thus obtain\n(W+i |W \u2212 i |W \u00b1 i )ReLU(h k\u22121 R ) = W + i h k\u22121 R +W \u00b1 i ReLU(h k\u22121 R ).\nWe update hi\u22121L,new = h i\u22121 L + W + i h i\u22121 R , by showing that W + i h i\u22121 R is still an (i \u2212 1)-layer network as follows. We recursively update weight matrices Wk,new = W+i Wk to obtain W+i h k = W+i h k\u22121 + Wk,new ReLU(h k\u22121 R ) until we have reached k = 1, where we have W+i h 1 = W+i bL +W + i W0,Lv +W1,new ReLU(bR +W0,Rx). \u25a1\nLemma 22 (Simplification of ReLU Sums w.r.t. \u2206). Let A \u2208 Rn\u00d71 and w \u2208 Rn. Then, we have\nh(z) = A\u22a4 ReLU(wz) \u2206 \u21dd h\u2032(z) = \u03b3z + \u03b1ReLU(z), where \u03b3 = \u2211\ni,wi<0 Aiwi and \u03b1 = \u2211 i,wi>0 Aiwi \u2212 \u03b3.\nProof. Both h and h\u2032 are CPWL functions with slope change only at z = 0. Thus they are fully defined by their value at the points zi \u2208 {\u22121, 0, 1}. Hence, we can show that h and h\u2032 encode the same function by showing their equivalence on these points: h(0) = 0 = h\u2032(0), h(\u22121) =\u2211\ni,wi<0 \u2212Aiwi = \u2212\u03b3 = h\u2032(\u22121), and h(1) = \u2211 i,wi>0\nAiwi = \u03b1+ \u03b3. As \u03b3z and \u03b1ReLU(z) are convex/concave, and their \u2206-analysis yields their convex hulls, the pointwise sum of their convex hulls, i.e. the \u2206-analysis of h\u2032, recovers the convex hull of h\u2032 by Lemma 14 and is thus at least as precise as any convex-relaxation of h. \u25a1\nLemma 23 (Simplification of Composed ReLUs w.r.t. \u2206). We have\nh(z) = ReLU(\u03b3z + \u03b1ReLU(z)) \u2206 \u21dd h\u2032(z) = \u03b3\u2032z + \u03b1\u2032 ReLU(z),\nwhere \u03b3\u2032 = \u2212ReLU(\u2212\u03b3) and \u03b1\u2032 = ReLU(\u03b1+ \u03b3)\u2212 \u03b3\u2032. Proof. We observe that h(z) is convex and piecewise-linear for any z \u2208 [l, u] \u2282 R with l < 0 < u and a slope change only at z = 0. Its convex hull is thus spanned by h(l) = ReLU(\u03b3l) = h\u2032(l), h(0) = h\u2032(0) = 0, and h(u) = ReLU((\u03b3+\u03b1)u) = h\u2032(u). We further observe that the \u2206-relaxation of ReLU(z) and z is their convex hull. Finally, the convex hull of the positive sum of convex functions is equal to the pointwise sum of their individual convex hulls (Lemma 14). Thus, the triangle-relaxation of h\u2032(z) recovers the convex hull of h(z) and thus the tightest possible convex relaxation. \u25a1\nFor convex functions f, g : R \u2192 R, we define the convex hull Hf ([l, u]) = {(x, y) | x \u2208 [l, u], f(x) \u2264 y \u2264 f(l) + f(u)\u2212f(l)u\u2212l (x\u2212 l)} over [l, u] \u2282 R. Further, we define the convex hull sum of f and g on [l, u] to beHf +Hg := {(x, y\u2032 + y\u2032\u2032) | (x, y\u2032) \u2208 Hf , (x, y\u2032\u2032) \u2208 Hg}. Lemma 14 (Convex Hull Sum). Given two convex functions f, g : R\u2192 R and the box [l, u]. Then, the pointwise sum of the convex hullsHf +Hg is identical to the convex hull of the sum of the two functionsHf+g = Hf +Hg . Proof. We first show that every point in Hf+g can be obtained from Hf + Hg . Let (x, y) \u2208 Hf+g([l, u]). Then we have\n(f + g)(x) \u2264y \u2264 (f + g)(l) + (f+g)(u)\u2212(f+g)(l)u\u2212l (x\u2212 l),\nf(x) + g(x) \u2264y \u2264 f(l) + f(u)\u2212f(l)u\u2212l (x\u2212 l) + g(l) + f(u)\u2212f(l) u\u2212l (x\u2212 l).\nThen we can find a partition of y = y\u2032 + y\u2032\u2032. We know for sure that there exists t \u2208 [0, 1] s.t.\ny = (1\u2212 t)(f + g)(x) + t((f + g)(l) + (f+g)(u)\u2212(f+g)(l)u\u2212l (x\u2212 l)). Hence if we pick for example\ny\u2032 = (1\u2212 t)f(x) + t(f(l) + f(u)\u2212f(l)u\u2212l (x\u2212 l)) \u2208 Hf y\u2032\u2032 = (1\u2212 t)g(x) + t(g(l) + g(u)\u2212g(l)u\u2212l (x\u2212 l)) \u2208 Hg,\nwe get immediately that (x, y) \u2208 Hf +Hg . The other direction is immediate. \u25a1\nUsing Lemma 23, we can show that these networks mapping R2 to R can be simplified further: Theorem 18 (Network Simplification). Let hk be a network as in Theorem 17 such that all ReLUs change activation state at z := w\u22a4x = 0 with w \u2208 Rd. We have\nhk = hk\u22121L +W ReLU(h k\u22121 R ) \u2206 \u21dd h(x) = b+Wx+ \u03b1ReLU(z),\nwhere h0(x) = b0 +W0x and all ReLU change state exactly at {x \u2208 X | w\u22a4x = 0}.\nNote that, hk, hk\u22121L and h map to R, while h k\u22121 R maps to some Rn.\nProof. We show a more general result on hk with possibly many output dimensions by induction:\nInduction Hypothesis: hi \u2206\u21dd bi +Wix+\u03b1i ReLU(z).\nBase Case: h0(x) = b0 +W0x satisfies the form h0(x) = b0 +W0x+\u03b10 ReLU(z) for \u03b10 = 0, thus we can replace h0(x) by itself.\nInduction Step: Using the induction hypothesis, we have Wi ReLU(hi\u22121R ) = Wi ReLU(bi\u22121 + Wi\u22121x + \u03b1i\u22121 ReLU(z)), which by Theorem 17 only changes its activation state at z = 0. Since ReLU(0) = 0, we must have bi\u22121 +Wi\u22121x = wz for some w (recall that z is the projection of x on a hyperplane in the input space). Further, applying Lemma 23, we obtain\nWi ReLU(h i\u22121 R ) =Wi ReLU(wz +\u03b1i\u22121 ReLU(z))\n\u2206 \u21dd\u03b3\u2032iz +\u03b1 \u2032 i ReLU(z) = b \u2032 i +W \u2032 ix+\u03b1 \u2032 i ReLU(z).\nUsing the induction hypothesis, we can thus rewrite:\nhi = hi\u22121 +Wi ReLU(h i\u22121 R ) \u2206 \u21dd b+Wx+\u03b1i ReLU(z).\n\u25a1\nTheorem 19 (Triangle max). ReLU-networks of the form h(x, y) = b+wxx+wyy+\u03b1ReLU(x\u2212 y) can not \u2206-express the function max: R2 \u2192 R. Proof. We first constrain our parameters by considering the following:\n\u2022 For x = y = 0, we have f(0, 0) = 0, leading to b = 0 = h(0, 0).\n\u2022 For x < y, we have f(x, y) = y = wxx+ wyy = h(x, y) and thus wx = 0, wy = 1.\n\u2022 For x > y, we have f(x, y) = x = y + \u03b1(x\u2212 y) = h(x, y) and thus \u03b1 = 1.\nHence we have h(x, y) = y +ReLU(x\u2212 y). 0\nx\u2212 y\n} \u2264 ReLU(x\u2212 y) \u2264 12 (x\u2212 y + 1)\nAdding y results in the following:\ny\nx\n} \u2264 h(x, y) \u2264 12 (x+ y + 1).\nThe maximum of the upper bound is attained at x = y = 1, where we get 32 which is larger than max(x, y) = 1 for x, y \u2208 [0, 1].\n\u25a1\nTheorem 24 (Triangle max \u2113p). ReLU-networks of the form h(x, y) = b + wxx + wyy + \u03b1ReLU(x \u2212 y) can not \u2206-express the function max: R2 \u2192 R for any \u2113p-norm bounded perturbation with p \u2265 1, i.e., input regions B\u03f5p(x) := {x\u2032 \u2208 X | \u2225x\u2212 x\u2032\u2225p \u2264 \u03f5}.\nProof. We first consider the case of p > 1 and f = max(x, y): We again constrain our parameters as in the proof of Theorem 19 to obtain h(x, y) = y +ReLU(x\u2212 y). We consider the input region B\u03f5=0.5p ([\nx0 y0\n] = [ 0.5 0.5 ]) . We can now use H\u00f6lder\u2019s inequality to compute bounds on the ReLU\ninput:\n\u22121 2 \u2225\u2225\u2225\u2225[ 1\u22121 ]\u2225\u2225\u2225\u2225\nq\ufe38 \ufe37\ufe37 \ufe38 cq :=\n= x0 \u2212 y0 \u2212 \u2225\u2225\u2225\u2225[ 1\u22121 ]\u2225\u2225\u2225\u2225 q \u03f5 \u2264 x\u2212 y \u2264 x0 \u2212 y0 + \u2225\u2225\u2225\u2225[ 1\u22121 ]\u2225\u2225\u2225\u2225 q \u03f5 = 1 2 \u2225\u2225\u2225\u2225[ 1\u22121 ]\u2225\u2225\u2225\u2225\nq\ufe38 \ufe37\ufe37 \ufe38 cq :=\n,\nwhere 1p + 1 q = 1. And thus obtain the following bounds on the ReLU output:\n0\nx\u2212 y\n} \u2264 ReLU(x\u2212 y) \u2264 12 (x\u2212 y + cq 2 ).\nAdding y results in the following: y\nx\n} \u2264 h(x, y) \u2264 12 (x+ y + cq 2 ).\nWe can again use H\u00f6lder\u2019s inequality to bound the upper bound:\n1 2 (x+ y + cq 2 ) \u2264 1 2 (x0 + y0 + cq 2 ) + \u2225\u2225\u2225\u2225[11 ]\u2225\u2225\u2225\u2225\nq\ufe38 \ufe37\ufe37 \ufe38 =cq\n\u03f5 2 = 1 2 + 1 2cq = 1 2 (1 + 2\n1 q ) > 1,\nWhere the last inequality holds due to p > 1 =\u21d2 q <\u221e. This upper bound is strictly greater than maxB0.5p max(x, y) = 1.\nWe now consider the case of p = 1 and the rotated max function f = max(x\u2212y\u221a 2 , x+y\u221a 2 ). We choose\nthe input region B \u03f5=\n1\u221a 2\np=1 ([ 1\u221a 2 0 ]) and observe that we recover the setting as discussed in the proof\nof Theorem 19 for p = \u221e rotated by 45\u25e6 around the origin and thus obtain the same imprecise bounds. \u25a1\nTheorem 20 (\u2206 Impossibility max). Finite ReLU networks can not \u2206-express the function max. Proof. We will prove this theorem in four steps."
        },
        {
            "heading": "First \u2013 Locality",
            "text": "1. There exists a point (x, y = x) with an \u03f5-neighborhood U \u2032 such that one of the following holds for any ReLU(v) with input v = hv(x, y) of the network h:\n\u2022 the ReLU is always active, i.e., \u2200(x, y) \u2208 U,ReLU(v) = v, \u2022 the ReLU is never active, i.e., \u2200(x, y) \u2208 U,ReLU(v) = 0, or \u2022 the ReLU changes activation state for x = y, i.e., \u2203v\u2032 \u2208 R, s.t.,ReLU(v) = ReLU(v\u2032(x\u2212 y)).\n2. Further, there exists a neighborhood U of (x, y) such that the above holds under \u2206-analysis, as it depends continuously on the input bounds and becomes exact for any network when the input bounds describe a point.\n3. Via rescaling and translation, we can assume that the point (x, y) is at 0 and that the neighborhood U covers [\u22121, 1]2.\nSecond \u2013 Network Form On the neighborhood U , any finite ReLU-network h can, w.r.t. \u2206, be replaced by hk = hk\u22121 +W ReLU(hk\u22121) with biases b \u2208 Rdk , weight matrices W \u2208 Rdk\u00d7dk\u22121 , and h0(v) = b+Wv, where all ReLUs change activation state exactly for x = y (Theorem 17).\nThird \u2013 Network Simplifications We can replace hk w.r.t. triangle with b+Wk(x, y)\u22a4+\u03b1kR(x\u2212 y) (Theorem 18).\nFourth \u2013 Conclusion Every finite ReLU network can be replaced w.r.t. \u2206 with a single layer network of the form h1(x, y) = b+W (x, y)\u22a4+\u03b1R(x\u2212y). However, there exists no such network encoding the max-function such that its \u2206-analysis is precise on the interval [0, 1]2 (Theorem 19).\n\u25a1\nCorollary 21 (\u2206 Impossibility). Finite ReLU networks can not \u2206-express the set of convex, monotone, CPWL functions mapping from some box I \u2282 R2 to R."
        },
        {
            "heading": "B DEFERRED PROOFS ON UNIVARIATE FUNCTIONS",
            "text": ""
        },
        {
            "heading": "B.1 BOX",
            "text": "Lemma 7 (Step Function). Let \u03b2 \u2208 R\u22650 and f \u2208 CPWL(I,R) s.t. f(x) = 0 for x < x0, f(x) = \u03b2 for x > x1 and linear in between. Then, \u03d5x0,x1,\u03b2(x) = \u03b2 \u2212 ReLU(\u03b2 \u2212 \u03b2 x1\u2212x0 ReLU(x \u2212 x0)) encodes f .\nProof. We prove the theorem by considering the three cases separately:\n1. For x \u2264 x0 we have\n\u03d5x0,x1,\u03b2(x) = \u2212ReLU(\u2212 \u03b2 x1\u2212x0 ReLU(x\u2212 x0) + \u03b2) + \u03b2\n= \u2212ReLU(\u2212 \u03b2x1\u2212x0 \u00b7 0 + \u03b2) + \u03b2 = \u2212ReLU(\u03b2) + \u03b2 = \u2212\u03b2 + \u03b2 = 0.\n2. For x0 \u2264 x \u2264 x1 we have\n\u03d5x0,x1,\u03b2(x) = \u2212ReLU(\u2212 \u03b2 x1\u2212x0 ReLU(x\u2212 x0) + \u03b2) + \u03b2\n= \u2212ReLU(\u2212 \u03b2x1\u2212x0 (x\u2212 x0) + \u03b2) + \u03b2 = \u03b2x1\u2212x0 (x\u2212 x0)\u2212 \u03b2 + \u03b2 = \u03b2x1\u2212x0 (x\u2212 x0).\n3. For x \u2265 x1 we have\n\u03d5x0,x1,\u03b2(x) = \u2212ReLU(\u2212 \u03b2 x1\u2212x0 ReLU(x\u2212 x0) + \u03b2) + \u03b2\n= \u2212ReLU(\u2212 \u03b2x1\u2212x0 (x\u2212 x0) + \u03b2) + \u03b2 = \u22120 + \u03b2 = \u03b2.\n\u25a1\nLemma 8 (Precise Step). The IBP-analysis of \u03d5x0,x1,\u03b2 is precise.\nProof. Consider the box [l, u] \u2286 R.\n\u03d5x0,x1,\u03b2([l, u]) = \u2212ReLU(\u2212 \u03b2 x1\u2212x0 ReLU([l, u]\u2212 x0) + \u03b2) + \u03b2\n= \u2212ReLU(\u2212 \u03b2x1\u2212x0 ReLU([l \u2212 x0, u\u2212 x0]) + \u03b2) + \u03b2 = \u2212ReLU(\u2212 \u03b2x1\u2212x0 [ReLU(l \u2212 x0),ReLU(u\u2212 x0)] + \u03b2) + \u03b2 = \u2212ReLU([ \u03b2x1\u2212x0 ReLU(u\u2212 x0), \u03b2 x1\u2212x0 ReLU(l \u2212 x0)] + \u03b2) + \u03b2 = \u2212ReLU([ \u03b2x1\u2212x0 ReLU(u\u2212 x0) + \u03b2, \u03b2 x1\u2212x0 ReLU(l \u2212 x0) + \u03b2]) + \u03b2 = \u2212ReLU([ \u03b2x1\u2212x0 ReLU(u\u2212 x0) + \u03b2, \u03b2 x1\u2212x0 ReLU(l \u2212 x0) + \u03b2]) + \u03b2 = \u2212[ReLU( \u03b2x1\u2212x0 ReLU(u\u2212 x0) + \u03b2),ReLU( \u03b2 x1\u2212x0 ReLU(l \u2212 x0) + \u03b2)] + \u03b2 = [ReLU( \u03b2x1\u2212x0 ReLU(l \u2212 x0) + \u03b2),ReLU( \u03b2 x1\u2212x0 ReLU(u\u2212 x0) + \u03b2)] + \u03b2 = [ReLU( \u03b2x1\u2212x0 ReLU(l \u2212 x0) + \u03b2) + \u03b2,ReLU( \u03b2\nx1\u2212x0 ReLU(u\u2212 x0) + \u03b2) + \u03b2] = [\u03d5x0,x1,\u03b2(l), \u03d5x0,x1,\u03b2(u)].\n\u25a1\nTheorem 9 (Precise Monotone). Finite ReLU networks can IBP-express the set of monotone CPWL(I,R) functions precisely. Proof. W.l.o.g. assume f is monotonously increasing. Otherwise, consider \u2212f . Let xi for i \u2208 {0, . . . , n} be the set of boundary points of the linear regions of f with x0 < \u00b7 \u00b7 \u00b7 < xn. We claim that\nh(x) = f(x0) + n\u22121\u2211 i=0 \u03d5xi,xi+1,f(xi+1)\u2212f(xi)(x)\nis equal to f on I and that the IBP-analysis of h is precise. We note that f(xi+1)\u2212 f(xi) > 0. We first show f = h on I. For cach x \u2208 I pick i \u2208 {1, . . . , n} such that xj\u22121 \u2264 x < xj . Then\nh(x) = f(x0) + n\u22121\u2211 i=0 \u03d5xi,xi+1,f(xi+1)\u2212f(xi)(x)\n= f(x0) + j\u2211 i=0 \u03d5xi,xi+1,f(xi+1)\u2212f(xi)(x)\n= f(x0) + j\u22121\u2211 i=0 \u03d5xi,xi+1,f(xi+1)\u2212f(xi)(x) + \u03d5xj ,xj+1,f(xj+1)\u2212f(xj)(x)\n= f(x0) + j\u22121\u2211 i=0 [f(xi+1)\u2212 f(xi)] + f(xj+1)\u2212f(xj)xj+1\u2212xj x = f(xj) + f(xj+1)\u2212f(xj)\nxj+1\u2212xj (x\u2212 xj)\n= f(x),\nwhere we used the piecewise linearity of f in the last step.\nNow we show that the analysis of IBP of h is precise. Consider the box [l, u] \u2286 I. We have\nh([l, u]) = f(x0) + n\u2211 i=1 \u03d5xi,xi+1,f(xi+1)\u2212f(xi)([l, u])\n= f(x0) + n\u22121\u2211 i=1 [\u03d5xi,xi+1,f(xi+1)\u2212f(xi)(l), \u03d5xi,xi+1,f(xi+1)\u2212f(xi)(u)]\n= [f(x0) + n\u22121\u2211 i=1 \u03d5xi,xi+1,f(xi+1)\u2212f(xi)(l), f(x0) + n\u22121\u2211 i=1 \u03d5xi,xi+1,f(xi+1)\u2212f(xi)(u)]\n= [h(l), h(u)]\n= [f(l), f(u)].\n\u25a1"
        },
        {
            "heading": "B.2 DEEPPOLY-0",
            "text": "Lemma 10 (Convex encoding). Let f \u2208 CPWL(I,R) be convex. Then f is encoded by\nh(x) = b+ cx+ n\u22121\u2211 i=1 \u03b3i ReLU(\u00b1i(x\u2212 xi)), (1)\nfor any choice \u00b1i \u2208 {\u22121, 1}, if b and c are set appropriately, where \u03b1i = f(xi+1)\u2212f(xi)xi+1\u2212xi is the slope between points xi and xi+1, and \u03b3i = \u03b1i \u2212 \u03b1i\u22121 > 0 the slope change at xi+1. Proof. First, we show that \u03b3j = \u03b1j \u2212 \u03b1j\u22121. Evaluating h(x) for xj \u2264 x \u2264 xj+1 yields\nh(x) = b+ cx+ n\u22121\u2211 i=1 \u03b3i ReLU(\u00b1i(x\u2212 xi))\n= b+ cx+ n\u22121\u2211 i=1 \u03b3i \u00b1i (x\u2212 xi)[\u00b1i = +, xi < x] + n\u22121\u2211 i=1 \u03b3i \u00b1i (x\u2212 xi)[\u00b1i = \u2212, xi > x]\n= b+ cx+ n\u22121\u2211 i=1 \u03b3i(x\u2212 xi)[\u00b1i = +, xi < x]\u2212 n\u22121\u2211 i=1 \u03b3i(x\u2212 xi)[\u00b1i = \u2212, xi > x]\n= b+ cx+ j\u2211 i=1 \u03b3i(x\u2212 xi)[\u00b1i = +]\u2212 n\u22121\u2211 i=j+1 \u03b3i(x\u2212 xi)[\u00b1i = \u2212].\nThe derivative of h evaluated at x for xj \u2264 x \u2264 xj+1 is \u03b1j :\n\u2202h \u2202x (x) = c+ j\u2211 i=1 \u03b3i[\u00b1i = +]\u2212 n\u22121\u2211 i=j+1 \u03b3i[\u00b1i = \u2212] = \u03b1j .\nBy choosing \u03f5 small enough we can ensure that xj + \u03f5 \u2208 [xj , xj+1] and xj\u2212 \u03f5 \u2208 [xj\u22121, xj ] and thus\n\u03b1j \u2212 \u03b1j\u22121 = \u2202h\n\u2202x (xj + \u03f5)\u2212\n\u2202h \u2202x (xj \u2212 \u03f5)\n= j\u2211 i=1 \u03b3i[\u00b1i = +]\u2212 n\u22121\u2211 i=j+1 \u03b3i[\u00b1i = \u2212]\u2212 j\u22121\u2211 i=1 \u03b3i[\u00b1i = +] + n\u22121\u2211 i=j \u03b3i[\u00b1i = \u2212]\n= \u03b3j [\u00b1j = +] + \u03b3j [\u00b1j = \u2212] = \u03b3j\nNext, we show that one can pick \u00b1i arbitrarily as long as b and c are set appropriately. Pick any choice of \u00b1i \u2208 {\u22121, 1} and set b and c to\nb := f(x0)\u2212 x0 f(x1)\u2212 f(x0)\nx1 \u2212 x0 \u2212 n\u22121\u2211 i=1 \u03b3ixi[\u00b1i = \u2212] = f(x0)\u2212 x0\u03b10 \u2212 n\u22121\u2211 i=1 \u03b3ixi[\u00b1i = \u2212]\nc := \u03b10 + n\u22121\u2211 i=1 \u03b3i[\u00b1i = \u2212].\nWe have h(x) = f(x). Indeed: For any x \u2208 [x0, xn] pick j s.t. x \u2208 [xj , xj+1]. Then\nh(x) = b+ cx+ j\u2211 i=1 \u03b3i(x\u2212 xi)[\u00b1i = +]\u2212 n\u22121\u2211 i=j+1 \u03b3i(x\u2212 xi)[\u00b1i = \u2212]\n= b\u2212 j\u2211\ni=1\n\u03b3ixi[\u00b1i = +] + n\u22121\u2211\ni=j+1 \u03b3ixi[\u00b1i = \u2212]\ufe38 \ufe37\ufe37 \ufe38 offset +x\nc+ j\u2211 i=1 \u03b3i[\u00b1i = +]\u2212 n\u22121\u2211 i=j+1 \u03b3i[\u00b1i = \u2212]  \ufe38 \ufe37\ufe37 \ufe38\nlinear\n.\nThe offset evaluates to b\u2212 j\u2211\ni=1\n\u03b3ixi[\u00b1i = +] + n\u22121\u2211\ni=j+1\n\u03b3ixi[\u00b1i = \u2212]\n= f(x0)\u2212 x0\u03b10 \u2212 n\u22121\u2211 i=1 \u03b3ixi[\u00b1i = \u2212]\u2212 j\u2211 i=1 \u03b3ixi[\u00b1i = +] + n\u22121\u2211 i=j+1 \u03b3ixi[\u00b1i = \u2212] = f(x0)\u2212 x0\u03b10 \u2212 j\u2211\ni=1\n\u03b3ixi[\u00b1i = \u2212]\u2212 j\u2211\ni=1\n\u03b3ixi[\u00b1i = +]\n= f(x0)\u2212 x0\u03b10 \u2212 j\u2211\ni=1\n\u03b3ixi\n= f(x0)\u2212 x0\u03b10 \u2212 \u03b31x1 \u2212 \u03b32x2 \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03b3jxj = f(x0)\u2212 x0\u03b10 \u2212 (\u03b11 \u2212 \u03b10)x1 \u2212 (\u03b12 \u2212 \u03b11)x2 \u2212 \u00b7 \u00b7 \u00b7 \u2212 (\u03b1j \u2212 \u03b1j\u22121)xj = f(x0)\u2212 x0\u03b10 + \u03b10x1 \u2212 \u03b11x1 + \u03b11x2 \u2212 \u03b12x2 \u2212 \u00b7 \u00b7 \u00b7+ \u03b1j\u22121xj \u2212 \u03b1jxj = f(x0) + (x1 \u2212 x0)\u03b10 + (x2 \u2212 x1)\u03b11 \u00b7 \u00b7 \u00b7+ (xj \u2212 xj\u22121)\u03b1j\u22121 \u2212 \u03b1jxj = f(x0) + (f(x1)\u2212 f(x0)) + (f(x2)\u2212 f(x1)) \u00b7 \u00b7 \u00b7+ (f(xj)\u2212 f(xj\u22121))\u2212 \u03b1jxj = f(xj)\u2212 \u03b1jxj ,\nwhere we used that \u03b3i = \u03b1i \u2212 \u03b1i\u22121 and \u03b1i = f(xi+1)\u2212f(xi)xi+1\u2212xi . The linear part evaluates to\nc+ j\u2211 i=1 \u03b3i[\u00b1i = +]\u2212 n\u22121\u2211 i=j+1 \u03b3i[\u00b1i = \u2212]\n= \u03b10 + n\u22121\u2211 i=1 \u03b3i[\u00b1i = \u2212] + j\u2211 i=1 \u03b3i[\u00b1i = +]\u2212 n\u22121\u2211 i=j+1 \u03b3i[\u00b1i = \u2212]\n= \u03b10 + j\u2211 i=1 \u03b3i[\u00b1i = \u2212] + j\u2211 i=1 \u03b3i[\u00b1i = +]\n= \u03b10 + j\u2211 i=1 \u03b3i\n= \u03b10 + j\u2211 i=1 \u03b1i \u2212 \u03b1i\u22121\n= \u03b1i.\nCombining the results, we get\nh(x) = f(xj)\u2212 \u03b1jxj + x\u03b1j = f(xj) + \u03b1j(x\u2212 xj) = f(x),\nby the piecewise linearity of f . \u25a1 Lemma 25 (DP-0 Monotone ReLU). The DP-0-analysis of the 1-layer ReLU network h(x) =\u2211n i=1 \u03b3i ReLU(x\u2212 xi) yields\nh(x),\nh(xj\u22121) + \u03b1j\u22121(x\u2212 xj\u22121)\n} \u2264 h(x) \u2264 { h(x), if all ReLUs are stable, h(u)\u2212h(l)\nu\u2212l (x\u2212 l) + h(l) otherwise.\nwhere xi \u2208 R s.t. 1 \u2264 i \u2264 n and i < p \u21d2 xi < xp, and \u03b3i are either all > 0 or all < 0. j is the smallest i such that xi \u2265 l and k is the largest i such that xi < u. Thus, DP-0 analysis for h(x) is precise.\nProof. W.o.l.g. assume h is monotonously increasing. Otherwise, consider \u2212h. The cases u < x1 and xn < l are immediate. Choose j as the smallest i such that xi \u2265 l and k as the largest i such that xi < u.\nDP-0 yields for ReLU(x\u2212 xi) on l \u2264 x \u2264 u\nx\u2212 xi if i \u2264 j, 0 if j < i < k, 0 if k \u2264 i,  \u2264 ReLU(x\u2212 xi) \u2264  x\u2212 xi if i < j, u\u2212xi u\u2212l (x\u2212 l) if j \u2264 i \u2264 k, 0 if k < i.\nThus we have\nj\u22121\u2211 i=1 \u03b3i(x\u2212 xi) \u2264 h(x) \u2264 j\u22121\u2211 i=1 \u03b3i(x\u2212 xi) + k\u2211 i=j \u03b3i u\u2212xi u\u2212l (x\u2212 l).\nThe term \u2211j\u22121\ni=1 \u03b3i(x\u2212 xi) can be simplified as follows\nj\u22121\u2211 i=1 \u03b3i(x\u2212 xi) = j\u22121\u2211 i=1 \u03b3ix\u2212 j\u22121\u2211 i=1 \u03b3ixi\n= x j\u22121\u2211 i=1 (\u03b1i \u2212 \u03b1i\u22121)\u2212 j\u22121\u2211 i=1 (\u03b1i \u2212 \u03b1i\u22121)xi\n= x(\u03b1j\u22121 \u2212 \u03b10)\u2212 j\u22121\u2211 i=1 \u03b1ixi + j\u22121\u2211 i=1 \u03b1i\u22121xi\n= x(\u03b1j\u22121 \u2212 \u03b10)\u2212 j\u22121\u2211 i=1 \u03b1ixi + j\u22122\u2211 i=1 \u03b1ixi+1 + \u03b10x1\n= x(\u03b1j\u22121 \u2212 \u03b10)\u2212 \u03b1j\u22121xj\u22121 \u2212 j\u22122\u2211 i=1 \u03b1ixi + j\u22122\u2211 i=1 \u03b1ixi+1 + \u03b10x1\n= x(\u03b1j\u22121 \u2212 \u03b10)\u2212 \u03b1j\u22121xj\u22121 + j\u22122\u2211 i=1 \u03b1i(xi+1 \u2212 xi) + \u03b10x1\n= \u03b1j\u22121(x\u2212 xj\u22121) + \u03b10(x1 \u2212 x) + j\u22122\u2211 i=1 (h(xi+1)\u2212 h(xi))\n= \u03b1j\u22121(x\u2212 xj\u22121) + \u03b10(x1 \u2212 x) + h(xj\u22121)\u2212 h(x1) = h(xj\u22121) + \u03b1j\u22121(x\u2212 xj\u22121),\nhence we have proven the lower bound.\nNow we consider the upper bound. We evaluate the upper bound at l and u. If the two linear upper bounds coincide there, they coincide everywhere:\nx = l \u2212\u2192 j\u22121\u2211 i=1 \u03b3i(l \u2212 xi) + k\u2211 i=j \u03b3i u\u2212xi u\u2212l (l \u2212 l)\n= j\u22121\u2211 i=1 \u03b3i(l \u2212 xi) = h(l) = h(u)\u2212h(l)u\u2212l (l \u2212 l) + h(l),\nx = u \u2212\u2192 j\u22121\u2211 i=1 \u03b3i(u\u2212 xi) + k\u2211 i=j \u03b3i u\u2212xi u\u2212l (u\u2212 l)\n= j\u22121\u2211 i=1 \u03b3i(u\u2212 xi) + k\u2211 i=j \u03b3i(u\u2212 xi)\n= k\u2211 i=1 \u03b3i(u\u2212 xi) = h(u) = h(u)\u2212h(l)u\u2212l (u\u2212 l) + h(l),\nhence we have proven the upper bound. \u25a1\nTheorem 11 (DP-0 Convex). For any convex CPWL function f : I \u2192 R, there exists exactly one network of the form h(x) = b+ \u2211 i\u2208I \u03b3i ReLU(\u00b1i(x\u2212 xi)) encoding f , with |I| = n\u2212 1 if f has slope zero on some segment and otherwise |I| = n, such that its DP-0-analysis is precise, where \u03b3i > 0 for all i.\nProof. The proof works as follows:\n\u2022 If the function is monotonously increasing, we show that using a local argument \u00b1j = + and if the function is monotonously decreasing, we argue \u00b1j = \u2212. If f has somewhere zero slope, it will be on a piecewise linear region at the boundary of [x0, xn], in which case we need 1 neuron less.\n\u2022 If the function is non-monotone and has slope zero somewhere, then there are two minima x\u2217 and x\u2217\u2217 that are also switching points. Hence f is for all xj > x \u2217+x\u2217\u2217\n2 increasing and for all xj < x \u2217+x\u2217\u2217\n2 decreasing, so we can reuse the argument from before and need n\u2212 1 neurons for that. Then we argue separately for the ReLU at x\u2217 and x\u2217\u2217.\n\u2022 If the function is non-monotone and has nowhere slope zero, then there exists a unique minimum xj . We then show that there is exactly one splitting of \u03b3j to ReLU(x\u2212 xj) and ReLU(\u2212x+ xj).\n\u2022 Finally, we prove that network is precise.\nWe know that there are finitely many switching points xi, 1 \u2264 i \u2264 n. Case 1: f is monotone. W.o.l.g. assume f is monotonously increasing; the proof is similar for monotonously decreasing f . Assume that \u00b1j = \u2212 for some j. Then there exists \u03f5 > 0 such that for all x \u2208 [xj \u2212 \u03f5, xj + \u03f5], all the ReLUs except ReLU(\u2212x + xj) are stable, i.e., either always 0 or always active. Further, for such inputs, DP-0 yields\n0 \u2264 \u03b3i ReLU(x\u2212 xj) \u2264 \u03b3j2 (\u2212x+ xj + \u03f5).\nAs f has no minimum, at least one other ReLU need to active at xj : If there would be no other active ReLU, then f would have a minimum. The active neuron(s) contributes a linear term of the form \u03b2x, \u03b2 \u0338= 0, hence we get for xj \u2212 \u03f5 \u2264 x \u2264 xj + \u03f5 and some b \u2208 R, DP-0 bounds are\nb+ \u03b2x \u2264 h(x) \u2264 b+ \u03b2x+ 12 (\u2212x+ xj + \u03f5).\nWe know that for x = xj , we get h(xj) and thus h(xj) = b+\u03b2xj . The slope of h at xj\u2212\u03f5 is \u03b2\u2212\u03b3j . Hence, we have h(xj \u2212 \u03f5) = h(xj)\u2212 (\u03b2\u2212 \u03b3j)\u03f5 > b+ \u03b2(xj \u2212 \u03f5) = h(xj)\u2212 \u03b2\u03f5 since \u03b3j\u03f5 > 0. This contradicts the assumption that we are precise since the lower bound from DP-0 analysis h(xj)\u2212\u03b2 is unequal to the actual lower bound h(xj \u2212 \u03f5). Hence, we have to have \u00b1j = + for all j. Case 2: f is not monotone and has slope 0 somewhere. Then, there are two minima which are also switching points, x\u2217 and x\u2217\u2217. The argument in Case 1 is a local one, hence we can use the same argument for all i such that xi /\u2208 {x\u2217, x\u2217\u2217}, so we only need to consider the cases for xj and xj+1 such that xj = x\u2217 and xj+1 = x\u2217\u2217. Since f is convex, the only possible case is that f is monotonously decreasing for x < xj and monotonously increasing for x > xj+1, thus we have \u00b1k = \u2212 for k < j and \u00b1k = + for k > j + 1. Now we claim that \u00b1j = \u2212 and \u00b1j+1 = +: We have either (\u00b1j ,\u00b1j+1) = (\u2212,+) or (\u00b1j ,\u00b1j+1) = (+,\u2212). If not, we would not get a unique minimum. The second case also leads directly to a contradiction: Not only would the pre-factors of the two ReLU need to coincide, i.e., \u03b3j = \u03b3j+1 (otherwise one would not have the minimum between them), the analysis of h on xj \u2212 \u03f5 \u2264 x \u2264 xj + \u03f5 yields (as only the neurons at xj and xj+1 are active there)\n0 \u2264 R(x\u2212 xj) \u2264 12 (x\u2212 xj + \u03f5), \u2212x+ xj+1 = R(\u2212x+ xj+1)\nb+ \u03b3j+1(\u2212x+ xj+1) \u2264 h(x) \u2264 b+ \u03b3j+1(\u2212x+ xj+1) + \u03b3j+12 (x\u2212 xj + \u03f5) Here the lower bound is b\u2212 \u03b3j+1\u03f5 < b, thus imprecise. Case 3: f is not monotone and has slope 0 nowhere. Then, there is one minimum x\u2217 = xj . For all xi \u0338= xj we can argue as before. So we just need to argue about xj . Assume we only have one ReLU involving xj . The only unstable ReLU leads to DP-0 lower bound 0, while all others together lead to a linear term ax + b for some a \u0338= 0, thus the overall lower bound from DP-0 is h(xj) \u2212 |a|\u03f5 < h(xj). Therefore, such network cannot be precise under DP-0 analysis. As one ReLU is not enough, we can try two at xj , namely \u03b3\u2032j ReLU(x\u2212 xj) and \u03b3\u2032\u2032j ReLU(\u2212x+ xj). As around \u03b3j no other ReLU is active, it immediately follows that we have \u03b3\u2032j = \u03b1j and \u03b3 \u2032\u2032 j = \u03b1j\u22121.\nNow we finally prove that the network constructed above is precise. Consider the input l \u2264 x \u2264 u. In the case where f is monotone on [l, u], we have the result immediately by using Lemma 25 as all ReLU with an opposite orientation are inactive. In the case where f is on [l, u] not monotone, the above construction yields\nh(x) = b+ \u2211\ni,\u00b1i=\u2212 \u03b3i ReLU(\u2212(x\u2212 xi)) + \u2211 i,\u00b1i=+ \u03b3i ReLU(x\u2212 xi).\nWe can apply Lemma 25 to \u2211 i,\u00b1i=\u2212 \u03b3i ReLU(\u2212(x\u2212xi)) and \u2211\ni,\u00b1i=+ \u03b3i ReLU(x\u2212xi) individually to get\n0 \u2264 \u2211\ni,\u00b1i=\u2212 \u03b3i ReLU(\u2212(x\u2212 xi)) \u2264 h(l)\u2212bu\u2212l (\u2212x+ u),\n0 \u2264 \u2211\ni,\u00b1i=+ \u03b3i ReLU(x\u2212 xi) \u2264 h(u)\u2212bu\u2212l (x\u2212 l).\nHence the combined bounds are b \u2264 h(x) \u2264 h(l) + h(u)\u2212h(l)u\u2212l (x\u2212 l). Evaluating the upper bounds at x = l and x = u yields h(l) and h(u) respectively, hence the bounds are precise. \u25a1"
        },
        {
            "heading": "B.3 DEEPPOLY-1",
            "text": "Corollary 12 (DP-1 ReLU). The ReLU network h(x) = x + ReLU(\u2212x) encodes the function f(x) = ReLU(x) and, the DP-1-analysis of h(x) is identical to the DP-0-analysis of ReLU. Further, the DP-0-analysis of h(x) is identical to the DP-1-analysis of ReLU.\nProof. We first prove that x+ReLU(\u2212x) = ReLU(x). x+ReLU(\u2212x) = ReLU(x)\u2212 ReLU(\u2212x) + ReLU(\u2212x) = ReLU(x).\nNext we show that the DP-0-analysis of R(x) coincides with the DP-1-analysis of x+ReLU(\u2212x).\n\u2022 Case l \u2264 0 \u2264 u, x \u2208 [l, u]: We have for DP-0 and ReLU(x):\n0 \u2264 ReLU(x) \u2264 uu\u2212l (x\u2212 l).\nFor DP-1 and x+ReLU(\u2212x) we have:\n\u2212x \u2264 ReLU(\u2212x) \u2264 lu\u2212l (x\u2212 u),\nHence 0 \u2264 x+ReLU(\u2212x) \u2264 lu\u2212l (x\u2212 u) + x = u u\u2212l (x\u2212 l).\n\u2022 Case 0 \u2264 l \u2264 u, x \u2208 [l, u]: We have for DP-0 and ReLU(x):\nReLU(x) = x.\nFor DP-1 and x+ReLU(\u2212x) we have:\nReLU(\u2212x) = 0,\nHence x+ReLU(\u2212x) = x.\n\u2022 Case l \u2264 u \u2264 0, x \u2208 [l, u]: We have for DP-0 and ReLU(x):\nReLU(x) = 0.\nFor DP-1 and x+ReLU(\u2212x) we have:\nReLU(\u2212x) = \u2212x,\nHence x+\u2212x = 0.\nTo show the opposite, we only need to prove for the case l \u2264 0 \u2264 u since DP-0 and DP-1 are identical when there is no unstable ReLU. For DP-1 and x+ReLU(\u2212x) we have:\n0 \u2264 ReLU(\u2212x) \u2264 lu\u2212l (x\u2212 u),\nHence x \u2264 x+ReLU(\u2212x) \u2264 lu\u2212l (x\u2212 u) + x = u u\u2212l (x\u2212 l).\n\u25a1\nCorollary 13 (DP-1 Approximation). Finite ReLU networks can DP-1- and DP-0-express the same function class precisely. In particular, they can DP-1-express the set of convex functions f \u2208 CPWL(I,R) and monotone functions f \u2208 CPWL(I,R) precisely. Proof. The follows immediately with Corollary 12 and the technique presented in Theorem 11. \u25a1"
        },
        {
            "heading": "B.4 TRIANGLE",
            "text": "Theorem 15 (\u2206 Convex). Let f \u2208 CPWL(I,R) be convex. Then, for any network h encoding f as in Lemma 10, we have that its \u2206-analysis is precise. In particular, \u00b1i can be chosen freely. Proof. The \u2206 analysis of a ReLU over some input range l \u2264 x \u2264 u results in the convex hull of ReLU on that range. With that, we can apply the Lemma 14 over the network h from Lemma 10 modeling f :\nh(x) = b+ cx+ n\u22121\u2211 i=1 \u03b3i ReLU(\u00b1i(x\u2212 xi))\nThis is regardless of the choice of \u00b1i a sum of convex functions (\u03b3i > 0). With Lemma 14 we get that the \u2206 analysis of h results in the convex hull of h, and thus is precise. \u25a1"
        },
        {
            "heading": "B.5 MULTI-NEURON-RELAXATIONS",
            "text": "Theorem 26 (Multi-Neuron Precision). For every f \u2208 CPWL(I,R), there exists a single layer ReLU network h encoding f , such that its multi-neuron analysis (considering all ReLUs jointly) is precise.\nProof. As the multi-neuron relaxation yields the exact convex hull of all considered neurons in their input-output space, it remains to show that every f \u2208 CPWL(I,R) can be represented using a single-layer ReLU network.\nRecall that every f \u2208 CPWL(I,R) can be defined by the points {(xi, yi)}i s.t. xi > xi\u22121 (Definition 1). We set h1(x) = (x\u2212 x0) y1\u2212y0x1\u2212x0 + y0 and now update it as follows:\nhi+1(x) = hi(x) + ( yi+1 \u2212 yi xi+1 \u2212 xi \u2212 yi \u2212 yi\u22121 xi \u2212 xi\u22121 ) ReLU(x\u2212 xi)\nWe observe that ReLU(x\u2212xi) = 0 for all xj such that j \u2264 i. As h(x) is CPWL, it is now sufficient to show that h(xi) = yi,\u2200i.\nh(xj) = (x\u2212 x0) y1\u2212y0x1\u2212x0 + y0 + \u2211 i=1 ( yi+1 \u2212 yi xi+1 \u2212 xi \u2212 yi \u2212 yi\u22121 xi \u2212 xi\u22121 ) ReLU(x\u2212 xi)\n= (xj \u2212 x0) y1\u2212y0x1\u2212x0 + y0 + xj j\u22121\u2211 i=1 ( yi+1 \u2212 yi xi+1 \u2212 xi \u2212 yi \u2212 yi\u22121 xi \u2212 xi\u22121 ) \u2212 j\u22121\u2211 i=1 ( yi+1 \u2212 yi xi \u2212 xi\u22121 \u2212 yi \u2212 yi\u22121 xi \u2212 xi\u22121 ) xi\n= (xj \u2212 x0) y1\u2212y0x1\u2212x0 + y0 + xj ( yj \u2212 yj\u22121 xj \u2212 xj\u22121 \u2212 y1 \u2212 y0 x1 \u2212 x0 ) \u2212 xj yj \u2212 yj\u22121 xj \u2212 xj\u22121\n+ j\u22121\u2211 i=1 yi+1 \u2212 yi xi+1 \u2212 xi (xi+1 \u2212 xi) + x1 y1 \u2212 y0 x1 \u2212 x0 + j\u22121\u2211 i=1 yi \u2212 yi\u22121 xi \u2212 xi\u22121 (xi \u2212 xi)\n= yj\n\u25a1"
        }
    ],
    "year": 2024
}