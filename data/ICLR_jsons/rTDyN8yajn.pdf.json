{
    "abstractText": "Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/paper list/Octavius.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zeren Chen"
        },
        {
            "affiliations": [],
            "name": "Ziqin Wang"
        },
        {
            "affiliations": [],
            "name": "Zhen Wang"
        },
        {
            "affiliations": [],
            "name": "Huayang Liu"
        },
        {
            "affiliations": [],
            "name": "Zhenfei Yin"
        },
        {
            "affiliations": [],
            "name": "Si Liu"
        },
        {
            "affiliations": [],
            "name": "Lu Sheng"
        },
        {
            "affiliations": [],
            "name": "Wanli Ouyang"
        },
        {
            "affiliations": [],
            "name": "Jing Shao"
        }
    ],
    "id": "SP:dcd9ee680044ccb261e082822fc25e2ee63e1710",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Ahmed Abdelreheem",
                "Fei Xia",
                "Mohamed Elhoseiny",
                "Leonidas Guibas"
            ],
            "title": "Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Brohan",
                "Noah Brown",
                "Justice Carbajal",
                "Yevgen Chebotar",
                "Xi Chen",
                "Krzysztof Choromanski",
                "Tianli Ding",
                "Danny Driess",
                "Avinava Dubey",
                "Chelsea Finn"
            ],
            "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "venue": "arXiv preprint arXiv:2307.15818,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Keqin Chen",
                "Zhao Zhang",
                "Weili Zeng",
                "Richong Zhang",
                "Feng Zhu",
                "Rui Zhao"
            ],
            "title": "Shikra: Unleashing multimodal llm\u2019s referential dialogue magic",
            "venue": "arXiv preprint arXiv:2306.15195,",
            "year": 2023
        },
        {
            "authors": [
                "Zhao Chen",
                "Vijay Badrinarayanan",
                "Chen-Yu Lee",
                "Andrew Rabinovich"
            ],
            "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "WL Chiang",
                "Z Li",
                "Z Lin",
                "Y Sheng",
                "Z Wu",
                "H Zhang",
                "L Zheng",
                "S Zhuang",
                "Y Zhuang",
                "JE Gonzalez"
            ],
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, mar",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas Funkhouser",
                "Matthias Nie\u00dfner"
            ],
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Nan Du",
                "Yanping Huang",
                "Andrew M Dai",
                "Simon Tong",
                "Dmitry Lepikhin",
                "Yuanzhong Xu",
                "Maxim Krikun",
                "Yanqi Zhou",
                "Adams Wei Yu",
                "Orhan Firat"
            ],
            "title": "Glam: Efficient scaling of language models with mixture-of-experts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jiafei Duan",
                "Samson Yu",
                "Hui Li Tan",
                "Hongyuan Zhu",
                "Cheston Tan"
            ],
            "title": "A survey of embodied ai: From simulators to research tasks",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "ijcv, 88(2):303\u2013338,",
            "year": 2010
        },
        {
            "authors": [
                "Lue Fan",
                "Ziqi Pang",
                "Tianyuan Zhang",
                "Yu-Xiong Wang",
                "Hang Zhao",
                "Feng Wang",
                "Naiyan Wang",
                "Zhaoxiang Zhang"
            ],
            "title": "Embracing single stride 3d object detector with sparse transformer",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Raia Hadsell",
                "Dushyant Rao",
                "Andrei A Rusu",
                "Razvan Pascanu"
            ],
            "title": "Embracing change: Continual learning in deep neural networks",
            "venue": "Trends in cognitive sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Yining Hong",
                "Haoyu Zhen",
                "Peihao Chen",
                "Shuhong Zheng",
                "Yilun Du",
                "Zhenfang Chen",
                "Chuang Gan"
            ],
            "title": "3d-llm: Injecting the 3d world into large language models",
            "venue": "arXiv preprint arXiv:2307.12981,",
            "year": 2023
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Shaohan Huang",
                "Li Dong",
                "Wenhui Wang",
                "Yaru Hao",
                "Saksham Singhal",
                "Shuming Ma",
                "Tengchao Lv",
                "Lei Cui",
                "Owais Khan Mohammed",
                "Qiang Liu"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "venue": "arXiv preprint arXiv:2302.14045,",
            "year": 2023
        },
        {
            "authors": [
                "Robert A Jacobs",
                "Michael I Jordan",
                "Steven J Nowlan",
                "Geoffrey E Hinton"
            ],
            "title": "Adaptive mixtures of local experts",
            "venue": "Neural computation,",
            "year": 1991
        },
        {
            "authors": [
                "Michael I Jordan",
                "Robert A Jacobs"
            ],
            "title": "Hierarchical mixtures of experts and the em algorithm",
            "venue": "Neural computation,",
            "year": 1994
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder"
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal",
                "Roberto Cipolla"
            ],
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Dmitry Lepikhin",
                "HyoukJoong Lee",
                "Yuanzhong Xu",
                "Dehao Chen",
                "Orhan Firat",
                "Yanping Huang",
                "Maxim Krikun",
                "Noam Shazeer",
                "Zhifeng Chen"
            ],
            "title": "Gshard: Scaling giant models with conditional computation and automatic sharding",
            "year": 2006
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu"
            ],
            "title": "Otter: A multi-modal model with in-context instruction tuning",
            "venue": "arXiv preprint arXiv:2305.03726,",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Large-scale celebfaces attributes (celeba) dataset",
            "venue": "Retrieved August,",
            "year": 2018
        },
        {
            "authors": [
                "Pan Lu",
                "Swaroop Mishra",
                "Tanglin Xia",
                "Liang Qiu",
                "Kai-Wei Chang",
                "Song-Chun Zhu",
                "Oyvind Tafjord",
                "Peter Clark",
                "Ashwin Kalyan"
            ],
            "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yao Mu",
                "Qinglong Zhang",
                "Mengkang Hu",
                "Wenhai Wang",
                "Mingyu Ding",
                "Jun Jin",
                "Bin Wang",
                "Jifeng Dai",
                "Yu Qiao",
                "Ping Luo"
            ],
            "title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought",
            "venue": "arXiv preprint arXiv:2305.15021,",
            "year": 2023
        },
        {
            "authors": [
                "Basil Mustafa",
                "Carlos Riquelme",
                "Joan Puigcerver",
                "Rodolphe Jenatton",
                "Neil Houlsby"
            ],
            "title": "Multimodal contrastive learning with limoe: the language-image mixture of experts",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Wenhui Wang",
                "Li Dong",
                "Yaru Hao",
                "Shaohan Huang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "title": "Kosmos-2: Grounding multimodal large language models to the world",
            "venue": "arXiv preprint arXiv:2306.14824,",
            "year": 2023
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Carlos Riquelme",
                "Joan Puigcerver",
                "Basil Mustafa",
                "Maxim Neumann",
                "Rodolphe Jenatton",
                "Andr\u00e9 Susano Pinto",
                "Daniel Keysers",
                "Neil Houlsby"
            ],
            "title": "Scaling vision with sparse mixture of experts",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Danila Rukhovich",
                "Anna Vorontsova",
                "Anton Konushin"
            ],
            "title": "Fcaf3d: Fully convolutional anchor-free 3d object detection",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "arXiv preprint arXiv:1701.06538,",
            "year": 2017
        },
        {
            "authors": [
                "Sheng Shen",
                "Zhewei Yao",
                "Chunyuan Li",
                "Trevor Darrell",
                "Kurt Keutzer",
                "Yuxiong He"
            ],
            "title": "Scaling vision-language models with sparse mixture of experts",
            "venue": "arXiv preprint arXiv:2303.07226,",
            "year": 2023
        },
        {
            "authors": [
                "Shaoshuai Shi",
                "Chaoxu Guo",
                "Li Jiang",
                "Zhe Wang",
                "Jianping Shi",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning",
            "year": 1929
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Wouter Van Gansbeke",
                "Marc Proesmans",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Multi-task learning for dense prediction tasks: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yaqing Wang",
                "Sahaj Agarwal",
                "Subhabrata Mukherjee",
                "Xiaodong Liu",
                "Jing Gao",
                "Ahmed Hassan Awadallah",
                "Jianfeng Gao"
            ],
            "title": "Adamix: Mixture-of-adaptations for parameter-efficient model tuning",
            "venue": "arXiv preprint arXiv:2210.17451,",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lemeng Wu",
                "Mengchen Liu",
                "Yinpeng Chen",
                "Dongdong Chen",
                "Xiyang Dai",
                "Lu Yuan"
            ],
            "title": "Residual mixture of experts",
            "venue": "arXiv preprint arXiv:2204.09636,",
            "year": 2022
        },
        {
            "authors": [
                "Le Xue",
                "Mingfei Gao",
                "Chen Xing",
                "Roberto Mart\u0131\u0301n-Mart\u0131\u0301n",
                "Jiajun Wu",
                "Caiming Xiong",
                "Ran Xu",
                "Juan Carlos Niebles",
                "Silvio Savarese"
            ],
            "title": "Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Qinghao Ye",
                "Haiyang Xu",
                "Guohai Xu",
                "Jiabo Ye",
                "Ming Yan",
                "Yiyang Zhou",
                "Junyang Wang",
                "Anwen Hu",
                "Pengcheng Shi",
                "Yaya Shi"
            ],
            "title": "mplug-owl: Modularization empowers large language models with multimodality",
            "venue": "arXiv preprint arXiv:2304.14178,",
            "year": 2023
        },
        {
            "authors": [
                "Zhenfei Yin",
                "Jiong Wang",
                "Jianjian Cao",
                "Zhelun Shi",
                "Dingning Liu",
                "Mukai Li",
                "Lu Sheng",
                "Lei Bai",
                "Xiaoshui Huang",
                "Zhiyong Wang"
            ],
            "title": "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark",
            "venue": "arXiv preprint arXiv:2306.06687,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier"
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2014
        },
        {
            "authors": [
                "Xumin Yu",
                "Lulu Tang",
                "Yongming Rao",
                "Tiejun Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Renrui Zhang",
                "Jiaming Han",
                "Aojun Zhou",
                "Xiangfei Hu",
                "Shilin Yan",
                "Pan Lu",
                "Hongsheng Li",
                "Peng Gao",
                "Yu Qiao"
            ],
            "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
            "venue": "arXiv preprint arXiv:2303.16199,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Yuanhan Zhang",
                "Qinghong Sun",
                "Yichun Zhou",
                "Zexin He",
                "Zhenfei Yin",
                "Kun Wang",
                "Lu Sheng",
                "Yu Qiao",
                "Jing Shao",
                "Ziwei Liu"
            ],
            "title": "Bamboo: Building mega-scale vision dataset continually with human-machine synergy",
            "venue": "arXiv preprint arXiv:2203.07845,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyun Zhao",
                "Haoxiang Li",
                "Xiaohui Shen",
                "Xiaodan Liang",
                "Ying Wu"
            ],
            "title": "A modulation module for multi-task learning with applications in image retrieval",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yanqi Zhou",
                "Tao Lei",
                "Hanxiao Liu",
                "Nan Du",
                "Yanping Huang",
                "Vincent Zhao",
                "Andrew M Dai",
                "Quoc V Le",
                "James Laudon"
            ],
            "title": "Mixture-of-experts with expert choice routing",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "arXiv preprint arXiv:2304.10592,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Multimodal Large Language Models (MLLMs) (Alayrac et al., 2022; Huang et al., 2023; Liu et al., 2023; Li et al., 2023a; Zhu et al., 2023) have been considered as promising general-purpose interfaces that can perform various multimodal tasks under few-/zero-shot settings. Apart from leveraging the powerful Large Language Models (LLMs) (OpenAI, 2023; Touvron et al., 2023a) as the universal interfaces that unify the responses to different types of tasks as task-specified textual sequences, the keys to the success of MLLMs are to reliably perceive more modalities and be efficiently fine-tuned to adapt more downstream tasks.\nTo achieve this goal, MLLMs rely on the instruction-tuning scheme (Ouyang et al., 2022) where the model is fine-tuned based on multimodal instruction-following dialogues orchestrated from various multimodal tasks. Moreover, thanks to the Parameter-Efficient Fine-Tuning (PEFT) techniques (e.g., LoRA (Hu et al., 2021) and Adapter (Houlsby et al., 2019)) where only small trainable components are injected in the model and updated during fine-tuning, recent MLLMs (Zhang et al., 2023; Yin et al., 2023; Ye et al., 2023) can efficiently learn to solve downstream tasks with a small scale of annotated data, while preserve the language proficiency and generalizability to novel situations. Remarkably, these models achieve comparable performance at low costs in comparison to LLaVA (Liu et al., 2023), KOSMOS series (Huang et al., 2023; Peng et al., 2023) and Shikra (Chen et al., 2023), which are learned by full model fine-tuning with a large amount of multimodal data.\nHowever, PEFT has to address the crucial tug-of-war problem (Hadsell et al., 2020), where simultaneously learning different tasks may cancel each task-specific optimization out, and ultimately com-\n\u2217Equal contribution. \u2020Project leader. \u2021Corresponding author.\npromise the performance of each downstream task. This problem is much more severe in MLLMs, especially when more modalities and tasks are involved, but only a few well-annotated data are available. First, the features from new modalities are not easy to be aligned with each other, not to mention compatible with the LLM-based language decoders. Second, simultaneously learning to acquire knowledge at distinct granularities, such as the instance-level perception (e.g., object detection) and logical reasoning (e.g., VQA), may lead to significant interference. Third, it is more complicated by using an LLM-based decoder to generate textual responses that meet the special requirements of tasks in modalities other than natural language, such as the bounding box coordinates in detection tasks, or action sequences for robotics.\nTo resolve this issue, we propose LoRA-MoE, which combines the well-known Mixture-of-Experts (MoE) (Jacobs et al., 1991; Jordan & Jacobs, 1994) and one of the representative PEFT techniques, i.e., LoRA (Hu et al., 2021). Based on LoRA-MoE, an LLM-based decoder can efficiently be involved in more downstream tasks and more modalities by learning more LoRA modules. Different from conventional MoE models (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022), we adopt a simple yet effective instance-based gate routing scheme, sparsely activating independent LoRA experts with instance-level instructions and further acquiring task-specific knowledge for better aligning different tasks. Notably, to the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address the tug-of-war problem.\nTo validate the effectiveness of LoRA-MoE, in this work, we investigate a more complicated scenario, where the MLLMs should simultaneously learn downstreaming tasks from more additional modalities, such as 2D images and 3D point clouds. This scenario is especially useful for embodied agents (Duan et al., 2022; Mu et al., 2023; Driess et al., 2023). Specifically, in addition to the offthe-shelf image encoder, we design a point cloud encoder called Object-As-Scene, which provides language-aligned scene-level point cloud representations. This encoder at first gathers languagealigned point cloud features of each instance (Xue et al., 2023) in a scene, which are then aggregated into a scene-level feature based on the attention operation guided by the input instructions.\nBased on the aforementioned contributions, we introduce a novel and extensive framework called Octavius, which learns the MLLMs upon the instruction-following datasets adapted from LAMM (Yin et al., 2023) and ScanNet (Dai et al., 2017). As shown in Figure 1, Octavius can successfully address various 2D/3D vision and language tasks, including but not limited to 2D detection, 2D captioning, 3D VQA, and 3D dense captioning. We conduct various experiments to validate the effectiveness and versatility of our design, improving multiple downstream tasks by about 20% while increasing only a few trainable parameters.\nImage\nPoint Cloud\nC LIP\nModality Embedding Question !!\"System !#$#\nD etector\nPoint-Bert\nFusion\nLearnable Queries\nTask-specific Response Caption Detection Classification\n\u2026\nMore tasksVQA\n!%&' !()*or\nMultimodal Instruction 2D Projector\n3D Projector\nLanguage Model with LoRA-MoE\nGating \"\nLarge Language Model LoRA-MoE LoRA #1+ LoRA #2 LoRA #3\n0.8 0. 0.2 0.\nLoRA #4\nTrainable Frozen\nor\nMultimodal Encoder\nFigure 2: Overall pipeline of Octavius. We design corresponding encoders for different modalities, with the primary objective of empowering the LLMs to gain a deeper understanding of visual features. Additionally, we propose a dynamic gating network that selects distinct LoRA experts based on input instructions, thereby proficiently mitigating interference arising from multimodal learning."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Large Language Models (LLMs) & PEFT. Recently, Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Chiang et al.; Touvron et al., 2023b) have gained significant attention due to their impressive capabilities in language generation (Zhang et al., 2022a), in-context learning (Wei et al., 2022), and reasoning (Touvron et al., 2023a). For both data- and compute-efficient adaptation on certain downstream tasks, several PEFT (Parameter-Efficient FineTuning) (Li & Liang, 2021; Houlsby et al., 2019; Karimi Mahabadi et al., 2021; Hu et al., 2021) are proposed. For instance, LoRA (Hu et al., 2021) represents weight updates using two smaller matrices through low-rank decomposition, where original weights are kept frozen while the new update matrices are trained. In this work, we adopt LoRA for efficient MLLMs fine-tuning.\nMultimodal Large Language Models (MLLMs). Several recent studies have attempted to extend the capability of LLMs to multimodal tasks. Alayrac et al. (2022); Li et al. (2023b); Liu et al. (2023); Zhang et al. (2023); Yin et al. (2023); Chen et al. (2023); Peng et al. (2023) introduce image modality in LLMs for comprehending 2D visual content. Hong et al. (2023) combines LLMs with 3D modality by rendering point clouds into 2D images and utilizing them to represent 3D visual features. Driess et al. (2023); Mu et al. (2023); Brohan et al. (2023) establish connections between visual inputs and embodied controls for robotic tasks. Despite its wide range of multimodal applications, the performance degradation caused by interference between tasks and modalities during fine-tuning in MLLMs receives inadequate attention.\nMixture-of-Experts (MoE). Deep MoE models are proposed to increase the number of model parameters without adding computational overhead in the field of computer vision (Riquelme et al., 2021; Mustafa et al., 2022; Shen et al., 2023) and natural language processing (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022). Different from these approaches, we aim to address conflicts between tasks with MoE. Adamix (Wang et al., 2022a), an approach related to but distinct from ours, randomly selects experts during training and uses the average weights of experts in inference, which may be analogous to dropout (Srivastava et al., 2014) in certain cases. In this paper, we desire for a dynamic gate routing strategy to automatically calculate the weights of each LoRA expert according to the input instructions, adapting MLLMs for broader multimodal applications."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "As illustrated in Figure 2, we propose an extensible framework called Octavius for multimodal instruction tuning. In Section 3.1, we first elaborate on the tug-of-war problem and propose a unified LoRA-MoE decoder to break through the bottleneck caused by interference between different tasks and modalities. We then verify our design using both image and point cloud modality in this work and describe corresponding encoders in Section 3.2."
        },
        {
            "heading": "3.1 MULTIMODAL DECODER",
            "text": ""
        },
        {
            "heading": "3.1.1 THE TUG-OF-WAR PROBLEM",
            "text": "Interference among different modalities and tasks is a common and critical issue (Zhao et al., 2018; Vandenhende et al., 2021) in multimodal and multitask learning. While MLLMs can alleviate this problem by adopting the same learning objective, i.e., next-token prediction loss, for all tasks, there still exists task-specific divergences that limit their potential in various downstream tasks. Since previous works have yet to delve into the tug-of-war phenomenon in MLLMs, we conduct a simple pilot study on image modality to reveal this problem.\nThe pipeline of image encoders in MLLMs are simple and similar in previous works. Here, we select LAMM (Yin et al., 2023) as our model due to its rich benchmarks on downstream tasks. We finetune LoRA and projector in LAMM following (Yin et al., 2023) and validate zero-shot performance on PASCAL VOC (Everingham et al., 2010) and ScienceQA (Lu et al., 2022) datasets. We report the recall and precision at an Intersection over Union (IoU) threshold of 0.5 on PASCAL VOC and the accuracy of multiple-choice questions on ScienceQA.\nThe results are shown in Figure 3. Although the original dataset of LAMM, referred to as \u201cLAMM v1\u201d, contains numerous images from MS-COCO (Lin et al., 2014), the lack of sufficient detection instructions results in poor performance on PASCAL VOC. To overcome this problem, we leverage the entire COCO detection annotations and GPT-API (OpenAI, 2023) to generate additional detection instructions as supplementation, constructing a new dataset called \u201cLAMM v2\u201d for better generalization of detection tasks. After verifying the detection ability of LAMM by using COCO detection instructions alone, we find using a mixed dataset does not lead to a huge improvement in detection performance. Also, there is a decline in the VQA tasks. Moreover, we can achieve the same results on another dataset used in LLaVA (Liu et al., 2023). It can be concluded that MLLMs suffer from a severe tug-of-war problem on image modality, not to mention incorporating more modalities for training simultaneously."
        },
        {
            "heading": "3.1.2 LORA-MOE",
            "text": "Some prior works (Kendall et al., 2018; Chen et al., 2018) have attempted to balance the magnitudes of losses or gradients across different tasks to address the tug-of-war issue. However, considering that different objectives are defined for each task in multitask learning, it is challenging to directly extend existing methods to MLLMs that adopt a unified optimization objective for all tasks. In this section, we introduce the concept of Mixture-of-Experts (Jacobs et al., 1991; Jordan & Jacobs,\n1994; Shazeer et al., 2017), proposing a unified LoRA-MoE decoder based on a instance-based gate routing strategy.\nRevisiting MoE Models. A typical MoE Model injects multiple MoE layers into LLM to accommodate a greater number of parameters. The MoE layer consists of a group of N expert networks E1, E2, ..., EN and a gating network G, taking the previous tokens as input and producing the probability of the next token:\ntoki = N\u2211 k G(tok0...i\u22121)kEk(tok0...i\u22121), (1)\nwhere toki denotes i-th token. We refer to this kind of gating network, which based on token-level input, as token-based gate. Furthermore, to prevent G from consistently producing imbalanced weights that favor only a few experts, an auxiliary loss Lbalance (Shazeer et al., 2017; Zhou et al., 2022; Wu et al., 2022) is introduced to balance gating routing. For example, Shazeer et al. (2017) minimize the coefficient of variation of the gate values for each token, encouraging all experts to have equal importance:\nLbalance = \u03b1 CV [\u2211\ni\nG(tok0...i\u22121)\n]2 , (2)\nwhere \u03b1 is a hyper-parameter and CV(\u00b7) is the coefficient of variation, which is the ratio of the standard deviation to the mean.\nLoRA-MoE and Instance-based Gate Routing. Different from token-based gate in LLM, we design a simple but effective routing strategy for MLLMs, assigning downstream tasks to independent experts for specific knowledge based on individual instances, called instance-based gate. It is motivated that the input questions zqu applied in multimodal instructions will substantially affect the responses generated by MLLMs, we take the questions as input to predict routing scores for each expert. Then, we select sparsely-activated experts based on routing scores for each individual instance to generate the entire sentence. In this work, the LoRA module is treated as an expert in MLLMs, combining instance-based gate with it to alleviate interference arise from multimodal learning, named LoRA-MoE. By replacing LoRA in each projection layer of language model fLLM with a group of independent LoRA experts {ELoRA}N , we can predict the i-th token value as follow:\ntoki = f LLM(tok0...i\u22121) + N\u2211 k G(zqu)kE LoRA k (tok0...i\u22121). (3)\nAdditionally, we find that Lbalance is incompatible and infeasible with LoRA-MoE in an instancebased gate scenario. For example, it is more reasonable to assign detection samples to a LoRA expert proficient in localizing than the other experts for the purpose of balancing. Therefore, we can observe some imbalance phenomenon in experiments (see Section 4.3 for details), unless the amount of data for each task in the whole dataset is balanced.\nCompared with previous MoE models, LoRA-MoE allows for efficient fine-tuning on small datasets and faster convergence with instance-based gate routing. During the inference phase, if the downstream tasks and input questions are specified, LoRA-MoE can also merge parameter weights with language model like vanilla LoRA to reduce storage requirements and inference costs.\nInstruction Tuning with LoRA-MoE. Given the target modal features zimg or zpcl, we construct image-text conversation pairs in an instruction-following format based on previous works (Zhang et al., 2023; Yin et al., 2023; Peng et al., 2023), as shown in Figure 4. The language model with LoRA-MoE is then trained to predict corresponding responses based on the system prompts, target modal features and questions."
        },
        {
            "heading": "3.2 MODALITY ENCODER",
            "text": ""
        },
        {
            "heading": "3.2.1 IMAGE ENCODER",
            "text": "Benefit from the pioneer vision-language model (Radford et al., 2021) that bridges the gap between the image and language modality, Li et al. (2023a); Liu et al. (2023); Chen et al. (2023); Zhang et al.\n(2023); Yin et al. (2023) achieve impressive results. We follow their pipeline to extract features for image modality. Specifically, for an image input I \u2208 RH\u00d7W\u00d73, we use the pre-trained CLIP visual encoder ViT-L/14 fCLIP (Radford et al., 2021) to extract the language-aligned visual feature himg, following by a trainable linear layer f proj to match the dimension of himg with the word embedding space in language model:\nhimg = fCLIP(I); zimg = f proj(himg), (4)\nwhere zimg is the output features of image modality for further instruction tuning."
        },
        {
            "heading": "3.2.2 POINT CLOUD ENCODER",
            "text": "Conventional 3D methods typically apply 3D CNNs (Yan et al., 2018; Shi et al., 2020; Qi et al., 2017) or Transformers (Zhao et al., 2021; Fan et al., 2022) as feature extractors to process sparse point cloud data. However, they still retain numerous background points with low-density information, which may confuse the subsequent language models in MLLMs, ignoring the pivotal elements in the scene. Besides, the unavailability of encoders capable of aligning scene-level 3D features with language may pose significant challenges for LLMs in comprehending the semantic information of the entire scene. To address these issues, we propose Object-As-Scene as our point cloud encoder dedicated to language-aligned scene-level 3D representation generation, as illustrated in Figure 5.\nStep 1: Locating Regional RoIs as Candidates. An intuitive way to avoid excessive background points is to identify specific regions in the scene that may contain instances or relevant semantic information and encode entire scene with these regions. Specifically, given a 3D point cloud scene, we employ a pre-trained object detector, i.e., FCAF3D (Rukhovich et al., 2022), to locate candidate RoIs (Region-of-Interest) {r}NRoI . Note that NRoI denotes the number of RoIs. Besides, for tasks such as captioning or classification that primarily focus on instances mentioned in the conversations, we directly use regional features associated with these instances as input.\nStep 2: Extracting RoI Features Aligned with Language and Image. Inspired by a recent work (Xue et al., 2023), we pre-train a Point-Bert (Yu et al., 2022) encoder fPoint-Bert aligned with both language and image modalities following a ULIP-like pre-training pipeline, allowing us to extract instance-level 3D visual features from points P \u2208 RN\u00d76 in candidate RoIs {r}NRoI during instruction tuning: {hpcl}NRoI = fPoint-Bert(P , {r}NRoI). (5) More details about improved ULIP-like pre-training pipeline can be found in the Appendix.\nStep 3: Aggregating RoI Features as Scene. Next, we adopt a fusion module with two stacked transformer layers to fuse scene-level features with RoI features. Specifically, we utilize multi-head cross-attention mechanism (Vaswani et al., 2017) (denoted as MHCA) to attend a group of trainable\nqueries eq with RoI features {hpcl}NRoI : hpclq = MHCA(q = eq,kv = {hpcl + fPE(r)}NRoI). (6)\nfPE is used to transform 3D bbox coordinates into positional embedding to enrich spatial information. And q,k and v denote query, key, and value in attention. As in image encoder (Section 3.2.1), a trainable linear layer is also applied for final 3D features zpcl:\nzpcl = f proj \u2032 (hpclq ) (7)\n3D Instruction Data. We construct a 3D instruction tuning dataset called Scan2Inst using ScanNet (Dai et al., 2017) as our 3D instruction tuning dataset due to its diverse tasks and annotated categories. Following Wang et al. (2022b), we use GPT-API to generate a total of 80k data pairs comprising instructions and responses based on original dataset."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENT SETUP",
            "text": "To explore the effectiveness of our framework in multimodal learning, we fine-tune Octavius in three modality setups: i.) image modality, ii.) point cloud modality and iii.) both image and point cloud modalities. We then evaluate the zero-shot performance using these three fine-tuned models on various downstream tasks. More details about architecture and training scheme are provided in Appendix.\nInstruction Datasets. For image modality, we follow Yin et al. (2023) to construct \u201cLAMM v2\u201d, an instruction dataset consisting of MS-COCO (Lin et al., 2014) and Bamboo (Zhang et al., 2022b), which includes object detection, classification, captioning, and other common 2D tasks. For point cloud modality, we utilize ScanNet (Dai et al., 2017) to generate an instruction dataset called \u201cScan2Inst\u201d which contains VQA, captioning, and classification tasks. In the multimodal learning (2D&3D) setup, we merge the image and point cloud instruction dataset to fine-tune the entire framework simultaneously.\nQuantitative Zero-shot Evaluation. We perform zero-shot evaluation on various downstream tasks for both image and point cloud modalities. For image modality, we perform Visual Question Answering (VQA) on ScienceQA (Lu et al., 2022), classification on CIFAR-10 (Krizhevsky et al., 2009), captioning on Flickr30K (Young et al., 2014) and facial attribute recognition on CelebA (Liu et al., 2018). Note that we evaluate performance in VQA tasks through multiple-choice selection. For point cloud modality, we perform classification on ShapeNet (Chang et al., 2015) (55 classes) and captioning on NR3D (Achlioptas et al., 2020). We also evaluate the performance of classification, captioning, and QA on the test split of ScanNet to verify the proposed Object-As-Scene encoder."
        },
        {
            "heading": "4.2 QUANTITATIVE RESULTS.",
            "text": "All results are provided in Table 1, 2, 3 for different modality setups. Severe interference can be found in the experimental results, especially between the localizing tasks and semantic understanding tasks like VQA and captioning. After equipping with LoRA-MoE, we can observe a remarkable\nimprovement of approximately 20% in all setups, demonstrating the effectiveness of our design in resolving the tug-of-war issue. Additionally, we compare our proposed point cloud encoder ObjectAs-Scene with a recent work, 3D-LLM (Hong et al., 2023) in Table 2. We achieve a comparable performance on Scan2Cap, and outperform 3D-LLM on ScanQA by a significant margin, suggesting a better scene-level understanding capability of Object-As-Scene. Besides, as shown in Table 3, as the complexity of interference among tasks increases, especially when tasks of different modalities are introduced, we can observe a huge performance drop when training the model with different modalities like image and point cloud simultaneously compared to separate training. Our proposed MoE-based decoder partially alleviates the degradation and even achieves comparable performance with separate training in some tasks."
        },
        {
            "heading": "4.3 ABLATION AND ANALYSIS",
            "text": "LoRA-MoE. The results are shown in Table 4. We first ablate on MoE architecture by individually employing dedicated LoRAs for each tasks (denoted as \u201cIndividual\u201d in the table). While the individual gate exhibits performance merits in specific tasks, its primary challenge is the difficulty in assigning suitable experts for tasks that are not encountered in the instruction dataset, thereby compromising the model\u2019s generalizability. Another observation is the superior efficacy of the sparse gate relative to the dense gate (denoted as \u201cWeighted Sum\u201d in the table), which is intuitive when considering that the dense gate can essentially be regarded as a singluar LoRA with additional parameters. Furthermore, we compare the performance of the sparse gate against the baseline model (single LoRA) under conditions of parameter-consistency during inference (i.e., sparse top-2 gate only uses half of rank in LoRA compared with baseline model). The enhanced performance of sparse gate demonstrate that MoE transcends a mere aggregation of parameters.\nGate Routing in MoE and Load Balancing. As shown in Figure 6, there is a huge discrepancy in expert selection between detection and VQA tasks, which demonstrates the tug-of-war phenomenon, and explains why using a single LoRA yields poor performance on both tasks simultaneously. Additionally, it is found that the routing weights of experts assigned by gate network tend to concentrate on a subset of specific experts. In particular, in a 4-expert model, despite the superior performance compared to a 3-expert model, the final converged model ends up utilizing only 3 of 4 available experts. To further explore this imbalance issue, we conduct several experiments with load balancing loss (Equation 2) in Table 5. As a result, no improvements and better routing results are\nTable 4: Ablation studies on MoE architecture on 2D tasks. \u201cSparse Top-2\u201d gate picks out top-2 ranked experts based on routing scores. \u201cWeighted Sum\u201d gate uses the weighted sum of all experts as output. \u201cIndividual\u201d gate employs different experts for each 2D tasks individually. \u201c#Trainable Param.\u201d denotes the proportion of trainable parameters to total parameters.\nGate Type LoRA-Rank Det. (VOC, IoU=0.5) VQA\n#Trainable Param. Recall Prec. Acc@1\n\u2013 (Baseline) 32 7.61 5.95 40.31 0.4% Sparse Top-2 32 39.04 35.21 46.95 1.6%\nWeighted Sum 32 9.78 5.33 44.71 1.6% Individual 32 28.38 25.64 48.54 2.4%\nSparse Top-2 16 32.81 24.46 39.11 0.8% Sparse Top-2 8 25.44 21.87 37.65 0.4%#0\n#3\nDet.\n#2\n#0\nVQA\n#2\n#3\nCls.\n#2\n#3\nCap.\n#0\n#1\n#2\nDet\nVQA\nCap.\nCls.\nFA Hair\n#2\n#3\nFA Hair\n#2\n#3\nFA Smile\n#0\n#1\n#2\n#3\nobserved, which is intuitive because directly employing load balancing strategies is incompatible and infeasible in an instance-based gate scenario (see Section 3.1.2). We also attempt to replace the instance-based gate with token-based gate used in conventional MoE methods (Shazeer et al., 2017; Lepikhin et al., 2020). However, since only the LoRA modules are trained in our approach, the number of trainable parameters differs from conventional MoE models where all parameters of the entire foundation model are trained, resulting in poor convergence.\nBesides, we also provide more ablations and analysis on point cloud encoder, LoRA-MoE, gate routing and qualitative results in Appendix."
        },
        {
            "heading": "5 CONCLUSION AND LIMITATIONS",
            "text": "In this paper, we propose Octavius, a unified multimodal framework, to effectively address the critical challenge of task interference in complex learning scenarios. By integrating the Mixtureof-Experts (MoE) with LoRA, we present LoRA-MoE decoder, which delivers specialized learning paths for different tasks and modalities. After the validation across multiple modalities and tasks, Octavius alleviates the severe tug-of-war issue and achieves a significant performance boost in both 2D and 3D tasks.\nLimitations. Compared to separate training on a single modality, introducing simultaneously multiple modalities for joint training may result in performance degradation, posing a challenge for future research. The combination of MLLMs and MoE still has great potential in addressing this problem, especially for a more complicated real-world system like embodied AI scenarios that require more modalities as input. Besides, we will further explore the token-based gate with load balancing strategies, especially when the number of downstream tasks increases.\nAcknowledgement. This work is supported in part by National Key Research and Development Program of China (NO. 2021YFB1714300) , and National Natural Science Foundation of China (62132001)."
        }
    ],
    "year": 2024
}