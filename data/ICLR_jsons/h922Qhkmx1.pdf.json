{
    "abstractText": "In this work, we define a diffusion-based generative model capable of both music synthesis and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.",
    "authors": [],
    "id": "SP:1c15b2b0d67ca4366428dee25fbad07b379bccd3",
    "references": [
        {
            "authors": [
                "Andrea Agostinelli",
                "Timo I Denk",
                "Zal\u00e1n Borsos",
                "Jesse Engel",
                "Mauro Verzetti",
                "Antoine Caillon",
                "Qingqing Huang",
                "Aren Jansen",
                "Adam Roberts",
                "Marco Tagliasacchi"
            ],
            "title": "Musiclm: Generating music from text",
            "venue": "arXiv preprint arXiv:2301.11325,",
            "year": 2023
        },
        {
            "authors": [
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Damien Vincent",
                "Eugene Kharitonov",
                "Olivier Pietquin",
                "Matt Sharifi",
                "Olivier Teboul",
                "David Grangier",
                "Marco Tagliasacchi",
                "Neil Zeghidour"
            ],
            "title": "Audiolm: a language modeling approach to audio generation",
            "venue": "arXiv preprint arXiv:2209.03143,",
            "year": 2022
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J Weiss",
                "Mohammad Norouzi",
                "William Chan"
            ],
            "title": "Wavegrad: Estimating gradients for waveform generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kin Wai Cheuk",
                "Ryosuke Sawata",
                "Toshimitsu Uesaka",
                "Naoki Murata",
                "Naoya Takahashi",
                "Shusuke Takahashi",
                "Dorien Herremans",
                "Yuki Mitsufuji"
            ],
            "title": "Diffroll: Diffusion-based generative music transcription with unsupervised pretraining capability",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Woosung Choi",
                "Minseok Kim",
                "Jaehwa Chung",
                "Soonyoung Jung"
            ],
            "title": "Lasaft: Latent source attentive frequency transformation for conditioned source separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez"
            ],
            "title": "Hybrid spectrogram and waveform source separation",
            "venue": "In Proceedings of the ISMIR 2021 Workshop on Music Source Separation,",
            "year": 2021
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez",
                "Nicolas Usunier",
                "L\u00e9on Bottou",
                "Francis Bach"
            ],
            "title": "Music source separation in the waveform domain",
            "venue": "arXiv preprint arXiv:1911.13254,",
            "year": 2019
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Heewoo Jun",
                "Christine Payne",
                "Jong Wook Kim",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Jukebox: A generative model for music",
            "venue": "arXiv preprint arXiv:2005.00341,",
            "year": 2020
        },
        {
            "authors": [
                "Chris Donahue",
                "Julian McAuley",
                "Miller Puckette"
            ],
            "title": "Adversarial audio synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Chris Donahue",
                "Antoine Caillon",
                "Adam Roberts",
                "Ethan Manilow",
                "Philippe Esling",
                "Andrea Agostinelli",
                "Mauro Verzetti",
                "Ian Simon",
                "Olivier Pietquin",
                "Neil Zeghidour"
            ],
            "title": "Singsong: Generating musical accompaniments from singing",
            "venue": "arXiv preprint arXiv:2301.12662,",
            "year": 2023
        },
        {
            "authors": [
                "Seth* Forsgren",
                "Hayk* Martiros"
            ],
            "title": "Riffusion - Stable diffusion for real-time music generation, 2022",
            "venue": "URL https://riffusion.com/about",
            "year": 2022
        },
        {
            "authors": [
                "Enric Gus\u00f3",
                "Jordi Pons",
                "Santiago Pascual",
                "Joan Serr\u00e0"
            ],
            "title": "On loss functions and evaluation metrics for music source separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Curtis Hawthorne",
                "Ian Simon",
                "Adam Roberts",
                "Neil Zeghidour",
                "Josh Gardner",
                "Ethan Manilow",
                "Jesse Engel"
            ],
            "title": "Multi-instrument music synthesis with spectrogram diffusion",
            "venue": "In International Society for Music Information Retrieval Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Qingqing Huang",
                "Aren Jansen",
                "Joonseok Lee",
                "Ravi Ganti",
                "Judith Yue Li",
                "Daniel P.W. Ellis"
            ],
            "title": "Mulan: A joint embedding of music audio and natural language",
            "venue": "In International Society for Music Information Retrieval Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Vivek Jayaram",
                "John Thickstun"
            ],
            "title": "Source separation with deep generative priors",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Vivek Jayaram",
                "John Thickstun"
            ],
            "title": "Parallel and flexible sampling from autoregressive models via langevin dynamics",
            "venue": "In Proc. ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusionbased generative models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yitzhak Katznelson"
            ],
            "title": "An Introduction to Harmonic Analysis",
            "venue": "doi: 10.1017/CBO9781139165372",
            "year": 2004
        },
        {
            "authors": [
                "Ilya Kavalerov",
                "Scott Wisdom",
                "Hakan Erdogan",
                "Brian Patton",
                "Kevin Wilson",
                "Jonathan Le Roux",
                "John R Hershey"
            ],
            "title": "Universal sound separation",
            "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Durk P Kingma",
                "Yann LeCun"
            ],
            "title": "Regularized estimation of image statistics by score matching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2010
        },
        {
            "authors": [
                "Qiuqiang Kong",
                "Yong Xu",
                "Wenwu Wang",
                "Philip J.B. Jackson",
                "Mark D. Plumbley"
            ],
            "title": "Single-channel signal separation and deconvolution with generative adversarial networks",
            "venue": "In Proc. IJCAI,",
            "year": 2411
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Felix Kreuk",
                "Gabriel Synnaeve",
                "Adam Polyak",
                "Uriel Singer",
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Devi Parikh",
                "Yaniv Taigman",
                "Yossi Adi"
            ],
            "title": "Audiogen: Textually guided audio generation",
            "venue": "arXiv preprint arXiv:2209.15352,",
            "year": 2022
        },
        {
            "authors": [
                "Junhyeok Lee",
                "Seungu Han"
            ],
            "title": "NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling",
            "venue": "In Proc. Interspeech",
            "year": 2021
        },
        {
            "authors": [
                "Liwei Lin",
                "Qiuqiang Kong",
                "Junyan Jiang",
                "Gus Xia"
            ],
            "title": "A unified model for zero-shot music source separation, transcription and synthesis, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Haohe Liu",
                "Zehua Chen",
                "Yi Yuan",
                "Xinhao Mei",
                "Xubo Liu",
                "Danilo Mandic",
                "Wenwu Wang",
                "Mark D Plumbley"
            ],
            "title": "Audioldm: Text-to-audio generation with latent diffusion models",
            "venue": "arXiv preprint arXiv:2301.12503,",
            "year": 2023
        },
        {
            "authors": [
                "Francesc Llu\u00eds",
                "Jordi Pons",
                "Xavier Serra"
            ],
            "title": "End-to-end music source separation: Is it possible in the waveform domain",
            "venue": "In INTERSPEECH,",
            "year": 2019
        },
        {
            "authors": [
                "Yen-Ju Lu",
                "Yu Tsao",
                "Shinji Watanabe"
            ],
            "title": "A study on speech enhancement based on diffusion probabilistic model",
            "venue": "In 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),",
            "year": 2021
        },
        {
            "authors": [
                "Yi Luo",
                "Nima Mesgarani"
            ],
            "title": "Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation",
            "venue": "IEEE/ACM transactions on audio, speech, and language processing,",
            "year": 2019
        },
        {
            "authors": [
                "Shahar Lutati",
                "Eliya Nachmani",
                "Lior Wolf"
            ],
            "title": "Separate and diffuse: Using a pretrained diffusion model for improving source separation",
            "venue": "arXiv preprint arXiv:2301.10752,",
            "year": 2023
        },
        {
            "authors": [
                "Ilaria Manco",
                "Emmanouil Benetos",
                "Elio Quinton",
                "Gy\u00f6rgy Fazekas"
            ],
            "title": "Learning music audio representations via weak language supervision",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Manilow",
                "Gordon Wichern",
                "Prem Seetharaman",
                "Jonathan Le Roux"
            ],
            "title": "Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity",
            "venue": "In Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
            "year": 2019
        },
        {
            "authors": [
                "Ethan Manilow",
                "Curtis Hawthorne",
                "Cheng-Zhi Anna Huang",
                "Bryan Pardo",
                "Jesse Engel"
            ],
            "title": "Improving source separation by explicitly modeling dependencies between sources",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Gautam Mittal",
                "Jesse Engel",
                "Curtis Hawthorne",
                "Ian Simon"
            ],
            "title": "Symbolic music generation with diffusion models",
            "venue": "In Proceedings of the 22nd International Society for Music Information Retrieval Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Vivek Narayanaswamy",
                "Jayaraman J. Thiagarajan",
                "Rushil Anirudh",
                "Andreas Spanias"
            ],
            "title": "Unsupervised audio source separation using generative priors",
            "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH,",
            "year": 2020
        },
        {
            "authors": [
                "Santiago Pascual",
                "Gautam Bhattacharya",
                "Chunghsin Yeh",
                "Jordi Pons",
                "Joan Serr\u00e0"
            ],
            "title": "Full-band general audio synthesis with score-based diffusion",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2023
        },
        {
            "authors": [
                "Gen\u00eds Plaja-Roglans",
                "Miron Marius",
                "Xavier Serra"
            ],
            "title": "A diffusion-inspired training strategy for singing voice extraction in the waveform domain",
            "venue": "In Proc. of the 23rd Int. Society for Music Information Retrieval,",
            "year": 2022
        },
        {
            "authors": [
                "Emilian Postolache",
                "Giorgio Mariani",
                "Michele Mancusi",
                "Andrea Santilli",
                "Luca Cosmo",
                "Emanuele Rodol\u00e0"
            ],
            "title": "Latent autoregressive source separation",
            "venue": "In Proc. AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Emilian Postolache",
                "Jordi Pons",
                "Santiago Pascual",
                "Joan Serr\u00e0"
            ],
            "title": "Adversarial permutation invariant training for universal sound separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2023
        },
        {
            "authors": [
                "Zafar Rafii",
                "Antoine Liutkus",
                "Fabian-Robert St\u00f6ter",
                "Stylianos Ioannis Mimilakis",
                "Rachel Bittner"
            ],
            "title": "The MUSDB18 corpus for music separation, December 2017",
            "venue": "URL https://doi.org/10. 5281/zenodo.1117372",
            "year": 2017
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Simon Rouard",
                "Ga\u00ebtan Hadjeres"
            ],
            "title": "CRASH: raw audio score-based generative modeling for controllable high-resolution drum sound synthesis",
            "venue": "In Proceedings of the 22nd International Society for Music Information Retrieval Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Le Roux",
                "Scott Wisdom",
                "Hakan Erdogan",
                "John R. Hershey"
            ],
            "title": "Sdr \u2013 half-baked or well done",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Koichi Saito",
                "Naoki Murata",
                "Toshimitsu Uesaka",
                "Chieh-Hsin Lai",
                "Yuhta Takida",
                "Takao Fukui",
                "Yuki Mitsufuji"
            ],
            "title": "Unsupervised vocal dereverberation with diffusion-based generative models",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2023
        },
        {
            "authors": [
                "Ryosuke Sawata",
                "Naoki Murata",
                "Yuhta Takida",
                "Toshimitsu Uesaka",
                "Takashi Shibuya",
                "Shusuke Takahashi",
                "Yuki Mitsufuji"
            ],
            "title": "A versatile diffusion-based generative refiner for speech enhancement",
            "venue": "arXiv preprint arXiv:2210.17287,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Scheibler",
                "Youna Ji",
                "Soo-Whan Chung",
                "Jaeuk Byun",
                "Soyeon Choe",
                "Min-Seok Choi"
            ],
            "title": "Diffusion-based generative speech source separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2023
        },
        {
            "authors": [
                "Flavio Schneider",
                "Zhijing Jin",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Mo\u00fbsai: Text-to-music generation with long-context latent diffusion",
            "venue": "arXiv preprint arXiv:2301.11757,",
            "year": 2023
        },
        {
            "authors": [
                "Joan Serr\u00e0",
                "Santiago Pascual",
                "Jordi Pons",
                "R Oguz Araz",
                "Davide Scaini"
            ],
            "title": "Universal speech enhancement with score-based diffusion",
            "venue": "arXiv preprint arXiv:2206.03065,",
            "year": 2022
        },
        {
            "authors": [
                "Jung-Eun Shin",
                "Adam J Riesselman",
                "Aaron W Kollasch",
                "Conor McMahon",
                "Elana Simon",
                "Chris Sander",
                "Aashish Manglik",
                "Andrew C Kruse",
                "Debora S Marks"
            ],
            "title": "Protein design and variant prediction using autoregressive generative models",
            "venue": "Nature communications,",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric A. Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "Proceedings ICML 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Y Cem Subakan",
                "Paris Smaragdis"
            ],
            "title": "Generative adversarial source separation",
            "venue": "In Proc. ICASSP,",
            "year": 2018
        },
        {
            "authors": [
                "Naoya Takahashi",
                "Nabarun Goswami",
                "Yuki Mitsufuji"
            ],
            "title": "Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation",
            "venue": "In Proc. IWAENC,",
            "year": 2018
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ],
            "title": "WaveNet: A Generative Model for Raw Audio",
            "venue": "In Proc. 9th ISCA Workshop on Speech Synthesis Workshop (SSW 9),",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural Computation,",
            "year": 2011
        },
        {
            "authors": [
                "Scott Wisdom",
                "Efthymios Tzinis",
                "Hakan Erdogan",
                "Ron Weiss",
                "Kevin Wilson",
                "John Hershey"
            ],
            "title": "Unsupervised sound separation using mixture invariant training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Dongchao Yang",
                "Jianwei Yu",
                "Helin Wang",
                "Wen Wang",
                "Chao Weng",
                "Yuexian Zou",
                "Dong Yu"
            ],
            "title": "Diffsound: Discrete diffusion model for text-to-sound generation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yun Yu",
                "Sung-Lin Yeh",
                "Gy\u00f6rgy Fazekas",
                "Hao Tang"
            ],
            "title": "Conditioning and sampling in variational diffusion models for speech super-resolution",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2023
        },
        {
            "authors": [
                "Neil Zeghidour",
                "Alejandro Luebs",
                "Ahmed Omran",
                "Jan Skoglund",
                "Marco Tagliasacchi"
            ],
            "title": "Soundstream: An end-to-end neural audio codec",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Ge Zhu",
                "Jordan Darefsky",
                "Fei Jiang",
                "Anton Selitskiy",
                "Zhiyao Duan"
            ],
            "title": "Music source separation with generative flow",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Song"
            ],
            "title": "By the Markov property of the forward diffusion process, y(t) is conditionally independent from x(0) given x(t) and we drop again the conditioning on y(0) from the first two terms, following Eq",
            "year": 2021
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "2022), we adopt a non-linear schedule for time discretization that gives more importance to lower noise levels",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Generative models have recently gained a lot of attention thanks to their successful application in many fields, such as NLP (OpenAI, 2023; Touvron et al., 2023), image synthesis (Ramesh et al., 2022; Rombach et al., 2022) or protein design (Shin et al., 2021). The audio domain is no exception to this trend (Agostinelli et al., 2023; Liu et al., 2023).\nA peculiarity of the audio domain is that an audio sample y can be seen as the sum of multiple individual source {x1, . . . ,xN}, resulting in a mixture y = \u2211N n=1 xn. Unlike in other sub-fields of the audio domain (e.g., speech), sources present in musical mixtures (stems) share a context given their strong interdependence. For example, the bass line of a song follows the drum\u2019s rhythm and harmonizes with the melody of the guitar. Mathematically, this fact can be expressed by saying that the joint distribution of the sources p(x1, . . . ,xN ) does not factorize into the product of individual source distributions {pn(xn)}n=1,...,N . Knowing the joint p(x1, . . . ,xN ) implies knowing the distribution over the mixtures p(y) since the latter can be obtained through the sum. The converse is more difficult mathematically, being an inverse problem.\nNevertheless, humans have developed the ability to process multiple sound sources simultaneously in terms of synthesis (i.e., musical composition or generation) and analysis (i.e., source separation). More specifically, composers can invent multiple sources {x1, . . . ,xN} that sum to a consistent mixture y and, extract information about the individual sources {x1, . . . ,xN} from a mixture y. This ability to compose and decompose sound is crucial for a generative music model. A model designed to assist in music composition should be capable of isolating individual sources within a mixture and allow for independent operation on each source. Such a capability would give the composer maximum control over what to modify and retain in a composition. Therefore, we argue that the task of compositional music generation is highly connected to the task of music source separation.\nTo the best of our knowledge, no model in deep learning literature can perform both tasks simultaneously. Models designed for the generation task directly learn the distribution p(y) over mixtures, collapsing the information needed for the separation task. In this case, we have accurate mixture modeling but no information about the individual sources. It is worth noting that approaches that model the distribution of mixtures conditioning on textual data (Schneider et al., 2023; Agostinelli et al., 2023) face the same limitations. Conversely, models for source separation (D\u00e9fossez et al., 2019) either target p(x1, . . . ,xN | y), conditioning on the mixture, or learn a single model pn(xn)\nfor each source distribution (in a weakly-supervised manner) and condition on the mixture during inference (Jayaram & Thickstun, 2020; Postolache et al., 2023a). In both cases, generating mixtures is impossible. In the first case, the model inputs a mixture, which hinders the possibility of unconditional modeling, not having direct access to p(x1, . . . ,xN ) (or equivalently to p(y)). In the second case, while we can accurately model each source independently, all essential information about their interdependence is lost, preventing the possibility of generating coherent mixtures.\nContribution. Our contribution is three-fold. (i) First, we bridge the gap between source separation and music generation by learning p(x1, . . . ,xN ), the joint (prior) distribution of contextual sources (i.e., those belonging to the same song). For this purpose, we use the denoising score-matching framework to train a Multi-Source Diffusion Model (MSDM). We can perform both source separation and music generation during inference by training this single model. Specifically, generation is achieved by sampling from the prior, while separation is carried out by conditioning the prior on the mixture and then sampling from the resulting posterior distribution. (ii) This new formulation opens the doors to novel tasks in the generative domain, such as source imputation, where we create accompaniments by generating a subset of the sources given the others (e.g., play a piano track that goes well with the drums). (iii) Lastly, to obtain competitive results on source separation with respect to state-of-the-art discriminative models (Manilow et al., 2022) on the Slakh2100 (Manilow et al., 2019) dataset, we propose a new procedure for computing the posterior score based on Dirac delta functions, exploiting the functional relationship between the sources and the mixture."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 GENERATIVE MODELS FOR AUDIO",
            "text": "Deep generative models for audio, learn, directly or implicitly, the distribution of mixtures, represented in our notation by p(y), possibly conditioning on additional data such as text. Various general-purpose generative models, such as autoregressive models, GANs (Donahue et al., 2019), and diffusion models, have been adapted for use in the audio field.\nAutoregressive models have a well-established presence in audio modeling (van den Oord et al., 2016). Jukebox (Dhariwal et al., 2020) proposed to model musical tracks with Scalable Transformers (Vaswani et al., 2017) on hierarchical discrete representations obtained through VQ-VAEs (van den Oord et al., 2017). Furthermore, using a lyrics conditioner, this method generated tracks with\nvocals following the text. However, while Jukebox could model longer sequences in latent space, the audio output suffered from quantization artifacts. By incorporating residual quantization (Zeghidour et al., 2021), newer latent autoregressive models (Borsos et al., 2022; Kreuk et al., 2022) can handle extended contexts and output more coherent and naturally sounding generations. State-of-the-art latent autoregressive models for music, such as MusicLM (Agostinelli et al., 2023), can guide generation by conditioning on textual embeddings obtained via large-scale contrastive pre-training (Manco et al., 2022; Huang et al., 2022). MusicLM can also input a melody and condition on text for style transfer. A concurrent work, SingSong (Donahue et al., 2023), introduces vocal-to-mixture accompaniment generation. Our accompaniment generation procedure differs from the latter since we perform generation at the stem level in a composable way, while the former outputs a single accompaniment mixture.\nDiffWave (Kong et al., 2021) and WaveGrad (Chen et al., 2021) were the first diffusion (score) based generative models in audio, tackling speech synthesis. Many subsequent models followed these preliminary works, mainly conditioned to solve particular tasks such as speech enhancement (Lu et al., 2021; Serr\u00e0 et al., 2022; Sawata et al., 2022; Saito et al., 2023), audio upsampling (Lee & Han, 2021; Yu et al., 2023), MIDI-to-waveform (Mittal et al., 2021; Hawthorne et al., 2022), or spectrogram-to-MIDI generation (Cheuk et al., 2023). The first work in source-specific generation with diffusion models is CRASH (Rouard & Hadjeres, 2021). (Yang et al., 2023; Pascual et al., 2023; Liu et al., 2023) proposed text-conditioned diffusion models to generate general sounds, not focusing on restricted classes such as speech or music. Closer to our work, diffusion models targeting the musical domain are Riffusion (Forsgren & Martiros, 2022) and Mo\u00fbsai (Schneider et al., 2023). Riffusion fine-tunes Stable Diffusion (Rombach et al., 2022), a large pre-trained text-conditioned vision diffusion model, over STFT magnitude spectrograms. Mo\u00fbsai performs generation in a latent domain, resulting in context lengths that surpass the minute. Our score network follows the design of the U-Net proposed in Mo\u00fbsai, albeit using the waveform data representation."
        },
        {
            "heading": "2.2 AUDIO SOURCE SEPARATION",
            "text": "Existing audio source separation models can be broadly classified into discriminative and generative. Discriminative source separators are deterministic parametric models that input the mixtures and systematically extract one or all sources, maximizing the likelihood of some underlying conditional distribution p(x1, . . . ,xN | y). These models are typically trained with a regression loss (Gus\u00f3 et al., 2022) on the estimated signal represented as waveform (Llu\u00eds et al., 2019; Luo & Mesgarani, 2019; D\u00e9fossez et al., 2019), STFT (Takahashi et al., 2018; Choi et al., 2021), or both (D\u00e9fossez, 2021). On the other hand, generative source separation models learn a prior model for each source, thus targeting the distributions {pn(xn)}n=1,...,N . The mixture is observed only during inference, where a likelihood function connects it to its constituent sources. The literature has explored different priors, such as GANs (Subakan & Smaragdis, 2018; Kong et al., 2019; Narayanaswamy et al., 2020), normalizing flows (Jayaram & Thickstun, 2020; Zhu et al., 2022), and autoregressive models (Jayaram & Thickstun, 2021; Postolache et al., 2023a). Some separation approaches also enable synthesis from MIDI (Lin et al., 2021).\nThe separation method closer to ours is the NCSN-BASIS algorithm (Jayaram & Thickstun, 2020). This method was proposed for source separation in the image domain, performing Langevin Dynamics for separating the mixtures with an NCSN score-based model. It employs a Gaussian likelihood function during inference, which, as we demonstrate experimentally, is sub-optimal compared to our novel Dirac-based likelihood function. The main difference between our method with respect to other generative source separation methods (including NCSN-BASIS) is the modeling of the full joint distribution. As such, we can perform source separation and generate mixtures or subsets of stems with a single model.\nContextual information between sources is explicitly modeled in (Manilow et al., 2022) and (Postolache et al., 2023b). The first work models the relationship between sources by training an orderless NADE estimator, which predicts a subset of the sources while conditioning on the input mixture and the remaining sources. The subsequent study achieves universal source separation (Kavalerov et al., 2019; Wisdom et al., 2020) through adversarial training, utilizing a context-based discriminator to model the relationship between sources. Both methods are discriminative, as they are conditioned on the mixtures architecturally. The same architectural limitation is present in discriminative approaches for source separation that use diffusion-based (Scheibler et al., 2023; Lutati et al., 2023) or\ndiffusion-inspired (Plaja-Roglans et al., 2022) methods. Our method sets itself apart as it proposes a model not constrained architecturally by a mixture conditioner, so we can also perform unconditional generation."
        },
        {
            "heading": "3 BACKGROUND",
            "text": "The foundation of our model lies in estimating the joint distribution of the sources p(x1, . . . ,xN ). Our approach is generative because we model an unconditional distribution (the prior). The different tasks are then solved at inference time, exploiting the prior.\nWe employ a diffusion-based (Sohl-Dickstein et al., 2015; Ho et al., 2020) generative model trained via denoising score-matching (Song & Ermon, 2019) to learn the prior. Specifically, we present our formalism by utilizing the notation and assumptions established in (Karras et al., 2022). The central idea of score-matching (Hyv\u00e4rinen, 2005; Kingma & LeCun, 2010; Vincent, 2011) is to approximate the \u201cscore\u201d function of the target distribution p(x), namely \u2207x log p(x), rather than the distribution itself. To effectively approximate the score in sparse data regions, denoising diffusion methods introduce controlled noise to the data and learn to remove it. Formally, the data distribution is perturbed with a Gaussian perturbation kernel:\np(x(t) | x(0)) = N (x(t);x(0), \u03c32(t)I) , (1)\nwhere the parameter \u03c3(t) regulates the degree of noise added to the data. Following the authors in (Karras et al., 2022), we consider an optimal schedule given by \u03c3(t) = t. With that choice of \u03c3(t), the forward evolution of a data point x(t) in time is described by a probability flow ODE (Song et al., 2021): dx(t) = \u2212\u03c3(t)\u2207x(t) log p(x(t)) dt . (2) For t = T >> 0, a data point x(T ) is approximately distributed according to a Gaussian distribution N (x(t);0, \u03c32(T )I), from which sampling is straightforward. Eq. (2) can be inverted in time, resulting in the following backward ODE that describes the denoising process:\ndx(t) = \u03c3(t)\u2207x(t) log p(x(t)) dt . (3)\nSampling can be performed from the data distribution integrating Eq. (3) with a standard ODE solver, starting from an initial (noisy) sample drawn from N (x(t);0, \u03c32(T )I). The score function, represented by a neural network S\u03b8(x(t), \u03c3(t)), is approximated by minimizing the following scorematching loss:\nEt\u223cU([0,T ])Ex(0)\u223cp(x(0))Ex(t)\u223cp(x(t)|x(0)) \u2225\u2225S\u03b8(x(t), \u03c3(t))\u2212\u2207x(t) log p (x(t) | x(0))\u2225\u222522 .\nBy expanding p(x(t) | x(0)) with Eq. (1), the score-matching loss simplifies to: Et\u223cU([0,T ])Ex(0)\u223cp(x(0))E\u03f5\u223cN (0,\u03c32(t)I) \u2225\u2225D\u03b8(x(0) + \u03f5, \u03c3(t))\u2212 x(0)\u2225\u22252\n2 ,\nwhere we define S\u03b8(x(t), \u03c3(t)) =: (D\u03b8(x(t), \u03c3(t))\u2212 x(t))/\u03c32(t)."
        },
        {
            "heading": "4 METHOD",
            "text": ""
        },
        {
            "heading": "4.1 MULTI-SOURCE AUDIO DIFFUSION MODELS",
            "text": "In our setup, we have N distinct source waveforms {x1, . . . ,xN} with xn \u2208 RD for each n. The sources coherently sum to a mixture y = \u2211N n=1 xn. We sometimes use the aggregated form x = (x1, . . . ,xN ) \u2208 RN\u00d7D. In this setting, multiple tasks can be performed: one may generate a consistent mixture y or separate the individual sources x from a given mixture y. We refer to the first task as generation and the second as source separation. A subset of sources can also be fixed in the generation task, and the others can be generated consistently. We call this task partial generation or source imputation. Our key contribution is the ability to perform all these tasks simultaneously by training a single multi-source diffusion model (MSDM), capturing the prior p(x1, . . . ,xN ). The model, illustrated in Figure 1, approximates the noisy score function:\n\u2207x(t) log p(x(t)) = \u2207(x1(t),...,xN (t)) log p(x1(t), . . . ,xN (t)) ,\nwith a neural network: S\u03b8(x(t), \u03c3(t)) : RN\u00d7D \u00d7 R \u2192 RN\u00d7D , (4)\nwhere x(t) = (x1(t), . . . ,xN (t)) denotes the sources perturbed with the Gaussian kernel in Eq. (1). We describe the three tasks (illustrated in Figure 2) using the prior distribution:\n\u2022 Total Generation. This task requires generating a plausible mixture y. It can be achieved by sampling the sources {x1, ...,xN} from the prior distribution and summing them to obtain the mixture y.\n\u2022 Partial Generation. Given a subset of sources, this task requires generating a plausible accompaniment. We define the subset of fixed sources as xI and generate the remaining sources xI by sampling from the conditional distribution p(xI | xI).\n\u2022 Source Separation. Given a mixture y, this task requires isolating the individual sources that compose it. It can be achieved by sampling from the posterior distribution p(x | y)."
        },
        {
            "heading": "4.2 INFERENCE",
            "text": "The three tasks of our method are solved during inference by discretizing the backward Eq. (3). Although different tasks require distinct score functions, they all originate directly from the prior score function in Eq. (4). We analyze each of these score functions in detail. For more details on the discretization method, refer to Section C.3."
        },
        {
            "heading": "4.2.1 TOTAL GENERATION",
            "text": "The total generation task is performed by sampling from Eq. (3) using the score function in Eq. (4). The mixture is then obtained by summing over all the generated sources."
        },
        {
            "heading": "4.2.2 PARTIAL GENERATION",
            "text": "In the partial generation task, we fix a subset of source indices I \u2282 {1, . . . , N} and the relative sources xI := {xn}n\u2208I . The goal is to generate the remaining sources xI := {xn}n\u2208I consistently, where I = {1, . . . , N} \u2212 I. To do so, we estimate the gradient of the conditional distribution:\n\u2207xI(t) log p(xI(t) | xI(t)). (5)\nAlgorithm 1 \u2018MSDM Dirac\u2019 sampler for source separation. Require: I number of discretization steps for the ODE, R number of corrector steps, {\u03c3i}i\u2208{0,...,I} noise\nschedule, Schurn 1: Initialize x\u0302 \u223c N (0, \u03c32II) 2: \u03b1\u2190 min(Schurn/I, \u221a 2\u2212 1) 3: for i\u2190 I to 1 do 4: for r \u2190 R to 0 do 5: \u03c3\u0302 \u2190 \u03c3i \u00b7 (\u03b1+ 1) 6: \u03f5 \u223c N (0, I) 7: x\u0302\u2190 x\u0302+ \u221a \u03c3\u03022 \u2212 \u03c32i \u03f5\n8: z\u2190 [x\u03021:N\u22121,y \u2212 \u2211N\u22121\nn=1 x\u0302n] 9: for n\u2190 1 to N \u2212 1 do\n10: gn \u2190 S\u03b8n(z, \u03c3\u0302)\u2212 S\u03b8N (z, \u03c3\u0302) 11: end for 12: g\u2190 [g1, . . . ,gN\u22121] 13: x\u03021:N\u22121 \u2190 x\u03021:N\u22121 + (\u03c3i\u22121 \u2212 \u03c3\u0302)g 14: x\u0302\u2190 [x\u03021:N\u22121,y \u2212 \u2211N\u22121 n=1 x\u0302n] 15: if r > 0 then 16: \u03f5 \u223c N (0, I) 17: x\u0302\u2190 x\u0302+ \u221a \u03c32i \u2212 \u03c32i\u22121\u03f5 18: end if 19: end for 20: end for 21: return x\u0302\nThis falls into the setting of imputation or, as it is more widely known in the image domain, inpainting. We approach imputation using the method in (Song et al., 2021). The gradient in Eq. (5) is approximated as follows:\n\u2207xI(t) log p([xI(t), x\u0302I(t)]) ,\nwhere x\u0302I is a sample from the forward process: x\u0302I(t) \u223c N (xI(t);xI(0), \u03c3(t)2I). The square bracket operator denotes concatenation. Approximating the score function, we write:\n\u2207xI(t) log p(xI(t) | xI(t)) \u2248 S \u03b8 I([xI(t), x\u0302I(t)], \u03c3(t)) ,\nwhere S\u03b8I denotes the entries of the score network corresponding to the sources indexed by I."
        },
        {
            "heading": "4.2.3 SOURCE SEPARATION",
            "text": "We view source separation as a specific instance of conditional generation, where we condition the generation process on the given mixture y = y(0). This requires computing the score function of the posterior distribution: \u2207x(t) log p(x(t) | y(0)) . (6) Standard methods for implementing conditional generation for diffusion models involve directly estimating the posterior score in Eq. (6) at training time (i.e., Classifier Free Guidance, as described in (Ho & Salimans, 2021)) or estimating the likelihood function p(y(0) | x(t)) and using the Bayes formula to derive the posterior. The second approach typically involves training a separate model, often a classifier, for the score of the likelihood function as in Classifier Guided conditioning, outlined in (Dhariwal & Nichol, 2021).\nIn diffusion-based generative source separation, learning a likelihood model is typically unnecessary because the relationship between x(t) and y(t) is represented by a simple function, namely the sum. A natural approach is to model the likelihood function based on such functional dependency. This is the approach taken by (Jayaram & Thickstun, 2020), where they use a Gaussian likelihood function:\np(y(t) | x(t)) = N (y(t) | N\u2211\nn=1\nxn(t), \u03b3 2(t)I), (7)\nwith the standard deviation given by a hyperparameter \u03b3(t). The authors argue that aligning the \u03b3(t) value to be proportionate to \u03c3(t) optimizes the outcomes of their NCSN-BASIS separator.\nWe present a novel approximation of the posterior score function in Eq. (6) by modeling p(y(t) | x(t)) as a Dirac delta function centered in \u2211N n=1 xn(t): p(y(t) | x(t)) = 1y(t)=\u2211Nn=1 xn(t) . (8) The complete derivation can be found in Appendix A, and we present only the final formulation, which we call \u2018MSDM Dirac\u2019. The method constrains a source, without loss of generality xN , by setting xN (t) = y(0)\u2212 \u2211N\u22121 n=1 xn(t) and estimates:\n\u2207xm(t) log p(x(t) | y(0)) \u2248 S \u03b8 m((x1(t), . . . ,xN\u22121(t),y(0)\u2212 N\u22121\u2211 n=1 xn(t)), \u03c3(t))\n\u2212 S\u03b8N ((x1(t), . . . ,xN\u22121(t),y(0)\u2212 N\u22121\u2211 n=1 xn(t)), \u03c3(t)) ,\nwhere 1 \u2264 m \u2264 N \u2212 1 and S\u03b8m, S\u03b8N denote the entries of the score network corresponding to the m-th and N -th sources. Our approach models the limiting case wherein \u03b3(t) \u2192 0 in the Gaussian likelihood function. This represents a scenario where the dependence between x(t) and y(t) becomes increasingly tight, sharpening the conditioning on the given mixture during the generation process.\nThe separation procedure can be additionally employed in the weakly-supervised source separation scenario, typically encountered in generative source separation (Jayaram & Thickstun, 2020; Zhu et al., 2022; Postolache et al., 2023a). This scenario pertains to cases where we know that specific audio data belongs to a particular instrument class, but we do not have access to sets of sources that share a context. To adapt to this scenario, we assume independence between sources p(x1, . . . ,xN ) = \u220fN n=1 pn(xn) and train a separate model for each source class. We call the resulting model \u2018Independent Source Diffusion Model with Dirac Likelihood\u2019 or \u2018ISDM Dirac\u2019. We derive its formula and formulas for the Gaussian likelihood versions \u2018MSDM Gaussian\u2019 and \u2018ISDM Gaussian\u2019 in Appendix B. While the ISDM method lacks generative capabilities, it enables us to demonstrate the effectiveness of generative source separation when combined with Dirac likelihood."
        },
        {
            "heading": "4.2.4 THE SAMPLER",
            "text": "Our approach utilizes a first-order ODE integrator, specifically the Euler method, and incorporates stochasticity via the Schurn mechanism as discussed in (Karras et al., 2022). Additionally, we apply a correction step as described in (Song et al., 2021; Jayaram & Thickstun, 2020). This correction procedure entails injecting additional noise and then re-denoising at each denoising step i employing the score network fixed at \u03c3i. This process is repeated R times for each denoising step i. The pseudocode for the \u2019MSDM Dirac\u2019 source separation sampler is outlined in Algorithm 1. Additional details are in Appendix C.3."
        },
        {
            "heading": "5 EXPERIMENTAL RESULTS",
            "text": "We perform experiments on Slakh2100 (Manilow et al., 2019), a standard dataset for music source separation. We chose Slakh2100 because it has a significantly larger quantity of data (145h) than\nother multi-source waveform datasets like MUSDB (Rafii et al., 2017) (10h). The amount of data plays a decisive role in determining the quality of a generative model, making Slakh2100 a preferable choice. Nevertheless we . More details on the dataset, architetural and training details and the sampler are provided in Appendix C."
        },
        {
            "heading": "5.1 MUSIC GENERATION",
            "text": "The performance of MSDM on the generative tasks is tested through subjective and objective evaluation. Subjective evaluation is carried out through listening tests, whose form format is reported in Appendix E. Concisely, we produced two forms, one used for the results shown in Table 1 and one for Table 3. In the first one subjects are asked to rate, from 1 to 10, the quality and instrument coherence of 30 generated chunks, of which 15 are generated from the mixture model and 15 from MSDM. In the second one, subjects are asked to rate, from 1 to 10, and knowing the fixed instruments, the quality and presence (density) of the generated accompaniment. We also provide music and accompaniment generation examples as supplementary material.\nAs for the objective evaluation of the generative tasks, we generalize the FAD protocol in Donahue et al. (2023) to our total generation task and to partial generation with more than one source. Given Dreal a dataset of ground truth mixtures chunks and I a set indexing conditioning sources (\u2205 for total generation), we build a dataset Dgen whose elements are the sum between conditioning sources (indexed by I) an the respective generated sources. We define the sub-FAD as FAD(Dreal, Dgen). Our method is the first able to generate any combination of partial sources, and as such, we do not have a competitor baseline. We thus report the sub-FAD results of our method as baseline metrics for future research, together with listening test results.\nResults for total and partial generations are reported and discussed in Tables 1 and 3 respectively. Concisely, Table 1 shows that the generative power of MSDM is the same of a model with the same architecture and trained on mixtures of the same dataset. Table 3 shows that the task of partial generation can be performed with non-trivial quality and can used as a baseline for future works on general accompaniment generation."
        },
        {
            "heading": "5.2 SOURCE SEPARATION",
            "text": "In order to evaluate source separation, we use the scale-invariant SDR improvement (SI-SDRi) metric (Roux et al., 2019). The SI-SDR between a ground-truth source xn and an estimate x\u0302n is defined as:\nSI-SDR(xn, x\u0302n) = 10 log10 \u2225\u03b1xn\u22252 + \u03f5\n\u2225\u03b1xn \u2212 x\u0302n\u22252 + \u03f5 ,\nwhere \u03b1 = x \u22a4 n x\u0302n+\u03f5\n\u2225xn\u22252+\u03f5 and \u03f5 = 10 \u22128. The improvement with respect to the mixture baseline is defined\nas SI-SDRi = SI-SDR(xn, x\u0302n)\u2212 SI-SDR(xn,y). On Slakh, we compare our supervised MSDM and weakly-supervised MSDM with the \u2018Demucs\u2019 (D\u00e9fossez et al., 2019) and \u2018Demucs + Gibbs (512 steps)\u2019 regressor baselines from (Manilow et al., 2022), the state-of-the-art for supervised music source separation on Slakh2100, aligning with the evaluation procedure of (Manilow et al., 2022). We evaluate over the test set of Slakh2100, using chunks of 4 seconds in length (with an overlap of two seconds) and filtering out silent chunks and chunks consisting of only one source, given the poor performance of SI-SDRi on such segments. We report results comparing our Dirac score posterior with the Gaussian score posterior of (Jayaram & Thickstun, 2020), using the best parameters of the ablations in Appendix D and 150 inference steps.\nResults are illustrated and discussed in Table 2. Concisely, MSDM proves to be very close to the state of the art. Moreover, the newly defined sampling procedure, when used in the weakly supervised flavor, yields results that are better than the state of the art for some stems."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We have presented a general method, based on denoising score-matching, for source separation, mixture generation, and accompaniment generation in the musical domain. Our approach utilizes a single neural network trained once, with tasks differentiated during inference. Moreover, we have defined a new sampling method for source separation. We quantitatively tested the model on source separation, obtaining results comparable to state-of-the-art regressor models. We qualitatively and quantitaively tested the model on total and partial generation. For the first one we showed the model has the same generative power of the same model trained on mixtures. For the latter, we showed the accompaniment generated are plausible and nontrivial.\nOur model\u2019s ability to handle both total and partial generation and source separation positions it as a significant step toward the development of general audio models. This flexibility paves the way for more advanced music composition tools, where users can easily control and manipulate individual sources within a mixture."
        },
        {
            "heading": "6.1 LIMITATIONS AND FUTURE WORK",
            "text": "The amount of available contextual data constrains the performance of our model. To address this, pre-separating mixtures and training on the separations, as demonstrated in (Donahue et al., 2023), may prove beneficial. Additionally, it would be intriguing to explore the possibility of extending our method to situations where the sub-signals are not related by addition but rather by a known but different function."
        },
        {
            "heading": "A DERIVATION OF MSDM DIRAC POSTERIOR SCORE FOR SOURCE SEPARATION",
            "text": "We prove the main result of Section 4.2.3. We condition the generative model over the mixture y(0) = y. As such, we compute the posterior:\np(x(t) | y(0)) = \u222b y(t) p(x(t),y(t) | y(0))dy(t) = \u222b y(t) p(x(t) | y(t),y(0))p(y(t) | y(0))dy(t) .\nThe first equality is given by marginalizing over y(t) and the second by the chain rule. Following Eq. (50) in Song et al. (2021), we can eliminate the dependency on y(0) from the first term, obtaining the approximation:\np(x(t) | y(0)) \u2248 \u222b y(t) p(x(t) | y(t))p(y(t) | y(0))dy(t) . (9)\nWe compute p(y(t) | y(0)), using the chain rule after marginalizing over x(0) and x(t): p(y(t) | y(0)) = \u222b x(0),x(t) p(y(t),x(t),x(0) | y(0))dx(0)dx(t)\n= \u222b x(0),x(t) p(y(t) | x(t),x(0),y(0))p(x(t) | x(0),y(0))p(x(0) | y(0))dx(0)dx(t) .\nBy the Markov property of the forward diffusion process, y(t) is conditionally independent from x(0) given x(t) and we drop again the conditioning on y(0) from the first two terms, following Eq. (50) in Song et al. (2021). As such, we have:\np(y(t) | y(0)) \u2248 \u222b x(0),x(t) p(x(0) | y(0))p(x(t) | x(0))p(y(t) | x(t))dx(0)dx(t) . (10)\nWe model the likelihood function p(y(t) | x(t)) with the Dirac delta function in Eq. (8). The posterior p(x(0) | y(0)) is obtained via Bayes theorem substituting the likelihood:\np(x(0) | y(0)) = p(x(0))1y(0)=\n\u2211N n=1 xn(0)\np(y(0)) =  p(x(0))p(y(0)) if \u2211N n=1 xn(0) = y(0)\n0 otherwise\nWe substitute it in Eq. (10), together with Eq. (1) and Eq. (8), obtaining:\u222b x(0): \u2211N n=1 x(0)=y(0) p(x(0)) p(y(0)) \u222b x(t)\nN (x(t);x(0), \u03c32(t)I)1y(t)=\u2211Nn=1 xn(t)dx(t)dx(0) . (11) We sum over the first N \u2212 1 sources in the inner integral, setting xN (t) = y(t)\u2212\n\u2211N\u22121 n=1 xn(t):\u222b\nx1:N\u22121(t) N (x1:N\u22121(t),y(t)\u2212 N\u22121\u2211 n=1 xn(t);x(0), \u03c3 2(t)I)dx1:N\u22121(t) (12)\n= \u222b x1:N\u22121(t) N\u22121\u220f n=1 N (xn(t);xn(0), \u03c32(t)I)N (y(t)\u2212 N\u22121\u2211 n=1 xn(t);xN (0), \u03c3 2(t)I)dx1:N\u22121(t)\n= N (y(t); N\u2211\nn=1\nxn(0), N\u03c3 2(t)I) . (13)\nThe second equality is obtained by factorizing the Gaussian, which has diagonal covariance matrix, while the last equality is obtained by iterative application of the convolution theorem Katznelson (2004). We substitute Eq. (13) in Eq. (11), obtaining:\np(y(t) | y(0)) \u2248 \u222b x(0): \u2211N n=1 xn(0)=y(0) p(x(0)) p(y(0)) N (y(t); N\u2211 n=1 xn(0), N\u03c3 2(t)I)dx(0)\n= N (y(t);y(0), N\u03c32(t)I) \u222b x(0): \u2211N n=1 xn(0)=y(0) p(x(0)) p(y(0)) dx(0) = N (y(t);y(0), N\u03c32(t)I) . (14)\nAt this point, we apply Bayes theorem in Eq. (9), substituting the Dirac likelihood: p(x(t) | y(0)) \u2248 \u222b y(t) p(x(t))p(y(t) | x(t)) p(y(t)) p(y(t) | y(0))dy(t) (15)\n= \u222b y(t) p(x(t))1y(t)= \u2211N n=1 xn(t) p(y(t)) p(y(t) | y(0))dy(t) (16)\n= p(x(t)) p( \u2211N n=1 xn(t)) p( N\u2211 n=1 xn(t) | y(0)) . (17)\nEstimating Eq. (17), however, requires knowledge of the mixture density p( \u2211N\nn=1 xn(t)), which we do not acknowledge. As such, we approximate Eq. (16) with Monte Carlo, using the mean of p(y(t) | y(0)), namely y(0) (see Eq. (14)), obtaining:\np(x(t) | y(0)) \u2248 p(x(t))1y(0)=\n\u2211N n=1 xn(t)\np(y(0)) =  p(x(t))p(y(0)) if \u2211N n=1 xn(t) = y(0)\n0 otherwise (18)\nSimilar to how we constrained the integral in Eq. (12), we parameterize the posterior, without loss of generality, using the first N \u2212 1 sources x\u0303(t) = (x1(t), . . . ,xN\u22121(t)). The last source is constrained setting xN (t) = y(0)\u2212 \u2211N\u22121 n=1 xn(t) and the parameterization is defined as:\nF (x\u0303(t)) = F (x1(t), . . . ,xN\u22121(t)) = (x1(t), . . . ,xN\u22121(t),y(0)\u2212 N\u22121\u2211 n=1 xn(t)) . (19)\nPlugging Eq. (19) in Eq. (18) we obtain the parameterized posterior:\np(F (x\u0303(t)) | y(0)) \u2248 p(F (x\u0303(t))) p(y(0))\n(20)\nAt this point, we compute the gradient of the logarithm of Eq. (20) with respect to x\u0303(t):\n\u2207x\u0303(t) log p(F (x\u0303(t)) | y(0)) \u2248 \u2207x\u0303(t) log p(F (x\u0303(t)))\np(y(0))\n= \u2207x\u0303(t) log p(F (x\u0303(t)))\u2212\u2207x\u0303(t) log p(y(0)) = \u2207x\u0303(t) log p(F (x\u0303(t))) . (21)\nUsing the chain-rule for differentiation on Eq. (21) we have: \u2207x\u0303(t) log p(F (x\u0303(t)) | y(0)) \u2248 \u2207F (x\u0303(t)) log p(F (x\u0303(t)))JF (x\u0303(t)), (22)\nwhere JF (x\u0303(t)) \u2208 R(N\u00d7D)\u00d7((N\u22121)\u00d7D) is the Jacobian of F computed in x\u0303(t), equal to:\nJF (x\u0303(t)) =  I 0 . . . 0 0 I . . . 0 ... ... . . . ... 0 0 . . . I\n\u2212I \u2212I . . . \u2212I  The gradient with respect to a source xm(t) with 1 \u2264 m \u2264 N \u2212 1 in Eq. (22) is thus equal to:\n\u2207xm(t) log p(F (x\u0303(t)) | y(0)) \u2248 [\u2207F (x\u0303(t)) log p(F (x\u0303(t))]m \u2212 [\u2207F (x\u0303(t)) log p(F (x\u0303(t))]N ,\nwhere we index the components of the m-th and N -th sources in \u2207F (x\u0303(t)) log p(F (x\u0303(t)). Finally, we replace the gradients with the score networks:\n\u2207xm(t) log p(F (x\u0303(t))|y(0)) \u2248 S \u03b8 m((x1(t), . . . ,xN\u22121(t),y(0)\u2212 N\u22121\u2211 n=1 xn(t)), \u03c3(t))\n\u2212 S\u03b8N ((x1(t), . . . ,xN\u22121(t),y(0)\u2212 N\u22121\u2211 n=1 xn(t)), \u03c3(t)) , (23)\nwhere S\u03b8m and S \u03b8 N are the entries of the score network corresponding to the m-th and N -th sources."
        },
        {
            "heading": "B DERIVATION OF GAUSSIAN AND WEAKLY-SUPERVISED POSTERIOR SCORES FOR SOURCE SEPARATION",
            "text": "In this Section we derive the formulas for \u2018MSDM Gaussian\u2019, \u2018ISDM Dirac\u2019 and \u2018ISDM Gaussian\u2019. We first adapt the Gaussian posterior introduced in Jayaram & Thickstun (2020) to continuous-time score-based diffusion models Karras et al. (2022). We plug the Gaussian likelihood function (Eq. (7)) into Eq. (15), obtaining:\np(x(t) | y(0)) \u2248 \u222b y(t) p(x(t))N (y(t); \u2211N n=1 xn(t), \u03b3 2(t)I) p(y(t)) p(y(t) | y(0))dy(t) (24)\nFollowing Jayaram & Thickstun (2020), y(t) is not re-sampled during inference and is always set to y(0). As such, we perform Monte Carlo in Eq. (24) with y(0), the mean of p(y(t) | y(0) (see Eq. (14)), obtaining:\np(x(t) | y(0)) \u2248 p(x(t))N (y(0);\n\u2211N n=1 xn(t), \u03b3 2(t)I)\np(y(0)) . (25)\nAt this point, we compute the gradient of the logarithm of Eq. (25) with respect to xm(t):\n\u2207xm(t) log p(x(t) | y(0)) \u2248 \u2207xm(t) log p(x(t))N (y(0);\n\u2211N n=1 xn(t), \u03b3 2(t)I)\np(y(0))\n= \u2207xm(t) log p(x(t)) +\u2207xm(t) logN (y(0); N\u2211\nn=1\nxn(t), \u03b3 2(t)I)\n= \u2207xm(t) log p(x(t)) \u2212 1\n2\u03b32(t) \u2207xm(t)\u2225y(0)\u2212 N\u2211 n=1 xn(t)\u222522\n= \u2207xm(t) log p(x(t)) \u2212 1\n\u03b32(t) (y(0)\u2212 N\u2211 n=1 xn(t)) . (26)\nWe obtain the \u2018MSDM Gaussian\u2019 posterior score by replacing the contextual prior with the score network:\n\u2207xm(t) log p(x(t) | y(0)) \u2248 S \u03b8 m((x1(t), . . . ,xN (t)), \u03c3(t))\u2212\n1\n\u03b32(t) (y(0)\u2212 N\u2211 n=1 xn(t)) . (27)\nThe weakly-supervised posterior scores are obtained by approximating:\np(x1(t), . . . ,xN (t)) \u2248 N\u220f\nn=1\npn(xn(t)) ,"
        },
        {
            "heading": "20 15.35 15.08 13.20 15.36 -46.86 12.89 12.21 10.87 9.32 8.32 6.47",
            "text": ""
        },
        {
            "heading": "60 16.21 15.57 15.51 14.20 -46.80 -46.85 14.06 12.57 11.83 10.81 9.24",
            "text": "where pn are estimated with independent score functions S\u03b8n. In the contextual samplers in Eq. (23) (\u2018MSDM Dirac\u2018) and Eq. (27) (\u2018MSDM Gaussian\u2018), S\u03b8n((x1(t), . . . ,xN (t)), \u03c3(t)) refers to a slice of the full score network on the components of the n\u2212th source. In the weakly-supervised cases, S\u03b8n is an individual function. To obtain the \u2018ISDM Dirac\u2019 posterior score, we factorize the prior in Eq. (21), then use the chain rule of differentiation, as in Appendix A, to obtain:\n\u2207xm(t) log p(F (x\u0303(t)) | y(0)) \u2248 \u2207xm(t) log pm(xm(t)) +\u2207xm(t) log pN (y(0)\u2212 N\u22121\u2211 n=1 xn(t))\n\u2248 S\u03b8m(xm(t), \u03c3(t))\u2212 S\u03b8N (y(0)\u2212 N\u22121\u2211 n=1 xn(t), \u03c3(t)) .\nWe obtain the \u2018ISDM Gaussian\u2019 posterior score by factorizing the joint prior in Eq. (26):\n\u2207xm(t) log p(x(t) | y(0)) \u2248 S \u03b8 m(xm(t), \u03c3(t))\u2212\n1\n\u03b32(t) (y(0)\u2212 N\u2211 n=1 xn(t)) ."
        },
        {
            "heading": "C EXPERIMENTAL SETUP",
            "text": "C.1 DATASET\nWe perform experiments on Slakh2100 (Manilow et al., 2019), a standard dataset for music source separation. Slakh2100 is a collection of multi-track waveform music data synthesized from MIDI files using virtual instruments of professional quality. The dataset comprises 2100 tracks, with a distribution of 1500 tracks for training, 375 for validation, and 225 for testing. Each track is accompanied by its stems, which belong to 31 instrumental classes. For a fair comparison, we only used the four most abundant classes as in (Manilow et al., 2022), namely Bass, Drums, Guitar, and Piano; these instruments are present in the majority of the songs: 94.7% (Bass), 99.3% (Drums), 100.0% (Guitar), and 99.3% (Piano).\nC.2 ARCHITECTURE AND TRAINING\nThe implementation of the score network is based on a time domain (non-latent) unconditional version of Mo\u00fbsai (Schneider et al., 2023).\nWe used the publicly available repository audio-diffusion-pytorch/v0.0.4321. The score network is a U-Net Ronneberger et al. (2015) comprised of encoder, bottleneck, and decoder with skip connections between the encoder and the decoder. The encoder has six layers comprising two convolutional ResNet blocks, followed by multi-head attention in the final three layers. The signal sequence is downsampled in each layer by a factor of 4. The number of channels in the encoder layers is [256, 512, 1024, 1024, 1024, 1024]. The bottleneck consists of a ResNet block, followed by self-attention, and another ResNet block (all with 1024 channel layers). The decoder follows a reverse symmetric structure with respect to the encoder. We employ audio-diffusion-pytorch-trainer2 for training. We downsample data to 22kHz and train the score network with four stacked mono channels for MSDM (i.e., one for each stem) and one mono channel for each model in ISDM, using a context length of \u223c 12 seconds. All our models were trained until convergence on an NVIDIA RTX 6000 GPU with 24 GB of VRAM. We trained all our models using Adam Kingma & Ba (2015), with a learning rate of 10\u22124, \u03b21 = 0.9, \u03b22 = 0.99 and a batch size of 16. Inference times and number of parameters are reported in Table 4.\nC.3 THE SAMPLER\nWe use a first-order ODE integrator based on the Euler method and introduce stochasticity following (Karras et al., 2022). The amount of stochasticity is controlled by the parameter Schurn. As shown in Appendix D and explained in detail in (Karras et al., 2022), stochasticity significantly improves sample quality. We implemented a correction mechanism (Song et al., 2021; Jayaram & Thickstun, 2020) iterating for R steps after each prediction step i, adding additional noise and re-optimizing with the score network fixed at \u03c3i. As per Karras et al. (2022), we adopt a non-linear schedule for time discretization that gives more importance to lower noise levels. It is defined as:\nti = \u03c3i = \u03c3 1 \u03c1 max + i\nI \u2212 1 (\u03c3\n1 \u03c1 min \u2212 \u03c3 1 \u03c1 max) \u03c1 ,\nwhere 0 \u2264 i < I , with I the number of discretization steps. We set \u03c3min = 10\u22124, \u03c3max = 1, \u03c1 = 7."
        },
        {
            "heading": "D HYPERPARAMETER SEARCH FOR SOURCE SEPARATION",
            "text": "We conduct a hyperparameter search over Schurn to evaluate the importance of stochasticity in source separation over a fixed subset of 100 chunks of the Slakh2100 test set, each spanning 12 seconds (selected randomly). To provide a fair comparison between the Dirac (\u2018MSDM Dirac\u2019, \u2018ISDM Dirac\u2019) and Gaussian (\u2018MSDM Gaussian\u2019, \u2018ISDM Gaussian\u2019) posterior scores, we execute a search over their specific hyperparameters, namely the constrained source for the Dirac separators and the \u03b3(t) coefficient for the Gaussian separators. Results are illustrated in Table 5. We observe that: (i) stochasticity proves beneficial for all separators, given that the highest values of SI-SDRi are achieved with Schurn = 20 and Schurn = 40, (ii) using the Dirac likelihood we obtain higher values of SI-SDRi with respect to the Gaussian likelihood, both with the MSDM and ISDM separators, and (iii) the ISDM separators perform better than the contextual MSDM separators (at the expense of not being able to perform total and partial generation).\n1https://github.com/archinetai/audio-diffusion-pytorch/tree/v0.0.43 2https://github.com/archinetai/audio-diffusion-pytorch-trainer/tree/\n79229912"
        },
        {
            "heading": "E EVALUATION",
            "text": "The details of the listening test are explained in Figure 3."
        },
        {
            "heading": "F DATA EFFICIENCY STUDY: RESULTS ON MUSDB",
            "text": "We report in Table 6 the results metric of MSDM and Demucs on MUSDB test set. We try three different strategies, we first check the out-of-distribution ability of the model trained on Slakh by testing directly on MUSDB. Then, we tried finetuning the model trained on Slakh2100 on MUSDB, and finally, we trained directly on MUSDB. Since the only stems that MUSDB and Slakh share are \u2018Bass\u2019 and \u2018Drums\u2019, the first and second strategies could be tested only on these two stems."
        }
    ],
    "year": 2023
}