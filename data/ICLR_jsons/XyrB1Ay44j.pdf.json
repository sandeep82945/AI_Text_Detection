{
    "abstractText": "Multi-modal models have shown a promising capability to effectively integrate information from various sources, yet meanwhile, they are found vulnerable to pervasive perturbations, such as uni-modal attacks and missing conditions. To counter these perturbations, robust multi-modal representations are highly expected, which are positioned well away from the discriminative multi-modal decision boundary. In this paper, different from conventional empirical studies, we focus on a commonly used joint multi-modal framework and theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness. This discovery can further explain the limitation of multi-modal robustness and the phenomenon that multi-modal models are often vulnerable to attacks on the specific modality. Moreover, our analysis reveals how the widespread issue, that the model has different preferences for modalities, limits the multi-modal robustness by influencing the essential components and could lead to attacks on the specific modality highly effective. Inspired by our theoretical finding, we introduce a training procedure called Certifiable Robust Multi-modal Training (CRMT), which can alleviate this influence from modality preference and explicitly regulate essential components to significantly improve robustness in a certifiable manner. Our method demonstrates substantial improvements in performance and robustness compared with existing methods. Furthermore, our training procedure can be easily extended to enhance other robust training strategies, highlighting its credibility and flexibility.",
    "authors": [
        {
            "affiliations": [],
            "name": "BUSTNESS WITH"
        },
        {
            "affiliations": [],
            "name": "MODALITY PREFERENCE"
        },
        {
            "affiliations": [],
            "name": "Zequn Yang"
        },
        {
            "affiliations": [],
            "name": "Yake Wei"
        },
        {
            "affiliations": [],
            "name": "Ce Liang"
        },
        {
            "affiliations": [],
            "name": "Di Hu"
        }
    ],
    "id": "SP:872aff1240857adebc4a7481822da12079cf9f90",
    "references": [
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "Vqa: Visual question answering",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Relja Arandjelovic",
                "Andrew Zisserman"
            ],
            "title": "Look, listen and learn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Tadas Baltru\u0161aitis",
                "Chaitanya Ahuja",
                "Louis-Philippe Morency"
            ],
            "title": "Multimodal machine learning: A survey and taxonomy",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Michal Bednarek",
                "Piotr Kicki",
                "Krzysztof Walas"
            ],
            "title": "On robustness of multi-modal fusion\u2014robotics perspective",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In 2017 ieee symposium on security and privacy (sp),",
            "year": 2017
        },
        {
            "authors": [
                "Honglie Chen",
                "Weidi Xie",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Vggsound: A large-scale audiovisual dataset",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Vishal Chudasama",
                "Purbayan Kar",
                "Ashish Gudmalwar",
                "Nirmesh Shah",
                "Pankaj Wasnik",
                "Naoyuki Onoe"
            ],
            "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In international conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Wenhao Ding",
                "Baiming Chen",
                "Bo Li",
                "Kim Ji Eun",
                "Ding Zhao"
            ],
            "title": "Multimodal safety-critical scenarios generation for decision-making algorithms evaluation",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Yunfeng Fan",
                "Wenchao Xu",
                "Haozhao Wang",
                "Junxiao Wang",
                "Song Guo"
            ],
            "title": "Pmr: Prototypical modal rebalance for multimodal learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Chris Finlay",
                "Adam M Oberman",
                "Bilal Abbasi"
            ],
            "title": "Improved robustness to adversarial examples using lipschitz regularization of the loss",
            "year": 2018
        },
        {
            "authors": [
                "Itai Gat",
                "Idan Schwartz",
                "Alex Schwing"
            ],
            "title": "Perceptual score: What data modalities does your model perceive",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Lei Huang",
                "Xianglong Liu",
                "Bo Lang",
                "Adams Yu",
                "Yongliang Wang",
                "Bo Li"
            ],
            "title": "Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Yu Huang",
                "Chenzhuang Du",
                "Zihui Xue",
                "Xuanyao Chen",
                "Hang Zhao",
                "Longbo Huang"
            ],
            "title": "What makes multi-modal learning better than single (provably)",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yu Huang",
                "Junyang Lin",
                "Chang Zhou",
                "Hongxia Yang",
                "Longbo Huang"
            ],
            "title": "Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably)",
            "venue": "arXiv preprint arXiv:2203.12221,",
            "year": 2022
        },
        {
            "authors": [
                "Xingyu Jiang",
                "Jiayi Ma",
                "Guobao Xiao",
                "Zhenfeng Shao",
                "Xiaojie Guo"
            ],
            "title": "A review of multimodal image matching: Methods and applications",
            "venue": "Information Fusion,",
            "year": 2021
        },
        {
            "authors": [
                "Will Kay",
                "Joao Carreira",
                "Karen Simonyan",
                "Brian Zhang",
                "Chloe Hillier",
                "Sudheendra Vijayanarasimhan",
                "Fabio Viola",
                "Tim Green",
                "Trevor Back",
                "Paul Natsev"
            ],
            "title": "The kinetics human action video dataset",
            "venue": "arXiv preprint arXiv:1705.06950,",
            "year": 2017
        },
        {
            "authors": [
                "Deepak Kumar",
                "Chetan Kumar",
                "Chun Wei Seah",
                "Siyu Xia",
                "Ming Shao"
            ],
            "title": "Finding achilles\u2019 heel: Adversarial attack on multi-modal action recognition",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Yi-Lun Lee",
                "Yi-Hsuan Tsai",
                "Wei-Chen Chiu",
                "Chen-Yu Lee"
            ],
            "title": "Multimodal prompting with missing modalities for visual recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Klas Leino",
                "Zifan Wang",
                "Matt Fredrikson"
            ],
            "title": "Globally-robust neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Guangyao Li",
                "Yake Wei",
                "Yapeng Tian",
                "Chenliang Xu",
                "Ji-Rong Wen",
                "Di Hu"
            ],
            "title": "Learning to answer questions in dynamic audio-visual scenarios",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Juncheng B Li",
                "Kaixin Ma",
                "Shuhui Qu",
                "Po-Yao Huang",
                "Florian Metze"
            ],
            "title": "Audio-visual event recognition through the lens of adversary",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Juncheng B Li",
                "Shuhui Qu",
                "Xinjian Li",
                "Po-Yao Bernie Huang",
                "Florian Metze"
            ],
            "title": "On adversarial robustness of large-scale audio visual learning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Siting Li",
                "Chenzhuang Du",
                "Yue Zhao",
                "Yu Huang",
                "Hang Zhao"
            ],
            "title": "What makes for robust multi-modal models in the face of missing modalities",
            "venue": "arXiv preprint arXiv:2310.06383,",
            "year": 2023
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Yiwei Lyu",
                "Xiang Fan",
                "Zetian Wu",
                "Yun Cheng",
                "Jason Wu",
                "Leslie Chen",
                "Peter Wu",
                "Michelle A Lee",
                "Yuke Zhu"
            ],
            "title": "Multibench: Multiscale benchmarks for multimodal representation learning",
            "venue": "arXiv preprint arXiv:2107.07502,",
            "year": 2021
        },
        {
            "authors": [
                "Zhaoyang Lyu",
                "Minghao Guo",
                "Tong Wu",
                "Guodong Xu",
                "Kehuan Zhang",
                "Dahua Lin"
            ],
            "title": "Towards evaluating and training verifiably robust neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Mengmeng Ma",
                "Jian Ren",
                "Long Zhao",
                "Davide Testuggine",
                "Xi Peng"
            ],
            "title": "Are multimodal transformers robust to missing modality",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Harsh Maheshwari",
                "Yen-Cheng Liu",
                "Zsolt Kira"
            ],
            "title": "Missing modality robustness in semi-supervised multi-modal semantic segmentation",
            "venue": "arXiv preprint arXiv:2304.10756,",
            "year": 2023
        },
        {
            "authors": [
                "Frank Nielsen",
                "Ke Sun"
            ],
            "title": "Guaranteed bounds on the kullback\u2013leibler divergence of univariate mixtures",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2016
        },
        {
            "authors": [
                "David A Noever",
                "Samantha E Miller Noever"
            ],
            "title": "Reading isn\u2019t believing: Adversarial attacks on multi-modal neurons",
            "venue": "arXiv preprint arXiv:2103.10480,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaokang Peng",
                "Yake Wei",
                "Andong Deng",
                "Dong Wang",
                "Di Hu"
            ],
            "title": "Balanced multimodal learning via on-the-fly gradient modulation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Elan Rosenfeld",
                "Ezra Winston",
                "Pradeep Ravikumar",
                "Zico Kolter"
            ],
            "title": "Certified robustness to labelflipping attacks via randomized smoothing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Hadi Salman",
                "Jerry Li",
                "Ilya Razenshteyn",
                "Pengchuan Zhang",
                "Huan Zhang",
                "Sebastien Bubeck",
                "Greg Yang"
            ],
            "title": "Provably robust deep learning via adversarially trained smoothed classifiers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Christian Schlarmann",
                "Matthias Hein"
            ],
            "title": "On the adversarial robustness of multi-modal foundation models",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Sahil Singla",
                "Surbhi Singla",
                "Soheil Feizi"
            ],
            "title": "Improved deterministic l2 robustness on cifar-10 and cifar-100",
            "venue": "arXiv preprint arXiv:2108.04062,",
            "year": 2021
        },
        {
            "authors": [
                "Khurram Soomro",
                "Amir Roshan Zamir",
                "Mubarak Shah"
            ],
            "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
            "venue": "arXiv preprint arXiv:1212.0402,",
            "year": 2012
        },
        {
            "authors": [
                "Yapeng Tian",
                "Chenliang Xu"
            ],
            "title": "Can audio-visual integration strengthen robustness under multimodal attacks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Samarth Tripathi",
                "Sarthak Tripathi",
                "Homayoon Beigi"
            ],
            "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
            "venue": "arXiv preprint arXiv:1804.05788,",
            "year": 2018
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Paul Pu Liang",
                "Amir Zadeh",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov"
            ],
            "title": "Learning factorized multimodal representations",
            "venue": "arXiv preprint arXiv:1806.06176,",
            "year": 2018
        },
        {
            "authors": [
                "Yusuke Tsuzuku",
                "Issei Sato",
                "Masashi Sugiyama"
            ],
            "title": "Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Hamid Reza Vaezi Joze",
                "Amirreza Shaban",
                "Michael L. Iuzzolino",
                "Kazuhito Koishida"
            ],
            "title": "Mmtm: Multimodal transfer module for cnn fusion",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Nishant Vishwamitra",
                "Hongxin Hu",
                "Ziming Zhao",
                "Long Cheng",
                "Feng Luo"
            ],
            "title": "Understanding and measuring robustness of multimodal learning",
            "venue": "arXiv preprint arXiv:2112.12792,",
            "year": 2021
        },
        {
            "authors": [
                "Weiyao Wang",
                "Du Tran",
                "Matt Feiszli"
            ],
            "title": "What makes training multi-modal classification networks hard",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yake Wei",
                "Di Hu",
                "Yapeng Tian",
                "Xuelong Li"
            ],
            "title": "Learning in audio-visual context: A review, analysis, and new perspective",
            "venue": "arXiv preprint arXiv:2208.09579,",
            "year": 2022
        },
        {
            "authors": [
                "Lily Weng",
                "Huan Zhang",
                "Hongge Chen",
                "Zhao Song",
                "Cho-Jui Hsieh",
                "Luca Daniel",
                "Duane Boning",
                "Inderjit Dhillon"
            ],
            "title": "Towards fast computation of certified robustness for relu networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Nan Wu",
                "Stanislaw Jastrzebski",
                "Kyunghyun Cho",
                "Krzysztof J Geras"
            ],
            "title": "Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Kaidi Xu",
                "Zhouxing Shi",
                "Huan Zhang",
                "Yihan Wang",
                "Kai-Wei Chang",
                "Minlie Huang",
                "Bhavya Kailkhura",
                "Xue Lin",
                "Cho-Jui Hsieh"
            ],
            "title": "Automatic perturbation analysis for scalable certified robustness and beyond",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Peng Xu",
                "Xiatian Zhu",
                "David A Clifton"
            ],
            "title": "Multimodal learning with transformers: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Ruize Xu",
                "Ruoxuan Feng",
                "Shi-Xiong Zhang",
                "Di Hu"
            ],
            "title": "Mmcosine: Multi-modal cosine loss towards balanced audio-visual fine-grained learning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Pinci Yang",
                "Xin Wang",
                "Xuguang Duan",
                "Hong Chen",
                "Runze Hou",
                "Cong Jin",
                "Wenwu Zhu"
            ],
            "title": "Avqa: A dataset for audio-visual question answering on videos",
            "venue": "In Proceedings of the 30th ACM International Conference on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuolin Yang",
                "Linyi Li",
                "Xiaojun Xu",
                "Bhavya Kailkhura",
                "Tao Xie",
                "Bo Li"
            ],
            "title": "On the certified robustness for ensemble models and beyond",
            "venue": "arXiv preprint arXiv:2107.10873,",
            "year": 2021
        },
        {
            "authors": [
                "Youngjoon Yu",
                "Hong Joo Lee",
                "Byeong Cheon Kim",
                "Jung Uk Kim",
                "Yong Man Ro"
            ],
            "title": "Investigating vulnerability to adversarial examples on multimodal data fusion in deep learning",
            "year": 2005
        },
        {
            "authors": [
                "Huan Zhang",
                "Tsui-Wei Weng",
                "Pin-Yu Chen",
                "Cho-Jui Hsieh",
                "Luca Daniel"
            ],
            "title": "Efficient neural network robustness certification with general activation functions",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jiaming Zhang",
                "Qi Yi",
                "Jitao Sang"
            ],
            "title": "Towards adversarial attack on vision-language pre-training models",
            "venue": "In Proceedings of the 30th ACM International Conference on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Meanwhile, our method are adaptable to other fusion mechanisms, where the modality-specific representations could interact earlier in the process",
            "year": 2024
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "As data are often presented from different perspectives, like text, images, and audio, how to effectively exploit and integrate information from multiple sources becomes important. This has given rise to the concept of multi-modal learning, which serves as a potent approach that enables a more comprehensive understanding of complex concepts and facilitates more effective knowledge acquisition for different sources of information (Wei et al., 2022) Nowadays, multi-modal learning has demonstrated its remarkable ability in various tasks, including scene understanding (Antol et al., 2015; Yang et al., 2022; Li et al., 2022a), and emotion recognition (Tripathi et al., 2018; Chudasama et al., 2022). However, real-world data are often perturbed, such as attacks, and missing modality, which impacts the performance of multi-modal model (Kumar et al., 2020). As a result, multi-modal robustness, which refers to the model\u2019s ability to defend against such perturbations, has received increasing attention in recent studies (Bednarek et al., 2020; Vishwamitra et al., 2021). Unlike data from a single modality, multi-modal data can be perturbed across all modalities. Therefore, for robustness, multimodal models should have the ability to resist attacks on both individual and multiple modalities. Nevertheless, experiments have suggested that multi-modal models could perform badly when encountering perturbation (Noever & Noever, 2021) or missing modality (Yu et al., 2020; Ma et al., 2022). Based on these observations about the vulnerability of the multi-modal model, how to achieve a robust multi-modal model arises and attaches more focus. To address this, previous methods improve the training strategies to obtain a robust multi-modal model (Liang et al., 2021; Ding et al., 2021). Specifically, certain studies extend uni-modal robust training strategies, like adversarial training and mixup, to learn a discriminative multi-modal decision boundary (Li et al., 2022b; Maheshwari et al., 2023). Others take steps like cross-modal\n\u2217Corresponding author\nalignment (Tian & Xu, 2021) to enhance the connection among modalities and obtain compact and robust multi-modal representation. However, even if they empirically improve robustness to some degree, they still lack in-depth theoretical analysis to understand the resilience of multi-modal models to perturbations, which is vital for safety-critical applications. More importantly, a universal phenomenon can be observed for these robust training methods that the adversarial attack on specific modalities could be more effective than others. As shown in Figure 1, the \u21132-PGD attack is more effective on modality #a than modality #v for both Joint Training and three widely used robust training methods on the Kinetics Sounds dataset.\nTo deeply understand multi-modal robustness and elucidate this phenomenon, we first depict the multimodal decision boundary by integrating uni-modal representation margins. Then, we derive a lower bound for the perturbation radius that the multi-modal model can consistently defend against. We discover that larger uni-modal margins coupled with reasonable integration are crucial for enhanced multi-modal robustness. With the above theoretical results, we investigate the pervasive issue (Wang et al., 2020; Peng et al., 2022) that multi-modal models exhibit a pronounced preference for a particular modality. This preferred modality profoundly influences the model\u2019s decision, then resulting in the above robustness phenomenon. On one hand, when a specific preferred modality is substantial enough to rely upon, the model will be reluctant to learn from other modalities, leading to the imbalance problem (Huang et al., 2022; Wu et al., 2022). This imbalance problem hinders the enhancement of the uni-modal margin on the\nother modality, thus limiting the robustness. On the other hand, since the multi-modal model heavily relies on the preferred modality, the corresponding factor used in modality integration becomes larger, which will amplify the variation of the uni-modal margin in decision-making. Hence, in case the preferred modality is vulnerable, the multi-modal model becomes more vulnerable to multi-modal attack. Further, this preference for vulnerable modality makes attacks on the preferred modality significantly more effective than other ones, explaining the observation in Figure 1. Since the essential components of robustness have an interrelation with each other, directly applying regulation is hard to guarantee higher robustness. To address this, we employ an orthogonal-based framework that formulates an alternative bound, which eliminates the interrelation and explicitly presents the integration. Building upon our theoretical analysis, we introduce a two-step Certifiable Robust Multi-modal Training (CRMT) procedure to ensure progressively superior robustness. Initially, we redefine uni-modal margins related to the reliability of the modality in this framework. Then we propose to regulate the unreliable modality by enlarging its margin, which can alleviate the imbalanced problem brought by modality preference. Further, our approach adjusts the integration of modalities considering the improvement of certified bound. These steps not only mitigate the large gap between robustness against attack on each modality but also credibly guarantee higher multimodal robustness. To validate our method, we conduct extensive experiments to present the advanced robustness against both uni-modal and multi-modal attacks. Our contributions are as follows:\n1. We focus on a commonly used multi-modal model, offering invaluable insights into the essential components influencing multi-modal robustness.\n2. We present analyses highlighting how multi-modal preference limits the multi-modal robustness and contributes to the vulnerability of multi-modal models towards specific modalities.\n3. Drawing from our theoretical findings, we introduce a two-step training procedure, alleviating the limitation brought by modality preference. Our method can effectively enhance both performance and robustness over three real-world multi-modal datasets."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Multi-modal Robustness Analysis. Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to attacks and perturbations (Goodfellow et al., 2014; Madry et al., 2017),\nraising significant concerns regarding their deployment. With the presence of multiple modalities, the forms of perturbations include uni-modal attacks, multi-modal attacks (Schlarmann & Hein, 2023), and modality missing (Lee et al., 2023). One possible way to enable the models to resist these perturbations is to design robust training strategies to obtain a reliable decision boundary that is distanced from samples (Li et al., 2021). Certain methods, such as multi-modal adversarial training (Li et al., 2022b) and incorporating uni-modal tasks (Ma et al., 2022), have proven pertinent in this context. On the other hand, studies focus on the latent consistency among modalities and propose to enhance modality interaction like alignment (Tsai et al., 2018) to obtain compact and robust representation (Bednarek et al., 2020). Among these empirical works, we identify a universal phenomenon that multi-modal models are commonly vulnerable to a certain modality (Liang et al., 2021), which currently lacks a comprehensive theoretical explanation. This motivates us to theoretically figure out the essential components determining the multi-modal robustness and explain the vulnerable modality phenomenon.\nMulti-modal imbalance problem. Multi-modal learning is a significant approach for achieving a richer understanding across various tasks (Baltrus\u030caitis et al., 2018; Jiang et al., 2021). However, even with the potential for a more comprehensive latent representation from multi-modal models (Huang et al., 2021), their performance might not always surpass that of the best uni-modal counterparts (Wang et al., 2020; Huang et al., 2022). Due to the inherent differences between modalities, a multi-modal model might prefer or lean towards a modality that is easier to learn, potentially overlooking others (Wu et al., 2022). In response, various strategies are proposed to strengthen the learning of uni-modalities, thereby enhancing the generalization of multi-modal model (Wang et al., 2020; Peng et al., 2022; Fan et al., 2023; Xu et al., 2023b). However, these researches do not elucidate how this modality preference problem impacts robustness against perturbations, which is our main focus.\nCertified robustness. To describe the robustness of the model, certified robustness is introduced to describe the size of permissible perturbations that a model can consistently defend against. Techniques such as randomized smoothing (Cohen et al., 2019; Rosenfeld et al., 2020; Yang et al., 2021; Salman et al., 2019) and interval bound propagation (Zhang et al., 2018; Lyu et al., 2021; Xu et al., 2020) are widely utilized to relax the model, which assists in determining the certificate bounds. Additionally, the Lipschitz constant can serve as an intuitive metric to illustrate how perturbations influence the decision of models (Weng et al., 2018; Leino et al., 2021). However, these methods are tailored for inputs with only a single modality but fail to focus on the primary concerns with multiple modalities. In our research, we establish certified robustness for multi-modal models and determine the essential components influenced by modality preference that limit the robustness."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "Multi-modal Framework. We consider a general K-way classification problem with a vectorized input sample x = (x(1),x(2)) \u2208 Rd1+d2 consisting of two modalities, and a ground truth label y \u2208 [K]. We consider the commonly used joint learning framework in multi-modal learning, where all of the modalities are projected into the shared space for downstream tasks (Huang et al., 2022). Concretely, the uni-modal representations are extracted by encoders \u03d5(m) and concatenated to form the joint representation, which is then mapped to the output space using a linear classifier, where W \u2208 RK\u00d7(dim(\u03d5(1))+dim(\u03d5(2))) and b \u2208 RK are the weight matrix and bias respectively, and dim(\u00b7) represents the dimension. The logits output of the multi-modal model can be denoted as h(x) = W [\u03d5(1)(x(1));\u03d5(2)(x(2))] + b = W (1)\u03d5(1)(x(1)) +W (2)\u03d5(2)(x(2)) + b, where W (m) \u2208 RK\u00d7dim(\u03d5(m)) is the part of classifier W related to the m-th modality.\nMulti-modal robustness. To assess the certified robustness of a multi-modal model, we can measure the radius of the smallest perturbation that shifts a sample to the decision boundary. In other words, any perturbation that falls within this radius can be defended against. Definition 3.1. Suppose the multi-modal model h can correctly classify the sample x, i.e. \u2200k \u0338= y, hy(x) > hk(x), where hk denotes the logit score of the k-th class. The robustness radius of the multi-modal model towards the sample x can be defined as:\nP (x) = min x\u2032\n\u2225x\u2212 x\u2032\u22252 s.t. \u2203j \u0338= y, hy(x \u2032) = hj(x \u2032). (1)\nEquation 1 seeks to identify the smallest perturbation, denoted as x\u2212 x\u2032, that results in reaching the decision boundary between the ground truth y and its nearest class j. Hence, any smaller perturbation can always be defended against. Here, we use \u21132-norm to measure the radius, reflecting the overall size of the perturbation (Tsuzuku et al., 2018; Carlini & Wagner, 2017)."
        },
        {
            "heading": "3.2 CERTIFIED ROBUSTNESS FOR MULTI-MODAL MODEL",
            "text": "In this section, we endeavor to uncover the essential components that impact multi-modal robustness. Referring to Figure 1, there is a notable variation in the effectiveness of perturbations on different modalities. This observation prompts us to distinguish the differences within each uni-modality. Therefore, we introduce the concept of uni-modal margin, which quantifies the distance between a uni-modal representation and the uni-modal decision boundary. To elaborate further, (W (m)y\u00b7 \u2212 W\n(m) k\u00b7 )\u03d5(x (m)) = 0 signifies that sample x(m) is positioned on the decision boundary between classes y and k for m-th modality. The uni-modal representation margin can be defined as: Definition 3.2. Given the uni-modal encoder \u03d5(m) and the classifier W (m), the margin on representation space between ground truth y and other label k is defined as:\n\u03b6 (m) k (x\n(m)) = (W (m) y\u00b7 \u2212W (m)k\u00b7 )\u03d5(m)(x(m))\u2225\u2225\u2225W (m)y\u00b7 \u2212W (m)k\u00b7 \u2225\u2225\u2225\n2\n. (2)\nIn this context, a larger margin indicates the uni-modality is more reliable in distinguishing these two classes. Using this margin definition, we can re-examine the constraint conditions in Equation 1, which outline the perturbed sample located on the multi-modal decision boundary where y and the closest class j are tied in the output space:\nhy(x \u2032)\u2212 hj(x\u2032) = c(1)j \u03b6 (1) j (x \u2032(1)) + c (2) j \u03b6 (2) j (x \u2032(2)) + \u03b2j = 0. (3)\nInspired by Equation 3, we observe that the multi-modal decision boundary can be described as integration of different uni-modal margins of the perturbed sample with factors c(m)j = \u2225W (m) y\u00b7 \u2212 W (m) j\u00b7 \u22252 and constant term \u03b2j = by \u2212 bj . The integration factors c (m) j quantify how variations in the uni-modal margin influence multi-modal decisions. When perturbing a sample x towards the decision boundary, a larger factor c(m)j indicates altering the margin of the corresponding modality is more effective. Meanwhile, when the sample is perturbed, how the uni-modal margin varies also depends on the size of the perturbation. Thus, we propose to introduce the Lipschitz constant \u03c4 (m)j for the uni-modal margin. This Lipschitz constant is the smallest constant to limit the local variation range of a certain function (Finlay et al., 2018), which is given by:\n|\u03b6(m)j (x (m))\u2212 \u03b6(m)j (x \u2032(m))| \u2264 \u03c4 (m)j \u2225\u2225\u2225x(m) \u2212 x\u2032(m)\u2225\u2225\u2225\n2 . (4)\nThen, we can provide the lower bound for the perturbation radius for multi-modal robustness.\nTheorem 3.3. Given an input x with ground-truth label y \u2208 [K] and the closest label j \u0338= y, \u03b6 (m) j (x (m)) as the representation margin for m-th modality with Lipschitz constraint \u03c4 (m)j , and the\nintegration factor c(m)j . Define x \u2032 as the perturbed sample, and x \u2212 x\u2032 as the perturbation. The lower bound for the perturbation radius can be described as:\nP (x) = min x\u2032\n\u2225x\u2212 x\u2032\u22252 \u2265 c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j\u221a (c\n(1) j \u03c4 (1) j ) 2 + (c (2) j \u03c4 (2) j ) 2\nwhere j \u0338= y s.t. c(1)j \u03b6 (1) j (x \u2032(1)) + c (2) j \u03b6 (2) j (x \u2032(2)) + \u03b2j = 0.\n(5)\nThe detailed proof can be found in the Appendix. Based on the certified bound above, we deduce that the multi-modal robustness depends on three primary factors: uni-modal representation margins \u03b6 (m) j , integration c (m) j , and bias difference \u03b2j . Firstly, robustness increases proportionally with the enhancement of the uni-modal representation margin \u03b6(m)j . Secondly, the integration c (m) j is related to both uni-modal margins and the uni-modal Lipschitz constant, the expected integration for robustness requires considering both. Thus, a reasonable choice of integration factor can benefit higher robustness. Thirdly, when the sample is perturbed, the bias difference term \u03b2j stays invariant, since it depends on class y and j rather than the specific sample. Based on this fact, the bias difference is not considered in our analysis about model robustness. In a nutshell, we recognize that uni-modal margins and the integration of modalities are two essential components. In the following section, we will analyze how these essential components vary and influence the robustness of multi-modal models, especially under modality preference."
        },
        {
            "heading": "3.3 ANALYSIS ABOUT MODALITY PREFERENCE FOR ROBUSTNESS",
            "text": "Since modalities have different amounts of information, some specific modalities might hold more significance than others in decision-making processes (Gat et al., 2021). As a result, it is widely recognized that multi-modal models tend to show a preference for and predominantly rely on specific modality (Huang et al., 2022; Wu et al., 2022). However, this preference or over-reliance on the specific modality poses challenges for achieving a robust multi-modal model. We delve deeper into how such preferences impact the two essential components of the certified bound in Equation 5.\nUni-modal representation margin. As shown in Equation 5, a larger uni-modal margin \u03b6(m)j (x(m)) determines higher certified robustness. However, when the learned information in the preferred modality is sufficiently reliable, the multi-modal model is reluctant to learn more information from other modalities (Wu et al., 2022; Huang et al., 2022). Thus, the modality preference leads to an imbalance problem hindering the development of uni-modal representations, resulting in a narrower representation margin and ultimately constraining the certified robustness of the multi-modal model.\nIntegration of modalities. According to modality preference, the decision of the multi-modal model highly depends on a specific modality. Thus, considering the integration of modalities, the preferred modality contributes more and is allocated a larger integration factor c(m)j , which could amplify the variation of the uni-modal margin in multi-modal decision-making. Since the preference is only determined by whether the modality with ideal discriminative ability, the multi-modal model could prefer a vulnerable modality, which has a larger \u03c4 (m)j . Thus, the perturbation for this preferred but vulnerable modality leads to larger variations in multi-modal margins, which is further amplified in decision-making. Motivated by this phenomenon, we define \u03b7(m) = c(m)j \u03c4 (m) j as the vulnerability indicator of modality m. When the multi-modal model exhibits a preference for a vulnerable modality 1, there is a significant imbalance in this indicator, with \u03b7(1) \u226b \u03b7(2). Consequently, we observe:\nP (x) \u2265 c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j\u221a (\u03b7(1))2 + (\u03b7(2))2 \u2248 c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j \u03b7(1) . (6)\nThat is to say, the modality preference on vulnerable modality leaves an unreasonable imbalance on \u03b7(m), thus the multi-modal robustness is highly dependent on the modality with a larger vulnerability indicator. In this way, distinctly attacking the vulnerable modalities alone is enough to obscure the model. Here we further provide the multi-modal robustness under uni-modal attack case:\nProposition 3.4. Following the setting in Theorem 3.3, w.l.o.g. considering the perturbation on 1-th modality x\u2032(1), the lower bound for the uni-modal perturbation radius can be described as:\nmin x\u2032(1) \u2225\u2225\u2225x(1) \u2212 x\u2032(1)\u2225\u2225\u2225 2 \u2265 c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j\nc (1) j \u03c4 (1) j\nwhere j \u0338= y s.t. c(1)j \u03b6 (1) j (x \u2032(1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j = 0.\n(7)\nThe lower bounds of different uni-modal perturbations share identical numerators but differ in denominators, the vulnerability indicator \u03b7(m). The larger indicator on preferred modality reduces the lower bound for perturbations on this modality, thus making attacks on this preferred modality more effective, explaining the observation in Figure 1."
        },
        {
            "heading": "3.4 CERTIFIABLE ROBUST MULTI-MODAL TRAINING",
            "text": "Based on the above analysis, we target to improve the uni-modal representation margin \u03b6(m)j and adjust the integration of modalities c(m)j as regulation for higher certified robustness. However, these regulations are intricately linked with the linear classifier W , potentially leading to conflicts in optimization objectives. As a result, stably enhancing the certified robustness presents challenges. Additionally, determining the Lipschitz constant for the margin is computationally demanding. To address these challenges, we propose to adopt orthogonality (Huang et al., 2018) within uni-modal linear classifiers as our framework. Detailedly, we ensure that in each modality, the class-specific vectors W\u0303 (m)k\u00b7 , k \u2208 [K] are unit and orthogonal. Since the valuable information between modalities is different, we apply the weight a(m) \u2208 RK to lead the model focusing on more reliable modalities. Integrating these weights can enhance the model\u2019s ability to effectively utilize the valuable information from each modality. Therefore, learning of uni-modal representations and integration of modalities can be decoupled. The score corresponding to the k-th class can be expressed as:\nh\u0303k(x) = a (1) k W\u0303 (1) k\u00b7 \u03d5 (1)(x(1)) + a (2) k W\u0303 (2) k\u00b7 \u03d5 (2)(x(2)) + b\u0303k, (8)\nwhere W\u0303 (m) \u2208 RK\u00d7dim(\u03d5(m)) is the matrix with orthogonal rows, satisfying W\u0303 (m)(W\u0303 (m))T = IK . We can use W\u0303 (m) k\u00b7 \u03a6(x\n(m)) as the uni-modal score, which represents whether the uni-modal representation is well learned toward the k-th class. With this framework in place, we are able to express the new Lipschitz constant \u03c4\u0303 (m)k for the uni-modal score on class k in the m-th modality as:\n|W\u0303 (m)k\u00b7 \u03d5 (m)(x(m))\u2212 W\u0303 (m)k\u00b7 \u03d5 (m)(x\u2032(m))| \u2264 \u03c4\u0303 (m)k \u2225\u2225\u2225x(m) \u2212 x\u2032(m)\u2225\u2225\u2225\n2 . (9)\nWith these definitions, we can derive the certified bound for the multi-modal perturbation radius, which is distinctly tailored to the framework employing orthogonal classifiers: Theorem 3.5. Given an input x with ground-truth label y \u2208 [K] and the closest label j \u0338= y, the orthogonal classifier W\u0303 (m), the modality-specific weight a(m), the Lipschitz constant \u03c4\u0303 (m)j , and the difference of the bias \u03b2\u0303j = b\u0303y \u2212 b\u0303j . The lower bound for the perturbation radius with the orthogonal-based framework can be described as:\nP (x) \u2265\n\u22112 m=1 ( a (m) y W\u0303 (m) y\u00b7 \u03d5 (m)(x(m))\u2212 a(m)j W\u0303 (m) j\u00b7 \u03d5 (m)(x(m)) ) + \u03b2\u0303j\u221a\u22112\nm=1\n( a (m) y \u03c4\u0303 (m) y + a (m) j \u03c4\u0303 (m) j )2 where j \u0338= y, s.t. h\u0303y(x\u2032) = h\u0303j(x\u2032).\n(10)\nThe proof positions in the Appendix. With this lower bound, we can discuss designing a regulation method to explicitly enhance the certified robustness. Firstly, as analyzed in Section 3.3, the imbalance problem brought by modality preference impacts the unreliable modal representation margin. To explicitly regulate the uni-modal encoder and classifier independent from integration, we redefine the margin as the difference in uni-modal score between ground truth and the other label, which can also reflect the reliability of uni-modality. Then we propose to enlarge the relatively small margin through regularization, which is expressed as follows:\nmax W\u0303 (m),\u03d5(m) min m;k \u0338=y\nW\u0303 (m) y\u00b7 \u03d5 (m)(x(m))\u2212 W\u0303 (m)k\u00b7 \u03d5 (m)(x(m)). (11)\nBased on Equation 11, we propose a regularization term, refining the maximum using the LogSumExp function to facilitate better optimization:\nL1 = 1\nN N\u2211 i=1 log\n( 2\u2211\nm=1\n\u2211 k \u0338=y exp(W\u0303 (m) k\u00b7 \u03d5 (m)(x (m) i ))\nexp(W\u0303 (m) y\u00b7 \u03d5(m)(x (m) i ))\n) , (12)\nwhich is detailed in the Appendix. Furthermore, we can adjust the integration of different modalities by enlarging the lower bound in Equation 10. Subsequently, we propose a two-step training procedure called Certifiable Robust Multi-modal Training (CRMT), which can credibly obtain a robust multimodal model. The training procedure of CRMT is as follows:\nStep 1: optimize with cross-entropy loss and margin regularization with term \u03c1: mina(m),W\u0303 (m),\u03d5(m) \u03c1L1 + 1 N \u2211N i=1 CE(h(xi), yi), where CE is the cross-entropy loss function.\nStep 2: fix W\u0303 (m), \u03d5(m), update a(m) to approach higher certified robustness: mina(m) L2 = \u2212 1N \u2211N i=1 r(xi), where r(x) is the lower bound in Equation 10.\nAs shown in the Appendix, we further demonstrate that only one iteration of our method can achieve considerable robustness, without a huge consumption of training cost."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 SETUPS",
            "text": "Dataset. We evaluate our method on different datasets including Kinetics-Sounds (Audio + Vision) (Arandjelovic & Zisserman, 2017), UCF101 (Optical flow + RGB) (Soomro et al., 2012), and VGGSound (Audio + Vision) (Chen et al., 2020). We use the backbone ResNet18 (He et al., 2016) as the encoder for each uni-modality. Details about these datasets are presented in the Appendix.\nMulti-modal models. In this study, the comparison methods are selected to improve multi-modal learning and be suitable for the multi-modal joint training strategy. Comparison methods can be divided into two distinct groups: Firstly, methods address the imbalance problem caused by modality preference: Gradient Blending (GB) (Wang et al., 2020), On-the-fly Gradient Modulation with generalization enhancement (OGM) (Peng et al., 2022), Prototypical Modal Rebalance (PMR) (Fan et al., 2023). Secondly, methods aim at improving multi-modal robustness: Multi-Modal Adversarial Training (MMAT) (Li et al., 2022b), Multi-modal mixup (Mixup) (Madry et al., 2017; Li et al., 2022b), MinSim+ExFMem (MSEFM) (Tian & Xu, 2021). Our method can be extended to different training strategies, denoted as Certifiable Robust Multi-modal Training with Joint Training (CRMT-JT), CRMT with Adversarial Training (CRMT-AT), and CRMT with Mixup (CRMT-Mix).\nAttack methods. Following previous work (Tsuzuku et al., 2018; Singla et al., 2021), we select Fast Gradient Method (FGM) (Goodfellow et al., 2014) and \u21132 Projected Gradient Descent (\u21132PGD) (Madry et al., 2017) as two attack methods with attack size \u03f5 = 0.5, which are widely used\nfor verifying multi-modal robustness with \u21132-norm. For uni-modal attack, we also introduce attacks FGM and \u21132-PGD with \u03f5 = 1.0, and missing on uni-modality."
        },
        {
            "heading": "4.2 ROBUSTNESS VALIDATION ON MULTI-MODAL AND UNI-MODAL ATTACK",
            "text": "Robustness against multi-modal attacks. We validate the robustness of our method under multimodal attacks. Based on the experimental results presented in Table 1, we have identified four key observations that warrant attention. Firstly, while imbalance methods effectively enhance performance on clean samples, robustness methods can demonstrate more notable defense capabilities against various attacks. Secondly, the imbalance method GB can be superior to the robustness method MSEFM under some situations. That is because GB introduces a multi-task method to enhance the learning of uni-modality and improve the uni-modal margin, thus it can enhance the robustness methods according to our analysis. Thirdly, our proposed CRMT-based methods surpass the performance of the compared methods across these datasets in most cases. This superiority stems from our approach to addressing the imbalance problem through improving uni-modal representation margins, and the certificate adjustment of modality-specific weights for heightened certified robustness. Fourthly, the improvement of results for CRMT-AT and CRMT-Mix suggests that our training procedure can serve as a valuable component applicable to other robust training methods.\nRobustness against distinct uni-modal attack. To substantiate the efficacy of our CRMT methods, we conducted additional experiments on the KS dataset, involving a broader range of different unimodal attacks. As demonstrated in Table 2, the absence of modality #a leads to a larger performance decline than #v, since it is more preferred by multi-modal models. As shown in Table 2, previous multi-modal methods demonstrated poor performance when subjected to attacks on the preferred modality #a, aligning with our analysis. In contrast, our CRMT-based methods consider addressing the imbalance problem introduced by modality preference and adjusting integration, thus making the model more robust against uni-modal attacks. Furthermore, when encountering different uni-modal attacks, our CRMT-based approach shows superior performance and consistently ensures higher robustness in various scenarios. This strongly demonstrates the effectiveness and versatility of our proposed method in enhancing the robustness of multi-modal models.\nTable 3: Extension results of adversarial accuracy with transformer-based method on VGGS dataset.\nAttack Clean Uni-modal attack Multi-modal attack\nMethod w\\o FGM #v FGM #a \u21132-PGD #v \u21132-PGD #a FGM \u21132-PGD MMT 0.465 0.433 0.259 0.431 0.228 0.331 0.326\nCRMT-MMT 0.471 0.440 0.276 0.428 0.237 0.344 0.340\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Radius\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAc cu\nra cy\nMMAT #v MMAT #a CRMT-AT #v CRMT-AT #a\n0.1\n0.2\n0.3\n0.4\n0.5\nAc c\n|# v\n- # a|\nMMAT |#v - #a| CRMT-AT |#v - #a|\nFigure 4: This figure presents the robustness accuracy against unimodal attacks with different sizes, where the dotted line signifies the difference in robustness accuracy between two uni-modalities.\nImbalance on uni-modal vulnerability indicators. We verify the conclusion drawn in Section 3.3 that the imbalance problem introduced by modality preference leads to the vulnerability of attack on specific modality, as illustrated in Figure 1. To achieve this, we compare MMAT with our CRMT-AT approach. As depicted in Equation 7, \u03b7(m) is utilized as a measurement to assess the robustness of multi-modal models against unimodal perturbations, and we employ the ratio of uni-modal vulnerability indicators \u03b7(v)/\u03b7(a) (see Figure 3). It can be demonstrated that our approach diligently works to reduce the imbalance in the indicator \u03b7, effectively darkening the heated map. Furthermore, according to the robustness accuracy, our approach significantly reduces the disparity in attack performance between the two modalities (see Figure 4). This evidence serves to illustrate the efficacy of our method in mitigating the\nimbalance problem and ultimately enhancing multi-modal robustness."
        },
        {
            "heading": "4.3 VALIDATION FOR EFFECTIVENESS AND SCALABILITY",
            "text": "Ablation studies. To delve deeper into the efficacy of our training procedure, we conduct an ablation analysis, aiming to elucidate the contribution of each component of the proposed training procedure toward the overall result. When compared with the baseline Joint Training (JT), our results include the orthogonalbased framework and our proposed training procedure. We report our findings related to solely using the orthogonal framework (OJT), CRMT focusing exclusively on each step (CRMT-step1 and CRMT-step-2), and our full proposal, CRMT-JT, which incorporates both steps. As depicted in Figure 5, it is evident that the orthogonal-based framework does not contribute to improved robustness. Meanwhile, both step-1 and step-2 of the training procedure can enhance robustness, particularly in the face of largersize attacks. Furthermore, the results highlight that each step\nconsistently enhances multi-modal robustness, making CRMT-JT superior in multi-modal robustness.\nExtension studies to transformer. We further provide experiment results of our methods extended to the Multi-Modal Transformer-based framework with hierarchical attention (Xu et al., 2023a) (MMT) on the VGGS dataset. Both visual and audio class tokens are concatenated and linearly projected into the output space. We mainly evaluate the robustness under the Transformer model accurately, hence we train from scratch instead of using the pre-trained model. To validate our method, we only introduce the CRMT procedure step-1 on this transformer architecture as a comparison. That is because both audio and visual input influence each token, which is beyond our assumption. Based on the results in Table 3, our method can also improve the robustness of transformer-based architecture in most cases, which proves the broad prospects of our approach."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this study, we present the essential components for multi-modal robustness and delve into the limitations imposed by modality preference. Additionally, we explore why multi-modal models exhibit vulnerabilities to attack specific modalities. Further, we introduce the Credible Multi-modal Robust Training procedure, a novel approach explicitly designed to boost the certified robustness."
        },
        {
            "heading": "6 ACKNOWLEDGEMENT",
            "text": "This research was supported by National Natural Science Foundation of China (NO.62106272), the Young Elite Scientists Sponsorship Program by CAST (2021QNRC001), and Public Computing Cloud, Renmin University of China."
        },
        {
            "heading": "7 DETAILED ANALYSIS AND PROOFS IN SECTION 3",
            "text": ""
        },
        {
            "heading": "7.1 PROOF OF THEOREM 1",
            "text": "In this section, we provide proof for two lower bounds in Equation 5 and Equation 10.\nTheorem 7.1. Given an input x = (x(1),x(2)) with ground-truth label y \u2208 [K] and the runner-up label j \u0338= y, \u03b6(m)j (x(m)) as the representation margin for m-th modality, satisfying the Lipschitz constraint \u03c4 (m)j , and the fusion factor c (m) j . Define x\n\u2032 = (x\u2032(1),x\u2018(2)) as the perturbed sample, and x\u2212 x\u2032 denotes the perturbation. The bound for the perturbation radius can be described as:\nP (x) = min x\u2032\n\u2225x\u2212 x\u2032\u22252 \u2265 c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j\u221a (c\n(1) j \u03c4 (1) j ) 2 + (c (2) j \u03c4 (2) j ) 2\ns.t. c (1) j \u03b6 (1) j (x \u2032(1)) + c (2) j \u03b6 (2) j (x \u2032(2)) + \u03b2j = 0.\n(13)\nProof. The key to this proof is how to bridge the gap between multi-modal perturbation and the uni-modal representation, by properly introducing the perturbed sample through the condition in Inequality 13 and the margin on perturbation \u03b6(m)j (x\n\u2032(m)). First, considering this constraint, we can obtain the following equation:\nhy(x)\u2212 hj(x) = c(1)j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j\n= c (1) j (\u03b6 (1) j (x (1))\u2212 \u03b6(1)j (x \u2032(1))) + c (2) j (\u03b6 (2) j (x (2))\u2212 \u03b6(2)j (x \u2032(2))),\n(14)\nwhere the formulation of the difference of margin occurs, which can serve as a bond to connect with the perturbation. Then consider the following inequality:\nc (1) j |\u03b6 (1) j (x (1))\u2212 \u03b6(1)j (x \u2032(1))|+ c(2)j |\u03b6 (2) j (x (2))\u2212 \u03b6(2)j (x \u2032(2))|\n\u2264 c(1)j \u03c4 (1) j \u2225\u2225\u2225x(1) \u2212 x\u2032(1)\u2225\u2225\u2225 2 + c (2) j \u03c4 (2) j \u2225\u2225\u2225x(2) \u2212 x\u2032(2)\u2225\u2225\u2225 2 .\n(15)\nOnce the condition \u2200m = 1, 2, \u03b6(m)j (x(m)) \u2265 \u03b6 (m) j (x \u2032(m)) is satisfied, which imposes the minimum requirement on x\u2032, as proven later, we can establish the relationship with the perturbation. Furthermore, by employing the Cauchy Inequality, we obtain the following inequality:\nc (1) j \u03c4 (1) j \u2225\u2225\u2225x(1) \u2212 x\u2032(1)\u2225\u2225\u2225 2 + c (2) j \u03c4 (2) j \u2225\u2225\u2225x(2) \u2212 x\u2032(2)\u2225\u2225\u2225 2 \u2264 \u221a (c (1) j \u03c4 (1) j ) 2 + (c (2) j \u03c4 (2) j )\n2 \u2225x\u2212 x\u2032\u22252 . (16)\nThus, we have successfully established the bound for perturbation. However, two challenges remain unresolved. Firstly, we need to minimize the objective function by finding an optimal perturbation x\u2032. Secondly, the validity of condition \u03b6(m)j (x\n(m)) \u2265 \u03b6(m)j (x\u2032(m)) cannot be guaranteed. In the subsequent sections, we will address these issues and provide formal proofs to support our claims.\nInitially, drawing inspiration from the Inequality 7-16, we perform a straightforward transformation on the Inequality 13, aiming to establish a connection between multi-modal perturbation and uni-modal perturbation:\nmin x\u2032\n\u221a (c\n(1) j \u03c4 (1) j ) 2 + (c (2) j \u03c4 (2) j ) 2 \u2225x\u2212 x\u2032\u22252 \u2265 c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j\ns.t. c (1) j \u03b6 (1) j (x \u2032(1)) + c (2) j \u03b6 (2) j (x \u2032(2)) + \u03b2j = 0.\n(17)\nSubsequently, we arrive at the following inequality:\nmin x\u2032\n\u221a (c\n(1) j \u03c4 (1) j ) 2 + (c (2) j \u03c4 (2) j ) 2 \u2225x\u2212 x\u2032\u22252\n\u2265 min x\u2032(1),x\u2032(2) c (1) j \u03c4 (1) j \u2225\u2225\u2225x(1) \u2212 x\u2032(1)\u2225\u2225\u2225 2 + c (2) j \u03c4 (2) j \u2225\u2225\u2225x(2) \u2212 x\u2032(2)\u2225\u2225\u2225 2\n(Cauchy Inequality)\n\u2265 min x\u2032(1),x\u2032(2) c (1) j |\u03b6 (1) j (x (1))\u2212 \u03b6(1)j (x \u2032(1))|+ c(2)j |\u03b6 (2) j (x (2))\u2212 \u03b6(2)j (x \u2032(2))| (Lipschitz condition)\n= c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j , (\u2217) (18) where the Equation (\u2217) is required to be proved. In the following, we prove this target equation in detail. The main concern is that whether \u03b6(m)j (x\n(m)) \u2265 \u03b6(m)j (x\u2032(m)) holds for each modality m. As the robustness analysis focuses solely on correctly classified samples, where \u2200j \u0338= y, hy(x) > hj(x), we can establish the following inequality:\nc (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j > 0. (19)\nThe condition stated in Inequality 13 can be expressed as follows:\nhy(x \u2032)\u2212 hj(x\u2032) = c(1)j \u03b6 (1) j (x \u2032(1)) + c (2) j \u03b6 (2) j (x \u2032(2)) + \u03b2j = 0. (20) By incorporating Inequality 19 and Equation 20, we can obtain the following inequality:\nc (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) > c (1) j \u03b6 (1) j (x \u2032(1)) + c (2) j \u03b6 (2) j (x \u2032(2)). (21)\nWithout loss of generality, we suppose \u03b6(1)j (x (1)) < \u03b6 (1) j (x \u2032(1)), and we have:\nc (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x \u2032(2)) + \u03b2j < 0. (22)\nHence, since the margin function \u03b6(2)j is continuous, we define an additional variable to describe this process. Let f(\u03bb) = c(1)j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (\u03bbx\n(2) + (1\u2212 \u03bb)x\u2032(2)) + \u03b2j , we can observe that both f(0) and f(1) equal zero. Applying the Zero Existence Theorem, we can always find a 0 < \u03bb\u0302 < 1, and a perturbation x\u0302\u2032(2) = \u03bb\u0302x(2) + (1\u2212 \u03bb\u0302)x\u2032(2). satisfying:\nc (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x\u0302 \u2032(2)) + \u03b2j = 0. (23)\nHence, x\u0302 = (x(1), x\u0302\u2032(2)) is also a feasible perturbation satisfying the condition of Equation 13. And obviously, \u2225x\u2212 x\u0302\u22252 = \u2225(1\u2212 \u03bb\u0302)(x(2) \u2212 x\u2032(2))\u22252 \u2264 \u2225x\u2212 x\u2032\u22252, (24) which contradicts the minimization constraint.\nIn conclusion, we have verify that \u03b6(m)j (x (m)) \u2265 \u03b6(m)j (x\u2032(m)) for each modality m. Thus, we can approach the target equation in Equation 18 by the following equation:\nmin x\u2032(1),x\u2032(2)\nc (1) j |\u03b6 (1) j (x (1))\u2212 \u03b6(1)j (x \u2032(1))|+ c(2)j |\u03b6 (2) j (x (2))\u2212 \u03b6(2)j (x \u2032(2))|\n= min x\u2032(1),x\u2032(2)\nc (1) j (\u03b6 (1) j (x (1))\u2212 \u03b6(1)j (x \u2032(1))) + c (2) j (\u03b6 (2) j (x (2))\u2212 \u03b6(2)j (x \u2032(2)))\n= c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j .\n(25)\nTherefore, combined with Equation 18, we can obtain the target inequality.\nmin x\u2032\n\u2225x\u2212 x\u2032\u22252 \u2265 c (1) j \u03b6 (1) j (x (1)) + c (2) j \u03b6 (2) j (x (2)) + \u03b2j\u221a (c\n(1) j \u03c4 (1) j ) 2 + (c (2) j \u03c4 (2) j ) 2\n. (26)"
        },
        {
            "heading": "7.2 PROOF OF THEOREM 2",
            "text": "Theorem 7.2. Given an input x = (x(1),x(2)) with ground-truth label y \u2208 [K] and the runner-up label j \u0338= y, the orthogonal classifier W\u0303 (m), the modality-specific weight a(m), the Lipschitz constant \u03c4\u0303 (m) j , and the difference of the bias \u03b2\u0303j = b\u0303y \u2212 b\u0303j . The bound for the perturbation radius can be described as:\nP (x) \u2265\n\u22112 m=1 ( a (m) y W\u0303 (m) y. \u03d5(m)(x(m))\u2212 a(m)j W\u0303 (m) j. \u03d5 (m)(x(m)) ) + \u03b2\u0303j\u221a\u22112\nm=1\n( a (m) y \u03c4\u0303 (m) y + a (m) j \u03c4\u0303 (m) j )2 s.t. \u2203j \u0338= y, h\u0303y(x\u2032) = h\u0303j(x\u2032).\n(27)\nProof. For simplify, here we use h\u0303(m)y (x(m)) = a (m) y W\u0303 (m) y. \u03d5(m)(x(m)) as the score of the sample x of y-th class in m-th modality. Hence, we have:\nh\u0303y = h\u0303 (1) y (x (1)) + h\u0303(2)y (x (2)) + by. (28)\nNotice that the numerator term of the lower bound equals to h\u0303y(x) \u2212 h\u0303j(x), combining with the condition, we have:\nh\u0303y(x)\u2212 h\u0303j(x) = h\u0303y(x)\u2212 h\u0303j(x)\u2212 (h\u0303y(x\u2032)\u2212 h\u0303j(x\u2032))\n= 2\u2211 m=1 h\u0303(m)y (x (m))\u2212 h\u0303(m)j (x (m))\u2212 (h\u0303(m)y (x\u2032(m))\u2212 h\u0303 (m) j (x \u2032(m))). (29)\nDenote \u03b3(m)j (x (m)) = h\u0303 (m) y (x(m))\u2212 h\u0303(m)j (x(m)). Inspired by the proof above, we have :\n\u03b3 (m) j (x (m)) \u2265 \u03b3(m)j (x \u2032(m)) (30)\nholds for each modality m, which requires the minimization condition of x\u2032 and will be proved later.\nThus, we can use the Lipschitz condition for j-th class:\n|W\u0303 (m)j. \u03d5 (m)(x(m))\u2212 W\u0303 (m)j. \u03d5 (m)(x\u2032(m))| \u2264 \u03c4\u0303 (m)j \u2225\u2225\u2225x(m) \u2212 x\u2032(m)\u2225\u2225\u2225\n2 . (31)\nand we have:\n2\u2211 m=1 |\u03b3(m)j (x (m))\u2212 \u03b3(m)j (x \u2032(m))|\n= 2\u2211 m=1 \u2223\u2223\u2223h\u0303(m)y (x(m))\u2212 h\u0303(m)y (x\u2032(m))\u2212 h\u0303(m)j (x(m)) + h\u0303(m)j (x\u2032(m))\u2223\u2223\u2223 \u2264\n2\u2211 m=1 \u2223\u2223\u2223a(m)y W\u0303 (m)y. \u03d5(m)(x(m))\u2212 a(m)y W\u0303 (m)y. \u03d5(m)(x\u2032(m))\u2223\u2223\u2223 + \u2223\u2223\u2223a(m)j W\u0303 (m)j. \u03d5(m)(x(m))\u2212 a(m)j W\u0303 (m)j. \u03d5(m)(x\u2032(m))\u2223\u2223\u2223\n\u2264 ( a(1)y \u03c4\u0303 (1) y + a (1) j \u03c4\u0303 (1) j ) \u2225x(1) \u2212 x\u2032(1)\u22252 + ( a(2)y \u03c4\u0303 (2) y + a (2) j \u03c4\u0303 (2) j ) \u2225x(2) \u2212 x\u2032(2)\u22252\n(32)\nThus, we can further solve it by employing the Cauchy Inequality.\n2\u2211 m=1 |\u03b3(m)j (x (m))\u2212 \u03b3(m)j (x \u2032(m))| \u2264 \u221a\u221a\u221a\u221a 2\u2211 m=1 ( a (m) y \u03c4\u0303 (m) y + a (m) j \u03c4\u0303 (m) j )2 \u2225x\u2212 x\u2032\u22252. (33)\nFurther, we verify that when x\u2032 sets the size of perturbation the minimum value, Equation 30 satisfies. The proof is similar to the proof above. As our robustness analysis focuses solely on correctly classified samples, we can establish the following inequality:\n\u2200j \u0338= y, h\u0303y(x)\u2212 h\u0303j(x) = \u03b3(1)j (x (1)) + \u03b3 (2) j (x (2)) > 0 (34)\nWithout loss of generality, we suppose \u03b3(1)j (x) < \u03b3 (1) j (x \u2032). Considering the conditions above, we have:\n\u03b3 (1) j (x (1)) + \u03b3 (2) j (x \u2032(2)) < \u03b3 (1) j (x \u2032(1)) + \u03b3 (2) j (x \u2032(2)) = 0 (35)\nHence, there exist a x\u0302 = (x(1), x\u0302(2)), subject to :\n\u03b3 (1) j (x (1)) + \u03b3 (2) j (x\u0302 (2)) = 0 and \u2225x\u2212 x\u0302\u22252 < \u2225x\u2212 x\u2032\u22252. (36)\nContradict with the assumption. Thus, the proposed bound can be verified.\nP (x) = min x\u2032\n\u2225x\u2212 x\u2032\u22252 \u2265\n\u22112 m=1 ( a (m) y W\u0303 (m) y. \u03d5(m)(x(m))\u2212 a(m)j W\u0303 (m) j. \u03d5 (m)(x(m)) ) + \u03b2\u0303j\u221a\u22112\nm=1\n( a (m) y \u03c4\u0303 (m) y + a (m) j \u03c4\u0303 (m) j )2 s.t. \u2203j \u0338= y, h\u0303y(x\u2032) = h\u0303j(x\u2032).\n(37)"
        },
        {
            "heading": "7.3 SMOOTHING PROCESS",
            "text": "In this section, we provide the smoothing process of the margin regularization. Since the imbalance problem is introduced by modality preference, the unpreferred modality can barely obtain discriminative representation. Hence, we aim to enhance the unpreferred uni-modal representation learning, through enhancing its margin. In detail, we focus on the following objectives:\nmax W\u0303 (m),\u03d5(m) min m;k \u0338=y\nW\u0303 (m)y. \u03d5 (m)(x(m))\u2212 W\u0303 (m)k. \u03d5 (m)(x(m)). (38)\nThis objective aims to improve the learning of the weaker uni-modality by encouraging the correct class and punishing others. For better optimization, we first the smooth minimization among class j through LogSumExp function (Nielsen & Sun, 2016):\nmax W\u0303 (m),\u03d5(m) min m\nW\u0303 (m)y. \u03d5 (m)(x(m))\u2212 log \u2211 k \u0338=y exp(W\u0303 (m) k. \u03d5 (m)(x(m)))  , (39) and further, smooth the minimization among modality m, and transform it into minimization problem:\nmin W\u0303 (m),\u03d5(m)\n\u2212 log\n( 2\u2211\nm=1\nexp(W\u0303 (m) y. \u03d5(m)(x(m)))\u2211\nk \u0338=y exp(W\u0303 (m) k. \u03d5 (m)(x(m)))\n) . (40)\nAbove all, we extend the regularization to the average of all samples and obtain the objective L1:\nmin W\u0303 (m),\u03d5(m)\n1\nN N\u2211 i=1 log\n( 2\u2211\nm=1\n\u2211 k \u0338=y exp(W\u0303 (m) k. \u03d5 (m)(x (m) i ))\nexp(W\u0303 (m) y. \u03d5(m)(x (m) i ))\n) , (41)\nwhich explicitly enlarges the uni-modal representation learning to solve the imbalance problem without changing the fusion factor a(m), which can lead to a better uni-modal representation."
        },
        {
            "heading": "7.4 EXTENSIVE ANALYSIS",
            "text": "Extended to different fusion strategies. In the main manuscript, our analysis mainly focuses on the representative fusion strategy, late fusion, which is widely used in multi-modal research Huang\net al. (2022); Wang et al. (2020). Meanwhile, our method are adaptable to other fusion mechanisms, where the modality-specific representations could interact earlier in the process. Previously, we defined the margin as \u03b6(m)j (x\n(m)), where j is the nearest class to calculate the margin, and the margin is only determined by uni-modal input x(m). To adapt our method for these scenarios including intermediate fusion, we can redefine the representation margin as \u03b6(m)j (x\n(1),x(2)), rather than the previously, indicating that both modalities\u2019 input influence the margin. This modification allows us to extend the framework to measure multi-modal perturbations in a more integrated manner. Additionally, we can adapt the definition of the Lipschitz constant in our theoretical analysis here to measure how the multi-modal perturbation influences the margin:\n|\u03b6(m)j (x (1),x(2))\u2212 \u03b6(m)j (x \u2032(1),x\u2032(2))| \u2264 \u03c4 (m1)j \u2225\u2225\u2225x(1) \u2212 x\u2032(1)\u2225\u2225\u2225\n2 + \u03c4\n(m2) j \u2225\u2225\u2225x(2) \u2212 x\u2032(2)\u2225\u2225\u2225 2 (42)\nwhere \u03c4 (m1)j represents the Lipschitz constant of modality m from modality 1. This constant can reflect how the alteration in modality 1 influences the margin in modality m. Then following the proof in the manuscript, we can observe that:\nc (1) j |\u03b6 (1) j (x (1),x(2))\u2212 \u03b6(1)j (x \u2032(1),x\u2032(2))|+ c(2)j |\u03b6 (2) j (x (1),x(2))\u2212 \u03b6(2)j (x \u2032(1),x\u2032(2))|\n\u2264 (c(1)j \u03c4 (11) j + c (2) j \u03c4 (21) j ) \u2225\u2225\u2225x(1) \u2212 x\u2032(1)\u2225\u2225\u2225 2 + (c (1) j \u03c4 (12) j + c (2) j \u03c4 (22) j ) \u2225\u2225\u2225x(2) \u2212 x\u2032(2)\u2225\u2225\u2225 2 .\n(43)\nThus, we can obtain the perturbation bound in this setting:\nmin x\u2032\n\u2225x\u2212 x\u2032\u22252 \u2265 c (1) j \u03b6 (1) j (x (1),x(2)) + c (2) j \u03b6 (2) j (x (1),x(2)) + \u03b2j\u221a (c\n(1) j \u03c4 (11) j + c (2) j \u03c4 (21) j ) 2 + (c (1) j \u03c4 (12) j + c (2) j \u03c4 (22) j ) 2\nwhere j \u0338= y s.t. c(1)j \u03b6 (1) j (x \u2032(1),x\u2032(2)) + c (2) j \u03b6 (2) j (x \u2032(1),x\u2032(2)) + \u03b2j = 0.\n(44)\nThe idea of the proof is similar to the one in Section 7.1.\nExtended to three modalities. In this study, our analysis discusses the universal situation of two modalities, and can also be extended to the scenario with more than two modalities. To consider more modalities, the key is to introduce the margin of these modalities\u2019s representation. Suppose we have l different modality, the input of the m-th modality is x(m), define the representation margin \u03b6 (m) j (x\n(m)), and the corresponding Lipschitz constant \u03c4 (m)j . Thus, our bound can be extended to the following formulation.\nmin x\u2032\n\u2225x\u2212 x\u2032\u22252 \u2265 \u2211l m=1 c (m) j \u03b6 (m) j (x\n(m)) + \u03b2j\u221a\u2211l m=1(c (m) j \u03c4 (m) j ) 2\nwhere j \u0338= y s.t. l\u2211\nm=1\nc (m) j \u03b6 (m) j (x \u2032(m)) + \u03b2j = 0.\n(45)"
        },
        {
            "heading": "8 ADDITION FOR EXPERIMENT",
            "text": ""
        },
        {
            "heading": "8.1 DETAILS FOR DATASETS",
            "text": "We conduct experiments on three datasets Kinetics-Sounds (Arandjelovic & Zisserman, 2017), UCF101 (Soomro et al., 2012), and VGGSound (Chen et al., 2020). Here are the details.\nKinetics-Sounds (KS) (Arandjelovic & Zisserman, 2017) is a dataset containing 31 human action classes selected from Kinetics dataset (Kay et al., 2017) which contains 400 classes of YouTube videos. All videos are manually annotated for human action using Mechanical Turk and cropped to 10 seconds long around the action. The 31 classes were chosen to be potentially manifested visually and aurally, such as playing various instruments. This dataset contains 22k 10-second video clips, and for each video, we select 3 frames (first, middle, and last frame)\nUCF101 (Soomro et al., 2012) consists of in-the-wild videos from 101 action classes, it is typically regarded as a multi-modal dataset with RGB and optical flow modalities. This dataset contains 13,320 videos of human actions across 101 classes that have previously been used for video synthesis and prediction. For each video, we select the middle frame of RGB and 10 frames of optimal flow for training and prediction. And our backbone is the pre-trained ResNet 18.\nVGGSound(VGGS) (Chen et al., 2020) is a large-scale video dataset that contains 309 classes, covering a wide range of audio events in everyday life. All videos in VGGSound are captured \u201cin the wild\u201d with audio-visual correspondence in the sense that the sound source is visually evident. The duration of each video is 10 seconds. In our experimental settings, 168,618 videos are used for training and validation, and 13,954 videos are used for testing because some videos are not available now on YouTube. For each video, we also select the middle frame for prediction."
        },
        {
            "heading": "8.2 EFFECTIVENESS VALIDATION",
            "text": ""
        },
        {
            "heading": "8.2.1 VALIDATION ACROSS VARIOUS MODELS",
            "text": "CRMT with different backbones To evaluate the adaptability and effectiveness of our method across diverse frameworks, we conduct comprehensive experiments on the Kinetic-Sounds dataset utilizing different backbone architectures. These architectures encompass the use of ResNet34 for both Vision (V) and Audio (A) modalities, as well as an integrated approach combining ResNet18 for Vision (V) and a Transformer model for Audio (A). The results in Table 4 of these experiments demonstrate both the improvement and flexibility of our method when implemented across different backbones.\nCRMT with intermediate fusion strategies. In this manuscript, our analysis mainly focuses on the widely used late fusion method. To further validate our effectiveness, we delve into a widely used intermediate fusion strategy, the Multi-Modal Transfer Module (MMTM) (Vaezi Joze et al., 2020), in which the interaction occurs among representations at mid-level. We compare MMTM and the one that implements our method (CRMT-MMTM) on the Kinetic-Sounds dataset. The experimental results in Table 5 indicate that our method can also boost the performance and robustness for more multi-modal fusion strategies.\nCRMT on pre-trained transformer. In the primary manuscript, we have verified the performance of our method using transformers trained from scratch. To extend the validation of our approach\u2019s effectiveness, we have conducted additional experiments utilizing an ImageNet-pretrained Transformer applied to the Kinetic-Sounds dataset. The findings of this extended analysis, detailed in Table 6, demonstrate that our method retains its effectiveness even when implemented with a pre-trained model. This further verifies the robustness and adaptability of our approach in diverse training scenarios."
        },
        {
            "heading": "8.2.2 VALIDATION UNDER DIFFERENT SETTING",
            "text": "Robustness evaluation under various multi-modal attack. In the Experiment section, we initially describe two multi-modal attack methods: Fast Gradient Method (FGM) and Projected Gradient Descent with \u21132 norm (PGD-\u21132). To further substantiate the robustness of our approach, this section introduces and evaluates three additional multi-modal attack methods. Firstly, we implement the Multi-modal Embedding Attack (MEA) method as proposed by Zhang et al. (2022), which focuses on shifting the joint representation rather than altering the prediction output. Secondly, we explore the efficacy of our method against the Multi-modal Gaussian Noise (MGN) and the Multi-modal Pixel Missing (MPM) attacks. The results in Table 7 of these expanded experiments demonstrate that our methodology is not only robust to the previously examined attacks (FGM and PGD-\u21132) but also capable of defending various multi-modal attacks.\nExtended to the UCF dataset with three modalities. During our analysis and experiments, we consider the universal situation containing two modalities. Our method can also be extended to the scenario with more than two modalities. We conduct experiments on the UCF101 dataset using three modalities: RGB, Optical Flow, and RGB Frame Difference. We compared the performance of methods for both trained from scratch and with pre-training ImageNet-pretrained ResNet18. The experimental results detailed in Table 8. The outcomes demonstrate the effectiveness of our method in enhancing multi-modal robustness in more than two modalities.\nExperiment on Image-Text dataset. We apply experiments on vision-text classification tasks to verify our effectiveness. We utilize a widely used Food101 dataset, which inputs an image-text pair for classification. We employ a Vision Transformer (ViT) as our image encoder and BERT as our text encoder, subsequently concatenating their outputs to achieve a unified joint representation. To evaluate robustness, we apply multiple attacks including modality missing and descent-based attacks (FGM and PGD-\u21132). It is important to note that attacks like the Fast Gradient Method (FGM) and Projected Gradient Descent with \u21132 norm (PGD-\u21132) are typically applied to continuous data. Given that text represents discontinuous data, we focus on implementing these attack methods on the image modality. Our results reveal that the text modality is more critical than the image modality, as its absence significantly impacts model performance, as shown in Table 9. Concentrating on the \u21132-norm, we achieve enhanced robustness under both FGM and PGD-\u21132 attack. Our method demonstrates a notable performance increase in scenarios where text is absent, though there is a slight decline\nin performance when the image modality is missing. This is attributed to the huge performance difference between text and image.\nComparison with additional robust methods In addition to the comparative analyses presented in the Experiment section, we further extend the validation of our method by contrasting it with additional multi-modal robustness approaches. Specifically, we compare our approach with two recent methods: Robust Multi-Task Learning (RMTL) as described by Ma et al. (2022) and Uni-Modal Ensemble with Missing Modality Adaptation (UME-MMA) proposed by Li et al. (2023). For the RMTL implementation, we adopt a multi-task learning framework to develop a robust multi-modal model, incorporating both full-modal and modality-specific tasks. These methods are applied and compared to the Kinetic-Sounds dataset. As illustrated in Table 10, our method demonstrates superior performance in scenarios involving both modality absence and targeted attacks. These results present the robustness and effectiveness of our approach and the validity of our proposed analysis."
        },
        {
            "heading": "8.3 ITERATIONS IN TRAINING PROCEDURE",
            "text": "In the method section\u2019s concluding part, we currently perform two sequential steps at once. However, it is worth noting that in various applications, incorporating more iterations in the training procedure is a common practice, which holds the potential to further enhance the robustness of our method. To validate the validity of our approach, we conducted experiments by extending our method to incorporate more iterations. As depicted in Table 11, the results of employing additional iterations demonstrate improved robustness capabilities. But only one iteration of our method can achieve considerable robustness, without a huge consumption of training cost."
        },
        {
            "heading": "8.4 EXPERIMENTS ABOUT MODALITY PREFERENCE",
            "text": ""
        },
        {
            "heading": "8.4.1 VERIFICATION OF VULNERABLE MODALITY",
            "text": "As shown in Figure 3 in the manuscript, we demonstrate how the ratio of the vulnerability indicator (\u03b7) varies in our method. Furthermore, to clearly explain this phenomenon, in Figure 8 (a, b), we provide the heat map of the indicator \u03b7 to represent the robustness of each uni-modality, where the smaller indicator means the modality is more robust. Meanwhile, we also provide the uni-modal perturbation radius to further verify this modality preference, where we list the percentage of safe samples that can always resist this size of perturbation, as shown in Figure 8 (c). It can be seen that\nwhen joint training is applied to the Kinetic-Sounds dataset, the audio modality is definitely more vulnerable than the vision modality, thus explaining the phenomenon in Figure 1 in the paper."
        },
        {
            "heading": "8.4.2 UNI-MODAL REPRESENTATION ANALYSIS",
            "text": "In this part, we verify the imbalanced problem brought by modality preference, which results in the low quality of uni-modal representation. Here, we apply t-distributed Stochastic Neighbor Embedding (t-SNE) method on each uni-modal representation and illustrate the visualization of uni-modal representation on KS dataset. As shown in Figure 7, in JT, MMAT, and Mixup, the quality of representation in modality #a is better than modality #v, which implies the margins of representation #v are relatively small. Moreover, by applying our training procedure (see CRMT-JT, CRMT-AT, CRMT-Mix), representations of both modalities show good discrimination, indicating that we enlarge the uni-modal representation margins and eventually enhance the robustness."
        },
        {
            "heading": "8.4.3 COMPLEMENT FOR ABLATION STUDIES",
            "text": "As shown in Figure 5, we verify the effectiveness of each step of our training process. Here we provide more evidence that the improved multi-modal robustness is because we alleviate the influence caused by modality preference. Similar to the ablation studies, we report our findings related to solely\nusing the orthogonal framework (OJT), our CRMT focusing exclusively on each step (CRMT-step-1 and CRMT-step-2), and our full method, CRMT-JT, which incorporates both steps. We also apply the ratio of uni-modal robustness indicators, to show the difference of resistance against uni-modal attack. From these heat maps of the ratio in Figure 8, we have the following observations. First, since JT and OJT are different frameworks, their imbalance problem occurs in different class pairs (i.e. entries in the heat map). And just applying the orthogonal framework could hardly solve the imbalance problem on the robustness indicator. Second, since the uni-modal representation may not be well learned, just applying step-2 of CRMT only brings marginal improvement. Third, CRMT step-1 can also alleviate the imbalance problem, since it enhances the learning of modality with less discriminative ability, and makes different modality representations more balanced. Fourth, equipped with both enlarging uni-modal representation margin and adjusting integration factors, our method can progressively alleviate the imbalanced problem in attack with different modalities, which further contributes to better robustness."
        }
    ],
    "year": 2024
}