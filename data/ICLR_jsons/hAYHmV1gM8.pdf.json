{
    "abstractText": "Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to nonindependent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients\u2019 data originate from diverse domains with distinct feature distributions, instead of label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while existing normalization techniques possess their own limitations. In order to address these issues, FedWon eliminates the normalization layers in FL and reparameterizes convolution layers with scaled weight standardization. Through extensive experimentation on five datasets and five models, our comprehensive experimental results demonstrate that FedWon surpasses both FedAvg and the current state-of-the-art method (FedBN) across all experimental setups, achieving notable accuracy improvements of more than 10% in certain domains. Furthermore, FedWon is versatile for both cross-silo and cross-device FL, exhibiting robust domain generalization capability, showcasing strong performance even with a batch size as small as 1, thereby catering to resource-constrained devices. Additionally, FedWon can also effectively tackle the challenge of skewed label distribution.",
    "authors": [
        {
            "affiliations": [],
            "name": "Weiming Zhuang"
        },
        {
            "affiliations": [],
            "name": "Lingjuan Lyu"
        }
    ],
    "id": "SP:bb6d8a90e9473cd30ac2b552dd7e401e03234a98",
    "references": [
        {
            "authors": [
                "Mathieu Andreux",
                "Jean Ogier du Terrail",
                "Constance Beguier",
                "Eric W. Tramel"
            ],
            "title": "Siloed federated learning for multi-centric histopathology datasets. In Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning, pp. 129\u2013139",
            "year": 2020
        },
        {
            "authors": [
                "Tobias Bernecker",
                "Annette Peters",
                "Christopher L Schlett",
                "Fabian Bamberg",
                "Fabian Theis",
                "Daniel Rueckert",
                "Jakob Wei\u00df",
                "Shadi Albarqouni"
            ],
            "title": "Fednorm: Modality-based normalization in federated learning for multi-modal liver",
            "venue": "segmentation. arXiv preprint arXiv:2205.11096,",
            "year": 2022
        },
        {
            "authors": [
                "Nils Bjorck",
                "Carla P Gomes",
                "Bart Selman",
                "Kilian Q Weinberger"
            ],
            "title": "Understanding batch normalization",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Brock",
                "Soham De",
                "Samuel L Smith"
            ],
            "title": "Characterizing signal propagation to close the performance gap in unnormalized resnets",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Andy Brock",
                "Soham De",
                "Samuel L Smith",
                "Karen Simonyan"
            ],
            "title": "High-performance large-scale image recognition without normalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bruno Casella",
                "Roberto Esposito",
                "Antonio Sciarappa",
                "Carlo Cavazzoni",
                "Marco Aldinucci"
            ],
            "title": "Experimenting with normalization layers in federated learning on non-iid scenarios",
            "venue": "arXiv preprint arXiv:2303.10630,",
            "year": 2023
        },
        {
            "authors": [
                "Woong-Gi Chang",
                "Tackgeun You",
                "Seonguk Seo",
                "Suha Kwak",
                "Bohyung Han"
            ],
            "title": "Domain-specific batch normalization for unsupervised domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Chen Chen",
                "Yuchen Liu",
                "Xingjun Ma",
                "Lingjuan Lyu"
            ],
            "title": "Calfat: Calibrated federated adversarial training with label skewness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jinghui Chen",
                "Dongruo Zhou",
                "Yiqi Tang",
                "Ziyan Yang",
                "Yuan Cao",
                "Quanquan Gu"
            ],
            "title": "Closing the generalization gap of adaptive gradient methods in training deep neural networks",
            "venue": "arXiv preprint arXiv:1806.06763,",
            "year": 2018
        },
        {
            "authors": [
                "Noel CF Codella",
                "David Gutman",
                "M Emre Celebi",
                "Brian Helba",
                "Michael A Marchetti",
                "Stephen W Dusza",
                "Aadi Kalloo",
                "Konstantinos Liopyris",
                "Nabin Mishra",
                "Harald Kittler"
            ],
            "title": "Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)",
            "venue": "IEEE 15th international symposium on biomedical imaging (ISBI",
            "year": 2018
        },
        {
            "authors": [
                "Marc Combalia",
                "Noel CF Codella",
                "Veronica Rotemberg",
                "Brian Helba",
                "Veronica Vilaplana",
                "Ofer Reiter",
                "Cristina Carrera",
                "Alicia Barreiro",
                "Allan C Halpern",
                "Susana Puig"
            ],
            "title": "Dermoscopic lesions in the wild",
            "venue": "arXiv preprint arXiv:1908.02288,",
            "year": 2000
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Soham De",
                "Sam Smith"
            ],
            "title": "Batch normalization biases residual blocks towards the identity function in deep networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhixu Du",
                "Jingwei Sun",
                "Ang Li",
                "Pin-Yu Chen",
                "Jianyi Zhang",
                "Hai\u201d Helen\u201d Li",
                "Yiran Chen"
            ],
            "title": "Rethinking normalization methods in federated learning",
            "venue": "In Proceedings of the 3rd International Workshop on Distributed Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Victor Lempitsky"
            ],
            "title": "Unsupervised domain adaptation by backpropagation",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Boqing Gong",
                "Yuan Shi",
                "Fei Sha",
                "Kristen Grauman"
            ],
            "title": "Geodesic flow kernel for unsupervised domain adaptation",
            "venue": "In 2012 IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Gregory Griffin",
                "Alex Holub",
                "Pietro Perona"
            ],
            "title": "Caltech-256 object category dataset",
            "year": 2007
        },
        {
            "authors": [
                "Boris Hanin",
                "David Rolnick"
            ],
            "title": "How to start training: The effect of initialization and architecture",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Hard",
                "Kanishka Rao",
                "Rajiv Mathews",
                "Swaroop Ramaswamy",
                "Fran\u00e7oise Beaufays",
                "Sean Augenstein",
                "Hubert Eichner",
                "Chlo\u00e9 Kiddon",
                "Daniel Ramage"
            ],
            "title": "Federated learning for mobile keyboard prediction",
            "venue": "arXiv preprint arXiv:1811.03604,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kevin Hsieh",
                "Amar Phanishayee",
                "Onur Mutlu",
                "Phillip Gibbons"
            ],
            "title": "The non-iid data quagmire of decentralized machine learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan J. Hull"
            ],
            "title": "A database for handwritten text recognition research",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 1994
        },
        {
            "authors": [
                "Sergey Ioffe"
            ],
            "title": "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank Reddi",
                "Sebastian Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wenqi Li",
                "Fausto Milletar\u0131",
                "Daguang Xu",
                "Nicola Rieke",
                "Jonny Hancox",
                "Wentao Zhu",
                "Maximilian Baust",
                "Yan Cheng",
                "S\u00e9bastien Ourselin",
                "M Jorge Cardoso"
            ],
            "title": "Privacy-preserving federated brain tumour segmentation",
            "venue": "In International Workshop on Machine Learning in Medical Imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoxiao Li",
                "Meirui Jiang",
                "Xiaofei Zhang",
                "Michael Kamp",
                "Qi Dou"
            ],
            "title": "Fedbn: Federated learning on non-iid features via local batch normalization",
            "venue": "arXiv preprint arXiv:2102.07623,",
            "year": 2021
        },
        {
            "authors": [
                "Yanghao Li",
                "Naiyan Wang",
                "Jianping Shi",
                "Jiaying Liu",
                "Xiaodi Hou"
            ],
            "title": "Revisiting batch normalization for practical domain adaptation",
            "venue": "arXiv preprint arXiv:1603.04779,",
            "year": 2016
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Wang Lu",
                "Jindong Wang",
                "Yiqiang Chen",
                "Xin Qin",
                "Renjun Xu",
                "Dimitrios Dimitriadis",
                "Tao Qin"
            ],
            "title": "Personalized federated learning with adaptive batchnorm for healthcare",
            "venue": "IEEE Transactions on Big Data,",
            "year": 2022
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "year": 2011
        },
        {
            "authors": [
                "Anh Nguyen",
                "Tuong Do",
                "Minh Tran",
                "Binh X Nguyen",
                "Chien Duong",
                "Tu Phan",
                "Erman Tjiputra",
                "Quang D Tran"
            ],
            "title": "Deep federated learning for autonomous driving",
            "venue": "arXiv preprint arXiv:2110.05754,",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "Matthias Paulik",
                "Matt Seigel",
                "Henry Mason",
                "Dominic Telaar",
                "Joris Kluivers",
                "Rogier van Dalen",
                "Chi Wai Lau",
                "Luke Carlson",
                "Filip Granqvist",
                "Chris Vandevelde"
            ],
            "title": "Federated evaluation and tuning for on-device personalization",
            "venue": "System design & applications. arXiv preprint arXiv:2102.08503,",
            "year": 2021
        },
        {
            "authors": [
                "Xingchao Peng",
                "Qinxun Bai",
                "Xide Xia",
                "Zijun Huang",
                "Kate Saenko",
                "Bo Wang"
            ],
            "title": "Moment matching for multi-source domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Jason Posner",
                "Lewis Tseng",
                "Moayad Aloqaily",
                "Yaser Jararweh"
            ],
            "title": "Federated learning in vehicular networks: opportunities and solutions",
            "venue": "IEEE Network,",
            "year": 2021
        },
        {
            "authors": [
                "Kate Saenko",
                "Brian Kulis",
                "Mario Fritz",
                "Trevor Darrell"
            ],
            "title": "Adapting visual category models to new domains",
            "venue": "In Computer Vision\u2013ECCV 2010: 11th European Conference on Computer Vision,",
            "year": 2010
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Andrew Ilyas",
                "Aleksander Madry"
            ],
            "title": "How does batch normalization help optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Yiqing Shen",
                "Yuyin Zhou",
                "Lequan Yu"
            ],
            "title": "Cd2-pfed: Cyclic distillation-guided channel decoupling for model personalization in federated learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Benyuan Sun",
                "Hongxing Huo",
                "Yi Yang",
                "Bo Bai"
            ],
            "title": "Partialfed: Cross-domain personalized federated learning via partial initialization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yue Tan",
                "Chen Chen",
                "Weiming Zhuang",
                "Xin Dong",
                "Lingjuan Lyu",
                "Guodong Long"
            ],
            "title": "Is heterogeneity notorious? taming heterogeneity to handle test-time shift in federated learning",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Jean Ogier du Terrail",
                "Samy-Safwan Ayed",
                "Edwige Cyffers",
                "Felix Grimberg",
                "Chaoyang He",
                "Regis Loeb",
                "Paul Mangold",
                "Tanguy Marchand",
                "Othmane Marfoq",
                "Erum Mushtaq"
            ],
            "title": "Flamby: Datasets and benchmarks for cross-silo federated learning in realistic healthcare settings",
            "venue": "arXiv preprint arXiv:2210.04620,",
            "year": 2022
        },
        {
            "authors": [
                "Philipp Tschandl",
                "Cliff Rosendahl",
                "Harald Kittler"
            ],
            "title": "The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions",
            "venue": "Scientific data,",
            "year": 2018
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Yanmeng Wang",
                "Qingjiang Shi",
                "Tsung-Hui Chang"
            ],
            "title": "Why batch normalization damage federated learning on non-iid data",
            "venue": "arXiv preprint arXiv:2301.02982,",
            "year": 2023
        },
        {
            "authors": [
                "Chun-Han Yao",
                "Boqing Gong",
                "Hang Qi",
                "Yin Cui",
                "Yukun Zhu",
                "Ming-Hsuan Yang"
            ],
            "title": "Federated multi-target domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Fisher Yu",
                "Haofeng Chen",
                "Xin Wang",
                "Wenqi Xian",
                "Yingying Chen",
                "Fangchen Liu",
                "Vashisht Madhavan",
                "Trevor Darrell"
            ],
            "title": "Bdd100k: A diverse driving dataset for heterogeneous multitask learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Yann N Dauphin",
                "Tengyu Ma"
            ],
            "title": "Fixup initialization: Residual learning without normalization",
            "venue": "arXiv preprint arXiv:1901.09321,",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Jan Bosch",
                "Helena Holmstr\u00f6m Olsson"
            ],
            "title": "End-to-end federated learning for autonomous driving vehicles",
            "venue": "In 2021 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2021
        },
        {
            "authors": [
                "Jie Zhang",
                "Chen Chen",
                "Weiming Zhuang",
                "Lingjuan Lv"
            ],
            "title": "Addressing catastrophic forgetting in federated class-continual learning",
            "venue": "arXiv preprint arXiv:2303.06937,",
            "year": 2023
        },
        {
            "authors": [
                "Yue Zhao",
                "Meng Li",
                "Liangzhen Lai",
                "Naveen Suda",
                "Damon Civin",
                "Vikas Chandra"
            ],
            "title": "Federated learning with non-iid data",
            "venue": "CoRR, abs/1806.00582,",
            "year": 2018
        },
        {
            "authors": [
                "Jike Zhong",
                "Hong-You Chen",
                "Wei-Lun Chao"
            ],
            "title": "Making batch normalization great in federated deep learning",
            "venue": "arXiv preprint arXiv:2303.06530,",
            "year": 2023
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Yonggang Wen",
                "Xuesen Zhang",
                "Xin Gan",
                "Daiying Yin",
                "Dongzhan Zhou",
                "Shuai Zhang",
                "Shuai Yi"
            ],
            "title": "Performance optimization of federated person re-identification via benchmark analysis",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Xin Gan",
                "Yonggang Wen",
                "Shuai Zhang",
                "Shuai Yi"
            ],
            "title": "Collaborative unsupervised visual representation learning from decentralized data",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Xin Gan",
                "Yonggang Wen",
                "Shuai Zhang"
            ],
            "title": "Easyfl: A low-code federated learning platform for dummies",
            "venue": "IEEE Internet of Things Journal,",
            "year": 2022
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Xin Gan",
                "Yonggang Wen",
                "Shuai Zhang"
            ],
            "title": "Optimizing performance of federated person re-identification: Benchmarking and analysis",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM),",
            "year": 2022
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Yonggang Wen",
                "Shuai Zhang"
            ],
            "title": "Divergence-aware federated self-supervised learning",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Lin"
            ],
            "title": "2017) and data augmentations. Table 11 shows the testing accuracy of FedAvg, FedBN, and our proposed FedWon across the six healthcare center domains. In this challenging setting, FedBN only achieves similar performance to FedAvg. In contrast, FedWon outperforms both FedAvg and FedBN in all domains by a significant margin. The results are inspiring and demonstrates the potential of deploying FedWon to healthcare",
            "year": 2017
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2021). However, our proposed FedWon achieves even better performance than FedBN on all domains and all levels of heterogeneity",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Federated learning (FL) has emerged as a promising method for distributed machine learning, enabling in-situ model training on decentralized client data. It has been widely adopted in diverse applications, including healthcare (Li et al., 2019; Bernecker et al., 2022), mobile devices (Hard et al., 2018; Paulik et al., 2021), and autonomous vehicles (Zhang et al., 2021; Nguyen et al., 2021; Posner et al., 2021). However, FL commonly suffers from statistical heterogeneity, where the data distributions across clients are non-independent and identically distributed (non-i.i.d) (Li et al., 2020a). This is due to the fact that data generated from different clients is highly likely to have different data distributions, which can cause performance degradation (Zhao et al., 2018; Hsieh et al., 2020; Tan et al., 2023) even divergence in training (Zhuang et al., 2020; 2022b; Wang et al., 2023).\nThe majority of studies that address the problem of non-i.i.d data focus on the issue of skewed label distribution, where clients have different label distributions (Li et al., 2020b; Hsieh et al., 2020; Wang et al., 2020; Chen et al., 2022). However, multi-domain FL, where clients\u2019 data are from different domains, has received less attention, despite its practicality in reality. Figure 1a depicts two practical examples of multi-domain FL. For example, multiple autonomous cars may collaborate on model training, but their data could originate from different weather conditions or times of day, leading to domain gaps in collected images (Cordts et al., 2016; Yu et al., 2020). Similarly, multiple healthcare institutions collaborating on medical imaging analysis may face significant domain gaps due to variations in imaging machines and protocols (Bernecker et al., 2022). Developing effective solutions for multi-domain FL is a critical research problem with broad implications.\n1\nClient 1: T1 Tumor Client 2: T2 Tumor Client K: Flair Tumor"
        },
        {
            "heading": "Autonomous Driving",
            "text": "Hospitals\nHowever, the existing solutions are unable to adequately address the problem of multi-domain FL. FedBN (Li et al., 2021) attempts to solve this problem by keeping batch normalization (BN) parameters and statistics (Ioffe & Szegedy, 2015) locally in the client, but it is only suitable for cross-silo FL (Kairouz et al., 2021), where clients are organizations like healthcare institutions, because it requires clients to be stateful (Karimireddy et al., 2020) (keeping states of BN information) and participate training in every round. As a result, FedBN is not suitable for cross-device FL, where the clients are stateless and only a fraction of clients participate in training. Besides, BN relies on the assumption that training data are from the same distribution, ensuring the mean and variance of each mini-batch are representative of the entire data distribution (Ioffe & Szegedy, 2015). Figure 1b shows that the running means and variances of BNs differ significantly between two FL clients from different domains, as well as between the server and clients (statistics of all BN layers are in Figure 12 in Appendix). Alternative normalizations like Layer Norm (Ba et al., 2016) and Group Norm (Wu & He, 2018) have not been studied for multi-domain FL, but they have limitations like requiring extra computation in inference.\nThis paper explores a fundamentally different approach to address multi-domain FL. Given that BN struggles to capture multi-domain data and other normalizations come with their own limitations, we further ask the question: is normalization indispensable to learning a general global model for multi-domain FL? In recent studies, normalization-free ResNets (Brock et al., 2021a) demonstrates comparable performance to standard ResNets(He et al., 2016). Inspired by these findings, we build upon this methodology and explore its untapped potential within the realm of multi-domain FL.\nWe introduce Federated learning Without normalizations (FedWon) to address the domain discrepancies among clients in multi-domain FL. FedWon follows FedAvg (McMahan et al., 2017) protocols for server aggregation and client training. Unlike existing methods, FedWon removes normalization layers (e.g., BN layers), and reparameterizes convolution layers with Scaled Weight Standardization (Brock et al., 2021a). We conduct extensive experiments on five datasets using five models. The experimental results indicate that FedWon outperforms state-of-the-art methods on all datasets and models. The general global model trained by FedWon can achieve more than 10% improvement on certain domains compared to the personalized models from FedBN (Li et al., 2021). Moreover, our empirical evaluation demonstrated three key benefits of FedWon: 1) FedWon is versatile to support both cross-silo and cross-device FL; 2) FedWon achieves competitive performance on small batch sizes (even on a batch size of 1), which is particularly useful for resource-constrained devices; 3) FedWon can also be applied to address the skewed label distribution problem.\nPublished as a conference paper at ICLR 2024\n3\nConv1 BN1 Client 1\nConv2\nBN2\nClient 2\nConv\nBN\nConv BN Client 1\nConv\nBN\nClient 2 Server Aggregation\nConv1 BN1 Client 1\nConv2\nBN2\nClient 2\nConv\nConv BN1 Client 1\nConv\nBN2 Client 2 Server Aggregation\nWSConv1 Client 1\nWSConv2\nClient 2\nWSConv\nWSConv Client 1\nWSConv\nClient 2 Server Aggregation\nConv1\nClient 1\nConv2\nClient 2\nConv\nConv\nClient 1\nConv\nClient 2 Server Aggregation\nConv1\nBN1 Client 1\nBN2\nClient 2\nConv\nConv\nBN1 Client 1\nBN2 Client 2 Server Aggregation\nConv1\nBN1 Client 1\nConv2\nBN2\nClient 2\nConv\nBN\nConv\nBN\nClient 1\nConv\nBN\nClient 2 Server Aggregation\nConv1\nBN1 Client 1\nConv2\nBN2\nClient 2\nConv\nConv\nBN1 Client 1\nConv\nBN2 Client 2 Server Aggregation\n3\nConv1 BN1 Client 1\nConv2\nBN2\nClient 2\nConv\nBN\nConv BN Client 1\nConv\nBN\nClient 2 Server Ag regation\nConv1 BN1 Client 1\nConv2\nBN2\nClient 2\nConv\nConv BN1 Client 1\nConv\nBN2 Client 2 Server Ag regation\nWSConv1 Client 1\nWSConv2\nClient 2\nWSConv\nWSConv Client 1\nWSConv\nClient 2 Server Ag regation\nConv1\nClient 1\nConv2\nClient 2\nConv\nConv\nClient 1\nConv\nClient 2 Server Aggregation\nConv1\nBN1 Client 1\nBN2\nClient 2\nConv\nConv\nBN1 Client 1\nBN2 Client 2 Server Aggregation\nConv1\nBN1 Client 1\nConv2\nBN2\nClient 2\nConv\nBN\nConv\nBN\nClient 1\nConv\nBN\nClient 2 Server Ag regation\nConv1\nBN1 Client 1\nConv2\nBN2\nClient 2\nConv\nConv\nBN1 Client 1\nConv\nBN2 Client 2"
        },
        {
            "heading": "Server",
            "text": "Ag regation\n3\nConv1\nBN1 Client 1\nConv2\nConv\nBN\nConv\nBN\nClient 1\nConv\nConv1\nBN1 Client 1\nConv\nConv\nConv\nBN1 Client 1\nConv\nWSConv1 Client 1\nWSConv2\nClient 2\nWSConv\nWSConv\nClient 1\nWSConv\nClient 2 Server Aggregation\nConv2\nClient 2\nConv\nConv\nClient 2 Server Aggregation\nConv1\nBN1 Client 1\nConv2\nBN2\nClient 2\nConv\nConv\nBN1 Client 1\nConv\nBN2 Client 2 Server Aggregation\nConv1\nBN1 Client 1\nConv2\nBN2\nClient 2\nConv\nBN\nConv\nBN\nClient 1\nConv\nBN\nClient 2 Server Aggregation\nConv1\nBN1 Client 1\nConv2\nBN2\nClient 2\nConv\nConv\nBN1 Client 1\nConv\nBN2 Client 2 Server Aggregation\nIn summary, our contributions are as follows:\n\u2022 We introduce FedWon, a simple yet effective method for multi-domain FL. By removing all normalization layers and using scaled weight standardization, FedWon is able to learn a general global model from clients with significant domain discrepancies.\n\u2022 To the best of our knowledge, FedWon is the first method that enables both cross-silo and cross-device FL without relying on any form of normalization. Our study also reveals the unexplored benefits of this method, particularly in the context of multi-domain FL.\n\u2022 Extensive experiments demonstrate that FedWon outperforms state-of-the-art methods on all the evaluated datasets and models, and is suitable for training with small batch sizes, which is especially beneficial for cross-device FL in practice."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "Before diving into the benefits brought by removing normalizations, we first introduce FL with batch normalization. Then, we review alternative normalization methods and normalization-free networks."
        },
        {
            "heading": "2.1 FEDERATED LEARNING WITH BATCH NORMALIZATION",
            "text": "Batch normalization (BN) (Ioffe & Szegedy, 2015), commonly used as a normalization layer, has been a fundamental component in deep neural networks (DNN). The BN operation is defined as:\nBN(x) = \u03b3 x\u2212 \u00b5\u221a \u03c32 + \u03f5 + \u03b2, (1)\nwhere mean \u00b5 and variance \u03c3 are computed from a mini-batch of data, and \u03b3 and \u03b2 are two learnable parameters. The term \u03f5 is a small positive value that is added for numerical stability.\nBN offers several benefits, including reducing internal covariate shift, stabilizing training, and accelerating convergence (Ioffe & Szegedy, 2015). Moreover, it is more robust to hyperparameters (Bjorck et al., 2018) and has smoother optimization landscapes (Santurkar et al., 2018). However, its effectiveness is based on the assumption that the training data is from the same domain, such that the mean \u00b5 and variance \u03c3 computed from a mini-batch are representative of the training data (Ioffe & Szegedy, 2015). In centralized training, BN has been found to struggle with modeling the statistics from multiple domains, leading to the development of domain-specific BN techniques (Li et al., 2016; Chang et al., 2019). Similarly, in multi-domain FL, DNN with BN can encounter difficulties in capturing the statistics of multiple domains while training a single global model.\nFederated learning (FL) trains machine learning models collaboratively from decentralized clients, coordinated by a central server (Kairouz et al., 2021). It enhances data privacy by keeping the raw data locally on clients. FedAvg (McMahan et al., 2017) is the most popular FL algorithm. A common issue in FL is non-i.i.d data across clients, which could lead to performance degradation and difficulties in convergence (Hsieh et al., 2020; Zhuang et al., 2021; 2022c; Wang et al., 2023).\nSkewed label distribution, where clients have different label distributions, is a widely discussed noni.i.d. problem with numerous proposed solutions (Li et al., 2020b; Zhang et al., 2023; Chen et al., 2022). To address this problem, multiple works provide solutions that introduce special operations on BN to personalize a model for each client (Lu et al., 2022). For example, SiloBN (Andreux et al., 2020) keeps BN statistics locally in clients. FixBN (Zhong et al., 2023) only trains BN statistics in the first stage of training and freezes them thereafter. FedTAN (Wang et al., 2023) tailors for BN by performing iterative layer-wise aggregations, introducing numerous extra communication rounds.\nIn contrast, multi-domain FL, where the data domains differ across clients, has received less attention (Chen et al., 2018; Shen et al., 2022). FedBN (Li et al., 2021) and FedNorm (Bernecker et al., 2022) addresses this issue by locally keeping the BN layers in clients and aggregating only the other model parameters. PartialFed (Sun et al., 2021) keeps model initialization strategies in clients and use them to load models in new training rounds. While these methods excel in cross-silo FL, where clients are stable and can retain statefulness, they are unsuitable for cross-device FL by design. In the latter scenario, clients are stateless, and only a fraction of clients participate in each round of training (Kairouz et al., 2021). Besides, FMTDA (Yao et al., 2022) adapts source domain data in the server to target domains in clients, whereas we do not assume availablility of data in the server."
        },
        {
            "heading": "2.2 ALTERNATIVE NORMALIZATION METHODS",
            "text": "BN has shown to be effective in modern DNNs (Ioffe & Szegedy, 2015), but it also has limitations in various scenarios. For example, BN struggles to model statistics of training data from multiple domains (Li et al., 2016; Chang et al., 2019), and it may not be suitable for small batch sizes (Ioffe, 2017; Wu & He, 2018). Researchers have proposed alternative normalizations such as Group Norm (Wu & He, 2018) and Layer Norm (Ba et al., 2016). Although these methods remove some of the constraints of BN, they come with their own limitations. For example, they require additional computation during inference, making them less practical for edge deployment.\nRecent studies have shown that BN may not work well in FL under non-i.i.d data (Hsieh et al., 2020), due to external covariate shift (Du et al., 2022) and mismatch between local and global statistics (Wang et al., 2023). Instead, researchers have adopted alternative normalizations such as GN (Hsieh et al., 2020; Casella et al., 2023) or LN (Du et al., 2022; Casella et al., 2023) to mitigate the problem. However, these methods inherit the limitations of GN and LN in centralized training, and the recent study by Zhong et al. (2023) shows that BN and GN have no consistent winner in FL."
        },
        {
            "heading": "2.3 NORMALIZATION-FREE NETWORKS",
            "text": "Several attempts have been made to remove normalization from DNNs in centralized training using weight initialization methods (Hanin & Rolnick, 2018; Zhang et al., 2019; De & Smith, 2020). Recently, Brock et al. (2021a) proposed a normalization-free network by analyzing the signal propagation through the forward pass of the network. Normalization-free network stabilizes training with scaled weight standardization that reparameterizes the convolution layer to prevent the mean shift in the hidden activations (Brock et al., 2021a). This approach achieves competitive performance compared to networks with BN on ResNet (He et al., 2016) and EfficientNet (Tan & Le, 2019). Building on this work, Brock et al. further introduced an adaptive gradient clipping (AGC) method that enables training normalization-free networks with large batch sizes (Brock et al., 2021b)."
        },
        {
            "heading": "3 FEDERATED LEARNING WITHOUT NORMALIZATION",
            "text": "In this section, we present the problem setup of multi-domain FL and propose FL without normalization to address the problem of multi-domain FL."
        },
        {
            "heading": "3.1 PROBLEM SETUP",
            "text": "The standard federated learning aims to train a model with parameters \u03b8 collaboratively from total N \u2208 N decentralized clients. The goal is to optimize the following problem:\nmin \u03b8\u2208Rd f(\u03b8) := K\u2211 k=1 pkfk(\u03b8) := K\u2211 k=1 pkE\u03bek\u223cDk [fk(\u03b8; \u03bek)], (2)\nwhere K \u2208 N is the number of participated clients (K \u2264 N ), fk(\u03b8) is the loss function of client k, pk is the weight for model aggregation in the server, and \u03bek is the data sampled from distribution Dk of client k. FedAvg (McMahan et al., 2017) sets pk to be proportional to the data size of client k. Each client trains for E \u2208 N local epochs before communicating with the server. Assume there are N clients in FL and each client k contains nk \u2208 N data samples {(xki , yki )} nk i=1. Skewed label distribution refers to the scenario where data in clients have different label distributions, i.e. the marginal distributions Pk(y) may differ across clients (Pk(y) \u2241 Pk\u2032(y) for different clients k and k\u2032). In contrast, this work focuses on multi-domain FL, where clients possess data from various domains, and data samples within a client belong to the same domain (Kairouz et al., 2021; Li et al., 2021). Specifically, the marginal distribution Pk(x) may vary across clients (Pk(x) \u2241 Pk\u2032(x) for different clients k and k\u2032). Within each client, the data samples, represented as xi and xj , drawn from the same marginal distribution Pk(x) holds that Pk(xi) \u223c Pk(xj) for all i, j \u2208 1, 2, ..., nk. Figure 1a illustrates practical examples of multi-domain FL. For example, autonomous cars in different locations could capture images under different weather conditions."
        },
        {
            "heading": "3.2 NORMALIZATION-FREE FEDERATED LEARNING",
            "text": "Figure 1b demonstrates that the BN statistics of clients with data from distinct domains are considerably dissimilar in multi-domain FL. Although various existing approaches have attempted to address this challenge by manipulating or replacing the BN layers with other normalization layers (Li et al., 2021; Du et al., 2022; Zhong et al., 2023), they come with their own set of limitations, such as additional computation cost during inference. To bridge this gap, we propose a novel approach called Federated learning Without normalizations (FedWon)that removes all normalization layers in FL.\nHowever, simply removing all normalization layers would lead to deteriorated performance in FL. Figure 3 compares the performance of training in a single dataset (SingleSet) and FedAvg without normalization on four domains of the Office-Caltech-10 dataset (Further details on the experimental setup are provided in Section 4). FedAvg without (w/o) BN yields inferior results compared to SingleSet w/o BN. The domain gaps among clients could amplify the challenges in FL when training without BNs.\nCompared with FedAvg (McMahan et al., 2017), our proposed FedWon completely removes the normalization layers in DNNs and further reparameterizes the convolutions layer. We employ the Scaled Weight Standardization technique proposed by Brock et al. (2021a) to reparameterize the convolution layers after removing BN. The reparameterization formula can be expressed as follows:\nW\u0302i,j = \u03b3 Wi,j \u2212 \u00b5i \u03c3i \u221a N , (3)\nwhere Wi,j is the weight matrix of a convolution layer with i as the output channel and j as the input channel, \u03b3 is a constant number, N is the fan-in of convolution layer, \u00b5i = (1/N) \u2211 j Wi,j\nand \u03c32i = (1/N) \u2211\nj(Wi,j \u2212 \u00b5i) are the mean and variance of the i-th row of Wi,j , respectively. By removing normalization layers, FedWon eliminates batch dependency, resolves discrepancies between training and inference, and does not require computation for normalization statistics in inference. We term this parameterized convolution as WSConv.\nFigure 2 highlights the algorithmic differences between our proposed FedWon and the other two FL algorithms: FedAvg (McMahan et al., 2017) and FedBN (Li et al., 2021). FedAvg aggregates both convolution and BN layers on the server; FedBN only aggregates the convolution layers and keeps BN layers locally in clients. Unlike these two methods, FedWon removes BN layers, replaces convolution layers with WSConv, and only aggregates these reparameterized convolution layers. Prior work theoretically shows that BN slows down and biases the FL convergence (Wang et al., 2023). FedWon circumvents these issues by removing BN while preserving the convergence speed that BN typically facilitates. Furthermore, FedWon offers unexplored benefits to multi-domain FL, including versatility for both cross-silo and cross-device FL, enhanced domain generalization, and compelling performance on small batch sizes, including a batch size as small as 1."
        },
        {
            "heading": "4 EXPERIMENTS ON MULTI-DOMAIN FL",
            "text": "In this section, we start by introducing the experimental setup for multi-domain FL. We then validate that FedWon outperforms existing methods in both cross-silo and cross-device FL and achieves comparable performance even with a batch size of 1. We end by providing ablation studies."
        },
        {
            "heading": "4.1 EXPERIMENT SETUP",
            "text": "Datasets. We conduct experiments for multi-domain FL using three datasets: Digits-Five (Li et al., 2021), Office-Caltech-10 (Gong et al., 2012), and DomainNet (Peng et al., 2019). Digits-Five consists of five sets of 28x28 digit images, including MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), USPS (Hull, 1994), SynthDigits (Ganin & Lempitsky, 2015), MNIST-M (Ganin & Lempitsky, 2015); each digit dataset represents a domain. Office-Caltech-10 consists of real-world object images from four domains: three domains (WebCam, DSLR, and Amazon) from Office-31 dataset (Saenko et al., 2010) and one domain (Caltech) from Caltech-256 dataset (Griffin et al., 2007). DomainNet (Peng et al., 2019) contains large-sized 244x244 object images in six domains: Clipart, Infograph, Painting, Quickdraw, Real, and Sketch. To mimic the realistic scenarios where clients may not collect a large volume of data, we use a subset of standard digits datasets (7,438 training samples for each dataset instead of tens of thousands) as adopted in Li et al. (2021). We evenly split samples of each dataset into 20 clients for cross-device FL with a total of 100 clients. Similarly, we tailor the DomainNet dataset to include only 10 classes of 2,000-5,000 images. To simulate multi-domain FL, we construct a client to contain images from a single domain.\nImplementation Details. We implement FedWon using PyTorch (Paszke et al., 2017) and run experiments on a cluster of eight NVIDIA T4 GPUs. We evaluate the algorithms using three architectures: 6-layer convolution neural network (CNN) (Li et al., 2021) for Digits-Five dataset, AlexNet (Krizhevsky et al., 2017) and ResNet-18 (He et al., 2016) for Office-Caltech-10 dataset, and AlexNet (Krizhevsky et al., 2017) for DomainNet dataset. We use cross-entropy loss and stochastic gradient optimization (SGD) as the optimizer with learning rates tuned over the range of [0.001, 0.1] for all methods. Based on SGD, we adopt adaptive gradient clipping (AGC) that is specially designed for normalization-free networks (Brock et al., 2021b). More details are provided in the supplementary."
        },
        {
            "heading": "4.2 PERFORMANCE EVALUATION",
            "text": "We compare the performance of our proposed FedWon with the following three types of methods: (1) state-of-the-art methods that employ customized approaches on BN, including SiloBN (Andreux et al., 2020), FedBN (Li et al., 2021), and FixBN (Zhong et al., 2023); (2) baseline algorithms, including FedProx (Li et al., 2020b), FedAvg (McMahan et al., 2017), and SingleSet (i.e. training a model independently in each client with a single dataset); (3) alternative normalization methods, including FedAvg+GN and FedAvg+LN that replace BN layers with GN and LN layers, respectively.\nTable 1 presents a comprehensive comparison of the aforementioned methods under cross-silo FL on Digits-Five, Office-Caltech-10, and DomainNet datasets. Our proposed FedWon outperforms the state-of-the-art methods on most of the domains across all datasets. Specifically, FedProx, which adds a proximal term based on FedAvg, performs similarly to FedAvg. These two methods are better than SingleSet in Digits-Five dataset, but they may exhibit inferior performance compared to SingleSet in certain domains on the other two more challenging datasets. SiloBN and FixBN perform similarly to FedAvg, in terms of average accuracy; they are not primarily designed for multi-domain FL and are only capable of achieving the baseline results. In contrast, FedBN is specifically designed to excel in multi-domain FL and outperforms these methods.\nTable 2: Performance comparison using small batch sizes B = {1, 2} on Office-Caltech-10 dataset. The abbreviations A, C, D, and W respectively represent 4 domains: Amazon, Caltech, DSLR, and WebCam. Our proposed FedWon achieves outstanding performance compared to existing methods.\nB Methods A C D W\n1 FedAvg+GN 60.4 52.0 87.5 84.8 FedAvg+LN 55.7 43.1 84.4 88.1 FedWon 66.7 55.1 96.9 89.8\n2 FedAvg 64.1 49.3 87.5 89.8 FedAvg+GN 63.5 52.0 81.3 84.8 FedAvg+LN 58.3 44.9 87.5 86.4 FixBN 66.2 50.7 87.5 88.1 SiloBN 61.5 47.1 87.5 86.4 FedBN 59.4 48.0 96.9 86.4 FedWon 66.2 54.7 93.8 89.8\nBesides, we discover that simply replacing BN with GN (FedAvg+GN) can boost the performance of FedAvg as GN does not depend on the batch statistics specific to domains; FedAvg+GN achieves comparable results as FedBN on DigitsFive and Office-Caltech-10 datasets. Notably, our proposed FedWon surpasses both FedAvg+GN and FedBN in terms of the average accuracy on all datasets. Although FedWon falls slightly behind FedBN by less than 1% on two domains across these datasets, it outperforms FedBN by more than 17% on certain domains. These results demonstrate the effectiveness of FedWon under the crosssilo FL scenario. We report the mean of three runs of experiments here and results with standard deviation in Table 22 in the Appendix.\nEffectiveness on Small Batch Size. Table 2 compares the performance of our proposed FedWon with state-of-the-art methods using small batch sizes B = {1, 2} on Office-Caltech-10 dataset. FedWon achieves outstanding performance, with competitive results even at a batch size of 1. While\nFedAvg+GN and FedAvg+LN also achieve comparable results on batch size B = 1, they require additional computational cost during inference to calculate the running mean and variance, whereas our method does not have such constraints and achieves even better performance. The capability of our method to perform well under small batch sizes is particularly important for cross-device FL, as some edge devices may only be capable of training with small batch sizes under constrained resources. We have fine-tuned the learning rates for all methods and reported the best ones.\nCross-device FL with Small Batch Size and Client Selection. We assess the impact of randomly selecting a fraction of clients to participate in training in each round, which is common in cross-device FL where not all clients join in training. We conduct experiments with fraction C = {0.1, 0.2} out of 100 clients on Digits-Five dataset, i.e., K = {10, 20} clients are selected to participate in training in each round. Table 3 shows that the performance of our FedWon is better than FedAvg under all client fractions. FedBN is not compared as it is not applicable in cross-device FL. We also evaluate small batch sizes in cross-device FL, with K = 10 clients selected per round. Figure 4 (left) shows that the performance of FedAvg degrades with batch size B = 2, while our proposed FedWon with batch sizes B = {1, 2} achieves consistently comparable results to running with larger batch sizes. Besides, Figure 4 (right) shows the changes in testing accuracy over the course of training. It indicates that FedWon achieves better convergence speed without BN.\nVisualization and Analysis of Feature Maps. We aim to further study the reason behind the superior performance of FedWon. Figure 5 (top) visualizes feature maps of the last convolution layer of two client local models and one server global model on the Office-Caltech-10 dataset. The feature maps of FedAvg without (w/o) BN have a limited focus on the object of interest. While FedAvg and FedBN perform better, their feature maps display noticeable disparities between client local models.\nIn contrast, FedWon showcases superior feature map visualizations, with subtle differences observed among feature maps from different models. To provide further insight, we present the average cosine similarity of all feature maps between client local models (C\u2194C) and between the server global model and a client local model (S\u2194C) in Figure 5 (bottom). These results demonstrate the effectiveness of FedWon, which achieves high similarity scores, approaching the maximum value of 1. This finding suggests that FedWon excels at effectively mitigating domain shifts across different domains. Building upon these insights, we extend our analysis to demonstrate that FedWon exhibits superior domain adaptation and generalization capabilities empirically in Table 12 in the Appendix.\nWe also demonstrate that FedWon achieves significantly superior performance on medical diagnosis in Appendix B.1, which is encouraging and shows the potential of FedWon in the healthcare field.\n4.3 ABLATION STUDIES\nWe conduct ablation studies to further analyze the impact of WSConv at batch sizes B = 32 and B = 2 on the Office-Caltech-10 dataset. Table 4 compares the performance with and without WSConv after removing all normalization layers. It demonstrates that replacing convolution layers with WSConv significantly enhances performance. These experiments use a learning rate of \u03b7 = 0.08 for B = 32 and \u03b7 = 0.01 for B = 2. We provide more experiment details in the Appendix B."
        },
        {
            "heading": "5 EXPERIMENTS ON SKEWED LABEL DISTRIBUTION",
            "text": "This section extends evaluation from multi-domain FL to skewed label distribution. We demonstrate that our proposed FedWon is also effective in addressing this problem.\nDataset and Implementation. We simulate skewed label distribution using CIFAR-10 dataset (Krizhevsky et al., 2009), which comprises 50,000 training samples and 10,000 testing samples. We split training samples into 100 clients and construct i.i.d data and three different levels of label skewness using Dirichlet process Dir(\u03b1) with \u03b1 = {0.1, 0.5, 1}, where Dir(0.1) is the most heterogeneous setting. We run experiments using MobileNetV2 (Sandler et al., 2018) with a fraction C = 0.1 randomly selected clients (i.e., K = 10) out of a total of 100 clients in each round.\nPerformance Comparison. Figure 6 (left) compares FedWon with FedAvg, FedAvg+GN, FedAvg+LN, and FixBN. FedWon achieves similar performance as FedAvg and FixBN on the i.i.d setting, but outperforms all methods across different degrees of label skewness. We do not compare with FedBN and SiloBN as they are not suitable for cross-device FL and provide the comparison of cross-silo FL scenario in Table 15 in the Appendix. Figure 6 (right) shows changes in testing accuracy over the course of training under the Dir (0.5) setting. FedWon converges to a better position than the other methods. These experiments indicate the possibility of employing our proposed FL without normalization to solve the skewed label distribution problem."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In conclusion, we propose FedWon, a new method for multi-domain FL by removing BN layers from DNNs and reparameterizing convolution layers with weight scaled convolution. Extensive experiments across four datasets and models demonstrate that this simple yet effective method outperforms state-of-the-art methods in a wide range of settings. Notably, FedWon is versatile for both cross-silo and cross-device FL. Its ability to train on small batch sizes is particularly useful for resource-constrained devices. Future work can conduct evaluations of this method under a broader range of datasets and backbones for skewed label distribution. Extending this paradigm from supervised to semi-supervised and unsupervised scenarios is also of interest."
        },
        {
            "heading": "A EXPERIMENTAL SETUP",
            "text": "In this section, we provide more details of experimental setups, including datasets, model architectures, and implementation details."
        },
        {
            "heading": "A.1 DATAETS",
            "text": "Figure 7, 8, and 9 visualize three multi-domain datasets used in this work; these three datasets are Digits-Five (Li et al., 2021), Office-Caltech-10 (Gong et al., 2012), and DomainNet (Peng et al., 2019), respectively. It shows that images under each dataset have significant domain gaps. We construct multi-domain FL by constraining each FL client to contain samples of the same domain. Each image is a sample from one client. Each FL client contains images of a dataset (domain). We follow FedBN (Li et al., 2021) to preprocess and transform these datasets."
        },
        {
            "heading": "A.2 MODEL ARCHITECTURES",
            "text": "Table 5 illustrates the model architectures for experiments on the Digits-Five dataset and Table 6 illustrates the model architectures for experiments on Office-Caltech-10 and DomainNet datasets. For the convolution layer (Conv2D), the hyperparameters are in the sequence of input dimension, output dimension, kernel size, stride, and padding. For the max pooling layer (MaxPool2D), the hyperparameters are kernel and stride. For the fully connected layer (FC), the hyperparameters are input and output dimensions. For the batch normalization (BN) layer, the hyperparameter is the number of channels. For group normalization, the hyperparameters are the number of groups and the number of channels. FedAvg+LN shares a similar model architecture as FedAvg+GN but sets the number of groups to 1. The methods with BN are Standalone, FedAvg (McMahan et al., 2017), FedProx (Li et al., 2020b), SiloBN (Andreux et al., 2020), FedBN (Li et al., 2021), and FixBN (Zhong et al., 2023). These methods share the same model architecture. Note that the model architecture is not exactly the same as the ones used in FedBN (Li et al., 2021), where they use a onedimension BN layer as regularizer between FC layers but we use Dropout such that the comparisons are fair in terms of model architectures.\nBesides, we use the default implementation of ResNet-18 (He et al., 2016) and MobileNetV2 (Paszke et al., 2017) in PyTorch (Paszke et al., 2017) for methods with BN on the Office-Caltech-10 dataset and CIFAR-10 dataset, respectively. FedWon replaces the convolution layers in ResNet-18 and MobileNetV2 with WSConv and removes all batch normalization layers. FedAvg+GN and FedAvg+LN replace BN layers with GN layers. Specifically, FedAvg+GN sets the number of groups to 32 by default, but sets it to 8 when the output dimension is smaller than 32, and to 24 when the output dimension is 144 (to ensure divisibility); FedAvg+LN sets the number of groups in GN to 1. The source code will be released.\nA.3 IMPLEMENTATION AND TRAINING DETAILS\nListing 1 provides the implementation of WSConv in PyTorch. We employ the architectures described in Section A.2 to implement FedWon, adhering to the client training and server aggregation protocols of FedAvg (McMahan et al., 2017). We implement FedWon based on both EasyFL (Zhuang et al., 2022a) for skewed label distribution experiments and FedBN original implementation for multi-domain FL experiments. For the implementation of FedBN, we reference the open-source code available in Github 1. To implement SiloBN (Andreux et al., 2020), we modify the FedBN implementation to aggregate only the BN parameters while keeping the BN statistics local. Unfortunately, as the source code for FixBN (Zhong et al., 2023) is not publicly available, we implement it based on the description provided in the paper.\nBesides, we summarize the compared algorithms in Table 7.\n1https://github.com/med-air/FedBN\n(a) Clipart (b) Infograph (c) Painting (d) Quickdraw (e) Real (f) Sketch\nBy default, we conduct experiments with local epochs E = 1 and batch size B = 32 across all datasets. Stochastic gradient optimization (SGD) is used as the optimizer, with learning rates tuned in the range of [0.001, 0.1] for all methods. Specifically, for FedWon experiments with a batch size of B = 32, we incorporate adaptive gradient clipping (AGC) (Brock et al., 2021b), which is specifically designed for normalization-free networks. AGC applies gradient clipping to the weight matrix W l \u2208 RN\u00d7M of the lth layer, where the gradient Gl \u2208 RN\u00d7M is clipped with a threshold \u03bb before updating the model. The clipping operation for each row i of Gl can be expressed as follows:\nGli =\n{ \u03bb ||W li || \u2217 F\n||Gli||F Gli, if ||Gli||F ||W li ||\u2217F > \u03bb,\nGli, otherwise, (4)\nwhere || \u00b7 ||F is the the Frobenius norm, i.e. ||W l||F = \u221a\u2211N\ni \u2211M j (Wi,j)\n2), ||W li ||\u2217F = max(||Wi||F , \u03f5) with default \u03f5 = 1e \u2212 3. We only use AGC for FedWon with batch size B = 32 and bypass AGC on small batch sizes such as B = {1, 2, 4}. The impact of AGC and the clipping threshold is further analyzed in Section B.\nWe tune the learning rates for the methods compared in the main manuscript and provide their specific learning rates below. Table 8 illustrates the learning rates of different methods on three datasets, corresponding to the experiments of Table 1 in the main manuscript. We use a clipping threshold of 0.64 for Digits-Five, 1.28 for Office-Caltech-10, and 1.28 for the DomainNet dataset. Additionally, Table 9 presents the learning rates used for experiments with small batch sizes B = {1, 2, 4} on Office-Caltech-10 and Digits-Five datasets. Table 10 displays the learning rates used for experiments using ResNet-18 as the backbone. All experiments on Digits-Five are trained for 100 rounds and experiments on Office-Caltech-10 and DomainNet are trained for 300 rounds.\nFor evaluation of skewed label distribution, all experiments are run with local epoch E = 5 for 300 rounds. We use SGD as the optimizer and tune the learning in the range of [0.001, 0.1] for different algorithms."
        },
        {
            "heading": "B EXPERIMENTS",
            "text": "This section provides more experiment results that provide further insights into the behavior of FedWon and shed light on the effects of different parameters."
        },
        {
            "heading": "B.1 EXPERIMENTS ON MEDICAL IMAGES",
            "text": "To further study how our proposed FedWon benefits multi-domain FL in real-world scenarios, we extend evaluation to diagnosis of skin lesions using datasets from ISIC2019 Challenge (Codella et al., 2018; Combalia et al., 2019) and the HAM10000 (Tschandl et al., 2018) dataset. The dataset contains images collected from four hospitals, where one hospital with 3 different imaging technologies. We follow Flamby (Terrail et al., 2022) to construct them as six different centers: BCN, Vidir-molemax, Vidir-modern, Rosendahl, MSK, and Vienna-dias, with each center\u2019s images representing a unique domain. In total, the dataset encompasses 23,247 images of skin lesions, including 9930 training samples and 2483 testing samples from BCN; 3163 training samples and 791 testing samples from Vidir-molemax, 2691 training samples and 672 testing samples from Vidir-modern, 1807 training samples and 452 testing samples from Rosendahl, 655 training samples and 164 testing samples from MSK, and 351 training samples and 88 samples from Vienna-dias. In the experimental setup, we simulate the scenarios where multiple healthcare centers collaborate to train a skin lesion diagnosis model, with each client representing a healthcare center. The task is to conduct image classification for 8 different melanoma classes.\nWe run the experiments using ResNet-18 (He et al., 2016) (without any pre-training) with local epoch E = 1 and batch size B = 64 for 50 rounds. We use SGD optimizer with learning rate \u03b7 = 0.005 for FedAvg and FedWon and \u03b7 = 0.001 for FedBN. The learning rate is tuned among {0.001, 0.005, 0.01, 0.05}. We follow the implementation in Flamby 2 to use a weighted focal loss (Lin et al., 2017) and data augmentations.\nTable 11 shows the testing accuracy of FedAvg, FedBN, and our proposed FedWon across the six healthcare center domains. In this challenging setting, FedBN only achieves similar performance to FedAvg. In contrast, FedWon outperforms both FedAvg and FedBN in all domains by a significant margin. The results are inspiring and demonstrates the potential of deploying FedWon to healthcare application scenarios, where data is often scarce, isolated, and spans multiple domains.\n2https://github.com/owkin/FLamby/"
        },
        {
            "heading": "B.2 ADDITIONAL ANALYSIS",
            "text": "Domain Generalization Capability. We expand our analysis to investigate the domain adaptation and generalization capabilities of FedWon. Our experiments are conducted on the Office-Caltech10 dataset, where we employ Amazon (A), Caltech (C), and DSLR (D) as the seen domains during training, while WebCam (W) is exclusively reserved as an unseen domain only for evaluation, specifically for zero-shot evaluation. We use the client local models to evaluate the seen domains and use the server global model to test on the unseen domain; while to be fair for FedBN, we employ a global model with averaged BN layer parameters from the seen domains. Table 12 presents compelling evidence that FedWon not only excels in performance on the seen domains but also exhibits the most robust generalization capabilities on the unseen domains. These results demonstrate an additional advantage of FedWon, highlighting its capability for domain generalization.\nEvaluation on Alternative Backbones. In addition to evaluating the effectiveness of FedWon using AlexNet (Krizhevsky et al., 2017) on the Office-Caltech-10 dataset, Table 13 also compares testing accuracy on a common backbone, ResNet-20 (He et al., 2016). Interestingly, replacing BN with GN or LN is not as effective on ResNet-20 as on AlexNet. FedAvg+GN and FedAvg+LN only achieve similar or even worse performance than FedAvg. FedBN (Li et al., 2021), instead, achieves better performance than the other existing methods. Nevertheless, our proposed FedWon consistently outperforms the state-of-the-art methods even with ResNet-20 as the backbone.\nAnalysis on Different Degrees of Domain Heterogeneity. We evaluate the performance of the proposed FedWon under different degrees of domain heterogeneity. To simulate varying degrees of domain heterogeneity, we follow the approach taken by FedBN (Li et al., 2021) and create different numbers of clients with the same domain on the Digits-Five dataset. We start with 5 clients, each containing data from one domain, and then add 5 clients at a time, with each new client containing one of the Digits-Five datasets, respectively. We evaluate the performance of the algorithms for\ndifferent numbers of clients from N = {5, 10, 15, ..., 50}. More clients represent less heterogeneity as more clients have overlapping domains of data. Figure 10b compares the performance of FedWon and FedBN under these settings. The results show that the performances of both FedWon and FedBN increase as the degree of heterogeneity decreases. FedBN outperforms FedAvg in all the settings as evidenced in Li et al. (2021). However, our proposed FedWon achieves even better performance than FedBN on all domains and all levels of heterogeneity.\nTesting Accuracy Changes Throughout Training. Figure 10 illustrates the changes in testing accuracy throughout the training process on the Digits-Five dataset. Specifically, Figure 10a compares the performance of FedWon and FedBN in a cross-silo FL involving a total of 5 clients (one client per domain) and a batch size of B = 32. FedWon outperforms FedBN in certain domains or demonstrates similar performance in others. Notably, FedWon achieves better performance in the early stage of training \u2013 FedWon exhibits faster convergence, achieving a satisfactory level of accuracy more quickly than FedBN. These results complement the results in Figure 4 (right) in the main manuscript that compares FedWon and FedAvg in a cross-device FL scenario.\nImpact of Local Epochs. Table 14 compares the performance of our proposed FedWon and FedBN (Li et al., 2021) under different local epochs E = {1, 4, 8} on Office-Caltech-10 dataset. FedWon maintains performance and consistently outperforms FedBN under different numbers of local epochs. We run these experiments with batch size B = 32 and the learning rate the same as the ones in Table 8 on the Office-Caltech-10 dataset.\nEvaluation on Cross-silo FL for Skewed Label Distribution. Table 15 compares different algorithms in cross-silo FL for skewed label distribution on the CIFAR-10 dataset, which complements cross-device FL experiments in Figure 6. FedWon also consistently outperforms all other methods in cross-silo FL. We run experiments with 10 clients under Dir(0.1) of non-i.i.d data, batch size B = 64, and local epoch E = 5 for 200 rounds.\nEvaluation on Cross-device FL of 1000 clients. Table 16 compares FedWon and FedAvg on a total of 1000 clients on Digits-Five dataset, with a selection of only 0.1 clients per round. FedWon also generally outperforms FedAvg under this setting. These experiments are run with batch size B = 2 and learning rate of 0.02.\nAddtional Comparison with PartialFed. Table 17 further compares FedWon with two variant implementations of PartialFed (Sun et al., 2021) on Office-Caltech-10 dataset. FedWon generally achieves superior performance to PartialFed, especially on the average testing accuracy."
        },
        {
            "heading": "B.3 ADDITIONAL ABLATION STUDIES",
            "text": "Impact of WSConv and AGC. We analyze the impact of WSConv and AGC, which supplements the ablation study presented in the main manuscript. Table 18 shows the impact of these two components with batch size B = 32 and small batch size B = 2 on the Office-Caltech-10 dataset. After removing the normalizations, using WSConv significantly improves the performance on both batch sizes. AGC, however, shows a positive impact only with batch size B = 32, as it is specifically designed for larger batch sizes. Consequently, we do not adopt AGC in the experiments with small batch sizes (B = {1, 2, 4}). We run these experiments with learning rate \u03b7 = 0.08 for B = 32 and \u03b7 = 0.01 for B = 2.\nImpact of Clipping Threshold \u03bb for AGC. We further extend to evaluate the impact of clipping threshold \u03bb under batch sizes B = 2 and B = 32. Figure 11 shows the average testing accuracy on the Office-Caltech-10 dataset using different clipping thresholds \u03bb = {0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56}. When the batch size B = 32, the performance\nis rather insensitive to different values of \u03bb when it is not too small (larger than 0.08). When the batch size B = 2, the best clipping threshold is \u03bb = 1.28 and the performance is sensitive to different values. Consistent with the finding in Table 18, we recommend avoiding using AGC when the batch size is small. These results provide insights into selecting an appropriate clipping threshold for multi-domain FL.\nImpact of AGC on Other Algorithms. Table 19 compares the results with and without AGC with the same learning rate for different methods. AGC also benefits other methods, while our proposed FedWon achieves the best overall performance."
        },
        {
            "heading": "B.4 COMPLEMENTARY EXPERIMENTS",
            "text": "Visualization of BN Statistics. Figure 12 visualizes the running mean and variance of BN layers of the 6-layer CNN. It complements Figure 1b in the main manuscript and shows the discrepancies of BN statistics between clients and between a client and the server in all BN layers.\nVisualization of Feature Maps. Figure 13 presents the visualization of feature maps obtained through three methods: FedAvg, FedBN, and our proposed FedWon. These feature maps are the output of each convolution layer in AlexNet on the Office-Caltech-10 dataset, which encompasses data from four distinct domains, namely Amazon, Caltech, DSLR, and WebCam. FedWon exhibits significantly enhanced feature maps on the object of interest compared to those produced by the FedAvg and FedBN.\nEffectiveness on Small Size. Table 20 compares performance of our proposed FedWon with existing methods on Office-Caltech-10 dataset with batch size B = 4. FedWon achieves the best performance also in this setting, complementing the experiments of batch size B = {1, 2} in Table 2 in the main manuscript. Additionally, Figure 14 compares the testing accuracy over the course of training of FedWon with batch sizes B = {1, 2, 4} on Digits-Five dataset. Different batch sizes tend to have a similar trend of convergence.\nEffectiveness on Selection of Clients. Table 21 compares the performance of FedAvg and FedWon on cross-device FL on the Digits-Five dataset with a fraction C = 0.4 of clients out of a total of 100 clients to participate in training each round. FedWon achieves superior performance also in this setting, complementing the experiments of C = {0.1, 0.2} in Table 3 in the main manuscript. Comparison of Methods with Variances. Table 22 presents testing accuracy comparison of different methods on three datasets with mean (standard deviation) of three runs of experiments. It complements the results in Table 1 in the main manuscript."
        }
    ],
    "title": "FEDWON: TRIUMPHING MULTI-DOMAIN FEDERATED LEARNING WITHOUT NORMALIZATION",
    "year": 2024
}