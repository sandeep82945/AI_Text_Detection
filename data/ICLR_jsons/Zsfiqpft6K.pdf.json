{
    "abstractText": "The objective for establishing dense correspondence between paired images consists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explicitly takes matching cost and injects the prior within generative process. However, limited resolution of the diffusion model is a major hindrance. We address this with a cascaded pipeline, starting with a low-resolution model, followed by a super-resolution model that successively upsamples and incorporates finer details to the matching field. Our experimental results demonstrate significant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. The code and pretrained weights will be available. (a) Source (b) Target (c) GLU-Net (d) GOCor (e) DiffMatch (f) Ground-truth Figure 1: Visualizing the effectiveness of the proposed DiffMatch. (a) source images, (b) target images, and warped source images using estimated correspondences by (c-d) state-of-the-art approaches (Truong et al., 2020b;a), (e) our DiffMatch, and (f) ground-truth. Compared to previous methods (Truong et al., 2020b;a) that discriminatively estimate correspondences, our diffusion-based generative framework effectively learns the matching field manifold, resulting in better estimating correspondences particularly at textureless regions, repetitive patterns, and large displacements.",
    "authors": [],
    "id": "SP:3a4b740b104fa220abcc41b4d6c71d5c58e46fa1",
    "references": [
        {
            "authors": [
                "Julien Abi-Nahed",
                "Marie-Pierre Jolly",
                "Guang-Zhong Yang"
            ],
            "title": "Robust active shape models: A robust, generic and simple automatic segmentation tool",
            "venue": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2006: 9th International Conference,",
            "year": 2006
        },
        {
            "authors": [
                "Tim Bailey",
                "Hugh Durrant-Whyte"
            ],
            "title": "Simultaneous localization and mapping (slam): Part ii",
            "venue": "IEEE robotics & automation magazine,",
            "year": 2006
        },
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro"
            ],
            "title": "ediffi: Text-to-image diffusion models with an ensemble of expert denoisers",
            "venue": "arXiv preprint arXiv:2211.01324,",
            "year": 2022
        },
        {
            "authors": [
                "Vassileios Balntas",
                "Karel Lenc",
                "Andrea Vedaldi",
                "Krystian Mikolajczyk"
            ],
            "title": "Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Connelly Barnes",
                "Eli Shechtman",
                "Adam Finkelstein",
                "Dan B Goldman"
            ],
            "title": "Patchmatch: A randomized correspondence algorithm for structural image editing",
            "venue": "ACM Trans. Graph.,",
            "year": 2009
        },
        {
            "authors": [
                "Georgios Batzolis",
                "Jan Stanczuk",
                "Carola-Bibiane Sch\u00f6nlieb",
                "Christian Etmann"
            ],
            "title": "Conditional image generation with score-based diffusion models",
            "venue": "arXiv preprint arXiv:2111.13606,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Brox",
                "Jitendra Malik"
            ],
            "title": "Large displacement optical flow: descriptor matching in variational motion estimation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2010
        },
        {
            "authors": [
                "Andr\u00e9s Bruhn",
                "Joachim Weickert"
            ],
            "title": "A confidence measure for variational optic flow methods",
            "venue": "Computational Imaging and Vision,",
            "year": 2006
        },
        {
            "authors": [
                "Michael Calonder",
                "Vincent Lepetit",
                "Christoph Strecha",
                "Pascal Fua"
            ],
            "title": "Brief: Binary robust independent elementary features",
            "venue": "In Computer Vision\u2013ECCV 2010: 11th European Conference on Computer Vision,",
            "year": 2010
        },
        {
            "authors": [
                "Ken Chatfield",
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Return of the devil in the details: Delving deep into convolutional nets",
            "venue": "arXiv preprint arXiv:1405.3531,",
            "year": 2014
        },
        {
            "authors": [
                "Shoufa Chen",
                "Peize Sun",
                "Yibing Song",
                "Ping Luo"
            ],
            "title": "Diffusiondet: Diffusion model for object detection",
            "venue": "arXiv preprint arXiv:2211.09788,",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Lala Li",
                "Saurabh Saxena",
                "Geoffrey Hinton",
                "David J Fleet"
            ],
            "title": "A generalist framework for panoptic segmentation of images and videos",
            "venue": "arXiv preprint arXiv:2210.06366,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaozhi Chen",
                "Kaustav Kundu",
                "Ziyu Zhang",
                "Huimin Ma",
                "Sanja Fidler",
                "Raquel Urtasun"
            ],
            "title": "Monocular 3d object detection for autonomous driving",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Ming-Ming Cheng",
                "Fang-Lue Zhang",
                "Niloy J Mitra",
                "Xiaolei Huang",
                "Shi-Min Hu"
            ],
            "title": "Repfinder: finding approximately repeated scene elements for image editing",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2010
        },
        {
            "authors": [
                "Seokju Cho",
                "Sunghwan Hong",
                "Sangryul Jeon",
                "Yunsung Lee",
                "Kwanghoon Sohn",
                "Seungryong Kim"
            ],
            "title": "Cats: Cost aggregation transformers for visual correspondence",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Seokju Cho",
                "Sunghwan Hong",
                "Seungryong Kim"
            ],
            "title": "Cats++: Boosting cost aggregation with convolutions and transformers",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "NeurIPS, 34:8780\u20138794,",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Philipp Fischer",
                "Eddy Ilg",
                "Philip Hausser",
                "Caner Hazirbas",
                "Vladimir Golkov",
                "Patrick Van Der Smagt",
                "Daniel Cremers",
                "Thomas Brox"
            ],
            "title": "Flownet: Learning optical flow with convolutional networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Marius Drulea",
                "Sergiu Nedevschi"
            ],
            "title": "Total variation regularization of local-global optical flow",
            "venue": "In 2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC),",
            "year": 2011
        },
        {
            "authors": [
                "Yiqun Duan",
                "Xianda Guo",
                "Zheng Zhu"
            ],
            "title": "Diffusiondepth: Diffusion denoising approach for monocular depth estimation",
            "venue": "arXiv preprint arXiv:2303.05021,",
            "year": 2023
        },
        {
            "authors": [
                "Olivier Duchenne",
                "Armand Joulin",
                "Jean Ponce"
            ],
            "title": "A graph-matching kernel for object categorization",
            "venue": "In 2011 International Conference on Computer Vision,",
            "year": 2011
        },
        {
            "authors": [
                "Hugh Durrant-Whyte",
                "Tim Bailey"
            ],
            "title": "Simultaneous localization and mapping: part i",
            "venue": "IEEE robotics & automation magazine,",
            "year": 2006
        },
        {
            "authors": [
                "Johan Edstedt",
                "Ioannis Athanasiadis",
                "M\u00e5rten Wadenb\u00e4ck",
                "Michael Felsberg"
            ],
            "title": "Dkm: Dense kernelized feature matching for geometry estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Giorgio Giannone",
                "Didrik Nielsen",
                "Ole Winther"
            ],
            "title": "Few-shot diffusion models",
            "venue": "arXiv preprint arXiv:2205.15463,",
            "year": 2022
        },
        {
            "authors": [
                "Dorothy M Greig",
                "Bruce T Porteous",
                "Allan H Seheult"
            ],
            "title": "Exact maximum a posteriori estimation for binary images",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1989
        },
        {
            "authors": [
                "Zhangxuan Gu",
                "Haoxing Chen",
                "Zhuoer Xu",
                "Jun Lan",
                "Changhua Meng",
                "Weiqiang Wang"
            ],
            "title": "Diffusioninst: Diffusion model for instance segmentation",
            "venue": "arXiv preprint arXiv:2212.02773,",
            "year": 2022
        },
        {
            "authors": [
                "Bumsub Ham",
                "Minsu Cho",
                "Cordelia Schmid",
                "Jean Ponce"
            ],
            "title": "Proposal flow",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Chitwan Saharia",
                "William Chan",
                "David J Fleet",
                "Mohammad Norouzi",
                "Tim Salimans"
            ],
            "title": "Cascaded diffusion models for high fidelity image generation",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Karl Holmquist",
                "Bastian Wandt"
            ],
            "title": "Diffpose: Multi-hypothesis human pose estimation using diffusion models",
            "venue": "arXiv preprint arXiv:2211.16487,",
            "year": 2022
        },
        {
            "authors": [
                "Sunghwan Hong",
                "Seungryong Kim"
            ],
            "title": "Deep matching prior: Test-time optimization for dense correspondence",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Berthold K.P. Horn",
                "Brian G. Schunck"
            ],
            "title": "Determining optical flow",
            "venue": "Artificial Intelligence,",
            "year": 1981
        },
        {
            "authors": [
                "Yuan-Ting Hu",
                "Jia-Bin Huang",
                "Alexander G Schwing"
            ],
            "title": "Videomatch: Matching based video object segmentation",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Tak-Wai Hui",
                "Xiaoou Tang",
                "Chen Change Loy"
            ],
            "title": "Liteflownet: A lightweight convolutional neural network for optical flow estimation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yuanfeng Ji",
                "Zhe Chen",
                "Enze Xie",
                "Lanqing Hong",
                "Xihui Liu",
                "Zhaoqiang Liu",
                "Tong Lu",
                "Zhenguo Li",
                "Ping Luo"
            ],
            "title": "Ddp: Diffusion model for dense visual prediction",
            "venue": "arXiv preprint arXiv:2303.17559,",
            "year": 2023
        },
        {
            "authors": [
                "Wei Jiang",
                "Eduard Trulls",
                "Jan Hosang",
                "Andrea Tagliasacchi",
                "Kwang Moo Yi"
            ],
            "title": "Cotr: Correspondence transformer for matching across images",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Gyeongnyeon Kim",
                "Wooseok Jang",
                "Gyuseong Lee",
                "Susung Hong",
                "Junyoung Seo",
                "Seungryong Kim"
            ],
            "title": "Dag: Depth-aware guidance with denoising diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2212.08861,",
            "year": 2022
        },
        {
            "authors": [
                "Jaechul Kim",
                "Ce Liu",
                "Fei Sha",
                "Kristen Grauman"
            ],
            "title": "Deformable spatial pyramid matching for fast dense correspondences",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Seungryong Kim",
                "Dongbo Min",
                "Bumsub Ham",
                "Sangryul Jeon",
                "Stephen Lin",
                "Kwanghoon Sohn"
            ],
            "title": "Fcss: Fully convolutional self-similarity for dense semantic correspondence",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Seungryong Kim",
                "Dongbo Min",
                "Stephen Lin",
                "Kwanghoon Sohn"
            ],
            "title": "Dctm: Discrete-continuous transformation matching for semantic flow",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Seungryong Kim",
                "Stephen Lin",
                "Sang Ryul Jeon",
                "Dongbo Min",
                "Kwanghoon Sohn"
            ],
            "title": "Recurrent transformer networks for semantic correspondence",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Claudia Kondermann",
                "Daniel Kondermann",
                "Bernd J\u00e4hne",
                "Christoph Garbe"
            ],
            "title": "An adaptive confidence measure for optical flows based on linear subspace projections",
            "venue": "In Pattern Recognition: 29th DAGM Symposium, Heidelberg,",
            "year": 2007
        },
        {
            "authors": [
                "Claudia Kondermann",
                "Rudolf Mester",
                "Christoph S Garbe"
            ],
            "title": "A statistical confidence measure for optical flows",
            "venue": "ECCV (3),",
            "year": 2008
        },
        {
            "authors": [
                "Jan Kybic",
                "Claudia Nieuwenhuis"
            ],
            "title": "Bootstrap optical flow confidence and uncertainty measure",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2011
        },
        {
            "authors": [
                "Zihang Lai",
                "Weidi Xie"
            ],
            "title": "Self-supervised learning for video correspondence flow",
            "venue": "arXiv preprint arXiv:1905.00875,",
            "year": 2019
        },
        {
            "authors": [
                "Jae Yong Lee",
                "Joseph DeGol",
                "Victor Fragoso",
                "Sudipta N Sinha"
            ],
            "title": "Patchmatch-based neighborhood consensus for semantic correspondence",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Maxime Lhuillier",
                "Long Quan"
            ],
            "title": "Robust dense matching using local and global geometric constraints",
            "venue": "In Proceedings 15th International Conference on Pattern Recognition. ICPR-2000,",
            "year": 2000
        },
        {
            "authors": [
                "Zhengqi Li",
                "Noah Snavely"
            ],
            "title": "Megadepth: Learning single-view depth prediction from internet photos",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013 ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Ce Liu",
                "Jenny Yuen",
                "Antonio Torralba"
            ],
            "title": "Sift flow: Dense correspondence across scenes and its applications",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2010
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "David G Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "International journal of computer vision,",
            "year": 2004
        },
        {
            "authors": [
                "Bruce D. Lucas",
                "Takeo Kanade"
            ],
            "title": "An iterative image registration technique with an application to stereo vision",
            "venue": "In Proceedings of the 7th International Joint Conference on Artificial Intelligence Volume",
            "year": 1981
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Oisin Mac Aodha",
                "Ahmad Humayun",
                "Marc Pollefeys",
                "Gabriel J Brostow"
            ],
            "title": "Learning a confidence measure for optical flow",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Iaroslav Melekhov",
                "Aleksei Tiulpin",
                "Torsten Sattler",
                "Marc Pollefeys",
                "Esa Rahtu",
                "Juho Kannala"
            ],
            "title": "Dgc-net: Dense geometric correspondence network",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2019
        },
        {
            "authors": [
                "Juhong Min",
                "Minsu Cho"
            ],
            "title": "Convolutional hough matching networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "David Nist\u00e9r",
                "Oleg Naroditsky",
                "James Bergen"
            ],
            "title": "Visual odometry",
            "venue": "In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2004
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Javier S\u00e1nchez P\u00e9rez",
                "Enric Meinhardt-Llopis",
                "Gabriele Facciolo"
            ],
            "title": "Tv-l1 optical flow estimation",
            "venue": "Image Processing On Line,",
            "year": 2013
        },
        {
            "authors": [
                "Anurag Ranjan",
                "Michael J Black"
            ],
            "title": "Optical flow estimation using a spatial pyramid network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jerome Revaud",
                "Philippe Weinzaepfel",
                "Zaid Harchaoui",
                "Cordelia Schmid"
            ],
            "title": "Epicflow: Edgepreserving interpolation of correspondences for optical flow",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Ignacio Rocco",
                "Relja Arandjelovic",
                "Josef Sivic"
            ],
            "title": "Convolutional neural network architecture for geometric matching",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Ignacio Rocco",
                "Mircea Cimpoi",
                "Relja Arandjelovi\u0107",
                "Akihiko Torii",
                "Tomas Pajdla",
                "Josef Sivic"
            ],
            "title": "Ncnet: Neighbourhood consensus networks for estimating image correspondences",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Dohoon Ryu",
                "Jong Chul Ye"
            ],
            "title": "Pyramidal denoising diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2208.01864,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Huiwen Chang",
                "Chris Lee",
                "Jonathan Ho",
                "Tim Salimans",
                "David Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Palette: Image-to-image diffusion models",
            "venue": "In ACM SIGGRAPH 2022 Conference Proceedings,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "Jonathan Ho",
                "William Chan",
                "Tim Salimans",
                "David J Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Image super-resolution via iterative refinement",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Paul-Edouard Sarlin",
                "Daniel DeTone",
                "Tomasz Malisiewicz",
                "Andrew Rabinovich"
            ],
            "title": "Superglue: Learning feature matching with graph neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Saurabh Saxena",
                "Charles Herrmann",
                "Junhwa Hur",
                "Abhishek Kar",
                "Mohammad Norouzi",
                "Deqing Sun",
                "David J Fleet"
            ],
            "title": "The surprising effectiveness of diffusion models for optical flow and monocular depth estimation",
            "venue": "arXiv preprint arXiv:2306.01923,",
            "year": 2023
        },
        {
            "authors": [
                "Saurabh Saxena",
                "Abhishek Kar",
                "Mohammad Norouzi",
                "David J. Fleet"
            ],
            "title": "Monocular depth estimation using diffusion models, 2023b. URL https://arxiv.org/abs/2302.14816",
            "year": 2023
        },
        {
            "authors": [
                "Johannes L Schonberger",
                "Jan-Michael Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Thomas Schops",
                "Johannes L Schonberger",
                "Silvano Galliani",
                "Torsten Sattler",
                "Konrad Schindler",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "A multi-view stereo benchmark with high-resolution images and multi-camera videos",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Junyoung Seo",
                "Gyuseong Lee",
                "Seokju Cho",
                "Jiyoung Lee",
                "Seungryong Kim"
            ],
            "title": "Midms: Matching interleaved diffusion models for exemplar-based image translation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Tianwei Shen",
                "Lei Zhou",
                "Zixin Luo",
                "Yao Yao",
                "Shiwei Li",
                "Jiahui Zhang",
                "Tian Fang",
                "Long Quan"
            ],
            "title": "Self-supervised learning of depth and motion under photometric inconsistency",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Eero P Simoncelli",
                "Edward H Adelson",
                "David J Heeger"
            ],
            "title": "Probability distributions of optical flow",
            "venue": "In CVPR,",
            "year": 1991
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Deqing Sun",
                "Stefan Roth",
                "John P Lewis",
                "Michael J Black"
            ],
            "title": "Learning optical flow",
            "venue": "In Computer Vision\u2013ECCV 2008: 10th European Conference on Computer Vision,",
            "year": 2008
        },
        {
            "authors": [
                "Deqing Sun",
                "Stefan Roth",
                "Michael J Black"
            ],
            "title": "Secrets of optical flow estimation and their principles",
            "venue": "IEEE computer society conference on computer vision and pattern recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Deqing Sun",
                "Xiaodong Yang",
                "Ming-Yu Liu",
                "Jan Kautz"
            ],
            "title": "Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Tatsunori Taniai",
                "Sudipta N Sinha",
                "Yoichi Sato"
            ],
            "title": "Joint recovery of dense correspondence and cosegmentation in two images",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Zachary Teed",
                "Jia Deng"
            ],
            "title": "Raft: Recurrent all-pairs field transforms for optical flow",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Guy Tevet",
                "Sigal Raab",
                "Brian Gordon",
                "Yonatan Shafir",
                "Daniel Cohen-Or",
                "Amit H Bermano"
            ],
            "title": "Human motion diffusion model",
            "venue": "arXiv preprint arXiv:2209.14916,",
            "year": 2022
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Luc V Gool",
                "Radu Timofte"
            ],
            "title": "Gocor: Bringing globally optimized correspondence volumes into your neural network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Radu Timofte"
            ],
            "title": "Glu-net: Global-local universal network for dense flow and correspondences",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Learning accurate dense correspondences and when to trust them",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Pdc-net+: Enhanced probabilistic dense correspondence network",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Benjamin Ummenhofer",
                "Huizhong Zhou",
                "Jonas Uhrig",
                "Nikolaus Mayer",
                "Eddy Ilg",
                "Alexey Dosovitskiy",
                "Thomas Brox"
            ],
            "title": "Demon: Depth and motion network for learning monocular stereo",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Anne S Wannenwetsch",
                "Margret Keuper",
                "Stefan Roth"
            ],
            "title": "Probflow: Joint optical flow and uncertainty estimation",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Philippe Weinzaepfel",
                "Jerome Revaud",
                "Zaid Harchaoui",
                "Cordelia Schmid"
            ],
            "title": "Deepflow: Large displacement optical flow with deep matching",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2013
        },
        {
            "authors": [
                "Manuel Werlberger",
                "Thomas Pock",
                "Horst Bischof"
            ],
            "title": "Motion estimation with non-local total variation regularization",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Pan Zhang",
                "Bo Zhang",
                "Dong Chen",
                "Lu Yuan",
                "Fang Wen"
            ],
            "title": "Cross-domain correspondence learning for exemplar-based image translation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Hong",
                "Kim"
            ],
            "title": "2021), and the performance also converges. In comparison, DMP (Hong & Kim, 2021), which optimizes the neural network to learn the matching prior of an image pair at test time, requires approximately 300 steps. These results highlight that DiffMatch finds a shorter and better path to accurate",
            "year": 2021
        },
        {
            "authors": [
                "2022 Gu et al",
                "2023b Saxena et al",
                "2023 Duan et al",
                "Saxena"
            ],
            "title": "2023a), applying a diffusion model for dense prediction, such as semantic segmentation (Ji et al., 2023; Gu et al., 2022), or monocular depth estimation (Ji et al., 2023",
            "venue": "Previous works (Ji et al.,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Establishing pixel-wise correspondences between pairs of images has been one of the crucial problems, as it supports a wide range of applications, including structure from motion (SfM) (Schonberger & Frahm, 2016), simultaneous localization and mapping (SLAM) (Durrant-Whyte & Bailey, 2006; Bailey & Durrant-Whyte, 2006), image editing (Barnes et al., 2009; Cheng et al., 2010; Zhang et al., 2020), and video analysis (Hu et al., 2018; Lai & Xie, 2019). In contrast to sparse correspondence (Calonder et al., 2010; Lowe, 2004; Sarlin et al., 2020) which detects and matches only a\nsparse set of key points, dense correspondence (P\u00e9rez et al., 2013; Rocco et al., 2017; Kim et al., 2018; Cho et al., 2021) aims to match all the points between input images.\nIn the probabilistic interpretation, the objective for dense correspondence can be defined with a data term, measuring matching evidence between source and target features, and a prior term, encoding prior knowledge of correspondence. Traditional methods (P\u00e9rez et al., 2013; Drulea & Nedevschi, 2011; Werlberger et al., 2010; Lhuillier & Quan, 2000; Liu et al., 2010; Ham et al., 2016) explicitly incorporated hand-designed prior terms to achieve smoother correspondence, such as total variation (TV) or image discontinuity-aware smoothness. However, the formulation of the hand-crafted prior term is notoriously challenging and may vary depending on the specific dense correspondence tasks, such as geometric matching (Liu et al., 2010; Duchenne et al., 2011; Kim et al., 2013) or optical flow (Weinzaepfel et al., 2013; Revaud et al., 2015).\nUnlike them, recent approaches (Kim et al., 2017a; Sun et al., 2018; Rocco et al., 2017; 2020; Truong et al., 2020b; Min & Cho, 2021; Kim et al., 2018; Jiang et al., 2021; Cho et al., 2021; 2022) have focused on solely learning the data term with deep neural networks. However, despite demonstrating certain performance improvements, these methods still struggle with effectively addressing the inherent ambiguities encountered in dense correspondence, including challenges posed by textureless regions, repetitive patterns, large displacements, or noises. We argue that it is because they concentrate on maximizing the likelihood, which corresponds to learning the data term only, and do not explicitly consider the matching prior. This limits their ability to learn ideal matching field manifold, and leads to poor generalization.\nOn the other hand, diffusion models (Ho et al., 2020; Song et al., 2020a; Song & Ermon, 2019; Song et al., 2020b) have recently demonstrated a powerful capability for learning posterior distribution and have achieved considerable success in the field of generative models (Karras et al., 2020). Building on these advancements, recent studies (Rombach et al., 2022; Seo et al., 2023; Saharia et al., 2022a; Lugmayr et al., 2022) have focused on controllable image synthesis by leveraging external conditions. Moreover, these advances in diffusion models have also led to successful applications in numerous discriminative tasks, such as depth estimation (Saxena et al., 2023b; Kim et al., 2022; Duan et al., 2023), object detection (Chen et al., 2022a), segmentation (Gu et al., 2022; Giannone et al., 2022), and human pose estimation (Holmquist & Wandt, 2022).\nInspired by the recent success of the diffusion model (Ho et al., 2020; Song et al., 2020a; Song & Ermon, 2019; Song et al., 2020b), we introduce DiffMatch, a conditional diffusion-based framework designed to explicitly model the matching field distribution within diffusion process. Unlike existing discriminative learning-based methods (Kim et al., 2017a; Jiang et al., 2021; Rocco et al., 2017; 2020; Teed & Deng, 2020) that focus solely on maximizing the likelihood, DiffMatch aims to learn the posterior distribution of dense correspondence. Specifically, this is achieved by a conditional denoising diffusion module designed to learn how to generate a correspondence field given feature descriptors as conditions. However, limited resolution of the diffusion model is a significant hindrance. To address this, we adopt a cascaded diffusion pipeline, starting with a low-resolution diffusion model, and then transitioning to a super-resolution diffusion model that successively upsamples the matching field and incorporates higher-resolution details.\nWe evaluate the effectiveness of DiffMatch using several standard benchmarks (Balntas et al., 2017; Schops et al., 2017), and show the robustness of our model with the corrupted datasets (Hendrycks & Dietterich, 2019; Balntas et al., 2017; Schops et al., 2017). We also conduct extensive ablation studies to validate our design choices and explore the effectiveness of each component."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Dense correspondence. Traditional methods for dense correspondence (Horn & Schunck, 1981; Lucas & Kanade, 1981) relied on hand-designed matching priors. Several techniques (Sun et al., 2010; Brox & Malik, 2010; Liu et al., 2010; Taniai et al., 2016; Kim et al., 2017b; Ham et al., 2016; Kim et al., 2013) introduced optimization methods, such as SIFT Flow (Liu et al., 2010), which designed smoothness and small displacement priors, and DCTM (Kim et al., 2017b), which introduced a discontinuity-aware prior term. However, manually designing the prior term is difficult. To address this, recent approaches (Dosovitskiy et al., 2015; Rocco et al., 2017; Shen et al., 2019; Melekhov et al., 2019; Ranjan & Black, 2017; Teed & Deng, 2020; Sun et al., 2018; Truong et al.,\n2020b; 2021; Jiang et al., 2021) have shifted to a learning paradigm, formulating an objective function to solely maximize likelihood. This assumes that an optimal matching prior can be learned from a large-scale dataset. DGC-Net (Melekhov et al., 2019) and GLU-Net (Truong et al., 2020b) proposed a coarse-to-fine framework using a feature pyramid, while COTR (Jiang et al., 2021) employed a transformer-based network. GOCor (Truong et al., 2020a) developed a differentiable matching module to learn spatial priors, addressing matching ambiguities. PDC-Net+(Truong et al., 2023) presented dense matching using a probabilistic model, estimating a flow field paired with a confidence map. DKM (Edstedt et al., 2023) introduced a kernel regression global matcher to find accurate global matches and their certainty.\nDiffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have been extensively researched due to their powerful generation capability. The Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020) proposed a diffusion model in which the forward and reverse processes exhibit the Markovian property. The Denoising Diffusion Implicit Models (DDIM) (Song et al., 2020a) accelerated DDPM by replacing the original diffusion process with non-Markovian chains to enhance the sampling speed. Building upon these advancements, conditional diffusion models that leverage auxiliary conditions for controlled image synthesis have emerged. Palette (Saharia et al., 2022a) proposed a general framework for image-to-image translation by concatenating the source image as an additional condition. Similarly, InstructPix2Pix (Brooks et al., 2023) trains a conditional diffusion model using a paired image and text instruction, specifically tailored for instruction-based image editing. On the other hand, several studies (Ho et al., 2022; Saharia et al., 2022b; Ryu & Ye, 2022; Balaji et al., 2022) have turned their attention to resolution enhancement, as the Cascaded Diffusion Model (Ho et al., 2022) adopts a cascaded pipeline to progressively interpolate the resolution of synthesized images using the diffusion denoising process.\nDiffusion model for discriminative tasks. Recently, the remarkable performance of the diffusion model has been extended to solve discriminative tasks, including image segmentation (Chen et al., 2022b; Gu et al., 2022; Ji et al., 2023), depth estimation (Saxena et al., 2023b; Kim et al., 2022; Duan et al., 2023; Ji et al., 2023), object detection (Chen et al., 2022a), and pose estimation (Tevet et al., 2022; Holmquist & Wandt, 2022). These approaches have demonstrated noticeable performance improvement using diffusion-based architecture. Our method represents the first application of the diffusion model to the dense correspondence task."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Probabilistic interpretation of dense correspondence. Let us denote a pair of images, i.e., source and target, as Isrc and Itgt that represent visually or semantically similar images, and feature descriptors extracted from Isrc and Itgt as Dsrc and Dtgt, respectively. The objective of dense correspondence is to find a correspondence field F that is defined at each pixel i, which warps Isrc towards Itgt such that Itgt(i) \u223c Isrc(i+ F (i)) or Dtgt(i) \u223c Dsrc(i+ F (i)). This objective can be formulated within probabilistic interpretation (Simoncelli et al., 1991; Sun et al., 2008; Ham et al., 2016; Kim et al., 2017b), where we seek to find F \u2217 that maximizes the posterior probability of the correspondence field given a pair of feature descriptors Dsrc and Dtgt, i.e., p(F |Dsrc, Dtgt). According to Bayes\u2019 theorem (Joyce, 2003), the posterior can be decomposed such that p(F |Dsrc, Dtgt) \u221d p(Dsrc, Dtgt|F ) \u00b7 p(F ). To find the matching field F \u2217 that maximizes the posterior, we can use the maximum a posteriori (MAP) approach (Greig et al., 1989):\nF \u2217 = argmax F p(F |Dsrc, Dtgt) = argmax F p(Dsrc, Dtgt|F ) \u00b7 p(F )\n= argmax F {log p(Dsrc, Dtgt|F ) \ufe38 \ufe37\ufe37 \ufe38\ndata term\n+ log p(F ) \ufe38 \ufe37\ufe37 \ufe38\nprior term\n}. (1)\nIn this probabilistic interpretation, the first term, referred to as data term, represents the matching evidence between feature descriptors Dsrc and Dtgt, and the second term, referred to as prior term, encodes prior knowledge of the matching field F .\nDiffusion models. The diffusion model is a type of generative model, and can be divided into two categories: unconditional models (Sohl-Dickstein et al., 2015; Ho et al., 2020) and conditional models (Batzolis et al., 2021; Dhariwal & Nichol, 2021). Specifically, unconditional diffusion models learn an explicit approximation of the data distribution, denoted as p(X). On the other hand,\nconditional diffusion models estimate the data distribution given a certain condition K, e.g., text prompt (Dhariwal & Nichol, 2021), denoted as p(X|K). In the conditional diffusion model, the data distribution is approximated by recovering a data sample from the Gaussian noise through an iterative denoising process. Given a sample X0, it is transformed to Xt through the forward diffusion process at a time step t \u2208 {T, T \u2212 1, . . . , 1}, which consists of Gaussian transition at each time step q(Xt|Xt\u22121) := N ( \u221a 1\u2212 \u03b2tXt\u22121, \u03b2tI). The forward diffusion process follows the pre-defined variance schedule \u03b2t such that\nXt = \u221a \u03b1tX0 + \u221a 1\u2212 \u03b1tZ, Z \u223c N (0, I), (2)\nwhere \u03b1t = \u220ft\ni=1(1\u2212 \u03b2i). After training, we can sample data from the learned distribution through iterative denoising with the pre-defined range of time steps, called the reverse diffusion process, following the non-Markovian process of DDIM (Song et al., 2020a), which is parametrized as another Gaussian transition p\u03b8(Xt\u22121 | Xt) := N (Xt\u22121;\u00b5\u03b8(Xt, t), \u03c3\u03b8(Xt, t)I). To this end, the diffusion network F\u03b8(Xt, t;K) predicts the denoised sample X\u03020,t given Xt, t and K. One step in the reverse diffusion process can be formulated such that\nXt\u22121 = \u221a \u03b1t\u22121F\u03b8(Xt, t;K) +\n\u221a\n1\u2212 \u03b1t\u22121 \u2212 \u03c32t\u221a 1\u2212 \u03b1t ( Xt \u2212 \u221a \u03b1tF\u03b8(Xt, t;K) ) + \u03c3tZ (3)\nwhere \u03c3t is the covariance value of Gaussian distribution at time step t.\nThis iterative denoising process can be viewed as finding X\u2217 = argmaxX log p(X|K) through the relationship between the conditional sampling process of DDIM (Song et al., 2020a) and conditional score-based generative models (Batzolis et al., 2021)."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "4.1 MOTIVATION",
            "text": "Recent learning-based methods (Kim et al., 2017a; Sun et al., 2018; Rocco et al., 2017; 2020; Truong et al., 2020b; Min & Cho, 2021; Kim et al., 2018; Jiang et al., 2021; Cho et al., 2021; 2022) have employed deep neural networks F(\u00b7) to directly approximate the data term, i.e., argmaxF log p(Dsrc, Dtgt|F ), without explicitly considering the prior term. For instance, GLUNet (Truong et al., 2020b) and GOCor (Truong et al., 2020a) construct a cost volume along candidates F between source and target features Dsrc and Dtgt, and regresses the matching fields F\n\u2217 within deep neural networks, which might be analogy to argmaxF log p(Dsrc, Dtgt|F ). In this setting, dense correspondence F \u2217 is estimated as follows:\nF \u2217 = F\u03b8(Dsrc, Dtgt) \u2248 argmax F log p(Dsrc, Dtgt|F ) \ufe38 \ufe37\ufe37 \ufe38\ndata term\n, (4)\nwhere F\u03b8(\u00b7) and \u03b8 represent a feed-forward network and its parameters, respectively. These approaches assume that the matching prior can be learned within the model architecture by leveraging\nthe high capacity of deep networks (Truong et al., 2020b; Jiang et al., 2021; Cho et al., 2021; 2022; Min & Cho, 2021) and the availability of large-scale datasets. While there exists obvious performance improvement, they typically focus on the data term without explicitly considering the matching prior. This can restrict ability of the model to learn the manifold of matching field and result in poor generalization."
        },
        {
            "heading": "4.2 FORMULATION",
            "text": "To address these limitations, for the first time, we explore a conditional generative model for dense correspondence to explicitly learn both the data and prior terms. Unlike previous discriminative learning-based approaches (P\u00e9rez et al., 2013; Drulea & Nedevschi, 2011; Werlberger et al., 2010; Kim et al., 2017a; Sun et al., 2018; Rocco et al., 2017), we achieve this by leveraging a conditional generative model that jointly learns the data and prior through optimization of the following objective that explicitly learn argmaxF p(F |Dsrc, Dtgt):\nF \u2217 = F\u03b8(Dsrc, Dtgt) \u2248 argmax F p(F |Dsrc, Dtgt)\n= argmax F {log p(Dsrc, Dtgt|F ) \ufe38 \ufe37\ufe37 \ufe38\ndata term\n+ log p(F ) \ufe38 \ufe37\ufe37 \ufe38\nprior term\n}. (5)\nWe leverage the capacity of a conditional diffusion model, which generates high-fidelity and diverse samples aligned with the given conditions, to search for accurate matching within the learned correspondence manifold.\nSpecifically, we define the forward diffusion process for dense correspondence as the Gaussian transition such that q(Ft|Ft\u22121) := N ( \u221a 1\u2212 \u03b2tFt\u22121, \u03b2tI), where \u03b2t is a predefined variance schedule. The resulting latent variable Ft can be formulated as Eq. 2:\nFt = \u221a \u03b1tF0 + \u221a 1\u2212 \u03b1tZ, Z \u223c N (0, I), (6)\nwhere F0 is the ground-truth correspondence. In addition, the neural network F\u03b8(\u00b7) is subsequently trained to reverse the forward diffusion process. During the reverse diffusion phase, the initial latent variable FT is iteratively denoised following the sequence FT\u22121, FT\u22122, . . . , F0, using Eq. 3:\nFt\u22121 = \u221a \u03b1t\u22121F\u03b8(Xt, t;Dsrc, Dtgt)+\n\u221a\n1\u2212 \u03b1t\u22121 \u2212 \u03c32t\u221a 1\u2212 \u03b1t ( Xt\u2212 \u221a \u03b1tF\u03b8(Ft, t;Dsrc, Dtgt) ) +\u03c3tZ,\n(7)\nwhere F\u03b8(Ft, t;Dsrc, Dtgt) directly predicts the denoised correspondence F\u03020,t with source and target features, Dsrc and Dtgt, as conditions.\nThe objective of this denoising process is to find the optimal correspondence field F \u2217 that satisfies argmaxF log p(F |Dsrc, Dtgt). The detailed explanation of the objective function of the denoising process will be explained in Section 4.5."
        },
        {
            "heading": "4.3 NETWORK ARCHITECTURE",
            "text": "In this section, we discuss how to design the network architecture F\u03b8(\u00b7). Our goal is to find accurate matching fields given feature descriptors Dsrc and Dtgt from Isrc and Itgt, respectively, as conditions. An overview of our proposed architecture is provided in Figure 2.\nCost computation. Following conventional methods (Rocco et al., 2020; Truong et al., 2020a), we first compute the matching cost by calculating the pairwise cosine similarity between localized deep\nfeatures from the source and target images. Given image features Dsrc and Dtgt, the matching cost is constructed by taking scalar products between all locations in the feature descriptors, formulated as:\nC(i, j) = Dsrc(i) \u00b7Dtgt(j)\n\u2225Dsrc(i)\u2225\u2225Dtgt(j)\u2225 , (8)\nwhere i \u2208 [0, hsrc)\u00d7 [0, wsrc), j \u2208 [0, htgt)\u00d7 [0, wtgt), and \u2225 \u00b7 \u2225 denotes l-2 normalization. Forming the global matching cost by computing all pairwise feature dot products is robust to longrange matching. However, it is computationally unfeasible due to its high dimensionality such that C \u2208 Rhsrc\u00d7wsrc\u00d7htgt\u00d7wtgt . To alleviate this, we can build the local matching cost by narrowing down the target search region j within a neighborhood of the source location i, constrained by a search radius R. Compared to the global matching cost, the local matching cost Cl \u2208 Rhsrc\u00d7wsrc\u00d7R\u00d7R is suitable for small displacements and, thanks to its constrained search range of R, is more feasible for large spatial sizes and can be directly used as a condition for the diffusion model. Importantly, the computational overhead remains minimal, with the only significant increase being R\u00d7R in the channel dimension.\nConditional denoising diffusion module. As illustrated in Figure 2, we introduce a modified U-Net architecture based on (Nichol & Dhariwal, 2021). Our aim is to generate an accurate matching field that aligns with the given conditions. A direct method to condition the model is simply concatenating Dsrc and Dtgt with the noisy flow input Ft. However, this led to suboptimal performance in our tests. Instead, we present two distinct conditions for our network: the initial correspondence and the local matching cost.\nFirst, our model is designed to learn the residual of the initially estimated correspondence, which leads to improved initialization and enhanced stability. Specifically, we calculate the initial correspondence Finit using the soft-argmax operation (Cho et al., 2021) based on the global matching cost C between Dsrc and Dtgt. This assists the model to find long-range matches. Secondly, we integrate pixel-wise interactions between paired images. For this, each pixel i in the source image is mapped to i\u2032 in the target image through the estimated initial correspondence Finit. We then compute the local matching cost Cl as an additional condition with Finit. This local cost guides the model to focus on the neighborhood of the initial estimation, helping to find a more refined matching field. With these combined, our conditioning strategies enable the model to precisely navigate the matching field manifold while preserving its generative capability. Finally, under the conditions Finit and C\nl, the noised matching field Ft at time step t passes through the modified U-net (Nichol & Dhariwal, 2021), which comprises convolution and attention, and generates the denoised matching field F\u0302t,0 aligned with the given conditions."
        },
        {
            "heading": "4.4 FLOW UPSAMPLING DIFFUSION MODULE",
            "text": "The inherent resolution limitations of the diffusion model is a major hindrance. Inspired by recent super-resolution diffusion models (Ho et al., 2022; Ryu & Ye, 2022), we propose a cascaded pipeline tailored for flow upsampling. Our approach begins with a low-resolution denoising diffusion module, followed by a super-resolution module, successively upsampling and adding fine-grained details to the matching field. To achieve this, we simply finetune the pretrained conditional denoising diffusion model for super-resolution task. Specifically, instead of using Finit from the global matching cost, we opt for a downsampled ground-truth flow field as Finit. This simple modification effectively harnesses the power of the pretrained diffusion module for flow upsampling. The efficacy of our flow upsampling module is demonstrated in Table 4."
        },
        {
            "heading": "4.5 TRAINING",
            "text": "In training phase, the denoising U-Net, as illustrated in Section 4.3, learns the prior knowledge of the matching field with the initial correspondence Finit to give a matching hint and the local matching cost Cl to provide additional pixel-wise interactions. In other words, we redefine the network F\u03b8(Ft, t;Dsrc, Dtgt) as F\u03b8(Ft, t;Finit, Cl), given that Finit and Cl are derived from Dsrc and Dtgt as described in Section 4.3. The loss function for training diffusion model is defined as follows:\nL = EF0,t,Z\u223cN (0,I) [\u2225 \u2225F0 \u2212F\u03b8(Ft, t;Finit, Cl) \u2225 \u2225 2 ] . (9)\nNote that for the flow upsampling diffusion module, we finetune the pretrained conditional denoising diffusion module with the downsampled ground-truth flow as Finit."
        },
        {
            "heading": "4.6 INFERENCE",
            "text": "During the inference phase, a Gaussian noise FT is gradually denoised into a more accurate matching field F0 under the given features Dsrc and Dtgt as conditions through the diffusion reverse process. To account for the stochastic nature of diffusion-based models, we propose utilizing multiple hypotheses by computing the mean of the estimated multiple matching fields from multiple initializations FT , which helps to reduce stochasticity of model while improving the matching performance. Further details and analyses are available in Appendix C.2."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 IMPLEMENTATION DETAILS",
            "text": "For the feature extractor backbone, we used VGG-16 (Chatfield et al., 2014) and kept all parameters frozen throughout all experiments. Our diffusion network is based on (Nichol & Dhariwal, 2021) with modifications to the channel dimension. The network was implemented using PyTorch (Paszke et al., 2019) and trained with the AdamW optimizer (Loshchilov & Hutter, 2017) at a learning rate of 1e\u22124 for the denoising diffusion module and 3e\u22125 for flow upsampling module. We conducted comprehensive experiments in geometric matching for four datasets: HPatches (Balntas et al., 2017), ETH3D (Schops et al., 2017), ImageNet-C (Hendrycks & Dietterich, 2019) corrupted HPatches and ImageNet-C corrupted ETH3D. For evaluation, we trained our network using COCOaugmented DPED-CityScape-ADE (Lin et al., 2014) following (Truong et al., 2020a; 2023). For a fair comparison, we benchmark our method against PDCNet (Truong et al., 2021) and PDCNet+ (Truong et al., 2023), both trained on the same synthetic dataset. Note that we strictly adhere to the training settings provided in their publicly available codebase. Further implementation details can be found in Appendix A."
        },
        {
            "heading": "5.2 MATCHING RESULTS",
            "text": "Our primary aim is to develop a robust generative prior that can effectively address inherent ambiguities in dense correspondence, such as textureless regions, repetitive patterns, large displacements, or noises. To evaluate the robustness of the proposed diffusion-based generative prior in challenging matching scenarios, we tested our approach against a series of common corruptions from ImageNet-\nC (Hendrycks & Dietterich, 2019). This benchmark includes 15 types of algorithmically generated corruptions, organized into four distinct categories. Additionally, we validate our method using the standard HPatches and ETH3D datasets. Further details on the corruptions and explanations for each evaluation dataset can be found in Appendix B.\nImageNet-C corruptions. In real-world matching scenarios, image corruptions such as weather variations or photographic distortions frequently occur. Therefore, it is crucial to establish robust dense correspondence under these corrupted conditions. However, existing discriminative methods (Truong et al., 2020b;a; 2021; 2023) solely rely on the correlation layer, focusing on point-to-point feature relationships, resulting in degraded performance in harsh-corrupted settings. In contrast, our framework learns not only the likelihood but also the prior knowledge of the matching field formation. We evaluated the robustness of our approach against the aforementioned methods on ImageNet-C corrupted scenarios (Hendrycks & Dietterich, 2019) of HPatches and ETH3D. As shown in Table 1, our method exhibits outstanding performance in harsh corruptions, especially in noise and weather. Additionally, our superior performance is visually evident in Figure 4. More qualitative results are available in Appendix D.\nHPatches. We evaluated DiffMatch on five viewpoints of HPatches (Balntas et al., 2017). Table 2 summarizes the quantitative results and demonstrates that our method surpasses state-of-the-art discriminative learning-based methods (Truong et al., 2020b;a; 2021; 2023). The qualitative result is presented in Figure 5. The effectiveness of our approach is evident from the quantitative results in Figure 1. This success can be attributed to the robust generative prior that learns a matching field manifold, which effectively addresses challenges faced by previous discriminative methods, such as textureless regions, repetitive patterns, large displacements or noises. More qualitative results are available in Appendix D.\nETH3D. As indicated in Table 2, our method demonstrates highly competitive performance compared to previous discriminative works (Truong et al., 2020b;a; 2021; 2023) on ETH3D (Schops et al., 2017). Notably, DiffMatch surpasses these prior works by a large margin, especially at interval rates of 13 and 15, which represent the most challenging settings. Additional qualitative results can be found in Appendix D.\n5.3 ABLATION STUDY\nEffectiveness of generative prior. We aim to validate our hypothesis that a diffusion-based generative prior is effective for finding a more accurate matching field. To achieve this, we train our network by directly regressing the matching field. Then we compare its performance with our diffusion-based method. As demonstrated in Table 3, our generative approach outperforms the regression-based baseline, thereby emphasizing the efficacy of the generative prior in dense correspondence\ntasks. The effectiveness of the generative matching prior is further analyzed in Appendix C.3.\nComponent analysis. In this ablation study, we provide a quantitative comparison between different configurations. The results are summarized in Table 4. (I) refers to the complete architecture of the conditional denoising diffusion module, as illustrated in Figure 2. (II) and (III) denote the conditional denoising diffusion module without the local cost and initial flow conditioning, respectively. (IV) represents the flow upsampling diffusion module.\nNotably, (I) outperforms both (II) and (III), emphasizing the effectiveness of the proposed conditioning method. The comparison between (I) and (IV) underlines the benefits of the flow upsampling diffusion module, which has only a minor increase in training time as it leverages the pretrained (I) at a lower resolution.\nTime Complexity. Diffusion models inherently exhibit high time consumption due to their iterative denoising process. In this ablation study, we compare the time consumption of our model against existing works (Truong et al., 2021; 2023). As previously mentioned in Sec. 4.6, our method uses multiple hypotheses during inference, averaging them for the final output. To investigate the computational cost associated with the number of inputs, we present the computing times for processing 1, 2, and 3 samples in Tab. 5, respectively. Note that we can reduce the time consumed by batch processing the multiple inputs, rather\nthan iterating over all samples individually. With a fixed sampling time step of 5, our approach matches the time efficiency of others using a single sample, maintaining comparable performance. Additionally, increasing inputs leads to only a negligible time increase compared to leading prior works. This time complexity can be further mitigated by reducing the number of sampling timesteps. We delve into the trade-off between timestep reduction and accuracy in Appendix C.1."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we propose a novel diffusion-based framework for dense correspondence, named DiffMatch, which jointly models the likelihood and prior distribution of the matching fields. This is achieved by the conditional denoising diffusion module, which operates based on initial correspondence and local costs derived from feature descriptors. To alleviate resolution constraint, we further propose a flow upsampling diffusion module that finetunes the pretrained denoising module, thereby injecting fine details into the matching field with minimal optimization. For the first time, we highlight the power of the generative prior in dense correspondence, achieving state-of-the-art performance on standard benchmarks. We further emphasize the effectiveness of our generative prior in harshly corrupted settings of the benchmarks. As a result, we demonstrate that our diffusion-based generative approach outperforms discriminative approaches in addressing inherent ambiguities present in dense correspondence."
        },
        {
            "heading": "A MORE IMPLEMENTATION DETAILS",
            "text": "Our baseline code is built upon the DenseMatching repository1. We implemented the network in PyTorch (Paszke et al., 2019) and used the AdamW optimizer (Loshchilov & Hutter, 2017). All our experiments were conducted on 6 24GB RTX 3090 GPUs. For diffusion reverse sampling, we employed the DDIM sampler (Song et al., 2020a) and set the diffusion timestep T to 5 during both the training and sampling phases. We set the default number of samples for multiple hypotheses to 3 for all our evaluation.\nIn our experiments, we trained two primary modules: the conditional denoising diffusion module and the flow upsampling diffusion module. For the denoising diffusion module, we train 121M modified U-Net based on (Nichol & Dhariwal, 2021) with the learning rate to 1\u00d7 10\u22124 and trained the model for 130,000 iterations with a batch size of 24. For the flow upsampling diffusion module, we used a learning rate of 3\u00d7 10\u22125 and finetuned the pretrained conditional denoising diffusion module for 20,000 iterations with a batch size of 2.\nFor the feature extraction backbone, we employed VGG-16, as described in (Truong et al., 2020b;a). We resized the input images to H \u00d7W = 512\u00d7 512 and extracted feature descriptors at Conv3-3, Conv4-3, Conv5-3, and Conv6-1 with resolutions H4 \u00d7W4 , H8 \u00d7W8 , H16\u00d7W16 , and H32\u00d7W32 , respectively. We used these feature descriptors to establish both global and local matching costs. The denoising diffusion module was trained at a resolution of 64, while the flow upsampling diffusion module was trained at a resolution of 256 to upsample the flow field from 64 to 256."
        },
        {
            "heading": "B EVALUATION DATASETS",
            "text": "We evaluated DiffMatch on standard geometric matching benchmarks: HPatches (Balntas et al., 2017), ETH3D (Schops et al., 2017). To further investigate the effectiveness of the diffusion generative prior, we also evaluated DiffMatch under the harshly corrupted settings (Hendrycks & Dietterich, 2019) of HPatches (Balntas et al., 2017) and ETH3D (Schops et al., 2017). Here, we provide detailed information about these datasets.\nHPatches. We evaluated our method on the challenging HPatches dataset (Balntas et al., 2017), consisting of 59 image sequences with geometric transformations and significant viewpoint changes. The dataset contains images with resolutions ranging from 450\u00d7 600 to 1, 613\u00d7 1, 210. ETH3D. We evaluated our framework on the ETH3D dataset (Schops et al., 2017), which consists of multi-view indoor and outdoor scenes with transformations not constrained to simple homographies. ETH3D comprises images with resolutions ranging from 480\u00d7 752 to 514\u00d7 955 and consists of 10 image sequences. For a fair comparison, we followed the protocol of (Truong et al., 2020b), which collects pairs of images at different intervals. We selected approximately 500 image pairs from these intervals.\nCorruptions. Our primary objective is to design a powerful generative prior that can effectively address the inherent ambiguities in dense correspondence tasks, including textureless regions, repetitive patterns, large displacements, or noises. To this end, to assess the robustness of our generative prior against more challenging scenarios, we subjected it to a series of common corruptions from ImageNet-C (Hendrycks & Dietterich, 2019). This benchmark consists of 15 types of algorithmically generated corruptions, which are grouped into four distinct categories: noise, blur, weather, and digital. Each corruption type includes five different severity levels, resulting in a total of 75 unique corruptions. For our evaluation, we specifically focused on severity level 5 to highlight the effectiveness of our generative prior. In the following, we offer a detailed breakdown of each corruption type.\n1DenseMatching repository: https://github.com/PruneTruong/DenseMatching.\nGaussian noise is a specific type of random noise that typically arises in low-light conditions. Shot noise, also known as Poisson noise, is an electronic noise that originates from the inherent discreteness of light. Impulse noise, a color analogue of salt-and-pepper noise, occurs due to bit errors within an image. Defocus blur occurs when an image is out of focus, causing a loss of sharpness. Frosted glass blur is commonly seen on frosted glass surfaces, such as panels or windows. Motion blur arises when the camera moves rapidly, while zoom blur occurs when the camera quickly zooms towards an object. Snow, a type of precipitation, can cause visual obstruction in images. Frost, created when ice crystals form on lenses or windows, can obstruct the view. Fog, which conceals objects and is usually rendered using the diamond-square algorithm, also affects visibility. Brightness is influenced by daylight intensity. Contrast depends on lighting conditions and the color of an object. Elastic transformations apply stretching or contraction to small regions within an image. Pixelation arises when low-resolution images undergo upsampling. JPEG, a lossy image compression format, introduces artifacts during image compression."
        },
        {
            "heading": "C ADDITIONAL ANALYSES",
            "text": "C.1 TRADE-OFF BETWEEN SAMPLING TIME STEPS AND ACCURACY.\nFigure 7 illustrates the trade-off between sampling time steps and matching accuracy. As the sampling time steps increase, the matching performance progressively improves in our framework. After time step 5, it outperforms all other existing methods (Truong et al., 2020b;a; Hui et al., 2018; Sun et al., 2018; Hong & Kim, 2021), and the performance also converges. In comparison, DMP (Hong & Kim, 2021), which optimizes the neural network to learn the matching prior of an image pair at test time, requires approximately 300 steps. These results highlight that DiffMatch finds a shorter and better path to accurate matches in relatively fewer steps during the inference phase.\nC.2 UNCERTAINTY ESTIMATION\nInterestingly, DiffMatch naturally derives the uncertainty of estimated matches by taking advantage of the inherent stochastic property of a generative model. We accomplish this by calculating the pixel-level variance in generated samples across various initializations of Gaussian noise FT . On the other hand, it is crucial to determine when and where to trust estimated matches in dense correspondence (Truong et al., 2021; Kondermann et al., 2008; Mac Aodha et al., 2012; Bruhn & Weickert, 2006; Kybic & Nieuwenhuis, 2011; Wannenwetsch et al., 2017; Ummenhofer et al., 2017). Earlier approaches (Kondermann et al., 2007; 2008; Mac Aodha et al., 2012) relied on post-hoc techniques to assess the reliability of models, while more recent model-inherent approaches (Truong\net al., 2021; Bruhn & Weickert, 2006; Kybic & Nieuwenhuis, 2011; Wannenwetsch et al., 2017; Ummenhofer et al., 2017) have developed frameworks specifically designed for uncertainty estimation. The trustworthiness of this uncertainty is showcased in figure 6. We found a direct correspondence between highly erroneous locations and high-variance locations, emphasizing the potential to interpret the variance as uncertainty. We believe this provides promising opportunities for applications demanding high reliability, such as medical imaging (Abi-Nahed et al., 2006) and autonomous driving (Nist\u00e9r et al., 2004; Chen et al., 2016).\nC.3 THE EFFECTIVENESS OF GENERATIVE MATCHING PRIOR\nDiffMatch effectively learns the matching manifold and finds natural and precise matches. In contrast, the raw correlation volume, which is computed by dense scalar products between the source and target descriptors, fails to find accurate point-to-point feature relationships in inherent ambiguities in dense correspondence, including repetitive patterns, textureless regions, large displacements, or noises. To highlight the effectiveness of our generative matching prior, we compare the matching performance evaluated by raw correlation and our method under harshly corrupted settings in Table 6 and figure 8.\nThe corruptions introduced by (Hendrycks & Dietterich, 2019) contain the inherent ambiguities in dense correspondence. For instance, snow and frost corruptions obstruct the image pairs by creating repetitive patterns, while fog and brightness corruptions form homogeneous regions. Under these conditions, raw correlation volume fails to find precise point-to-point feature relationships. Conversely, our method effectively finds natural and exact matches within the learned matching manifold, even under severely corrupted conditions. These results highlight the efficacy of our generative prior, which learns both the likelihood and the matching prior, thereby finding the natural matching field even under extreme corruption.\nAs earlier methods (P\u00e9rez et al., 2013; Drulea & Nedevschi, 2011; Werlberger et al., 2010; Lhuillier & Quan, 2000; Liu et al., 2010; Ham et al., 2016) design a hand-crafted prior term as a smoothness\nconstraint, we can assume that the smoothness of the flow field is included in this prior knowledge of the matching field. Based on this understanding, we reinterpret Table 6 and Figure 8, showing the comparison between the results of raw correlation and learning-based methods (Truong et al., 2020b;a; 2023; 2021). Previous learning-based approaches predict the matching field with raw correlation between an image pair as a condition. We observe that despite the absence of an explicit prior term in these methods, the qualitative results from them exhibit notably smoother results compared to raw correlation. This difference serves as indicative evidence that the neural network architecture may implicitly learn the matching prior with a large-scale dataset.\nHowever, it is important to note that the concept of prior extends beyond mere smoothness. This broader understanding underlines the importance of explicitly learning both the data and prior terms simultaneously, as demonstrated in our performance.\nC.4 COMPARISON WITH DIFFUSION-BASED DENSE PREDICTION MODELS\nPrevious works (Ji et al., 2023; Gu et al., 2022; Saxena et al., 2023b; Duan et al., 2023; Saxena et al., 2023a), applying a diffusion model for dense prediction, such as semantic segmentation (Ji et al., 2023; Gu et al., 2022), or monocular depth estimation (Ji et al., 2023; Duan et al., 2023; Saxena et al., 2023a), use a single RGB image or its feature descriptor as a condition to predict specific dense predictions, such as segmentation or depth map, aligned with the input RGB image. A concurrent study (Saxena et al., 2023a) has applied a diffusion model to predict optical flow, concatenating feature descriptors from both source and target images as input conditions. However, it is\nnotable that this model is limited to scenarios involving small displacements, typical in optical flow tasks, which differ from the main focus of our study. In contrast, our objective is to predict dense correspondence between two RGB images, source Isrc and target Itgt, in more challenging scenarios such as image pairs containing textureless regions, repetitive patterns, large displacements, or noise. To achieve this, we introduce a novel conditioning method which leverages a local cost volume Cl and initial flow field Finit between two images as conditions, containing the pixel-wise interaction between the given images and the initial guess of dense correspondence, respectively.\nTo validate the effectiveness of our architecture design, we further train our model using only feature descriptors from source and target, Dsrc and Dtgt, as conditions. This could be a similar architecture design to DDP (Ji et al., 2023) and DDVM (Saxena et al., 2023a), which only condition the feature descriptors from input RGB images. In Table 7, we present quantitative results to compare different conditioning methods and observe that the results with our conditioning method significantly outperform those using two feature descriptors. We believe that the observed results are attributed to the considerable architectural design choice, specifically tailored for dense correspondence."
        },
        {
            "heading": "D ADDITIONAL RESULTS",
            "text": "D.1 MORE QUALITATIVE COMPARISON ON HPATCHES AND ETH3D\nWe provide a more detailed comparison between our method and other state-of-the-art methods on HPatches (Balntas et al., 2017) in figure 9 and ETH3D (Schops et al., 2017) in figure 10.\nD.2 MORE QUALITATIVE COMPARISON IN CORRUPTED SETTINGS\nWe also present a qualitative comparison on corrupted HPatches (Balntas et al., 2017) and ETH3D (Schops et al., 2017) in figure 11 and figure 12, respectively.\nD.3 MEGADEPTH\nTo further evaluate the generalizability of our method, we expanded our evaluation to include the MegaDepth dataset (Li & Snavely, 2018), known for its extensive collection of image pairs exhibiting extreme variations in viewpoint and appearance. Following the procedures used in PDC-Net+ (Truong et al., 2023), we tested our method on 1,600 images.\nThe quantitative results, presented in Table 8, demonstrate that our approach surpasses PDC-Net+ in performance on the MegaDepth dataset, thereby highlighting the potential for generalizability of our method."
        },
        {
            "heading": "E LIMITATIONS AND FUTURE WORK",
            "text": "To the best of our knowledge, we are the first to formulate the dense correspondence task using a generative approach. Through various experiments, we have demonstrated the significance of learning the manifold of matching fields in dense correspondence. However, our method exhibits slightly lower performance on ETH3D (Schops et al., 2017) during intervals with small displacements. We believe this is attributed to the input resolution of our method. Although we introduced the flow upsampling diffusion module, our resolution still remains lower compared to prior works (Truong et al., 2020b;a; 2021; 2023). We conjecture that this limitation could be addressed by adopting a higher resolution and by utilizing inference techniques specifically aimed at detailed dense correspondence, such as zoom-in (Jiang et al., 2021) and patch-match techniques (Barnes et al., 2009; Lee et al., 2021). In future work, we aim to enhance the matching performance by leveraging feature extractors more advanced than VGG-16 (Simonyan & Zisserman, 2014). Moreover, we plan to improve our architectural designs, increase resolution, and incorporate advanced inference techniques to more accurately capture matches."
        },
        {
            "heading": "F BROADER IMPACT",
            "text": "Dense correspondence applications have diverse uses, including simultaneous localization and mapping (SLAM)(Durrant-Whyte & Bailey, 2006; Bailey & Durrant-Whyte, 2006), structure from motion (SfM)(Schonberger & Frahm, 2016), image editing (Barnes et al., 2009; Cheng et al., 2010; Zhang et al., 2020), and video analysis (Hu et al., 2018; Lai & Xie, 2019). Although there is no inherent misuse of dense correspondence, it can be misused in image editing to produce doctored images of real people. Such misuse of our techniques can lead to societal problems. We strongly discourage the use of our work for disseminating false information or tarnishing reputations."
        }
    ],
    "year": 2023
}