{
    "abstractText": "Graph Neural Networks (GNNs) have shown great promise in learning node embeddings for link prediction (LP). While numerous studies aim to improve the overall LP performance of GNNs, none have explored its varying performance across different nodes and its underlying reasons. To this end, we aim to demystify which nodes will perform better from the perspective of their local topology. Despite the widespread belief that low-degree nodes exhibit poorer LP performance, our empirical findings provide nuances to this viewpoint and prompt us to propose a better metric, Topological Concentration (TC), based on the intersection of the local subgraph of each node with the ones of its neighbors. We empirically demonstrate that TC has a higher correlation with LP performance than other node-level topological metrics like degree and subgraph density, offering a better way to identify low-performing nodes than using degree. With TC, we discover a novel topological distribution shift issue in which newly joined neighbors of a node tend to become less interactive with that node\u2019s existing neighbors, compromising the generalizability of node embeddings for LP at testing time. To make the computation of TC scalable, We further propose Approximated Topological Concentration (ATC) and theoretically/empirically justify its efficacy in approximating TC and reducing the computation complexity. Given the positive correlation between node TC and its LP performance, we explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing and discuss its effectiveness with limitations. Our code is publicly available at https://github.com/submission2024/Topo Concentration.",
    "authors": [],
    "id": "SP:0d1b82bafe59f9706386b89efc17d248af6f0817",
    "references": [
        {
            "authors": [
                "Sergey Brin",
                "Lawrence Page"
            ],
            "title": "The anatomy of a large-scale hypertextual web search engine",
            "venue": "Computer networks and ISDN systems,",
            "year": 1998
        },
        {
            "authors": [
                "Benjamin Paul Chamberlain",
                "Sergey Shirobokov",
                "Emanuele Rossi",
                "Fabrizio Frasca",
                "Thomas Markovich",
                "Nils Hammerla",
                "Michael M Bronstein",
                "Max Hansmire"
            ],
            "title": "Graph neural networks for link prediction with subgraph sketching",
            "venue": "arXiv preprint arXiv:2209.15486,",
            "year": 2022
        },
        {
            "authors": [
                "Deli Chen",
                "Yankai Lin",
                "Guangxiang Zhao",
                "Xuancheng Ren",
                "Peng Li",
                "Jie Zhou",
                "Xu Sun"
            ],
            "title": "Topology-imbalance learning for semi-supervised node classification",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Haochen Chen",
                "Syed Fahad Sultan",
                "Yingtao Tian",
                "Muhao Chen",
                "Steven Skiena"
            ],
            "title": "Fast and accurate network embeddings via very sparse random projection",
            "venue": "In Proceedings of the 28th ACM international conference on information and knowledge management,",
            "year": 2019
        },
        {
            "authors": [
                "Huiyuan Chen",
                "Lan Wang",
                "Yusan Lin",
                "Chin-Chia Michael Yeh",
                "Fei Wang",
                "Hao Yang"
            ],
            "title": "Structured graph convolutional networks with stochastic masks for recommender systems",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Manqing Dong",
                "Feng Yuan",
                "Lina Yao",
                "Xiwei Xu",
                "Liming Zhu"
            ],
            "title": "Mamo: Memory-augmented meta-optimization for cold-start recommendation",
            "venue": "In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2020
        },
        {
            "authors": [
                "Wenqi Fan",
                "Yao Ma",
                "Qing Li",
                "Yuan He",
                "Eric Zhao",
                "Jiliang Tang",
                "Dawei Yin"
            ],
            "title": "Graph neural networks for social recommendation",
            "venue": "In The world wide web conference,",
            "year": 2019
        },
        {
            "authors": [
                "Hao-Ming Fu",
                "Patrick Poirson",
                "Kwot Sin Lee",
                "Chen Wang"
            ],
            "title": "Revisiting neighborhood-based link prediction for collaborative filtering",
            "venue": "In Companion Proceedings of the Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Stefan Wei\u00dfenberger",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Diffusion improves graph learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Liang Ge",
                "Aidong Zhang"
            ],
            "title": "Pseudo cold start link prediction with multiple sources in social networks",
            "venue": "In Proceedings of the 2012 SIAM International Conference on Data Mining,",
            "year": 2012
        },
        {
            "authors": [
                "Marco Gori",
                "Augusto Pucci",
                "V Roma",
                "I Siena"
            ],
            "title": "Itemrank: A random-walk based scoring algorithm for recommender engines",
            "venue": "In IJCAI,",
            "year": 2007
        },
        {
            "authors": [
                "Aditya Grover",
                "Jure Leskovec"
            ],
            "title": "node2vec: Scalable feature learning for networks",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Xiao Han",
                "Leye Wang",
                "Son N Han",
                "Chao Chen",
                "No\u00ebl Crespi",
                "Reza Farahbakhsh"
            ],
            "title": "Link prediction for new users in social networks",
            "venue": "IEEE International Conference on Communications (ICC),",
            "year": 2015
        },
        {
            "authors": [
                "Bowen Hao",
                "Jing Zhang",
                "Hongzhi Yin",
                "Cuiping Li",
                "Hong Chen"
            ],
            "title": "Pre-training graph neural networks for cold-start users and items representation",
            "venue": "In Proceedings of the 14th ACM International Conference on Web Search and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Xiangnan He",
                "Kuan Deng",
                "Xiang Wang",
                "Yan Li",
                "Yongdong Zhang",
                "Meng Wang"
            ],
            "title": "Lightgcn: Simplifying and powering graph convolution network for recommendation",
            "venue": "In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Jin",
                "Haitao Mao",
                "Zheng Li",
                "Haoming Jiang",
                "Chen Luo",
                "Hongzhi Wen",
                "Haoyu Han",
                "Hanqing Lu",
                "Zhengyang Wang",
                "Ruirui Li"
            ],
            "title": "Amazon-m2: A multilingual multi-locale shopping session dataset for recommendation and text generation",
            "venue": "arXiv preprint arXiv:2307.09688,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907,",
            "year": 2016
        },
        {
            "authors": [
                "Sang Gyu Kwak",
                "Jong Hae Kim"
            ],
            "title": "Central limit theorem: the cornerstone of modern statistics",
            "venue": "Korean journal of anesthesiology,",
            "year": 2017
        },
        {
            "authors": [
                "Vincent Leroy",
                "B Barla Cambazoglu",
                "Francesco Bonchi"
            ],
            "title": "Cold start link prediction",
            "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2010
        },
        {
            "authors": [
                "Juanhui Li",
                "Harry Shomer",
                "Jiayuan Ding",
                "Yiqi Wang",
                "Yao Ma",
                "Neil Shah",
                "Jiliang Tang",
                "Dawei Yin"
            ],
            "title": "Are message passing neural networks really helpful for knowledge graph completion",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Juanhui Li",
                "Harry Shomer",
                "Haitao Mao",
                "Shenglai Zeng",
                "Yao Ma",
                "Neil Shah",
                "Jiliang Tang",
                "Dawei Yin"
            ],
            "title": "Evaluating graph neural networks for link prediction: Current pitfalls and new benchmarking",
            "venue": "arXiv preprint arXiv:2306.10453,",
            "year": 2023
        },
        {
            "authors": [
                "Yunqi Li",
                "Hanxiong Chen",
                "Zuohui Fu",
                "Yingqiang Ge",
                "Yongfeng Zhang"
            ],
            "title": "User-oriented fairness in recommendation",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "David Liben-Nowell",
                "Jon Kleinberg"
            ],
            "title": "The link prediction problem for social networks",
            "venue": "In Proceedings of the twelfth international conference on Information and knowledge management,",
            "year": 2003
        },
        {
            "authors": [
                "Blerina Lika",
                "Kostas Kolomvatsos",
                "Stathes Hadjiefthymiades"
            ],
            "title": "Facing the cold start problem in recommender systems",
            "venue": "Expert systems with applications,",
            "year": 2014
        },
        {
            "authors": [
                "Zemin Liu",
                "Trung-Kien Nguyen",
                "Yuan Fang"
            ],
            "title": "Tail-gnn: Tail-node graph neural networks",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Yao Ma",
                "Ziyi Guo",
                "Zhaocun Ren",
                "Jiliang Tang",
                "Dawei Yin"
            ],
            "title": "Streaming graph neural networks",
            "venue": "In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Haitao Mao",
                "Zhikai Chen",
                "Wei Jin",
                "Haoyu Han",
                "Yao Ma",
                "Tong Zhao",
                "Neil Shah",
                "Jiliang Tang"
            ],
            "title": "Demystifying structural disparity in graph neural networks: Can one size fit all",
            "venue": "arXiv preprint arXiv:2306.01323,",
            "year": 2023
        },
        {
            "authors": [
                "Mark EJ Newman"
            ],
            "title": "Clustering and preferential attachment in growing networks",
            "venue": "Physical review E,",
            "year": 2001
        },
        {
            "authors": [
                "John Palowitch",
                "Anton Tsitsulin",
                "Brandon Mayer",
                "Bryan Perozzi"
            ],
            "title": "Graphworld: Fake graphs bring real insights for gnns",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Liming Pan",
                "Cheng Shi",
                "Ivan Dokmani\u0107"
            ],
            "title": "Neural link prediction with walk pooling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Joonhyung Park",
                "Jaeyun Song",
                "Eunho Yang"
            ],
            "title": "Graphens: Neighbor-aware ego network synthesis for class-imbalanced node classification",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Bryan Perozzi",
                "Rami Al-Rfou",
                "Steven Skiena"
            ],
            "title": "Deepwalk: Online learning of social representations",
            "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2014
        },
        {
            "authors": [
                "Hossein A Rahmani",
                "Mohammadmehdi Naghiaei",
                "Mahdi Dehghan",
                "Mohammad Aliannejadi"
            ],
            "title": "Experiments on generalizability of user-oriented fairness in recommender systems",
            "venue": "In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2022
        },
        {
            "authors": [
                "Emanuele Rossi",
                "Ben Chamberlain",
                "Fabrizio Frasca",
                "Davide Eynard",
                "Federico Monti",
                "Michael Bronstein"
            ],
            "title": "Temporal graph networks for deep learning on dynamic graphs",
            "venue": "arXiv preprint arXiv:2006.10637,",
            "year": 2020
        },
        {
            "authors": [
                "Ryan A. Rossi",
                "Nesreen K. Ahmed"
            ],
            "title": "The network data repository with interactive graph analytics and visualization",
            "venue": "In AAAI,",
            "year": 2015
        },
        {
            "authors": [
                "Benedek Rozemberczki",
                "Charles Tapley Hoyt",
                "Anna Gogleva",
                "Piotr Grabowski",
                "Klas Karis",
                "Andrej Lamov",
                "Andriy Nikolov",
                "Sebastian Nilsson",
                "Michael Ughetto",
                "Yu Wang"
            ],
            "title": "Chemicalx: A deep learning library for drug pair scoring",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "MN Sanders"
            ],
            "title": "Characteristic function of the central chi-squared distribution",
            "year": 2009
        },
        {
            "authors": [
                "Hibiki Taguchi",
                "Xin Liu",
                "Tsuyoshi Murata"
            ],
            "title": "Graph convolutional networks for graphs containing missing features",
            "venue": "Future Generation Computer Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xianfeng Tang",
                "Huaxiu Yao",
                "Yiwei Sun",
                "Yiqi Wang",
                "Jiliang Tang",
                "Charu Aggarwal",
                "Prasenjit Mitra",
                "Suhang Wang"
            ],
            "title": "Investigating and mitigating degree-related biases in graph convoltuional networks",
            "venue": "In Proceedings of the 29th ACM International Conference on Information & Knowledge Management,",
            "year": 2020
        },
        {
            "authors": [
                "Hongwei Wang",
                "Fuzheng Zhang",
                "Miao Zhao",
                "Wenjie Li",
                "Xing Xie",
                "Minyi Guo"
            ],
            "title": "Multi-task feature learning for knowledge graph enhanced recommendation",
            "venue": "In The world wide web conference,",
            "year": 2019
        },
        {
            "authors": [
                "Ruijia Wang",
                "Xiao Wang",
                "Chuan Shi",
                "Le Song"
            ],
            "title": "Uncovering the structural fairness in graph contrastive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xiyuan Wang",
                "Haotong Yang",
                "Muhan Zhang"
            ],
            "title": "Neural common neighbor with completion for link prediction",
            "venue": "arXiv preprint arXiv:2302.00890,",
            "year": 2023
        },
        {
            "authors": [
                "Yiwei Wang",
                "Wei Wang",
                "Yuxuan Liang",
                "Yujun Cai",
                "Bryan Hooi"
            ],
            "title": "Mixup for node and graph classification",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Yu Wang",
                "Tyler Derr"
            ],
            "title": "Degree-related bias in link prediction",
            "venue": "IEEE International Conference on Data Mining Workshops (ICDMW),",
            "year": 2022
        },
        {
            "authors": [
                "Yu Wang",
                "Yuying Zhao",
                "Neil Shah",
                "Tyler Derr"
            ],
            "title": "Imbalanced graph classification via graph-ofgraph neural networks",
            "venue": "In Proceedings of the 31st ACM International Conference on Information & Knowledge Management,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqiang Wang",
                "Jiye Liang",
                "Ru Li",
                "Yuhua Qian"
            ],
            "title": "An approach to cold-start link prediction: Establishing connections between non-topological and topological information",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2016
        },
        {
            "authors": [
                "Tianxin Wei",
                "Jingrui He"
            ],
            "title": "Comprehensive fair meta-learned recommender system",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 1999
        },
        {
            "authors": [
                "Linchuan Xu",
                "Xiaokai Wei",
                "Jiannong Cao",
                "Philip S Yu"
            ],
            "title": "On learning mixed community-specific similarity metrics for cold-start link prediction",
            "venue": "In Proceedings of the 26th International Conference on World Wide Web Companion,",
            "year": 2017
        },
        {
            "authors": [
                "Ming Yan",
                "Jitao Sang",
                "Tao Mei",
                "Changsheng Xu"
            ],
            "title": "Friend transfer: Cold-start friend recommendation with cross-platform transfer learning of social knowledge",
            "venue": "IEEE International Conference on Multimedia and Expo (ICME),",
            "year": 2013
        },
        {
            "authors": [
                "Haoteng Yin",
                "Muhan Zhang",
                "Yanbang Wang",
                "Jianguo Wang",
                "Pan Li"
            ],
            "title": "Algorithm and system co-design for efficient subgraph-based graph representation learning",
            "venue": "arXiv preprint arXiv:2202.13538,",
            "year": 2022
        },
        {
            "authors": [
                "Rex Ying",
                "Ruining He",
                "Kaifeng Chen",
                "Pong Eksombatchai",
                "William L Hamilton",
                "Jure Leskovec"
            ],
            "title": "Graph convolutional neural networks for web-scale recommender systems",
            "venue": "In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2018
        },
        {
            "authors": [
                "Seongjun Yun",
                "Seoyoon Kim",
                "Junhyun Lee",
                "Jaewoo Kang",
                "Hyunwoo J Kim"
            ],
            "title": "Neo-gnns: Neighborhood overlap-aware graph neural networks for link prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Muhan Zhang",
                "Yixin Chen"
            ],
            "title": "Link prediction based on graph neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Tianxiang Zhao",
                "Xiang Zhang",
                "Suhang Wang"
            ],
            "title": "Graphsmote: Imbalanced node classification on graphs with graph neural networks",
            "venue": "In Proceedings of the 14th ACM international conference on web search and data mining,",
            "year": 2021
        },
        {
            "authors": [
                "Tong Zhao",
                "Yozen Liu",
                "Leonardo Neves",
                "Oliver Woodford",
                "Meng Jiang",
                "Neil Shah"
            ],
            "title": "Data augmentation for graph neural networks",
            "venue": "In Proceedings of the aaai conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Tong Zhao",
                "Gang Liu",
                "Daheng Wang",
                "Wenhao Yu",
                "Meng Jiang"
            ],
            "title": "Learning from counterfactual links for link prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyu Zhao",
                "Haochen Liu",
                "Wenqi Fan",
                "Hui Liu",
                "Jiliang Tang",
                "Chong Wang",
                "Ming Chen",
                "Xudong Zheng",
                "Xiaobing Liu",
                "Xiwang Yang"
            ],
            "title": "Autoemb: Automated embedding dimensionality search in streaming recommendations",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2021
        },
        {
            "authors": [
                "Xiangyu Zhao",
                "Haochen Liu",
                "Hui Liu",
                "Jiliang Tang",
                "Weiwei Guo",
                "Jun Shi",
                "Sida Wang",
                "Huiji Gao",
                "Bo Long"
            ],
            "title": "Autodim: Field-aware embedding dimension searchin recommender systems",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Jiong Zhu",
                "Yujun Yan",
                "Lingxiao Zhao",
                "Mark Heimann",
                "Leman Akoglu",
                "Danai Koutra"
            ],
            "title": "Beyond homophily in graph neural networks: Current limitations and effective designs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jiong Zhu",
                "Ryan A Rossi",
                "Anup Rao",
                "Tung Mai",
                "Nedim Lipka",
                "Nesreen K Ahmed",
                "Danai Koutra"
            ],
            "title": "Graph neural networks with heterophily",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Dong"
            ],
            "title": "low-activity and high-activity users",
            "year": 2021
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2019) define cold-start users/nodes as the ones with few/little",
            "year": 2019
        },
        {
            "authors": [
                "X@Ki"
            ],
            "title": "X@Kj , di = dj , we model X@K|d as a random variable and expect to find the relationship between its expectation and the node degree d, i.e., f : E(X@K|d) = f(d). Following many existing ranking works (He et al., 2020; Chen et al., 2021b), we assume without loss of generalizability that the link predictor P ranking the predicted neighbors based on",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent years have witnessed unprecedented success in applying link prediction (LP) in real-world applications (Ying et al., 2018; Rozemberczki et al., 2022; Li et al., 2023a). Compared with heuristic-based (Brin & Page, 1998; Liben-Nowell & Kleinberg, 2003) and shallow embeddingbased LP approaches (Perozzi et al., 2014; Grover & Leskovec, 2016), GNN-based ones (Zhang & Chen, 2018; Chamberlain et al., 2022) have achieved state-of-the-art (SOTA) performance; these methods first learn node/subgraph embeddings by applying linear transformations with messagepassing and a decoder/pooling layer to predict link scores/subgraph class. While existing works are dedicated to boosting overall LP performance (Zhao et al., 2022; Chen et al., 2021b) by more expressive message-passing or data augmentation, it is heavily under-explored whether different nodes within a graph would obtain embeddings of different quality and have varying LP performance.\nPrevious works have explored GNNs\u2019 varying performance on nodes within a graph, considering factors like local topology (e.g., degree and homophily/heterophily) (Tang et al., 2020; Mao et al., 2023), feature quality (Taguchi et al., 2021), and class quantity (Zhao et al., 2021a). While these studies have provided significant insights, their focus has primarily remained on node/graph-level tasks, leaving the realm of LP unexplored. A more profound examination of the node-varying LP performance can enhance our comprehension of network dynamics (Liben-Nowell & Kleinberg, 2003), facilitate the timely detection of nodes with ill-topology (Lika et al., 2014), and pave the way for customized data-driven strategies to elevate specific nodes\u2019 LP performance (Zhao et al., 2021b). Recognizing the criticality of studying the node-varying LP performance and the apparent gap in the existing literature, we ask:\nCan we propose a metric that measures GNNs\u2019 varying LP performance across different nodes?\nTo answer a related question in the node classification task, prior works observed that GNNs perform better on high-degree nodes than low-degree nodes (Tang et al., 2020; Liu et al., 2021). Similarly, the persistent sparse topology issue in the general LP domain and recommender systems (Hao et al., 2021; Li et al., 2021) indicates that nodes with zero-to-low degrees lag behind their highdegree counterparts. However, as surprisingly shown in Figure 1(a)-(b), GNN-based LP on these two large-scale social networks does not exhibit a consistent performance trend as the node degree increases. For example, the performance@10 on Collab under all evaluation metrics decreases as the node degree increases, while on Citation2, performance@10 first increases and then decreases. This counter-intuitive observation indicates the weak correlation between the node degree and LP performance, which motivates us to design a more correlated metric to answer the above question.\nFollowing (Zhang & Chen, 2018) that the link formation between each pair of nodes depends on the interaction between their local subgraphs, we probe the relation between the local subgraphs around each node (i.e., its computation tree) and its GNN-based LP performance. Specifically, we propose Topological Concentration (TC) and its scalable version, Approximated Topological Concentration (ATC), to measure the topological interaction between the local subgraph of each node and the local subgraphs of the neighbors of that node. Our empirical observations show that TC offers a superior characterization of node LP performance in GNNs, leading to 82.10% more correlation with LP performance and roughly 200% increase in the performance gap between the identified under-performed nodes and their counterparts than degree. Moreover, with TC, we discover a novel topological distribution shift (TDS) in which newly joined neighbors of a node tend to become less interactive with that node\u2019s existing neighbors. This TDS would compromise the generalizability of the learned node embeddings in LP at the testing time. Given the closer correlation between TC and LP performance, we reweigh the edges in message-passing to enhance TC and discuss its efficacy/limitations in boosting LP performance. Our contributions are summarized as follows:\n\u2022 We propose Topological Concentration (TC) and demonstrate it leads to 82.10% more correlation with LP performance and roughly 200% increase in the performance gap between the identified under-performed nodes and their counterparts than node degree, shedding new insights on degreerelated issues in LP. We further propose Approximated Topological Concentration (ATC) and demonstrate it maintains high correlations to the LP performance similar to TC while significantly reducing the computation complexity.\n\u2022 We uncover a novel Topological Distribution Shift (TDS) issue according to TC and demonstrate its negative impact at the node/graph level for link prediction at the testing time. Moreover, we discover that different nodes within the same graph can have varying amounts of TDS.\n\u2022 We design a TC inspired message-passing where a node aggregates more from neighbors who are better connected within its computational tree, which can enhance the node\u2019s weighted TC. We observe this empirically boosts LP performance and lastly discuss its noncausal limitations."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Varying Performance of GNNs on Node/Graph Classification. GNNs\u2019 efficacy in classification differs across nodes/graphs with varying label quantity (e.g., imbalanced node/graph classification (Zhao et al., 2021a; Wang et al., 2022b)) and varying topology quality (e.g., long-tailed (Tang et al., 2020; Liu et al., 2021)/heterophily node classification (Zhu et al., 2020; Mao et al., 2023)). To enhance GNNs\u2019 performance for the disadvantaged nodes/graphs in these two varying conditions, previous works either apply data augmentations to derive additional supervision (Wang et al., 2021; Park et al., 2021) or design expressive graph convolutions to mitigate structural bias (Zhu et al., 2021). However, none of them tackle the varying performance of nodes in LP. We fill this gap by studying the relationship between node LP performance and its local topology.\nGNN-based LP and Node-Centric Evaluation. GNN-based LP works by first learning node embeddings/subgraph embeddings through linear transformation and message-passing, and then applying the scoring function to predict link probability/subgraph class (Zhang & Chen, 2018; Yin et al., 2022). It has achieved new SOTA performance owing to using the neural network to extract taskrelated information and the message-passing to encode the topological properties (e.g., common neighbors) (Yun et al., 2021; Chamberlain et al., 2022). Existing GNN-based LP baselines evaluate performance by computing the average rank of each link against the randomly sampled negative links (Hu et al., 2020). However, because these sampled negative links only count a tiny portion of the quadratic node pairs, this evaluation contains positional bias (Li et al., 2023b). In view of this issue, we leverage the node-centric evaluation metrics (Precision/F1/NDCG/Recall/HitsN@K) that are frequently used in recommender systems (Gori et al., 2007; He et al., 2020) and rank each node against all other nodes in predicting the incoming neighbors. Detailed definitions of these evaluation metrics are provided in Appendix C.\nVarying Performance of GNNs on LP. As LP nowadays has been heavily used to enhance user experience in social/e-commerce recommendations (Fan et al., 2019; Jin et al., 2023), studying its varying performance across different users has real-world applications such as identify users with ill-topology and take augmentation strategies. Although no efforts have been investigated into the node-varying performance in GNN-based LP, prior work (Li et al., 2021; Rahmani et al., 2022) have investigated the relation of node-varying LP performance with its degree, and both claimed that users/nodes with higher activity levels/degrees tend to possess better recommendation performance than their less active counterparts, which also aligns with observations in GNN-based node classification (Tang et al., 2020; Liu et al., 2021). However, Figure 1(c)-(d) has already raised concern over the validity of this claim in LP. we follow (Wang & Derr, 2022) and theoretically discover that some node-centric evaluation metrics have degree-related bias in Appendix D.2, implying that the GNNs\u2019 varying LP performance could be partially attributed to the choice of evaluation metrics. To mitigate this bias, we employ a full spectrum of evaluation metrics and find that degree is not so correlated with the node LP performance. This motivates us to devise a better topological metric than the degree. Note that although some prior works also define cold-start nodes to be the ones with few degrees, we systematically review their connections and differences in Appendix B."
        },
        {
            "heading": "3 TOPOLOGICAL CONCENTRATION",
            "text": ""
        },
        {
            "heading": "3.1 NOTATIONS",
            "text": "Let G = (V, E ,X) be an attributed graph, where V = {vi}ni=1 is the set of n nodes (i.e., n = |V|) and E \u2286 V \u00d7 V is the set of m observed training edges (i.e., m = |E|) with eij denoting the edge between the node vi and vj , and X \u2208 Rn\u00d7d represents the node feature matrix. The observed adjacency matrix of the graph is denoted as A \u2208 {0, 1}n\u00d7n with Aij = 1 if an observed edge exists between node vi and vj and Aij = 0 otherwise. The diagonal matrix of node degree is notated as D \u2208 Zn\u00d7n with the degree of node vi being di = Dii = \u2211n j=1 Aij . For the LP task, edges are usually divided into three groups notated as T = {Train,Val,Test}, i.e., training, validation, and testing sets, respectively. We denote N ti , t \u2208 T as node vi\u2019s 1-hop neighbors according to edge group t. Furthermore, we denote the set of nodes that have at least one path of length k to node i based on observed training edges as Hki and naturally H1i = N Traini . Note that H k1 i \u2229 H k2 i is not necessarily empty since neighbors that are k1-hops away from vi could also have paths of length k2 reaching vi. We collect vi\u2019s neighbors at all different hops until K to form the K-hop computation tree centered on vi as SKi = {Hki }Kk=1. We summarize all notations in Table 2 in Appendix A."
        },
        {
            "heading": "3.2 TOPOLOGICAL CONCENTRATION: INTUITION AND FORMALIZATION",
            "text": "As the link formation between a node pair heavily depends on the intersection between their local subgraphs (Zhang & Chen, 2018; Chamberlain et al., 2022), we similarly hypothesize the predictability of a node\u2019s neighbors relates to the intersection between this node\u2019s subgraph and the subgraphs of that node\u2019s neighbors, e.g., the prediction of the links {(i, jk)}2k=0 in Figure 2(a) depends on the intersection between SKi and {SKjk} 2 k=0. A higher intersection leads to higher LP performance. For example, in Figure 2(c)-(d), v0 neighbors closely interact with themselves while v\u20320 neighbors do not, posing different topological conditions for the LP on v0 and v \u2032 0. From graph heuristics perspective, v0 shares common neighbors v1, v2, v3 with its incoming validation neighbors v4, v5 while v\u20320 shares no neighbors with v \u2032 4, v \u2032 5. From the message-passing perspective, the propagated embeddings of v0 and v4, v5 share common components since they all aggregate {vk}3k=1 embeddings while v\u20320 and v \u2032 4, v \u2032 5 do not share any common embeddings among {v\u2032k}3k=1. When the subgraph (i.e., computation tree) surrounding a node increasingly overlaps with the subgraphs of its neighbors, more paths originating from that node are likely to loop nearby and eventually return to it, resulting in a more dense/concentrated local topology for that node. Inspired by this observation, we introduce Topological Concentration to measure the average level of intersection among these local subgraphs as follows:\nDefinition 1. Topological Concentration (TC): The Topological Concentration CK,ti for node vi \u2208 V is defined as the average intersection between vi\u2019s K-hop computation tree (SKi ) and the computation trees of each of vi\u2019s type t neighbors:\nCK,ti = Evj\u223cN ti I(S K i ,SKj ) = Evj\u223cN ti\n\u2211K k1=1 \u2211K k2=1\n\u03b2k1+k2\u22122|Hk1i \u2229H k2 j |\u2211K\nk1=1 \u2211K k2=1 \u03b2k1+k2\u22122g(|Hk1i |, |H k2 j |)\n(1)\n\u2200vi \u2208 V,\u2200t \u2208 T , where I(SKi ,SKj ) quantifies the intersection between the K-hop computation trees around vi and vj , and is decomposed into the ratio of the observed intersections |Hk1i \u2229 H k2 j | to the total possible intersections g(Hk1i ,H k2 j ) between neighbors that are k1 and k2 hops away as shown in Figure 2(b). \u03b2k1+k2\u22122 accounts for the exponential discounting effect as the hop increases. The normalization term g is a function of the size of the computation trees of node vi, vj (Fu et al., 2022). Although computation trees only consist of edges from the training set, vi\u2019s neighbors N ti in Eq. (1) could come from training/validation/testing sets, and we term the corresponding TC as TCTrain, TCVal, TCTest and their values as CK,Traini , C K,Val i , C K,Test i . We verify the correlation between TC and the node LP performance in Section 3.3."
        },
        {
            "heading": "3.3 TOPOLOGICAL CONCENTRATION: OBSERVATION AND ANALYSIS",
            "text": "In this section, we draw three empirical observations to delve into the role of TC in GNN-based LP. For all experiments, we evaluate datasets with only the topology information using LightGCN and those also having node features using GCN/SAGE (Kipf & Welling, 2016; Hamilton et al., 2017; He et al., 2020)1. Detailed experimental settings are described in Appendix F. Please note that while the findings illustrated in this section are limited to the presented datasets due to page limitation, more comprehensive results are included in Appendix G.\nObs. 1. TC correlates to LP performance more than other node topological properties. In Figure 3 (a)/(d), we group nodes in Collab/Citation2 based on their TCTrain and visualize the average performance of each group. Unlike Figure 1(a)/(b), where there is no apparent relationship between the performance and the node degree, the performance almost monotonically increases as the node TCTrain increases regardless of the evaluation metrics. This demonstrates the capability of TCTrain in characterizing the quality of nodes\u2019 local topology for their LP performance. Moreover, we quantitatively compare the Pearson Correlation of the node LP performance with TCTrain and other commonly used node local topological properties, DegreeTrain (i.e., the number of training edges incident to a node) and SubGraph Density (i.e., the density of the 1-hop training subgraph centering around a node). As shown in Figure 3(b)/(c), TCTrain almost achieves the highest Pearson Correlation with the node LP performance across every evaluation metric than the other two topological properties except for the precision metric. This is due to the degree-related evaluation bias implicitly encoded in the precision metric, i.e., even for the untrained link predictor, the precision of a node still increases linearly as its degree increases, as proved in Theorem 3. Note that the node\u2019s 1-hop Subgraph Density equals its local clustering coefficient (LCC), and one previous work (Pan et al., 2022) has observed its correlation with node LP performance. To justify the advantages of TCTrain over LCC, we provide a concrete example in Appendix E. Additionally, Figure 3(e) shows that TCTrain also positively correlated with LP performance across various networks, depicting a preliminary benchmark for GNNs\u2019 LP performance at the graph level (Palowitch et al., 2022). The LightGCN architecture exhibits a steeper slope than GCN, as it relies exclusively on network topology without leveraging node features and thus is more sensitive to changes in the purely topological metric, TC. The deviation of Collab under both GCN and LightGCN baselines from the primary linear trend might be attributed to the duplicated edges in the network that create the illusion of a higher TCTrain (Hu et al., 2020).\n1Due to GPU memory limitation, we choose SAGE for Citation2.\nObs. 2. TC better identifies low-performing nodes than degree, and lower-degree nodes may not necessarily have lower LP performance.\nAs previously shown in Figure 1(c)/(d), when the node degree is at the very low regime, we do not observe a strict positive relationship between node DegreeTrain and its LP performance. For example, the node Recall/MRR/NDCG@10 in Collab decreases as DegreeTrain increases and HitsN /F1/Precision@10 first increases and then decreases. These contradicting observations facilitate our hypothesis that the degree might not fully capture the local topology in characterizing the underperforming nodes. Conversely, in Figure 4(a)/(d) on Collab/Citation2, nodes with lower TCTrain almost always have worse LP performance under all evaluation metrics except when TCTrain is between [0, 0.02). For this extreme case, we ascribe it to the distribution shift as nodes with extremely low TCTrain generally have a decent TCTest (shown in Figure 20) and sustain a reasonable LP performance. We thoroughly investigate this distribution shift issue in Obs. 3. Furthermore, we adjust the DegreeTrain from 1 to 10 to group nodes into \u2018Lower-degree/Higher-degree\u2019 and adjust TCTrain from 0.01 to 0.1 to group nodes into \u2018Concentrated/Non-Concentrated\u2019. We compare their average LP performance on Collab/Citation2 in Figure 4(b)/(e). Intriguingly, Lower-degree nodes always perform better than their Higher-degree counterparts across all DegreeTrain thresholds. This brings nuances into the conventional understanding that nodes with a weaker topology (lower degree) would yield inferior performance (Li et al., 2021; Tang et al., 2020; Liu et al., 2021). In contrast, with our defined TCTrain metric, Non-concentrated nodes (lower TCTrain) generally underperform by a noticeable margin than their concentrated counterparts (higher TCTrain).\nWe further visualize the relation between DegreeTrain and TCTrain in Figure 4(c)/(f). When node DegreeTrain increases from 1 to 4, the ratio of nodes owning higher TCTrain also increases because these newly landed nodes start interactions and create their initial topological context. Since we have already observed the positive correlation of TCTrain to nodes\u2019 LP performance previously, the LP performance for some evaluation metrics also increases as the DegreeTrain initially increases from 0 to 4 observed in Figure 1(c)/(d). When DegreeTrain increases further beyond 5, the ratio of nodes owning higher TCTrain gradually decreases, leading to the decreasing performance observed in the later stage of Figure 1(c)/(d). This decreasing TCTrain is because, for high DegreeTrain nodes, their neighbors are likely to lie in different communities and share fewer connections among themselves. For example, in social networks, high-activity users usually possess diverse relations in different online communities, and their interacted people are likely from significantly different domains and hence share less common social relations themselves Zhao et al. (2021c).\nObs. 3. Topological Distribution Shift compromises the LP performance at testing time, and TC can measure its negative impact at both graph and node level. In real-world LP scenarios, new nodes continuously join the network and form new links with existing nodes, making the whole network evolve dynamically (Ma et al., 2020; Rossi et al., 2020). Here, we discover a new Topological Distribution Shift (TDS) issue, i.e., as time goes on, the newly joined neighbors of a node become less interactive with that node\u2019s old neighbors. Since the edges serving message-passing and providing supervision only come from the training set, TDS would compromise the capability of the learned node embeddings for predicting links in the testing set. As verified in Figure 5(a), the performance gap between validation and testing sets on Collab where edges are split according to time is much more significant than the one on Cora/Citeseer where edges are split randomly. Note that the significantly higher performance on predicting training edges among all these three datasets is because they have already been used in the training phase (Wang et al., 2023), and this distribution shift is different from TDS. As TC essentially measures the interaction level among neighbors of a particular node, we further visualize the distribution of the difference between TCVal and TCTest in Figure 5(b). We observe a slight shift towards the right on Collab rather than on Cora/Citeseer, demonstrating nodes\u2019 testing neighbors become less interactive with their training neighbors than their validation neighbors. Figure 4(c) further demonstrates the influence of this shift at the node level by visualizing the relationship between TDS and the performance gap. We can see that as the strength of such shift increases (evidenced by the larger difference between TCVal and TCTest), the performance gap also increases. This suggests that nodes within the same graph display varying levels of TDS. As one potential application, we can devise adaptive data valuation techniques or message-passing mechanisms to selectively use neighborhood information (i.e., emphasize less on stale edges in LP as Chamberlain et al. (2022) did). We demonstrate one use-case in Appendix K."
        },
        {
            "heading": "3.4 TOPOLOGICAL CONCENTRATION: COMPUTATIONAL COMPLEXITY AND OPTIMIZATION",
            "text": "Calculating TC following Eq. (1) involves counting the intersection between two neighboring sets that are different hops away from the centering nodes in two computation trees. Assuming the average degree of the network is d\u0302, the time complexity of computing CK,ti for all nodes in the network is O(|E| \u2211K k=1 \u2211K k=1 min(d\u0302\nk1 , d\u0302k2)) = O(K2|E||V|) \u2248 O(K2|V|2) for sparse networks, which increases quadratically as the size of the network increases and is hence challenging for largescale networks. To handle this issue, we propagate the randomly initialized Gaussian embeddings in the latent space to approximate TC in the topological space and propose Approximated Topological Concentration as follows:\nDefinition 2. Approximated Topological Concentration (ATC): Approximated topological concentration C\u0303K,ti for vi \u2208 V is the average similarity between vi and its neighbors\u2019 embeddings initialized from Gaussian Random Projection (Chen et al., 2019) followed by row-normalized graph diffusion A\u0303k (Gasteiger et al., 2019), with \u03d5 as the similarity metric function:\nC\u0303K,ti = Evj\u223cN ti \u03d5(Ni,Nj), N = K\u2211\nk=1\n\u03b1kA\u0303 kR, R \u223c N (0d,\u03a3d) (2)\nTheorem 1. Assuming g(|Hk1i |, |H k2 j |) = |H k1 i ||H k2 j | in Eq. (1) and let \u03d5 be the dot-product based similarity metric (He et al., 2020), then node vi\u2019s 1-layer Topological Concentration C 1,t i is linear correlated with the mean value of the 1-layer Approximated Topological Concentration \u00b5C\u0303K,ti as:\nC1,ti \u2248 d \u22121\u00b5Evj\u223cNti (E 1 j ) \u22a4E1i = d\u22121\u00b5C\u03031,ti , (3)\nwhere E1 \u2208 Rn\u00d7d denotes the node embeddings after 1-layer SAGE-style message-passing and d is the embedding dimension. The full proof is in Appendix D. This theorem bridges the gap between TC defined in the topological space and ATC defined in the latent space, which theoretically justifies the effectiveness of this approximation. Computationally, obtaining node embeddings N in Eq. (2) is free from optimization, and the graph diffusion can be efficiently executed via power iteration, which reduces the complexity to O(Kd(|E| + |V|)). Note that although we only demonstrate the approximation power for the case of 1-layer message-passing, we empirically verify the efficacy for higher-layer message-passing in the following.\nHere, we compare TCTrain and ATCTrain under various number of hops in terms of their computational time and their correlation with LP performance in Figure 6. As the number of hops increases, the running time for computing TC increases exponentially (especially for large-scale datasets like Collab, we are only affordable to compute its TCTrain up to 3 hops) while ATC stays roughly the same. This aligns with the quadratic/linear time complexity O(K2|V|2)/O(Kd(|E| + |V|)) we derived earlier for TC/ATC. Moreover, ATC achieves a similar level of correlation to TC at all different hops. For\nboth TC and ATC, their correlations to LP performance increase as the number of hops K used in Eq. (1)-Eq. (2) increases. This is because larger K enables us to capture intersections among larger subgraphs and hence accounts for more common neighbor signals (Chamberlain et al., 2022). We include comprehensive analysis including (A)TCVal/(A)TCTest in Appendix J."
        },
        {
            "heading": "4 TOPOLOGICAL CONCENTRATION: BOOSTING GNNS\u2019 LP PERFORMANCE",
            "text": "From the aforementioned observations, TC consistently exhibits a stronger correlation with GNNs\u2019 LP performance than other commonly used node topological metrics. This insight motivates us to explore the potential of boosting GNNs\u2019 LP performance via enhancing TCTrain. Specifically, we propose to vary the edge weight used in message-passing by aggregating more information from neighbors that contribute more to TCTrain. We theoretically and empirically show that using this way could enhance 1-layer TCTrain in Theorem 4 and Figure 7(a). Since neighbors owning more connections with the whole neighborhood have higher LP scores with the average neighborhood embeddings, we update the adjacency matrix used for message-passing as:\nA\u0303\u03c4ij = A\u0303\u03c4\u22121ij + \u03b3 exp(g\u0398g (N \u03c4\u22121 i ,H \u03c4\u22121 j ))\u2211n j=1 exp(g\u0398g (N \u03c4\u22121 i ,H \u03c4\u22121 j )) , ifAij = 1\n0, ifAij = 0 , \u2200vi, vj \u2208 V, (4)\nwhere H\u03c4\u22121 = f\u0398f (A\u0303 \u03c4\u22121,X) is the node embeddings obtained from GNN model f\u0398f , N \u03c4\u22121 = A\u0303H\u03c4\u22121 is the average neighborhood embeddings by performing one more SAGE-style propagation, \u03b3 is the weight coefficient, A\u03030 = A\u0303, and g\u0398g is the link predictor. The detailed algorithm is presented in Appendix H. Note that this edge reweighting strategy directly operates on the original adjacency matrix and hence shares the same time/space complexity as the conventional message-passing O(Ld(|E + V|)) with L being the number of message-passing layers. For baselines obtaining node embeddings without using message-passing such as BUDDY Chamberlain et al. (2022), we reweigh edges in computing the binary cross entropy loss in the training stage. More details are attached Appendix I. Specifically, we equip four baselines, GCN/SAGE/NCN/BUDDY, with our designed edge reweighting strategy and present their LP performance in Table 11. Appendix F thoroughly describes the experimental settings. For GCN, SAGE and BUDDY, equipping with our proposed edge reweighting strategy enhances their LP performance in most cases. This demonstrates that by pushing up the TCTrain of the whole graph, the LP performance can be boosted to a certain level,\nwhich is also verified by the increasing TC shown in Figure 7(a). We hypothesize that neighbors connecting more to the overall neighborhood likely have greater interactions with incoming neighbors. Thus, aggregating more information from them inherently captures the common neighbor signals of these incoming neighbors. Meanwhile, as NCN already explicitly accounts for this common neighbor signal in its decoder, the performance gains from our strategy are relatively modest. Furthermore, the positive trend observed in Figure 7(b) verifies that the larger enhancement in node TCTrain leads to a larger performance boost in its HitsN@10. However, we note that LP performance is evaluated on the incoming testing neighbors rather than training neighbors, so TCTrain is more of a correlated metric than causal. To illustrate, consider an extreme case where a node vi has training neighbors forming a complete graph with no connection to its incoming neighbors. In such a scenario, even if we achieve the maximum TCTrain = 1 and a significantly high training performance, it still hardly generalizes to predicting testing links. This could be reflected in the inconsistent trend in\nFigure 21/22 in Appendix G.7. In addition, Figure 7(c) verifies the assumption made in Theorem 4 and in the reweighting model design that nodes with larger TCTrain have higher average embedding similarity to their neighbors.\nFigure 7: (a) Change of Hits@50 and Reweighted TC along the training process in SAGE. (b) Nodes with enhanced TCTrain exhibit a surge in performance. (c) Embedding similarity during LightGCN training increases faster and higher for nodes with higher TCTrain."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Although many recent works have achieved unprecedented success in enhancing link prediction (LP) performance with GNNs, demystifying the varying levels of embedding quality and LP performance across different nodes within the graph is heavily under-explored yet fundamental. In this work, we take the lead in understanding the nodes\u2019 varying performance from the perspective of their local topology. In view of the connection between link formation and the subgraph interaction, we propose Topological Concentration (TC) to characterize the node LP performance and demonstrate its superiority in leading higher correlation and identifying more low-performing nodes than other common node topological properties. Moreover, we discover a novel topological distribution shift (TDS) issue by observing the changing LP performance over time and demonstrate the capability of using TC to measure this distribution shift. Our work offers the community strong insights into which local topology enables nodes to have better LP performance with GNNs. Moving forward, we plan to investigate the causal relationship between TC and LP performance. Additionally, we aim to utilize TC for data valuation to select consistently crucial edges in dynamic link prediction."
        },
        {
            "heading": "A Notations 17",
            "text": "B Systematic review about the cold-start issue in link prediction and recommender systems 18"
        },
        {
            "heading": "C Link-centric and Node-centric Evaluation Metrics 19",
            "text": "C.1 Link-Centric Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.2 Node-Centric Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "D Proof of Theorems 20",
            "text": "D.1 Approximation power of ATC for TC . . . . . . . . . . . . . . . . . . . . . . . 20 D.2 Degree-related Bias of Evaluation Metrics . . . . . . . . . . . . . . . . . . . . 21 D.3 Reweighting by LP Score Enhance 1-layer TC . . . . . . . . . . . . . . . . . . 23"
        },
        {
            "heading": "E Example demonstrating the advantages of TC over LCC 24",
            "text": ""
        },
        {
            "heading": "F Datasets and Experimental Settings 25",
            "text": "F.1 Dataset Introduction and Statistics . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.2 Hyperparameter Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25"
        },
        {
            "heading": "G Additional Results 26",
            "text": "G.1 Link prediction performance grouped by TCTest . . . . . . . . . . . . . . . . . 26 G.2 Link prediction performance grouped by TCTrain . . . . . . . . . . . . . . . . . 27 G.3 Link prediction performance grouped by DegreeTest . . . . . . . . . . . . . . . 28 G.4 Link prediction performance grouped by DegreeTrain . . . . . . . . . . . . . . . 29 G.5 Relation between LP performance and TC at Graph-level . . . . . . . . . . . . . 30 G.6 Relation between TCTrain and TCTest . . . . . . . . . . . . . . . . . . . . . . . 30 G.7 Difference in TC vs Difference in Performance before/after applying reweighting 30 G.8 Correlation of the performance with TC and Degree . . . . . . . . . . . . . . . 30"
        },
        {
            "heading": "H Edge Reweighting Algorithm 39",
            "text": ""
        },
        {
            "heading": "I Reweigh edges for baselines without message-passing 39",
            "text": ""
        },
        {
            "heading": "J Comparing the Efficiency between baseline and their augmented version by TC 40",
            "text": "K Reweighting training neighbors based on their connections to training neighbors or validation neighbors 40\nL Explaining why the curve has several fast down of performances 41"
        },
        {
            "heading": "A NOTATIONS",
            "text": "This section summarizes all notations used throughout this paper.\nSKi = {Hki }Kk=1 K-hop computational tree centered on the node vi CK,ti \\C\u0303 K,t i (Approximated) Topological concentration for node vi considering the intersection among K-hop computational trees among its type t neighbors.\nEki Embedding of the node vi after k th-layer message-passing\nRij Sample from gaussian random variable N (0, 1/d) g\u0398g Link predictor parameterized by \u0398g E\u0303i, E\u0302i Predicted and ground-truth neighbors of node vi HG Hypergeometric distribution LP Link Prediction\n(A)TC (Approxminated) Topological Concentration TDS Topological Distribution Shift \u03b2 Exponential discounting effect as the hop increases \u03b1k Weighted coefficient of layer k in computing ATC \u00b5 Mean of the distribution L Number of message-passing layers \u03b3 Coefficients measuring the contribution of updating adjacency matrix"
        },
        {
            "heading": "B SYSTEMATIC REVIEW ABOUT THE COLD-START ISSUE IN LINK PREDICTION AND RECOMMENDER SYSTEMS",
            "text": "One line of the research (Wang et al., 2016; Han et al., 2015; Yan et al., 2013; Xu et al., 2017; Leroy et al., 2010; Ge & Zhang, 2012) defines the cold-start nodes as the ones with little to no topological information (isolated user) and augment these disadvantaged groups with auxiliary information, e.g., user profile/rich text information, community information, and group membership. Specifically, (Yan et al., 2013) derive the auxiliary information based on the interactions of these disadvantageous nodes/users from their cross-platform behaviors. (Leroy et al., 2010) constructs the probabilistic graph and then refines it by considering the transitivity of the contract relationship. (Ge & Zhang, 2012) incorporates feature selection and regularization to avoid overfitting. The other line of research (Li et al., 2021; Rahmani et al., 2022; Wei & He, 2022; Dong et al., 2020; Hao et al., 2021; Wang et al., 2019) studies the cold-start issue from the user perspective in recommender systems. They usually define cold-start nodes/users as the ones with no to sparse/low activities. (Li et al., 2021; Rahmani et al., 2022) devises a re-ranking strategy by optimizing the performance gap between low-activity and high-activity users. (Wei & He, 2022; Dong et al., 2020) design multiple meta-learning frameworks to learn user preferences based on his/her only a few past interactions. (Wang et al., 2019) uses knowledge graph embedding to assist with recommendation tasks for lowactivity users while (Hao et al., 2021) trains GNNs to adapt to cold-start nodes by mimicking the cold-start scenario for warm users.\nFollowing the above second line of research, we study the cold-start link prediction at the node level since our paper targets demystifying the varying link prediction performance across different nodes. Therefore, we follow some conventional literature (Li et al., 2021; Wei & He, 2022; Dong et al., 2020; Wang et al., 2019) and deem the nodes with generally few degrees as cold-start ones. Particularly, in Figure 4(b)/(e), we change the degree threshold from 1 to 10, divide nodes into two groups at each degree threshold, and further visualize the average performance for each group. We can see that nodes in the lower-degree groups generally have higher performance than nodes in the higher-degree groups. The above observation has two promising insights compared with conventional literature:\n\u2022 (1) Many existing recommendation-based papers (Li et al., 2021; Newman, 2001; Dong et al., 2020; Wang et al., 2019) define cold-start users/nodes as the ones with few/little interactions/topological signals. However, our paper empirically demonstrates that nodes with lower degree even exhibit higher LP performance.\n\u2022 (2) Many existing node classification papers (Tang et al., 2020; Wang et al., 2022a; Chen et al., 2021a) find nodes with low degrees have lower performance. However, our work sheds new insights on the degree-related bias in link prediction where nodes with lower degrees actually possess higher performance.\nWe justify the above 1st insight by relating to real-world scenarios where users with high degrees usually tend to possess diverse interests (nodes with higher degrees may tend to belong to diverse communities) and therefore, using the equal capacity of embedding cannot equally characterize all of their interests (Zhao et al., 2021d).\nWe justify the above 2st insight by relating to the inherent difference between the mechanism of node classification and the mechanism of link prediction. For node classification, high-degree nodes are more likely to obtain the supervised signals from labeled nodes in the same class (Chen et al., 2021a). For link prediction, the ground-truth class for each node is actually its testing neighbors and hence when performing message-passing, beneficial supervision signals are not guaranteed to be captured more by high-degree nodes.\nIn our paper, we focus on the performance difference between low-degree nodes and high-degree nodes rather than the cold-start issue. However, if we also consider cold-start nodes as the ones with sparse interactions as some previous work did (Li et al., 2021; Rahmani et al., 2022), then our analysis and observation can also apply there."
        },
        {
            "heading": "C LINK-CENTRIC AND NODE-CENTRIC EVALUATION METRICS",
            "text": "In addition to the conventional link-centric evaluation metrics used in this work, node-centric evaluation metrics are also used to mitigate the positional bias caused by the tiny portion of the sampled negative links. We introduce their mathematical definition respectively as follows:\nC.1 LINK-CENTRIC EVALUATION\nFollowing (Hu et al., 2020), we rank the prediction score of each link among a set of randomly sampled negative node pairs and calculate the link-centric evaluation metric Hits@K as the ratio of positive edges that are ranked at K th-place or above. Note that this evaluation may cause bias as the sampled negative links only count a tiny portion of the quadratic node pairs (Li et al., 2023b). Hereafter, we introduce the node-centric evaluation metrics and specifically denote the node-level Hit ratio as HitsN@K to differentiate it from the link-centric evaluation metric Hits@K.\nC.2 NODE-CENTRIC EVALUATION\nFor each node vi \u2208 V , the model predicts the link formation score between vi and every other node, and selects the top-K nodes to form the potential candidates E\u0303i. Since the ground-truth candidates for node vi is N Testi (hereafter, we notate as E\u0302i), we can compute the Recall(R), Precision(P), F1, NDCG(N), MRR and HitsN of vi as follows:\nR@Ki = |E\u0303i \u2229 E\u0302i| |E\u0302i| , P@Ki = |E\u0303i \u2229 E\u0302i| K (5)\nF1@Ki = 2|E\u0303i \u2229 E\u0302i| K + |E\u0302i| , N@Ki =\n\u2211K k=1 1[v \u03d5k i \u2208(E\u0303i\u2229E\u0302i)]\nlog2(k+1)\u2211K k=1 1 log2(k+1)\n(6)\nMRR@Ki = 1\nminv\u2208(E\u0303i\u2229E\u0302i) Rankv , HitsN@Ki = 1[|E\u0302i \u2229 E\u0303i| > 0], (7)\nwhere \u03d5ki denotes vi\u2019s k th preferred node according to the ranking of the link prediction score, Rankv is the ranking of the node v and 1 is the indicator function equating 0 if the intersection between E\u0302 i \u2229 E\u0303i is empty otherwise 1. The final performance of each dataset is averaged across each node:\nX@K = Evi\u2208VX@Ki,X \u2208 {R,P,F1,N,MRR,Hits N} (8)\nBecause for each node, the predicted neighbors will be compared against all the other nodes, there is no evaluation bias compared with the link-centric evaluation, where only a set of randomly selected negative node pairs are used."
        },
        {
            "heading": "D PROOF OF THEOREMS",
            "text": "D.1 APPROXIMATION POWER OF ATC FOR TC\nTheorem 1. Assuming g(|Hk1i |, |H k2 j |) = |H k1 i ||H k2 j | in Eq. (1) and let \u03d5 be the dot-product based similarity metric (He et al., 2020), then node vi\u2019s 1-layer Topological Concentration C 1,t i is linear correlated with the mean value of the 1-layer Approximated Topological Concentration \u00b5C\u0303K,ti as:\nC1,ti \u2248 d \u22121\u00b5Evj\u223cNti (E 1 j ) \u22a4E1i = d\u22121\u00b5C\u03031,ti , (9)\nwhere E1 \u2208 Rn\u00d7d denotes the node embeddings after 1-layer SAGE-style message-passing over the node embeddings R \u223c N (0d,\u03a3d) and \u03c7 is the approximation error.\nProof. Assuming without loss of generalizability that the row-normalized adjacency matrix A\u0303 = D\u22121A is used in aggregating neighborhood embeddings. We focus on a randomly selected node Ei \u2208 Rd,\u2200vi \u2208 V and its 1-layer ATC given by Eq. (2) is:\nC\u03031,ti = Evj\u223cN ti (E 1 j ) \u22a4E1i = Evj\u223cN ti (A\u0303R) \u22a4 j (A\u0303R)i\n= Evj\u223cN ti 1\n|N Trainj ||N Traini | ( \u2211 vm\u2208N Trainj Rm) \u22a4( \u2211 vn\u2208N Traini Rn)\n= Evj\u223cN ti 1 |N Trainj ||N Traini | \u2211\n(vm,vn)\u2208N Trainj \u00d7N Traini\n(Rm) \u22a4Rn\n= Evj\u223cN ti 1\n|H1i ||H1j | ( \u2211 (vm,vn)\u2208N Trainj \u00d7N Train i ,\nvm \u0338=vn\n(Rm) \u22a4Rn\n\ufe38 \ufe37\ufe37 \ufe38 Non-common neighbor embedding pairs\n+ \u2211\nvk\u2208N Trainj \u2229N Traini\n(Rk) \u22a4Rk\n\ufe38 \ufe37\ufe37 \ufe38 Common neighbor embedding pairs\n),\n(10)\nNote that the first term is the dot product between any pair of two non-common neighbor embeddings, which is essentially the dot product between two independent samples from the same multivariate Gaussian distribution (note that here we do not perform any training optimization, so the embeddings of different nodes are completely independent) and by central limit theorem (Kwak & Kim, 2017) approaches the standard Gaussian distribution with 0 as the mean, i.e., \u00b5(Rm)\u22a4Rn = 0. In contrast, the second term is the dot product between any Gaussian-distributed sample and itself, which can be essentially characterized as the sum of squares of d independent standard normal random variables and hence follows the chi-squared distribution with d degrees of freedom, i.e., (Rk) \u22a4Rk \u223c \u03c72d (Sanders, 2009). By Central Limit Theorem, limd\u2192\u221e P ( \u03c72d\u2212d\u221a 2d \u2264 z) = PN (0,1)(z) and hence limd\u2192\u221e \u03c72d = N (d, 2d), i.e., \u00b5(Rk)\u22a4Rk = d. Then we obtain the mean value of Evj\u223cN ti (E 1 j ) \u22a4E1i :\n\u00b5C\u03031,ti = \u00b5Evj\u223cNti (E 1 j ) \u22a4E1i \u2248 Evj\u223cN ti\n1 |H1i ||H1j | (\u00b5\u2211 (vm,vn)\u2208N Trainj \u00d7N Train i ,\nvm \u0338=vn\n(Rm)\u22a4Rn + \u00b5 \u2211\nvk\u2208N Train j \u2229NTrain i\n(Rk)\u22a4Rk)\n\u2248 Evj\u2208N ti d|N Traini \u2229N Trainj |\n|H1i ||H1j | = Evj\u2208N ti d|H1i \u2229H1j | |H1i ||H1j | = dC1,ti .\n(11)\nThe second approximation holds since we set d to be at least 64 for all experiments in this paper. We next perform Monte-Carlo Simulation to verify that by setting d = 64, the obtained distribution is very similar to the Gaussian distribution. Assuming without loss of generality that the embedding dimension is 64 with the mean vector \u00b5 = 064 \u2208 R64 and the identity covariance matrix \u03a364 = I \u2208 R64\u00d764, we randomly sample 1000 embeddings from N (\u00b5,\u03a3).\nWe visualize the distributions of the inner product between the pair of non-common neighbor embeddings, i.e., the first term in Eq. (10) (Rm)\u22a4Rn, vm \u0338= vn, and the pair of common neighbor embeddings, i.e., the second term in Eq. (10) (Rk)\u22a4Rk, vk \u2208 N Trainj \u2229 N Traini in Figure 8. We can see that the distribution of the dot product between the pair of non-common neighbor embeddings behaves like a Gaussian distribution centering around 0. In contrast, the distribution of the dot product between the pair of common neighbor embeddings behaves like a chi-square distribution of degree 64, which also centers around 64, and this in turn verifies the Gaussian approximation. Note that the correctness of the first approximation in Eq. (11) relies on the assumption that the average of the inverse of the node\u2019s neighbors should be the same across all nodes. Although it cannot be theoretically satisfied, we still empirically verify the positive correlation between TC and the link prediction performance shown in Figure 3.\nThe above derivation bridges the gap between the Topological Concentration (TC) defined in the topological space and the Approximated Topological Concentration (ATC) defined in the latent space, which theoretically justifies the approximation efficacy of ATC.\nD.2 DEGREE-RELATED BIAS OF EVALUATION METRICS\nOne previous work (Wang & Derr, 2022) has empirically shown the degree-related bias of evaluation metrics used in link prediction models. Following that, we go one step further and theoretically derive the concrete format of the evaluation bias in this section. We leverage an untrained link prediction model to study the bias. This avoids any potential supervision signal from training over observed links and enables us to study the evaluation bias exclusively. Since two nodes with the same degree may end up with different performances, i.e., X@Ki \u0338= X@Kj , di = dj , we model X@K|d as a random variable and expect to find the relationship between its expectation and the node degree d, i.e., f : E(X@K|d) = f(d). Following many existing ranking works (He et al., 2020; Chen et al., 2021b), we assume without loss of generalizability that the link predictor P ranking the predicted neighbors based on their embedding similarity with embeddings noted as E, then we have:\nLemma 1. For any untrained embedding-based link predictor P , given the existing k \u2212 1 predicted neighbors for the node vi \u2208 V , the kth predicted neighbor is generated by randomly selecting a node without replacement from the remaining nodes with equal opportunities, i.e., P (v\u03d5ki = v|{v\u03d51i , v\u03d52i , ..., v\u03d5k\u22121i }) = 1 N\u2212(k\u22121) .\nWithout any training, Lemma 1 trivially holds since embeddings of all nodes are the same, which trivially leads to the following theorem:\nTheorem 2. Given the untrained embedding-based link predictor P , the size of the intersection between any node\u2019s predicted list E\u0303i and its ground-truth list E\u0302i follows a hypergeometric distribution: |E\u0303i \u2229 E\u0302i| \u223c HG(|V|,K, |E\u0302i|) where |V| is the population size (the whole node space), K is\nthe number of trials and |E\u0302i| is the number of successful states (the number of node\u2019s ground-truth neighbors).\nProof. Given the ground-truth node neighbors E\u0302i, the predicted neighbors E\u0303i = {v\u03d5ki } K k=1 is formed by selecting one node at a time without replacement K times from the whole node space V . Since any selected node v\u03d5ki can be classified into one of two mutually exclusive categories E\u0302i or V\\E\u0302i and by Lemma 1, we know that for any untrained link predictor, each unselected node has an equal opportunity to be selected in every new trial, we conclude that |E\u0303i \u2229 E\u0302i| \u223c HG(|V|,K, |E\u0302i|) and by default E(|E\u0303i \u2229 E\u0302i|) = |E\u0303i| |E\u0302i||V| = K |E\u0302i| |V| .\nFurthermore, we present Theorem 3 to state the relationships between the LP performance under each evaluation metric and the node degree:\nTheorem 3. Given that |E\u0303i \u2229 E\u0302i| follows hyper-geometric distribution, we have:\nE(R@Ki|d) = K N , \u2202E(R@K|d) \u2202d = 0, (12)\nE(P@K|di) = \u03b1d N , \u2202E(P@K|d) \u2202d = \u03b1 N , (13)\nE(F1@K|d) = 2K N \u03b1d K + \u03b1d , \u2202E(F1@K|d) \u2202d = 2\u03b1K2 N\n1\n(K + \u03b1d)2 , (14)\nE(N@K|d) = \u03b1d N , \u2202E(N@K|d) \u2202d = \u03b1 N . (15)\nProof.\nE(R@Ki|d) = E( |E\u0303i \u2229 E\u0302i| |E\u0302i| ) = E(|E\u0303i \u2229 E\u0302i|) |E\u0302i| =\n|E\u0302i| |V| K\n|E\u0302i| =\nK N (16)\nE(P@Ki|d) = E( |E\u0303i \u2229 E\u0302i|\nK ) = E(|E\u0303i \u2229 E\u0302i|) K =\n|E\u0302i| |V| K\nK =\n\u03b1d N (17)\nE(F1@Ki|d) = E( 2|E\u0303i \u2229 E\u0302i| K + |E\u0302i| ) = 2E(|E\u0303i \u2229 E\u0302i|) K + \u03b1d = 2K N \u03b1d K + \u03b1d (18)\nE(N@Ki|d) = E( \u2211K k=1 1[v \u03d5k \u2208(E\u0303i\u2229E\u0302i)] log2(k+1)\u2211K\nk=1 log2(k + 1) ) =\nE( \u2211K\nk=1\n1[v \u03d5k \u2208(E\u0303i\u2229E\u0302i)] log2(k+1) )\u2211K k=1 1 log2(k+1)\n(19)\nTo calculate the numerator DCG, i.e., E( \u2211K\nk=1\n1[v \u03d5k \u2208(E\u0303i\u2229E\u0302i)] log2(k+1) ) in Eq. (19), we model the link pre-\ndiction procedure as 1) randomly select K nodes from the whole node space V; 2) calculate |E\u0303i\u2229E\u0302i|, i.e., how many nodes among the selected nodes E\u0303i are in the ground-truth neighborhood list E\u0302i; 3) randomly select |E\u0303i \u2229 E\u0302i| slots to position nodes in E\u0303i \u2229 E\u0302i and calculate DCG. The above steps can be mathematically formulated as:\nK\u2211 i=0 C(N \u2212 \u03b1d,K \u2212 i)C(\u03b1d, i) C(N,K) C(K,i)\u2211 j=1 p(O (K,i) j ) K\u2211 k=1 1[O (K,i) jk = 1] log2(k + 1) , (20)\nwhere O(K,i) \u2208 {0, 1}C(K,i)\u00d7K represents all C(K, i) possible positional indices of putting i nodes into K candidate slots. Specifically O(K,i)j \u2208 {0, 1}K indicates the jth positional configuration of i nodes where O(K,i)jk = 1 if an node is positioned at k th slot and O(K,i)jk = 0 otherwise. Since our link predictor has no bias in positioning nodes in the K slots by Lemma 1, we have p(O(K,i)j ) = 1 C(K,i) and Eq. (20) can be transformed as:\nK\u2211 i=0 C(N \u2212 \u03b1d,K \u2212 i)C(\u03b1d, i) C(N,K)\n1\nC(K, i) C(K,i)\u2211 j=1 K\u2211 k=1 1[O (K,i) jk = 1] log2(k + 1) . (21)\nWe know that only when the kth slot is positioned a node can we have O(K,i)jk = 1 and among the total C(K, i) selections, every candidate slot k \u2208 {1, 2, ...,K} would be selected C(K \u2212 1, i \u2212 1) times to position a node, which hence leads to:\nC(K,i)\u2211 j=1 K\u2211 k=1 1[O (K,i) jk = 1] log2(k + 1) = K\u2211 k=1 C(K \u2212 1, i\u2212 1) log2(k + 1) . (22)\nWe then substitute Eq. (22) into Eq. (21) as:\nK\u2211 i=0 C(N \u2212 \u03b1d,K \u2212 i)C(\u03b1d, i) C(N,K)\n1\nC(K, i) K\u2211 k=1 C(K \u2212 1, i\u2212 1) log2(k + 1)\n= K\u2211 i=0 C(N \u2212 \u03b1d,K \u2212 i)C(\u03b1d, i) C(N,K) C(K \u2212 1, i\u2212 1) C(K, i) K\u2211 k=1\n1\nlog2(k + 1) .\n(23)\nFurther substituting Eq. (23) into Eq. (19), we finally get:\nE(N@K|di) = K\u2211 i=0 C(N \u2212 \u03b1d,K \u2212 i)C(\u03b1d, i) C(N,K) C(K \u2212 1, i\u2212 1) C(K, i)\n= K\u2211 i=0 C(N \u2212 \u03b1d,K \u2212 i)C(\u03b1d, i) C(N,K) (K\u22121)! (i\u22121)!(K\u2212i)! K! i!(K\u2212i)!\n= 1\nK K\u2211 i=0 i C(N \u2212 \u03b1d,K \u2212 i)C(\u03b1d, i)\nC(N,K)\ufe38 \ufe37\ufe37 \ufe38 E(|E\u0303i\u2229E\u0302i|)\n= 1\nK\n\u03b1d N \u2217K = \u03b1d N\n(24)\nBased on Theorem 3, Precision, F1, and NDCG increase as node degree increases even when no observed links are used to train the link predictor, which informs the degree-related evaluation bias and causes the illusion that high-degree nodes are more advantageous than low-degree ones observed in some previous works (Li et al., 2021; Rahmani et al., 2022).\nD.3 REWEIGHTING BY LP SCORE ENHANCE 1-LAYER TC\nTheorem 4. Taking the normalization term g(|H1i |, |H1j |) = |H1i | and also assume that that higher link prediction score Sij between vi and its neighbor vj corresponds to more number of connections between vj and the neighborhood NTraini , i.e., Sij > Sik \u2192 |N 1,Train j \u2229 N 1,Train i | > |N 1,Train k \u2229 N 1,Traini |,\u2200vj , vk \u2208 N Train,1 i , then we have:\nC\u03021,Traini = \u2211\nvj\u223cNTraini\nSij |H1i \u2229H1j | |H1i | \u2265 Evj\u223cNTraini |H1i \u2229H1j | |H1i | = C1,Traini (25)\nProof. By definition, we have H1i = N 1,Train i , then the computation of 1-layer TC Train is transformed as:\nC1,Traini = Evj\u223cNTraini I(S 1 i ,S1j ) = Evj\u223cNTraini |NTraini \u2229NTrainj | |NTraini | = 1 |NTraini | Evj\u223cNTraini (|N Train i \u2229NTrainj |). (26) On the other hand, we also transform weighted TC as:\nC\u03021,Traini = 1 |NTraini | \u2211\nvj\u223cNTraini\n(Sij |NTraini \u2229NTrainj |). (27)\nBy the relation that:\nSij > Sik \u2192 |N 1,Trainj \u2229N 1,Train i | > |N 1,Train k \u2229N 1,Train i |,\u2200vj , vk \u2208 N Train,1 i , (28)\nThen we have: C\u03021,Traini \u2265 C 1,Train i (29)\nMoreover, we include Figure 9 to illustrate the idea of enhancing TC via assigning higher weights to edges connecting neighbors that have higher connections to the whole neighborhoods. We can see in this case, weighted TC in Figure 9(a) is naturally higher than the one in Figure 9(b)"
        },
        {
            "heading": "E EXAMPLE DEMONSTRATING THE ADVANTAGES OF TC OVER LCC",
            "text": "According to the definition of local clustering coefficient (LCC) and TC, we respectively calculate their values for node v1 in Figure 10. v2, v3, v4 do not have any connection among themselves, indicating node v1 prefer interacting with nodes coming from significantly different domain/community. Subsequently, the incoming neighbors v5, v6 of v1 are likely to also come from other communities and hence share no connections with v2, v3, v4, which leads to the ill topological condition for predicting links of v1. However, in this case, the clustering coefficient still maintains 0.5 because of the connections between v1 and v2/v3/v4, which cannot precisely capture the ill-topology of v1 in this case. Conversely, our TCTrain equals 0, reflecting the ill topological condition of v1."
        },
        {
            "heading": "F DATASETS AND EXPERIMENTAL SETTINGS",
            "text": "This section introduces datasets and experimental settings used in this paper.\nF.1 DATASET INTRODUCTION AND STATISTICS\nWe use five widely employed datasets for evaluating the link prediction task, including four citation networks: Cora, Citeseer, Pubmed, and Citation2, and 1 human social network Collab. We further introduce two real-world animal social networks, Reptile and Vole, based on animal interactions. \u2022 Cora/Citeseer/Pubmed: Following (Zhao et al., 2022; Chamberlain et al., 2022; Wang et al.,\n2023), we randomly split edges into 70%/10%/20% so that there is no topological distribution shift in these datasets. We use Hits@100 to evaluate the final performance.\n\u2022 Collab/Citation2: We leverage the default edge splitting from OGBL (Hu et al., 2020). These two datasets mimic the real-life link prediction scenario where testing edges later joined in the network than validation edges and further than training edges. This would cause the topological distribution shift observed in the Obs.3 of Section 3.3. For Collab, different from (Chamberlain et al., 2022; Wang et al., 2023), our setting does not allow validation edges to join the network for message-passing when evaluating link prediction performance. Therefore, the edges used for message-passing and supervision come from edges in the training set.\n\u2022 Reptile/Vole: we obtain the dataset from Network Repository (Rossi & Ahmed, 2015). To construct this network, a bipartite network was first constructed based on burrow use - an edge connecting a tortoise node to a burrow node indicated a burrow used by the individual. Social networks of desert tortoises were then constructed by the bipartite network into a single-mode projection of tortoise nodes. Node features are initialized by a trainable embedding layer, and we leverage the same edge splitting 70%/10%/20% as Cora/Citeseer/Pubmed for training/evaluation.\nF.2 HYPERPARAMETER DETAILS\nFor all experiments, we select the best configuration on validation edges and report the model performance on testing edges. The search space for the hyperparameters of the GCN/SAGE/LightGCN baselines and their augmented variants GCNrw/SAGErw are: graph convolutional layer {1, 2, 3}, hidden dimension of graph encoder {64, 128, 256}, the learning rate of the encoder and predictor {0.001, 0.005, 0.01}, dropout {0.2, 0.5, 0.8}, training epoch {50, 100, 500, 1000}, batch size {256, 1152, 64 \u2217 1024} (Hu et al., 2020; Chamberlain et al., 2022; Wang et al., 2023), weights \u03b1 \u2208 {0.5, 1, 2, 3, 4}, the update interval \u03c4 \u2208 {1, 2, 10, 20, 50}, warm up epochs Twarm \u2208 {1, 2, 5, 10, 30, 50}. For baseline NCN2, we directly run their code using their default bestperforming configurations on Cora/Citeseer/Pubmed/Collab but for Citation2, due to memory limitation, we directly take the result from the original paper. We use cosine similarity metric as the similarity function \u03d5 in computing ATC.\n2https://github.com/GraphPKU/NeuralCommonNeighbor"
        },
        {
            "heading": "G ADDITIONAL RESULTS",
            "text": "To demonstrate that the observations made previously in Section 3 can also generalize to other datasets, here we present the comprehensive results on all datasets we study in this paper as follows.\nG.1 LINK PREDICTION PERFORMANCE GROUPED BY TCTest\nG.2 LINK PREDICTION PERFORMANCE GROUPED BY TCTrain\nG.3 LINK PREDICTION PERFORMANCE GROUPED BY DEGREETest\nG.4 LINK PREDICTION PERFORMANCE GROUPED BY DEGREETrain\nG.5 RELATION BETWEEN LP PERFORMANCE AND TC AT GRAPH-LEVEL\nG.6 RELATION BETWEEN TCTrain AND TCTest\nG.7 DIFFERENCE IN TC VS DIFFERENCE IN PERFORMANCE BEFORE/AFTER APPLYING REWEIGHTING\nG.8 CORRELATION OF THE PERFORMANCE WITH TC AND DEGREE\nHere we present the comprehensive correlation of the performance with TCTrain/TCVal/TCTest and DegreeTrain. As the performance is evaluated under different K, we further define the absolute average/the typical average correlation across different K values to reflect the absolute correlation strength/the consistency of the correlation average:\nAbsolute Avg.X@K = 1\n4 \u2211 k\u2208{5,10,20,50} |X@k|, Basic Avg.X@K = 1 4 \u2211 k\u2208{5,10,20,50} X@k"
        },
        {
            "heading": "H EDGE REWEIGHTING ALGORITHM",
            "text": "Here we present our edge reweigting algorithm to enhance the link prediction performance by modifying the graph adjacency matrix in message-passing. We normalize the adjacency matrix to get A\u0303 and A\u0302 as defined in the algorithm below.\nAlgorithm 1: Edge Reweighting to Boost LP performance Input: The input training graph (A,X, ETrain,D), graph encoder f\u0398f , link predictor g\u0398g , update interval\n\u2206, training epochs T , warm up epochs Twarm and weights \u03b3 for combining the original adjacency matrix and the updated adjacency matrix. The validation adjacency/degree matrix AVal/DVal that only includes edges in the validation set.\n1 Compute the normalized adjacency matrices A\u0302 = D\u22120.5AD\u22120.5, A\u0303 = D\u22121A, A\u0303Val = DVal \u22121 AVal\n2 A\u03030 = A\u0302 3 for \u03c4 = 1, . . . , T do 4 if \u03c4%\u2206 \u0338= 0 or \u03c4 \u2264 Twarm then 5 A\u0303\u03c4 = A\u0303\u03c4\u22121\n/* Message-passing and LP to update model parameters */\n6 for mini-batch of edges Eb \u2286 ETrain do 7 Sample negative edges Eb,\u2212, s.t., |Eb,\u2212| = |Eb| 8 Compute node embeddings H\u03c4 = f\n\u0398\u03c4\u22121 f\n(A\u0303\u03c4 ,X)\n9 Compute link prediction scores E\u03c4ij = g\u0398\u03c4\u22121g (H \u03c4 i ,H \u03c4 j ), \u2200(i, j) \u2208 Eb \u222a ETrain 10 Lb,\u03c4 = \u2212 1|Eb| ( \u2211 eij\u2208Eb logE \u03c4 ij + \u2211 emn\u2208Eb,\u2212 log(1\u2212E \u03c4 mn)) 11 Update \u0398\u03c4g \u2190 \u0398\u03c4\u22121g \u2212\u2207\u0398\u03c4\u22121g L b,\u03c4 , \u0398\u03c4f \u2190 \u0398\u03c4\u22121f \u2212\u2207\u0398\u03c4\u22121 f Lb,\u03c4\u22121\n/* Update adjacency matrix to enhance weighted TC */ 12 if \u03c4%\u2206 == 0 and \u03c4 > Twarm then 13 Compute node embeddings H\u03c4 = f\n\u0398\u03c4\u22121 f\n(A\u0303\u03c4\u22121,X);\n14 if Using training neighbors to reweigh then 15 Average pooling the neighborhood embeddings N\u03c4 = A\u0303H\u03c4\n16 if Using validation neighbors to reweigh then 17 Average pooling the neighborhood embeddings N\u03c4 = A\u0303ValH\u03c4\n18 Compute the link prediction scores S\u03c4ij = exp(g\u0398\u03c4g (N\u03c4i ,H \u03c4 j ))\u2211n\nj=1 exp(g\u0398\u03c4g (N\u03c4i ,H \u03c4 j ))\n19 Update the adjacency matrix A\u0303\u03c4 \u2190 A\u0302+ \u03b3S\u03c4\n20 Return: A\u0303\u03c4 , f\u0398\u03c4 f , g\u0398\u03c4g"
        },
        {
            "heading": "I REWEIGH EDGES FOR BASELINES WITHOUT MESSAGE-PASSING",
            "text": "As discussed in Section 4, we enhance node TCTrain by reweighing the edges in message-passing. However, for some state-of-the-art baselines Chamberlain et al. (2022) that directly employ the neural transformation rather than message-passing to obtain node embeddings, we reweigh edges in computing the binary cross entropy loss in the training stage as follows:\nL = \u2212 1 |Eb| \u2211 e ij\u2208Eb (wij \u2211 eij\u2208Eb logE\u03c4ij + wmn \u2211 emn\u2208Eb,\u2212 log(1\u2212E\u03c4mn)), (30)\nwhere wij = \u03c3(\u03d5(Ni,Nj)) quantifies the edge weight between vi and vj with \u03c3 being the Sigmoid function and \u03d5 being the cosine similarity. Ni is the node embedding of vi obtained in Eq. (2)."
        },
        {
            "heading": "J COMPARING THE EFFICIENCY BETWEEN BASELINE AND THEIR",
            "text": "Here we compare the running time (s) of each baseline and their corresponding augmented version by uniformly testing them on the same machine in Table 11. We can see that equipping our proposed reweighting strategy could enhance the performance but only lead to marginal computational overhead. This is because firstly, we only change the weight of existing edges and hence make it linear to the network size. Secondly, we leverage the pre-computed node embeddings to compute the edge weights. Thirdly, we only periodically update the edge weights."
        },
        {
            "heading": "K REWEIGHTING TRAINING NEIGHBORS BASED ON THEIR CONNECTIONS TO TRAINING NEIGHBORS OR VALIDATION NEIGHBORS",
            "text": "As discussed in Obs. 3, due to the topological distribution shift, the newly joined neighbors of one node become less and less connective to the previous neighbors of that node. Therefore, the training neighbors of one node share fewer connections with the testing neighbors of that node than the validation neighbors. This motivates us to improve further our reweighting strategy based on validation neighbors rather than training neighbors. The intuition is that when performing messagepassing to aggregate training neighbors\u2019 information for each node, we want to incorporate those training neighbors with more connections to that node\u2019s validation neighbors instead of those training neighbors with more connections to the training neighbors themselves. Technically, we include additional steps 14-17 to consider two scenarios in Algorithm H: reweighting based on the connections of training neighbors to training neighbors and reweighting based on the connections of training neighbors to validation neighbors. We experiment on Collab to compare the performance of these two scenarios in Table 12. We can see the performance of reweighting based on validation neighbors is higher than reweighting based on training neighbors. This demonstrates that the validation neighbors are more connected to the testing neighbors, justifying the existence of the topological distribution shift."
        },
        {
            "heading": "L EXPLAINING WHY THE CURVE HAS SEVERAL FAST DOWN OF PERFORMANCES",
            "text": "Here we delve deep into the reason why we encounter several fast-down performances in Figure 7(a). We ascribe it to the weight clip 3. We hypothesize that the loss landscape has several local minimums and hence by weight clipping with higher upper bound constraints, our learning step would be also larger so that the model could jump out of its original local optimum and keep finding some other better local optimum, which corresponds to the fast downtrend (first jump away from one local minimum and then find another better local minimum). We further empirically verify our hypothesis by visualizing the performance curve for each training process with different clipping weights in Figure 23. We can clearly see that as the clipping threshold becomes lower (upper bound decreases), we observe less fast downtrend decreases.\n3Following the publically available implementation on GCN/SAGE on Collab link prediction, we employ the weight clip every time after parameter update"
        }
    ],
    "title": "A TOPOLOGICAL PERSPECTIVE ON DEMYSTIFYING GNN-BASED LINK PREDICTION PERFORMANCE",
    "year": 2023
}