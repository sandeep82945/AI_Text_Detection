{
    "abstractText": "Weight initialization plays an important role in neural network training. Widely used initialization methods are proposed and evaluated for networks that are trained from scratch. However, the growing number of pretrained models now offers new opportunities for tackling this classical problem of weight initialization. In this work, we introduce weight selection, a method for initializing smaller models by selecting a subset of weights from a pretrained larger model. This enables the transfer of knowledge from pretrained weights to smaller models. Our experiments demonstrate that weight selection can significantly enhance the performance of small models and reduce their training time. Notably, it can also be used together with knowledge distillation. Weight selection offers a new approach to leverage the power of pretrained models in resource-constrained settings, and we hope it can be a useful tool for training small models in the large-model era.",
    "authors": [],
    "id": "SP:e9ec15cbbb3735c66756c3b7a05056893d3415bf",
    "references": [
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "BEiT: BERT pre-training of image transformers",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Am\u00e9lie Royer",
                "Larisa Markeeva",
                "Rohan Anil",
                "Alexander Kolesnikov"
            ],
            "title": "Knowledge distillation: A good teacher is patient and consistent",
            "year": 2022
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101 \u2013 mining discriminative components with random forests",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "M. Cimpoi",
                "S. Maji",
                "I. Kokkinos",
                "S. Mohamed",
                "A. Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In CVPR,",
            "year": 2014
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning"
            ],
            "title": "ELECTRA: Pre-training text encoders as discriminators rather than generators",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Coates",
                "Andrew Ng",
                "Honglak Lee"
            ],
            "title": "An analysis of single-layer networks in unsupervised feature learning",
            "venue": "AISTATS, volume 15 of Proceedings of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In CVPR Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Hanan Gani",
                "Muzammal Naseer",
                "Mohammad Yaqub"
            ],
            "title": "How to train vision transformer on small-scale datasets",
            "venue": "arXiv preprint arXiv:2210.07240,",
            "year": 2022
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In AISTATS,",
            "year": 2010
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "John Tran",
                "William Dally"
            ],
            "title": "Learning both weights and connections for efficient neural network",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IGARSS",
            "year": 2018
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Joan Puigcerver",
                "Jessica Yung",
                "Sylvain Gelly",
                "Neil Houlsby"
            ],
            "title": "Big transfer (bit): General visual representation learning",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Kr\u00e4henb\u00fchl",
                "Carl Doersch",
                "Jeff Donahue",
                "Trevor Darrell"
            ],
            "title": "Data-dependent initializations of convolutional neural networks",
            "venue": "arXiv preprint arXiv:1511.06856,",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "John S Denker",
                "Sara A Solla"
            ],
            "title": "Optimal brain damage",
            "venue": "In NeurIPS,",
            "year": 1990
        },
        {
            "authors": [
                "Hao Li",
                "Asim Kadav",
                "Igor Durdanovic",
                "Hanan Samet",
                "Hans Peter Graf"
            ],
            "title": "Pruning filters for efficient convnets",
            "year": 2017
        },
        {
            "authors": [
                "Yi Li",
                "Haozhi Qi",
                "Jifeng Dai",
                "Xiangyang Ji",
                "Yichen Wei"
            ],
            "title": "Fully convolutional instance-aware semantic segmentation",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Ye Lin",
                "Yanyang Li",
                "Ziyang Wang",
                "Bei Li",
                "Quan Du",
                "Tong Xiao",
                "Jingbo Zhu"
            ],
            "title": "Weight distillation: Transferring the knowledge in neural network parameters",
            "year": 2009
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Mingjie Sun",
                "Tinghui Zhou",
                "Gao Huang",
                "Trevor Darrell"
            ],
            "title": "Rethinking the value of network pruning",
            "year": 2018
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "title": "A convnet for the 2020s",
            "year": 2022
        },
        {
            "authors": [
                "Zhuang Liu",
                "Zhiqiu Xu",
                "Joseph Jin",
                "Zhiqiang Shen",
                "Trevor Darrell"
            ],
            "title": "Dropout reduces underfitting",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Masci",
                "Ueli Meier",
                "Dan Cire\u015fan",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Stacked convolutional autoencoders for hierarchical feature extraction",
            "venue": "ICANN 2011: 21st International Conference on Artificial Neural Networks,",
            "year": 2011
        },
        {
            "authors": [
                "Dmytro Mishkin",
                "Jiri Matas"
            ],
            "title": "All you need is a good init",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Vinod Nair",
                "Geoffrey E Hinton"
            ],
            "title": "Rectified linear units improve restricted boltzmann machines",
            "venue": "In ICML,",
            "year": 2010
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y. Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "In NeurIPS,",
            "year": 2011
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "pp. 722\u2013729,",
            "year": 2008
        },
        {
            "authors": [
                "Omkar M. Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "C.V. Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "In CVPR,",
            "year": 2012
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "PyTorch: An imperative style, high-performance deep learning library",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf"
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "year": 1910
        },
        {
            "authors": [
                "Andrew M Saxe",
                "James L McClelland",
                "Surya Ganguli"
            ],
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
            "venue": "arXiv preprint arXiv:1312.6120,",
            "year": 2013
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Sam Shleifer",
                "Alexander M Rush"
            ],
            "title": "Pre-trained summarization distillation",
            "venue": "arXiv preprint arXiv:2010.13002,",
            "year": 2020
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Two-stream convolutional networks for action recognition in videos",
            "venue": "In NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Andreas Steiner",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "Ross Wightman",
                "Jakob Uszkoreit",
                "Lucas Beyer"
            ],
            "title": "How to train your vit? data, augmentation, and regularization in vision transformers",
            "venue": "arXiv preprint arXiv:2106.10270,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "year": 2016
        },
        {
            "authors": [
                "JingLei Tang",
                "Dong Wang",
                "ZhiGuang Zhang",
                "LiJun He",
                "Jing Xin",
                "Yang Xu"
            ],
            "title": "Weed identification based on k-means feature learning combined with convolutional neural network",
            "venue": "Computers and electronics in agriculture,",
            "year": 2017
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive representation distillation",
            "venue": "arXiv preprint arXiv:1910.10699,",
            "year": 2019
        },
        {
            "authors": [
                "Ilya O Tolstikhin",
                "Neil Houlsby",
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Jessica Yung",
                "Andreas Steiner",
                "Daniel Keysers",
                "Jakob Uszkoreit"
            ],
            "title": "Mlp-mixer: An all-mlp architecture for vision",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "year": 2012
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Alexandre Sablayrolles",
                "Gabriel Synnaeve",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Going deeper with image transformers",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Trieu H Trinh",
                "Minh-Thang Luong",
                "Quoc V Le"
            ],
            "title": "Selfie: Self-supervised pretraining for image embedding",
            "venue": "arXiv preprint arXiv:1906.02940,",
            "year": 2019
        },
        {
            "authors": [
                "Asher Trockman",
                "J Zico Kolter"
            ],
            "title": "Mimetic initialization of self-attention layers",
            "venue": "arXiv preprint arXiv:2305.09828,",
            "year": 2023
        },
        {
            "authors": [
                "Asher Trockman",
                "Devin Willmott",
                "J Zico Kolter"
            ],
            "title": "Understanding the covariance structure of convolutional filters",
            "venue": "arXiv preprint arXiv:2210.03651,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Eugene Vorontsov",
                "Chiheb Trabelsi",
                "Samuel Kadoury",
                "Chris Pal"
            ],
            "title": "On orthogonality and learning recurrent networks with long term dependencies",
            "venue": "In ICML. PMLR,",
            "year": 2017
        },
        {
            "authors": [
                "Ross Wightman"
            ],
            "title": "GitHub repository: Pytorch image models",
            "venue": "https://github.com/ rwightman/pytorch-image-models,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 1910
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Zhun Zhong",
                "Liang Zheng",
                "Guoliang Kang",
                "Shaozi Li",
                "Yi Yang"
            ],
            "title": "Random erasing data augmentation",
            "venue": "In AAAI,",
            "year": 2020
        },
        {
            "authors": [
                "Fuzhen Zhuang",
                "Zhiyuan Qi",
                "Keyu Duan",
                "Dongbo Xi",
                "Yongchun Zhu",
                "Hengshu Zhu",
                "Hui Xiong",
                "Qing He"
            ],
            "title": "A comprehensive survey on transfer learning",
            "venue": "Proceedings of the IEEE,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The initialization of neural network weights is crucial for their optimization. Proper initialization aids in model convergence and prevents issues like gradient vanishing. Two prominent initialization techniques, Xavier initialization (Glorot & Bengio, 2010a) and Kaiming initialization (He et al., 2015), have played substantial roles in neural network training. They remain the default methods in modern deep learning libraries like PyTorch (Paszke et al., 2019).\nThese methods were developed for training neural network from random initialization. At that time, it was the common practice. However, the landscape has changed. A variety of pretrained models are now readily available, thanks to collective efforts from the community (Wolf et al., 2019; Wightman, 2019). These models are trained on large datasets like ImageNet21K (Deng et al., 2009) and LAION-5B (Schuhmann et al., 2022) and are often optimized by experts. As a result, fine-tuning from these pretrained models (Kolesnikov et al., 2020; Hu\net al., 2021) is usually considered a preferred option today, rather than training models from scratch.\nHowever, these pretrained large models can be prohibitive in their resource demand, preventing their wide adoption for resource-constrained settings, e.g., on mobile devices. For many pretrained model families, even the smallest model instance can be considered extremely large in certain context. For example, masked autoencoders (MAE) (He et al., 2021) and CLIP (Radford et al., 2021) both provide ViT-Base (Dosovitskiy et al., 2021), a 80M-parameter architecture, as their smallest pretrained Transformer model. This is already too large for applications on edge devices, and the smallest LLaMA (Touvron et al., 2023) model is even another 100 times larger, with 7B paramemters. With few small pretrained models available, developers would have to train them from scratch on target datasets to suit their needs. This approach misses the opportunity to utilize large pretrained models, whose knowledge is learned from extensive training on large data.\nIn this work, we tackle this issue by introducing a weight initialization method that uses large pretrained models to train small models. Specifically, we introduce weight selection, where a subset of weights from a pretrained large model is selected to initialize a smaller model. This allows for\nknowledge learned by the large model to transfer to the small model through its weights. Thanks to the modular design of modern neural networks, weight selection involves only three simple steps: layer selection, component mapping, and element selection. This method can be applied to any smaller model within the same model family as the large model. Using weight selection for initializing a small model is straightforward and adds no extra computational cost compared to training from scratch. It could also be useful even for large model training, e.g., initializing a LLaMA-7B with trained weights from LLaMA-30B.\nWe apply weight selection to train small models on image classification datasets of different scales. We observe significant improvement on accuracy across datasets and models compared with baselines. Weight selection also substantially reduces the training time required to reach the same level of accuracy. Additionally, it can work alongside another popular method for knowledge transfer from large models \u2013 knowledge distillation (Hinton et al., 2015). We believe weight selection can be a general technique for training small models. Our work also encourage further research on utilizing pretrained models for efficient deployment."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Weight initialization. Weight initialization is a crucial aspect of model training. Glorot & Bengio (2010a) maintains constant variance by setting the initial values of the weights using a normal distribution, aiming to prevent gradient vanishing or explosion. Later, He et al. (2015) modified it to adapt to ReLU activations (Nair & Hinton, 2010). Mishkin & Matas (2016) crafted the orthogonality in weight matrices to keep gradient from vanishing or exploding. Saxe et al. (2013) and Vorontsov et al. (2017) put soft constraints on weight matrices to ensure orthogonality.\nThere are methods that use external sources of knowledge like data distribution or unsupervised training to initialize neural networks. A data-dependent initialization can be obtained from performing K-means clustering or PCA (Kra\u0308henbu\u0308hl et al., 2015; Tang et al., 2017) on the training dataset. Larochelle et al. (2009), Masci et al. (2011), Trinh et al. (2019), and Gani et al. (2022) show training on unsupervised objectives can provide a better initialization for supervised training.\nUtilizing pretrained models. Transfer learning (Zhuang et al., 2020) is a common framework for using model weights pretrained from large-scale data. Model architecture is maintained and the model is fine-tuned on specific downstream tasks (Kolesnikov et al., 2020). Knowledge distillation involves training usually a smaller student model to approximate the output of a teacher model (Hinton et al., 2015; Tian et al., 2019; Beyer et al., 2022). This allows the student model to maintain the performance of a teacher while being computationally efficient. Another alternative approach for using pretrained models is through weight pruning (LeCun et al., 1990; Han et al., 2015; Li et al., 2017b; Liu et al., 2018). It involves removing less significant weights from the model, making it more efficient without significantly compromising performance.\nLin et al. (2020) transforms parameters of a large network to an analogous smaller one through learnable linear layers using knowledge distillation to match block outputs. Sanh et al. (2019) and Shleifer & Rush (2020) creates smaller models by initializing with a subset of layers from a pretrained BERT (Devlin et al., 2018). This method requires the smaller model to have the same width as teacher\u2019s. Trockman et al. (2022) initializes convolutional layers with Gaussian distribution according to pretrained model\u2019s covariance. Similarly, Trockman & Kolter (2023) initializes self-attention layers according to observed diagonal patterns from pretrained ViTs. These two methods use statistics from, but do not directly utilize pretrained parameters. Weight selection, in contrast, directly utilizes pretrained parameters, does not require extra training, and is suitable for initializing any smaller variants of the pretrained model."
        },
        {
            "heading": "3 APPROACH",
            "text": "Given a pretrained model, our goal is to seek a good weight initialization for a smaller-size model within the same model family. Borrowing terminology from knowledge distillation, we refer to the pretrained model as teacher and the model we aim to initialize as student."
        },
        {
            "heading": "3.1 WEIGHT SELECTION",
            "text": "Modern neural network architectures often follow a modular approach: design a layer and replicate it to build the model (He et al., 2016; Vaswani et al., 2017; Tolstikhin et al., 2021; Dosovitskiy et al., 2021; Liu et al., 2022). This design ethos promotes scalability: models can be widened by increasing the embedding dimension or the number of channels in each block, and deepened by stacking more layers. It also enables us to perform weight selection following three steps: selecting layers, mapping components within one layer, and selecting elements.\nLayer selection. Selecting layers from teacher is the first step. The procedure for layer selection is slightly different for isotropic architectures and hierarchical architectures. An isotropic architecture refers to the neural network where each layer exhibits a consistent and uniform design and behavior throughout the model. ViT (Dosovitskiy et al., 2021) and MLP-mixer (Tolstikhin et al., 2021) belong to isotropic architectures. A hierarchical architecture is characterized by multi-scale representations and a hierarchy in embedding dimensions. Hierarchical architectures typically have stages with varying scales and embedding dimensions. For example, classic convolutional networks like VGG (Simonyan & Zisserman, 2014) progressively decrease spatial dimensions while increasing channel dimensions, capturing multi-scale features. Modern architectures like Swin-Transformer (Liu et al., 2021) and ConvNeXt (Liu et al., 2022) also employ this hierarchical design.\nFor isotropic architectures, we select first N layers from the teacher, where N represents the student\u2019s layer count. We denote this method of layer selection as first-N selection. When dealing with hierarchical structures like ConvNeXt (Liu et al., 2022), first-N selection is applied at each individual stage. As an illustrative example, consider initializing ConvNeXt-F using ConvNeXt-T. Both networks comprise four stages, with depths of [2, 2, 6, 2] and [3, 3, 9, 3], respectively. A direct application of first-N selection would involve selecting the first 2 layers from the 1st, 2nd, and 4th stages, and the first 6 layers from the 3rd stage.\nComponent mapping. After the previous step, we have obtained the layer mapping from teacher to student. The task is then reduced to initializing one student\u2019s layer with one teacher\u2019s layer. Thanks to the modular approach adopted by modern neural network design, layers in models of the same family have an identical set of components, only differing in their width. The process for matching the corresponding components is thus a natural one-to-one mapping.\nElement selection. Upon establishing component mapping, the next step is to initialize the student\u2019s component using its larger counterpart from the teacher. The default method for element selection is uniform selection, where we select evenly-spaced elements from teacher\u2019s tensor as shown in figure 2. Details on uniform selection and other element selection variants will be introduced in the next part."
        },
        {
            "heading": "3.2 METHODS FOR ELEMENT SELECTION",
            "text": "In this part, we formulate element selection and introduce different selection criteria. Consider a weight tensor Ws from the student that we seek to initialize with the teacher\u2019s weight tensor Wt. If\nWt has the shape t1, t2, ..., tn, then Ws, which is of the same component type with Wt, will also span n dimensions. Our goal is to create Ws by selecting a subset of Wt\u2019s elements. We discuss several possible implementations of element selection in this section.\nUniform selection (default). For each dimension i of Wt, select evenly-spaced si slices out of ti. For example, to initialize a linear layer Ws of shape 2 \u00d7 3 with a linear layer Wt of shape 4 \u00d7 6, we select 1st and 3rd slice along the first dimension, and 1st, 3rd, and 5th slice along the second dimension. We present pseudocode for uniform selection in Algorithm 1. The algorithm starts with a copy of teacher\u2019s weight tensor Wt and iteratively perform selection on all dimensions of Wt to reach the desired shape for student. Notably, in architectures that incorporate grouped components \u2014 such as the multi-head attention module in ViTs and the grouped convolution in ResNeXt (Xie et al., 2017) \u2014 uniform selection absorbs information from all groups. For example, when applied on ViTs, uniform selection will select parameters from all heads in the attention block, which is likely to be beneficial for inheriting knowledge from the pretrained ViTs.\nAlgorithm 1 Uniform element selection from teacher\u2019s weight tensor Input: Wt \u25b7 teacher\u2019s weight tensor Input: s shape \u25b7 desired dimension for student\u2019s weight tensor Output: Ws with shape s shape\n1: procedure UNIFORMELEMENTSELECTION(Wt, student shape) 2: Ws \u2190 Copy of Wt \u25b7 student\u2019s weight tensor 3: n\u2190 length of Wt.shape 4: for i = 1\u2192 n do 5: dt \u2190Wt.shape[i] 6: ds \u2190 s shape[i] 7: indices\u2190 Select ds evenly spaced numbers from 1 to dt 8: Ws \u2190 Select indices along Ws\u2019s ithdimension 9: end for\n10: return Ws 11: end procedure\nConsecutive selection. For each dimension i of Wt, select consecutive si slices out of ti. In contrast to uniform selection, for architectures with grouped components, consecutive selection select some entire groups while omitting the contrast. For architectures without such grouped components, consecutive selection is equivalent to uniform selection.\nRandom selection. For all weight tensors, and for each dimension i of Wt, select the same randomlygenerated set of si slices out of ti. This method stems from the existence of residual connections \u2014 neurons that get added in the teacher model will have their interaction preserved in the student. Furthermore, complete neurons with their inputs and outputs are preserved since only weights with consistent positions get selected. It is worth noting that uniform selection and consecutive selection are special instances of random selection.\nRandom selection (without consistency). Along every dimension i of Wt, randomly select si slices out of ti. Unlike random selection, this method does not require selecting the same indices for every weight tensor. We create this method for comparison to examine the importance of consistency.\nWe compare the performance of these element selection methods in Section 4.3. We default to uniform selection as our method, which shows its superiority in our empirical experiments."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 SETTINGS",
            "text": "Datasets. We evaluate weight selection on 9 image classification datasets including ImageNet1K (Deng et al., 2009), CIFAR-10 (Krizhevsky et al., a), CIFAR-100 (Krizhevsky et al., b), Flowers (Nilsback & Zisserman, 2008), Pets (Parkhi et al., 2012), STL-10 (Coates et al., 2011), Food101 (Bossard et al., 2014)), DTD (Cimpoi et al., 2014), SVHN (Netzer et al., 2011) and EuroSAT (Helber et al., 2019; 2018). These datasets vary in scales ranging from 5K to 1.2M training images.\nModels. We perform experiments on ViT-T/16 (Touvron et al., 2020) and ConvNeXt-F (Liu et al., 2022), with ImageNet-21K pretrained ViT-S/16 and ConvNeXt-T as their teachers respectively. We obtain weights for ImageNet-21K pretrained ViT-S/16 from Steiner et al. (2021) and ImageNet-21K pretrained ConvNeXt-T from Liu et al. (2022).\nTraining. We follow the training recipe from ConvNeXt (Liu et al., 2022) with adjustments to batch size, learning rate, and stochastic depth rate for different datasets. See Appendix A for details. To ensure a fair comparison, we adapt hyperparameters only for baseline training, and the same set of hyperparameters is used for training models with weight selection.\nRandom initialization baseline. We utilize the model-specific default initialization from the timm library (Wightman, 2019), a popular computer vision library with reliable reproducibility. Its default initialization of ViT-T and ConvNeXt-F employs a truncated normal distribution with a standard deviation of 0.02 for linear and convolution layers. The truncated normal distribution, due to its property to clip initialization values, is adopted to develop modern neural networks (Liu et al., 2022)."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "Our experiment results are presented in Table 1. Across all 9 image classification datasets (ordered by scale in the table), weight selection consistently boosts test accuracy, especially for smaller datasets. Weight selection addresses the well-known challenge of training ViT on small datasets, which likely contributes to the significant accuracy improvement for ViT. Training curves for ImageNet-1K are shown in Figure 3. Both models benefit from weight selection early on and maintain this advantage throughout training."
        },
        {
            "heading": "4.3 COMPARISONS",
            "text": "We compare weight selection and its variants with two widely-adopted initialization method: Xavier initialization (Glorot & Bengio, 2010b) and Kaiming initialization (He et al., 2015) as shown in Table 2. Weight selection with uniform, consecutive, and random selection yields considerably better results than all classic initialization methods. In addition, we observe a sharp drop in performance for weight selection without consistency, which proves the importance of maintaining consistency. Nonetheless, even weight selection without consistency exhibits a better performance than baseline on ViT-T and comparable performance to the best random initialization method on ConvNeXt-T."
        },
        {
            "heading": "4.4 COMPATIBILITY WITH KNOWLEDGE DISTILLATION",
            "text": "Weight selection transfers knowledge from pretrained models via parameters. Another popular approach for knowledge transferring is knowledge distillation (Hinton et al., 2015), which utilizes outputs from pretrained models. Here we explore the compatibility of these two techniques.\nSettings. We evaluate the performance of combining weight selection with two different approaches in knowledge distillation \u2013 logit-based distillation and feature-based distillation. Logit-based distillation uses KL-divergence as the loss function for matching student\u2019s and teacher\u2019s logits. Denote the student\u2019s output probabilities as ps, and teacher\u2019s output probabilities as pt, the loss for logit-based distillation can be formulated as\nL = Lclass + \u03b1 \u00b7KL(pt||ps) (1)\nwhere Lclass is supervised loss, and \u03b1 is the coefficient for distillation loss. Note that matching logits requires the teacher to be trained on the same dataset as the student. For logit-based distillation, We perform ImageNet-1K training experiments on ViT-T and use an ImageNet-1K pretrained ViT-S model from DeiT (Touvron et al., 2020) as the teacher for both knowledge distillation and weight selection. We set \u03b1 to 1 in this experiment.\nFeature-based distillation steps in when a classification head of target dataset is not available. Denote teacher\u2019s output as Ot, and student\u2019s output as Os. Feature-based distillation can be formulated as\nL = Lclass + \u03b1 \u00b7 L1(Ot,MLP (Os)) (2)\nAn MLP is used to project student\u2019s output to teacher\u2019s embedding dimension, and L1 loss is used to match the projected student\u2019s output and teacher\u2019s output. For feature-based distillation, we perform CIFAR-100 training experiments on ViT-T, using ImageNet-21K pretrained ViT-S as the teacher for both knowledge distillation and weight selection. We tune \u03b1 on distillation trials, and use the same value for \u03b1 for experiments with both distillation and weight selection.\nResults. Table 3 provides results for knowledge distillation and weight selection when applied individually or together. Introduction of logit-based distillation improves the ImageNet-1K accuracy by 0.9%. Similarly, feature-based distillation on CIFAR-100 increases accuracy by 6.4%. Without incurring additional inference cost, employing weight selection only produces a better result than the vanilla logit-based distillation and feature-based distillation. More importantly, the combination of distillation with weight selection delivers the best results, boosting accuracies to 76.0% on ImageNet1K and 83.9% on CIFAR-100. These results further confirm weight selection\u2019s usefulness as an independent technique and the compatibility between weight selection and knowledge distillation."
        },
        {
            "heading": "5 ANALYSIS",
            "text": "We perform comprehensive analysis on weight selection. Unless otherwise specified, we use weight selection from the ImageNet-21K pretrained ViT-S to initialize ViT-T, and the reported test accuracy is evaluated on CIFAR-100.\nReduction in training time. We directly measure the reduction in training time by training ViT-T with weight selection for different numbers of epochs and present the results in Figure 4. Warmup epochs is modified to maintain its ratio with total epochs. With weight selection, the same performance on CIFAR-100 can be obtained with only 1/3 epochs compared to training from random initialization.\nPretrained models. We study the effect of using different pretraining models. In addition to supervised learning, vision transformers can be used as the backbone for vision-language foundation models and self-supervised learning. We evaluate the performance of ViT-B as teacher under different pretraining regimes: CLIP (Radford et al., 2021), MAE (He et al., 2021), and DINO (Caron et al., 2021). Table 4 presents the results. Across all datasets, initializing with pretrained weights consistently outperforms the random initialization. The teacher model, when pretrained with supervised learning on ImageNet-21K, provides the most effective initialization. Note that for this experiment, we use ViT-B as the teacher, since it\u2019s the smallest model that MAE and CLIP provide.\nLayer selection. Shleifer & Rush (2020) selects maximally spaced layers from BERT to initialize small models. In this part, we compare two layer selection methods: maximally spaced layer selection and first-N layer selection. To evaluate different layer selection methods, We create ViT-A which has half the depth of ViT-S. In this experiment, we use ViT-A and ConvNeXt-F as student, and ImageNet-21K pretrained ViT-S and ConvNeXt-T as their teacher for weight selection. From Table 5, we find that first-N selection performs consistently better than evenly-spaced selection on both ViT-Atto and ConvNeXt-Femto. Presumably, since layers initialized by first-N selection are naturally contiguous, they offer a more effective initialization for smaller models.\nComparison with pruning.We test the existing structured and unstructured pruning methods on reducing pretrained ViT-S to ViT-T. An important thing to note is that our setting is different from neural network pruning. Structured pruning (Li et al., 2017a) typically only prunes within residual blocks for networks with residual connections, and unstructured pruning (Han et al., 2015) prune weights by setting weights to zero instead of removing it. Despite that these pruning methods are not designed for our setting, we can extend structured and unstructured pruning methods to be applied here. Specifically, we can adopt L1 pruning and magnitude pruning for element selection. For magnitude pruning, we squeeze the parameters into smaller matrices as the initialization for ViT-T.\nWe present results in Table 6. L1 pruning yields better results compared to random initialization baseline. Its gap with weight selection can be explained by absence of consistency on residual connections. Magnitude pruning only produces marginally better results over random initialization, presumably because of the squeezing operation, which does not preserve the original structure.\nTeacher\u2019s size. The size of the teacher model can be a crucial factor for the performance of weight selection. A larger teacher means a higher percentage of parameters will be discarded, which may affect the knowledge transferring process of weight selection. We present the result for using ViT-S, ViT-B, and ViT-L as the teacher in weight selection to initialize ViT-T in Table 7. Initializing from a teacher of closer size produces better results. Interestingly, even selecting 5M parameters from t301M parameters in ViT-L is effective, outperforming the random initialization baseline by 4.5%.\nLinear probing. We use linear probing to directly measure the raw model\u2019s ability as a feature extractor, which can be a good indicator of the initialization quality. Linear probing is a technique used to assess a pretrained model\u2019s representations by training a linear classifier on top of the fixed features extracted from the model.\nFollowing the recipe in He et al. (2021), we apply linear probing on CIFAR-100 to evaluate ViT-T and ConvNeXt-F initialized with weight selection from their ImageNet-21K pretrained teachers, ViT-S and ConvNeXt-T respectively. We compare the result between random initialization and weight selection with different element selection variants. As shown in Table 8, even without any training, the model from weight selection performs significantly better than random initialization as a feature extractor. Moreover, uniform, first, and random selection methods perform significantly better than inconsistent random selection, demonstrating the importance of consistency in weight selection.\nLonger training on ImageNet-1K. To assess if our initialization method remains beneficial for extended training durations, we use the improved training recipe from Liu et al. (2023). Specifically, the total epochs are extended to 600, and mixup / cutmix are reduced to 0.3. The results, as displayed in Table 9, affirm that our method continues to provide an advantage even under extended training durations. Both ViT-T and ConvNeXt-F, when initialized using weight selection, consistently surpass models initialized randomly. This confirms that our method does not compromise the model\u2019s capacity to benefit from longer training.\nMimetic initialization. Mimetic initialization (Trockman & Kolter, 2023) uses the diagonal properties of trained self-attention layer\u2019s weights to initialize ViTs. We present results for mimetic initialization in Table 10. Mimetic initialization improves upon random initialization baseline. By directly utilizing pretrained parameters, weight selection outperforms mimetic initialization by a large margin. In addition, we visualize the product of WqWTk and VWproj the first head in the first attention block of ViT-T with random initialization, pretrained ViT-S, and ViT-T with weight selection. As shown in Figure 5, weight selection enables small models to inherit the desirable diagonal properties in its self-attention layers, which only exists in pretrained models."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose weight selection, a novel initialization method that utilizes large pretrained models. With no extra cost, it is effective for improving the accuracy of a small model and reducing its training time needed to reach a certain accuracy level. We extensively analyze its properties and compare it with alternative methods. We hope our research can inspire further exploration into algorithms for training small neural networks.\nReproducibility Statement: We provide our training recipe and hyperparameters in detail in Appendix A. Our code is available at this anonymous GitHub link: https://github.com/ anonymous-wivwauyg/weight-selection."
        },
        {
            "heading": "A TRAINING SETTINGS",
            "text": "Training recipe. We provide our training recipe with configurations in Table 11. The recipe is adapted from ConvNeXt (Liu et al., 2022).\nHyper-parameters. Table 12 and Table 13 record batch size, warmup epochs, and training epochs of ConvNeXt-F and Vit-T, respectively, for each dataset. The batch size of each dataset is chosen proportional to its total size. The warmup epochs are set as around one-fifth of the total training epochs. Base learning rates for ConvNeXt-F and ViT-T are 4e-3 and 2e-3 respectively."
        },
        {
            "heading": "B WEIGHT COMPONENTS",
            "text": "We conduct ablation studies on ViT-T to understand the influence of distinct model components on performance. In particular, we evaluate the performance of weight selection without one of the following particular type of layers: patch embedding, position embedding, attention block, normalization layer, or MLP layer. As illustrated in Table 14, excluding component from initialization leads to substantial drops in accuracy for all datasets. The results confirm that initializing with all components from pretrained models is necessary."
        }
    ],
    "year": 2023
}