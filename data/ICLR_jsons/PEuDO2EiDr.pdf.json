{
    "abstractText": "Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the prior SOTA method in both inference speed and separation quality while reducing the number of parameters by 90% and MACs by 83%. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.",
    "authors": [
        {
            "affiliations": [],
            "name": "ELLING FOR"
        },
        {
            "affiliations": [],
            "name": "Samuel Pegg"
        },
        {
            "affiliations": [],
            "name": "Kai Li"
        },
        {
            "affiliations": [],
            "name": "Xiaolin Hu"
        }
    ],
    "id": "SP:508cea28949e6e3bcf7882d73c24c7e5af1ca16c",
    "references": [
        {
            "authors": [
                "Triantafyllos Afouras",
                "Joon Son Chung",
                "Andrew Senior",
                "Oriol Vinyals",
                "Andrew Zisserman"
            ],
            "title": "Deep audio-visual speech recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Triantafyllos Afouras",
                "Joon Son Chung",
                "Andrew Zisserman"
            ],
            "title": "Lrs3-ted: a large-scale dataset for visual speech recognition, 2018b",
            "year": 2018
        },
        {
            "authors": [
                "Triantafyllos Afouras",
                "Andrew Owens",
                "Joon Son Chung",
                "Andrew Zisserman"
            ],
            "title": "Self-supervised learning of audio-visual objects from video",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "T Alfouras",
                "JS Chung",
                "A Zisserman"
            ],
            "title": "The conversation: deep audio-visual speech enhancement",
            "venue": "In Interspeech,",
            "year": 2018
        },
        {
            "authors": [
                "Adelbert Bronkhorst"
            ],
            "title": "The cocktail party phenomenon: A review of research on speech intelligibility in multiple-talker conditions",
            "venue": "Acta Acustica united with Acustica, 86:117\u2013128,",
            "year": 2000
        },
        {
            "authors": [
                "Jingjing Chen",
                "Qirong Mao",
                "Dong Liu"
            ],
            "title": "Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation",
            "venue": "In Proc. Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "E. Colin Cherry"
            ],
            "title": "Some Experiments on the Recognition of Speech, with One and with Two Ears",
            "venue": "The Journal of the Acoustical Society of America, 25(5):975\u2013979,",
            "year": 2005
        },
        {
            "authors": [
                "J Chung",
                "A Nagrani",
                "A Zisserman"
            ],
            "title": "Voxceleb2: Deep speaker recognition",
            "venue": "In Interspeech,",
            "year": 2018
        },
        {
            "authors": [
                "Martin Cooke",
                "Jon Barker",
                "Stuart Cunningham",
                "Xu Shao"
            ],
            "title": "An audio-visual corpus for speech perception and automatic speech recognition",
            "venue": "The Journal of the Acoustical Society of America,",
            "year": 2006
        },
        {
            "authors": [
                "Ruohan Gao",
                "Kristen Grauman"
            ],
            "title": "Visualvoice: Audio-visual speech separation with crossmodal consistency",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Simon Haykin",
                "Zhe Chen"
            ],
            "title": "The Cocktail Party Problem",
            "venue": "Neural Computation,",
            "year": 1875
        },
        {
            "authors": [
                "John R Hershey",
                "Zhuo Chen",
                "Jonathan Le Roux",
                "Shinji Watanabe"
            ],
            "title": "Deep clustering: Discriminative embeddings for segmentation and separation",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2016
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "Xiaolin Hu",
                "Kai Li",
                "Weiyi Zhang",
                "Yi Luo",
                "Jean-Marie Lemercier",
                "Timo Gerkmann"
            ],
            "title": "Speech separation using an asynchronous fully recurrent convolutional neural network",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Le Roux",
                "Scott Wisdom",
                "Hakan Erdogan",
                "John R Hershey"
            ],
            "title": "Sdr\u2013half-baked or well done",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Jiyoung Lee",
                "Soo-Whan Chung",
                "Sunok Kim",
                "Hong-Goo Kang",
                "Kwanghoon Sohn"
            ],
            "title": "Looking into your speech: Learning cross-modal affinity for audio-visual speech separation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Tao Lei",
                "Yu Zhang",
                "Sida I Wang",
                "Hui Dai",
                "Yoav Artzi"
            ],
            "title": "Simple recurrent units for highly parallelizable recurrence",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2018
        },
        {
            "authors": [
                "Kai Li",
                "Fenghua Xie",
                "Hang Chen",
                "Kexin Yuan",
                "Xiaolin Hu"
            ],
            "title": "An audio-visual speech separation model inspired by cortico-thalamo-cortical circuits",
            "venue": "arXiv preprint arXiv:2212.10744,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Li",
                "Runxuan Yang",
                "Xiaolin Hu"
            ],
            "title": "An efficient encoder-decoder architecture with top-down attention for speech separation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Jiuxin Lin",
                "Xinyu Cai",
                "Heinrich Dinkel",
                "Jun Chen",
                "Zhiyong Yan",
                "Yongqing Wang",
                "Junbo Zhang",
                "Zhiyong Wu",
                "Yujun Wang",
                "Helen Meng"
            ],
            "title": "Av-sepformer: Cross-attention sepformer for audiovisual target speaker extraction",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Philipos C Loizou"
            ],
            "title": "Speech enhancement: theory and practice",
            "venue": "CRC press,",
            "year": 2013
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Yi Luo",
                "Nima Mesgarani"
            ],
            "title": "Tasnet: time-domain audio separation network for real-time, singlechannel speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Yi Luo",
                "Nima Mesgarani"
            ],
            "title": "Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Yi Luo",
                "Jianwei Yu"
            ],
            "title": "Music source separation with band-split rnn",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Yi Luo",
                "Cong Han",
                "Nima Mesgarani",
                "Enea Ceolini",
                "Shih-Chii Liu"
            ],
            "title": "Fasnet: Low-latency adaptive beamforming for multi-microphone audio processing",
            "venue": "In Automatic Speech Recognition and Understanding (ASRU),",
            "year": 2019
        },
        {
            "authors": [
                "Yi Luo",
                "Zhuo Chen",
                "Takuya Yoshioka"
            ],
            "title": "Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "H\u00e9ctor Martel",
                "Julius Richter",
                "Kai Li",
                "Xiaolin Hu",
                "Timo Gerkmann"
            ],
            "title": "Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model",
            "venue": "In Proc. Interspeech",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Owens",
                "Alexei A Efros"
            ],
            "title": "Audio-visual scene analysis with self-supervised multisensory features",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Antony W Rix",
                "John G Beerends",
                "Michael P Hollier",
                "Andries P Hekstra"
            ],
            "title": "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings (Cat. No. 01CH37221),",
            "year": 2001
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Joon Son Chung",
                "Andrew Senior",
                "Oriol Vinyals",
                "Andrew Zisserman"
            ],
            "title": "Lip reading sentences in the wild",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Cem Subakan",
                "Mirco Ravanelli",
                "Samuele Cornell",
                "Mirko Bronzi",
                "Jianyuan Zhong"
            ],
            "title": "Attention is all you need in speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Thanh-Dat Truong",
                "Chi Nhan Duong",
                "Hoang Anh Pham",
                "Bhiksha Raj",
                "Ngan Le",
                "Khoa Luu"
            ],
            "title": "The right to talk: An audio-visual transformer approach",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Emmanuel Vincent",
                "R\u00e9mi Gribonval",
                "C\u00e9dric F\u00e9votte"
            ],
            "title": "Performance measurement in blind audio source separation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2006
        },
        {
            "authors": [
                "DeLiang Wang",
                "Guy J Brown"
            ],
            "title": "Computational auditory scene analysis: Principles, algorithms, and applications",
            "venue": "Wiley-IEEE press,",
            "year": 2006
        },
        {
            "authors": [
                "Zhong-Qiu Wang",
                "Samuele Cornell",
                "Shukjae Choi",
                "Younglo Lee",
                "Byeong-Yeol Kim",
                "Shinji Watanabe"
            ],
            "title": "Tf-gridnet: Making time-frequency domain models great again for monaural speaker separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Jian Wu",
                "Yong Xu",
                "Shi-Xiong Zhang",
                "Lian-Wu Chen",
                "Meng Yu",
                "Lei Xie",
                "Dong Yu"
            ],
            "title": "Time domain audio visual speech separation",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2019
        },
        {
            "authors": [
                "Lei Yang",
                "Wei Liu",
                "Weiqin Wang"
            ],
            "title": "Tfpsnet: Time-frequency domain path scanning network for speech separation",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Kaipeng Zhang",
                "Zhanpeng Zhang",
                "Zhifeng Li",
                "Yu Qiao"
            ],
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "venue": "IEEE signal processing letters,",
            "year": 2016
        },
        {
            "authors": [
                "Li"
            ],
            "title": "Echoing the findings",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023) directly to the AVSS setting. This model is powerful, and so with only 4 layers",
            "year": 2023
        },
        {
            "authors": [
                "VP Block"
            ],
            "title": "We used a TDANet block (Li et al., 2023) with a hidden dimension of 64, a kernel size of 3, an up-sampling depth of 4, replaced the gLNs with batch normalization and set the number of attention heads in the MHSA to 8. The feed forward network (FFN) had 128 channels. Model code was supplied by the original auther. AP and RTFS Blocks. We used a hidden dimension of D = 64 as the reduced channel dimension",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023)\u2019s paper. CAF Block. We used h = 4 attention heads. H.1 REDUCED MODEL The reduced model used in Table 2 was achieved by using a modified version of the RTFS-Net-4 configuration defined above. Firstly, we set the hidden dimension of the RTFS Blocks to D",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The \u2018cocktail party problem\u2019 (Bronkhorst, 2000; Haykin & Chen, 2005; Cherry, 2005) describes our ability to focus on a single speaker\u2019s voice amidst numerous overlapping voices in a noisy environment. While humans effortlessly tackle this problem, replicating this ability in machines remains a longstanding challenge in the field of signal-processing. Audio-only Speech Separation (AOSS) methods (Luo & Mesgarani, 2019; Luo et al., 2020; Subakan et al., 2021), solely utilizing the mixed-speaker audio signal, face limitations in scenarios with strong background noise, reverberation, or heavy voice overlap. To overcome these issues, researchers turned to a multi-modal approach: Audio-visual Speech Separation (AVSS) (Gao & Grauman, 2021; Lee et al., 2021; Li et al., 2022). AVSS methods integrate additional visual cues into the paradigm and can be generally split into two main classifications: Time-domain (T-domain) and Time-Frequency domain (TF-domain) methods, each with their own benefits and challenges.\nT-domain methods (Wu et al., 2019; Li et al., 2022; Martel et al., 2023; Lin et al., 2023) work on the long, uncompressed and high-dimensional audio features returned by the 1D convolutional encoder design proposed by Luo & Mesgarani (2018). This approach facilitates fine-grained, high quality audio separation, but the high parameter count and computational complexity leads to extended training periods, intensive GPU usage and slow inference speeds. On the other hand, TF-domain\n\u2217Made significant contributions. \u2020 Corresponding author.\nmethods (Afouras et al., 2018a; Alfouras et al., 2018; Gao & Grauman, 2021) apply their algorithms on the complex 2D representation yielded by the Short-Time Fourier Transform (STFT). Typically, the STFT uses large windows and hop lengths to compress the data, resulting in more computationally efficient separation methods. However, from a historical perspective, all TF-domain methods have been substantially outperformed by T-domain methods. Based on our research, this gap in performance stems from three critical factors.\nFirstly, while some attempts have been made (Afouras et al., 2020; Lee et al., 2021) to model amplitude and phase separately, no TF-domain AVSS methods explore the independent and tailored modelling of the two acoustic dimensions (time and frequency) in order to exploit this domain\u2019s advantage over T-domain methods. Indeed, recent research by Luo & Yu (2023) and Wang et al. (2023) in the AOSS domain have capitalized on this advantage and outperformed their T-domain counterparts by large margins. However, their usage of large LSTM (Hochreiter & Schmidhuber, 1997) and transformer (Vaswani et al., 2017) architectures leads to a heavy computational burden, making them an unattractive solution in the AVSS space. Secondly, while existing AVSS studies (Li et al., 2022; Lin et al., 2023) have explored various audio-visual fusion strategies, they neglect using visual features from multiple receptive fields to increase model performance. This type of visual information serves as important a\u0304 prio\u0304r\u0131\u0304 knowledge crucial for accurately extracting the target speaker\u2019s voice. Thirdly, TF-domain AVSS studies (Afouras et al., 2020; Lee et al., 2021; Gao & Grauman, 2021) often overlook the underlying complex nature of the features, and hence lose critical amplitude and phase information when extracting the target speaker\u2019s voice from the audio mixture. This degrades the reconstruction performance of the inverse STFT (iSTFT) and leads to poorer model performance.\nIn this work, we propose a novel TF-domain AVSS method: Recursive Time-Frequency Separation Network (RTFS-Net, Figure 1) that provides a computationally efficient solution to the cocktail party problem using an STFT based audio encoder, a high-fidelity pretrained video encoder and a series of recursive RTFS Blocks to perform target speaker extraction. Our contributions are threefold:\n1. Our RTFS Blocks tackle the first issue by projecting the features to a compressed subspace. There, we explicitly model both acoustic dimensions individually, then in tandem before subsequently applying an attentional mechanism (TF-AR) to restore the dimensions with minimal information loss, allowing us to reap the benefits of independent time-frequency processing without bearing a substantial computational cost.\n2. Our Cross-dimensional Attention Fusion (CAF) Block provides a low parameter, computationally efficient solution to the second issue by aggregating the multi-modal information through a multi-head attention strategy in order to optimally fuse the visual cues of the target speaker into the audio features, facilitating high quality separation.\n3. Our Spectral Source Separation (S3) Block addresses the third issue by explicitly reconstructing the plural features of the target speaker, achieving a higher quality separation without increasing computational cost.\nWe conducted comprehensive experimental evaluations on three widely used datasets: LRS2 (Afouras et al., 2018a), LRS3 (Afouras et al., 2018b) and VoxCeleb2 (Chung et al., 2018), to demonstrate the value of each contribution. To the best of our knowledge, RTFS-Net is the first TF-domain AVSS method to outperform all contemporary T-domain methods, achieving this while also exhibiting a clear advantage by reducing computational complexity by 83% and reducing the parameter count by 90%. We additionally provide a Web page where sample results can be listened to, and our code will be open-sourced after publication for reproducibility purposes."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Audio-only speech separation. Modern deep learning advances introduced neural networks for speaker-agnostic speech separation, with Conv-TasNet (Luo & Mesgarani, 2019) and DualPathRNN (Luo et al., 2020) making significant T-domain breakthroughs. However, T-domain methods (Luo & Mesgarani, 2019; Luo et al., 2020) often exhibit a marked performance degradation in reverberant conditions, attributed to their neglect of explicit frequency-domain modeling. Consequently, recent research focuses on high-performance speech separation models in the TF-domain. For instance, TFPSNet (Yang et al., 2022) incorporates the transformer from DPTNet (Chen et al., 2020) to\nassimilate spectral-temporal information. TF-GridNet (Wang et al., 2023) extends this by introducing a cross-frame self-attention module in order to achieve SOTA performance on the WSJ0-2mix (Hershey et al., 2016). However, the inclusion of many LSTM and Transformer layers results in extremely high computational complexity, leading to increased training times and GPU memory requirements. Lightweight models like A-FRCNN (Hu et al., 2021) and TDANet (Li et al., 2023) strike a balance between performance and efficiency by using an encoder-decoder paradigm with recurrent connections and top-down attention. However, their separation prowess in noisy scenarios remains suboptimal compared to multi-modal approaches.\nAudio-visual speech separation. Conventional AVSS statistical methods relied on knowledgedriven modeling (Loizou, 2013; Wang & Brown, 2006). Early deep-learning explorations into AVSS mainly occurred in the TF-domain, focusing on tasks like amplitude and phase reconstruction for target speakers (Afouras et al., 2018a; Alfouras et al., 2018). With the advent of AV-ConvTasNet (Wu et al., 2019), it became a prevailing belief that T-domain AVSS methods consistently outperformed TF-domain methods. Notably, CTCNet (Li et al., 2022), inspired by thalamic brain structures, introduced a unique multiple-fusion approach. Leveraging multiscale contexts from visual and auditory cues, this module greatly enhanced the spatial scale of fused features, thus improving the model\u2019s separation capacity. However, as mentioned previously, T-domain methods come with a higher computational load. TF-domain AVSS techniques often use larger windows and hop sizes, which curtails computational complexity. Nevertheless, their full potential is yet to be realized. The recent Visualvoice model (Gao & Grauman, 2021) combined a multi-task learning framework for both AVSS and cross-modal speaker embedding, incorporating facial expressions, lip movements, and audio cues. Despite its touted efficiency, Visualvoice lacks robust modeling, and it lags behind many modern T-domain methods."
        },
        {
            "heading": "3 METHODS",
            "text": "Expanding on prior SOTA methods (Wu et al., 2019; Li et al., 2022), we present our AVSS pipeline, illustrated in Figure 1. The mono-aural mixed-speech audio signal, x \u2208 R1\u00d7La , in conjunction with the video frames capturing the target speaker\u2019s lip movements, y \u2208 R1\u00d7Lv\u00d7H\u00d7W , are used as inputs to RTFS-Net in order to derive the target speaker\u2019s estimated audio signal, s\u0302 \u2208 R1\u00d7La . In this context, La and Lv signify the durations of the audio and video inputs, while H and W correspond to the dimensions of the single-channel (grey-scale) video frames.\nFirstly, the audio and video encoders extract auditory a0 and visual v0 features. These serve as the inputs for our separation network, which fuses these features and extracts the salient multimodal features, aR. Next, our Spectral Source Separation (S3) method is applied to separate the target speaker\u2019s audio z from the encoded audio signal a0 using aR. Finally, the target speaker\u2019s estimated audio feature map z is decoded into the estimated audio stream s\u0302 and compared with the groundtruth signal s for training."
        },
        {
            "heading": "3.1 ENCODERS",
            "text": "Our encoders distill relevant features from both audio and visual inputs. For the video encoder Ev(\u00b7), we employ the CTCNet-Lip (Li et al., 2022) pretrained network to extract the visual features\nv0 of the target speaker, v0 = Ev(y), v0 \u2208 RCv\u00d7Tv . (1)\nFor the audio encoder, we firstly define \u03b1 as the complex-valued hybrid TF-domain bins obtained using the STFT on x. For a mixed audio of nspk speakers, we define si as the speech of speaker i and \u03f5 as the presence of some background noise, music, or other extraneous audio sources. Then,\n\u03b1(t, f) = \u03f5(t, f) + nspk\u2211 i=1 si(t, f), \u03b1(t, f) \u2208 C \u2200 (t, f), (2)\nwhere t \u2208 [0, Ta] is the time dimension and f \u2208 [0, F ] is the frequency dimension. We concatenate the real (Re) and imaginary (Im) parts of\u03b1 along a new \u2018channels\u2019 axis, then apply a 2D convolution Ea(\u00b7) with a 3\u00d73 kernel and Ca output channels across the time and frequency dimensions to obtain the auditory embedding a0 of x. Using the symbol || for concatenation, we write,\na0 = Ea (Re(\u03b1)||Im(\u03b1)) , a0 \u2208 RCa\u00d7Ta\u00d7F . (3)"
        },
        {
            "heading": "3.2 SEPARATION NETWORK",
            "text": "The core of RTFS-Net is a separation network that uses recursive units to facilitate information interaction in the two acoustic dimensions, and efficiently aggregates multimodal features using an attention-based fusion mechanism. The initial step is to preprocess the auditory a0 and visual v0 features separately in preparation for fusion. For the Visual Preprocessing (VP) Block we adopt a modified version of the TDANet Block (Li et al., 2023), see Appendix A. For the Audio Preprocessing (AP) Block we use a single RTFS Block, whose structure will be defined in Section 3.2.2. The outputs of the two preprocessing blocks are fed to our proposed CAF Block to fuse the multimedia features into a single enriched feature map (see Figure 1 and 2). This audio-visual fusion is subsequently processed with an additional R stacked RTFS Blocks. Following CTCNet (Li et al., 2022), these R sequential blocks share parameters (including the AP Block), which has been shown to reduce model size and increase performance, since it turns the series of blocks in to a recurrent neural architecture (see Appendix B)."
        },
        {
            "heading": "3.2.1 CROSS-DIMENSIONAL ATTENTION FUSION BLOCK",
            "text": "The CAF Block (Figure 2) is a depth-wise and group-convolution based architecture designed to consume as little resources as possible while fusing the 2D visual data into the 3D audio data. It involves two separate fusion operations that we call the attention fusion (f 1) and the gated fusion (f 2). The attention fusion considers multiple visual sub-representation spaces to aggregate information from a wide receptive field and apply attention to the audio features. The gated fusion up-samples the visual information\u2019s time dimension, then expands the visual features into the TF-domain using F gates produced from the preprocessed audio features. We use Vaswani et al. (2017)\u2019s \u2018keys\u2019 and \u2018values\u2019 nomenclature.\nFirstly, let both P1 and P2 denote a depth-wise convolution with a 1\u00d71 kernel and following global layer normalization (gLN) (Luo & Mesgarani, 2019). We generate the audio \u2018value\u2019 embeddings and the aforementioned \u2018gate\u2019 from the preprocessed audio signal a1.\naval = P1(a1), agate = ReLU (P2(a1)) , aval, agate \u2208 RCa\u00d7Ta\u00d7F . (4)\nAttention Fusion. We apply a 1D group convolution F1 with Ca groups and Ca\u00d7h output channels to v1, followed by a gLN layer. By chunking across the channels, we can decompose the visual features into h distinct sub-feature representations, or attention \u2018heads\u2019, vh. Next, we take the \u2018mean\u2019 of the h heads to aggregate the information from the different sub-feature representations into vm, then subsequently apply the Softmax operation in order to create a multi-head attention style set of features vattn with values between 0 and 1. To align the video frame length Tv with the audio\u2019s time dimension Ta, we use nearest neighbor interpolation, \u03d5. This is written,\nvh = F1(v1), vh \u2208 R(Ca\u00d7h)\u00d7Tv , (5) vm = mean(vh[1], . . . , vh[h])), vm \u2208 RCa\u00d7Tv , (6)\nvattn = \u03d5(Softmax(vm)), vattn \u2208 RCa\u00d7Ta . (7)\nOur attention mechanism is applied to each of the F \u2018value\u2019 slices of aval with length Ta,\nf 1[i] = vattn \u2299 aval[i], \u2200i \u2208 {1, . . . , F}, (8)\nwhere f 1[i] \u2208 RCa\u00d7Ta \u2200i =\u21d2 f 1 \u2208 RCa\u00d7Ta\u00d7F . Gated Fusion. We use a 1D convolutional layer F2 with kernel size 1, Ca output channels and Ca groups (since Ca < Cv), followed by a gLN layer to align Cv with Ca. Next, we again use interpolation \u03d5 to align Tv with Ta and generate the visual \u2018key\u2019 embeddings.\nvkey = \u03d5 (F2(v1)) , vkey \u2208 RCa\u00d7Ta . (9)\nNext, we utilize all F of the Ta-dimensional slices of agate as unique gates to comprehensively expand the visual information into the TF-domain,\nf 2[i] = agate[i]\u2299 vkey, \u2200i \u2208 {1, . . . , F}, (10)\nwhere f 2[i] \u2208 RCa\u00d7Ta \u2200i =\u21d2 f 2 \u2208 RCa\u00d7Ta\u00d7F . CAF Block. Finally, we sum the two fused features together. We can denote our CAF Block, \u03a6, as:\na2 = \u03a6(a1, v1) = f 1 + f 2, a2 \u2208 RCa\u00d7Ta\u00d7F . (11)"
        },
        {
            "heading": "3.2.2 RTFS BLOCKS",
            "text": "Compared to previous TF-domain AVSS methods (Afouras et al., 2020; Gao & Grauman, 2021; Alfouras et al., 2018; Lee et al., 2021), our RTFS Blocks use a dual-path architecture to explicitly model audio in both acoustic dimensions to improve the separation quality, as shown in Figure 3. We denote the auditory features input into the RTFS Block as A. Given our recurrent structure, A represents either a0 (input to AP Block) or the output from the previous RTFS Block with a skip connection: aj + a0 for j \u2208 {1, ..., R}. Note that in Figure 1 the residual connection is not shown for simplicity. Our RTFS Block processes auditory features in the four steps discussed below.\nCompression of time and frequency resolution. We use a 2D convolution with a 1 \u00d7 1 kernel to convert A to a smaller channel dimension D < Ca. This means we can effectively employ a larger Ca value for detailed fusion (CAF) and separation (S3), while maintaining a lightweight and efficient block design. Similar to Li et al. (2023), in the compression phase we employ q stacked 2D depth-wise convolutional layers with 4 \u00d7 4 kernels and stride 2, see Appendix C for the effects\nof different q values. The resultant multi-scale set with varying temporal and frequency resolutions can be denoted {Ai|i \u2208 {0, q \u2212 1}}, where Ai \u2208 RD\u00d7 Ta 2i \u00d7 F 2i . We use adaptive average pooling p to compress each member of the set to the dimensions of the smallest member, Aq\u22121, then sum to obtain the compressed global features AG. This is written:\nAG = q\u22121\u2211 i=0 p(Ai), AG \u2208 RD\u00d7 Ta 2q\u22121 \u00d7 F 2q\u22121 . (12)\nDual-path architecture. The Dual-path RNN architecture has been extensively deployed in AOSS tasks (Luo et al., 2020; Chen et al., 2020; Wang et al., 2023). However, their usage of large LSTMs leads to high parameter counts and an elevated computational complexity. In natural language processing, Simple Recurrent Units (SRU (Lei et al., 2018)) were introduced to replace LSTMs by processing most of the operations in parallel, speeding up model training and inference time. Inspired by this, we adopt SRUs for the AVSS task, see Appendix D for a detailed analysis between different recurrent architectures.\nAs seen in Figure 3, we first process the frequency dimension, then the time dimension. As with all dual-path methods, the SRUs are applied across each slice in the time dimension to process the frequency dimension. Similar to (Wang et al., 2023), we unfold1 the features by zero-padding the frequency dimension of AG, then unfolding with kernel size 8 and stride 1,\nR\u0307f = [ Unfold(AG[:, t]), t \u2208 { 0, . . . , Ta/2 q\u22121}] \u2208 R8D\u00d7 Ta2q\u22121 \u00d7F \u2032 , (13) where F \u2032 is the resulting padded and unfolded frequency dimension. Note that while this process results in high input dimensions for the SRUs, which significantly increases computational complexity, it is essential for achieving outstanding separation performance. This is why it is so important that we first compress the time-frequency resolution. After unfolding, layer normalization is applied in the channel dimension, and then a bidirectional, 4 layer SRU with hidden size ha is applied,\nR\u0308f = [ SRU(R\u0307f [:, t]), t \u2208 {0, . . . , Ta/2q\u22121} ] \u2208 R2ha\u00d7 Ta 2q\u22121 \u00d7F \u2032 . (14)\nA transposed convolution T with kernel 8 and stride 1 is used to restore the unfolded dimensions, ... R f = T ( R\u0308f ) +AG, ... R f \u2208 RD\u00d7 Ta 2q\u22121 \u00d7 F 2q\u22121 . (15)\nWe next process the time dimension using the same method, and then finally apply Wang et al. (2023)\u2019s TF-domain self-attention network, denoted Attn. These two steps are expressed below as:\n... R t = T ( R\u0308t ) + ... R f , A\u0304G = Attn( ... R t) + ... R t,\n... R t, A\u0304G \u2208 RD\u00d7 Ta 2q\u22121 \u00d7 F 2q\u22121 . (16)\nReconstruction of time and frequency resolution. Reconstruction of high-quality temporal and frequency features presents a formidable challenge. Delving into the underlying causes, the reconstruction process often relies on interpolation or transposed convolutions for up-sampling, resulting in the emergence of checkerboard artifacts in the reconstructed outputs. To solve this problem, we propose the Temporal-Frequency Attention Reconstruction (TF-AR) unit, denoted I(\u00b7, \u00b7). This unit prioritizes the reconstruction of key features by exploiting an attention mechanism, thus reducing information loss. For two tensors m and n, we define the TF-AR unit as:\nI(m,n) = \u03d5 (\u03c3 (W1 (n)))\u2299W2 (m) + \u03d5 (W3 (n)) , (17) where W1(\u00b7), W2(\u00b7) and W3(\u00b7) denote 2D depth-wise convolutions with 4 \u00d7 4 kernels followed by a gLN layer. We use the notation \u03c3 for the sigmoid function, \u2299 for element-wise multiplication and \u03d5 for nearest neighbour interpolation (up-sampling). To conduct the reconstruction, we firstly use q TF-AR units to fuse A\u0304G with every element of Ai,\nA\u2032i = I(Ai, A\u0304G), A \u2032 i \u2208 R D\u00d7Ta 2i \u00d7 F 2i \u2200i \u2208 {0, . . . , q \u2212 1}. (18)\nNext, the multi-scale features are continuously up-sampled and aggregated using q \u2212 1 additional TF-AR units to obtain the finest-grained auditory features, A\u2032\u20320 \u2208 RD\u00d7Ta\u00d7F . A residual connection to {Ai|i \u2208 {0, q \u2212 2}} is crucial, and creates a U-Net (Ronneberger et al., 2015) style structure:\nA\u2032\u2032q\u22121\u2212i = I(A \u2032 q\u22121\u2212i,A \u2032 q\u2212i) +Aq\u22121\u2212i, \u2200i \u2208 {0, . . . , q \u2212 2}. (19)\nFinally, A\u2032\u20320 is converted back to Ca channels using a 2D convolution with a 1 \u00d7 1 kernel and a residual connection to the input of the RTFS Block is added. The output features are used as the audio input for the CAF Block after the AP Block, and as the input to the next RTFS Block during the repeated R stacked RTFS Blocks stage of the separation network.\n1https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html"
        },
        {
            "heading": "3.3 SPECTRAL SOURCE SEPARATION",
            "text": "The majority of existing T-domain AVSS methods (Wu et al., 2019; Li et al., 2022; Martel et al., 2023) generate a mask m from the refined features aR, then use element-wise multiplication \u2299 between the encoded audio mixture a0 and the mask in order to obtain the target speaker\u2019s separated speech z . This is written, z =m \u2299 a0. (20) Some TF-domain AVSS methods (Afouras et al., 2018a; Gao & Grauman, 2021; Lee et al., 2021) directly apply this approach to the TF-domain setting without modification, while other methods (Alfouras et al., 2018; Owens & Efros, 2018) choose not to use masks at all, and directly passes the output of the separation network aR to the decoder. However, we found both of these TFdomain methods for target speaker extraction to be suboptimal. We need to pay attention to the underlying complex nature of the audio features produced by the STFT in order to obtain a clearer distinction. This leads us to introduce our S3 Block, which utilizes a high-dimensional application of the multiplication of complex numbers, Equation 21, to better preserve important acoustic properties during the speaker extraction process, see Section 5.2 and Appendix E.\n(a+ bi)(c+ di) = ac\u2212 bd+ i(ad+ bc). (21) Firstly, a mask m is generated from aR using a 2D convolution M with a 1\u00d7 1 kernel, m = ReLU (M (PReLU(aR))) , m \u2208 RCa\u00d7Ta\u00d7F . (22) Without loss of generality, we choose the top half of the channels as the real part, and the bottom half of the channels as the imaginary part. We hence define,\nmr =m [0 : Ca/2\u2212 1] , E r = a0 [0 : Ca/2\u2212 1] , (23) mi =m [Ca/2 : Ca] , E\ni = a0 [Ca/2 : Ca] . (24) Next, with || denoting concatenation along the channel axis, we apply Equation 21 and calculate:\nzr =mr \u2299E r \u2212mi \u2299E i, (25) z i =mr \u2299E i +mi \u2299E r, (26) z = (zr || z i), z \u2208 RCa\u00d7Ta\u00d7F , (27)\nto obtain z , the target speaker\u2019s separated encoded audio features."
        },
        {
            "heading": "3.4 DECODER",
            "text": "The decoder D(\u00b7) takes the separated target speaker\u2019s audio features z and reconstructs the estimated waveform s\u0302 = D(z), where s\u0302 \u2208 R1\u00d7La . Specifically, z is passed through a transposed 2D convolution with a 3 \u00d7 3 kernel and 2 output channels. Mirroring the encoder, we take the first channel as the real part and the second channel as the imaginary part and form a complex tensor. This tensor is passed to the iSTFT to recover the estimated target speaker audio."
        },
        {
            "heading": "4 EXPERIMENTAL SETUP",
            "text": "Datasets. We utilized the same AVSS datasets as previous works (Gao & Grauman, 2021; Li et al., 2022) in the field in order to create a fair comparison of performance: LRS2-2Mix (Afouras et al., 2018a), LRS3-2Mix (Afouras et al., 2018b) and VoxCeleb2-2Mix (Chung et al., 2018). The models were trained and tested on two second 25fps video clips with an audio sampling rate of 16 kHz. This equates to 32,000 audio frames and 50 video frames, see Appendix F for more details.\nEvaluation. Following recent literature (Li et al., 2022), SI-SNRi and SDRi were used to evaluate the quality of the separated speeches, see Appendix G. We also provided PESQ (Rix et al., 2001) for reference. For these metrics, a higher value indicates better performance. The parameter counts displayed in the results tables are the number of trainable parameters, excluding the pretrained video model. Likewise, the number of Multiply-ACcumulate (MAC) operations indicates the MACs used while processing two seconds of audio at 16 kHz, excluding the pretrained video network. In our main results table, we also include inference time: the time taken to process 2 seconds of audio on a NVIDIA 2080 GPU. For parameters, MACs and inference speed, a lower value is preferable and is the main focus of this work. Model hyperparameter configurations are available in Appendix H. All training settings are available in Appendix I."
        },
        {
            "heading": "5 RESULTS",
            "text": ""
        },
        {
            "heading": "5.1 COMPARISONS WITH STATE-OF-THE-ART METHODS",
            "text": "The goal of this research is to obtain SOTA performance while significantly reducing computational complexity and model size with a lightweight, streamlined approach. In the comprehensive comparison presented in Table 1, we evaluated RTFS-Net against a range of SOTA AVSS methods. We explored three variants of RTFS-Net, corresponding to R = {4, 6, 12} RTFS Blocks, including the AP Block. On the LRS2-2Mix dataset, RTFS-Net-4 demonstrated an SI-SNRi of 14.1 dB, only marginally lower than the 14.3 dB achieved by the previous SOTA, CTCNet. However, this was achieved with a tenfold reduction in model parameters and an eightfold decrease in computational cost. Meanwhile, RTFS-Net-6 surpassed CTCNet on the LRS2-2Mix dataset and delivered comparable results on VoxCeleb2-2Mix, striking a good balance between performance and efficiency. RTFS-Net-12 outperformed all other techniques across all datasets, showcasing its superiority in complex environments and the robustness of our TF-domain approach. Moreover, despite utilizing 12 layers, RTFS-Net-12 still manages to reduce the computational costs of CTCNet by threefold while using only a tenth of the parameters. To our knowledge, RTFS-Net is the first AVSS method to employ fewer than 1 million parameters, and the first TF-domain model to outperform all T-domain counterparts.\nImportant Note: we investigated higher R values in Appendix J and found that RTFS-Net-20 can outperform CTCNet by a substantial 1.1 dB SI-SNRi and 1 dB SDRi. In Appendix K, we randomly selected two long, uninterrupted audio samples from the VoxCeleb2 test set and mixed them together to generate sample outputs for RTFS-Net, AV-ConvTasNet, VisualVoice, AVLIT and the previous SOTA method, CTCNet. In Appendix L we studied additional downstream tasks to test the generalizability of RTFS-Net."
        },
        {
            "heading": "5.2 ABLATION STUDY",
            "text": "Cross-dimensional Attention Fusion (CAF). To demonstrate the value of our CAF Block, we used a reduced version of RTFS-Net-4 that limited the power of the separation network and emphasised the fusion network, see Appendix H.1. As a baseline, we substituted the CAF block with a TF-domain adaptation of CTCNet\u2019s fusion strategy. The previous SOTA method, CTCNet, uses a concatenation approach: it uses interpolation to upscale the video dimensions (Cv \u00d7 Tv) to the\ndimensions of the audio (Ca \u00d7 Ta), concatenates along the channels and then uses a convolution to restore the audio channel dimension, Ca. Similar to Equation 8, we adapted this to a 3D TF-domain audio setting. In Table 2 we observed that our CAF Block outperformed the baseline by a large margin while reducing the number of parameters by 96.4% and cutting the number of MACs by 98.7%, suggesting multimodal fusion can be effectively performed with a well-designed, small network.\nTemporal-Frequency Attention Reconstruction (TF-AR). To test the efficacy of our RTFS Block\u2019s reconstruction method, i.e. the TF-AR units, we compared RTFS-Net-4 with a baseline that used interpolation and addition to perform the work of the TF-AR units, similar to the upsampling process in U-Net (Ronneberger et al., 2015). We observe in Table 3 that our TF-AR units boosted performance by a substantial 1dB in both performance metrics with an almost negligible affect on computational complexity due to the depth-wise convolutional nature of our TF-AR units.\nSpectral Source Separation (S3). In these experiments we further reduced the model configuration seen in Appendix H.1 by setting Ca = 128. We tested our S3 Block against four alternative methods. Regression gives aR directly to the decoder to decode into the target speaker\u2019s audio. Mask is the approach used by many SOTA AOSS methods such as Luo et al. (2020) and discussed in Section 3.3. Mask + Gate and Mask + DW-Gate both apply a convolutional gate, as seen in Luo et al. (2019), after the mask. The mask is fed into two convolutions with respective Tanh and Sigmoid activations. The outputs are multiplied together to form the final mask. DW here indicates the usage of depth-wise convolutions.\nTable 4 shows the Regression approach was the least effective, with significantly lower metrics than the other methods. However, it also utilized the fewest MACs and parameters. For a very small increase in parameters, the Mask and Mask+DW-Gate approaches yielded far better results. A non depth-wise gate increased the performance further, but the number of MACs and parameters is significantly higher. However, all methods were eclipsed by the performance of our S3 Block. This mathematics-based approach does not significantly increase the parameter or MAC count, thereby providing a performance enhancement to all TF-domain AVSS methods at no additional cost \u2013 likely benefiting AOSS methods as well. A visualization is available in Appendix E."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this study we introduced RTFS-Net, a novel approach to AVSS that explicitly models time and frequency dimensions at a compressed subspace to improve performance and increase efficiency. Empirical evaluations across multiple datasets demonstrate the superiority of our approach. Notably, our method achieves remarkable performance improvements while maintaining a significantly reduced computational complexity and parameter count. This indicates that enhancing AVSS performance does not necessarily require larger models, but rather innovative and efficient architectures that better capture the intricate interplay between audio and visual modalities.\nREPRODUCIBILITY STATEMENT\nThe code for RTFS-Net was written in Python 3.10 using standard Python deep learning libraries, specifically PyTorch and PyTorch Lightning. In order to accommodate full reproducibility, we will open-source the code for RTFS-Net under the MIT licence on GitHub once this paper has been accepted into the conference. The code shall include all files used to reproduce the experiments seen in Table 1, including the conda environment we used to run the code, the full config files for RTFSNet-4, RTFS-Net-6 and RTFS-Net-12, the weights for the pretrained video model and the RTFS-Net model code itself, including TDANet and all layers, blocks and networks mentioned in this paper. Datasets must be obtained separately from the references provided, see Appendix F, as they are the property of the respective publishers, but we provide the data-preprocessing scripts alongside this code-base. The GPU optimized PyTorch implementation of the SRU is already open-source and available on PyPi2, supplied by the original author. Experimentation and training was accomplished using a single server with 8 NVIDIA 3080 GPUs, see Appendix I for additional details. For those who wish to recreate the code themselves, all hyperparameters are listed in Appendix H. Evaluation metrics and loss functions are described and mathematically defined in Appendix I.1 and G respectively, but we will additionally provide these in the code-base."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work was supported in part by the National Key Research and Development Program of China (No. 2021ZD0200301) and the National Natural Science Foundation of China (Nos. 62061136001, 61836014)."
        },
        {
            "heading": "B SHARING PARAMETERS",
            "text": "In table 5 we consider the effects of sharing parameters between the RTFS Blocks, including the AP block. Sharing parameters means we instantiate a single RTFS Block instance, and use this single block as the AP Block and as all of the subsequent RTFS Blocks after the fusion. In other words, we take the output of the first RTFS Block, add the residual connection, and then pass that data to the exact same block again as an input. This means that all R RTFS Blocks share the same weights and biases. Not sharing the parameters means that we instead instantiate R separate RTFS Blocks, and iterate through them in turn.\nEchoing the findings of Li et al. (2022), our study observed a small \u2013 but significant \u2013 increase in model performance, evidenced by the 0.1 dB increase in both SI-SNRi and SDRi. Additionally, sharing the parameters cuts the model size down from 2 million parameters down to just 739 thousand. The number of MACs is unchanged, as the number of calculations remains the same."
        },
        {
            "heading": "C RESULTS FOR VARIOUS COMPRESSION SCALES",
            "text": "In Table 6 we compared using different compression levels q using the same RTFS-Net-4 configuration in Table 1. Increasing the compression factor results in a large drop in performance. However,\n3https://github.com/JusperLee/TDANet\nthe reduction in computational complexity is correspondingly large. RTFS-Net with q = 2 and q = 3 both outperform the previous TF-domain SOTA method, VisualVoice (Gao & Grauman, 2021). As seen in Table 1, adding more layers (increasing R) will increase performance significantly, so using 12 or 16 layer configurations with a larger q value would also be a valid option. However, we chose a lower q and R for simplicity. The RTFS-Net with q = 1 shows the result of applying TF-GridNet (Wang et al., 2023) directly to the AVSS setting. This model is powerful, and so with only 4 layers it nearly obtains the performance of our RTFS-Net-12 model in Table 1. However, the memory consumption is extremely large, and the model is extremely slow to train. A batch size of 1 is required for most standard GPUs. In addition, the purpose of this research is not to outperform the previous SOTA method, but to provide a lightweight and efficient solution to AVSS problem."
        },
        {
            "heading": "D STRUCTURES FOR INDEPENDENTLY MODELING TIME AND FREQUENCY",
            "text": "In Table 7, we examine the effects of using different RNN structures as described in Equation 14. Each model employs the same RTFS-Net-4 configuration as presented in Table 1. We observe that both LSTM and SRU yield similar results when modeling the time and frequency dimensions. However, SRU demonstrates advantages over LSTM in terms of parameters, computational complexity, inference time, and training memory usage.\nAdditionally, we tested the effects of employing transformers. Specifically, we replaced the inter and intra SRUs with DPTNet (Chen et al., 2020) and adopted the transformer architecture from TDANet (Li et al., 2023) to avoid using heavy BiLSTM operations post-MHSA. The time-frequency domain transformer Wang et al. (2023) remained unchanged following the dual-path processing. In the table, this model is denoted as MHSA \u2013 it possesses fewer parameters and half the number of MACs, but shows an increase in training memory due to the transformer operations. Additionally, both the SISNRi and the SDRi metrics exhibited a 1dB decrease in performance, a significant drop, leading us to opt against this architecture despite its compact size.\nWe hypothesized that the unfolding before both SRUs might enhance transformer performance as well. However, this approach resulted in an extremely large model in all metrics: the training memory surged to 5GB, and the parameter count nearly reached 3 million. This configuration applies only the MHSA component of the transformer. Incorporating the FFN (MHSA-Unfold+FFN) led to an even larger and slower model. Despite these modifications, the performance remained substantially lower than that of the SRU configuration.\nWe can conclude that RTFS-Net\u2019s improvement in separation quality comes from the model design, not the choice of RNN. The SRU is implemented specifically to improve inference time and reduce the computational complexity."
        },
        {
            "heading": "E LOSS OF AMPLITUDE AND PHASE INFORMATION IN THE TRADITIONAL MASK APPROACH",
            "text": "In Figure 4 we examined the separation quality of two different target speaker extraction methods: our proposed S3 Block and the traditional masking technique discussed in Section 3.3. For these results, we reused the S3 and Mask models trained on the LRS2-2Mix dataset and shown in Table\n4. We then selected three audio mixtures from the VoxCeleb2-2Mix dataset, creating an extremely challenging setting for the models, and applied the two models on each mixture.\nIn Figure 4a we can see that the traditional masking approach completely failed to capture the gold, vertical column in the middle of the expanded section. On the other hand, our S3 not only captured this information, but also managed to preserve the striations seen in the original spectrogram.\nIn Figure 4b we see a similar result, our S3 approach managed to accurately capture the gold stripes seen just right of the centre of the expanded section, whereas the the traditional masking approach completely excluded this part of the audio signal, resulting in a purple/black region in its place.\nIn Figure 4c we can see that our S3 approach managed to accurately maintain the complex horizontal striations of the original audio, whereas the traditional masking approach produced a much more blurred result, with large purple regions showing it completely disregarded large chunks of important acoustic information.\nThe spectrograms in Figure 4, alongside the clear performance advantage seen in the evaluation metrics of Table 4, show the importance of respecting the underlying complex nature of the audio features obtained from the STFT. Failing to do so results in critical acoustic information being lost during the element-wise multiplication of the traditional masking approach, and hence the result obtained from the iSTFT is visibly degraded."
        },
        {
            "heading": "F DATASET DETAILS",
            "text": "We conducted our AVSS experiments on the LRS2 (Afouras et al., 2018a), LRS3 (Afouras et al., 2018b), and VoxCeleb2 (Chung et al., 2018) datasets. The different speakers are separated into different folders, allowing us to explicitly test speaker-independent performance. Additionally, we used a dataset partitioning consistent with existing AVSS methods to ensure fairness in obtaining results. For lip motion video acquisition, we used the FFmpeg tool4 to change the sampling rate of all videos to 25 FPS. We then used an existing face extraction network (Zhang et al., 2016) to segment the lip region from the image and adjusted it to a 96 \u00d7 96 grayscale frame that contains only the image frames of the speaker\u2019s lip motion. For audio extraction, we similarly extracted the audio from the video using the FFmpeg tool and uniformly changed the sampling rate to 16 KHz. For comparison with existing AVSS methods, we used a mix of two speakers for training, but this dataset creation method can be used to create datasets for three or more speakers too.\nThe LRS2-2Mix (Afouras et al., 2018a) dataset is derived from BBC television broadcasts. The videos contain a variety of natural speakers and lighting conditions, and the audio contains background noise to bring it closer to real-world environments. Compared to previous lip-reading datasets (e.g. GRID (Cooke et al., 2006) and LRW (Son Chung et al., 2017)), LRS2-2Mix contains unrestricted vocabulary. It has full sentences rather than isolated words and shows greater diversity in terms of speakers and recording conditions. The dataset contains 11 hours of training, 3 hours of validation, and 1.5 hours of testing data.\n4https://ffmpeg.org/\nThe LRS3-2Mix (Afouras et al., 2018b) dataset contains over 400 hours of video extracted from 5,594 TED and TEDx English-language talks downloaded from YouTube. TED talks are professionally produced and speakers typically use microphones, so it represents a relatively clean environment. The dataset includes 28 hours of training, 3 hours of validation and 1.5 hours of testing data.\nThe VoxCeleb2-2Mix (Chung et al., 2018) dataset was collected from YouTube videos containing naturally occurring noises such as laughter, cross-talk and different room acoustics. This provides a diverse and challenging set of speech samples for training the separation system. In addition, the faces in the videos differ in terms of pose, lighting, and image quality. This suggests that VoxCeleb22Mix provides a more challenging real-world environment. The dataset consists of 56 hours of training, 3 hours of validation and 1.5 hours of testing data."
        },
        {
            "heading": "G EVALUATION METRICS FOR AUDIO-VISUAL SPEECH-SEPARATION QUALITY",
            "text": "Following recent literature (Li et al., 2022), the scale-invariant signal-to-noise ratio improvement (SI-SNRi) and signal-to-noise ratio improvement (SDRi) were used to evaluate the quality of the separated speeches. These metrics were calculated based on the scale-invariant signal-to-noise ratio (SI-SNR) (Le Roux et al., 2019) and source-to-distortion ratio (SDR) (Vincent et al., 2006). For these evaluation metrics, a higher value indicates better performance. SI-SNRi and SDRi are defined as:\nSI-SNRi(x, s, s\u0302) = SI-SNR(s, s\u0302)\u2212 SI-SNR(s, x), SDRi(x, s, s\u0302) = SDR(s, s\u0302)\u2212 SDR(s, x). (30)"
        },
        {
            "heading": "H MODEL HYPERPARAMETERS",
            "text": "For all model versions, R \u2208 {4, 6, 12}, we used the same hyperparamer settings. Encoder. The STFT used a Hanning analysis window with a window size of 256 and a hop length of 128. The encoded feature dimension was Ca = 256.\nVP Block. We used a TDANet block (Li et al., 2023) with a hidden dimension of 64, a kernel size of 3, an up-sampling depth of 4, replaced the gLNs with batch normalization and set the number of attention heads in the MHSA to 8. The feed forward network (FFN) had 128 channels. Model code was supplied by the original auther.\nAP and RTFS Blocks. We used a hidden dimension of D = 64 as the reduced channel dimension for the RTFS Blocks. We chose q = 2 up-sampling layers in order to halve the length of the time and frequency dimensions, see Appendix C for different choices of q. The 4-layer bidirectional SRUs had a hidden dimension of ha = 32, and an input dimension of 64 \u00d7 8 = 256 after unfolding. We used the 4-head attention configuration for the TF-domain attention (Wang et al., 2023), as defined in their paper. The model code for the TF-domain attention was taken from the ESPnet GitHub project5, where it was published after the release of Wang et al. (2023)\u2019s paper.\nCAF Block. We used h = 4 attention heads.\nH.1 REDUCED MODEL\nThe reduced model used in Table 2 was achieved by using a modified version of the RTFS-Net-4 configuration defined above. Firstly, we set the hidden dimension of the RTFS Blocks to D = 32, replaced the multi-layer bidirectional SRUs with single layer unidirectional SRUs, and changed the kernel size and stride of the unfolding stage in Equation 13 to 4 and 2 respectively. This configuration is also used in Table 4, but with Ca = 128.\n5https://github.com/espnet/espnet.git"
        },
        {
            "heading": "I TRAINING HYPERPARAMETERS",
            "text": "For training we used a batch size of 4 and AdamW (Loshchilov & Hutter, 2018) optimization with a weight decay of 1\u00d7 10\u22121. The initial learning rate used was 1\u00d7 10\u22123, but this value was halved whenever the validation loss did not decrease for 5 epochs in a row. We used gradient clipping to limit the maximum L2 norm of the gradient to 5. Training was left running for a maximum of 200 epochs, but early stopping was also applied. The loss function used for training was the SISNR (Le Roux et al., 2019) between the estimated s\u0302 and original s target speaker audio signals, see Appendix I.1.\nI.1 TRAINING OBJECTION FUNCTION\nThe training objective used is the scale-invariant source-to-noise ratio (SI-SNR) between the target speaker\u2019s original audio signal s and the estimate returned by the model s\u0302. The SI-SNR is defined as follows,\nSI-SNR(s, s\u0302) = 10 log10\n( ||\u03c9 \u00b7 s||2\n||s\u0302 \u2212\u03c9 \u00b7 s||2\n) , \u03c9 := s\u0302Ts\nsTs . (31)\nJ INCREASING THE NUMBER OF LAYERS\nObtaining SOTA performance was not the focus of this paper. Rather, we aimed to produce a model with high efficiency while utilizing a small number of parameters. However, as an ablation study we explore higher values of R in Table 8, including R = 16 and R = 20.\nFrom Table 8 we observed that increasing R results in a significant increase in performance, with an associated increase in computational complexity. Note that since the RTFS Blocks share parameters, increasing R does not increase the model size. RTFS-Net-20 outperforms CTCNet by over 1dB in both SDRi and SI-SNRi, while still utilizing only 91G MACs \u2013 just over half the MACs utilized by CTCNet. However, 20 RTFS Blocks requires a much larger GPU memory for training. As a result, NVIDIA 4090s were necessary in order to obtain these results.\nK VISUALIZATION SAMPLES ON THE WEB PAGE\nIn this section, we aim to detail how we constructed the AVSS samples seen on our Web page6. To validate the generalizability of our approach, we purposefully chose videos from different scenarios and different genders with the aim of demonstrating the robustness of our model across a wide range of environments. We selected six males and two females to form four different mixed audio scenarios, including a variety of environments such as broadcasting studios, outdoors and variety shows.\nWe selected 8 videos from the VoxCeleb2 (Chung et al., 2018) test set, meaning none of the models saw these videos in their training or validation data. In order to extract the facial features of the target speaker, we used an existing face detection model (Zhang et al., 2016) as described in Appendix F. We then spliced the audios together in order to create the audio mixtures for the models, and placed the two original videos side by side for the demo.\n6https://anonymous.4open.science/w/RTFS-Net/AV-Model-Demo.html\nFor each model, we passed in the mixed audio signal and the target speaker\u2019s facial features to obtain the target speaker\u2019s estimated separated audio stream for each of the 8 speakers. On our Web page, under each video is a link for each of the models tested. Upon opening the video, it will play the mixed audio in time to the video. Hovering the mouse over the desired speaker will switch the audio to the model\u2019s estimated separated audio stream for that speaker."
        },
        {
            "heading": "L GENERALIZING TO OTHER TASKS",
            "text": "In this section we explore how RTFS-Net can generalize to other tasks, namely: the presence of more speakers, and word error rate. These downstream tasks further test the robustness of our method.\nL.1 MORE SPEAKERS\nOur model incorporates visual information from a single \u201ctarget\u201d speaker to isolate and extract the audio of this target speaker, hence we are not limited by the number of speakers present in the mixture. This integration of visual cues plays a crucial role in enhancing the accuracy of both our model and other AVSS models. We randomly sampled speakers from the LRS2-2Mix test set to obtain a dataset with three speakers. Adding more speakers increases the difficulty of the task, as from the perspective of extracting the target the speaker, the amount of background/extraneous noise has increased.\nIn Table 9 we observed that RTFS-Net-12 scaled much better to this harder task and outperformed CTCNet by a significant margin. While the presence of multiple speakers somewhat impacts the models\u2019 performance, it is important to note that these models were not trained on the dataset with three speakers, only tested on it.\nL.2 WORD ERROR RATE\nIn Table 10 we tested the speech recognition accuracy on the LRS2-2Mix test dataset. We utilized the publicly available Google Speech-to-Text API to obtain the recognition results. Our focus was on measuring the Word Error Rate (WER), where a lower rate is indicative of superior performance. The experimental results demonstrate that our model can achieve competitive speech recognition accuracy, validating the performance of our model in downstream tasks."
        }
    ],
    "title": "RTFS-NET: RECURRENT TIME-FREQUENCY MOD-",
    "year": 2024
}