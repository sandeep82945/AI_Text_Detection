{
    "abstractText": "Learning expressive stochastic policies instead of deterministic ones has been proposed to achieve better stability, sample complexity, and robustness. Notably, in Maximum Entropy Reinforcement Learning (MaxEnt RL), the policy is modeled as an expressive Energy-Based Model (EBM) over the Q-values. However, this formulation requires the estimation of the entropy of such EBMs, which is an open problem. To address this, previous MaxEnt RL methods either implicitly estimate the entropy, resulting in high computational complexity and variance (SQL), or follow a variational inference procedure that fits simplified actor distributions (e.g., Gaussian) for tractability (SAC). We propose Stein Soft Actor-Critic (S2AC), a MaxEnt RL algorithm that learns expressive policies without compromising efficiency. Specifically, S2AC uses parameterized Stein Variational Gradient Descent (SVGD) as the underlying policy. We derive a closed-form expression of the entropy of such policies. Our formula is computationally efficient and only depends on first-order derivatives and vector products. Empirical results show that S2AC yields more optimal solutions to the MaxEnt objective than SQL and SAC in the multi-goal environment, and outperforms SAC and SQL on the MuJoCo benchmark. Our code is available at: https://github.com/SafaMessaoud/ S2AC-Energy-Based-RL-with-Stein-Soft-Actor-Critic",
    "authors": [
        {
            "affiliations": [],
            "name": "Safa Messaoud"
        },
        {
            "affiliations": [],
            "name": "Billel Mokeddem"
        },
        {
            "affiliations": [],
            "name": "Zhenghai Xue"
        },
        {
            "affiliations": [],
            "name": "Linsey Pang"
        },
        {
            "affiliations": [],
            "name": "Bo An"
        },
        {
            "affiliations": [],
            "name": "Haipeng Chen"
        },
        {
            "affiliations": [],
            "name": "Sanjay Chawla"
        }
    ],
    "id": "SP:2ee694a2c514c63a4609612093089de739308104",
    "references": [
        {
            "authors": [
                "Abbas Abdolmaleki",
                "Jost Tobias Springenberg",
                "Yuval Tassa",
                "Remi Munos",
                "Nicolas Heess",
                "Martin Riedmiller"
            ],
            "title": "Maximum a posteriori policy optimisation",
            "venue": "arXiv preprint arXiv:1806.06920,",
            "year": 2018
        },
        {
            "authors": [
                "Rishabh Agarwal",
                "Max Schwarzer",
                "Pablo Samuel Castro",
                "Aaron Courville",
                "Marc G Bellemare"
            ],
            "title": "Deep reinforcement learning at the edge of the statistical precipice",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "I. Ahmad",
                "Pi-Erh Lin"
            ],
            "title": "A nonparametric estimation of the entropy for absolutely continuous distributions (corresp.)",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 1976
        },
        {
            "authors": [
                "Jan Beirlant",
                "M.C.A van Zuijlen"
            ],
            "title": "The empirical distribution function and strong laws for functions of order statistics of uniform spacings",
            "venue": "J. Multivar. Anal.,",
            "year": 1985
        },
        {
            "authors": [
                "Jan Beirlant",
                "Edward J Dudewicz",
                "L\u00e1szl\u00f3 Gy\u00f6rfi",
                "Edward C Van der Meulen"
            ],
            "title": "Nonparametric entropy estimation: An overview",
            "venue": "IJSRMSS,",
            "year": 1997
        },
        {
            "authors": [
                "Laura T Bernhofen",
                "Edward J Dudewicz",
                "Janos Levendovszky",
                "Edward C van der Meulen"
            ],
            "title": "Ranking of the best random number generators via entropy-uniformity theory",
            "venue": "AJMMS,",
            "year": 1996
        },
        {
            "authors": [
                "Peter J. Bickel",
                "Leo Breiman"
            ],
            "title": "Sums of Functions of Nearest Neighbor Distances, Moment Bounds",
            "venue": "Limit Theorems and a Goodness of Fit Test. Ann. Probab.,",
            "year": 1983
        },
        {
            "authors": [
                "Giuseppe Carleo",
                "Matthias Troyer"
            ],
            "title": "Solving the quantum many-body problem with artificial neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Nicolas Castanet",
                "Olivier Sigaud"
            ],
            "title": "Stein variational goal generation for adaptive exploration in multi-goal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Edoardo Cetin",
                "Oya Celiktutan"
            ],
            "title": "Policy gradient with serial markov chain reasoning",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Jiayu Chen",
                "Yuanxin Zhang",
                "Yuanfan Xu",
                "Huimin Ma",
                "Huazhong Yang",
                "Jiaming Song",
                "Yu Wang",
                "Yi Wu"
            ],
            "title": "Variational automatic curriculum learning for sparse-reward cooperative multi-agent problems",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas M Cover"
            ],
            "title": "Elements of information theory",
            "year": 1999
        },
        {
            "authors": [
                "Noel Cressie"
            ],
            "title": "Power results for tests based on high-order gaps",
            "year": 1978
        },
        {
            "authors": [
                "Bo Dai",
                "Hanjun Dai",
                "Arthur Gretton",
                "Le Song",
                "Dale Schuurmans",
                "Niao He"
            ],
            "title": "Kernel exponential family estimation via doubly dual embedding",
            "venue": "AISTAT,",
            "year": 2019
        },
        {
            "authors": [
                "Bo Dai",
                "Zhen Liu",
                "Hanjun Dai",
                "Niao He",
                "Arthur Gretton",
                "Le Song",
                "Dale Schuurmans"
            ],
            "title": "Exponential family estimation via adversarial dynamics embedding. NeurIPS, 2019b",
            "year": 2019
        },
        {
            "authors": [
                "Zihang Dai",
                "Amjad Almahairi",
                "Philip Bachman",
                "Eduard Hovy",
                "Aaron Courville"
            ],
            "title": "Calibrating energy-based generative adversarial networks",
            "venue": "arXiv preprint arXiv:1702.01691,",
            "year": 2017
        },
        {
            "authors": [
                "Jay L Devore",
                "Kenneth N Berk",
                "Matthew A Carlton"
            ],
            "title": "Modern mathematical statistics with applications",
            "year": 2012
        },
        {
            "authors": [
                "Yan Duan",
                "Xi Chen",
                "Rein Houthooft",
                "John Schulman",
                "Pieter Abbeel"
            ],
            "title": "Benchmarking deep reinforcement learning for continuous control",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Edward J Dudewicz",
                "Edward C Van Der Meulen"
            ],
            "title": "Entropy-based tests of uniformity",
            "venue": "JASA,",
            "year": 1981
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Sergey Levine"
            ],
            "title": "Maximum entropy rl (provably) solves some robust rl problems",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Yihao Feng",
                "Dilin Wang",
                "Qiang Liu"
            ],
            "title": "Learning to draw samples with amortized stein variational gradient descent",
            "venue": "arXiv preprint arXiv:1707.06626,",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Sergey Levine",
                "Pieter Abbeel"
            ],
            "title": "Guided cost learning: Deep inverse optimal control via policy optimization",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Ruiqi Gao",
                "Yang Song",
                "Ben Poole",
                "Ying Nian Wu",
                "Diederik P Kingma"
            ],
            "title": "Learning energy-based models by diffusion recovery likelihood",
            "venue": "arXiv preprint arXiv:2012.08125,",
            "year": 2020
        },
        {
            "authors": [
                "Xun Gao",
                "Lu-Ming Duan"
            ],
            "title": "Efficient representation of quantum many-body states with deep neural networks",
            "venue": "Nature communications,",
            "year": 2017
        },
        {
            "authors": [
                "Wenbo Gong",
                "Yingzhen Li",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Meta-learning for stochastic gradient mcmc",
            "venue": "arXiv preprint arXiv:1806.04522,",
            "year": 2018
        },
        {
            "authors": [
                "Shixiang Gu",
                "Timothy Lillicrap",
                "Zoubin Ghahramani",
                "Richard E Turner",
                "Sergey Levine"
            ],
            "title": "Q-prop: Sample-efficient policy gradient with an off-policy critic",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "L\u00e1szl\u00f3 Gy\u00f6rfi",
                "Edward C. van der Meulen"
            ],
            "title": "Density-free convergence properties of various estimators of entropy",
            "venue": "CSDA,",
            "year": 1987
        },
        {
            "authors": [
                "Tuomas Haarnoja"
            ],
            "title": "Acquiring diverse robot skills via maximum entropy deep reinforcement learning (Ph.D. thesis)",
            "venue": "University of California,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Haoran Tang",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Reinforcement learning with deep energy-based policies",
            "venue": "In ICML,",
            "year": 2017
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Kristian Hartikainen",
                "George Tucker",
                "Sehoon Ha",
                "Jie Tan",
                "Vikash Kumar",
                "Henry Zhu",
                "Abhishek Gupta",
                "Pieter Abbeel"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "arXiv preprint arXiv:1812.05905,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Hall"
            ],
            "title": "On powerful distributional tests based on sample spacings",
            "venue": "JMVA,",
            "year": 1986
        },
        {
            "authors": [
                "Hideitsu Hino",
                "Noboru Murata"
            ],
            "title": "A conditional entropy minimization criterion for dimensionality reduction and multiple kernel learning",
            "venue": "Neural Comput.,",
            "year": 2010
        },
        {
            "authors": [
                "Matthew D Hoffman",
                "Andrew Gelman"
            ],
            "title": "The No-U-Turn sampler: Adaptively setting path lengths in hamiltonian monte carlo",
            "year": 2014
        },
        {
            "authors": [
                "Aleksandr Vasil\u2019evich Ivanov",
                "MN Rozhkova"
            ],
            "title": "On properties of the statistical estimate of the entropy of a random vector with a probability density",
            "venue": "Problemy Peredachi Informatsii,",
            "year": 1981
        },
        {
            "authors": [
                "Harry Joe"
            ],
            "title": "Estimation of entropy and other functionals of a multivariate density",
            "venue": "Ann. Inst. Stat. Math.,",
            "year": 1989
        },
        {
            "authors": [
                "Hilbert J Kappen"
            ],
            "title": "Path integrals and symmetry breaking for optimal control theory",
            "venue": "JSTAT,",
            "year": 2005
        },
        {
            "authors": [
                "Ivan Kobyzev",
                "Simon JD Prince",
                "Marcus A Brubaker"
            ],
            "title": "Normalizing flows: An introduction and review of current methods",
            "year": 2020
        },
        {
            "authors": [
                "Lyudmyla F Kozachenko",
                "Nikolai N Leonenko"
            ],
            "title": "Sample estimate of the entropy of a random vector",
            "venue": "Problemy Peredachi Informatsii,",
            "year": 1987
        },
        {
            "authors": [
                "Rithesh Kumar",
                "Anirudh Goyal",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Engan: Latent space mcmc and maximum entropy generators for energy-based models",
            "year": 2019
        },
        {
            "authors": [
                "Alessandro Lazaric",
                "Marcello Restelli",
                "Andrea Bonarini"
            ],
            "title": "Reinforcement learning in continuous action spaces through sequential monte carlo methods",
            "venue": "NeurIPS,",
            "year": 2007
        },
        {
            "authors": [
                "Erik G Learned-Miller",
                "John W Fisher III"
            ],
            "title": "Ica using spacings estimates of entropy",
            "year": 2003
        },
        {
            "authors": [
                "Yann LeCun",
                "Sumit Chopra",
                "Raia Hadsell",
                "M Ranzato",
                "Fujie Huang"
            ],
            "title": "A tutorial on energy-based learning",
            "venue": "Predicting Structured Data,",
            "year": 2006
        },
        {
            "authors": [
                "Kyowoon Lee",
                "Sol-A Kim",
                "Jaesik Choi",
                "Seong-Whan Lee"
            ],
            "title": "Deep reinforcement learning in continuous action spaces: a case study in the game of simulated curling",
            "year": 2018
        },
        {
            "authors": [
                "Sergey Levine",
                "Vladlen Koltun"
            ],
            "title": "Guided policy search",
            "venue": "In ICML,",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Levy",
                "Matthew D Hoffman",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Generalizing hamiltonian monte carlo with neural networks",
            "venue": "arXiv preprint arXiv:1711.09268,",
            "year": 2017
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Jonathan J Hunt",
                "Alexander Pritzel",
                "Nicolas Heess",
                "Tom Erez",
                "Yuval Tassa",
                "David Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971,",
            "year": 2015
        },
        {
            "authors": [
                "Haozhe Liu",
                "Bing Li",
                "Haoqian Wu",
                "Hanbang Liang",
                "Yawen Huang",
                "Yuexiang Li",
                "Bernard Ghanem",
                "Yefeng Zheng"
            ],
            "title": "Combating mode collapse in gans via manifold entropy estimation",
            "year": 2022
        },
        {
            "authors": [
                "Qiang Liu"
            ],
            "title": "Stein variational gradient descent as gradient flow",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Qiang Liu",
                "Dilin Wang"
            ],
            "title": "Stein variational gradient descent: a general purpose bayesian inference algorithm",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Yang Liu",
                "Prajit Ramachandran",
                "Qiang Liu",
                "Jian Peng"
            ],
            "title": "Stein variational policy gradient",
            "year": 2017
        },
        {
            "authors": [
                "Jan R Magnus",
                "Heinz Neudecker"
            ],
            "title": "Matrix differential calculus with applications in statistics and econometrics",
            "year": 2019
        },
        {
            "authors": [
                "Shie Mannor",
                "Dori Peleg",
                "Reuven Rubinstein"
            ],
            "title": "The cross entropy method for classification",
            "venue": "In ICML,",
            "year": 2005
        },
        {
            "authors": [
                "Joseph Marino",
                "Alexandre Pich\u00e9",
                "Alessandro Davide Ialongo",
                "Yisong Yue"
            ],
            "title": "Iterative amortized policy optimization",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Bogdan Mazoure",
                "Thang Doan",
                "Audrey Durand",
                "Joelle Pineau",
                "R Devon Hjelm"
            ],
            "title": "Leveraging exploration in off-policy algorithms via normalizing flows",
            "venue": "CoRL,",
            "year": 2020
        },
        {
            "authors": [
                "Roger G Melko",
                "Giuseppe Carleo",
                "Juan Carrasquilla",
                "J Ignacio Cirac"
            ],
            "title": "Restricted boltzmann machines in quantum physics",
            "venue": "Nature Physics,",
            "year": 2019
        },
        {
            "authors": [
                "Safa Messaoud"
            ],
            "title": "Toward More Scalable Structured Models",
            "venue": "PhD thesis, University of Illinois Urbana-Champaign,",
            "year": 2021
        },
        {
            "authors": [
                "Safa Messaoud",
                "David Forsyth",
                "Alexander G Schwing"
            ],
            "title": "Structural consistency and controllability for diverse colorization",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Safa Messaoud",
                "Maghav Kumar",
                "Alexander G Schwing"
            ],
            "title": "Can we learn heuristics for graphical model inference using reinforcement learning",
            "venue": "In CVPR Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Mcmc using hamiltonian dynamics",
            "venue": "Handbook of Markov Chain Monte Carlo,",
            "year": 2011
        },
        {
            "authors": [
                "Brendan O\u2019Donoghue",
                "Remi Munos",
                "Koray Kavukcuoglu",
                "Volodymyr Mnih"
            ],
            "title": "Combining policy gradient and q-learning",
            "venue": "arXiv preprint arXiv:1611.01626,",
            "year": 2016
        },
        {
            "authors": [
                "Bo Pang",
                "Tianyang Zhao",
                "Xu Xie",
                "Ying Nian Wu"
            ],
            "title": "Trajectory prediction with latent belief energy-based model",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Liam Paninski"
            ],
            "title": "Estimation of entropy and mutual information",
            "venue": "Neural Comput.,",
            "year": 2003
        },
        {
            "authors": [
                "Fernando P\u00e9rez-Cruz"
            ],
            "title": "Estimation of information theoretic measures for continuous random variables",
            "venue": "In NeurIPS,",
            "year": 2008
        },
        {
            "authors": [
                "Konrad Rawlik",
                "Marc Toussaint",
                "Sethu Vijayakumar"
            ],
            "title": "On stochastic optimal control and reinforcement learning by approximate inference",
            "venue": "Proceedings of Robotics: Science and Systems VIII,",
            "year": 2012
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In ICML. PMLR,",
            "year": 2015
        },
        {
            "authors": [
                "\u00c9dgar Rold\u00e1n",
                "J\u00e9r\u00e9mie Barral",
                "Pascal Martin",
                "Juan MR Parrondo",
                "Frank J\u00fclicher"
            ],
            "title": "Quantifying entropy production in active fluctuations of the hair-cell bundle from time irreversibility and uncertainty relations",
            "venue": "New J. Phys.,",
            "year": 2021
        },
        {
            "authors": [
                "Reuven Y. Rubinstein",
                "Dirk P. Kroese"
            ],
            "title": "The Cross Entropy Method: A Unified Approach To Combinatorial Optimization, Monte-Carlo Simulation (Information Science and Statistics)",
            "venue": "SpringerVerlag,",
            "year": 2004
        },
        {
            "authors": [
                "Ruslan Salakhutdinov",
                "Andriy Mnih",
                "Geoffrey Hinton"
            ],
            "title": "Restricted boltzmann machines for collaborative filtering",
            "venue": "In ICML,",
            "year": 2007
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Claude Elwood Shannon"
            ],
            "title": "A mathematical theory of communication",
            "venue": "ACM SIGMOBILE,",
            "year": 2001
        },
        {
            "authors": [
                "David Silver",
                "Guy Lever",
                "Nicolas Heess",
                "Thomas Degris",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "In ICML,",
            "year": 2014
        },
        {
            "authors": [
                "Richard S Sutton",
                "David McAllester",
                "Satinder Singh",
                "Yishay Mansour"
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "NeurIPS,",
            "year": 1999
        },
        {
            "authors": [
                "Yunhao Tang",
                "Shipra Agrawal"
            ],
            "title": "Boosting trust region policy optimization by normalizing flows policy",
            "venue": "arXiv preprint arXiv:1809.10326,",
            "year": 2018
        },
        {
            "authors": [
                "F.P. Tarasenko"
            ],
            "title": "On the evaluation of an unknown probability density function, the direct estimation of the entropy from independent observations of a continuous random variable, and the distributionfree entropy test of goodness-of-fit",
            "venue": "Proceedings of the IEEE,",
            "year": 1968
        },
        {
            "authors": [
                "Evangelos Theodorou",
                "Jonas Buchli",
                "Stefan Schaal"
            ],
            "title": "A generalized path integral control approach to reinforcement learning",
            "year": 2010
        },
        {
            "authors": [
                "Emanuel Todorov"
            ],
            "title": "Linearly-solvable markov decision problems",
            "venue": "NeurIPS,",
            "year": 2006
        },
        {
            "authors": [
                "Giacomo Torlai",
                "Guglielmo Mazzola",
                "Juan Carrasquilla",
                "Matthias Troyer",
                "Roger Melko",
                "Giuseppe Carleo"
            ],
            "title": "Neural-network quantum state tomography",
            "venue": "Nature Physics,",
            "year": 2018
        },
        {
            "authors": [
                "Marc Toussaint"
            ],
            "title": "Robot trajectory optimization using approximate inference",
            "venue": "In ICML,",
            "year": 2009
        },
        {
            "authors": [
                "Don Van Ravenzwaaij",
                "Pete Cassey",
                "Scott D Brown"
            ],
            "title": "A simple introduction to markov chain monte\u2013carlo sampling",
            "venue": "Psychon. Bull. Rev.,",
            "year": 2018
        },
        {
            "authors": [
                "Oldrich Vasicek"
            ],
            "title": "A Test for Normality",
            "venue": "Based on Sample Entropy. JSTOR,",
            "year": 2015
        },
        {
            "authors": [
                "Nino Vieillard",
                "Olivier Pietquin",
                "Matthieu Geist"
            ],
            "title": "Munchausen reinforcement learning",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Dilin Wang",
                "Qiang Liu"
            ],
            "title": "Learning to draw samples: With application to amortized mle for generative adversarial learning",
            "venue": "arXiv preprint arXiv:1611.01722,",
            "year": 2016
        },
        {
            "authors": [
                "Max Welling",
                "Yee W Teh"
            ],
            "title": "Bayesian learning via stochastic gradient langevin dynamics",
            "venue": "In ICML,",
            "year": 2011
        },
        {
            "authors": [
                "Ying Nian Wu",
                "Jianwen Xie",
                "Yang Lu",
                "Song-Chun Zhu"
            ],
            "title": "Sparse and deep generalizations of the frame model",
            "venue": "Annals of Mathematical Sciences and Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Markus Wulfmeier",
                "Peter Ondruska",
                "Ingmar Posner"
            ],
            "title": "Maximum entropy deep inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1507.04888,",
            "year": 2015
        },
        {
            "authors": [
                "Jianwen Xie",
                "Yang Lu",
                "Song-Chun Zhu",
                "Yingnian Wu"
            ],
            "title": "A theory of generative convnet",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Jianwen Xie",
                "Zilong Zheng",
                "Ruiqi Gao",
                "Wenguan Wang",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Generative voxelnet: learning energy-based models for 3d shape synthesis and analysis",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Jianwen Xie",
                "Zilong Zheng",
                "Xiaolin Fang",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Learning cycleconsistent cooperative networks via alternating mcmc teaching for unsupervised cross-domain translation",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Jianwen Xie",
                "Zilong Zheng",
                "Ping Li"
            ],
            "title": "Learning energy-based model with variational auto-encoder as amortized sampler",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Jianwen Xie",
                "Yaxuan Zhu",
                "Jun Li",
                "Ping Li"
            ],
            "title": "A tale of two flows: Cooperative learning of langevin flow and normalizing flow toward energy-based model",
            "venue": "arXiv preprint arXiv:2205.06924,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Zhao",
                "Jianwen Xie",
                "Ping Li"
            ],
            "title": "Learning energy-based generative models via coarse-to-fine expanding and sampling",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Zilong Zheng",
                "Jianwen Xie",
                "Ping Li"
            ],
            "title": "Patchwise generative convnet: Training energy-based models from a single natural image for internal learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Mo Zhou",
                "Jianfeng Lu"
            ],
            "title": "Single timescale actor-critic method to solve the linear quadratic regulator with convergence guarantees",
            "year": 2023
        },
        {
            "authors": [
                "Brian D Ziebart"
            ],
            "title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy",
            "venue": "Carnegie Mellon University,",
            "year": 2010
        },
        {
            "authors": [
                "Brian D Ziebart",
                "Andrew L Maas",
                "J Andrew Bagnell",
                "Anind K Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In AAAI,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nS!AC (ours)\nSQL (Haarnoja et al., ICML 17)\nSAC (Haarnoja et al., ICML 18)\nExplicit entropy evaluation\nNum. SVGD steps = 0\nFigure 1: Comparing S2AC to SQL and SAC. S2AC with a parameterized policy is reduced to SAC if the number of SVGD steps is 0. SQL becomes equivalent to S2AC if the entropy is evaluated explicitly with our derived formula.\nMaxEnt RL (Todorov, 2006; Ziebart, 2010; Haarnoja et al., 2017; Kappen, 2005; Toussaint, 2009; Theodorou et al., 2010; Abdolmaleki et al., 2018; Haarnoja et al., 2018a; Vieillard et al., 2020) has been proposed to address challenges hampering the deployment of RL to real-world applications, including stability, sample efficiency (Gu et al., 2017), and robustness (Eysenbach & Levine, 2022). Instead of learning a deterministic policy, as in classical RL (Sutton et al., 1999; Schulman et al., 2017; Silver et al., 2014; Lillicrap et al., 2015), MaxEnt RL learns a stochastic policy that captures the intricacies of the action space. This enables better exploration during training and eventually better robustness to environmental perturbations at\ntest time, i.e., the agent learns multimodal action space distributions which enables picking the next best action in case a perturbation prevents the execution of the optimal one. To achieve this, MaxEnt RL models the policy using the expressive family of EBMs (LeCun et al., 2006). This translates into learning policies that maximize the sum of expected future reward and expected future entropy. However, estimating the entropy of such complex distributions remains an open problem. To address this, existing approaches either use tricks to go around the entropy computation or make limiting assumptions on the policy. This results in either poor scalability or convergence to suboptimal solutions. For example, SQL (Haarnoja et al., 2017) implicitly incorporates entropy in the Q-function computation. This requires using importance sampling, which results in high variability and hence poor training stability and limited scalability to high dimensional action spaces. SAC (Haarnoja\net al., 2018a), on the other hand, follows a variational inference procedure by fitting a Gaussian distribution to the EBM policy. This enables a closed-form evaluation of the entropy but results in a suboptimal solution. For instance, SAC fails in environments characterized by multimodal action distributions. Similar to SAC, IAPO (Marino et al., 2021) models the policy as a uni-modal Gaussian. Instead of optimizing a MaxEnt objective, it achieves multimodal policies by learning a collection of parameter estimates (mean, variance) through different initializations for different policies. To improve the expressiveness of SAC, SSPG (Cetin & Celiktutan, 2022) and SAC-NF (Mazoure et al., 2020) model the policy as a Markov chain with Gaussian transition probabilities and as a normalizing flow (Rezende & Mohamed, 2015), respectively. However, due to training stability issues, the reported results in Cetin & Celiktutan (2022) show that though both models learn multi-modal policies, they fail to maximize the expected future entropy in positive rewards setups. We propose a new algorithm, S2AC, that yields a more optimal solution to the MaxEnt RL objective. To achieve expressivity, S2AC models the policy as a Stein Variational Gradient Descent (SVGD) (Liu, 2017) sampler from an EBM over Q-values (target distribution). SVGD proceeds by first sampling a set of particles from an initial distribution, and then iteratively transforming these particles via a sequence of updates to fit the target distribution. To compute a closed-form estimate of the entropy of such policies, we use the change-of-variable formula for pdfs (Devore et al., 2012). We prove that this is only possible due to the invertibility of the SVGD update rule, which does not necessarily hold for other popular samplers (e.g., Langevin Dynamics (Welling & Teh, 2011)). While normalizing flow models (Rezende & Mohamed, 2015) are also invertible, SVGD-based policy is more expressive as it encodes the inductive bias about the unnormalized density and incorporates a dispersion term to encourage multi-modality, whereas normalizing flows encode a restrictive class of invertible transformations (with easy-to-estimate Jacobian determinants). Moreover, our formula is computationally efficient and only requires evaluating first-order derivatives and vector products. To improve scalability, we model the initial distribution of the SVGD sampler as an isotropic Gaussian and learn its parameters, i.e., mean and standard deviation, end-to-end. We show that this results in faster convergence to the target distribution, i.e., fewer SVGD steps. Intuitively, the initial distribution learns to contour the high-density region of the target distribution while the SVGD updates result in better and faster convergence to the modes within that region. Hence, our approach is as parameter efficient as SAC, since the SVGD updates do not introduce additional trainable parameters. Note that S2AC can be reduced to SAC when the number of SVGD steps is zero. Also, SQL becomes equivalent to S2AC if the entropy is computed explicitly using our formula (the policy in SQL is an amortized SVGD sampler). Beyond RL, the backbone of S2AC is a new variational inference algorithm with a more expressive and scalable distribution characterized by a closed-form entropy estimate. We believe that this variational distribution can have a wider range of exciting applications. We conduct extensive empirical evaluations of S2AC from three aspects. We start with a sanity check on the merit of our derived SVGD-based entropy estimate on target distributions with known entropy values (e.g., Gaussian) or log-likelihoods (e.g., Gaussian Mixture Models) and assess its\nsensitivity to different SVGD parameters (kernel, initial distribution, number of steps and number of particles). We observe that its performance depends on the choice of the kernel and is robust to variations of the remaining parameters. In particular, we find out that the kernel should be chosen to guarantee inter-dependencies between the particles, which turns out to be essential for invertibility. Next, we assess the performance of S2AC on a multi-goal environment (Haarnoja et al., 2017) where different goals are associated with the same positive (maximum) expected future reward but different (maximum) expected future entropy. We show that S2AC learns multimodal policies and effectively maximizes the entropy, leading to better robustness to obstacles placed at test time. Finally, we test S2AC on the MuJoCo benchmark (Duan et al., 2016). S2AC yields better performances than the baselines on four out of the five environments. Moreover, S2AC shows higher sample efficiency as it tends to converge with fewer training steps. These results were obtained from running SVGD for only three steps, which results in a small overhead compared to SAC during training. Furthermore, to maximize the run-time efficiency during testing, we train an amortized SVGD version of the policy to mimic the SVGD-based policy. Hence, this reduces inference to a forward pass through the policy network without compromising the performance."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 SAMPLERS FOR ENERGY-BASED MODELS",
            "text": "In this work, we study three representative methods for sampling from EBMs: (1) Stochastic Gradient Langevin Dynamics (SGLD) & Deterministic Langevin Dynamics (DLD) (Welling & Teh, 2011), (2) Hamiltonian Monte Carlo (HMC) (Neal et al., 2011), and (3) Stein Variational Gradient Descent (SVGD) (Liu & Wang, 2016). We review SVGD here since it is the sampler we eventually use in S2AC, and leave the rest to Appendix C.1. SVGD is a particle-based Bayesian inference algorithm. Compared to SGLD and HMC which have a single particle in their dynamics, SVGD operates on a set of particles. Specifically, SVGD samples a set of m particles {aj}mj=1 from an initial distribution q0 which it then transforms through a sequence of updates to fit the target distribution. Formally, at every iteration l, SVGD applies a form of functional gradient descent f that minimizes the KL-divergence between the target distribution p and the proposal distribution ql induced by the particles, i.e., the update rule for the ith particles is: al+1i = a l i + \u270f f(a l i) with\nf(ali) = Ealj\u21e0ql \u21e5 k(ali, a l j)ralj log p(a l j) +raljk(a l i, a l j) \u21e4 . (1)\nHere, \u270f is the step size and k(\u00b7, \u00b7) is the kernel function, e.g., the RBF kernel: k(ai, aj) = exp(||ai aj ||2/2 2). The first term within the gradient drives the particles toward the high probability regions of p, while the second term serves as a repulsive force to encourage dispersion."
        },
        {
            "heading": "2.2 MAXIMUM-ENTROPY RL",
            "text": "We consider an infinite horizon Markov Decision Process (MDP) defined by a tuple (S,A, p, r), where S is the state space, A is the action space and p : S \u21e5A\u21e5 S ! [0,1] is the state transition probability modeling the density of the next state st+1 2 S given the current state st 2 S and action at 2 A. Additionally, we assume that the environment emits a bounded reward function r 2 [rmin, rmax] at every iteration. We use \u21e2\u21e1(st) and \u21e2\u21e1(st, at) to denote the state and state-action marginals of the trajectory distribution induced by a policy \u21e1(at|st). We consider the setup of continuous action spaces Lazaric et al. (2007); Lee et al. (2018); Zhou & Lu (2023). MaxEnt RL (Todorov, 2006; Ziebart, 2010; Rawlik et al., 2012) learns a policy \u21e1\u21e4(at|st), that instead of maximizing the expected future reward, maximizes the sum of the expected future reward and entropy:\n\u21e1\u21e4 = argmax\u21e1 X\nt tE(st,at)\u21e0\u21e2\u21e1\n\u21e5 r(st, at) + \u21b5H(\u21e1(\u00b7|st)) \u21e4 , (2)\nwhere \u21b5 is a temperature parameter controlling the stochasticity of the policy and H(\u21e1(\u00b7|st)) is the entropy of the policy at state st. The conventional RL objective can be recovered for \u21b5 = 0. Note that the MaxEnt RL objective above is equivalent to approximating the policy, modeled as an EBM over Q-values, by a variational distribution \u21e1(at|st) (see proof of equivalence in Appendix D), i.e.,\n\u21e1\u21e4 = argmin\u21e1 X\nt Est\u21e0\u21e2\u21e1\n\u21e5 DKL \u21e1(\u00b7|st)k exp(Q(st, \u00b7)/\u21b5)/Z \u21e4 , (3)\nwhere DKL is the KL-divergence and Z is the normalizing constant. We now review two landmark MaxEnt RL algorithms: SAC (Haarnoja et al., 2018a) and SQL (Haarnoja et al., 2017).\nSAC is an actor-critic algorithm that alternates between policy evaluation, i.e., evaluating the Q-values for a policy \u21e1\u2713(at|st):\nQ (st, at) r(st, at) + Est+1,at+1\u21e0\u21e2\u21e1\u2713 \u21e5 Q (st+1, at+1) + \u21b5H(\u21e1\u2713(\u00b7|st+1)) \u21e4 (4)\nand policy improvement, i.e., using the updated Q-values to compute a better policy:\n\u21e1\u2713 = argmax\u2713 X\nt Est,at\u21e0\u21e2\u21e1\u2713\n\u21e5 Q (at, st) + \u21b5H(\u21e1\u2713(\u00b7|st)) \u21e4 . (5)\nSAC models \u21e1\u2713 as an isotropic Gaussian, i.e., \u21e1\u2713(\u00b7|s) = N (\u00b5\u2713, \u2713I). While this enables computing a closed-form expression of the entropy, it incurs an over-simplification of the true action distribution, and thus cannot represent complex distributions, e.g., multimodal distributions.\nSQL goes around the entropy computation, by defining a soft version of the value function V = \u21b5 log R A exp 1 \u21b5Q (st, a 0) da0 . This enables expressing the Q-value (Eq (4)) independently from the entropy, i.e., Q (st, at) = r(st, at) + Est+1\u21e0p[V (st+1)]. Hence, SQL follows a soft value iteration which alternates between the updates of the \u201csoft\u201d versions of Q and value functions:\nQ (st, at) r(st, at) + Est+1\u21e0p[V (st+1)], 8(st, at) (6) V (st) \u21b5 log R A exp 1 \u21b5Q (st, a 0) da0 , 8st. (7)\nOnce the Q and V functions converge, SQL uses amortized SVGD Wang & Liu (2016) to learn a stochastic sampling network f\u2713(\u21e0, st) that maps noise samples \u21e0 into the action samples from the EBM policy distribution \u21e1\u21e4(at|st) = exp 1 \u21b5 (Q \u21e4(st, at) V \u21e4(st)) . The parameters \u2713 are obtained by minimizing the loss J\u2713(st) = DKL \u21e1\u2713(\u00b7|st)|| exp 1 \u21b5 (Q \u21e4 (st, \u00b7) V \u21e4 (st))\nwith respect to \u2713. Here, \u21e1\u2713 denotes the policy induced by f\u2713. SVGD is designed to minimize such KL-divergence without explicitly computing \u21e1\u2713. In particular, SVGD provides the most greedy direction as a functional f\u2713(\u00b7, st) (Eq (1)) which can be used to approximate the gradient @J\u2713/@at. Hence, the gradient of the loss J\u2713 with respect to \u2713 is: @J\u2713(st)/@\u2713 / E\u21e0 \u21e5 f\u2713(\u21e0, st)@f\u2713(\u21e0, st)/@\u2713 \u21e4 . Note that the integral in Eq (7) is approximated via importance sampling, which is known to result in high variance estimates and hence poor scalability to high dimensional action spaces. Moreover, amortized generation is usually unstable and prone to mode collapse, an issue similar to GANs. Therefore, SQL is outperformed by SAC Haarnoja et al. (2018a) on benchmark tasks like MuJoCo."
        },
        {
            "heading": "3 APPROACH",
            "text": "We introduce S2AC, a new actor-critic MaxEnt RL algorithm that uses SVGD as the underlying actor to generate action samples from policies represented using EBMs. This choice is motivated by the expressivity of distributions that can be fitted via SVGD. Additionally, we show that we can derive a closed-form entropy estimate of the SVGD-induced distribution, thanks to the invertibility of the update rule, which does not necessarily hold for other EBM samplers. Besides, we propose a parameterized version of SVGD to enable scalability to high-dimensional action spaces and nonsmooth Q-function landscapes. S2AC is hence capable of learning a more optimal solution to the MaxEnt RL objective (Eq (2)) as illustrated in Figure 2."
        },
        {
            "heading": "3.1 STEIN SOFT ACTOR CRITIC",
            "text": "Like SAC, S2AC performs soft policy iteration which alternates between policy evaluation and policy improvement. The difference is that we model the actor as a parameterized sampler from an EBM. Hence, the policy distribution corresponds to an expressive EBM as opposed to a Gaussian.\nCritic. The critic\u2019s parameters are obtained by minimizing the Bellman loss as traditionally:\n\u21e4 = argmin E(st,at)\u21e0\u21e2\u21e1\u2713 \u21e5 (Q (st, at) y\u0302)2 \u21e4 , (8)\nwith the target y\u0302 = rt(st, at) + E(st+1,at+1)\u21e0\u21e2\u21e1 \u21e5 Q\u0304(st+1, at+1) + \u21b5H(\u21e1(\u00b7|st+1)) \u21e4 . Here \u0304 is an exponentially moving average of the value network weights (Mnih et al., 2015).\nActor as an EBM sampler. The actor is modeled as a sampler from an EBM over the Q-values. To generate a set of valid actions, the actor first samples a set of particles {a0} from an initial distribution q0 (e.g., Gaussian). These particles are then updated over several iterations l 2 [1, L], i.e., {al+1} {al}+ \u270fh({al}, s) following the sampler dynamics characterized by a transformation h (e.g., for SVGD, h = f in Eq (1)). If q0 is tractable and h is invertible, it\u2019s possible to compute a closed-form expression of the distribution of the particles at the lth iteration via the change of variable formula Devore et al. (2012): ql(al|s) = ql 1(al 1|s) det(I + \u270fralh(al, s)) 1 , 8l 2 [1, L]. In this case, the policy is represented using the particle distribution at the final step L of the sampler dynamics, i.e., \u21e1(a|s) = qL(aL|s) and the entropy can be estimated by averaging log qL(aL|s) over a set of particles (Section 3.2). We study the invertibility of popular EBM samplers in Section 3.3.\nParameterized initialization. To reduce the number of steps required to converge to the target distribution (hence reducing computation cost), we further propose modeling the initial distribution as a parameterized isotropic Gaussian, i.e., a0 \u21e0 N (\u00b5\u2713(s), \u2713(s)). The parameterization trick is then used to express a0 as a function of \u2713. Intuitively, the actor would learn \u2713 such that the initial distribution is close to the target distribution. Hence, fewer steps are required to converge, as illustrated in Figure 3. Note that if the number of steps L = 0, S2AC is\nreduced to SAC. Besides, to deal with the non-smooth nature of deep Q-function landscapes which might lead to particle divergence in the sampling process, we bound the particle updates to be within a few standard deviations (t) from the mean of the learned initial distribution, i.e., t \u2713  al\u2713  t \u2713, 8l 2 [1, L]. Eventually, the initial distribution q0\u2713 learns to contour the high-density region of the target distribution and the following updates refine it by converging to the spanned modes. Formally, the parameters \u2713 are computed by minimizing the expected KL-divergence between the policy qL\u2713 induced by the particles from the sampler and the EBM of the Q-values:\n\u2713\u21e4=argmax\u2713Est\u21e0D,aL\u2713 \u21e0\u21e1\u2713 \u21e5 Q (st, a L \u2713 ) \u21e4 + \u21b5Est\u21e0D [H(\u21e1\u2713(\u00b7|st))]\ns.t. t \u2713  al\u2713  t \u2713, 8l 2 [1, L]. (9) Here, D is the replay buffer. The derivation is in Appendix E. Note that the constraint does not truncate the particles as it is not an invertible transformation which then violates the assumptions of the change of variable formula. Instead, we sample more particles than we need and select the ones that stay within the range. We call S2AC( , \u2713) and S2AC( ) as two versions of S2AC with/without the parameterized initial distribution. The complete S2AC algorithm is in Algorithm 1 of Appendix A."
        },
        {
            "heading": "3.2 A CLOSED-FORM EXPRESSION OF THE POLICY\u2019S ENTROPY",
            "text": "A critical challenge in MaxEnt RL is how to efficiently compute the entropy term H(\u21e1(\u00b7|st+1)) in Eq (2). We show that, if we model the policy as an iterative sampler from the EBM, under certain conditions, we can derive a closed-form estimate of the entropy at convergence. Theorem 3.1. Let F : Rn ! Rn be an invertible transformation of the form F (a) = a + \u270fh(a). We denote by qL(aL) the distribution obtained from repeatedly applying F to a set of samples {a0} from an initial distribution q0(a0) over L steps, i.e., aL = F F \u00b7 \u00b7 \u00b7 F (a0). Under the condition \u270f||ralih(ai)||1 \u2327 1, 8l 2 [1, L], the distribution of the particles at the L th step is:\nlog qL(aL) \u21e1 log q0(a0) \u270f XL 1\nl=0 Tr(ralh(al)) +O(\u270f2dL). (10)\nHere, d is the dimensionality of a, i.e., a 2 Rd and O(\u270f2dL) is the order of approximation error. Proof Sketch: As F is invertible, we apply the change of variable formula (Appendix C.2) on the transformation F F \u00b7 \u00b7 \u00b7F and obtain: log qL(aL) = log q0(a0) PL 1 l=0 log det(I + \u270fralh(al)) . Under the assumption \u270f||raih(ai)||1 \u2327 1, we apply the corollary of Jacobi\u2019s formula (Appendix C.3) and get Eq. (10). The detailed proof is in Appendix F. Note that the condition \u270f||raih(ai)||1 \u2327 1 can always be satisfied when we choose a sufficiently small step size \u270f, or the gradient of h(a) is small, i.e., h(a) is Lipschitz continuous with a sufficiently small constant. It follows from the theorem above, that the entropy of a policy modeled as an EBM sampler (Eq (9)) can be expressed analytically as:\nH(\u21e1\u2713(\u00b7|s))= Ea0\u2713\u21e0q0\u2713 h log qL\u2713 (a L \u2713 |s) i \u21e1 Ea0\u2713\u21e0q0\u2713 h log q0\u2713(a 0|s) \u270f XL 1 l=0 Tr \u21e3 ral\u2713h(a l \u2713, s) \u2318i . (11)\nIn the following, we drop the dependency of the action on \u2713 for simplicity of the notation."
        },
        {
            "heading": "3.3 INVERTIBLE POLICIES",
            "text": "Next, we study the invertibility of three popular EBM samplers: SVGD, SGLD, and HMC as well as the efficiency of computing the trace, i.e., Tr(ralh(al, s)) in Eq (10) for the ones that are invertible. Proposition 3.2 (SVGD invertibility). Given the SVGD learning rate \u270f and RBF kernel k(\u00b7, \u00b7) with variance , if \u270f\u2327 , the update rule of SVGD dynamics defined in Eq (1) is invertible.\nProof Sketch: We use the explicit function theorem to show that the JacobianraF (a, s) of the update rule F (a, s) is diagonally dominated and hence invertible. This yields invertibility of F (a, s). See detailed proof in Appendix G.3. Theorem 3.3. The closed-form estimate of log qL(aL|s) for the SVGD based sampler with an RBF kernel k(\u00b7, \u00b7) is log qL(aL|s)\u21e1 logq0(a0|s)+\n\u270f m 2\nL 1X\nl=0\nmX\nj=1,al 6=alj\nk(alj , a l) \u21e3 (al alj)\n> ralj Q(s, alj)+ \u21b5 2 kal aljk 2 d\u21b5\n\u2318 .\nHere, (\u00b7)> denotes the transpose of a matrix/vector. Note that the entropy does not depend on any matrix computation, but only on vector dot products and first-order vector derivatives. The proof is in Appendix H.1. Intuitively, the derived likelihood is proportional to (1) the concavity of the curvature of the Q-landscape, captured by a weighted average of the neighboring particles\u2019 Q-value gradients and (2) pairwise-distances between the neighboring particles (\u21e0kali aljk2 \u00b7 exp (kali aljk2)), i.e., the larger the distance the higher is the entropy. We elaborate on the connection between this formula and non-parametric entropy estimators in Appendix B. Proposition 3.4 (SGLD, HMC). The SGLD and HMC updates are not invertible w.r.t. a. Proof Sketch: SGLD is stochastic (noise term) and thus not injective. HMC is only invertible if conditioned on the velocity v. Detailed proofs are in Appendices G.1-G.2. From the above theoretic analysis, we can see that SGLD update is not invertible and hence is not suitable as a sampler for S2AC. While the HMC update is invertible, its derived closed-form entropy involves calculating Hessian and hence computationally more expensive. Due to these considerations, we choose to use SVGD with an RBF kernel as the underlying sampler of S2AC."
        },
        {
            "heading": "4 RESULTS",
            "text": "We first evaluate the correctness of our proposed closed-form entropy formula. Then we present the results of different RL algorithms on multigoal and MuJoCo environments."
        },
        {
            "heading": "4.1 ENTROPY EVALUATION",
            "text": "This experiment tests the correctness of our entropy formula. We compare the estimated entropy for distributions (with known ground truth entropy or log-likelihoods) using different samplers and study the sensitivity of the formula to different samplers\u2019 parameters. (1) Recovering the ground truth entropy. In Figure 4a, we plot samples (black dots) obtained by SVGD, SGLD, DLD and HMC at convergence to a Gaussian with ground truth entropy H(p) = 3.41, starting from the same initial distribution (leftmost sub-figure). We also report the entropy values computed via Eq.(11). Unlike SGLD, DLD, and HMC, SVGD recovers the ground truth entropy. This empirically supports Proposition 3.4 that SGLD, DLD, and HMC are not invertible. (2) Effect of the kernel variance. Figure 4b shows the effect of different SVGD kernel variances , where we use the same initial Gaussian from Figure 4a. We also visualize the particle distributions after L SVGD steps for the different configurations in Figure 9 of Appendix I. We can see that when the kernel variance is too small (e.g., =0.1), the invertibility is violated, and thus the estimated entropy is wrong even at convergence. On the other extreme when the kernel variance is too large (e.g., =100), i.e., when the particles are too scattered initially, the particles do not converge to the target Gaussian due to noisy gradients in the first term of Eq.(1). The best configurations hence lie somewhere in between (e.g., 2{3, 5, 7}). (3) Effect of SVGD steps and particles. Figure 4c and Figure 10b (Appendix. I) show the behavior of our entropy formula under different configurations of the number of SVGD steps and particles, on two settings: (i) GMM M with an increasing number of components M , and (ii) distributions with increasing ground truth entropy values, i.e., Gaussians with increasing variances . Results show that our entropy consistently grows with an increasing M (Figure 4c) and increasing (Figure 10b), even when a small number of SVGD steps and particles is used (e.g., L = 10,m = 10).\n4.2 MULTI-GOAL EXPERIMENTS\n! = 20\n(', (( ()\nMultigoal Environment\nTo check if S2AC learns a better solution to the max-entropy objective (Eq (2)), we design a new multi-goal environment as shown in Figure 5. The agent is a 2D point mass at the origin trying to reach one of the goals (in red). Q-landscapes are depicted by level curves. Actions are bounded in [ 1, 1] along both axes. Critical states for the analysis are marked with blue crosses. It is built on the multi-goal environment in Haarnoja et al. (2017) with modifications such that all the goals have (i) the same maximum expected future reward (positive) but (ii) different maximum expected future entropy. This is achieved by asymmetrically placing the goals (two goals on the left side and one on the right, leading to a higher\nexpected future entropy on the left side) while assigning the same final rewards to all the goals. The problem setup and hyperparameters are detailed in Appendix J. (1) Multi-modality. Figure 6 visualizes trajectories (blue lines) collected from 20 episodes of S2AC( , \u2713), S2AC( ), SAC, SQL and SAC-NF (SAC with a normalizing flow policy, Mazoure et al. (2020)) agents (rows) at test time for increasing entropy weights \u21b5 (columns). S2AC and SQL consistently cover all the modes for all \u21b5 values, while this is only achieved by SAC and SAC-NF for large \u21b5 values. Note that, in the case of SAC, this comes at the expense of accuracy. Although normalizing flows are expressive enough in theory, they are known to quickly collapse to local optima in practice Kobyzev et al. (2020). The dispersion term in S2AC encodes an inductive bias to mitigate this issue. (2) Maximizing the expected future entropy. We also see that with increasing \u21b5, more S2AC and SAC-NF trajectories converge to the left goals (G2/G3). This shows both models learn to maximize the expected future entropy. This is not the case for SQL whose trajectory distribution remains uniform across the goals. SAC results do not show a consistent trend. This validates the hypothesis that the entropy term in SAC only helps exploration but does not lead to maximizing future entropy. The quantified distribution over reached goals is in Figure 12 of Appendix J. (3) Robustness/adaptability. To assess the robustness of the learned policies, we place an obstacle (red bar in Figure 7) on the path to G2. We show the test time trajectories of 20 episodes using S2AC, SAC, SQL and SAC-NF agents trained with different \u21b5\u2019s. We observe that, for S2AC and SAC-NF, with increasing \u21b5, more trajectories reach the goal after hitting the obstacles. This is not the case for SAC, where many trajectories hit the obstacle without reaching the goal. SQL does not manage to escape the barrier even with higher \u21b5. Additional results on the (4) effect of parameterization of q0, and the (5) entropy\u2019s effect on the learned Q-landscapes are respectively reported in Figure 11 and Figure 14 of Appendix J."
        },
        {
            "heading": "4.3 MUJOCO EXPERIMENTS",
            "text": "We evaluate S2AC on five environments from MuJoCo (Brockman et al., 2016): Hopper-v2, Walker2dv2, HalfCheetah-v2, Ant-v2, and Humanoid-v2. As baselines, we use (1) DDPG (Gu et al., 2017), (2) PPO (Schulman et al., 2015), (3) SQL (Haarnoja et al., 2017), (4) SAC-NF (Mazoure et al., 2020), and (5) SAC (Haarnoja et al., 2018a). Hyperparameters are in Appendix K. (1) Performance and sample efficiency. We train five different instances of each algorithm with different random seeds, with each performing 100 evaluation rollouts every 1000 environment steps. Performance results are in Figure 8(a)-(e). The solid curves correspond to the mean returns over the five trials and the shaded region represents the minimum and maximum. S2AC( , \u2713) is consistently better than SQL and SAC-NF across all the environments and has superior performance than SAC in four out of five environments. Results also show that the initial parameterization was key to ensuring the scalability (S2AC( ) has poor performance compared to S2AC( , \u2713)). Figure 8(f)-(j) demonstrate the statistical significance of these gains by leveraging statistics from the rliable library (Agarwal et al., 2021) which we detail in Appendix K.\n(2) Run-time. We report the run-time of action selection of SAC, SQL, and S2AC algorithms in Table 1. S2AC( , \u2713) run-time increases linearly with the action space. To improve the scalability, we train an amortized version that we deploy at test-time, following (Haarnoja et al., 2017). Specifically, we train a feed-forward deepnet f (s, z) to mimic the SVGD dynamics during testing, where z is a random vector that allows mapping the same state to different particles. Note that we cannot use f (s, z) during training\nas we need to estimate the entropy in Eq (11), which depends on the unrolled SVGD dynamics (details in Appendix K). The amortized version S2AC( , \u2713, ) has a similar run-time to SAC and SQL with a slight tradeoff in performance (Figure 8)."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "MaxEnt RL (Todorov, 2006; Ziebart, 2010; Rawlik et al., 2012) aims to learn a policy that gets high rewards while acting as randomly as possible. To achieve this, it maximizes the sum of expected future reward and expected future entropy. It is different from entropy regularization (Schulman et al., 2015; O\u2019Donoghue et al., 2016; Schulman et al., 2017) which maximizes entropy at the current time step. It is also different from multi-modal RL approaches (Tang & Agrawal, 2018) which recover different modes with equal frequencies without considering their future entropy. MaxEnt RL has been broadly incorporated in various RL domains, including inverse RL (Ziebart et al., 2008; Finn et al., 2016), stochastic control (Rawlik et al., 2012; Toussaint, 2009), guided policy search (Levine & Koltun, 2013), and off-policy learning (Haarnoja et al., 2018a;b). MaxEnt RL is shown to maximize a lower bound of the robust RL objective (Eysenbach & Levine, 2022) and is hence less sensitive\nto perturbations in state and reward functions. From the variational inference lens, MaxEnt RL aims to find the policy distribution that minimizes the KL-divergence to an EBM over Q-function. The desired family of variational distributions is (1) expressive enough to capture the intricacies of the Q-value landscape (e.g., multimodality) and (2) has a tractable entropy estimate. These two requirements are hard to satisfy. SAC (Haarnoja et al., 2018a) uses a Gaussian policy. Despite having a tractable entropy, it fails to capture arbitrary Q-value landscapes. SAC-GMM (Haarnoja, 2018) extends SAC by modeling the policy as a Gaussian Mixture Model, but it requires an impractical grid search over the number of components. Other extensions include IAPO (Marino et al., 2021) which also models the policy as a uni-modal Gaussian but learns a collection of parameter estimates (mean, variance) through different initializations. While this yields multi-modality, it does not optimize a MaxEnt objective. SSPG (Cetin & Celiktutan, 2022) and SAC-NF (Mazoure et al., 2020) respectively improve the policy expressivity by modeling the policy as a Markov chain with Gaussian transition probabilities and as a normalizing flow. Due to training instability, the reported multi-goal experiments in (Cetin & Celiktutan, 2022) show that, though both models capture multimodality, they fail to maximize the expected future entropy in positive reward setups. SQL (Haarnoja et al., 2017), on the other hand, bypasses the explicit entropy computation altogether via a soft version of value iteration. It then trains an amortized SVGD (Wang & Liu, 2016) sampler from the EBM over the learned Q-values. However, estimating soft value functions requires approximating integrals via importance sampling which is known to have high variance and poor scalability. We propose a new family of variational distributions induced by a parameterized SVGD sampler from the EBM over Q-values. Our policy is expressive and captures multi-modal distributions while being characterized by a tractable entropy estimate. EBMs (LeCun et al., 2006; Wu et al., 2018) are represented as Gibbs densities p(x) = expE(x)/Z, where E(x) 2 R is an energy function describing inter-variable dependencies and Z = R expE(x) is the partition function. Despite their expressiveness, EBMs are not tractable as the partition function requires integrating over an exponential number of configurations. Markov Chain Monte Carlo (MCMC) methods (Van Ravenzwaaij et al., 2018) (e.g., HMC (Hoffman & Gelman, 2014), SGLD (Welling & Teh, 2011)) are frequently used to approximate the partition function via sampling. There have been recent efforts to parameterize these samplers via deepnets (Levy et al., 2017; Gong et al., 2018; Feng et al., 2017) to improve scalability. Similarly to these methods, we propose a parameterized variant of SVGD (Liu & Wang, 2016) as an EBM sampler to enable scalability to highdimensional action spaces. Beyond sampling, we derive a closed-form expression of the sampling distribution as an estimate of the EBM. This yields a tractable estimate of the entropy. This is opposed to previous methods for estimating EBM entropy which mostly rely on heuristic approximation, lower bounds Dai et al. (2017; 2019a), or neural estimators of mutual information (Kumar et al., 2019). The idea of approximating the entropy of EBMs via MCMC sampling by leveraging the change of variable formula was first proposed in Dai et al. (2019b). The authors apply the formula to HMC and LD, which, as we show previously, violate the invertibility assumption. To go around this, they augment the EBM family with the noise or velocity variable for LD and HMC respectively. But the derived log-likelihood of the sampling distribution turns out to be \u2013counter-intuitively\u2013 independent of the sampler\u2019s dynamics and equal to the initial distribution, which is then parameterized using a flow model (details in Appendix B.2). We show that SVGD is invertible, and hence we sample from the original EBM, so that our derived entropy is more intuitive as it depends on the SVGD dynamics. SVGD-augmented RL (Liu & Wang, 2016) has been explored under other RL contexts. Liu et al. (2017) use SVGD to learn a distribution over policy parameters. While this leads to learning diverse policies, it is fundamentally different from our approach as we are interested in learning a single multi-modal policy with a closed-form entropy formula. Castanet et al. (2023); Chen et al. (2021) use SVGD to sample from multimodal distributions over goals/tasks. We go beyond sampling and use SVGD to derive a closed-form entropy formula of an expressive variational distribution."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose S2AC, an actor-critic algorithm that yields a more optimal solution to the MaxEnt RL objective than previously proposed approaches. S2AC achieves this by leveraging a new family of variational distributions characterized by SVGD dynamics. The proposed distribution has high expressivity, i.e., it is flexible enough to capture multimodal policies in high dimensional spaces, and a tractable entropy estimate. Empirical results show that S2AC learns expressive and robust policies while having superior performance than other MaxEnt RL algorithms. For future work, we plan to study the application of the proposed variational distribution to other domains and develop benchmarks to evaluate the robustness of RL agents."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Bo An is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISGAward No: AISG2-GC-2023-009). Haipeng Chen is supported by William & Mary FRC Faculty Research Grants."
        }
    ],
    "title": "S2AC: ENERGY-BASED REINFORCEMENT LEARNING WITH STEIN SOFT ACTOR CRITIC",
    "year": 2024
}