{
    "abstractText": "Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging K\u2013tree representation, originally designed for lossless graph compression. The K\u2013tree representation encompasses inherent hierarchy while enabling compact graph generation. In addition, we make contributions by (1) presenting a sequential K\u2013tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yunhui Jang"
        },
        {
            "affiliations": [],
            "name": "Dongwoo Kim"
        },
        {
            "affiliations": [],
            "name": "Sungsoo Ahn"
        }
    ],
    "id": "SP:7a93721f2a0e5ff7ff9867ec79e8e42a5c38f620",
    "references": [
        {
            "authors": [
                "Sungsoo Ahn",
                "Binghong Chen",
                "Tianzhe Wang",
                "Le Song"
            ],
            "title": "Spanning tree-based graph generation for molecules",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "R\u00e9ka Albert",
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si"
            ],
            "title": "Statistical mechanics of complex networks",
            "venue": "Reviews of modern physics,",
            "year": 2002
        },
        {
            "authors": [
                "Reet Barik",
                "Marco Minutoli",
                "Mahantesh Halappanavar",
                "Nathan R Tallent",
                "Ananth Kalyanaraman"
            ],
            "title": "Vertex reordering for real-world graphs and applications: An empirical evaluation",
            "venue": "IEEE International Symposium on Workload Characterization (IISWC),",
            "year": 2020
        },
        {
            "authors": [
                "Maciej Besta",
                "Torsten Hoefler"
            ],
            "title": "Survey and taxonomy of lossless graph compression and spaceefficient graph representations",
            "venue": "arXiv preprint arXiv:1806.01799,",
            "year": 2018
        },
        {
            "authors": [
                "Paolo Boldi",
                "Massimo Santini",
                "Sebastiano Vigna"
            ],
            "title": "Permuting web and social graphs",
            "venue": "Internet Mathematics,",
            "year": 2009
        },
        {
            "authors": [
                "Giorgos Bouritsas",
                "Andreas Loukas",
                "Nikolaos Karalias",
                "Michael Bronstein"
            ],
            "title": "Partition and code: learning how to compress graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nieves R Brisaboa",
                "Susana Ladra",
                "Gonzalo Navarro"
            ],
            "title": "k2-trees for compact web graph representation",
            "venue": "In SPIRE,",
            "year": 2009
        },
        {
            "authors": [
                "Xiaohui Chen",
                "Jiaxing He",
                "Xu Han",
                "Li-Ping Liu"
            ],
            "title": "Efficient and degree-guided graph generation via discrete diffusion modeling",
            "venue": "arXiv preprint arXiv:2305.04111,",
            "year": 2023
        },
        {
            "authors": [
                "Elizabeth Cuthill",
                "James McKee"
            ],
            "title": "Reducing the bandwidth of sparse symmetric matrices",
            "venue": "In Proceedings of the 1969 24th national conference,",
            "year": 1969
        },
        {
            "authors": [
                "Nathaniel Lee Diamant",
                "Alex M Tseng",
                "Kangway V Chuang",
                "Tommaso Biancalani",
                "Gabriele Scalia"
            ],
            "title": "Improving graph generation by restricting graph bandwidth",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Paul Erd\u0151s",
                "Alfr\u00e9d R\u00e9nyi"
            ],
            "title": "On the evolution of random graphs",
            "venue": "Publ. Math. Inst. Hung. Acad. Sci,",
            "year": 1960
        },
        {
            "authors": [
                "Nikhil Goyal",
                "Harsh Vardhan Jain",
                "Sayan Ranu"
            ],
            "title": "Graphgen: a scalable approach to domainagnostic labeled graph generation",
            "venue": "In Proceedings of The Web Conference",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Grover",
                "Aaron Zweig",
                "Stefano Ermon"
            ],
            "title": "Graphite: Iterative generative modeling of graphs",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "John J Irwin",
                "Teague Sterling",
                "Michael M Mysinger",
                "Erin S Bolstad",
                "Ryan G Coleman"
            ],
            "title": "Zinc: a free tool to discover chemistry for biology",
            "venue": "Journal of chemical information and modeling,",
            "year": 2012
        },
        {
            "authors": [
                "Wengong Jin",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Junction tree variational autoencoder for molecular graph generation",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Wengong Jin",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Hierarchical generation of molecular graphs using structural motifs",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jaehyeong Jo",
                "Seul Lee",
                "Sung Ju Hwang"
            ],
            "title": "Score-based generative modeling of graphs via the system of stochastic differential equations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Lingkai Kong",
                "Jiaming Cui",
                "Haotian Sun",
                "Yuchen Zhuang",
                "B. Aditya Prakash",
                "Chao Zhang"
            ],
            "title": "Autoregressive diffusion model for graph generation, 2023",
            "venue": "URL https://openreview. net/forum?id=98J48HZXxd5",
            "year": 2023
        },
        {
            "authors": [
                "Mario Krenn",
                "Florian H\u00e4se",
                "A Nigam",
                "Pascal Friederich",
                "Al\u00e1n Aspuru-Guzik"
            ],
            "title": "SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry",
            "venue": "arXiv preprint arXiv:1905.13741,",
            "year": 1905
        },
        {
            "authors": [
                "N Jesper Larsson",
                "Alistair Moffat"
            ],
            "title": "Off-line dictionary-based compression",
            "venue": "Proceedings of the IEEE,",
            "year": 2000
        },
        {
            "authors": [
                "Yujia Li",
                "Oriol Vinyals",
                "Chris Dyer",
                "Razvan Pascanu",
                "Peter Battaglia"
            ],
            "title": "Learning deep generative models of graphs",
            "venue": "arXiv preprint arXiv:1803.03324,",
            "year": 2018
        },
        {
            "authors": [
                "Renjie Liao",
                "Yujia Li",
                "Yang Song",
                "Shenlong Wang",
                "Will Hamilton",
                "David K Duvenaud",
                "Raquel Urtasun",
                "Richard Zemel"
            ],
            "title": "Efficient graph generation with graph recurrent attention networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jenny Liu",
                "Aviral Kumar",
                "Jimmy Ba",
                "Jamie Kiros",
                "Kevin Swersky"
            ],
            "title": "Graph normalizing flows",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Meng Liu",
                "Keqiang Yan",
                "Bora Oztekin",
                "Shuiwang Ji"
            ],
            "title": "Graphebm: Molecular graph generation with energy-based models",
            "venue": "arXiv preprint arXiv:2102.00546,",
            "year": 2021
        },
        {
            "authors": [
                "Tianze Luo",
                "Zhanfeng Mo",
                "Sinno Jialin Pan"
            ],
            "title": "Fast graph generative model via spectral diffusion",
            "venue": "arXiv preprint arXiv:2211.08892,",
            "year": 2022
        },
        {
            "authors": [
                "Youzhi Luo",
                "Keqiang Yan",
                "Shuiwang Ji"
            ],
            "title": "Graphdf: A discrete flow model for molecular graph generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kaushalya Madhawa",
                "Katushiko Ishiguro",
                "Kosuke Nakago",
                "Motoki Abe"
            ],
            "title": "Graphnvp: An invertible flow model for generating molecular graphs",
            "venue": "arXiv preprint arXiv:1905.11600,",
            "year": 1905
        },
        {
            "authors": [
                "Karolis Martinkus",
                "Andreas Loukas",
                "Nathana\u00ebl Perraudin",
                "Roger Wattenhofer"
            ],
            "title": "Spectre: Spectral conditioning helps to overcome the expressivity limits of one-shot graph generators",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "\u0141ukasz Maziarka",
                "Agnieszka Pocha",
                "Jan Kaczmarczyk",
                "Krzysztof Rataj",
                "Tomasz Danel",
                "Micha\u0142 Warcho\u0142"
            ],
            "title": "Mol-cyclegan: a generative model for molecular optimization",
            "venue": "Journal of Cheminformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Chris Mueller"
            ],
            "title": "Sparse matrix reordering algorithms for cluster identification",
            "venue": "Machune Learning in Bioinformatics,",
            "year": 2004
        },
        {
            "authors": [
                "Chenhao Niu",
                "Yang Song",
                "Jiaming Song",
                "Shengjia Zhao",
                "Aditya Grover",
                "Stefano Ermon"
            ],
            "title": "Permutation invariant graph generation via score-based generative modeling",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sriram Raghavan",
                "Hector Garcia-Molina"
            ],
            "title": "Representing web graphs",
            "venue": "In Proceedings 19th International Conference on Data Engineering (Cat. No. 03CH37405),",
            "year": 2003
        },
        {
            "authors": [
                "Raghunathan Ramakrishnan",
                "Pavlo O Dral",
                "Matthias Rupp",
                "O Anatole Von Lilienfeld"
            ],
            "title": "Quantum chemistry structures and properties of 134 kilo molecules",
            "venue": "Scientific data,",
            "year": 2014
        },
        {
            "authors": [
                "Ida Schomburg",
                "Antje Chang",
                "Christian Ebeling",
                "Marion Gremse",
                "Christian Heldt",
                "Gregor Huhn",
                "Dietmar Schomburg"
            ],
            "title": "Brenda, the enzyme database: updates and major new developments",
            "venue": "Nucleic acids research,",
            "year": 2004
        },
        {
            "authors": [
                "Marwin HS Segler",
                "Thierry Kogej",
                "Christian Tyrchan",
                "Mark P Waller"
            ],
            "title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "venue": "ACS central science,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Shaw",
                "Jakob Uszkoreit",
                "Ashish Vaswani"
            ],
            "title": "Self-attention with relative position representations",
            "venue": "arXiv preprint arXiv:1803.02155,",
            "year": 2018
        },
        {
            "authors": [
                "Chence Shi",
                "Minkai Xu",
                "Zhaocheng Zhu",
                "Weinan Zhang",
                "Ming Zhang",
                "Jian Tang"
            ],
            "title": "Graphaf: a flow-based autoregressive model for molecular graph generation",
            "venue": "arXiv preprint arXiv:2001.09382,",
            "year": 2020
        },
        {
            "authors": [
                "Martin Simonovsky",
                "Nikos Komodakis"
            ],
            "title": "Graphvae: Towards generation of small graphs using variational autoencoders",
            "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN 2018: 27th International Conference on Artificial Neural Networks,",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Clement Vignac",
                "Igor Krawczuk",
                "Antoine Siraudin",
                "Bohan Wang",
                "Volkan Cevher",
                "Pascal Frossard"
            ],
            "title": "Digress: Discrete denoising diffusion for graph generation",
            "venue": "arXiv preprint arXiv:2209.14734,",
            "year": 2022
        },
        {
            "authors": [
                "Soojung Yang",
                "Doyeong Hwang",
                "Seul Lee",
                "Seongok Ryu",
                "Sung Ju Hwang"
            ],
            "title": "Hit and lead discovery with explorative rl and fragment-based molecule generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhitao Ying",
                "Jiaxuan You",
                "Christopher Morris",
                "Xiang Ren",
                "Will Hamilton",
                "Jure Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jiaxuan You",
                "Rex Ying",
                "Xiang Ren",
                "William Hamilton",
                "Jure Leskovec"
            ],
            "title": "Graphrnn: Generating realistic graphs with deep auto-regressive models",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Hao Yu",
                "Xu Sun",
                "Wei Deng Solvang",
                "Xu Zhao"
            ],
            "title": "Reverse logistics network design for effective management of medical waste in epidemic outbreaks: Insights from the coronavirus disease",
            "venue": "International journal of environmental research and public health,",
            "year": 2019
        },
        {
            "authors": [
                "Chengxi Zang",
                "Fei Wang"
            ],
            "title": "Moflow: an invertible flow model for generating molecular graphs",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "\u2208 V"
            ],
            "title": "dim(A) to denote the number of rows(or columns) n of the square matrix A \u2208 {0, 1}n\u00d7n. We describe the full procedure in Algorithm 1. Note that the time complexity of the procedure is O(N2) (Brisaboa et al., 2009), where N denotes the number of nodes in the graph G",
            "year": 2009
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Generating graph-structured data is a challenging problem in numerous fields, such as molecular design (Li et al., 2018; Maziarka et al., 2020), social network analysis (Grover et al., 2019), and public health (Yu et al., 2020). Recently, deep generative models have demonstrated significant potential in addressing this challenge (Simonovsky & Komodakis, 2018; Jo et al., 2022; Vignac et al., 2022). In contrast to the classic random graph models (Albert & Barab\u00e1si, 2002; Erdo\u030bs et al., 1960), these methods leverage powerful deep generative paradigms, e.g., variational autoencoders (Simonovsky & Komodakis, 2018), normalizing flows (Madhawa et al., 2019), and diffusion models (Jo et al., 2022).\nThe graph generative models can be categorized into three types by the graph representation the models generate. First, an adjacency matrix is the most common representation (Simonovsky & Komodakis, 2018; Madhawa et al., 2019; Liu et al., 2021). Secondly, a string-based representation extracted from depth-first tree traversal on a graph can represent the graph as a sequence (Ahn et al., 2022; Goyal et al., 2020; Krenn et al., 2019). Finally, representing a graph as a composition of connected motifs, i.e., frequently appearing subgraphs, can preserve the high-level structural properties (Jin et al., 2018; 2020). We describe the representations on the left of Figure 1.\nAlthough there is no consensus on the best graph representation, two factors drive their development. First is the need for compactness to reduce the complexity of graph generation and simplify the search space over graphs. For example, to generate a graph with N vertices and M edges, the adjacency matrix requires specifying N2 elements. In contrast, the string representation typically requires specifying O(N +M) elements, leveraging the graph sparsity (Ahn et al., 2022; Goyal et al., 2020; Segler et al., 2018). Motif representations also save space by representing frequently appearing subgraphs by basic building blocks (Jin et al., 2018; 2020).\nThe second factor driving the development of new graph representations is the presence of a hierarchy in graphs. For instance, community graphs possess underlying clusters, molecular graphs consist of distinct chemical fragments, and grid graphs exhibit a repetitive coarse-graining structure. In this context, motif representations (Jin et al., 2018; 2020) address the presence of a hierarchy in graphs; however, they are limited to a fixed vocabulary of motifs observed in the dataset or a specific domain.\nContribution. In this paper, we propose a novel graph generation framework, coined Hierarchical Graph Generation with K2\u2013Tree (HGGT), which can represent not only non-attributed graphs but also attributed graphs in a compact and hierarchical way without domain-specific rules. The right-side table of Figure 1 emphasizes the benefits of HGGT. Since the K2\u2013tree recursively redefines\na graph into K2 substructures, our representation becomes more compact and enables consideration of hierarchical structure in adjacency matrices.1\nSpecifically, we model the process of graph generation as an autoregressive construction of the K2\u2013tree. To this end, we design a sequential K2\u2013tree representation that recovers the original K2\u2013tree when combined sequentially. In particular, we propose a two-stage procedure where (1) we prune the K2\u2013tree to remove redundancy arising from the symmetric adjacency matrix for undirected graphs and (2) subsequently flatten and tokenize the K2\u2013tree into a sequence to minimize the number of decisions required for the graph generation.\nWe employ the Transformer architecture (Vaswani et al., 2017) to generate the sequential K2\u2013tree representation of a graph. To better incorporate the positional information of each node in a tree, we design a new positional encoding scheme specialized to the K2\u2013tree structure. Specifically, we represent the positional information of a node by its pathway from the root node; the proposed encoding enables the reconstruction of the full K2\u2013tree given just the positional information.\nTo validate the effectiveness of our algorithm, we test our method on popular graph generation benchmarks across six graph datasets: Community, Enzymes (Schomburg et al., 2004), Grid, Planar, ZINC (Irwin et al., 2012), and QM9 (Ramakrishnan et al., 2014). Our empirical results confirm that HGGT significantly outperformed existing graph generation methods on five out of six benchmarks, verifying the capability of our approach for high-quality graph generation across diverse applications.\nTo summarize, our key contributions are as follows:\n\u2022 We propose a new graph generative model based on adopting the K2\u2013tree as a compact, hierarchical, and domain-agnostic representation of graphs.\n\u2022 We introduce a novel, compact sequential K2\u2013tree representation obtained from pruning, flattening, and tokenizing the K2\u2013tree.\n\u2022 We propose an autoregressive model to generate the sequential K2\u2013tree representation using Transformer architecture with a specialized positional encoding scheme.\n\u2022 We validate the efficacy of our framework by demonstrating state-of-the-art graph generation performance on five out of six graph generation benchmarks."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Graph representations for graph generation. The choice of graph representation is a crucial aspect of graph generation, as it significantly impacts the efficiency and allows faithful learning of the generative model. The most widely used one is the adjacency matrix, which simply encodes the pairwise relationship between nodes (Jo et al., 2022; Vignac et al., 2022; You et al., 2018; Liao et al., 2019; Shi et al., 2020; Luo et al., 2021; Kong et al., 2023; Chen et al., 2023). However, several methods (Vignac et al., 2022; You et al., 2018; Jo et al., 2022) suffer from the high complexity in generating the adjacency matrix, especially for large graphs.\n1This differs from the conventional hierarchical community structure. We provide the discussion in Appendix H.\nTo address this issue, researchers have developed graph generative models that employ alternative graph representations such as motif-based representations and string-based representations. For instance, Ahn et al. (2022); Segler et al. (2018) proposed to generate molecule-specific string representations, and Jin et al. (2018; 2020); Yang et al. (2021) suggested generative models that extract reasonable fragments from data and generate the set of motifs. However, these methods rely on domain-specific knowledge and are restricted to molecular data.\nLossless graph compression. Lossless graph compression (Besta & Hoefler, 2018) aims to reduce the size and complexity of graphs while preserving their underlying structures. Specifically, several works (Brisaboa et al., 2009; Raghavan & Garcia-Molina, 2003) introduced hierarchical graph compression methods that compress graphs leveraging their hierarchical structure. In addition, Bouritsas et al. (2021) derived the compressed representation using a learning-based objective."
        },
        {
            "heading": "3 K2\u2013TREE REPRESENTATION OF A GRAPH",
            "text": "In this section, we introduce the K2\u2013tree as a hierarchical and compact representation of graphs, as originally proposed for graph compression (Brisaboa et al., 2009). In essence, the K2\u2013tree is a K2-ary ordered tree that recursively partitions the adjacency matrix into K \u00d7K submatrices.2 Its key idea is to summarize the submatrices filled only with zeros with a single tree-node, exploiting the sparsity of the adjacency matrix. From now on, we indicate the tree-node as a node. The representation is hierarchical, as it associates each parent and child node pair with a matrix and its corresponding submatrix, respectively, as described in Figure 2.\nTo be specific, we consider the K2\u2013tree representation (T ,X ) of an adjacency matrix A as a K2-ary tree T = (V, E) associated with binary node attributes X = {xu : u \u2208 V}. Every non-root node is uniquely indexed as (i, j)-th child of its parent node for some i, j \u2208 {1, . . . ,K}. The tree T is ordered so that every (i, j)-th child node is ranked K(i\u22121)+ j among its siblings. Then the K2\u2013tree satisfies the following conditions:\n\u2022 Each node u is associated with a submatrix A(u) of the adjacency matrix A.\n\u2022 If the submatrix A(u) for a node u is filled only with zeros, xu = 0. Otherwise, xu = 1.\n\u2022 A node u is a leaf node if and only if xu = 0 or the matrix A(u) is a 1\u00d7 1 matrix. 2By default, we assume the number of nodes in the original graph to be the power of K2.\n\u2022 Let B1,1, . . . , BK,K denote the K \u00d7K partitioning of the matrix A(u) with i, j corresponding to row- and column-wise order, respectively. The child nodes v1,1, . . . , vK,K of the tree-node u are associated with the submatrices B1,1, . . . , BK,K , respectively.\nThe generated K2\u2013tree is a compact description of graph G as any node u with xu = 0 and du < maxu du where du is the distance from the root. summarizes a large submatrix filled only with zeros. In the worst-case scenario, the size of the K2\u2013tree is MK2(logK2(N\n2/M) + O(1)) (Brisaboa et al., 2009), where N and M denote the number of nodes and edges in the original graph, respectively. This constitutes a significant improvement over the N2 size of the full adjacency matrix.\nAdditionally, the K2\u2013tree is hierarchical ensuring that (1) each tree node represents the connectivity between a specific set of nodes, and (2) nodes closer to the root correspond to a larger set of nodes. We emphasize that the nodes associated with submatrices overlapping with the diagonal of the original adjacency matrix indicate intra-connectivity within a group of nodes. In contrast, the remaining nodes describe the interconnectivity between two distinct sets of nodes.\nWe also describe the detailed algorithms for constructing a K2\u2013tree from a given graph G and recovering a graph from the K2\u2013tree in Appendices A and B, respectively. It is crucial to note that the ordering of the nodes in the adjacency matrix influences the K2\u2013tree structure. Inspired by Diamant et al. (2023), we adopt Cuthill-McKee (C-M) ordering as our ordering scheme. We empirically discover that C-M ordering (Cuthill & McKee, 1969) provides the most compact K2\u2013tree.3 Our explanation is that the C-M ordering is specifically designed to align the non-zero elements of a matrix near its diagonal so that there is a higher chance of encountering large submatrices filled only with zeros, which can be efficiently summarized in the K2\u2013tree representation."
        },
        {
            "heading": "4 HIERARCHICAL GRAPH GENERATION WITH K2\u2013TREES",
            "text": "In this section, we present our novel method, hierarchical graph generation with K2\u2013trees (HGGT), exploiting the hierarchical and compact structure of the K2\u2013tree representation of a graph. In detail, we transform the K2\u2013tree into a highly compressed sequence through a process involving pruning and tokenization. Subsequently, we employ a Transformer enhanced with tree-based positional encodings, for the autoregressive generation of this compressed sequence."
        },
        {
            "heading": "4.1 SEQUENTIAL K2\u2013TREE REPRESENTATION",
            "text": "Here, we propose an algorithm to flatten the K2\u2013tree into a sequence, which is essential for the autoregressive generation of the K2\u2013tree. In particular, we aim to design a sequential representation that is even more compact than the K2\u2013tree to minimize the number of decisions required for the generation of the K2\u2013tree. To this end, we propose (1) pruning K2\u2013tree by removing redundant nodes, (2) flattening the pruned K2\u2013tree into a sequence, and (3) applying tokenization based on the K2\u2013tree structure. We provide an illustration of the overall process in Figure 3.\n3We provide the results in Section 5.3.\nPruning the K2\u2013tree. To obtain the pruned K2\u2013tree, we identify and eliminate redundant nodes due to the symmetry of the adjacency matrix for undirected graphs. In particular, without loss of generality, such nodes are associated with submatrices positioned above the diagonal since they mirror the counterparts located below the diagonal.\nTo this end, we now describe a formula to identify redundant nodes based on the position of a submatrix A(u), tied to a specific node u at depth L, within the adjacency matrix A. Let v0, v1, . . . , vL be a sequence of nodes representing a downward path from the root node r = v0 to the node u = vL. With (iv\u2113 , jv\u2113) denoting the order of v\u2113 among its K\u00d7K siblings, the node position can be represented as pos(u) = ((iv1 , jv1), . . . , (ivL , jvL)). Note that node u at depth L corresponds to an element of KL \u00d7KL partitions of the adjacency matrix A. The row and column indexes of the submatrix A(u) are derived as the (pu, qu) = ( \u2211L \u2113=1 K L\u2212\u2113(iv\u2113 \u2212 1) + 1, \u2211L \u2113=1 K L\u2212\u2113(jv\u2113 \u2212 1) + 1) as illustrated in Figure 4. As a result, we eliminate any node associated with a submatrix above the diagonal, i.e., we remove node u when pu < qu.\nConsequently, the pruned K2\u2013tree maintains only the nodes associated with submatrices devoid of redundant nodes, i.e., those containing elements of the adjacency matrix positioned at the diagonal or below the diagonal. Notably, following this pruning process, the K2\u2013tree no longer adheres to the structure of a K \u00d7K-ary tree. Additionally, consider a non-leaf node u is associated with a submatrix A(u) that includes any diagonal elements of the adjacency matrix A. Then the node u possess K(K + 1)/2 child nodes after pruning K(K \u2212 1)/2 child nodes associated with the redundant submatrices. Otherwise, the non-leaf node u remains associated with K \u00d7K child nodes. Note that our framework can be extended to directed graphs by omitting the pruning process.\nFlattening and tokenization of the pruned K2\u2013tree. Next, we explain how to obtain a sequential representation of the pruned K2\u2013tree based on flattening and tokenization. Our idea is to flatten a K2\u2013tree as a sequence of node attributes {xu : u \u2208 V} using breadth-first traversal and then to tokenize the sequence by grouping the nodes that share the same parent node, i.e., sibling nodes.\nFor this purpose, we denote the sequence of nodes obtained from a breadth-first traversal of nonroot nodes in the K2\u2013tree as u1, . . . , u|V|\u22121, and the corresponding sequence of node attributes as x = (x1, . . . , x|V|\u22121). It is important to note that sibling nodes sharing the same parent appear sequentially in the breadth-first traversal.\nNext, by grouping the sibling nodes, we tokenize the sequence x. As a result, we obtain a sequence y = (y1, . . . , yT ) where each element is a token representing a group of attributes associated with sibling nodes. For example, the t-th token corresponding to a group of K2 sibling nodes is represented by yt = (xv1,1 , . . . , xvK,K ) where v1,1, . . . , vK,K share the same parent node u. Such tokenization allows representing the whole K2\u2013tree using M(logK2(N\n2/M) + O(1)) space, where N and M denote the number of nodes and edges in the original graph, respectively.\nWe highlight that the number of elements in each token yt may vary due to the pruned K2\u2013tree no longer being a K \u00d7K-ary tree, as mentioned above. With this in consideration, we generate a vocabulary of 2K 2\n+ 2K(K+1)/2 potential configurations for each token yt. This vocabulary size is small in practice since we set the value K to be small, e.g., setting K = 2 induces the size of 24.\nIn particular, we remark that a token with K(K + 1)/2 elements carries different semantics from another token with K2 elements. The former corresponds to a submatrix situated on the adjacency matrix\u2019s diagonal, thus indicating connectivity within a set of nodes. In contrast, the latter relates to a submatrix illustrating connectivity between pairs of node sets. This supports our decision to assign distinct values to a token with K(K + 1)/2 elements and another with K2 elements, even when the tokens might represent the same combination of node features in the unpruned tree.\nGenerating featured graphs. We also extend our HGGT to graphs with node and edge-wise features, e.g., molecular graphs. At a high level, we apply our algorithm to the featured adjacency matrix, where each diagonal element corresponds to a node feature and each non-diagonal element corresponds to an edge feature. node attributes of leaf nodes in K2\u2013tree correspond to node and edge features, while attributes of non-leaf nodes are the same with the non-attributed K2\u2013trees (i.e., ones and zeros). See Figure 5 for an illustration and Appendix C for a complete description."
        },
        {
            "heading": "4.2 GENERATING K2\u2013TREE WITH TRANSFORMER AND K2\u2013TREE POSITIONAL ENCODING",
            "text": "We describe our algorithm to generate the sequence of K2\u2013tree representation y = (y1, . . . , yT ). We utilize the masked Transformer (Vaswani et al., 2017) to make predictions on p\u03b8(yt|yt\u22121, . . . , y1). To improve the model\u2019s understanding of the tree structure, we devise a tree-positional encoding. We also offer an algorithm to construct the K2\u2013tree from the sequence generated by the Transformer.\nTransformer with K2\u2013tree positional encoding. We first introduce the Transformer architecture to parameterize the distribution p\u03b8(yt|yt\u22121, . . . , y1) for autorgressive generation. Briefly, the model is trained with self-attention, and during inference, it generates the sequence one token at a time, relying on the previously generated sequence. To account for tree structural information, we incorporate tree-positional encodings for each time-step t.\nDuring training, we mask the attention layer to ensure that predictions at each step are not influenced by future tokens of the sequence. The objective function is maximum likelihood, denoted by max log p(y), where p(y) = p(y1)\u03a0Tt=2p(yt|y1:t\u22121). This objective aims to maximize the probability of predicting the next token correctly based on the preceding tokens.\nFor inference, we begin the process with a begin-of-sequence (BOS) token as the input to our trained Transformer decoder. The model then computes the distribution of potential tokens for the next step, denoted by p(yt|y1:t\u22121), and the next token is sampled from this distribution. This token is appended to the input sequence, and the extended sequence is fed back into the model to generate the subsequent token. This iterative procedure is terminated when a predefined maximum length is reached or an end-of-sequence (EOS) token emerges.\nTo enhance the input yt, we incorporate the positional encoding for u. As outlined in Section 4.1, the node attributes in yt are associated with child nodes of a particular node u. Therefore, the encoding is based on the downward path from the root node r = v0 to the node u = vL, represented as (v0, . . . , vL). In this context, the order of v\u2113 amongst its siblings in the non-pruned K2\u2013tree is denoted as a tuple (iv\u2113 , jv\u2113). Subsequently, we further update the input feature yt with positional\nencoding, which is represented as PE(u) = \u2211L\n\u2113=1 \u03d5\u2113(iv\u2113 , jv\u2113), where \u03d5 denotes the embedding function that converts the order tuple into vector representations and ((iv1 , jv1), . . . , (ivL , jvL)) is the sequence of orders of a downward path from r to u.\nConstructing K2\u2013tree from the sequential representation. We next explain the algorithm to recover a K2\u2013tree from its sequential representation y. In particular, we generate the K2\u2013tree simultaneously with the sequence to incorporate the tree information for each step of the autoregressive generation. The algorithm begins with an empty tree containing only a root node and iteratively expands each \u201cfrontier\u201d node based on the sequence of the decisions made by the generative model. To facilitate a breadth-first expansion approach, the algorithm utilizes a first-in-first-out (FIFO) queue, which contains node candidates to be expanded.\nTo be specific, our algorithm initializes a K2\u2013tree T = ({r}, \u2205) with the root node r associated with the node attribute xr = 1. It also initializes the FIFO queue Q with r. Then at each t-th step, our algorithm expands the node u popped from the queue Q using the token yt. To be specific, for each node attribute x in yt, our algorithm adds a child node v with xv = x. If x = 1 and the size of A(v) is larger than 1\u00d7 1, the child node v is inserted into the queue Q. This algorithm is designed to retrieve the pruned tree, which allows the computation of positional data derived from the yt information."
        },
        {
            "heading": "5 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "5.1 GENERIC GRAPH GENERATION",
            "text": "Experimental setup. We first validate the general graph generation performance of our HGGT on four popular graph benchmarks: (1) Community-small, 100 community graphs, (2) Planar, 200\nplanar graphs, (3) Enzymes (Schomburg et al., 2004), 587 protein tertiary structure graphs, and (4) Grid, 100 2D grid graphs. Following baselines, we adopt maximum mean discrepancy (MMD) to compare three graph property distributions between generated graphs and test graphs: degree (Deg.), clustering coefficient (Clus.), and 4-node-orbit counts (Orb.). We conduct all the experiments using a single RTX 3090 GPU. The detailed descriptions of our experimental setup are in Appendix D.\nBaselines. We compare our HGGT with twelve graph generative models: GraphVAE (Simonovsky & Komodakis, 2018), GraphRNN (You et al., 2018), GNF Liu et al. (2019), GRAN (Liao et al., 2019), EDP-GNN (Niu et al., 2020), GraphGen (Goyal et al., 2020), GraphAF (Shi et al., 2020), GraphDF (Luo et al., 2021), SPECTRE (Martinkus et al., 2022), GDSS (Jo et al., 2022), DiGress (Vignac et al., 2022), and GDSM (Luo et al., 2022). A detailed implementation description is in Appendix E.\nResults. Table 1 shows the experimental results. We observe that HGGT outperforms all baselines on all datasets. Note that our model consistently outperforms all baselines regardless of the graph sizes, indicating better generalization performance across various environments. In particular, we observe how the performance of HGGT is extraordinary for Grid. We hypothesize that HGGT better captures the hierarchical structure and repetitive local connectivity of the grid graphs than the other baselines. We also provide visualizations of the generated graphs in Figure 6."
        },
        {
            "heading": "5.2 MOLECULAR GRAPH GENERATION",
            "text": "Experimental setup. To test the ability of HGGT on featured graphs, we further conduct an evaluation of molecule generation tasks. We use two molecular datasets: QM9 (Ramakrishnan et al., 2014) and ZINC250k (Irwin et al., 2012). Following the previous work (Jo et al., 2022), we evaluate 10,000 generated molecules using five metrics: (a) validity (Val.), (b) neighborhood subgraph pairwise distance kernel (NSPDK), (c) Frechet ChemNet Distance (FCD), (d) uniqueness (Uniq.), and (e) novelty (Nov.). Note that NSPDK and FCD are measured between the generated samples and the test set. The validity, uniqueness, and novelty metrics are measured within the generated samples.\nBaselines. We compare HGGT with eight deep graph generative models: EDP-GNN (Niu et al., 2020), MoFlow (Zang & Wang, 2020), GraphAF (Shi et al., 2020), GraphDF (Luo et al., 2021), GraphEBM (Liu et al., 2021), GDSS (Jo et al., 2022), DiGress (Vignac et al., 2022), and GDSM(Luo et al., 2022). We provide a detailed implementation description in Appendix E.\nResults. The experimental results are reported in Table 2. We observe that HGGT showed competitive results on all the baselines on most of the metrics. The results suggest that the model can generate chemically valid features, i.e., atom types, accordingly, along with the structure of the graphs. In particular, for the ZINC250k dataset, we observe a large gap between our method and the baselines in NSPDK and FCD scores while showing competitive performance in the other metrics. Since FCD and NSPDK measure the similarity between molecular features and subgraph structures, respectively, HGGT can generate similar features and subgraphs observed in the real molecules."
        },
        {
            "heading": "5.3 ABLATION STUDIES",
            "text": "Time complexity. We conduct experiments to measure the inference time of the proposed algorithm. The results are presented in the upper left table of Figure 7, where we report the time to generate\na single sample. We can observe that HGGT generates a graph faster than the others due to the simplified representation.\nAdjacency matrix orderings. It is clear that the choice of node ordering influences the size of K2\u2013tree. We validate our choice of Cuthill-McKee (C-M) ordering (Cuthill & McKee, 1969) by comparing its compression ratio to other node orderings: breadth-first search (BFS) and depth-first search (DFS). The compression ratio is defined as the number of elements in K2\u2013tree divided by N2. In the left below table of Figure 7, we present the compression ratios for each node ordering. One can observe that C-M ordering shows the best ratio in all the datasets compared to others.\nPositional encoding. In this experiment, we assess the impact of various positional encodings in our method. We compare our tree positional encoding (TPE) to absolute positional encoding (APE) (Vaswani et al., 2017) and relative positional encoding (RPE) (Shaw et al., 2018) on the Planar dataset. Our findings, as presented in the right figure of Figure 7, demonstrate that TPE outperforms other positional encodings with faster convergence of training loss. These observations highlight the importance of appropriate positional encoding for generating high-quality graphs.\nAblation of algorithmic components. We introduce three components to enhance the performance of HGGT: grouping into tokens (Group), incorporating tree positional encoding (TPE), and pruning the K2\u2013tree (Prune). To verify the effectiveness of each component, we present the experimental results for our method with incremental inclusion of these components. The experimental results are reported in Table 3. The results demonstrate the importance of each component in improving graph generation performance, with grouping being particularly crucial, thereby validating the significance of our additional components to the sequential K2\u2013tree representation."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we presented a novel K2\u2013tree-based graph generative model (HGGT) which enables a compact, hierarchical, and domain-agnostic generation. Our experimental evaluation demonstrated state-of-the-art performance across various graph datasets. An interesting avenue for future work is the broader examination of other graph representations to graph generation, e.g., a plethora of representations (Boldi et al., 2009; Larsson & Moffat, 2000).\nReproducibility All experimental code related to this paper is available at https://github. com/yunhuijang/HGGT. Detailed insights regarding the experiments, encompassing dataset and model specifics, are available in Section 5. For intricate details like hyperparameter search, consult Appendix D. In addition, the reproduced dataset for each baseline is in Appendix E.\nAcknowledgements This work partly was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. IITP-2019-0-01906, Artificial Intelligence Graduate School Program (POSTECH)), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2022R1C1C1013366), Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2022R1A6A1A0305295413, 2021R1C1C1011375), and the Technology Innovation Program (No. 20014926, Development of BIT Convergent AI Architecture, Its Validation and Candidate Selection for COVID19 Antibody, Repositioning and Novel Synthetic Chemical Therapeutics) funded by the Ministry of Trans, Industry & Energy (MOTIE, Korea)."
        },
        {
            "heading": "A CONSTRUCTION OF A K2\u2013TREE FROM THE GRAPH",
            "text": "Algorithm 1 K2\u2013tree construction Input:Adjacency matrix A and partitioning factor K.\n1: Initialize the tree T \u2190 (V, E) with V = \u2205, E = \u2205. \u25b7 K2\u2013tree. 2: Initialize an empty queue Q. \u25b7 Candidates to be expanded into child nodes. 3: Set V \u2190 V \u222a {r}, xr \u2190 1 and let A(r) \u2190 A. Insert r into the queue Q. \u25b7 Add root node r. 4: while Q \u0338= \u2205 do 5: Pop u from Q. 6: if xu = 0 then \u25b7 Condition for not expanding the node u. 7: Go to line 4. 8: end if 9: Update s\u2190 dim(A(u))/K 10: for i = 1, . . . ,K do \u25b7 Row-wise indices. 11: for j = 1, . . . ,K do \u25b7 Column-wise indices. 12: Set Bi,j \u2190 A(u)[(i\u2212 1)s : is, (j \u2212 1)s : js]. \u25b7 Operation to obtain s\u00d7 s submatrix Bi,j of A(u). 13: If Bi,j is filled with zeros, set xv \u2190 1. Otherwise, set xv \u2190 0. \u25b7 Update tree-node attribute. 14: If dim(vi,j) > 1, update Q \u2190 vi,j . 15: end for 16: end for 17: Set V \u2190 V \u222a {v1,1, . . . , vK,K}. \u25b7 Update tree nodes. 18: Set E \u2190 E \u222a {(u, v1,1), . . . , (u, vK,K)}. \u25b7 Update tree edges. 19: end while\nOutput: K2\u2013tree (T ,X ) where X = {xu : u \u2208 V}.\nIn this section, we explain our algorithm to construct a K2\u2013tree (T ,X ) from a given graph G = A where G is a symmetric non-featured graph and A is an adjacency matrix. Note that he K2-ary tree T = (V, E) is associated with binary node attributes X = {xu : u \u2208 V}. In addition, let dim(A) to denote the number of rows(or columns) n of the square matrix A \u2208 {0, 1}n\u00d7n. We describe the full procedure in Algorithm 1. Note that the time complexity of the procedure is O(N2) (Brisaboa et al., 2009), where N denotes the number of nodes in the graph G."
        },
        {
            "heading": "B CONSTRUCTING A GRAPH FROM THE K2\u2013TREE",
            "text": "Algorithm 2 Graph G construction\nInput: K2\u2013tree (T ,X ) and partitioning factor K. Set m\u2190 KDT . \u25b7 Full adjacency matrix size. Initialize A \u2208 {0, 1}m\u00d7m with zeros. for u \u2208 L do \u25b7 For each leaf node with xu = 1.\npos(u) = ((iv1 , jv1), . . . , (ivL , jvL)). \u25b7 Position of node u. (pu, qu) = ( \u2211L \u2113=1 K L\u2212\u2113(iv\u2113 \u2212 1) + 1, \u2211L \u2113=1 K L\u2212\u2113(jv\u2113 \u2212 1) + 1). \u25b7 Location of node u.\nSet Apu,qu \u2190 1. end for Output: Adjacency matrix A.\nWe next describe the algorithm to generate a graph G = A given the K2\u2013tree (T ,X ) with tree depth DT . Let L \u2282 V be the set of leaf nodes in K2\u2013tree with node attributes 1. Note that we represent the tree-node position of u \u2208 V as pos(u) = ((iv1 , jv1), . . . , (ivL , jvL)) based on a downward path v0, v1, . . . , vL from the root node r = v0 to the tree-node u = vL. In addition, the location of corresponding submatrix A(u) is denoted as (pu, qu) = ( \u2211L \u2113=1 K L\u2212\u2113(iv\u2113 \u2212 1) + 1, \u2211L \u2113=1 K L\u2212\u2113(jv\u2113 \u2212 1) + 1) in as described in Section 4.1. We describe the full procedure as in Algorithm 2. Note that the time complexity of the procedure is O(N2), where N denotes the number of nodes in the graph G, since it requires querying for each element in the adjacency matrix."
        },
        {
            "heading": "C GENERALIZING K2\u2013TREE TO ATTRIBUTED GRAPHS",
            "text": "Algorithm 3 Featured K2\u2013tree construction Input: Modified adjacency matrix A and partitioning factor K.\n1: Initialize the tree T \u2190 (V, E) with V = \u2205, E = \u2205. \u25b7 Featured K2\u2013tree. 2: Initialize an empty queue Q. \u25b7 Candidates to be expanded into child nodes. 3: Set V \u2190 V \u222a {r}, xr \u2190 1 and let A(r) \u2190 A. Insert r into the queue Q. \u25b7 Add root node r. 4: while Q \u0338= \u2205 do 5: Pop u from Q. 6: if xu = 0 then \u25b7 Condition for not expanding the node u. 7: Go to line 4. 8: end if 9: Update s\u2190 dim(A(u))/K 10: for i = 1, . . . ,K do \u25b7 Row-wise indices. 11: for j = 1, . . . ,K do \u25b7 Column-wise indices. 12: Set Bi,j \u2190 A(u)[(i\u2212 1)s : is, (j \u2212 1)s : js]. \u25b7 Operation to obtain s\u00d7 s submatrix Bi,j of A(u). 13: if Bi,j is filled with zeros then \u25b7 Update tree-node attribute. 14: Set xv \u2190 0. 15: else if |Bi,j | > 1 then \u25b7 Non-leaf tree-nodes with attribute 1. 16: Set xv \u2190 1. 17: else \u25b7 Leaf tree-nodes with node features and edge features. 18: Set xv \u2190 Bi,j . \u25b7 We treat 1\u00d7 1 matrix Bi,j as a scalar. 19: end if 20: if dim(Bi,j) > 1 then Q \u2190 vi,j . 21: end if 22: end for 23: end for 24: Set V \u2190 V \u222a {v1,1, . . . , vK,K}. \u25b7 Update tree nodes. 25: Set E \u2190 E \u222a {(u, v1,1), . . . , (u, vK,K)}. \u25b7 Update tree edges. 26: end while\nOutput: Featured K2\u2013tree (T ,X ) where X = {xu : u \u2208 V}.\nAlgorithm 4 Featured graph G construction\n1: Input: Featured K2\u2013tree (T ,X ) and partitioning factor K. 2: m\u2190 KDT \u25b7 Full adjacency matrix size. 3: Initialize A \u2208 {0, 1}m\u00d7m with zeros. 4: for u \u2208 L do \u25b7 For each leaf node with xu \u0338= 0. 5: pos(u) = ((iv1 , jv1), . . . , (ivL , jvL)). \u25b7 Position of node u. 6: (pu, qu) = ( \u2211L \u2113=1 K L\u2212\u2113(iv\u2113 \u2212 1) + 1, \u2211L \u2113=1 K L\u2212\u2113(jv\u2113 \u2212 1) + 1). \u25b7 Location of node u. 7: Set Apu,qu \u2190 xu. 8: end for 9: Output: Modified adjacency matrix A.\nIn this section, we describe a detailed process to construct a K2\u2013tree for featured graphs with node features and edge features (e.g., molecular graphs), which is described briefly in Section 4.1. We modify the original adjacency matrix by incorporating categorical features into each element, thereby enabling the derivation of the featured K2\u2013tree from the modified adjacency matrix.\nEdge features. Integrating edge features into the adjacency matrix is straightforward. It can be accomplished by simply replacing the ones with the appropriate categorical edge features.\nNode features. Integrating node features into the adjacency matrix is more complex than that of edge features since the adjacency matrix only describes the connectivity between node pairs. To address this issue, we assume that all graph nodes possess self-loops, which leads to filling ones to the diagonal elements. Then we replace ones on the diagonal with categorical node features that correspond to the respective node positions.\nLet xu \u2208 X be the non-binary tree-node attributes that include node features and edge features and L be the set of leaf nodes in K2\u2013tree with non-zero node attributes. Then we can construct a featured K2\u2013tree with a modified adjacency matrix and construct a graph G from the featured K2\u2013tree as described in Algorithm 3 and Algorithm 4, respectively."
        },
        {
            "heading": "D EXPERIMENTAL DETAILS",
            "text": "In this section, we provide the details of the experiments. Note that we chose k = 2 in all experiments and provide additional experimental results for k = 3 in Appendix G."
        },
        {
            "heading": "D.1 GENERIC GRAPH GENERATION",
            "text": "We used the same split with GDSS (Jo et al., 2022) for Community-small, Enzymes, and Grid datasets. Otherwise, we used the same split with SPECTRE (Luo et al., 2022) for the Planar dataset. We fix k = 2 and perform the hyperparameter search to choose the best learning rate in {0.0001, 0.0002, 0.0005, 0.001} and the best dropout rate in {0, 0.1}. We select the model with the best MMD with the lowest average of three graph statistics: degree, clustering coefficient, and orbit count. Finally, we provide the hyperparameters used in the experiment in Table 6."
        },
        {
            "heading": "D.2 MOLECULAR GRAPH GENERATION",
            "text": "The statistics of training molecular graphs (i.e., QM9 and ZINC250k datasets) are summarized in Table 5 and we used the same split with GDSS (Jo et al., 2022) for a fair evaluation. We fix k = 2 and perform the hyperparameter search to choose the best number of layers in {2, 3} and select the model with the best validity. In addition, we provide the hyperparameters used in the experiment in Table 6.\nE IMPLEMENTATION DETAILS"
        },
        {
            "heading": "E.1 COMPUTING RESOURCES",
            "text": "We used PyTorch (Paszke et al., 2019) to implement HGGT and train the Transformer (Vaswani et al., 2017) models on a single GeForce RTX 3090 GPU.\nE.2 MODEL ARCHITECTURE\nWe describe the architecture of the proposed transformer generator of HGGT in Figure 8. The generator takes a sequential representation of K2\u2013tree as input and generates the output probability of each token as described in Section 4.2. The model consists of a token embedding layer, transformer encoder(s), and multilayer perceptron layer with tree positional encoding."
        },
        {
            "heading": "E.3 DETAILS FOR BASELINE IMPLEMENTATION",
            "text": "Generic graph generation. The baseline results from prior works are as follows. Results for GraphVAE (Simonovsky & Komodakis, 2018), GraphRNN (You et al., 2018), GNF (Liu et al., 2019), EDPGNN (Niu et al., 2020), GraphAF (Shi et al., 2020),\nGraphDF (Luo et al., 2021), and GDSS (Jo et al., 2022) are obtained from GDSS, while the results for GRAN (Liao et al., 2019), SPECTRE (Martinkus et al., 2022), and GDSM (Luo et al., 2022) are derived from their respective paper. Additionally, we reproduced DiGress (Vignac et al., 2022) and GraphGen (Goyal et al., 2020) using their open-source codes. We used original hyperparameters when the original work provided them. DiGress takes more than three days for the Planar, Enzymes, and Grid datasets, so we report the results from fewer epochs after convergence.\nMolecular graph generation. The baseline results from prior works are as follows. The results for EDP-GNN (Niu et al., 2020), MoFlow (Zang & Wang, 2020), GraphAF (Shi et al., 2020), GraphDF (Luo et al., 2021), GraphEBM (Liu et al., 2021), and GDSS (Jo et al., 2022) are from GDSS, and the GDSM (Luo et al., 2022) result is extracted from the corresponding paper. Moreover, we reproduced DiGress (Vignac et al., 2022) using their open-source codes."
        },
        {
            "heading": "E.4 DETAILS FOR THE IMPLEMENTATION",
            "text": "We adapted node ordering code from (Diamant et al., 2023), evaluation scheme from (Jo et al., 2022; Martinkus et al., 2022), and NSPDK computation from (Goyal et al., 2020)."
        },
        {
            "heading": "F GENERATED SAMPLES",
            "text": "In this section, we provide the visualizations of the generated graphs for generic and molecular graph generation."
        },
        {
            "heading": "F.1 GENERIC GRAPH GENERATION",
            "text": "Community-small\nWe present visualizations of graphs from the training dataset and generated samples from GraphGen, DiGress, GDSS, and HGGT in Figure 9, Figure 10, Figure 11, and Figure 12. Note that we reproduced GraphGen and DiGress using open-source codes while utilizing the provided checkpoints for GDSS. However, given that the checkpoints provided for GDSS do not include the Planar dataset, we have omitted GDSS samples for this dataset. We additionally give the number of nodes and edges of each graph.\nEnzymes\nGrid"
        },
        {
            "heading": "F.2 MOLECULAR GRAPH GENERATION",
            "text": "We present visualizations of generated molecules from HGGT in Figure 13 and Figure 14. Note that the 24 molecules are non-cherry-picked and randomly sampled."
        },
        {
            "heading": "G ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "In this section, we report additional experimental results."
        },
        {
            "heading": "G.1 GENERIC GRAPH GENERATION",
            "text": "We provide generic graph generation results for k = 3. Increasing k decreases the sequence length, while vocabulary size increases to 23 2 + 26 = 578.\nWe used Community-small and Planar datasets and measured MMD between the test graphs and generated graphs. We perform the same hyperparameter search for a fair evaluation as k = 2. The results are in Figure 15. We can observe that HGGT still outperforms the baselines even with different k."
        },
        {
            "heading": "G.2 MOLECULAR GRAPH GENERATION",
            "text": "We additionally report seven metrics of the generated molecules: (a) fragment similarity (Frag.), which measures the BRICS fragment frequency similarity between generated molecules and test molecules, (b) internal diversity (Intdiv.), which measures the chemical diversity in generated molecules, (c) quantitative estimation of drug-likeness (QED), which measures the drug-likeness similarity between generated molecules and test molecules, (d) synthetic accessibility score (SA), which compares the synthetic accessibility between generated molecules and test molecules, (e) similarity to the nearest neighbor (SNN), an average of Tanimoto similarity between the fingerprint of a generated molecule and test molecule, (f) scaffold similarity (Scaf.), the Bemis-Murcko scaffold frequency similarity between generated molecules and test molecules, and (g) weight, the atom weight similarity between generated molecules and test molecules. The results are in Table 7."
        },
        {
            "heading": "H DISCUSSION",
            "text": ""
        },
        {
            "heading": "H.1 HIERARCHY OF K2\u2013TREE REPRESENTATION",
            "text": "K2\u2013tree representation is hierarchical as it forms a parent-child hierarchy between nodes. In detail, each node in K2\u2013tree corresponds to a block (i.e., submatrix) in the adjacency matrix. Given a child and its parent node, the child node block is a submatrix of the parent block node, which enables K2\u2013tree to represent a hierarchical structure between the blocks. While this hierarchy may differ from the exact hierarchical community structure, the K2\u2013tree representation still represents a valid hierarchy present in the adjacency matrix. We also note that our K2\u2013tree representation should not be confused with the hierarchical representation learned by graph neural networks with pooling functions (Ying et al., 2018).\nNevertheless, bandwidth minimization algorithms (including C-M node ordering) often induce node orderings that align with underlying clusters. Prior works (Barik et al., 2020; Mueller, 2004) have empirically figured out that bandwidth minimization tends to cluster the points along the diagonal, which leads to a partial capture of the underlying community structure. This also supports our statement that K2\u2013tree representation is hierarchical."
        },
        {
            "heading": "H.2 COMPARISON WITH PRIOR WORKS ON AUTOREGRESSIVE GRAPH GENERATIVE MODEL",
            "text": "In this section, we compare HGGT to two prior works on autoregressive graph generative models: GraphRNN (You et al., 2018) and GRAN (Liao et al., 2019). The main difference comes from the key idea of HGGT: the ability to capture recurring patterns and the hierarchy of the adjacency matrix. In detail, HGGT maps the recurring patterns in the dataset (large zero-filled block matrices) into a simple object (a zero-valued node in the K2\u2013tree that the model can easily generate. This mapping allows the generative model to focus on learning instance-specific details rather than the generation of the whole pattern that is common across the dataset. In addition, K2\u2013tree representation can represent a valid hierarchy present in the adjacency matrix, as described in Appendix H.1\nComparison to GraphRNN. Both HGGT and GraphRNN reduce the representation size, which removes the burden of the graph generative models learning long-range dependencies. In detail, HGGT reduces the representation size by leveraging K2\u2013tree, pruning, and tokenization. Otherwise, GraphRNN employs BFS node ordering constraining the upper-corner elements of the adjacency matrix to be consecutively zero as described in Figure 16.\nDataset Rep. size N2\nComm. 48 400 Enzymes 238 15625 Planar 230 4696 Grid 706 130321\nThe reduction of HGGT is higher as shown in Table 8 that reports the average size of the representation empirically. Note that the representation size of HGGT and GraphRNN indicates the number of tokens and the number of elements limited by the maximum size of the BFS queue, respectively. While the comparison is not fair due to different vocabulary sizes, one could expect HGGT to suffer less from the long-range dependency problem due to the shorter representation.\nComparison to GRAN. The main difference between HGGT and GRAN comes from the different generated representations. HGGT generates K2\u2013tree representation with large zero-filled blocks, which is further summarized into a single node in the K2\u2013tree while GRAN generates the conventional adjacency matrix. Notably, the block of nodes of GRAN is solely used for parallel decoding, which is conceptually irrelevant to the graph representation.\nIn addition, the concept of block and the decoding process of the blocks differ in both methods. On one hand, the square-shaped HGGT block defines connectivity between a pair of equally-sized node sets. HGGT sequentially specifies these block matrix elements in a hierarchical way, i.e., first specifying whether the whole block matrix is filled with zeros and then specifying its smaller submatrices. On the other hand, the rectangular-shaped GRAN block defines connectivity between a set of newly added nodes and the existing nodes. GRAN can optionally decode the block matrix elements in parallel, speeding up the decoding process at the cost of lower performance."
        }
    ],
    "year": 2024
}