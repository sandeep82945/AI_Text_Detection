{
    "abstractText": "Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language. However, scalable representation for 3D objects and scenes is relatively unexplored. In this work, we present Uni3D, a 3D foundation model to explore the unified 3D representation at scale. Uni3D uses a 2D initialized ViT end-to-end pretrained to align the 3D point cloud features with the image-text aligned features. Via the simple architecture and pretext task, Uni3D can leverage abundant 2D pretrained models as initialization and image-text aligned models as the target, unlocking the great potential of 2D models and scaling-up strategies to the 3D world. We efficiently scale up Uni3D to one billion parameters, and set new records on a broad range of 3D tasks, such as zero-shot classification, few-shot classification, openworld understanding and zero-shot part segmentation. We show that the strong Uni3D representation also enables applications such as 3D painting and retrieval in the wild. We believe that Uni3D provides a new direction for exploring both scaling up and efficiency of the representation in 3D domain.",
    "authors": [],
    "id": "SP:d270f5c2853748a9db99feb61b92e688cf61df09",
    "references": [
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Songhao Piao",
                "Furu Wei"
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Mehdi Cherti",
                "Romain Beaumont",
                "Ross Wightman",
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Cade Gordon",
                "Christoph Schuhmann",
                "Ludwig Schmidt",
                "Jenia Jitsev"
            ],
            "title": "Reproducible scaling laws for contrastive language-image learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jasmine Collins",
                "Shubham Goel",
                "Kenan Deng",
                "Achleshwar Luthra",
                "Leon Xu",
                "Erhan Gundogdu",
                "Xi Zhang",
                "Tomas F Yago Vicente",
                "Thomas Dideriksen",
                "Himanshu Arora"
            ],
            "title": "Abo: Dataset and benchmarks for real-world 3d object understanding",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas Funkhouser",
                "Matthias Nie\u00dfner"
            ],
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Matt Deitke",
                "Ruoshi Liu",
                "Matthew Wallingford",
                "Huong Ngo",
                "Oscar Michel",
                "Aditya Kusupati",
                "Alan Fan",
                "Christian Laforte",
                "Vikram Voleti",
                "Samir Yitzhak Gadre"
            ],
            "title": "Objaverse-xl: A universe of 10m+ 3d objects",
            "venue": "arXiv preprint arXiv:2307.05663,",
            "year": 2023
        },
        {
            "authors": [
                "Matt Deitke",
                "Dustin Schwenk",
                "Jordi Salvador",
                "Luca Weihs",
                "Oscar Michel",
                "Eli VanderBilt",
                "Ludwig Schmidt",
                "Kiana Ehsani",
                "Aniruddha Kembhavi",
                "Ali Farhadi"
            ],
            "title": "Objaverse: A universe of annotated 3d objects",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Yuxin Fang",
                "Wen Wang",
                "Binhui Xie",
                "Quan Sun",
                "Ledell Wu",
                "Xinggang Wang",
                "Tiejun Huang",
                "Xinlong Wang",
                "Yue Cao"
            ],
            "title": "Eva: Exploring the limits of masked visual representation learning at scale",
            "venue": "arXiv preprint arXiv:2211.07636,",
            "year": 2022
        },
        {
            "authors": [
                "Yuxin Fang",
                "Quan Sun",
                "Xinggang Wang",
                "Tiejun Huang",
                "Xinlong Wang",
                "Yue Cao"
            ],
            "title": "Eva-02: A visual representation for neon genesis",
            "venue": "arXiv preprint arXiv:2303.11331,",
            "year": 2023
        },
        {
            "authors": [
                "Huan Fu",
                "Rongfei Jia",
                "Lin Gao",
                "Mingming Gong",
                "Binqiang Zhao",
                "Steve Maybank",
                "Dacheng Tao"
            ],
            "title": "3d-future: 3d furniture shape with texture",
            "venue": "International Journal of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Eleni Gregoromichelaki",
                "Arash Eshghi",
                "Christine Howes",
                "Gregory J Mills",
                "Ruth Kempson",
                "Julian Hough",
                "Patrick GT Healey",
                "Matthew Purver"
            ],
            "title": "Language and cognition as distributed process interactions",
            "venue": "In Proceedings of the 26th Workshop on the Semantics and Pragmatics of Dialogue,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Deepti Hegde",
                "Jeya Maria Jose Valanarasu",
                "Vishal M Patel"
            ],
            "title": "Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition",
            "venue": "arXiv preprint arXiv:2303.11313,",
            "year": 2023
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Tianyu Huang",
                "Bowen Dong",
                "Yunhan Yang",
                "Xiaoshui Huang",
                "Rynson WH Lau",
                "Wanli Ouyang",
                "Wangmeng Zuo"
            ],
            "title": "Clip2point: Transfer clip to point cloud classification with image-depth pre-training",
            "venue": "arXiv preprint arXiv:2210.01055,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoshui Huang",
                "Sheng Li",
                "Wentao Qu",
                "Tong He",
                "Yifan Zuo",
                "Wanli Ouyang"
            ],
            "title": "Frozen clip model is efficient point cloud backbone",
            "venue": "arXiv preprint arXiv:2212.04098,",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Weixian Lei",
                "Yixiao Ge",
                "Jianfeng Zhang",
                "Dylan Sun",
                "Kun Yi",
                "Ying Shan",
                "Mike Zheng Shou"
            ],
            "title": "Vit-lens: Towards omni-modal representations",
            "venue": "arXiv preprint arXiv:2308.10185,",
            "year": 2023
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang"
            ],
            "title": "Grounded language-image pre-training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yanghao Li",
                "Haoqi Fan",
                "Ronghang Hu",
                "Christoph Feichtenhofer",
                "Kaiming He"
            ],
            "title": "Scaling language-image pre-training via masking",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Minghua Liu",
                "Ruoxi Shi",
                "Kaiming Kuang",
                "Yinhao Zhu",
                "Xuanlin Li",
                "Shizhong Han",
                "Hong Cai",
                "Fatih Porikli",
                "Hao Su"
            ],
            "title": "Openshape: Scaling up 3d shape representation towards open-world understanding",
            "venue": "arXiv preprint arXiv:2305.10764,",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Xu Ma",
                "Can Qin",
                "Haoxuan You",
                "Haoxi Ran",
                "Yun Fu"
            ],
            "title": "Rethinking network design and local geometry in point cloud: A simple residual mlp framework",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Amit H Bermano"
            ],
            "title": "Clipcap: Clip prefix for image captioning",
            "venue": "arXiv preprint arXiv:2111.09734,",
            "year": 2021
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Zekun Qi",
                "Runpei Dong",
                "Guofan Fan",
                "Zheng Ge",
                "Xiangyu Zhang",
                "Kaisheng Ma",
                "Li Yi"
            ],
            "title": "Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining",
            "venue": "arXiv preprint arXiv:2302.02318,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Samyam Rajbhandari",
                "Jeff Rasley",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "Zero: Memory optimizations toward training trillion parameter models",
            "venue": "In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Guangyi Chen",
                "Yansong Tang",
                "Zheng Zhu",
                "Guan Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Denseclip: Language-guided dense prediction with context-aware prompting",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Rasley",
                "Samyam Rajbhandari",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Quan Sun",
                "Yuxin Fang",
                "Ledell Wu",
                "Xinlong Wang",
                "Yue Cao"
            ],
            "title": "Eva-clip: Improved training techniques for clip at scale",
            "venue": "arXiv preprint arXiv:2303.15389,",
            "year": 2023
        },
        {
            "authors": [
                "Mikaela Angelina Uy",
                "Quang-Hieu Pham",
                "Binh-Son Hua",
                "Thanh Nguyen",
                "Sai-Kit Yeung"
            ],
            "title": "Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Hanchen Wang",
                "Qi Liu",
                "Xiangyu Yue",
                "Joan Lasenby",
                "Matt J Kusner"
            ],
            "title": "Unsupervised point cloud pre-training via occlusion completion",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yue Wang",
                "Yongbin Sun",
                "Ziwei Liu",
                "Sanjay E Sarma",
                "Michael M Bronstein",
                "Justin M Solomon"
            ],
            "title": "Dynamic graph cnn for learning on point clouds",
            "venue": "ACM Transactions on Graphics (tog),",
            "year": 2019
        },
        {
            "authors": [
                "Zhirong Wu",
                "Shuran Song",
                "Aditya Khosla",
                "Fisher Yu",
                "Linguang Zhang",
                "Xiaoou Tang",
                "Jianxiong Xiao"
            ],
            "title": "3d shapenets: A deep representation for volumetric shapes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Le Xue",
                "Mingfei Gao",
                "Chen Xing",
                "Roberto Mart\u0131\u0301n-Mart\u0131\u0301n",
                "Jiajun Wu",
                "Caiming Xiong",
                "Ran Xu",
                "Juan Carlos Niebles",
                "Silvio Savarese"
            ],
            "title": "Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Le Xue",
                "Ning Yu",
                "Shu Zhang",
                "Junnan Li",
                "Roberto Mart\u0131\u0301n-Mart\u0131\u0301n",
                "Jiajun Wu",
                "Caiming Xiong",
                "Ran Xu",
                "Juan Carlos Niebles",
                "Silvio Savarese"
            ],
            "title": "Ulip-2: Towards scalable multimodal pre-training for 3d understanding",
            "venue": "arXiv preprint arXiv:2305.08275,",
            "year": 2023
        },
        {
            "authors": [
                "Zhao Yang",
                "Jiaqi Wang",
                "Yansong Tang",
                "Kai Chen",
                "Hengshuang Zhao",
                "Philip HS Torr"
            ],
            "title": "Lavt: Language-aware vision transformer for referring image segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Li Yi",
                "Vladimir G Kim",
                "Duygu Ceylan",
                "I-Chao Shen",
                "Mengyan Yan",
                "Hao Su",
                "Cewu Lu",
                "Qixing Huang",
                "Alla Sheffer",
                "Leonidas Guibas"
            ],
            "title": "A scalable active framework for region annotation in 3d shape collections",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2016
        },
        {
            "authors": [
                "Tianwei Yin",
                "Xingyi Zhou",
                "Philipp Krahenbuhl"
            ],
            "title": "Center-based 3d object detection and tracking",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xumin Yu",
                "Yongming Rao",
                "Ziyi Wang",
                "Zuyan Liu",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Pointr: Diverse point",
            "year": 2024
        },
        {
            "authors": [
                "2020. Junsheng Zhou",
                "Baorui Ma",
                "Yu-Shen Liu",
                "Yi Fang",
                "Zhizhong Han"
            ],
            "title": "Learning consistency-aware",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "3D representation learning is one of the most fundamental problems in 3D computer vision, especially with the rapid development of 3D sensors (e.g., LiDAR) and the growing demands in realworld applications, e.g., autonomous driving, augmented/virtual reality and robotics. Existing methods make great progress in 3D model architecture (Qi et al., 2017a;b; Yu et al., 2021; Wang et al., 2019), learning objective (Yu et al., 2022; Wang et al., 2021), task-oriented modeling (Zhou et al., 2020; Yin et al., 2021; Zhao et al., 2021), etc. However, most of the works explore at a relatively small scale, with limited parameters, data, and task scenarios. Learning scalable 3D representation that can transfer in the wild is relatively unexplored and remains a challenging problem.\nIn the past few years, scaling up pre-trained language models (Brown et al., 2020; Liu et al., 2019; Raffel et al., 2020) has largely revolutionized natural language processing. Some recent works (Radford et al., 2021; Dosovitskiy et al., 2020; Bao et al., 2021; He et al., 2022; Fang et al., 2022) translate the progress from language to 2D vision via model and data scaling. Motivated by their success, it is appealing that we can also lift this success from 2D to 3D, i.e., to learn a scalable 3D representation model that can transfer in the 3D world. Recently, as the release of a large-scale 3D dataset Objaverse (Deitke et al., 2023b), a few works have tried to explore scalable pretraining in 3D, but either still limit to the small-scale 3D backbones (Xue et al., 2023a;b), or can hardly scale to a relatively larger size (Liu et al., 2023), e.g., 72M in Fig. 1.\nIn this work, we propose Uni3D, a unified and scalable 3D pretraining framework for large-scale 3D representation learning, and explore its limits at the scale of one billion parameters with a million 3D shapes and 10 million images paired with 70 million texts. Uni3D uses a 2D ViT as the 3D encoder initialized with the best 2D prior, which is then end-to-end pre-trained to align the 3D point cloud features with the image-text aligned features. Via the simple architecture and pretext task, Uni3D can leverage abundant 2D pre-trained models as initialization (Fang et al., 2022; Caron et al., 2021), and image-text aligned models as the target (Radford et al., 2021; Sun et al., 2023; Cherti et al., 2023), unlocking the great potential of 2D models and scaling-up strategies to the 3D world.\nIn addition, we systematically study the scalability and flexibility of Uni3D in terms of 1) model scaling from 6M to 1B parameters, 2) 2D initialization from visual self-supervised to text supervised,\nand 3) text-image aligned target model from 150M to 5B parameters. We observe continuous performance improvements as the scaling of each component under the flexible and unified framework. The sharable 2D prior and scale-up strategies also largely benefit the large-scale 3D representation learning.\nFor the first time, we demonstrate a billion-scale 3D representation model that transfers well to various downstream tasks and scenarios. As shown in Fig. 2, Uni3D yields a boost compared to prior art in various zero-shot and few-shot 3D tasks. Specifically, Uni3D achieves a zero-shot classification accuracy of 88.2% on ModelNet, which surprisingly performs on par with some supervision methods. Uni3D also achieves state-of-the-art performance on other representative 3D tasks such as open-world understanding, part segmentation, etc. In addition, we present some interesting applications with the strong 3D representation learned by Uni3D, such as point cloud painting and text/image-based 3D shape retrieval.\nBy scaling up 3D foundation models with a simple and unified pre-training to learn strong 3D representation across tasks, we hope Uni3D would bridge the gap between 2D and 3D vision, and contribute to the big convergence across different modalities. To facilitate future research, we will release all the code and 3D foundation models."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "3D Representation Learning. Learning representations from point clouds for 3D understanding (Qi et al., 2017a;b; Wang et al., 2019; Yu et al., 2021) has been fully explored in recent years. Some works further studied self-supervised pretraining for point clouds by specific 3D pretext tasks like self-reconstruction (Zhou et al., 2022), mask point modeling (Yu et al., 2022) and contrastive learning (Qi et al., 2023). These works merely explore under limited 3D data (e.g. ShapeNet (Chang et al., 2015)) and do not investigate multi-modal representation from 2D/NLP to 3D.\nWith the recent success in learning visual concepts from raw text with contrastive learning like CLIP (Radford et al., 2021; Jia et al., 2021; Li et al., 2022; Ramesh et al., 2022; Gregoromichelaki et al., 2022), recent works (Liu et al., 2023; Qi et al., 2023; Xue et al., 2023a; Hegde et al., 2023; Lei et al., 2023) seek to learn 3D representations by aligning text, image, and point cloud features through in a similar contrastive learning way. Recently, as the release of a large-scale 3D dataset Objaverse (Deitke et al., 2023b), OpenShape (Liu et al., 2023) and ULIP2 (Xue et al., 2023b) have tried to explore scalable pretraining in 3D, but either still limit to the small-scale 3D backbones (Xue et al., 2023b), or can hardly scale to a relatively larger size (Liu et al., 2023). In this work, We aim to explore a unified and scalable 3D pretraining framework, i.e., Uni3D, for large-scale 3D representation learning and explore its limits in billion-scale model sizes.\nFoundation models. Recently, it has been drawing significant attention to design foundation models for unifying and scaling up representations under different modalities (e.g. NLP, 2D vision). Starting from NLP, recent works in scaling up pre-trained language models (Brown et al., 2020; Liu et al., 2019; Raffel et al., 2020) have largely revolutionized natural language processing. Some research in 2D vision (Radford et al., 2021; Dosovitskiy et al., 2020; Bao et al., 2021; He et al., 2022; Fang et al., 2022) translates the progress from language to 2D vision via model and data scaling. However, such a phenomenon has not been well-established and explored in the 3D domain, due to the limited 3D data and difficulties in unifying and scaling up 3D backbones. Meta-Transformer (Zhang et al., 2023) and FrozeCLIP (Huang et al., 2022b) have indicated a promising future for developing a unified framework with a modality-shared encoder. However, they require retraining task-specific heads with labor-intensive manual labeling of ground truth for different downstream tasks, which leads to a lack of out-of-domain zero-shot capabilities. In this work, we design the first billion-scale 3D foundation model with a unified 3D representation. The unified ViT architecture allows us to simply scale up Uni3D with the well-studied unified 2D/NLP scaling-up strategies. We anticipate Uni3D to serve as a bridge between 2D and 3D vision, facilitating significant convergence across various modalities."
        },
        {
            "heading": "3 METHOD",
            "text": "We introduce Uni3D, a unified and scalable 3D pretraining framework for large-scale 3D representation learning by aligning 3D point cloud features with the image-text aligned features. The overview of Uni3D is shown in Fig. 3. We first present how we design, scale up and initialize a unified 3D representation in Uni3D in Sec. 3.1. We then introduce the multi-modal contrastive learning for aligning image and language with point cloud in Sec. 3.2. More training details are provided in Sec. A of the appendix."
        },
        {
            "heading": "3.1 UNIFIED 3D REPRESENTATION",
            "text": "Uni3D leverages a unified vanilla transformer structurally equivalent to 2D Vision Transformer (ViT) (Dosovitskiy et al., 2020) as the backbone. The only difference here is that we replace the patch embedding layer in ViT with a specific point tokenizer to achieve 3D embeddings. The point tokenizer keeps the same as PointBERT (Yu et al., 2022) to first group points into local patches with FPS (farthest point sampling) and kNN (k nearest neighbor), and then extract token embeddings with a tiny PointNet (Qi et al., 2017a) for each patch. The vanilla transformer is then applied to the 3D tokens to extract the 3D representations.\nScaling Up Uni3D. Previous works on point cloud representation learning merely focus on designing specific model architectures for pursuing better performances in different applications and are limited to a certain small-scale dataset (e.g. ShapeNet (Chang et al., 2015), ModelNet (Wu et al., 2015)). With the recent successes in large-scale 3D data (e.g. Objaverse (Deitke et al., 2023b;a)), a few recent works (Xue et al., 2023a; Liu et al., 2023; Xue et al., 2023b) have tried to explore scalable pretraining in 3D, but either still limit to the small-scale 3D backbones (Xue et al., 2023a), or can hardly scale to a relatively larger size (Liu et al., 2023). The difficulties lie in the un-unified backbones and pretraining in 3D domain, where each backbone requires a specific scaling-up strategy, which is rarely explored. Moreover, some backbones (e.g. PointMLP (Ma et al., 2021), DGCNN (Wang et al., 2019)) require modeling local patterns completely on dense points, which brings extensive computational cost when scaling up.\nWe justify that Uni3D, which directly leverages the vanilla transformer structurally equivalent to ViT, can naturally solve the difficulties by simply scaling up the model size with the well-studied unified 2D/NLP scaling-up strategies. Specifically, we leverage the strategy of ViT which gradually scales up Transformer from Tiny (6 M), Small (23M), Base (88 M), Large (307 M) to giant (1B) and replace the Transformer of Uni3D with different sizes of ViT as the scaled-up version of Uni3D at different model sizes. The effectiveness and efficiency of our scaling-up strategy are fully demonstrated by the comprehensive exploration of scaling up ViT in the 2D vision domain. As shown in Fig. 1 and Tab. 5, we observe continuous performance improvements as the scaling of model size under the flexible and unified framework.\nGiven the unified scaling-up strategy, we train the largest 3D presentation model with one billion parameters under the multi-modal alignment learning objective, in a large-scale dataset of nearly one million 3D shapes, along with paired 10 million images and 70 million texts. For the first time, we demonstrate a billion-scale 3D representation model that transfers well to various downstream tasks and scenarios.\nInitializing Uni3D. Another challenge that prevents previous works in scaling up 3D backbones is that larger model sizes lead to overfitting and difficulties in convergence. A naive solution is to pretrain each 3D backbone with specific 3D pretext tasks (e.g. PointBERT (Yu et al., 2022), OcCo (Wang et al., 2021)), and leverage the pretrained parameters as the initialization. However, this results in expensive training costs, and the relatively limited scale of 3D data for pretraining makes it challenging to establish a robust initialization for stabilizing cross-modal contrastive learning.\nIn Uni3D, we directly leverage the vanilla transformer structurally equivalent to ViT as the 3D backbone, which brings a new perspective of introducing pretrained priors. Specifically, we can naturally adopt the pretrained large models in other modalities which share the same vanilla transformer as ours to initialize Uni3D, such as the 2D pretrained model DINO (Caron et al., 2021), EVA (Fang et al., 2022), EVA-02 (Fang et al., 2023) and the cross-modal models CLIP (Radford et al., 2021), EVA-CLIP (Sun et al., 2023), etc. These pretrained models are trained in datasets consisting of billions of images and texts, which already learn rich underlying representational abilities for Transformer and have the potential to enhance and stabilize the learning of large-scale 3D representations. Uni3D is not limited to a specific pretrained model for initialization, where we can flexibly leverage any off-the-shelf Transformer-based pretrained models at any modalities for pushing the performance and exploring the cross-modal pretraining (please refer to Sec. 4.7 for detailed analysis)."
        },
        {
            "heading": "3.2 MULTI-MODAL ALIGNMENT",
            "text": "We train Uni3D to learn the multi-modal alignment across language, image and point cloud following a similar paradigm as ULIP (Xue et al., 2023a) and OpenShape (Liu et al., 2023).\nDatasets. In order to keep the experimental settings consistent with other methods for a fair comparison, we adopt the ensembled 3D dataset provided by OpenShape for training, which consists of four 3D dataset, i.e., Objaverse (Deitke et al., 2023b), ShapeNet (Chang et al., 2015), 3D-FUTURE (Fu et al., 2021) and ABO (Collins et al., 2022). We sample 10,000 points from the mesh surface with colors and render 10 color images from different views that uniformly cover the whole shape. The point cloud-text-image triplets are conducted in the same way as OpenShape.\nObjective. The illustration of the multi-modal alignment is shown in Fig. 3. We initialize the Uni3D point encoder fP with pretrained 2D ViT models and obtain the text encoder fT and image encoder\nfI from CLIP models. We train fP to learn 3D representations by aligning them to well-learned 2D / Language representations of CLIP models and distills cross-modal knowledge. Both fI and fT are frozen since they are well-optimized, and only fP are learnable during training. Given a batch of N triplets {(Pi, Ii, Ti)}Ni=1, where Pi, Ii , Ti donate a point cloud and its corresponding image and text obtained from the same 3D shape. We first achieve the normalized feature for the sampled triplets as {(ePi = fP (Pi)/|fP (Pi)|, eIi = fI(Ii)/|fI(Pi)|, eTi = fT (Ti)/|fT (Ti)|)}Ni=1. The contrastive loss is then formulated as:\n\u2212 1\n4N N\u2211 i=1\n( log exp(ePi \u00b7 e T i /\u03c4)\u2211\nj exp(e P i \u00b7 eTj /\u03c4)\n+ log exp(eTi \u00b7 e P i /\u03c4)\u2211\nj exp(e T i \u00b7 ePj /\u03c4)\n+ log exp(ePi \u00b7 e I i /\u03c4)\u2211\nj exp(e P i \u00b7 eIj/\u03c4)\n+ log exp(eIi \u00b7 e P i /\u03c4)\u2211\nj exp(e I i \u00b7 ePj /\u03c4)\n) ,\n(1)\nwhere \u03c4 is a learnable temperature. The training target is to minimize the triplet contrastive loss.\nImage-Text Aligned Target. We further justify that Uni3D is not limited to a specific CLIP teacher, where we can switch it to off-the-shelf SoTA CLIP models with different model scales flexibly to achieve better performance. For example, we can simply change the CLIP source from OpenAICLIP (Radford et al., 2021), OpenCLIP (Cherti et al., 2023) to the best EVA-CLIP (Sun et al., 2023), and probably to the better CLIP in the future. We can also directly scale up the CLIP teacher from EVA-CLIP-B (150 M) to EVA-CLIP-E (5 B). This demonstrates the flexibility and scalability of Uni3D and shows the potential of Uni3D to progress with the progress of CLIP models."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "4.1 ZERO-SHOT SHAPE CLASSIFICATION",
            "text": "We first evaluate Uni3D under the zero-shot shape classification task. We conduct experiments under three benchmarks: ModelNet (Wu et al., 2015), ScanObjNN (Uy et al., 2019) and Objaverse-LVIS (Deitke et al., 2023b). ModelNet and ScanObjNN are widely-used datasets which contains 15 and 40 common categories, respectively. The Objaverse-LVIS benchmark is an annotated and cleaned subset of Objaverse which contains 46,832 shapes of 1,156 LVIS categories. We follow the settings of OpenShape (Liu et al., 2023) to conduct evaluations. For Objaverse-LVIS, we use 10,000 sampled colored points as input. For ModelNet40, we utilize 10,000 sampled points without color as input. For ScanObjNN, the input is 2,048 sampled points without color from the OBJ ONLY version. We compare Uni3D with the previous SoTA methods in the zero-shot shape classification task, such as PointCLIP (Zhang et al., 2022), PointCLIP V2 (Zhu et al., 2022), ULIP (Xue et al., 2023a) and OpenShape (Liu et al., 2023). Note that PointCLIP and PointCLIP V2 directly project point clouds into images and leverage 2D CLIP for classification, while other methods adopt a similar schema to train a native 3D backbone for aligning 3D representations with image and text representations produced by a pretrained CLIP. We follow OpenShape (Liu et al., 2023) to report the performance under two different training settings. \u201cEnsembled\u201d indicates that the backbones are trained under all the four datasets same as OpenShape and \u201cEnsembled (no LVIS)\u201d further excludes the shapes from the Objaverse-LVIS subset. We justify that even when LVIS shapes are included in the training shapes, i.e., the \u201cEnsembled\u201d dataset, their test-time category labels are probably not included in\nthe training texts. The quantitative comparison is shown in Tab. 1, where Uni3D significantly outperforms the previous state-of-the-art methods under different settings.\n4.2 FEW-SHOT LINEAR PROBING\nLinear probing is a widely used approach for evaluating the learned representation of a model. To evaluate the linear probing ability of Uni3D, we follow the common setting as OpenShape (Liu et al., 2023) to freeze the parameters of Uni3D and only train a linear classifier on few-shot class labels. We conduct few-shot linear probing under the difficult Objaverse-LVIS dataset with labeled training samples per class from 1, 2, 4, 8 to 16. Fig. 4 summarizes the performance of Uni3D in comparison with OpenShape (Liu et al., 2023) (PointBERT backbone and SparseConv backbone), ULIP (Xue et al., 2023a) (official release and the version retrained on the large ensembled dataset) and PointCLIP V2 (Zhu et al., 2022). Uni3D significantly outperforms all the other methods by a large margin under all the few-shot settings."
        },
        {
            "heading": "4.3 OPEN-WORLD UNDERSTANDING",
            "text": "To evaluate the capability of Uni3D in 3D understanding of real-world shapes and scenes, we follow CLIP2 (Zeng et al., 2023) to conduct experiments under ScanNet (Dai et al., 2017) to explore the zero-shot recognition performance of Uni3D under real-world scenarios. Note that the ground truth instant segmentation is available for all the methods and the target is to recognize the category of each instant of the scene in a zero-shot way. ScanNet (Dai et al., 2017) is a popular real-scanned 3D dataset containing 1.5K reconstructed meshes of real-world scenes. We adopt the same setting as CLIP2 to split classes and evaluate the results under the test set of ScanNet.\nWe compare our proposed Uni3D with the state-of-the-art methods PointCLIP (Zhang et al., 2022), PointCLIP V2 (Zhu et al., 2022), CLIP2Point (Huang et al., 2022a) and CLIP2 (Zeng et al., 2023). The quantitative comparison is shown in Tab. 2. \u201cPointCLIP w/TP\u201d and \u201cCLIP2Point w/TP\u201d donate training PointCLIP and CLIP2Point with the real-world data provided by CLIP2. Note that \u201cPointCLIP w/TP\u201d, \u201cCLIP2Point w/TP\u201d and CLIP2 are trained under 1.6M triplets of real-world point cloud-image-text samples, while Uni3D is only trained under available synthetic data. Nonetheless, Uni3D achieves the best performance among all the previous methods. The results demonstrate the capability of Uni3D to perform real-world recognition and understanding even without training under real-world data. The reason is that Uni3D distills some perceptions of the real world from the CLIP models which are trained under large-scale real-world images and text. Moreover, by scaling up model size, Uni3D achieves a larger representation bandwidth, leading to superior performance under difficult real-world rscenarios. The qualitative comparison is shown in Fig. 5, where Uni3D produces much more accurate zero-shot recognition results than PointCLIP V2 and CLIP2Point. We do not visually compare with CLIP2 since its code and model are not publicly available."
        },
        {
            "heading": "4.4 ZERO / ONE / TWO-SHOT PART SEGMENTATION",
            "text": "Some prior methods (Rao et al., 2022; Yang et al., 2022) have demonstrated that transferring the knowledge gained from image-text contrastive learning, i.e., CLIP, can yield significant performance improvements in 2D dense prediction tasks (e.g. segmentation and detection). However, transferring\nthis knowledge to 3D dense prediction tasks is barely explored. We propose a novel approach for 3D dense prediction with Uni3D, and justify the effectiveness with part segmentation experiment. For more details on the approach, please refer to Sec. B of the appendix.\nWe conduct part segmentation experiments under ShapeNetPart dataset (Yi et al., 2016) with Uni3DBase. The results in Tab. 3 demonstrate that when supervised with only 1 or 2 samples per class, Uni3D outperforms PointBERT by +13.3%/+9.8%. Moreover, we largely increase the training samples used for comparative methods to 10% or 20% of the training set. These settings surpass training samples in Uni3D\u2019s one-shot or two-shot settings by two orders of magnitude. Even in the face of such a discrepancy\nin the number of training samples, Uni3D still achieves the most outstanding performance in terms of overall mIoU. The visual comparisons with PointBERT are provided in Sec. B of the appendix.\n\u201cZero-shot transfer to unseen parts\u201d quantifies the ability of Uni3D to learn fine-grained semantic information of local point clouds during multi-modal contrastive pre-training. We partitioned the ShapeNetPart dataset into two subsets: \u201cSeen Categories\u201d and \u201cUnseen Categories.\u201d In the \u201cSeen Categories\u201d subset, the text of ground-truth part labels serve as training samples of Uni3D for learning part semantics, while in the \u201cUnseen Categories\u201d subset, the text of\nground-truth part labels is unseen during training and is only utilized for zero-shot testing. The superior performance of Uni3D in Tab. 4 demonstrates its ability to discern fine-grained 3D patterns in a zero-shot manner, even for part-level semantic concepts not encountered in the \u201cSeen Categories\u201d."
        },
        {
            "heading": "4.5 POINT CLOUD PAINTING",
            "text": "We propose to leverage the trained Uni3D for painting point clouds by exploring the learned 3D semantic patterns in Uni3D. Specifically, given an initial point cloud and an input prompt, we optimize the appearance, i.e., RGB channel of the point cloud, by maximizing the cosine similarity between the feature of the point cloud extracted by Uni3D and the feature of the prompt extracted with CLIP text encoder. The painting for a point cloud can be achieved within one minute in a single V100 GPU. We show the paintings in Fig. 6, where Uni3D successfully optimizes the point cloud by revealing complex semantics from the prompt. The results demonstrate that Uni3D has learned abundant and diverse 3D patterns via contrastive pretraining."
        },
        {
            "heading": "4.6 CROSS-MODAL RETRIEVAL",
            "text": "With the learned multi-modal representations of Uni3D, we can naturally retrieve 3D shapes from images or text. Specifically, we retrieve 3D shapes from the large 3D dataset (Deitke et al., 2023b)\nby calculating the cosine similarity between the embedding of a query image or a query text prompt and the embedding of 3D shapes. We then perform kNN to achieve the most similar 3D shapes of the query. In Fig. 7, we show that Uni3D successfully retrieves 3D shapes from real-world images. Note that the images for training are only renderings, and there is a big gap between the training images and the real-world images. We also take two images as inputs and retrieve the shape similar to both two images by calculating the cosine similarity between the average of the embedding of two images and the embedding of 3D shapes. The interesting results demonstrate that Uni3D learns a diverse 3D representation with the ability to perceive multiple 2D signals. We further show the results of leveraging Uni3D to retrieve 3D shapes from the input texts in Fig. 7. More visualization results are provided in Sec. C of the appendix."
        },
        {
            "heading": "4.7 ABLATION STUDY",
            "text": "We then conduct ablation studies to justify the effectiveness of each design in Uni3D. The default setting is to use the ViT-Base as the backbone with an initialization of EVA (Fang et al., 2022), and the default CLIP teacher is EVA-CLIP-E (Sun et al., 2023). The default data setting is \u201cEnsembled (no-LVIS)\u201d. We keep the default experimental setting during ablation studies except for the modified part described in each ablation experiment below.\nScaling Up Model Size. We first explore the effectiveness of scaling up the model size of Uni3D in Tab. 5. Since we leverage a unified vanilla transformer structurally equivalent to ViT as the foundational 3D representation model, we can simply scale up Uni3D with the well-studied unified 2D/NLP scaling-up strategies. Specifically, we follow the scaling up principles of the plain ViT (Dosovitskiy et al., 2020) to increase parameters from 6 M (Tiny), 23 M (Small), 88 M (Base), 307 M (Large) to 1 B (giant). The hyper-parameters on the model architecture are detailed in Tab. 5.\nTable 7: Initializing Uni3D with different pretrained large models.\nInit variant O-LVIS\nNone 44.8 DINO 45.0 EVA-CLIP 45.2 EVA 45.8\nEVA + Freeze ViT 15.7\nThe performance under different model scales demonstrates that scaling up the model size of Uni3D can significantly improve the 3D representation.\nSwitching / Scaling Up CLIP Teachers. We justify that Uni3D is a flexible framework where we can switch the off-the-shelf SoTA CLIP models as the teacher. To this end, we investigate the performances of Uni3D with different CLIP teachers at different scales. Specifically, we evaluate various CLIP models (e.g. OpenAI-CLIP (Radford et al., 2021), OpenCLIP (Cherti et al., 2023) and EVA-CLIP (Sun et al., 2023)), and also explore large scale CLIP models (e.g., OpenCLIP-bigG, EVA-CLIP-E). The quantitative comparison is shown in Tab. 6, with the best performance achieved by the largest CLIP model EVA-CLIP-E. The results show that the capability and model scale of CLIP teachers are key factors for achieving better performance in Uni3D. Moreover, it indicates the potential of Uni3D to progress with the progress of CLIP models by switching state-of-the-art CLIP teachers.\nInitializing Transformer. We further conduct ablation studies to explore the effectiveness of initializing Uni3D with 2D pretraining or multi-modal large models. In Tab. 7, we report the performance of training Uni3D from scratch (None) and initializing Uni3D with off-the-shelf 2D pretraining model DINO (Caron et al., 2021) / EVA (Fang et al., 2022) and SoTA CLIP model EVA-CLIP (Sun et al., 2023). The best performance is achieved by initializing Uni3D with the SoTA 2D pretraining model EVA (Fang et al., 2022). We also demonstrate that leveraging the frozen parameters from the 2D pretrained ViT model may fail to provide strong 3D understanding without fine-tuning, as shown in \u201cEVA + Freeze ViT\u201d of Tab. 7. For more analysis on initializing Uni3D, please refer to Sec. D of the appendix."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We present Uni3D, a unified framework that scales up a 3D representation model to one billion parameters. We directly leverage a unified vanilla transformer structurally equivalent to ViT as the model, which allows us to simply scale up Uni3D with the well-studied unified 2D/NLP scalingup strategies. Moreover, Uni3D can leverage abundant 2D pretrained models as initialization and image-text aligned models as the target, unlocking the great potential of 2D models and strategies to the 3D world. We train Uni3D under a large dataset containing about one million 3D point clouds, 10 million images and 70 million texts to explore powerful 3D representations by aligning the 3D point cloud features with the image-text aligned features. Uni3D achieves state-of-the-art performance in various 3D understanding tasks including zero-shot and few-shot classification, open-world understanding, zero-shot part segmentation, etc. We believe that Uni3D can serve as a 3D foundation model to enable many applications in the 3D community."
        },
        {
            "heading": "A TRAINING DETAILS",
            "text": "We freeze the CLIP text and image encoders while focusing on training the 3D encoder utilizing the cross-modal contrastive loss. We employ the Adam (Kingma & Ba, 2014) optimizer with a peak learning rate of 1e-3 that gradually decreases following a cosine learning rate schedule. To enhance training stability, we adopt stochastic depth (Huang et al., 2016) regularization. We also leverage the FLIP (Li et al., 2023) technique, which randomly masks 50% of point tokens during training, reducing time complexity by half. We precache text and image CLIP embeddings of all shapes, allowing us to increase the total batch size to 1152 and greatly accelerating training. To further improve the training process, we adopt DeepSpeed (Rasley et al., 2020) with ZeRO stage-1 optimizer and fp16 precision with dynamic loss scaling (Rajbhandari et al., 2020). Taking advantage of the aforementioned strategies, our largest model, i.e., Uni3D-g with one billion parameters, converges in approximately 20 hours with 24 \u00d7 NVIDIA-A100-SXM4-40GB GPUs."
        },
        {
            "heading": "B PART SETMENTATION DETAILS",
            "text": "Some prior methods (Rao et al., 2022; Yang et al., 2022) have demonstrated that transferring the knowledge gained from image-text contrastive learning, i.e., CLIP, can yield significant performance improvements in 2D dense prediction tasks (e.g. segmentation and detection). However, transferring this knowledge to 3D dense prediction tasks is barely explored. PointCLIP V2 (Zhu et al., 2022) explores the zero-shot 3D part segmentation by projecting the overall shape into multiple views. However, it suffers from occlusions and cannot be directly applied within the native 3D backbones. In light of these challenges, we seek to find a way to convert the global point cloud-text alignment\nlearned by Uni3D into a local point-text alignment. We aim to demonstrate that the object-level pre-training in Uni3D is sufficient for learning detailed local 3D visual concepts. Specifically, we select the features from 4th, 8th and the last layer of the ViT in Uni3D, denoted as H4, H8 and H12. Following PointNet++ (Qi et al., 2017b), we employ feature propagation to upsample group features H4, H8 and H12 into point-wise features. During training, we freeze the Uni3D backbone and only optimize the parameters in the feature propagation layer, with supervision to align pointwise features and text features of ground-truth part labels, which are extracted by the CLIP text encoder. By freezing the parameters of learned Uni3D, we focus on effectively exploring the pretrained fine-grained knowledge.\nThe visual comparison in Fig. 8 shows that our method can produce more accurate segmentation results in the one-shot part segmentation setting. We further report the detailed quantitative results of Tab. 3 in Tab. 8, and provide the detailed quantitative results of Tab. 4 in Tab. 9."
        },
        {
            "heading": "C MORE VISUALIZATION OF CROSS-MODAL RETRIEVAL",
            "text": "In Fig. 9, we visualize more 3D shapes retrieved from real-world one or multiple images. We further show the results of leveraging Uni3D to retrieve 3D shapes from the input texts in Fig. 10."
        },
        {
            "heading": "D MORE ANALYSIS ON INITIALIZING UNI3D",
            "text": "As demonstrated in Tab. 7, the best performance is achieved by initializing Uni3D with the SoTA 2D pretraining model EVA (Fang et al., 2022). The reason is that EVA model learns powerful and general representations to serve as a fine initialization of cross-modal contrastive learning (e.g. CLIP) as demonstrated in EVA (Fang et al., 2022), EVA-02 (Fang et al., 2023) and EVA-CLIP (Sun et al., 2023).\nWe justify that Uni3D also learns a cross-modal representation similar to CLIP, where the general patterns learned by EVA play a key role in improving and stabilizing the training of Uni3D. The analysis is further supported by the results of two-modal contrastive learning as shown in Tab. 10. Specifically, we conduct experiments to train Uni3D with only contrastive loss with CLIP image features (+only image) or CLIP text features (+only text), respectively. The results show that the optimization crashed without EVA initialization in the difficult situation where only images are available (20.7 vs. 40.1) or only texts are available (12.4 vs. 26.3)."
        },
        {
            "heading": "E FEW-SHOT RESULTS",
            "text": "We conduct few-shot linear probing under the difficult Objaverse-LVIS dataset with labeled training samples per class from 1, 2, 4, 8 to 16. The comparison is shown in Fig. 4. We further provide the detailed quantitative results in Tab. 11."
        },
        {
            "heading": "F POINT CLOUD CAPTIONING WITH UNI3D",
            "text": "With the learned 3D representations which are aligned with text/image embeddings, we can further apply Uni3D to the point cloud captioning task with CLIP-based image captioning approaches. Specifically, we follow OpenShape (Liu et al., 2023) to feed our 3D shape embeddings to CLIPCap (Mokady et al., 2021) for performing point cloud captioning. We directly employ the off-the-shelf CLIPCap model without any fine-tuning. CLIPCap adopts the image embeddings achieved with OpenAI-CLIP-B/32 for captioning, therefore, we train a variant of Uni3D with the OpenAI-CLIPB/32 teacher to align with CLIPCap. The visualizations of captioning 3D point clouds with Uni3D are shown in Fig. 11."
        }
    ],
    "year": 2023
}