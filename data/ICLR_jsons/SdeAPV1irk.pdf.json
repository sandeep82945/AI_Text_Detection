{
    "abstractText": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of approximate model from scratch.",
    "authors": [],
    "id": "SP:1a4cdd52efdcbad79ebde2adb5fb17389d53ab3f",
    "references": [
        {
            "authors": [
                "Alan Agresti",
                "Brent A. Coull"
            ],
            "title": "Approximate is better than \u201cexact\u201d for interval estimation of binomial proportions",
            "venue": "The American Statistician,",
            "year": 1998
        },
        {
            "authors": [
                "Filippo Amato",
                "Alberto L\u00f3pez",
                "Eladia Mar\u00eda Pe\u00f1a-M\u00e9ndez",
                "Petr Va\u0148hara",
                "Ale\u0161 Hampl",
                "Josef Havel"
            ],
            "title": "Artificial neural networks in medical diagnosis",
            "venue": "Journal of Applied Biomedicine,",
            "year": 2013
        },
        {
            "authors": [
                "Wolfgang Balzer",
                "Masanobu Takahashi",
                "Jun Ohta",
                "Kazuo Kyuma"
            ],
            "title": "Weight quantization in boltzmann machines",
            "venue": "Neural Networks,",
            "year": 1991
        },
        {
            "authors": [
                "Dirk Beyer",
                "Stefan L\u00f6we",
                "Evgeny Novikov",
                "Andreas Stahlbauer",
                "Philipp Wendler"
            ],
            "title": "Precision reuse for efficient regression verification",
            "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,",
            "year": 2013
        },
        {
            "authors": [
                "Mariusz Bojarski",
                "Davide Del Testa",
                "Daniel Dworakowski",
                "Bernhard Firner",
                "Beat Flepp",
                "Prasoon Goyal",
                "Lawrence D Jackel",
                "Mathew Monfort",
                "Urs Muller",
                "Jiakai Zhang"
            ],
            "title": "End to end learning for self-driving cars",
            "venue": "arXiv preprint arXiv:1604.07316,",
            "year": 2016
        },
        {
            "authors": [
                "Aleksandar Bojchevski",
                "Johannes Gasteiger",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Rudy Bunel",
                "Jingyue Lu",
                "Ilker Turkaslan",
                "Pushmeet Kohli",
                "P Torr",
                "P Mudigonda"
            ],
            "title": "Branch and bound for piecewise linear neural network verification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Tianqi Chen",
                "Thierry Moreau",
                "Ziheng Jiang",
                "Lianmin Zheng",
                "Eddie Yan",
                "Meghan Cowan",
                "Haichen Shen",
                "Leyuan Wang",
                "Yuwei Hu",
                "Luis Ceze",
                "Carlos Guestrin",
                "Arvind Krishnamurthy"
            ],
            "title": "Tvm: An automated end-to-end optimizing compiler for deep learning",
            "venue": "In Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation,",
            "year": 1931
        },
        {
            "authors": [
                "Tianqi Chen",
                "Lianmin Zheng",
                "Eddie Yan",
                "Ziheng Jiang",
                "Thierry Moreau",
                "Luis Ceze",
                "Carlos Guestrin",
                "Arvind Krishnamurthy"
            ],
            "title": "Learning to optimize tensor programs",
            "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "C.J. Clopper",
                "E.S. Pearson"
            ],
            "title": "The use of confidence or fiducial limits illustrated in the case of the binomial",
            "venue": "Biometrika, 26(4):404\u2013413,",
            "year": 1934
        },
        {
            "authors": [
                "Jeremy M. Cohen",
                "Elan Rosenfeld",
                "J. Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Krishnamurthy (Dj) Dvijotham",
                "Jamie Hayes",
                "Borja Balle",
                "Zico Kolter",
                "Chongli Qin",
                "Andras Gyorgy",
                "Kai Xiao",
                "Sven Gowal",
                "Pushmeet Kohli"
            ],
            "title": "A framework for robustness certification of smoothed classifiers using f-divergences",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Emile Fiesler",
                "Amar Choudry",
                "H John Caulfield"
            ],
            "title": "Weight discretization paradigm for optical neural networks",
            "venue": "In Optical interconnections and networks,",
            "year": 1990
        },
        {
            "authors": [
                "Marc Fischer",
                "Maximilian Baader",
                "Martin Vechev"
            ],
            "title": "Certified defense to image transformations via randomized smoothing",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Marc Fischer",
                "Maximilian Baader",
                "Martin Vechev"
            ],
            "title": "Scalable certified segmentation via randomized smoothing, 2022a",
            "year": 2022
        },
        {
            "authors": [
                "Marc Fischer",
                "Christian Sprecher",
                "Dimitar I. Dimitrov",
                "Gagandeep Singh",
                "Martin T. Vechev"
            ],
            "title": "Shared certificates for neural network verification",
            "venue": "Computer Aided Verification - 34th International Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "In Proc. International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Zhidong Gao",
                "Rui Hu",
                "Yanmin Gong"
            ],
            "title": "Certified robustness of graph classification against topology attack with randomized smoothing",
            "venue": "GLOBECOM",
            "year": 2020
        },
        {
            "authors": [
                "Mikl\u00f3s Z. Horv\u00e1th",
                "Mark Niklas Mueller",
                "Marc Fischer",
                "Martin Vechev"
            ],
            "title": "Boosting randomized smoothing with variance reduced classifiers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Steven A Janowsky"
            ],
            "title": "Pruning versus clipping in neural networks",
            "venue": "Physical Review A,",
            "year": 1989
        },
        {
            "authors": [
                "Jinyuan Jia",
                "Xiaoyu Cao",
                "Binghui Wang",
                "Neil Zhenqiang Gong"
            ],
            "title": "Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kenneth Johnson",
                "Radu Calinescu",
                "Shinji Kikuchi"
            ],
            "title": "An incremental verification framework for component-based software systems",
            "venue": "In Proceedings of the 16th International ACM Sigsoft Symposium on Component-Based Software Engineering,",
            "year": 2013
        },
        {
            "authors": [
                "Kyle D. Julian",
                "Mykel J. Kochenderfer",
                "Michael P. Owen"
            ],
            "title": "Deep neural network compression for aircraft collision avoidance systems",
            "year": 2018
        },
        {
            "authors": [
                "Guy Katz",
                "Clark W. Barrett",
                "David L. Dill",
                "Kyle Julian",
                "Mykel J. Kochenderfer"
            ],
            "title": "Reluplex: An efficient SMT solver for verifying deep neural networks",
            "venue": "In Computer Aided Verification - 29th International Conference,",
            "year": 2017
        },
        {
            "authors": [
                "Aounon Kumar",
                "Alexander Levine",
                "Soheil Feizi",
                "Tom Goldstein"
            ],
            "title": "Certifying confidence via randomized smoothing",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Marta Kwiatkowska",
                "Xiyue Zhang"
            ],
            "title": "When to trust ai: Advances and challenges for certification of neural networks",
            "venue": "arXiv preprint arXiv:2309.11196,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Laurel",
                "Rem Yang",
                "Shubham Ugare",
                "Robert Nagel",
                "Gagandeep Singh",
                "Sasa Misailovic"
            ],
            "title": "A general construction for abstract interpretation of higher-order automatic differentiation",
            "venue": "Proc. ACM Program. Lang.,",
            "year": 2022
        },
        {
            "authors": [
                "Guang-He Lee",
                "Yang Yuan",
                "Shiyu Chang",
                "Tommi Jaakkola"
            ],
            "title": "Tight certificates of adversarial robustness for randomly smoothed classifiers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Levine",
                "Soheil Feizi"
            ],
            "title": "de)randomized smoothing for certifiable defense against patch attacks",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Linyi Li",
                "Maurice Weber",
                "Xiaojun Xu",
                "Luka Rimanic",
                "Bhavya Kailkhura",
                "Tao Xie",
                "Ce Zhang",
                "Bo Li"
            ],
            "title": "Tss: Transformation-specific smoothing for robustness certification",
            "venue": "In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, CCS",
            "year": 2021
        },
        {
            "authors": [
                "Hongbin Liu",
                "Jinyuan Jia",
                "Neil Zhenqiang Gong"
            ],
            "title": "Pointguard: Provably robust 3d point cloud classification, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jeet Mohapatra",
                "Ching-Yun Ko",
                "Tsui-Wei Weng",
                "Pin-Yu Chen",
                "Sijia Liu",
                "Luca Daniel"
            ],
            "title": "Higherorder certification for randomized smoothing, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Peter W. O\u2019Hearn"
            ],
            "title": "Continuous reasoning: Scaling the impact of formal methods",
            "venue": "Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science,",
            "year": 2018
        },
        {
            "authors": [
                "Russell Reed"
            ],
            "title": "Pruning algorithms-a survey",
            "venue": "IEEE transactions on Neural Networks,",
            "year": 1993
        },
        {
            "authors": [
                "Elan Rosenfeld",
                "Ezra Winston",
                "Pradeep Ravikumar",
                "J. Zico Kolter"
            ],
            "title": "Certified robustness to label-flipping attacks via randomized smoothing, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Jan Schuchardt",
                "Aleksandar Bojchevski",
                "Johannes Gasteiger",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Collective robustness certificates: Exploiting interdependence in graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Hashim Sharif",
                "Prakalp Srivastava",
                "Muhammad Huzaifa",
                "Maria Kotsifakou",
                "Keyur Joshi",
                "Yasmin Sarita",
                "Nathan Zhao",
                "Vikram S. Adve",
                "Sasa Misailovic",
                "Sarita Adve"
            ],
            "title": "Approxhpvm: A portable compiler ir for accuracy-aware optimizations",
            "venue": "Proc. ACM Program. Lang.,",
            "year": 2019
        },
        {
            "authors": [
                "Benno Stein",
                "Bor-Yuh Evan Chang",
                "Manu Sridharan"
            ],
            "title": "Demanded abstract interpretation",
            "venue": "PLDI \u201921: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation,",
            "year": 2021
        },
        {
            "authors": [
                "M\u00e5ns Thulin"
            ],
            "title": "The cost of using exact confidence intervals for a binomial proportion",
            "venue": "Electronic Journal of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "Vincent Tjeng",
                "Kai Xiao",
                "Russ Tedrake"
            ],
            "title": "Evaluating robustness of neural networks with mixed integer programming",
            "venue": "arXiv preprint arXiv:1711.07356,",
            "year": 2017
        },
        {
            "authors": [
                "Shubham Ugare",
                "Gagandeep Singh",
                "Sasa Misailovic"
            ],
            "title": "Proof transfer for fast certification of multiple approximate neural networks",
            "venue": "Proc. ACM Program. Lang.,",
            "year": 2022
        },
        {
            "authors": [
                "Shubham Ugare",
                "Debangshu Banerjee",
                "Sasa Misailovic",
                "Gagandeep Singh"
            ],
            "title": "Incremental verification of neural networks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Willem Visser",
                "Jaco Geldenhuys",
                "Matthew B. Dwyer. Green"
            ],
            "title": "Reducing, reusing and recycling constraints in program analysis",
            "venue": "In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,",
            "year": 2012
        },
        {
            "authors": [
                "Binghui Wang",
                "Jinyuan Jia",
                "Xiaoyu Cao",
                "Neil Zhenqiang Gong"
            ],
            "title": "Certified robustness of graph neural networks against adversarial structural perturbation",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, KDD",
            "year": 2021
        },
        {
            "authors": [
                "Shiqi Wang",
                "Huan Zhang",
                "Kaidi Xu",
                "Xue Lin",
                "Suman Jana",
                "Cho-Jui Hsieh",
                "J Zico Kolter"
            ],
            "title": "Betacrown: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification",
            "venue": "arXiv preprint arXiv:2103.06624,",
            "year": 2021
        },
        {
            "authors": [
                "Tianhao Wei",
                "Changliu Liu"
            ],
            "title": "Online verification of deep neural networks under domain or weight",
            "venue": "shift. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Edwin B. Wilson"
            ],
            "title": "Probable inference, the law of succession, and statistical inference",
            "venue": "Journal of the American Statistical Association,",
            "year": 1927
        },
        {
            "authors": [
                "Greg Yang",
                "Tony Duan",
                "J. Edward Hu",
                "Hadi Salman",
                "Ilya Razenshteyn",
                "Jerry Li"
            ],
            "title": "Randomized smoothing of all shapes and sizes, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Guowei Yang",
                "Matthew B. Dwyer",
                "Gregg Rothermel"
            ],
            "title": "Regression model checking",
            "venue": "IEEE International Conference on Software Maintenance,",
            "year": 2009
        },
        {
            "authors": [
                "Ping yeh Chiang",
                "Michael J. Curry",
                "Ahmed Abdelkader",
                "Aounon Kumar",
                "John Dickerson",
                "Tom Goldstein"
            ],
            "title": "Detection as regression: Certified object detection by median smoothing, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Dinghuai Zhang",
                "Mao Ye",
                "Chengyue Gong",
                "Zhanxing Zhu",
                "Qiang Liu"
            ],
            "title": "Black-box certification with randomized smoothing: A functional optimization based framework",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aojun Zhou",
                "Anbang Yao",
                "Yiwen Guo",
                "Lin Xu",
                "Yurong Chen"
            ],
            "title": "Incremental network quantization: Towards lossless CNNs with low-precision weights",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Ensuring the robustness of deep neural networks (DNNs) to input perturbations is gaining increased attention from both users and regulators in various application domains (Bojarski et al., 2016; Amato et al., 2013; Julian et al., 2018; ISO). Out of many techniques for obtaining robustness certificaties (Kwiatkowska and Zhang, 2023), statistical methods currently offer the greatest scalability. Randomized smoothing (RS) is a popular statistical certification method by constructing a smoothed model g from a base network f under noise (Cohen et al., 2019). To certify the model g on an input, RS certification checks if the estimated lower bound on the probability of the top class is greater than the upper bound on the probability of the runner-up class (with high confidence). RS certification computes the certified accuracy metric of the DNN on the set of test inputs as a proxy for the DNN robustness. However, despite its effectiveness, RS-based certification can be computationally expensive as it requires DNN inference on a large number of corruptions per input.\nThe high cost of certification complicates the DNN deployment process, which has become increasingly iterative: the networks are often modified post-training to improve their execution time and/or accuracy. Especially, deploying DNNs on real-world systems with bounded computing resources (e.g., edge devices or GPUs with limited memory), have led to various techniques for approximating DNNs. Common approximation techniques include quantization \u2013 reducing the numerical precision of weights (Fiesler et al., 1990; Balzer et al., 1991), and pruning \u2013 removing weights that have minimal impact on accuracy (Janowsky, 1989; Reed, 1993).\nCommon to all of these approximations is that the network behavior (e.g., the classifications) remains the same on most inputs, its architecture does not change, and many weights are only slightly changed. When a user seeks to select a robust and accurate DNN from these possible approximations, RS needs to be performed to compute the robustness of all candidate networks. For instance, in the context of approximation tuning, there are multiple choices for approximation where different quantization or pruning strategies are applied at different layers. Tools such as (Chen et al., 2018b;a; Sharif et al., 2019) use approximations iteratively and test the network at each step. To ensure DNN robustness when using such tools, one would need to check certified accuracy, computed using RS on test data in each step. However, performing RS to compute certified accuracy from scratch can take hours as shown in our experiments even for a single network (with only 500 test images). Therefore, a major encumbrance of almost all existing RS-based certification practices in the above setting, is that the\nexpensive certification needs to be re-run from scratch for each approximate network. Overcoming this main limitation requires addressing the following fundamental problem:\nHow can we leverage the information generated while certifying a given network to speed up the certification of similar networks?\nThis Work. We present the first incremental RS-based certification framework called Incremental Randomized Smoothing (IRS) to answer this question. The primary objective of our work is to improve the sample complexity of the certification process of similar networks on a predefined test set. Improved sample complexity results in overall speedup in certification, and it reduces the energy requirement and memory footprint of the certification. Given a network f and its smoothed version g, and a modified network fp with its smoothed version gp, IRS incrementally certifies the robustness of gp by reusing the information from the execution of RS certification on g.\nIRS optimizes the process of certifying the robustness of smoothed classifier gp on an input x, by estimating the disparity \u03b6x \u2013 the upper bound on the probability that outputs of f and fp are distinct. Our new algorithm is based on three key insights about disparity:\n1. Common approximations yield small \u03b6x values \u2013 for instance, it is below 0.01 for int8 quantization for multiple large networks in our experiments.\n2. Estimating \u03b6x through binomial confidence interval requires fewer samples as it is close to 0 \u2013 it is, therefore, less expensive to certify with this probability than directly working with lower and upper probability bounds in the original RS algorithm.\n3. We can leverage \u03b6x alongside the bounds in the certified radius of g around x to compute the certified radius for gp \u2013 thus soundly reusing the samples from certifying g.\nWe extensively evaluate the performance of IRS when applying several common DNN approximations such as pruning and quantization on state-of-the-art DNNs on CIFAR10 (ResNet-20, ResNet-110) and ImageNet (ResNet-50) datasets.\nContributions. The main contributions of this paper are:\n\u2022 We propose a novel concept of incremental RS certification of the robustness of the updated smoothed classifier by reusing the certification guarantees for the original smoothed classifier.\n\u2022 We design the first algorithm IRS for incremental RS that efficiently computes the certified radius of the updated smoothed classifier.\n\u2022 We present an extensive evaluation of the performance of IRS speedups of up to 4.1x over the standard non-incremental RS baseline on state-of-the-art classification models."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Randomized Smoothing. Let f : Rm \u2192 Y be an ordinary classifier. A smoothed classifier g : Rm \u2192 Y can be obtained from calculating the most likely result of f(x+\u03f5) where \u03f5 \u223c N (0, \u03c32I).\ng(x) := argmax c\u2208Y\nP\u03f5(f(x+ \u03f5) = c)\nThe smoothed network g satisfies following guarantee for a single network f : Theorem 1. [From (Cohen et al., 2019)] Suppose cA \u2208 Y , pA, pB \u2208 [0, 1]. if\nP\u03f5(f(x+ \u03f5) = cA) \u2265 pA \u2265 pB \u2265 max c\u0338=cA P\u03f5(f(x+ \u03f5) = c),\nthen g(x + \u03b4) = cA for all \u03b4 satisying \u2225\u03b4\u22252 \u2264 \u03c32 (\u03a6 \u22121(pA) \u2212 \u03a6\u22121(pB)), where \u03a6\u22121 denotes the inverse of the standard Gaussian CDF. Computing the exact probabilities P\u03f5(f(x+ \u03f5) = c) is generally intractable. Thus, for practical applications, CERTIFY (Cohen et al., 2019) (Algorithm 1) utilizes sampling: First, it takes n0 samples to determine the majority class, then n samples to compute a lower bound pA to the success probability with confidence 1\u2212 \u03b1 via the Clopper-Pearson lemma (Clopper and Pearson, 1934). If pA > 0.5, we set pB = 1\u2212 pA and obtain radius R = \u03c3 \u00b7 \u03f5 \u00b7 \u03a6\u22121(pA) via Theorem 1, else we return ABSTAIN.\nAlgorithm 1 RS certification (Cohen et al., 2019) Inputs: f: DNN, \u03c3: standard deviation, x: input to the DNN, n0: number of samples to predict the top class, n: number of samples for computing pA, \u03b1: confidence parameter 1: function CERTIFY(f, \u03c3, x, n0, n, \u03b1) 2: counts0 \u2190 SampleUnderNoise(f, x, n0, \u03c3) 3: c\u0302A \u2190 top index in counts0 4: counts\u2190 SampleUnderNoise(f, x, n, \u03c3) 5: pA \u2190 LowerConfidenceBound(counts[c\u0302A], n, 1\u2212\u03b1) 6: if pA > 12 then 7: return prediction c\u0302A and radius \u03c3 \u00b7 \u03a6\u22121(pA) 8: else 9: return ABSTAIN DNN approximation. DNN weights need to be quantized to the appropriate datatype for deploying them on various edge devices. DNN approximations are used to compress the model size at the time of deployment, to allow inference speedup and energy savings without significant accuracy loss. While IRS can work with most of these approximations, for the evaluation, we focus on quantization and pruning as these are the most common ones (Zhou et al., 2017; Frankle and Carbin, 2019)."
        },
        {
            "heading": "3 INCREMENTAL RANDOMIZED SMOOTHING",
            "text": "Figure 1 illustrates the high-level idea behind the workings of IRS. It takes as input the classifier f , the updated classifier fp, and an input x. Let g and gp denote the smoothed network obtained from f and fp using RS respectively. IRS reuses the pA and pB estimates computed for g to compute the certified radius for gp."
        },
        {
            "heading": "3.1 MOTIVATION",
            "text": "Insight 1: Similarity in approximate networks We observe that for many practical approximations,\nf and fp produce the same result on most inputs. In this experiment, we estimate the disparity between f and fp on Gaussian corruptions of the input x. We compute a lower confidence bound \u03b6x such that P\u03f5(f(x+ \u03f5) \u0338= fp(x+ \u03f5)) \u2264 \u03b6x for \u03f5 \u223c N (0, \u03c32I). Table 1 presents empirical average \u03b6x for int8 quantization and pruning 10% lowest magnitude weights for some of the networks in our experiments computed over 500 inputs. We\ncompute \u03b6x value as the binomial confidence upper limit using (Clopper and Pearson, 1934) method with n = 1000 samples with \u03c3 = 1. The results show that the \u03b6x value is quite small in all the cases.\nInsight 2: Sample reduction through \u03b6x estimation We demonstrate that \u03b6x estimation for approximate networks is more efficient than running certification from scratch. Fig. 2 shows that for the fixed target error \u03c7, confidence (1 \u2212 \u03b1) and estimation technique, the number of samples required for estimation peaks, when the actual parameter value is around 0.5 and is smallest around\nthe boundaries. For example, when \u03c7 = 0.5% and \u03b1 = 0.01 estimating the unknown binomial proportion will take 41, 500 samples if the actual parameter value is 0.05 while achieving the same target error and confidence takes 216, 900 samples (5.22x higher) if the actual parameter value is 0.5. As observed in the previous section, \u03b6x\u2019s value for many practical approximations is close to 0.\nLeveraging the observation shown in Fig. 2 and given actual value \u03b6x is close to 0, estimating \u03b6x with existing binomial proportion estimation techniques is efficient and requires a smaller number of samples. In Appendix A.7, we show the distribution of pA and pB for various cases. We see that pA and pB do not always lie close to 0 or 1 and have a more dispersed distribution. Thus, estimating those requires more samples. Prior work (Thulin, 2013) has theoretically shown that the expected length of the confidence interval for Clopper-Pearson follows a similar trend as in Fig. 2. This theoretical result supports our observation. We show in Appendix A.1 that this observation is not contingent on a specific estimation method and holds for other popular estimation techniques, e.g., (Wilson, 1927), (Agresti and Coull, 1998).\nInsight 3: Computing the approximate network\u2019s certified radius using \u03b6x For certification of the approximate network gp, our main insight is that estimating \u03b6x and using that value to compute the certified radius is more efficient than computing RS certified radius from scratch. The next theorem shows how to use estimated value of \u03b6x to certify gp (the proof is in Appendix A.2): Theorem 2. If a classifier fp is such that for all x \u2208 Rm,P\u03f5(f(x + \u03f5) \u0338= fp(x + \u03f5)) \u2264 \u03b6x, and classifier f satisfies P\u03f5(f(x + \u03f5) = cA) \u2265 pA \u2265 pB \u2265 maxc\u0338=cA P\u03f5(f(x + \u03f5) = c) and pA \u2212 \u03b6x \u2265 pB + \u03b6x then gp satisfies gp(x + \u03b4) = cA for all \u03b4 satisying \u2225\u03b4\u22252 \u2264 \u03c32 (\u03a6 \u22121(pA \u2212 \u03b6x)\u2212 \u03a6\u22121(pB + \u03b6x))\nTheorem 1 considers standard RS for a single network. Our Theorem 2 shows how to use the estimated value of \u03b6x to transfer the certification guarantees across two networks f and fp."
        },
        {
            "heading": "3.2 IRS CERTIFICATION ALGORITHM",
            "text": "Algorithm 2 IRS algorithm: Certification with cache Inputs: fp: DNN obtained from approximating f , \u03c3: standard deviation, x: input to the DNN, np: number of Gaussian samples used for certification, Cf : stores the information to be reused from certification of f , \u03b1 and \u03b1\u03b6 : confidence parameters, \u03b3: threshold hyperparameter to switch between estimation methods 1: function CERTIFYIRS(fp, \u03c3, x, np, Cf , \u03b1, \u03b1\u03b6 , \u03b3) 2: c\u0302A \u2190 top index in Cf [x] 3: pA \u2190 lower confidence of f from Cf [x] 4: if pA < \u03b3 then 5: \u03b6x \u2190 EstimateZeta(fp, \u03c3, x, np, Cf , \u03b1\u03b6) 6: if pA \u2212 \u03b6x > 12 then 7: return prediction c\u0302A and radius \u03c3\u03a6\u22121(pA \u2212 \u03b6x) 8: else 9: counts\u2190 SampleUnderNoise(fp, x, np, \u03c3) 10: p\u2032A \u2190 LowerConfidenceBound( 11: counts[c\u0302A], np, 1\u2212 (\u03b1+ \u03b1\u03b6)) 12: if p\u2032A > 12 then 13: return prediction c\u0302A and radius \u03c3\u03a6\u22121(p\u2032A) 14: return ABSTAIN The Algorithm 2 presents the pseudocode for the IRS algorithm, which extends RS certification from Algorithm 1. The algorithm takes the modified classifier fp and certifies the robustness of gp around x. The input np denotes the number of Gaussian corruptions used by the algorithm. The IRS algorithm utilizes a cache Cf , which stores information obtained from the RS execution of the classifier f for each input x. The cached information is crucial for the operation of IRS. Cf stores the top predicted class index c\u0302A and its lower confidence bound pA for f on input x. The standard RS algorithm takes a conservative value of pB by letting pB = 1 \u2212 pA. This works reasonably well in practice and simplifies the computation of certified radius \u03c3 2 (\u03a6\n\u22121(pA)\u2212 \u03a6\u22121(pB)) to \u03c3\u03a6\u22121(pA). We make a similar choice in IRS, which simplifies the certified radius calculation from \u03c3 2 (\u03a6\n\u22121(pA \u2212 \u03b6x)\u2212\u03a6\u22121(pB + \u03b6x)) of Theorem 2 to \u03c3\u03a6\u22121(pA \u2212 \u03b6x) as we state in the next theorem (the proof is in Appendix A.2):\nTheorem 3. If pA \u2212 \u03b6x \u2265 12 , then \u03c3\u03a6 \u22121(pA \u2212 \u03b6x) \u2264 \u03c32 (\u03a6 \u22121(pA \u2212 \u03b6x)\u2212 \u03a6\u22121(pB + \u03b6x))\nAs per our insight 2 (Section 3.1), binomial confidence interval estimation requires fewer samples for binomial with actual probability close to 0 or 1. IRS can take advtange of this when pA is not close to 1. However, when pA is close to 1 then there is no benefit of using \u03b6x-based certified radius for gp. Therefore, the algorithm uses a threshold hyperparameter \u03b3 close to 1 that is used to switch between certified radius from Theorem 2 and standard RS from Theorem 1.\nIf the pA is less than the threshold \u03b3, then an estimate of \u03b6x for classifier fp and the classifier f is computed using the EstimateZeta function. We discuss EstimateZeta procedure in the next section. If the pA \u2212 \u03b6x is greater than 12 , then the top predicted class in the cache is returned as the prediction with the radius \u03c3\u03a6\u22121(pA \u2212 \u03b6x) as computed in Theorem 3. In case, pA is greater than the threshold \u03b3, similar to standard RS, the IRS algorithm draws np samples of fp(x + \u03f5) by running np noise-corrupted copies of x through the classifier fp. The function SampleUnderNoise(fp, x, np, \u03c3) in the pseudocode draws np samples of noise, \u03f51 . . . \u03f5np \u223c N (0, \u03c32I), runs each x + \u03f5i through the classifier fp, and returns a vector of class counts. If the lower confidence bound is greater than 12 , the top predicted class is returned as the prediction with a radius based on the lower confidence bound pA.\nIf the function does certify the input in both of the above cases, it returns ABSTAIN.\nThe hyperparameters \u03b1 and \u03b1\u03b6 denote confidence of IRS results. The IRS algorithm result is correct with confidence at least 1 \u2212 (\u03b1 + \u03b1\u03b6). For the case pA \u2265 \u03b3, this holds since we follow the same steps as standard RS. The function LowerConfidenceBound(counts[c\u0302A], np, 1 \u2212 (\u03b1 + \u03b1\u03b6)) in the pseudocode returns a one-sided 1\u2212 (\u03b1+ \u03b1\u03b6) lower confidence interval for the Binomial parameter p given a sample counts[c\u0302A] \u223c Binomial(np, p). We next state the theorem that shows the confidence of IRS results in the other case when pA < \u03b3 (the proof is in Appendix A.2):\nTheorem 4. If P\u03f5(f(x + \u03f5) = fp(x + \u03f5)) > 1 \u2212 \u03b6x with confidence at least 1 \u2212 \u03b1\u03b6 . If classifier f satisfies P\u03f5(f(x + \u03f5) = cA) \u2265 pA with confidence at least 1 \u2212 \u03b1. Then for classifier fp, P\u03f5(fp(x+ \u03f5) = cA) \u2265 pA \u2212 \u03b6x with confidence at least 1\u2212 (\u03b1+ \u03b1\u03b6)\n3.3 ESTIMATING THE UPPER CONFIDENCE BOUND \u03b6x\nIn this section, we present our method for estimating \u03b6x such that P\u03f5(f(x+ \u03f5) \u0338= fp(x+ \u03f5)) \u2264 \u03b6x with high confidence (Algorithm 3). We use the Clopper-Pearson (Clopper and Pearson, 1934) method to estimate the upper confidence bound \u03b6x.\nAlgorithm 3 Estimate \u03b6x Inputs: fp: DNN obtained from approximating f , \u03c3: standard deviation, x: input to the DNN, np: number of Gaussian samples used for estimating \u03b6x, Cf : stores the information to be reused from certification of f , \u03b1\u03b6 : confidence parameter Output: Estimated value of \u03b6x 1: function ESTIMATEZETA(fp, \u03c3, x, np, Cf , \u03b1\u03b6) 2: n\u2206 \u2190 0 3: seeds\u2190 seeds for original samples from Cf [x] 4: predictions\u2190 f \u2019s predictions on samples from Cf [x] 5: for i \u2208 {1, . . . np} do 6: \u03f5 \u223c N (0, \u03c32I) using seeds[i] 7: cf \u2190 predictions[i] 8: cfp \u2190 fp(x+ \u03f5) 9: n\u2206 \u2190 n\u2206 + I(cf \u0338= cfp) 10: return UpperConfidenceBound(n\u2206, np, 1\u2212 \u03b1\u03b6) We store the seeds used for randomly generating Gaussian samples while certifying the function f in the cache, and we reuse these seeds to generate the same Gaussian samples. seeds[i] stores the seed used for generating i-th sample in the RS execution of f , and predictions[i] stores the prediction of f on the corrsponding x + \u03f5. We evaluate fp on each corruption \u03f5 generated from seeds and match them to predictions by f . cf and cfp represent the top class prediction by f and fp respectively. n\u2206 is the count of the number of corruptions \u03f5 such that f and fp do not match on x+ \u03f5.\nThe function UpperConfidenceBound(n\u2206, np, 1\u2212 \u03b1\u03b6) in the pseudocode returns a onesided 1 \u2212 \u03b1\u03b6 upper confidence interval for the Binomial parameter p given a sample n\u2206 \u223c Binomial(np, p). We compute this upper confidence bound using the Clopper-Pearson method. This is similar to how the lower confidence bound is computed in the standard RS Algorithm 1. It is sound since the Clopper-Pearson method is conservative.\nReusing the seeds for generating noisy samples does not change the certified radius and is 2x faster compared to naive Monte Carlo estimation of \u03b6x with fresh Gaussian samples. Storing the seeds used in the cache results in a small memory overhead (less than 2MBs for our largest benchmark). We use the same Gaussian samples for estimations of pA and \u03b6x. This is equivalent to estimating two functions, p(X) and q(X), of a random variable X , where the same set of samples of X can be employed for their respective estimations. Theorem 4 makes no assumptions about the independence of estimating pA and \u03b6x, thus we can soundly reuse the same Gaussian samples for both estimations."
        },
        {
            "heading": "4 EXPERIMENTAL METHODOLOGY",
            "text": "Networks and Datasets. We evaluate IRS on CIFAR-10 (Krizhevsky et al.) and ImageNet (Deng et al., 2009). On each dataset, we use several classifiers, each with a different \u03c3\u2019s. For an experiment that adds Gaussian corruptions with \u03c3 to the input, we use the network that is trained with Gaussian augmentation with variance \u03c32. On CIFAR-10 we use the base classifier a 20-layer and 110-layer residual network. On ImageNet our base classifier is a ResNet-50.\nNetwork Approximations. We evaluate IRS on multiple approximations. We consider float16 (fp16), bfloat16 (bf16), and int8 quantizations (Section 5.1). We show the effectiveness of IRS on pruning approximation in Section 5.3. For int8 quantization, we use dynamic per-channel quantization mode. from (Paszke et al., 2019) library. For float16 and bfloat16 quantization, we change the data type of the DNN weights from float32 to the respective types. We perform float32, float16, and bfloat16 inferences on the GPU and int8 inferences on CPU since Pytorch does not support int8 quantization for GPUs yet (PyTorch). For the pruning experiment, we perform the lowest weight magnitude (LWM) pruning. The top-1 accuracy of the networks used in the evaluation and the approximate networks is discussed in Appendix A.3.\nExperimental Setup. We ran experiments on a 48-core Intel Xeon Silver 4214R CPU with 2 NVidia RTX A5000 GPUs. IRS is implemented in Python and uses PyTorch 2.0.1. (Paszke et al., 2019).\nHyperparameters. We use confidence parameters \u03b1 = 0.001 for the certification of g, and \u03b1\u03b6 = 0.001 for the estimation of \u03b6x. To establish a fair comparison, we set the baseline confidence with \u03b1b = \u03b1 + \u03b1\u03b6 = 0.002. This choice ensures that both the baseline and IRS, provide certified radii with equal confidence. We use grid search to choose an effective value for \u03b3. A detailed description of our hyperparameter search and its results are described in Section 5.4.\nAverage Certified Radius. We compute the certified radius r when the certification algorithm did not abstain and returned the correct class with radius r, for both IRS (Algorithm 2) and the baseline (Algorithm 1). In other cases, we say that the certified radius r = 0. We compute the average certified radius (ACR) by taking the mean of certified radii computed for inputs in the test set. Higher ACR indicates stronger robustness certification guarantees.\nSpeedup. IRS is applicable while certifying multiple similar networks, where it can reuse the certification of one of the networks for faster certification of all other similar networks. We demonstrate the effectiveness of IRS by comparing IRS\u2019s certification time for these other similar networks with the baseline certification from scratch. We do not include the certification time of the first network in the comparison as it adds the same time for both IRS and baseline."
        },
        {
            "heading": "5 EXPERIMENTAL RESULTS",
            "text": "We now present our main evaluation results. We consider the float32 representation of the DNN as f and a particular approximation as fp. However, IRS can be used with any similar f and fps, e.g., where f is an int8 quantized network and fp is the float32 network. In all of our experiments, we follow a specific procedure: 1. We certify the smoothed classifier g using standard RS with a sample size of n. 2. We approximate the base classifier f with fp. 3. Using the IRS, we certify smoothed classifier gp by employing Algorithm 2 and utilizing the\ncached information Cf obtained from the certification of g. We compare IRS to the baseline that uses standard non-incremental RS (Algorithm 1), to certify gp. Our results compare ACR and certification time between IRS and the baseline for various np values."
        },
        {
            "heading": "5.1 EFFECTIVENESS OF IRS",
            "text": "We compare the ACR and the certification time of the baseline and IRS for the common int8 quantization. We use n = 105 samples for certification of g. For certifying gp, we consider np values from {5%, . . . 50%} of n and \u03c3 = 1. We perform experiments on 500 images and compute the total time for certifying gp.\nFigure 3 presents the comparison between IRS and RS for int8 quantization. The x-axis displays the ACR and the y-axis displays the certification time. The plot consists of 10 markers each for the IRS and the baseline representing a specific value of np. Expectedly, the higher the value of np, the higher the average time and ACR. The marker coordinate denotes the ACR and the time for an experiment. In all the cases, IRS consistently takes less certification time to obtain the same ACR.\nFigure 3a, for ResNet-110 on CIFAR10, shows that IRS reduced the certification time from 2.91 hours (baseline) to 0.71 hours, resulting in time savings of 2.12 hours (4.1x faster). Moreover, we see that IRS achieves an ACR of more than 0.565, whereas the baseline does not reach this ACR for any of the np values in our experiments.\nFigure 3b, for ResNet-50 on ImageNet, for certifying an ACR of 0.875, IRS substantially reduced certification time from 27.82 hours (baseline) to 22.45 hours, saving approximately 5.36 hours (1.24x faster). Additionally, IRS achieved an ACR of 0.90 and reduced the certification time from 53.93 hours (baseline) to 40.58 hours, resulting in substantial time savings of 13.35 hours (1.33x faster)."
        },
        {
            "heading": "5.2 IRS SPEEDUPS ON DIFFERENT QUANTIZATIONS",
            "text": "Next, we study if IRS can handle other kinds of quantization. We perform experiments for 10 different values of np along with distinct approximations, and 3 values of \u03c3. Since this would take months of experiment time with n and np values from Section 5.1, for the rest of the experiments we use smaller values for these parameters. In these experiments, we compute the relative speedup due to IRS in comparison to the baseline. We use n = 104 for samples for certification of g. For certifying gp, we consider np values from {1%, . . . 10%} of n. For CIFAR10, we consider \u03c3 \u2208 {0.25, 0.5, 1.0}, and for Ima-\ngeNet, we consider \u03c3 \u2208 {0.5, 1.0, 2.0} as in the previous work(Cohen et al., 2019). We validated that the speedups for int8 quantization in this section for ResNet-50-ImageNet and ResNet-110-CIFAR10 are similar to those studied in Section 5.1.\nTo quantify IRS\u2019s average speedup over the baseline, we employ an approximate area under the curve (AOC) analysis. Specifically, we plot the certification time against the ACR. In most cases, IRS\ncertifies a larger ACR compared to the baseline, resulting in regions on the x-axis where IRS exists but the baseline does not. To ensure a conservative estimation, we calculate the speedup only within the range where both IRS and the baseline exist. We determine the speedup by computing the ratio of the AOC for IRS to the AOC for the baseline within this common range. Table 2 summarizes the average speedups for all quantization experiments.\nWe observe that IRS gets a larger speedup for smoothing with larger \u03c3 since on average the pA values are smaller. Appendix A.7 presents a further justification for this observation. Appendix A.9 presents further experiments with all combinations of DNNs, \u03c3, and quantizations."
        },
        {
            "heading": "5.3 IRS SPEEDUPS ON PRUNED MODELS",
            "text": "In this experiment, we study IRS\u2019s ability to certify beyond quantized models. We employ l1 unstructured pruning, which prunes the fraction of the lowest l1 magnitude weights from the DNN. Table 3 presents the average IRS speedup for DNNs obtained by pruning 5%, 10% and 20% weights. The speedups range from 0.99x to 2.7x. As the DNN is pruned more aggressively, it\u2019s expected that IRS\u2019s speedup will be lower. This is due to higher values of \u03b6x associated with aggressive pruning. In Appendix A.4, we provide average \u03b6x values for all approximations. Compared to pruning, quantization typi-\ncally yields smaller \u03b6x values, making IRS more effective for quantization."
        },
        {
            "heading": "5.4 ABLATION STUDIES",
            "text": "Next, we show the effect of \u03b3 on ACR. In Appendix A.5 we show IRS speedup on distinct values of n.\nSensitivity to threshold \u03b3. For each DNN architecture, we chose the hyperparameter \u03b3 by running IRS to certify a small subset of the validation set images for certifying the int8 quantized DNN and comparing the ACR. The choice of \u03b3 has no effect on certification time, as we perform np inferences in both cases, pA < \u03b3 and pA > \u03b3. We use the same \u03b3 for each DNN irrespective of the approximation and \u03c3. We use the grid search to choose the best value of gamma from the set\n{0.9, 0.95, 0.975, 0.99, 0.999}. Table 4 presents the ACR obtained for each \u03b3. We chose \u03b3 as 0.99 for CIFAR10 networks and 0.995 for the ImageNet networks since they result in the highest ACR."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Incremental Program Verification. The scalability of traditional program verification has been significantly improved by incremental verification, which has been applied on an industrial scale (Johnson et al., 2013; O\u2019Hearn, 2018; Stein et al., 2021). Incremental program analysis tasks achieve faster analysis of individual commits by reusing partial results (Yang et al., 2009), constraints (Visser et al., 2012), and precision information (Beyer et al., 2013) from previous runs.\nIncremental DNN Certification. Several methods have been introduced in recent years to certify the properties of DNNs deterministically (Tjeng et al., 2017; Bunel et al., 2020; Katz et al., 2017; Wang et al., 2021b; Laurel et al., 2022) and probabilisticly (Cohen et al., 2019). Researchers used incremental certification speed up DNN certification (Fischer et al., 2022b; Ugare et al., 2022; Wei and Liu, 2021; Ugare et al., 2023) \u2013 these works apply complete and incomplete deterministic\ncertification using formal logic cannot scale to e.g., ImageNet. In contrast, we propose incremental probabilistic certification with Randomized Smoothing, which enables much greater scalability.\nRandomized Smoothing. Cohen et al. (2019) introduced the addition of Gaussian noise to achieve l2-robustness results. Several extensions to this technique utilize different types of noise distributions and radius calculations to determine certificates for general lp-balls. Yang et al. (2020) and Zhang et al. (2020) derived recipes for determining certificates for p = 1, 2, and \u221e. Lee et al. (2019), Wang et al. (2021a), and Schuchardt et al. (2021) presented extensions to discrete perturbations such as l0-perturbations, while Bojchevski et al. (2023), Gao et al. (2020), Levine and Feizi (2020), and Liu et al. (2021) explored extensions to graphs, patches, and point cloud manipulations. Dvijotham et al. (2020) presented theoretical derivations for the application of both continuous and discrete smoothing measures, while Mohapatra et al. (2020) improved certificates by using gradient information. Horv\u00e1th et al. (2022) used ensembles to improve the certificate.\nBeyond norm-balls certificates, Fischer et al. (2020) and Li et al. (2021) presented how geometric operations such as rotation or translation can be certified via Randomized Smoothing. yeh Chiang et al. (2022) and Fischer et al. (2022a) demonstrated how the certificates can be extended from the setting of classification to regression (and object detection) and segmentation, respectively. For classification, Jia et al. (2020) extended certificates from just the top-1 class to the top-k classes, while Kumar et al. (2020) certified the confidence of the classifier, not just the top-class prediction. Rosenfeld et al. (2020) used Randomized Smoothing to defend against data poisoning attacks. These RS extensions (using different noise distributions, perturbations, and geometric operations) are orthogonal to the standard RS approach from Cohen et al. (2019). While these extensions have been shown to improve the overall bredth of RS, IRS is complementary to these extensions."
        },
        {
            "heading": "7 LIMITATIONS",
            "text": "We showed that IRS is effective at certifying the smoothed version of the approximated DNN. However, there are certain limitations to the effectiveness of IRS. First, the IRS algorithm requires a cache with the top predicted class index, its lower confidence bound, and the seeds for Gaussian corruptions obtained from the RS execution of the original classifier. However, storing this additional information is reasonable since it has negligible memory overhead and is a byproduct of certification (as trustworthy ML matures, we anticipate that this information will be shipped with pre-certified networks for reproducibility purposes).\nThe smoothing parameter \u03c3 used in IRS affects its efficiency, with larger values of \u03c3 generally leading to better results. As a consequence, we observed a smaller speedup when using a smaller value of \u03c3 (e.g., 0.25 on CIFAR10) compared to a larger value (e.g., 1 on CIFAR10). The value of \u03c3 offers a trade-off between robustness and accuracy. By choosing a larger \u03c3, one can improve robustness but it may lead to a loss of accuracy in the model.\nIRS targets fast certification while maintaining a sufficiently large radius. Therefore, we considered np smaller than 50% of n for our evaluation. However, IRS certified radius can be smaller than the non-incremental RS, provided the user has a larger sample budget. In our experiment in Appendix A.6 we test IRS on larger np and observe that IRS is better than baseline for np less than 70% of n. This is particularly advantageous when computational resources are limited."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "We propose IRS, the first incremental approach for probabilistic DNN certification. IRS leverages the certification guarantees obtained from the smoothed model to certify a smoothed approximated model with very few samples. Reusing the computation of original guarantees significantly reduces the computational cost of certification while maintaining strong robustness guarantees. IRS speeds up certification up to 4.1x over the standard non-incremental RS baseline on state-of-the-art classification models. We anticipate that IRS can be particularly useful for approximate tuning when the users need to analyze the robustness of multiple similar networks. Further, one can easily ship the certification cache to allow other users to further modify these networks based on their specific device and application needs and recertify the new network. We believe that our approach paves the way for efficient and effective certification of DNNs in real-world applications."
        }
    ],
    "title": "INCREMENTAL RANDOMIZED SMOOTHING CERTIFICATION"
}