{
    "abstractText": "Most existing works focus on direct perturbations to the victim\u2019s state/action or the underlying transition dynamics to demonstrate the vulnerability of reinforcement learning agents to adversarial attacks. However, such direct manipulations may not be always realizable. In this paper, we consider a multi-agent setting where a well-trained victim agent \u03bd is exploited by an attacker controlling another agent \u03b1 with an adversarial policy. Previous models do not account for the possibility that the attacker may only have partial control over \u03b1 or that the attack may produce easily detectable \u201cabnormal\u201d behaviors. Furthermore, there is a lack of provably efficient defenses against these adversarial policies. To address these limitations, we introduce a generalized attack framework that has the flexibility to model to what extent the adversary is able to control the agent, and allows the attacker to regulate the state distribution shift and produce stealthier adversarial policies. Moreover, we offer a provably efficient defense with polynomial convergence to the most robust victim policy through adversarial training with timescale separation. This stands in sharp contrast to supervised learning, where adversarial training typically provides only empirical defenses. Using the Robosumo competition experiments, we show that our generalized attack formulation results in much stealthier adversarial policies when maintaining the same winning rate as baselines. Additionally, our adversarial training approach yields stable learning dynamics and less exploitable victim policies.",
    "authors": [
        {
            "affiliations": [],
            "name": "A GENER"
        }
    ],
    "id": "SP:80d4f9083eaaa30f83cec192fc3226c681673911",
    "references": [
        {
            "authors": [
                "Alekh Agarwal",
                "Sham M Kakade",
                "Jason D Lee",
                "Gaurav Mahajan"
            ],
            "title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2021
        },
        {
            "authors": [
                "Maruan Al-Shedivat",
                "Trapit Bansal",
                "Yuri Burda",
                "Ilya Sutskever",
                "Igor Mordatch",
                "Pieter Abbeel"
            ],
            "title": "Continuous adaptation via meta-learning in nonstationary and competitive environments",
            "venue": "arXiv preprint arXiv:1710.03641,",
            "year": 2017
        },
        {
            "authors": [
                "David Balduzzi",
                "Marta Garnelo",
                "Yoram Bachrach",
                "Wojciech Czarnecki",
                "Julien Perolat",
                "Max Jaderberg",
                "Thore Graepel"
            ],
            "title": "Open-ended learning in symmetric zero-sum games",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Trapit Bansal",
                "Jakub Pachocki",
                "Szymon Sidor",
                "Ilya Sutskever",
                "Igor Mordatch"
            ],
            "title": "Emergent complexity via multi-agent competition",
            "venue": "arXiv preprint arXiv:1710.03748,",
            "year": 2017
        },
        {
            "authors": [
                "Trapit Bansal",
                "Jakub Pachocki",
                "Szymon Sidor",
                "Ilya Sutskever",
                "Igor Mordatch"
            ],
            "title": "Emergent complexity via multi-agent competition, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Jan Blumenkamp",
                "Amanda Prorok"
            ],
            "title": "The emergence of adversarial communication in multi-agent reinforcement learning, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Noam Brown",
                "Adam Lerer",
                "Sam Gross",
                "Tuomas Sandholm"
            ],
            "title": "Deep counterfactual regret minimization",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jinglin Chen",
                "Nan Jiang"
            ],
            "title": "Information-theoretic considerations in batch reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Dylan J Foster",
                "Noah Golowich"
            ],
            "title": "Independent policy gradient methods for competitive reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Stratis Skoulakis",
                "Manolis Zampetakis"
            ],
            "title": "The complexity of constrained min-max optimization",
            "venue": "In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Tanner Fiez",
                "Lillian J Ratliff"
            ],
            "title": "Local convergence analysis of gradient descent ascent with finite timescale separation",
            "venue": "In Proceedings of the International Conference on Learning Representation,",
            "year": 2021
        },
        {
            "authors": [
                "Marc Fischer",
                "Matthew Mirman",
                "Steven Stalder",
                "Martin Vechev"
            ],
            "title": "Online robustness training for deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1911.00887,",
            "year": 2019
        },
        {
            "authors": [
                "Tim Franzmeyer",
                "Jo\u00e3o F Henriques",
                "Jakob N Foerster",
                "Philip HS Torr",
                "Adel Bibi",
                "Christian Schroeder de Witt"
            ],
            "title": "Illusionary attacks on sequential decision makers and countermeasures",
            "venue": "arXiv preprint arXiv:2207.10170,",
            "year": 2022
        },
        {
            "authors": [
                "Drew Fudenberg",
                "David K Levine"
            ],
            "title": "Consistency and cautious fictitious play",
            "venue": "Journal of Economic Dynamics and Control,",
            "year": 1995
        },
        {
            "authors": [
                "Matthias Gerstgrasser",
                "David C Parkes"
            ],
            "title": "Oracles & followers: Stackelberg equilibria in deep multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Gleave",
                "Michael Dennis",
                "Cody Wild",
                "Neel Kant",
                "Sergey Levine",
                "Stuart Russell"
            ],
            "title": "Adversarial policies: Attacking deep reinforcement learning",
            "year": 1905
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Sven Gowal",
                "Krishnamurthy Dvijotham",
                "Robert Stanforth",
                "Rudy Bunel",
                "Chongli Qin",
                "Jonathan Uesato",
                "Relja Arandjelovic",
                "Timothy Mann",
                "Pushmeet Kohli"
            ],
            "title": "On the effectiveness of interval bound propagation for training verifiably robust models",
            "venue": "arXiv preprint arXiv:1810.12715,",
            "year": 2018
        },
        {
            "authors": [
                "Sven Gowal",
                "Krishnamurthy Dj Dvijotham",
                "Robert Stanforth",
                "Rudy Bunel",
                "Chongli Qin",
                "Jonathan Uesato",
                "Relja Arandjelovic",
                "Timothy Mann",
                "Pushmeet Kohli"
            ],
            "title": "Scalable verified training for provably robust image classification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Wenbo Guo",
                "Xian Wu",
                "Sui Huang",
                "Xinyu Xing"
            ],
            "title": "Adversarial policy learning in two-player competitive games",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jamie Hayes"
            ],
            "title": "Extensions and limitations of randomized smoothing for robustness guarantees",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Heinrich",
                "David Silver"
            ],
            "title": "Deep reinforcement learning from self-play in imperfectinformation games",
            "venue": "arXiv preprint arXiv:1603.01121,",
            "year": 2016
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sandy Huang",
                "Nicolas Papernot",
                "Ian Goodfellow",
                "Yan Duan",
                "Pieter Abbeel"
            ],
            "title": "Adversarial attacks on neural network policies, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Chi Jin",
                "Praneeth Netrapalli",
                "Michael Jordan"
            ],
            "title": "What is local optimality in nonconvex-nonconcave minimax optimization",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Sham Kakade",
                "John Langford"
            ],
            "title": "Approximately optimal approximate reinforcement learning",
            "venue": "Proc. 19th International Conference on Machine Learning. Citeseer,",
            "year": 2002
        },
        {
            "authors": [
                "Harold W Kuhn"
            ],
            "title": "A simplified two-person poker",
            "venue": "Contributions to the Theory of Games,",
            "year": 1950
        },
        {
            "authors": [
                "Aounon Kumar",
                "Alexander Levine",
                "Soheil Feizi"
            ],
            "title": "Policy smoothing for provably robust reinforcement learning",
            "venue": "arXiv preprint arXiv:2106.11420,",
            "year": 2021
        },
        {
            "authors": [
                "Marc Lanctot",
                "Vinicius Zambaldi",
                "Audrunas Gruslys",
                "Angeliki Lazaridou",
                "Karl Tuyls",
                "Julien P\u00e9rolat",
                "David Silver",
                "Thore Graepel"
            ],
            "title": "A unified game-theoretic approach to multiagent reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Marc Lanctot",
                "Edward Lockhart",
                "Jean-Baptiste Lespiau",
                "Vinicius Zambaldi",
                "Satyaki Upadhyay",
                "Julien P\u00e9rolat",
                "Sriram Srinivasan",
                "Finbarr Timbers",
                "Karl Tuyls",
                "Shayegan Omidshafiei"
            ],
            "title": "Openspiel: A framework for reinforcement learning in games",
            "venue": "arXiv preprint arXiv:1908.09453,",
            "year": 2019
        },
        {
            "authors": [
                "Yongyuan Liang",
                "Yanchao Sun",
                "Ruijie Zheng",
                "Furong Huang"
            ],
            "title": "Efficient adversarial training without attacking: Worst-case-aware robust reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jieyu Lin",
                "Kristina Dzeparoska",
                "Sai Qian Zhang",
                "Alberto Leon-Garcia",
                "Nicolas Papernot"
            ],
            "title": "On the robustness of cooperative multi-agent reinforcement learning",
            "venue": "IEEE Security and Privacy Workshops (SPW),",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Chen Lin",
                "Zhang-Wei Hong",
                "Yuan-Hong Liao",
                "Meng-Li Shih",
                "Ming-Yu Liu",
                "Min Sun"
            ],
            "title": "Tactics of adversarial attack on deep reinforcement learning agents, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Michael L Littman"
            ],
            "title": "Markov games as a framework for multi-agent reinforcement learning",
            "venue": "In Machine learning proceedings",
            "year": 1994
        },
        {
            "authors": [
                "Xiangyu Liu",
                "Hangtian Jia",
                "Ying Wen",
                "Yujing Hu",
                "Yingfeng Chen",
                "Changjie Fan",
                "Zhipeng Hu",
                "Yaodong Yang"
            ],
            "title": "Towards unifying behavioral and response diversity for open-ended learning in zero-sum games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Edward Lockhart",
                "Marc Lanctot",
                "Julien P\u00e9rolat",
                "Jean-Baptiste Lespiau",
                "Dustin Morrill",
                "Finbarr Timbers",
                "Karl Tuyls"
            ],
            "title": "Computing approximate equilibria in sequential adversarial games by exploitability descent",
            "year": 1903
        },
        {
            "authors": [
                "Bj\u00f6rn L\u00fctjens",
                "Michael Everett",
                "Jonathan P How"
            ],
            "title": "Certified adversarial robustness for deep reinforcement learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Stephen McAleer",
                "John Lanier",
                "Roy Fox",
                "Pierre Baldi"
            ],
            "title": "Pipeline psro: A scalable approach for finding approximate nash equilibria in large games",
            "venue": "arXiv preprint arXiv:2006.08555,",
            "year": 2020
        },
        {
            "authors": [
                "Stephen McAleer",
                "John B Lanier",
                "Kevin A Wang",
                "Pierre Baldi",
                "Roy Fox"
            ],
            "title": "Xdo: A double oracle algorithm for extensive-form games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Rupert Mitchell",
                "Jan Blumenkamp",
                "Amanda Prorok"
            ],
            "title": "Gaussian process based message filtering for robust multi-agent cooperation in the presence of adversarial communication, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Adria Puigdomenech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Paul Muller",
                "Shayegan Omidshafiei",
                "Mark Rowland",
                "Karl Tuyls",
                "Julien Perolat",
                "Siqi Liu",
                "Daniel Hennes",
                "Luke Marris",
                "Marc Lanctot",
                "Edward Hughes"
            ],
            "title": "A generalized training approach for multiagent learning",
            "year": 1909
        },
        {
            "authors": [
                "R\u00e9mi Munos"
            ],
            "title": "Error bounds for approximate policy iteration",
            "venue": "In ICML,",
            "year": 2003
        },
        {
            "authors": [
                "Tuomas Oikarinen",
                "Tsui-Wei Weng",
                "Luca Daniel"
            ],
            "title": "Robust deep reinforcement learning through adversarial loss, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Anay Pattanaik",
                "Zhenyi Tang",
                "Shuijing Liu",
                "Gautham Bommannan",
                "Girish Chowdhary"
            ],
            "title": "Robust deep reinforcement learning with adversarial attacks, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Nicolas Perez-Nieves",
                "Yaodong Yang",
                "Oliver Slumbers",
                "David H Mguni",
                "Ying Wen",
                "Jun Wang"
            ],
            "title": "Modelling behavioural diversity for learning in open-ended games",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Julien Perolat",
                "Remi Munos",
                "Jean-Baptiste Lespiau",
                "Shayegan Omidshafiei",
                "Mark Rowland",
                "Pedro Ortega",
                "Neil Burch",
                "Thomas Anthony",
                "David Balduzzi",
                "Bart De Vylder"
            ],
            "title": "From poincar\u00e9 recurrence to convergence in imperfect information games: Finding equilibrium via regularization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Lerrel Pinto",
                "James Davidson",
                "Rahul Sukthankar",
                "Abhinav Gupta"
            ],
            "title": "Robust adversarial reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "You Qiaoben",
                "Chengyang Ying",
                "Xinning Zhou",
                "Hang Su",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Understanding adversarial attacks on observations in deep reinforcement learning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Aditi Raghunathan",
                "Jacob Steinhardt",
                "Percy Liang"
            ],
            "title": "Certified defenses against adversarial examples",
            "venue": "arXiv preprint arXiv:1801.09344,",
            "year": 2018
        },
        {
            "authors": [
                "Aditi Raghunathan",
                "Jacob Steinhardt",
                "Percy S Liang"
            ],
            "title": "Semidefinite relaxations for certifying robustness to adversarial examples",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Alessio Russo",
                "Alexandre Proutiere"
            ],
            "title": "Balancing detectability and performance of attacks on the control channel of markov decision processes",
            "venue": "arXiv preprint arXiv:2109.07171,",
            "year": 2021
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Ali Shafahi",
                "Mahyar Najibi",
                "Mohammad Amin Ghiasi",
                "Zheng Xu",
                "John Dickerson",
                "Christoph Studer",
                "Larry S Davis",
                "Gavin Taylor",
                "Tom Goldstein"
            ],
            "title": "Adversarial training for free",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Silver",
                "Julian Schrittwieser",
                "Karen Simonyan",
                "Ioannis Antonoglou",
                "Aja Huang",
                "Arthur Guez",
                "Thomas Hubert",
                "Lucas Baker",
                "Matthew Lai",
                "Adrian Bolton"
            ],
            "title": "Mastering the game of go without human knowledge",
            "year": 2017
        },
        {
            "authors": [
                "Samuel Sokota",
                "Ryan D\u2019Orazio",
                "J Zico Kolter",
                "Nicolas Loizou",
                "Marc Lanctot",
                "Ioannis Mitliagkas",
                "Noam Brown",
                "Christian Kroer"
            ],
            "title": "A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games",
            "venue": "arXiv preprint arXiv:2206.05825,",
            "year": 2022
        },
        {
            "authors": [
                "Sriram Srinivasan",
                "Marc Lanctot",
                "Vinicius Zambaldi",
                "Julien P\u00e9rolat",
                "Karl Tuyls",
                "R\u00e9mi Munos",
                "Michael Bowling"
            ],
            "title": "Actor-critic policy optimization in partially observable multiagent environments",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jianwen Sun",
                "Tianwei Zhang",
                "Xiaofei Xie",
                "Lei Ma",
                "Yan Zheng",
                "Kangjie Chen",
                "Yang Liu"
            ],
            "title": "Stealthy and efficient adversarial attacks against deep reinforcement learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yanchao Sun",
                "Ruijie Zheng",
                "Yongyuan Liang",
                "Furong Huang"
            ],
            "title": "Who is the strongest enemy? towards optimal and efficient evasion attacks in deep rl",
            "venue": "arXiv preprint arXiv:2106.05087,",
            "year": 2021
        },
        {
            "authors": [
                "Yanchao Sun",
                "Ruijie Zheng",
                "Parisa Hassanzadeh",
                "Yongyuan Liang",
                "Soheil Feizi",
                "Sumitra Ganesh",
                "Furong Huang"
            ],
            "title": "Certifiably robust policy learning against adversarial communication in multiagent systems",
            "venue": "arXiv preprint arXiv:2206.10158,",
            "year": 2022
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian J. Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Chen Tessler",
                "Yonathan Efroni",
                "Shie Mannor"
            ],
            "title": "Action robust reinforcement learning and applications in continuous control",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "James Tu",
                "Tsunhsuan Wang",
                "Jingkang Wang",
                "Sivabalan Manivasagam",
                "Mengye Ren",
                "Raquel Urtasun"
            ],
            "title": "Adversarial attacks on multi-agent communication",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Eric Wong",
                "Zico Kolter"
            ],
            "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Fan Wu",
                "Linyi Li",
                "Zijian Huang",
                "Yevgeniy Vorobeychik",
                "Ding Zhao",
                "Bo Li"
            ],
            "title": "Crop: Certifying robust policies for reinforcement learning through functional smoothing",
            "venue": "arXiv preprint arXiv:2106.09292,",
            "year": 2021
        },
        {
            "authors": [
                "Xian Wu",
                "Wenbo Guo",
                "Hua Wei",
                "Xinyu Xing"
            ],
            "title": "Adversarial policy training against deep reinforcement learning",
            "venue": "In 30th {USENIX} Security Symposium ({USENIX} Security",
            "year": 2021
        },
        {
            "authors": [
                "Wanqi Xue",
                "Wei Qiu",
                "Bo An",
                "Zinovi Rabinovich",
                "Svetlana Obraztsova",
                "Chai Kiat Yeo"
            ],
            "title": "Misspoke or mis-lead: Achieving robustness in multi-agent communicative reinforcement learning",
            "venue": "In Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sihan Zeng",
                "Thinh Doan",
                "Justin Romberg"
            ],
            "title": "Regularized gradient descent ascent for two-player zero-sum markov games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Huan Zhang",
                "Tsui-Wei Weng",
                "Pin-Yu Chen",
                "Cho-Jui Hsieh",
                "Luca Daniel"
            ],
            "title": "Efficient neural network robustness certification with general activation functions",
            "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Chaowei Xiao",
                "Sven Gowal",
                "Robert Stanforth",
                "Bo Li",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Towards stable and efficient training of verifiably robust neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Chaowei Xiao",
                "Bo Li",
                "Duane S Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust deep reinforcement learning against adversarial perturbations on observations. 2020b",
            "year": 2020
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Duane S Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust reinforcement learning on state observations with learned optimal adversary",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Zhuoran Yang",
                "Han Liu",
                "Tong Zhang",
                "Tamer Basar"
            ],
            "title": "Fully decentralized multiagent reinforcement learning with networked agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Runyu Zhang",
                "Zhaolin Ren",
                "Na Li"
            ],
            "title": "Gradient play in stochastic games: stationary points, convergence, and sample complexity",
            "venue": "arXiv preprint arXiv:2106.00198,",
            "year": 2021
        },
        {
            "authors": [
                "Perolat"
            ],
            "title": "A ADDITIONAL RELATED WORK Two-player zero-sum games. The interaction between the attacker and the victim can be modeled as a two-player zero-sum game. There is a large body of work using RL to solve Nash equilibrium (NE)",
            "year": 2021
        },
        {
            "authors": [
                "Lockhart"
            ],
            "title": "2021) deal with more general two-player zero-sum games and propose population-based RL algorithms showing empirical success but lacking provable finite-time guarantees",
            "venue": "Daskalakis et al",
            "year": 2022
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "Markov game, which is a special case of our defense problem against unconstrained attack and serves as the inspiration for us to develop provable adversarial training algorithms. Adversarial attacks on RL policies. As deep neural networks are shown to be vulnerable to adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015), the adversarial robustness of deep RL policies",
            "year": 2017
        },
        {
            "authors": [
                "Sun"
            ],
            "title": "2020b) with the framework of PA-MDP for better efficiency. The majority of related work on adversarial RL focuses on perturbing state observations (Huang et al., 2017; Oikarinen et al., 2020; Sun et al., 2021), and assumes that the perturbation is small in lp distance",
            "year": 2021
        },
        {
            "authors": [
                "Gleave"
            ],
            "title": "not restrict the perturbation distance in every single step, allowing for more flexible and practical attack models. Adversarial attacks on multi-agent RL (MARL). Gleave et al. (2019) investigate adversarial policies in a two-player zero-sum game, where a victim can be exploited and significantly misled by the opponent\u2019s changed",
            "year": 2021
        },
        {
            "authors": [
                "2018a Zhang et al",
                "Wong",
                "2018 Kolter",
                "2020a Zhang et al",
                "Gowal"
            ],
            "title": "2018), randomized smoothing of a classifier (Cohen et al., 2019; Hayes, 2020), etc. In an effort to certify RL agents\u2019 robustness, some approaches (L\u00fctjens et al., 2020; Zhang et al., 2020b; Oikarinen et al., 2020; Fischer et al., 2019) apply network certification tools to bound the Q",
            "venue": "(Gowal et al.,",
            "year": 2021
        },
        {
            "authors": [
                "Sun"
            ],
            "title": "2022) propose a certifiable defense against adversarial communication in MARL systems. To the best of our knowledge, our paper is the first to provide provable convergence guarantees for adversarial training against adversarial attacks on the behaviors of other agents in the environment",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Despite the huge success of deep reinforcement learning (RL) algorithms across various domains (Silver et al., 2017; Mnih et al., 2015; Schulman et al., 2015), it has been shown that deep reinforcement learning policies are highly vulnerable to adversarial attacks. That is, a well-trained agent can produce wrong decisions under small perturbations, making it risky to deploy RL agents in real-life applications with noise and high stakes. The most popular attack methods focus on fooling the RL agent by adversarially perturbing the states, actions, or transition dynamics of the victim (Huang et al., 2017; Pattanaik et al., 2017; Zhang et al., 2020b; Sun et al., 2021; Tessler et al., 2019).\nHowever, in practice, many applications, such as air traffic control systems, are well-protected, meaning that direct perturbations to the observations or actions may not always be feasible. For instance, to perturb the readings of an air traffic control radar, an attacker might need to physically manipulate the sensor or infiltrate the communication system, tasks that can require significant effort. In this context, our paper explores attacks on a victim agent, \u03bd, executed by an attacker controlling another agent, \u03b1, in the same environment. Specifically, in the air traffic control example, an attacker could manipulate a commercial drone to interfere with the radar system of the victim. The strategy employed by the attacker in this scenario is known as an \u201cadversarial policy\u201d.\nPrevious works (Gleave et al., 2019; Wu et al., 2021b; Guo et al., 2021) have adopted principled approaches to attack well-trained RL agents by developing an adversarial policy to directly minimize the expected return of the victim. These methods have effectively defeated state-of-the-art agents trained through self-play (Bansal et al., 2018), even when the adversarial policy has been trained for less than 3% of the self-play training time steps. However, existing models do not adequately address\nsituations where the attacker might face resistance and only achieve partial control, which also lead to conspicuous behaviors. Despite the common occurrence of such attacks, provably efficient defenses are not well investigated yet.\nTo address these issues, our generalized attack formulation introduces an \u201cattack budget\u201d, effectively capturing the attacker\u2019s partial control. This metric accurately reflects the attacker\u2019s capacity to degrade a victim\u2019s performance. Within this framework, the attacker can self-regulate the attack budget, aligning with state distributions (Gleave et al., 2019) and marginalized transition dynamics (Russo & Proutiere, 2021; Franzmeyer et al., 2022) to craft stealthier, less detectable attacks. Notably, our attack model extends single-agent action adversarial RL (Tessler et al., 2019) to multiagent setting. On the defense side, merely retraining the victim agent against specific strong attacks may not necessarily improve overall robustness; in some cases, it could even worsen performance against other potential attacks. We propose an adversarial training algorithm featuring timescale separation, which avoids overfitting to specific attacks and focuses on optimizing the agent\u2019s worstcase performance. Unlike existing methods of timescale separation in GANs and adversarial training in supervised learning, which may converge to local solutions (Heusel et al., 2017) or provide only empirical defenses (Madry et al., 2017; Shafahi et al., 2019), our algorithm converges to the most robust policy globally, offering defenses with provably efficient guarantees \u2014 even in the face of the problem\u2019s non-convexity and non-concavity.\nOur key contributions in the realm of both attack and defense are summarized as follows. (1) We introduce a generalized attack formulation that captures the \u201cpartial control\u201d of the attacker. This formulation allows for stealthier attacks and extends the concept of action adversarial RL to more generalized settings. (2) We address the issue of non-convergence in adversarial training within our attack framework. By incorporating the principle of timescale separation, we achieve provable defenses and ensure theoretical guarantees for convergence to the globally most robust policy. (3) Empirical results affirm the efficacy of our generalized attack formulation in minimizing state distribution shifts and generating stealthier behaviors, compared to baseline unconstrained methods. Additionally, in tasks like Kuhn Poker and Robosumo, our timescale-separated adversarial training demonstrates superior stability and robustness when compared to popular baselines, including single-timescale adversarial training, self-play, and fictitious-play."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "The extension of Markov decision processes (MDPs) with more than one agent is commonly modeled as Markov games (Littman, 1994). A Markov game with N agents is defined by a tuple G =< N,S, {Ai}Ni=1, P, {ri}Ni=1, \u03c1, \u03b3 >, where S denotes the state space and Ai is the action space for agent i. The function P controls the state transitions by the current state and one action from each agent: P : S \u00d7 A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 AN \u2192 \u2206(S), where \u2206(S) denotes the set of probability distributions over the state space S. Given the current state st and the joint action (a1, . . . , aN ), the transition probability to st+1 is given by P (st+1|st, a1, . . . , aN ). The initial state is sampled from the initial state distribution \u03c1 \u2208 \u2206(S). Each agent i also has an associated reward function ri : S \u00d7 A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 AN \u2192 [0, 1], whose goal is to maximize the \u03b3-discounted expected return E[ \u2211\u221e\nt=0 \u03b3 tri(st, a t i, a t \u2212i)], where \u2212i is a compact representation of all complementary agents of i.\nIn Markov games, each agent is equipped with a policy \u03c0i : S \u2192 \u2206(Ai) in policy class \u03a0i and the joint policy is defined as \u03c0(a|s) = \u03a0Ni=1\u03c0i(ai|s). Specifically, the value function for the victim agent \u03bd given joint policy (\u03c0\u03bd , \u03c0\u03b1) is defined by Vs(\u03c0\u03bd , \u03c0\u03b1) = E\u03c0\u03bd ,\u03c0\u03b1 [ \u2211\u221e t=0 \u03b3\ntr\u03bd(st,at) | s0 = s] , where agent \u03bd attempts to maximize the value function and attacker aims to minimizes it. We abuse the notation to use V\u03c1(\u03c0\u03bd , \u03c0\u03b1) := Es\u223c\u03c1[Vs(\u03c0\u03bd , \u03c0\u03b1)]. We further define state visitation, which reflects how often the policy visits different states in the state space. Definition 2.1. (Stationary State Visitation) Let d\u03c0\u03c1 \u2208 \u2206(S) denote the normalized distribution of state visitation by following the joint policy \u03c0 in the environment: d\u03c0\u03c1 (s) = (1 \u2212 \u03b3)Es0\u223c\u03c1 \u2211\u221e t=0 \u03b3 tP\u03c0(st = s|s0)."
        },
        {
            "heading": "3 A GENERALIZED ATTACK FORMULATION",
            "text": "Problem description. For simplicity, we consider a multi-agent system with two agents, \u03bd and \u03b1, following policies \u03c0\u0302\u03bd and \u03c0\u0302\u03b1 respectively. The interactions between these agents can be cooperative, competitive, or mixed. As motivated earlier, we consider the attack scenario as described in Gleave et al. (2019); Wu et al. (2021b); Guo et al. (2021), where the threat comes from an attacker controlling agent \u03b1. This attacker deviates from \u03c0\u0302\u03b1 to an adversarial policy \u03c0\u0303\u03b1, aiming to minimize the\nperformance of the victim agent \u03bd. Correspondingly, the victim\u2019s goal is to develop a more robust policy \u03c0\u03bd in anticipation of such adversarial policies. The interaction between the attacker and the victim can thus be modeled as a zero-sum game, regardless of the initial relationship between the two agents.1 This framework can also be extended to settings with more than two agents, where the attacker controls multiple agents and adopts a joint adversarial policy.\nAttack formulation. Although such attacks can effectively exploit the victim, in many practical scenarios, unlike the attacks in Gleave et al. (2019); Wu et al. (2021b); Guo et al. (2021), the attacker may face resistance and achieve only partial control of agent \u03b1, e.g., in a hijack scenario. Therefore, we propose a more generalized attack framework. Here, the attacker aims to manipulate agent \u03b1 using an adversarial policy \u03c0\u0303\u03b1, but may not fully control the agent, which can still follow its original benign policy \u03c0\u0302\u03b1 with probability 1\u2212 \u03f5\u03c0 at each time step, where \u03f5\u03c0 \u2208 [0, 1]. Formally, under these conditions, the attacker solves the following attack objective:\nmin \u03c0\u0303\u03b1 V\u03c1(\u03c0\u0302\u03bd , \u03c0\u03b1) (3.1)\ns.t. \u03c0\u03b1(\u00b7 | s) = (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1(\u00b7 | s) + \u03f5\u03c0\u03c0\u0303\u03b1(\u00b7 | s), \u2200s \u2208 S. (3.2)\nObjective 3.1 is a standard attack objective (Gleave et al., 2019; Guo et al., 2021; Wu et al., 2021b), focused on minimizing the value of the victim \u03bd. The additional constraint 3.2 captures the probability \u03f5\u03c0 at which the attacker can control agent \u03b1. When \u03f5\u03c0 = 1, the setting degenerates to full control, aligning with Gleave et al. (2019); Guo et al. (2021); Wu et al. (2021b). Conversely, at \u03f5\u03c0 = 0, no attack occurs. This probability, denoted as the attack budget, effectively models the resistance encountered by the attacker. Its suitability as a budget will be further connected to action adversarial RL later (Tessler et al., 2019).\n(a) Effects of the attack budget. Our proposed attack budget effectively characterizes the victim\u2019s performance degradation, serving as a viable measure of agent vulnerability. To substantiate this, we note a key observation related to a standard discrepancy measure (Kakade & Langford, 2002; Schulman et al., 2015). Given the constraint in Equation 3.2, for any \u03c0\u0303\u03b1, the inequality DmaxTV (\u03c0\u03b1||\u03c0\u0302\u03b1) \u2264 \u03f5\u03c0 holds. Here, DmaxTV (\u03c0\u03b1||\u03c0\u0302\u03b1) is defined as maxs DTV(\u03c0\u03b1(\u00b7|s)||\u03c0\u0302\u03b1(\u00b7|s)), and DTV(p||q) := 12 \u2211 i |pi\u2212qi|. This observation allows us to establish an upper bound on the victim\u2019s performance under an attack budget of \u03f5\u03c0 .\nProposition 3.1 (Bounded policy discrepancy induces bounded value discrepancy). For two policy pairs (\u03c0\u0302\u03bd , \u03c0\u0302\u03b1) and (\u03c0\u0302\u03bd , \u03c0\u03b1) such that DmaxTV (\u03c0\u03b1||\u03c0\u0302\u03b1) \u2264 \u03f5\u03c0 , the difference between the victim value can be bounded as: |V\u03c1(\u03c0\u0302\u03bd , \u03c0\u0302\u03b1)\u2212 V\u03c1(\u03c0\u0302\u03bd , \u03c0\u03b1)| \u2264 2\u03f5\u03c0(1\u2212\u03b3)2 . This establishes a link between the attack budget and |V\u03c1(\u03c0\u0302\u03bd , \u03c0\u0302\u03b1) \u2212 V\u03c1(\u03c0\u0302\u03bd , \u03c0\u03b1)|. Specifically, the value function inherently satisfies a global Lipschitz condition. This implies that the attacker needs a sufficiently large attack budget \u03f5\u03c0 to cause significant degradation in performance. This contrasts with supervised learning attacks at test time, where small perturbations can result in large performance shifts. Although this is a worst-case upper bound and may not be tight \u2014 especially when \u03b3 is close to 1 \u2014 it still indicates that a longer effective game horizon grants the attacker greater capacity to degrade the victim\u2019s performance.\n(b) Attack model\u2019s stealthiness and detectability. While the unconstrained attack in Gleave et al. (2019) significantly impairs the victim\u2019s performance, its overt nature makes it easy to detect even through static images. This deviates from the stealthy ethos of adversarial attacks in supervised learning (Goodfellow et al., 2014). In contrast, our generalized attack framework allows for partial control of an agent and enables stealthier attacks by regulating the attack budget \u03f5\u03c0 . Specifically, leveraging insights that static images alone can reveal attacks, we use generative modeling techniques for distribution matching to align the state distributions d\u03c0\u0302\u03bd ,\u03c0\u0302\u03b1\u03c1 and d \u03c0\u0302\u03bd ,\u03c0\u03b1 \u03c1 induced by (\u03c0\u0302\u03bd , \u03c0\u0302\u03b1) and (\u03c0\u0302\u03bd , \u03c0\u03b1) respectively. Though exact state visitation is difficult to compute, regulating \u03f5\u03c0 allows us manage the discrepancy between these distributions. We adopt total variation distance as our measure of discrepancy, offering the following guarantees.\nProposition 3.2 (Bounded policy discrepancy induces bounded state distribution discrepancy). Fix any \u03f5\u03c0 \u2208 [0, 1]. For two policy pairs (\u03c0\u0302\u03bd , \u03c0\u0302\u03b1) and (\u03c0\u0302\u03bd , \u03c0\u03b1) such that DmaxTV (\u03c0\u03b1||\u03c0\u0302\u03b1) \u2264 \u03f5\u03c0 , the discrepancy between the state distributions can be bounded as: ||d\u03c0\u0302\u03bd ,\u03c0\u0302\u03b1\u03c1 \u2212 d\u03c0\u0302\u03bd ,\u03c0\u03b1\u03c1 ||1 \u2264 2\u03b3\u03f5\u03c0 1\u2212\u03b3 .\n1Even if \u03c0\u0302\u03bd and \u03c0\u0302\u03b1 are trained competitively, successful attacks can still occur due to sub-optimality of training Gleave et al. (2019); Bansal et al. (2017).\nProposition 3.2 demonstrates that as long as \u03f5\u03c0 is sufficiently small, the state distribution is well preserved, thus yielding images that are visually more similar to the original ones. This suggests that in practice, \u03f5\u03c0 can be treated as a hyper-parameter, balancing the attacker\u2019s performance with stealthiness. Finally, comparing actions or rewards is also a viable method to detect potential attacks. However, in many practical multi-agent systems, agents are decentralized, and actions or rewards are private to each agent (Zhang et al., 2018b), not always available to humans aiming to detect potential adversarial attacks.\nComparison with single-agent stealthy attacks. Russo & Proutiere (2021); Franzmeyer et al. (2022) consider stealthy attacks in a different setting, involving adversarial state or action perturbations, within single-agent RL. Their concept of unstealthiness or detectability is predicated on the inconsistencies in the transition dynamics when states or actions are adversarially perturbed, necessitating an accurate world model. However, in our scenario, even if such a world model is accessible, the (global) transition dynamics P remain unaffected by the adversarial policy. Concurrently, comparing the marginalized transition dynamics induced by \u03c0\u0302\u03b1 and \u03c0\u03b1 is plausible from the perspective of the victim \u03bd. Based on Proposition 3.3, inconsistencies in the marginalized transition dynamics can also be upper-bounded by the variation in the policy space, assuring low detectability as considered in Russo & Proutiere (2021); Franzmeyer et al. (2022). Thus, even if humans or detectors can access the private actions of the agent \u03bd and establish accurate corresponding marginalized transition dynamics, discrepancies might remain undetected as long as \u03f5\u03c0 is maintained minimal.\nProposition 3.3 (Bounded policy discrepancy induces bounded marginalized transition dynamics inconsistencies). We define the marginalized transition dynamic of agent \u03bd as P\u03c0\u03b1\u03bd (s\u2032 | s, a\u03bd) := Ea\u03b1\u223c\u03c0\u03b1(\u00b7 | s)[P (s\u2032 | s, a\u03b1, a\u03bd)] for given \u03c0\u03b1. P \u03c0\u0302\u03b1\u03bd is defined similarly for the policy \u03c0\u0302\u03b1. Then for any s \u2208 S and a\u03bd \u2208 A\u03bd , we have Df ( P\u03c0\u03b1\u03bd (\u00b7 | s, a\u03bd) || P \u03c0\u0302\u03b1\u03bd (\u00b7 | s, a\u03bd) ) \u2264 Df ( \u03c0\u03b1(\u00b7 | s) || \u03c0\u0302\u03b1(\u00b7 | s) ) ,\nwhere Df is any f -divergence, which includes DTV, connecting back to the attack budget.\n(c) Connection to action adversarial RL. Intriguingly, our attack formulation also extends the single-agent action adversarial RL (Tessler et al., 2019) to a multi-agent setting. Specifically, in PR-MDP (cf. Definition 1 of Tessler et al. (2019)), the policy under attack aligns with our Equation 3.2. In the context of single-agent action adversarial RL, the policy \u03c0\u0303\u03b1 is only a part of the finally executed policy, while the policy \u03c0\u0302\u03b1 represents the victim. Thus, our formulation broadens the attack setting of Tessler et al. (2019) to multi-agent RL, considering the other agent \u03bd as the victim instead of the agent \u03b1 itself. Moreover, determining the most robust policy for the victim using the policy iteration scheme for PR-MDP from Tessler et al. (2019) becomes inefficient in our context due to the absence of specific structures inherent in PR-MDP (Section 4 of Tessler et al. (2019)).\nHenceforth, we will abbreviate \u03c0\u0303\u03b1 as \u03c0\u03b1 without ambiguity, and the actually deployed policy for agent \u03b1 is represented as (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u03b1. Detailed proofs and discussions related to this section are available in \u00a7D."
        },
        {
            "heading": "4 IMPROVED ADVERSARIAL TRAINING WITH TIMESCALE SEPARATION",
            "text": "On the necessity and challenge of provably efficient defenses. As discussed before, to provide effective defenses, there are unique challenges standing out compared with single-agent robust RL (Tessler et al., 2019). Meanwhile, finding the celebrated solution concept Nash Equilibrium (NE) between the attacker and the victim suffices for finding the most robust victim policy during robust training but may not be necessary since NE guarantees that the attacker is also non-exploitable. We provide more detailed discussions on the relationship between NE and robustness in \u00a7B. There are a bunch of existing works solving NE for structured extensive-form games (Lockhart et al., 2019; Brown et al., 2019; Sokota et al., 2022) or for general games but without provably efficient guarantees (Fudenberg & Levine, 1995; Lanctot et al., 2017; Balduzzi et al., 2019; Muller et al., 2019). In practice, general game-theoretical methods often require solving best response problems iteratively, thus being computationally expensive. In theory, simply plugging in black-box NE solvers may not solve our problem with provable efficiencies since finding even only local NE for a general nonconvex-nonconcave problem is computationally hard (Daskalakis et al., 2021). Therefore, instead of adopting a black-box game-theoretical solver, we shall investigate adversarial training, a popular and more efficient paradigm for robust RL (Pinto et al., 2017; Zhang et al., 2021a; Sun et al., 2021).\nThere are prior works that utilize well-trained attacks for re-training to fortify the robustness of the victim (Gleave et al., 2019; Guo et al., 2021; Wu et al., 2021b). However, it has been demonstrated that while re-training against a specific adversarial policy does augment robustness against it, the performance against other policies may be compromised as validated by Gleave et al. (2019). Intuitively, if the victim is retrained against a specific attacker, its policy might be overfitted to that attacker. Thus, it is vital to uphold the performance of the victim against all potential attackers. Rather than merely re-training against a specific attacker, adversarial training methods have been shown to be effective in bolstering robustness against a broad spectrum of adversarial attacks. In these methods, the victim and the attacker are trained alternatively or simultaneously (Zhang et al., 2020b; Pinto et al., 2017). Here, we re-examine adversarial training in the RL domain and demonstrate that prevalent adversarial training methods encounter a non-converging problem with either alternative or simultaneous training, for which we defer examples and detailed discussions to \u00a7C. To address these issues formally, we contemplate the robustness of the victim and define the exploitability of \u03c0\u03bd under the worst-case attack as follows.\nDefinition 4.1 ((One-side) exploitability). Given \u03f5\u03c0 \u2208 [0, 1] and \u03c0\u0302\u03b1, for a victim policy \u03c0\u03bd , we measure the robustness of \u03c0\u03bd by: Expl(\u03c0\u03bd) = \u2212min\u03c0\u03b1 V\u03c1(\u03c0\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u03b1).\nIntuitions of timescale separation. The smaller Expl(\u03c0\u03bd) is, the more robust \u03c0\u03bd is. Therefore, to ensure the worst-case performance against the strongest adversarial policy, the victim should optimize the policy according to min\u03c0\u03bd Expl(\u03c0\u03bd). Ideally, if we can derive an analytical form of the function Expl(\u00b7) or compute its gradient, then we can simply run gradient descent to optimize it. Unfortunately, it is not obvious how to derive an analytical form and the function may not be even differentiable, let alone computing the gradient since the function relies on solving a minimization problem. However, it is possible to first solve the minimization problem, getting \u03c0\u22c6\u03b1, and compute the gradient w.r.t \u03c0\u03bd , namely \u2207\u03c0\u03bd \u2212 V\u03c1(\u03c0\u03bd , (1 \u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1), as if \u03c0\u22c6\u03b1 is fixed, hoping it could serve as a good descent direction. Formalizing this intuition, we propose to improve adversarial training via timescale separation with Min oracle (shown in Algorithm 1), where timescale separation comes from the fact the attacker takes a min step against the victim in line 3 while the victim takes only one gradient update in line 4. Note Lockhart et al. (2019) also considers directly minimizing the exploitability function but the algorithm and analysis are only applicable to extensive-form games. Finally, we remark that Algorithm 1 is consistent with the leader-follower update style that is developed for Stackelberg equilibrium in multi-agent RL (Gerstgrasser & Parkes, 2023).\nAlgorithm 1 Adversarial Training with Min-oracle\n1: Input: random policy \u03c00\u03bd , learning rate sequence {\u03b7t} 2: for t = 0 to T do 3: \u03c0t\u03b1 \u2190 argmin\u03c0\u03b1 V\u03c1(\u03c0t\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u03b1). 4: \u03c0t+1\u03bd \u2190 P\u03a0\u03bd (\u03c0t\u03bd + \u03b7t\u2207\u03c0\u03bdV\u03c1(\u03c0t\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0t\u03b1). // projection onto the simplex 5: Output: sample \u03c0t\u03bd with probability proportional to \u03b7t.\nEfficient approximation. The min oracle used in Algorithm 1 can be implemented with standard RL algorithms like PPO. When the game has special structures like extensive-form games (Lockhart et al., 2019) or one agent has a substantially smaller state/action space, such min oracle can be even implemented quite efficiently. However, in general, to make one gradient update for agent \u03bd, agent \u03b1 needs to compute a complete best response, which is computationally expensive in practice. To further fix this issue, we utilize the idea of using a much faster update scale for agent \u03b1 so that when agent \u03bd performs the gradient update, the policy \u03c0t\u03b1 is always and already an approximate solution of argmin\u03c0\u03b1 V\u03c1(\u03c0 t \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1+ \u03f5\u03c0\u03c0\u03b1). Formally, in addition to Algorithm 1, we present an alternative efficient Algorithm 2, where the min oracle is replaced by a gradient update with a larger step size and both agents only need to perform a gradient update independently. Therefore, our final algorithm is simple and compatible with standard RL algorithms, like PPO to implement the gradient update step for both agents, avoiding solving best responses at each iteration under popular game-theoretical approaches (Fudenberg & Levine, 1995; Lanctot et al., 2017; Balduzzi et al., 2019; Muller et al., 2019). To validate our intuitions and verify that our adversarial training with timescale separation (including the one with a min oracle and the one with two timescales) does provide provably efficient defenses, we shall prove the convergence guarantee of both algorithms in the next section.\nAlgorithm 2 Adversarial Training with Two Timescales\n1: Input: random policy \u03c00\u03bd , \u03c00\u03b1, learning rate sequence {\u03b7t\u03bd}, {\u03b7t\u03b1}, such that \u03b7t\u03bd \u226a \u03b7t\u03b1. 2: for t = 0 to T do 3: \u03c0t+1\u03b1 \u2190 P\u03a0\u03b1(\u03c0t\u03b1 \u2212 \u03b7t\u03b1\u2207\u03c0\u03b1V\u03c1(\u03c0t\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0t\u03b1)). 4: \u03c0t+1\u03bd \u2190 P\u03a0\u03bd (\u03c0t\u03bd + \u03b7t\u03bd\u2207\u03c0\u03bdV\u03c1(\u03c0t\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0t\u03b1)). 5: Output: sample \u03c0t\u03bd with probability proportional to \u03b7t\u03bd ."
        },
        {
            "heading": "5 THEORETICAL ANALYSIS",
            "text": "To understand and verify our approaches, we start by considering direct policy parameterization for both agents \u03bd and \u03b1, which is already challenging due to the non-convexity, non-concavity, and serves as the first step to analyze more complex function approximations.\nDefinition 5.1 (Direct parameterization). The policies \u03c0\u03bd and \u03c0\u03b1 have the parameterization \u03c0\u03bd(a | s) = \u03bds,a, \u03c0\u03b1(a | s) = \u03b1s,a, where \u03bd \u2208 \u2206(A\u03bd)|S| and \u03b1 \u2208 \u2206(A\u03b1)|S|.\nFor convenience of discussions, given \u03f5\u03c0 and \u03c0\u0302\u03b1, we will write J\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1) := V\u03c1(\u03c0\u03bd , (1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+ \u03f5\u03c0\u03c0\u03b1). Before proving the convergence of our methods, we define the mismatch coefficient to measure the intrinsic hardness of the environment. This is achieved by comparing the stationary state occupancy frequencies under certain policies against the initial state distribution. In simpler terms, a smaller value of this quantity indicates that the environment is more easily explorable.\nDefinition 5.2. Given the Markov game G, benign policy \u03c0\u0302\u03b1, and attack budget \u03f5\u03c0 , we define the minimax mismatch coefficient as\nC\u03f5\u03c0G := max\n{ max\n\u03c0\u03bd\u2208\u03a0\u03bd min\n\u03c0\u03b1\u2208\u03a0\u22c6\u03b1(\u03c0\u03bd) \u2225\u2225\u2225\u2225\u2225d\u03c0\u03bd ,(1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+\u03f5\u03c0\u03c0\u03b1\u03c1 \u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e , max \u03c0\u03b1\u2208\u03a0\u03b1 min \u03c0\u03bd\u2208\u03a0\u22c6\u03bd(\u03c0\u03b1) \u2225\u2225\u2225\u2225\u2225d\u03c0\u03bd ,(1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+\u03f5\u03c0\u03c0\u03b1\u03c1 \u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e } ,\nwhere \u03a0\u22c6\u03b1 (\u03c0\u03bd) := argmin\u03c0\u03b1\u2208\u03a0\u03b1 J\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1), and \u03a0 \u22c6 \u03bd (\u03c0\u03b1) := argmax\u03c0\u03bd\u2208\u03a0\u03bd J\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1).\nWith those two definitions, we can analyze how the robustness of the victim improves during adversarial training as follows:\nTheorem 5.3. Fix any \u03b4 > 0, \u03f5\u03c0 \u2208 [0, 1]. For Algorithm 1, suppose the learning rate \u03b7t\u03bd \u224d \u03b4, after T iterations, it is guaranteed that 1T \u2211T t=1 Expl(\u03c0 t \u03bd) \u2264 min\u03c0\u03bd Expl(\u03c0\u03bd) + \u03b4, where T =\n1 \u03b42 poly(C \u03f5\u03c0 G , |S|, |A\u03b1|, |A\u03bd |, 1 1\u2212\u03b3 ); while for Algorithm 2, suppose the learning rate \u03b7 t \u03bd \u224d \u03b48, \u03b7t\u03b1 \u224d \u03b44, after T iterations, it is guaranteed that 1T \u2211T t=1 Expl(\u03c0 t \u03bd) \u2264 min\u03c0\u03bd Expl(\u03c0\u03bd) + \u03b4, where T = poly( 1\u03b4 , C \u03f5\u03c0 G , |S|, |A\u03b1|, |A\u03bd |, 1 1\u2212\u03b3 ).\nRemark 5.4. To get a non-vacuous finite time convergence, the mismatch coefficient needs to be bounded, which is standard and necessary in the analysis of policy gradient methods (Daskalakis et al., 2020; Agarwal et al., 2021), where our definition of C\u03f5\u03c0G is based on the definition in (Daskalakis et al., 2020). It is worth noticing that such an assumption is weaker than other similar notions such as concentrability (Munos, 2003; Chen & Jiang, 2019), without requiring every visitable state to be visited at the first time step.\nImplications. This theorem demonstrates that, on average, the victim policy \u03c0t\u03bd is assured to converge to the most robust one; that is, the solution of argmax\u03c0\u03bd min\u03c0\u03b1 V\u03c1(\u03c0\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1+ \u03f5\u03c0\u03c0\u03b1). Theorem 5.3 reveals that Algorithm 1 achieves better iteration complexity owing to a larger learning rate. Meanwhile, the convergence for Algorithm 2 also substantiates the necessity of timescale separation due to \u03b7t\u03bd \u226a \u03b7t\u03b1. To the best of our knowledge, the analysis for Algorithm 1 is new, even for the \u03f5\u03c0 = 1 case, and the analysis for Algorithm 2 leverages Daskalakis et al. (2020); Jin et al. (2020); Zhang et al. (2021b) but addresses the more prevalent discounted reward setting without assuming the game to cease at every state with a positive probability. It also permits the attacker to exert only partial control over the agent \u03b1 within any stipulated attack budget \u03f5\u03c0 . Importantly, our theorem can be readily extended to encompass results of last-iterate convergence using the regularization techniques highlighted in Zeng et al. (2022) and stochastic gradients by deploying popular gradient estimators from finite samples, with the central concept remaining timescale separation. Detailed proofs are available in \u00a7D."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Stealthy adversarial attacks in RL. To render attacks on RL policies more feasible and practical, Sun et al. (2020) demonstrates that targeting critical points can facilitate efficient and stealthy attacks. Russo & Proutiere (2021) optimizes both attack detectability and victim performance, analyzing the trade-off between them. Franzmeyer et al. (2022) introduces an illusionary attack that maintains consistency with the environment\u2019s transition dynamics, necessitating a world model. The aforementioned stealthy attacks focus on perturbing the victim\u2019s state observations/actions. In contrast, this paper presents a generalized attack framework for multi-agent systems, allowing for stealth by managing the attack budget, wherein the attacker indirectly influences the victim by altering another agent\u2019s policy.\nTimescale separation for adversarial training. Adversarial training is a widely-adopted method for cultivating models robust against adversarial attacks. The efficacy of timescale separation in this context has been empirically affirmed; having increased loops for the inner attack subroutine translates to enhanced robustness Madry et al. (2017); Shafahi et al. (2019). Likewise, in training GANs (Heusel et al., 2017), utilizing a larger learning rate for the discriminator surpasses conventional GAN training and ensures convergence to a local NE. Moreover, Fiez & Ratliff (2021) explores more general non-convex non-concave zero-sum games and elucidates the local convergence to strict local minimax equilibrium with finite timescale separation. Contrarily, our adversarial training algorithm is assured to converge to the (globally) most robust policy.\nAdditional related works on zero-sum game, attacks in RL and multi-agent RL, and provably robust defenses are discussed in \u00a7A."
        },
        {
            "heading": "7 EXPERIMENTS",
            "text": "Our experiments utilize two standard environments: Kuhn Poker (Kuhn, 1950; Lanctot et al., 2019) and RoboSumo (Al-Shedivat et al., 2017). Detailed introductions to these environments, implementation specifics, and hyper-parameters are outlined in \u00a7E. Unless specified otherwise, the results are averaged over 5 seeds. In this section, we seek to address the following pivotal questions: \u25b7 Q1. Can our generalized attack formulation produce fewer state distribution variations and more\nstealthy behaviors compared to an unconstrained attack? \u25b7 Q2. Will adversarial training with timescale separation (involving a min oracle and two\ntimescales) exhibit more stable learning dynamics, and can the two timescales algorithm effectively approximate the adversarial training with a min oracle? \u25b7 Q3. In extensive and intricate environments, where a min oracle may not be available, can adversarial training with two timescales enhance robustness compared to prevalent and acclaimed baselines?\nControllable adversarial attack (Q1). We conduct experiments on the Robosumo environment. To verify that our attack formulation indeed achieves smaller state distribution variations, we choose an unconstrained adversarial policy and a constrained policy with the same winning rate for fair comparisons. Specifically, we investigate the distribution shift in the victim\u2019s observation part of the state features.\nTo select essential state features, we employ the variancebased feature importance method to filter out state features with small variances as they are deemed unimportant. The sorted feature importance for the RoboSumo games is depicted in Figure E.2. Figure 1 demonstrates that the adversarial policy, derived from our generalized attack framework, induces a much smaller state distribution shift compared to the unconstrained adversarial policy when assessed under the same winning rate. Additionally, we quantify the state-distribution shift brought about by the constrained and unconstrained attacks by calculating the Wasserstein-2 distance between their state distributions, as illustrated in Figure 3. Our constrained attack results in a significantly lower state distribution shift compared to the unconstrained one. To confirm that our generalized attack methods, with a regulated attack budget, do indeed produce more stealthy behaviors,\nwe visualize agents with \u03f5\u03c0 \u2208 0.3, 0.7, 1; here, a smaller \u03f5\u03c0 induces behaviors that are visually more similar to the system without an attack (see gifs at https://sites.google.com/view/stealthyattack). An ablation study on the trade-off between stealthiness and the attacker\u2019s performance is also presented in \u00a7E.4, also validating the effectiveness of \u03f5\u03c0 in regulating the attacker\u2019s strength, in line with Proposition 3.1. We additionally show in \u00a7E.4, even when there is a mismatch for attack budgets between training time and test time, the victim policy still shows greatly improved robustness.\nAdversarial Training with Timescale Separation (Q2&3). To address Q2&3, we examine the learning dynamics and robustness of policies trained with and without timescale separation, comparing these to other baselines in both Kuhn Poker and Robosumo environments.\nKuhn Poker. We implement Algorithm 1 and 2 under OpenSpiel (Lanctot et al., 2019), with the min oracle achieved through game tree search. For gradient update, we utilize Regret Policy Gradient (RPG) (Srinivasan et al., 2018) and Advantage Actor-Critic (A2C) (Mnih et al., 2016) in lieu of the vanilla policy gradient. Figure 2 illustrates the exploitability of the victim policy \u03c0\u03bd , where adversarial training with the min oracle exhibits the fastest convergence, lowest exploitability, and least variance. Meanwhile, policies trained with a sufficient timescale separation parameter \u03b7t\u03b1/\u03b7t\u03bd closely approximate the algorithm with a min oracle, outperforming single timescale algorithms.\nRobosumo Competition. To demonstrate the scalability of timescale separation, we evaluate our methods on Robosumo\u2014a high-dimensional, continuous control task, representing a significant challenge in terms of both training and evaluation. Although a min oracle with game tree search is unattainable in such a continuous control task, earlier experiments suggest that an ample timescale separation ratio can effectively approximate it. We monitor the score, V\u03c1(\u03c0t\u03bd , (1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+\u03f5\u03c0\u03c0t\u03b1), and the norm of gradient \u2207\u03c0\u03bdV\u03c1(\u03c0t\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0t\u03b1) during adversarial training. As per Figure 4a and 4b, we ascertain that single timescale training results in unstable behaviors, large variance, and gradient norm, while the two timescale training achieves quick convergence and significantly smaller gradient norm. Crucially, to affirm the robustness of our methods, we compare them with single timescale adversarial training, self-play (Bansal et al., 2017), and fictitious-play (Heinrich & Silver, 2016). Self-play and fictitious-play-based methods are renowned for training adversarially robust RL agents (Pinto et al., 2017; Zhang et al., 2021a; Tessler et al., 2019). We employ PPO to calculate the best response of the finalized robustified victim policy, illustrating the performance of the attacker during the victim attack process in Figure 4c. Here, a lower winning rate for the attacker signifies enhanced robustness. This demonstrates that our adversarial training with two timescales leads to victim policies with enhanced robustness compared to popular baseline methods, single timescale adversarial training, self-play, and fictitious-play."
        },
        {
            "heading": "8 DISCUSSION AND LIMITATIONS",
            "text": "In this paper, we reassess the threats posed to RL agents by adversarial policies and introduce a generalized attack formulation. Additionally, we develop the first provably efficient defense algorithm, \u201cadversarial training with timescale separation\u201d, with only gradient updates and converge to the most robust policy under mild conditions. Meanwhile, how to scale our formulation to accommodate multiple independent attackers wielding self-interested adversarial policies remains unaddressed. The convergence guarantees of the last iterate and the investigation of complex function approximations also require further analysis."
        },
        {
            "heading": "Appendix for \u201cRethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL\u201d",
            "text": "Table of Contents"
        },
        {
            "heading": "A Additional related work 15",
            "text": ""
        },
        {
            "heading": "B Relationship between NE and robustness 16",
            "text": ""
        },
        {
            "heading": "C Motivation and examples of timescale separation 17",
            "text": ""
        },
        {
            "heading": "D Full proof 18",
            "text": "D.1 Proof of Proposition 3.1 and 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . 18 D.2 Proof of Proposition 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Proof of Theorem 5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"
        },
        {
            "heading": "E Experimental details 24",
            "text": "E.1 Introduction to the environment . . . . . . . . . . . . . . . . . . . . . . . . . . 24 E.2 Feature importance in Robosumo environments . . . . . . . . . . . . . . . . . . 24 E.3 Implementation details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.4 Additional experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . 25"
        },
        {
            "heading": "A ADDITIONAL RELATED WORK",
            "text": "Two-player zero-sum games. The interaction between the attacker and the victim can be modeled as a two-player zero-sum game. There is a large body of work using RL to solve Nash equilibrium (NE). For example, Sokota et al. (2022); Perolat et al. (2021); McAleer et al. (2021); Brown et al. (2019); Lockhart et al. (2019) considers the normal-form and extensive-form games. Heinrich & Silver (2016); Lanctot et al. (2017); McAleer et al. (2020); Perez-Nieves et al. (2021); Liu et al. (2021) deal with more general two-player zero-sum games and propose population-based RL algorithms showing empirical success but lacking provable finite-time guarantees. Daskalakis et al. (2020); Zeng et al. (2022) analyze the theory of independent gradient with different learning rates in Markov game, which is a special case of our defense problem against unconstrained attack and serves as the inspiration for us to develop provable adversarial training algorithms.\nAdversarial attacks on RL policies. As deep neural networks are shown to be vulnerable to adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015), the adversarial robustness of deep RL policies has also attracted increasing attention. One of the earliest works by Huang et al. (2017) reveals the vulnerability of neural policies by adapting various adversarial attacks from supervised learning to RL policies. Lin et al. (2019) consider an efficient attack that only perturbs the agent at a subset of time steps. There has been subsequent research in developing stronger pixel-based attacks (Qiaoben et al., 2021; Pattanaik et al., 2017; Oikarinen et al., 2020). Zhang et al. (2020b) built the theoretical framework SA-MDP for adversarial state perturbation and proposed the corresponding regularizer for more robust reinforcement learning policies. Subsequent work by Sun et al. (2021) improves Zhang et al. (2020b) with the framework of PA-MDP for better efficiency. The majority of related work on adversarial RL focuses on perturbing state observations (Huang et al., 2017; Oikarinen et al., 2020; Sun et al., 2021), and assumes that the perturbation is small in \u2113p distance. In contrast, our paper considers the attack generated by other agents in a multi-agent system and does not restrict the perturbation distance in every single step, allowing for more flexible and practical attack models.\nAdversarial attacks on multi-agent RL (MARL). Gleave et al. (2019) investigate adversarial policies in a two-player zero-sum game, where a victim can be exploited and significantly misled by the opponent\u2019s changed behavior. Guo et al. (2021) remove the zero-sum assumption in Gleave et al.\n(2019) and construct a new formulation for the adversarial policy. Lin et al. (2020) study adversarial attacks in cooperative MARL systems and reveal that attacking one of the agents in the team can greatly reduce the total reward. However, these adversarial policies are unconstrained and could cause abnormal behaviors that are easily detectable, while our attack model can be made stealthy by restricting the state distribution shifts. We also propose provably robust defense algorithms to learn a policy that is unexploitable.\nAttacks and defenses on communication in MARL. There is also a line of work studying attacks and defenses on communications in MARL (Blumenkamp & Prorok, 2020; Tu et al., 2021; Mitchell et al., 2020; Xue et al., 2022), where the communication among cooperative agents could be perturbed to influence the decisions of victims. However, we consider adversarial behaviors or policies of other agents, which affect the victim by the actions taken by other agents.\nProvably robust defenses. To provide guaranteed robustness for deep neural networks, many approaches have been developed to certify the performance of neural networks, including semidefinite programming-based defenses (Raghunathan et al., 2018a;b), convex relaxation of neural networks (Gowal et al., 2019; Zhang et al., 2018a; Wong & Kolter, 2018; Zhang et al., 2020a; Gowal et al., 2018), randomized smoothing of a classifier (Cohen et al., 2019; Hayes, 2020), etc. In an effort to certify RL agents\u2019 robustness, some approaches (Lu\u0308tjens et al., 2020; Zhang et al., 2020b; Oikarinen et al., 2020; Fischer et al., 2019) apply network certification tools to bound the Q networks. Kumar et al. (2021) and Wu et al. (2021a) apply randomized smoothing (Cohen et al., 2019) to RL to achieve provable robustness. These defenses mainly focus on the adversarial perturbations directly applied to the agent\u2019s inputs. Sun et al. (2022) propose a certifiable defense against adversarial communication in MARL systems. To the best of our knowledge, our paper is the first to provide provable convergence guarantees for adversarial training against adversarial attacks on the behaviors of other agents in the environment.\nImproving policy robustness by adversarial training. Prior work shows that the competition between the victim agent and the adversary can be regarded as a two-player zero-sum game while training agents with learned adversarial attacks can improve the robustness of the victim. Such an adversarial training paradigm has been shown effective under state perturbations (Zhang et al., 2021a; Sun et al., 2021; Liang et al., 2022) and action perturbations (Pinto et al., 2017; Tessler et al., 2019) on a single victim. In the context of perturbing the actions of the victim, Tessler et al. (2019) presents a two-player policy iteration algorithm that is proved to converge to the Nash Equilibrium. In contrast, we consider the adversarial behaviors of other agents, and we provide both theoretical guarantees and empirical evidence for the effectiveness of our adversarial training."
        },
        {
            "heading": "B RELATIONSHIP BETWEEN NE AND ROBUSTNESS",
            "text": "To understand the relationship between NE and robustness, we formally define the NE as follows.\nDefinition B.1 (Nash equilibrium). Fix \u03c0\u0302\u03b1 and \u03f5\u03c0 \u2265 0. We say a pair of policy (\u03c0\u22c6\u03bd , \u03c0\u22c6\u03b1) the Nash equilibrium for the zero-sum game between the victim and the attacker if it holds that for any \u03c0\u2032\u03bd and \u03c0\u2032\u03b1:\nV\u03c1(\u03c0 \u22c6 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1) \u2265 V\u03c1(\u03c0\u2032\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1), (B.1) V\u03c1(\u03c0 \u22c6 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1) \u2264 V\u03c1(\u03c0\u22c6\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u2032\u03b1). (B.2)\nNow we have the following proposition showing the robustness of the NE.\nProposition B.2. Fix \u03c0\u0302\u03b1 and \u03f5\u03c0 \u2208 [0, 1]. If (\u03c0\u22c6\u03bd , \u03c0\u22c6\u03b1) is the Nash equilibrium for the zero-sum game between the victim and the attacker, then \u03c0\u22c6\u03bd is the minimizer for the function Expl(\u03c0 \u2032 \u03bd).\nProof. According to Definition B.1, it holds that\nmin \u03c0\u2032\u03b1\nV\u03c1(\u03c0 \u22c6 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u2032\u03b1) = V\u03c1(\u03c0\u22c6\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1) = max\n\u03c0\u2032\u03bd V\u03c1(\u03c0\n\u2032 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1).\n(B.3)\nCombining Equation equation B.3 with the fact that for any function f it holds that minx maxy f(x, y) \u2265 maxy minx f(x, y), we conclude that\nV\u03c1(\u03c0 \u22c6 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1) = max\n\u03c0\u2032\u03bd min \u03c0\u2032\u03b1\nV\u03c1(\u03c0 \u2032 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u2032\u03b1)\n= min \u03c0\u2032\u03b1 max \u03c0\u2032\u03bd\nV\u03c1(\u03c0 \u2032 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u2032\u03b1).\nNow given an NE pair (\u03c0\u22c6\u03bd , \u03c0 \u22c6 \u03b1), we consider the exploitability of \u03c0 \u22c6 \u03bd :\nExpl(\u03c0\u22c6\u03bd) = \u2212min \u03c0\u03b1 V\u03c1(\u03c0 \u22c6 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u03b1)\n= \u2212V\u03c1(\u03c0\u22c6\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u22c6\u03b1) = min\n\u03c0\u2032\u03bd \u2212min \u03c0\u2032\u03b1 V\u03c1(\u03c0 \u2032 \u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u2032\u03b1)\n= min \u03c0\u2032\u03bd\nExpl(\u03c0\u2032\u03bd).\nTherefore, the solution concept NE guarantees the most robust policy for the victim. Indeed, it is stronger than the goal of the most robust policy since by following exactly the same procedure, one can verify NE also guarantees the most robust policy for the attacker. However, as we remarked previously, we only care about the robustness of the victim."
        },
        {
            "heading": "C MOTIVATION AND EXAMPLES OF TIMESCALE SEPARATION",
            "text": "To motivate the necessity of timescale separation in adversarial training for robust RL policy, we revisit some known issues of naive single-timescale methods including Gradient Descent Ascent (GDA) and iterative best response (IBR) with both simultaneous and alternating update using a simple normal-form game Rock-Paper-Scissor, which corresponds to our defense problem with a single state and \u03f5\u03c0 = 1.\nExample C.1. The zero-sum game Rock-Paper-Scissor includes two players with the same action\nspace A = {Rock, Paper, Scissor}. The payoff matrix P is given as P = [ 0 1 \u22121 \u22121 0 1 1 \u22121 0 ] for the row player. The row player has the mixed strategy x \u2208 X = \u2206(A), where xi represents the probability of choosing ith action. The column player holds a similar mixed strategy y \u2208 Y . The corresponding payoff is given by V (x, y) = x\u22a4Py. Our objective is given by maxx\u2208X miny\u2208Y x\u22a4Py. Note here \u03f5\u03c0 = 1.\nFormally, we compare the following 5 methods; the first 4 methods have only a single timescale while the last GAMin method highlights timescale separation, where one player takes a gradient step, and then another one takes the best response.\n\u2022 Simultaneous gradient descent ascent (SGDA): yt+1 = PY(yt\u2212\u03b7P\u22a4xt), xt+1 = PX (xt+ \u03b7Pyt). \u2022 Alternate gradient descent ascent (AGDA): yt+1 = PY(yt \u2212 \u03b7P\u22a4xt), xt+1 = PX (xt + \u03b7Pyt+1) \u2022 Simultaneous iterative best response (SIBR): yt+1 = argminy\u2208Y x\u22a4t Py, xt+1 = argminx\u2208X x\n\u22a4Pyt \u2022 Alternate iterative best response (AIBR): yt+1 = argminy\u2208Y x\u22a4t Py, xt+1 = argminx\u2208X x\n\u22a4Pyt+1. \u2022 (With timescale separation) Gradient ascent with min oracle (GAMin): yt+1 = argminy\u2208Y x \u22a4 t Py, xt+1 = PX (xt + \u03b7Pyt+1).\nWe show the exploitability of the x player during the learning process in Figure 5. It is clear that the first four single-timescale training methods (SGDA, AGDA, SIBR, AIBR) fail to achieve low exploitability, while only GAMin achieves the near-optimal exploitability 0."
        },
        {
            "heading": "D FULL PROOF",
            "text": "D.1 PROOF OF PROPOSITION 3.1 AND 3.2\nProof. For simplicity, we shall prove Proposition 3.2 first, and then prove Proposition 3.1.\nLet us first review the following facts for any joint policy \u03c0 = (\u03c0\u03bd , \u03c0\u03b1), \u03c0\u2032 = (\u03c0\u03bd , \u03c0\u2032\u03b1) such that DmaxTV (\u03c0\u03b1||\u03c0\u2032\u03b1) \u2264 \u03f5\u03c0 and the transition matrix P\u03c0 , where P\u03c0(s\u2032, s) = \u2211 a \u03c0(a|s)P (s\u2032|s,a). In the following proof, we use P\u03c0(i, j) to denote P\u03c0(si, sj).\n\u2022 d\u03c0\u03c1 = (1\u2212 \u03b3)(I \u2212 \u03b3P\u03c0)\u22121\u03c1. \u2022 ||P\u03c0||1 = 1 and ||(I \u2212 \u03b3P\u03c0)\u22121||1 \u2264 11\u2212\u03b3 . \u2022 ||P\u03c0 \u2212 P\u03c0\u2032 ||1 \u2264 2\u03f5\u03c0\nAccording to Definition 2.1, one can verify that d\u03c0\u03c1 satisfies that:\nd\u03c0\u03c1 = (1\u2212 \u03b3)\u03c1+ \u03b3P\u03c0d\u03c0\u03c1 ,\nwhich gives the solution d\u03c0\u03c1 = (1\u2212 \u03b3)(I \u2212 \u03b3P\u03c0)\u22121\u03c1. For P\u03c0:\n||P\u03c0||1 = max j \u2211 i |P\u03c0(i, j)|\n=max j \u2211 i \u2211 a \u03c0(a|sj)P (si|sj ,a)\n=max j \u2211 a \u03c0(a|sj) \u2211 i P (si|sj ,a)\n=1\nFor ||(I \u2212 \u03b3P\u03c0)\u22121||1:\n||(I \u2212 \u03b3P\u03c0)\u22121||1 = || \u221e\u2211 k=0 (\u03b3P\u03c0) k||1 \u2264 \u221e\u2211 k=1 ||(\u03b3P\u03c0)k||1 \u2264 \u221e\u2211 k=1 \u03b3k||P\u03c0||k1 = 1 1\u2212 \u03b3 .\nFor ||P\u03c0 \u2212 P\u03c0\u2032 ||1:\nTo begin with, since \u03c0\u03bd remains unchanged in our proof, let us abuse the notation a little bit and define the marginalized transition P (s\u2032|s, a\u03b1) = \u2211 a\u03bd \u03c0\u03bd(a\u03bd |s)P (s\u2032|s, a\u03bd , a\u03b1). We have\n||P\u03c0 \u2212 P\u03c0\u2032 ||1 = max j \u2211 i |P\u03c0(i, j)\u2212 P\u03c0\u2032(i, j)|\n= max j \u2211 i | \u2211 a\u03b1 (\u03c0\u03b1(a\u03b1|sj)\u2212 \u03c0\u2032\u03b1(a\u03b1|sj) \u2211 a\u03bd \u03c0\u03bd(a\u03bd |sj)P (si|sj , a\u03bd , a\u03b1)|\n= max j \u2211 i | \u2211 a\u03b1 (\u03c0\u03b1(a\u03b1|sj)\u2212 \u03c0\u2032\u03b1(a\u03b1|sj))P (si|sj , a\u03b1)|.\nNow fix any index j, define m\u22a4i = (P (si|sj , ak\u03b1)) |A\u03b1| k=1 , M \u22a4 = (m1, \u00b7 \u00b7 \u00b7 ,m|S|), and n\u22a4 = (\u03c0\u03b1(a k \u03b1|sj)\u2212 \u03c0\u2032\u03b1(ak\u03b1|sj))\n|A\u03b1| k=1 . Then the following holds\u2211\ni | \u2211 a\u03b1 (\u03c0\u03b1(a\u03b1|sj)\u2212 \u03c0\u2032\u03b1(a\u03b1|sj))P (si|sj , a\u03b1)| = \u2211 i |m\u22a4i n| = ||Mn||1 \u2264 ||M ||1||n||1 = 2\u03f5\u03c0||M ||1.\nAccording to the definition of M , it is easy to check\n||M ||1 = max k \u2211 i |P (si|sj , ak\u03b1)| = 1.\nTherefore, we conclude that for any fixed index j, we have\u2211 i | \u2211 a\u03b1 (\u03c0\u03b1(a\u03b1|sj)\u2212 \u03c0\u2032\u03b1(a\u03b1|sj))P (si|sj , a\u03b1)| \u2264 2\u03f5\u03c0,\nwhich proves ||P\u03c0 \u2212 P\u03c0\u2032 ||1 \u2264 2\u03f5\u03c0 . Now we are ready to prove Proposition 3.2.\n||d\u03c0\u03c1 \u2212 d\u03c0 \u2032 \u03c1 ||1 = ||(1\u2212 \u03b3)(I \u2212 \u03b3P\u03c0)\u22121\u03c1\u2212 (1\u2212 \u03b3)(I \u2212 \u03b3P\u03c0\u2032)\u22121\u03c1||1 \u2264 (1\u2212 \u03b3)||(I \u2212 \u03b3P\u03c0)\u22121 \u2212 (I \u2212 \u03b3P\u03c0\u2032)\u22121||1||\u03c1||1 \u2264 (1\u2212 \u03b3)||(I \u2212 \u03b3P\u03c0\u2032)\u22121||1||\u03b3(P\u03c0 \u2212 P\u03c0\u2032)||1||(I \u2212 \u03b3P\u03c0)\u22121||1||\u03c1||1\n\u2264 2\u03f5\u03c0\u03b3 1\u2212 \u03b3 .\nNow we can use Proposition 3.2 to prove Proposition 3.1. To begin with, it is easy to verify that the following holds\nV\u03c1(\u03c0) = \u2211 s d\u03c0\u03c1 (s) \u2211 a \u03c0(a|s)r(s,a).\nNow let us define the marginalized reward r\u03c0(s) = \u2211\na \u03c0(a|s)r(s,a), and further define the vector notation r\u22a4\u03c0 = (r\u03c0(s k)) |S| k=1. Then for the difference of the value function, it holds that\n|V\u03c1(\u03c0)\u2212 V\u03c1(\u03c0\u2032)| = 1\n1\u2212 \u03b3 |\u27e8d\u03c0\u03c1 , r\u03c0\u27e9 \u2212 \u27e8d\u03c0\n\u2032\n\u03c1 , r\u03c0\u2032\u27e9|\n= 1\n1\u2212 \u03b3 |\u27e8d\u03c0\u03c1 , r\u03c0\u27e9 \u2212 \u27e8d\u03c0\u03c1 , r\u03c0\u2032\u27e9+ \u27e8d\u03c0\u03c1 , r\u03c0\u2032\u27e9 \u2212 \u27e8d\u03c0\n\u2032\n\u03c1 , r\u03c0\u2032\u27e9|\n\u2264 1 1\u2212 \u03b3 (|\u27e8d\u03c0\u03c1 , r\u03c0\u27e9 \u2212 \u27e8d\u03c0\u03c1 , r\u03c0\u2032\u27e9|+ |\u27e8d\u03c0\u03c1 , r\u03c0\u2032\u27e9 \u2212 \u27e8d\u03c0 \u2032 \u03c1 , r\u03c0\u2032\u27e9|)\n\u2264 1 1\u2212 \u03b3 (||d\u03c0\u03c1 ||1||r\u03c0 \u2212 r\u03c0\u2032 ||\u221e + ||d\u03c0\u03c1 \u2212 d\u03c0 \u2032 \u03c1 ||1||r\u03c0\u2032 ||\u221e)\n\u2264 1 1\u2212 \u03b3 (2\u03f5\u03c0 + 2\u03f5\u03c0\u03b3 1\u2212 \u03b3 )\n\u2264 2\u03f5\u03c0 (1\u2212 \u03b3)2 .\nD.2 PROOF OF PROPOSITION 3.3\nProof. For any fixed s \u2208 S and a\u03bd \u2208 A\u03bd , the channel that produces the distribution of next state s\u2032 based on the input distribution of a\u03b1 is exactly P (\u00b7 | s, a\u03bd , \u00b7). Therefore, applying the data processing inequality for f -divergence, we prove our proposition.\nD.3 PROOF OF THEOREM 5.3\nIn the following discussions, we will use J\u03f5\u03c0 (\u03bd, \u03b1) and J\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1) interchangeably according to Definition 5.1. Once we have a bounded mismatch coefficient in Definition 5.2, we are ready to analyze the properties of the function J\u03f5\u03c0 (\u03bd, \u03b1) in the following lemma.\nLemma D.1. For any \u03bd, \u03bd\u2032 \u2208 \u2206(A\u03bd)|S| and \u03b1, \u03b1\u2032 \u2208 \u2206(A\u03b1)|S|, the function J\u03f5\u03c0 satisfies \u2022 Lipschitzness\n\u2225\u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1)\u2225 \u2264 \u221a |A\u03bd |\n(1\u2212 \u03b3)2 , \u2225\u2207\u03b1J\u03f5\u03c0 (\u03bd, \u03b1)\u2225 \u2264 \u03f5\u03c0 \u221a |A\u03b1|\n(1\u2212 \u03b3)2 .\n\u2022 Smoothness \u2225\u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1)\u2212\u2207\u03bdJ\u03f5\u03c0 (\u03bd\u2032, \u03b1\u2032)\u2225 \u2264 2 \u221a |A\u03bd | (1\u2212 \u03b3)3 ( \u221a |A\u03bd |\u2225\u03bd \u2212 \u03bd\u2032\u2225+ \u221a |A\u03b1|\u2225\u03b1\u2212 \u03b1\u2032\u2225),\n\u2225\u2207\u03b1J\u03f5\u03c0 (\u03bd, \u03b1)\u2212\u2207\u03b1J\u03f5\u03c0 (\u03bd\u2032, \u03b1\u2032)\u2225 \u2264 2\u03f5\u03c0\n\u221a |A\u03b1|\n(1\u2212 \u03b3)3 ( \u221a |A\u03bd |\u2225\u03bd \u2212 \u03bd\u2032\u2225+ \u221a |A\u03b1|\u2225\u03b1\u2212 \u03b1\u2032\u2225).\n\u2022 Gradient domination\nJ\u03f5\u03c0 (\u03bd, \u03b1)\u2212min \u03b1\u2032 J\u03f5\u03c0 (\u03bd, \u03b1 \u2032) \u2264 C\u03f5\u03c0G 1\u2212 \u03b3 max \u03b1\u0304 \u27e8\u2207\u03b1J\u03f5\u03c0 (\u03bd, \u03b1), \u03b1\u2212 \u03b1\u0304\u27e9, (D.1)\nmax \u03bd\u2032\nJ\u03f5\u03c0 (\u03bd \u2032, \u03b1)\u2212 J\u03f5\u03c0 (\u03bd, \u03b1) \u2264 C\u03f5\u03c0G 1\u2212 \u03b3 max \u03bd\u0304 \u27e8\u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1), \u03bd\u0304 \u2212 \u03bd\u27e9. (D.2)\nProof. With the definition of the function J\u03f5\u03c0 , we have J\u03f5\u03c0 (\u03bd, \u03b1) = V\u03c1(\u03c0\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u03b1). \u2022 For Lipschitzness, it is easy to compute of gradient of J\u03f5\u03c0 with respect to \u03bd and \u03b1 using\nchain rules. Let\u2019s denote \u03c0mix\u03b1 = (1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+\u03f5\u03c0\u03c0\u03b1. Then the following holds using chain rules and standard policy gradient expression Zhang et al. (2021b).\n\u2202J\u03f5\u03c0 (\u03bd, \u03b1)\n\u2202\u03bds,a\u03bd =\n\u2202V\u03c1(\u03c0\u03bd , \u03c0 mix \u03b1 )\n\u2202\u03bds,a =\n1\n1\u2212 \u03b3 d \u03c0\u03bd ,\u03c0\nmix \u03b1\n\u03c1 (s)Ea\u03b1\u223c\u03c0mix\u03b1 [Q \u03c0\u03bd ,\u03c0 mix \u03b1 (s, a\u03bd , a\u03b1)] \u2264\nd \u03c0\u03bd ,\u03c0\nmix \u03b1\n\u03c1 (s) (1\u2212 \u03b3)2 ,\n\u2202J\u03f5\u03c0 (\u03bd, \u03b1)\n\u2202\u03b1s,a\u03b1 =\n\u2202V\u03c1(\u03c0\u03bd , \u03c0 mix \u03b1 )\n\u2202\u03b1s,a\u03b1 = \u03f5\u03c0 1\u2212 \u03b3 d \u03c0\u03bd ,\u03c0 mix \u03b1 \u03c1 (s)Ea\u03bd\u223c\u03c0\u03bd [Q\u03c0\u03bd ,\u03c0 mix \u03b1 (s, a\u03bd , a\u03b1)] \u2264\n\u03f5\u03c0d \u03c0\u03bd ,\u03c0\nmix \u03b1\n\u03c1 (s)\n(1\u2212 \u03b3)2 .\nTherefore, we have \u2225\u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1)\u2225 \u2264 \u221a |A\u03bd | (1\u2212\u03b3)2 , \u2225\u2207\u03b1J\u03f5\u03c0 (\u03bd, \u03b1)\u2225 \u2264 \u03f5\u03c0 \u221a |A\u03b1| (1\u2212\u03b3)2 .\n\u2022 For Smoothness, it holds that \u2225\u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1)\u2212\u2207\u03bdJ\u03f5\u03c0 (\u03bd\u2032, \u03b1\u2032)\u2225 = \u2225\u2207\u03c0\u03bdV (\u03c0\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u03b1)\u2212\u2207\u03c0\u03bdV (\u03c0\u2032\u03bd , (1\u2212 \u03f5\u03c0)\u03c0\u0302\u03b1 + \u03f5\u03c0\u03c0\u2032\u03b1)\u2225\n\u2264 2 \u221a |A\u03bd | (1\u2212 \u03b3)3 ( \u221a |A\u03bd |\u2225\u03bd \u2212 \u03bd\u2032\u2225+ \u221a |A\u03b1|\u2225\u03b1\u2212 \u03b1\u2032\u2225).\nwhere the last step comes from Lemma 19 in (Zhang et al., 2021b). Similarly, using chain rules, it holds that \u2225\u2207\u03b1J\u03f5\u03c0 (\u03bd, \u03b1)\u2212\u2207\u03b1J\u03f5\u03c0 (\u03bd\u2032, \u03b1\u2032)\u2225 = \u03f5\u03c0\u2225\u2207\u03c0mix\u03b1 V (\u03c0\u03bd , \u03c0 mix \u03b1 ) \u2223\u2223 \u03c0mix\u03b1 =(1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+\u03f5\u03c0\u03c0\u03b1 \u2212\u2207\u03c0mix\u03b1 V (\u03c0 \u2032 \u03bd , \u03c0 mix \u03b1 ) \u2223\u2223 \u03c0mix\u03b1 =(1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+\u03f5\u03c0\u03c0\u2032\u03b1 \u2225\n\u2264 2\u03f5\u03c0\n\u221a |A\u03b1|\n(1\u2212 \u03b3)3 ( \u221a |A\u03bd |\u2225\u03bd \u2212 \u03bd\u2032\u2225+ \u221a |A\u03b1|\u2225\u03b1\u2212 \u03b1\u2032\u2225).\n\u2022 The most challenging step is establishing the gradient domination properties. We will show that our objective J\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1) is indeed a value function of some other Markov games G\u0303 with modified transition and reward compared with the original G, which is defined as for any given s, a\u03bd , a\u03b1, s\u2032,\nrmix\u03bd (s, a\u03bd , a\u03b1) = (1\u2212 \u03f5\u03c0) \u2211 a\u2032\u03b1 r\u03bd(s, a\u03bd , a \u2032 \u03b1)\u03c0\u0302\u03b1(a \u2032 \u03b1 | s) + \u03f5\u03c0r\u03bd(s, a\u03bd , a\u03b1),\nPmix(s\u2032|s, a\u03bd , a\u03b1) = (1\u2212 \u03f5\u03c0) \u2211 a\u2032\u03b1 P (s\u2032 | s, a\u03bd , a\u2032\u03b1)\u03c0\u0302\u03b1(a\u2032\u03b1 | s) + \u03f5\u03c0P (s\u2032 | s, a\u03bd , a\u03b1).\nFor this modified Markov game, we denote the corresponding value function as V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), and the stationary state visitation as d\u0303\u03c0\u03bd ,\u03c0\u03b1\u03c1 . Then it is easy to verify that for any \u03c0\u03bd \u2208 \u03a0\u03bd , \u03c0\u03b1 \u2208 \u03a0\u03b1\nJ\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1) = V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), (D.3)\nd\u03c0\u03bd ,(1\u2212\u03f5\u03c0)\u03c0\u0302\u03b1+\u03f5\u03c0\u03c0\u03b1\u03c1 = d\u0303 \u03c0\u03bd ,\u03c0\u03b1 \u03c1 . (D.4)\nSimilarly, we define the mismatch coefficient for G\u0303 as follows:\nCG\u0303 := max{ max\u03c0\u03bd\u2208\u03a0\u03bd min\n\u03c0\u03b1\u2208\u03a0\u0303\u22c6\u03b1(\u03c0\u03bd) \u2225\u2225\u2225\u2225\u2225 d\u0303\u03c0\u03bd ,\u03c0\u03b1\u03c1\u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e , max \u03c0\u03b1\u2208\u03a0\u03b1 min \u03c0\u03bd\u2208\u03a0\u0303\u22c6\u03bd(\u03c0\u03b1) \u2225\u2225\u2225\u2225\u2225 d\u0303\u03c0\u03bd ,\u03c0\u03b1\u03c1\u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e },\nwhere \u03a0\u0303\u22c6\u03b1 (\u03c0\u03bd) := argmin\u03c0\u03b1\u2208\u03a0\u03b1 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), and \u03a0\u0303 \u22c6 \u03bd (\u03c0\u03b1) := argmax\u03c0\u03bd\u2208\u03a0\u03bd V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1). Next, we use the gradient domination property of the value function of Lemma 3 (Zhang et al., 2021b). It holds that for any \u03c0\u2032\u03b1 and \u03c0 \u2032 \u03bd :\nV\u0303\u03c1(\u03c0 \u2032 \u03bd , \u03c0\u03b1)\u2212 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1) \u2264 \u2225\u2225\u2225\u2225\u2225 d\u0303 \u03c0\u2032\u03bd ,\u03c0\u03b1 \u03c1\nd\u0303\u03c0\u03bd ,\u03c0\u03b1\u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e max \u03c0\u0304\u03bd \u27e8\u2207\u03c0\u03bd V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u0304\u03bd \u2212 \u03c0\u03bd\u27e9,\nV\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1)\u2212 V\u03c1(\u03c0\u03bd , \u03c0\u2032\u03b1) \u2264 \u2225\u2225\u2225\u2225\u2225 d\u0303 \u03c0\u03bd ,\u03c0 \u2032 \u03b1 \u03c1\nd\u0303\u03c0\u03bd ,\u03c0\u03b1\u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e max \u03c0\u0304\u03b1 \u27e8\u2207\u03c0\u03b1 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u03b1 \u2212 \u03c0\u0304\u03b1\u27e9.\nTo prove Equation equation D.1, we denote \u03c0\u22c6\u03b1 \u2208 \u03a0\u0303\u22c6\u03b1(\u03c0\u03bd) to be the policy minimizing the quantity \u2225\u2225\u2225\u2225 d\u0303\u03c0\u03bd,\u03c0\u03b1\u03c1 \u03c1 \u2225\u2225\u2225\u2225\n\u221e . Then, we have\nJ\u03f5\u03c0 (\u03bd, \u03b1)\u2212min \u03b1\u2032 J\u03f5\u03c0 (\u03bd, \u03b1 \u2032) = V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1)\u2212min \u03c0\u2032\u03b1 V\u0303\u03c1(\u03c0\u03bd , \u03c0 \u2032 \u03b1)\n= V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1)\u2212 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u22c6\u03b1)\n\u2264 \u2225\u2225\u2225\u2225\u2225 d\u0303 \u03c0\u03bd ,\u03c0 \u22c6 \u03b1 \u03c1\nd\u0303\u03c0\u03bd ,\u03c0\u03b1\u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e max \u03c0\u0304\u03b1 \u27e8\u2207\u03c0\u03b1 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u0304\u03b1 \u2212 \u03c0\u03b1\u27e9\n\u2264 1 1\u2212 \u03b3 \u2225\u2225\u2225\u2225\u2225 d\u0303 \u03c0\u03bd ,\u03c0 \u22c6 \u03b1 \u03c1 \u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e max \u03c0\u0304\u03b1 \u27e8\u2207\u03c0\u03b1 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u0304\u03b1 \u2212 \u03c0\u03b1\u27e9 \u2264 1 1\u2212 \u03b3 CG\u0303 max\u03c0\u0304\u03b1 \u27e8\u2207\u03c0\u03b1 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u0304\u03b1 \u2212 \u03c0\u03b1\u27e9.\nNote due to Equation equation D.3 and equation D.4, we have CG\u0303 = C \u03f5\u03c0 G , and \u2207\u03c0\u03b1 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1) = \u2207\u03c0\u03b1J\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1) = \u2207\u03b1J\u03f5\u03c0 (\u03bd, \u03b1), proving Equation equation D.1.\nTo prove Equation equation D.2, we denote \u03c0\u22c6\u03bd \u2208 \u03a0\u0303\u22c6\u03bd(\u03c0\u03b1) to be the policy minimizing the quantity \u2225\u2225\u2225\u2225 d\u0303\u03c0\u03bd,\u03c0\u03b1\u03c1 \u03c1 \u2225\u2225\u2225\u2225\n\u221e . Therefore, we have that\nmax \u03bd\u2032\nJ\u03f5\u03c0 (\u03bd \u2032, \u03b1)\u2212 J\u03f5\u03c0 (\u03bd, \u03b1) = max\n\u03c0\u2032\u03bd V\u0303\u03c1(\u03c0\n\u2032 \u03bd , \u03c0\u03b1)\u2212 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1)\n= V\u0303\u03c1(\u03c0 \u22c6 \u03bd , \u03c0\u03b1)\u2212 V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1)\n\u2264 \u2225\u2225\u2225\u2225\u2225 d\u0303 \u03c0\u22c6\u03bd ,\u03c0\u03b1 \u03c1\nd\u0303\u03c0\u03bd ,\u03c0\u03b1\u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e max \u03c0\u0304\u03bd \u27e8\u2207\u03c0\u03bd V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u0304\u03bd \u2212 \u03c0\u03bd\u27e9\n\u2264 1 1\u2212 \u03b3 \u2225\u2225\u2225\u2225\u2225 d\u0303 \u03c0\u22c6\u03bd ,\u03c0\u03b1 \u03c1 \u03c1 \u2225\u2225\u2225\u2225\u2225 \u221e max \u03c0\u0304\u03bd \u27e8\u2207\u03c0\u03bd V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u0304\u03bd \u2212 \u03c0\u03bd\u27e9 \u2264 1 1\u2212 \u03b3 CG\u0303 max\u03c0\u0304\u03bd \u27e8\u2207\u03c0\u03bd V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1), \u03c0\u0304\u03bd \u2212 \u03c0\u03bd\u27e9.\nNote due to Equation equation D.3 and equation D.4, we have CG\u0303 = C \u03f5\u03c0 G , and \u2207\u03c0\u03bd V\u0303\u03c1(\u03c0\u03bd , \u03c0\u03b1) = \u2207\u03c0\u03bdJ\u03f5\u03c0 (\u03c0\u03bd , \u03c0\u03b1) = \u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1), proving Equation equation D.2.\nBefore proving Theorem 5.3, we need the following additional lemmas. Firstly, we define \u03d5(\u00b7) := min\u03b1 J\u03f5\u03c0 (\u00b7, \u03b1), and the Moreau envelope for any \u03bb > 0 as \u03d5\u03bb(\u03bd) := max\u03bd\u2032 \u03d5(\u03bd\u2032)\u2212 12\u03bb\u2225\u03bd \u2212 \u03bd \u2032\u22252.\nLemma D.2 (Theorem 31 (Jin et al., 2020)). Suppose the function J\u03f5\u03c0 is l-smooth and L-Lipschitz. Then the output of Algorithm 1 with step size \u03b7t = 1\u221a\nT+1 satisfies\n1\nT T\u2211 t=1 \u2225\u2207\u03d51/2l(\u03bdt)\u22252 \u2264 2 \u00b7 max\u03bd \u03d5(\u03bd)\u2212 \u03d5(\u03bd0) + lL2\u221a T + 1 .\nLemma D.3 (Lemma 12 (Daskalakis et al., 2020)). Suppose J\u03f5\u03c0 is l-smooth and L-Lipschitz. If there is some u\u03bd such that for any \u03bd, \u03b1\nmax \u03bd\u22c6\nJ\u03f5\u03c0 (\u03bd \u22c6, \u03b1)\u2212 J\u03f5\u03c0 (\u03bd, \u03b1) \u2264 u\u03bd max \u03bd\u0304 \u27e8\u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1), \u03bd\u0304 \u2212 \u03bd\u27e9,\nthen it holds that\nmax \u03bd\u22c6\n\u03d5(\u03bd\u22c6)\u2212 \u03d5(\u03bd) \u2264 (u\u03bd + L\n2l )\u2225\u2207\u03d51/2l(\u03bd)\u22252.\nLemma D.4 (Theorem 2a (Daskalakis et al., 2020)). Suppose J\u03f5\u03c0 is l-smooth and L-Lipschitz. If there is some u\u03bd , u\u03b1 such that for any \u03bd, \u03b1\nmax \u03bd\u22c6\nJ\u03f5\u03c0 (\u03bd \u22c6, \u03b1)\u2212 J\u03f5\u03c0 (\u03bd, \u03b1) \u2264 u\u03bd max \u03bd\u0304 \u27e8\u2207\u03bdJ\u03f5\u03c0 (\u03bd, \u03b1), \u03bd\u0304 \u2212 \u03bd\u27e9,\nJ\u03f5\u03c0 (\u03bd, \u03b1)\u2212min \u03b1\u22c6 J\u03f5\u03c0 (\u03bd, \u03b1 \u22c6) \u2264 u\u03b1 max \u03b1\u0304 \u27e8\u2207\u03b1J\u03f5\u03c0 (\u03bd, \u03b1), \u03b1\u2212 \u03bd\u0304\u27e9.\nThen it holds that for \u03b7t\u03b1 = \u03b7\u03b1 = \u0398 ( \u03b44u2\u03b1 l3L2(L/l+1)2 ) , and \u03b7t\u03bd = \u03b7\u03bd = \u0398 ( min{ \u03b4 8u4\u03b1 l5(L/l+1)4L4 , \u03b42 lL2 } )\n, we have that\n1\nT T\u2211 t=1 max \u03bd\u22c6 \u03d5(\u03bd\u22c6)\u2212 \u03d5(\u03bdt) \u2264 ( u\u03bd + L 2l ) \u03b4,\nfor T \u2265 \u2126 (\n(D\u03a0\u03bd+D\u03a0\u03b1 )L \u03b42\u03b7\u03bd ) iterations, where D\u03a0\u03bd and D\u03a0\u03b1 is the radius of the set \u03a0\u03bd , and \u03a0\u03b1.\nNow we are ready to prove Theorem 5.3.\nProof. We start from proving the convergence of Algorithm 1. By substituting the parameters into Lemma D.2, we notice that the parameters satisfy that L \u2264 |A\u03bd |+|A\u03b1|(1\u2212\u03b3)2 , l \u2264 2(|A\u03bd |+|A\u03b1|) (1\u2212\u03b3)3 , u\u03bd = C\u03f5\u03c0G 1\u2212\u03b3 and \u03d5(\u03bd) \u2264 11\u2212\u03b3 . Therefore, we get\n1\nT T\u2211 t=1 \u2225\u2207\u03d51/2l(\u03bdt)\u22252 \u2264 2 \u00b7 max\u03bd \u03d5(\u03bd)\u2212 \u03d5(\u03bd0) + lL2\u221a T + 1 \u2264 2 (1\u2212 \u03b3) \u221a T + 1 + 4(|A\u03bd |+ |A\u03b1|)2 (1\u2212 \u03b3)7 \u221a T + 1 .\nNow we use Lemma D.3, and conclude that\n1\nT T\u2211 t=1 max \u03bd\u22c6 \u03d5(\u03bd\u22c6)\u2212 \u03d5(\u03bdt) \u2264 1\u221a T + 1 poly( 1 1\u2212 \u03b3 , |S|, |A\u03bd |, |A\u03b1|, C\u03f5\u03c0G ).\nTherefore, for Algorithm 1 to achieve accuracy \u03b4, combined with the fact that \u03d5(\u00b7) = \u2212Expl(\u00b7), one needs T = 1\u03b42 poly( 1 1\u2212\u03b3 , |S|, |A\u03b1|, |A\u03bd |, C \u03f5\u03c0 G ) total number of iterations, concluding the proof for Algorithm 1.\nThe proof for Algorithm 2 follows similar steps. We notice the instantiation of the parameters L \u2264 |A\u03bd |+|A\u03b1|(1\u2212\u03b3)2 , l \u2264 2(|A\u03bd |+|A\u03b1|) (1\u2212\u03b3)3 , u\u03bd , u\u03b1 \u2264 C\u03f5\u03c0G 1\u2212\u03b3 , D\u03a0\u03bd , D\u03a0\u03b1 \u2264 \u221a |S|. Bt substituting the parameters into Lemma D.4 and combing it with the fact that \u03d5(\u00b7) = \u2212Expl(\u00b7), we set total number of iterations T = poly( 1\u03b4 , C \u03f5\u03c0 G , |S|, |A\u03b1|, |A\u03bd |, 1 1\u2212\u03b3 ) to achieve accuracy \u03b4, concluding the proof for Algorithm 2."
        },
        {
            "heading": "E EXPERIMENTAL DETAILS",
            "text": "E.1 INTRODUCTION TO THE ENVIRONMENT\nKuhn Poker is a popular research game, which is extensive-form and zero-sum with discrete observation and action space. There exists an efficient min oracle with game tree search. For Robosumo competition, both agents are multi-leg robots and observe the position, velocity, and contact forces of joints in their body, and the position of their opponent\u2019s joints, which is much more challenging due to the high dimensional observation and action space.\nE.2 FEATURE IMPORTANCE IN ROBOSUMO ENVIRONMENTS\nWe show the important features for the Robosumo environment in Table 1, 2, 3.\n."
        },
        {
            "heading": "142 146 21",
            "text": ""
        },
        {
            "heading": "144 23 24",
            "text": "."
        },
        {
            "heading": "193 27 33",
            "text": ""
        },
        {
            "heading": "191 199 36",
            "text": ""
        },
        {
            "heading": "199 34 34",
            "text": ".\nE.3 IMPLEMENTATION DETAILS.\nKuhn Poker. For implementing our adversarial training approach, we adopt both advanced A2C and RPG policy gradient methods. The policy is parameterized by an MLP with a hidden layer size of 128. We use a batch size of 16 and SGD as the optimizer.\nRobosumo competition. We use PPO as our base policy optimization algorithm. We use a clip range of 0.2 and 0.95 for GAE. The number of hidden units for parameterization of the policy is 64. For all adversarial training algorithms with both a single timescale and two timescales and the baseline algorithm self-play and fictitious-play, we train by 3e7 timesteps with a batch size of 64. For attack, we train for 2e7 timesteps with a batch size of 2048. We use Adam as the optimizer.\nE.4 ADDITIONAL EXPERIMENTAL RESULTS\nTrade-off between stealthiness and winning rate. We have mentioned that there is a trade-off between stealthiness and the winning rate. To quantitatively show the trade-off, we show the comparison of our constrained attack and unconstrained one in Figure 6, where \u03f5\u03c0 = \u03bb in the figures.\nAdditional results on the state-distribution shifts. To quantify the state-distribution shift induced due to the unconstrained (global) and constraint (local) attacks, in addition to the Wasserstein\ndistance in Figure 3, we also compute the KL-Divergence and Hellinger distance between the statedistribution of the unattacked policy with the global and local attacks respectively. We observe that the state-distribution shift incurred is much lesser in local attacks than global, as also can be seen in\nFigure 1,3, highlighting the stealthiness of our attack.\nMismatch of attack budgets between training time and test time. We show the attacker\u2019s performance when there is a mismatch between the actual attack budget and the budget used to train the robust victim in the following table. It is clear that even if the defense budget is not correctly specified, the victim policy still shows greatly improved robustness, although a bit worse than the case when the defense budget is correctly specified."
        }
    ],
    "year": 2023
}