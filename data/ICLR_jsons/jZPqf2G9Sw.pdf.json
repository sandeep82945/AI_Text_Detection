{
    "abstractText": "Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein\u2019s dynamical properties for its function, conditioning on these dynamics remains elusive. We present a new approach to protein generative modeling by leveraging Normal Mode Analysis that enables us to capture dynamical properties too. We introduce a method for conditioning the diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding SDE theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of the dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the opensource unconditional protein diffusion model Genie into the conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible. Our work represents a first step towards incorporating dynamical behaviour in protein design and may open the door to designing more flexible and functional proteins in the future.",
    "authors": [],
    "id": "SP:ed6d54e71b5e451d7cf4a659ee0245e88e25c2ef",
    "references": [
        {
            "authors": [
                "Ivet Bahar",
                "Ali Rana Atilgan",
                "Burak Erman"
            ],
            "title": "Direct evaluation of thermal fluctuations in proteins using a single-parameter harmonic potential",
            "venue": "Folding and Design,",
            "year": 1997
        },
        {
            "authors": [
                "Ivet Bahar",
                "Timothy R. Lezon",
                "Ahmet Bakan",
                "Indira H. Shrivastava"
            ],
            "title": "Normal Mode Analysis of Biomolecular Structures: Functional Mechanisms of Membrane Proteins",
            "venue": "Chemical Reviews,",
            "year": 2010
        },
        {
            "authors": [
                "Jacob A. Bauer",
                "Jelena Pavlovi\u0107",
                "Vladena Bauerov\u00e1-Hlinkov\u00e1"
            ],
            "title": "Normal mode analysis as a routine part of a structural investigation",
            "venue": "Sep",
            "year": 2019
        },
        {
            "authors": [
                "Nathaniel Bennett",
                "Brian Coventry",
                "Inna Goreshnik",
                "Buwei Huang",
                "Aza Allen",
                "Dionne Vafeados",
                "Ying Po Peng",
                "Justas Dauparas",
                "Minkyung Baek",
                "Lance Stewart",
                "Frank DiMaio",
                "Steven De Munck",
                "Savvas N. Savvides",
                "David Baker"
            ],
            "title": "Improving de novo protein binder design with deep learning",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Bernard Brooks",
                "Martin Karplus"
            ],
            "title": "Normal modes for specific motions of macromolecules: application to the hinge-bending mode of lysozyme",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1985
        },
        {
            "authors": [
                "Patrick Bryant"
            ],
            "title": "Structure prediction of alternative protein conformations",
            "venue": "bioRxiv, 2023",
            "year": 2023
        },
        {
            "authors": [
                "John-Marc Chandonia",
                "Lindsey Guan",
                "Shiangyi Lin",
                "Changhua Yu",
                "Naomi K Fox",
                "Steven E Brenner"
            ],
            "title": "SCOPe: improvements to the structural classification of proteins \u2013 extended database to facilitate variant interpretation and machine learning",
            "venue": "Nucleic Acids Research, 50(D1):D553\u2013 D559,",
            "year": 2021
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Jeongsol Kim",
                "Michael Thompson Mccann",
                "Marc Louis Klasky",
                "Jong Chul Ye"
            ],
            "title": "Diffusion posterior sampling for general noisy inverse problems",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Byeongsu Sim",
                "Dohoon Ryu",
                "Jong Chul Ye"
            ],
            "title": "Improving diffusion models for inverse problems using manifold constraints",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Justas Dauparas",
                "Ivan Anishchenko",
                "Nathaniel Bennett",
                "Hua Bai",
                "Robert J Ragotte",
                "Lukas F Milles",
                "Basile IM Wicky",
                "Alexis Courbet",
                "Rob J de Haas",
                "Neville Bethel"
            ],
            "title": "Robust deep learning\u2013 based protein sequence design using proteinmpnn",
            "venue": "Science, 378(6615):49\u201356,",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Quinn Nichol"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "NK. Fox",
                "Steven E. Brenner",
                "JM Chandonia"
            ],
            "title": "Scope: Structural classification of proteins\u2014extended, integrating scop and astral data and classification of new structures",
            "venue": "Nucleic Acids Research,",
            "year": 2014
        },
        {
            "authors": [
                "Jean-Fran\u00e7ois Gibrat",
                "Nobuhiro G\u014d"
            ],
            "title": "Normal mode analysis of human lysozyme: study of the relative motion of the two domains and characterization of the harmonic motion",
            "venue": "Proteins: Structure, Function, and Bioinformatics,",
            "year": 1990
        },
        {
            "authors": [
                "Jeanette Held",
                "Sander van Smaalen"
            ],
            "title": "The active site of hen egg-white lysozyme: flexibility and chemical bonding",
            "venue": "Acta Crystallographica Section D: Biological Crystallography,",
            "year": 2014
        },
        {
            "authors": [
                "Ralf Herbrich",
                "Thore Graepel",
                "Colin Campbell"
            ],
            "title": "Bayes point machines",
            "venue": "Journal of Machine Learning Research,",
            "year": 2001
        },
        {
            "authors": [
                "Konrad Hinsen",
                "Gerald R. Kneller"
            ],
            "title": "A simplified force field for describing vibrational protein dynamics over the whole frequency range",
            "venue": "The Journal of Chemical Physics,",
            "year": 1999
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Video diffusion models, 2022",
            "venue": "URL https://arxiv. org/abs/2204.03458",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "V\u0131\u0301ctor Garcia Satorras",
                "Cl\u00e9ment Vignac",
                "Max Welling"
            ],
            "title": "Equivariant diffusion for molecule generation in 3D",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "John Ingraham",
                "Max Baranov",
                "Zak Costello",
                "Vincent Frappier",
                "Ahmed Ismail",
                "Shan Tie",
                "Wujie Wang",
                "Vincent Xue",
                "Fritz Obermeyer",
                "Andrew Beam",
                "Gevorg Grigoryan"
            ],
            "title": "Illuminating protein space with a programmable generative model",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Jing",
                "Stephan Eismann",
                "Pratham N. Soni",
                "Ron O. Dror"
            ],
            "title": "Equivariant graph neural networks for 3d macromolecular structure, 2021a",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Jing",
                "Stephan Eismann",
                "Patricia Suriana",
                "Raphael John Lamarre Townshend",
                "Ron Dror"
            ],
            "title": "Learning from protein structure with geometric vector perceptrons",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Jing",
                "Ezra Erives",
                "Peter Pao-Huang",
                "Gabriele Corso",
                "Bonnie Berger",
                "Tommi Jaakkola"
            ],
            "title": "EigenFold: Generative Protein Structure Prediction with Diffusion Models",
            "year": 2023
        },
        {
            "authors": [
                "Sebastian Bodenstein",
                "David Silver",
                "Oriol Vinyals",
                "Andrew W Senior",
                "Koray Kavukcuoglu",
                "Pushmeet Kohli",
                "Demis Hassabis"
            ],
            "title": "Highly accurate protein structure prediction with AlphaFold",
            "venue": "Nature,",
            "year": 2021
        },
        {
            "authors": [
                "W. Kabsch"
            ],
            "title": "A solution for the best rotation to relate two sets of vectors",
            "venue": "Acta Crystallographica Section A,",
            "year": 1976
        },
        {
            "authors": [
                "W. Kabsch"
            ],
            "title": "A discussion of the solution for the best rotation to relate two sets of vectors",
            "venue": "Acta Crystallographica Section A,",
            "year": 1978
        },
        {
            "authors": [
                "Prashant M Khade",
                "Amit Kumar",
                "Robert L Jernigan"
            ],
            "title": "Characterizing and predicting protein hinges for mechanistic insight",
            "venue": "Journal of Molecular Biology,",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Kunzmann",
                "Kay Hamacher"
            ],
            "title": "Biotite: a unifying open source computational biology framework in python",
            "venue": "BMC Bioinformatics,",
            "year": 2018
        },
        {
            "authors": [
                "Yeqing Lin",
                "Mohammed AlQuraishi"
            ],
            "title": "Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Zeming Lin",
                "Halil Akin",
                "Roshan Rao",
                "Brian Hie",
                "Zhongkai Zhu",
                "Wenting Lu",
                "Nikita Smetanin",
                "Allan dos Santos Costa",
                "Maryam Fazel-Zarandi",
                "Tom Sercu",
                "Sal Candido"
            ],
            "title": "Language models of protein sequences at the scale of evolution enable accurate structure prediction",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Lou",
                "Stefano Ermon"
            ],
            "title": "Reflected diffusion models",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2023
        },
        {
            "authors": [
                "Karolis Martinkus",
                "Andreas Loukas",
                "Nathanael Perraudin",
                "Roger Wattenhofer"
            ],
            "title": "Spectre : Spectral conditioning helps to overcome the expressivity limits of one-shot graph generators",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Christine A Orengo",
                "Alex D Michie",
                "Susan Jones",
                "David T Jones",
                "Mark B Swindells",
                "Janet M Thornton"
            ],
            "title": "Cath\u2013a hierarchic classification of protein domain structures",
            "year": 1997
        },
        {
            "authors": [
                "David Perahia",
                "Liliane Mouawad"
            ],
            "title": "Computation of low-frequency normal modes in macromolecules: improvements to the method of diagonalization in a mixed basis and application to hemoglobin",
            "venue": "Computers & chemistry,",
            "year": 1995
        },
        {
            "authors": [
                "Herbert E. Robbins. An empirical bayes approach to statistics."
            ],
            "title": "URL https://api",
            "venue": "semanticscholar.org/CorpusID:26161481.",
            "year": 1956
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Florence Tama",
                "Yves-Henri Sanejouand"
            ],
            "title": "Conformational change of proteins arising from normal mode calculations",
            "venue": "Protein engineering,",
            "year": 2001
        },
        {
            "authors": [
                "Brian L. Trippe",
                "Jason Yim",
                "Doug Tischer",
                "David Baker",
                "Tamara Broderick",
                "Regina Barzilay",
                "Tommi S. Jaakkola"
            ],
            "title": "Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "David J Vocadlo",
                "Gideon J Davies",
                "Roger Laine",
                "Stephen G Withers"
            ],
            "title": "Catalysis by hen egg-white lysozyme proceeds via a covalent intermediate",
            "year": 2001
        },
        {
            "authors": [
                "Kevin E. Wu",
                "Kevin K. Yang",
                "Rianne van den Berg",
                "James Y. Zou",
                "Alex X. Lu",
                "Ava P. Amini"
            ],
            "title": "Protein structure generation via folding diffusion, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jinrui Xu",
                "Yang Zhang"
            ],
            "title": "How significant is a protein structure similarity with tm-score = 0.5? Bioinformatics, 26(7):889\u201395",
            "venue": "Apr 2010. doi: 10.1093/bioinformatics/btq066. URL https: //doi.org/10.1093/bioinformatics/btq066",
            "year": 2010
        },
        {
            "authors": [
                "Zhang Yi",
                "Yan Fu",
                "Hua Jin Tang"
            ],
            "title": "Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix",
            "venue": "Computers Mathematics with Applications,",
            "year": 2004
        },
        {
            "authors": [
                "Jason Yim",
                "Brian L Trippe",
                "Valentin De Bortoli",
                "Emile Mathieu",
                "Arnaud Doucet",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Se (3) diffusion model with application to protein backbone generation",
            "venue": "arXiv preprint arXiv:2302.02277,",
            "year": 2023
        },
        {
            "authors": [
                "tension Springcraft (Kunzmann",
                "Hamacher"
            ],
            "title": "2018), which we used and rewrote into a PyTorch differentiable version. Before any equations of motion can be written, one must specify a force field that describes interactions between residues. We use a Hinsen force-field (Hinsen & Kneller, 1999) with a cutoff of 16 \u00c5. For the choice of the strain target we perform the strain-energy calculation as described in Hinsen ",
            "year": 1999
        },
        {
            "authors": [
                "C HINGE"
            ],
            "title": "TARGETS DESCRIPTION To extract targets with a prominent hinge motion we performed a literature survey",
            "venue": "We identified the lysozyme (Gibrat & Go\u0304,",
            "year": 1990
        },
        {
            "authors": [
                "Held",
                "van Smaalen"
            ],
            "title": "To obtain the NMA target for this motif, we perform an NMA with an invariant force-field and with a 13 \u00c5 distance threshold on the C\u03b1-backbone of the native protein (6lyz) and extract the lowest non-trivial normal mode displacements for the motif residues as NMA target",
            "year": 2014
        },
        {
            "authors": [
                "Yim"
            ],
            "title": "2023)) for motif-only as well as motif+NMA conditioning. NMA loss The presence of a functional motif defines a reference coordinate system, namely the coordinate system in which the coordinates of the to-be-scaffolded motif are given in. Notably, this means that the normal mode displacements at the motif residues are also given in the motif\u2019s",
            "year": 2023
        },
        {
            "authors": [
                "Yim"
            ],
            "title": "met the criteria of scTM>0.5, scRMSD<2\u00c5, with confidence of ESMFold predictions thresholded at pLDDT>70, pAE<10, which aligns with definitions in prior work (Watson et al",
            "venue": "AlQuraishi",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Generative Artificial Intelligence (AI) has rapidly accelerated protein design research. A common problem tackled with AI is the task of protein backbone design, which is finding a new and realistic 3D structure tailored to the specific biological function. Recently, AI models based on the probabilistic diffusion framework have shown remarkable success in generating realistic protein backbones, especially backbones with pre-defined, fixed parts (Watson et al., 2022; Trippe et al., 2023). Since many functions have been linked to the presence of various functional motifs, enforcing the generation process to preserve such substructures is crucial in meaningful protein design. However, all of the recent protein diffusion models overlook an important aspect of protein design - structure alone is not enough to determine the protein\u2019s functional properties. Information about protein flexibility, especially about its low-frequency collective motion, is crucial in determining protein functional properties (Bauer et al., 2019). In this work, we address this research gap and provide a framework for a diffusion model conditioned not only on the structure but also on the protein dynamics.\nWe analyse the protein dynamics through the lens of the Normal Mode Analysis (NMA) (Bahar et al., 2010). This is a simple, yet powerful method for obtaining eigenvectors of the motion of the protein residues and their relative displacements in each mode. After performing NMA on a reallife protein with known functionality, the obtained eigenvectors can be used as the dynamics targets when using a diffusion model to sample a novel backbone. We are particularly interested in the case of protein hinges, which are widely related to a number of protein functions and which are strongly constrained in both structure and dynamics (Khade et al., 2020). Protein hinges usually involve two secondary structure elements rotating against each other about the common axis, similar to how a hinge at the door frame has closing and opening motions.\nOur contributions are as follows:\n\u2022 We introduce a new methodology for conditioning protein generation on the dynamical properties. Our approach is based on NMA which is easy to compute and captures collective motions related to the protein functions. Moreover, we demonstrate how the conditioning on the desired relative displacements, which we refer to as dynamics conditioning, can be accompanied by structure conditioning. To substantiate this joint conditioning theoretically, we present a formal interpretation in terms of stochastic differential equations."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 DIFFUSION PROBABILISTIC MODELING",
            "text": "The generative process in the diffusion probabilistic models (Sohl-Dickstein et al., 2015) starts with a sample from the standard normal distribution, xT \u223c N (0, 1). The goal of this process is to transform xT into the sample x0 from the real-life data distribution p0(x0), initially unknown and indirectly accessed by the trained model.\nThe key idea is to formulate the model training as the forward diffusion process in which the model predicts how much noise was added to the original sample. For the sample from the training set\nx0, the forward process is defined as iteratively adding a small amount of Gaussian noise to the sample in T steps, which produces a sequence of noisy samples x0:T such that the final sample xT \u223c N (0, 1). In the Denoising Diffusion Probabilistic Modeling (DDPM) framework (Ho et al., 2020) the noise magnitude at each step is defined by a variance schedule {\u03b2t, t \u2208 [0 : T ]} such that\npt(xt|xt\u22121) = N (xt, \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI). (1)\nThe above transition defines a Markov process in which the original data is transformed into a standard normal distribution. It is possible to write the density of xt given x0 in a closed form Fixed typo Eq. 2\npt(xt|x0) = N (xt, \u221a \u03b1\u0304tx0, (1\u2212 \u03b1\u0304t)I), (2)\nwhere \u03b1\u0304t = \u220ft\ni \u03b1i and \u03b1i = 1\u2212 \u03b2i. This property allows for expressing xt in terms of the original sample x0 and Gaussian noise \u03f5t \u223c N (0, 1):\nxt = \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5t. (3)\nThen, transforming a sample xT into the sample x0 is done in a number of updates that reverses the destructive noising, given by a reverse sampling scheme\nxt\u22121 = 1 \u221a \u03b1t\n( xt \u2212\n\u221a 1\u2212 \u03b1t 1\u2212 \u03b1\u0304t \u03f5\u03b8(xt, t)\n) + (1\u2212 \u03b1t)z. (4)\nwhere z \u223c N (0, 1). The neural network \u03f5\u03b8 (the denoiser) should be trained to predict noise added Addressed RFtGWto x0. Ho et al. (2020) showed the following loss function is sufficient\nL = Ex0,t ( ||\u03f5t \u2212 \u03f5\u03b8( \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5t, t)||2 ) . (5)\nSong et al. (2021) state that the DDPM is an example from the larger class of models called scorebased models. They demonstrated that the discrete forward and reverse diffusion processes have their continuous time equivalents, that is, forward Stochastic Differential Equation\ndx = \u22121 2 \u03b2(t)xdt+\n\u221a \u03b2(t)dw, (6)\nand its reverse\ndx = [ \u22121 2 \u03b2(t)\u2212 \u03b2(t)\u2207x ln pt(x) ] dt+ \u221a \u03b2(t)dw\u0304, (7)\nwhere the quantity \u2207xt ln pt(xt) is called the score and is closely related to the noise in DDPM by the equivalence \u2207xt ln pt(xt) = \u2212\u03f5t/ \u221a 1\u2212 \u03b1\u0304t (derivation in the Appendix E). Any model trained to predict the noise can be written in terms of the score, which is an essential property of our work. Whenever we derive some expression with respect to the score, we can revert it into the noise and use the noise-based formulation for forward and reverse diffusion processes simply substituting \u03f5t = \u2212 \u221a 1\u2212 \u03b1\u0304t\u2207xt ln pt(xt).\nRelated work on Diffusion Probabilistic Models for protein design. In the context of protein generative modeling, the real data samples x0 are often represented by coordinates of backbone C\u03b1 atoms, optionally with amino-acid identity as a scalar features. Protein diffusion models operating Addressed RFtGWon such representations were shown to generate designable and novel samples to various degrees (Lin & AlQuraishi, 2023; Ingraham et al., 2022; Watson et al., 2022; Yim et al., 2023). Some of those were additionally designed to condition the sample on properties such as substructure, symmetry or structural motif; however, none of those works link the function to dynamics. Motif scaffolding has been done with methods such as providing the denoised motif residues positions in the conditional training (Watson et al., 2022), by the particle filtering method (Trippe et al., 2023), or by empirically estimating the chances that the sample will have the query motif (Ingraham et al., 2022). Eigenfold (Jing et al., 2023) attempts to incorporate the physical constraints for oscillations into the diffusion kernel, however, it did not improve the sample quality and it was not tested to check whether it changes the novel sample dynamics."
        },
        {
            "heading": "2.2 NORMAL MODE ANALYSIS",
            "text": "NMA is a technique for describing collective motions of protein residues. It assumes that a protein is in the energy minimum state in a given force field, and the protein residues will undergo the\nharmonic motions about their minima (Bahar et al., 2010). Amplitudes and frequencies of such oscillations are the solutions to the equations of motions for all residues. Those equations of motions are compactly written in the matrix form Mx\u0308 = \u2212Kx, where x \u2208 R3N is a flattened vector of coordinates of N residues, M \u2208 R3N\u00d73N is a mass matrix and K \u2208 R3N\u00d73N is interaction constants matrix derived from the force-field that describes strength of the interactions between Addressed RFtGWresidues. Despite the simplistic assumptions about the form of the force fields, NMA has been shown to successfully explain a number of phenomena amongst numerous proteins (Gibrat & Go\u0304, 1990; Tama & Sanejouand, 2001; Bahar et al., 1997). Most of protein functional properties are related to the low-frequency motions, mathematically represented as the lowest non-trivial eigenvectors of the matrix equation."
        },
        {
            "heading": "3 METHODS",
            "text": "Consider the following problem: given a target matrix yD \u2208 R|C|\u00d73, where rows correspond to displacement vectors of C residues, we aim to generate a new protein whose selected residues have the displacement vectors in their non-trivial lowest normal mode close to those defined by yD. We use a coarse-grained protein representation, where each residue is represented with the C\u03b1 carbon Addressed RFtGWonly and we aim to obtain new C\u03b1 chains that satisfy the dynamics constraint. To tackle this problem we employ score-based generative modeling (Song et al., 2021). We formulate the agreement of the displacement with a target as a condition in the reverse process and quantify the notion of \u2018similar dynamics\u2019 with a custom loss function."
        },
        {
            "heading": "3.1 CONDITIONING DIFFUSION MODELS",
            "text": "The goal of conditional generative modeling is to sample from the posterior p(x0|y) such that new samples x0 satisfy some chosen property y. We specify the following model\np(x0|y) = p(x0) exp[\u2212l(y, v(x0))]\u222b p(x0) exp[\u2212l(y, v(x0))]dx0\nand \u03ba(y) = \u222b p(x0) exp[\u2212l(y, v(x0))]dx0 (8)\nwhere l(y, v(x0)) measures the loss for a measurement of y at x0, \u03ba(y) is the normalisation constant and v(x) maps to the relevant physical quantity represented by y. This specification, as shown in Song et al. (2023), allows for guiding a trained unconditional model along the path specified by the loss l. Finding an appropriate p(y|x0) is where the novelty of our method lies. For the dynamics target y, if p(y|x0) was a neural network, it would need to approximate the eigenvectors of an arbitrary symmetric matrix. To the best of our knowledge, finding matrix eigenvectors for ANY variable size symmetric matrix with a neural network is not considered a solved problem yet (there exist neural network approaches to find eigenvectors, but those require retraining for every new matrix (Gemp et al., 2021; Yi et al., 2004), and are not suitable for a large dataset of backbone structures). A method to reconstruct a graph structure from a set of learned eigenvectors via an interactive Laplacian matrix refined is presented in Martinkus et al. (2022). however this approach has never been tested for a reverse reconstruction. We escape the need to train a neural network and equate p(y|x0) to a simple analytical function. One of the most common mathematical frameworks to obtain a novel sample with any desired property y consists of estimating conditional scores. Different approximations for estimating said score have given rise to a variety of methods such as classifier guidance (Dhariwal & Nichol, 2021), classifier free guidance (Ho & Salimans, 2022), and \u2018reconstruction guidance\u2019 (Ho et al.; Chung et al., 2022a). What all these approaches have in common is that they decompose the conditional score as\n\u2207xt ln pt(xt|y) = \u2207xt ln p(y|xt) +\u2207xt ln pt(xt), (9) where p(y|xt) is a probability that the sample meets the condition at t = 0 given the state xt at some other time. Following Chung et al. (2022a), we re-express it with the integral\np(y|xt) = \u222b p(y|x0)p0(x0|xt)dx0. (10)\nThe integral is intractable and furthermore, we can not evaluate p0(x0|xt) directly. But as in Chung et al. (2022a) we overcome this via the approximation of the denoiser\u2019s transition density with a delta function centred at the mean\np0(x0|xt) \u2248 \u03b4E [x0|xt](x0). (11)\nSuch approximations to the posteriors via point masses centred at their means rather than their modes (MAP) are known as Bayes point machines (Herbrich et al., 2001), and have been shown to outperform MAP. Under this approximation, the entire integral simplifies to\np(y|xt) \u2248 p(y|E [x0|xt]). (12)\nVia the Tweedies formula (Chung et al., 2022a), the expected output of the model at t = 0 is\nE [x0|xt] = xt + (1\u2212 \u03b1\u0304t)s(xt, y)\u221a\n\u03b1\u0304t . (13)\nUnder our model specification, via Bayes rule\np(y|E [x0|xt]) = p(E [x0|xt]|y)p(y)/p(E [x0|xt]), (14)\nsubstituting back into the score we obtain\n\u2207xt ln\n( p(E [x0|xt]) exp[\u2212l(y, v(E [x0|xt]))]\np(E [x0|xt])\u03ba(y) p(y)\n) = \u2212\u2207xt l(y, v(E [x0|xt])). (15)\nDepending on the quantity y, different losses must be used in Equation 15. Note that even though the derivations are done in continuous time, the equivalence of the score and the noise still applies, and we can use the discretised sampling scheme as in Equation 4. Now we explain our choices for the dynamics and structure conditioning losses."
        },
        {
            "heading": "3.2 DYNAMICS LOSS",
            "text": "The next step is to define the loss function in Equation 15 that enforces the targeted dynamics while being invariant to the protein rotations and translations. Knowing the expected residues\u2019 positions at t = 0 and the expected components of the normal mode of the conditioned residues given structure xt at some time t, the invariance is preserved if one compares the relative pairwise angles between the displacement vectors and their relative magnitudes. Moreover, this makes the conditioning target independent of the protein length: eigenvectors are normalised hence the amplitudes of displacements of a subset of residues depend on the protein length. Therefore, we propose to use the following loss in Equation 15 which is a simple combination of amplitude and angle terms between all pairwise residues. For the rest of this work, we refer to it as the NMA-loss.\nlNMA(yD, v(x)) = langle(yD, v(x)) + lampl(yD, v(x)), (16) langle = \u2211 i,j\u2208C | cos(yD,i, yD,j)\u2212 cos(v(xt)i, v(x)j)|, (17)\nlampl = \u2211 i\u2208C \u2223\u2223\u2223\u2223 ||yD,i||||yD|| \u2212 ||v(x)i||||v(x)|| \u2223\u2223\u2223\u2223 . (18)\nIn this invariant loss, yD,i and v(x)i are displacement vectors of residue i \u2208 C in the target yD and in the displacements matrix v(x) \u2208 R|C|\u00d73 derived from expected positions at t = 0. The amplitude Addressed RFtGWterms are normalised such that only their relative sizes matter, consistent with the fact that amplitude information from NMA can only make relative statements about the participation of a given residue in a mode (Bahar et al., 2010). For the combined loss, in the process of minimisation of NMA-loss in the sampling steps, the lampl is scaled by 2, such that its contribution is similar in magnitude to langle. We compute the NMA-loss using a differentiable implementation of the eigenvector calculations assuming the Hinsen force-field ((Hinsen & Kneller, 1999), more details in the Appendix B.2)."
        },
        {
            "heading": "3.3 STRUCTURE LOSS AND JOINT CONDITIONING",
            "text": "The essential part of our work is building a connection between conditioning on dynamics and conditioning on structure. Even though dynamics and structure are correlated, there are many structures that will have similar low-frequency eigenvectors, and there is no guarantee that the particular protein packing will correspond to the biological function for which the dynamics were designed. Therefore, the dynamics conditioning must be accompanied by structure conditioning. Structure conditioning enforces the novel protein to have a subset of residues CM positioned in pre-defined\nrelative positions. For example, structure conditioning might enforce the presence of a given functional motif M somewhere in the arbitrarily rotated protein. We denote the target positions as yM \u2208 R|CM |\u00d73, and xCM \u2208 R|CM |\u00d73 is the prediction of conditioned residues\u2019 positions at t = 0 Addressed RFtGWthat model makes in the sampling. In the language of score-based generative modeling, the conditional score for the joint target (yD, yM ) will be now decomposed into three terms\n\u2207xt ln pt(xt|yD, yM ) = \u2207xt ln p(yD|xt) +\u2207xt ln p(yM |xt) +\u2207xt ln pt(xt). (19)\nFinally, the appropriate structure loss should be substituted to the\u2207xt ln p(yM |xt) term. We define the structure loss to be the misalignment between yM and xCM , specifically the L1 loss between all CM residues\u2019 coordinates. In order not to violate equivariance, we use our custom differentiable implementation of the Kabsch algorithm (Kabsch, 1976; 1978) to find the best fit of the target residues yM and xCM at the reverse diffusion step and only then compute the misalignment. In the discussion of the results, we report the final root-mean-square deviation (RMSD), which is related to but different from the structure loss (see Section 5.2)."
        },
        {
            "heading": "4 MODELS AND THE EXPERIMENTAL SETUP",
            "text": "The aim of the experimental evaluation is two-fold. Firstly, we test whether the proposed conditioning method indeed results in better agreement between the target and the novel structure\u2019s dynamics. To do so, we use our custom denoiser model, perform conditional sampling using a large number of dynamics targets and examine the conditioning effectiveness. Secondly, we utilise Genie (Lin & AlQuraishi, 2023), the diffusion model able to produce high-quality samples, and modify its sampling scheme with our joint conditioning. We therefore demonstrate the universality of our framework which leaves an open path to transferring our method to other large protein diffusion models. Modified Genie produces samples conditioned on the hinge targets which we thoroughly evaluate with respect to designability."
        },
        {
            "heading": "4.1 MODELS",
            "text": "Custom GVP. The main building block of our model is an equivariant denoiser. We use a modified version of the Geometric Vector Perceptron-Graph Neural Network (Jing et al., 2021b;a) with 5 layers (details in the Appendix B.1). The denoiser was trained with the loss function given by Equation 5. We use the Hoogeboom schedule (Hoogeboom et al., 2022) with a 250-step DDPM discretisation scheme. The model was trained for 1000 epochs with a learning rate of 1e-4.\nGenie. Genie (Lin & AlQuraishi, 2023) is a diffusion probabilistic model with the DDPM discretisation. It takes advantage of the protein geometry by extracting the Frenet-Serret frames of residues at each noise prediction step, which are then passed to the SE(3)-equivariant denoiser. Ge- Addressed RE2Vz nie outperformed other models such as ProtDiff (Trippe et al., 2023), FoldingDiff (Wu et al., 2022) or FrameDiff (Yim et al., 2023), and remains comparable to RFDiffusion (Watson et al., 2022). For our experiments, we used the published weights of the model trained on the SCOPe dataset (Fox et al., 2014; Chandonia et al., 2021) able to work with proteins up to 256 residues long."
        },
        {
            "heading": "4.2 DATASET AND TARGETS",
            "text": "For our custom model training, we extract all short monomeric CATHv4.3 domains (Orengo et al., 1997) for structures with high resolution (< 3A\u030a), of lengths between 21-112 amino acids, clustered 95% sequence similarity to remove redundancy. The resulting dataset contained 10037 protein structures. We extract random and strain dynamics targets from the proteins in the validation set. Random targets are the displacements in the randomly chosen sets of 10 consecutive residues; for the strain targets, we perform strain-energy calculation (Hinsen & Kneller, 1999) (details in the Appendix B.2) and choose 10 consecutive residues with the largest summed energy.\nJoint conditioning imposes constraints on both the protein normal mode and the specific residues\u2019 positions. Biologically relevant targets that require such constraints are the hinge parts of proteins. Three proteins were selected from the literature: lysozyme (PDB ID: 6lyz), adenylate kinase (PDB ID: 3adk), and haemoglobin (PDB ID: 2hhb). In each protein we found which residues participate\nin the hinge motions \u2013 those residues constitute the yM targets. For each protein we perform NMA calculation to obtain the displacements of the hinge residues \u2013 the yD targets (details in Appendix C)."
        },
        {
            "heading": "4.3 EVALUATION METRICS",
            "text": "Population level. For the first set of experiments investigating the dynamics conditioning, we focus on the quick-to-compute statistics of the large sample set to understand the expected effects of the conditioning on the sample quality. Apart from the NMA-loss, we check the sample quality using: (1) the mean chain distance (C\u03b1 \u2212 C\u03b1) that should be close to 3.8 A\u030a (2) the radius of Addressed RFtGWgyration of the backbone, which is an indicator of whether the model produces samples with an adequate compactness; (3) secondary structure statistics (SSE), that is, the proportion of \u03b1-helices, \u03b2-sheets and disordered loops; (4) novelty in terms of the TM-score to the closest structure in the train set. TM-score measures the topological similarity of protein structures and has values in the range [0, 1]. TM-score > 0.5 suggests two structures are in the same fold (Xu & Zhang, 2010).\nDetailed statistics. In the case of joint conditioning, we sample novel protein backbones using Addressed RFtGWGenie and check the designability of the new samples using the same in silico evaluation pipeline as in benchmarking unconditional Genie. For each backbone sample, we obtain 8 ProteinMPNN generated sequences and fold each sequence with ESMFold (Lin et al., 2022). We calculate the self- Addressed RFtGWconsistency TM-scores (smTM-scores), that is, the TM-scores between the input structure and each of the ESMFold predictions. scTM-score was also considered in other works (Trippe et al., 2023; Lin & AlQuraishi, 2023) as one of the standard metrics for sample quality evaluation. We report the proportion of conditional samples whose best scTM-score to one of the ESMFold designed structures is > 0.5, in the same fashion as in Trippe et al. (2023) that tackles a similar motif conditioning problem."
        },
        {
            "heading": "4.4 SAMPLING DETAILS.",
            "text": "Dynamics conditioning with GVP. The sampling process consisted of 250 reverse diffusion steps. We do not add noise at the last two generation time steps, since we found that this results in chain distances remaining closer to 3.8 A\u030a. It is a common practice to upscale the conditional term \u2207xt ln p(y|x) by some guidance scale (Dhariwal & Nichol, 2021). Guidance scales for strain targets were time-dependent and equal to 200\u03b1t for strain targets and 400\u03b1t for random targets. Conditioning was switched on in the middle of the generation process. We extracted 300 strain and 300 random targets from 300 randomly sampled proteins from the validation set. For each target, we took 3 conditional and unconditional samples, and from each 3 we selected the one with the lowest NMA-loss. Each sample had the same length as the protein from which the target was extracted, which assures that potential differences observed in SSE cannot be attributed to differences in the protein length distributions.\nJoint conditioning with Genie. The original Genie sampling loop with 1000 time steps in the generation was modified to include the conditional score. The guidance scales were different for each target and in the order of 2000-3000. More details about Genie sampling are in the Appendix B.3."
        },
        {
            "heading": "5 RESULTS AND DISCUSSION",
            "text": ""
        },
        {
            "heading": "5.1 STRAIN AND RANDOM DYNAMICS TARGETS",
            "text": "Here we present the results for the strain and random dynamics targets. At the start, we filter out the \u2018low quality\u2019 samples whose mean chain distance is outside [3.75, 3.85] A\u030a interval (proteins with mean chain distance more extreme are rare in nature (Voet & Voet, 2010)). Occasionally during Addressed RFtGWconditional sampling a coordinate increases by orders of magnitude along the sampling trajectory, or even becomes NaN. This divergence effect has also been observed in many conditional diffusion models (Lou & Ermon, 2023) and in our case tends to happen when the conditioning pushes a sample\u2019s coordinates outside of the realm of observed samples for which the denoiser was trained, underlines that finding a right balance between the NMA-loss driven part of the score and the unconditional part of the score requires striking a delicate balance. Those diverged samples are also filtered out. In the end, about 20% of samples in each category were filtered out due to low-quality\nchain distances. First, we examine if the conditioning has the desired effect of enforcing the target normal mode. Figure 2 shows that indeed, the NMA-loss is successfully minimised in the conditional samples as compared to the unconditional ones. There is a significant shift of the NMA loss towards lower values, which is the first sign of the method\u2019s success. Note that both the target normal mode and the mode of the newly sampled structure must obey some physical constraints imposed on all proteins and the degrees of freedom of all relative displacements are limited, therefore it is occasionally possible to obtain low loss for the unconditional sample. Encouraged by this finding, we proceed to the visual inspection of the samples. Figure 3 shows a pair of conditional and unconditional samples for one of the strain targets (additional sampled pairs are in Appendix F). There is a better alignment of the displacement vectors and target vectors for the conditional sample as compared to the unconditional one, which we also consistently observed for the rest of the sampled pairs. We conclude that our conditioning has the desired effect of enforcing the target dynamics, therefore, we proceed to the quality check of the samples \u2013 we must ensure the conditioning does not compromise the backbone structure. To ensure that the sampled proteins are still biologically valid, we evaluate their geometry. In the end, we investigate the samples\u2019 novelty as our sanity check whether the diffusion model has not simply memorised the train set.\nFigure 4 shows the SSE and Rg of the samples compared to the train CATH dataset. Unconditional samples show a variety of SSE in proportions close to the CATH dataset. Interestingly, we found that conditioning increases the proportion of \u03b2-sheets at the expense of \u03b1-helices. Rg distributions of both unconditional and conditional samples have a visible overlap with the CATH Rg distribution, the second one is shifted to larger values (but remains within the Rg values observed in CATH). Therefore, while the conditional samples do not violate the physical constraints, the dynamics conditioning introduces the changes in the protein packing. Whether this effect is significant for the potential downstream applications, and how strong it is when the conditioning is transferred into problem-specific models, should be an object of further investigation. Respective Figures for the random targets can be found in Appendix A. Lastly, we calculate the novelty of the samples\nexpressed in terms of TM-score to the closest structure in the train set. Both unconditional and conditional samples of both target types were highly novel, with TM-score lower than 0.5 in 90% of the samples."
        },
        {
            "heading": "5.2 HINGE TARGET",
            "text": "Finally, we present results for the joint conditioning. The conditional samples were filtered using criteria of mean chain distances outside [3.75, 3.85] A\u030a interval and RMSD with respect to the motif smaller than 1 A\u030a. These constraints left us with 43%, 60% and 23% of the conditional samples for lysozyme, adenylate kinase and haemoglobin, respectively, such that we ended up with 27 conditional samples. To match that number, we sampled 27 unconditional ones. In the analysis of the remaining samples, we considered the distributions of NMA-loss (Figure 5) and scTM-score. The Addressed RE2Vz distribution of the NMA-loss confirms that our method can enforce the specific dynamics and conditions on the structure at the same time. Analysis of the designability revealed that the distribution of scTM-scores depends on the target we use. The proportions of conditional samples with scTMscore > 0.5 were 0.48, 0.78, 0.41 for lysozyme, adenylate kinase and haemoglobin, respectively. Interestingly, when we sampled 27 structures just with the hinge dynamics conditioning, those values were 0.93, 1.0, and 0.89, respectively, and the decrease in designability can be attributed purely to the difficulties in the structure conditioning (more details in Appendix D). We finish with the visual investigation of the generated hinge structures. Figure 1 shows pairs of the targets and the new samples (more examples in the Appendix F). The new samples indeed possess the hinge structure, as well as the hinge-like low-frequency motion."
        },
        {
            "heading": "6 CONCLUSIONS AND FURTHER WORK",
            "text": "For the first time, we condition the protein diffusion model on dynamics, thus paving a way to designing more functional proteins in the future. We also make the code publicly available1. We generate novel proteins with a pre-defined lowest non-trivial normal mode of oscillation for a subset of residues. The large-scale statistics show that the conditioning is effective and can be transferred to already trained unconditional models. The extended version of the conditioning that includes the structure conditioning is implemented as part of an unconditional model Genie and we produce novel proteins that exhibit hinge structure and dynamics while remaining designable by the scTM criteria. Further work includes integrating the dynamics conditioning with other types of structure conditioning, different from the one based on the Kabsch alignment. Also, more diverse targets should be analysed in future evaluation to fully assess the dynamics conditioning for varied types of motion."
        },
        {
            "heading": "A POPULATION STATISTICS FOR RANDOM TARGETS",
            "text": "B IMPLEMENTATION DETAILS\nB.1 CUSTOM MODEL DETAILS\nThe custom model was based on the Geometric Vector Perceptron-Graph Neural Network architecture Jing et al. (2021b;a). Each protein was represented as a fully connected graph. The node scalar features were sinusoidal positional embeddings of the residues\u2019 order in the chain concatenated with a normalised time step feature. We perform a message-passing on a fully connected graph of C\u03b1 carbons. Edge features were distances between nodes in terms of 16 Gaussian radial basis functions and the unit vectors pointing along the edge. The model used 5 GVP-Convolutions layers and the output of the network (the noise) had the centre of mass subtracted to ensure equivariance.\nB.2 NMA CALCULATIONS DETAILS\nNowadays quick and ready-to-use implementations of NMA are available, such as the Biotite extension Springcraft (Kunzmann & Hamacher, 2018), which we used and rewrote into a PyTorch differentiable version.\nBefore any equations of motion can be written, one must specify a force field that describes interactions between residues. We use a Hinsen force-field (Hinsen & Kneller, 1999) with a cutoff of 16 A\u030a. For the choice of the strain target we perform the strain-energy calculation as described in Hinsen & Kneller (1999)\nEi = 1\n2 N\u2211 j k(Rij) |(di \u2212 dj) \u00b7Rij |2 |Rij |22 (20)\nwhere Ei is the energy of residue i, Rij is a vector that is the equilibrium separation between the residues i, j, k(Rij) is the interaction constant, and di,dj are the displacements of residues i, j in the mode to be analyzed (here, the lowest non-trivial normal mode).\nB.3 SAMPLING DETAILS\nTo take samples with the Genie model we used an additional parameter \u03b7 to downscale the noise in the reverse process, as recommended in the Genie publication (Lin & AlQuraishi, 2023). We set \u03b7 = 0.4 which was shown to achieve the best trade-off between designability and diversity. We found achieving the balance between the conditional parts of the score for dynamics and for the structure to be the most problematic aspect to optimise. With the means of trial and error fine-tuning of the guidance scales, we arrived at different values per each hinge target. The guidance scales for the dynamics term and structure term were 3000 and 2500 for 6lys; 3000 and 2000 for 3adk; 2500 and 2000 for 2hhb. These constants were scaled by the time-dependent factors: \u03b1t for dynamics and\u221a 1.5\u2212 \u03b1t for structure. The conditioning was switched on in the middle of the generation. Within the Kabsh algorithm in the structure conditioning, we found the translation vector and rotation matrix to get the best alignment of the target residues with the residues\u2019 positions at t = 0, applied those transformations to the target and calculated RMSD. The translation and rotation were recalculated each 5 time steps."
        },
        {
            "heading": "C HINGE TARGETS DESCRIPTION",
            "text": "To extract targets with a prominent hinge motion we performed a literature survey. We identified the lysozyme (Gibrat & Go\u0304, 1990), adenylate kinase (AdK) (Tama & Sanejouand, 2001), and haemoglobin (Perahia & Mouawad, 1995) as three prominent examples for which the lowest normal mode is known to correlate strongly with functional motion. To extract hinge motion, we perform an anisotropic elastic network formulation of normal mode analysis with an invariant force field on alpha carbon atoms, using a distance cut-off of 13 A\u030a. The lowest non-trivial normal mode is then computed from the Hessian, and the 16 residues with the largest displacement components are\nextracted as motifs to scaffold with the targeted motion. The target motifs are shown in the top column of Fig. 1. For lysozyme and adenylate kinase hinges, the newly sampled backbones had length max(hinge residue order) + 10, and for the haemoglobin max(hinge residue order) + 20, where (hinge residue order) is the order of non-consecutive hinge residues in the original backbone. Since haemoglobin is larger than the maximal backbone length that can fit to genie, the haemoglobin hinge was modified - the backbone order for all hinge residues was shifted down by 190 residues, additionally the number of residues between the hinge arms was decreased by 250 residues."
        },
        {
            "heading": "D DESIGNABILITY IN DYNAMICS CONDITIONING VS STRUCTURE CONDITIONING",
            "text": "Since experimental verification of protein designs is time-consuming and expensive, the research community has developed in silico methods to assess design success computationally. Many of them fall under the framework of so-called self-consistency metrics (Trippe et al., 2023), meaning that the designed structure is evaluated by predicting a sequence for it via inverse folding models like ProteinMPNN (Dauparas et al., 2022), predicting the resulting structure via structure prediction methods like AlphaFold2 (Jumper et al., 2021) or ESMFold (Lin et al., 2022) and comparing this predicted structure to the designed one via structural similarity metrics. The most common computational design criteria are the following:\n\u2022 scTM > 0.5: the TM-score between the designed structure and the self-consistency predicted structure as described above. With the scTM-score ranging from 0 to 1, higher numbers correspond to an increased likelihood of the input structure being designable. A threshold of 0.5 is often chosen and the percentage of samples above this threshold is reported.\n\u2022 scRMSD < 2 A\u030a: The scRMSD metric is similar to the scTM metric, however instead of the TM-score the RMSD between the designed and predicted structure is calculated. It is a much more stringent criterion than scTM since RMSD is a local metric that is more sensitive to small structural differences.\n\u2022 pLDDT > 70 and pAE < 10: Since both scTM and scRMSD rely on a structure prediction method like AlphaFold2 to be reliable metrics, confidence metrics of these models like pLDDT and pAE are used as additional metrics to ensure the reliability of self-consistency metrics. Low scRMSD and high pLDDT have been linked to the experiment success of designing the backbone (Bennett et al., 2022).\nscTm score alone is a good indicator of whether two structures are in the same fold and for that reason, it has been used in previous works for assessing the general sample quality (Trippe et al., 2023; Yim et al., 2023). However, more recent works such as Genie (Lin & AlQuraishi, 2023) apply more stringent criteria of pLDDT > 70, pAE < 10, scTM > 0.5. In our experiments with hinge targets, if those additional requirements were incorporated, the proportion of samples meeting those criteria in joint conditioning dropped to 0.04 for 6lys, 0.15 for 3adk, and 0.0 for 2hhb. When we incorporated the last most stringent criterion that scRMSD < 2 A\u030a, those proportions dropped to 0.00, 0.04 and 0.0. Further investigation revealed that the lack of confidence in ESMFold predictions is due to the difficulty in structure conditioning. When only the dynamics conditioning was used (with the same guidance scale as when being part of the joint conditioning) the proportions of designable structures without scRMSD criterion were 0.6 for 6lys, 0.82 for 3adk, 0.52 for 2hhb, and with scRMSD criterion 0.41, 0.63 and 0.37 respectively.\nTo put these values into perspective, we note that low designability scores are not uncommon for the models tackling motif scaffolding problem. The current SOTA model RFDiffusion has designability 0, or close to 0, for some of the more difficult functional site targets ((Watson et al., 2022), Supplementary Methods Table 10). Since our targets were extracted from a flexible part of the protein, consist of discontinuous motifs and have not been used as targets in the literature elsewhere, it is difficult to assess what designability scores might be considered \u2018good\u2019 for those targets. Moreover, we note that the confidence metric of AF2/ESMFold might not be well suited for the assessment of the quality of the flexible regions. As observed in Bryant (2023), pLDDT is a \u2018good\u2019 metric if a single protein conformation is considered, however, it becomes less informative as alternative conforma-\ntions are included. The regions with lower pLDDT tend to be flexible regions with conformational changes, which might explain why proteins with a hinge structure tend to have lower pLDDT."
        },
        {
            "heading": "E SCORE-NOISE EQUIVALENCE",
            "text": "For completeness, we provide a short derivation of the score-noise equivalence \u2207xt log q(xt|x0) = \u2207xt logN ( xt; \u221a \u03b1\u0304tx0, (1\u2212 \u03b1\u0304t)I ) (21)\n\u2207xt logN ( xt; \u221a \u03b1\u0304tx0, (1\u2212 \u03b1\u0304t)I ) = \u2212\u2207xt (xt \u2212 \u221a \u03b1\u0304tx0) 2\n2(1\u2212 \u03b1\u0304t) (22)\n\u2212\u2207xt (xt \u2212\n\u221a \u03b1\u0304tx0) 2\n2(1\u2212 \u03b1\u0304t) = \u2212 (xt \u2212\n\u221a \u03b1\u0304tx0)\n(1\u2212 \u03b1\u0304t) (23)\n\u2212 (xt \u2212 \u221a \u03b1\u0304tx0)\n(1\u2212 \u03b1\u0304t) = \u2212 \u03f5t\u221a 1\u2212 \u03b1\u0304t (24)"
        },
        {
            "heading": "F ADDITIONAL SAMPLES",
            "text": "Green arrows are the displacements of conditioned residues in the sampled protein, purple arrows are the targets rotated to fit the green arrows best.\nG REBUTTAL: ADDITIONAL EXPERIMENTS Addressed questions related to evaluation and biological application scenarios by all reviewers. G.1 TARGET DEFINITION\nWe investigate the following question: Can we design a new backbone, such that the functional motif and its key dynamical behavior, represented by the lowest non-trivial normal mode components of the motif in the target structure, are preserved?\nWe focused on the biologically relevant scenario and chose to model a dynamically relevant segment spanning the two active site residues in hen-egg white lysozyme as target motif. Lysozyme was chosen as a case study since during function (Bauer et al. (2019); Brooks & Karplus (1985)) it undergoes a well-studied hinge motion, which is well captured by lowest non-trivial mode in normal mode analysis. The motif is illustrated in Fig. 10 and consists of 22 residues of the original structure (PDB: 6lyz, 129 residues), including the active site residues GLU-35 and ASP-52 (Vocadlo et al. (2001); Held & van Smaalen (2014)). To obtain the NMA target for this motif, we perform an NMA with an invariant force-field and with a 13 A\u030a distance threshold on the C\u03b1-backbone of the native protein (6lyz) and extract the lowest non-trivial normal mode displacements for the motif residues as NMA target.\nG.2 MODELLING\nImproved motif conditioning model When using the original, guidance-based formulation for motif conditioning used in the main text, we found that motif conditioning continued to be the primary difficulty. This made it harder to perform and analyse NMA conditioning jointly with motif conditioning, because for those situations the NMA condition only makes sense for a reasonably well formed motif.\nFor the additional rebuttal experiments, we therefore sought to improve motif conditioning by using a Genie model that we re-trained with explicit motif conditioning for 80% of the time and unconditional sampling for 20% of the time, removing the problem of having to tune two guidance scales at one time. This required us to perform a minor modification to the input layer of the unconditional Genie model (Lin & AlQuraishi (2023)). We add an additional conditional pair feature network that takes the target motif coordinates and frames as input with zero-padding for all non-motif coordinates and frames. The features of this motif-conditional pair feature network are fused with the output of the original unconditional pair feature network in genie via concatenation along the feature dimension, followed by a linear projection down to the channel size of the unconditional model. The remainder of the Genie model then proceeds unchanged. This minor architectural modification means our conditional Genie network has 4.162M parameters while the unconditional Genie network has 4.087M parameters (\u223c 1.8% fewer). The conditional Genie model was trained for 4\u2019000\nepochs on 4 A100 GPUs (\u223c 300 A100 hours in total). We stopped training at this point, as we observed almost comparable performance to the publicly available model weights (which were obtained after training for 50\u2019000 epochs). We use these model for all the additional experiments in this section.\nGuidance schedule We use the a modulated step-function guidance schedule\n\u03b3(t) = { \u03b30(1\u2212 \u03b1t) if t < tstart 0 if t \u2265 tstart , (25)\nwith a guidance scale \u03b30 and starting point tstart. For tmax = 1000, we fixed tstart = 500 (i.e. conditioning starts halfway through the reverse diffusion process) and identified \u03b30 = 500 as an adequate guidance scale through a logarithmic scan of \u03b30 values. Similar to other work on diffusion models for protein backbone generation, we reduce the noise scale by a factor \u03b7 = 0.4, which improves the quality of generated samples (Yim et al. (2023)) for motif-only as well as motif+NMA conditioning.\nNMA loss The presence of a functional motif defines a reference coordinate system, namely the coordinate system in which the coordinates of the to-be-scaffolded motif are given in. Notably, this means that the normal mode displacements at the motif residues are also given in the motif\u2019s coordinate system. Any designed backbone should be invariant to translations, but equivariant to rotations of this coordinate system, which correspond to rotations of the motif and the associated displacement vectors.\nTo better comply with these symmetry requirements, we adapt the invariant loss lNMA in Eq. 16 to make use of this reference coordinate system. Using the notation of the main text, yM \u2208 Rm\u00d73 and xM \u2208 Rm\u00d73 represent the target motif coordinates and sample motif coordinates for a motif of m residues respectively. Similarly, vM (y) \u2208 Rm\u00d73 and vM (x) \u2208 Rm\u00d73 respectively refer to the matrix of displacement vectors in the lowest non-trivial normal mode for the target and the sample. The rotation matrix R(yM , xM ) transforms the coordinate frame of the target motif yM to that of\nxM . With these definitions, the updated NMA loss for the additional experiments is l\u2032NMA = 2ldirection ( R(yM , xM )vM (y), vM (x) ) + lmagnitude ( R(yM , xM )vM (y), vM (x) ) (26)\nldirection(v1, v2) = 1\u2212 \u2223\u2223\u2223\u2223 v1\u2225v1\u2225 \u00b7 v2\u2225v2\u2225 \u2223\u2223\u2223\u2223 = 1\u2212 | cos (v1, v2)| (27) lmagnitude(v1, v2) = | \u2225v1\u2225 \u2212 \u2225v2\u2225 |. (28)\nHere, v(y) and v(x) are understood as flattened vectors in R3m, and therefore ldirection directly captures the relative contributions of each residue\u2019s displacement. The factor 2 was added to align the min and max ranges of the two components of l\u2032NMA. We obtain R(y\nM , xM ) through a differentiable implementation of the Kabsch alignment algorithm (Kabsch (1976)) and v(x) from a differentiable implementation of NMA on the sampled backbone x, which is then subset to the motif coordinates, as in the main text. Guidance is then performed via\nxt \u2190 xt \u2212 \u03b3(t)\u2207xt l\u2032NMA ( R(yM , x\u0302M0 (xt))v M (y), vM (x\u03020(xt)) ) , (29)\nwith x\u03020(xt) indicating the current estimate of the denoised structure via Tweedie\u2019s formula (Robbins (1956)) as in reconstruction guidance (Chung et al. (2022b)).\nEvaluation pipeline The evaluation proceeds similarly as in the main body. For each C\u03b1-only backbone sample, we derived 8 sequences with ProteinMPNN (Tsampling = 0.1). In those sequences, the amino acid identities of the motif residues known from the lysozyme target were kept fixed, such that only the scaffold was predicted by ProteinMPNN. Each of the 8 sequences is then re-folded with ESMFold, and self-consistency scores (scNMA, scRMSD, scTM) are calculated with respect to the original backbone sample. The original backbone sample is then paired with the ESMFold-ed design that had the lowest scRMSD (out of 8 ESMFold designs). We deemed the structure designable if it met the criteria of scTM>0.5, scRMSD<2A\u030a, with confidence of ESMFold predictions thresholded at pLDDT>70, pAE<10, which aligns with definitions in prior work (Watson et al. (2022); Yim et al. (2023); Lin & AlQuraishi (2023)). Moreover, we evaluate an additional motif scaffolding metric, scMOTIF-RMSD, that is RMSD between the motif residues in the ESMFold obtained structure and the target motif.\nG.3 RESULTS\nThe analysis of the NMA loss of the Genie generated C\u03b1-only backbone and scNMA-score of the ESMFold design confirmed that the dynamics conditioning indeed results in the C\u03b1 backbones that match the target, however there is no clear direct correspondence scNMA-scores to the original backbone NMA-loss for the dynamics-conditioned samples. Surprisingly, we found while motif conditioning improved upon making the structure conditioning inherent to the conditional Genie model, performing the motif scaffolding was still challenging to the model. In the remainder of this discussion, we call all the structure-only conditioned samples motif-only, and all jointly structure and dynamics conditioned - motif+NMA.\nDiscussion of the motif scaffolding success rate As a first part of the evaluation, we calculated the proportion of Genie generated backbones of the motif-only and motif+NMA samples that meet the designability criteria and have backbone design motif-RMSD< 1A\u030a. Out of 150 motif-only samples, 1 is designable and has motif-RMSD< 1A\u030a, while 2 out of 43 motif+NMA ones are. Moreover, the scMOTIF-RMSD, that is RMSD to the motif structure after folding inferred sequences for said backbone with ESMFold, does not achieve values lower than 1A\u030a for any of motif+NMA and motif-only conditioned samples. Figure 11 shows in detail how scMOTIF-RMSD correlates with scRMSD.\nWe believe this is a combination of (1) the limited training and capacity of our model and (2) the challenging nature of our target motif, which is a segment of 2 paired, anti-parallel beta-sheets connected to the end of a helix. To these points into context, our model was trained for the motifscaffolding task for 300 A100 GPU-hours, compared to state-of-the-art models such as RFDiffusion, which are trained for over 25\u2032000s of GPU hours when considering the RosettaFold2 pre-training. Yet, despite the significantly higher model capacity of RFDiffusion (42 Mio. parameters) as well as the longer training, design success rates (according to the criteria outlined above) of RFDiffusion\ncan also at or below 1% for some challenging, contiguous functional motifs (e.g. targets 5WN9 or 4JHW in the RFDiffusion benchmark in the supplementary material of Watson et al. (2022)). It is therefore possible that the in-silico success rates for our model with a lower capacity are be below the detection threshold for this particular motif scaffolding problem.\nNonetheless, the ESMFold designed backbones achieving the lowest scMOTIF-RMSD and the lowest scRMSD belong to the motif+NMA conditioned group, which illustrates that our NMAconditioning approach has no discernable negative impact on the designability of samples. We believe it is therefore still meaningful to gleam insights from this set of samples, despite the challenging nature of the motif-scaffolding for our chosen target.\nDiscussion of the scNMA-score The distribution of NMA-loss in the motif+NMA and motif-only Genie backbone samples is consistent with our previous findings that the dynamics-conditioning leads to the targeted dynamics in the raw backbone (Figure 12). Only 2/150 (\u223c 1.3%) of motifonly backbones achieve origNMA-loss < 0.5, as opposed to 10/43 (\u223c 23%) for motif+NMA \u2013 corresponding to a roughly 17x-fold enrichment, while achieving comparable motif scaffolding performance (c.f. Fig. 11). However, much of this benefit appears to disappear in the process in inverse-folding and the subsequent re-folding. Joint motif+NMA conditioning still increases the relative chance of obtaining a sample with a low scNMA-score (3/43 \u223c 7% of samples below 0.5) as compared to motif-only conditioning (3/150 \u223c 2% below 0.5), roughly 3-fold, but the difference is much less pronounced than for the NMA-loss of the designed backbone (original NMA-loss). Figure 13 shows the scNMA-loss distribution for the motif+NMA and motif-only ESMFold designs. The best sample with low original NMA-loss is therefore not guaranteed to have similarly low scNMA-score. The pipeline inverse-folding and re-folding has also a surprising effect on the motif-only samples. Samples with high values of original NMA-loss are occasionally corrected to better NMA scores in the pipeline and match the targeted motif\u2019s dynamics better. Still, the introduction of the dynamics conditioning increases the relative chance of obtaining a sample with a low scNMA-score as compared to motif-only sampling. We leave the interesting question of how to retain high NMA-scores through inverse folding and re-folding pipelines as an interesting future work.\nLastly, we investigate how the scNMA-score correlates with the scMOTIF-RMSD. While the region where scMOTIF-RMSD<1A\u030a remains unachievable for both motif+NMA and motif-only samples as previously discussed, the best samples (scMOTIF-RMSD and scNMA as low as possible) from all samples taken belong to the dynamics-conditioned subset.\nG.4 ADDITIONAL ALPHAFOLD2 DESIGNS\nTo give a visual intuition of the scores introduced above, we show the AlphaFold2 (AF2) designs of the Genie backbones - one motif-only conditioned, and one motif+NMA conditioned. Those backbones were deemed designable and close to designable by ESMFold - their scRMSD and pLDDT respectively were 1.923 A\u030a, 73.6 for motif-only conditioned sample and 1.897 A\u030a, 68.1 for motif+NMA conditioned sample. We repeated the inverse-folding and folding steps for these two selected samples with state-of-the-art AF2, and we computed the self-consistency scores again. The C\u03b1-only backbones derived from the AF2 designs are presented in the Figures 15 and 16. The displacement vectors in the lowest normal mode are attached to the points of the conditioned residues."
        }
    ],
    "year": 2023
}