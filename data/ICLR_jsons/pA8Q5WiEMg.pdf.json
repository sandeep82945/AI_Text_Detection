{
    "abstractText": "Online-Within-Online (OWO) meta learning stands for the online multi-task learning paradigm in which both tasks and data within each task become available in a sequential order. In this work, we study the OWO meta learning of the initialization and step size of within-task online algorithms in the non-convex setting, and provide improved regret bounds under mild assumptions of loss functions. Previous work analyzing this scenario has obtained for bounded and piecewise Lipschitz functions an averaged regret bound O(( \u221a m T 1/4 + (logm) log T \u221a T + V ) \u221a m) across T tasks, with m iterations per task and V the task similarity. Our first contribution is to modify the existing non-convex OWO meta learning algorithm and improve the regret bound to O(( 1 T 1/2\u2212\u03b1 + (log T ) 9/2 T + V ) \u221a m), for any \u03b1 \u2208 (0, 1/2). The derived bound has a faster convergence rate with respect to T , and guarantees a vanishing task-averaged regret with respect to m (for any fixed T ). Then, we propose a new algorithm of regret O(( log T T + V ) \u221a m) for non-convex OWO meta learning. This regret bound exhibits a better asymptotic performance than previous ones, and holds for any bounded (not necessarily Lipschitz) loss functions. Besides the improved regret bounds, our contributions include investigating how to attain generalization bounds for statistical meta learning via regret analysis. Specifically, by online-to-batch arguments, we achieve a transfer risk bound for batch meta learning that assumes all tasks are drawn from a distribution. Moreover, by connecting multi-task generalization error with taskaveraged regret, we develop for statistical multi-task learning a novel PAC-Bayes generalization error bound that involves our regret bound for OWO meta learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiechao Guan"
        },
        {
            "affiliations": [],
            "name": "Hui Xiong"
        },
        {
            "affiliations": [],
            "name": "HKUST Fok"
        },
        {
            "affiliations": [],
            "name": "Ying Tung"
        }
    ],
    "id": "SP:440dc7db34bce2f41a1aa68ed56d17b1cdad7599",
    "references": [
        {
            "authors": [
                "Durmus Alp Emre Acar",
                "Ruizhao Zhu",
                "Venkatesh Saligrama"
            ],
            "title": "Memory efficient online meta learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Alquier"
            ],
            "title": "Non-exponentially weighted aggregation: Regret bounds for unbounded loss functions",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Alquier",
                "The Tien Mai",
                "Massimiliano Pontil"
            ],
            "title": "Regret bounds for lifelong learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2017
        },
        {
            "authors": [
                "Ron Amit",
                "Ron Meir"
            ],
            "title": "Meta-learning by adjusting priors based on extended PAC-Bayes theory",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Rie Kubota Ando",
                "Tong Zhang"
            ],
            "title": "A framework for learning predictive structures from multiple tasks and unlabeled data",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2005
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Travis Dick",
                "Ellen Vitercik"
            ],
            "title": "Dispersion for data-driven algorithm design, online learning, and private optimization",
            "venue": "In IEEE Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2018
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Mikhail Khodak",
                "Ameet Talwalkar"
            ],
            "title": "Provable guarantees for gradientbased meta-learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Mikhail Khodak",
                "Dravyansh Sharma",
                "Ameet Talwalkar"
            ],
            "title": "Learning-tolearn non-convex piecewise-lipschitz functions",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Baxter"
            ],
            "title": "Learning internal representations",
            "venue": "In Conference on Learning Theory (COLT),",
            "year": 1995
        },
        {
            "authors": [
                "Jonathan Baxter"
            ],
            "title": "A model of inductive bias learning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2000
        },
        {
            "authors": [
                "Shai Ben-David",
                "Reba Schuller Borbely"
            ],
            "title": "A notion of task relatedness yielding provable multipletask learning guarantees",
            "venue": "Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Nicol\u00f2 Cesa-Bianchi",
                "G\u00e1bor Lugosi"
            ],
            "title": "Prediction, learning, and games",
            "year": 2006
        },
        {
            "authors": [
                "Jiaxin Chen",
                "Xiao-Ming Wu",
                "Yanke Li",
                "Qimai LI",
                "Li-Ming Zhan",
                "Fu-Lai Chung"
            ],
            "title": "A closer look at the training strategy for modern meta-learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Giulia Denevi",
                "Carlo Ciliberto",
                "Massimiliano Pontil"
            ],
            "title": "Online-within-online meta-learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Aravind Rajeswaran",
                "Sham M. Kakade",
                "Sergey Levine"
            ],
            "title": "Online meta-learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Bela A. Frigyik",
                "Santosh Srivastava",
                "Maya R. Gupta"
            ],
            "title": "Functional bregman divergence and bayesian estimation of distributions",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2008
        },
        {
            "authors": [
                "Xiang Gao",
                "Xiaobo Li",
                "Shuzhong Zhang"
            ],
            "title": "Online learning with non-convex losses and nonstationary regret",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2018
        },
        {
            "authors": [
                "Jiechao Guan",
                "Zhiwu Lu"
            ],
            "title": "Fast-rate pac-bayesian generalization bounds for meta-learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Jiechao Guan",
                "Yong Liu",
                "Zhiwu Lu"
            ],
            "title": "Fine-grained analysis of stability and generalization for modern meta learning algorithms",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Elad Hazan"
            ],
            "title": "Introduction to online convex optimization",
            "venue": "arXiv preprint arXiv:1909.05207v3,",
            "year": 2023
        },
        {
            "authors": [
                "Mikhail Khodak",
                "Maria-Florina Balcan",
                "Ameet Talwalkar"
            ],
            "title": "Adaptive gradient-based metalearning methods",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Jianhua Lin"
            ],
            "title": "Divergence measures based on the shannon entropy",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1991
        },
        {
            "authors": [
                "Tongliang Liu",
                "Dacheng Tao",
                "Mingli Song",
                "Stephen J. Maybank"
            ],
            "title": "Algorithm-dependent generalization bounds for multi-task learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),",
            "year": 2017
        },
        {
            "authors": [
                "G\u00e1bor Lugosi",
                "Gergely Neu"
            ],
            "title": "Online-to-pac conversions: Generalization bounds via regret analysis",
            "venue": "arXiv preprint arXiv.2305.19674,",
            "year": 2023
        },
        {
            "authors": [
                "Andreas Maurer"
            ],
            "title": "Algorithmic stability and meta-learning",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2005
        },
        {
            "authors": [
                "Andreas Maurer",
                "Massimiliano Pontil",
                "Bernardino Romera-Paredes"
            ],
            "title": "The benefit of multitask representation learning",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2016
        },
        {
            "authors": [
                "Anastasia Pentina",
                "Christoph H. Lampert"
            ],
            "title": "A PAC-Bayesian bound for lifelong learning",
            "venue": "In International Conference of Machine Learning (ICML),",
            "year": 2014
        },
        {
            "authors": [
                "Anastasia Pentina",
                "Christoph H. Lampert"
            ],
            "title": "Lifelong learning with non-i.i.d",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2015
        },
        {
            "authors": [
                "Shai Shalev-Shwartz"
            ],
            "title": "Online learning and online convex optimization",
            "venue": "Found. Trends Mach. Learn.,",
            "year": 2012
        },
        {
            "authors": [
                "Shai Shalev-Shwartz",
                "Sham M. Kakade"
            ],
            "title": "Mind the duality gap: Logarithmic regret algorithms for online optimization",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2008
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel"
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Arun Sai Suggala",
                "Praneeth Netrapalli"
            ],
            "title": "Online non-convex learning: Following the perturbed leader is optimal",
            "venue": "In International Conference on Algorithmic Learning Theory (ALT),",
            "year": 2020
        },
        {
            "authors": [
                "Eiji Takimoto",
                "Manfred K. Warmuth"
            ],
            "title": "The minimax strategy for gaussian density estimation",
            "venue": "pp. In Annual Conference on Computational Learning Theory (COLT),",
            "year": 2000
        },
        {
            "authors": [
                "Sebastian Thrun",
                "Lorien Pratt"
            ],
            "title": "Learning to Learn",
            "venue": "Kluwer Academic Publishers,",
            "year": 1998
        },
        {
            "authors": [
                "Han-Jia Ye",
                "Hexiang Hu",
                "De-Chuan Zhan",
                "Fei Sha"
            ],
            "title": "Few-shot learning via embedding adaptation with set-to-set functions",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "James Yeh"
            ],
            "title": "Real Analysis: Theory of Measure and Integration",
            "venue": "World Scientific,",
            "year": 2014
        },
        {
            "authors": [
                "Niloofar Yousefi",
                "Yunwen Lei",
                "Marius Kloft",
                "Mansooreh Mollaghasemi",
                "Georgios C. Anagnostopoulos"
            ],
            "title": "Local rademacher complexity-based learning guarantees for multi-task learning",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2018
        },
        {
            "authors": [
                "Zhenxun Zhuang",
                "Yunlong Wang",
                "Kezi Yu",
                "Songtao Lu"
            ],
            "title": "No-regret non-convex online metalearning",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Martin Zinkevich"
            ],
            "title": "Online convex programming and generalized infinitesimal gradient ascent",
            "venue": "In International Conference of Machine Learning (ICML), pp",
            "year": 2003
        },
        {
            "authors": [
                "Lampert",
                "Amit",
                "Meir",
                "Guan",
                "Lu"
            ],
            "title": "But the PAC-Bayes multi-task generalization bounds in meta learning theory assume the priors for different training tasks are the same and are sampled from the same hyper-prior distribution. Therefore, to the best of our knowledge, there is still no explicit PAC-Bayes generalization bound for standard statistical multi-task learning setting",
            "venue": "B EXPLICIT FORM OF REGRET BOUNDS FOR OWO META LEARNING",
            "year": 2022
        },
        {
            "authors": [
                "\u221a mM"
            ],
            "title": "The vanilla averaged regret bound (Alquier, 2021, Thm 2.1) for EWA algorithm (with the step size \u03bbt = 1/(M",
            "year": 2021
        },
        {
            "authors": [],
            "title": "`T be a sequence of functions such that for all t \u2208 [T ], `t is \u03c3t-strongly convex. Assume that the FTL algorithm runs on this sequence and for each t \u2208 [T ], let gt be in \u2202`t(\u03b8t)",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "\u221a m T 1/4 + (logm) log T\u221a T + V ) \u221a m)\nacross T tasks, with m iterations per task and V the task similarity. Our first contribution is to modify the existing non-convex OWO meta learning algorithm and improve the regret bound to O(( 1\nT 1/2\u2212\u03b1 + (log T )\n9/2 T + V ) \u221a m), for any\n\u03b1 \u2208 (0, 1/2). The derived bound has a faster convergence rate with respect to T , and guarantees a vanishing task-averaged regret with respect to m (for any fixed T ). Then, we propose a new algorithm of regret O(( log TT + V ) \u221a m) for non-convex OWO meta learning. This regret bound exhibits a better asymptotic performance than previous ones, and holds for any bounded (not necessarily Lipschitz) loss functions. Besides the improved regret bounds, our contributions include investigating how to attain generalization bounds for statistical meta learning via regret analysis. Specifically, by online-to-batch arguments, we achieve a transfer risk bound for batch meta learning that assumes all tasks are drawn from a distribution. Moreover, by connecting multi-task generalization error with taskaveraged regret, we develop for statistical multi-task learning a novel PAC-Bayes generalization error bound that involves our regret bound for OWO meta learning."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Meta learning, also referred to as learning to learn, is a multi-task learning paradigm that transfers knowledge from past tasks to the new task for fast adaptation (Thrun & Pratt, 1998). Due to the advantage of reducing annotation cost and training time for novel task, meta learning has received increasing attention over the last decade, both from practical (Finn et al., 2017; Snell et al., 2017; Ye et al., 2020) and theoretical perspectives (Baxter, 2000; Balcan et al., 2019; Chen et al., 2020).\nTraditional meta learning theory mainly investigates generalization bounds for statistical/batch meta learning (Baxter, 2000; Pentina & Lampert, 2014; Maurer et al., 2016; Guan et al., 2022). In this setting, samples within each task are assumed to be drawn from the same data distribution and processed in a batch, and different tasks are drawn from the same task distribution. Recently, there emerges an interest in studying online meta learning, where tasks are observed sequentially and require real-time processing (Alquier et al., 2017; Finn et al., 2019). According to (Denevi et al., 2019), there are two main frameworks in online meta learning setting: (1) Online-Within-Batch (OWB) framework, where tasks are available in a sequential order but the data within each task are available in one batch, see (Finn et al., 2019; Zhuang et al., 2020; Acar et al., 2021). (2) OnlineWithin-Online (OWO) framework, where both tasks and data within each task are available and processed sequentially, see (Alquier et al., 2017; Denevi et al., 2019; Khodak et al., 2019; Balcan et al.,\n\u2217Corresponding Author.\n2019; 2021). The goal of this paper is to investigate the OWO meta learning of initialization-based online optimization algorithms in the non-convex setting, and provide strong theoretical guarantees.\nInitialization-based online algorithms (e.g. online gradient descent (Zinkevich, 2003)) typically have two hyper-parameters to learn: the initialization of model\u2019s parameter, and step size to update the parameter. OWO meta learning aims to utilize knowledge from previous tasks to set a good initialization and step size of online algorithm on novel task for fast adaptation. Recall that the metric used to measure the performance of online single-task learner is the so-called regret, which is defined as the gap between cumulative loss of online learner and the cumulative loss of the best fixed strategy in hindsight. Analogously, the metric used to measure the performance of OWO meta-learner is the averaged regret across T training tasks, denoted as task-averaged regret (Balcan et al., 2019). Previous work (Balcan et al., 2021) analyzing the non-convex OWO meta learning of initialization-based online algorithms has attained for bounded and piecewise Lipschitz functions a task-averaged regret bound O(( \u221a m\nT 1/4 + (logm) log T\u221a T +V )\n\u221a m), with m iterations per task and V the\ntask similarity. However, this bound has two limitations: (1) For any fixed T , it cannot guarantee a vanishing regret with respect to (w.r.t.) m; (2) The convergence rate w.r.t. T is slow. In this work, we attempt to address the two issues and provide improved regret bounds with fine-grained analysis.\nOur first contribution is based on the modification of existing non-convex OWO meta learning algorithm from Balcan et al. (2021). We improve the regret bounds of online algorithms for learning initialization and step size respectively, and combine them to get the sharper task-averaged regret bound of O(( 1\nT 1/2\u2212\u03b1 + (log T )\n9/2 T +V ) \u221a m), for any \u03b1 \u2208 (0, 1/2). The derived bound has a faster\nconvergence rate w.r.t. T , and guarantees a vanishing task-averaged regret w.r.t. m (for any fixed T ). Then, we propose a new and more efficient algorithm of regret O(( log TT +V ) \u221a m) for non-convex OWO meta learning. This regret bound exhibits a better asymptotic performance than previous ones, and holds for any bounded (not necessarily Lipschitz) loss functions. Furthermore, we show how to attain generalization bounds for statistical meta learning via regret analysis. Specifically, by onlineto-batch arguments, we achieve a new transfer risk bound for non-convex batch meta learning in which different tasks are drawn from a task distribution. Moreover, by revealing the connection between multi-task generalization error and task-averaged regret, we develop for statistical multi-task learning a novel PAC-Bayes generalization error bound that involves our improved regret bound for non-convex OWO meta learning setting. To the best of our knowledge, this is the first PAC-Bayes bound for batch/online multi-task setting that imposes distribution assumption over the data per task.\nTo summarize, our contributions are four-fold: (1) For non-convex OWO meta learning, we improve regret bound from O(( \u221a m\nT 1/4 + (logm) log T\u221a T + V )\n\u221a m) to O(( 1\nT 1/2\u2212\u03b1 + (log T )\n9/2 T + V ) \u221a m) (\u03b1 \u2208\n(0, 1/2)) for bounded and piecewise Lipschitz functions. (2) We design a new and efficient OWO meta learning algorithm of sharper regret bound O(( log TT +V ) \u221a m) for bounded (not necessarily Lipschitz) functions. (3) We obtain a new transfer risk bound for statistical meta learning via regret analysis. (4) We derive a PAC-Bayes bound for multi-task learning, shedding light on proving PACBayes multi-task generalization bound with the regret bound from non-convex OWO meta learning."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Regret Bounds for Online-Within-Batch Meta Learning. In OWB meta learning setting, the meta parameter is transferred and updated sequentially across T tasks. Concretely, when encountering a training task, OWB meta learning algorithm takes the meta parameter and the training data from the task as input, and outputs a model suitable for that task. The incurred loss of the task-specific model over the evaluation data is denoted as the online loss on the task. The gap between the cumulative loss of updated meta parameters across T tasks and the cumulative loss of the best fixed meta parameter in hindsight is defined as the regret. Thus, the regret bound for OWB meta learning is of O(log T ) in the strongly-convex setting (see (Finn et al., 2019, Cor 2)), and of O( \u221a T ) in the convex/non-convex setting (see (Acar et al., 2021, Thm 1) and (Zhuang et al., 2020, Thm 1)). These regret bounds are irrelevant to the sample size m per task, and are achieved under very strong assumptions of loss functions (e.g. including Lipschitzness, smoothness, and Hessian smoothness).\nRegret Bounds for Online-Within-Online Meta Learning. OWO meta learning is more challenging than OWB meta learning. Existing algorithms in OWO meta learning can be categorized into two groups: (1) The first group learns a common meta parameter (e.g. a feature map\n(Alquier et al., 2017) or a meta regularization function (Denevi et al., 2019)) shared across T tasks and uses the meta parameter to learn specific model for each task. The averaged regret bound across m iterations and T tasks in (Alquier et al., 2017, Thm 3.1) is of O(1/ \u221a m + 1/ \u221a T ).\nThe regret bound in (Denevi et al., 2019, Thm 1) is of O( \u221a logm/m + 1/ \u221a T ). However, both works focus on convex OWO meta learning, and it is hard to extend their analysis (e.g. especially the primal-dual technique in Denevi et al. (2019)) to the non-convex setting. (2) The second group learns the initialization and step size of online algorithms for each task, and combines the regrets for learning these two hyper-parameters to obtain ultimate averaged regret across T tasks. Concretely, the task-averaged regret bound in (Khodak et al., 2019, Thm 5.1) is of O((log T/T + log T/ \u221a T + V ) \u221a m) for convex and Lipschitz functions. The bound in (Balcan et al., 2021, Thm 3.3) is of O(( \u221a m/T 1/4 + (logm) log T/ \u221a T + V ) \u221a m) for non-convex and piecewise Lipschitz functions. Our work focuses on non-convex setting, improves regret bound to O((1/T 1/2\u2212\u03b1+(log T )9/2/T+V ) \u221a m) in Theorem 2 for piecewise Lipschitz functions, and further obtains a sharper regret bound of O((log T/T + V ) \u221a m) in Theorem 3 for non-Lipschitz functions.\nGeneralization Bounds for Statistical Meta Learning. The basic assumption in statistical meta learning theory is that all tasks are sampled from the same task distribution (Baxter, 2000). Under this assumption, Guan et al. (2022) summarize existing generalization bounds for meta learning into three groups: (1) Generalization bounds based on hypothesis space complexity (e.g. covering number based ones (Baxter, 2000) and Gaussian complexity based ones (Maurer et al., 2016)). (2) Generalization bounds based on PAC-Bayes theory (Pentina & Lampert, 2014; 2015; Guan & Lu, 2022). (3) Generalization bounds based on algorithmic stability (Maurer, 2005; Chen et al., 2020). However, with the tool of stability analysis, Guan et al. (2022) point out that, under the statistical task distribution assumption, the optimal generalization bound for meta learning is of O(1/ \u221a T ), where T is the number of training tasks. The optimal bound is irrelevant to the sample size m per task and hence is slow, indicating the limitation of task distribution assumption. Our transfer risk bound in Theorem 4 via regret analysis is different from the above generalization bounds for meta learning. Our bound of O(V 2/ \u221a m) decreases with higher similarity among T training tasks, shedding more light on the generalization ability of meta learning models. More discussions about generalization bounds for statistical multi-task learning can be found in Section A of the Appendix."
        },
        {
            "heading": "3 PRELIMINARY",
            "text": "In online learning, an action space \u0398 is a compact subset of Rd. A loss function ` : \u0398 7\u2192 R\u22650 is called \u03b1-strongly convex with respect to (w.r.t.) certain norm \u2016\u00b7\u2016 in Rd, if for any x, y \u2208 \u0398, we have `(x)\u2212 `(y) \u2265 \u3008g, x\u2212 y\u3009+ \u03b12 \u2016x\u2212 y\u2016\n2, where g \u2208 \u2202`(y) and \u2202`(y) denotes the set of subgradients of ` at y. If ` is differentiable, \u2202` denotes the set of (unique) gradient of `, i.e. \u2202` = {\u2207`}. When \u03b1 = 0, ` is called convex. ` is called uniformly L-Lipschitz over \u0398 w.r.t. the norm \u2016 \u00b7 \u2016, if for any x, y \u2208 \u0398, |`(x)\u2212 `(y)| \u2264 L\u2016x\u2212 y\u2016. Let P(\u0398) be the set of all probability distributions over action space \u0398. For any \u03c1 \u2208 P(\u0398) and any loss function `, we use \u3008\u03c1, `\u3009 = E\u03b8\u223c\u03c1`(\u03b8) for brevity when the context is clear. We use boldface letter x to denote the real vector in high-dimensional space, and x\u0302 as its normalized version x/\u2016x\u2016. We also use abbreviation [m] = {1, 2, ...,m} for any integer m. Online Learning. Vanilla online learning (also called online single-task learning setting) is always cast as a m-round optimization problem . At each round i \u2208 [m], the online learner selects an action \u03b8i \u2208 \u0398. Then a loss function `i : \u0398 7\u2192 R\u22650 is revealed from the nature and the online learner suffers loss `i(\u03b8i) of the chosen action \u03b8i. The quantity used to measure the performance of online learner is the so-called regret, which is the difference between the cumulative loss of chosen actions during m rounds and the cumulative loss of the best fixed action in hindsight. Detailed introduction of regret bounds for convex/non-convex online learning can be found in Section A of the Appendix.\nOnline-Within-Online Meta Learning. In non-convex OWO meta learning setting, the online meta-learner will encounter T tasks and each task t \u2208 [T ] is composed of a sequence of m loss functions {`ti : \u0398 7\u2192 R\u22650}i\u2208[m]. Concretely, at i-th round of the t-th task, the online meta-learner selects an action \u03b8ti \u2208 \u0398, and then suffers the loss `ti(\u03b8ti), with `ti chosen adversarially from the nature. The regret for task t is defined asRt,m , \u2211m i=1 E\u03b8\u223c\u03c1ti`ti(\u03b8)\u2212min\u03b8\u2208\u0398 \u2211m i=1 `ti(\u03b8). Then, the quantity to measure the performance of online meta-learner is the following task-averaged regret:\nR\u0304T,m= 1\nT T\u2211 t=1 Rt,m = 1 T T\u2211 t=1 m\u2211 i=1 E\u03b8\u223c\u03c1ti`ti(\u03b8)\u2212`ti(\u03b8\u2217t ), where \u03b8\u2217t \u2208 arg min \u03b8\u2208\u0398 m\u2211 i=1 `ti(\u03b8). (1)\nIf we set the probability distribution \u03c1\u2217t as Dirac measure \u03b4\u03b8\u2217t that only has mass 1 at the point \u03b8 \u2217 t , we can rewrite the above task-averaged regret in a concise form: R\u0304T,m = 1T \u2211T t=1 \u2211m i=1\u3008\u03c1ti\u2212\u03c1\u2217t , `ti\u3009. The upper bound on R\u0304T,m is denoted as U\u0304T,m. The goal of OWO meta learning is to improve the single-task regret by leveraging information from other tasks. Formally speaking, we expect that when T is large enough, the task-averaged regret R\u0304T,m is smaller than the single-task regret Rt,m."
        },
        {
            "heading": "4 REGRET BOUNDS FOR NON-CONVEX ONLINE META LEARNING",
            "text": "Following previous work (Balcan et al., 2021), we study the non-convex OWO meta learning of the initialization and step size of the Exponentially Weighted Aggregation (EWA) algorithm. We first provide a general framework for analyzing the regret bounds for non-convex OWO meta Learning. Such framework is motivated by the form of regret upper bound of EWA (see regret bound for piecewise Lipschitz functions in Eq. (2) and regret bound for non-Lipschitz functions in Eq. (3)): Ut(\u03c1t1, \u03bbt) = mb\u03bbt + V (\u03c1t1,\u03c1 \u2217 t ) 2\n\u03bbt + g(m), where b > 0, \u03bbt is the step size, \u03c1t1 \u2208 P(\u0398) the ini-\ntialization distribution over the action space \u0398 for task t, \u03c1\u2217t \u2208 P(\u0398) may have some dependence on the optimal action \u03b8\u2217t , V (\u03c1t1, \u03c1 \u2217 t )\n2 is the non-negative function of \u03c1t1 and \u03c1\u2217t (e.g. the divergence between \u03c1t1 and \u03c1\u2217t ), g(m) is the term that cannot be optimized by meta learning. We then set V 2 = min\u03c1\u2208P(\u0398) 1T \u2211T t=1 V (\u03c1, \u03c1 \u2217 t )\n2 as the task similarity among T tasks. We will also use ft(\u03c1) = V (\u03c1, \u03c1 \u2217 t )\n2 for brevity when the context is clear. Using the scaling technique, the regret bound Ut(\u03c1, \u03bb) of the EWA algorithm run with initialization \u03c1 \u2208 P(\u0398) and step size \u03bb = v/ \u221a mb for v > 0 can be rewritten as an equivalent form: Ut(\u03c1, v) = (v+ ft(\u03c1) v ) \u221a mb+g(m). Therefore the OWO meta learning algorithm always consists of two online sub-algorithms: one is to play action over functions {fs(\u03c1) = V (\u03c1, \u03c1\u2217s)2}s\u2208[t\u22121] to determine the initialization distribution \u03c1t1 for task t, another is to play action over functions {hs(v) = v + fs(\u03c1s1)/v}s\u2208[t\u22121] to determine the step size vt. The process of utilizing {\u03c1\u2217s}s\u2208[t\u22121] and {vs}s\u2208[t\u22121] to determine the initialization distribution \u03c1t1 and the step size vt for EWA algorithm on task t can be considered as transferring knowledge from past tasks to the novel task. Combining the regret bounds for this two online sub-algorithms attains the following task-averaged regret bound for non-convex OWO meta learning algorithms with general functions (i.e. without boundedness, convexity or Lipschitzness assumptions of {`ti}t,i\u22651).\nTheorem 1 Assume that the upper regret bound for each task t \u2208 [T ] has the form of Ut(\u03c1, v) = (v+ ft(\u03c1)v ) \u221a mb+g(m). Assume we have a sub-algorithm that achieves FT (\u03c1) regret w.r.t. any \u03c1 \u2208 P(\u0398) by setting distributions \u03c1t1 on ft(\u03c1) = V (\u03c1, \u03c1\u2217t )2, and another sub-algorithm that achieves non-increasingHT (v) regret w.r.t. any v > 0 by playing actions vt > 0 on ht(v) = v+ ft(\u03c1t1) v for all t \u2208 [T ]. Then, running the OWO meta learning algorithm (consisting of these two sub-algorithms) with the step size vt/ \u221a mb and initialization \u03c1t1 at each task t, for \u03c1\u2217 = arg min\u03c1\u2208P(\u0398) \u2211T t=1 ft(\u03c1) the optimal initialization and V the task-similarity, we get the task-averaged regret upper bound:\n1\nT T\u2211 t=1 m\u2211 i=1 \u3008\u03c1ti \u2212 \u03c1\u2217t , `ti\u3009 \u2264 (HT (V ) T + min{FT (\u03c1 \u2217) V T , 2 \u221a FT (\u03c1\u2217) T }+ 2V )\u221a mb+ g(m).\nWe need to mention that the task-averaged regret upper bound framework in Theorem 1 is not only suitable to analyze EWA, but also applicable to any online algorithm (e.g. Follow-The-PerturbedLeader in (Suggala & Netrapalli, 2020, Thm 1)) with the regret bound of form Ut(\u03c1, v) = (v + ft(\u03c1) v ) \u221a mb + g(m). In Sections 4.1-4.2, we leverage different strategies to learn initialization \u03c1t1 for Lipschitz and non-Lipschitz functions. For learning the step size vt, we consistently use FollowThe-Leader (FTL) algorithm, as running FTL can achieve the following logarithmic regret bound.\nProposition 1 Assume that FTL algorithm runs on the sequence of functions {ht(v) = v + ft(\u03c1t1) v }t\u2208[T ] over the domain [0, D], where D\n2 \u2265 maxt\u2208[T ] ft(\u03c1t1), then we have the regret bound: T\u2211 t=1 ht(vt)\u2212 min v\u2208[0,D] T\u2211 t=1 ht(v) \u2264 D3 4 T\u2211 t=1 \u2223\u22231\u2212\u2211t\u22121s=1 ft(\u03c1t1)/\u2211t\u22121s=1 fs(\u03c1s1)\u2223\u22232\u2211t s=1 fs(\u03c1s1) .\nFurthermore, if for all t \u2208 [T ], ft(\u03c1t1) \u2208 [B2, D2] with D \u2265 B > 0, then we have the logarithmic regret upper bound: \u2211T t=1 ht(vt)\u2212minv\u2208[0,D] \u2211T t=1 ht(v) \u2264 D7 4B6 (log T + 1)."
        },
        {
            "heading": "4.1 REGRET BOUNDS FOR NON-CONVEX PIECEWISE LIPSCHITZ FUNCTIONS",
            "text": "In this section, we let \u03c1 : \u0398 7\u2192 R\u22650 be an unnormalized distribution over \u0398. For any loss function ` : \u0398 7\u2192 R, \u3008\u03c1, `\u3009 = \u222b \u0398 `(\u03b8)\u03c1(\u03b8)d\u03b8/ \u222b \u0398 \u03c1(\u03b8)d\u03b8. The update rules of Exponentially Weighted Aggregation (EWA) algorithm (with initialized distribution \u03c11 and step size \u03bb) for each round i can be summarized as follows: (1) Set normalization factor Pi = \u222b \u0398 \u03c1i(\u03b8)d\u03b8; (2) Sample \u03b8i with probability pi(\u03b8i) = \u03c1i(\u03b8i) Pi\n; (3) Suffer `i(\u03b8i) and observe `i(\u00b7); (4) \u2200\u03b8 \u2208 \u0398, set \u03c1i+1(\u03b8) = e\u2212\u03bb`i(\u03b8)\u03c1i(\u03b8). We next give the formal definition of piecewise Lipschitz functions proposed by Balcan et al. (2018).\nDefinition 1 (Piecewise Lipschitzness) The sequence of random loss functions {`i}mi=1 is piecewise L-Lipschitz (L > 0) that are \u03b2-dispersed, if \u2200m,\u2200 \u2265 m\u2212\u03b2 , in expectation over the randomness of the loss functions, we have E[max \u2016\u03b8\u2212\u03b8\u2032\u20162\u2264 \u2223\u2223{i \u2208 [m] | `i(\u03b8)\u2212 `i(\u03b8\u2032) > L\u2016\u03b8 \u2212 \u03b8\u2032\u20162}\u2223\u2223] \u2264 O\u0303( m). Piecewise Lipschitzness is a weakly Lipschitz condition, indicating that the loss functions are discontinuous in a concentrated small region. The soft-O suppresses dependence on logarithmic terms. Let {`i : \u0398 7\u2192 [0,M ]}i\u2208[m] be a sequence of piecewise L-Lipschitz functions that are \u03b2-dispersed. Let V (\u03c1, \u03b8\u2217)2 = \u2212 log ( \u222b B(\u03b8\u2217,m\u2212\u03b2) \u03c1(\u03b8)d\u03b8/ \u222b \u0398 \u03c1(\u03b8)d\u03b8), where B(\u03b8\u2217, ) is the -radius ball around the minimizer \u03b8\u2217. Then, adapting the analysis from (Balcan et al., 2021, Thm 2.1) to theM -bounded functions obtains the regret bound for EWA algorithm with the initialization \u03c11 and the step size \u03bb:\nm\u2211 i=1 E\u03b8\u223c\u03c1i`i(\u03b8)\u2212 n\u2211 i=1 `i(\u03b8 \u2217) \u2264 \u03bbM2m+ V (\u03c11, \u03b8 \u2217)2 \u03bb + O\u0303((L+ 1)m1\u2212\u03b2). (2)\nDetailed proofs of Eq. (2) can be found in Lemma D.1. Therefore, the task-similarity is defined as V 2 = min\u03c1:\u03987\u2192R\u22650, \u222b \u0398 \u03c1(\u03b8)d\u03b8=1\u2212 1T \u2211T t=1 log \u222b B(\u03b8\u2217t ,m\u2212\u03b2) \u03c1(\u03b8)d\u03b8. At task t, denote \u0398t = B(\u03b8\u2217t ,m\u2212\u03b2), we need to design an algorithm to minimize the function ft(\u03c1) = \u2212 log( \u222b \u0398t \u03c1(\u03b8)d\u03b8 / \u222b \u0398 \u03c1(\u03b8)d\u03b8). We borrow the idea from Balcan et al. (2021) to discretize \u0398 and translate the minimization problem min\u03c1:\u03987\u2192R\u22650 ft(\u03c1) over the set of distributions into an online convex optimization problem. Concretely, at task t, define the discretization Dt = {A = \u2229s\u2264t\u0398 (\u03b8[s]) s : \u03b8 \u2208 {0, 1}t, vol(A) > 0} of \u0398, where \u0398(0)s = \u0398s,\u0398(1)s = \u0398\\\u0398s. Then we use elements of these discretization to construct non-negative vectors in R|Dt|\u22650 : for any unnormalized distribution \u03c1 : \u0398 7\u2192 R\u22650, let \u03c1(t) \u2208 R|Dt|\u22650 denote the vector with entries \u03c1(t)[A] =\u222b A \u03c1(\u03b8)d\u03b8 for A \u2208 Dt. We will use \u03bd as the uniform measure, i.e. \u03bd(t)[A] = vol(A), and use \u03bd = \u03bd(T ),\u03c1 = \u03c1(T ) for brevity. With simple calculations, it is not difficult to see that ft(\u03c1) = \u2212 log ( \u222b \u0398t \u03c1(\u03b8)d\u03b8/ \u222b \u0398 \u03c1(\u03b8)d\u03b8) = \u2212 log \u3008\u03c1\u2217t , \u03c1\u0302\u3009, where \u03c1\u2217t [A] = 1A\u2282\u0398t . Notice that min\u2016\u03c1\u20161=1 ft(\u03c1) = \u2212 log \u3008\u03c1\u2217t ,\u03c1\u3009 is a convex optimization problem over the |DT |-dimensional simplex. Thus, we can use the optimal Follow-The-Regularized-Leader (FTRL) algorithm with the KL-divergence regularization to solve this problem. The regret bound is exhibited in Proposition 2.\nAlgorithm 1 Non-convex OWO meta learning algorithm for bounded piecewise Lipschitz functions. 1: Input: step size \u03b7 for FTRL, mixture parameter \u03b3 \u2208 (0, 1], domain upper bound D; initialized\ndistribution \u03c111 : \u0398 7\u2192 R\u22650 and initialized step size \u03bb1 = \u221a\n(D2 \u2212 log \u03b3)/(mM2) for EWA. 2: for task t \u2208 [T ] do 3: for round i \u2208 [m] do 4: Set Pti = \u222b \u0398 \u03c1ti(\u03b8)d\u03b8 and sample \u03b8ti with probability pti(\u03b8ti) = \u03c1ti(\u03b8ti)/Pti 5: Suffer loss `ti(\u03b8ti), observe `ti(\u00b7), and update \u03c1t,i+1(\u03b8) = e\u2212\u03bbt`ti(\u03b8)\u03c1ti(\u03b8) // EWA step 6: Sample \u03b8\u2217t with probability ptm and obtain task-t optimum \u03b8 \u2217 t \u2208 \u0398 7: Set 1B(\u03b8\u2217t ,m\u2212\u03b2) to be the function that is 1 in the ball B(\u03b8 \u2217 t ,m\n\u2212\u03b2) and otherwise 0 8: Update \u03c1t+1,1 to \u03c1t+1(t) = arg min\u2016\u03c1\u20161=1,\u03c1\u2265\u03b3\u03bd\u0302(t) KL(\u03c1||\u03bd\u0302(t)) \u2212 \u03b7 \u2211 s\u2264t log \u3008\u03c1\u2217s(t),\u03c1\u3009,\n\u03bbt+1 =\n\u221a \u2212 \u2211 s\u2264t log \u3008\u03c1\u2217s(s),\u03c1s(s)\u3009\ntmM2 // meta-update step\nProposition 2 Assume that FTRL algorithm with initialization \u03bd\u0302 and regularization KL(\u03c1||\u03bd\u0302) runs on functions {ft(\u03c1t)}t\u2208[T ] over the set \u2206 = {\u03c1 : \u2016\u03c1\u20161 = 1,\u03c1 \u2265 \u03b3\u03bd\u0302}, then \u2211T t=1 ft(\u03c1t)\u2212ft(\u03c1\u2217) \u2264 2GL \u221a T/\u03b3, where \u03c1\u2217 \u2208 arg min\u03c1\u2208\u2206 \u2211T t=1 ft(\u03c1), G 2 \u2265 KL(\u03c1\u2217||\u03bd\u0302), L2 = 1T \u2211T t=1 vol(\u0398)2 vol(\u0398t)2 .\nNotice that vol(\u0398t) = Vdm\u2212\u03b2d, where Vd = \u03c0 d/2\n\u0393(d/2+1) is the volume of the unit ball in the ddimensional Euclidean space, \u0393(x+ 1) = x\u0393(x) is the Gamma function. Then, apply Proposition 1 to learn the step size vt in the piecewise Lipschitz setting and choose a proper \u03b3, we attain the regret.\nProposition 3 Assume that FTL algorithm runs on the sequence of functions {ht(v) = v + ft(\u03c1t1)v }t\u2208[T ] with ft(\u03c1t1) = \u2212 log \u3008\u03c1 \u2217 t ,\u03c1t1\u3009 over the domain [0, D], where D2 = maxt\u2208[T ] log vol(\u0398) \u03b3vol(\u0398t)\n\u2265 ft(\u03c1t1), for any t \u2208 [T ]. Denote B2 = mint\u2208[T ] log vol(\u0398)\u03b3vol(\u0398t)+(1\u2212\u03b3)vol(\u0398) . Then if we set \u03b3 = md\u03b2/T\u03b1 \u2208 (0, 1] with \u03b1 \u2208 (0, 1/2), we have the following regret upper bound:\nT\u2211 t=1 ht(vt)\u2212 min v\u2208[0,D] T\u2211 t=1 ht(v) \u2264 (\u03b1 log T + log (vol(\u0398)/Vd)) 7/2(log T + 1) 4B6 .\nFor large enough T , if we set \u03b1 \u2208 [lnmd\u03b2/lnT , ln ( em d\u03b2 e\u22121 \u2212 Vd (e\u22121)vol(\u0398) )/lnT ] \u2282 (0, 1 2 ), we have B2 \u2265 1, and the regret bound in Proposition 3 has the order ofO((log T )9/2) (see more explanations in Remark D.1 of the Appendix). We list the whole pseudo code of the OWO meta learning algorithm for piecewise Lipschitz functions {`ti}t,i\u22651 in Algorithm 1, and achieve our first improved regret bound for this OWO meta learning algorithm by combining regret bounds in Propositions 2-3.\nTheorem 2 Under the conditions of Theorem 1, for any task t \u2208 [T ], let {`ti : \u0398 7\u2192 [0,M ]}i\u2208[m] be a sequence of piecewise L-Lipschitz functions that are \u03b2-dispersed. Set \u03b1 \u2208 (0, 12 ), let g(m) = O\u0303((L+ 1)m1\u2212\u03b2), then using FTRL algorithm in Proposition 2 and FTL algorithm in Proposition 3 respectively to learn the initialization and step size in Algorithm 1 obtains the regret upper bound:\nR\u0304T,m \u2264g(m) + { (\u03b1 log T + log (vol(\u0398)/Vd))7/2(log T + 1)\n4TB6\n+ min {vol(\u0398)(KL(\u03c1\u2217||\u03bd\u0302) + 1)\nVdV T 1 2\u2212\u03b1\n, 2\n\u221a vol(\u0398)(KL(\u03c1\u2217||\u03bd\u0302) + 1)\nVdT 1 2\u2212\u03b1\n} + 2V }\u221a mM.\nRemark 1 Our task-averaged regret bound U\u0304T,m in Theorem 2 has 3 improvements over the regret bound in (Balcan et al., 2021, Thm 3.3) that is obtained under the same assumptions (see Table 1 for details): (1) Our bound guarantees a vanishing task-averaged regret. For any fixed T , our U\u0304T,m is a vanishing bound w.r.t. m (i.e. U\u0304T,m = o(m)). However, in (Balcan et al., 2021, Thm 3.3) the bound U\u0304T,m = O((md/2/T 1/4 + (logm) log T/ \u221a T + V ) \u221a m) is not a vanishing bound w.r.t. m since the action space dimension d \u2265 1. (2) Our regret bound of O(1/T 1/2\u2212\u03b1) (\u03b1 \u2208 (0, 1/2)) for learning the initialization distribution in each task is sharper than that of O(md/2/T 1/4) in Balcan et al. (2021). (3) Our regret bound of O((log T )9/2/T ) for learning the step size has a better convergence rate than that of O((logm) log T/ \u221a T ) in Balcan et al. (2021). More comparisons between our regret bound and that in Balcan et al. (2021) can be found in Table B.1 of the Appendix.\nAlgorithm 2 Non-convex OWO meta learning algorithm for bounded non-Lipschitz functions. 1: Input: initialized distribution \u03c111 \u2208 P(\u0398) and learning rate \u03bb1 > 0. 2: for task t \u2208 [T ] do 3: for round i \u2208 [m] do 4: \u03c1ti = arg min\u03c1\u2208P(\u0398) KL(\u03c1||\u03c1t1) + \u03bbt \u2211i\u22121 j=1\u3008`tj , \u03c1\u3009 // EWA step\n5: Suffer loss \u3008\u03c1ti, `ti\u3009 and observe `ti(\u00b7) 6: Update \u03c1t+1,1 = 1t \u2211t s=1 \u03c1sm, \u03bbt+1 = \u221a\u2211t s=1 KL(\u03c1sm||\u03c1s1) tmM2 // meta-update step"
        },
        {
            "heading": "4.2 REGRET BOUNDS FOR NON-CONVEX NON-LIPSCHITZ FUNCTIONS",
            "text": "In this section, we will describe the problem with the language of measure theory. For probability distributions \u03c1, \u03c0 \u2208 P(\u0398), define the Radon-Nikodym (RN) derivative of \u03c1 w.r.t. \u03c0 as d\u03c1d\u03c0 , when \u03c1 is absolutely continuous w.r.t. \u03c0 (i.e. \u03c1 \u03c0). If \u03c1 6 \u03c0, we simply set d\u03c1d\u03c0 = +\u221e. We refer readers to Page 247 in Yeh (2014) for more properties of RN derivative. For distribution \u03c1, \u03c0, the KL-divergence between them is defined as KL(\u03c1||\u03c0) = E\u03b8\u223c\u03c1 log d\u03c1d\u03c0 , and the \u03c7\n2-divergence is \u03c72(\u03c1||\u03c0) = E\u03b8\u223c\u03c0 [ ( d\u03c1d\u03c0 ) 2 \u2212 1 ] . Then the update rule at i-th round of exponentially weighted aggregation (EWA) algorithm (with initialization distribution \u03c11 and step size \u03bb) can be summarized as \u03c1i = arg min\u03c1\u2208P(\u0398) \u2211i\u22121 j=1\u3008\u03c1, `j\u3009+ KL(\u03c1||\u03c11)/\u03bb, and the posterior \u03c1i has an analytic for-\nm: \u03c1i(d\u03b8) = exp{\u2212\u03bb \u2211i\u22121 j=1 `j(\u03b8)}\u03c11(d\u03b8) /\u222b exp{\u2212\u03bb \u2211i\u22121 j=1 `j(\u03b8)}\u03c11(d\u03b8). According to (Alquier, 2021, Thm 2.1), for any loss function `i : \u0398 7\u2192 [0,M ], EWA has the following regret upper bound: m\u2211 i=1 \u3008\u03c1i \u2212 \u03c1, `i\u3009 \u2264 \u03bbM2m+ KL(\u03c1||\u03c11) \u03bb , \u2200\u03c1 \u2208 P(\u0398). (3)\nTherefore, the task similarity V 2 = min\u03c1\u2208P(\u0398) 1T \u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1). Then, we need to choose online algorithms to learn respectively the initialization \u03c1t1 and step size vt in Eq. (3) of EWA on task t. For learning \u03c1t1, we choose FTL algorithm over functions {KL(\u03c1\u2217t ||\u03c1t1)}t\u22651 to yield the regret bound.\nProposition 4 Given a sequence of distributions {\u03c1\u2217t }t\u2208[T ], assume that FTL algorithm runs on the sequence of {KL(\u03c1\u2217t ||\u03c1)}t\u2208[T ] to determine \u03c1, i.e. \u03c1t1 = arg min\u03c1\u2208P(\u0398) \u2211t\u22121 s=1 KL(\u03c1 \u2217 s||\u03c1), and further assume G2 \u2265 maxt\u2208[T ] \u03c72(\u03c1\u2217t ||\u03c1t1), then we can obtain the following regret upper bound: T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t1)\u2212 min \u03c1\u2208P(\u0398) T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1) \u2264 T\u2211 t=1 \u03c72(\u03c1\u2217t ||\u03c1t1) t \u2264 G2(log T + 1). The above logarithmic regret bound is non-trivial, since KL(\u03c1\u2217t ||\u03c1t1) is the functional of distribution \u03c1t1, and it is hard to find a norm in the space of distributions \u03c1t1 to verify the strong-convexity or Lipschitzness of functional KL(\u03c1\u2217t ||\u03c1t1). Thus, we are unable to utilize regret analysis for the functions over Euclidean space (Shalev-Shwartz, 2012). Instead, we use the analytic form of the optimal distribution of FTL and the properties of RN derivative to get this logarithmic regret (see Section E). Then, applying Proposition 1 to learn the step size achieves the following regret bound.\nProposition 5 Given a sequence of functions {ft(\u03c1t1) = KL(\u03c1\u2217t ||\u03c1t1)}t\u2208[T ], assume that there exist D2 \u2265 maxt\u2208[T ] KL(\u03c1\u2217t ||\u03c1t1), B2 \u2264 mint\u2208[T ] KL(\u03c1\u2217t ||\u03c1t1) with B > 0. Assume that FTL algorithm runs on the sequence of functions {ht(v) = v+ ft(\u03c1t1)v }t\u2208[T ] over the domain [0, D]. Then we have\nT\u2211 t=1 ht(vt)\u2212 min v\u2208[0,D] T\u2211 t=1 ht(v) \u2264 D7 4B6 (log T + 1).\nIf we use EWA algorithm to learn distribution \u03c1\u2217t (i.e. set \u03c1 \u2217 t = \u03c1tm) for task t, then the above condition B > 0 truly holds (see proof at the end of Section E). The pseudo code of OWO meta learning algorithm with \u03c1\u2217t = \u03c1tm is listed in Algorithm 2 for non-Lipschitz functions. The corresponding task-averaged regret upper bound is given as follow by combining results in Propositions 4-5, achieving the fastest convergence rate O((log T/T +V ) \u221a m) with respect to T in the current work.\nTheorem 3 Under the conditions of Theorem 1, for any task t \u2208 [T ], let {`ti : \u0398 7\u2192 [0,M ]}i\u2208[m] be a sequence of M -bounded functions. Let G2 \u2265 maxt\u2208[T ] \u03c72(\u03c1\u2217t ||\u03c1t1), D2 \u2265 maxt\u2208[T ] KL(\u03c1\u2217t ||\u03c1t1)\nand mint\u2208[T ] KL(\u03c1\u2217t ||\u03c1t1) \u2265 B2 > 0, then using FTL algorithm to respectively learn the initialization and step size of EWA algorithm in Algorithm 2 attains the task-averaged regret upper bound:\n1\nT T\u2211 t=1 m\u2211 i=1 \u3008\u03c1ti\u2212\u03c1\u2217t , `ti\u3009\u2264 (D7(log T + 1) 4TB6 +min{G 2(log T + 1) V T , 2G \u221a log T + 1 T }+2V )\u221a mM.\nRemark 2 Let\u2019s compare the regret bound in Theorem 3 (for bounded functions) with the bound in Theorem 2 (for bounded piecewise Lipschitz functions) and the bound in (Khodak et al., 2019, Thm 3.2) (for convex and Lipschitz functions) in Table 1. (1) In Theorem 3, the regret bounds for learning the initialization and the step size are O(log T/T ), both of which are sharper than the corresponding regret bounds of O(1/T 1/2\u2212\u03b1) and O((log T )9/2/T ) in Theorem 2. Besides, Theorem 3 obtains improved regret bounds without piecewise Lipschitz assumption of loss functions. (2) When compared with Khodak et al. (2019), both our regret bound and theirs for learning the initialization have the same order of O(log T/T ); but our bound for learning the step size is of O(log T/T ), sharper than theirs of O(log T/ \u221a T ). The improvement is obtained by using primaldual technique from Shalev-Shwartz & Kakade (2008) to analyze the strong-convexity of functions {ht(v) = v + ft(\u03c1t1)/v}t\u2208[T ]. Besides, we achieve the improved result for bounded functions, without convexity or Lipschitzness assumptions of loss functions {`ti}t\u2208[T ],i\u2208[m]. We further discuss the limitations of our Theorem 3 and our Algorithm 2 in Remark B.1 and Remark H.1, respectively."
        },
        {
            "heading": "5 GENERALIZATION BOUNDS FOR META LEARNING VIA REGRET ANALYSIS",
            "text": "In this section, we show how to derive generalization bound for statistical meta learning via regret analysis. Concretely, in Section 5.1, we provide a novel transfer risk bound for non-convex batch meta learning under the task distribution assumption. In Section 5.2, we yield a PAC-Bayes generalization bound for statistical multi-task learning that supposes independence between tasks."
        },
        {
            "heading": "5.1 IMPROVED TRANSFER RISK BOUNDS FOR STATISTICAL META LEARNING",
            "text": "In statistical meta learning, let \u03c4 be a probability measure over the set of all data distributions \u00b5 on bounded loss functions ` : \u0398 7\u2192 [0,M ]. A sequence of loss function {`ti}t\u2208[T ],i\u2208[m] is generated by drawing m loss functions i.i.d. from each in a sequence of distributions {\u00b5t}t\u2208[T ], where each \u00b5t is regarded as a random variable and is drawn i.i.d. from \u03c4 . We use the cumulative information {\u03c1t1, \u03bbt}t\u2208[T ] from previous T training tasks in OWO meta learning to run an online learning algorithm EWA on the novel task with loss functions {`i}i\u2208[m] generated i.i.d. from distribution \u00b5 that is drawn i.i.d. from \u03c4 , to output a sequence of probability distributions {\u03c1i}i\u2208[m]. Using online-to-batch arguments, we achieve the transfer risk bound for non-convex batch meta learning.\nTheorem 4 Assume that for each task t \u2208 [T ], there exist G2 \u2265 maxt\u2208[T ] \u03c72(\u03c1\u2217t ||\u03c1t1), and mint\u2208[T ] KL(\u03c1 \u2217 t ||\u03c1t1) \u2265 B2 > 0. Assume that the novel task consists of loss functions {`i}i\u2208[m]\ni.i.d.\u223c \u00b5, \u00b5 i.i.d.\u223c \u03c4 , and for any optimal distribution \u03c1\u2217 over task \u00b5, there exists H > 0 such that KL(\u03c1\u2217|| 1T \u2211T t=1 \u03c1t1) \u2264 H . Then we use ( 1 T \u2211T t=1 \u03c1t1, \u221a\u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1t1)/(TmM2)) to run EWA algorithm for novel task \u00b5 \u223c \u03c4 with loss functions {`i}i\u2208[m] to output probability distributions {\u03c1i}i\u2208[m]. Then let \u03c1\u0304 = 1m \u2211m i=1 \u03c1i, for the optimal distribution \u03c1\n\u2217 over task \u00b5 that does not dependent on {`i}i\u2208[m], with probability 1\u2212 \u03b4 over the draw of probability distributions {\u00b5t}Tt=1:\nE\u00b5\u223c\u03c4E{`i}mi=1\u223c\u00b5mE`\u223c\u00b5E\u03b8\u223c\u03c1\u0304`(\u03b8)\u2264E\u00b5\u223c\u03c4E`\u223c\u00b5E\u03b8\u223c\u03c1\u2217`(\u03b8)+M( \u221a 4G2 log T\nTm + 3V 2\u221a mB + H B\n\u221a log 1/\u03b4\n2Tm ).\nRemark 3 (Comparisons among different transfer risk bounds) (1) The first group of bounds holds for meta learning algorithms that learn a shared representation across tasks: The bound in (Alquier et al., 2017, Thm 6.1) is O(1/ \u221a m+ 1/ \u221a T ). The bound in (Denevi et al., 2019, Cor 42)\nis O( \u221a logm/m + 1/ \u221a T ). (2) The second group holds for algorithms that learn the initialization and step size for each task: The bound in (Khodak et al., 2019, Thm 5.1) is O(log T/(m \u221a T ) + 1/ \u221a m + 1/ \u221a Tm). The bound in Theorem 4 O( \u221a log T/(Tm) + 1/ \u221a m + 1/ \u221a Tm) is slightly\nlarger, because our work focuses on non-convex setting and \u2211 t ft(\u03c1)/v is not convex w.r.t. (\u03c1, v).\nHowever, (Khodak et al., 2019) focuses on convex loss and \u2211 t \u2016\u03b8 \u2212 \u03b8\u2217t \u201622/v is convex w.r.t. (\u03b8, v), hence able to use Jensen inequality to get a slightly better transfer risk bound (see their Thm E.1)."
        },
        {
            "heading": "5.2 PAC-BAYES GENERALIZATION BOUNDS FOR STATISTICAL MULTI-TASK LEARNING",
            "text": "In statistical multi-task learning, each task t has the training dataset St = {zti}mi=1, where zti is drawn i.i.d. from the sample space Z according to the data distribution \u00b5t. We also suppose the independence between datasets Si and Sj (i 6= j) drawn from different tasks. We use Fti = \u03c3({zsj}1\u2264s\u2264t,1\u2264j\u2264i) to denote the \u03c3-algebra induced by the sequence of random variables up until the end of i-round at t-th task. Formally, an online learning algorithm \u03a0tm = {\u03c1ti}mi=1 is the set of m probability distributions over the action space \u0398. For convenience, we abbreviate \u03c1ti = \u03c1ti({zsj}1\u2264s\u2264t\u22121,1\u2264j\u2264m \u222a {ztj}1\u2264j\u2264i\u22121). A random variable associated to the online learning algorithm \u03a0tm is M\u03a0tm = 1 m \u2211m i=1\u3008\u03c1ti, cti\u3009, where cti(\u03b8) =\nEz\u223c\u00b5t [`(\u03b8, z)|\u03b8] \u2212 1/m \u2211m i=1 `(\u03b8, zti). Note that M\u03a0tm is a sum of normalized martingale differences since E[\u3008\u03c1ti, cti\u3009|Ft,i\u22121] = 0. The generalization error of the action \u03b8 \u2208 \u0398 is defined as: gen(\u03b8, St) = Ez\u223c\u00b5t [`(\u03b8, z)|\u03b8]\u22121/m \u2211m i=1 `(\u03b8, zti). In PAC-Bayes learning, for each task t, the algorithmAt takes the training sample St and a prior as input and outputs a posteriorAt(St) \u2208 P(\u0398). The generalization error of posteriorAt(St) is defined as gen(At, St) = E\u03b8\u223cAt(St)gen(\u03b8, St). Then we obtain the following proposition that connects the multi-task generalization error of posteriors {At(St)}t\u22651 in statistical multi-task learning and the task-averaged regret in OWO meta learning.\nProposition 6 Let R\u0304T,m= \u2211T,m t,i=1\u3008\u03c1ti\u2212At(St), `ti T \u3009, then \u2211T t=1gen(At, St)= R\u0304T,m m \u2212 1 T \u2211T t=1M\u03a0tm .\nUsing U\u0304T,m in our Theorem 3 to upper bound the task-averaged regret R\u0304T,m, and applying simple concentration inequality to upper bound the sum of martingale differences \u2211 tM\u03a0tm , we obtain for statistical multi-task learning a novel PAC-Bayes bound that does not appear in existing literature.\nTheorem 5 LetAt be the statistical learning algorithm for task t \u2208 [T ]. Assume that for all actions \u03b8 \u2208 \u0398, samples z \u2208 Z , `(\u03b8, z) \u2208 [0,M ]. Then, with probability at least 1 \u2212 \u03b4 over the draw of {St}t\u2208[T ], the multi-task generalization error of statistical learning algorithms {At}t\u2208[T ] satisfies:\n1\nT T\u2211 t=1 E\u03b8\u223cAt(St) [ Ez\u223c\u00b5t`(\u03b8, z)\u2212 1 m T\u2211 i=1 `(\u03b8, zti) ] \u2264 U\u0304T,m m +M \u221a 2 log 1\u03b4 Tm .\nRemark 4 We give two insights into the PAC-Bayes bound in our Theorem 5. (1) The most related work to our bound is the PAC-Bayes bound for multi-task generalization error in PAC-Bayes meta learning theory (in the batch setting), which assumes that the priors for each task are the same and are sampled from the hyper-posterior Q. The tightest PAC-Bayes bound in this field is from (Guan & Lu, 2022, Prop 3) of O({EP\u223cQ \u2211T t=1 KL(At(St)||P ) + ln (Tm)}/(Tm)) \u2248 O(1/m + ln (Tm)/(Tm)). Such PAC-Bayes bound for batch multi-task learning is sharper than our bound of O(V/ \u221a m + 1/ \u221a Tm) in our Theorem 5 for statistical multi-task learning, indicating the difficulty of online meta learning w.r.t. batch meta learning. (2) Our Proposition 6 for the first time reveals the connection between multi-task generalization error in PAC-Bayes theory and the task-averaged regret in non-convex OWO meta learning. This gives a promising direction of proving PAC-Bayes multi-task generalization bounds by applying regret analysis and simple concentration inequality."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We study the non-convex online-within-online (OWO) meta learning of the initialization and step size of exponentially weighted aggregation (EWA) algorithm. We extend the averaged regret upper bound analysis to the non-convex setting, and typically propose to learn the step size with FollowThe-Leader (FTL) algorithm to guarantee the logarithmic regret. For learning the initialization, we develop two algorithms based on the type of loss functions. For piecewise Lipschitz functions, we choose Follow-The-Regularized-Leader algorithm to learn the discrete initialization distribution and achieve a sublinear regret. For non-Lipschitz functions, we utilize FTL algorithm to learn the continuous initialization distribution and derive a logarithmic regret. Both strategies lead to improved regret bound for non-convex OWO meta learning. Furthermore, by online-to-batch arguments, we yield a new transfer risk bound for batch meta learning. By online-to-PAC techniques, we achieve a novel PAC-Bayes generalization bound for statistical multi-task learning, revealing a promising framework of proving PAC-Bayes bounds for multi-task setting via regret analysis. Our ongoing research includes exploring the OWO meta learning of other online algorithms (e.g. Follow-ThePerturbed-Leader), and investigating whether we can obtain its optimal task-averaged regret bound."
        },
        {
            "heading": "7 ACKNOWLEDGEMENT",
            "text": "We would like to thank all reviewers for their constructive suggestions to improve the quality of this paper. This work was partially supported by National Natural Science Foundation of China (Grant No.92370204), Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Science and Technology Planning Project of Guangdong Province(Grant No.2023A0505050111), Education Bureau of Guangzhou Municipality, and Guangdong Science and Technology Department."
        },
        {
            "heading": "A ADDITIONAL RELATED WORK",
            "text": "Online Convex Optimization. In online convex optimization (OCO) setting where the loss functions {`i}i\u2208[m] are convex, the regret is defined as Rm , \u2211m i=1 `i(\u03b8i)\u2212 `i(\u03b8\u2217), where \u03b8\u2217 =\narg min\u03b8\u2208\u0398 \u2211m i=1 `i(\u03b8). The upper bound Um \u2265 Rm on the regret is always called regret bound. The goal of online learner is to choose a sequence of actions {\u03b8i}i\u2208[m] that will guarantee the vanishing regret, i.e. Rm = o(m) or Um = o(m). In OCO problem, two famous algorithms are Follow-The-Leader (FTL) and Follow-The-Regularized-Leader (FTRL). At i-th round, FTL chooses the action \u03b8i = arg min\u03b8\u2208\u0398 \u2211i\u22121 j=1 `j(\u03b8); and FTRL selects the action\n\u03b8i = arg min\u03b8\u2208\u0398 \u2211i\u22121 j=1 `j(\u03b8) + R(\u03b8), where R : \u0398 7\u2192 R\u22650 is always a strongly-convex regularization function like `2-norm. When {`i}i\u2208[m] are convex and Lipschitz, FTRL achieves optimal regret of O( \u221a m) (see (Shalev-Shwartz, 2012, Thm 2.11) for upper bound and (Hazan, 2023, Thm 3.2) for lower bound). When {`i}i\u2208[m] are strongly-convex and Lipschitz, FTL achieves optimal regret of O(logm) (see (Shalev-Shwartz & Kakade, 2008, Cor 1) for upper bound and (Takimoto & Warmuth, 2000, Thm 4) for lower bound). We introduce FTRL and FTL in detail because these two algorithms are utilized as the main components of our OWO meta learning algorithm in Section 4.\nOnline Non-Convex Optimization. However, in online non-convex optimization setting where the loss functions {`i}i\u2208[m] are non-convex, minimizing regret is more difficult than that in convex case. It has been demonstrated that all deterministic online algorithms (e.g. the aforementioned FTL and FTRL for OCO) cannot obtain vanishing regret in the non-convex setting (see (Suggala & Netrapalli, 2020, Prop 3) and (Cesa-Bianchi & Lugosi, 2006, Sect 4.1) for details). Therefore, at i-th round we need to add randomness into the algorithm, and choose the action drawn from certain probability distribution \u03c1i \u2208 P(\u0398). The corresponding regret is defined as Rm , \u2211m i=1 E\u03b8\u223c\u03c1i`i(\u03b8)\u2212 `i(\u03b8\u2217),\nwhere \u03b8\u2217 \u2208 arg min\u03b8\u2208\u0398 \u2211m i=1 `i(\u03b8). If {`i}i\u2208[m] are Lipschitz, using Follow-The-Perturbed-Leader\nalgorithm allows to achieve the optimal regret O( \u221a m) (see (Suggala & Netrapalli, 2020, Thm 1)).\nGeneralization Bounds for Statistical Multi-Task Learning. Statistical multi-task learning refers to the multi-task setting where data within each task are assumed to be independently sampled from the same distribution. The investigation of generalization bounds for statistical multi-task learning has a long history and dates back to (Baxter, 1995). In the last decades, different generalization bounds are proposed for multi-task learning, mostly based on model-capacity theory: for example, the generalization bound based on covering number complexity (Ando & Zhang, 2005), the bounds based on the VC-dimension of hypothesis space (Baxter, 2000; Ben-David & Borbely, 2008), the bound based on Gaussian complexity (Maurer et al., 2016). Apart from them, there also exist generalization bounds for multi-task learning that are derived based on algorithmic stability analysis, like (Maurer, 2005; Liu et al., 2017). Recent works also use localized Rademacher complexity analysis to obtain improved generalization bounds (Yousefi et al., 2018). However, little literature use PAC-Bayes theory to prove bounds for multi-task learning, except for the PAC-Bayes generalization bounds in statistical meta leaning that involves the multi-task generalization bounds (Pentina & Lampert, 2014; Amit & Meir, 2018; Guan & Lu, 2022). But the PAC-Bayes multi-task generalization bounds in meta learning theory assume the priors for different training tasks are the same and are sampled from the same hyper-prior distribution. Therefore, to the best of our knowledge, there is still no explicit PAC-Bayes generalization bound for standard statistical multi-task learning setting."
        },
        {
            "heading": "B EXPLICIT FORM OF REGRET BOUNDS FOR OWO META LEARNING",
            "text": "Remark B.1 (Limitations of the regret bound in Theorem 3) For any task t \u2208 [T ], if we set the distribution \u03c1\u2217t as Dirac measure \u03b4\u03b8\u2217t that only has mass 1 at the minimizer \u03b8 \u2217 t , the regret in Theorem 3 degenerates to the regret defined in Eq. (1) (i.e. the regret in our Theorem 2). To let KL(\u03c1\u2217t ||\u03c1t1) (where \u03c1t1 = 1/(t\u22121) \u2211t\u22121 s=1 \u03c1 \u2217 s) make sense, there should exist at least one s \u2208 [t\u22121],\nsuch that \u03c1\u2217s({\u03b8\u2217t }) > 0 (otherwise the RN derivative d\u03c1\u2217t /d( \u2211t\u22121 s=1 \u03c1 \u2217 s) =\u221e). However, the optimal actions for the past tasks may not be the optimal one for future tasks. Hence the regret bound in our Theorem 3 may be vacuous under the regret definition in Eq. (1). This also indicates to some extent the limitation of the regret (defined as the gap between the cumulative loss w.r.t. {\u03c1ti}mi=1 and the loss w.r.t. the optimal distribution \u03c1\u2217t ) and the f -divergence based regret upper bounds (which, including the f -divergence between \u03c1\u2217t and the initial distribution \u03c1t1, will become vacuous if we set \u03c1\u2217t as a Dirac measure. See more details in (Alquier, 2021, Thm 2.1)) for EWA-type algorithm.\nTa bl\ne B\n.1 :\nD iff\ner en\nt ta\nsk -a\nve ra\nge d\nre gr\net bo\nun ds\nfo r\nO W\nO m\net a\nle ar\nni ng\nal go\nri th\nm s\nun de\nr di\nff er\nen ce\nas su\nm pt\nio ns\nof lo\nss fu\nnc tio\nns {` ti } t \u2208\n[T ], i\u2208\n[m ].\nIn th\nes e\nbo un\nds , T\nis th\ne nu\nm be\nr of\nta sk\ns, an\nd m\nis th\ne nu\nm be\nr of\nite ra\ntio ns\npe r\nta sk\n. C\non cr\net el\ny, th\ne ta\nsk -a\nve ra\nge d\nre gr\net up\npe r\nbo un\nd = ( Bou\nnd I\n+ B\nou nd\nII +\nV ) \u221a m\n+ g (m\n), w\nhe re\nB ou\nnd Ii\ns th\ne re\ngr et\nup pe\nrb ou\nnd fo\nrl ea\nrn in\ng th\ne in\niti al\niz at\nio n,\nB ou\nnd II\nis th\ne re\ngr et\nup pe\nrb ou\nnd fo\nrl ea\nrn in\ng th\ne st\nep si\nze ,V\nre pr\nes en ts th e ta sk si m ila ri ty am on g T di ff er en tt as ks . Fo r al lt \u2208 [T ], vo l( \u0398 t ) = V d m \u2212 d \u03b2 . In (K ho da k et al ., 20 19 ,T hm 3. 2) an d ou r T he or em 3, g (m ) = 0. In (B al ca n et al ., 20 21 ,T hm 3. 3) an d ou r T he or em 2, g (m ) = O (m 1 \u2212 \u03b2 ). W e se tO (m in {F T (\u03c1 \u2217 ) V T ,2 \u221a F T (\u03c1 \u2217 ) T }) = O ( F T (\u03c1 \u2217 ) V T ) = O ( F T (\u03c1 \u2217 ) T ) fo r al lB ou nd I in Ta bl e 1 of th e m ai n te xt fo rb re vi ty .I n th e bo un d of K ho da k et al .( 20 19 ), d \u03c6 (\u03b8 \u2217 t ||\u03b8 t1 ) re pr es en ts th e B re gm an di ve rg en ce be tw ee n ve ct or s \u03b8\u2217 t an d \u03b8 t 1 w ith re sp ec tt o fu nc tio n \u03c6 .\nE xi\nst in\ng W\nor ks\nTa sk\n-A ve\nra ge\nd R\neg re\nt A\nss um\npt io\nns of\n{` ti } t \u2208\n[T ], i\u2208\n[m ]\nB ou\nnd I\nB ou\nnd II\nTa sk\nSi m\nila ri\nty V\n2\nK ho\nda k\net al\n.( 20\n19 )\n1 T\n\u2211 T,m t,i =\n1 ` t i( \u03b8 t i) \u2212 ` t i( \u03b8\u2217 t\n) C\non ve\nx &\nL ip\nsc hi\ntz\nO ( mi\nn { ma\nx t\u2208\n[T ] \u2016\u03b8 \u2217 t \u20162\nlo g T V T ,\nm a x t\u2208\n[T ] \u2016\u03b8 \u2217 t \u2016\u221a lo\ng T T\n}) O ( mi\nn { 1 V\u221a\nT ,\n1 T 1 4 } + lo g T \u221a T ( d \u03c6( \u03b8\u2217 t ||\u03b8 t1\n))3 2) m\nin \u03b8\n1 T\n\u2211 T t= 1 d \u03c6 (\u03b8 \u2217 t ||\u03b8\n)\nB al\nca n\net al\n.( 20\n21 )\n1 T\n\u2211 T,m t,i =\n1 \u3008\u03c1 ti ,` ti \u3009\u2212\n` t i( \u03b8\u2217 t\n) B\nou nd\ned &\nPi ec\new is\ne L\nip sc\nhi tz\nO ( mi\nn { KL\n(\u03c1 \u2217 ||\u03bd\u0302\n) 1 4\nv o l(\n\u0398 )\n1 2\nV T\n1 4 v o l(\n\u0398 t )\n1 2 ,\nK L\n(\u03c1 \u2217 ||\u03bd\u0302\n) 1 8\nv o l(\n\u0398 )\n1 4\nT 1 8\nv o l(\n\u0398 t )\n1 4\n}) O ( mi\nn { 1 V\u221a\nT ,\n1 T 1 4 } + lo g T \u221a T ( log v o l( \u0398 ) 3 4 T 1 4 v o l( \u0398 t ) 3 4 K L (\u03c1 \u2217 || \u03bd\u0302 ) 1 2\n)3 2) m\nin \u03c1 \u2212\n1 T\n\u2211 T t= 1\nlo g \u222b B(\u03b8\u2217 t\n, ) \u03c1 (d \u03b8)\nO ur\nT he\nor em\n2 1 T\n\u2211 T,m t,i =\n1 \u3008\u03c1 ti ,` ti \u3009\u2212\n` t i( \u03b8\u2217 t\n) B\nou nd\ned &\nPi ec\new is\ne L\nip sc\nhi tz\nO ( mi\nn { KL\n(\u03c1 \u2217 ||\u03bd\u0302\n)v o l(\n\u0398 )\nV T\n1 2 \u2212 \u03b1\n,\nK L\n(\u03c1 \u2217 ||\u03bd\u0302\n) 1 2\nv o l(\n\u0398 )\n1 2\nT 1 4 \u2212 \u03b1 2\n}) O (( lo\ng T\n+ lo\ng v o l(\n\u0398 )) 7 2 lo\ng T\nT\n) m\nin \u03c1 \u2212\n1 T\n\u2211 T t= 1\nlo g \u222b B(\u03b8\u2217 t\n, ) \u03c1 (d \u03b8)\nO ur\nT he\nor em\n3 1 T\n\u2211 T,m t,i =\n1 \u3008\u03c1 ti \u2212 \u03c1 \u2217 t, ` t i\u3009\nB ou\nnd ed\nO ( mi\nn { max\nt \u2208\n[T ] \u03c7\n2 (\u03c1 \u2217 t ||\u03c1 t 1 ) lo\ng T\nV T\n,\n\u221a ma x t\u2208\n[T ] \u03c7\n2 (\u03c1 \u2217 t| |\u03c1 t1\n)\u221a lo g T T\n}) O ( log\nT ( max\nt \u2208\n[T ] K\nL (\u03c1 \u2217 t ||\u03c1 t 1 )) 7 2\nT ( min\nt \u2208\n[T ] K\nL (\u03c1 \u2217 t ||\u03c1 t 1 )) 3\n) m\nin \u03c1\n1 T\n\u2211 T t= 1\nK L\n(\u03c1 \u2217 t| |\u03c1\n)\nRemark B.2 (Our Three Technical Novelties in Deriving Improved Regret Bounds for OWO meta learning) (1) The first novelty lies in deriving improved regret bound for the online algorithm that runs over the functions {ht(v) = v + ft(\u03c1t1)/v}t\u2208[T ] on the domain [B2, D2] to learn the step size vt of EWA algorithm. Throughout the whole paper, we choose the efficient Follow-The-Leader (FTL) algorithm to learn the step size vt = arg minv\u2208[B2,D2] \u2211t\u22121 s=1 hs(v) = (\u2211t\u22121 s=1 fs(\u03c1s1)/(t\u2212 1)\n)1/2 and use the primal-dual analysis from (Shalev-Shwartz & Kakade, 2008, Cor 1) to derive a logarithmic regret bound of O(log T ). The key step in obtaining the logarithmic regret O(log T ) is to show that mint\u2208[T ] ft(\u03c1t1) is strictly positive (i.e. B > 0) to guarantee the strong-convexity of ht(v) and the boundedness of \u2202ht(vt) (i.e. the Lipschitz property of ht(v) at the point vt). The positiveness of mint\u2208[T ] ft(\u03c1t1) is guaranteed for piecewise Lipschitz functions in Proposition 3 and for nonLipschitz functions in Proposition 5 respectively, via a fine-grained estimation of the lower bound B2 of ft(\u03c1t1). In contrast, existing works (Khodak et al., 2019, Prop B.2) and (Balcan et al., 2021, Cor 3.2) both choose -FTL algorithm to optimize the functions {ht(v) = v+(ft(\u03c1t1)+ 2)/v}t\u2208[T ] on the domain [0, D2] (where D2 \u2265 maxt ft(\u03c1t1)) to learn the step size vt, and lead to the regret O(T 2 + (log T )/ 2), which is of O( \u221a T log T ) if we set = 1/T 1/4 and is slower than our bound. (2) The second novelty lies in deriving improved regret bound for the online algorithm that runs over the functions {ft(\u03c1) = V (\u03c1, \u03c1\u2217t )}t\u2208[T ] in the piecewise Lipschitz case. We use Follow-TheRegularized-Leader (FTRL) algorithm to achieve the regret of O(T 1/2+\u03b1), \u03b1 \u2208 (0, 12 ), and the mixture parameter \u03b3 = md\u03b2/T\u03b1 is irrelevant to the optimal \u03c1\u2217 (hence \u03b3 can be set in advance). Existing work (Balcan et al., 2021, Thm 3.2) also uses FTRL algorithm, but Balcan et al. (2021) leverage a more complicated analysis, attaining a larger regret bound O(md/2T 3/4). Nevertheless, the choice of \u03b3 in (Balcan et al., 2021) depends on the knowledge of optimal parameter \u03c1\u2217 that contains information of T training tasks, which is unfeasible in the sequential online meta learning setting. (3) The third novelty lies in deriving improved regret bound for the online algorithm that runs over the functions {ft(\u03c1) = KL(\u03c1\u2217t ||\u03c1)}t\u2208[T ] in the non-Lipschitz case. Obtaining regret bounds for FTL algorithm run over the functions {KL(\u03c1\u2217t ||\u03c1)}t\u2208[T ] is hard, because KL(\u03c1\u2217t ||\u03c1) is the functional of the probability distribution \u03c1, and thus we are unable to use traditional regret analysis (e.g. the gradient boundedness and strong convexity analysis of the functions) for the functions on Euclidean space Shalev-Shwartz (2012). Instead, we leverage an insightful lemma from (Frigyik et al., 2008, Thm II.1) to obtain the analytic form \u03c1t1 = arg min\u03c1\u2208P(\u03b8) \u2211t\u22121 s=1 KL(\u03c1 \u2217 s||\u03c1) = 1t\u22121 \u2211t\u22121 s=1 \u03c1 \u2217 s of the solution of FTL algorithm. Then, we use this analytic form, as well as the properties of RN derivative to estimate the upper bound of the regret and ultimately obtain a non-trivial logarithmic regret O(log T ), achieving so far the tightest regret bound for learning the initialization of EWA algorithm.\nRemark B.3 (Comparisons between our Task-Averaged Regret Bound and the Vanilla Averaged Regret Bound for EWA Algorithm). Recall that in Theorem 3, G2 \u2265 maxt\u2208[T ] \u03c72(\u03c1\u2217t ||\u03c1t1), D2\u2265 maxt\u2208[T ] KL(\u03c1 \u2217 t ||\u03c1t1), mint\u2208[T ] KL(\u03c1\u2217t ||\u03c1t1) \u2265 B2, then our task-averaged regret upper bound is\n1\nT T\u2211 t=1 m\u2211 i=1 \u3008\u03c1ti\u2212\u03c1\u2217t , `ti\u3009\u2264 (D7(log T + 1) 4TB6 +min{G 2(log T + 1) V T , 2G \u221a log T + 1 T }+2V )\u221a mM. The vanilla averaged regret bound (Alquier, 2021, Thm 2.1) for EWA algorithm (with the step size \u03bbt = 1/(M \u221a m) and the initialization \u03c1t1 for task t) across T tasks without knowledge transfer is\n1\nT T\u2211 t=1 m\u2211 i=1 \u3008\u03c1ti \u2212 \u03c1\u2217t , `ti\u3009\u2264 1 T T\u2211 t=1 (\u03bbtM 2m+ KL(\u03c1\u2217t ||\u03c1t1) \u03bbt ) = \u221a mM(1 + 1 T T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t1)).\nWe can observe that: (1) If we assume the same upper bound D2 on KL(\u03c1\u2217t ||\u03c1t1), the vanilla averaged regret bound is of O( \u221a mMD2), which is independent of T and could not decrease with the increase of T . This demonstrates that we are unable to obtain tighter regret bound for multi-task learning than that for single-task learning, if we do not share any knowledge across different tasks. (2) When T is large enough, our task-averaged regret bound is of O( \u221a mMV ). Such regret bound is typically sharper than the vanilla averaged regret bound O( \u221a mMD2) when V << D2, i.e. when different tasks share a high degree of similarity. This indicates that, when T training tasks are similar enough, leveraging knowledge from previous tasks can achieve better theoretical guarantee than single-task learning, validating the advantages of OWO meta learning. (3) In our Theorem 3, the proposed task similarity notion V 2 = min\u03c1 1T \u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1) = 1T \u2211T t=1 KL(\u03c1 \u2217 t || 1T \u2211T t=1 \u03c1 \u2217 t ) is actually the so-called generalized Jensen-Shannon divergence, which is always used to measure the similarity among different distributions {\u03c1\u2217t }t\u2208[T ] in information theory (Lin, 1991, Section V).\nRemark B.4 (More Discussions between our Task-Averaged Bound for OWO Meta Learning and the Task-Averaged Regret Bound obtained via Dynamic Regret Analysis). One of the anonymous Reviewers suggest we make a comparison between our task-averaged regret in Theorem 3 and the task-averaged regret obtained via dynamic regret analysis: (1) First, we need to derive a taskaveraged regret bound for OWO meta learning through the lens of dynamic regret analysis. Denote \u03c6t(\u03b8) = \u2211m i=1 `ti(\u03b8), then the regret for OWO meta learning in our Eq.(1) can be rewritten roughly\nas 1T \u2211T t=1 E\u03b8\u223c\u03c1\u0304t\u03c6t(\u03b8)\u2212\u03c6t(\u03b8\u2217t ) (actually, we think we cannot rigorously rewrite the task-averaged\nregret as the equivalent form of dynamic regret, because we cannot write \u2211m i=1 E\u03b8\u223c\u03c1ti`ti(\u03b8) as the\nexpectation of \u03c6t(\u03b8) over a common distribution \u03c1 \u2208 P(\u0398)), where \u03c1\u0304t = 1m \u2211m i=1 \u03c1ti. Assume that\u2211T\u22121\nt=1 \u2016\u03c6t \u2212 \u03c6t+1\u2016 \u2264 VT , then according to the latest dynamic regret bound in (Gao et al., 2018, Thm 1) (to the best of our knowledge this is the latest dynamic regret bound for non-convex online learning), the task-averaged regret is bounded by 1T \u2211T t=1 E\u03b8\u223c\u03c1\u0304t\u03c6t(\u03b8)\u2212 \u03c6t(\u03b8\u2217t ) \u2264 O( \u221a T+VTT T ) = O( \u221a\n1+VT\u221a T ). (2) Next, we compare our regretO(( log TT +V ) \u221a m) and the regretO( \u221a 1+VT\u221a T\n) obtained via dynamic regret analysis, from 3 aspects: (i) Our regret analysis does not adopt the bounded total variation assumption (i.e. VT is a bounded constant), when compared with dynamic regret analysis. (ii) Our regret bound O(( log TT + V ) \u221a m) is more informative, revealing the importance of task similarity V to the generalization of OWO meta leaning algorithm. (iii) Our task-averaged regret bound O(( log TT + V ) \u221a m) has a faster convergence rate w.r.t. T when compared with O( \u221a 1+VT\u221a T ).\nRemark B.5 (More Discussions on the Advantages of our Theorem 2 when compared with our Theorem 3) One of the anonymous Reviewers suggest we discuss more the value/advantages of the first improved regret bound when compared with the second improved regret bound. The detailed explanations between their differences lie in the following 4 aspects: (1) The first improved regret bound in our Theorem 2 is not a special case of our Theorem 3. The main reason is that Theorem 2 uses V 2 = min\u03c1:\u03987\u2192R\u22650, \u222b \u0398 \u03c1(\u03b8)d\u03b8=1 \u2212 1T \u2211T t=1 log \u222b B(\u03b8\u2217t ,m\u2212\u03b2) \u03c1(\u03b8)d\u03b8 as the similarity notion between different tasks, but Theorem 3 uses the task similarity notion V 2 = min\u03c1\u2208P(\u0398) 1 T \u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1). Besides, the task similarity in Theorem 2 is defined according to the specific property (i.e. -radius) of the piecewise-Lipschitz function, and hence is particularly applicable to the piecewise-Lipschitz setting. (2) The action-space-discretization technique (described in Section 4.1 to obtain the first regret bound in Theorem 2) is of independent interest. The defined task similarity in Theorem 2 also requires a novel action-space-discretization method to translate the minimization problem min\u03c1\u2208\u0398 ft(\u03c1) over the set of distributions into a tractable online convex optimization problem. (3) At present, the first regret bound has higher application value than the second regret bound. The Algorithm 1 (corresponding to the first regret bound in our Theorem 2) can be applied in the continuum domain, but Algorithm 2 (corresponding to the second regret bound in our Theorem 3) at the current stage is still not easy to be applied in the continuum domain (see more explanations in our Remark H.1). (4) The first improved regret is obtained under the same assumptions (i.e. piecewise Lipschitzness and bounded loss functions) as that in (Balcan et al., 2021, Thm 3.3), via a more technical analysis. We list the improved regret in our paper to make a fair comparison and show rigorous improvements over existing result (Balcan et al., 2021).\nRemark B.6 (The Potential Improvement Space of our Theoretical Results). According to Theorem 1, we decompose the task-averaged regret bound problem into two subproblems: (1) minimizing {ft(\u03c1) = V (\u03c1, \u03c1\u2217t )2}t\u2208[T ] to learn initialization \u03c1, and (2) minimizing {ht(v) = v+ft(\u03c1t1)/v}t\u2208[T ] to learn step size v. Combining the above two results leads to task-averaged regret bounds in our Theorems 2-3, which are actually optimal w.r.t. m (i.e. of order O( \u221a m)). Therefore, what we can improve is the convergence rate w.r.t. T , and our explanations are three-fold: (1) For learning the initialization \u03c1, our Proposition 4 achieves a logarithmic regret O(log T ). According to the related work of OCO in Appendix A, the optimal regret for strongly-convex online optimization is O(log T ). Therefore, we believe that our Proposition 4 achieves the optimal regret for learning the initialization. (2) For learning the step size v, our Propositions 3 and 5 actually achieve the (polynomial) logarithmic regret O(log T ). Therefore, we also obtain optimal or near optimal regret for learning the step size v. (3) Consider (1) and (2), if we still adopt the regret upper bound decomposition framework, we should refine the proof in Theorem 1. For example, there seems to be some improvement space in the 4-th inequality in the proof of our Theorem 1, since other inequalities in this proof hold due to the definition of regret bound; If not, we should use other task-averaged regret analysis to see whether we could achieve better convergence rate w.r.t. T or a smaller multiplier constant."
        },
        {
            "heading": "C PROOFS OF THE TASK-AVERAGED REGRET BOUND FRAMEWORK FOR NON-CONVEX OWO META LEARNING",
            "text": "Proof of Theorem 1.\n1\nT T\u2211 t=1 m\u2211 i=1 \u3008\u03c1ti \u2212 \u03c1\u2217t , `ti\u3009\n\u2264 1 T T\u2211 t=1 [ mb\u03bbt + V (\u03c1t1, \u03c1 \u2217 t ) 2 \u03bbt + g(m) ] = 1 T T\u2211 t=1 Ut(\u03c1t1, \u03bbt)\n= 1\nT T\u2211 t=1 [\u221a mb\u03bbt + V (\u03c1t1, \u03c1 \u2217 t ) 2 \u221a mb\u03bbt ]\u221a mb+ g(m)\n= 1\nT T\u2211 t=1 [ vt + ft(\u03c1t1) vt ]\u221a mb+ g(m) (vt = \u221a mb\u03bbt)\n\u2264min v>0 \u221a mb [ 1 T T\u2211 t=1 (v + ft(\u03c1t1) v ) + HT (v) T ] + g(m)\n= min v>0\n\u221a mb [ v + \u2211T t=1 ft(\u03c1t1)\nTv + HT (v) T\n] + g(m)\n\u2264 min v>0,\u03c1\u2208P(\u0398)\n\u221a mb [ v + \u2211T t=1 ft(\u03c1) + FT (\u03c1)\nTv + HT (v) T\n] + g(m)\n\u2264min v>0\n\u221a mb [ v + \u2211T t=1 ft(\u03c1 \u2217) + FT (\u03c1 \u2217)\nTv + HT (v) T\n] + g(m)\n= min v>0\n\u221a mb [ v + V 2\nv + FT (\u03c1\n\u2217)\nTv + HT (v) T\n] + g(m)\n\u2264 (HT (V )\nT + min{FT (\u03c1\n\u2217)\nV T , 2\n\u221a FT (\u03c1\u2217)\nT }+ 2V\n)\u221a mb+ g(m),\nwhere the last inequality holds by taking v = V or v = \u221a V 2 + FT (\u03c1\u2217)/T .\nLemma C.1 (Shalev-Shwartz & Kakade, 2008, Corollary 1) Let `1, ..., `T be a sequence of functions such that for all t \u2208 [T ], `t is \u03c3t-strongly convex. Assume that the FTL algorithm runs on this sequence and for each t \u2208 [T ], let gt be in \u2202`t(\u03b8t). Then\nT\u2211 t=1 `t(\u03b8t)\u2212min \u03b8\u2208\u0398 T\u2211 t=1 `t(\u03b8) \u2264 1 2 T\u2211 t=1 \u2016gt\u20162 \u03c31:t .\nFurthermore, let L = maxt \u2016gt\u2016 and assume that for all t \u2208 [T ], \u03c3t \u2265 \u03c3. Then the regret is bounded by L 2\n2\u03c3 (log T + 1).\nProof of Proposition 1. Notice that for all t \u2208 [T ], dht(v)dv = 1 \u2212 ft(\u03c1t1) v2 , d2ht(v) dv2 = 2ft(\u03c1t1) v3 \u2265\n2ft(\u03c1t1) D3 . Therefore ht(v) is a 2ft(\u03c1t1) D3 -strongly convex function. Besides, since vt = \u221a\u2211t\u22121 s=1 fs(\u03c1s1) t\u22121 ,\nthen dht(v)dv \u2223\u2223\u2223 v=vt = 1\u2212 \u2211t\u22121 s=1 ft(\u03c1t1)\u2211t\u22121 s=1 fs(\u03c1s1) , and applying Lemma C.1 obtains the first result. For the second\nresult, using ft(\u03c1t1) \u2208 [B2, D2], we bound the above regret as follow:\nD3\n4 T\u2211 t=1 \u2223\u22231\u2212\u2211t\u22121s=1 ft(\u03c1t1)/\u2211t\u22121s=1 fs(\u03c1s1)\u2223\u22232\u2211t s=1 fs(\u03c1s1)\n\u2264 D 3\n4B2 T\u2211 t=1\n[ max{1,\n\u2211t\u22121 s=1 ft(\u03c1t1)\u2211t\u22121 s=1 fs(\u03c1s1)\n} ]2\nt\n\u2264 D 3\n4B2 T\u2211 t=1\n[ max{1, (t\u22121)D 2 (t\u22121)B2 } ]2\nt\n= D7\n4B6 T\u2211 t=1 1 t \u2264 D 7 4B6 ( log T + 1 ) ."
        },
        {
            "heading": "D PROOFS OF THE REGRET BOUND FOR OWO META LEARNING WITH BOUNDED PIECEWISE-LIPSCHITZ FUNCTIONS",
            "text": "Lemma D.1 Let `1, ..., `m : \u0398 7\u2192 [0,M ] be any sequence of piecewise L-Lipschitz functions that are \u03b2-dispersed. Let \u03b8\u2217 \u2208 arg min\u03b8\u2208\u0398 \u2211m i=1 `i(\u03b8), and V (\u03c1, \u03b8 \u2217)2 = \u2212 log \u222b B(\u03b8\u2217,m\u2212\u03b2) \u03c1(\u03b8)d\u03b8\u222b\n\u0398 \u03c1(\u03b8)d\u03b8 , where \u03c1 is the initial distribution over \u0398 in EWA Algorithm. Then EWA has the following regret bound:\nm\u2211 i=1 E\u03b8\u223c\u03c1i`i(\u03b8)\u2212 n\u2211 i=1 `i(\u03b8 \u2217) \u2264 \u03bbM2m+ V (\u03c1, \u03b8 \u2217)2 \u03bb + O\u0303((L+ 1)m1\u2212\u03b2).\nProof. The demonstration strategy is to upper bound Pm+1 and lower bound Pm+1 respectively. For any i \u2208 [m], define the utility function ui(\u03b8) = M\u2212`i(\u03b8). It is not difficult to see that running EWA algorithm on utility functions {ui}i\u2208[m] obtains the same sequence of actions {\u03b8i}i\u2208[m] as running EWA on {`i}i\u2208[m]. For upper-bounding Pm+1, applying Jensen\u2019s inequality and basic inequality 1 + x \u2264 ex, we have:\nPi+1 Pi =\n\u222b \u0398 e\u03bbui(\u03b8)\u03c1i(\u03b8)d\u03b8\nPi\n= \u222b \u0398 e\u03bbui(\u03b8)pi(\u03b8)d\u03b8\n= \u222b \u0398 e\u03bbM ui(\u03b8) M +0(1\u2212 ui(\u03b8) M )pi(\u03b8)d\u03b8\n\u2264 \u222b\n\u0398\n{ui(\u03b8) M e\u03bbM + [1\u2212 ui(\u03b8) M ]e0 } pi(\u03b8)d\u03b8\n= \u222b \u0398 { 1 + ui(\u03b8) M (e\u03bbM \u2212 1) } pi(\u03b8)d\u03b8 = 1 + e\u03bbM \u2212 1 M \u222b \u0398 ui(\u03b8)pi(\u03b8)d\u03b8\n\u2264 exp {e\u03bbM \u2212 1\nM\n\u222b \u0398 ui(\u03b8)pi(\u03b8)d\u03b8 } .\nThen Pm+1P1 = \u220fm i=1 Pi+1 Pi \u2264 exp { e\u03bbM\u22121 M \u2211m i=1 E\u03b8\u223cpiui(\u03b8) } and we can upper-bound Pm+1 .\nFor lower-bounding Pm+1, let us first upper bound \u2211m i=1 ui(\u03b8\n\u2217) \u2212 ui(\u03b8), \u2200\u03b8 \u2208 B(\u03b8\u2217, ), where = m\u2212\u03b2 . Notice that the utility functions {ui}i\u2208[m] are also dispersed L-Lipschitz. Suppose there exist at most k discontinuities w.r.t. to {ui}i\u2208[m] in the ball B(\u03b8\u2217, ), then for any \u03b8 \u2208 B(\u03b8\u2217, ):\nm\u2211 i=1 ui(\u03b8 \u2217)\u2212 ui(\u03b8) \u2264 kmax i |ui(\u03b8\u2217)\u2212 ui(\u03b8)|+ (m\u2212 k)L\u2016\u03b8 \u2212 \u03b8\u2217\u2016 \u2264 kM +mL . (4)\nWith Eq. (4), we can lower-bound Pm+1:\nPm+1 = \u222b \u0398 e\u03bb \u2211m i=1 ui(\u03b8)\u03c11(\u03b8)d\u03b8\n\u2265 \u222b B(\u03b8\u2217, ) e\u03bb \u2211m i=1 ui(\u03b8)\u03c11(\u03b8)d\u03b8\n\u2265 \u222b B(\u03b8\u2217, ) e\u03bb [\u2211m i=1 ui(\u03b8 \u2217)\u2212kM\u2212mL ] \u03c11(\u03b8)d\u03b8\n= e\u03bb [\u2211m i=1 ui(\u03b8 \u2217)\u2212kM\u2212mL ] \u222b B(\u03b8\u2217, ) \u03c11(\u03b8)d\u03b8.\nCombining the upper and lower bounds for Pm+1, we have: exp { \u03bb [ m\u2211 i=1 ui(\u03b8 \u2217)\u2212kM\u2212mL ]} \u222b B(\u03b8\u2217, ) \u03c11(\u03b8)d\u03b8 \u2264 Pm+1 \u2264 exp {e\u03bbM \u2212 1 M m\u2211 i=1 E\u03b8\u223cpiui(\u03b8) }\u222b \u0398 \u03c11(\u03b8)d\u03b8\nRearranging the terms, we have\nm\u2211 i=1 ui(\u03b8 \u2217)\u2212 m\u2211 i=1 E\u03b8\u223cpiui(\u03b8) \u2264 kM +mL + e\u03bbM \u2212 1\u2212 \u03bbM \u03bbM m\u2211 i=1 E\u03b8\u223cpiui(\u03b8) + V (\u03c11, \u03b8 \u2217)2 \u03bb\n\u2264 kM +mL + \u03bbM2m+ V (\u03c11, \u03b8 \u2217)2\n\u03bb ,\nwhere in the last inequality we use the fact that \u2200x \u2208 [0, 1], ex \u2264 1+x+x2 and ui(\u03b8) \u2264M . Taking expectation of both sides w.r.t. the randomness of {ui}i\u2208[m] and noticing Ek \u2264 O\u0303( m) according to Definition 1 gives the result.\nCorollary D.1 If the non-convex loss functions {`i}i\u2208[m] are uniformly L-Lipschitz over \u0398, then the number k of discontinuities in Eq. (4) becomes 0, and the expected regret Rm \u2264 Lm1\u2212\u03b2 + \u03bbmM2 + V (\u03c11,\u03b8 \u2217)2\n\u03bb .\nProof of Proposition 2. Note that ft(\u03c1) = \u2212 log \u3008\u03c1\u2217t ,\u03c1\u3009 is a 1\u03b3vol(\u0398t) -Lipschitz function w.r.t. \u2016 \u00b7 \u20161 over the constraint simplex domain {\u03c1 \u2208 R|DT | : \u2016\u03c1\u20161 = 1,\u03c1 \u2265 \u03b3\u03bd\u0302}, since\nmax \u2016\u03c1\u20161=1,\u03c1\u2265\u03b3\u03bd\u0302 \u2016\u2207ft(\u03c1)\u2016\u221e = max \u2016\u03c1\u20161=1,\u03c1\u2265\u03b3\u03bd\u0302\n\u2016 \u03c1 \u2217 t\n\u3008\u03c1\u2217t ,\u03c1\u3009 \u2016\u221e\n= max A,\u03c1\u2265\u03b3\u03bd\u0302\n\u03c1\u2217t [A]\n\u3008\u03c1\u2217t ,\u03c1\u3009 \u2264 max \u03c1\u2265\u03b3\u03bd\u0302\n1 \u3008\u03c1\u2217t ,\u03c1\u3009 \u2264 1 \u3008\u03c1\u2217t , \u03b3\u03bd\u0302\u3009\n= 1 \u03b3 \u2211 A\u2208DT ,A\u2282\u0398t \u03bd\u0302[A] = vol(\u0398) \u03b3vol(\u0398t) .\nThen applying the regret bound in (Shalev-Shwartz, 2012, Thm 2.11) for FTRL algorithm with convex and Lipschitz functions, and noticing that KL(\u03c1\u2217||\u03bd) is 1-strongly convex w.r.t. its first argument, we can obtain\nT\u2211 t=1 ft(\u03c1t)\u2212 ft(\u03c1\u2217) \u2264 KL(\u03c1\u2217||\u03bd\u0302) \u03b7 + \u03b7 T\u2211 t=1 vol(\u0398)2 \u03b32vol(\u0398t)2\n\u2264G 2\n\u03b7 + \u03b7TL2/\u03b32.\nSetting \u03b7 = G\u03b3/L \u221a T gives the result.\nProposition D.1 (Proposition 3 in the main paper) Assume that the FTL algorithm runs on the sequence of functions {ht(v) = v + ft(\u03c1t1)v }t\u2208[T ] with ft(\u03c1t1) = \u2212 log \u3008\u03c1 \u2217 t ,\u03c1t1\u3009 over\nthe domain [0, D], where D2 = maxt\u2208[T ] log vol(\u0398) \u03b3vol(\u0398t) \u2265 maxt\u2208[T ] ft(\u03c1t1). Denote B2 = log vol(\u0398)\u03b3vol(\u0398t)+(1\u2212\u03b3)vol(\u0398) . Then we have\nT\u2211 t=1 ht(vt)\u2212 min v\u2208[0,D] T\u2211 t=1 ht(v) \u2264 D7 ( log T + 1 ) 4B6 .\nFurthermore, if we set \u03b3 = md\u03b2/T\u03b1 with \u03b1 \u2208 (0, 1/2), we have the regret bound \u2211T t=1 ht(vt) \u2212\nminv\u2208[0,D] \u2211T t=1 ht(v) \u2264 (\u03b1 log T+log (vol(\u0398)/Vd)) 7 2 (log T+1) 4B6 .\nProof. According to the proof of Proposition 2, the upper bound D2 of ft(\u03c1t1) is: ft(\u03c1t1) = log 1\u3008\u03c1\u2217t ,\u03c1t1\u3009 \u2264 log vol(\u0398)\u03b3vol(\u0398t) = D 2. Then we need to derive the lower bound B2 of ft(\u03c1t1). Let us first consider the maximum of \u3008\u03c1\u2217t ,\u03c1\u3009, under the conditions of \u2016\u03c1\u20161 = 1 and \u03c1 \u2265 \u03b3\u03bd\u0302. Using the Ho\u0308lder inequality, we can simply obtain that \u3008\u03c1\u2217t ,\u03c1\u3009 \u2264 \u2016\u03c1\u2217t \u2016\u221e\u2016\u03c1\u20161 = 1. But this means that log 1\u3008\u03c1\u2217t ,\u03c1\u3009 \u2265 log 1 = 0, so the lower bound of log 1 \u3008\u03c1\u2217t ,\u03c1\u3009\nis zero, and thus we cannot use Lemma C.1. Actually, considering the additional condition \u03c1 \u2265 \u03b3\u03bd\u0302, we can use Lagrange multiplier method to calculate the maximum, or just simply achieve the maximum by observing the fact that: apart from the measure \u03b3 \u2211 A\u2208DT \u03bd\u0302[A] = \u03b3, the rest measure 1 \u2212 \u03b3 of \u03c1 should be assigned uniformly to the indices A \u2208 DT where \u03c1\u2217t [A] = 1 (i.e. A \u2282 \u0398t) to maximize \u3008\u03c1\u2217t ,\u03c1\u3009 . Assuming the cardinality of the set {A\n\u2223\u2223A \u2208 DT , A \u2282 \u0398t} is n, then max\u03c1\u2265\u03b3\u03bd\u0302\u3008\u03c1\u2217t ,\u03c1\u3009 = 1\u2212\u03b3 n n + \u2211 A\u2208DT ,A\u2282\u0398t \u03b3\u03bd\u0302[A] = 1 \u2212 \u03b3 + \u03b3vol(\u0398t) vol(\u0398) , which is strictly less than 1. Thus we have log 1\u3008\u03c1\u2217t ,\u03c1\u3009 \u2265 log 1\n1\u2212\u03b3+ \u03b3vol(\u0398t) vol(\u0398)\n= log vol(\u0398)\u03b3vol(\u0398t)+(1\u2212\u03b3)vol(\u0398) . Applying this lower\nbound and the upper bound D2 of ft(\u03c1t1) into Lemma C.1, we obtain the first result. For the second result, noticing D2 = log vol(\u0398)\u03b3vol(\u0398t) = log md\u03b2vol(\u0398) \u03b3Vd = log T \u03b1vol(\u0398) Vd completes the whole proof.\nRemark D.1 Assume that T is large enough. Then if \u03b1 \u2208 [ lnm d\u03b2 lnT , ln ( emd\u03b2 e\u22121 \u2212 Vd (e\u22121)vol(\u0398) ) lnT ] \u2282 (0, 1 2 ), we have m d\u03b2\nT\u03b1 \u2264 1 and B 2 = log vol(\u0398)\u03b3vol(\u0398t)+(1\u2212\u03b3)vol(\u0398) \u2265 log e = 1. The regret bound for learning\nthe step size of EWA algorithm in Proposition 3 is of order O((log T ) 9 2 )\nProof of Theorem 2. According to Lemma D.1, we know that the constant b in Theorem 1 is equal to b = M2. Then setting the distribution \u03c1\u2217t in Theorem 1 as the Dirac measure on the optimal action \u03b8\u2217t , and applying the regret bound in Theorem 1, we have\n1\nT T\u2211 t=1 m\u2211 i=1 E [ \u3008\u03c1ti, `ti\u3009\u2212 `ti(\u03b8\u2217t ) ] \u2264 (HT (V ) T +min{FT (\u03c1 \u2217) V T , 2 \u221a FT (\u03c1\u2217) T }+2V )\u221a mM +g(m),\nwhere the expectation in the left-hand side is taken over the randomness of loss functions {`ti}t,i\u22651. It remains to provide the regret bounds for learning the initialization and for learning the step size. For learning the initialization, note that \u03b3 = m \u03b2d\nT\u03b1 , L 2 = 1T \u2211T t=1 vol(\u0398)2 vol(\u0398t)2 = m 2\u03b2dvol(\u0398)2 V 2d , then the\nregret of FTRL in Proposition 2 for learning the initialization satisfies:\nFT (\u03c1 \u2217) = KL(\u03c1\u2217||\u03bd\u0302) \u03b7 + \u03b7TL2 \u03b32\n= KL(\u03c1\u2217||\u03bd\u0302)\n\u03b7 + \u03b7T 1+2\u03b1vol(\u0398)2/V 2d\n\u2264 T 12 +\u03b1vol(\u0398)(KL(\u03c1\u2217||\u03bd\u0302) + 1)/Vd,\nwhere the last inequality holds by setting \u03b7 = VdT\u2212 1+2\u03b1 2 vol(\u0398)\u22121. For learning the step size, it suffices to directly apply Proposition 3. Thus we can upper bound the task-averaged regret:\nR\u0304T,m \u2264g(m) + { (\u03b1 log T + log (vol(\u0398)/Vd)) 72 (log T + 1)\n4TB6\n+ min {vol(\u0398)(KL(\u03c1\u2217||\u03bd\u0302) + 1)\nVdV T 1 2\u2212\u03b1\n, 2\n\u221a vol(\u0398)(KL(\u03c1\u2217||\u03bd\u0302) + 1)\nVdT 1 2\u2212\u03b1\n} + 2V }\u221a mM."
        },
        {
            "heading": "E PROOFS OF THE REGRET BOUND FOR OWO META LEARNING WITH BOUNDED NON-LIPSCHITZ FUNCTIONS",
            "text": "Lemma E.1 (Minimizer of the Expected Bregman Divergence) (Frigyik et al., 2008, Thm II.1) Let C be a set of functions that lie on a finite-dimensional manifold \u2126, and have associated differential element d\u2126. Suppose there is a probability distribution PF defined over the set C. Suppose the function g\u2217 minimizes the expected Bregman divergence d\u03c6 between the random function F and any function g \u2208 A such that g\u2217 = arg infg\u2208A EPF [d\u03c6(F, g)]. Then, if g\u2217 exists, it is given by g\u2217 = \u222b \u2126 fP (f)d\u2126 = EPF [F ].\nProof of Proposition 4. In the proof, we write \u03c1t1 in the main text as \u03c1t for brevity. According to Lemma E.1, we have\n\u03c1t = arg min \u03c1\u2208P(\u0398) t\u22121\u2211 s=1 KL(\u03c1\u2217t ||\u03c1) = arg min \u03c1\u2208P(\u0398) 1 t\u2212 1 t\u22121\u2211 s=1 KL(\u03c1\u2217s||\u03c1) = 1 t\u2212 1 t\u22121\u2211 s=1 \u03c1\u2217s.\nThe existence of \u03c1t can be guaranteed by the convexity of KL-divergence w.r.t. its second argument. We also have \u03c1\u2217 = arg min\u03c1\u2208P(\u0398) \u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1) = 1T \u2211T t=1 \u03c1 \u2217 t . Then we have\nT\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t)\u2212 T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1\u2217)\n\u2264 T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t)\u2212 T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t+1)\n= T\u2211 t=1 \u222b log d\u03c1\u2217t d\u03c1t \u03c1\u2217t (d\u03b8)\u2212 \u222b log d\u03c1\u2217t d\u03c1t+1 \u03c1\u2217t (d\u03b8)\n= T\u2211 t=1 \u222b log [d\u03c1\u2217t d\u03c1t / d\u03c1\u2217t d\u03c1t+1 ] \u03c1\u2217t (d\u03b8)\n= T\u2211 t=1 \u222b log [d\u03c1t+1 d\u03c1t ] \u03c1\u2217t (d\u03b8)\n= T\u2211 t=1 \u222b log [1 t d ( (t\u2212 1)\u03c1t + \u03c1\u2217t ) d\u03c1t ] \u03c1\u2217t (d\u03b8)\n= T\u2211 t=1 \u222b log [ t\u2212 1 t + 1 t d\u03c1\u2217t d\u03c1t ] \u03c1\u2217t (d\u03b8)\n\u2264 T\u2211 t=1 log \u222b [ t\u2212 1 t + 1 t d\u03c1\u2217t d\u03c1t ] \u03c1\u2217t (d\u03b8)\n= T\u2211 t=1 log [ t\u2212 1 t + 1 t \u222b d\u03c1\u2217t d\u03c1t \u03c1\u2217t (d\u03b8) ]\n= T\u2211 t=1 log [ 1 + 1 t \u222b [ ( d\u03c1\u2217t d\u03c1t )2 \u2212 1]\u03c1t(d\u03b8)] =\nT\u2211 t=1 log [ 1 + 1 t \u03c72(\u03c1\u2217t ||\u03c1t) ] \u2264\nT\u2211 t=1 \u03c72(\u03c1\u2217t ||\u03c1t) t ,\nwhere the first inequality holds due to the the follow-the-leader lemma (i.e. (Shalev-Shwartz, 2012, Lemma 2.1)), the second inequality holds due to Jensen\u2019s inequality, and the last step due to the fact that log (1 + x) \u2264 x.\nProof of Proposition 5. Applying Lemma C.1 finishes the whole proof. It remains to verify that the lower bound of ft(\u03c1t1) is strictly greater that 0. Actually for each task t \u2208 [T ], we use EWA algorithm to learn posterior \u03c1\u2217t , then we have d\u03c1\u2217t d\u03c1t1 (\u03b8) = exp{\u2212\u03bbt \u2211m i=1 `ti(\u03b8)}\u222b exp{\u2212\u03bbt \u2211m i=1 `ti(\u03b8)}\u03c1t1(\u03b8)\n6\u2261 constant. If not, then exp{\u2212\u03bbt \u2211m i=1 `ti(\u03b8)} \u2261 constant and hence \u2211m i=1 `ti(\u03b8) = constant for all \u03b8 \u2208 \u0398, which is trivial. Therefore \u03c1\u2217t 6= \u03c1t1 and KL(\u03c1\u2217t ||\u03c1t1) > 0 for all t \u2208 [T ]. Proof of Theorem 3. According to Lemma, set the positive constant b in Theorem 1 as M2, the additional term g(m) = 0. Then integrating the regret bounds from Proposition 4 and Proposition 5 into the regret bound in Theorem 1, we finish the proof."
        },
        {
            "heading": "F PROOFS OF THE TRANSFER RISK BOUND",
            "text": "Proposition F.1 Let a sequence of non-negative loss functions {`i : \u0398 7\u2192 R\u22650}i\u2208[m] drawn i.i.d. from some distribution \u00b5 be given to an online algorithm that generates a sequence of probability distributions {\u03c1i \u2208 P(\u0398)}i\u2208[m] (i.e. \u03c1i = \u03c1i({`j}i\u22121j=1)). Assume the regret upper bound for the online algorithm is Um. Then let \u03c1\u0304 = 1m \u2211m i=1 \u03c1i, for any \u03c1\n\u2217 \u2208 P(\u0398) that does not depend on the choice of the sequence of loss functions {`i}i\u2208[m], we have the following bound:\nE{`i}mi=1\u223c\u00b5mE`\u223c\u00b5E\u03b8\u223c\u03c1\u0304`(\u03b8) \u2264 Um m + E`\u223c\u00b5E\u03b8\u223c\u03c1\u2217`(\u03b8).\nProof. E{`i}mi=1E`\u223c\u00b5E\u03b8\u223c\u03c1\u0304`(\u03b8)\n=E{`i}mi=1E`\u223c\u00b5 \u222b `(\u03b8)\u03c1\u0304(d\u03b8)\n=E{`i}mi=1E`\u223c\u00b5 1\nm m\u2211 i=1 \u222b `(\u03b8)\u03c1i(d\u03b8)\n=E{`i}mi=1 1\nm m\u2211 i=1 \u222b E`\u2032i\u223c\u00b5` \u2032 i(\u03b8)\u03c1i(d\u03b8)\n=E{`i}mi=1 1\nm m\u2211 i=1 [ \u222b E`\u2032i\u223c\u00b5` \u2032 i(\u03b8)\u2212 `i(\u03b8)\u03c1i(d\u03b8) + \u222b `i(\u03b8)\u03c1i(d\u03b8) ] = 1\nm m\u2211 i=1 E{`i}mi=1 [ \u222b E`\u2032i\u223c\u00b5` \u2032 i(\u03b8)\u2212 `i(\u03b8)\u03c1i(d\u03b8) ] + E{`i}mi=1 1 m m\u2211 i=1 \u222b `i(\u03b8)\u03c1i(d\u03b8)\n= 1\nm m\u2211 i=1 E{`j}i\u22121j=1\u223c\u00b5i\u22121 [ \u222b E`\u2032i\u223c\u00b5` \u2032 i(\u03b8)\u2212 E`i\u223c\u00b5`i(\u03b8)\u03c1i(d\u03b8) ] + E{`i}mi=1 1 m m\u2211 i=1 \u222b `i(\u03b8)\u03c1i(d\u03b8)\n=E{`i}mi=1 1\nm m\u2211 i=1 \u222b `i(\u03b8)\u03c1i(d\u03b8)\n=E{`i}mi=1 1\nm m\u2211 i=1 [ E\u03b8\u223c\u03c1i`i(\u03b8)\u2212 E\u03b8\u223c\u03c1\u2217`i(\u03b8) ] + E{`i}mi=1 1 m m\u2211 i=1 E\u03b8\u223c\u03c1\u2217`i(\u03b8)\n\u2264Um m + E{`i}mi=1 1 m m\u2211 i=1 E\u03b8\u223c\u03c1\u2217`i(\u03b8)\n= Um m + E`\u223c\u00b5E\u03b8\u223c\u03c1\u2217`(\u03b8),\nwhere the third equality holds due to the independence between ` \u223c \u00b5 and \u03c1i, the Fubini-Tonelli Theorem for changing the order of integrals of non-negative function, as well as the fact that `\u2032i is the i.i.d. copy of `i; in the six-th equality, {`j}i\u22121j=1 \u223c \u00b5i\u22121 is the abbreviation for the notation `j \u223c \u00b5,\u2200j \u2208 [i \u2212 1]; and \u00b5i\u22121 = \u00b5 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u00b5 is the product measure of i \u2212 1 measures \u00b5. Both the six-th and the last equality hold due to the independence between \u03c1\u2217 and {`i}mi=1 (i.e. Fubini\u2019s Theorem for exchanging the order of integrals).\nProposition F.2\n1\nT T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1)\u2212 1 T T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t1) \u2264 1 T T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1\u2217t ) = V 2. (5)\nFurthermore, assume G2 \u2265 maxt\u2208[T ] \u03c72(\u03c1\u2217t ||\u03c1t1), we have\n1\nT T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1) \u2264 2V 2 + G2(log T + 1) T . (6)\nProof. (1) For the first inequality. Using the joint convexity of KL-divergence w.r.t. its pair argument and Jensen\u2019s inequality, we have\nKL( 1\nT T\u2211 t=1 \u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1) \u2264 1 T T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t1).\nPlug the above result into the left-hand-sight of inequality in proposition, we have\n1\nT T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1)\u2212 1 T T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t1)\n\u2264 1 T T\u2211 t=1 [ KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1)\u2212KL( 1 T T\u2211 t=1 \u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1) ]\n= 1\nT T\u2211 t=1 [ E\u03b8\u223c\u03c1\u2217t ln\nd\u03c1\u2217t d 1T \u2211T t=1 \u03c1t1 \u2212 E\u03b8\u223c 1T \u2211T t=1 \u03c1 \u2217 t\nln d 1T \u2211T t=1 \u03c1 \u2217 t\nd 1T \u2211T t=1 \u03c1t1 ] = 1\nT T\u2211 t=1 [ E\u03b8\u223c\u03c1\u2217t ln ( d\u03c1\u2217t d 1T \u2211T t=1 \u03c1 \u2217 t \u00b7 d 1T \u2211T t=1 \u03c1 \u2217 t d 1T \u2211T t=1 \u03c1t1 ) \u2212 E\u03b8\u223c 1T \u2211T t=1 \u03c1 \u2217 t ln d 1T \u2211T t=1 \u03c1 \u2217 t d 1T \u2211T t=1 \u03c1t1 ] = 1\nT T\u2211 t=1 [ E\u03b8\u223c\u03c1\u2217t ln ( d\u03c1\u2217t d 1T \u2211T t=1 \u03c1 \u2217 t ) + E\u03b8\u223c\u03c1\u2217t ln ( d 1T \u2211Tt=1 \u03c1\u2217t d 1T \u2211T t=1 \u03c1t1 ) \u2212 E\u03b8\u223c 1T \u2211T t=1 \u03c1 \u2217 t ln d 1T \u2211T t=1 \u03c1 \u2217 t d 1T \u2211T t=1 \u03c1t1 ] = 1\nT T\u2211 t=1 E\u03b8\u223c\u03c1\u2217t ln d\u03c1\u2217t d 1T \u2211T t=1 \u03c1 \u2217 t\n= 1\nT T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1\u2217t ).\n(2) For the second inequality, notice the following decomposition\n1\nT T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1)\u2212 1 T T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1\u2217t )\n= ( 1 T T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1t1)\u2212 1 T T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t1) ) + ( 1 T T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1t1)\u2212 1 T T\u2211 t=1 KL(\u03c1\u2217t || 1 T T\u2211 t=1 \u03c1\u2217t ) ) .\nTherefore, we can upper bound the first part of the right-hand-side of above equality by applying the first inequality in this proposition (i.e. Eq. (5)), and upper bound the second part of the right-handside of above equality by applying the regret bound in Proposition 4. Combining both upper bounds yields the second inequality in this proposition (i.e. Eq. (6)). Proof of Theorem 4. Denote \u03c1T+1,1 = 1T \u2211T t=1 \u03c1t1, \u03bbT+1 = \u221a\u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1t1)\nTmM2 for brevity, then applying Proposition F.1, we have\nE\u00b5\u223c\u03c4E{`i}mi=1\u223c\u00b5mE`\u223c\u00b5E\u03b8\u223c\u03c1\u0304`(\u03b8) \u2264 E\u00b5\u223c\u03c4 ( E`\u223c\u00b5E\u03b8\u223c\u03c1\u2217`(\u03b8) + Um(\u03c1T+1,1, \u03bbT+1)\nm\n) .\nWe next give an upper bound on E\u00b5\u223c\u03c4 Um(\u03c1T+1,1,\u03bbT+1)m . Actually,\nE\u00b5\u223c\u03c4 Um(\u03c1T+1,1, \u03bbT+1)\nm\n=E\u00b5\u223c\u03c4 \u03bbT+1M 2m+ KL(\u03c1\u2217||\u03c1T+1,1)/\u03bbT+1 m\n=M\n\u221a\u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1t1)\nTm + E\u00b5\u223c\u03c4\nM \u221a T KL(\u03c1\u2217||\u03c1T+1,1) \u221a m \u221a\u2211T\nt=1 KL(\u03c1 \u2217 t ||\u03c1t1) \u2264M \u221a TV 2 +G2(log T + 1)\nTm + E\u00b5\u223c\u03c4 M KL(\u03c1\u2217||\u03c1T+1,1)\u221a mB\n=M\n\u221a V 2\nm + G2(log T + 1) Tm + MH\u221a mB E\u00b5\u223c\u03c4 KL(\u03c1\u2217||\u03c1T+1,1) H\n\u2264M \u221a V 2\nm + G2(log T + 1) Tm + MH\u221a mB ( 1 T T\u2211 t=1 KL(\u03c1\u2217t ||\u03c1T+1,1) H + \u221a log 1/\u03b4 2T ) \u2264M \u221a V 2\nm + G2(log T + 1) Tm + MH\u221a mB (2V 2 H + G2(log T + 1) TH + \u221a log 1/\u03b4 2T ) \u2264M ( V\u221a m + \u221a G2(log T + 1) Tm + 2V 2\u221a mB + G2(log T + 1) T \u221a mB + H B \u221a log 1/\u03b4 2Tm ) ,\nwhere the first inequality holds due to the regret bound in Proposition 4 , the second inequality due to Hoeffding\u2019s inequality holds with probability 1 \u2212 \u03b4, the third inequality holds due to Eq. (6) in Proposition F.2. Combining these results completes the whole proof."
        },
        {
            "heading": "G PROOFS OF THE PAC-BAYESIAN GENERALIZATION BOUNDS FOR STATISTICAL MULTI-TASK LEARNING",
            "text": "Lemma G.1 (Lugosi & Neu, 2023, Lemma 26) Let {Xt}nt=1 be a sequence of non-negative random variables and for t \u2265 0, let Ft denote the \u03c3-algebra generated by X1, . . . , Xt. Assume that Xt has finite conditional mean \u00b5t = E[Xt|Ft\u22121] and second moment \u03c32t = E[X2t |Ft\u22121]. Then, for any \u03bb > 0, the following bound holds with probability at least 1\u2212 \u03b4:\nn\u2211 t=1 ( \u00b5t \u2212Xt ) \u2264 \u03bb 2 n\u2211 t=1 \u03c32t + log 1\u03b4 \u03bb \u2264 \u221a\u221a\u221a\u221a2( n\u2211 t=1 \u03c32t ) log 1 \u03b4 .\nProof of Proposition6. For any t \u2208 [T ], define the regret of online algorithm \u03a0tm w.r.t. the probability distribution \u03c1\u2217t as R\u03a0tm(\u03c1 \u2217 t ) = \u2211m i=1\u3008\u03c1ti\u2212\u03c1\u2217t , cti\u3009. According to (Lugosi & Neu, 2023, Thm 1), for any task t \u2208 [T ], we have gen(At, St) = R\u03a0tm (At(St)) m \u2212M\u03a0tm . Then we have\n1\nT T\u2211 t=1 gen(At, St) = 1 T T\u2211 t=1 (R\u03a0tm(At(St)) m \u2212M\u03a0tm ) = R\u0304T,m m \u2212 1 T T\u2211 t=1 M\u03a0tm .\nProof of Theorem 5. Notice that E[\u3008\u03c1ti, cti\u30092|Ft,i\u22121] \u2264 M2, then applying Lemma G.1 to bound the sum of normalized martingale differences \u2212 1Tm \u2211T t=1 \u2211m i=1\u3008\u03c1ti, cti\u3009 in Proposition6, we have\nR\u0304T,m m \u2212 1 T T\u2211 t=1 M\u03a0tm \u2264 U\u0304T,m m \u2212 1 Tm T\u2211 t=1 m\u2211 i=1 \u3008\u03c1ti, cti\u3009 \u2264 U\u0304T,m m +\n\u221a 2M2 log 1\u03b4\nTm ,\nwhere the last inequality holds with probability at least 1\u2212 \u03b4.\nRemark G.1 (Two Technical Novelties of deriving our Generalization Bounds for Statistical Meta Learning). The technical novelties of our generalization bounds for statistical meta learning lie in the following two aspects: (1) For the transfer risk bound in Theorem 4: the novelties of\nour online-to-batch technique lie in 2 aspects: (i) The first novelty lies in bounding KL(\u03c1\u2217||\u03c1T+1,1) with Proposition 4 and Proposition F.2, both of which require the technical analysis of the properties of the Radon-Nikodym derivative. To the best of our knowledge, such analysis does not exist in the previous online-to-batch literature. (ii) We further use concentration inequality to bound E\u00b5\u223c\u03c4 KL(\u03c1 \u2217||\u03c1T+1,1) H , leading to the transfer risk bound in the non-convex setting. Such transfer risk bound, as shown in our Remark 3, has almost the same convergence rate when compared with the latest transfer risk bound in the convex setting (i.e. (Balcan et al., 2021, Thm E.1), which is obtained by Jensen inequality of convex loss), demonstrating the novelty of the online-to-batch analysis developed in our Theorem 4. (2) For the PAC-Bayes generalization bound in Theorem 5: first we need to admit that our Proposition 6 is a direct corollary of recent online-to-PAC result in (Lugosi & Neu, 2023, Thm 1) (i.e. we extent the result from single-task learning (Lugosi & Neu, 2023) to our multi-task learning setting), but the novelty of our Theorem 5 lies in the combination of the online-to-PAC analysis and the task-averaged regret analysis developed in our Theorem 3. Such combination pioneers a new research direction to demonstrating PAC-Bayes generalization error bounds for statistical multi-task learning. If we do not combine the aforementioned two analysis tools, but just use the online-to-PAC analysis in (Lugosi & Neu, 2023, Thm 1) and the traditional regret analysis for single-task learning in (Alquier, 2021, Thm 2.1), we can only obtain the following trivial (to some extent) PAC-Bayes generalization error bound for statistical multi-task learning:\n1\nT T\u2211 t=1 E\u03b8\u223cAt(St) [ Ez\u223c\u00b5t`(\u03b8, z)\u2212 1 m T\u2211 i=1 `(\u03b8, zti) ] \u2264 \u221a mM(1 + 1T \u2211T t=1 KL(\u03c1 \u2217 t ||\u03c1t1)) m +M \u221a 2 log 1\u03b4 Tm ,\nwhich is less informative than our PAC-Bayes bound for statistical multi-task learning in Theorem 5."
        },
        {
            "heading": "H EXPERIMENTS",
            "text": "We conduct k-center clustering experiment to verify the convergence performance of task-averaged regret R\u0304T,m/m of our Algorithm 1 for non-convex OWO meta learning. We follow the existing work (Balcan et al., 2021) and meta learn the hyper-parameter \u03b1 in the \u03b1-Lloyd\u2019s clustering algorithm.\nIntroduction of \u03b1-Lloyd\u2019s Algorithm. \u03b1-Lloyd\u2019s algorithm consists of two phases: seeding phase and local search phase. The goal of the seeding phase is to output k initial centers with d\u03b1-sampling. Concretely, each point v is sampled with probability proportional to minc\u2208C d(v, c)\u03b1, where d(\u00b7, \u00b7) is the distance metric and C is the set of centers updated so far. The goal of local search phase is to run an iterative two-step procedure to output final centers. Specifically, the first step is to create a Voronoi tiling of all points induced by the initial set of centers from seeding phase; then, the new set of centers is updated by computing the centroid (e.g. median or mean) of each Voronoi tile. The \u03b1-Lloyd\u2019s algorithm family include popular clustering methods like randomly initialized k-means (\u03b1 = 0) and farthest-first traversal (\u03b1 = \u221e). The indicator used to measure the performance of \u03b1-Lloyd\u2019s algorithm is the Hamming loss between the outputted clustering and the optimal target clustering (which is given in advance). The Hamming loss is a piecewise constant function of \u03b1 and hence is a piecewise Lipschitz function. More explanations for this algorithm family and proof for the piecewise constant property of Hamming loss can be found in (Balcan et al., 2021, Section 4.1).\nExperimental Setting. We conduct k-center clustering experiment on both synthetic and real-world datasets, under the same settings as that in Balcan et al. (2021). On the one hand, we create a Gaussian mixture binary classification dataset, where each class is a 2-dimensional diagonal Guassian distribution with variance \u03c3 and 2\u03c3, as well as the expectation (0, 0) and (b\u03c3, 0). We set b \u2208 [2, 3] to generate different tasks. On the other hand, we utilize the split of the real-world Omniglot dataset to create clustering tasks, by drawing random samples each composed of five characters among which four are constant throughout. We set the number T \u2208 [1, 10] of training tasks and the number m \u2208 [5, 50] of samples per task for online optimization. Analogous to Balcan et al. (2021), we set the parameters \u03b3 = \u03b7 = 0.01 (not hyper-parameter searched), and set the step size \u03bb in EWA algorithm to minimize the regret in Eq. (2) (not meta-learned). For any fixed T and m, we run our non-convex OWO meta learning algorithm to learn the hyper-parameter \u03b1 in the \u03b1-Lloyd\u2019s algorithm, and calculate the normalized task-averaged regret R\u0304T,m/m over the T training tasks. In Figure 1, we show the convergence performance of R\u0304T,m/m with respect to different number T \u2208 [1, 10] of training tasks, and validate the advantage of OWO meta learning over single-task learning. Each training task has m samples and is called m-shot learning. In Figure 2, we exhibit the asymptotic performance of averaged regret R\u0304T,m/mwith respect to different numberm of samples on one task.\nExperimental Results in Figure 1. We can observe that: (1) On both synthetic and real-world clustering dataset, running OWO meta learning algorithm can achieve sharper regret than that of online single-task algorithm, indicating the advantage of online meta learning framework. (2) When the training task number T \u2265 8, the task-averaged regret R\u0304T,m/m of \u201810-shot multi-task\u2019 algorithm in Figure 1(a) is smaller than the regret of \u201820-shot single-task\u2019 algorithm. This shows that, leveraging meta learning paradigm, even with less samples per task, can achieve the same or better performance compared with the single-task learning paradigm where the task has more training samples, hence alleviating the cost of collecting labeled data. (3) On both synthetic and real-world clustering dataset, the normalized task-averaged regret R\u0304T,m/m always decreases with the increase of the number T of training tasks, empirically validating the convergence performance of our regret for OWO meta learning. However, we need to point out that, in some cases of Figures 1(a)-(b),regret R\u0304T,m/m does not decrease and even slightly increase when T becomes larger. We attribute the counter-intuitive phenomenon to the encountering of a tough training task that is dissimilar to previous tasks and can lead to the increase of the value V defined in Section 4. Therefore, the task-averaged regret bound that involves the task similarity V in our Theorem 2 possibly becomes larger, and so does the regret. (4) On the real-world dataset Omniglot, the phenomenon that R\u0304T,m/m slightly increases with the larger T appears more frequently than on the synthetic Gaussian mixture dataset. This implies the higher degree of similarity of Gaussian mixture clustering task than that of Omniglot splitting tasks, and attaches great importance of task similarity to the success of online meta learning algorithms.\nExperimental Results in Figure 2. We can observe that: (1) On both Gaussian mixture and Omniglot datasets, for any fixed T , the normalized task-averaged regret R\u0304T,m/m decreases with the increase of the number m of samples per task, verifying the vanishing regret property (i.e. R\u0304T,m = o(m)) of our results. (2) Leveraging OWO meta learning paradigm (i.e. when T = 5 or T = 10) achieves sharper regret than online single-task learning paradigm (i.e. T = 1). (3) When using more training tasks, the regret improvements on Gaussian mixture dataset are larger\nthan that on Omniglot dataset. This is consistent with the observation in Figure 1 that Gaussian mixture clustering tasks share a higher degree of task similarity than that of Omniglot splitting tasks, revealing the great effect of task similarity to the performance of online meta learning algorithms.\nRemark H.1 (More Discussions of our Algorithm 2) (1) The Limitation Aspect. We need to admit that at the current stage we are unable to run our Algorithm 2 in practice, because it is hard to compute the analytic form of RN derivative of \u03c1t+1,m w.r.t. the Lebesgue measure \u03bd (if we want to implement our algorithm in real-life applications, we need to compute d\u03c1tid\u03bd to let \u03c1ti be tractable in Euclidean space). The computation problem actually lies in line 6 (i.e. the meta update step of \u03c1t+1,1 = 1 t \u2211t s=1 \u03c1sm) in Algorithm 2, even though \u03c1sm is the Lebesgue measure (i.e. corresponding to uniform distribution) or Gaussian measure (i.e. corresponding to Gaussian distribution). Concretely, consider the simplest case where \u03c111 is the Lebesgue measure \u03bd, and hence using EWA obtains the Gaussian measure \u03c11m (with density N (\u00b51, \u03c31)). Assume that \u03c12m is also a Gaussian measure (with density N (\u00b52, \u03c32)), then \u03c131 = \u03c11m+\u03c12m2 and the density of \u03c13m w.r.t. to \u03bd is d\u03c13m\nd\u03bd = d\u03c13m d\u03c131 \u00b7 d(\u03c11m+\u03c12m)2d\u03bd \u221d exp{\u2212 \u2211 i\u2208[m] `3i} \u00b7 (N (\u00b51, \u03c31) +N (\u00b52, \u03c32)), indicating that \u03c13m is not a Gaussian measure and it is hard to compute the precise form of the density function d\u03c13md\u03bd . Nevertheless, it will also be difficult to compute the precise form of the density function d\u03c1t+1,1d\u03bd , as well as the precise value of the normalized constant \u222b exp{\u2212\u03bbt+1 \u2211m j=1 `t+1,j(\u03b8)}\u03c1t+1,1(d\u03b8). Therefore, it is also not easy to compute the precise form of d\u03c1t+1,md\u03c1t+1,1 over the continuous domain. (2) The Potential Application Aspect. However, we point out that our Algorithm 2 is potentially applicable to the discrete domain where the updating rule \u03c1t+1,1 = \u2211t s=1 \u03c1sm/t of our Algorithm 2 corresponds to the averaging of previous t discrete probability density vectors, which is more feasible. Thus our algorithm may be utilized to the Expert Advice problem for online model selection (see (Cesa-Bianchi & Lugosi, 2006, Sect 2)), and this serves as one of our ongoing research directions.\nRemark H.2 (Detailed Comparisons between our Algorithm 1 and other Algorithms for NonConvex OWO Meta Learning) We discuss more differences between our Algorithm 1 and the stateof-the-art algorithm from (Balcan et al., 2021, Alg 3) as well as other baseline for OWO meta learning. The detailed explanations are three-fold: (1) The comparisons with the single-task algorithm. Actually, we choose single-task EWA algorithm as our baseline in all experiments. In both Figure 1 (m-shot multi-task method v.s. single-task baseline) and Figure 2 (T = 5 multi-task method v.s. T = 1 single-task baseline), we show advantages of the meta-learning based EWA algorithm over the single-task EWA algorithm baseline, and verify the convergence performance of task-averaged regret of our meta learning algorithm. (2) The comparisons with the state-ofthe-art (Balcan et al., 2021, Alg 3) for non-convex piecewise-Lipschitz OWO meta learning. Our Algorithm 1 is actually based on the modification of (Balcan et al., 2021, Alg 3), and the two main differences are as follows: (i) For learning the initialization \u03c1, we use FTRL algorithm to achieve the regret of O(T 1/2+\u03b1), \u03b1 \u2208 (0, 12 ), and the mixture parameter \u03b3 = m\nd\u03b2/T\u03b1 is irrelevant to the optimal \u03c1\u2217 (hence \u03b3 can be set in advance). Existing work (Balcan et al., 2021) also uses FTRL algorithm, but (Balcan et al., 2021) leverages a more complicated analysis, attaining a larger regret bound O(md/2T 3/4). Nevertheless, their choice of \u03b3 = KL(\u03c1 \u2217||\u03bd\u0302)1/4md\u03b2/2\nT 1/4V 1/2 d\nin (Balcan et al., 2021, Thm 3.2) depends on the knowledge of optimal distribution \u03c1\u2217 that contains information of T training tasks, which is unfeasible in the sequential online meta learning setting. (ii) For learning the step size v, we choose FTL algorithm to learn the step size vt = argminv\u2208[B2,D2] \u2211t\u22121 s=1 hs(v) = (\u2211t\u22121 s=1 fs(\u03c1s1)/(t\u2212 1) )1/2 to derive a logarithmic regret bound of O(log T ). In contrast, (Balcan et al., 2021) chooses -FTL algorithm to optimize the functions {ht(v) = v + (ft(\u03c1t1) + 2)/v}t\u2208[T ] on the domain [0, D2] (where D2 \u2265 maxt ft(\u03c1t1)) to learn the step size vt = arg minv\u2208[0,D2] \u2211t\u22121 s=1 hs(v) = (\u2211t\u22121 s=1 fs(\u03c1s1)/(t\u2212 1) + 2 )1/2 , and lead to the regret O(T 2 + (log T )/ 2), which is of O( \u221a T log T ) if we set = 1/T 1/4 and is slower than our regret bound. (3) Practical implementation of meta learning algorithm. However, (Balcan et al., 2021) uses a heuristic method to learn (instead of meta learn) the step size \u03bbt, so their implementation is not rigorously the same as their pseudo code in (Balcan et al., 2021, Alg 3). We follow almost the same implementation details as (Balcan et al., 2021) (which is also explained in detail in the Experimental Setting part), with the main difference being that we verify the regret bound over training tasks but (Balcan et al., 2021) verify the regret bound over test tasks. Thus, we do not compare the convergence performance of regret of (Balcan et al., 2021, Alg 3) in our experiments."
        }
    ],
    "title": "IMPROVED REGRET BOUNDS FOR NON-CONVEX ONLINE-WITHIN-ONLINE META LEARNING",
    "year": 2024
}