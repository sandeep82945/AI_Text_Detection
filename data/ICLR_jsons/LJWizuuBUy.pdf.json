{
    "abstractText": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei You"
        },
        {
            "affiliations": [],
            "name": "Hei Victor Cheng"
        }
    ],
    "id": "SP:64c82495d56d6bec7c50a07de2a047725f32bda0",
    "references": [
        {
            "authors": [
                "Fan Ang",
                "Li Chen",
                "Nan Zhao",
                "Yunfei Chen",
                "Weidong Wang",
                "F Richard Yu"
            ],
            "title": "Robust federated learning with noisy communication",
            "venue": "IEEE Transactions on Communications,",
            "year": 2020
        },
        {
            "authors": [
                "Riade Benbaki",
                "Wenyu Chen",
                "Xiang Meng",
                "Hussein Hazimeh",
                "Natalia Ponomareva",
                "Zhe Zhao",
                "Rahul Mazumder"
            ],
            "title": "Fast as chita: Neural network pruning with combinatorial optimization",
            "venue": "arXiv preprint arXiv:2302.14623,",
            "year": 2023
        },
        {
            "authors": [
                "Kush Bhatia",
                "Prateek Jain",
                "Purushottam Kar"
            ],
            "title": "Robust regression via hard thresholding",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Wenyu Chen",
                "Rahul Mazumder"
            ],
            "title": "Multivariate convex regression at scale",
            "venue": "arXiv preprint arXiv:2005.11588,",
            "year": 2020
        },
        {
            "authors": [
                "Wenyu Chen",
                "Riade Benbaki",
                "Xiang Meng",
                "Rahul Mazumder"
            ],
            "title": "Network pruning at scale: A discrete optimization approach",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Sahaj Garg",
                "Joe Lou",
                "Anirudh Jain",
                "Zhimu Guo",
                "Bhavin J Shastri",
                "Mitchell Nahmias"
            ],
            "title": "Dynamic precision analog computing for neural networks",
            "venue": "IEEE Journal of Selected Topics in Quantum Electronics,",
            "year": 2022
        },
        {
            "authors": [
                "Aude Genevay",
                "L\u00e9naic Chizat",
                "Francis Bach",
                "Marco Cuturi",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Sample complexity of sinkhorn divergences",
            "venue": "In The 22nd international conference on artificial intelligence and statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten Borgwardt",
                "Malte Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alex Smola"
            ],
            "title": "A kernel method for the two-sample-problem",
            "venue": "Advances in neural information processing systems,",
            "year": 2006
        },
        {
            "authors": [
                "Babak Hassibi",
                "David Stork"
            ],
            "title": "Second order derivatives for network pruning: Optimal brain surgeon",
            "venue": "Advances in neural information processing systems,",
            "year": 1992
        },
        {
            "authors": [
                "Hussein Hazimeh",
                "Rahul Mazumder"
            ],
            "title": "Fast best subset selection: Coordinate descent and local combinatorial optimization algorithms",
            "venue": "Operations Research,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Andrew G Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "venue": "arXiv preprint arXiv:1704.04861,",
            "year": 2017
        },
        {
            "authors": [
                "Hong Huang",
                "Lan Zhang",
                "Chaoyue Sun",
                "Ruogu Fang",
                "Xiaoyong Yuan",
                "Dapeng Wu"
            ],
            "title": "Fedtiny: Pruned federated learning towards specialized tiny models",
            "year": 1977
        },
        {
            "authors": [
                "Berivan Isik",
                "Kristy Choi",
                "Xin Zheng",
                "Tsachy Weissman",
                "Stefano Ermon",
                "H-S Philip Wong",
                "Armin Alaghi"
            ],
            "title": "Neural network compression for noisy storage devices",
            "venue": "ACM Transactions on Embedded Computing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Hicham Janati",
                "Boris Muzellec",
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Entropic optimal transport between unbalanced gaussian measures has a closed form",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny",
            "year": 2009
        },
        {
            "authors": [
                "Yann LeCun",
                "John Denker",
                "Sara Solla"
            ],
            "title": "Optimal brain damage",
            "venue": "Advances in neural information processing systems,",
            "year": 1989
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE signal processing magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Maren Mahsereci",
                "Lukas Balles",
                "Christoph Lassner",
                "Philipp Hennig"
            ],
            "title": "Early stopping without a validation",
            "venue": "set. arXiv preprint arXiv:1703.09580,",
            "year": 2017
        },
        {
            "authors": [
                "Michael C Mozer",
                "Paul Smolensky"
            ],
            "title": "Using relevance to reduce network size automatically",
            "venue": "Connection Science,",
            "year": 1989
        },
        {
            "authors": [
                "Kimia Nadjahi",
                "Alain Durmus",
                "L\u00e9na\u0131\u0308c Chizat",
                "Soheil Kolouri",
                "Shahin Shahrampour",
                "Umut Simsekli"
            ],
            "title": "Statistical and topological properties of sliced probability divergences",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kimia Nadjahi",
                "Alain Durmus",
                "Pierre E Jacob",
                "Roland Badeau",
                "Umut Simsekli"
            ],
            "title": "Fast approximation of the sliced-wasserstein distance using concentration of random projections",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9"
            ],
            "title": "Course notes on computational optimal transport",
            "year": 2019
        },
        {
            "authors": [
                "Galen Reeves"
            ],
            "title": "Conditional central limit theorems for gaussian projections",
            "venue": "IEEE International Symposium on Information Theory (ISIT),",
            "year": 2017
        },
        {
            "authors": [
                "Julien Niklas Siems",
                "Aaron Klein",
                "Cedric Archambeau",
                "Maren Mahsereci"
            ],
            "title": "Dynamic pruning of a neural network via gradient signal-to-noise ratio",
            "venue": "In 8th ICML Workshop on Automated Machine Learning (AutoML),",
            "year": 2021
        },
        {
            "authors": [
                "Sidak Pal Singh",
                "Dan Alistarh"
            ],
            "title": "Woodfisher: Efficient second-order approximation for neural network compression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Steinhardt",
                "Pang Wei W Koh",
                "Percy S Liang"
            ],
            "title": "Certified defenses for data poisoning attacks",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Vladimir Nikolaevich Sudakov"
            ],
            "title": "Typical distributions of linear functionals in finite-dimensional spaces of higher dimension",
            "venue": "In Doklady Akademii Nauk,",
            "year": 1978
        },
        {
            "authors": [
                "Tiffany Tuor",
                "Shiqiang Wang",
                "Bong Jun Ko",
                "Changchang Liu",
                "Kin K Leung"
            ],
            "title": "Overcoming noisy and irrelevant data in federated learning",
            "venue": "In 2020 25th International Conference on Pattern Recognition (ICPR),",
            "year": 2021
        },
        {
            "authors": [
                "Berkay Turan",
                "C\u00e9sar A Uribe",
                "Hoi-To Wai",
                "Mahnoosh Alizadeh"
            ],
            "title": "Robust distributed optimization with randomly corrupted gradients",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Yang",
                "Tao Jiang",
                "Yuanming Shi",
                "Zhi Ding"
            ],
            "title": "Federated learning via over-the-air computation",
            "venue": "IEEE transactions on wireless communications,",
            "year": 2022
        },
        {
            "authors": [
                "Xin Yu",
                "Thiago Serra",
                "Srikumar Ramalingam",
                "Shandian Zhe"
            ],
            "title": "The combinatorial brain surgeon: Pruning weights that cancel one another in neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The advent of deep learning has revolutionized various domains of artificial intelligence, with neural networks showing remarkable performance across an array of applications. Nonetheless, the increase in model complexity has led to escalating computational demands and substantial memory requirements. This poses significant challenges for deploying these models in resource-constrained environments such as mobile or internet of things (IoT) devices. Therefore, the concept of neural network pruning emerges as a critical solution. It aims to optimize the network by removing less important parameters, which reduces computational overhead while maintaining the performance of the original model.\nIn the realm of state-of-the-art deep learning, the models often exhibit substantial size and complexity, with up to trillions of parameters, as exemplified by models such as GPT-4. The immense computational demand, energy inefficiency, and the challenges with model interpretability associated with these models highlight the need for innovative and efficient optimization techniques. These techniques should ideally minimize the model size while improving their robustness and interpretability. Considering the limitations of previous work, especially those arising from the influence of noisy data and noisy gradients, the paper proposes a promising pathway for robust pruning.\nBelow, we inspect the network pruning problem from an optimization perspective, with a concise introduction of the most relevant existing works. Then a sketch of our approach is given.\nRelated Work on Pruning as Optimization. Denote by w\u0304 P Rp a trained model and Lpwq the loss function given arbitrary model w. The loss function can be locally approximated around w\u0304 with\n\u02daCorrespondence to both leiyo@dtu.dk and hvc@ece.au.dk. Lei You is supported by Thomas B. Thriges Fond 5041-2402. Hei Victor Cheng is supported by the Aarhus Universitets Forskningsfond under Project AUFF 39001.\nTaylor Expansion as shown in (1).\nLpwq \u201c Lpw\u0304q ` \u2207Lpw\u0304qJpw \u00b4 w\u0304q ` 1 2 pw \u00b4 w\u0304qJ\u22072Lpw\u0304qpw \u00b4 w\u0304q `Op\u2225w \u00b4 w\u0304\u22253q (1)\nConsider a neural network with a loss function Lpwq \u201c 1N \u0159N i\u201c1 \u2113ipwq, where \u2113ipwq P Rp is the loss incurred at data point i (i \u201c 1, . . . , N ). The goal of network pruning is to find a set of w such that there are k (k \u0103 p) elements of w being zero while keeping the newly obtained model w\u2019s performance as good as possible to the original one w\u0304. Mathematically, we want to find some w P Rp that satisfies both Lpwq \u00ab Lpw\u0304q and \u2225w\u22250 \u010f k, with k \u0103 p. This line of research can be dated back to (LeCun et al., 1989), where the approximation in equation (1) is adopted. Under the assumption that gradient \u2207Lpw\u0304q \u00ab 0 when the network is trained, the network weights are pruned one-by-one in decreasing order based on the value of pw\u00b4w\u0304qJHpw\u00b4w\u0304q. In their approach, the H is approximated as a diagonal matrix; this is later extended in (Hassibi & Stork, 1992) to include the whole Hessian matrix, and the authors also proposed using the Fisher information matrix (FIM) as an approximation to the Hessian. Later, (Singh & Alistarh, 2020) proposed to reduce the computation complexity by using block diagonal Hessian, and FIM is approximated using a small subset of the training data.\nThese approaches all use equation (1) to prune the network in a one-by-one manner, namely the weight with the least importance is set to zero according to the different approximations of equation (1). In this way, the potential interactions of pruning multiple weights are ignored. To explore this, the network pruning problem is formulated as a mixed integer quadratic programming (MIQP) in (Yu et al., 2022). Namely, an objective function\nfpwq \u201c pw \u00b4 w\u0304qJHpw \u00b4 w\u0304q ` \u03bb \u2225w \u00b4 w\u0304\u22252 (\u03bb \u011b 0) (2)\nis minimized, and Hessian is approximated as H \u00ab \u22072Lpw\u0304q, subject to the sparsity constraint \u2225w\u22250 \u010f k, where \u03bb is a regularization parameter. Although this approach shows significant improvements, it suffers from scalability issues as a full Hessian matrix is required.\nSparse Linear Regression (LR) Formulation. To reduce the computational complexity, the Hessian matrix can be approximated by the empirical FIM, using n samples as in (Chen et al., 2022; Benbaki et al., 2023). Denote G \u201c r\u2207\u21131, . . . ,\u2207\u2113nsJ P Rn\u02c6p, where \u2207\u2113i \u201c \u2207\u2113ipw\u0304q. For simplicity, \u2207\u2113i is used in this document to represent the derivative of the data point i\u2019s loss at w\u0304 consistently in this paper unless specified otherwise. The Hessian is approximated through the expression H \u00ab p1{nq\n\u0159n i\u201c1 \u2207\u2113i\u2207\u2113 J i \u201c p1{nqGJG, which is the so-called FIM. Denote xi \u201c \u2207\u2113 J i w and\nyi \u201c \u2207\u2113Ji w\u0304, (2) is formulated to the sparse LR problem shown in (3) below.\nmin w\nQ\u0304pwq \u201c n \u00ff\ni\u201c1 \u2225xipwq \u00b4 yi\u22252 ` n\u03bb \u2225w \u00b4 w\u0304\u22252 , s.t. }w}0 \u010f k (3)\nThis formulation has a computational advantage, as empirical FIM needs not to be computed explicitly. It is shown that the formulation scales to large neural network pruning (Chen et al., 2022).\nMotivation of Combating Against Noise. In practice, it is not always easy to obtain the correct gradients for pruning large neural networks. There can be noise contained in the data samples, and the gradients can also be corrupted due to various reasons, e.g., distributed or federated learning (Turan et al., 2022), or adversarial attacks such as data poisoning (Steinhardt et al., 2017).\nAs pointed out by (Mahsereci et al., 2017; Siems et al., 2021), conditioning on the underlying true gradient \u2207Lpw\u0304q \u201c 0, there are mini-batch gradients which are not informative anymore as it can be fully explained by sample noise and the vanishing gradients. These gradients would not contribute to the covariance information of the empirical FIM but serve as outliers in Hessian approximation.\nIn the scenarios of federated learning (FL), gradients computed by different clients are skewed and consequently, local models move away from globally optimal models (Huang et al., 2022), imposing challenges for constructing informative FIM. Besides, noise can be added to the gradient for privacy concerns in communications (Li et al., 2020). Additionally, the clients usually have inevitable noisy samples and labels, making models suffer from a significant performance drop (Tuor et al., 2021). Additionally, over-the-air communications itself suffer from unavoidable noises (Ang et al., 2020; Yang et al., 2020). These lead to concerns for network pruning with noisy gradients. Finally, analog\nmemory recently gained attention for deep learning model deployment (Garg et al., 2022). When neural network parameters and data are stored in these analog devices, they are susceptible to devicerelated noise, affecting the performance of network compression (Isik et al., 2023).\nApproach Sketch. We revisit the MIQP network pruning optimization from a perspective of entropic Wasserstein regression (EWR), which leverages Wasserstein distance to model the dissimilarity between two distributions. In our context, it measures the dissimilarity of distributions relevant to model parameters and gradient magnitudes before and after pruning. Namely, \u2207\u2113 is a p dimensional distribution, capturing geometric properties of the loss at w\u0304 before pruning. Both w\u0304 and w perform projections for \u2207\u2113 to a 1-D distribution respectively as \u2207\u2113Jw\u0304 and \u2207\u2113Jw. Computing the distance between \u2207\u2113Jw\u0304 and \u2207\u2113Jw falls into the framework of sliced probability divergence (Nadjahi et al., 2020). Under this framework, pruning optimization essentially fine-tunes w and selectively reserves its elements such that the divergence is minimized subject to the sparsity constraint.\nOur approach\u2019s effectiveness in combating noisy gradients is established both analytically and numerically. We demonstrate that pruning through the Wasserstein regression implicitly enacts gradient averaging using Neighborhood Interpolation. This entails a nuanced balance between capturing gradient covariance and diminishing gradient noise. Notably, the sparse LR formulation is merely a specific instance of ours. Yet, our proposed algorithm doesn\u2019t demand a markedly higher computational expense. This modest additional effort bestows upon us enhanced robustness."
        },
        {
            "heading": "2 PROBLEM SETUP AND FORMULATION",
            "text": "We first introduce the optimal transport (OT) problem in Kantorovich formulation with entropic regularization, which measures the distance between two distributions, defined in (4) below. The Wasserstein regression formulation as a generalization of the LR formulation is then proposed.\nThe Kantorovich Problem. Denote P2 the set of probability measures with finite second moments. Let \u00b5, \u03bd P P2 and let \u03a0p\u00b5, \u03bdq denote the set of probability measures in P2 with marginal distributions equal to \u00b5 and \u03bd. The 2-Wasserstein distance is defined as\nW 22 p\u00b5, \u03bdq \u201c inf \u03c0P\u03a0p\u00b5,\u03bdq\n\u017c\nRd\u02c6d \u2225x\u00b4 y\u22252 d\u03c0px, yq ` \u03b5\n\u017c\nRd\u02c6d log\n\u02c6\nd\u03c0\nd\u00b5d\u03bd\n\u02d9\nd\u03c0. (4)\nThis is also referred to as the entropic OT problem, where the first term is the transportation cost between the two measures and the second term is the entropic regularization with multiplier \u03b5.\nSparse EWR Formulation. The pruning problem formulation is defined in (5) below. min w Qpwq \u201c W 22 pxpwq, yq ` \u03bb \u2225w \u00b4 w\u0304\u2225 2 (5a)\ns.t. \u2225w\u22250 \u010f k (5b) The term W 22 pxpwq, yq is a Wasserstein distance between the two one-dimensional distributions x and y (or a sliced Wasserstein distance for \u2207\u2113 with two one-dimensional projections). The optimization is to alter w such that the distance between the two distributions is minimized.\nLet x and y follow the empirical distributions txiuni\u201c1 and tyiuni\u201c1. Denote by \u00b5i and \u03bdi the mass of the data points xi and yi, respectively. We use \u03a0 to refer to a matrix representing the transportation probability between x and y, and \u03a0 the set of all such matrices, i.e. \u03a0 \u201c t\u03a0|\n\u0159n i\u201c1 \u03c0ij \u201c\n\u00b5j @j and \u0159n j\u201c1 \u03c0ij \u201c \u03bdi @iu, where \u00b5i and \u03bdj are marginal distributions. Then (5) reads:\nmin w Qpwq \u201c inf \u03a0P\u03a0\n#\nn \u00ff\ni\u201c1\nn \u00ff j\u201c1 \u2225xipwq \u00b4 yi\u22252 \u03c0ij ` \u03b5 n \u00ff i\u201c1 n \u00ff j\u201c1 log \u02c6 \u03c0ij \u00b5i\u03bdj \u02d9 \u03c0ij\n+\n` \u03bb \u2225w \u00b4 w\u0304\u22252 (6a)\ns.t. \u2225w\u22250 \u010f k (6b)\nLR as a Special Case. Let \u03b5 \u201c 0. Once we set \u03a0 to be a diagonal matrix with constant value 1{n, i.e. diagp1{nq, the mass transportation happens only between data point pairs pxi, yiq for i \u201c 1, . . . , n. Therefore we have\nQ\u03a0\u201cdiagp1{nqpwq \u201c 1\nn\nn \u00ff i\u201c1 \u2225xipwq \u00b4 yi\u22252 ` \u03bb \u2225w \u00b4 w\u0304\u22252 \u201c 1 n Q\u0304pwq, (7)\ni.e. the formulation (6) in this case degrades to the LR formulation in (3)."
        },
        {
            "heading": "3 THEORETICAL ASPECTS",
            "text": "This section reveals some good theoretical properties of the Sparse EWR formulation for network pruning. We start with Proposition 1 below (proof in Appendix A.1) that states a geometry property of OT with squared Euclidean distance cost. Additionally, we demonstrate the Neighborhood Interpolation mechanism that happens implicitly in solving the EWR. Moreover, we show that such a mechanism strikes a balance in capturing gradient covariance and reducing gradient noise, with a brief discussion on the advantage of using entropic regularization in terms of sample complexity. Proposition 1 (Convex Hull Distance Equality). Consider a set S and its convex hull ConvpSq in a Euclidean space, and an arbitrary point x in the space. For any probability measure \u03bd\u0302 on S, we can find a point y1 in ConvpSq as y1 \u201c \u015f y d\u03bdpyq such that }x \u00b4 y1}2 \u201c \u015f\n}x \u00b4 y}2 d\u03bd\u0302pyq, where \u03bd is a measure on ConvpSq.\nNeighborhood Interpolation. In formulation (5), let Wx be the first term of W 22 for an arbitrary given x, i.e., Wx \u201c \u015f\n\u2225xpwq \u00b4 y\u22252 d\u03c0p\u00a8|xqpyq, where \u03c0p\u00a8|xqpyq is a conditional measure given x. Now, divide the Euclidean space Rd by subspaces Sy1 , S y 2 . . . , S y n for y. For any conditional measure \u03c0p\u00a8|xqpyq defined on any Syi (i \u201c 1, . . . , n), there exists a measure \u03bdpyq defined on ConvpS y i q such that the weighted distance from Syi to x equals the distance from x to a point y 1 in ConvpSyi q. Hence\nWx \u201c \u017c\nSy1 YS y 2 Y\u00a8\u00a8\u00a8YS y n\n}x\u00b4 y}2 d\u03c0p\u00a8|xqpyq\n\u201c 1 n\nn \u00ff\ni\u201c1\n\u203a \u203ax\u00b4 y1i \u203a \u203a 2 s.t. y1i \u201c\n\u017c\nConvpSyi q y d\u03bdpyq, \u03bd P Vipxq, i \u201c 1, . . . n.\nwhere Vipxq is the set of measures \u03bd that make the equality holds and Vipxq \u2030 \u2205 by Proposition 1. Similarly, we define Wy for any given y and subspaces Sx1 , S x 2 . . . , S x n,\nWy \u201c \u017c\nSx1 YSx2 Y\u00a8\u00a8\u00a8YSxn }x\u00b4 y}2 d\u03c0p\u00a8|yqpxq\n\u201c 1 n\nn \u00ff\ni\u201c1\n\u203a \u203ax1i \u00b4 y \u203a \u203a 2 s.t. x1i \u201c\n\u017c\nConvpSxi q x d\u00b5pxq, \u00b5 P Uipyq, i \u201c 1, . . . n.\nwhere \u00b5 is a measure defined on ConvpSxi q and Uipyq \u2030 \u2205.\ny1 y2\ny3\ny4\ny5\nx\ny1\nWe demonstrate the concept of \u201cNeighborhood Interpolation\u201d through an empirical distribution example. Define S as a subset of y such that for every element yi P S, \u03c0x,i \u0105 0. Without loss of generality, we can denote S \u201c ty1, y2, y3, y4, y5u. The area shaded in gray, denoted as ConvpSq, represents the convex hull of S. Wx computes a weighted summation of distances between x and the points y1, . . . , y5. The weights \u03c0x,1, . . . , \u03c0x,5 are decided by OT. A significant \u03c0x,i typically implies that yi is in proximity to x, indicating a neighborhood relation. By Proposition 1, this weighted distance is analogous to the distance between x and y1, where y1 is derived from ConvpSq.\nRevisit the EWR formulation. The integral of either Wx or Wy respectively on x or y gives the first term of W 22 . One can then reformulate (5) as (8) below.\nmin w:}w}0\u010fk Qpwq \u201c 1 2 inf \u03c0\n\" \u017c\nWxpwqd\u00b5pxq ` \u017c Wypwqd\u03bdpyq * ` \u03bb \u2225w \u00b4 w\u0304\u22252 . (8)\nInterpretation: The objective function calculates the Euclidean distance between a point x and n distinct points. These n points originate from n convex hulls, each shaped by different n subspaces within y. Similarly, the function measures the distance between each point y and m unique points derived from m convex hulls, each formed by distinct m subspaces within x.\nWe claim that the EWR formulation is more resilient to noisy gradient than its counterpart, the LR formulation given by (3). To understand this claim better, let us reimagine the problem using empirical distributions, as indicated by (6). In this context, we use xi and yi as substitutes for Sxi and Syi . Moreover, the integration in both Wx and Wy is replaced with summations, offering a more insightful version of our initial EWR formulation, shown as (9).\nmin w:\u2225w\u22250\u010fk Qpwq \u201c inf \u03a0\n#\nQ\u03a0pwq ` \u03b5 n \u00ff\ni\u201c1\nn \u00ff j\u201c1 log \u02c6 \u03c0ij \u00b5i\u03bdj \u02d9 \u03c0ij\n+\n(9)\nThe notation Q\u03a0pwq defined in (10) denotes the part of the objective function given fixed \u03a0:\nQ\u03a0pwq \u201c n \u00ff\ni\u201c1\nn \u00ff j\u201c1 \u2225xipwq \u00b4 yj\u22252 \u03c0ij ` \u03bb \u2225w \u00b4 w\u0304\u22252 (10a)\n\u201c 1 2\nn \u00ff\ni\u201c1 \u2225\u2225xipwq \u00b4 y1i\u2225\u22252 loooooooooomoooooooooon\nK p1q \u03a0\n`1 2\nn \u00ff\ni\u201c1 \u2225\u2225x1ipwq \u00b4 yi\u2225\u22252 loooooooooomoooooooooon\nK p2q \u03a0\n`\u03bb \u2225w \u00b4 w\u0304\u22252 (10b)\nIn Q\u03a0pwq, for each index i, points x1i and y1i are chosen from the convex hulls formed by points in x and y, as per the guidelines of Proposition 1. Now, contrasting this with the LR model in (3), the objective Q\u0304pwq aims for regression directly over the data points whereas every point from one empirical set is matched for Euclidean distance computation to a point derived from a convex combination of the other.\nThe infimum in (9) seeks the OT plan, \u03a0, that aligns the empirical distributions x and y closely. In practical terms, for each data point xi, only a subset of tyiuni\u201c1 will transport a substantial mass, rather than the entire set. This behavior of \u03a0 effectively defines n \u201dneighborhoods\u201d for each data point xi within the empirical distribution of y. Here, a \u201dneighborhood\u201d refers to a group of data points in y that are proximate to a specific xi in the Euclidean sense.\nNeighborhood Size Control. A critical aspect of this formulation is the entropic regularization term, which is used to modulate the size of these neighborhoods. Specifically, increasing the value of \u03b5 amplifies the impact of the entropy term. This change broadens the neighborhoods, drawing more data points into the fold of the associated convex hulls. An illustrative extreme case is when \u03b5 \u201c 0. Here, the OT does one-to-one matching, implying that each data point yi primarily forms the convex hull independently. On the contrary, when \u03b5 \u00d1 8, all data points are equally weighted by \u03a0 and hence involved in forming the convex hull as a neighborhood.\nCapturing Covariance With Gradient Noise Reduction. For an arbitrary w, the EWR formulation essentially strikes a balance between gradient noise reduction and covariance capturing. We show the analysis for Kp1q\u03a0 in (10), and K p2q \u03a0 pwq follows similarly. Note that y1i \u201c \u0159n j\u201c1 \u03bd piq j yj \u201c \u0159n j\u201c1 \u03bd piq j \u2207\u2113 J j w\u0304, where \u03bd piq are convex combination coefficients by Proposition 1. Denote \u2207\u21131Ji \u201c \u0159n j\u201c1 \u03bd piq j \u2207\u2113 J j , and G 1 \u201c r\u2207\u211311, . . . ,\u2207\u21131nsJ. The term K p1q \u03a0 expands as follows.\nK p1q \u03a0 \u201c\nn \u00ff i\u201c1 p\u2207\u2113Ji w \u00b4 \u2207\u2113 1J i w\u0304qJp\u2207\u2113 J i w \u00b4 \u2207\u2113 1J i w\u0304q\n\u201c n \u00ff\ni\u201c1 pwJ\u2207\u2113i\u2207\u2113Ji w \u00b4 wJ\u2207\u2113i\u2207\u2113 1 i Jw\u0304 \u00b4 w\u0304J\u2207\u21131i\u2207\u2113iJw ` w\u0304J\u2207\u2113 1 i\u2207\u2113 1 i Jw\u0304q (11)\nExamining Kp1q\u03a0 from (11), we see that it effectively replaces half of \u2207\u2113i with \u2207\u2113 1 i, a version obtained through weighted gradient averaging. Now let\u2019s compare the covariance between \u2207\u2113i and \u2207\u21131i. Assume that \u2207\u2113i (1 \u010f i \u010f n) are i.i.d. with the same covariance matrix \u03a3, then G1 is with equal or less noise than G. To show this, denote the covariance matrix of each \u2207\u21131i by\n\u03a31i \u201c Cov \u201e n \u00ff\nj\u201c1 \u03bd\npiq j \u2207\u2113j\n\u0237 \u201c n \u00ff\nj\u201c1 r\u03bdpiqj s 2\u03a3.\nThe total variance of each gradient in G1 (i.e., the trace of \u03a31i) is then\ntracep\u03a31iq \u201c trace \u02c6 n \u00ff\nj\u201c1\n\u201c \u03bd piq j \u20302 \u03a3\n\u02d9 \u201c n \u00ff\nj\u201c1\n\u201c \u03bd piq j \u20302 tracep\u03a3q \u010f tracep\u03a3q.\nThe last inequality follows from the fact that \u0159n j\u201c1r\u03bd piq j s2 \u010f 1, which is a consequence of the Cauchy-Schwarz inequality given that the coefficients \u03bdpiqj form a convex combination.\nOriginally, the covariance information of all data points is embedded in \u2207\u2113i\u2207\u2113Ji for i \u201c 1, 2, . . . , n. An alternative representation is \u2207\u21131i\u2207\u2113 1J i , which prioritizes noise reduction, but sacrifices some covariance information. Both \u2207\u21131i\u2207\u2113 J i and \u2207\u2113i\u2207\u2113 1J i highlight a trade-off. Notably, both the original covariance \u2207\u2113i\u2207\u2113Ji and its noise-reduced counterpart \u2207\u2113 1 i\u2207\u2113 1J i are retained in (11).\nDifference From Averaging Prior to Optimization: Next, we show that such gradient averaging differs from the averaging operation conducted prior to optimization. Let G1 \u201c r\u2207\u211311,\u2207\u2113 1 2, . . . ,\u2207\u2113 1 nsJ such that G1 represents the row-wise convex combination of G. Approximating the Hessian of the MIQP (2), two scenarios emerge: using G that not performing averaging (case 1) and G1 that performs averaging before optimization (case 2).\nCase 1 is the original LR formulation (3). Denote by K below its term corresponding to Kp1q\u03a0 :\nK \u201c pw \u00b4 w\u0304qJGJGpw \u00b4 w\u0304q\n\u201c n \u00ff\ni\u201c1 pwJ\u2207\u2113i\u2207\u2113Ji w \u00b4 wJ\u2207\u2113i\u2207\u2113 J i w\u0304 \u00b4 w\u0304J\u2207\u2113i\u2207\u2113 J i w ` w\u0304J\u2207\u2113i\u2207\u2113 J i w\u0304q (12)\nCase 2 uses the less-noisy row-wise convex combination matrix G1 instead of G. Yet, the original covariance \u2207\u2113i\u2207\u2113Ji is lost: Denote by K 1 the corresponding term, and we have\nK 1 \u201c n \u00ff\ni\u201c1 pwJ\u2207\u21131i\u2207\u2113 1J i w \u00b4 wJ\u2207\u2113 1 i\u2207\u2113 1J i w\u0304 \u00b4 w\u0304J\u2207\u2113 1 i\u2207\u2113 1J i w ` w\u0304J\u2207\u2113 1 i\u2207\u2113 1J i w\u0304q (13)\nInspecting the expressions, it can be observed that Kp1q\u03a0 (also K p2q \u03a0 ) strikes a balance between K and K 1. There are two notable extreme cases for \u03a0:\n1. \u03a0 \u201c diagp1{nq. This corresponds to the LR formulation, as detailed in Section 2. A smaller value of \u03b5 steers the optimization in this direction.\n2. \u03a0 \u201c p1{n2q1 \u00a8 1J. This arises when \u03b5 \u00d1 8, meaning the entropy term holds sway in the OT optimization. Here, mutual information is minimized to ensure an even contribution from data points in the convex combination. Both x1i and y 1 i are the arithmetic means of\ntheir respective sets, and all \u2207\u21131i are equivalent to the averaged gradient over the n points. Importantly, the original covariance remains intact even in this edge case.\nAs n grows indefinitely, the empirical OT formulation from (6) approaches its continuous counterpart given by (5). Intuitively, a large dataset of high-quality training samples makes the empirical fisher a close approximation to the true fisher. In such situations, \u03b5 is set to zero. Brenier\u2019s theorem (Peyre\u0301, 2019) then suggests that the OT plan turns into a monotone map for costs represented by squared Euclidean distances. This means \u03a0 tends towards diagp1{nq. Consequently, the Wasserstein distance formulation reduces to the Euclidean distance formulation, delivering optimal performance with ample data.\nAn advantage of employing the EWR formulation is its inherent capability of gradient averaging. This approach negates the need to manually determine the convex combination coefficients or resort to density estimation to pinpoint the nearest gradient neighbors for averaging. Importantly, this seamless trade-off has an advantage over using Euclidean distance with gradient averaging performed prior to optimization. The reason is that the original covariance information will inevitably be lost in the formulation (13), irrespective of the chosen averaging method. Sample Compexity of W 22 p\u00b5, \u03bdq is narrowed to Op1{ ? nq from Op1{n 14 q by the entropic regularization term. Please see Appendix A.2 for details."
        },
        {
            "heading": "4 ALGORITHM DESIGN",
            "text": "Algorithmic Framework. The algorithm addresses the network pruning problem defined in (5). Drawing inspiration from (Chen et al., 2022), the algorithm incrementally adjusts the sparsity of the weights vector w by using a descending sequence of non-zero elements k0, . . . , kT . During each sparsity level, the weights w and the transportation plan \u03a0 (can be obtained with efficient algorithms; see Appendix A.3) are refined iteratively.\nAlgorithm 1 Sparse Entropic WAsserstein Regression Pruning (SWAP) Input: Number of pruning stages T , pre-pruning weights w\u0304, target sparsity k, regularization pa-\nrameter \u03bb, \u03b5, batches B0,B1 . . . ,BT , optimization step size \u03c4 \u0105 0. Output: Post-pruning weights w, satisfying \u2225w\u2225 \u010f k\n1: Set k0, k1, . . . , kT as a descending sequence, with k0 \u0103 p and kT \u201c k. 2: wp0q \u00d0 w\u0304 3: for t \u00d0 0, 1, . . . , T do 4: Compute G \u201c r\u2207\u21131pw\u0304q, . . .\u2207\u2113npw\u0304qsJ with batch Bt 5: x,y \u00d0 Gwptq,Gw\u0304 6: Compute the pairwise Euclidean distance matrix C between x and y 7: Compute OT planning \u03a0ptq (see Appendix A.3) 8: \u2207Q \u00d0 GJp\u03a0pGwptq \u00b4 Gw\u0304qq ` \u03bbpwptq \u00b4 w\u0304q 9: wpt` 1 2 q \u00d0 wptq \u00b4 \u03c4\u2207Q\n10: wpt`1q \u00d0 Select from wpt` 12 q kt components having largest absolute values; Others zero 11: w\u0304 \u00d0 wpt`1q 12: end for 13: w \u201c wpT`1q\nWeights Optimization. The weights w are optimized using the stochastic gradient descent (SGD) paired with the iterative hard thresholding (IHT) algorithm. We use \u2207Q to represent the derivative of Qpwq for brevity, with its comprehensive derivation in Appendix A.4. The expression is\n\u2207Q \u201c GJp\u03a0pGw \u00b4 Gw\u0304qq ` \u03bbpw \u00b4 w\u0304q. (14)\nFollowing the weight updates driven by SGD (as seen in line 9 of Algorithm 1), the IHT method is applied. Here, the kt components of w with the largest magnitudes are retained, while the remaining are set to zero, ensuring adherence to the sparsity criteria.\nA vital component of the optimization process is the choice of the stepsize \u03c4 (referenced in line 9). Although a straightforward approach might be to set \u03c4 \u201c 1L (where L denotes the Lipschitz constant of Q), better performance can be achieved when the stepsize is optimized using the methodology proposed in (Chen et al., 2022, Algorithm 2). For the quadratic function Q, the Lipschitz constant L is given by L \u201c n\u03bb` \u2225G\u22252op, where \u2225\u00a8\u2225op indicates the foremost singular value.\nLine 10 in Algorithm 1 employs the IHT method that is commonly used in sparse learning, which together with line 9, forms a projected gradient descent algorithm. It finds a sparse representation of the updated gradient in line 9. Intuitively, IHT keeps the dominant model weights and essentially preserves the most impactful aspects of the to-be-trimmed model. Although there exist alternative strategies for refining the IHT solution\u2014including active set updates, coordinate descent, and the Woodbury formula\u2019s back-solving\u2014a discussion on these falls outside the scope of this paper. For in-depth exploration, especially with respect to the specialized case described in (7), one can consult (Bhatia et al., 2015; Chen & Mazumder, 2020; Hazimeh & Mazumder, 2020; Benbaki et al., 2023)."
        },
        {
            "heading": "5 NUMERICAL RESULTS",
            "text": "Our method is compared with several existing SoTA methods including MP (magnitude pruning (Mozer & Smolensky, 1989)), WF (WoodFisher (Singh & Alistarh, 2020)), CBS (Combinatorial Brain Surgeon (Yu et al., 2022)), and LR (i.e. the sparse LR formulation adopted by (Chen et al., 2022)). We refer to our proposed method as EWR (i.e. sparse entropic Wasserstein regression).\nNote that LR is a special instance of EWR, with \u03a0 \u201c diagp1{nq. All the methods are benchmarked on pre-trained neural networks: MLPNet (30K parameters) trained on MNIST (LeCun et al., 1998), ResNet20 (200K parameters) and ResNet50 (25M parameters) (He et al., 2016) trained on CIFAR10 (Krizhevsky et al., 2009), and MobileNetV1 (Howard et al., 2017) (4.2M parameters) trained on ImageNet (Deng et al., 2009). The experiment setup for reproducibility1 is detailed in AppendixA.5. We deliver more experiments results in Appendix A.6-A.9.\nModel Accuracy Performance Benchmarking. Table 1 compares different networks across various sparsity levels, utilizing different methods. MLPNet\u2019s performance on MNIST is consistent across different sparsity levels, with both LR and the proposed EWR method showing superior performance. The advantages of EWR over the others are reflected by the three more challenging tasks ResNet20 and ResNet50 on CIFAR10 and MobileNetV1 on ImageNet, especially in the presence of noisy gradients. In summary, the proposed EWR method consistently outperforms or matches other methods. The LR method performs well at lower sparsity levels but is surpassed otherwise.\n1The code is available on https://github.com/youlei202/Entropic-Wasserstein-Pruning\nRobustness with Noisy Gradient. From Section 3, EWR differs from LR in terms of gradient noise reduction achieved by solving the OT problem to obtain a group of non-trivial data pair weighting coefficients. Hence, LR that has the transportation plan \u03a0 fixed to diagp1{nq naturally serves as a baseline for evaluating the effectiveness of such optimization in terms of robustness against noise. In two noisy scenarios, 10%, and 25%, we evaluate loss at noise levels of \u03c3 and 2\u03c3 across varying sparsity. Tables 2 and 3 contrast the loss difference between LR and EWR. EWR consistently outperforms LR in both ResNet20 and MobileNetV1, most notably in noisy conditions and at higher sparsity. The peak performance difference is 8.13% favoring EWR on MobileNetV1 at 0.75 sparsity with 25% noise. Hence, EWR outperforms LR."
        },
        {
            "heading": "6 CONCLUSIONS AND FUTURE IMPACT",
            "text": "The paper offers a novel formulation based on EWR, which strikes a balance between covariance information preservation and noise reduction. The work suggested promising avenues for applications in large-scale model compression, though it may require further empirical validation and exploration of practical implementations."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF PROPOSITION 1",
            "text": "(Convex Hull Distance Equality) Consider a set S and its convex hull ConvpSq in a Euclidean space, and an arbitrary point x in the space. For any probability measure \u03bd\u0302 on S, we can find a point y1 in ConvpSq as y1 \u201c \u015f y d\u03bdpyq such that }x \u00b4 y1}2 \u201c \u015f\n}x \u00b4 y}2 d\u03bd\u0302pyq, where \u03bd is a measure on ConvpSq.\nProof. We define a function fp\u03bdq \u201c \u2225x\u00b4 y1\u22252, where y1 \u201c \u015f yd\u03bdpyq. This function takes the empirical measure \u03bd as input and computes the squared Euclidean distance between x and y1. Similarly, we define a function gp\u03bdq \u201c \u015f\n\u2225x\u00b4 y\u22252 d\u03bdpyq, which computes the weighted average of squared Euclidean distances between x and the points in the set S according to the probability measure \u03bd.\nWithout loss of generality, let\u2019s assume that S is contained within its convex hull ConvpSq. Then, the right-hand side of the equation to be proved in the theorem takes the minimum and maximum values, respectively, at points within ConvpSq, i.e.,\ngmin \u201c inf \u03bd\n\u017c\n}x\u00b4 y}2 d\u03bdpyq\nand gmax \u201c sup\n\u03bd\n\u017c\n}x\u00b4 y}2 d\u03bdpyq.\nThe function fp\u03bdq takes its maximum value at z, where z is the farthest point inside ConvpSq from x, i.e.,\nfmax \u201c sup \u03bd\n}x\u00b4 y1}2 \u201c }x\u00b4 z}2.\nSimilarly, the minimum value is obtained at y1 \u201c z1, where z1 is the closest point inside ConvpSq to x. The minimum value can reach zero if x is inside ConvpSq. Formally,\nfmin \u201c inf \u00b5\n\u2423 }x\u00b4 z}2 \u02c7 \u02c7 z P ConvpSq ( .\nTo establish that fmax \u011b gmax, we consider the maximum values of the two functions. The function fp\u03bdq takes its maximum value at z, which is the farthest point inside ConvpSq from x. This means that fmax is the squared Euclidean distance between x and z, i.e., fmax \u201c |x \u00b4 z|2. On the other hand, the function gp\u03bdq computes the weighted average of squared Euclidean distances between x and the points in S. The maximum value of gp\u03bdq, denoted as gmax, corresponds to the squared Euclidean distance between x and the farthest point in S. Since ConvpSq contains S, it follows that z is farther from x than any point in S. Therefore, fmax \u011b gmax. Similarly, since ConvpSq contains S and z1 is the closest point in ConvpSq to x, it follows that fmin \u010f gmin. We apply the intermediate value theorem to finish the proof. For any measure \u03bd, let hp\u00b5q \u201c fp\u00b5q \u00b4 gp\u03bdq. Once can find two measures \u00b51 and \u00b52 such that hp\u00b51q \u010f 0 and hp\u00b52q \u011b 0, hence the zero point of h exists, followed by that there is a corresponding value of \u00b5 such that fp\u00b5q \u201c gp\u03bdq. Hence, the conclusion."
        },
        {
            "heading": "A.2 SAMPLE COMPLEXITY",
            "text": "The efficiency of an estimator is often measured by its ability to deliver accurate estimates with fewer samples, a trait referred to as \u2019good sample complexity\u2019. The entropic Wasserstein distance formulation is posited to enhance noise reduction by optimizing this sample complexity. When \u03b5 \u201c 0, the sample complexity of W 22 p\u00b5, \u03bdq stands at Op1{n 1 4 q. Specifically,\nlim n\u00d18\nErW 22 p\u00b5n, \u03bdnq \u00b4W 22 p\u00b5, \u03bdqs \u201c Op1{n 1 4 q\nas per (Nadjahi et al., 2020, Corollary 2). As \u03b5 approaches infinity,W 22 p\u00b5, \u03bdq leverages the beneficial characteristics of the maximum mean discrepancies (MMD) (Gretton et al., 2006), which narrows the sample complexity to Op1{ ? nq according to (Genevay et al., 2019, Theorem 3). The coefficient \u03b5 acts as a regulator, adjusting the sample complexity within this specified range."
        },
        {
            "heading": "A.3 OT PLAN OPTIMIZATION",
            "text": "For an arbitrary wptq, the transportation plan \u03a0ptq is obtained by solving W 22 pxpwptqq, yq. The optimization of the transportation plan \u03a0 in line 7 of Algorithm 1 is based on the Sinkhorn-Knopp algorithm, shown in Algorithm 2 below.\nAlgorithm 2 Sinkhorn-Knopp Algorithm for Regularized OT Input: C (n \u02c6 n Euclidean distance matrix), \u00b5, \u03bd (probability mass of x and y), \u03b5 (regularization\nparameter), \u03f5 (tolerance for stopping criterion) Output: \u03a0\n1: Initialize K \u201c expp\u00b4C{\u03b5q, u \u201c 1n{n, v \u201c 1n{n 2: repeat 3: u \u201c \u00b5{Kv 4: v \u201c \u03bd{KJu 5: until sup \u2423 \u2225a \u00b4 diagpuqKv\u22258 , \u2225\u2225b \u00b4 diagpvqKJu\u2225\u22258( \u0103 \u03f5 6: return \u03a0 \u201c diagpuq \u00a8 K \u00a8 diagpvq\nThe Sinkhorn-Knopp algorithm is an iterative method used to solve regularized OT problems, aiming to find a transportation plan that minimizes total cost while adhering to specific source and target probability distributions. In the algorithm, an initial matrix K is formed using the exponential of the negative cost matrix divided by a regularization parameter, and two vectors are initialized as uniform distributions. These vectors are then iteratively updated using rules derived from the Kullback-Leibler divergence, seeking to align the row and column sums of the resulting matrix with the given source and target distributions. The process continues until the maximum difference between the actual and desired row and column sums falls below a specified tolerance, ensuring the solution is feasible.\nNote that x and y are one-dimensional projections of high-dimensional random variables \u2207\u2113, of which the dimension is the number of parameters of the neural network, a.k.a. p. Prior research has demonstrated that when certain moderate criteria are met, the distribution of lower-dimensional versions of high-dimensional random variables tends to closely follow a Gaussian (or normal) distribution (Sudakov, 1978; Reeves, 2017; Nadjahi et al., 2021). In the research by (Janati et al., 2020), it is highlighted that given x and y obeying the Gaussian distribution, W 22 px, yq can be reduced to a concise closed-form solution. As illustrated in Algorithm 3, the OT plan can be directly computed relying solely on the statistics of the empirical distributions x and y.\nAlgorithm 3 Closed Form Algorithm for Regularized OT\nInput: \u03c8 \u201c a \u03b5 2 , a \u201c meanpxq, b \u201c meanpyq, \u03c3 2 a \u201c varpxq, \u03c32b \u201c varpyq, d\u03c8 \u201c p4\u03c3a\u03c32b\u03c3a`\u03c84q 1 2 Output:\n\u03a0 \u201e N \u02c6\u201e\na b\n\u0237\n,\n\u201e\n\u03c32a 1 2\u03c3ad\u03c8\u03c3 \u00b41 a\n1 2\u03c3ad\u03c8\u03c3 \u00b41 a \u03c3 2 b\n\u0237\u02d9\nSpecifically, the OT plan, represented as \u03a0, is governed by the mean and variance of both x and y, in tandem with the regularization parameter \u03b5. To derive the n\u02c6n matrix \u03a0, pxi, yiq, where i spans from 1 to n, are employed to extract n samples from the distribution produced in Algorithm 3.\nAs depicted in Figure 1, a comparative evaluation of the two algorithms in terms of attaining the OT objective, W 22 , is presented. The entropic regularization term\u2019s value has been omitted considering its non-impact on the pruning optimization of w, given a constant \u03a0. Notably, the disparity between the two algorithms in their objective optimization magnifies as \u03b5 increases. Typically, in real-world applications, the value of \u03b5 oscillates between 0 and 10 (and we use \u03b5 \u201c 1 most frequently in our experiments). The variance in the performance of the two algorithms concerning OT planning remains trivial, echoing our practical observations during the algorithmic implementation in this study.\nA.4 DERIVATIVE OF Qpwq\nLet\u2019s start by revisiting the function Qpwq:\nQpwq \u201c # n \u00ff\ni\u201c1\nn \u00ff j\u201c1 \u2225xipwq \u00b4 yj\u22252 \u03c0ij\n+\n` \u03bb \u2225w \u00b4 w\u0304\u22252 (15)\nGiven:\nxipwq \u201c \u2207\u2113Ji w yj \u201c \u2207\u2113Jj w\u0304\nDifferentiating Qpwq with respect to w:\n\u2207Qpwq \u201c \u2207 \u00ab# n \u00ff\ni\u201c1\nn \u00ff\nj\u201c1\n\u2225\u2225\u2225\u2207\u2113Ji w \u00b4 \u2207\u2113Jj w\u0304\u2225\u2225\u22252 \u03c0ij + ` \u03bb \u2225w \u00b4 w\u0304\u22252 ff\n\u201c 2\u03bbpw \u00b4 w\u0304q ` \u2207 \u00ab# n \u00ff\ni\u201c1\nn \u00ff\nj\u201c1\n\u2225\u2225\u2225\u2207\u2113Ji w \u00b4 \u2207\u2113Jj w\u0304\u2225\u2225\u22252 \u03c0ij +ff\n(16)\nFor the gradient of the inner term, consider:\n\u2207\n\u00ab#\nn \u00ff\ni\u201c1\nn \u00ff\nj\u201c1\n\u2225\u2225\u2225\u2207\u2113Ji w \u00b4 \u2207\u2113Jj w\u0304\u2225\u2225\u22252 \u03c0ij +ff \u201c n \u00ff\ni\u201c1\nn \u00ff j\u201c1 2\u03c0ijp\u2207\u2113Ji w \u00b4 \u2207\u2113 J j w\u0304q\u2207\u2113i (17)\nExpressing in Matrix Form. Given matrices:\nG \u201c\n\u00bb\n\u2014\n\u2014\n\u2013\n\u2207\u21131 \u2207\u21132\n... \u2207\u2113n\nfi\nffi\nffi\nfl\n\u03a0 \u201c Matrix with elements \u03c0ij\nConsider the double summation: n \u00ff\ni\u201c1\nn \u00ff j\u201c1 2\u03c0ijp\u2207\u2113Ji w \u00b4 \u2207\u2113 J j w\u0304q\u2207\u2113i\nFor each i, the term \u2207\u2113Ji w projects vector w onto \u2207\u2113i. To compute this for every i simultaneously, it is Gw. This results in an n\u02c6 1 column vector. Similarly, for each j, it is Gw\u0304. This also produces an n\u02c6 1 column vector. The difference between these two projections for each i and j is Gw\u00b4Gw\u0304. This results in an n\u02c61 column vector. To incorporate the \u03c0ij weights, we have\n\u03a0 pGw \u00b4 Gw\u0304q\nThis operation gives an n\u02c61 column vector, where each element is a summation over j for the term \u03c0ijp\u2207\u2113Ji w \u00b4 \u2207\u2113 J j w\u0304q.\nTo finalize the summation, we multiply it by GJ, yielding\nGJ p\u03a0 pGw \u00b4 Gw\u0304qq . (18)\nCombining (16), (17), and (18), we get:\n\u2207Qpwq \u201c 2\u03bbpw \u00b4 w\u0304q ` 2GJp\u03a0pGw \u00b4 Gw\u0304qq \u201c 2rGJp\u03a0pGw \u00b4 Gw\u0304qq ` \u03bbpw \u00b4 w\u0304qs (19)"
        },
        {
            "heading": "A.5 EXPERIMENT SETUP",
            "text": "The models MLPNet, ResNet20, and MobileNetV1 underwent a pre-training phase of 100 epochs utilizing 4 NVIDIA Tesla V100 32 GB GPUs connected with NVlink. The training times were approximately 1 hour for MLPNet, 3 hours for ResNet20, and 1 day for MobileNetV1. Pre-pruning accuracy levels for these models are detailed under their respective names in Table 1. For the pruning process, we either utilized 2 NVIDIA Tesla V100 32 GB GPUs with NVlink or a single Tesla A100 PCIE (available in 40 or 80 GB configurations). It\u2019s worth emphasizing the time-intensive nature of training and pruning the MobileNetV1 on ImageNet; thus, harnessing multiple GPUs for concurrent training is highly recommended.\nIn Table 1, we set the pruning stage of LR and EWR to be 15 for MLPNet and ResNet20 and 10 for MobileNetV1. The sparsity k1, k2, . . . kT in Algorithm 1 is arranged following an exponential gradual pruning schedule\nkt \u201c kT ` pk0 \u00b4 kT q \u02c6 1 \u00b4 t T\n\u02d93\nwith the initial sparsity k0 set to zero. The fisher sample size setup follows (Chen et al., 2022, Table 2), shown as Table 4 of this paper below.\nIn Table 2, Table 3, Table 6, Table 7, and additional results provided in the Appendix, sparsity is set using a linear gradual pruning strategy, progressing from 0 to 0.75 or 0.95 across ten distinct stages for MLPNet and ResNet, and from 0 to 0.75 across eight distinct stages for MobileNetV1. The values are computed with linear incremental steps, from zero to the target sparsity. Notably, all recorded loss values are captured immediately post-pruning, devoid of any subsequent fine-tuning.\nThis approach ensures that the loss values exclusively reflect the impact of the pruning algorithms, without being clouded by external factors. Contrasting with Table 1, where each row represents a full pruning cycle, the loss values here are recorded across the ten incremental pruning stages. The empirical fisher is computed based on 100 samples with a batch size of 1.\nIn calibrating noise for data in neural networks, we start with a well-trained network. First, we calculate the standard deviation \u03c3 of the network\u2019s derivative. Then, we add Gaussian noise with zero mean to the data. After adding the noise, the standard deviation of the network\u2019s derivative changes to a new value, \u03c31, which is always greater than sigma. The goal is to adjust the standard deviation of the Gaussian noise so that \u03c31 becomes \u03c31 \u201c \u03c3 ` \u03c3 (referred to as noise level being \u03c3) or \u03c31 \u201c \u03c3 ` 2\u03c3 (referred to as noise level being 2\u03c3). Throughout the paper, we set \u03bb in the optimization problem (6) to 0.01. The regularization multiplier \u03b5 is set to 1 unless specified otherwise. The noise level \u03c3 is set to be the standard deviation of the original gradients."
        },
        {
            "heading": "A.6 ABLATION STUDY",
            "text": "The ablation study centered on ResNet20 gives insight into the influence of the entropic regularization multiplier, \u03b5, on pruning. The aim is to understand how different values of this parameter affect the loss Lpwq during the pruning process. Observations from Loss Values. From Table 5, we can glean the following points. For higher sparsity levels (0.95, 0.84, 0.74), EWR Loss consistently outperforms the LR Loss, except for \u03b5 \u201c\n8. The optimum performance of the EWR Loss, across varying sparsities, tends to occur when \u03b5 is set at values between 1 and 2. As \u03b5 tends towards infinity, the EWR loss exceeds the LR loss. This phenomenon aligns with the neighborhood interpolation elaborated upon in Section 3. Specifically, incorporating an excessive number of distant data points into the interpolation detrimentally impacts performance. For lower sparsity levels (from 0.63 downwards), the differences between LR Loss and EWR Loss across different \u03b5 values are minuscule.\nLoss Reduction Insights. Referencing Figure 2, it\u2019s evident that the loss reduction (difference between LR and EWR) is more pronounced at higher sparsity levels. For a sparsity of 0.84, \u03b5 \u201c 1 demonstrates the most significant loss reduction. The trend of EWR loss for different \u03b5 values is consistent across varying sparsity levels. The impact \u03b5 is clearly visible at higher sparsity levels. For mid to high sparsity levels, lower values of \u03b5 (specifically 1 and 2) seem to achieve the best balance in terms of loss.\nFigure 3 illustrates EWR Loss for various \u03b5 values against sparsity, there is a clear trend of decreasing loss as sparsity decreases, consistent across all \u03b5 values. Particularly at \u03b5 \u201c 8, there\u2019s a pronounced increase in loss at higher sparsity, suggesting that extreme entropic regularization might hinder optimal pruning. However, at low sparsity levels, the loss is consistent across all \u03b5, emphasizing the minimal impact of \u03b5 in limited pruning scenarios. This underscores the significance of choosing an appropriate \u03b5, balancing between regularization and pruning efficiency.\nThe ablation study provides insights into the role of the parameter \u03b5 in pruning. Its influence diminishes at low sparsity levels but becomes significant at extremely high sparsity. For ResNet20, an optimal range for \u03b5 appears to be between 1 and 2, ensuring effective pruning. Generally, while the exact choice of \u03b5 doesn\u2019t drastically alter the pruning outcome, exceedingly high values, such as 1 \u02c6 108, might lead to less than ideal pruning decisions in very sparse networks."
        },
        {
            "heading": "A.7 ANALYSIS OF PRUNING STAGE VERSUS PERFORMANCE",
            "text": "Analysis of Loss. Figure 4 depicts the relationship between the sparsity and the difference in loss (LR - EWR) under the influence of varying noise levels.\nThe left plot, representing data with a 10% noise level, depicts a prominent decrease in the difference of loss as sparsity is reduced for both noise levels 2\u03c3 and \u03c3. Notably, in the 2\u03c3 noise scenario, there is a sharp decline in loss difference when sparsity transitions from 0.95 to 0.53. Beyond this threshold, the loss difference stabilizes and remains near zero. For the noise level \u03c3, the decrease\n10% Noisy Data\n25% Noisy Data\n10% Noisy Data\n25% Noisy Data\nappears more gradual. It is of interest to observe that the loss difference diminishes more swiftly for 2\u03c3 compared to \u03c3. The error bars offer insights into data variability, showcasing broader intervals at elevated sparsity levels, which suggests greater unpredictability at these levels, particularly in the 2\u03c3 setting.\nThe right plot represents data with 25% noise. While the trends in loss difference share similarities with the 10% noisy data, the exact values differ slightly. In this 25% noise setting, the decline in loss difference between the two noise levels is similar. The error bars, indicating confidence intervals,\nPruning Stage vs. Accuracy (%)\nhighlight the increased variability at larger sparsity levels. This variability is most noticeable for the 2\u03c3 setting at the top sparsity levels.\nFigure 5 depicts the difference in loss between LR and EWR algorithms applied to the MobileNetV1. In both two cases 10% and 25% noisy data, as sparsity increases, the loss difference diminishes, particularly when sparsity is approximately 0.42 or less. The 10% noisy data reveals that the difference in loss for noise level 2\u03c3 is marginally higher than that of \u03c3 for most sparsity levels. In contrast, the 25% noisy data sometimes exhibits a reversal in this trend, especially at the highest sparsity level of 0.74.\nConfidence intervals provided at select data points underscore the reliability of the data, with the 25% noisy data showing tighter intervals compared to the 10% scenario. This infers a higher consistency in the measurements or a minimized effect of outliers in the 25% noisy data. These plots accentuate the interplay between sparsity, noise, and the performance difference between the two al-\nPruning Stage vs. Accuracy (%)\ngorithms, emphasizing the significance of noise levels in algorithmic performance evaluations across various sparsity conditions. The underlying rationale is that as the noise intensity is too high, the performance of both LR and EWR tends to deteriorate. Consequently, their performances converge, resulting in a diminished differential between the two.\nAnalysis of Accuracy. The box plots in Figure 6 together with Table 6 show how the ResNet20 model performs at different pruning stages in terms of top-1 accuracy (also referred to as accuracy or the overall accuracy) and top-5 accuracy difference. Top-5 accuracy means any of our model\u2019s top 5 highest probability answers match with the expected answer. The difference is computed for EWR, using LR as the baseline. When we look closely, we can see patterns that help us understand how much pruning affects the model.\nIn the earlier pruning stages, both the overall accuracy and the top 5 accuracy differences for EWR are minimal, suggesting that EWR remains closely aligned with the baseline LR in terms of per-\nformance. This minimal deviation can be viewed as an advantage, as it implies that even with simplifications brought by pruning, EWR retains its effectiveness compared to LR.\nHowever, as pruning intensifies, the patterns begin to reveal more about EWR\u2019s relative strengths. Although the accuracy difference increases, this increase in the context of the baseline suggests that EWR might be better at handling intense pruning than LR. Notably, in the top 5 accuracy, the discrepancies remain relatively low compared to the overall accuracy up until the more aggressive pruning stages. This suggests that while the model\u2019s primary prediction confidence may decrease, the true class is still frequently among its top 5 predictions. In essence, EWR seems to retain a broader spectrum of potential correct classifications even when it\u2019s uncertain about the primary prediction.\nFigure 7 together with Table 7 shows the performance difference between EWR and LR using MobileNetV1 across various pruning stages. In the first plot showcasing the overall accuracy, there\u2019s an evident upward trend in the median accuracy difference as pruning intensifies. Initially, the difference is marginal, which suggests that EWR\u2019s performance closely mirrors LR during the early\nScalability Test on ResNet50\npruning stages. However, as we progress to the 5th and 6th stages, the accuracy difference widens considerably, indicating that EWR might be outpacing LR. The 7th stage is particularly striking, with a median accuracy difference surpassing 6%, pointing towards a potential superiority of EWR in extreme pruning scenarios.\nThe second plot focuses on the top 5 accuracy differences, presenting a more varied pattern. The early stages indicate a tight performance race between EWR and LR. By the 2nd stage, there\u2019s a minor dip, hinting at EWR\u2019s possible underperformance. This scenario changes by the 3rd stage as EWR regains momentum. The later stages, especially the 5th and 6th, denote a significant rise in the median difference in EWR\u2019s favor. Much like the accuracy chart, the 7th stage is distinct, with EWR showcasing a considerable advantage in the top 5 accuracy over LR."
        },
        {
            "heading": "A.8 ALGORITHM SCALABILITY",
            "text": "In this section, we analyze the scalability of Algorithm 1 with respect to the number of model parameters involved in pruning. The result is shown in Figure 8. It can be observed that the execution time scales linearly with the number of pruning parameters. The extra cost of solving the OT is marginal. Theoretically, one could derive this linear scalability by inspecting Line 9, which is the most time-consuming step. The required operations can be decomposed as sequential operations: matrix-vector multiplications Gw and Gw\u0304 in Opnpq, the vector subtraction Gw \u00b4 Gw\u0304 in Opnq, a matrix-matrix multiplication \u03a0pGw \u00b4 Gw\u0304q in Opn2q, a matrix transposition and multiplication with G in Opnpq, and the vector subtraction and scalar multiplication \u03bbpw \u00b4 w\u0304q in Oppq. Thus, the overall complexity isOpnpq, with p significantly larger than n practically. Given fixed fisher sample size n, the loop of Algorithm 1 scales linearly with the number of pruning parameters p."
        },
        {
            "heading": "A.9 OPTIMAL TRANSPORT VISUALIZATION",
            "text": "In this section, we showcase the optimized OT plan denoted as \u03a0. This was derived from the pruning applied to ResNet20. Figure 9 displays two data sets: 1) txiuni\u201c1: This is Gw where w is the pruned model. 2) tyiuni\u201c1: This represents Gw\u0304 where w\u0304 is the original unpruned model.\nIn Figure 10, the matrix \u03a0 is shown through vibrant heatmaps that adjust with varying \u03b5 values. For small \u03b5, the majority of the data remains near a diagonal. As \u03b5 increases, there\u2019s a broader data distribution, notably for central data points."
        }
    ],
    "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
    "year": 2024
}