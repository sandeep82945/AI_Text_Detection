{
    "abstractText": "Deep neural networks have become a standard building block for designing models that can perform multiple dense computer vision tasks such as depth estimation and semantic segmentation thanks to their ability to capture complex correlations in high dimensional feature space across tasks. However, the cross-task correlations that are learned in the unstructured feature space can be extremely noisy and susceptible to overfitting, consequently hurting performance. We propose to address this problem by introducing a structured 3D-aware regularizer which interfaces multiple tasks through the projection of features extracted from an image encoder to a shared 3D feature space and decodes them into their task output space through differentiable rendering. We show that the proposed method is architecture agnostic and can be plugged into various prior multi-task backbones to improve their performance; as we evidence using standard benchmarks NYUv2 and PASCAL-Context.",
    "authors": [],
    "id": "SP:2acb0336d84eff8575795ce5ca8d03cc28b57166",
    "references": [
        {
            "authors": [
                "Titas Anciukevi\u010dius",
                "Zexiang Xu",
                "Matthew Fisher",
                "Paul Henderson",
                "Hakan Bilen",
                "Niloy J Mitra",
                "Paul Guerrero"
            ],
            "title": "Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Deblina Bhattacharjee",
                "Sabine S\u00fcsstrunk",
                "Mathieu Salzmann"
            ],
            "title": "Vision transformer adapters for generalizable multitask learning",
            "venue": "arXiv preprint arXiv:2308.12372,",
            "year": 2023
        },
        {
            "authors": [
                "Hakan Bilen",
                "Andrea Vedaldi"
            ],
            "title": "Integrated perception with recurrent multi-task neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Felix JS Bragman",
                "Ryutaro Tanno",
                "Sebastien Ourselin",
                "Daniel C Alexander",
                "Jorge Cardoso"
            ],
            "title": "Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "David Bruggemann",
                "Menelaos Kanakis",
                "Stamatios Georgoulis",
                "Luc Van Gool"
            ],
            "title": "Automated search for resource-efficient branched multi-task networks",
            "venue": "arXiv preprint arXiv:2008.10292,",
            "year": 2020
        },
        {
            "authors": [
                "David Bruggemann",
                "Menelaos Kanakis",
                "Anton Obukhov",
                "Stamatios Georgoulis",
                "Luc Van Gool"
            ],
            "title": "Exploring relational context for multi-task dense prediction",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Shengqu Cai",
                "Anton Obukhov",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Pix2nerf: Unsupervised conditional p-gan for single image to neural radiance fields translation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Rich Caruana"
            ],
            "title": "Multitask learning",
            "venue": "Machine learning,",
            "year": 1997
        },
        {
            "authors": [
                "Eric R Chan",
                "Marco Monteiro",
                "Petr Kellnhofer",
                "Jiajun Wu",
                "Gordon Wetzstein"
            ],
            "title": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "year": 2022
        },
        {
            "authors": [
                "Eric R Chan",
                "Koki Nagano",
                "Matthew A Chan",
                "Alexander W Bergman",
                "Jeong Joon Park",
                "Axel Levy",
                "Miika Aittala",
                "Shalini De Mello",
                "Tero Karras",
                "Gordon Wetzstein"
            ],
            "title": "Generative novel view synthesis with 3d-aware diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Xianjie Chen",
                "Roozbeh Mottaghi",
                "Xiaobai Liu",
                "Sanja Fidler",
                "Raquel Urtasun",
                "Alan Yuille"
            ],
            "title": "Detect what you can: Detecting and representing objects using holistic models and body parts",
            "year": 1971
        },
        {
            "authors": [
                "Zhao Chen",
                "Vijay Badrinarayanan",
                "Chen-Yu Lee",
                "Andrew Rabinovich"
            ],
            "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Zhao Chen",
                "Jiquan Ngiam",
                "Yanping Huang",
                "Thang Luong",
                "Henrik Kretzschmar",
                "Yuning Chai",
                "Dragomir Anguelov"
            ],
            "title": "Just pick a sign: Optimizing deep multitask models with gradient sign dropout",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Zitian Chen",
                "Yikang Shen",
                "Mingyu Ding",
                "Zhenfang Chen",
                "Hengshuang Zhao",
                "Erik G Learned-Miller",
                "Chuang Gan"
            ],
            "title": "Mod-squad: Designing mixtures of experts as modular multi-task learners",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Sumanth Chennupati",
                "Ganesh Sistu",
                "Senthil Yogamani",
                "Samir A Rawashdeh"
            ],
            "title": "Multinet++: Multistream feature aggregation and geometric loss strategy for multi-task learning",
            "venue": "In CVPR Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "David Eigen",
                "Rob Fergus"
            ],
            "title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture",
            "venue": "In IEEE International Conference on Computer Vision, pp",
            "year": 2015
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "year": 2010
        },
        {
            "authors": [
                "Zhiwen Fan",
                "Rishov Sarkar",
                "Ziyu Jiang",
                "Tianlong Chen",
                "Kai Zou",
                "Yu Cheng",
                "Cong Hao",
                "Zhangyang Wang"
            ],
            "title": "M3vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator",
            "venue": "co-design. Neurips,",
            "year": 2022
        },
        {
            "authors": [
                "Jiatao Gu",
                "Lingjie Liu",
                "Peng Wang",
                "Christian Theobalt"
            ],
            "title": "Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis",
            "venue": "arXiv preprint arXiv:2110.08985,",
            "year": 2021
        },
        {
            "authors": [
                "Michelle Guo",
                "Albert Haque",
                "De-An Huang",
                "Serena Yeung",
                "Li Fei-Fei"
            ],
            "title": "Dynamic task prioritization for multitask learning",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Pengsheng Guo",
                "Chen-Yu Lee",
                "Daniel Ulbricht"
            ],
            "title": "Learning to branch for multi-task learning",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal",
                "Roberto Cipolla"
            ],
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Iasonas Kokkinos"
            ],
            "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Abhijit Kundu",
                "Kyle Genova",
                "Xiaoqi Yin",
                "Alireza Fathi",
                "Caroline Pantofaru",
                "Leonidas J Guibas",
                "Andrea Tagliasacchi",
                "Frank Dellaert",
                "Thomas Funkhouser"
            ],
            "title": "Panoptic neural fields: A semantic object-aware neural scene representation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Hong Li",
                "Hakan Bilen"
            ],
            "title": "Knowledge distillation for multi-task learning",
            "venue": "In ECCV Workshop on Imbalance Problems in Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Hong Li",
                "Xialei Liu",
                "Hakan Bilen"
            ],
            "title": "Learning multiple dense prediction tasks from partially annotated data",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Hong Li",
                "Xialei Liu",
                "Hakan Bilen"
            ],
            "title": "Universal representations: A unified look at multiple task and domain learning",
            "venue": "arXiv preprint arXiv:2204.02744,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Liang",
                "Elliot Meyerson",
                "Risto Miikkulainen"
            ],
            "title": "Evolutionary architecture search for deep multitask networks",
            "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: High-resolution text-to-3d content creation",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Bo Liu",
                "Xingchao Liu",
                "Xiaojie Jin",
                "Peter Stone",
                "Qiang Liu"
            ],
            "title": "Conflict-averse gradient descent for multi-task learning",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Liyang Liu",
                "Yi Li",
                "Zhanghui Kuang",
                "Jing-Hao Xue",
                "Yimin Chen",
                "Wenming Yang",
                "Qingmin Liao",
                "Wayne Zhang"
            ],
            "title": "Towards impartial multi-task learning",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Shikun Liu",
                "Edward Johns",
                "Andrew J Davison"
            ],
            "title": "End-to-end multi-task learning with attention",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Yajing Liu",
                "Yuning Lu",
                "Hao Liu",
                "Yaozu An",
                "Zhuoran Xu",
                "Zhuokun Yao",
                "Baofeng Zhang",
                "Zhiwei Xiong",
                "Chenguang Gui"
            ],
            "title": "Hierarchical prompt learning for multi-task learning",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Lombardi",
                "Tomas Simon",
                "Jason Saragih",
                "Gabriel Schwartz",
                "Andreas Lehrmann",
                "Yaser Sheikh"
            ],
            "title": "Neural volumes: Learning dynamic renderable volumes from images",
            "venue": "ACM Trans. Graph.,",
            "year": 2019
        },
        {
            "authors": [
                "Kevis-Kokitsi Maninis",
                "Ilija Radosavovic",
                "Iasonas Kokkinos"
            ],
            "title": "Attentive single-tasking of multiple tasks",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Nelson Max"
            ],
            "title": "Optical models for direct volume rendering",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 1995
        },
        {
            "authors": [
                "Moustafa Meshry",
                "Dan B Goldman",
                "Sameh Khamis",
                "Hugues Hoppe",
                "Rohit Pandey",
                "Noah Snavely",
                "Ricardo Martin-Brualla"
            ],
            "title": "Neural rerendering in the wild",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Elliot Meyerson",
                "Risto Miikkulainen"
            ],
            "title": "Pseudo-task augmentation: From deep multitask learning to intratask sharing\u2014and back",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Ishan Misra",
                "Abhinav Shrivastava",
                "Abhinav Gupta",
                "Martial Hebert"
            ],
            "title": "Cross-stitch networks for multi-task learning",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Lars Mescheder",
                "Michael Oechsle",
                "Andreas Geiger"
            ],
            "title": "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Ruder"
            ],
            "title": "An overview of multi-task learning in deep neural networks",
            "venue": "arXiv preprint arXiv:1706.05098,",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Joachim Bingel",
                "Isabelle Augenstein",
                "Anders S\u00f8gaard"
            ],
            "title": "Latent multi-task architecture learning",
            "venue": "In AAAI,",
            "year": 2019
        },
        {
            "authors": [
                "Johannes Lutz Sch\u00f6nberger",
                "Jan-Michael Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Johannes Lutz Sch\u00f6nberger",
                "Enliang Zheng",
                "Marc Pollefeys",
                "Jan-Michael Frahm"
            ],
            "title": "Pixelwise view selection for unstructured multi-view stereo",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "Ozan Sener",
                "Vladlen Koltun"
            ],
            "title": "Multi-task learning as multi-objective optimization",
            "venue": "NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Nathan Silberman",
                "Derek Hoiem",
                "Pushmeet Kohli",
                "Rob Fergus"
            ],
            "title": "Indoor segmentation and support inference from rgbd images",
            "venue": "In European conference on computer vision,",
            "year": 2012
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Justus Thies",
                "Felix Heide",
                "Matthias Nie\u00dfner",
                "Gordon Wetzstein",
                "Michael Zollhofer"
            ],
            "title": "Deepvoxels: Learning persistent 3d feature embeddings",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Gjorgji Strezoski",
                "Nanne van Noord",
                "Marcel Worring"
            ],
            "title": "Many task learning with task routing",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Mihai Suteu",
                "Yike Guo"
            ],
            "title": "Regularizing deep multi-task networks using orthogonal gradients",
            "venue": "arXiv preprint arXiv:1912.06844,",
            "year": 2019
        },
        {
            "authors": [
                "Justus Thies",
                "Michael Zollh\u00f6fer",
                "Matthias Nie\u00dfner"
            ],
            "title": "Deferred neural rendering: Image synthesis using neural textures",
            "year": 2019
        },
        {
            "authors": [
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Bert De Brabandere",
                "Luc Van Gool"
            ],
            "title": "Branched multi-task networks: deciding what layers to share",
            "venue": "bmvc,",
            "year": 2020
        },
        {
            "authors": [
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Luc Van Gool"
            ],
            "title": "Mti-net: Multi-scale task interaction networks for multi-task learning",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Wouter Van Gansbeke",
                "Marc Proesmans",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Multi-task learning for dense prediction tasks: A survey",
            "year": 2021
        },
        {
            "authors": [
                "Jingdong Wang",
                "Ke Sun",
                "Tianheng Cheng",
                "Borui Jiang",
                "Chaorui Deng",
                "Yang Zhao",
                "Dong Liu",
                "Yadong Mu",
                "Mingkui Tan",
                "Xinggang Wang"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "year": 2020
        },
        {
            "authors": [
                "Dan Xu",
                "Wanli Ouyang",
                "Xiaogang Wang",
                "Nicu Sebe"
            ],
            "title": "Pad-net: Multi-tasks guided prediction-anddistillation network for simultaneous depth estimation and scene parsing",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Hanrong Ye",
                "Dan Xu"
            ],
            "title": "Inverted pyramid multi-task transformer for dense scene understanding",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Hanrong Ye",
                "Dan Xu"
            ],
            "title": "Taskprompter: Spatial-channel multi-task prompting for dense scene understanding",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Hanrong Ye",
                "Dan Xu"
            ],
            "title": "Taskexpert: Dynamically assembling multi-task representations with memorial mixture-of-experts",
            "venue": "In ICCV, pp",
            "year": 2023
        },
        {
            "authors": [
                "Alex Yu",
                "Vickie Ye",
                "Matthew Tancik",
                "Angjoo Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Tianhe Yu",
                "Saurabh Kumar",
                "Abhishek Gupta",
                "Sergey Levine",
                "Karol Hausman",
                "Chelsea Finn"
            ],
            "title": "Gradient surgery for multi-task learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jingbo Zhang",
                "Xiaoyu Li",
                "Ziyu Wan",
                "Can Wang",
                "Jing Liao"
            ],
            "title": "Text2nerf: Text-driven 3d scene generation with neural radiance fields",
            "venue": "arXiv preprint arXiv:2305.11588,",
            "year": 2023
        },
        {
            "authors": [
                "Yu Zhang",
                "Qiang Yang"
            ],
            "title": "A survey on multi-task learning",
            "venue": "arXiv preprint arXiv:1707.08114,",
            "year": 2017
        },
        {
            "authors": [
                "Zhenyu Zhang",
                "Zhen Cui",
                "Chunyan Xu",
                "Zequn Jie",
                "Xiang Li",
                "Jian Yang"
            ],
            "title": "Joint task-recursive learning for semantic segmentation and depth estimation",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Zhenyu Zhang",
                "Zhen Cui",
                "Chunyan Xu",
                "Yan Yan",
                "Nicu Sebe",
                "Jian Yang"
            ],
            "title": "Pattern-affinitive propagation across depth, surface normal and semantic segmentation",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Shuaifeng Zhi",
                "Tristan Laidlow",
                "Stefan Leutenegger",
                "Andrew J Davison"
            ],
            "title": "In-place scene labelling and understanding with implicit scene representation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ling Zhou",
                "Zhen Cui",
                "Chunyan Xu",
                "Zhenyu Zhang",
                "Chaoqun Wang",
                "Tong Zhang",
                "Jian Yang"
            ],
            "title": "Pattern-structure diffusion for multi-task learning",
            "venue": "In CVPR,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Learning models that can perform multiple tasks coherently while efficiently sharing computation across tasks are the central focus of multi-task learning (MTL) (Caruana, 1997). Deep neural networks (DNNs), which have become the standard solution for various computer vision problems, provide at least two key advantages for MTL. First, they allow for sharing a significant portion of features and computation across multiple tasks, hence they are computationally efficient for MTL. Second, thanks to their hierarchical structure and high-dimensional representations, they can capture complex cross-task correlations at several abstraction levels (or layers).\nYet designing multi-task DNNs that perform well in all tasks is extremely challenging. This often requires careful engineering of mechanisms that allow for the sharing of relevant features between tasks, while also maintaining task-specific features. Many multi-task methods (Vandenhende et al., 2021) can be decomposed into shared feature encoder across all tasks and following task-specific decoders to generate predictions. The technical challenge here is to strike a balance between the portion of the shared and task-specific features to achieve good performance-computation trade-off. To enable more flexible feature sharing and task-specific adaptation, Liu et al. (2019) propose to use \u2018soft\u2019 task-specific attention modules appended to the shared encoder that effectively shares most features and parameters across the tasks while adapting them to each task through light-weight attention modules. However, these attention modules are limited to share features across tasks only within each layer (or scale). Hence, recent works (Vandenhende et al., 2020b; Bruggemann et al., 2021) propose to aggregate features from different layers and to capture cross-task relations from the multi-scale features. More recently, Ye & Xu (2022) demonstrates that capturing long-range spatial correlations across multiple tasks achieves better MTL performance through use of vision transformer modules (Dosovitskiy et al., 2020).\nIn this paper we propose an approach orthogonal to existing MTL methods and hypothesize that highdimensional and unstructured features, shared across tasks, are prone to capturing noisy cross-task correlations and hence hurt performance. To this end, we propose regulating the feature space of shared representations by introducing a structure that is valid for all considered tasks. In particular, we look at dense prediction computer vision problems such as monocular depth estimation, semantic segmentation where each input pixel is associated with a target value, and represent their shared intermediate features in a 3D-aware feature space by leveraging recent advances in 3D modeling and differentiable rendering (Niemeyer et al., 2020; Mildenhall et al., 2020; Chan et al., 2022; 2023;\nAnciukevic\u030cius et al., 2023). Our key intuition is that the physical 3D world affords us inherent and implicit consistency between various computer vision tasks. Hence, by projecting high-dimensional features to a structured 3D-aware space, our method eliminates multiple geometrically-inconsistent cross-task correlations.\nTo this end, we propose a novel regularization method that can be plugged into diverse prior MTL architectures for dense vision problems including both convolutional (Vandenhende et al., 2020b) and transformer (Ye & Xu, 2022) networks. Prior MTL architectures are typically composed of a shared feature extractor (encoder) and multiple task-specific decoders. Our regularizer, instantiated as a deep network, connects to the output of the shared feature encoder, maps the encodings to three groups of feature maps and further uses these to construct a tri-plane representing planes x\u2212y, x\u2212z, y\u2212z, in similar fashion to Chan et al. (2022). We are able to query any 3D position by projecting it onto the tri-plane and retrieve a corresponding feature vector through bi-linear interpolation across the planes, passing them through light-weight, task-specific decoders and then rendering the outputs as predictions for each task by raycasting, as in Mildenhall et al. (2020). Once the model has been optimized by minimizing each task loss for both the base model and regularizer, the regularizer is removed. Hence our method does not bring any additional inference cost. Importantly, the regularizer does not require multiple views for each scene and learns 3D-aware representations from a single view. Additionally, the model generalizes to unseen scenes, as the feature encoder is shared across different scenes.\nOur method relates to both MTL and 3D modelling work. It is orthogonal to recent MTL contributions that focus rather on designing various cross-task interfaces (Vandenhende et al., 2020a; Liu et al., 2019), or optimization strategies that may obtain more balanced performance across tasks (Kendall et al., 2018; Chen et al., 2018). Alternatively, our main focus is to learn better MTL representations by enforcing 3D structure upon them, through our 3D-aware regularizer. We show that our method can be incorporated with several recent MTL methods and improve their performance. Most related to ours, Zhi et al. (2021) and Kundu et al. (2022) extend the well-known neural radiance field (NeRF) (Mildenhall et al., 2020) to semantic segmentation and panoptic 3D scene reconstruction, respectively. First, unlike them, our main focus is to jointly perform multiple tasks that include depth estimation, boundary detection, surface normal estimation, in addition to semantic segmentation. Second, uniquely, our method does not require multiple views. Finally, our method is not scene-specific, can learn multiple scenes in a single model and generalizes to unseen scenes.\nTo summarize, our main contribution is a novel 3D-aware regularization method for the MTL of computer vision problems. Our method is architecture agnostic, does not bring any additional computational cost for inference, and yet can significantly improve the performance of state-of-the-art MTL models as evidenced under two standard benchmarks; NYUv2 and PASCAL-Context."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Multi-task Learning MTL (Caruana, 1997) commonly aims to learn a single model that can accurately generate predictions for multiple desired tasks, given an input (see Figure 2 (a)). We refer to Ruder (2017); Zhang & Yang (2017); Vandenhende et al. (2021) for comprehensive literature review. The prior works in computer vision problems can be broadly divided into two groups. The first group focuses on improving network architecture via more effective information sharing across tasks (Kokkinos, 2017; Ruder et al., 2019; Vandenhende et al., 2020a; Liang et al., 2018; Bragman et al., 2019; Strezoski et al., 2019; Xu et al., 2018; Zhang et al., 2019; Bruggemann et al., 2021; Bilen & Vedaldi, 2016; Zhang et al., 2018; Xu et al., 2018), by designing cross-task attention mechanisms Misra et al. (2016), task-specific attention modules (Liu et al., 2019; Bhattacharjee et al., 2023), cross-tasks feature interaction (Ye & Xu, 2022; Vandenhende et al., 2020b), gating strategies or mixture of experts modules (Bruggemann et al., 2020; Guo et al., 2020; Chen et al., 2023; Fan et al., 2022; Ye & Xu, 2023b), visual prompting (Ye & Xu, 2023a; Liu et al., 2023) etc. The second group aims to address the unbalanced optimization for joint minimization of multiple task-specific loss functions, where each may exhibit varying characteristics. This is achieved through either actively changing loss term weights (Kendall et al., 2018; Liu et al., 2019; Guo et al., 2018; Chen et al., 2018; Lin et al., 2019; Sener & Koltun, 2018; Liu et al., 2021b) and / or modifying the gradients of loss functions, w.r.t. shared network weights to alleviate task conflicts (Yu et al., 2020; Liu et al., 2021a; Chen et al., 2020; Chennupati et al., 2019; Suteu & Guo, 2019) and / or knowledge distillation (Li &\nBilen, 2020; Li et al., 2022b). Unlike these methods, our work aims to improve MTL performance by regularizing deep networks through the introduction of 3D-aware representations (see Figure 2 (c)).\nNeural Rendering Our approach also relates to the line of work that learns a 3D scene representations for performing novel view synthesis (Lombardi et al., 2019; Meshry et al., 2019; Sitzmann et al., 2019; Thies et al., 2019; Mildenhall et al., 2020; Chan et al., 2021; Cai et al., 2022; Gu et al., 2021), creating explicit 3D object (Lin et al., 2023), view editing (Zhang et al., 2023). Prior methods with few exceptions can represent only a single scene per model, require many calibrated views, or are not able to perform other tasks than novel view synthesis such as semantic segmentation, depth estimation (see Figure 2 (b)). PixelNeRF (Yu et al., 2021) conditions a neural radiance field (NeRF) (Mildenhall et al., 2020) on image inputs through an encoder, allows for the modeling of multiple scenes jointly and generalizes to unseen scenes, however, the work focuses only on synthesizing novel views. Zhi et al. (2021) extend the standard NeRF pipeline through a parallel semantic segmentation branch to jointly encode semantic information of the 3D scene, and obtain 2D segmentations by rendering the scene for a given view using raycasting. However, their model is scene-specific and does not generalize to unseen scenes. Panoptic Neural Fields (Kundu et al., 2022) predict a radiance field that represents the color, density, instance and category label of any 3D point in a scene through the combination of multiple encoders for both background and each object instance. The work was designed for predicting those tasks only on novel views of previously seen scenes, hence it cannot be applied to new scenes without further training on them and is also limited to handle only rigid objects (c.f . non-rigid, deformable). In contrast, our method can be used to efficiently predict multiple tasks in novel scenes, without any such restrictions on object type, can be trained from a single view and is further not limited to a fixed architecture or specific set of tasks. Finally, our work harnesses efficient triplane 3D representations from (Chan et al., 2022) that is originally designed to generate high-quality, 3D-aware representations from a collection of single-view images. Our method alternatively focuses on the joint learning of dense vision problems and leverages 3D understanding to bring a beneficial structure to the learned representations."
        },
        {
            "heading": "3 METHOD",
            "text": "We next briefly review the problem settings for MTL and neural rendering to provide required background and then proceed to describe our proposed method."
        },
        {
            "heading": "3.1 MULTI-TASK LEARNING",
            "text": "Our goal is to learn a model y\u0302 that takes in an RGB image I as input and jointly predicts ground-truth labels Y = {y1, . . . ,yT } for T tasks . In this paper, we focus on dense prediction problems such as semantic segmentation, depth estimation where input image and labels have the same dimensionality. While it is possible to learn an independent model for each task, a more efficient design involves sharing a large portion of the computation across the tasks, via a common feature encoder f . Encoder f then takes in an image as input and outputs a high-dimensional feature map which has smaller width and height than the input. In this setting, the encoder is followed by multiple task-specific decoders ht that each ingests f(I) to predict corresponding task labels i.e., ht(f(I)), as depicted in Fig. 1 (a). Given a labeled training set D with N image-label pairs, the model weights can be\noptimized as:\nmin f,{ht}Tt=1\n1\nN \u2211 (I,Y )\u2208D \u2211 yt\u2208Y Lt(ht \u25e6 f(I)),yt), (1)\nwhere Lt is the loss function for task t, i.e., cross entropy loss for semantic segmentation, L1 loss for depth estimation. We provide more details in Sec. 4,"
        },
        {
            "heading": "3.2 3D-AWARE MULTI-TASK LEARNING",
            "text": "An ideal feature extractor f is expected to extract both task-agnostic and task-specific information, towards enabling the following task-specific decoders to solve their respective target tasks accurately. However, in practice, the combination of high-dimensional feature space and highly non-linear mappings from input to output is prone to overfitting to data and learning of noisy correlations. To mitigate these issues, we propose a new 3D-aware regularization technique that first maps extracted features to 3D neural codes, projects them to task-specific fields and finally renders them to obtain predictions for each target task through differentiable rendering. In the regularization, outputs for all tasks are conditioned on observations that lie on a low-D manifold (the density (Mildenhall et al., 2020)), enforcing 3D consistency between tasks.\n3D representations. Training the state-of-the-art MTL models (e.g. Vandenhende et al. (2020b); Ye & Xu (2022)) on high resolution input images for multiple dense prediction tasks simultaneously is computation and memory intensive. Hence, naively mapping their multi-scale high-resolution features to 3D is not feasible due to memory limitations in many standard GPUs. Hence, we adopt the hybrid explicit-implicit tri-plane representations of Chan et al. (2022). In particular, we first feed I into a shared encoder and obtain a W\u00d7H\u00d7C-dimensional feature map where H and W are the height and width. Then, through a tri-plane encoder e, we project the feature map to three explicit W\u00d7H\u00d7C \u2032 dimensional feature maps, exy, eyz, exz , that represent axis aligned orthogonal feature planes. We can query any 3D coordinate (x, y, z) by projecting it onto each plane, then retrieve the respective features from three planes via bi-linear interpolation and finally aggregate features using summation to obtain the 3D representation (e(x, y, z) = exy(x, y)+eyz(y, z)+exz(x, z)) as in Chan et al. (2022).\nNeural task fields. For each task, we use an additional light-weight network nt, implemented as a small MLP, to estimate both a density value and task-specific vector, where this element pair can be denoted as a neural task field for the aggregated 3D representation. We are then able to render these quantities via neural volume rendering Max (1995); Mildenhall et al. (2020) through a differentiable renderer r to obtain predictions for each task.\nIn particular, for the tasks including semantic segmentation, part segmentation, surface normal estimation, boundary detection, saliency prediction, we estimate prediction for each point of a given ray (e.g. logits for segmentation) and integrate them over the ray. We normalize the predictions after rendering for surface normal and apply softmax after rendering for segmentation tasks. For depth estimation task, we use the raw prediction as depth maps.\nIn summary the sequence of mappings can be summarized as; firstly mapping the shared feature encoding f(I) to tri-plane features through e, further mapping it to neural task fields through nt, finally rendering these to obtain predictions for task t, i.e. gt \u25e6 f(I) where gt = r \u25e6 nt \u25e6 e is the regularizer for task t.\nDiscussion. While novel view synthesis methods such as NeRF require the presence of multiple views and knowledge of the camera matrices, here we assume a single view to extract the corresponding 3D representations and to render them as task predictions. For rendering, we assume that the camera is orthogonal to image center here, and depict r as a function that takes only the output of nt but not the viewpoint as input. In the experiments, we show that our model consistently improves the MTL performance, even when learned from a single view per scene, thanks to the 3D structure of representations imposed by our regularizer.\nOptimization. We measure the mismatch between ground-truth labels and the predictions obtained from our 3D-aware model branch and use this signal to jointly optimize the model along with the original common task losses found in Eq. (1):\nmin f,{ht,gt}Tt=1\n1\nN \u2211 (I,Y )\u2208D \u2211 yt\u2208Y Lt(ht \u25e6 f(I),yt) + \u03b1t Lt(gt \u25e6 f(I)),yt)\ufe38 \ufe37\ufe37 \ufe38 3D-aware regularizer , (2)\nwhere \u03b1t is a hyperparameter balancing loss terms.\nCross-view consistency. Though our 3D-aware regularizer does not require multiple views of the same scene to be presented, it can be easily extended to penalize the cross-view inconsistency on the predictions when multiple views of the same scene are available, e.g. video frames. Let I and I \u2032 be two views of a scene with their camera viewpoints V and V \u2032, respectively. In addition to the regularization term in Eq. (2), here we also compute predictions for I \u2032 but by using I as the input and render it by using the relative camera transformation \u2206V from V to V \u2032. We then penalize the inconsistency between this prediction and ground-truth labels of I \u2032:\nmin f,{ht,gt}Tt=1\n1\nN \u2211 {(I,Y ),\n(I\u2032,Y \u2032)}\u2208D \u2211 yt\u2208Y, y\u2032t\u2208Y \u2032 Lt(ht\u25e6f(I),yt)+\u03b1t Lt(gt \u25e6 f(I)),yt)\ufe38 \ufe37\ufe37 \ufe38 3D-aware regularizer +\u03b1\u2032t Lt(g\u2206Vt \u25e6 f(I)),y\u2032t)\ufe38 \ufe37\ufe37 \ufe38 cross-view regularizer ,\n(3) where \u03b1t and \u03b1\u2032t are hyperparameters balancing loss terms and we set \u03b1t = \u03b1 \u2032 t. Note that in this case gt is a function of \u2206V , as the relative viewpoint \u2206V is used by the renderer r."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Here we first describe the benchmarks used and our implementation details, then present a quantitative and qualitative analysis of our method."
        },
        {
            "heading": "4.1 DATASET",
            "text": "NYUv2 (Silberman et al., 2012): It contains 1449 RGB-D images, sampled from video sequences from a variety of indoor scenes, which we use to perform four tasks; namely 40-class semantic segmentation, depth estimation, surface normal estimation and boundary detection in common with prior work (Ye & Xu, 2022; Bruggemann et al., 2021). Following the previous studies, we use the true depth data recorded by the Microsoft Kinect and surface normals provided in the prior work (Eigen & Fergus, 2015) for depth estimation and surface normal estimation tasks.\nNYUv2 video frames: In addition to the standard data split, NYUv2 (Silberman et al., 2012) also provides additional video frames1 which are labeled only for depth estimation. Only for the cross-view consistent regularization experiments, we merge the original split with video frames, and train multi-task learning models by minimizing loss on available labeled tasks, i.e. all four tasks on the original data and only the depth on video frames. To estimate the relative camera pose \u2206V between the frames, we use COLMAP (Scho\u0308nberger & Frahm, 2016; Scho\u0308nberger et al., 2016).\n1https://www.kaggle.com/datasets/soumikrakshit/nyu-depth-v2\nPASCAL-Context (Chen et al., 2014): PASCAL (Everingham et al., 2010) is a commonly used image benchmark for dense prediction tasks. We use the data splits from PASCAL-Context (Chen et al., 2014) which has annotations for semantic segmentation, human part segmentation and semantic edge detection. Additionaly, following (Vandenhende et al., 2021; Ye & Xu, 2022), we also consider surface normal prediction and saliency detection using the annotations provided by Vandenhende et al. (2021)."
        },
        {
            "heading": "4.2 IMPLEMENTATION DETAILS",
            "text": "Our regularizer is architecture agnostic and can be applied to different architectures. In our experiments, it is incorporated into two state-of-the-art (SotA) MTL methods; MTI-Net (Vandenhende et al., 2020b) and InvPT (Ye & Xu, 2022) which builds on the convolutional neural network (CNN), HRNet-48 (Wang et al., 2020) and transformer based ViT-L (Dosovitskiy et al., 2020) respectively. In all experiments, we follow identical training, evaluation protocols (Ye & Xu, 2022). We append our 3D-aware regularizer to these two models using two convolutional layers, followed by BatchNorm and ReLU, to project feature maps to the tri-plane space resulting in a common size and channel width (64). A 2-layer MLP is used to render each task as in Chan et al. (2022). We use identical hyper-parameters; learning rate, batch size, loss weights, loss functions, pre-trained weights, optimizer, evaluation metrics as MTI-Net and InvPT, respectively. We jointly optimize task-specific losses and losses arising from our 3D regularization. During inference, the regularizer is discarded. We refer to the supplementary material for further details. Our code and models will be made public based upon acceptance."
        },
        {
            "heading": "4.3 RESULTS",
            "text": "Comparison with SotA methods. We compare our method with the SotA MTL methods on NYUv2 and PASCAL-Context datasets and report results in Tab. 1 and Tab. 2, respectively. Following Bruggemann et al. (2021), we use HRNet-48 (Wang et al., 2020) as backbone when comparing to CNN based methods; Cross-Stitch (Misra et al., 2016), PAP (Zhang et al., 2019), PSD (Zhou et al., 2020), PAD-Net (Xu et al., 2018), ATRC (Bruggemann et al., 2021), MTI-Net (Vandenhende et al., 2020b). We use ViT-L (Dosovitskiy et al., 2020) as backbone when comparing to InvPT (Ye & Xu, 2022).\nIn NYUv2 (see Table 1), when using HRNet-48 as backbone, we observe that ATRC (Bruggemann et al., 2021) and MTI-Net (Vandenhende et al., 2020b) obtain the best performance. By incorporating our method to MTI-Net (Vandenhende et al., 2020b), we improve its performance on all tasks and outperform all CNN based MTL methods. In comparison, the InvPT approach (Ye & Xu, 2022) achieves superior MTL performance by leveraging both the ViT-L (Dosovitskiy et al., 2020) backbone and multi-scale cross-task interaction modules. Our method is also able to quantitatively improve upon the base InvPT by integrating our proposed 3D-aware regularizer, e.g. +1.31 mIoU on Seg. The results evidence that both geometric information is beneficial for jointly learning multiple dense prediction tasks.\nTable 2 depicts experimental results on the PASCAL-Context dataset where previous method results are reproduced from Ye & Xu (2022). We also report results from our local implementation of the MTI-Net, denoted by \u2018MTI-Net*\u2019, where we found that our implementation obtains better performance. We observe that the performance of existing methods is better than in the previous NYUv2 experiment (Tab. 1), as PASCAL-Context has significantly more images available for training. From Tab. 2 we observe that our method, incorporating our proposed regularizer to MTINet (Vandenhende et al., 2020b), can improve the performance on all tasks with respect to our base\nMTI-Net implementation, e.g. +2.29 mIoU on Seg, and obtains the best performance on most tasks compared with MTL methods that use the HRNet-48 backbone. As in NYUv2, the InvPT model (Ye & Xu, 2022) achieves better performance on a majority of tasks over existing methods. Our method with InvPT again obtains improvements on all tasks over InvPT, e.g. +1.51 mIoU on PartSeg and +1.00 odsF on Boundary. This result further suggests that our method is effective for enabling the MTL network to learn beneficial geometric cues and that the technique can be incorporated with various MTL methods for comprehensive task performance improvements.\n3D regularizer with multiple views. Here we investigate whether learning stronger 3D consistency across multiple views with our regularizer further improves the performance in multiple tasks. To this end, we merge the NYUv2 dataset with the additional video frames possessing only depth annotation and train the base InvPT, our method and our method with cross-view consistency on the merged data. For InvPT, we train the model by minimizing losses over the labeled tasks. We train our method by minimizing both the supervised losses and the 3D-aware regularization loss. We further include the cross-view consistency loss. To regularize multi-view consistency, we sample two views of the same scene and we feed the first view and transform the 3D representations to the second view, rendering the depth, which is aligned with the ground-truth of the second view.\nResults of the three approaches are reported in Tab. 3. Compared with the results in Tab. 1, we can see that including video frames for training improves the performance of InvPT on depth and surface normal tasks while yielding comparable performance on remaining tasks. We also see that our method obtains consistent improvement over the InvPT on four tasks with applying 3D-aware regularization using only a single view. Adding the cross-view consistency loss term to our method, we can observe further performance improvement beyond using only single view samples. This suggests that better 3D geometry learning through multi-view consistency is beneficial, however, the improvements are modest. We argue that coarse 3D scene information obtained from single views can be sufficient to learn more structured and regulate inter-task relations.\nWe also note that this experimental setting is also related to the recent MTL work (Li et al., 2022a) that can learn from partially annotated data by exploiting cross-task relations. However we here focus on an orthogonal direction and believe our complementary works have scope to be integrated together. We leave this as a promising direction for future work.\nComparison with auxiliary network heads. Prior work suggests that the addition of auxiliary heads performing the same task with identical head architectures yet with different weight initializations can be further helpful to performance (Meyerson & Miikkulainen, 2018). To verify whether the improvements obtained by our regularizer is not due to the additional heads solely but introduced 3D structure, we conduct a comparison with our baseline and report results in Tab. 4. The results show that adding auxiliary heads (\u2018InvPT + Aux. Heads\u2019) does not necessarily lead to better performance on all tasks; e.g. Seg, whereas our method can be seen to outperform this baseline on all tasks suggesting the benefit of introducing 3D-aware structure across tasks.\n3D-aware regularizer predictions. Though we discard the regularizer during inference, the regularizer can also be used to produce predictions for the tasks. To investigate their estimation utility, we report task performance using the default task specific heads ht, the regularizer output (regularizer)\nand finally using the averaged predictions over two in Tab. 5. We observe that the regularizer alone estimations are worse than the task-specific heads, however, the performance of their averaged output yields marginal improvements to the boundary detection task. The lower performance of using the regularizer alone may be explained by the fact that the rendering image size is typically small (e.g. we render 56\u00d772 images for NYUv2). The addition of a super-resolution module, similar to previous work (Chan et al., 2022), can further improve the quality of the related predictions. We leave this to future work.\nTasks for 3D-aware regularizer. Our regularizer renders predictions for all learning tasks by default. We further study the effect of isolating different tasks for rendering with the regularizer in Tab. 6. Specifically; we jointly optimize the MTL network with a regularizer that renders only one individual task predictions. From Tab. 6 we observe that rendering different individual tasks in the regularizer leads to only marginally differing results and yet using all tasks for rendering can help to better learn the geometric information for multi-task learning, i.e. \u2018All tasks\u2019 obtains the best performance on the majority of tasks.\nUsing less data. We further investigate the performance gain obtained by our method when trained with fewer training samples. To this end, we train the baseline InvPT (Ye & Xu, 2022) and our method on 25% and 50% of the NYUv2 data after randomly subsampling the original training set. The results are reported in Tab. 7. As expected, more training samples result in better performance in all cases. Our method consistently outperforms the baseline on all tasks in all label regimes with higher margins when more data is available. As the full NYUv2 training set is relatively small, contains only 795 images, our regularizer learns better 3D consistency across tasks from more data too, hence resulting enhanced task performance."
        },
        {
            "heading": "4.4 QUALITATIVE RESULTS",
            "text": "We visualize the task predictions for both our method and the base InvPT method on an NYUv2 sample in Fig. 3. Our method can be observed to estimate better predictions consistently for four tasks. For example, our method estimates more accurate predictions around the boundary of the refrigerator, stove and less noisy predictions within objects like curtain and stove. The geometric information learned in our method helps distinguish different adjacent objects, avoids noisy predictions within object boundaries and also improves the consistency across tasks as in the regularizer, all tasks predictions are rendered based on the same density.\nWe then visualize the predictions of our method\u2019s regularizer and the task-specific decoder NYUv2 in Fig. 3. As shown in the figure, our regularizer can also render high quality predictions for different tasks yet it was observed to obtain worse quantitative performance than the task-specific decoders. As\ndiscussed, this is due to the rendering image size being usually small (e.g. we render 56\u00d772 images for NYUv2)."
        },
        {
            "heading": "5 CONCLUSION AND LIMITATIONS",
            "text": "We demonstrate that encouraging 3D-aware interfaces between different related tasks including depth estimation, semantic segmentation and surface normal estimation consistently improves the multitask performance when incorporated to the recent MTL techniques in two standard dense prediction benchmarks. Our model can be successfully used with different backbone architectures and does not bring any additional inference costs. Our method has limitations too. Despite the efficient 3D modeling through the triplane encodings, representing 3D representations for higher resolution 3D volumes is still expensive in terms of memory or computational cost. Rendering specular objects will require different rendering or objects with high frequency 3D details may require more accurate 3D modeling. Though our proposed method obtains performance gains consistently over multiple tasks, we balance loss functions with fixed cross-validated hyperparameters, while it would be more beneficial to use adaptive loss balancing strategies (Kendall et al., 2018) or discarding conflicting gradients (Liu et al., 2021a). Finally, in the cross-view consistency experiments where only some of the images are labeled for all the tasks, our method does not make use of semi-supervised learning or view-consistency for the tasks with missing labels which can be further improve the performance of our model."
        }
    ],
    "year": 2023
}