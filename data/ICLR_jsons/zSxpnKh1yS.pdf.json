{
    "abstractText": "Unsupervised reinforcement learning (URL) aims to learn general skills for unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL by maximizing the mutual information between states and skills but lacks sufficient theoretical analysis, e.g., how well its learned skills can initialize a downstream task\u2019s policy. Our new theoretical analysis in this paper shows that the diversity and separability of learned skills are fundamentally critical to downstream task adaptation but MISL does not necessarily guarantee these properties. To complement MISL, we propose a novel disentanglement metric LSEPIN. Moreover, we build an information-geometric connection between LSEPIN and downstream task adaptation cost. For better geometric properties, we investigate a new strategy that replaces the KL divergence in information geometry with Wasserstein distance. We extend the geometric analysis to it, which leads to a novel skill-learning objective WSEP. It is theoretically justified to be helpful to downstream task adaptation and it is capable of discovering more initial policies for downstream tasks than MISL. We finally propose another Wasserstein distance-based algorithm PWSEP that can theoretically discover all optimal initial policies.",
    "authors": [],
    "id": "SP:527184680d949f4fe567199c6b2a8bbf492875a5",
    "references": [
        {
            "authors": [
                "Joshua Achiam",
                "Harrison Edwards",
                "Dario Amodei",
                "Pieter Abbeel"
            ],
            "title": "Variational option discovery algorithms",
            "venue": "arXiv preprint arXiv:1807.10299,",
            "year": 2018
        },
        {
            "authors": [
                "Lucas Nunes Alegre",
                "Ana L.C. Bazzan",
                "Bruno C. da Silva"
            ],
            "title": "Optimistic linear support and successor features as a basis for optimal policy transfer",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Brandon Amos",
                "J. Zico Kolter"
            ],
            "title": "Optnet: Differentiable optimization as a layer in neural networks",
            "venue": "In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Stephen P. Boyd",
                "Lieven Vandenberghe"
            ],
            "title": "Convex Optimization",
            "venue": "ISBN 978-0-521-83378-3. doi: 10.1017/CBO9780511804441. URL https://web",
            "year": 2014
        },
        {
            "authors": [
                "Yuri Burda",
                "Harrison Edwards",
                "Amos J. Storkey",
                "Oleg Klimov"
            ],
            "title": "Exploration by random network distillation",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Victor Campos",
                "Alexander Trott",
                "Caiming Xiong",
                "Richard Socher",
                "Xavier Gir\u00f3-i-Nieto",
                "Jordi Torres"
            ],
            "title": "Explore, discover and learn: Unsupervised discovery of state-covering skills",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Eric Carlen",
                "Wilfrid Gangbo"
            ],
            "title": "Constrained steepest ascent in the 2-wasserstein metric",
            "venue": "Annals of Mathematics, 157:807\u2013846,",
            "year": 2003
        },
        {
            "authors": [
                "Thomas M. Cover",
                "Joy A. Thomas"
            ],
            "title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)",
            "year": 2006
        },
        {
            "authors": [
                "Robert Dadashi",
                "L\u00e9onard Hussenot",
                "Matthieu Geist",
                "Olivier Pietquin"
            ],
            "title": "Primal wasserstein imitation learning",
            "venue": "arXiv preprint arXiv:2006.04678,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Kien Do",
                "Truyen Tran"
            ],
            "title": "Theory and evaluation metrics for learning disentangled representations, 2019a. URL https://arxiv.org/abs/1908.09961",
            "year": 1908
        },
        {
            "authors": [
                "Kien Do",
                "Truyen Tran"
            ],
            "title": "Theory and evaluation metrics for learning disentangled representations",
            "venue": "arXiv preprint arXiv:1908.09961,",
            "year": 2019
        },
        {
            "authors": [
                "Ishan Durugkar",
                "Steven Hansen",
                "Stephen Spencer",
                "Volodymyr Mnih"
            ],
            "title": "Wasserstein distance maximizing intrinsic control",
            "venue": "CoRR, abs/2110.15331,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Abhishek Gupta",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Diversity is all you need: Learning skills without a reward function",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Ruslan Salakhutdinov",
                "Sergey Levine"
            ],
            "title": "The information geometry of unsupervised reinforcement learning",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Carlos Florensa",
                "Yan Duan",
                "Pieter Abbeel"
            ],
            "title": "Stochastic neural networks for hierarchical reinforcement learning",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Karol Gregor",
                "Danilo Jimenez Rezende",
                "Daan Wierstra"
            ],
            "title": "Variational intrinsic control",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Nikolaus Hansen",
                "Andreas Ostermeier"
            ],
            "title": "Completely derandomized self-adaptation in evolution strategies",
            "venue": "Evol. Comput.,",
            "year": 2001
        },
        {
            "authors": [
                "Steven Hansen",
                "Will Dabney",
                "Andr\u00e9 Barreto",
                "David Warde-Farley",
                "Tom Van de Wiele",
                "Volodymyr Mnih"
            ],
            "title": "Fast task inference with variational intrinsic successor features",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Shuncheng He",
                "Yuhang Jiang",
                "Hongchang Zhang",
                "Jianzhun Shao",
                "Xiangyang Ji"
            ],
            "title": "Wasserstein unsupervised reinforcement learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Olivier Henaff"
            ],
            "title": "Data-efficient image recognition with contrastive predictive coding",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "J.D. Hunter"
            ],
            "title": "Matplotlib: A 2d graphics environment",
            "venue": "Computing in Science & Engineering,",
            "year": 2007
        },
        {
            "authors": [
                "Jaekyeom Kim",
                "Seohong Park",
                "Gunhee Kim"
            ],
            "title": "Unsupervised skill discovery with bottleneck option learning",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bahare Kiumarsi",
                "Kyriakos G Vamvoudakis",
                "Hamidreza Modares",
                "Frank L Lewis"
            ],
            "title": "Optimal and autonomous control using reinforcement learning: A survey",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2017
        },
        {
            "authors": [
                "Soheil Kolouri",
                "Kimia Nadjahi",
                "Umut Simsekli",
                "Roland Badeau",
                "Gustavo Rohde"
            ],
            "title": "Generalized sliced wasserstein distances",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yehuda Koren"
            ],
            "title": "On spectral graph drawing",
            "venue": "Computing and Combinatorics, 9th Annual International Conference,",
            "year": 2003
        },
        {
            "authors": [
                "Michael Laskin",
                "Denis Yarats",
                "Hao Liu",
                "Kimin Lee",
                "Albert Zhan",
                "Kevin Lu",
                "Catherine Cang",
                "Lerrel Pinto",
                "Pieter Abbeel"
            ],
            "title": "URLB: unsupervised reinforcement learning benchmark",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Michael Laskin",
                "Hao Liu",
                "Xue Bin Peng",
                "Denis Yarats",
                "Aravind Rajeswaran",
                "Pieter Abbeel"
            ],
            "title": "CIC: contrastive intrinsic control for unsupervised skill discovery",
            "venue": "CoRR, abs/2202.00161,",
            "year": 2022
        },
        {
            "authors": [
                "Lisa Lee",
                "Benjamin Eysenbach",
                "Emilio Parisotto",
                "Eric P. Xing",
                "Sergey Levine",
                "Ruslan Salakhutdinov"
            ],
            "title": "Efficient exploration via state marginal matching",
            "year": 1906
        },
        {
            "authors": [
                "Hao Liu",
                "Pieter Abbeel"
            ],
            "title": "Behavior from the void: Unsupervised active pre-training",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Hao Liu",
                "Pieter Abbeel"
            ],
            "title": "APS: active pretraining with successor features",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Playing atari with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1312.5602,",
            "year": 2013
        },
        {
            "authors": [
                "Pierre-Yves Oudeyer",
                "Frederic Kaplan"
            ],
            "title": "What is intrinsic motivation? a typology of computational approaches",
            "venue": "Frontiers in neurorobotics,",
            "year": 2009
        },
        {
            "authors": [
                "Seohong Park",
                "Jongwook Choi",
                "Jaekyeom Kim",
                "Honglak Lee",
                "Gunhee Kim"
            ],
            "title": "Lipschitz-constrained unsupervised skill discovery, 2022b. URL https://arxiv.org/abs/2202.00914",
            "year": 2022
        },
        {
            "authors": [
                "Seohong Park",
                "Kimin Lee",
                "Youngwoon Lee",
                "Pieter Abbeel"
            ],
            "title": "Controllability-aware unsupervised skill discovery",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport",
            "venue": "Found. Trends Mach. Learn.,",
            "year": 2200
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Mark Rowland",
                "Jiri Hron",
                "Yunhao Tang",
                "Krzysztof Choromanski",
                "Tamas Sarlos",
                "Adrian Weller"
            ],
            "title": "Orthogonal estimation of wasserstein distances",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Filippo Santambrogio"
            ],
            "title": "Optimal transport for applied mathematicians",
            "venue": "Birka\u0308user, NY,",
            "year": 2015
        },
        {
            "authors": [
                "Max Schwarzer",
                "Ankesh Anand",
                "Rishab Goel",
                "R Devon Hjelm",
                "Aaron Courville",
                "Philip Bachman"
            ],
            "title": "Data-efficient reinforcement learning with self-predictive representations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ramanan Sekar",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "Pieter Abbeel",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Planning to explore via self-supervised world models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Archit Sharma",
                "Shixiang Gu",
                "Sergey Levine",
                "Vikash Kumar",
                "Karol Hausman"
            ],
            "title": "Dynamics-aware unsupervised discovery of skills",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. nature,",
            "year": 2016
        },
        {
            "authors": [
                "Harshinder Singh",
                "Neeraj Misra",
                "Vladimir Hnizdo",
                "Adam Fedorowicz",
                "Eugene Demchuk"
            ],
            "title": "Nearest neighbor estimates of entropy",
            "venue": "American Journal of Mathematical and Management Sciences,",
            "year": 2003
        },
        {
            "authors": [
                "Harshinder Singh",
                "Neeraj Misra",
                "Vladimir Hnizdo",
                "Adam Fedorowicz",
                "Eugene Demchuk"
            ],
            "title": "Nearest neighbor estimates of entropy",
            "venue": "American Journal of Mathematical and Management Sciences,",
            "year": 2003
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Igor Babuschkin",
                "Wojciech M Czarnecki",
                "Micha\u00ebl Mathieu",
                "Andrew Dudzik",
                "Junyoung Chung",
                "David H Choi",
                "Richard Powell",
                "Timo Ewalds",
                "Petko Georgiev"
            ],
            "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Yifan Wu",
                "George Tucker",
                "Ofir Nachum"
            ],
            "title": "The laplacian in RL: learning representations with efficient approximations",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Denis Yarats",
                "Rob Fergus",
                "Alessandro Lazaric",
                "Lerrel Pinto"
            ],
            "title": "Reinforcement learning with prototypical representations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "LEARNING",
                "SKILL LEARNING SETTING Kim"
            ],
            "title": "2021) adopted metrics SEPIN@k and WSEPIN (Do & Tran, 2019a) from representation learning to measure the disentanglement of the skill latent. They focused on the informativeness and separability (independencies) between different dimensions of the representation. SEPIN@k is the top k average",
            "year": 2019
        },
        {
            "authors": [
                "Hansen"
            ],
            "title": "Abbeel (2021b) are the practical algorithms closer to the setting (optimizing eq",
            "year": 2020
        },
        {
            "authors": [
                "WSEP. In He"
            ],
            "title": "2022), approaches like Sliced Wasserstein Distances (SWD) (Kolouri",
            "year": 2022
        },
        {
            "authors": [
                "APT Liu",
                "Abbeel (2021a",
                "RND Burda"
            ],
            "title": "MISL methods from Lee et al",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Unsupervised reinforcement learning (URL) aims to learn general skills for unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL by maximizing the mutual information between states and skills but lacks sufficient theoretical analysis, e.g., how well its learned skills can initialize a downstream task\u2019s policy. Our new theoretical analysis in this paper shows that the diversity and separability of learned skills are fundamentally critical to downstream task adaptation but MISL does not necessarily guarantee these properties. To complement MISL, we propose a novel disentanglement metric LSEPIN. Moreover, we build an information-geometric connection between LSEPIN and downstream task adaptation cost. For better geometric properties, we investigate a new strategy that replaces the KL divergence in information geometry with Wasserstein distance. We extend the geometric analysis to it, which leads to a novel skill-learning objective WSEP. It is theoretically justified to be helpful to downstream task adaptation and it is capable of discovering more initial policies for downstream tasks than MISL. We finally propose another Wasserstein distance-based algorithm PWSEP that can theoretically discover all optimal initial policies."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Reinforcement learning (RL) has drawn growing attention by its success in autonomous control (Kiumarsi et al., 2017), Go (Silver et al., 2016) and video games (Mnih et al., 2013; Vinyals et al., 2019). However, a primary limitation of the current RL is its high sample complexity. Inspired by the successful pretrain-finetune paradigm in other deep learning fields like natural language processing (Radford et al., 2019; Devlin et al., 2019) and computer vision (Henaff, 2020; He et al., 2020), there has been growing work studying the pretraining of RL. RL agent receives no task-related reward during pretraining and learns by its intrinsic motivations (Oudeyer & Kaplan, 2009). Some of these intrinsic motivations can help the agent to learn representations of the observations (Schwarzer et al., 2021) and some learn the dynamics model (Ha & Schmidhuber, 2018; Sekar et al., 2020). In this work, we focus on Unsupervised RL (URL) that learns a set of skills without external reward and the learned skills are expected to be quickly adapted to unseen downstream tasks.\nA common approach for skill discovery of URL is Mutual Information Skill Learning (MISL) (Eysenbach et al., 2022) that maximizes the mutual information between state and skill latent (Eysenbach et al., 2019; Florensa et al., 2017; Hansen et al., 2020; Liu & Abbeel, 2021b). The intuition is that by maximizing this mutual information the choice of skills can effectively affect where the states are distributed so that these skills could be potentially used for downstream tasks. There are more algorithms using objectives modified on this mutual information. For example, Lee et al. (2019); Liu & Abbeel (2021b) added additional terms for better exploration, and Sharma et al. (2020); Park et al. (2022a) focus on modified input structure to prepare the agent for specific kinds of downstream tasks.\nDespite the popularity of MISL, there has been little theoretical analysis of how well the MISLlearned skills can be applied as downstream task initializations. Previous work Eysenbach et al. (2022) has tried to analyze MISL but they consider an impractical downstream task adaptation procedure that uses the average state distribution of all learned skills as initialization instead of directly using\nthe learned skills. Therefore, it is still unclear how well the MISL-learned skills can be applied as downstream task initializations.\nIn this work, we theoretically analyze the connection between the properties of learned skills and their downstream task performance. Our results show that the diversity and separability of learned skills are fundamentally critical to downstream task adaptation, whose importance is empirically demonstrated in previous works (Eysenbach et al., 2019; Kim et al., 2021; He et al., 2022; Laskin et al., 2022). Our results also show that MISL alone does not necessarily guarantee these properties. To complement MISL, we propose a novel disentanglement metric that is able to measure the diversity and separability of learned skills. Our theoretical analysis relates the disentanglement metric to downstream task adaptation.\nIn particular, we introduce a novel disentanglemen metric \u201cLeast SEParability and INformativeness (LSEPIN)\u201d, which is directly related to the task adaptation cost from learned skills and complementary to the widely adopted mutual information objective of MISL. LSEPIN captures both the informativeness, diversity, and separability of the learned skills, which are critical to downstream tasks and can be used to design better URL objectives. We relate LSEPIN to Worst-case Adaptation Cost (WAC), which measures the largest possible distance between a downstream task\u2019s optimal feasible state distribution and its closest learned skill\u2019s state distribution. Our results show increasing LSEPIN could potentially result in lower WAC.\nIn addition, we show that optimizing MISL and LSEPIN are essentially maximizing distances measured by KL divergences between state distributions. However, a well-known issue is that KL divergence is not a true metric, i.e., it is not symmetric and does not satisfy the triangle inequality. This motivates us to investigate whether an alternative choice of distance can overcome the limitations of MISL. Wasserstein distance is a symmetric metric satisfying the triangle inequality and has been feasibly applied for deep learning implementations (Arjovsky et al., 2017; Dadashi et al., 2020), so we investigate a new strategy that replaces the KL divergence in MISL with Wasserstein distance and exploits its better geometric properties for theoretical analysis. This leads to new skill learning objectives for URL and our results show that the objective built upon Wasserstein distance, \u201cWasserstein SEParatibility (WSEP)\u201d, is able to discover more potentially optimal skills than MISL. Furthermore, we propose and analyze an unsupervised skill-learning algorithm \u201cProjected SEP\u201d (PWSEP) that has the favored theoretical property to discover all potentially optimal skills and is able to solve the open question of \"vertex discovery\" from Eysenbach et al. (2022).\nAnalysis of LSEPIN is complement to prior work to extend the theoretical analysis of MISL to practical downstream task adaptation, while the analysis of WSEP and PWSEP opens up a new unsupervised skill learning approach. Our results also answer the fundamental question of URL about what properties of the learned skills lead to better downstream task adaptation and what metrics can measure these properties.\nOur main contributions can be summarized in the following:\n1. We theoretically study a novel but practical task adaptation cost (i.e., WAC) for MISL, which measures how well the MISL-learned skills can be applied as downstream task initializations.\n2. We propose a novel disentanglement metric (i.e., LSEPIN) that captures both the informativeness and separability of skills. LSEPIN is theoretically related to WAC and can be used to develop URL objectives.\n3. We propose a new URL formulation based on Wasserstein distance and extend the above theoretical analysis to it, resulting in novel URL objectives for skill learning. Besides also promoting separability, they could discover more skills than existing MISL that are potentially optimal for downstream tasks.\nAlthough our contribution is mainly theoretical, in appendices G and H we show the feasibility of practical algorithm design with our proposed metrics and empirical examples to validate our results. A summary of our proposed metrics and algorithm is in appendix A and frequently asked questions are answered in appendix B."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "We consider infinite-horizon MDPs M = (S,A, P, p0, \u03b3) without external rewards with discrete states S and actions A, dynamics P (st+1|, st, at), initial state distribution p0(s0), and discount factor \u03b3 \u2208 [0, 1]. A policy \u03c0(a|s) has its discounted state occupancy measure as p\u03c0(s) = (1\u2212 \u03b3) \u2211\u221e t=0 \u03b3 tP\u03c0t (s), where P \u03c0 t (s) is the probability that policy \u03c0 visits state s at time t. There can be downstream tasks that define extrinsic reward as a state-dependent function r(s), where action-dependent reward functions can be handled by modifying the state to include the previous action. The cumulative reward of the corresponding downstream task is Ep\u03c0(s)[r(s)].\nWe formulate the problem of unsupervised skill discovery as learning a skill-conditioned policy \u03c0(at|st, z) where z \u2208 Z represents the latent skill and Z is a discrete set. H(\u00b7) and I(\u00b7; \u00b7) denote entropy and mutual information, respectively. W (\u00b7, \u00b7) denotes Wasserstein distance. We use uppercase letters for random variables and lower-case letters for samples, eg. s \u223c p(S)."
        },
        {
            "heading": "2.1 MUTUAL INFORMATION SKILL LEARNING",
            "text": "Unsupervised skill learning algorithms aim to learn a policy \u03c0(A|S,Z) conditioned on a latent skill z. Their optimization objective is usually the mutual information I(S;Z) and they differ on the prior or approximation of this objective (Gregor et al., 2017; Eysenbach et al., 2019; Achiam et al., 2018; Hansen et al., 2020).\nIn practical algorithms, the policy is generally denoted as \u03c0\u03b8(A|S, zinput) with parameters \u03b8 and conditioned on an skill latent zinput \u223c p(Zinput). Let p\u03c0\u03b8 (S|zinput) denote the state distribution of policy. The practical objective of MISL could be:\nmax \u03b8,p(Zinput)\nI(S;Zinput) = Ep(Zinput)[DKL(p \u03c0\u03b8 (S|zinput) \u2225 p\u03c0\u03b8 (S))], (1)\nPolicy parameters \u03b8 and the latent variable Zinput can be composed into a single representation, z = (\u03b8, zinput), then \u03c0\u03b8(A|S, zinput) = \u03c0(A|S, z). We call representation z \u201cskill\u201d in the following paper. Then, MISL is learned by finding an optimal p(Z) that solves\nmax p(Z)\nI(S;Z) = Ep(Z)[DKL(p(S|z) \u2225 p(S))]., (2)\nwhere p(S) = Ep(Z)[p(S|z)], is the average state distribution of discovered skills.\n2.2 INFORMATION GEOMETRY OF MISL\nPrior work Eysenbach et al. (2022) shows that the set C of state distributions feasible under the dynamics of the MDP constitutes a convex polytope lying on a probability simplex of state distributions, where every point in the polytope C is represented by a skill latent z and its state distribution is p(S|z). For any downstream task defined by a reward function r : S \u2192 R, because of the linearity of Ep(s)[r(s)] and convexity of C, the state distribution that maximizes the cumulative reward Ep(s)[r(s)]\nlies at one of the vertices of C (Boyd & Vandenberghe, 2014). Equation (2) shows that MISL learns a skill distribution z \u223c p(Z) to put weight on skills that have maximum KL divergence to the average state distribution. It can be considered as finding skills that lie on the unique (uniqueness proved in appendix D) \u201ccircle\u201d with maximum \u201cradius\u201d inside the polytope C, thus the discovered skills lie at the vertices of polytope C, as shown in Lemma 6.5 of Eysenbach et al. (2022) by Theorem 13.11 of Cover & Thomas (2006). So the skills discovered by MISL are optimal for some downstream tasks. An intuitive example of the skills discovered by MISL is shown in fig. 1b."
        },
        {
            "heading": "3 THEORETICAL RESULTS",
            "text": "Although MISL discovers some vertices that are potentially optimal for certain downstream tasks, when the downstream task favors target state distributions at the undiscovered vertices, which often happens in practice that the learned skills are not optimal for downstream tasks, there exists a \"distance\" from discovered vertices to the target vertex, and the \"distance\" from the initial skill for adaptation to the target state distribution can be considered as the adaptation cost. The prior work only analyzes the adaptation cost from the average state distribution of skills p(S) = Ez[p(S|z)] to the target state distribution. Because most practical MISL algorithms initialize the adaptation procedure from one of the learned skills (Lee et al., 2019; Eysenbach et al., 2019; Liu & Abbeel, 2021b; Laskin et al., 2021) instead of the average p(S), the prior analysis provides little insight on why these practical algorithms work. The fundamental question for unsupervised skill learning remains unanswered: How the learned skills can be used for downstream task adaptation and what properties of the learned skills are desired for better downstream task adaption?\nWe have answered this question with theoretical analysis in this section, empirical validation of the theories is in appendix H. Our informal results are as follows:\n1. In order to have a low adaptation cost when initializing from one of the learned skills, the learned skills need to be diverse and separate from each other. Separability means the discriminability between states inferred by different skills.\n2. MISL alone does not necessarily guarantee diversity and separability. We propose a disentanglement metric LSEPIN to complement MISL for diverse and separable skills.\n3. MISL discovers limited vertices, we propose WSEP metric based on Wasserstein distance that can promote diversity and separability as well as discover more vertices than MISL. One Wasserstein distance-based algorithm PWSEP can even discover all vertices.\nThe first point is intuitive that the diverse and separable skills are likely to cover more potentially useful skills, as shown by empirical results in Eysenbach et al. (2019); Park et al. (2022b); Laskin et al. (2022). The second point claims MISL alone does not guarantee diversity and separability, and this can be seen from the example in fig. 2. In this case, there are two sets of |S| = 3 skills Za : {z1, z4, z5} and Zb : {z2, z3, z5} both on the maximum \u201ccircle\u201d solving MISL. Because z2 and z3 have\nclose state distributions, skills of Zb are less diverse and less separable. There can be more than |S| vertices on the maximum \u201ccircle\u201d in the case of fig. 2 because, unlike prior work Eysenbach et al. (2022), we do not take into account the \u201cnon-concyclic\u201d assumption that limits the number of vertices on the same \u201ccircle\u201d to be |S|. Our proposed disentanglement metric LSEPIN would favor Za over Zb, and theoretical analysis of LSEPIN is conducted in section 3.2 to show its relation to downstream task adaptation cost. The downstream task procedure we consider is initialized from one of the learned tasks, for the case in fig. 2, when the target state distribution is pr, we consider adapting from the skill in Za that is closest to pr (blue arrow), which is z1, while the prior work adapts from p(S) (brown arrow).\nThe advantages of Wasserstein distance are that it is a true metric satisfying symmetry and triangle inequality. We can use it to measure distances that can not be measured by KL divergences. Optimizing these distances also promotes diversity and separability as well as results in better vertex discovery, even capable of discovering all vertices and solving the open question of \"vertex discovery\" from Eysenbach et al. (2022). Details about Wasserstein distance skill learning are shown in section 3.3. A summary of all proposed metrics and algorithm is in appendix A."
        },
        {
            "heading": "3.1 HOW TO MEASURE DIVERSITY AND SEPARATABILITY OF LEARNED SKILLS",
            "text": "Many prior MISL algorithms (Eysenbach et al., 2019; Gregor et al., 2017; Sharma et al., 2020) emphasized the importance of diversity and tried to promote diversity by using uniform p(Zinput) for eq. (1). However, uniform p(Zinput) for objective eq. (1) does not ensure diverse z for p(Z) in eq. (2) since z = (\u03b8, zinput) also depends on the learned parameter \u03b8. Empirical discussions in Park et al. (2022b); Laskin et al. (2022) also mentioned that the learned skills of these MISL methods often lack enough diversity and separability. Moreover, as previously mentioned by the example in fig. 2, even when I(S;Z) in eq. (2) is maximized, learned skills could still lack diversity and separability of skills. To complement MISL, we propose a novel metric to explicitly measure the diversity and separability of learned skills.\nWe consider I(S;1z) (1z is the binary indicator function of Z = z) to measure the informativeness and separability of an individual skill z. In the context of unsupervised skill learning, informativeness should refer to the information shared between a skill and its inferred states. As mentioned, separability means the states inferred by different skills should be discriminable. We analyze the minimum of I(S;1z) over learned skills. We name it Least SEParability and INformativeness (LSEPIN)\nLSEPIN = min z I(S;1z). (3)\nI(S;1z) is related to how much states inferred by skill z and states not inferred by z are discriminable from each other, so it covers not only informativeness but also separability of skills. In the context of representation learning, the metrics capturing informativeness and separability are called the disentanglement metrics (Do & Tran, 2019b; Kim et al., 2021), so we also call LSEPIN as a disentanglement metric for unsupervised skill learning. More details about the difference between disentanglement for representation learning and disentanglement for our skill learning setting are in appendix E."
        },
        {
            "heading": "3.2 HOW DISENTANGLEMENT AFFECTS DOWNSTREAM TASK ADAPTATION",
            "text": "We provide a theoretical justification for the proposed disentanglement metric, showing that it can be a complement of I(S;Z) to evaluate how well the URL agent is prepared for downstream tasks by the following theorems. Definition 3.1 (Worst-case Adaptation Cost). Worst-case Adaptation Cost (WAC) is defined as\nWAC = max r min z\u2208Z\u2217 DKL(p(S|z) \u2225 pr), (4)\nwhere pr is the optimal feasible state marginal distribution for the downstream task defined by r, and Z\u2217 is the set of learned skills.\nThe following theoretical results show how the LSEPIN metric is related to the WAC in definition 3.1. Theorem 3.1. When learned skill sets Zi, i = 1, 2, ... with N \u2264 |S| skills (N skills have p(z) > 0) sharing the same skill z are all MISL solutions, The skill set with the higher I(S;1z) will have higher p(z) and lower adaptation cost for all rz in the set Rz , where Rz is the set of downstream tasks always satisfying \u2200i,\u2200r \u2208 Rz , z = argmaxz\u2032\u2208Zi DKL(p(S|z\n\u2032) \u2225 prz ). And the maximum of this adaptation cost has the following formulation:\nICz = max r\u2208Rz Cz(r)\u2212 p(z)Dz(r) 1\u2212 p(z) , (5)\nwhere\nCz(r) = I(S;Z) +DKL(p(S) \u2225 pr), (6) Dz(r) = DKL(p(S|z) \u2225 pr). (7)\nTheorem 3.1 provides a correlation between our proposed metric I(S;1z) the adaptation cost ICz from a skill in Z \\ {z} that is closest to the downstream task optimal distribution and its detailed proof is in appendix C.1. To better understand the claim of this theorem, we can look at the intuitive example shown in fig. 3. In this case |S| = 3. When MISL is maximized by 3 skills, the skill combinations as MISL solutions could be Z\u22171 : {z1, z2, z3} and Z\u22172 : {z1, z\u20322, z\u20323}. Z\u22172 has higher I(S;1z1) than Z\u22171 . By theorem 3.1, solution Z\u22172 should have lower cost to adapt to the optimal distribution prz1 of the downstream task rz1 . Corollary 3.1.1. When the MISL objective I(S,Z) is maximized by N \u2264 |S| skills, WAC is bounded of a solution Z\u2217 by\nWAC \u2264 max z\u2208Z\u2217 ICz = max z\u2208Z\u2217 max r\u2208Rz Cz(r)\u2212 p(z)Dz(r) 1\u2212 p(z) . (8)\nWAC is the worst-case adaptation cost defined in definition 3.1, Cz and Dz are as defined in eqs. (6) and (7). Rz here needs to satisfy \u2200r \u2208 Rz , z = argmaxz\u2032\u2208Z\u2217 DKL(p(S|z\u2032) \u2225 pr).\nCorollary 3.1.1 provides an upper bound for WAC. The proof is deferred to appendix C.2. The results in Corollary 3.1.1 and theorem 3.1 considered situations when MISL is solved and I(S;Z) is maximized, we also discussed how I(S;1z1) and LSEPIN affects learned skills and adaptation cost when I(S;Z) is not maximized in appendix C.4.\nBy theorem 3.1 we know that higher I(S;1z) implies lower ICz , but how much ICz associated with an individual skill z contribute to the overall WAC can not be known in prior and it depends on specific Cz and Dz . Moreover, specific Cz and Dz depend on the \u201cshape\u201d of the undiscovered parts of C and can not be known before the discovery of all vertices. Therefore, in practice, like existing work (Durugkar et al., 2021; He et al., 2022) treating the desired properties of each skill equally in practical algorithms, we could treat every ICz equally. We have the following theorem showing under which assumptions we can treat every ICz equally for WAC. Theorem 3.2. When 1. the optimal state distribution for the downstream task is far from p(S) and 2. The state space is large, i.e. |S| is large. ICz of all learned skills can be considered equally contribute to WAC.\nBoth assumptions for this theorem are practical and can commonly happen in complex and highdimensional environments. When every ICz is treated equally for WAC, optimizing LSEPIN could lead to lower WAC. It is formally analyzed and proven in appendix C.3.\nIn summary, we have provided theoretical insight on how I(S;1z) affects downstream task adaptation and how optimizing LSEPIN could lower WAC under practical assumptions. We do not assume \u201cnon-concyclic\u201d vertices and we consider the practical approach of directly adapting from learned skills instead of the average state distribution. LSEPIN is a complement to the mutual information objective I(S;Z). Compared to I(S;Z), it provides a better metric to evaluate the effectiveness of learned MISL skills for potential downstream tasks. Our results have shown the diversity and separability of the learned skills measured by I(S;1z) and LSEPIN are desired for better downstream task adaptation. Remark 3.2.1. One limitation with MISL even with LSEPIN is that even without the limitation of the number of skills to have p(z) > 0, it still can not discover vertices v such that\nDKL(p(S|v) \u2225 p(S)) < max p(z) Ep(z) [DKL(p(S|z) \u2225 p(S))]\nVertex prz1 in fig. 3 belongs to such vertices."
        },
        {
            "heading": "3.3 SKILL LEARNING WITH WASSERSTEIN DISTANCE",
            "text": "In this subsection, we analyze a new strategy that replaces the KL divergence in information geometry with Wasserstein distance for better geometric properties to overcome the limitation of MISL shown in remark 3.2.1.\nMaximizing I(S;Z) and LSEPIN are essentially maximizing distances measured by KL divergences between points in a polytope. KL divergence is not symmetric and does not satisfy the triangle inequality, so KL divergences between points of the polytope could be incomparable when two KL divergences don\u2019t share a same point. We study the strategy that replaces the KL divergence in MISL with Wasserstein distance since Wasserstein distance is a true metric. Then we conduct further theoretical analysis to exploit its better geometrical properties such as symmetry and triangle inequality.\nIn this section, we will introduce the learning objectives as well as evaluation metrics for Wasserstein Distance Skill Learning (WDSL), analyze what kind of skills these objectives can learn, where the learned skills lie in the polytope, and how these learned skills contribute to downstream task adaptation. Theoretically, the favored property of WDSL is that it discovers more vertices in C that are potentially optimal for downstream tasks than MISL, and one WDSL algorithm can discover all vertices."
        },
        {
            "heading": "3.3.1 OBJECTIVES FOR WASSERSTEIN DISTANCE SKILL LEARNING",
            "text": "First of all, we can trivially replace the KL divergences in the MISL objective eq. (2) with Wasserstein distance and obtain a basic WDSL objective\nmax p(z)\nEp(z) [W (p(S|z), p(S))] . (9)\nWe name it Average Wasserstein skill learning Distance (AWD), similar to the MISL objective in eq. (2), this objective also learns skills that lie on a hyper ball with a maximum radius. Because this objective is not our main proposition and also suffers from the limitation of remark 3.2.1, we put the analysis of this objective in appendix F.1.\nWe mainly analyze this objective for WDSL: WSEP = \u2211 zi\u2208Z \u2211 zj\u2208Z,i\u0338=j W (p(S|zi), p(S|zj)), (10)\nwhere Z is the set of skills with p(z) > 0. We call this objective Wasserstein SEParatibility (WSEP), it can be considered as a disentanglement for WDSL as it measures the Wasserstein distance between learned skills. Recall that separability for MISL is defined as how discriminable the state is, Wasserstein distances between skills can not only represent discriminability but also can express the distance between trajectories when there are no overlappings."
        },
        {
            "heading": "3.3.2 GEOMETRY OF LEARNED SKILLS",
            "text": "As mentioned before in section 2.2, the skills that are potentially optimal for downstream tasks lie at the vertices of the polytope C of feasible state distributions. By the following lemma, we show that optimizing WSEP will push the learned skills to the vertices of the polytope.\nLemma 3.3. When WSEP is maximized, all learned skills with p(z) > 0 must lie at the vertices of the polytope.\nProof of this lemma is in appendix F.2.\nThe previous theoretical results of disentanglement metric LSEPIN depend on the maximization of I(S;Z), so as mentioned in remark 3.2.1, it still only discover vertices with maximum \u201cdistances\u201d to the average distribution p(S). However, WSEP does not depend on the maximization of other objectives, e.g., eq. (9), so there is no distance restriction on the vertices discovered by WSEP. Therefore, it is possible for WSEP to discover all vertices of the feasible polytope C, thus discovering all optimal skills for potential downstream tasks. For example, in an environment with a polytope shown in fig. 1b, MISL only discovers 3 vertices on the \u201ccircle\u201d with maximum \u201cradius\u201d while WSEP is able to discover all 5 vertices.\nRemark 3.3.1. When there is no limitation on the number of skills with positive probability, maximizing WSEP could discover all vertices, and the agent would be optimal for every possible downstream task defined by r(s)."
        },
        {
            "heading": "3.3.3 HOW WSEP AFFECTS DOWNSTREAM TASK ADAPTATION",
            "text": "Then, we propose a theorem about how WSEP metric is related to downstream task adaptation when there is a limitation on the quantity of learned skills. Definition 3.2. Mean Adaptation Cost (MAC): mean of the Wasserstein distances between the undiscovered vertices and the learned skills closest to them.\nMAC = 1 |V \\ Z\u2217| \u2211\nz\u2032\u2208V\\Z\u2217 min z\u2208Z\u2217\nW (p(S|z\u2032), p(S|z)) (11)"
        },
        {
            "heading": "V is the set of all skills that have their conditional state distribution at vertices of the MDP\u2019s feasible",
            "text": "state distribution polytope, and Z\u2217 is all learned skills with p(z) > 0. Theorem 3.4. When WSEP is maximized by |Z\u2217| skills, the MAC can be upper-bounded:\nMAC \u2264 \u2211 z\u2208Z\u2217 L z V \u2212WSEP\n|V \\ Z\u2217||Z\u2217| (12) MAC \u2264 \u2211\nz\u2208Z\u2217 LV \u2212WSEP |V \\ Z\u2217||Z\u2217| , (13)\nwhere\nLzV = \u2211 v\u2208V W (p(S|v), p(S|z))\nLV =max v\u2032\u2208V \u2211 v\u2208V W (p(S|v), p(S|v\u2032)) (14)\nTheorem 3.4 shows the relation between WSEP and the upper bounds of adaptation cost MAC in the practical setting, where the number of skills to be learned is limited. The proof is in appendix F.3.\nIn a stationary MDP, the polytope is fixed, so the edge lengths LzV and LV are constant. Larger WSEP seems to tighten the bounds, but different WSEP also means a different Z\u2217 set of learned skills, thus a different \u2211 z\u2208Z\u2217 L z V . Therefore, increasing WSEP only tightens the bound in eq. (13) but not necessarily the bound in eq. (12). Remark 3.4.1. More distance is not always good: WSEP as a disentanglement metric promotes the distances between learned skills and These two bounds of MAC show that maximizing WSEP can indeed help with downstream task adaptation, but this does not mean that learned skills with more WSEP will always result in lower MAC. An illustrative example is shown in appendix F.5, where more distant skills with higher WSEP do not have lower adaptation costs Remark 3.4.2. If we replace the Wasserstein distances in WSEP with KL divergences, we get a symmetric formulation of KLSEP = \u2211 zi\u2208Z \u2211 zj\u2208Z,i\u0338=j DKL(p(S|zi) \u2225 p(S|zj)). It is symmetric, but it does not promote diversity and separability because KL divergence does not satisfy the triangle inequality. More details are analyzed in appendix F.7\nWSEP does not suffer from the limitation of remark 3.2.1 because it does not try to find skills on a maximum \u201ccircle\u201d. Although WSEP can potentially discover more vertices than MISL, we find that it may not be able to discover all vertices of the feasible state distribution polytope C in appendix F.6."
        },
        {
            "heading": "3.3.4 SOLVING THE VERTEX DISCOVERY PROBLEM",
            "text": "The following theorem shows a learning procedure based on Wasserstance distance capable of discovering all vertices of feasible state distribution polytope C. Theorem 3.5. When V is the set of all vertices of the feasible state distribution polytope C, all |V| vertices can be discovered by |V| iterations of maximizing\nPWSEP(i) : min \u03bb\nW ( p(S|zi), \u2211 zj\u2208Zi \u03bbjp(S|zj) ) , (15)\nwhere Zi is the set of skills discovered from iteration 0 to i\u2212 1 and zi is the skill being learned at ith iteration. \u03bb is a convex coeffcient of dimension i\u2212 1 that every element \u03bbj \u2265 0,\u2200j \u2208 {0, 1, .., i\u2212 1} and \u2211 j\u2208{0,1,..,i\u22121} \u03bb j = 1.\nIn the initial iteration when Zi = \u2205, PWSEP(0) can be W (p(S|z0), p(S|zrand)) with zrand to be a randomly initialized skill.\nPWSEP(i) can be considered as a projection to the convex hull of Zi, so we call it Projected WSEP and this learning procedure the PWSEP algorithm. It can discover all |V| vertices with only |V| skills. Although lemma 3.3 shows that maximizing WSEP also discovers vertices, the discovered vertices could be duplicated (shown in appendix F.6). Maximizing projected distance PWSEP(i) could ensure the vertex learned at each new iteration is not discovered before. Proof and more analysis of the vertex discovery problem can be found in appendix F.4."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "MISL is widely implemented and has been the backbone of many URL algorithms (Achiam et al., 2018; Florensa et al., 2017; Hansen et al., 2020). Prior work Eysenbach et al. (2022) tried to provide theoretical justification for the empirical prevalence of MISL from an information geometric (Amari & Nagaoka, 2000), but their analysis mainly considered an unpractical downstream task adaptation procedure. Works like Eysenbach et al. (2019); Park et al. (2022b; 2023); He et al. (2022); Laskin et al. (2022) showed the empirical advantages of favored properties such as diversity and separability of learned skills. Our work provides theoretical justification for these favored properties and shows they benefit the practical downstream task adaptation procedure that initializes from learned skills.\nIn Kim et al. (2021) the concept of disentanglement was mentioned. They used the SEPIN@k and WSEPIN metrics from representation learning (Do & Tran, 2019b) to promote the informativeness and separability between different dimensions of the skill latent. However, properties of latent representations could be ensured by optimization only in the representation space, so they do not explicitly regulate the state distributions of learned skills like our proposed LSEPIN and WSEP do. Appendix E discussed more details.\nRecent practical unsupervised skill learning algorithms (He et al., 2022; Durugkar et al., 2021) maximize a lower bound of WSEP, so our analysis on WSEP provides theoretical insight on why these Wasserstein distance-based unsupervised skill learning algorithms work empirically. Their empirical results showed the feasibility and usefulness of skill discovery with Wasserstein distance.\nSuccessor feature (SF) method SFOLS (Alegre et al., 2022) also claimed to be able to discover all vertices. In appendix F.4.2, the difference between the SF setting and our skill learning setting is discussed in detail, as well as the comparison of theoretical properties between our proposed PWSEP and SFOLS. Other methods like Hansen et al. (2020); Liu & Abbeel (2021b) combined MISL with SF for URL, and they are shown to accelerate downstream task adaptation. Since they are MISL methods adapting from one of the learned skills, our theoretical results also apply to them."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We investigated the geometry of task adaptation from skills learned by unsupervised reinforcement learning. We proposed a disentanglement metric LSEPIN for mutual information skill learning to capture the diversity and separability of learned skills, which are critical to task adaptation. Unlike the prior analysis, we are able to build a theoretical connection between the metric and the cost of downstream task adaptation. We further proposed a novel strategy that replaces KL divergence with Wasserstein distance and extended the geometric analysis to it, which leads to novel objective WSEP and algorithm PWSEP for unsupervised skill learning. Our theoretical result shows why they should work, what could be done, and what limitations they have. Specifically, we found that optimizing the proposed WSEP objective can discover more optimal policies for potential downstream tasks than previous methods maximizing the mutual information objective I(S;Z). Moreover, the proposed PWSEP algorithm based on Wasserstein distance can theoretically discover all optimal policies for potential downstream tasks.\nOur theoretical results could inspire new algorithms using LSEPIN or Wasserstein distance for unsupervised skill learning. For Wasserstein distance, the choice of transport cost is important, which may require strong prior knowledge. Our future work will develop practical algorithms that learn deep representations such that common transport costs such as L2 distance in the representation space can accurately reflect the difficulty of traveling from one state to the other."
        },
        {
            "heading": "A SUMMARY OF PROPOSED METRICS AND ALGORITHM",
            "text": "Our results are mainly theoretical, showing how properties measured by the following metrics are related to downstream task adaptation. The results can inspire new algorithms with these metrics.\nWe proposed I(S;1z) to measure the separability of an individual skill z, more about separability is discussed in appendix E. It is linked to the adaption cost ICz that initializes from the closest skill other than skill z (in the set Z \\ {z}) to the downstream task optimal distribution. We proposed LSEPIN = minz I(S;1z) to measure the overall separability and diversity of learned skills. It is linked to the adaption cost WAC defined in eq. (4) that initializes from the closest skill to the downstream task optimal distribution in the learned skill set Z . We proposed WSEP = \u2211 zi\u2208Z \u2211 zj\u2208Z,i\u0338=j W (p(S|zi), p(S|zj)) to measure the overall separability and diversity of learned skills. It is linked to the adaption cost MAC defined in eq. (11) that initializes from the closest skill to the downstream task optimal distribution in the learned skill set Z . WSEP can discover more skills that are potentially optimal for downstream tasks.\nWe proposed the PWSEP algorithm. Theoretically, it can discover all skills potentially optimal for downstream tasks.\nTable 1 displays the summarised properties of the proposed metrics."
        },
        {
            "heading": "B FREQUENTLY ASKED QUESTIONS",
            "text": ""
        },
        {
            "heading": "B.1 WHAT\u2019S THE OVERLAPS AND DIFFERENCES BETWEEN OUR WORK AND EYSENBACH",
            "text": "ET AL. (2022)?\nDifferences: Their analysis of how MISL affects downstream task adaptation is limited to the adaptation procedure of initializing from the average state distribution p(S) = Ez\u2208Z [p(S|z)] of learned skill set Z . Because most practical MISL algorithms initialize the adaptation procedure from one of the learned skills (Lee et al., 2019; Eysenbach et al., 2019; Hansen et al., 2020; Liu & Abbeel, 2021b; Laskin et al., 2021) instead of the average p(S), their analysis provides little insight on why these practical algorithms work. Unlike Eysenbach et al. (2022), our work analyzed the practical and popular adaptation procedure of initializing from the learned skills. Our theoretical results not only provide insight on why the above-mentioned practical algorithms work but also answer the fundamental question for unsupervised skill learning: How the learned skills can be used for downstream task adaptation, and what properties of the learned skills are desired for better downstream task adaption? Moreover, our proposed metrics and algorithm are novel and our analysis for the Wasserstein distance offers an innovative perspective on unsupervised skill learning.\nOverlap: The only overlap between our work and the prior work (Eysenbach et al., 2022) is that we both analyze the geometry of the state distributions of the skills learned by unsupervised skill learning methods.\nB.2 WHAT\u2019S THE DIFFERENCE BETWEEN I(S;1z) AND I(S;Z)?\nHigh-level Intuitive difference:\nBinary indicator 1z is a random variable that takes value in {0, 1} with the distribution p(1z), which only describes skill z, while Z is a random variable with the distribution p(Z), which describes all skills. I(S;1z) enables to measure how well every single skill is learned in terms of informativeness\nand separability, so we can evaluate the minimum minz I(S;1z) by LSEPIN and the median by MSEPIN(L719); While the default Mutual Information Skill Learning (MISL) objective I(S;Z) only measures the overall informativeness of all skills."
        },
        {
            "heading": "Formal difference:",
            "text": "I(S;1z) can be decomposed as\nI(S;1z) = p(z)DKL(p(S|z)||S) + p(Z \u0338= z)DKL(p(S|Z \u0338= z)||S), (16) while I(S;Z) can be decomposed as\nI(S;Z) = Ez\u223cp(Z)[DKL(p(S|z)||S)]. (17)\nThe distribution p(S|Z \u0338= z) = Ez\u2032\u223cp(Z\u2032 \u0338=z|z)[p(S|z\u2032)] is in the composition of I(S;1z) but not in the composition of I(S;Z). Notice that the second term in the for decomposition of I(S;1z):\nDKL(p(S|Z \u0338= z)||S) \u0338= Ez\u2032\u223cp(Z\u2032 \u0338=z|z)[DKL(p(S|z\u2032)||S)]. (18)\nSo these are two different formulations.\nAdvantages of I(S;1z):\n1. I(S;1z) can be used to evaluate every single skill. 2. Besides informativeness, I(S;1z) explicitly encourages separability, more details about\nseparability are discussed in appendix E."
        },
        {
            "heading": "B.3 WHY NOT REPLACE THE WASSERSTEIN DISTANCE IN WSEP OR PWSEP WITH KL DIVERGENCE?",
            "text": "Because KL divergence does not satisfy the triangle inequality, maximizing the replaced WSEP objective could jeopardize the diversity of learned skills as shown in appendix F.7. Appendix F.4 also discussed why KL divergence can not replace the Wasserstein distance in PWSEP.\nB.4 IS IT PRACTICAL TO TREAT THE DISENTANGLEMENT OF EVERY SKILL EQUALLY?\nYes, theoretically, as the proof of theorem 3.2 in appendix C.3 shows, the contribution of ICz to WAC can be treated equally for each skill z under two assumptions:\n1. The downstream task favored state distribution is far from the average state distribution p(S).\n2. The state space is large.\nBoth could commonly happen in high-dimensional practical environments. When the contribution of ICz to WAC can be treated equally for each skill z, the disentanglement metric I(S;1z) for each skill z can be considered equally important.\nPractically, existing unsupervised skill learning algorithms such as He et al. (2022); Durugkar et al. (2021) treat the desired properties of every skill equally."
        },
        {
            "heading": "B.5 CAN ONE INFER THE SHAPE OF C OR PERFORM MAP ESTIMATION TO GET THE EXACT",
            "text": "WEIGHT OF EACH ICz FOR WAC?\nIt is unnecessary and could be infeasible. As mentioned, it is practical to treat ICz of each skill z equally important for WAC. By Maximum A Posteriori (MAP) estimation, one still needs a prior of the polytope shape, then update the belief of the shape by the samples of state distributions instead of just state samples. One needs to know which kind of state distribution is feasible for this particular MDP instead of just exploring the state space.\nUndiscovered vertices are not within the convex hull of discovered skills, so you can not combine the discovered skills to get an undiscovered state distribution. You will have zero samples of the KL divergence between learned skills and the undiscovered state distributions, on which the MAP update depends.\nIt might be possible for the specific setting where you can infer the possible state distributions by the explored states. However, the purpose of this estimation is just to obtain a weight about treating some skills more important than others. which can be too expensive for practical algorithm design, and bias or lag in estimation could make it worse than just treating every skill equally.\nB.6 DOES I(S;1z) AND LSEPIN AFFECT DOWNSTREAM TASK PERFORMANCE WHEN I(S;Z) IS NOT MAXIMIZED?\nYes, empirically, the importance of diversity and separability of skills are mentioned in many prior works (Eysenbach et al., 2019; Laskin et al., 2022; He et al., 2022; Park et al., 2022b), disentanglement metrics like I(S;1z) and LSEPIN can explicitly measure these properties. Theoretically, it is discussed in appendix C.4."
        },
        {
            "heading": "C PROOF FOR THEORETICAL RESULTS IN SECTION 3.2",
            "text": ""
        },
        {
            "heading": "C.1 PROOF FOR THEOREM 3.1",
            "text": "Theorem 3.1. When learned skill sets Zi, i = 1, 2, ... with N \u2264 |S| skills (N skills have p(z) > 0) sharing the same skill z are all MISL solutions, The skill set with the higher I(S;1z) will have higher p(z) and lower adaptation cost for all rz in the set Rz , where Rz is the set of downstream tasks always satisfying \u2200i, \u2200r \u2208 Rz , z = argmaxz\u2032\u2208Zi DKL(p(S|z\n\u2032) \u2225 prz ). And the maximum of this adaptation cost has the following formulation:\nICz = max r\u2208Rz Cz(r)\u2212 p(z)Dz(r) 1\u2212 p(z) , (5)\nwhere Cz(r) = I(S;Z) +DKL(p(S) \u2225 pr), (6) Dz(r) = DKL(p(S|z) \u2225 pr). (7)\nProof. First, it is not necessary for the skill sets to be all MISL solutions. The proof of this theorem only needs the skill sets to satisfy these necessary conditions:\n1. All Zi share the same average state distribution p(S) = Ez\u2032\u2208Zi [p(S|z\u2032)]\n2. DKL(p(S|z\u2032) \u2225 p(S)) is constant for all z\u2032 in all Zi.\n3. All Zi share the same skill z\nIn this proof, we use z to denote the skill shared among the skill sets and z\u2032 \u2208 Zi to denote a general skill in set Zi. A MISL solution is a skill distribution p(Z) with N skills having positive probabilities. Different solutions have the same p(S) = Ez\u2032\u2208Zip(S|z\u2032). Because for MISL solutions, every learned skill z\u2032 \u2208 Zi should satisfy p(S|z\u2032) = argmaxp\u2208C DKL(p \u2225 p(S)). It is like the skill-conditioned state distributions of MISL solutions are all on the \u201clargest circle\" that is centered in p(S), with the \u201cradius\" being maxp\u2208C DKL(p \u2225 p(S)) (see Section 6.2 of Eysenbach et al. (2022)). Therefore, MISL solutions sharing the same z satisfy the necessary conditions.\nThe proof of Lemma 6.3 in Eysenbach et al. (2022) has shown that average state marginal p(S) is a vector in |S|-dimensional space with |S| \u2212 1 degrees of freedom. MISL can recover at most |S| unique skills because every learned skill z\u2032 should satisfy p(S|z\u2032) = argmaxp\u2208C DKL(p \u2225 p(S)), thus more than |S| unique skills put more than |S| distance constraints on p(S), making it overly specified and ill-defined. Using a fixed number of skills is common in practice, so we use N \u2264 |S| skills for the MISL problem.\nBecause we do not consider the assumption in Eysenbach et al. (2022) restricting at most |S| skills on the maximum \"circle\" to solve MISL, the could be more than |S| skills on the maximum \"circle\", so N can be |S| and different |Zi| can all have |S| skills. The sketch of the proof goes like the following:\n1. The sum of KL divergences from learned skills to the optimal target distribution for a downstream task is constant.\n2. The upper bound of ICz depends this constant and p(z), and the upper bound decreases monotonically with higher p(z).\n3. Increasing I(S;1z) results in higher p(z) thus lower upper bound of ICz .\nWe can see that for a solution Zi the weighted sum of the KL divergence between the optimal vertex of considered downstream task r \u2208 Rz and the MISL learned skills is:\u2211\nz\u2032\u2208Zi\np(z\u2032)DKL(p(S|z\u2032) \u2225 pr)\n= \u2211 z\u2032\u2208Zi p(z\u2032)Ep(S|z\u2032)[log p(S|z\u2032) pr ]\n(19)\n= \u2211 z\u2032\u2208Zi \u2211 S p(S, z\u2032)[log p(S|z\u2032) pr ] (20) =\u2212H(S|Z) +H(p(S), pr) (21) =H(S)\u2212H(S|Z)\u2212H(S) +H(p(S), pr) (22) =I(S;Z) +DKL(p(S) \u2225 pr), (23)\nwhere H(p(S), pr) denotes the cross-entropy of the pr relative to p(S).\nBecause I(S;Z) = Ez\u2032\u2208Zi [DKL(p(S|z\u2032) \u2225 p(S))] and p(S) are constant under the necessary conditions 1 and 2, the sum only depends on r \u2208 Rz and we can consider\u2211\nz\u2032\u2208Zi\np(z)DKL(p(S|z\u2032) \u2225 pr) = Cz(r), \u2200i (24)\nBecause the minimum is less than the mean, we have\n(1\u2212 p(z)) min z\u2032\u2208Zi,z\u2032 \u0338=z DKL(p(S|z\u2032) \u2225 pr) \u2264\u2211 z\u2032\u2208Zi,z\u2032 \u0338=z p(z\u2032)DKL(p(S|z\u2032) \u2225 pr), \u2200i (25)\nthen\nmin z\u2032\u2208Zi,z\u2032 \u0338=z\nDKL(p(S|z\u2032) \u2225 pr) \u2264 Cz(r)\u2212 p(z)DKL(p(S|z) \u2225 pr)\n1\u2212 p(z) , \u2200i (26)\nWe have assumed z = argmaxz DKL(p(S|z) \u2225 pr), so argminz\u2032 DKL(p(S|z\u2032) \u2225 pr) is chosen from z\u2032 \u2208 Zi, z\u2032 \u0338= z, so\nmin z\u2032\u2208Zi\nDKL(p(S|z\u2032) \u2225 pr) \u2264 Cz(r)\u2212 p(z)Dz(r)\n1\u2212 p(z) , \u2200i (27)\nand we can bound minz\u2032 DKL(p(S|z\u2032) \u2225 pr) by eq. (27) and obtain the formulation of eq. (5). For any given downstream task r \u2208 Rz , different MISL solutions Zi have the same Cz(r) and Dz(r) but different p(z), so next we show how a higher I(S;1z) affects p(z) and then results in a tighter upper bound of minz\u2032 DKL(p(S|z\u2032) \u2225 pr). Because\nI(S;1z) =p(z)DKL(p(S|z) \u2225 p(S))) + (1\u2212 p(z))DKL(p(S|Z \u0338= z) \u2225 p(S)),\n(28)\nand\np(S|Z \u0338= z) = p(S)\u2212 p(z)p(S|z) 1\u2212 p(z) , (29)\nfor the same skill z, the value of I(S;1z) could only be changed by p(z). If \u2202I(S;1z) \u2202p(z) is non-negative, a higher I(S;1z) would indicate a higher p(z).\n\u2202I(S;1z) \u2202p(z) = (1\u2212 p(z))\u2202DKL(p(S|Z \u0338= z) \u2225 p(S)) \u2202p(z)\n+DKL(p(S|z) \u2225 p(S)))\u2212DKL(p(S|Z \u0338= z) \u2225 p(S)) (30)\nBy the necessary condition 2 and by convexity of KL divergence, because p(S|Z \u0338= z) is a convex combination of skills not equal to z, we have\nDKL(p(S|z) \u2225 p(S)) \u2265 DKL(p(S|Z \u0338= z) \u2225 p(S)) (31)\n. then \u2202I(S;1z)\u2202p(z) is non-negative if\n\u2202DKL(p(S|Z \u0338= z) \u2225 p(S)) \u2202p(z) \u2265 0. (32)\nBy eq. (29), we have:\np\u2032(S|Z \u0338= z) =p(S)\u2212 p \u2032(z)p(S|z)\n1\u2212 p\u2032(z) (33)\nwhere p\u2032(z) = p(z) + \u2206, and \u2206 is a small positive value that keeps the changed p\u2032(Z) satisfying the necessary conditions described at the beginning of the proof. Then\np(S|Z \u0338= z) =\u03bbp\u2032(S|Z \u0338= z) + (1\u2212 \u03bb)p(S) (34)\n\u03bb = 1\u2212 p\u2032(z) 1\u2212 p(z) p(z) p\u2032(z) (35)\nSo 0 < \u03bb < 1. By convexity of KL divergence, we have\nDKL(p(S|Z \u0338= z) \u2225 p(S)) (36) =DKL(\u03bbp\n\u2032(S|Z \u0338= z) + (1\u2212 \u03bb)p(S) \u2225 p(S)) (37) \u2264\u03bbDKL(p\u2032(S|Z \u0338= z) \u2225 p(S)) + (1\u2212 \u03bb) \u00b7 0 (38) <DKL(p\n\u2032(S|Z \u0338= z) \u2225 p(S)) (39) and\n\u2202DKL(p(S|Z \u0338= z) \u2225 p(S)) \u2202p(z)\n(40)\n= lim \u2206\u21920+\nDKL(p \u2032(S|Z \u0338= z) \u2225 p(S))\n\u2206\n\u2212 DKL(p(S|Z \u0338= z) \u2225 p(S)) \u2206\n(41)\n\u22650 (42)\nTherefore, \u2202I(S;1z)\u2202p(z) is non-negative, a higher I(S;1z) would indicate a higher p(z).\nWith this result, we go back to eq. (27). Because DKL(p(S|z) \u2225 pr) is required to be maxz\u2032\u2208Zi DKL(p(S|z\u2032) \u2225 pr),\u2200i,\u2200r \u2208 Rz , we have\nDKL(p(S|z) \u2225 pr) = Dz(r) \u2265 \u2211 z\u2032\u2208Zi p(z\u2032)DKL(p(S|z\u2032) \u2225 pr) = Cz(r), \u2200i,\u2200r \u2208 Rz (43)\nand because of eq. (43)\n\u2202 Cz(r)\u2212p(z)Dz(r)1\u2212p(z)\n\u2202p(z) \u2264 0,\u2200r \u2208 Rz (44)\nNow we have proved that a higher I(S;1z) indicates a higher p(z), thus a tighter upper bound in eq. (27) for all r \u2208 Rz , including the worst-case defined in eq. (5). The theorem is then proved."
        },
        {
            "heading": "C.2 PROOF OF COROLLARY 3.1.1",
            "text": "Corollary 3.1.1. When the MISL objective I(S,Z) is maximized by N \u2264 |S| skills, WAC is bounded of a solution Z\u2217 by\nWAC \u2264 max z\u2208Z\u2217 ICz = max z\u2208Z\u2217 max r\u2208Rz Cz(r)\u2212 p(z)Dz(r) 1\u2212 p(z) . (8)\nWAC is the worst-case adaptation cost defined in definition 3.1, Cz and Dz are as defined in eqs. (6) and (7). Rz here needs to satisfy \u2200r \u2208 Rz , z = argmaxz\u2032\u2208Z\u2217 DKL(p(S|z\u2032) \u2225 pr).\nProof. The formulation can be directly obtained from eq. (27) and definition 3.1. The remaining problem here is whether \u22c3\nz\u2032\u2208Z\u2217 Rz contains all possible downstream tasks. If there exists a downstream task r not in \u22c3\nz\u2032\u2208Z\u2217 Rz , then this r should satisfy that argmaxz\u2032\u2208Z\u2217 DKL(p(S|z\u2032) \u2225 prz ) is not in Z\u2217, which is contradictory. Therefore \u22c3 z\u2032\u2208Z\u2217 Rz contains all possible downstream tasks and this corollary is proved."
        },
        {
            "heading": "C.3 DIRECT CORRELATION BETWEEN WAC AND LSEPIN",
            "text": "By theorem 3.1 we know that increasing I(S;1z) tightens the upper bound of the adaptation cost ICz associated with an individual skill z. However, how much ICz of an individual z contributes to the overall WAC depends on the specific Cz and Dz of each skill z, and Cz and Dz cannot be known in prior. Therefore, I(S;1z) of certain skills could be more important than others for WAC, which can only be known when we already discovered all vertices of the polytope C. Without prior knowledge, we could only treat every ICz equally and assume that the cost associated with individual skills would have the same importance on the WAC. The following results provide assumptions that are sufficient to treat the cost of individual skills equally and directly associate LSEPIN with WAC under these assumptions, which require:\n\u2022 The optimal state distribution for the downstream task to be far away from p(S) (eq. (46))\n\u2022 The state space is large (eq. (45))\nBoth requirements are common in practice.\nLemma C.1. When the MISL objective I(S;Z) is maximized by N \u2264 |S| skills, the solution set of skills is Z\u2217, assume that \u2200z \u2208 Z\u2217,\nDm = min r\u2208Rz\nDKL(p(S|z) \u2225 pr) (45)\nC = DKL(p(S) \u2225 pr), \u2200r \u2208 Rz (46)\nwhere C and Dm are two constants, Rz is the set of reward functions that satisfy \u2200r \u2208 Rz , z = argmaxz\u2032\u2208Z\u2217 DKL(p(S|z\u2032) \u2225 prz ). Then, higher LSEPIN results in lower WAC.\nProof. We denote\nDz(r) = DKL(p(S|z) \u2225 pr) (47) Cz(r) = \u2211 z\u2032 p(z\u2032)DKL(p(S|z\u2032) \u2225 pr) (48)\nFirst, by eqs. (23) and (46), we have\nCz(r) = \u2212H(S|Z) +H(p(S), pr) (49) = H(S)\u2212H(S|Z) +DKL(p(S) \u2225 pr) (50) = I(S;Z) +DKL(p(S) \u2225 pr) (51) = I(S;Z) + C = Cm (52)\nCm is a constant because for MISL solutions, I(S;Z) is maximized and is a constant.\nBecause 0 \u2264 p(z) \u2264 1, we have, \u22000 \u2264 p(z) \u2264 1:\nargmax r\u2208Rz Cz(r)\u2212 p(z)Dz(r) 1\u2212 p(z) = argmax r\u2208Rz Cm \u2212 p(z)Dz(r) 1\u2212 p(z) = argmin r\u2208Rz Dz(r) (53)\nCombine eqs. (8), (45) and (53), we have\nWAC \u2264 max z\u2208Z\u2217 Cm \u2212 p(z)Dm 1\u2212 p(z)\n(54)\nBecause Cm and Dm is constant across different z, and Dm \u2265 Cm by eqs. (45), (46) and (48), increasing minz\u2208Z\u2217 p(z) decreases right hand side of eq. (54). By eqs. (28) and (29) and nonnegativity of \u2202I(S;1z)\u2202p(z) , increasing LSEPIN could lead to higher minz\u2208Z\u2217 p(z) thus lower WAC.\nTheorem 3.2. When 1. the optimal state distribution for the downstream task is far from p(S) and 2. The state space is large, i.e. |S| is large. ICz of all learned skills can be considered equally contribute to WAC.\nProof. Lemma C.1 shows the direct correlation between WAC and LSEPIN under assumptions in eq. (46) and eq. (45).\nC in Eq. 46 could assume the feasible optimal state distribution for downstream task rz to have maximum \u201cdistance\u201d from the average distribution p(S). It is common in practice that the optimal state distributions for downstream tasks are \u201cfar\u201d from the average state distribution p(S) of MISL solutions. Eq. 45 assumes a constant Dm, this can apply to practical situations when |S| is large. By definition of Rz in theorem 3.1, we can see\nmin r\u2208Rz DKL(p(S|z)||pr) = max z\u2032 \u0338=z max r\u2208Rz DKL(p(S|z\u2032)||pr) = Dm(z) (55)\nlet Z \u2032 denote the set containing learned skills that maximizes maxz\u2032 \u0338=z maxr\u2208Rz DKL(p(S|z\u2032)||pr) and z\u2032 \u0338= z. By eq. (48), we have\nDm(z) \u2264 Cm p(z) + \u2211\nz\u2032\u2208Z\u2032 p(z \u2032)\n(56)\nBecause eq. (46) assumed pr to be on a \"maximum circle\" centered in p(S), pr \u2217\nshould have |S| \u2212 2 degrees of freedom, where r\u2217 solves minr\u2208Rz DKL(p(S|z)||pr). Including z, there should be |S|\u22121 skills that satisfies DKL(p(S|z\u2032)||pr \u2217 ) = Dm(z) to determine pr \u2217 and Dm(z), which together have |S| \u2212 1 degrees of freedom. So we have |Z \u2032| = |S| \u2212 2 excluding z, and\nDm(z) \u2264 Cm p(z) + \u2211\nz\u2032\u2208Z\u2032 p(z \u2032)\n= Cm 1\u2212 \u2211\nz\u2032\u2032\u2208Z\u2217\\Z\u2032\\{z} p(z \u2032\u2032)\n(57)\nWithout loss of generality, we consider situations when |Z\u2217| = |S|, when Z\u2217 \\ Z \u2032 \\ {z} contains only one skill. We denote it z\u2032\u2032, then:\nDm(z) \u2264 Cm\n1\u2212 p(z\u2032\u2032) (58)\nBecause optimizing LSEPIN increases p(z) for all z \u2208 Z\u2217, the larger |S| is, the tighter the bound in eq. (58) is. And because Dm(z) \u2265 Cm by definition, when |S| is large, all Dm(z) can be considered as constant close to Cm.\nTherefore, combining the above analysis on the assumptions of eqs. (45) and (46) and lemma C.1, we proved the claim of theorem 3.2.\nC.4 HOW I(S;1z) AFFECTS THE LEARNED SKILLS WHEN MISL IS NOT OPTIMIZED\nAs mentioned by the proof of theorem 3.1 in appendix C.1, the claim that higher I(S;1z) leads to lower ICz applies also to any skills sets satisfying the necessary conditions:\n1. All Zi share the same average state distribution p(S) = Ez\u2032\u2208Zi [p(S|z\u2032)] 2. DKL(p(S|z\u2032) \u2225 p(S)) is constant for all z\u2032 in all Zi. 3. All Zi share the same skill z\nIn practice, there could be a gap between optimal maxp(Z) Ep(Z)DKL(p(S|z) \u2225 p(S)) and a learned suboptimal dm = maxi Ez\u2208ZiDKL(p(S|z) \u2225 Ez\u2032\u2208Zi [p(S|z\u2032)]), where Zi are the skill sets that can be learned. This gap could be a result of a limitation on the skill number |Zi| or a result of an optimization error.\nIn order to maximize I(S;Z) = Ep(Z)DKL(p(S|z) \u2225 p(S)) The learned p(Z) would be positive only for the skills that have DKL(p(S|z) \u2225 p(S)) = dm. For two skill sets Z1,Z2 maximizing I(S;Z) to the same suboptimal value dm, they satisfy the necessary condition 2 since every skill has DKL(p(S|z) \u2225 p(S)) = dm. Exploration is important when I(S;Z) is suboptimal as empirically shown in Laskin et al. (2021), If Z1,Z2 satisfy the necessary condition 1 and share the same p(S), they would have the same exploration of the state space.\nTherefore, our theoretical results in theorems 3.1 and 3.2 also apply to practical situations when MISL is not optimally solved, and they showed that when two sets have the same exploration while maximizing I(S;Z) to the same degree, the set with better LSEPIN is preferred for downstream task adaptation."
        },
        {
            "heading": "D UNIQUENESS OF THE \u201cCIRCLE\u201d LEARNED FROM MISL",
            "text": "Maximizing I(S;Z) leads to unique average state distribution p(S), and it can be proven by the following:\n1. By Lemma 6.5 of Eysenbach et al. (2022), MISL objective is equivalent to minp(S) maxz DKL(p(S|z) \u2225 p(S)).\n2. Because of the strict convexity of KL divergence, maxz DKL(p(S|z) \u2225 p(S)) is also strictly convex.\n3. p(S) is constrained in a convex polytope C.\nTherefore it is a convex optimization problem with a strictly convex objective function. The solution is unique. With unique p(S) to be the \"center\", the \"circle\" of maximum \"radius\" DKL(p(S|z) \u2225 p(S)) is also unique."
        },
        {
            "heading": "E DIFFERENCE BETWEEN DISENTANGLEMENT FOR REPRESENTATION LEARNING AND SKILL LEARNING SETTING",
            "text": "Kim et al. (2021) adopted metrics SEPIN@k and WSEPIN (Do & Tran, 2019a) from representation learning to measure the disentanglement of the skill latent. They focused on the informativeness and separability (independencies) between different dimensions of the representation.\nSEPIN@k is the top k average of I(S,Zi|Z \u0338=i) while WSEPIN is the average of I(S,Zi|Z \u0338=i) weighed on I(S,Zi), where Zi denotes ith dimension of the skill latent.\nHowever, by I(S,Zi|Z \u0338=i) they care about how to efficiently represent the skill with the latent representation rather than promoting the separability and diversity of the trajectories induced by different skills. It can be maximized by only optimization in the latent space regardless of the state distributions in the state space. Therefore, this metric does not directly encourage diversity and separability of state distributions of skills thus not contributing to adaptation costs.\nIf we directly adopt I(S,Zi|Z \u0338=i) for our setting, Z should be a one-hot vector. We have I(S,Zi|Z\u0338=i) = H(S|Z \u0338=i)\u2212H(S|Zi, Z\u0338=i),\nBecause Z is a one-hot vector, Zi is completely dependent on Z \u0338=i, i.e. knowing Z \u0338=i also means knowing Zi. Therefore, H(S|Zi, Z\u0338=i) = H(S|Z \u0338=i) and I(S,Zi|Z \u0338=i) is constantly zero and we can\nnot directly use the disentanglement metrics composed of I(S,Zi|Z \u0338=i) to explicitly measure the diversity and separability of learned skills.\nInstead, we propose to use I(S;1z) to measure the informativeness and separability of each individual skill z, where 1z is the binary indicator for Z = z. It is a mutual information metric intrinsically measuring informativeness. We will show that it also measures separability.\nIn the URL skill learning setting, separability between one skill and others entails that the states inferred by this skill should share little overlap with the states inferred by others. Therefore, it means that the states should be certain to be inferred by one single skill, thus meaning low H(1z|S). Because I(S;1z) = H(S)\u2212H(S|1z), for situations with the same state and skill distributions, the one with larger I(S;1z) would be lower in H(S|1z). since H(1z|S) = H(S|1z) +H(1z)\u2212H(S), lower H(S|1z) means also low in H(1z|S) thus better separability. Moreover, a larger I(S;1z) explicity encourages a larger DKL(p(S|Z \u0338= z) \u2225 p(S|z)). By the proof of eq. (40) in Appendix B.1, we have shown when I(S;Z is fixed a higher I(S;1z) results in a higher DKL(p(S|Z \u0338= z) \u2225 p(S)). By convexity of KL divergence:\nDKL(p(S|Z \u0338= z) \u2225 p(S)) \u2264 p(z)DKL(p(S|Z \u0338= z) \u2225 p(S|z))+ (1\u2212 p(z))DKL(p(S|Z \u0338= z) \u2225 p(S|Z \u0338= z))\n(59)\n\u2264 p(z)DKL(p(S|Z \u0338= z) \u2225 p(S|z)) + 0 (60) < p(z)DKL(p(S|Z \u0338= z) \u2225 p(S|z)) (61)\nSo increasing I(S;1z) also increases the lower bound of DKL(p(S|Z \u0338= z) \u2225 p(S|z)), thus explicitly encouraging skills to be separated from each other. Therefore, we compose metrics from I(S;1z) to measure the disentanglement of learned skills."
        },
        {
            "heading": "F PROOF FOR SKILL LEARNING WITH WASSERSTEIN DISTANCES",
            "text": "When using KL divergence, solving eq. (2) learns skills that lie at the vertices of the polytope. This is actually because of the strict convexity KL divergence not mentioned by prior work (Eysenbach et al., 2022), which leads to\nDKL(\u03bbp1 + (1\u2212 \u03bb)p2 \u2225 p(S)) < \u03bbDKL(p1 \u2225 p(S)) + (1\u2212 \u03bb)DKL(p2 \u2225 p(S)),\n(62)\nwhere 0 < \u03bb < 1 and p1 \u0338= p2. Therefore, the skills at the vertices of the convex polytope always have higher KL divergence from the average state distribution than other skills within the polytope or on the faces of the polytope, because those skills are just convex combinations of the skills at the vertices.\nIn general, Wasserstein distance W (\u00b7, p) is commonly considered to be convex (Santambrogio, 2015; Peyr\u00e9 & Cuturi, 2019). And the common and 2-Wasserstein distances are considered strictly convex by (2.12) of Carlen & Gangbo (2003). In practice, for image observations, similar to Liu & Abbeel (2021a;b) that apply L2 norm-based particle based-entropy (Singh et al., 2003b) in the representation space, we can also apply the 2-Wasserstein distance-based metrics to the representations of observations. So it is practical to consider strictly convex Wasserstein distances for our analysis."
        },
        {
            "heading": "F.1 ANALYSIS ON THE BASIC WDSL OBJECTIVE",
            "text": "We can trivially replace the KL divergences in the MISL objective eq. (2) with Wasserstein distance and obtain a basic WDSL objective\nmax p(z)\nEp(z) [W (p(S|z), p(S))] . (63)\nSimilar to Lemma 6.5 of Eysenbach et al. (2022), optimizing eq. (9) is equivalent to solving\nmin p(S) max p(z)\nW (p(S|z), p(S)). (64)\nWe name it Average Wasserstein skill learning Distance (AWD). Therefore, when the adaptation cost for downstream tasks is defined as the Wasserstein distance between the state distributions of the initialization policy and the optimal policy since maximizing AWD learns an average state distribution that is close to worst case policy, this average state distribution will be a worst-case robust initialization.\nWe have the following Lemma: Lemma F.1. When AWD is maximized, all learned skills with p(z) > 0 must lie at the vertices of the polytope.\nProof. If a skill is not at a vertex of the polytope, it can be represented by a strictly convex combination of some vertices, and for any skill z not at a vertex,\nW (p(S), p(S|z)) < \u2211 z\u2032\u2208Vz \u03bbz\u2032W (p(S), p(S|z\u2032)) \u2264 max z\u2032\u2208Vz W (p(S), p(S|z\u2032)), (65)\nwhere Vz is the set containing the vertices such that z is within the convex hull of them, namely p(S|z) = \u2211 z\u2032\u2208Vz \u03bbz\u2032p(S|z \u2032), 0 < \u03bbz\u2032 < 1 for all z\u2032 \u2208 Vz and \u2211\nz\u2032\u2208Vz \u03bbz\u2032 = 1. The first inequality is by strict convexity, and the second is because the maximum should be not less than the mean.\nWe can see when p(S|z) is not at one of the vertices, there must be a vertex with a higher distance to p(S). Therefore, just like MISL, WDSL with AWD also discovers skills at the vertices.\nAlthough optimizing AWD discovers vertices like MISL, it also suffers from the limitation of remark 3.2.1 because it also only discovers vertices at the maximum \u201ccircle\u201d with maximum Wasserstein distance to the average."
        },
        {
            "heading": "F.2 PROOF FOR LEMMA 3.3",
            "text": "Lemma 3.3. When WSEP is maximized, all learned skills with p(z) > 0 must lie at the vertices of the polytope.\nProof. Similar to the proof of lemma F.1, for any skill zj , when p(S|zj) is not at the vertices, it can be represented by a strictly convex combination of some vertices, and for all zi \u2208 Z ,\nW (p(S|zi), p(S|zj)) < \u2211\nz\u2032\u2208Vzj\n\u03bbz\u2032W (p(S|zi), p(S|z\u2032)) \u2264 max z\u2032\u2208Vz W (p(S), p(S|z\u2032)), (66)\nwhere Vzj is the set containing vertices such that zj is within the convex hull of them, namely p(S|zj) = \u2211 z\u2032\u2208Vzj \u03bbz\u2032p(S|z\u2032), 0 < \u03bbz\u2032 < 1 for all z\u2032 \u2208 Vzj and \u2211 z\u2032\u2208Vzj \u03bbz\u2032 = 1.\nWhen p(S|zj) is not at one of the vertices, there must be a vertex with a higher distance to p(S|zi) for all zi \u2208 Z . Therefore, WSEP is not maximized when there is still a skill not at the vertices."
        },
        {
            "heading": "F.3 PROOF FOR THEOREM 3.4",
            "text": "Theorem 3.4. When WSEP is maximized by |Z\u2217| skills, the MAC can be upper-bounded: MAC \u2264 \u2211 z\u2208Z\u2217 L z V \u2212WSEP\n|V \\ Z\u2217||Z\u2217| (12) MAC \u2264 \u2211\nz\u2208Z\u2217 LV \u2212WSEP |V \\ Z\u2217||Z\u2217| , (13)\nwhere\nLzV = \u2211 v\u2208V W (p(S|v), p(S|z))\nLV =max v\u2032\u2208V \u2211 v\u2208V W (p(S|v), p(S|v\u2032)) (14)\nProof. By definition,\nMAC \u2264 \u2211 t\u2208V\\Z \u2211\nz\u2208Z W (p(S|t), p(S|z)) |V \\ Z||Z| . (67)\nAnd we can see \u2211 t\u2208V\\Z \u2211 z\u2208Z W (p(S|t), p(S|z))\n= \u2211 z\u2208Z LzV \u2212 \u2211 z\u2208Z \u2211 z\u2032\u2208Z W (p(S|z\u2032), p(S|z))\n= \u2211 z\u2208Z LzV \u2212WSEP.\n(68)\nThen the first inequality eq. (12) is proved by combining eq. (67) and eq. (68). Because by definition, for all z \u2208 Z , LzV \u2264 LV , so the second inequality eq. (13) also holds."
        },
        {
            "heading": "F.4 PROOF FOR THEOREM 3.5 AND THE VERTEX DISCOVERY PROBLEM",
            "text": ""
        },
        {
            "heading": "F.4.1 PROOF AND ALGORITHM FOR THEOREM 3.5",
            "text": "Theorem 3.5. When V is the set of all vertices of the feasible state distribution polytope C, all |V| vertices can be discovered by |V| iterations of maximizing\nPWSEP(i) : min \u03bb\nW ( p(S|zi), \u2211 zj\u2208Zi \u03bbjp(S|zj) ) , (15)\nwhere Zi is the set of skills discovered from iteration 0 to i\u2212 1 and zi is the skill being learned at ith iteration. \u03bb is a convex coeffcient of dimension i\u2212 1 that every element \u03bbj \u2265 0,\u2200j \u2208 {0, 1, .., i\u2212 1} and \u2211 j\u2208{0,1,..,i\u22121} \u03bb j = 1.\nIn the initial iteration when Zi = \u2205, PWSEP(0) can be W (p(S|z0), p(S|zrand)) with zrand to be a randomly initialized skill.\nProof. We first see that for the initial iteration, by eq. (66), maximizing W (p(S|z0), p(S|zrand)) learns a z0 at one of the vertices.\nThen in later iterations when Zi is not empty, we need to prove maximizing PWSEP(i) can learn a new vertex that was not discovered before.\nFor every skill z\u2032i not at the vertices and\n\u03bb\u2032 = argmin \u03bb\nW ( p(S|z\u2032i), \u2211 zj\u2208Zi \u03bbjp(S|zj) ) , (69)\nby eq. (66) there must be a vertex vi to satisfy\nW ( p(S|vi), \u2211 zj\u2208Zi \u03bb\u2032jp(S|zj) ) > W ( p(S|z\u2032i), \u2211 zj\u2208Zi \u03bb\u2032jp(S|zj) )\n(70)\nSo the solution of PWSEP(i) at every iteration must lie at one of the vertices. The remaining is to prove that every new iteration discovers a new vertex.\nIf the newly discovered vertex zi is not new and has been discovered before, it should be in Zi, and PWSEP(i) would be 0.\nAn undiscovered new vertex v at iteration i should not be in the convex hull of Zi: If the convex hull of Zi contains v, the vertex v should be either one of the points in set Zi or a strict convex combination of points in Zi. Since the polytope C is convex, its vertices can not be a strict convex combination of other points in C, the vertex v can only be in the set of Zi as an already discovered vertex if it is in the convex hull of Zi.\nTherefore, when the set Zi does not contain vi, vi should not be in the convex hull of Zi, resulting\nmin \u03bb\nW ( p(S|vi), \u2211 zj\u2208Zi \u03bbjp(S|zj) ) > 0 (71)\nThen, if there still exists undiscovered new vertices, optimal PWSEP(i) would be larger than 0, and optimizing PWSEP(i) finds a new vertex at each new iteration.\nIn conclusion, PWSEP discovers a new vertex at each new iteration, so it can discover all |V| vertices with |V| iterations of policy learning.\nOther statistical distances like total variation distance and Heilinger distance are also true metrics satisfying symmetry, and triangle inequality, but they are not strictly convex. KL divergence is strictly convex, and the information projection p\u2217 = argminp\u2208P DKL(p \u2225 q) has a unique solution for the convex P set, so it seems that the Wasserstein distance projection in the PWSEP algorithm can be replaced by information projection. however, when the new skill zi does not cover all the states in the support of distributions in Zi, which could happen a lot in practice, this projected KL divergence would be infinite and no longer convex in the union of their supports. For the moment projection, which is the reverse information projection p\u2217 = argminp\u2208P DKL(q \u2225 p), it would also be infinite whenever the new skill zi discovers new states that are not in the support of distributions in Zi. Notice that, the sum of the projected Wasserstein distances\nSPWD = \u2211\nzm\u2208Z min \u03bb\nW ( p(S|zm), \u2211 zn\u2208Z\\{zm} \u03bbnp(S|zn) )\n(72)\nis not an evaluation metric for skill learning. This means that a higher value of SPWD does not mean better skills for downstream task adaptation. An extreme example is that for |S| = 3 and the polytope C is a Chiliagon with |V| = 1000 vertices. Then the optimal skill set covering all 1000 vertices would have an SPWD near zero. For another skill set with 999 skills at the same vertex and 1 skill at another vertex with maximum distance from the prior one, then it is possible that this set could have a greater SPWD than the optimal skill set and this skill set is clearly not optimal since it only covers 2 vertices.\nTherefore, unlike I(S;Z), LSEPIN, and WSEP, SPWD is not an evaluation metric and can not be used as an optimization objective for unsupervised skill learning. To discover all vertices, the projected Wasserstein distance needs to be optimized iteratively as the PWSEP algorithm.\nAlgorithm 1 PWSEP algorithm Initialize a random point \u03c0rand, a real value v > 0 i\u2190 0 \u03c0zi \u2190 argmax\u03c0zi W (p(S|zi), p\n\u03c0rand) \u25b7 Initial iteration \u03a01 \u2190 {\u03c0zi} while v>0 do\ni\u2190 i+ 1 \u03c0zi \u2190 argmax\u03c0zi PWSEP(i) \u25b7 eq. (15) \u03a0i+1 \u2190 \u03a0i \u222a {\u03c0zi} v \u2190 maxzi PWSEP(i)\nend while\nAlgorithm 1 is the algorithm described in theorem 3.5, its practical implementation is discussed in appendix G.3 and its empirical performance is discussed in appendix H."
        },
        {
            "heading": "F.4.2 ABOUT RELATED WORK ON VERTEX DISCOVERY",
            "text": "To the best of our knowledge, there has been a previous work of successor feature-based method SFOLS (Alegre et al., 2022) claiming to solve the vertex discovery problem. However, we found their claim questionable.\nSFOLS is designed to find a CCS set defined in Eq.(22) from Appendix.A.5 in their paper. When written in our notation, it would be\nCCS = {\u03c1\u03c0z |\u2203w, s.t.\u03c1\u03c0zw \u2265 \u03c1\u03c0 \u2032 zw}, (73)\nwhere \u03c1\u03c0z is the occupancy measure of skill z, w is equivalent to the reward function of states r. This definition of CCS also applies to any skill that lies at the edges or faces of the polytope, not necessarily at the vertices, so SFOLS is not guaranteed to solve the vertex discovery problem with a finite number of skills.\nEven for their more rigorous definition in their Eq.(6), where they constrained the skills to be in the non-dominated multi-objective set F , it still does not exclude skills at the edges or faces, because vertices do not Pareto dominate the points at one of its edges, i.e. there always exist tasks on which the vertices have fewer accumulative rewards than points at their edges.\nTherefore, even with a number of skills |Z| greater than the number of vertices |V|, SFOLS is not guaranteed to discover all vertices and solve the vertex discovery problem, while PWSEP is guaranteed to discover all |V| vertices with only |V| skills. To contain only vertices, a set modified from CCS should be:\n{\u03c1\u03c0z |\u2203W, s.t.|W| \u2265 |S| \u2212 1, its elements are linearly independent and \u2200w \u2208 W, \u03c1\u03c0zw \u2265 \u03c1\u03c0 \u2032 zw}, (74) A vertex should be optimal for any task that has optimal solutions on its adjacent edges or faces, so a vertex should be optimal for at least |S| \u2212 1 linearly independent w. This could inspire future successor feature algorithms to learn more general policies that lie at the vertices instead of faces of polytope C. Moreover, there are fundamental differences between our setting of skill discovery and the setting of successor feature methods. For unsupervised skill learning with MISL or Wasserstein distance, the latent representation for skills is only used for indexing, its exact value does not affect the state distributions of the learned skills; While for the successor feature setting, the value of the weight directly determines its associated state distribution, which is the reason why optimization in the weight space is required for the successor feature methods.\nThat is to say, for an ideal unsupervised skill learning algorithm like PWSEP, it should be able to discover |V | vertices with a skill space containing only |V | points. This is impossible for successor feature learning that needs a weight (used as skills) space R|S| (Appendix.A.5 of Alegre et al. (2022)) containing infinite points.\nHowever, successor feature methods can be considered directly solving eq. (2) instead of eq. (1). Because each weight w determines an optimal parameter \u03b8(w) for its optimal policy \u03c0\u03b8(w). Equation (1) for successor feature setting becomes:\nmax \u03b8,p(W )\nI(S;W ) = Ep(W )[DKL(p(S|w, \u03b8) \u2225 p(S, \u03b8))]\n= Ep(W )[DKL(p(S|w, \u03b8(w)) \u2225 Ep(W )[p(S|w, \u03b8(w))])] = Ep(W )[DKL(p(S|w) \u2225 p(S))]\n(75)\nTherefore, methods that combine successor features with MISL such as Hansen et al. (2020); Liu & Abbeel (2021b) are the practical algorithms closer to the setting (optimizing eq. (2)) of our theoretical analysis."
        },
        {
            "heading": "F.5 EXAMPLE WHEN HIGHER WSEP RESULTS IN HIGHER MAC",
            "text": "In fig. 4. It shows a polytope with 4 vertices in a 3-dimensional space. Suppose in a situation where two skills are learned, and the learned skills are at vertices a and c. This situation would be optimal for maximizing WSEP, its MAC is the mean of the length of edge cd and edge cb, and its worst-case adaptation cost is the length of edge cb. Then for another situation where two skills lie at vertices a and d, it has a lower WSEP, but its MAC is the mean of the length of edge cd and edge db, which is lower than the former situation. Its worst-case adaptation cost is the length of edge cd, which is also lower. In this case, LdV < L c V , so from\n26\nskill combination a and c to skill combination a and d, although WSEP decreased, \u2211 z\u2208Z L z V \u2212WSEP also decreased due to the change of LzV , resulting in tighter upper bound in eq. (12).\nFor a practical example, this situation can be an environment with a state space of one-dimensional integers. If you have two skills, for maximum WSEP, they would put their probability mass close to \u2212\u221e and\u221e , which could be infinitely far away from any sampled downstream task optimal distribution."
        },
        {
            "heading": "F.6 WSEP CAN NOT ALWAYS DISCOVER ALL VERTICES",
            "text": "For a |S| = 4 case, suppose the polytope C of feasible state distributions is 3-dimensional and there are 4 vertices {v1, v2, v3, v4}, and\n\u2211 v\u2208{v1,v3} W (p(S|v), p(S|v2)) << \u2211 v\u2208{v2,v3} W (p(S|v), p(S|v1)), (76)\nthen if v4 is very close to v2, comparing v4 to a point v\u2032 on the surface of v1, v2, v3 and very close to v1, it is possible that\n\u2211 v\u2208{v1,v2,v3} W (p(S|v), p(S|v4)) \u2248 \u2211 v\u2208{v1,v3} W (p(S|v), p(S|v2))\n< \u2211\nv\u2208{v1,v2,v3}\nW (p(S|v), p(S|v\u2032)) \u2248 \u2211\nv\u2208{v2,v3}\nW (p(S|v), p(S|v1)). (77)\nThe skill set of {v1, v2, v3, v\u2032} can result in higher WSEP than {v1, v2, v3, v4}, so maximizing WSEP can not always discover all vertices."
        },
        {
            "heading": "F.7 ABOUT WSEP FORMULATION WITH KL DIVERGENCES",
            "text": "Defined in remark 3.4.2 KLSEP = \u2211 zi\u2208Z \u2211 zj\u2208Z,i\u0338=j DKL(p(S|zi) \u2225 p(S|zj))\nis a sum of DKL(p(S|zi) \u2225 p(S|zj)) +DKL(p(S|zj) \u2225 p(S|zi))\n.\nBecause KL divergence does not satisfy the triangle inequality, it is possible that\nDKL(p(S|zi) \u2225 p(S|zj)) +DKL(p(S|zj) \u2225 p(S|zi)) >DKL(p(S|zi) \u2225 p(S|zk)) +DKL(p(S|zk) \u2225 p(S|zi)) +DKL(p(S|zi) \u2225 p(S|zk)) +DKL(p(S|zk) \u2225 p(S|zi)).\n(78)\nThen learning another skill close to zi or zj could result in a higher sum of these KL divergences than learning a new skill zk that is far from zi and zj , resulting in poor diversity and separability."
        },
        {
            "heading": "G PRACTICAL ALGORITHM DESIGN",
            "text": "Although our main contributions are theoretical analyses, we provide some methods here for approximation of the proposed metrics and show how they can be used for practical algorithm design.\nTable 2 shows an overview of methods to approximate the metrics."
        },
        {
            "heading": "G.1 PRACTICAL ALGORITHM TO INCORPORATE LSEPIN",
            "text": "For practical algorithms, LSEPIN could play an important role since it measures the informativeness and separability of individual skills and it encourages every learned skill to be distinctive and potentially useful. It is totally feasible to approximate LSEPIN with Particle-Based Entropy (PBE) (Singh et al., 2003a) (details in appendix G.5) or a parametric discriminator that takes state and skill as input D\u03d5(s, z). This discriminator can be learned by:\nmax \u03d5\nEz\u223cp(Z) [ E(s,z\u2032)\u223cp(S|Z \u0338=z)[log (1\u2212D\u03d5(s, z))] + Es\u223cp(S|z)[logD\u03d5(s, z)] ] (79)\nThen LSEPIN can be estimated by:\nmin z\u2208Z\nI(S;1z) = H(1z)\u2212H(1z|S) (80)\n= constant+ Es\u223cp(S|z)[log p(1z|S)] (81) \u2248 constant+ Es\u223cp(S|z)[logD\u03d5(s, z)] (82)\nPractical algorithms solve eq. (1) instead of eq. (2), so the desired state distribution of skills p\u03c0\u03b8 (S|Zinput) is determined by the both p(Zinput) and learned \u03b8. In order to let every input skill zinput have equal steps of roll-out, the input skill distribution p(Zinput) is often set as a uniform prior (Eysenbach et al., 2019; Sharma et al., 2020). Therefore, H(1z) can be considered as a constant. Then the intrinsic reward to increase LSEPIN can be logD\u03d5(s, z) for states inferred by skill z, and log(1\u2212D\u03d5(s, z)) for states not inferred by skill z."
        },
        {
            "heading": "G.2 PRACTICAL ALGORITHM FOR WSEP",
            "text": "How to efficiently compute the Wasserstein Distances between states of different skills is fundamental for WSEP. In He et al. (2022), approaches like Sliced Wasserstein Distances (SWD) (Kolouri et al., 2019) and Projected Wasserstein distance (PWD) (Rowland et al., 2019) have been demonstrated useful for unsupervised skill discovery. Here we propose an alternative approach to inspire future algorithm design. We can parametrize a test function that also takes state and skill as input D\u03d5(s, z), and this test function is learned by:\nmin \u03d5\nEz\u223cp(Z) [ E(s,z\u2032)\u223cp(S|Z \u0338=z)[D\u03d5(s, z)]\u2212 Es\u223cp(S|z)[D\u03d5(s, z)] ] + \u03bbE(s,z)\u223cP\u0302 (s,z)[(\u2225\u2207sD\u03d5(s, z)\u22252 \u2212 1) 2] (83)\nwhere the last is a Lagrange term that restricts the test function to be 1-Lipschitz with respect to s, and P\u0302 (s, z) is the distribution of interpolated samples between states inferred by skill z and states not inferred by skill z. Test function D\u03d5 learned by this loss can be used to estimate the dual form Peyr\u00e9 & Cuturi (2019) of the 1-Wasserstein distance between states inferred or not inferred by any skill z. Then, the intrinsic reward to increase WSEP can be D\u03d5(s, z)\u2212 Ez\u2032 \u0338=zD\u03d5(s, z\u2032) for states inferred by skill z.\nWe can see that both eq. (79) and eq. (83) share similarities with the discriminator loss in generative adversarial networks (GANs) (Goodfellow et al., 2020) and Wasserstein GANs (Arjovsky et al., 2017). Therefore, it is encouraging better separability between states inferred by different skills thus resulting in more distinctive skills."
        },
        {
            "heading": "G.3 PRACTICAL IMPLEMENTATION FOR PWSEP",
            "text": "The key to implementing algorithm 1 for PWSEP is how to efficiently compute the projection\nPWSEP(i) = min \u03bb\nW ( p\u03c0\u03b8i (S|zi), \u2211 zj\u2208Zi \u03bbjp\u03c0\u03b8j (S|zj) )\n(84)\nWe can see that this problem is convex for \u03bb. The derivative \u2202PWSEP(i)\u2202\u03bb can be approximated by sensitivity analysis of the linear programming W ( p\u03c0\u03b8i (S|zi), \u2211 zj\u2208Zi \u03bb jp\u03c0\u03b8j (S|zj) )\n. However, with a larger batch size B of (s, z) samples, it could be infeasible to obtain this derivative from implicit function theorem and KKT conditions like the approach from OptNet (Amos & Kolter, 2017), because each derivative calculation requires solving an inverse or pseudo-inverse of a B dimensional matrix. We have compared it with the derivative-free method CMA-ES (Hansen & Ostermeier, 2001), finding that although CMA-ES computes the Wasserstein distance multiple times for one update of \u03bb, the total time cost of CMA-ES is much more efficient in terms of time cost. Besides, because we have separated parameter \u03b8i for each skill policy, and p\n\u03c0\u03b8j (S|zj) of other skills remain unchanged when updating \u03b8i, the optimal \u03bb after an \u03b8i update would be close to the optimal \u03bb before the update. Therefore, the computation for \u03bb to be optimal could be large at the beginning then it gets small."
        },
        {
            "heading": "G.4 CHOICE OF TRANSPORTATION COST FOR WASSERSTEIN DISTANCE",
            "text": "Wasserstein distance provides stable and smooth measurements when the transportation cost can be defined meaningfully. For example, in environments with physical dynamics, the transportation cost between states could be defined as the physical distance, in certain discrete environments, it can be defined as the Manhattan distance. These are not only more intuitive but also provide smooth and stable measurements with meaningful information on how difficult it is for an agent to reach from one state to the other. However, the choice of them requires prior knowledge of the environment. For completely unsupervised transport cost design, our idea for future work is to learn a representation so that the basic distances like L2 in the representation space can capture how difficult it is to travel from one state to another, which could be related to previous work about graph Laplacian representation (Wu et al., 2019). Their method uses spectral graph drawing (Koren, 2003) to learn a state representation so that dynamically consecutive states have small L2 distance to each other in the representation space."
        },
        {
            "heading": "G.5 PRACTICAL NON-PARAMETRIC ESTIMATION OF LSEPIN",
            "text": "LSEPIN can be approximated by particle-based entropy (Singh et al., 2003a; Liu & Abbeel, 2021b):\nI(S;1z) \u2248 H\u0302PB(S)\u2212 H\u0302PB(S|1z). (85)\nWhere H\u0302PB(\u00b7) is the particle-based entropy that can be estimated as:\nH\u0302PB(S) := n\u2211 i=1 log c+ 1 k \u2211 s (j) i \u2208Nk(si) \u2225si \u2212 s(j)i \u2225  , (86) where Nk(\u00b7) denotes the k nearest neighbors around a particle, c is a constant for numerical stability (commonly fixed to 1).\nThe second term of Equation 85 is the state entropy conditioned on knowing whether skill equals z, and it is defined as\nH(S;1z) = H(S|Z = z) +H(S|Z \u0338= z). (87)\nThe first term of Equation 87 can be estimated by sampling from the states generated with skill z. The second term is a little more tricky for parametric methods that try to approximate log p(s|z), but for particle-based entropy, it can still be conveniently estimated by sampling states from all skills not equal to z."
        },
        {
            "heading": "H EMPIRICAL VALIDATION",
            "text": "We provide some empirical examples to validate the proposed theoretical results. For more intuition, we consider the skill trajectories in a visualized maze environment with continuous state and action spaces from Campos et al. (2020) and a higher dimensional Mujoco Ant environment (Todorov et al., 2012). Maze navigation and high-dimensional control are major test grounds for previous MISL methods (Eysenbach et al., 2019; Kim et al., 2021; Park et al., 2022b)."
        },
        {
            "heading": "H.1 CONTINUOUS MAZE ENVIRONMENT",
            "text": "To get an intuitive understanding of what kind of skills are encouraged by our proposed metrics, we first look at the trajectories in the maze environment.\nThe corresponding metrics for the agents in fig. 5 are listed in table 3. For stable estimation, the metrics are estimated by non-parametric approaches using a large number of samples. Metrics related to mutual information (I(S;Z) and LSEPIN) are estimated by particle-based entropy eq. (86) with n = 4000, k = 2000. WSEP is estimated by sliced Wasserstein distance (Kolouri et al., 2019) using 4000 samples of (s, z). The magnitude of particle-based entropy depends on the number of samples n because it is a summation instead of a mean, while the magnitude of sliced Wasserstein distance is a mean and it is less affected by the number of samples. As mentioned in appendix F.4, the sum of PWSEP(i) in eq. (72) can not serve as evaluation metrics like LSEPIN and WSEP, so we consider to evaluate with I(S;Z), LSEPIN and WSEP.\nParametric LSEPIN encourages the discriminability among skills, which is shown by comparing figs. 5a and 5b. The agent learned with parametric LSEPIN has less overlapping among skills thus better discriminability. WSEP encourages more Wasserstein distances between skills and skills with more distances from each other are also more discriminable. Because particle-based entropies are calculated by the distances between samples, which is the same as the transportation cost of Wasserstein distance, the mutual information calculated with particle-based entropies measures not\nonly discriminability but also some kind of \u201cdistance\u201d. This is why the WSEP agent has high particlebased LSEPIN. PWSEP learns the skills by the order of the Matplotlib (Hunter, 2007) color cycle, so the red skill is learned in the end, so it goes away from other skills instead of going downwards to the undiscovered branch of the tree maze, which could be a local optimum for PWSEP(i). In practice, because of local optimums and a limited number of skills, despite its favored theoretical properties, the PWSEP algorithm might perform the best.\nBy comparing figures in fig. 5 and table 3, we can see that LSEPIN and WSEP capture the diversity and separatability of skills."
        },
        {
            "heading": "H.2 ANT ENVIRONMENT",
            "text": "Ant is one of the most common environments for URL (Sharma et al., 2020; Park et al., 2022a; Kim et al., 2021). It is challenging high-dimension continuous control but the skills can be visualized by top-down view of x-y dimension. This environment has downstream tasks that require navigating the ant agent to designated positions. We compare the top-down visualizations, downstream task performance, and our proposed metrics to see whether the correlation claimed by our theorems exists.\nFigure 6 shows the top-down view of skills of three URL agents. Their skills are in different colors. Their corresponding metrics are shown in table 4. DADS origin was implemented with prior knowledge and specific tuning. For example, there is a specific logarithmic transformation of its intrinsic rewards which does not apply to our implementations.\n\u201cR\u201d is the downstream task performance, MSEPIN is the median of I(S;1z). Similarly, WSEP is estimated by sliced Wasserstein distance. I(S;1z) and I(S;Z) are estimated by particle-based entropy.\nThe correlation coefficient between downstream task performance and the metrics are shown in table 5. We can see that there is a strong correlation between disentanglement metrics (LSEPIN/WSEP) and downstream task performance. This empirically validates our proposed theorems which claim a correlation between disentanglement metrics and downstream task performance."
        },
        {
            "heading": "H.3 SCALABILITY OF PARAMETRIC OBJECTIVES",
            "text": "Although we are not focusing on the empirical performance, we find that the proposed practical algorithm design in appendix G has the scalability for a potentially larger number of skills.\nThe agents in fig. 7 are learned by 8 skills by our introduced practical algorithms. Empirical algorithm design is not our main focus but we can see that optimizing parametric WSEP (eq. (83)) alone has comparable state coverage as MISL with state entropy for exploration"
        },
        {
            "heading": "H.4 ABOUT UNSUPERVISED REINFORCEMENT LEARNING BENCHMARK",
            "text": "There exists an unsupervised reinforcement learning benchmark (URLB) (Laskin et al., 2021) using DeepMind Control environments, but their downstream task setting is not suitable for evaluating vanilla MISL methods that approximate and maximize I(S;Z). MISL with I(S;Z) is essentially partitioning the state space and labeling each part with a skill latent z. The downstream task in Laskin et al. (2021) focuses on movements like \u201cwalk\u201d, \u201crun\u201d and \u201cflip\u201d, which require learning a specific sequence of state progression instead of reaching a certain part of the state space. So it can be considered that the downstream task setting in URLB is too complicated for current MISL methods. And the downstream finetuning performance of pure exploration methods like ProtoRL Yarats et al. (2021), APT Liu & Abbeel (2021a), RND Burda et al. (2019) exceeds MISL methods from Lee et al. (2019); Eysenbach et al. (2019); Liu & Abbeel (2021b) for by large margin, this might be because the state distributions of the MISL learned skills cover fewer states that are useful for the downstream task than the pure exploration methods. The CIC method Laskin et al. (2022) optimizes a modified MISL objective I(\u03c4 ;Z) and outperforms the pure exploration methods, they claim CIC promotes better diversity and discriminability than previous MISL methods, which also accords with our results."
        }
    ],
    "year": 2023
}