{
    "abstractText": "The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide Images (WSIs) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. To address this, we propose the ContextAware Multiple Instance Learning (CAMIL) architecture. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving test AUCs of 97.5%, 95.9%, and 88.1%, respectively, outperforming other state-of-the-art methods. Additionally, CAMIL enhances model interpretability by identifying regions of high diagnostic value.",
    "authors": [],
    "id": "SP:363160f338c2e471f370ebc43d9b20695775cdc8",
    "references": [
        {
            "authors": [
                "Mohammed Adnan",
                "Shivam Kalra",
                "Hamid R. Tizhoosh"
            ],
            "title": "Representation learning of histopathology images using graph neural networks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR Workshops",
            "year": 2020
        },
        {
            "authors": [
                "A\u0131\u0308cha BenTaieb",
                "Ghassan Hamarneh"
            ],
            "title": "Predicting cancer with a recurrent visual attention model for histopathology",
            "venue": "21st International Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Richard Chen",
                "Fabian Both",
                "J\u00f6rg Franke",
                "Heidi K\u00fcsters-Vandevelde",
                "Willem Vreuls",
                "Peter Bult",
                "Bram van Ginneken",
                "Jeroen van der Laak",
                "Geert Litjens"
            ],
            "title": "From detection of individual metastases to classification of lymph node status at the patient level: The camelyon17 challenge",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2018
        },
        {
            "authors": [
                "Gabriele Campanella",
                "Matthew G. Hanna",
                "Luke Geneslaw",
                "Allen P. Miraflor",
                "Vitor Werneck Krauss Silva",
                "Klaus J. Busam",
                "Edi Brogi",
                "Victor E. Reuter",
                "David S. Klimstra",
                "Thomas J. Fuchs"
            ],
            "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images",
            "venue": "Nature Medicine,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey E. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Cheng",
                "Yuting Liu",
                "Wei Huang",
                "Wenhui Hong",
                "Lingling Wang",
                "Xiaohui Zhan",
                "Zhi Han",
                "Dong Ni",
                "Kun Huang",
                "Jie Zhang"
            ],
            "title": "Computational Image Analysis Identifies Histopathological Image Features Associated With Somatic Mutations and Patient Survival in Gastric Adenocarcinoma",
            "venue": "Frontiers in Oncology,",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Courtiol",
                "Eric W. Tramel",
                "Marc Sanselme",
                "Gilles Wainrib"
            ],
            "title": "Classification and disease localization in histopathology using only global labels: A weakly-supervised approach",
            "year": 2018
        },
        {
            "authors": [
                "Kausik Das",
                "Sailesh Conjeti",
                "Abhijit Guha Roy",
                "Jyotirmoy Chatterjee",
                "Debdoot Sheet"
            ],
            "title": "Multiple instance learning of deep convolutional neural networks for breast histopathology whole slide classification",
            "venue": "IEEE International Symposium on Biomedical Imaging, ISBI 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2015
        },
        {
            "authors": [
                "Le Hou",
                "Dimitris Samaras",
                "Tahsin M. Kur\u00e7",
                "Yi Gao",
                "James E. Davis",
                "Joel H. Saltz"
            ],
            "title": "Patchbased convolutional neural network for whole slide tissue image classification",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Maximilian Ilse",
                "Jakub M. Tomczak",
                "Max Welling"
            ],
            "title": "Attention-based deep multiple instance learning",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Gabriel Landini",
                "Giovanni Martinelli",
                "Filippo Piccinini"
            ],
            "title": "Colour deconvolution: stain unmixing in histological",
            "venue": "imaging. Bioinformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Bin Li",
                "Yin Li",
                "Kevin W Eliceiri"
            ],
            "title": "Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jiayun Li",
                "Wenyuan Li",
                "Anthony E. Sisk",
                "Huihui Ye",
                "W. Dean Wallace",
                "William Speier",
                "Corey W. Arnold"
            ],
            "title": "A multi-resolution model for histopathology image classification and localization with multiple instance learning",
            "venue": "Comput. Biol. Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Pei Liu",
                "Luping Ji",
                "Xinyu Zhang",
                "Feng Ye"
            ],
            "title": "Pseudo-bag mixup augmentation for multiple instance learning-based whole slide image classification, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ming Y Lu",
                "Drew FK Williamson",
                "Tiffany Y Chen",
                "Richard J Chen",
                "Matteo Barbieri",
                "Faisal Mahmood"
            ],
            "title": "Data-efficient and weakly supervised computational pathology on whole-slide images",
            "venue": "Nature Biomedical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Jaime Melendez",
                "Bram van Ginneken",
                "Pragnya Maduskar",
                "Rick H.H.M. Philipsen",
                "Klaus Reither",
                "Marianne Breuninger",
                "Ifedayo M.O. Adetifa",
                "Rahmatulai Maane",
                "Helen Ayles",
                "Clara I. S\u00e1nchez"
            ],
            "title": "A novel multiple-instance learning-based approach to computer-aided detection of tuberculosis on chest x-rays",
            "venue": "IEEE Trans. Medical Imaging,",
            "year": 2015
        },
        {
            "authors": [
                "Sandra Morales",
                "Kjersti Engan",
                "Valery Naranjo"
            ],
            "title": "Artificial intelligence in computational pathology - challenges and future directions",
            "venue": "Digit. Signal Process.,",
            "year": 2021
        },
        {
            "authors": [
                "Gw\u00e9nol\u00e9 Quellec",
                "Mathieu Lamard",
                "Michel Cozic",
                "Gouenou Coatrieux",
                "Guy Cazuguel"
            ],
            "title": "Multiple-instance learning for anomaly detection in digital mammography",
            "venue": "IEEE Trans. Medical Imaging,",
            "year": 2016
        },
        {
            "authors": [
                "Zhuchen Shao",
                "Hao Bian",
                "Yang Chen",
                "Yifeng Wang",
                "Jian Zhang",
                "Xiangyang Ji",
                "Yongbing Zhang"
            ],
            "title": "Transmil: Transformer based correlated multiple instance learning for whole slide image classification",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Zhuchen Shao",
                "Liuxi Dai",
                "Yifeng Wang",
                "Haoqian Wang",
                "Yongbing Zhang"
            ],
            "title": "Augdiff: Diffusion based feature augmentation for multiple instance learning in whole slide",
            "year": 2023
        },
        {
            "authors": [
                "Yash Sharma",
                "Aman Shrivastava",
                "Lubaina Ehsan",
                "Christopher A. Moskaluk",
                "Sana Syed",
                "Donald E. Brown"
            ],
            "title": "Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole slide image classification",
            "venue": "Medical Imaging with Deep Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Chetan L. Srinidhi",
                "Ozan Ciga",
                "Anne L. Martel"
            ],
            "title": "Deep neural network models for computational histopathology: A survey",
            "year": 1912
        },
        {
            "authors": [
                "David Tellez",
                "Geert Litjens",
                "Jeroen van der Laak",
                "Francesco Ciompi"
            ],
            "title": "Neural image compression for gigapixel histopathology image analysis",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2021
        },
        {
            "authors": [
                "Tong Tong",
                "Robin Wolz",
                "Qinquan Gao",
                "Ricardo Guerrero",
                "Joseph V. Hajnal",
                "Daniel Rueckert"
            ],
            "title": "Multiple instance learning for classification of dementia in brain MRI",
            "venue": "Medical Image Anal.,",
            "year": 2014
        },
        {
            "authors": [
                "Paul Tourniaire",
                "Marius Ilie",
                "Paul Hofman",
                "Nicholas Ayache",
                "Herv\u00e9 Delingette"
            ],
            "title": "MS-CLAM: mixed supervision for the classification and localization of tumors in whole slide images",
            "venue": "Medical Image Anal.,",
            "year": 2023
        },
        {
            "authors": [
                "Paul Tourniaire",
                "Marius Ilie",
                "Paul Hofman",
                "Nicholas Ayache",
                "Herv\u00e9 Delingette"
            ],
            "title": "Ms-clam: Mixed supervision for the classification and localization of tumors in whole slide images",
            "venue": "Medical Image Analysis,",
            "year": 2023
        },
        {
            "authors": [
                "Ming Tu",
                "Jing Huang",
                "Xiaodong He",
                "Bowen Zhou"
            ],
            "title": "Multiple instance learning with graph neural networks",
            "year": 1906
        },
        {
            "authors": [
                "Xiaodong Wang",
                "Ying Chen",
                "Yunshu Gao",
                "Huiqing Zhang",
                "Zehui Guan",
                "Zhou Dong",
                "Yuxuan Zheng",
                "Jiarui Jiang",
                "Haoqing Yang",
                "Liming Wang",
                "Xianming Huang",
                "Lirong Ai",
                "Wenlong Yu",
                "Hongwei Li",
                "Changsheng Dong",
                "Zhou Zhou",
                "Xiyang Liu",
                "Guanzhen Yu"
            ],
            "title": "Predicting gastric cancer outcome from resected lymph node histopathology images using deep learning",
            "venue": "Nature Communications,",
            "year": 2021
        },
        {
            "authors": [
                "Xinggang Wang",
                "Yongluan Yan",
                "Peng Tang",
                "Xiang Bai",
                "Wenyu Liu"
            ],
            "title": "Revisiting multiple instance neural networks",
            "venue": "Pattern Recognit.,",
            "year": 2018
        },
        {
            "authors": [
                "Chensu Xie",
                "Hassan Muhammad",
                "Chad M. Vanderbilt",
                "Raul Caso",
                "Dig Vijay Kumar Yarlagadda",
                "Gabriele Campanella",
                "Thomas J. Fuchs"
            ],
            "title": "Beyond classification: Whole slide tissue histopathology analysis by end-to-end part learning",
            "venue": "International Conference on Medical Imaging with Deep Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yunyang Xiong",
                "Zhanpeng Zeng",
                "Rudrasis Chakraborty",
                "Mingxing Tan",
                "Glenn Fung",
                "Yin Li",
                "Vikas Singh"
            ],
            "title": "Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention",
            "venue": "In Thirty-Fifth AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Li"
            ],
            "title": "Leveraging the pre-trained SimCLR weights lays the foundation for generating meaningful similarity scores and, therefore, is crucial for the optimal performance of the neighbor-constrained attention mask similar to many similaritybased histopathology approaches. Incorporating ImageNet weights directly into our model notably decreases its performance (as it does with most models Tourniaire et al",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning (DL) methods have revolutionized the development of highly accurate diagnostic machines (Morales et al., 2021) that rival or even surpass the performance of expert pathologists (Tong et al., 2014; Melendez et al., 2015; Quellec et al., 2016; Das et al., 2018; Srinidhi et al., 2019; Wang et al., 2021). These advancements have been facilitated by the emergence of weakly supervised learning, which eliminates the need for laborious pixel-level annotations. Models trained using weakly supervised learning, relying solely on slide-level labels, have demonstrated exceptional classification accuracy on whole slide imaging (WSI) data, paving the way for scalable computational decision support systems in clinical practice (Xu et al., 2014; Courtiol et al., 2018; Xu et al., 2019; Zhou et al., 2021).\nIn the context of cancer histopathology, WSIs are not processed as a single image by DL models. Instead, WSIs are frequently subdivided into smaller tiles, which serve as an input. The task is, then, to classify the WSI based on the features extracted from the individual tiles. Most current methods for weakly supervised WSI classification use the Multiple Instance Learning (MIL) framework, which considers each WSI as a \u2018bag\u2019 of tiles and attempts to learn the slide-level label without prior knowledge about the labels of the individual tiles.\nA major bottleneck in the deployment of MIL models, and the weakly-supervised learning paradigm in general, is that the MIL model is either permutation invariant, meaning that the tiles within a WSI exhibit no ordering among each other (Sharma et al., 2021; Xie et al., 2020), or permutation-aware without explicit information guidance. In other words, the spatial relationship of one tile to another is either ignored, or the dependencies between the tiles are implicitly modeled during training without requiring direct instructions (Shao et al., 2021; Landini et al., 2020; Campanella et al., 2019).\nHowever, explicit knowledge about a tile\u2019s spatial arrangement is particularly relevant in cancer histopathology, where cancer and normal cells are not necessarily distributed randomly inside an image. Contextual insights into the cellular landscape, such as the spatial dispersion of cells, the arrangement of cell clusters, and the broader characteristics of the tissue micro environment, provide\na more comprehensive view of the tile\u2019s local environment, enabling a better assessment of subtle variations and abnormalities that may indicate the presence of cancer.\nIn this paper, we propose a novel framework dubbed Context-Aware Multiple Instance Learning (CAMIL) to harness the dependencies among the individual tiles within a WSI and impose contextual constraints as prior knowledge on the multiple instance learning model. By explicitly accounting for contextual information, CAMIL aims to enhance the detection and classification of localized tumors and mitigate the potential misclassification of isolated or noisy instances, thereby contributing to an overall improvement in performance for both individual tiles and WSIs. Moreover, the attention weights enhance the interpretability of the model by highlighting sub-regions of high diagnostic value within the WSI."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Under the MIL formulation, the prediction of a WSI label (i.e., cancerous or not) can come either directly from the tile predictions (instance-based) (Campanella et al., 2019; Landini et al., 2020; Hou et al., 2016; Xu et al., 2019), or from a higher-level bag representation resulting from the aggregation of the tile features (bag embedding-based) (Ilse et al., 2018; Lu et al., 2021; Sharma et al., 2021; Wang et al., 2018). The bag embedding-based approach has empirically demonstrated superior performance (Sharma et al., 2021; Wang et al., 2018). Most recent bag embedding-based approaches employ attention mechanisms (Vaswani et al., 2017), which assign an attention score to every tile reflecting its relative contribution to the collective WSI-level representation. Attention scores enable the automatic localization of sub-regions of high diagnostic value in addition to informing the WSIlevel label (Zhang et al., 2021; BenTaieb & Hamarneh, 2018; Lu et al., 2021).\nAttention-based MIL models vary in how they explore tissue structure in WSIs. Many are permutation invariant, assuming the tiles are independent and identically distributed. Building upon this assumption, Ilse et al. (2018) proposed a learnable attention-based MIL pooling operator that computes the bag embedding as the average of all tile features in the WSI weighted by their respective attention score. This operator has been widely adopted and modified with the addition of a clustering layer (Lu et al., 2021; Li et al., 2021b; Yao et al., 2020) to further encourage the learning of semantically-rich, separable and class-specific features. Another variation of the same model uses \u2018pseudo bags\u2019 (Zhang et al., 2022), splitting the WSI into several smaller bags to alleviate the issue of the limited number of training data. Recently, data augmentation has been adopted to inflate the number of bags (Gadermayr et al., 2023; Liu et al., 2023; Shao et al., 2023).\nHowever, permutation invariant operators cannot inherently capture the structural dependencies among different tiles at the input. The lack of bio-topological information has partially been remedied by the introduction of feature similarity scores instead of positional encodings to model the mutual tile dependencies within a WSI (Xie et al., 2020; Tellez et al., 2021; Adnan et al., 2020). For instance, DSMIL (Li et al., 2021a) utilizes a non-local operator to compute an attention score for each tile by measuring the similarity of its feature representation against that of a critical tile. To consider the correlations between the different tiles of a WSI, transformer-based architectures have been introduced, which usually make use of a learnable position-dependent signal to incorporate the spatial information of the image (Zhao et al.; Tu et al., 2019). For instance, TransMIL (Shao et al., 2021) is a transformer-like architecture trained end-to-end to optimize for the classification task and produce attention scores while simultaneously learning the positional embeddings.\nIn CAMIL, we provide explicit guidance regarding the context of every tile as we argue that it can provide a valuable, rich source of information. Unlike most existing MIL approaches where the relationships developed between neighboring tiles are omitted, in our approach, we propose a neural network architecture that explicitly leverages the dependencies between neighboring tiles of a WSI by enforcing bio-topological constraints to enhance performance effectively."
        },
        {
            "heading": "3 MATERIAL AND METHODS",
            "text": "CAMIL operates on the principle that the context and characteristics of a tile\u2019s surroundings hold substantial potential for enhancing the accuracy of whole slide classification. To illustrate this concept, we can draw a parallel between our framework and the examination process of a pathologist\nanalyzing a biopsy slide. Similar to how a pathologist zooms out of a specific sub-region to comprehensively understand its broader surroundings, CAMIL utilizes this zooming-out approach by extending the tile\u2019s view to examine the broader neighborhood of each tile thoroughly. This extension allows CAMIL to gather additional information and facilitates a better understanding and assessment of the surrounding microenvironment and tissue context.\nIn CAMIL, we recalibrate each tile\u2019s individual attention score by aggregating the attention scores of its surroundings. For example, tiles with high attention scores surrounded by other high-scoring tiles should be considered important. Conversely, the presence of a tile classified by the model as important in a low-scoring neighborhood could be, in some cases, attributed to noise, and this should be reflected in its final attention score.\nThe overview of CAMIL can be seen in Figure 1. It can be decomposed into five elements:\n1. A WSI-preprocessing phase automatically segments the tissue region of each WSI and divides it into many smaller tiles (e.g., 256 x 256 pixels).\n2. A tile and feature extraction module, consisting of a stack of convolutional, max pooling, and linear layers transform the original tile input to low dimensional feature representations: H = {h1, . . . ,hi, . . . ,hN},hi \u2208 Rn\u00d7d, where d is the embedding dimensions of a tile, n the number of tiles within a WSI (n differs among different WSIs), and N the number of WSIs.\n3. A Nystromformer module (Xiong et al., 2021) transforms the tile embeddings to a concise, descriptive hidden feature representation. It is crucial in aggregating global contexts, capturing the overall information and patterns across multiple tiles.\n4. A neighbor-constrained attention mechanism in CAMIL, coupled with a contrastive learning block, encapsulates the neighborhood prior and focuses on aggregating local concepts.\n5. The feature aggregator and classification layer combine the local concepts derived from the previous layer with the features that describe the global contexts obtained from the transformer module. These features are merged to generate a prediction at the slide level.\nWe elaborate on each step in the following subsections."
        },
        {
            "heading": "3.1 WSI PREPROCESSING",
            "text": "We followed methods in Lu et al. (2021) to segment tissue regions and split the whole slide image into individual non-overlapping tiles (1. in Figure 1) (details of the hyperparameters used here are shown in the appendix)."
        },
        {
            "heading": "3.2 FEATURE EXTRACTOR",
            "text": "Effective feature representations significantly impact predictive accuracy, as demonstrated by the success of self-supervised contrastive learning (Chen et al., 2020). Therefore, to extract rich, meaningful feature representations from individual tiles, we first train a feature extractor following the SimCLR (Chen et al., 2020) approach. SimCLR is one of the most popular self-supervised learning frameworks that enable semantically rich feature representations to be learned by minimizing the distance between different augmented versions of the same image data.\nSimilar to the training approach followed by Li et al. (2021a), the data sets utilized in SimCLR are composed of patches derived from WSIs. These patches are densely cropped with no overlap and treated as separate images for the purpose of SimCLR training. During training, two different augmentations are done on the same tile. These two augmentations are chosen from four possible augmentations (color distortion, zoom, rotation, and reflection) using a stochastic data augmentation module. These two augmentations of the same tile are fed through a ResNet-18 (He et al., 2015) pretrained on ImageNet (Deng et al., 2009) with an additional projection head, which is a multi-layer perceptron (MLP) with two hidden layers that map the feature representations to a space where a contrastive loss is applied. The final convolutional block of ResNet-18 and the projection head are then fine-tuned by minimizing the contrastive loss (temperature-scaled cross entropy) between zi, zj , corresponding to two \u2018correlated\u2019 (differentially augmented) views of the same tile. Here, we minimized the normalized temperature-scaled cross entropy (NT-Xent) defined as\nlij = \u2212 log exp(sim(zi, zj)/\u03c4)\u2211N\nk=1 1[k \u0338=i] exp(sim(zi, zk)/\u03c4) (1)\nThe trained network is then used as the base feature extractor (F (x) in Figure 1) to produce the feature representations H = {h1, . . . ,hi, . . . ,hN},hi \u2208 Rn\u00d7d of each WSI, where n is the number of tiles and d is the embedding dimension to represent each tile, and N the number of WSIs. This trained feature extractor is frozen when training CAMIL and is only used to extract features and calculate distances between neighboring patches. These distances between neighboring patches are calculated using the sum of squared differences between the features and are used in the following neighbor-constrained attention module."
        },
        {
            "heading": "3.3 TRANSFORMER MODULE TO CAPTURE GLOBAL CONTEXTS",
            "text": "To encode the feature embeddings H , our approach focuses on capturing the inter-tile relationships and dependencies, enhancing the global context understanding, and facilitating comprehensive feature aggregation. This is achieved using a transformer layer, represented as T (h) in Figure 1, which is particularly effective for managing the complex structure of WSIs. To address the challenge of memory overload due to the long-range dependencies in large WSIs, we adopt the Nystromformer architecture (Xiong et al., 2021), enabling CAMIL to model intricate feature interactions through an efficient, approximate self-attention mechanism. This produces a \u201ctransformed\u201d feature set T = {t1, . . . , ti, . . . , tN}, with each ti \u2208 Rn\u00d7d, where,\nti = softmax\n( Q1(hi)K\u0303\nT 1 (hi)\u221a\ndk\n)( A )+ softmax ( Q\u03031(hi)K T 1 (hi)\u221a\ndk\n) V1(hi), (2)\nwhere Q\u03031(hi) and K\u03031(hi) are the m selected landmarks (see Xiong et al. (2021) and Appendix)\nfrom the original n-dimensional sequence of Q1 and K1, A+ = softmax\n( Q\u03031(hi)K\u0303\nT 1 (hi)\u221a\ndk\n)+ is\nthe approximate inverse of A. Softmax is applied along the rows of the matrix. K1(hi), Q1(hi), and V1(hi) are the first key, query, and value representations of hi shown as T (h) in Figure 1 and defined in Vaswani et al. (2017)."
        },
        {
            "heading": "3.4 NEIGHBOR-CONSTRAINED ATTENTION MODULE TO CAPTURE LOCAL CONTEXTS",
            "text": "The neighbor-constrained attention module in CAMIL is designed to capture specific features and patterns within localized areas of the slide. It focuses on the immediate neighborhood of each tile and aims to capture the local relationships and dependencies within that specific region. By doing so, the module can emphasize the relevance and importance of nearby tiles, effectively incorporating fine-grained details and local nuances into the model.\nTo model the tile and its surroundings, we construct a weighted adjacency matrix. Consider an undirected graph G = (V,E), where V represents the set of nodes representing image tiles, and E represents the set of edges between nodes indicating adjacency. The graph can be represented by an adjacency matrix A with elements Ai,j , where Ai,j = sij if there exists an edge (vi, vj) \u2208 E and Ai,j = 0 otherwise. Each image tile must be connected to other tiles and can be surrounded by at most eight adjacent patches. Therefore, the sum of each row or column of the adjacency matrix is at least one and, at most, 8, reflecting the neighboring relationships of the tiles. Each element of the matrix sij represents the degree of similarity or resemblance between two connected tiles and is calculated as follows:\nsij =\n{ exp\u2212( \u221a (hi \u2212 hj)2) , (vi, vj) \u2208 E\n0 , otherwise . (3)\nThis design ensures injecting a bio-topological prior such that the weight of a tile is dependent on adjacent tiles with a similar pattern.\nThe transformed tile representations T = {t1, . . . , ti, . . . , tN} are again transformed by the weight matrices Wq \u2208 Rn\u00d7dq , Wk \u2208 Rn\u00d7dk and Wv \u2208 Rn\u00d7dv into three distinct representations: the query representation Q(ti) = W \u22baq ti, the key representation K(ti) = W \u22ba k ti and the value representation V (ti) = W \u22bav ti, where dq = dk = dv = d. The dot product of every query with all the key vectors produces an attention matrix whose elements determine the correlation between the different tiles of a WSI (4. in Figure 1).\nThe similarity mask is element-wise multiplied with the dot product of the query and key embeddings, generating a masked attention matrix whose non-zero elements reflect the contribution of a tile\u2019s neighbors to the tile score.\nAfter obtaining the attention coefficients that correspond to the neighbors of every tile, the last step is to aggregate this contextual information to generate a single attention weight. For each tile, we sum the coefficients of their neighbors. The resultant tile score vector is passed through a softmax function to ensure all weights sum to one.\nTherefore, the attention coefficient of the ith tile of a WSI is given by the following equation, where \u27e8.\u27e9 denotes the inner product between two vectors:\nwi =\nexp (\u2211N j=1\u27e8Q(ti),K(tj)\u27e9sij ) \u2211N\nk=1 exp (\u2211n j=1\u27e8Q(tk),K(tj)\u27e9skj ) . (4) The feature embeddings t \u2208 Rn\u00d7dv are then computed and weighted by their respective attention score to give a neighbor-constrained feature vector, li, for each tile:\nli = wiV (ti) (5)"
        },
        {
            "heading": "3.5 FEATURE AGGREGATION AND SLIDE-LEVEL PREDICTION",
            "text": "The mechanism utilized to fuse the local and global value vectors allows for the adaptive blending of local and global information described in Equation 6. The sigmoid function applied to the local values serves as a weighting factor, enabling the model to emphasize local characteristics when they are deemed more relevant while still retaining the contribution from the global contexts.\nm = \u03c3(l)\u2299 l+ (1\u2212 \u03c3(l))\u2299 t, (6)\nwhere \u03c3() denotes the sigmoid non-linearity.\nThe collective, WSI-level representation m \u2208 Rn\u00d7d is adaptively computed as the weighted average of the z fused vector:\nz = N\u2211 i=1 ai(mi) , (7)\nsuch that:\nai = expwT (tanh(V tTi )\u2299 \u03c3(UtTi ))\u2211K j=1 expw T (tanh(V tTj )\u2299 \u03c3(UtTj )) , (8)\nwhere U , V , and w are learnable parameters, \u2299 is an element-wise multiplication, and tanh() is the hyperbolic tangent function.\nCAMIL achieves a synergistic effect by combining the value vector of the neighbor-constrained attention module with that of the transformer layer. The transformer layer captures global interactions and dependencies across the entire slide, while the neighbor-constrained attention module complements it by capturing local details and context. Together, they enable CAMIL to integrate both local and global perspectives effectively.\nFinally, the slide-level prediction is given via the classification layer Wc \u2208 Rc\u00d7d:\nyslide = Wc \u00b7 (\u2211 i zi )T (9)\nwhere c corresponds to the number of classes and \u2211\nthe sum pooling operation applied on z. The representation obtained from the high-attended patches is used to minimize a cross-entropy loss, and a final classification score is produced."
        },
        {
            "heading": "4 EXPERIMENTS AND RESULTS",
            "text": "To demonstrate the performance of CAMIL in capturing informative contextual relationships and improving classification and localization, various experiments were performed on three histopathology datasets: CAMELYON16 (Ehteshami Bejnordi et al., 2017), CAMELYON17 (Ba\u0301ndi et al., 2019), and TCGA-NSCLC. Additional information about the datasets, including details about the training and test sets and our baseline models, can be found in the appendix."
        },
        {
            "heading": "4.1 CLASSIFICATION PERFORMANCE",
            "text": "We evaluated the performance of our context-aware pooling operator by comparing its performance other attention-based MIL models, including CLAM-SB, CLAM-MB (Lu et al., 2021), TransMIL (Shao et al., 2021), DTFD-MIL (Zhang et al., 2022), DSMIL Li et al. (2021a) and GTP (Zheng et al., 2022). CLAM-SB and CLAM-MB utilize an attention-based pooling operator within the AttentionBased MIL (AB-MIL) framework (Ilse et al., 2018). They focus on the features of individual tiles and incorporate a clustering layer to enhance performance further. TransMIL is a transformer-based aggregator operator, DTFD-MIL leverages class activation maps to estimate the probability of an instance being positive under the AB-MIL framework, and DSMIL uses dual instance and bag classifiers to refine predictions. Lastly, GTP combines a graph-based representation of a WSI and a vision transformer.\nThe results of using CAMIL to classify WSI in the CAMELYON16, TCGA-NSCLC, and CAMELYON17 datasets are presented in Table 1. The evaluation of the model\u2019s performance in all experiments includes the area under the receiver operating characteristic curve (AUC) and the slide-level accuracy (ACC), which is determined by the threshold of 0.5. When implementing our baselines, we fine-tuned the hyperparameters used in the previously published original work to achieve the best\nperformance. The GTP model has the same configurations as the one described in the original paper (Zheng et al., 2022). However, for the CAMELYON16 dataset, the batch size (k) was set to 2 due to memory limitations.\nCAMIL outperforms other MIL models in ACC and AUC across CAMELYON16, TCGA-NSCLC, and CAMELYON17 datasets, narrowly trailing DTFD-MIL on CAMELYON17 by 0.003 in AUC. Its effectiveness notably identifies sparse cancerous regions in WSI, where tumor cells are often minimal, such as in the CAMELYON datasets, where tumor cells may account for as little as 5% of any WSI, which is particularly common in metastatic sites (Cheng et al., 2021). Specifically, CAMIL outperforms the other models on the CAMELYON16 dataset by significant margins, achieving at least 0.9% better in the AUC and 1.2% in ACC than the existing models on CAMELYON16 and 3.8% in ACC on CAMELYON17.\nGTP, with its MinCUT pooling layer, which aims to reduce the complexity of self-attention, performs well on TCGA-NSCLC but less so on CAMELYON datasets. Reducing the complexity may make computation more manageable. However, on large and complex datasets such as CAMELYON16 and CAMELYON17, this pooling operation may result in a loss of information, particularly in that of fine-grained details. TCGA-NSCLC is a smaller dataset, allowing the MinCUT pooling in GTP to retain sufficient information and remain competitive.\n4.2 LOCALIZATION\nTo evaluate the localization capability of CAMIL compared to our baselines, we examine both qualitative and quantitative evidence. Similar to the experimental design of Tourniaire et al. (2023b), we compute the Dice score to quantify the ability of the different models to identify cancerous evidence in cancerous slides. For normal slides, we compute the tile-level specificity. The reference ground-truth masks are computed at the 5th magnification level using expert tumor delineations. Additionally, a tile is considered cancerous if it contains at least 20% annotated tumor. To produce the predicted masks, we use the scaled attention scores for CAMIL, both CLAM models, TransMIL and DSMIL, the tile level logits for DTFD-MIL, and the GraphCAM for GTP.\nWe apply a threshold of 0.5 to the model\u2019s output probabilities to generate the masks from the tile-level predictions. The results for the Dice score and Specificity are shown in Table 2\nCAMIL performs well, albeit slightly behind the DTFD-MIL model. We believe the decreased localization performance might be attributed to integrating the Nystromformer module in our model design, akin to its role in TransMIL. TransMIL, as indicated by the attention maps in our qualitative assessment and its slide-level performance, demonstrates the ability to grasp the general patterns\nwithin a WSI and distinguish between normal and cancerous slides. However, despite this, confirmed by its low Dice score, it falls short in effectively pinpointing specific cancerous evidence within slides. Integrating the Nystromformer into our model design might introduce a trade-off between slide-level accuracy and localization performance, resulting in improved slide-level accuracy with an expense of slightly decreased localization performance.\nFigure 2 provides a qualitative comparison of the attention maps generated by CAMIL, GTP, and TransMIL on the CAMELYON16 cancer dataset. We visually compare these methods as they leverage spatial information to enhance prediction. These attention maps underscore CAMIL\u2019s high localization performance, as it can discern the boundaries separating normal tissue from tumor tissue. Although effectively pinpointing the regions of interest, GTP attention maps appear fragmented and less dense in cancer-associated regions. This fragmentation could be attributed to the MinCUTpooling operation, which may reduce the representation\u2019s granularity and affect the heatmap\u2019s coherence.\nTransMIL appears to sufficiently capture long-term dependencies within the WSIs, as evidenced by the attention maps of Figure 2. Specifically, TransMIL can identify the presence of cancer and precisely pinpoint the cancer-associated regions. However, these maps also reveal TransMIL\u2019s inability to capture intricate details and local nuances. The attention scores are not only confined to the cancer regions but expand beyond those to the surrounding normal tissue, impeding the precise localization of tumor boundaries, indicating the model\u2019s limitations in representing close proximity relationships within the WSIs."
        },
        {
            "heading": "5 ABLATION STUDIES",
            "text": "Additionally, we performed ablation studies to evaluate the effectiveness of the Nystromformer module and that of the neighboring-constrained attention module in our model. Specifically, we examined the effect of the Nystromformer block by retaining it while excluding the neighbor-constrained attention module denoted as CAMIL-G. Table 1 demonstrates that using only the Nystromformer block leads to satisfactory performance comparable to that of the TransMIL model, which also incorporates the Nystromformer. In a distinct ablation study, we omitted the Nystromformer block and retained the neighbor-constrained attention module, referred to as CAMIL-L. CAMIL-L exhibits a marginal improvement over the CAMIL-G model, thereby underlining its crucial role in augmenting the model\u2019s performance. Optimal results were achieved through the amalgamation of both models,\nharnessing both the potential of the Nystromformer block to model long-term dependencies and the prowess of the neighbor-attention module to comprehend local visual concepts."
        },
        {
            "heading": "5.1 VISUALISING GLOBAL AND LOCAL CONCEPTS",
            "text": "We also visualized the attention maps generated by the two versions of our model. Notably, in Figure 3, CAMIL-G demonstrates commendable performance in understanding global concepts and overall patterns, as it effectively detects tumor regions. However, it struggles with highly localized tumors, encountering difficulties capturing intricate, short-term dependencies within the image.\nOn the other hand, the attention maps produced by CAMIL-L excel in capturing the fine details within a WSI. This proficiency can be attributed to the context-aware module, which utilizes a similarity mask. This mask aggregates attention weights of similar neighboring feature representations, resulting in robust activations. In contrast, less favorable weights, particularly those associated with negative regions, do not contribute as strongly. This observation is substantiated by the histograms of the unnormalized attention coefficients, which are not constrained within the range of 0 to 1 (before applying the softmax function) provided in Figure 4. These histograms underscore a notable pattern: while a significant portion of the attention coefficients falls below the 0.5 threshold, cancerous cases display outliers within the range of 0.5 to 2. These outliers represent stronger activations, which distinctly characterize regions affected by cancer. Conversely, all other activations consistently remain below the 0.5 threshold, signifying a reduced emphasis on non-cancerous areas."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We have introduced CAMIL, a novel MIL vision transformer-based method that considers the tumor microenvironment context while determining tile-level labels in WSIs, mirroring the approach of a skilled pathologist. This is achieved by employing a unique neighbor-constrained attention mechanism, which assesses the dependencies between tiles within a WSI and incorporates contextual constraints as prior knowledge into the MIL model. We have demonstrated that using the transformer and the neighborhood-attention mechanism together is imperative in successful performance across datasets through our ablation studies. Importantly, CAMIL achieves state-of-the-art across multiple datasets regarding tile-level ACC, AUC, and F1 scores and patch-level localization and interpretability."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "All weakly-supervised deep learning models are trained using NVIDIA Tesla P100 GPUs. One GPU is used for training in each experiment. We intend to make the source code of our algorithm publicly available soon."
        },
        {
            "heading": "A APPENDIX",
            "text": "Here, we provide ACC, F1, and AUC for both datasets. F1 scores were omitted from the main text to save space."
        },
        {
            "heading": "A.1 PERFORMANCE METRICS",
            "text": ""
        },
        {
            "heading": "A.2 WSI PRE-PROCESSING",
            "text": "Using the publicly available WSI-prepossessing toolbox developed by (Lu et al., 2021), for both datasets, we first automatically segmented the tissue region from each slide and exhaustively divided\nit into 256\u00d7256 non-overlapping patches using \u00d720 magnification (Figure 1). Otsu\u2019s method was used to perform automatic WSI thresholding.\nTo avoid the computational overhead and capitalize on the rich feature representations already learned during its previous training on CAMELYON16, CAMELYON17, and TCGA-NSCLC datasets, we opted to use the pre-trained ResNet-18 feature extractor provided by (Li et al., 2021a). This model was extensively trained on a large set of tiles from the CAMELYON16 dataset, densely cropped without overlap, making it a powerful feature extractor. To make a fair comparison, we used the same contrastive learning-based model as the feature extractor for all our baselines."
        },
        {
            "heading": "A.3 DATASETS",
            "text": "CAMELYON16 is a significant publicly available Whole Slide Image (WSI) dataset for lymph node classification and metastasis detection. It includes 270 training and 129 test slides from two medical centers, all meticulously annotated by pathologists. Some slides have partial annotations, making it a challenging benchmark due to varying metastasis sizes.\nThe CAMELYON17 dataset consists of 1000 WSIs of a similar type to the CAMELYON16 dataset. However, only half (500 WSI) of these images are labeled and accessible publicly. These images are expertly annotated by pathologic lymph node classification into pN-stage:\n\u2022 pN0: No micro-metastases or macro-metastases, or isolated tumor cells (ITCs) found.\n\u2022 pN0(i+): Only ITCs found.\n\u2022 pN1mi: Micro-metastases found, but no macro-metastases found.\n\u2022 pN1: Metastases found in 1\u20133 lymph nodes, of which at least one is a macro-metastasis.\n\u2022 pN2: Metastases found in 4\u20139 lymph nodes, of which at least one is a macro-metastasis.\nAdopting the approach discussed in Tourniaire et al. (2023b), we created a binary classification problem by treating pN0 as normal and unifying all classes that were not pN0 into a single class, cancerous.\nThe TCGA-NSCLC dataset comprises two non-small cell lung cancer subtypes, LUAD and LUSC, with 541 slides. Unlike CAMELYON16, it lacks annotations."
        },
        {
            "heading": "A.4 DATA SPLITS",
            "text": "In the case of CAMELYON16, the WSIs are partitioned into a training and test set. The 270 WSIs of the training set are split five times into a training (80%) and a validation (20%) set in a 5-fold cross-validation fashion, and the average performance of the model on the competition test set is reported. The official test set comprising 129 WSIs is used for evaluation. For CAMELYON17, we used a 4-fold validation 65%, 15%, and 25% train, validation, and test splits. Regarding the TCGA-NSCLC dataset, a 5-fold cross-validation across the available images is performed. For each fold, the training set is split into 80% for training purposes and 20% for validation."
        },
        {
            "heading": "A.5 TRANSFORMER MODULE TO CAPTURE GLOBAL CONTEXTS",
            "text": "Working with large WSIs can lead to memory overload, as the self-attention mechanism used in the transformer layer requires computing pairwise interactions between all of the tiles in each WSI. To circumvent the memory overload associated with the long-range dependencies of large WSIs, we adopt the Nystromformer architecture (Xiong et al., 2021) to model feature interactions that otherwise would be intractable.\nThe Nystromformer approach is based on the Nystrom method, which is a technique for approximating a kernel matrix by selecting a small subset of its rows and columns. In the context of the transformer layer, this means selecting a subset of \u201dlandmark\u201d tiles from each WSI to represent the full set of tiles. The landmark tiles are chosen randomly, and their embeddings are used to compute a low-rank approximation of the self-attention matrix. This approximation is then used instead of the full self-attention matrix to compute the final output of the transformer layer.\nThe Nystromformer architecture is highly scalable regarding sequence length, making it well-suited for processing large WSIs (Xiong et al., 2021). Additionally, by reducing the time complexity of the self-attention mechanism from O(n2) to O(n), the Nystromformer approach can significantly reduce the computational cost of processing each WSI."
        },
        {
            "heading": "A.6 LOCALISATION",
            "text": "In addition to the quantitative outcomes, we assessed our approach qualitatively by visually presenting attention maps generated by different validation runs of our model overlaid on expert-annotated\ntumor regions (Figure 5). These attention maps pinpoint diagnostically significant locations in the image that are crucial for accurate tumor identification.\nWe notice a substantial agreement between the regions of interest identified by experts and those generated by our attention maps. These steps are harmonized with an attention map, forming a transformer relevancy map. Notably, our method consistently highlights the same regions within Whole Slide Images (WSIs) across different cross-validation folds, underscoring the reliability and robustness of our model."
        },
        {
            "heading": "A.7 SIMCLR ABLATION STUDY",
            "text": "SimCLR feature extraction backbone is an integral part of our model, and it plays a pivotal role in generating concise and descriptive feature representations. Leveraging the pre-trained SimCLR weights lays the foundation for generating meaningful similarity scores and, therefore, is crucial for the optimal performance of the neighbor-constrained attention mask similar to many similaritybased histopathology approaches. Incorporating ImageNet weights directly into our model notably decreases its performance (as it does with most models Tourniaire et al. (2023a); Li et al. (2021a)), emphasizing the necessity of SimCLR within our model architecture. Table 5 shows these results when using a ResNet-18 pre-trained on ImageNet on CAMELYON16 and TCGA-NSCLC."
        }
    ],
    "title": "ING IN WHOLE SLIDE IMAGES",
    "year": 2023
}