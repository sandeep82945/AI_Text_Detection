{
    "abstractText": "Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series analysis and makes efforts to bring convolution back to the arena of time series analysis. To this end, we modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. As the outcome, we propose ModernTCN and successfully solve this open question through a seldom-explored way in time series community. As a pure convolution structure, ModernTCN still achieves the consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance than state-of-the-art Transformer-based and MLP-based models. Our study further reveals that, compared with previous convolution-based models, our ModernTCN has much larger effective receptive fields (ERFs), therefore can better unleash the potential of convolution in time series analysis. Code is available at this repository: https://github.com/luodhhh/ModernTCN.",
    "authors": [
        {
            "affiliations": [],
            "name": "Donghao Luo"
        },
        {
            "affiliations": [],
            "name": "Xue Wang"
        }
    ],
    "id": "SP:3b4467cb8ecf22511031ace42935314dbc90b168",
    "references": [
        {
            "authors": [
                "Ahmed Abdulaal",
                "Zhuanghua Liu",
                "Tomer Lancewicki"
            ],
            "title": "Practical approach to asynchronous multivariate time series anomaly detection and localization",
            "venue": "In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining,",
            "year": 2021
        },
        {
            "authors": [
                "Anthony Bagnall",
                "Hoang Anh Dau",
                "Jason Lines",
                "Michael Flynn",
                "James Large",
                "Aaron Bostrom",
                "Paul Southam",
                "Eamonn Keogh"
            ],
            "title": "The uea multivariate time series classification archive, 2018",
            "venue": "arXiv preprint arXiv:1811.00075,",
            "year": 2018
        },
        {
            "authors": [
                "Shaojie Bai",
                "J Zico Kolter",
                "Vladlen Koltun"
            ],
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "venue": "arXiv preprint arXiv:1803.01271,",
            "year": 2018
        },
        {
            "authors": [
                "Cristian Challu",
                "Kin G Olivares",
                "Boris N Oreshkin",
                "Federico Garza Ramirez",
                "Max Mergenthaler Canseco",
                "Artur Dubrawski"
            ],
            "title": "Nhits: Neural hierarchical interpolation for time series forecasting",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Razvan-Gabriel Cirstea",
                "Chenjuan Guo",
                "Bin Yang",
                "Tung Kieu",
                "Xuanyi Dong",
                "Shirui Pan"
            ],
            "title": "Triformer: Triangular, variable-specific attentions for long sequence multivariate time series forecasting\u2013full version",
            "venue": "arXiv preprint arXiv:2204.13767,",
            "year": 2022
        },
        {
            "authors": [
                "Angus Dempster",
                "Fran\u00e7ois Petitjean",
                "Geoffrey I Webb"
            ],
            "title": "Rocket: exceptionally fast and accurate time series classification using random convolutional kernels",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Ningning Ma",
                "Jungong Han",
                "Guiguang Ding",
                "Jian Sun"
            ],
            "title": "Repvgg: Making vgg-style convnets great again",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "title": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Yves Franceschi",
                "Aymeric Dieuleveut",
                "Martin Jaggi"
            ],
            "title": "Unsupervised scalable representation learning for multivariate time series",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Milton Friedman"
            ],
            "title": "The interpolation of time series by related series",
            "venue": "J. Amer. Statist. Assoc,",
            "year": 1962
        },
        {
            "authors": [
                "Albert Gu",
                "Karan Goel",
                "Christopher R\u00e9"
            ],
            "title": "Efficiently modeling long sequences with structured state spaces",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning",
            "venue": "Image Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Andrew G Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "venue": "arXiv preprint arXiv:1704.04861,",
            "year": 2017
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Kyle Hundman",
                "Valentino Constantinou",
                "Christopher Laporte",
                "Ian Colwell",
                "Tom Soderstrom"
            ],
            "title": "Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding",
            "venue": "In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2018
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Bum Jun Kim",
                "Hyeyeon Choi",
                "Hyeonah Jang",
                "Dong Gu Lee",
                "Wonseok Jeong",
                "Sang Woo Kim"
            ],
            "title": "Dead pixel test using effective receptive field",
            "venue": "Pattern Recognition Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Taesung Kim",
                "Jinhee Kim",
                "Yunwon Tae",
                "Cheonbok Park",
                "Jang-Ho Choi",
                "Jaegul Choo"
            ],
            "title": "Reversible instance normalization for accurate time-series forecasting against distribution shift",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Nikita Kitaev",
                "\u0141ukasz Kaiser",
                "Anselm Levskaya"
            ],
            "title": "Reformer: The efficient transformer",
            "venue": "arXiv preprint arXiv:2001.04451,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Guokun Lai",
                "Wei-Cheng Chang",
                "Yiming Yang",
                "Hanxiao Liu"
            ],
            "title": "Modeling long-and short-term temporal patterns with deep neural networks",
            "venue": "In SIGIR,",
            "year": 2018
        },
        {
            "authors": [
                "Guokun Lai",
                "Wei-Cheng Chang",
                "Yiming Yang",
                "Hanxiao Liu"
            ],
            "title": "Modeling long-and short-term temporal patterns with deep neural networks",
            "venue": "In The 41st international ACM SIGIR conference on research & development in information retrieval,",
            "year": 2018
        },
        {
            "authors": [
                "Shiyang Li",
                "Xiaoyong Jin",
                "Yao Xuan",
                "Xiyou Zhou",
                "Wenhu Chen",
                "Yu-Xiang Wang",
                "Xifeng Yan"
            ],
            "title": "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Shiyang Li",
                "Xiaoyong Jin",
                "Yao Xuan",
                "Xiyou Zhou",
                "Wenhu Chen",
                "Yu-Xiang Wang",
                "Xifeng Yan"
            ],
            "title": "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zhe Li",
                "Shiyi Qi",
                "Yiduo Li",
                "Zenglin Xu"
            ],
            "title": "Revisiting long-term time series forecasting: An investigation on linear mapping",
            "venue": "arXiv preprint arXiv:2305.10721,",
            "year": 2023
        },
        {
            "authors": [
                "Zhe Li",
                "Zhongwen Rao",
                "Lujia Pan",
                "Zenglin Xu"
            ],
            "title": "Mts-mixers: Multivariate time series forecasting via factorized temporal and channel mixing",
            "venue": "arXiv preprint arXiv:2302.04501,",
            "year": 2023
        },
        {
            "authors": [
                "Bryan Lim",
                "Stefan Zohren"
            ],
            "title": "Time-series forecasting with deep learning: a survey",
            "venue": "Philosophical Transactions of the Royal Society A,",
            "year": 2020
        },
        {
            "authors": [
                "Minhao Liu",
                "Ailing Zeng",
                "Z Xu",
                "Q Lai",
                "Q Xu"
            ],
            "title": "Scinet: time series modeling and forecasting with sample convolution and interaction",
            "venue": "In 36th Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Shiwei Liu",
                "Tianlong Chen",
                "Xiaohan Chen",
                "Xuxi Chen",
                "Qiao Xiao",
                "Boqian Wu",
                "Mykola Pechenizkiy",
                "Decebal Mocanu",
                "Zhangyang Wang"
            ],
            "title": "More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity",
            "venue": "arXiv preprint arXiv:2207.03620,",
            "year": 2022
        },
        {
            "authors": [
                "Shizhan Liu",
                "Hang Yu",
                "Cong Liao",
                "Jianguo Li",
                "Weiyao Lin",
                "Alex X Liu",
                "Schahram Dustdar"
            ],
            "title": "Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yong Liu",
                "Haixu Wu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Non-stationary transformers: Rethinking the stationarity in time series forecasting",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie. A convnet for the"
            ],
            "title": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp",
            "venue": "11976\u201311986, 2022d.",
            "year": 2020
        },
        {
            "authors": [
                "Wenjie Luo",
                "Yujia Li",
                "Raquel Urtasun",
                "Richard Zemel"
            ],
            "title": "Understanding the effective receptive field in deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Spyros Makridakis",
                "Evangelos Spiliotis",
                "Vassilios Assimakopoulos"
            ],
            "title": "The m4 competition: Results, findings, conclusion and way forward",
            "venue": "International Journal of Forecasting,",
            "year": 2018
        },
        {
            "authors": [
                "Aditya P Mathur",
                "Nils Ole Tippenhauer"
            ],
            "title": "Swat: A water treatment testbed for research and training on ics security. In 2016 international workshop on cyber-physical systems for smart water networks (CySWater)",
            "year": 2016
        },
        {
            "authors": [
                "Yuqi Nie",
                "Nam H Nguyen",
                "Phanwadee Sinthong",
                "Jayant Kalagnanam"
            ],
            "title": "A time series is worth 64 words: Long-term forecasting with transformers",
            "venue": "arXiv preprint arXiv:2211.14730,",
            "year": 2023
        },
        {
            "authors": [
                "Boris N Oreshkin",
                "Dmitri Carpov",
                "Nicolas Chapados",
                "Yoshua Bengio"
            ],
            "title": "N-beats: Neural basis expansion analysis for interpretable time series forecasting",
            "year": 1905
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Rajat Sen",
                "Hsiang-Fu Yu",
                "Inderjit S Dhillon"
            ],
            "title": "Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Ya Su",
                "Youjian Zhao",
                "Chenhao Niu",
                "Rong Liu",
                "Wei Sun",
                "Dan Pei"
            ],
            "title": "Robust anomaly detection for multivariate time series through stochastic recurrent neural network",
            "venue": "In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Huiqiang Wang",
                "Jian Peng",
                "Feihu Huang",
                "Jince Wang",
                "Junhui Chen",
                "Yifei Xiao"
            ],
            "title": "Micn: Multi-scale local and global context modeling for long-term series forecasting",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Qingsong Wen",
                "Tian Zhou",
                "Chaoli Zhang",
                "Weiqi Chen",
                "Ziqing Ma",
                "Junchi Yan",
                "Liang Sun"
            ],
            "title": "Transformers in time series: A survey",
            "venue": "arXiv preprint arXiv:2202.07125,",
            "year": 2022
        },
        {
            "authors": [
                "Haixu Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Haixu Wu",
                "Jialong Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Flowformer: Linearizing transformers with conservation flows",
            "venue": "arXiv preprint arXiv:2202.06258,",
            "year": 2022
        },
        {
            "authors": [
                "Haixu Wu",
                "Tengge Hu",
                "Yong Liu",
                "Hang Zhou",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Timesnet: Temporal 2d-variation modeling for general time series analysis",
            "venue": "arXiv preprint arXiv:2210.02186,",
            "year": 2023
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jiehui Xu",
                "Haixu Wu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Anomaly transformer: Time series anomaly detection with association discrepancy",
            "venue": "arXiv preprint arXiv:2110.02642,",
            "year": 2021
        },
        {
            "authors": [
                "Wang Xue",
                "Tian Zhou",
                "QingSong Wen",
                "Jinyang Gao",
                "Bolin Ding",
                "Rong Jin"
            ],
            "title": "Make transformer great again for time series forecasting: Channel aligned robust dual transformer",
            "year": 2023
        },
        {
            "authors": [
                "Lexiang Ye",
                "Eamonn Keogh"
            ],
            "title": "Time series shapelets: a new primitive for data mining",
            "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2009
        },
        {
            "authors": [
                "Ailing Zeng",
                "Muxi Chen",
                "Lei Zhang",
                "Qiang Xu"
            ],
            "title": "Are transformers effective for time series forecasting",
            "venue": "arXiv preprint arXiv:2205.13504,",
            "year": 2022
        },
        {
            "authors": [
                "Tianping Zhang",
                "Yizhuo Zhang",
                "Wei Cao",
                "Jiang Bian",
                "Xiaohan Yi",
                "Shun Zheng",
                "Jian Li"
            ],
            "title": "Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures",
            "venue": "arXiv preprint arXiv:2207.01186,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyu Zhang",
                "Xinyu Zhou",
                "Mengxiao Lin",
                "Jian Sun"
            ],
            "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yunhao Zhang",
                "Junchi Yan"
            ],
            "title": "Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Haoyi Zhou",
                "Shanghang Zhang",
                "Jieqi Peng",
                "Shuai Zhang",
                "Jianxin Li",
                "Hui Xiong",
                "Wancai Zhang"
            ],
            "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Zhou",
                "Ziqing Ma",
                "Qingsong Wen",
                "Xue Wang",
                "Liang Sun",
                "Rong Jin"
            ],
            "title": "Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "C EXPERIMENT DETAILS C"
            ],
            "title": "LONG-TERM FORECASTING Implementation Details Our method is trained with the L2 loss, using the ADAM (Kingma & Ba, 2014) optimizer with an initial learning rate of 10\u22124. The default training process is 100 epochs with proper early stopping. The mean square error (MSE) and mean absolute error (MAE) are used",
            "year": 2014
        },
        {
            "authors": [
                "Nie"
            ],
            "title": "To validate our model, we conduct experiments with different input lengths under the same prediction length. As shown in Figure 7, in general, our model gains performance improvement with increasing input length, indicating our model can effectively extract useful information from longer history and capture long-term dependency",
            "venue": "However, some Transformer-based models (Wu et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2022b), adding a parallel Structural Re-parameterization branch with a small kernel can help to train the large kernel convolution layer. To further figure out the impact of different small kernel sizes, we perform experiments with five different small kernel sizes ranging from 1 to 9. As shown in Table 17, the performance is robust to the choice of small",
            "venue": "RESULT WITH DIFFERENT SMALL KERNELS According to (Ding et al.,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Time series analysis is widely used in extensive applications, such as industrial forecasting (Zhou et al., 2021), missing value imputation (Friedman, 1962), action recognition (Ye & Keogh, 2009), and anomaly detection (Xu et al., 2021). Because of the immense practical value, the past few years have witnessed the rapid development in time series analysis(Wen et al., 2022; Lim & Zohren, 2021). Among them, the rise of Transformer-based methods and MLP-based models is especially compelling (Nie et al., 2023; Zhang & Yan, 2023; Zhou et al., 2022; Cirstea et al., 2022; Wu et al., 2021; Liu et al., 2021a; Li et al., 2019b; Kitaev et al., 2020; Vaswani et al., 2017) (Li et al., 2023b; Zhang et al., 2022; Zeng et al., 2022). But around the same time, convolution-based models have received less attention for a long time.\nIt\u2019s non-trivial to use convolution in time series analysis for it provides a better balance of efficiency and performance. Date back to the 2010s, TCN and its variants (Bai et al., 2018; Sen et al., 2019) are widely-used in many time series tasks. But things have changed in 2020s. Transformerbased models and MLP-based models have emerged rapidly and achieved impressive performance in recent years. Thanks to their global effective receptive fields (ERFs), they can better capture the long-term temporal (cross-time) dependency and thus outperform traditional TCNs by a significant margin. As a result, convolution-based models are losing steam nowadays due to their limited ERFs.\nSome previous convolution-based models (Wang et al., 2023; Liu et al., 2022a) try to bring convolution back to the arena of time series analysis. But they mainly focus on designing extra sophisticated structures to work with the traditional convolution, ignoring the importance of updating the convolution itself. And they still cannot achieve comparable performance to the state-of-the-art Transformer-based and MLP-based models. The reason behind can be explained by Figure 1. Increasing the ERF is the key to bringing convolution back to time series analysis. But previous convolution-based models\nstill have limited ERFs, which prevents their further performance improvements. How to better use convolution in time series analysis is still a non-trivial and open question.\nAs another area where convolution is widely used, computer vision (CV) took a very different path to explore the convolution. Unlike recent studies in time series community, lastest studies in CV focus on optimizing the convolution itself and propose modern convolution (Liu et al., 2022d; Ding et al., 2022; Liu et al., 2022b). Modern convolution is a new convolution paradigm inspired by Transformer. Concretely, modern convolution block incorporates some architectural designs in Transformer and therefore has a similar structure to Transformer block (Figure 2 (a) and (b)). Meanwhile, to catch up with the gloabal ERF in Transformer, modern convolution usually adopat a large kernel as it can effectively increase the ERF (Figure 1). Although the effectiveness of modern convolution has been demonstrated in CV, it has still received little attention from the time series community. Based on above findings, we intend to first modernize the convolution in time series analysis to see whether it can increase the ERF and bring performance improvement.\nBesides, convolution is also a potentially efficient way to capture cross-variable dependency. Crossvariable dependency is another critical dependency in time series in addition to the cross-time one. It refers to the dependency among variables in multivariate time series. And early study(Lai et al., 2018b) has already tried to use convolution in variable dimension to capture the cross-variable dependency. Although its performance is not that competitive nowadays, it still demonstrates the feasibility of convolution in capturing cross-variable dependency. Therefore, it\u2019s reasonable to believe that convolution can become an efficient and effective way to capture cross-variable dependency after proper modifications and optimizations.\nBased on above motivations, we take a seldom-explored way in time series community to successfully bring convolution-based models back to time series analysis. Concretely, we modernize the traditional TCN and conduct some time series related modifications to make it more suitable for time series tasks. As the outcome, we propose a modern pure convolution structure, namely ModernTCN, to efficiently utilize cross-time and cross-variable dependency for general time series analysis. We evaluate ModernTCN on five mainstream analysis tasks, including long-term and short-term forecasting, imputation, classification and anomaly detection. Surprisingly, as a pure convolution-based model, ModernTCN still achieves the consistent state-of-the-art performance on these tasks. Meanwhile, ModernTCN also maintains the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance. Our contributions are as follows:\n\u2022 We dive into the question of how to better use convolution in time series and propose a novel solution. Experimental results show that our method can better unleash the potential of convolution in time series analysis than other existing convolution-based models.\n\u2022 ModernTCN achieves the consistent state-of-the-art performance on multiple mainstream time series analysis tasks, demonstrating the excellent task-generalization ability.\n\u2022 ModernTCN provides a better balance of efficiency and performance. It maintains the efficiency advantage of convolution-based models while competing favorably with or even better than state-of-the-art Transformer-based models in terms of performance."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 CONVOLUTION IN TIME SERIES ANALYSIS",
            "text": "Convolution used to be popular in time series analysis in 2010s. For example, TCN and its variants (Bai et al., 2018; Sen et al., 2019; Franceschi et al., 2019) adopt causal convolution to model the\ntemporal causality. But they suffer from the limited ERFs. With the rapid development of Transformerbased and MLP-based models, convolution has received less attention in recent years. Some studies try to bring convolution back to time series community. MICN (Wang et al., 2023) goes beyond causal convolution and proposes a multi-scale convolution structure to combine local features and global correlations in time series. SCINet (Liu et al., 2022a) removes the idea of causal convolution and introduces a recursive downsample-convolve-interact architecture to model time series with complex temporal dynamics. But they still have difficulty in modeling long-term dependency due to the limited ERFs. TimesNet (Wu et al., 2023) is special in the family of convolution-based models. Different from other models that mainly use 1D convolution, it transforms 1D time series into 2D-variations and uses 2D convolution backbones in CV to obtain informative representations."
        },
        {
            "heading": "2.2 MODERN CONVOLUTION IN COMPUTER VISION",
            "text": "Convolutional neural networks (ConvNets) (Krizhevsky et al., 2017; Simonyan & Zisserman, 2014; He et al., 2015; Xie et al., 2017; Huang et al., 2017) used to be the dominant backbone architectures in CV. But in 2020s, Vision Transformers (ViTs) (Dosovitskiy et al., 2020; Liu et al., 2021b) are proposed and outperform previous standard ConvNets. To catch up with the performance of ViTs, modern convolution in 2020s are introduced. Inspried by the architectural designs in Transformers, ConvNeXt(Liu et al., 2022d) re-design the convolution block to make it more similar to the Transformer block. To further catch up with the global ERF of Transformers, RepLKNet (Ding et al., 2022) scales the kernel size to 31\u00d731 with the help of Structural Reparameter technique. Further more, SLaK (Liu et al., 2022b) enlarges the kernel size to 51\u00d751 by decomposing a large kernel into two rectangular, parallel kernels and by using dynamic sparsity. Inspired by above studies, we modernize and modify 1D convolution in time series community to make it more suitable for time series analysis tasks."
        },
        {
            "heading": "3 MODERNTCN",
            "text": "In this section, we first provide a design roadmap for ModernTCN block to introduce how we modernize and optimize the traditional 1D convolution block in time series community. Then we introduce the overall structure of ModernTCN. And more related details are in Appendix G."
        },
        {
            "heading": "3.1 MODERNIZE THE 1D CONVOLUTION BLOCK",
            "text": "Following the idea of (Liu et al., 2022d), we firstly re-design the 1D convolution block as shown in Figure 2 (b). DWConv is responsible for learning the temporal information among tokens on a per-feature basis, which plays the same role as the self-attention module in Transformer. ConvFFN is similar to the FFN module in Transformer. It consists of two PWConvs and adopts an inverted bottleneck structure, where the hidden channel of the ConvFFN block is r times wider than the input channel. This module is to learn the new feature representation of each token independently.\nAbove design leads to a separation of temporal and feature information mixing. Each of DWConv and ConvFFN only mixes information across one of the temporal or feature dimension, which is\ndiffernet from the traditional convolution that jointly mixes information on both dimensions. This decoupling design can make the object tasks easier to learn and reduce the computational complexity.\nBased on above design, we borrow from the success of CV and modernize the 1D convolution. But we find that simply modernizing the convolution in the same way as CV brings little to no performance improvement in time series tasks. In fact, above design does not take into account the characteristics of time series. In addition to feature dimension and temporal dimension, time series also has a variable dimension. But the backbone stacked by convolution blocks as designed in Figure 2 (b) cannot handle the variable dimension properly. Since cross-variable information is also critical in multivariate time series (Zhang & Yan, 2023; Li et al., 2023b), more time series related modifications are still needed to make the modern 1D convolution more suitable for time series analysis."
        },
        {
            "heading": "3.2 TIME SERIES RELATED MODIFICATIONS",
            "text": "Maintaining the Variable Dimension In CV, before the backbone, we embed 3 channel RGB features at each pixel into a D-dimensional vector to mix the information from RGB channels via the embedding layer. But the similar variable-mixing embedding (e.g., simply embed M variables into a D-dimensional vector per time step) is not suitable for time series. Firstly, the difference among variables in time series is much greater than that among RGB channels in a picture (Cirstea et al., 2022). Just an embedding layer fails to learn the complex dependency across variables and even loses the independent characteristics of variables for not considering their different behaviors. Secondly, such embedding design leads to the discard of variable dimension, making it unable to further study the cross-variable dependency. To this issue, we propose patchify variable-independent embedding.\nWe denote Xin \u2208 RM\u00d7L as the M variables input time series of length L and will further divide it into N patches of patch size P after proper padding (Padding details are in Appendix B). The stride in the patching process is S, which also serves as the length of non overlapping region between two consecutive patches. Then the patches will be embedded into D-dimensional embedding vectors:\nXemb = Embedding(Xin) (1) Xemb \u2208 RM\u00d7D\u00d7N is the input embedding. Different from previous studies (Nie et al., 2023; Zhang & Yan, 2023), we conduct this patchify embedding in an equivalent fully-convolution way for a simpler implementation. After unsqueezing the shape to Xin \u2208 RM\u00d71\u00d7L, we feed the padded Xin into a 1D convolution stem layer with kernel size P and stride S. And this stem layer maps 1 input channel into D output channels. In above process, each of the M univariate time series is embedded independently. Therefore, we can keep the variable dimension. And followings are modifications to make our structure able to capture information from the additional variable dimension.\nDWConv DWConv is originally designed for learning the temporal information. Since it\u2019s more difficult to jointly learn the cross-time and cross-variable dependency only by DWConv, it\u2019s inappropriate to make DWConv also responsible for mixing information across variable dimension. Therefore, we modify the original DWConv from only feature independent to both feature and variable independent, making it learn the temporal dependency of each univariate time series independently. And we adopt large kernel in DWConv to increase ERFs and improve the temporal modeling ability.\nConvFFN Since DWConv is feature and variable independent, ConvFFN should mix the information across feature and variable dimensions as a complementary. A naive way is to jointly learn the dependency among features and variables by a single ConvFFN. But such method leads to higher computational complexity and worse performance. Therefore, we further decouple the single ConvFFN into ConvFFN1 and ConvFFN2 by replacing the PWConvs with grouped PWConvs and setting different group numbers. The ConvFFN1 is responsible for learning the new feature representations per variable and the ConvFFN2 is in charge of capturing the cross-variable dependency per feature.\nAfter above modifications, we have the final ModernTCN block as shown in Figure 2 (d). And each of DWConv, ConvFFN1 and ConvFFN2 only mixes information across one of the temporal, feature or variable dimension, which maintains the idea of the decoupling design in modern convolution."
        },
        {
            "heading": "3.3 OVERALL STRUCTURE",
            "text": "After embedding, Xemb is fed into the backbone to capture both the cross-time and cross-variable dependency and learn the informative representation Z \u2208 RM\u00d7D\u00d7N :\nZ = Backbone(Xemb) (2)\nBackbone(\u00b7) is the stacked ModernTCN blocks. Each ModernTCN block is organized in a residual way (He et al., 2015). The forward process in the i-th ModernTCN block is:\nZi+1 = Block(Zi) + Zi (3)\nWhere Zi \u2208 RM\u00d7D\u00d7N , i \u2208 {1, ...,K} is the i-th block\u2019s input,\nZi =\n{ Xemb , i = 1\nBlock(Zi\u22121) + Zi\u22121 , i > 1 (4)\nBlock(\u00b7) denotes the ModernTCN block. Then the final represnetation Z = Block(ZK) + ZK will be further used for multiple time series analysis tasks. See Appendix B for pipelines of each task."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate ModernTCN on five mainstream analysis tasks, including long-term and short-term forecasting, imputation, classification and anomaly detection to verify the generality of ModernTCN.\nBaselines Since we attempt to propose a foundation model for time series analysis, we extensively include the latest and advanced models in time series community as basic baselines, which includes the Transformer-based models: PatchTST (2023), Crossformer (2023) and FEDformer (2022); MLPbased models: MTS-Mixer (2023b), LightTS (2022), DLinear (2022), RLinear and RMLP (2023a); Convolution-based Model: TimesNet (2023), MICN (2023) and SCINet (2022a). We also include the state-of-the-art models in each specific task as additional baselines for a comprehensive comparison.\nMain Results As shown in Figure 3, ModernTCN achieves consistent state-of-the-art performance on five mainstream analysis tasks with higher efficiency. Detailed discussions about experimental results are in Section 5.1. We provide the experiment details and results of each task in following subsections. In each table, the best results are in bold and the second best are underlined."
        },
        {
            "heading": "4.1 LONG-TERM FORECASTING",
            "text": "Setups We conducted long-term forecasting experiments on 9 popular real-world benchmarks, including Weather (Wetterstation), Traffic (PeMS), Electricity (UCI), Exchange (Lai et al., 2018a), ILI (CDC) and 4 ETT datasets (Zhou et al., 2021). Following (Nie et al., 2023; Zhang & Yan, 2023), we re-run all baselines with various input lengths and choose the best results to avoid under-estimating the baselines and provide a fairer comparison. We calculate the MSE and MAE of multivariate time series forecasting as metrics.\nResults Table 1 shows the excellent performance of ModernTCN in long-term forecasting. Concretely, ModernTCN gains most of the best performance in above 9 cases, surpassing extensive state-of-the-art MLP-based and Transformer-based models. It competes favorably with the best Transformer-based model PatchTST in terms of performance while having faster speed and less memory usage (Figure 3 right), therefore providing a better balance of performance and efficiency. It\u2019s notable that ModernTCN surpasses existing convolution-based models by a large margin (27.4% reduction on MSE and 15.3% reduction on MAE), indicating that our design can better unleash the potential of convolution in time series forecasting.\nModels ModernTCN CARD PatchTST Crossformer FEDformer MTS-Mixer RLinear DLinear TimesNet MICN SCINet N-HiTS N-BEATS (Ours) (2023) (2023) (2023) (2022) (2023b) (2023a) (2022) (2023) (2023) (2022a) (2023) (2019)\nSMAPE 11.698 11.815 11.807 13.474 12.840 11.892 12.473 13.639 11.829 13.130 12.369 11.927 11.851 MASE 1.556 1.587 1.590 1.866 1.701 1.608 1.677 2.095 1.585 1.896 1.677 1.613 1.599 OWA 0.838 0.850 0.851 0.985 0.918 0.859 0.898 1.051 0.851 0.980 0.894 0.861 0.855"
        },
        {
            "heading": "4.2 SHORT-TERM FORECASTING",
            "text": "Setups We adopt M4 dataset (Makridakis et al., 2018) as the short-term forecasting benchmark. Following (Wu et al., 2023), we fix the input length to be 2 times of prediction length and calculate Symmetric Mean Absolute Percentage Error (SMAPE), Mean Absolute Scaled Error (MASE) and Overall Weighted Average (OWA) as metrics. We include additional baselines like CARD (2023), N-BEATS (2019) and N-HiTS (2023) for this specific task. Since the M4 dataset only contains univariate time series, we remove the cross-variable component in ModernTCN and Crossformer.\nResults The results are summarized in Table 2. Short-term forecasting in M4 dataset is a much more challenging task because the time series samples are collected from different sources and have quite different temporal properties. Our ModernTCN still achieves the consistent state-of-the-art in this difficult task, demonstrating its excellent temporal modeling ability."
        },
        {
            "heading": "4.3 IMPUTATION",
            "text": "Setups Imputation task aims to impute the missing values based on the partially observed time series. Due to unexpected accidents like equipment malfunctions or communication error, missing values in time series are very common. Since missing values may harm the performance of downstream analysis, imputation task is of high practical value. Following (Wu et al., 2023), we mainly focus on electricity and weather scenarios where the data-missing problem happens commonly. We select the datasets from these scenarios as benchmarks, including ETT (Zhou et al., 2021), Electricity (UCI) and Weather (Wetterstation). We randomly mask the time points in ratios of {12.5%, 25%, 37.5%, 50%} to compare the model capacity under different proportions of missing data.\nResults Table 3 shows the compelling performance of ModernTCN in imputation tasks. ModernTCN achieves 22.5% reduction on MSE and 12.9% reduction on MAE compared with previous state-of-the-art baseline TimesNet (2023). Due to the missing values, the remaining observed time series is irregular, making it more difficult to capture cross-time dependency. Our ModernTCN still\nachieves the best performance in this challenging task, verifying the model capacity in capturing temporal dependency under extremely complicated situations.\nIt\u2019s also notable that cross-variable dependency plays a vital role in imputation task. Since in some time steps, only part of the variables are missing while others are still remaining, utilizing the cross-variable dependency between missing variables and remaining variables can help to effectively impute the missing values. Therefore, some variable-independent methods like PatchTST (2023) and DLinear (2022) fail in this task for not taking cross-variable dependency into consideration."
        },
        {
            "heading": "4.4 CLASSIFICATION AND ANOMALY DETECTION",
            "text": "Setups For classification, we select 10 multivariate datasets from UEA Time Series Classification Archive (Bagnall et al., 2018) for benchmarking and pre-process the datasets following (Wu et al., 2023). We include some task-specific state-of-the-art methods like LSTNet (2018b), Rocket (2020) and Flowformer (2022) as additional baselines.\nFor anomaly detection, we compare models on five widely-used benchmarks: SMD (Su et al., 2019), SWaT (Mathur & Tippenhauer, 2016), PSM (Abdulaal et al., 2021), MSL and SMAP (Hundman et al., 2018). We include Anomaly transformer (2021) as additional baselines. Following it, we adopt the classical reconstruction task and choose the reconstruction error as the anomaly criterion.\nResults Time series classification is a classic task in time series community and reflects the model capacity in high-level representation. As shown in Figure 4, ModernTCN achieves the best performance with an average accuracy of 74.2% . It\u2019s notable that some MLP-based models fail in classification tasks. This is because MLP-based models prefer to discard the feature dimension to\nobtain a lightweight backbone, which leads to the insufficient representation capability and inferior classification performance. Anomaly detection results are shown in Figure 4. ModernTCN achieves competitive performance with previous state-of-the-art baseline TimesNet (2023). Meanwhile, compared with TimesNet, ModernTCN saves 55.1% average training time per epoch (3.19s vs 7.10s) in classification task and saves 57.3% average training time per epoch (132.65s vs 310.62s) in anomaly detection task, providing a better balance of efficiency and performance in both tasks."
        },
        {
            "heading": "5 MODEL ANALYSIS",
            "text": ""
        },
        {
            "heading": "5.1 COMPREHENSIVE COMPARISON OF PERFORMANCE AND EFFICIENCY",
            "text": "Summary of Experimental Results ModernTCN achieves consistent state-of-the-art performance on five mainstream analysis tasks compared with other task-specific models or previous state-of-theart baselines, demonstrating its excellent task-generality and highlighting the potential of convolution in time series analysis (Figure 3 left). ModernTCN also has more advantage in efficiency, therefore providing a better balance of efficiency and performance (Figure 3 right). It\u2019s worth noting that our method surpasses existing convolution-based models by a large margin, indicating that our design can provide a better solution to the problem of how to better use convolution in time series analysis.\nCompared with Transformer-based and MLP-based Models Unlike previous convolution-based models, ModernTCN competes favorably with or even better than state-of-the-art Transformer-based models in terms of performance. Meanwhile, as a pure convolution model, ModernTCN has higher efficiency than Transformer-based models. As shown in Figure 3 right, ModernTCN has faster training speed and less memory usage, which demonstrates the efficiency superiority of our model.\nModernTCN outperforms all MLP-based baselines in all five tasks thanks to the better representation capability in ModernTCN blocks. In contrast, MLP-based models prefer to adopt a lightweight backbone for a smaller memory usage. But such design in MLP-based models also leads to the insufficient representation capability and inferior performance. Although ModernTCN is sightly inferior in memory usage, it still has almost the same running time efficiency as some MLP-based baselines thanks to the fast floating point operation speed in convolution. Considering both performance and efficiency, ModernTCN has more advantage in general time series analysis.\nCompared with TimesNet (2023) In addition to ModernTCN, TimesNet also demonstrates excellent generality in five mainstream tasks. It\u2019s worth noting that both models are convolution-based models, which further reveals that convolution has a better comprehensive ability in time series analysis. Meanwhile, both methods are inspired by CV and intend to make the time series analysis take advantage of the development of CV community. But the two methods take different paths to accomplish this goal. TimesNet makes efforts to transform the 1D time series into 2D space, making the time series can be modeled by the 2D ConvNets in CV community. But the additional data transformation and aggregation modules also bring extra memory usage and slower training speed. Different from TimesNet, our ModernTCN maintains the 1D time series and turns to modernize and optimize the 1D convolution in time series community. Therefore, we design a modern pure convolution structure that without any additional modules. The fully-convolutional nature in our design brings higher efficiency and makes it extremely simple to implement, therefore leading to the both performance and efficiency superiority than TimesNet (Figure 3 left and right)."
        },
        {
            "heading": "5.2 ANALYSIS OF EFFECTIVE RECEPTIVE FIELD (ERF)",
            "text": "Enlarging the ERF is the key to bring convolution back to time series analysis. In this section, we will discuss why ModernTCN can provide better performance than previous convolution-based models from the perspective of ERF. Firstly, rather than stacking more layers like other traditional TCNs (2018), ModernTCN increases the ERF by enlarging the kernel size. And in a pure convolution structure, enlarging the kernel size is a much more effective way to increases ERF. According to the theory of ERF in pure convolution-based models (Luo et al., 2016), ERF is proportion to O(ks\u00d7 \u221a nl), where ks and nl refers to the kernel size and the number of layers respectively. ERF grows linearly with the kernel size while sub-linearly with the layer number. Therefore, by enlarging the kernel size, ModernTCN can easily obtain a larger ERF and further bring perfomance improvement.\nExcept for enlarging the kernel size and stacking more layers, some previous convolution-based methods in time series community (MICN (2023) and SCINet (2022a)) prefer to adopt some sophisticated structures to cooperate with the traditional convolution, intending to enlarge their ERFs. Since they are not pure convolution structures, it\u2019s hard to analyse their ERFs theoretically. Therefore, we visualize the ERFs of these methods for intuitive comparision. Following (Kim et al., 2023), we sample 50 length-336 input time series from the validation set in ETTh1 for the visualization. The idea behind is to visualize how many points in the input series can make contribution to the middle point of the final feature map. As shown in Figure 1, our method can obtain a much larger ERF than previous convolution-based methods. Therefore our method can better unleash the potential of convolution in time series and sucessfully bring performance improvements in multiple time series analysis tasks."
        },
        {
            "heading": "5.3 ABLATION STUDY",
            "text": "Ablation of ModernTCN Block Design To validate the effectiveness of our design in ModernTCN block, we conduct ablation study in long-term forecasting tasks. Results are shown on Table 4. Discard Variable Dimension cannot provide ideal performance, which confirms our arguement that simply modernizing the convolution in the same way as CV could not bring performance improvement for omitting the importance of variable dimension. To better handle the variable dimension, we decouple a single ConvFFN into ConvFFN1 and ConvFFN2 in our design. As shown in Table 4, the undecoupled ConvFFN provide the worst performance and the combination of our decoupled two ConvFFNs (ConvFFN1+ConvFFN2) achieve the best, which proves the necessity and effectiveness of our decouple modification to ConvFFN module. Please see Appendix H for more details.\nAblation of Cross-variable Component As an important time series related modification in our design, we design the ConvFFN2 as a cross-variable component to capture the cross-variable dependency. We conduct ablation studies in imputation tasks and anomaly detection tasks. As shown in Table 5, without the ConvFFN2 will cause severe performance degradation in these two tasks, which emphasizes the importance of cross-variable dependency in time series analysis."
        },
        {
            "heading": "6 CONCLUSION AND FUTURE WORK",
            "text": "In this paper, we take a seldom-explored way in time series community to solve the question of how to better use convolution in time series analysis. By modernizing and modifying the traditional TCN block with time series related modifications, we propose ModernTCN and successfully bring convolution back to the arena of time series analysis. Experimental results show the great task generality of ModernTCN. While performing on par with or better than state-of-the-art Transformer-based models in terms of performance, ModernTCN maintains the efficiency advantage of convolution-based models, therefore providing a better balance of performance and efficiency. Since convolution-based models have received less attention in time series analysis for a long time, we hope that the new results reported in this study will bring some fresh perspectives to time series community and prompt people to rethink the importance of convolution in time series analysis."
        },
        {
            "heading": "7 REPRODUCIBILITY STATEMENT",
            "text": "The model architecture is introduced in details with equations and figures in the main text. And all the implementation details are included in the Appendix, including dataset descriptions, metrics of each task, model configurations and experiment settings. Code is available at this repository: https://github.com/luodhhh/ModernTCN."
        },
        {
            "heading": "8 ACKNOWLEDGEMENT",
            "text": "This work was supported by the Guangdong Key Area Research and Development Program (2019B010154001) and Hebei Innovation Plan (20540301D)."
        },
        {
            "heading": "A DATASETS",
            "text": ""
        },
        {
            "heading": "A.1 LONG-TERM FORECASTING AND IMPUTATION DATASETS",
            "text": "We evaluate the long-term forecasting performance on 9 popular real-world datasets, including Weather, Traffic, Electricity, Exchange, ILI and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). And for imputation tasks, we choose Weather, Electricity and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2) for benchmarking. These datasets have been extensively utilized for benchmarking and cover many aspects of life.\nThe dataset size (total timesteps), variable number and sampling frequency of each dataset are summarized in Table 6 . We follow standard protocol (Zhou et al., 2021) and split all datasets into training, validation and test set in chronological order by the ratio of 6:2:2 for the ETT dataset and 7:1:2 for the other datasets. And training, validation and test sets are zero-mean normalized with the mean and standard deviation of training set. Each of above datasets only contains one continuous long time series, and we obtain samples by sliding window.\nMore introduction of the datasets are as follow:\n1) Weather1 contains 21 meteorological indicators such as humidity and air temperature for 2020 whole year in Germany.\n2) Traffic2 contains the road occupancy rates measured by 862 different sensors on San Francisco Bay area freeways in 2 years. Data is collected from California Department of Transportation.\n3) Electricity3 contains hourly electricity consumption of 321 clients from 2012 to 2014. 4) Exchange4 the daily exchange rates of eight different countries ranging from 1990 to 2016. 5) ILI(Influenza-Like Illness)5 contains the weekly recorded influenza-like illness (ILI) patients\ndata in the United States between 2002 and 2021. It contains 7 indicators like the numbers of ILI patients under different age ranges and the ratio of ILI patients to the total patients. Data is provided by Centers for Disease Control and Prevention of the United States.\n6) ETT(Electricity Transformer Temperature)6 contains the data collected from electricity transformers with 7 sensors, including load, oil temperature, etc. It contains two sub-dataset labeled with 1 and 2, corresponding to two different electric transformers from two separated counties in China. And each of them contains 2 different resolutions (15 minutes and 1 hour) denoted with m and h. Thus, in total we have 4 ETT datasets: ETTh1, ETTh2, ETTm1, ETTm2."
        },
        {
            "heading": "A.2 SHORT-TERM FORECASTING DATASETS",
            "text": "M4 involves 100,000 different time series samples collected in different domains with different frequencies, covering a wide range of economic, industrial, financial and demographic areas.\nIt\u2019s notable that M4 dataset is different from the long-term forecasting datasets. Each of long-term forecasting dataset only contains one continuous long time series, and we obtain samples by sliding window. Therefore all samples are come from the same source time series and more likely to have similar temporal property. But the samples in M4 datasets are collected from different sources.\n1https://www.bgc-jena.mpg.de/wetter/ 2https://pems.dot.ca.gov/ 3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 4https://github.com/laiguokun/multivariate-time-series-data 5https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html 6https://github.com/zhouhaoyi/ETDataset\nTherefore they may have quite different temporal property, making the forecasting tasks in M4 datasets more difficult. Table 7 summarizes details of statistics of short-term forecasting M4 datasets."
        },
        {
            "heading": "A.3 CLASSIFICATION DATASETS",
            "text": "UEA dataset involves many time series samples collected in different domains for classification, covering the recognition tasks based on face, gesture, action and audio as well as other practical tasks like industry monitoring, health monitoring and medical diagnosis based on heartbeat. Most of them have 10 classes. Table 8 summarizes details of statistics of classification UEA datasets."
        },
        {
            "heading": "A.4 ANOMALY DETECTION DATASETS",
            "text": "We adopt datasets from different domains like server machine, spacecraft and infrastructure for benchmarking. Each dataset is divided into training, validation and testing sets. Each dataset contains one continuous long time series, and we obtain samples from the continuous long time series with a fixed length sliding window. Table 9 summarizes details of statistics of the datasets."
        },
        {
            "heading": "B.1 PADDING DETAILS IN PATCHIFY VARIABLE-INDEPENDENT EMBEDDING",
            "text": "Before patching and embedding, we adopt a padding operation on the original time series Xin to keep N = L//S. Specifically, we repeat Xin\u2019s last value (P \u2212 S) times and then pad them back to the end of Xin.\nDenoted Xin \u2208 RM\u00d7L as the M variables input time series of length L, the overall process of Patchify Variable-independent Embedding is as follows:\n1) Unsqueezing its shape to Xin \u2208 RM\u00d71\u00d7L. 2) Adopting above padding operation on it.\n3) Feeding the padded Xin to the 1D convolution stem layer for patching and embedding."
        },
        {
            "heading": "B.2 PIPELINE FOR REGRESSION TASKS",
            "text": "The pipeline for forecasting, imputation and anomaly detection is shown as Figure 5.\nAfter the backbone, we have Z \u2208 RM\u00d7D\u00d7N . Then the linear head with a flatten layer is used to obtain the final prediction:\nX\u0302 = Head(Flatten(Z)) (5)\nWhere X\u0302 \u2208 RM\u00d7T is the prediction of length T with M variables. Flatten(\u00b7) denotes a flatten layer that changes the final representation\u2019s shape to Z \u2208 RM\u00d7(D\u00d7N). Head(\u00b7) indicates the linear projection layer that maps the final representation to the final prediction.\nStationary Technique RevIN (Kim et al., 2021) is a special instance normalization for time series to mitigate the distribution shift between the training and testing data. In norm phase, we normalize the input time series per variable with zero mean and unit standard deviation before patching and embedding. Then in de-norm phase, we add the mean and deviation back to the final prediction per variable after the forward process.\nLow Rank Approximation for Traffic Datasets To Traffic dataset that contains much more variables than others, directly applying our model to Traffic dataset leads to heavy memory usage. Since the variables in multivariate times series have dependency on each other, a possible way to solve this problem is to find a low rank approximation of these M variables when M is a very big number. For example, FEDformer (2022) uses a low rank approximated transformation in frequency domain for better memory efficiency. And Crossformer (2023) also uses a small fixed number of routers to aggregate messages from all variables to save memory usage.\nIn this paper, we design a bottleneck structure as a simple and direct method to achieve this goal. In details, before fed into the ConvFFN1 and ConvFFN2, the variable number will be projected to M \u2032 by a projection layer, where M \u2032 is much smaller than M . Then after the ConvFFN1 and ConvFFN2 process, another projection layer is used to project the variable number back to M .\nAnd we conduct experiments with different M \u2032 to verify this solution. As shown in Table 10, our method can significantly reduce memory usage with only a little performance degradation. This result proves the fact that there is redundancy between the 862 variables in Traffic dataset. Therefore we can learn a low rank approximation of these 862 variables based on their dependency on each other. And such low rank approximation can help to reduce memory usage without too much performance degradation.\nInput The input is M variables time series with input length L. In imputaiton tasks, the input series will further element-wise multipy with a mask matrix to represent the randomly missing values.\nOutput In forecastiong tasks, the output is the prediction time series with prediction length T . In impuatation tasks, the output is the imputed input time series with input length L. In anomaly detection tasks, the output is the reconstructed input time series with input length L."
        },
        {
            "heading": "B.3 PIPELINE FOR CLASSIFICATION TASKS",
            "text": "The pipeline for classification is shown as Figure 6.\nThere are two difference: (1) We remove the RevIN; (2) The flatten layer is different. In classification tasks, the flatten layer changes the final representation\u2019s shape to Z \u2208 R1\u00d7(M\u00d7D\u00d7N). Then a projection layer with SoftMax activation is to map the final representation to the final classification result X\u0302 \u2208 R1\u00d7Cls , where Cls is the number of classes. Followings are implementation details and model parameters of each tasks."
        },
        {
            "heading": "C EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 LONG-TERM FORECASTING",
            "text": "Implementation Details Our method is trained with the L2 loss, using the ADAM (Kingma & Ba, 2014) optimizer with an initial learning rate of 10\u22124. The default training process is 100 epochs with proper early stopping. The mean square error (MSE) and mean absolute error (MAE) are used as metrics. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results. All the deep learning networks are implemented in PyTorch(Paszke et al., 2019) and conducted on NVIDIA A100 40GB GPU.\nAll of the models are following the same experimental setup with prediction length T \u2208 {24, 36, 48, 60} for ILI dataset and T \u2208 {96, 192, 336, 720} for other datasets as (Nie et al., 2023). We collect some baseline results from (Nie et al., 2023) where all the baselines are re-run with various input length L and the best results are chosen to avoid under-estimating the baselines. For other baselines, we follow the official implementation and run them with vary input length L \u2208 {24, 36, 48, 60, 104, 144} for ILI dataset and L \u2208 {96, 192, 336, 512, 672, 720} for other\ndatasets and choose the best results. All experiments are repeated five times. We calculate the MSE and MAE of multivariate time series forecasting as metrics.\nModel Parameter By default, ModernTCN contains 1 ModernTCN block with the channel number (dimension of hidden states) D = 64 and FFN ratio r = 8. The kernel size is set as large size = 51 and small size = 5. Patch size and stride are set as P = 8, S = 4 in the patchify embedding process. For bigger datasets (ETTm1 and ETTm2), we stack 3 ModernTCN blocks to improve the representation capability. For small datasets (ETTh1, ETTh2, Exchange and ILI), we recommend a small FFN ratio r = 1 to mitigate the possible overfitting and for better memory efficiency.\nFor baseline models, if the original papers conduct long-term forecasting experiments on the dataset we use, we follow the recommended model parameters in the original papers, including the number of layers, dimension of hidden states, etc. But we re-run them with vary input lengths as mentioned in Section 4.1 and choose the best results to obtain a strong baseline.\nMetric We adopt the mean square error (MSE) and mean absolute error (MAE) for long-term forecasting.\nMSE = 1\nT T\u2211 i=0 (X\u0302i \u2212Xi)2\nMAE = 1\nT T\u2211 i=0 \u2223\u2223\u2223X\u0302i \u2212Xi\u2223\u2223\u2223 where X\u0302,X \u2208 RT\u00d7M are the M variables prediction results of length T and corresponding ground truth. Xi means the i-th time step in the prediction result."
        },
        {
            "heading": "C.2 SHORT-TERM FORECASTING",
            "text": "Implementation Details Our method is trained with the SMAPE loss, using the ADAM (Kingma & Ba, 2014) optimizer with an initial learning rate of 5\u00d7 10\u22124. The default training process is 100 epochs with proper early stopping. The symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE) and overall weighted average (OWA) are used as metrics. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results.\nFollowing (Wu et al., 2023), we fix the input length to be 2 times of prediction length for all models. Since the M4 dataset only contains univariate time series, we remove the cross-variable component in ModernTCN and Crossformer.\nModel Parameter By default, ModernTCN contains 2 ModernTCN blocks with the channel number (dimension of hidden states) D = 2048 and FFN ratio r = 1. The kernel size is set as large size = 51 and small size = 5. For datasets of less samples (M4 Weekly, M4 Daily and M4 Hourly), we use a smaller channel number D = 1024. Patch size and stride are set as P = 8, S = 4 in the patchify embedding process. For datasets with shorter input length, we reduce the patch size and stride (e.g., P = 3, S = 3 in M4 Yearly and P = 2, S = 2 in M4 Quarterly).\nMetric For the short-term forecasting, following (Oreshkin et al., 2019), we adopt the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE) and overall weighted average (OWA) as the metrics, which can be calculated as follows:\nSMAPE = 200\nT T\u2211 i=1 |Xi \u2212 X\u0302i| |Xi|+ |X\u0302i| , MAPE = 100 T T\u2211 i=1 |Xi \u2212 X\u0302i| |Xi| ,\nMASE = 1\nT T\u2211 i=1 |Xi \u2212 X\u0302i| 1 T\u2212p \u2211T j=p+1 |Xj \u2212Xj\u2212p| , OWA = 1 2 [ SMAPE SMAPENa\u0131\u0308ve2 + MASE MASENa\u0131\u0308ve2 ] ,\nwhere p is the periodicity of the data. X\u0302,X \u2208 RT\u00d7M are the M variables prediction results of length T and corresponding ground truth. Xi means the i-th time step in the prediction result.\nC.3 IMPUTATION\nImplementation Details Our method is trained with the L2 loss, using the ADAM (Kingma & Ba, 2014) optimizer with an initial learning rate of 10\u22123. The default training process is 100 epochs with proper early stopping. The mean square error (MSE) and mean absolute error (MAE) are used as metrics. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results.\nWe use a mask matrix C \u2208 RL\u00d7M to represent the missing values in input time series Xin.\ncml = { 0 if xml is not observed 1 otherwise .\nXin \u2208 RL\u00d7M is the M variables input time series of length L. And L is set as 96 in imputation tasks. xml is the value at l-th timestep in the m-th univariate time series.\nThe input is the partially observed time series C Xin and the output is the imputed time series of the input. indicates the element-wise multiplication between the two tensors Xin and C. And we only calculate MSE loss on masked tokens.\nModel Parameter By default, ModernTCN has 1 ModernTCN block with channel number D = 128 and FFN ratio r = 1. The kernel size is set as large size = 71 and small size = 5. Patch size and stride are set as P = 1, S = 1 to avoid mixing the masked and un-maskded tokens.\nMetric We adopt the mean square error (MSE) and mean absolute error (MAE) for imputation."
        },
        {
            "heading": "C.4 CLASSIFICATION",
            "text": "Implementation Details Our method is trained with the Cross Entropy Loss, using the ADAM (Kingma & Ba, 2014) optimizer with an initial learning rate of 10\u22123. The default training process is 30 epochs with proper early stopping. The classification accuracy is used as metrics. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results.\nModel Parameter By default, ModernTCN has 2 ModernTCN blocks. The channel number D is decided by min{max{2dlogMe, dmin}, dmax} (dmin is 32 and dmax is 512) following (Wu et al., 2023). The FFN ratio is r = 1. Patch size and stride are set as P = 1, S = 1 in the patchify embedding process.\nMetric For classification, we calculate the accuracy as metric."
        },
        {
            "heading": "C.5 ANOMLY DETECTION",
            "text": "Implementation Details We takes the classical reconstruction task and train it with the L2 loss. We use the ADAM (Kingma & Ba, 2014) optimizer with an initial learning rate of 3\u00d7 10\u22124. The default training process is 10 epochs with proper early stopping. We use the reconstruction error (MSE) as the anomaly criterion. The F1-Score is used as metric. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results.\nModel Parameter By default, ModernTCN has 1 ModernTCN block. The channel number D is decided by min{max{2dlogMe, dmin}, dmax} (dmin is 8 and dmax is 256) following (Wu et al., 2023). The FFN ratio is r = 1. The kernel size is set as large size = 51 and small size = 5. Patch size and stride are set as P = 8, S = 4 in the patchify embedding process.\nMetric For anomaly detection, we adopt the F1-score, which is the harmonic mean of precision and recall.\nF1-Score = 2\u00d7 Precision\u00d7 Recall\nPrecision + Recall"
        },
        {
            "heading": "D MORE ABLATION STUDIES",
            "text": "We conduct more ablation studies in long-term forecasting tasks."
        },
        {
            "heading": "D.1 RESULTS WITH DIFFERENT MODEL PARAMETERS",
            "text": "To see whether ModernTCN is sensitive to the choice of model parameters, we perform experiments with varying model parameters, including number of layers (number of ModernTCN blocks) ranging from K = {1, 2, 3, 4, 5}, channel number (dimension of hidden states) ranging from D = {32, 64, 128} and FFN ratio ranging from r = {1, 2, 4, 8}. In general, except ILI dataset reveals high variance with different model parameter settings, other datasets are robust to the choice of model parameters. We conduct three experiments to figure out the impact of above three model parameters respectively. Detailed results are described in following paragraphs.\nResults with Different Channel Numbers Table 11 shows the impact of different channel numbers D. Considering both the parameter efficiency and forecasting performance, we set the default channel number as D = 64. And the default channel number D = 64 works well for most of the datasets.\nResults with Different FFN Ratios Table 12 shows the impact of different FFN Ratios r. Except for ILI dataset, our model is robust to the choice of the FFN ratio r in other datasets. We recommend r = 8 for most of the datasets. And for small datasets like ETTh1, ETTh2, Exchange and ILI, we recommend a small FFN ratio like r = 1 to mitigate the possible overfitting and for better memory efficiency.\nResults with Different Numbers of Layers Table 13 shows the impact of different numbers of layers (numbers of ModernTCN blocks) K. Considering both performance and efficiency, one ModernTCN block is enough for most of the datasets. But for bigger datasets like ETTm1 and ETTm2, we recommend to stack more ModernTCN blocks like K = 3 for better representation capability.\nD.2 IMPACT OF INPUT LENGTH AND PATCHING SETTINGS\nImpact of Input Length. Since a longer input length indicates more historical information an algorithm can utilize in time series forecasting, a model with strong ability to capture long-term temporal dependency should perform better when input length increases (Zeng et al., 2022; Wang et al., 2023; Nie et al., 2023). To validate our model, we conduct experiments with different input lengths under the same prediction length. As shown in Figure 7, in general, our model gains performance improvement with increasing input length, indicating our model can effectively extract useful information from longer history and capture long-term dependency. However, some Transformer-based models (Wu et al., 2021; Zhou et al., 2021; Vaswani et al., 2017) suffer from performance degradation with increasing input length owing to the repeated short-term patterns according to (Zhou et al., 2021).\nImpact of Patch size and Stride To verify the impact of patch size P and stride S, we perform experiments with two patching modes (P = S and P = 2 \u00d7 S) and two different S. Results are shown on Table 14. In general, the performance doesn\u2019t vary significantly with different P and S, indicating the robustness of our model against these two hyperparameters. The ideal P and S may vary from different datasets. We recommend P = 8 and S = 4 as general good choice for most of the datasets."
        },
        {
            "heading": "E UNIVARIATE LONG-TERM FORECASTING RESULTS",
            "text": "Here we provide the univariate long-term forecasting results on 4 ETT datasets. There is a target feature oil temperature within those datasets, which is the univariate time series that we are trying to forecast. Since it\u2019s a univariate tasks, we mainly focus on capturing cross-time information and don\u2019t need to capture the cross-variable information. Thus, we remove the cross-variable component in ModernTCN. As shown in Table 15, thanks to the larger ERF and better temporal modeling ability in DWConv, our ModernTCN can achieve comparable performance with the state-of-theart Transformer-based model PatchTST(2023) and MLP-based model DLinear(2022) in univariate forecasting tasks."
        },
        {
            "heading": "F TECHNIQUE TO BETTER TRAIN A LARGE KERNEL",
            "text": "F.1 INTRODUCTION ABOUT STRUCTURAL RE-PARAMETERIZATION\nAccording to (Ding et al., 2022; Liu et al., 2022b), we can use the Structural Re-parameterization technique to better train a large kernel convolution. In training phase, additional Batch normalization (BN) layers (Ioffe & Szegedy, 2015) are used following the depth-wise convolution layers to form convolution-BN branches for a better training result. Then the large size depth-wise convolution-BN branch is trained with a parallel small size depth-wise convolution-BN branch to make up the optimization issue of large kernel. After training, each of these two branches is transformed into a single depth-wise convolution layer by fusing the BN parameters into the convolution kernels (Ding et al., 2021). Then the small kernel is zero-padded on both side to large size. After two kernels aligned to the same size, the two depth-wise convolution layers add up to merge into a single large kernel depth-wise convolution layer. Then the fused single large kernel depth-wise convolution can be used in inference phase. It\u2019s notable that the resultant large kernel model is totally equivalent to the model in training phase but no longer has small kernels. See Figure 8 for an example of Structural Re-parameterization after training."
        },
        {
            "heading": "F.2 ABLATION OF STRUCTURAL RE-PARAMETERIZATION",
            "text": "F.2.1 IMPACT OF LARGE KERNEL SIZE\nAccording to (Ding et al., 2022), a large kernel size is the key to obtain a large ERF in 2D convolution. To verify whether this finding still works on 1D convolution and to figure out the impact of kernel size, we perform experiments with 3 different kernel sizes ranging from small to large on 3 datasets. Results on Table 16 show that increasing the kernel size leads to performance improvement. The experiment results indicate that directly enlarging the kernel size in 1D convolution layer and training it with Structural Re-parameterization technique can effectively increase ERF and help convolution layer to better capture temporal dependency."
        },
        {
            "heading": "F.2.2 RESULT WITH DIFFERENT SMALL KERNELS",
            "text": "According to (Ding et al., 2022; Liu et al., 2022b), adding a parallel Structural Re-parameterization branch with a small kernel can help to train the large kernel convolution layer. To further figure out the impact of different small kernel sizes, we perform experiments with five different small kernel sizes ranging from 1 to 9. As shown in Table 17, the performance is robust to the choice of small kernel sizes as long as they\u2019re much smaller than the large kenrel. And small size = 5 is a general good choice."
        },
        {
            "heading": "F.2.3 RESULT WITH MORE PARALLEL BRANCHES",
            "text": "In Structural Re-parameterization technique, we usually add one additional Structural Reparameterization branch paralleled to the large kernel convolution branch to make up its optimization issue. Here we perform experiments with more branches to see the impact of parallel branches number. As shown in Table 18, the performance is robust to the choice of parallel branches numbers. Considering both performance and efficiency, we only add one Structural Re-parameterization branch in our main experiments."
        },
        {
            "heading": "G DETAILS OF MODERNTCN BLOCKS",
            "text": ""
        },
        {
            "heading": "G.1 BACKGROUND",
            "text": "Group Convolution Group convolution (Krizhevsky et al., 2017; Xie et al., 2017; Zhang et al., 2018) divides the convolution channels into separate groups. Only the channels in the same group can interact with each other while the channels in different groups are independent.\nDepthwise Separable Convolution Depthwise convolution (Howard et al., 2017) can be seen as a special group convolution in which the number of groups is equal to the number of channels. Therefore, all channels in depthwise convolution are independent. The depthwise convolution only mixes information among tokens across the temporal dimension.\nSince depthwise convolution layer is totally channel-independent. It can not combine channels to create new representations. Therefore, the pointwise convolution layers, which are in charge of mixing channel information to create new representations, should be used following the depthwise convolution as a complementary. The kernel size of pointwise convolution is 1. Therefore it is applied to each token independently and only mixes information among channels.\nModern Convolution Modern convolution (Liu et al., 2022d; Ding et al., 2022; Liu et al., 2022b) is a new convolution paradigm inspired by Transformer. The basic components in a modern convolution block are depthwise and pointwise convolution layers. Inspired by the architectural designs in Transformer, the depthwise and pointwise convolution layers are organized into a similar structure to Transformer block, which is shown in Figure 2 (a) and (b).\nAs shown in Figure 2 (b), the DWConv is a depthwise convolution layer which is responsible for learning the temporal information among tokens on a per-channel basis. It plays the same role as the self-attention module in Transformer. Meanwhile, a large kernel is adopted in DWConv to catch up with the gloabl effective receptive field in Transformer.\nConvFFN consists of two point-wise convolution layers (PWConvs) and adopts an inverted bottleneck structure, where the hidden channel of the ConvFFN block is r times wider than the input channel (Figure 2 (c)). ConvFFN plays the same role as the FFN module in Transformer blocks, which is to learn the new representation of each token independently."
        },
        {
            "heading": "G.2 DETAILS OF MODERNTCN BLOCK DESIGNS",
            "text": "The input to the i-th ModernTCN block is Zi \u2208 RM\u00d7D\u00d7N , where M , D and N are the size of variable dimension, feature dimension and temporal dimension respectively. We merge the feature dimension and variable dimension before feeding the embedded time series into the ModernTCN block. Therefore the convolution channel number is M \u00d7D. As shown in Figure 2 (d), in the ModernTCN block, DWConv is a depthwise convolution layer which maps M \u00d7D input channels to M \u00d7D output channels. The group number in DWConv is set as M \u00d7D to make each channel independent. Therefore, DWConv is both variable and feature independent. It only mixes information across temporal dimension. And we set a large kernel size for DWConv to enlarge its effective receptive field and improve its temporal modeling ability.\nThe ConvFFN1 and ConvFFN2 in ModernTCN block are the decoupled version of ConvFFN based on the idea of group convolution. We replace the two pointwise convolution layers in ConvFFN with two group pointwise convolution layers with group number as M to obtain ConvFFN1. Similarly, we replace the two pointwise convolution layers in ConvFFN with two group pointwise convolution layers with group number as D to obtain ConvFFN2.\nIn details, the input channel number in ConvFFN1 is M \u00d7D. By setting group number as M , the M \u00d7D input channels will be divided into M groups. And only the D features in the same group can interact with each other. Since each group represents a variable, it means that only the features of the same variable can interact with each other to create new feature representation while the features of different variables are independent. Therefore ConvFFN1 can learn the new feature representation for each variable independently.\nThe same goes for ConvFFN2. After permute operation, the input channel number in ConvFFN2 is D \u00d7M . By setting group number as D, the D \u00d7M input channels will be divide into D groups. And only the M variables in the same group can interact with each other. Therefore ConvFFN2 can capturing the cross-variable dependency per feature."
        },
        {
            "heading": "G.3 PARAMETERS OF CONVOLUTION LAYERS IN MODERNTCN BLOCK",
            "text": "We provide the parameters of convolution layers in ModernTCN block in Table 19."
        },
        {
            "heading": "H DETAILS OF ABLATION STUDY",
            "text": "In Section 5.3, we provide an ablation study on our model designs. In this appendix, we provide more details about this ablation study."
        },
        {
            "heading": "H.1 DETAIL OF DIFFERENT EMBEDDING SETTINGS",
            "text": "In Section 3.2, we design the patchify variable-independent embedding to maintain the variable dimension, replacing the common patchify variable-mixing embedding which will lead to the discard of variable dimension in the embedded series. The difference of the two embeddings is shown on Figure 9 and introduced as follows:\n\u2022 In variable-independent embedding, we treat each univariate time series in the multi-variate time series as an individual sample with 1 feature and embed them independently.\n\u2022 In variable-mixing embedding, we treat the whole multi-variate time series as a single sample with M features.\nThe essential difference behind these two embedding methods is the opinion of whether we should maintain the variable dimension when analysing multivariate time series. Since applying the variable-mixing embedding to the multivariate time series will discard its variable dimension, we denote it as the setting Discard Variable Dimension. To verifies our opinion to maintain the variable dimension, we use the setting Discard Variable Dimension for comparision. In this setting, we apply\nvariable-mixing embedding to the multivariate input time series to discard its variable dimension. Then the embedded series is fed into a backbone stacked by convolution blocks like Figure 10 (a) to learn representation. As shown in Table 4, the setting Discard Variable Dimension leads to significant performance degradation, which verifies the effectiveness of our variable-independent embedding design and highlights the importance to maintain the variable dimension."
        },
        {
            "heading": "H.2 DETAIL OF DIFFERENT BLOCK DESIGN SETTINGS",
            "text": "Then we conduct experiments to study the impact of different block designs. All block design settings we used are shown in Figure 10. Since variable-mixing embedding will lead to severe performance degradation, we conduct these experiments with our variable-independent embedding.\nGiven that variable-independent embedding will maintain the variable dimension, we need to make our structure able to capture information from the additional variable dimension. A direct and naive way is to jointly learn the dependency among features and variables only by a single ConvFFN, which is denoted as Undecoupled ConvFFN. But such setting leads to higher computational complexity and worse performance, which is shown in Table 4. In contrast, the combination of our decoupled two ConvFFNs (denoted as ConvFFN1+ConvFFN2) achieve the best performance, which proves the necessity and effectiveness of our decouple modification to ConvFFN module.\nWe further study how much ConvFFN1 and ConvFFN2 contribute to the forecasting respectively. Only ConvFFN1 can learn each variable\u2019s deep feature representation independently but doesn\u2019t take cross-variable dependency into consideration. Only ConvFFN2 can only capture the cross-variable dependency per feature. But to each variable, it omits to learn their deep feature representations. As a result, the performance decreases on both settings.\nAnd we also include settings like ConvFFN1+ConvFFN1 and ConvFFN2+ConvFFN2 to eliminate the impact of the number of ConvFFNs. The results show that the combination of ConvFFN1 and ConvFFN2 is the key to performance improvement, not the the number of ConvFFNs."
        },
        {
            "heading": "I MORE COMPARISON WITH TIMESNET IN TERMS OF ERF",
            "text": "ModernTCN and TimesNet successfully enlarge the ERFs. But the ERFs of these two models have quite different property (Figure 11).\nThe ERFs of ModernTCN and other 1D convolution-based models are concentrated at the middle point and continuously expand to both ends, which means that the final representation of the middle point is highly related to its adjacent points. Such phenomenon demonstrates the locality, which is a common property of convolution.\nThe ERF of ModernTCN can expand to a wider range. Therefore ModernTCN has larger ERF than other 1D convolution-based models. But at the same time, the ERF of ModernTCN is still concentrated at the middle point. So ModernTCN is able to capture long-term dependency while focusing on the local context.\nHowever, the ERF of TimesNet is discrete and not concentrated at the middle point, which doesn\u2019t reflect the locality of convolution. This is because the additional 2D data transformation in TimesNet also influences the pattern of ERF. In 2D data transformation, the time series is divided into several segments and further rearranged in 2D space based on periods. In this process, a time point may be separated from its adjacent points, thereby losing continuity and locality. As a result, the ERF and performance of TimesNet are also highly (or even mainly) related to its special 2D data transformation, but not just depend on convolution.\nIn summary, influenced by its special 2D data transformation, TimesNet\u2019 ERF is of quite different property from other 1D convolution-based models\u2019 and doesn\u2019t reflect the locality of convolution. Although both ModernTCN and TimesNet can successfully enlarge the ERFs, ModernTCN can also maintain the locality when enlarging the ERF, therefore providing better performance than TimesNet."
        },
        {
            "heading": "J APPLY CROSS-VARIABLE COMPONENT TO VARIABLE-INDEPENDENT MODELS",
            "text": "We apply our cross-variable component (ConvFFNs) to some variable-independent models like DLinear (2022) and PatchTST (2023). We make some necessary modifications to our cross-variable component to adapt it to these two models, which is summarized as follows and shown on Figure 12.\n\u2022 For DLinear which only has two individual Linear layers (one for trend part and the other for seasonal part), we add an additional ConvFFN module before the Linear layer. This additional ConvFFN module can mix information across variable dimension to incorporate the cross-variable information.\n\u2022 For PatchTST, we replace the original FFN module in PatchTST block with our ConvFFNs but with some difference in residual connections to align with the block design in Transformer.\nWe conduct experiments in imputation tasks, where cross-variable dependency plays an important role. We report the performance promotion of each model in Table 20. In imputation tasks, equipped with our cross-variable component achieves averaged 28.7% promotion on PatchTST and 19.7% promotion on DLinear. The result validates that our cross-variable component can be used on top of other variable-independent models, helping them incorporate the cross-variable information and improving their performance."
        },
        {
            "heading": "K SHORT-TERM FORECASTING RESULTS ON MULTIVARIATE TIME SERIES DATASETS",
            "text": "We conduct short-term forecasting on 9 multivariate time series datasets, including Weather (Wetterstation), Traffic (PeMS), Electricity (UCI), Exchange (Lai et al., 2018a), ILI (CDC) and four ETT datasets (Zhou et al., 2021).\nThe experiment details are as follows:\n\u2022 We choose the prediction lengths as T \u2208 {6, 12, 18}, which meets the prediction length in M4 datasets. And following M4 short-term forecasting tasks, we set input length L to be 2 times of prediction length T .\n\u2022 We choose models that perform well in multivariate datasets (DLinear, RLinear, RMLP and PatchTST) or perform well in short-term forecasting tasks (PatchTST, CARD and TimesNet) as strong baselines.\n\u2022 All models follow their official configurations on above datasets, we only change the input lengths and prediction lengths.\n\u2022 We calculate MSE and MAE of the multivariate prediction results as metric.\nThe results are shown in Table 21. ModernTCN can outperform above competitive baselines in most cases, indicating that ModernTCN can better use limited input information to provide better forecasting results."
        },
        {
            "heading": "L ABLATION STUDY ABOUT REVIN ON REGRESSION TASKS",
            "text": ""
        },
        {
            "heading": "L.1 ABLATION STUDY ABOUT REVIN ON LONG-TERM FORECASTING",
            "text": "We conduct ablation study on Weather, ETTh1 and ETTm1 datasets for long-term forecasting tasks.\nThe results in Table 22 show that:\n\u2022 Although there is a slight degradation in performance without RevIN, our ModernTCN still achieves competitive performance. ModernTCN can still achieve significantly better performance than some baselines and compete favorably with PatchTST in the same case of no RevIN. The results indicate that our designs in ModernTCN also make great contribution to the performance improvement.\n\u2022 We also find that RevIN doesn\u2019t provide consistent improvement under some settings (e.g., prediciton length 192 and 720 in Weather dataset), which means our ModernTCN can directly predict from the non-stationary time series to some degree. This finding further confirms that our designs in ModernTCN can bring great performance improvement on their own. And such phenomenon mainly happens in Weather dataset for its degree of stationarity is not very high according to (Liu et al., 2022c)."
        },
        {
            "heading": "L.2 THE IMPACT OF REVIN ON DIFFERENT TASKS AND MODELS",
            "text": "To study the impact of RevIN on different tasks and models, we further conduct experiments on four regression tasks with 6 models: ModernTCN, PatchTST (2023), TimesNet(2023), SCINet(2022a), RMLP and RLinear(2023a). Details are as follows:\n\u2022 We conduct ablation study about RevIN on long-term forecasting with more models. And the results are provided in Table 23.\n\u2022 We conduct ablation study about RevIN on imputation tasks in Weather, ETTh1 and ETTm1 datasets. And we report the MSE and MAE under four different mask ratios as metrics. Results are shown in Table 24.\n\u2022 We conduct ablation study about RevIN on short-term forecasting tasks in M4 datasets and roport the weighted averaged results from six M4 sub-datasets as metric. Results are shown in Table 25.\n\u2022 We conduct ablation study about RevIN on anomaly detection tasks in SMD, MSL and PSM datasets and roport the F1-score as metric. Results are shown in Table 26.\nRevIN\u2019s impact on different tasks In general, removing the RevIN will lead to performance degradation in all four tasks because it is more difficult to predict from the non-stationary time series (Liu et al., 2022c; Kim et al., 2021). Given that stationary techniques like RevIN or decomposition (e.g., MICN, DLinear, FEDformer) can transform the input into stationary series that is easier to analyze with deep learning methods, it is generally necessary to adopt stationary techniques in regression tasks.\nWe also find that short-term forecasting is more sensitive to RevIN. The samples in M4 datasets are collected from different sources and have quite different temporal property. Therefore, there is a greater need of RevIN to mitigate the distribution shift.\nRevIN\u2019s impact on different models Although removing RevIN will cause performance degradation on all 6 models in our experiments, our ModernTCN is one of the less influenced models, indicating that our ModernTCN is robust to the usage of RevIN.\nThe extent of RevIN\u2019s influence on the model is related to the model\u2019s mechanisms. For example, an important step in TimesNet is calculate the periods of time series based on FFT. Since FFT mainly works well on stationary signals, there is a greater need for the time series to be stationary. As a result, TimesNet\u2019s performance is highly related to RevIN."
        },
        {
            "heading": "M FULL RESULTS",
            "text": "Due to the space limitation of the main text, we place the full results of all experiments in the following: long-term forecasting in Table 27, short-term forecasting in Table 28, imputation in Table 29, classification in Table 30 and anomaly detection in Table 31.\nAnd some showcases are provided in Appendix N."
        },
        {
            "heading": "M.1 LONG-TERM",
            "text": ""
        },
        {
            "heading": "M.2 SHORT-TERM",
            "text": "M.3 IMPUTATION"
        },
        {
            "heading": "M.4 CLASSIFICATION",
            "text": "M.5 ANOMALY DETECTION"
        },
        {
            "heading": "N SHOWCASES",
            "text": "We provide showcases to the regression tasks, including the imputation (Figure 13), long-term forecasting (Figure 14 and 15) and short-term forecasting (Figure 16), as an intuitive comparison among different models.\nN.1 IMPUTATION"
        },
        {
            "heading": "N.2 LONG-TERM FORECASTING",
            "text": ""
        },
        {
            "heading": "N.3 SHORT-TERM FORECASTING",
            "text": ""
        }
    ],
    "title": "MODERNTCN: A MODERN PURE CONVOLUTION STRUCTURE FOR GENERAL TIME SERIES ANALYSIS",
    "year": 2024
}