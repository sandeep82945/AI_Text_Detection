{
    "abstractText": "Hypernetworks, neural networks that predict the parameters of another neural network, are powerful models that have been successfully used in diverse applications from image generation to multi-task learning. Unfortunately, existing hypernetworks are often challenging to train. Training typically converges far more slowly than for non-hypernetwork models, and the rate of convergence can be very sensitive to hyperparameter choices. In this work, we identify a fundamental and previously unidentified problem that contributes to the challenge of training hypernetworks: a magnitude proportionality between the inputs and outputs of the hypernetwork. We demonstrate both analytically and empirically that this can lead to unstable optimization, thereby slowing down convergence, and sometimes even preventing any learning. We present a simple solution to this problem using a revised hypernetwork formulation that we call Magnitude Invariant Parametrizations (MIP). We demonstrate the proposed solution on several hypernetwork tasks, where it consistently stabilizes training and achieves faster convergence. Furthermore, we perform a comprehensive ablation study including choices of activation function, normalization strategies, input dimensionality, and hypernetwork architecture; and find that MIP improves training in all scenarios. We also provide easy-to-use code that can turn existing networks into MIP-based hypernetworks.",
    "authors": [],
    "id": "SP:760b3d7a2d2d8b985617c7bbd7ff59d71bb479aa",
    "references": [
        {
            "authors": [
                "Yuval Alaluf",
                "Omer Tov",
                "Ron Mokady",
                "Rinon Gal",
                "Amit Bermano"
            ],
            "title": "Hyperstyle: Stylegan inversion with hypernetworks for real image editing",
            "venue": "In Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Juhan Bae",
                "Michael R Zhang",
                "Michael Ruan",
                "Eric Wang",
                "So Hasegawa",
                "Jimmy Ba",
                "Roger Grosse"
            ],
            "title": "Multi-rate vae: Train once, get the full rate-distortion curve",
            "venue": "arXiv preprint arXiv:2212.03905,",
            "year": 2022
        },
        {
            "authors": [
                "Guha Balakrishnan",
                "Amy Zhao",
                "Mert R Sabuncu",
                "John Guttag",
                "Adrian V Dalca"
            ],
            "title": "Voxelmorph: a learning framework for deformable medical image registration",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Ivana Bala\u017eevi\u0107",
                "Carl Allen",
                "Timothy M Hospedales"
            ],
            "title": "Hypernetwork knowledge graph embeddings",
            "venue": "In International Conference on Artificial Neural Networks,",
            "year": 2019
        },
        {
            "authors": [
                "Lukas Balles",
                "Philipp Hennig"
            ],
            "title": "Dissecting adam: The sign, magnitude and variance of stochastic gradients",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Beck",
                "Matthew Thomas Jackson",
                "Risto Vuorio",
                "Shimon Whiteson"
            ],
            "title": "Hypernetworks in meta-reinforcement learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Raphael Bensadoun",
                "Shir Gur",
                "Tomer Galanti",
                "Lior Wolf"
            ],
            "title": "Meta internal learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nils Bjorck",
                "Carla P Gomes",
                "Bart Selman",
                "Kilian Q Weinberger"
            ],
            "title": "Understanding batch normalization",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Oscar Chang",
                "Lampros Flokas",
                "Hod Lipson"
            ],
            "title": "Principled weight initialization for hypernetworks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Lee R Dice"
            ],
            "title": "Measures of the amount of ecologic association between species",
            "year": 1945
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Josip Djolonga"
            ],
            "title": "You only train once: Loss-conditional training of deep networks",
            "venue": "In International conference on learning representations,",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin Ehret",
                "Christian Henning",
                "Maria R. Cervera",
                "Alexander Meulemans",
                "Johannes von Oswald",
                "Benjamin F. Grewe"
            ],
            "title": "Continual learning in recurrent neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity mappings in deep residual networks",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Andrew Hoopes",
                "Malte Hoffman",
                "Douglas N. Greve",
                "Bruce Fischl",
                "John Guttag",
                "Adrian V. Dalca"
            ],
            "title": "Learning the effect of registration hyperparameters with hypermorph",
            "venue": "Machine Learning for Biomedical Imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Ioffe"
            ],
            "title": "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models",
            "venue": "arXiv preprint arXiv:1702.03275,",
            "year": 2017
        },
        {
            "authors": [
                "Rie Johnson",
                "Tong Zhang"
            ],
            "title": "Accelerating stochastic gradient descent using predictive variance reduction",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Boris Knyazev",
                "Michal Drozdzal",
                "Graham W Taylor",
                "Adriana Romero Soriano"
            ],
            "title": "Parameter prediction for unseen deep architectures",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Li",
                "Zheng Xu",
                "Gavin Taylor",
                "Christoph Studer",
                "Tom Goldstein"
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Lorraine",
                "David Duvenaud"
            ],
            "title": "Stochastic hyperparameter optimization through hypernetworks",
            "venue": "arXiv preprint arXiv:1802.09419,",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Fixing weight decay regularization in adam",
            "venue": "CoRR, abs/1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Ekdeep S Lubana",
                "Robert Dick",
                "Hidenori Tanaka"
            ],
            "title": "Beyond batchnorm: Towards a unified understanding of normalization in deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew MacKay",
                "Paul Vicol",
                "Jon Lorraine",
                "David Duvenaud",
                "Roger Grosse"
            ],
            "title": "Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions",
            "year": 1903
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Sebastian Ruder",
                "Mostafa Dehghani",
                "James Henderson"
            ],
            "title": "Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks",
            "venue": "arXiv preprint arXiv:2106.04489,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel S Marcus",
                "Tracy H Wang",
                "Jamie Parker",
                "John G Csernansky",
                "John C Morris",
                "Randy L Buckner"
            ],
            "title": "Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, nondemented, and demented older adults",
            "venue": "Journal of cognitive neuroscience,",
            "year": 2007
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Introductory lectures on convex optimization: A basic course, volume 87",
            "venue": "Springer Science  Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "M-E Nilsback",
                "Andrew Zisserman"
            ],
            "title": "A visual vocabulary for flower classification",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),",
            "year": 2006
        },
        {
            "authors": [
                "Jose Javier Gonzalez Ortiz",
                "John Guttag",
                "Adrian V. Dalca"
            ],
            "title": "Amortized learning of dynamic feature scaling for image segmentation",
            "venue": "arXiv preprint arXiv:2304.05448,",
            "year": 2023
        },
        {
            "authors": [
                "Nick Pawlowski",
                "Andrew Brock",
                "Matthew CH Lee",
                "Martin Rajchl",
                "Ben Glocker"
            ],
            "title": "Implicit weight uncertainty in neural networks",
            "venue": "arXiv preprint arXiv:1711.01297,",
            "year": 2017
        },
        {
            "authors": [
                "Ning Qian"
            ],
            "title": "On the momentum term in gradient descent learning algorithms",
            "venue": "Neural networks,",
            "year": 1999
        },
        {
            "authors": [
                "Siyuan Qiao",
                "Huiyu Wang",
                "Chenxi Liu",
                "Wei Shen",
                "Alan Yuille"
            ],
            "title": "Micro-batch training with batchchannel normalization and weight standardization",
            "year": 1903
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "In NIPS,",
            "year": 2007
        },
        {
            "authors": [
                "Prajit Ramachandran",
                "Barret Zoph",
                "Quoc V Le"
            ],
            "title": "Searching for activation functions",
            "venue": "arXiv preprint arXiv:1710.05941,",
            "year": 2017
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In International Conference on Medical image computing and computerassisted intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Nicolas Le Roux",
                "Mark Schmidt",
                "Francis Bach"
            ],
            "title": "A stochastic gradient method with an exponential convergence rate for finite training sets",
            "venue": "arXiv preprint arXiv:1202.6258,",
            "year": 2012
        },
        {
            "authors": [
                "Tim Salimans",
                "Durk P Kingma"
            ],
            "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks. Advances in neural information processing",
            "year": 2016
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Andrew Ilyas",
                "Aleksander M\u0105dry"
            ],
            "title": "How does batch normalization help optimization",
            "venue": "In Proceedings of the 32nd international conference on neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Joan Serr\u00e0",
                "Santiago Pascual",
                "Carlos Segura"
            ],
            "title": "Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion",
            "venue": "arXiv preprint arXiv:1906.00794,",
            "year": 2019
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien Martel",
                "Alexander Bergman",
                "David Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Ahmed Murtadha",
                "Bo Wen",
                "Yunfeng Liu"
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "venue": "arXiv preprint arXiv:2104.09864,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Matthew Tancik",
                "Pratul P Srinivasan",
                "Ben Mildenhall",
                "Sara Fridovich-Keil",
                "Nithin Raghavan",
                "Utkarsh Singhal",
                "Ravi Ramamoorthi",
                "Jonathan T Barron",
                "Ren Ng"
            ],
            "title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "venue": "arXiv preprint arXiv:2006.10739,",
            "year": 2020
        },
        {
            "authors": [
                "Kenya Ukai",
                "Takashi Matsubara",
                "Kuniaki Uehara"
            ],
            "title": "Hypernetwork-based implicit posterior estimation and model averaging of cnn",
            "venue": "In Asian Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Dmitry Ulyanov",
                "Andrea Vedaldi",
                "Victor Lempitsky"
            ],
            "title": "Instance normalization: The missing ingredient for fast stylization",
            "venue": "arXiv preprint arXiv:1607.08022,",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Johannes von Oswald",
                "Christian Henning",
                "Benjamin F. Grewe",
                "Jo\u00e3o Sacramento"
            ],
            "title": "Continual learning with hypernetworks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Alan Q Wang",
                "Adrian V Dalca",
                "Mert R Sabuncu"
            ],
            "title": "Regularization-agnostic compressed sensing mri reconstruction with hypernetworks",
            "venue": "arXiv preprint arXiv:2101.02194,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew D Zeiler"
            ],
            "title": "Adadelta: an adaptive learning rate method",
            "venue": "arXiv preprint arXiv:1212.5701,",
            "year": 2012
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Dominic Zhao",
                "Johannes von Oswald",
                "Seijin Kobayashi",
                "Jo\u00e3o Sacramento",
                "Benjamin F Grewe"
            ],
            "title": "Meta-learning via hypernetworks",
            "year": 2020
        },
        {
            "authors": [
                "Andrey Zhmoginov",
                "Mark Sandler",
                "Maksym Vladymyrov"
            ],
            "title": "Hypertransformer: Model generation for supervised and semi-supervised few-shot learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Hypernetworks, neural networks that predict the parameters of another neural network, are increasingly important models in a wide range of applications such as Bayesian optimization (Krueger et al., 2017; Pawlowski et al., 2017), generative models (Alaluf et al., 2022; Zhang & Agrawala, 2023), amortized model learning (Bae et al., 2022; Dosovitskiy & Djolonga, 2020; Hoopes et al., 2022), continual learning (Ehret et al., 2021; von Oswald et al., 2020), multi-task learning (Mahabadi et al., 2021; Serr\u00e0 et al., 2019), and meta-learning (Bensadoun et al., 2021; Zhao et al., 2020). Despite their advantages and growing use, training hypernetworks is challenging. Compared to non-hypernetwork-based models, training existing hypernetworks is often unstable. At best this increases training time, and at worst it can prevent training from converging at all. This burden limits their adoption, negatively impacting many applications. Existing hypernetwork heuristics, like gradient clipping (Ha et al., 2016; Krueger et al., 2017), are most often insufficient, while existing techniques to improve standard neural network training often fail when applied to hypernetworks.\nThis work addresses a cause of training instability. We identify and characterize a previously unstudied hypernetwork design problem and provide a straightforward solution to address it. We demonstrate analytically and empirically that the typical choices of architecture and parameter initialization in hypernetworks cause a proportionality relationship between the scale of hypernetwork inputs and the scale of parameter outputs (Fig. 1a). The resulting fluctuations in predicted parameter scale lead to large variability in the scale of gradients during optimization, resulting in unstable training and slow convergence. In some cases, this phenomenon prevents any meaningful learning. To overcome the identified magnitude proportionality issue, we propose a revision to hypernetwork models: Magnitude Invariant Parametrizations (MIP). MIP effectively eliminates the influence of the scale of hypernetwork inputs on the scale of the predicted parameters, while retaining the representational power of existing formulations. We demonstrate the proposed solution across several hypernetwork learning tasks, providing evidence that hypernetworks using MIP achieve faster convergence and more stable training than typical hypernetwork formulation (Fig. 1b).\nOur main contributions are:\n\u2022 We characterize a previously unidentified optimization problem in hypernetwork training, and show that it leads to large gradient variance and unstable training dynamics.\n\u2022 We propose a solution: Magnitude Invariant Parametrizations (MIP), a hypernetwork formulation that addresses the issue without introducing additional training or inference costs.\n\u2022 We rigorously study the proposed parametrization. We first compare it with the standard formulation and against popular normalization strategies, showing that it consistently leads to faster convergence and more stable training. We then extensively test it using various choices of optimizer, input dimensionality, hypernetwork architecture, and activation function, finding that it improves hypernetwork training in all evaluated settings.\n\u2022 We release our implementation as an open-source PyTorch library, HyperLight. HyperLight facilitates the development of hypernetwork models and provides principled choices for parametrizations and initializations, making hypernetwork adoption more accessible. We also provide code that enables using MIP seamlessly with existing models."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Parameter Initialization. Deep neural networks experience unstable training dynamics in the presence of exploding or vanishing gradients (Goodfellow et al., 2016). Weight initialization plays a critical role in the magnitude of gradients, particularly during the early stages of training. Commonly, weight initialization strategies focus on preserving the magnitude of activations during the forward pass and maintaining the magnitude of gradients during the backward pass (Glorot & Bengio, 2010; He et al., 2015). In the context of hypernetworks, early work made use of Glorot and Kaiming initialization (Bala\u017eevic\u0301 et al., 2019; Pawlowski et al., 2017), while more recent work proposes initializations that accounts for the architectural properties of the primary network (Beck et al., 2023; Chang et al., 2019; Knyazev et al., 2021; Zhmoginov et al., 2022). However, these works assume hypernetwork inputs to be categorical embeddings, which limits their applicability, and makes their formulations susceptible to the proportionality issue that we identify in this work.\nNormalization Techniques. Normalization techniques control the distribution of weights and activations, often leading to improvements in convergence by smoothing the loss surface (Bjorck et al., 2018; Ioffe, 2017; Lubana et al., 2021; Santurkar et al., 2018). Batch normalization is widely used to normalize activations using minibatch statistics, and methods like layer or group normalization instead normalize across features (Ba et al., 2016; Ulyanov et al., 2016; Wu & He, 2018). Other methods reparametrize the weights using weight-normalization strategies to decompose direction and magnitude (Qiao et al., 2019; Salimans & Kingma, 2016). As we show in our experiments, these strategies fail to resolve the proportionality issue we study. They either maintain the propor-\ntionality relationship, or eliminate proportionality by rendering the predicted weights independent of the hypernetwork input, eliminating the utility of the hypernetwork itself.\nAdaptive Optimization. High gradient variance can be detrimental to model convergence in stochastic gradient methods (Johnson & Zhang, 2013; Roux et al., 2012). Solutions to mitigate this issue encompass adaptive optimization techniques, which aim to decouple the effect of gradient direction and magnitude by normalizing by a history of gradient magnitudes (Kingma & Ba, 2014; Zeiler, 2012). Similarly, applying momentum reduces the instantaneous impact of stochastic gradients by using parameter updates based on an exponentially decaying average of past gradients (Nesterov, 2013; Qian, 1999). These strategies are implemented by many widely-used optimizers, such as Adam (Balles & Hennig, 2018; Kingma & Ba, 2014). We show experimentally that although adaptive optimizers like Adam enhance hypernetwork optimization, they do not address the root cause of the identified proportionality issue.\nFourier Features. High-dimensional Fourier projections have been used in feature engineering (Rahimi et al., 2007) and for positional encodings in language modeling applications to account for both short and long range relationships (Su et al., 2021; Vaswani et al., 2017). Additionally, implicit neural representation models benefit from sinusoidal representations (Sitzmann et al., 2020; Tancik et al., 2020). Our work also uses low dimensional Fourier projections. We demonstrate their use as a means to project hypernetwork inputs to a vector space with constant Euclidean norm.\nResidual Forms. Residual and skip connections are widely used in deep learning models and often improve model training, particularly with increasing network depth (He et al., 2016a;b; Li et al., 2018; Vaswani et al., 2017). Building on this intuition, instead of the hypernetworks predicting the network parameters directly, our proposed hypernetworks predict parameter changes, mitigating part of the proportionality problem at hand."
        },
        {
            "heading": "3 THE HYPERNETWORK PROPORTIONALITY PROBLEM",
            "text": "Preliminaries. Deep learning tasks generally involve a model f(x; \u03b8) \u2192 y, with learnable parameters \u03b8. In hierarchical models using hypernetworks, the parameters \u03b8 of the primary network f are predicted by a hypernetwork h(\u03b3;\u03c9) \u2192 \u03b8 based on a input vector \u03b3. Instead of learning parameters \u03b8 of the primary network f , only the learnable parameters \u03c9 of the hypernetwork h are optimized using backpropagation. The specific nature of the hypernetwork inputs \u03b3 varies across applications, but regularly corresponds to a low dimensional quantity that models properties of the learning task, and is often a simple scalar or embedding vector (Dosovitskiy & Djolonga, 2020; Hoopes et al., 2022; Lorraine & Duvenaud, 2018; Ukai et al., 2018; Wang et al., 2021).\nAssumptions. For analysis, we assume the following about the hypernetwork formulation: 1) The architecture is a series of fully connected layers \u03d5(Wx + b) where W are the parameters, b the biases and \u03d5(x) the non-linear activation function; 2) The nonlinear activation is a piece-wise linear function with a single switch at the origin. Namely, it satisfies \u03d5(x) = 1[x\u22650](\u03b1x) + 1[x<0](\u03b2x), for \u03b1, \u03b2 > 0 (e.g., LeakyReLU) 3) Bias vectors b are initialized to zero. Existing hypernetworks satisfy these properties for the large majority of applications (Dosovitskiy & Djolonga, 2020; Ha et al., 2016; Lorraine & Duvenaud, 2018; MacKay et al., 2019; Ortiz et al., 2023; Ukai et al., 2018; von Oswald et al., 2020; Wang et al., 2021).\nInput-Output Proportionality. We demonstrate that under these widely-used settings, hypernetwork inputs and outputs involve a proportionality relationship, and describe how this can impede hypernetwork training. We show that 1) at initialization, any intermediate feature vector x(k) at layer k will be proportional to hypernetwork input \u03b3, even under the presence of non-linear activation functions, and 2) this leads to large gradient magnitude fluctuations detrimental to optimization.\nWe first consider the case where \u03b3 \u2208 R is a scalar value. Let h(\u03b3;\u03c9) use a fully connected architecture composed of a series of fully connected layers\nh(\u03b3;\u03c9) = W (n)x(n) + b(n)\nx(k+1) = \u03d5(W (k)x(k) + b(k))\nx(1) = \u03b3\n(1)\nwhere x(k) is the input vector of the kth fully connected layer with learnable parameters W (k) and biases b(k). To prevent gradients from exploding or vanishing when chaining several layers, it is common to initialize the parameters W (i) and biases b(i) so that either the magnitude of the activations is approximately constant across layers in the forward pass (known as fan in), or so that the magnitude of the gradients is constant across layers in the backward pass (known as fan out) (Glorot & Bengio, 2010; He et al., 2015). In both settings, the parameters W (i) are initialized using a zero mean Normal distribution and bias vectors b(i) are initialized to zero. If \u03b3 > 0, and \u03d5(x) has the common form specified above, at initialization the ith entry of vector x(2) is\nx (2) i = \u03d5(W (1) i \u03b3 + b (1)) = \u03b3\u03d5(W (1) i ) \u221d \u03b3, (2)\nsince b(1) = 0 and \u03d5(W (1)i ) is independent of \u03b3. Using induction, we assume that for layer k, x (k) j \u221d \u03b3 \u2200j, and show this property for layer k+1. The value of the ith element of vector x(k+1) is\nx (k+1) i = \u03d5 ( b (k) i + \u2211 j W (k) ij x (k) j ) = \u03b3 \u03d5 (\u2211 j W (k) ij \u03b1 (k) j ) \u221d \u03b3, (3)\nsince b(k)i = 0, and the term inside \u03d5 is independent of \u03b3. If \u03b3 is not strictly positive, we can reach the same proportionality result, but with separate constants for the positive and the negative range. This dependency holds regardless of the number of layers and the number of neurons per hidden layer, and also holds when residual connections are employed. When \u03b3 is a vector input, we find a similar relationship with the overall magnitude of the input and the magnitude of the output. Given the absence of bias terms, and the lack of multiplicative interactions in the architecture, the fully connected network propagates magnitude changes in the input.\nTraining implications. Since \u03b8 = x(n+1), this result leads to a proportionality relationship for the magnitude of the predicted parameters ||\u03b8||2 \u221d ||\u03b3|| and their variance Var(\u03b8) \u221d ||\u03b3||2. As the scale of the primary network parameters \u03b8 will depend on \u03b3, this will affect the scale of the layer outputs and gradients of the primary network. In turn, these large gradient magnitude fluctuations lead to unstable training dynamics for stochastic gradient descent methods (Glorot & Bengio, 2010).\nFurther Considerations. Our analysis relies on biases being at zero, which only holds at initialization, and does not include normalization layers that are sometimes used. However, in our experiments, we find that biases remain near zero during early training, and hypernetworks with alternative choices of activation function, input dimensionality, or with normalization layers, still suffer from the identified issue and consistently benefit from our proposed parametrization (see Section 6)."
        },
        {
            "heading": "4 MAGNITUDE INVARIANT PARAMETRIZATIONS",
            "text": "To address the proportionality dependency, we make two straightforward changes to the typical hypernetwork formulation: 1) We introduce an encoding function that maps inputs into a constantnorm vector space, and 2) we treat hypernetwork predictions as additive changes to the main network parameters, rather than as the parameters themselves. These changes make the primary network weight distribution non-proportional to the hypernetwork input and stable across the range of hypernetwork inputs. Figure 2 illustrates these changes to the hypernetwork.\nInput Encoding. To address the proportionality problem, we map the inputs \u03b3 \u2208 [0, 1] to a space with a constant Euclidean norm ||EL2(\u03b3)||2 = 1 using the function EL2(\u03b3) = [cos(\u03b3\u03c0/2), sin(\u03b3\u03c0/2)]. With this change, the input magnitude to the hypernetwork is constant, so ||x(1)|| \u0338\u221d \u03b3. For higher-dimensional inputs, we apply this transformation to each input individually, leading to an output vector with double the number of dimensions. This transformation results in an input representation with a constant norm, thereby eliminating the proportionality effect.\nFor our input encoding, we first map each dimension of the input vector to the range [0, 1] to maximize output range of EL2. We use min-max scaling of the input: \u03b3\u2032 = (\u03b3 \u2212 \u03b3min)/(\u03b3max \u2212 \u03b3min). For unconstrained inputs, such as Gaussian variables, we first apply the logistic function \u03c3(x) = 1/(1 + exp(\u2212x)). If inputs span several orders of magnitude, we take the log before the min-max scaling as in (Bae et al., 2022; Dosovitskiy & Djolonga, 2020).\nOutput Encoding. Residual forms have become a cornerstone in contemporary deep learning architectures (He et al., 2016a; Li et al., 2018; Vaswani et al., 2017). Motivated by these methods, we\nreplace the standard hypernetwork framework with one that learns both primary network f parameters (as is typically learned in existing formulations) and hypernetwork predictions, which are used as additive changes to these primary parameters. We introduce a set of learnable parameters \u03b80, and compute the primary network parameters as \u03b8 = \u03b80 + h(EL2(\u03b3);\u03c9).\nParameter Initialization We initialize the hypernetwork weights \u03c9 using common initialization methods for fully connected layers that consider the number of input and output neurons to each layer, such as Kaiming or Glorot initialization. Then, we initialize the independent parameters \u03b80 in the same manner that we would initialize the parameters of an equivalent regular network. We provide further details and examples in section A of the supplement."
        },
        {
            "heading": "5 EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "5.1 TASKS",
            "text": "We evaluate our proposed parametrization on several tasks involving hypernetwork-based models.\nBayesian Neural Networks. Hypernetwork models have been used to learn families of functions conditioned on a prior distribution (Ukai et al., 2018). During training, the prior representation \u03b3 \u2208 Rd is sampled from the prior distribution \u03b3 \u223c p(\u03b3) and used to condition the hypernetwork h(\u03b3;\u03c9) \u2192 \u03b8 to predict the parameters of the primary network model f(x; \u03b8). Once trained, the family of posterior networks is then used to estimate parameter uncertainty or to improve model calibration. For illustrative purposes we first evaluate a setting where f(x; \u03b8) is a feed-forward neural network used to classify the MNIST dataset. Then, we tackle a more complex setting where f(x; \u03b8) is a ResNet-like model trained the OxfordFlowers-102 dataset (Nilsback & Zisserman, 2006). In both settings, we use the prior N (0, 1) for each input. Hypermorph. Learning-based medical image registration networks f(xm, xf ; \u03b8) \u2192 \u03d5 register a moving image xm to a fixed image xf by predicting a flow or deformation field \u03d5 between them. The common (unsupervised) loss balances an image alignment term Lsim and a spatial regularization (smoothness) term Lreg. The learning objective is then L = (1 \u2212 \u03b3)Lsim(xm \u25e6 \u03d5, xf ) + \u03b3Lreg(\u03d5), where \u03b3 controls the trade-off. In Hypermorph (Hoopes et al., 2022), multiple regularization settings for medical image registration are learned jointly using hypernetworks. The hypernetwork is given the trade-off parameter \u03b3 as input, sampled stochastically from U(0, 1) during training. We follow the same experimental setup, using a U-Net architecture for the primary (registration) network and training with MSE for Lsim and total variation for Lreg. We train models on the OASIS dataset. For evaluation, we use the predicted flow field to warp anatomical segmentation label maps of the moving image, and measure the volume overlap to the fixed label maps (Balakrishnan et al., 2019).\nScale-Space Hypernetworks. We also use a hypernetwork to efficiently learn a family of models with varying internal rescaling factors in the downsampling and upsampling layers, as done in Ortiz et al. (2023). In this setting, \u03b3 corresponds to the scale factor. Given hypernetwork input \u03b3, the\nhypernetwork h(\u03b3;\u03c9) \u2192 \u03b8 predicts the parameters of the primary network, which performs the spatial rescaling operations according to the value of \u03b3. We study a setting where f(x; \u03b8) is a convolutional network with variable resizing layers, the rescaling factor is sampled from U(0, 0.5), and evaluate using the OxfordFlowers-102 classification problem and the OASIS segmentation task."
        },
        {
            "heading": "5.2 EXPERIMENT DETAILS",
            "text": "Model. We implement the hypernetwork as a neural network with fully connected layers and LeakyReLU activations for all but the last layer, which has linear output. Hypernetwork weights are initialized using Kaiming initialization on fan out mode and biases are initialized to zero. Unless specified otherwise, the hypernetwork architecture has two hidden layers with 16 and 128 neurons respectively. We use this implementation for both the default (existing) hypernetworks, and our proposed (MIP) hypernetworks.\nTraining. We use two popular choices of optimizer: SGD with Nesterov momentum, and Adam. We search over a range of initial learning rates and report the best performing models; further details are included in section B of the supplement.\nImplementation. An important contribution of our work is HyperLight, our PyTorch hypernetwork framework. HyperLight implements the proposed hypernetwork parametrization, but also provides a modular and composable API that facilitates the development of hypernetwork models. Using HyperLight, practitioners can employ existing non-hypernetwork model definitions and pretrained model weights, and can easily build models using hierarchical hypernetworks. Anonymized source code is available at https://github.com/anonresearcher8/hyperlight."
        },
        {
            "heading": "6 EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "6.1 EFFECT OF PROPORTIONALITY ON PARAMETER AND GRADIENT DISTRIBUTIONS",
            "text": "First, we empirically show how the proportionality phenomenon affects the distribution of predicted weights \u03b8 and their corresponding gradients for the Bayesian neural networks on MNIST. Figures 3a\nand 3b compare the distributions of the primary network weights and layer outputs for a range of values of hypernetwork input \u03b3. The default hypernetwork parametrization is highly sensitive to changes in the input, in contrast, MIP eliminates this dependency, with the resulting distribution closely matching that of the non-hypernetwork models. Figure 1a (in the introduction), shows that using the default formulation, the scale of the weights correlates linearly with the value of the hypernetwork input, and that, crucially, this correlation is still present after the training process ends. In contrast, MIP parametrizations lead to a weight distribution that is robust to the input \u03b3, both at the start and end of training.\nWe also analyze how the proportionality affects the early phase of hypernetwork optimization by studying the distribution of gradient norms during training. Figure 3c shows the norm of the predicted parameter gradients ||\u2207\u03b8L|| as training progresses. Consistent with our analysis, hypernetworks with default parametrization experience large swings in gradient magnitude because of the proportionality relationship between inputs and predicted parameters. In contrast, the MIP strategy leads to a substantially smaller variance and more stable gradient magnitude."
        },
        {
            "heading": "6.2 MODEL TRAINING IMPROVEMENTS",
            "text": "In this experiment, we analyze how MIP affects model convergence for the considered tasks. For all experiments, we found that MIP hypernetworks did not introduce a measurable impact in training runtime, so we report per-epoch steps.\nFigure 1b (in the introduction) shows the training loss for Bayesian networks trained on MNIST. We find that MIP parametrizations result in smaller loss sooner during training, and the default parametrization suffers from sporadic training instabilities (spikes in the training loss), while MIP leads to stable training. Similarly, Figure 4a shows the test accuracy for Bayesian networks trained on OxfordFlowers. In this task, MIP also achieves faster convergence and better final model accuracy for both choices of optimizer.\nFigures 4b and 4c present convergence curves for the other two tasks. For Hypermorph, MIP parametrizations are crucial when using SGD with momentum since otherwise the model fails to meaningfully train. For all choices of learning rate the default hypernetwork failed to converge, whereas with MIP parametrization it converged for a large range of values. With Adam, networks train meaningfully, and MIP models consistently achieve similar Dice scores substantially faster. They are less sensitive to weight initializations. Though the Adam optimizer partially mitigates the gradient variance issue by normalizing by a history of previous gradients, the MIP parametrization leads to substantially faster convergence. Furthermore, for the Scale-Space segmentation, we find that for both optimizers MIP models achieve substantially faster convergence and better final accuracy compared to those with the default parametrization.\nComparison to normalization strategies. We compare the proposed parametrization to popular choices of normalization layers found in the deep learning literature. Using the default formulation, where the predicted weights start proportional to the hypernetwork input, we found that existing normalization strategies fall into two categories: they either keep the proportionality relationship present (such as batch normalization), or remove the proportionality by making the predicted weights independent of the hypernetwork input (such as layer or weight normalization). We provide further details in Section C of the supplemental material.\nWe test several of these normalization strategies. BatchNorm-P, adds batch normalization layers to the primary network. LayerNorm-P, adds feature normalization layers to the primary network. LayerNorm-H, adds feature normalization layers to the hypernetwork layers. WeightNorm, performs weight normalization, which decouples the gradient magnitude and direction, to weights predicted by the hypernetwork (Ba et al., 2016; Ioffe, 2017; Salimans & Kingma, 2016). Figure 5a shows the evolution of the test accuracy for the Scale-Space hypernetworks trained on OxfordFlowers. We report wall clock time, since some normalization strategies, such as BatchNorm, substantially increase the computation time required per iteration. For networks trained with SGD, these normalization strategies enable training, but do not significantly improve on default hypernetworks when trained with Adam. Models trained with SGD momentum and hypernetwork feature normalization (LayerNorm-H) diverged early into training for all considered hyperparameter settings. Models trained with the proposed MIP parametrization lead to substantially faster convergence and better final model accuracy.\nInitialization Schemes. We compare MIP and the default hypernetworks to hypernetworks that use the Hyperfan-in and Hyperfan-out initialization strategies from Chang et al. (2019). The Hyperfan initialization takes into account the hypernetwork and primary network architectures when initializing model weights, improving model convergence and training stability. However, Hyperfan initializations are not designed for magnitude-encoded inputs, so they are susceptible to the proportionality issue we identify.\nFigure 5b presents convergence results for the Hypermorph task with SGD and Adam. We find that Hyperfan initializations do not resolve the training challenges when using SGD. For hypernetworks trained with Adam, MIP outperforms both Hyperfan variants.\nAblation Analysis. We study the contribution of each of the two main components of the MIP parametrizations: input encoding and additive output formulation. Figure 5c shows the effect on convergence for two tasks. We found that both components reduce the proportionality dependency between the hypernetwork inputs and outputs, and that each component independently achieves substantial improvements in model convergence. However, we find that best results (fastest convergence) are consistently achieved when both components are used jointly during training."
        },
        {
            "heading": "6.3 ROBUSTNESS ANALYSIS",
            "text": "Hypernetwork Input Dimensionality. We study the effect of the number of dimensions of the input to the hypernetwork model. We evaluate on the Bayesian neural network task, and we vary the number of dimensions of the input prior. We train models with geometrically increasing number of input dimensions, dim(\u03b3) = 1, 2, . . . , 32. Figure 6 (in section C.1 of the supplement) shows that the proposed MIP strategy leads to improvements in model convergence and final model accuracy as we increase the dimension of the hypernetwork input \u03b3.\nChoice of Hypernetwork Architecture. We assess model performance when varying the properties of the hypernetwork architecture. We vary the width (number of hidden neurons per layer) and depth (number of layers)\u2013 fully connected networks with 3, 4 and 5 layers and with 16 and 128 neurons per layer, as well as an exponentially growing number of neurons per layer Dim(xn) = 16 \u00b7 2n.\nFigures 7 and 8 (in section C.2 of the supplement) show that the MIP improvements generalize to the all tested hypernetwork architectures with analogous improvements in model training.\nChoice of Nonlinear Function Activation. While our method is motivated by the training instability present in hypernetworks with (Leaky)-ReLU nonlinear activation functions, we explored applying it to other common choices of activation functions found in the literature: Tanh, GELU and SiLU (Hendrycks & Gimpel, 2016; Ramachandran et al., 2017). Figure 9 (in section C.3 of the supplement) shows that MIP consistently helps for all choices of nonlinear activation function, and the improvements are similar to those of the LeakyReLU models."
        },
        {
            "heading": "7 LIMITATIONS",
            "text": "All hypernetwork models used in our experiments are composed of fully connected layers and use activation and initialization choices commonly recommended in the literature. Similarly, we focused on two optimizers in our experiments, SGD with momentum and Adam. We believe that we would see similar results for other less common architectures and optimizers, but this remains to be investigated. Furthermore, we focus on training models from scratch. As hypernetworks become popular in transfer learning, we believe this will be an interesting avenue for future analysis of MIP."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "We showed through analysis and experimentation that traditional hypernetwork formulations are susceptible to training instability, caused by the effect of the magnitude of hypernetwork input values on primary network weights and gradients, and that standard methods such as batch and layer normalization do not solve the problem. We then proposed the use of a new method, Magnitude Invariant Parametrizations (MIP), for addressing this problem. Through extensive experiments, we demonstrated that MIP leads to substantial improvements in convergence times and model accuracy across multiple hypernetwork architectures, training scenarios, and tasks. Given that using MIP never reduces model performance and can dramatically improve training, we expect the method to be widely useful for training hypernetworks."
        }
    ],
    "title": "MAGNITUDE INVARIANT PARAMETRIZATIONS IMPROVE HYPERNETWORK LEARNING"
}