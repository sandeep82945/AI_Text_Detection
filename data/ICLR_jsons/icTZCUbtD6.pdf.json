{
    "abstractText": "Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \u201chard\u201d samples. However, there is a lack of consensus regarding the definition and evaluation of \u201chardness\u201d. Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nabeel Seedat"
        },
        {
            "affiliations": [],
            "name": "Fergus Imrie"
        },
        {
            "affiliations": [],
            "name": "Mihaela van der Schaar"
        }
    ],
    "id": "SP:13bb110706c9fdf904008817e33ffb5d03e8bcf5",
    "references": [
        {
            "authors": [
                "Chirag Agarwal",
                "Daniel D\u2019souza",
                "Sara Hooker"
            ],
            "title": "Estimating example difficulty using variance of gradients",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Rushil Anirudh",
                "Jayaraman J Thiagarajan"
            ],
            "title": "Out of distribution detection via neural network anchoring",
            "venue": "In Asian Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Octavio Arriaga",
                "Sebastian Palacio",
                "Matias Valdenegro-Toro"
            ],
            "title": "Difficulty estimation with action scores for computer vision tasks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "HeeSun Bae",
                "Seungjae Shin",
                "Byeonghu Na",
                "JoonHo Jang",
                "Kyungwoo Song",
                "Il-Chul Moon"
            ],
            "title": "From noisy prediction to true label: Noisy prediction calibration via generative model",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Robert Baldock",
                "Hartmut Maennel",
                "Behnam Neyshabur"
            ],
            "title": "Deep learning through the lens of example difficulty",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chris Bedi",
                "Edward Cone",
                "Brian Foster",
                "Riva Froymovich",
                "Colleen Haikes",
                "Michael Hubbard",
                "Brian Klapac",
                "Alan Marks",
                "Debbie O\u2019Brien",
                "Kathy O\u2019Connell",
                "Gaby Pacini-Pinto",
                "Kevin Palmer",
                "Rob Pickering",
                "Chris Pope",
                "Dan Rogers",
                "Julia Smith",
                "Dave Wright"
            ],
            "title": "The Global CIO Point of View The New Agenda for Transformative Leadership: Reimagine Business for Machine Learning",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Ulfar Erlingsson",
                "Nicolas Papernot"
            ],
            "title": "Distribution density, tails, and outliers in machine learning: Metrics and applications",
            "year": 1910
        },
        {
            "authors": [
                "Haihua Chen",
                "Jiangping Chen",
                "Junhua Ding"
            ],
            "title": "Data evaluation and enhancement for quality improvement of machine learning",
            "venue": "IEEE Transactions on Reliability,",
            "year": 2021
        },
        {
            "authors": [
                "Janez Dem\u0161ar"
            ],
            "title": "Statistical comparisons of classifiers over multiple data sets",
            "venue": "The Journal of Machine learning research,",
            "year": 2006
        },
        {
            "authors": [
                "Simao Eduardo",
                "Kai Xu",
                "Alfredo Nazabal",
                "Charles Sutton"
            ],
            "title": "Repairing systematic outliers by learning clean subspaces in VAEs",
            "venue": "arXiv preprint arXiv:2207.08050,",
            "year": 2022
        },
        {
            "authors": [
                "Kawin Ethayarajh",
                "Yejin Choi",
                "Swabha Swayamdipta"
            ],
            "title": "Understanding dataset difficulty with v-usable information",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Vitaly Feldman"
            ],
            "title": "Does learning require memorization? A short tale about a long tail",
            "venue": "In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "James Zou"
            ],
            "title": "Data Shapley: Equitable valuation of data for machine learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Mark S Graham",
                "Petru-Daniel Tudosiu",
                "Paul Wright",
                "Walter Hugo Lopez Pinaya",
                "U Jean-Marie",
                "Yee H Mah",
                "James T Teo",
                "Rolf Jager",
                "David Werring",
                "Parashkev Nachev"
            ],
            "title": "Transformer-based out-of-distribution detection for clinically safe segmentation",
            "venue": "In International Conference on Medical Imaging with Deep Learning,",
            "year": 2022
        },
        {
            "authors": [
                "L\u00e9o Grinsztajn",
                "Edouard Oyallon",
                "Ga\u00ebl Varoquaux"
            ],
            "title": "Why do tree-based models still outperform deep learning on typical tabular data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Nitin Gupta",
                "Hima Patel",
                "Shazia Afzal",
                "Naveen Panwar",
                "Ruhi Sharma Mittal",
                "Shanmukha Guttula",
                "Abhinav Jain",
                "Lokesh Nagalapatti",
                "Sameep Mehta",
                "Sandeep Hans"
            ],
            "title": "Data quality toolkit: Automatic assessment of data quality and remediation for machine learning datasets",
            "venue": "arXiv preprint arXiv:2108.05935,",
            "year": 2021
        },
        {
            "authors": [
                "Isabelle Guyon"
            ],
            "title": "The data-centric era: How ML is becoming an experimental science",
            "venue": "Keynote: Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Tongliang Liu",
                "Gang Niu",
                "Ivor W Tsang",
                "James T Kwok",
                "Masashi Sugiyama"
            ],
            "title": "A survey of label-noise representation learning: Past, present and future",
            "year": 2011
        },
        {
            "authors": [
                "Lasse Hansen",
                "Nabeel Seedat",
                "Mihaela van der Schaar",
                "Andrija Petrovic"
            ],
            "title": "Reimagining synthetic tabular data generation through data-centric AI: A comprehensive benchmark",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Mantas Mazeika",
                "Duncan Wilson",
                "Kevin Gimpel"
            ],
            "title": "Using trusted data to train deep networks on labels corrupted by severe noise",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Elisabeth RM Heremans",
                "Nabeel Seedat",
                "Bertien Buyse",
                "Dries Testelmans",
                "Mihaela van der Schaar",
                "Maarten De Vos"
            ],
            "title": "U-PASS: An uncertainty-guided deep learning pipeline for automated sleep staging",
            "venue": "Computers in Biology and Medicine,",
            "year": 2024
        },
        {
            "authors": [
                "Sara Hooker"
            ],
            "title": "Moving beyond \u201calgorithmic bias is a data problem",
            "year": 2021
        },
        {
            "authors": [
                "Sara Hooker",
                "Aaron Courville",
                "Gregory Clark",
                "Yann Dauphin",
                "Andrea Frome"
            ],
            "title": "What do compressed deep neural networks forget",
            "year": 1911
        },
        {
            "authors": [
                "Abhinav Jain",
                "Hima Patel",
                "Lokesh Nagalapatti",
                "Nitin Gupta",
                "Sameep Mehta",
                "Shanmukha Guttula",
                "Shashank Mujumdar",
                "Shazia Afzal",
                "Ruhi Sharma Mittal",
                "Vitobha Munigala"
            ],
            "title": "Overview and importance of data quality for machine learning tasks",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad Hossein Jarrahi",
                "Ali Memariani",
                "Shion Guha"
            ],
            "title": "The principles of data-centric AI (DCAI)",
            "venue": "arXiv preprint arXiv:2211.14611,",
            "year": 2022
        },
        {
            "authors": [
                "Qingrui Jia",
                "Xuhong Li",
                "Lei Yu",
                "Jiang Bian",
                "Penghao Zhao",
                "Shupeng Li",
                "Haoyi Xiong",
                "Dejing Dou"
            ],
            "title": "Learning from training dynamics: Identifying mislabeled data beyond manually designed features",
            "venue": "arXiv preprint arXiv:2212.09321,",
            "year": 2022
        },
        {
            "authors": [
                "Ziheng Jiang",
                "Chiyuan Zhang",
                "Kunal Talwar",
                "Michael C Mozer"
            ],
            "title": "Characterizing structural regularities of labeled data in overparameterized models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sanjay Krishnan",
                "Jiannan Wang",
                "Eugene Wu",
                "Michael J Franklin",
                "Ken Goldberg"
            ],
            "title": "ActiveClean: Interactive data cleaning for statistical modeling",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2016
        },
        {
            "authors": [
                "Yongchan Kwon",
                "James Zou"
            ],
            "title": "Beta Shapley: A unified and noise-reduced data valuation framework for machine learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Yann LeCun",
                "Corinna Cortes",
                "CJ Burges"
            ],
            "title": "MNIST handwritten digit database",
            "venue": "ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist,",
            "year": 2010
        },
        {
            "authors": [
                "Peng Li",
                "Xi Rao",
                "Jennifer Blase",
                "Yue Zhang",
                "Xu Chu",
                "Ce Zhang"
            ],
            "title": "CleanML: A study for evaluating the impact of data cleaning on ML classification tasks",
            "venue": "IEEE 37th International Conference on Data Engineering (ICDE),",
            "year": 2021
        },
        {
            "authors": [
                "Weixin Liang",
                "Girmaw Abebe Tadesse",
                "Daniel Ho",
                "L Fei-Fei",
                "Matei Zaharia",
                "Ce Zhang",
                "James Zou"
            ],
            "title": "Advances, challenges and opportunities in creating data for trustworthy AI",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zachary C Lipton",
                "Jacob Steinhardt"
            ],
            "title": "Research for practice: Troubling trends in machine-learning scholarship",
            "venue": "Communications of the ACM,",
            "year": 2019
        },
        {
            "authors": [
                "Zifan Liu",
                "Zhechun Zhou",
                "Theodoros Rekatsinas"
            ],
            "title": "Picket: Guarding against corrupted data in tabular data during learning and inference",
            "venue": "The VLDB Journal,",
            "year": 2022
        },
        {
            "authors": [
                "Pratyush Maini",
                "Saurabh Garg",
                "Zachary Chase Lipton",
                "J Zico Kolter"
            ],
            "title": "Characterizing datapoints via second-split forgetting",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Anna Majkowska",
                "Sid Mittal",
                "David F Steiner",
                "Joshua J Reicher",
                "Scott Mayer McKinney",
                "Gavin E Duggan",
                "Krish Eswaran",
                "Po-Hsuan Cameron Chen",
                "Yun Liu",
                "Sreenivasa Raju Kalidindi"
            ],
            "title": "Chest radiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population-adjusted evaluation",
            "year": 2020
        },
        {
            "authors": [
                "S\u00f6ren Mindermann",
                "Jan M Brauner",
                "Muhammed T Razzak",
                "Mrinank Sharma",
                "Andreas Kirsch",
                "Winnie Xu",
                "Benedikt H\u00f6ltgen",
                "Aidan N Gomez",
                "Adrien Morisot",
                "Sebastian Farquhar"
            ],
            "title": "Prioritized training on points that are learnable, worth learning, and not yet learnt",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Norman Mu",
                "Justin Gilmer"
            ],
            "title": "MNIST-C: A robustness benchmark for computer vision",
            "venue": "arXiv preprint arXiv:1906.02337,",
            "year": 2019
        },
        {
            "authors": [
                "Jishnu Mukhoti",
                "Tsung-Yu Lin",
                "Bor-Chun Chen",
                "Ashish Shah",
                "Philip HS Torr",
                "Puneet K Dokania",
                "Ser-Nam Lim"
            ],
            "title": "Raising the bar on the evaluation of out-of-distribution detection",
            "year": 2022
        },
        {
            "authors": [
                "Afshan Nabi",
                "Berke Dilekoglu",
                "Ogun Adebali",
                "Oznur Tastan"
            ],
            "title": "Discovering misannotated lncrnas using deep learning training",
            "venue": "dynamics. Bioinformatics,",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Ng",
                "Lora Aroyo",
                "Cody Coleman",
                "Greg Diamos",
                "Vijay Janapa Reddi",
                "Joaquin Vanschoren",
                "Carole-Jean Wu",
                "Sharon Zhou"
            ],
            "title": "URL https: //datacentricai.org",
            "venue": "NeurIPS data-centric AI workshop,",
            "year": 2021
        },
        {
            "authors": [
                "Curtis Northcutt",
                "Lu Jiang",
                "Isaac Chuang"
            ],
            "title": "Confident learning: Estimating uncertainty in dataset labels",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2021
        },
        {
            "authors": [
                "Curtis G Northcutt",
                "Anish Athalye",
                "Jonas Mueller"
            ],
            "title": "Pervasive label errors in test sets destabilize machine learning benchmarks",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Mansheej Paul",
                "Surya Ganguli",
                "Gintare Karolina Dziugaite"
            ],
            "title": "Deep learning on a data diet: Finding important examples early in training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Geoff Pleiss",
                "Tianyi Zhang",
                "Ethan Elenberg",
                "Kilian Q Weinberger"
            ],
            "title": "Identifying mislabeled data using the area under the margin ranking",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Cedric Renggli",
                "Luka Rimanic",
                "Nezihe Merve G\u00fcrel",
                "Bojan Karlas",
                "Wentao Wu",
                "Ce Zhang"
            ],
            "title": "A data quality-driven view of MLOps",
            "venue": "IEEE Data Engineering Bulletin,",
            "year": 2021
        },
        {
            "authors": [
                "Nithya Sambasivan",
                "Shivani Kapania",
                "Hannah Highfill",
                "Diana Akrong",
                "Praveen Paritosh",
                "Lora M Aroyo"
            ],
            "title": "Everyone wants to do the model work, not the data work\u201d: Data cascades in high-stakes AI",
            "venue": "In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nabeel Seedat",
                "Jonathan Crabb\u00e9",
                "Ioana Bica",
                "Mihaela van der Schaar"
            ],
            "title": "Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Nabeel Seedat",
                "Jonathan Crabb\u00e9",
                "Mihaela van der Schaar"
            ],
            "title": "Data-SUITE: Data-centric identification of in-distribution incongruous examples",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Nabeel Seedat",
                "Jonathan Crabb\u00e9",
                "Zhaozhi Qian",
                "Mihaela van der Schaar"
            ],
            "title": "TRIAGE: Characterizing and auditing training data for improved regression",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Nabeel Seedat",
                "Fergus Imrie",
                "Mihaela van der Schaar"
            ],
            "title": "Navigating data-centric artificial intelligence with DC-Check: Advances, challenges, and opportunities",
            "venue": "IEEE Transactions on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Shwartz",
                "Uri Stern",
                "Daphna Weinshall"
            ],
            "title": "The dynamic of consensus in deep networks and the identification of noisy labels",
            "venue": "arXiv preprint arXiv:2210.00583,",
            "year": 2022
        },
        {
            "authors": [
                "Jasper Snoek",
                "Alex Wiltschko",
                "Ali Rahimi"
            ],
            "title": "Winner\u2019s curse? On pace, progress, and empirical rigor",
            "venue": "In International Conference on Learning Representations - Workshop,",
            "year": 2018
        },
        {
            "authors": [
                "Hwanjun Song",
                "Minseok Kim",
                "Dongmin Park",
                "Yooju Shin",
                "Jae-Gil Lee"
            ],
            "title": "Learning from noisy labels with deep neural networks: A survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ben Sorscher",
                "Robert Geirhos",
                "Shashank Shekhar",
                "Surya Ganguli",
                "Ari S Morcos"
            ],
            "title": "Beyond neural scaling laws: Beating power law scaling via data pruning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Joan Bruna",
                "Manohar Paluri",
                "Lubomir Bourdev",
                "Rob Fergus"
            ],
            "title": "Training convolutional networks with noisy labels",
            "venue": "In 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Hao Sun",
                "Alex J Chan",
                "Nabeel Seedat",
                "Alihan H\u00fcy\u00fck",
                "Mihaela van der Schaar"
            ],
            "title": "When is off-policy evaluation useful? a data-centric perspective",
            "venue": "arXiv preprint arXiv:2311.14110,",
            "year": 2023
        },
        {
            "authors": [
                "Hao Sun",
                "Boris van Breugel",
                "Jonathan Crabb\u00e9",
                "Nabeel Seedat",
                "Mihaela van der Schaar"
            ],
            "title": "What is flagged in uncertainty quantification? latent density models for uncertainty categorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Swabha Swayamdipta",
                "Roy Schwartz",
                "Nicholas Lourie",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Noah A Smith",
                "Yejin Choi"
            ],
            "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Junjiao Tian",
                "Yen-Chang Hsu",
                "Yilin Shen",
                "Hongxia Jin",
                "Zsolt Kira"
            ],
            "title": "Exploring covariate and concept shift for detection and confidence calibration of out-of-distribution data",
            "venue": "arXiv preprint arXiv:2110.15231,",
            "year": 2021
        },
        {
            "authors": [
                "Mariya Toneva",
                "Alessandro Sordoni",
                "Remi Tachet des Combes",
                "Adam Trischler",
                "Yoshua Bengio",
                "Geoffrey J Gordon"
            ],
            "title": "An empirical study of example forgetting during deep neural network learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Joaquin Vanschoren",
                "Jan N Van Rijn",
                "Bernd Bischl",
                "Luis Torgo"
            ],
            "title": "OpenML: Networked science in machine learning",
            "venue": "ACM SIGKDD Explorations Newsletter,",
            "year": 2014
        },
        {
            "authors": [
                "Jiachen T Wang",
                "Ruoxi Jia"
            ],
            "title": "Data banzhaf: A robust data valuation framework for machine learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Amanda West"
            ],
            "title": "Machine Learning Models for the Future: The rise of the data scientist",
            "venue": "Refinitiv Labs,",
            "year": 2020
        },
        {
            "authors": [
                "Steven Euijong Whang",
                "Yuji Roh",
                "Hwanjun Song",
                "Jae-Gil Lee"
            ],
            "title": "Data collection and quality challenges in deep learning: A data-centric AI perspective",
            "venue": "The VLDB Journal,",
            "year": 2023
        },
        {
            "authors": [
                "Jim Winkens",
                "Rudy Bunel",
                "Abhijit Guha Roy",
                "Robert Stanforth",
                "Vivek Natarajan",
                "Joseph R Ledsam",
                "Patricia MacWilliams",
                "Pushmeet Kohli",
                "Alan Karthikesalingam",
                "Simon Kohl"
            ],
            "title": "Contrastive training for improved out-of-distribution detection",
            "venue": "arXiv preprint arXiv:2007.05566,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaobo Xia",
                "Tongliang Liu",
                "Bo Han",
                "Mingming Gong",
                "Jun Yu",
                "Gang Niu",
                "Masashi Sugiyama"
            ],
            "title": "Sample selection with uncertainty of losses for learning with noisy labels",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jingkang Yang",
                "Kaiyang Zhou",
                "Ziwei Liu"
            ],
            "title": "Full-spectrum out-of-distribution detection",
            "venue": "International Journal of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Linjun Zhang",
                "James Zou",
                "Carlos Guestrin"
            ],
            "title": "Beyond confidence: Reliable models should also consider atypicality",
            "venue": "arXiv preprint arXiv:2305.18262,",
            "year": 2023
        },
        {
            "authors": [
                "Shujian Zhang",
                "Chengyue Gong",
                "Xingchao Liu",
                "Pengcheng He",
                "Weizhu Chen",
                "Mingyuan Zhou"
            ],
            "title": "ALLSH: Active learning guided by local sensitivity and hardness",
            "venue": "In Findings of the Association for Computational Linguistics: NAACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Yivan Zhang",
                "Gang Niu",
                "Masashi Sugiyama"
            ],
            "title": "Learning noise transition matrix from only noisy labels via total variation regularization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Zhaowei Zhu",
                "Jialu Wang",
                "Yang Liu"
            ],
            "title": "Beyond images: Label noise transition matrix estimation for tasks with lower-quality features",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nData quality, an important ML problem. Data quality is crucial to the performance and robustness of machine learning (ML) models (Jain et al., 2020; Gupta et al., 2021; Renggli et al., 2021; Sambasivan et al., 2021; Li et al., 2021). Unfortunately, challenges arise in real-world data that make samples \u201chard\u201d for ML models to learn from effectively, including but not limited to mislabeling, outliers, and insufficient coverage (Chen et al., 2021; Li et al., 2021). These \u201chard\u201d samples or data points can significantly hamper the performance of ML models, creating a barrier to ML adoption in practical applications (Bedi et al., 2019; West, 2020). For instance, a model trained on mislabeled samples can lead to inaccurate predictions (Krishnan et al., 2016; Gupta et al., 2021). Outliers can bias the model to learn suboptimal decision boundaries (Liu et al., 2022; Eduardo et al., 2022; Krishnan et al., 2016), harming model performance. Long tails of samples can result in poor model performance for these cases (Feldman, 2020; Hooker et al., 2019; Hooker, 2021; Agarwal et al., 2022). Consequently, \u201chard\u201d samples can pose serious challenges for training and the performance of ML models, making it crucial to identify these samples. This is especially important in where manually identifying \u201chard\u201d samples is expensive, impractical, or time-consuming given the scale.\nCharacterizing hardness, a growing area. Recent interest in data-centric AI, which aims to ensure and improve data quality, has led to the development of systematic methods to characterize the data used to train ML models (Liang et al., 2022; Seedat et al., 2023b; 2022b). Data characterization typically assigns scores to each sample based on its learnability and utility for an ML task, thereby facilitating the identification of \u201chard\u201d samples. We collectively refer to methods that perform the characterization as Hardness Characterization Methods (HCMs).\nAfter samples are characterized, how and for what purpose they are used can differ. For example: (1) curating datasets via sample selection to improve model performance (Maini et al., 2022; Swayamdipta et al., 2020; Seedat et al., 2022a; Northcutt et al., 2021a; Pleiss et al., 2020; Agarwal\net al., 2022; Seedat et al., 2023a), (2) sculpting the dataset to reduce computational requirements while maintaining performance (Toneva et al., 2019; Paul et al., 2021; Sorscher et al., 2022; Mindermann et al., 2022), (3) guiding acquisition of additional samples (Zhang et al., 2022), or (4) understanding learning behavior from a theoretical perspective (Baldock et al., 2021; Shwartz et al., 2022; Jiang et al., 2021).\nChallenges in definition and evaluation. A fundamental and overlooked aspect is that while different HCMs tackle the issue of \u201chardness\u201d, it remains a vague and ill-defined term in the literature. The lack of a clear definition of hardness types has led HCMs, seemingly tackling the same problem, to unintentionally target and assess different aspects of hardness (Table 1). The lack of clarity is further exacerbated by: (1) qualitative evaluation: a significant focus on post hoc qualitative assessment and downstream improvement, instead of the fundamental hardness identification task, and (2) narrow and unrepresentative scope: even when quantitative evaluation has been performed, it has typically focused on a single hardness type, neglecting different manifestations of hardness. The lack of comprehensive and quantitative evaluation means we do not know how different HCMs perform on different hardness types and whether they indeed identify the correct samples of interest.\nCan we define sample hardness manifestations and then comprehensively and systematically evaluate the capabilities of different HCMs to correctly detect the hard samples?\nUnified taxonomy and benchmarking framework. To answer this question, we begin by defining a taxonomy of hardness types across three broad categories: (a) Mislabeling, (b) OoD/Outlier, (c) Atypical. We then introduce the Hardness-Characterization Analysis Toolkit (H-CAT), which, to the best of our knowledge, is the first unified data characterization benchmarking framework focused on hardness. Using H-CAT, we comprehensively and quantitatively benchmark 13 state-of-the-art HCMs across various hardness types. In doing so, we address recent calls for more rigorous benchmarking (Guyon, 2022) and understanding of existing ML methods (Lipton & Steinhardt, 2019; Snoek et al., 2018). We make the following contributions:\nContributions: 1\u20dd Hardness taxonomy: we formalize a systematic taxonomy of sample-level hardness types, addressing the current literature\u2019s ad hoc and narrow scope. By defining the different dimensions of hardness, our taxonomy paves the way for a more rigorous evaluation of HCMs. 2\u20dd Benchmarking framework: we propose H-CAT, which is both (i) a benchmarking standard to evaluate the strengths of different HCMs across the hardness taxonomy and (ii) a unified software tool integrating 13 different HCMs. With extensibility in mind, H-CAT can easily incorporate new HCMs, hardness types, and datasets, thus enhancing its utility for both researchers and practitioners. 3\u20dd Systematic & Quantitative HCM evaluation: we use H-CAT to comprehensively benchmark and evaluate 13 different HCMs across 8 different hardness types, comprising over 14K experimental setups. 4\u20dd Insights: our benchmark provides novel insights into the capabilities of different HCMs when dealing with different hardness types and offers practical usage tips for researchers and practitioners. The variability in HCM performance across hardness types underscores the importance of multi-dimensional evaluation, exposing gaps and opportunities in current HCMs. We hope H-CAT will promote rigorous HCM evaluations and inspire new advances in data-centric AI.\n2 HARDNESS CHARACTERIZATION AND TAXONOMY We now outline the hardness characterization problem and formalize a hardness taxonomy for HCMs.\nLearning problem. Consider the typical supervised learning setting, with X and Y input and output spaces, respectively. We assume a k-class classification problem, i.e. Y = [k], where [k] = {1, . . . , k}, with a training dataset D = {(xi, yi) | i \u2208 [N ]} with N \u2208 N+ samples, where xi \u2208 X and yi \u2208 Y . The goal is to learn a model f\u03b8 : X \u2192 Y , parameterized by \u03b8 \u2208 \u0398. Hardness problem. To understand the intricacies of data hardness, we start by providing a broad definition of the hardness problem common to all HCMs. As a starting point, let us frame the hardness problem in general terms: specifically, some samples or data points are easier for a model to learn from, whilst others may either be harder to learn from or harm the model\u2019s performance.\nFormally, this assumes that the training dataset can be decomposed as D = De \u222a Dh, where De are easy samples and Dh are hard samples. We denote the corresponding joint distributions as PXY , PeXY and PhXY . Going beyond this general definition to different manifestations of hardness requires a more rigorous characterization. However, what constitutes a \u201chard\u201d sample has not been rigorously\ndefined in the literature. To address this gap, we first formalize a taxonomy of hardness in Sec. 2.1, providing a systematic and formal definition of different types of sample-level hardness.\nData characterization. The goal of data characterization is to assign a scalar \u201chardness\u201d score to each sample inD, thereby allowing us to order samples inD according to their scores. Typically, a selector function applies a threshold \u03c4 and then assigns a hardness label g \u2208G, where G = {Easy,Hard}, to each sample (xi, yi), i.e. assign samples in D to either De or Dh. We refer to these methods as Hardness Characterization Methods (HCM), having the following input-output paradigm, where methods differ based on their scoring mechanism:\n\u25a0 Inputs: (i) Dataset D = {(xi, yi)} drawn from both PXY . (ii) Learning algorithm f\u03b8. \u25a0 Outputs: Assign a score si to sample (xi, yi). Apply threshold \u03c4 to assign a hardness label g \u2208G, where G = {Easy,Hard}, which is then used to partition D = De \u222a Dh. We group the HCMs into broad classes (see Table 1) determined based on the core metric or approach each method uses to characterize example hardness, namely (1) Learning dynamics-based: relying on metrics computed during the training process itself to characterize example hardness; (2) Distancebased: using the distance or similarity of examples in an embedding space; (3) Statistical-based: using statistical metrics computed over the data to characterize example hardness.\n2.1 TAXONOMY OF HARDNESS\nHardness taxonomy formalism. The term \u201chardness\u201d is broad and can manifest in various ways, as demonstrated by the different types of hardness previously examined (see Table 1). Therefore, we must first formalize a taxonomy representative of the different types of hardness we might expect in practice. Each type of hardness is characterized by a latent or unseen hardness perturbation function h that creates hard samples Dh from samples in D. We denote hardness perturbations on X as X \u2192 X\u2217 and Y as Y \u2192 Y \u2217. The effect of each hardness perturbation is explained in terms of the relationship between the joint probability distributions P eXY (x, y) and P h XY (x, y).\nThe taxonomy deals with three broad types (and corresponding subtypes) of hardness: (1) Mislabeling, (2) OoD/Outlier, and (3) Atypical, as illustrated in Figure 1. We anchor the different hardness manifestations with respect to relevant literature for each subtype. We define the various types next.\nMislabeling: Samples where the true label is replaced with an incorrect label, such that sample (x, y)\u2192 (x, y\u2217). The main distinction between easy and hard samples lies in the label space, leading to different conditional probability distributions: P eY |X(y|x) \u0338= P h Y |X(y|x). Note, the marginal probability distributions are the same P eX(x) = P h X(x).\nWe consider three subtypes of mislabeling: (i) Uniform, (ii) Asymmetric, and (iii) Instance. The HCM literature primarily focuses on evaluation with a uniform noise model (Paul et al., 2021; Swayamdipta et al., 2020; Pleiss et al., 2020; Toneva et al., 2019; Maini et al., 2022; Jiang et al., 2021; Mindermann et al., 2022; Baldock et al., 2021), with equal probability of mislabeling across classes. However, in reality, mislabeling is often asymmetric, where mislabeling is label-dependent (Northcutt et al., 2021a; Sukhbaatar et al., 2015) or instance-specific, where certain mislabeling is more likely given the sample (Jia et al., 2022; Han et al., 2020; Hendrycks et al., 2018; Song et al., 2022). For example, mislabeling an image of a car as a truck is more likely than mislabeling a car as a dog.\nFormally, the subtypes differ in their noise models to perturb the true labels, defined by the probabilities P (Y \u2217 = j|Y = i), where i and j are the true and perturbed labels, respectively. \u2022 Uniform: P (Y \u2217 = j|Y = i) = 1/k\u22121 \u2200i, j \u2208 [k], i \u0338= j \u2022 Asymmetric: P (Y \u2217 = j|Y = i) = pij \u2200i, j \u2208 [k], i \u0338= j } Instance Independent\n\u2022 Instance: P (Y \u2217 = j|Y = i,X = x) = pij(x) \u2200i, j \u2208 [k], i \u0338= j } Instance Dependent\nOoD/Outlier: Samples where the covariates undergo a transformation/shift, such that sample (x, y)\u2192 (x\u2217, y). The distinction between easy and hard samples lies in the feature space, leading to different marginal probability distributions P eX(x) \u0338= PhX(x). Further, for any subset S within the support of PX , PhX(S) = 0. Note, conditional probability distributions remain consistent where P eY |X(y|x) = P h Y |X(y|x). We consider two subtypes differing in degree of shift, for clarity denoted by an arbitrary distance measure between distributions dist(\u00b7, \u00b7). \u2022 Near OoD (Anirudh & Thiagarajan, 2023; Mu & Gilmer, 2019; Hendrycks & Dietterich, 2018;\nSun et al., 2023b; Yang et al., 2023; Tian et al., 2021): samples which have their features transformed or shifted such that they remain proximal to the original samples inD, e.g, introducing noise, pixelating an image, or adding subtle texture changes. In this case, dist(PhX , PX) is positive but relatively small, indicating the nearness of the perturbed distribution to the original. We can represent this bounded distance such that 0 < dist(PhX , PX) \u2264 \u03f5, with \u03f5 > 0.\n\u2022 Far OoD (Mukhoti et al., 2022; Winkens et al., 2020; Graham et al., 2022; Yang et al., 2023): samples which are distinct and likely unrelated to samples in D, often not belonging to the same data-generating process. This could be by sampling from a different dataset or a perturbation s.t. dist(PhX , PX) is significantly large, i.e. dist(P h X , PX)\u226b \u03f5. For example, for a dataset of digits,\nimages of dogs or cats are distinctly different and unrelated. They are not just rare occurrences but images of dogs or cats represent a different data generation process compared to the digits.\nAtypical: Samples that, although rare, are still valid instances deviating from common patterns (Yuksekgonul et al., 2023). Atypical samples are inherently part of the primary data distribution, but located in its long tail or less frequent regions (Feldman, 2020; Hooker et al., 2019; Hooker, 2021; Agarwal et al., 2022). The distinction between easy and hard samples leads to different marginal probability distributions P eX(x) \u0338= PhX(x), where PhX(x) is very small, highlighting their rarity or infrequency. Here, for any subset S within the support of PX , PhX(S) > 0. This signifies that the long-tail samples, though rarer in occurrence, are still within the bounds of the primary data-generating process. For example, these could be images with atypical variations or vantage points compared to the standard pattern (Agarwal et al., 2022).\nContrasting OoD/Outlier and Atypical. Both have different marginal probability distributions P eX(x) \u0338= PhX(x). The difference is that OoD/Outliers come from a shifted or completely different distribution than the original data, falling outside the support of PX . In contrast, atypical samples are rare samples from the tails and could naturally arise, falling within the support of PX .\nFrom a practitioner\u2019s standpoint, these distinctions can dictate different courses of action. OoD/outliers represent likely anomalies or errors that we should detect for potential sculpting or filtering from the dataset. In contrast, atypical samples are rare cases deviating from the \u201cnorm\u201d, with no or limited similar examples. They are still valid points and should not necessarily be discarded; in fact, there might be a need to gather more of such samples. The goal of surfacing atypical samples is both for dataset auditing and understanding edge cases. We provide additional example images to provide further intuition of the difference between OoD and Atypical in Appendix A, Fig. 3.\n2.2 GAPS AND LIMITATIONS OF CURRENT HCMS WITHIN OUR TAXONOMY\nTo provide a more comprehensive perspective, we critically analyze various HCMs within our proposed taxonomy. Alarmingly, we find inconsistent and diverse definitions of \u201chardness\u201d, even within the same class of methods (refer to Table 1). Such discrepancies, as evidenced by their experimental evaluations, point to a challenge: HCMs, despite appearing to measure similar constructs, in fact, evaluate different \u201chardness\u201d dimensions. Table 1 highlights two critical deficiencies discussed below, underscoring the urgent need for a systematic evaluation framework that accurately captures the scope and applicability of each HCM across different hardness types.\nIssue 1: Qualitative or indirect measures. Many HCMs limit their evaluation to (i) qualitative analyses: merely showcasing flagged samples, or (ii) indirect measures: for example, showing downstream performance improved when removing flagged samples without directly quantifying what the HCM captures. This overlooks the necessity for an objective, quantitative evaluation.\nIssue 2: Narrow and unrepresentative scope. HCMs that conduct quantitative evaluation focus on a single hardness type and often target the simplest manifestation. An example of this shortfall is seen in the handling of mislabeling. Many HCMs only focus on uniform mislabeling, thereby failing to account for the more realistic and complex scenarios of asymmetric or instance-wise mislabeling. Beyond this, many HCMs only test on mislabeling, overlooking other types of hardness.\nThese limitations emphasize the value of our fine-grained taxonomy in categorizing and evaluating various hardness types. By laying a solid foundation for the systematic evaluation and comparison of HCMs, our taxonomy enables the design and selection of HCMs that are tailored to specific hardness challenges, thereby promoting the development of more robust ML systems.\n3 H-CAT: A BENCHMARKING FRAMEWORK FOR HCMS To address the aforementioned limitations and facilitate benchmarking of HCMs, we propose the Hardness Characterization Analysis Toolkit (H-CAT). H-CAT serves two purposes: (1) empirical benchmarking standard: supporting comprehensive and quantitative benchmarking of HCMs on different hardness types within the taxonomy across multiple data modalities and (2) software toolkit: H-CAT unifies multiple HCMs under a single unified interface for easy usage by practitioners.\nH-CAT Design. H-CAT has four core modules as described below, which are called sequentially (Figure 2). The framework follows widely adopted objected-oriented paradigms with fit-predict interfaces (here, update-score). The workflow is simple with single-chain API calls. The stepwise composability aims to facilitate easy benchmarking and allows H-CAT to be used outside of benchmarking as a data characterization tool by practitioners. \u2022 Dataloader module: loads a variety of data types for ease of use, including Torch datasets, NumPy\narrays, and Pandas DataFrames, allowing users to easily use H-CAT with their chosen datasets. \u2022 Hardness module: generates controllable hardness for different hardness types in the taxonomy. \u2022 HCM module: provides a unified HCM interface with 13 HCMs implemented. It wraps the trainer\nmodule which is a conventional PyTorch training loop. \u2022 Evaluator module: computes the HCM evaluation metrics to correctly identify the ground-truth\nhard samples using the scores provided by each HCM. User-specified metrics can easily be included by operating on the raw HCM scores.\nExtensibility. H-CAT is easily extendable to include new HCMs, hardness types, or datasets by defining a simple wrapper class. For details and step-by-step code examples, refer to Appendix C.\n4 BENCHMARKING FRAMEWORK SETUP\nWe now describe three key aspects of the benchmarking setup \u2014 the implementation of the Hardness Module, HCM Module and Evaluator Module.\n4.1 HARDNESS MODULE\nWe describe the implementation of our hardness perturbations h. We focus on image data here, as most HCMs (10/13) have been developed for this modality. However, we discuss the corresponding hardness perturbations for tabular data in the Appendix.\nMislabeling: (i) Uniform: random mislabeling, drawn uniformly from all possible class labels. (ii) Asymmetric: random mislabeling, drawn asymmetrically via a Dirichlet distribution (Zhang et al., 2021; Bae et al., 2022; Zhu et al., 2022). We denote a special case of asymmetric applicable to ordinal labels, namely adjacent. Here, the mislabeling is to the nearest numerical class, e.g. an MNIST digit 3 mislabeled as 2 or 4. (iii) Instance: mislabeling probability is conditioned on an instance (reflecting human mislabeling). This can often be determined by domain/user knowledge, e.g. for MNIST, 1 is likely mislabeled as 7; for CIFAR-10, an automobile could be mislabeled as a truck.\nOoD/Outlier: (i) Near OoD: pertubed data is different but related. Covariate Shift via Gaussian noise as performed in MNIST-C (Mu & Gilmer, 2019) or Domain Shift: image texture from the original photographic image is changed via edge detection and smoothing (Median filter). (ii) Far OoD: perturbed data is distinctly different and unrelated. We replace a subset of data with unrelated data, e.g. for MNIST replace with CIFAR-10 images and vice versa.\nAtypical: (i) Shift: translate and shift the image, causing portions to be cut off in an atypical manner. (ii) Zoom: create an atypical perspective by magnifying (X2) features usually seen at a smaller scale. 4.2 HCM MODULE\nWe include 13 widely used HCMs applicable to supervised classification under a unified interface. The HCMs span the range of HCM classes, at least one per class from Table 1: \u25a0 Learning-based1 (Uncertainty): Data Maps (Swayamdipta et al., 2020) and Data-IQ (Seedat et al., 2022a); \u25a0 Learningbased (Loss): Sample-Loss (Xia et al., 2021; Arriaga et al., 2023) ; \u25a0 Learning-based (Margin): Area-under-the-margin (AUM) (Pleiss et al., 2020); \u25a0 Learning-based (Gradient): GraNd (Paul et al., 2021) and VoG (Agarwal et al., 2022); \u25a0 Learning-based (Statistics): EL2N (Paul et al., 2021) and Noise detector (Jia et al., 2022), \u25a0 Learning-based (Forgetting): Forgetting scores (Toneva et al., 2019); \u25a0 Statistical measures: Cleanlab (Northcutt et al., 2021a), ALLSH (Zhang et al., 2022), Agreement (Carlini et al., 2019); \u25a0 Distance-based: Prototypicality (Sorscher et al., 2022).\nSpecifically, we focus on HCMs that plug into the training loop and do not: (i) alter training, (ii) require repeated training, (iii) need additional datasets beyond the training set, or (iv) require training additional models. Consequently, we exclude the following: RHO-Loss (Mindermann et al., 2022) requires an additional irreducible loss model, SSFT (Maini et al., 2022) requires fine-tuning on a validation dataset, PVI (Ethayarajh et al., 2022) requires training a Null model. We also do not consider Data Shapley (Ghorbani & Zou, 2019) and variants (e.g. Beta Shapley (Kwon & Zou, 2022)), which have been shown to be computationally infeasible with numerical instabilities for higher dimensional data such as MNIST and CIFAR-10 with > 1000 samples (Wang & Jia, 2023). 4.3 EVALUATOR MODULE\nWe directly assess the HCM\u2019s capability to detect the hard samples. Recall that HCMs assign a score s to each sample (x, y) and then apply a threshold \u03c4 to assign samples a group g \u2208G, where G = {Easy,Hard}. Many HCMs do not explicitly state how to define \u03c4 ; hence to account for this we compute two widely used metrics: AUPRC (Area Under Precision-Recall Curve) and AUROC (Area Under Receiver Operating Curve) \u2014 for hard sample detection performance, which we denote D-AUPRC and D-AUROC 2. User-specified metrics are easily computed on raw HCM scores.\n1Learning-based generally refers to learning/training dynamics based HCMs 2to distinguish them from the typical downstream performance metrics.\n5 COMPREHENSIVE EVALUATION OF HCMS USING H-CAT\nWe evaluate 13 different HCMs (spanning a range of techniques) across 8 distinct hardness types. To the best of our knowledge, this represents the first comprehensive HCM evaluation, encompassing over 14K experimental setups (specific combination of HCM, hardness type, perturbation proportion, dataset, model, and seed).\nWe primarily focus on image datasets, as this is the modality for which the majority of the HCMs (10/13) have been developed. We use the MNIST (LeCun et al., 2010) and CIFAR-10 (Krizhevsky et al.) datasets, as their use is well-established in the HCM literature (Paul et al., 2021; Swayamdipta et al., 2020; Pleiss et al., 2020; Toneva et al., 2019; Maini et al., 2022; Jiang et al., 2021; Mindermann et al., 2022; Baldock et al., 2021). Importantly, they are realistic images yet contain almost no/little mislabeling (<0.5%) (Northcutt et al., 2021b). This contrasts other common image datasets like ImageNet that contain significant mislabeling (over 5%), hence we cannot perform controlled experiments. Furthermore, to show generalizability across modalities, we also evaluate on tabular datasets \u201cCovertype\u201d and \u201cDiabetes130US\u201d benchmark datasets (Grinsztajn et al., 2022) from OpenML (Vanschoren et al., 2014) (see Appendix D.6).\nTo assess HCM sensitivity to the backbone model, we use two different models for our image experiments with different degrees of parameterization, LeNet and ResNet-18. All experiments are repeated with 3 random seeds and for varying proportions p of hard samples.\nWe present aggregated results in Figs. 4-7, with more granular results in Appendix D \u2014 along with additional experiments. The main paper shows results for 6 out of 8 hardness types. We include results for other sub-types not covered in the main paper in Appendix D including: Domain shift (a type of Near OoD), Zoom shift (a type of Atypical), and Adjacent (a special case of Asymmetric mislabeling), offering similar conclusions. We investigate three aspects of HCMs (A-C), distilling the results into benchmarking takeaways and practical tips.\nTakeaway A1: Comprehensive testing is vital. Many HCMs are assessed on a single hardness type. The reality is hardness manifests in many ways. We show HCM performance varies across hardness types and across proportions of hardness, with some more challenging than others, e.g. instance is harder than uniform. This result shows the critical need for comprehensive HCM evaluation.\nTakeaway A2: Hardness types vary in difficulty. We find that different types of hardness are easier or harder to characterize. For instance, uniform mislabeling or Far-OoD are much easier than data-specific hardness like instance and Atypical. Given the performance differences of HCMs on different hardness types, it becomes important to understand the hardness type expected in practice.\nTakeaway A3: Learning dynamics-based methods with respect to output confidence are effective general-purpose HCMs. In selecting a general-purpose HCM, we find that HCMs that characterize samples using learning dynamics on the confidence \u2014 uncertainty-based methods, which use probabilities (DataMaps, Data-IQ) or logits (AUM), are the best performing in terms of AUPRC across the board.\nTakeaway A4: HCMs typically used for computational efficiency are surprisingly uncompetitive. We find that HCMs that leverage gradient changes (e.g. GraNd), typically used for selection to improve computational efficiency, fare well at low p. However, at higher p, they become notably less competitive compared to simpler and computationally cheaper methods.\nPractical Tip A1: HCMs should only be used when hardness proportions are low. In general, different HCMs have significantly reduced performance at higher proportions of hardness. This is expected as we get closer to 0.5 since it\u2019s harder to identify a clear difference between samples.\nB. Rankings and Statistical Significance. Compare the ranking of methods, as well as assess statistical significance of performance differences using critical difference diagrams (CD diagram) (Dem\u0161ar, 2006) based on the Siegel-Friedman method (p \u2264 0.05) \u2014 see Figs. 5 and 6.\nTakeaway B1: Individual HCMs within a broad \u201cclass\u201d of methods are NOT statistically different. We find from the critical difference diagrams that methods falling into the same class of characterization are not statistically significant from one another (based on the black connected lines), despite the minor performance differences between them. Hence, practitioners should select an HCM within the broad HCM class most suitable for the application.\nPractical Tip B1: Selecting an HCM based on the hardness is useful. We find that confidence is a good general-purpose tool if one does not know the type of hardness. However, if one knows the hardness, one can better select the HCM. For example, Prototypicality, as expected, is very strong on instance hardness as we are able to match samples via similarity of classes in embedding space.\nPractical Tip B2: Divergence and distance-based methods are suitable primarily for distributional changes. Divergence and distance-based methods such as ALLSH and Prototypicality should primarily be used if the hardness is with respect to a shift of the data itself rather than mislabeling.\nC. Stability/Consistency. The rank ordering of samples is important in data characterization (Maini et al., 2022; Wang & Jia, 2023; Seedat et al., 2022a). Hence, we desire HCM scores to be stable to ensure consistent insights. As is standard (Maini et al., 2022; Seedat et al., 2022a), we compute the Spearman rank correlation across multiple runs \u2014 see Fig. 7. We also assess HCM stability/consistency across backbone models and parameterizations in Appendix D.\nTakeaway C1: Learning dynamics-based HCMs using output metrics or distance-based HCMs are most stable and consistent. We find across all hardness types that the Spearman rank correlation is highest for HCMs that use learning dynamics on the outputs or use distance measures \u2014 specifically Uncertainty (Probabilities), Margins (Logits), Loss, or Prototypicality. The low correlation for other HCMs highlights sensitivity to the run itself when characterizing data, which would lead to inconsistent ordering.\nTakeaway C2: Insights consistent across backbone models & parameterizations. As shown in Appendix D, we find similar results (as above) for the Spearman rank correlation of HCM scores across different backbone models and parameterizations. As the scores are consistent, this indicates that the findings and insights computed on those scores will also be consistent, i.e. those HCMs which were the most stable and consistent remain the most stable and consistent.\nPractical Tip C1: Select a stable HCM. Certain HCMs are more sensitive to randomness. Hence, it is advised to select stable and consistent HCMs (higher Spearman correlation) to avoid such effects.\n6 DISCUSSION We introduce H-CAT, a comprehensive benchmarking framework for hardness characterization in data-centric AI. We analyzed 13 HCMs spanning a range of techniques for a variety of setups - over 14K. We hope our framework and insights addressing calls for rigorous benchmarking (Guyon, 2022) and understanding of existing ML methods (Lipton & Steinhardt, 2019; Snoek et al., 2018) will spur advancements in data-centric AI and that H-CAT helps practitioners better evaluate and use HCMs.\nLimitations & Future Work. No benchmark can exhaustively test all the possible hardness manifestations and this work is no different. However, we cover multiple instances from the three fundamental types of hardness, which is significantly broader than any prior work. Building on this work, future research could investigate cases where multiple types of \u201chardness\u201d manifest simultaneously (e.g. Near-OoD and mislabeling together) or where hardness is continuous rather than binary. To spur this, we provide an example of simultaneous hardness in Appendix D. From a usage perspective, future work could also look into the best way hardness scores could be used to guide better model training (e.g. data curriculum). Finally, we highlight that HCMs, and by extension H-CAT, cannot tell you which hardness type exists in a dataset; rather, H-CAT serves to benchmark the capabilities of HCMs or as a unified HCM interface.\nACKNOWLEDGEMENTS\nThe authors would like to thank Nicolas Huynh, Alicia Curth and the anonymous reviewers for their helpful comments and discussions on earlier drafts of this paper and Robert Davis for discussions on the code. NS and FI gratefully acknowledge funding from the Cystic Fibrosis Trust and NSF grant (1722516), respectively. This work was supported by Azure sponsorship credits granted by Microsoft\u2019s AI for Good Research Lab.\nETHICS AND REPRODUCIBILITY STATEMENTS\nEthics Statement. HCMs that accurately identify hard samples can help make models more robust and reliable. This paper highlights the importance of rigorously evaluating HCMs to better understand their capabilities and guide better usage. This paper aims to enable the community to conduct a more systematic hardness characterization through the proposed taxonomy and benchmarking framework.\nReproducibility Statement. We include implementation details about our benchmark in Sec. 3 and 4, as well as in Appendix B. The code for the H-CAT framework can be found at https:// github.com/seedatnabeel/H-CAT or https://github.com/vanderschaarlab/ H-CAT. Step-by-step code examples can be found in the repository and in Appendix C along with a guide on how to extend H-CAT to new HCMs, hardness types, and datasets.\nREFERENCES Chirag Agarwal, Daniel D\u2019souza, and Sara Hooker. Estimating example difficulty using variance\nof gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10368\u201310378, 2022.\nRushil Anirudh and Jayaraman J Thiagarajan. Out of distribution detection via neural network anchoring. In Asian Conference on Machine Learning, pp. 32\u201347. PMLR, 2023.\nOctavio Arriaga, Sebastian Palacio, and Matias Valdenegro-Toro. Difficulty estimation with action scores for computer vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 245\u2013253, 2023.\nHeeSun Bae, Seungjae Shin, Byeonghu Na, JoonHo Jang, Kyungwoo Song, and Il-Chul Moon. From noisy prediction to true label: Noisy prediction calibration via generative model. In International Conference on Machine Learning, pp. 1277\u20131297. PMLR, 2022.\nRobert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of example difficulty. Advances in Neural Information Processing Systems, 34:10876\u201310889, 2021.\nChris Bedi, Edward Cone, Brian Foster, Riva Froymovich, Colleen Haikes, Michael Hubbard, Brian Klapac, Alan Marks, Debbie O\u2019Brien, Kathy O\u2019Connell, Gaby Pacini-Pinto, Kevin Palmer, Rob Pickering, Chris Pope, Dan Rogers, Julia Smith, and Dave Wright. The Global CIO Point of View The New Agenda for Transformative Leadership: Reimagine Business for Machine Learning. Oxford Econometric & Service Now, 2019.\nNicholas Carlini, Ulfar Erlingsson, and Nicolas Papernot. Distribution density, tails, and outliers in machine learning: Metrics and applications. arXiv preprint arXiv:1910.13427, 2019.\nHaihua Chen, Jiangping Chen, and Junhua Ding. Data evaluation and enhancement for quality improvement of machine learning. IEEE Transactions on Reliability, 70(2):831\u2013847, 2021.\nJanez Dem\u0161ar. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine learning research, 7:1\u201330, 2006.\nSimao Eduardo, Kai Xu, Alfredo Nazabal, and Charles Sutton. Repairing systematic outliers by learning clean subspaces in VAEs. arXiv preprint arXiv:2207.08050, 2022.\nKawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In International Conference on Machine Learning, pp. 5988\u20136008. PMLR, 2022.\nVitaly Feldman. Does learning require memorization? A short tale about a long tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 954\u2013959, 2020.\nAmirata Ghorbani and James Zou. Data Shapley: Equitable valuation of data for machine learning. In International Conference on Machine Learning, pp. 2242\u20132251. PMLR, 2019.\nMark S Graham, Petru-Daniel Tudosiu, Paul Wright, Walter Hugo Lopez Pinaya, U Jean-Marie, Yee H Mah, James T Teo, Rolf Jager, David Werring, Parashkev Nachev, et al. Transformer-based out-of-distribution detection for clinically safe segmentation. In International Conference on Medical Imaging with Deep Learning, pp. 457\u2013476. PMLR, 2022.\nL\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems, 35: 507\u2013520, 2022.\nNitin Gupta, Hima Patel, Shazia Afzal, Naveen Panwar, Ruhi Sharma Mittal, Shanmukha Guttula, Abhinav Jain, Lokesh Nagalapatti, Sameep Mehta, Sandeep Hans, et al. Data quality toolkit: Automatic assessment of data quality and remediation for machine learning datasets. arXiv preprint arXiv:2108.05935, 2021.\nIsabelle Guyon. The data-centric era: How ML is becoming an experimental science. Keynote: Advances in Neural Information Processing Systems, 2022.\nBo Han, Quanming Yao, Tongliang Liu, Gang Niu, Ivor W Tsang, James T Kwok, and Masashi Sugiyama. A survey of label-noise representation learning: Past, present and future. arXiv preprint arXiv:2011.04406, 2020.\nLasse Hansen, Nabeel Seedat, Mihaela van der Schaar, and Andrija Petrovic. Reimagining synthetic tabular data generation through data-centric AI: A comprehensive benchmark. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2018.\nDan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. Advances in neural information processing systems, 31, 2018.\nElisabeth RM Heremans, Nabeel Seedat, Bertien Buyse, Dries Testelmans, Mihaela van der Schaar, and Maarten De Vos. U-PASS: An uncertainty-guided deep learning pipeline for automated sleep staging. Computers in Biology and Medicine, pp. 108205, 2024.\nSara Hooker. Moving beyond \u201calgorithmic bias is a data problem\u201d. Patterns, 2(4):100241, 2021.\nSara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do compressed deep neural networks forget? arXiv preprint arXiv:1911.05248, 2019.\nAbhinav Jain, Hima Patel, Lokesh Nagalapatti, Nitin Gupta, Sameep Mehta, Shanmukha Guttula, Shashank Mujumdar, Shazia Afzal, Ruhi Sharma Mittal, and Vitobha Munigala. Overview and importance of data quality for machine learning tasks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3561\u20133562, 2020.\nMohammad Hossein Jarrahi, Ali Memariani, and Shion Guha. The principles of data-centric AI (DCAI). arXiv preprint arXiv:2211.14611, 2022.\nQingrui Jia, Xuhong Li, Lei Yu, Jiang Bian, Penghao Zhao, Shupeng Li, Haoyi Xiong, and Dejing Dou. Learning from training dynamics: Identifying mislabeled data beyond manually designed features. arXiv preprint arXiv:2212.09321, 2022.\nZiheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. In International Conference on Machine Learning, pp. 5034\u20135044. PMLR, 2021.\nSanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J Franklin, and Ken Goldberg. ActiveClean: Interactive data cleaning for statistical modeling. Proceedings of the VLDB Endowment, 9(12): 948\u2013959, 2016.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced Research). URL http://www.cs.toronto.edu/~kriz/cifar.html.\nYongchan Kwon and James Zou. Beta Shapley: A unified and noise-reduced data valuation framework for machine learning. In International Conference on Artificial Intelligence and Statistics, pp. 8780\u20138802. PMLR, 2022.\nYann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.\nPeng Li, Xi Rao, Jennifer Blase, Yue Zhang, Xu Chu, and Ce Zhang. CleanML: A study for evaluating the impact of data cleaning on ML classification tasks. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pp. 13\u201324. IEEE, 2021.\nWeixin Liang, Girmaw Abebe Tadesse, Daniel Ho, L Fei-Fei, Matei Zaharia, Ce Zhang, and James Zou. Advances, challenges and opportunities in creating data for trustworthy AI. Nature Machine Intelligence, 4(8):669\u2013677, 2022.\nZachary C Lipton and Jacob Steinhardt. Research for practice: Troubling trends in machine-learning scholarship. Communications of the ACM, 62(6):45\u201353, 2019.\nZifan Liu, Zhechun Zhou, and Theodoros Rekatsinas. Picket: Guarding against corrupted data in tabular data during learning and inference. The VLDB Journal, pp. 1\u201329, 2022.\nPratyush Maini, Saurabh Garg, Zachary Chase Lipton, and J Zico Kolter. Characterizing datapoints via second-split forgetting. In Advances in Neural Information Processing Systems, 2022.\nAnna Majkowska, Sid Mittal, David F Steiner, Joshua J Reicher, Scott Mayer McKinney, Gavin E Duggan, Krish Eswaran, Po-Hsuan Cameron Chen, Yun Liu, Sreenivasa Raju Kalidindi, et al. Chest radiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population-adjusted evaluation. Radiology, 294(2):421\u2013431, 2020.\nS\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pp. 15630\u201315649. PMLR, 2022.\nNorman Mu and Justin Gilmer. MNIST-C: A robustness benchmark for computer vision. arXiv preprint arXiv:1906.02337, 2019.\nJishnu Mukhoti, Tsung-Yu Lin, Bor-Chun Chen, Ashish Shah, Philip HS Torr, Puneet K Dokania, and Ser-Nam Lim. Raising the bar on the evaluation of out-of-distribution detection. arXiv preprint arXiv:2209.11960, 2022.\nAfshan Nabi, Berke Dilekoglu, Ogun Adebali, and Oznur Tastan. Discovering misannotated lncrnas using deep learning training dynamics. Bioinformatics, 39(1):btac821, 2023.\nAndrew Ng, Lora Aroyo, Cody Coleman, Greg Diamos, Vijay Janapa Reddi, Joaquin Vanschoren, Carole-Jean Wu, and Sharon Zhou. NeurIPS data-centric AI workshop, 2021. URL https: //datacentricai.org/.\nCurtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. Journal of Artificial Intelligence Research, 70:1373\u20131411, 2021a.\nCurtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning benchmarks. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021b.\nMansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. Advances in Neural Information Processing Systems, 34: 20596\u201320607, 2021.\nGeoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kilian Q Weinberger. Identifying mislabeled data using the area under the margin ranking. Advances in Neural Information Processing Systems, 33: 17044\u201317056, 2020.\nCedric Renggli, Luka Rimanic, Nezihe Merve G\u00fcrel, Bojan Karlas, Wentao Wu, and Ce Zhang. A data quality-driven view of MLOps. IEEE Data Engineering Bulletin, 2021.\nNithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. \u201cEveryone wants to do the model work, not the data work\u201d: Data cascades in high-stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1\u201315, 2021.\nNabeel Seedat, Jonathan Crabb\u00e9, Ioana Bica, and Mihaela van der Schaar. Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data. In Advances in Neural Information Processing Systems, 2022a.\nNabeel Seedat, Jonathan Crabb\u00e9, and Mihaela van der Schaar. Data-SUITE: Data-centric identification of in-distribution incongruous examples. In International Conference on Machine Learning, pp. 19467\u201319496. PMLR, 2022b.\nNabeel Seedat, Jonathan Crabb\u00e9, Zhaozhi Qian, and Mihaela van der Schaar. TRIAGE: Characterizing and auditing training data for improved regression. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.\nNabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Navigating data-centric artificial intelligence with DC-Check: Advances, challenges, and opportunities. IEEE Transactions on Artificial Intelligence, 2023b.\nDaniel Shwartz, Uri Stern, and Daphna Weinshall. The dynamic of consensus in deep networks and the identification of noisy labels. arXiv preprint arXiv:2210.00583, 2022.\nJasper Snoek, Alex Wiltschko, and Ali Rahimi. Winner\u2019s curse? On pace, progress, and empirical rigor. In International Conference on Learning Representations - Workshop, 2018.\nHwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S Morcos. Beyond neural scaling laws: Beating power law scaling via data pruning. In Advances in Neural Information Processing Systems, 2022.\nSainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training convolutional networks with noisy labels. In 3rd International Conference on Learning Representations, ICLR 2015, 2015.\nHao Sun, Alex J Chan, Nabeel Seedat, Alihan H\u00fcy\u00fck, and Mihaela van der Schaar. When is off-policy evaluation useful? a data-centric perspective. arXiv preprint arXiv:2311.14110, 2023a.\nHao Sun, Boris van Breugel, Jonathan Crabb\u00e9, Nabeel Seedat, and Mihaela van der Schaar. What is flagged in uncertainty quantification? latent density models for uncertainty categorization. Advances in Neural Information Processing Systems, 36, 2023b.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9275\u20139293, 2020.\nJunjiao Tian, Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Exploring covariate and concept shift for detection and confidence calibration of out-of-distribution data. arXiv preprint arXiv:2110.15231, 2021.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations, 2019.\nJoaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. OpenML: Networked science in machine learning. ACM SIGKDD Explorations Newsletter, 15(2):49\u201360, 2014.\nJiachen T Wang and Ruoxi Jia. Data banzhaf: A robust data valuation framework for machine learning. In International Conference on Artificial Intelligence and Statistics, pp. 6388\u20136421. PMLR, 2023.\nAmanda West. Machine Learning Models for the Future: The rise of the data scientist. Refinitiv Labs, 2020.\nSteven Euijong Whang, Yuji Roh, Hwanjun Song, and Jae-Gil Lee. Data collection and quality challenges in deep learning: A data-centric AI perspective. The VLDB Journal, pp. 1\u201323, 2023.\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam, Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.\nXiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. In International Conference on Learning Representations, 2021.\nJingkang Yang, Kaiyang Zhou, and Ziwei Liu. Full-spectrum out-of-distribution detection. International Journal of Computer Vision, pp. 1\u201316, 2023.\nMert Yuksekgonul, Linjun Zhang, James Zou, and Carlos Guestrin. Beyond confidence: Reliable models should also consider atypicality. arXiv preprint arXiv:2305.18262, 2023.\nShujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. ALLSH: Active learning guided by local sensitivity and hardness. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 1328\u20131342, 2022.\nYivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only noisy labels via total variation regularization. In International Conference on Machine Learning, pp. 12501\u201312512. PMLR, 2021.\nZhaowei Zhu, Jialu Wang, and Yang Liu. Beyond images: Label noise transition matrix estimation for tasks with lower-quality features. In International Conference on Machine Learning, pp. 27633\u201327653. PMLR, 2022.\nAppendix: Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI Table of Contents\nA Additional background and details 17 A.1 Description of HCMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.1.1 Learning-based (Margin) . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.1.2 Learning-based (Uncertainty) . . . . . . . . . . . . . . . . . . . . . . . 18 A.1.3 Learning-based (Loss) . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.1.4 Learning-based (Gradient) . . . . . . . . . . . . . . . . . . . . . . . . 19 A.1.5 Learning-based (Forgetting) . . . . . . . . . . . . . . . . . . . . . . . . 19 A.1.6 Learning-based (Statistics) . . . . . . . . . . . . . . . . . . . . . . . . 19 A.1.7 Distance-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.1.8 Statistical-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nA.2 Practical applications of HCMs. . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.3 Hardness taxonomy for tabular data. . . . . . . . . . . . . . . . . . . . . . . . 21\nB Experimental details 23 B.1 Computational resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.3 Backbone models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.4 HCMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.5 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nC Using H-CAT 26 C.1 H-CAT Benchmarking Framework Repository . . . . . . . . . . . . . . . . . . 26 C.2 Tutorial on using H-CAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.3 Extending H-CAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nC.3.1 Adding a new HCM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.3.2 Using a different dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nD Additional experiments 31 D.1 Multiple hardness types simultaneously . . . . . . . . . . . . . . . . . . . . . . 31 D.2 Stability/Consistency: Backbone model type . . . . . . . . . . . . . . . . . . . 32 D.3 Stability/Consistency: Parameterizations . . . . . . . . . . . . . . . . . . . . . 33 D.4 Additional aggregated results . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 D.5 Individual results - Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nD.5.1 Mislabeling: Adjacent . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 D.5.2 Mislabeling: Asymmetric . . . . . . . . . . . . . . . . . . . . . . . . . 38 D.5.3 Mislabeling: Instance . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 D.5.4 Mislabeling: Uniform . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 D.5.5 Near OOD: Covariate . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 D.5.6 Near OOD: Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 D.5.7 Far OoD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 D.5.8 Atypical: Crop Shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 D.5.9 Atypical: Zoom Shift . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\nD.6 Individual results - Tabular . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\nD.6.1 Mislabeling: Uniform . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 D.6.2 Mislabeling: Asymmetric . . . . . . . . . . . . . . . . . . . . . . . . . 63 D.6.3 Mislabeling: Instance . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 D.6.4 Near OOD: Covariate . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 D.6.5 Far OOD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 D.6.6 Atypical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\nD.7 Additional results - Medical X-Ray data . . . . . . . . . . . . . . . . . . . . . . 68 D.8 Insights: effect of severity of perturbation on hardness scores . . . . . . . . . . . 70\nA ADDITIONAL BACKGROUND AND DETAILS\nThis appendix provides additional details and background on the different HCMs.\nA.1 DESCRIPTION OF HCMS\nWe now provide a detailed description of the HCMs supported by H-CAT grouped by their class. We briefly describe how they characterize data and the modalities to which the HCMs have been previously applied.\nWe group the HCMs into broad classes determined based on the core metric or approach each method uses to characterize example hardness, namely\n\u2022 Learning dynamics-based: These HCMs rely on metrics computed during the training process itself to characterize example hardness. As we describe in the paper, we further divide these into sub-categories based on the specific training dynamic metric used: (i) Margin, (ii) Uncertainty, (iii) Loss, (iv) Gradient, (v) Forgetting.\n\u2022 Distance-based: These HCMs characterize hardness based on the distance or similarity of examples in an embedding space.\n\u2022 Statistical-based: These methods use statistical metrics computed over the data to characterize example hardness.\nRecall HCMs generally follow the following input-output paradigm:\n\u25a0 Inputs: (i) Dataset D = {(xi, yi)} drawn from both PXY . (ii) Learning algorithm f\u03b8. \u25a0 Outputs: Assign a score si to sample (xi, yi). Apply threshold \u03c4 to assign a hardness label g \u2208G, where G = {Easy,Hard}, which is then used to partition D = De \u222a Dh. Consequently, we define the scoring function S as the general notation applicable to each HCM. Note that whether a high score or low score indicates a \u201chard\u201d sample is method-dependent. We highlight HCMs included in H-CAT with a * and formalize those specifically with notation.\nWe provide a Table below to clarify whether high or low scores mean \u201chard\u201d for specific HCMs.\nWe now outline different HCMs. Specifically, we formalize the HCMs included in H-CAT in a unified notation of S(x, y).\nA.1.1 LEARNING-BASED (MARGIN)\nAUM (Pleiss et al., 2020)* AUM (Area under the margin) characterizes data examples based on the margin of a classifier \u2013 i.e. the difference between the logit values of the correct class and the next class. It has been applied to mislabeling for computer vision (CV).\nM (t)(x, y) = assigned logit\ufe37 \ufe38\ufe38 \ufe37 z(t)y (x) \u2212 largest other logit\ufe37 \ufe38\ufe38 \ufe37 maxi \u0338=y z (t) i (x) . (1)\nOver all epochs the AUM is then computed as the average of these margin calculations. We define it in terms of S. S(x, y) = AUM(x, y) = 1T \u2211T t=1 M (t)(x, y), (2)\nS(x, y) = 1T \u2211T t=1( assigned logit\ufe37 \ufe38\ufe38 \ufe37 z(t)y (x) \u2212 largest other logit\ufe37 \ufe38\ufe38 \ufe37 maxi\u0338=y z (t) i (x)) (3)\nA.1.2 LEARNING-BASED (UNCERTAINTY)\nData-IQ (Seedat et al., 2022a)* Data-IQ computes the aleatoric uncertainty and confidence to characterize the data into easy, ambiguous, and hard examples and has been studied for the tabular domain.\nThe aleatoric uncertainty is computed as val(x) = 1T \u2211T\nt=1 P(x, \u03b8t)(1\u2212 P(x, \u03b8t)) and confidence over epochs as P(x, \u03b8t). For hard samples, the score for an input (x,y) is computed primarily on the confidence with uncertainty used to delineate ambiguous:\nS(x, y) = 1\nT T\u2211 t=1 P(x, \u03b8t) (4)\nC(x, y) = 1\nT T\u2211 t=1 P(x, \u03b8t) (5)\nData Maps (Swayamdipta et al., 2020)* Data Maps focuses on measuring variability (epistemic uncertainty) and confidence to characterize the data into easy, ambiguous, and hard examples and has been studied for natural language processing (NLP) tasks.\nThe epistemic uncertainty is computed as vep(x) = 1T \u2211T t=1 [ P(x, \u03b8t)\u2212 P\u0304(x) ]2 and confidence over epochs as P(x, \u03b8t). For hard samples, the score for an input (x,y) is computed primarily on the confidence with uncertainty used to delineate ambiguous:\nS(x, y) = P(x, \u03b8e) (6)\nA.1.3 LEARNING-BASED (LOSS)\nSmall Loss (Xia et al., 2021)* Small Loss characterized data based on sample-level loss magnitudes and has been studied for computer vision (CV) tasks.\nThe small loss for an input x, y is computed as:\nS(x, y) = 1\nt t\u2211 i=1 li, (7)\nwhere li = \u2113(f(w;x), y).\nRHO-Loss (Mindermann et al., 2022) RHO-Loss estimates the sample loss on a holdout set and has been applied in CV tasks. We do not include it in H-CAT as it requires training an irreducible loss model.\nA.1.4 LEARNING-BASED (GRADIENT)\nGraNd (Paul et al., 2021)* GraNd measures the gradient norm to characterize data, particularly for computational reasons in computer vision (CV) tasks.\nThe gradient norm at epoch t for an input x, y is computed as :\nS(x, y) = E \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\n\u2207f(k)\u2113 (ft(x), y) T \u03c8 (k) t (x) \u2225\u2225\u2225\u2225\u2225 2 where\u03c8 (k) t (x) = \u2207wtf (k) t (x). (8)\nVoG (Agarwal et al., 2022)* VoG (Variance of gradients) estimates the variance of gradients for each sample over training, particularly in computer vision (CV) tasks.\nThe VoG at epoch t for an input x, y is computed as :\nS(x, y) = 1\nN N\u2211 t=1 (V oGp(x, y)), (9)\nV oGp(x, y) = \u221a\u221a\u221a\u221a 1 K K\u2211 t=1 (Mt(x, y)\u2212 \u00b5)2. (3)\nwhere \u00b5 = 1K \u2211K t=1Mt\nNote Mt is the gradient matrix.\nA.1.5 LEARNING-BASED (FORGETTING)\nForgetting scores (Toneva et al., 2019)* Forgetting scores analyze example transitions through training. i.e., the time a sample correctly learned at one epoch is then forgotten. It is commonly used in CV tasks.\nThe Forgetting score an input x, y is computed as : S(x, y)\u2190 S(x, y) + 1(prev_acci > acc(x, y)) (10)\nSSFT (Maini et al., 2022) SSFT (Second Split Forgetting Time) measures the time of forgetting when fine-tuning models on a second split. It has been applied in CV tasks. We do not include it in H-CAT as it requires fine-tuning on a validation dataset,\nA.1.6 LEARNING-BASED (STATISTICS)\nEL2N (Paul et al., 2021)* EL2N (L2 norm of error over training) calculates the L2 norm of error over training in order to characterize data for computational purposes. It is predominantly applied in CV tasks.\nThe EL2N at epoch t for an input x, y is computed as : S(x, y) = E\u2225p(wt, x)\u2212 y\u22252. (11)\nNoise detector (Jia et al., 2022)* Noise detector utilizes a trained detector model to predict noisy samples. The detector is applied to training dynamics and has been used in CV tasks.\nNoise detector uses a trained detector g to score x,y as:\nS(xi, yi) = g(Lyi(i)) (12) where L(i, t) = f (t)(xi)[training dynamic]\nA.1.7 DISTANCE-BASED\nPrototypicality (Sorscher et al., 2022)* Prototypicality calculates the latent space clustering distance to the centroid as the metric to characterize data. It has been applied to CV tasks.\nPrototypicality score for x,y is computed as:\nS(x, y) = \u2225\u03d5(x)\u2212 \u00b5y\u22252 (13)\nHere, \u03d5(x) represents the mapping of x to the latent space, and \u00b5y is the centroid of the class y in the latent space. The norm \u2225 \u00b7 \u22252 denotes the Euclidean distance.\nA.1.8 STATISTICAL-BASED\nCleanlab (Northcutt et al., 2021a)* Cleanlab estimates the joint distribution of noisy and true labels \u2014 characterizing data as easy and hard for mislabeling. It can be applied to any modality.\nThe Cleanlab score for an input x, y is computed as :\nS(x, y) = {x \u2208 X | y = y\u2217 \u0338= j} for x from the off-diagonals of Cyj ,y\u2217 . (14)\nALLSH (Zhang et al., 2022)* ALLSH computes the KL divergence of softmax outputs between original and augmented samples to characterize data.\nThe Allsh score for an input x, y is computed as :\nS(x, y) = D(p\u03b8(\u00b7|x), p\u03b8(\u00b7|x\u2032)) (15)\nwhere x\u2032 is a augmented x and D a distance measure (e.g KL-Divergence).\nAgreement (Carlini et al., 2019)* Agreement measures the agreement of predictions on the same example. It is commonly used in CV tasks.\nThe Agreement for an input x, y is computed as :\nS(x, y) = 1\nN N\u2211 i=1 max fi(x) (16)\nData Shapley (Ghorbani & Zou, 2019) Data Shapley computes the Shapley value to characterize the value of including a data point. It is commonly used in CV tasks. We also do not consider Data Shapley in H-CAT and variants (e.g. Beta Shapley (Kwon & Zou, 2022)), which have been shown to be computationally unfeasible with numerical instabilities for higher dimensional data such as MNIST and CIFAR-10 with > 1000 samples (Wang & Jia, 2023).\nA.2 PRACTICAL APPLICATIONS OF HCMS.\nTo illustrate the practical value and need for HCMs in creating high-quality datasets, we highlight the following application domains:\n\u2022 The Cleanlab HCM was used to audit ML benchmark datasets for mislabeling (Northcutt et al., 2021b).\n\u2022 The Data Maps HCM was used to audit and create high-quality RNA data in biology (Nabi et al., 2023).\n\u2022 The Data-IQ HCM was used to audit and curate high-quality sleep staging data (Heremans et al., 2024).\n\u2022 The Data-IQ and Cleanlab HCMs have been used to guide synthetic data generation (Hansen et al., 2023).\n\u2022 The Data Maps and Data-IQ HCMs where re-purposed to assess samples for data-centric off-policy evaluation (Sun et al., 2023a).\nA.3 HARDNESS TAXONOMY FOR TABULAR DATA.\nIn the main paper, we primarily discussed our manifestation of the hardness taxonomy with respect to images. We describe below how this isn\u2019t confined to images and how these hardness types could manifest, for instance, in tabular data.\nMislabeling: Mislabeling in tabular data can occur when the label attributed to a particular row (representing a sample) is incorrect. For instance, in a medical dataset, this could occur when a patient with a certain condition is wrongly labeled as not having the condition (or vice versa). Note mislabeling is no different from the image domain as the perturbation is applied on y.\nHence, the various sub-types of mislabeling can still apply. Uniform mislabeling means any class of disease can be mislabeled as any other with equal probability. Asymmetric mislabeling would mean certain classes of disease are more likely to be confused with others, for instance, certain diseases might be more commonly misdiagnosed as each other. Consequently, in implementation uniform and asymmetric do not change from images. Adjacent mislabeling (special case of asymmetric) could refer to situations where labels are ordinal and a higher value might be mislabeled as a lower one (or vice versa). Instance-specific mislabeling refers to instances where certain individual observations are more likely to be mislabeled based on their specific attributes. We could still use \u201chuman knowledge\u201d of similar classes for tabular data. We demonstrate the use of PCA to embed the data and then find the nearest class for a specific label in embedding space as the instance-wise mislabeling.\nNear-OoD: Near-OoD in tabular data could refer to rows of data where the feature values are still within the same domain but differ in some way from typical observations. For example, in a financial dataset, a Near OoD sample might be an account with an unusually high number of transactions, which, although not completely unusual, stands apart from typical transaction behavior. This could represent a Covariate Shift where the distribution of the predictors changes, or Domain Shift where the rules of data generation change slightly.\nAlternatively, in a dataset of regular health screenings, a Near OoD instance might be a health record of a patient who shows high values in their lab results, which although still plausible, stand apart from typical health metrics. For instance, a patient with very high cholesterol levels would be a Near OoD example in this context, indicating a Covariate Shift where the distribution of predictors changes. In terms of implementation, the continuous variables of our tabular data might be noisy or covariate shifted data. For example, Gaussian noise to continuous features.\nFar-OoD: Far Out-of-Distribution in tabular data refers to rows that are dramatically different from typical observations in a dataset, or might even represent entirely new categories of data. Using the financial dataset example, a Far OoD sample might be transactions in a new foreign currency not previously seen in the dataset. This type of data is distinctly different and unrelated to the rest of the dataset.\nIn a medical dataset, Far-OoD might refer to patient records that are substantially different from typical observations or that fall into entirely new categories. For instance, imagine a dataset mainly consisting of adult patients, and suddenly you get data from pediatric patients. Children have different medical characteristics and range for many medical metrics (like heart rate, blood pressure, etc.), so pediatric patient data would be Far OoD in this context. Another example could be the inclusion of a new disease class not previously part of the dataset.\nPractically, for a given tabular dataset, this could be data from a different distribution (e.g. shuffled data) or feature relationships that don\u2019t match \u2014 e.g. older people having a specific feature linked to a disease, but these relationships are now broken. For example, within the framework this could be implemented as follows: for binary covariates, we can swap the features (i.e. flip male to female etc.) \u2014 to break the feature relationships such that they are drawn from a completely different distribution. For categorical variables, we can flip the category of a subset of variables in order to break the feature relationships.\nAtypical: Atypical data in a tabular dataset can refer to observations that, while not incorrect or out of distribution, deviate from common patterns in the data, i.e. they are rare but valid rows of the table (e.g. outliers, extremes \u2014 samples from tails of marginal). For example, in a dataset of real estate properties, an atypical sample could be a property with an unusually large number of rooms or an\nunusually low price for its size. While such a property could exist (so it\u2019s not necessarily mislabeled), it deviates from the common pattern in the data and might be found in the long tail of the distribution.\nIn a medical dataset, it might refer to patient records that, while not incorrect or out of distribution, deviate from common patterns in the data. For instance, consider a dataset of prostate cancer patients, where naturally, most patients are older. An atypical example might be a young adult with prostate cancer, which would represent an unusual instance, as they fall outside the common pattern of older patients with prostate cancer. From a framework implementation perspective, we could sample and replace with values from the tails of the marginals for the most predictive long-tailed feature (via feature-label correlation).\nB EXPERIMENTAL DETAILS\nThis appendix describes our experimental details, computational resources, HCMs, and datasets below.\nB.1 COMPUTATIONAL RESOURCES\nExperiments are parallelized to run different setups using three NVIDIA RTX A4000s, with an 18-Core Intel Core i9 and 16 gb of RAM. Each experimental setup can be run on a single GPU.\nB.2 DATASETS\nWe run our experiments on MNIST and CIFAR-10 datasets. However, these experiments can be run on alternative datasets, as shown in Appendix C.\nMNIST (LeCun et al., 2010) : consists of 70,000 small square 28x28 pixel grayscale images of digits from 0 to 9. Each image is a handwritten digit, with 60,000 of the images dedicated for training data and the remaining 10,000 for testing data.\nCIFAR-10 (Krizhevsky et al.) : It consists of 60,000 32x32 color images distributed among 10 different classes such as airplanes, dogs, cats, and trucks. The dataset is divided into 50,000 training images and 10,000 testing images. Each class has an equal representation in the dataset with 6,000 images per class.\nNote on selection of MNIST & CIFAR-10. We chose the MNIST and CIFAR-10 datasets as benchmarks because they are well-established datasets in the literature. Notably, many HCMs employ MNIST and CIFAR-10 within the context of their limited quantitative evaluation. Further, the noisy label literature frequently uses them for controlled experiments. This prevalence underscores their appropriateness for controlled benchmarking.\nLet\u2019s compare potential alternatives.\n\u2022 Completely synthetic data: This would be 100% clean, but too simple and toy-like.\n\u2022 Large and highly complicated datasets like ImageNet: These contain significant levels of mislabeling (over 5%) (Northcutt et al., 2021b), hence we cannot do controlled experiments.\n\u2022 MNIST and CIFAR-10: These are real image datasets. But they have been shown to contain almost no/little mislabeling (under 0.5%) (Northcutt et al., 2021b),.\nTabular data : We conducted analogous experiments to the image experiments on tabular datasets, specifically, the OpenML (Vanschoren et al., 2014) datasets \u2019cover\u2019 and \u2019diabetes130US\u2019. Note these datasets are included in a recent NeurIPS benchmark for tabular models (Grinsztajn et al., 2022). Additionally, this allows us to flexibly use the H-CAT framework with any tabular dataset as long as it conforms to the OpenML structure.\nB.3 BACKBONE MODELS\nWe use the following backbone models: LeNet (7-layer convolutional neural network) and ResNet-18 (18-layer deep convolutional neural network with \"skip\" or \"residual\" connections). We train the backbones with a cross-entropy loss using the Adam optimizer. Our learning rate of 1e-3 and batch size of 32 are kept fixed as they showed good convergence. As discussed, we explore different training epochs of 10 and 20 and show consistency irrespective, in Appendix D. This is reasonable as has been shown by (Seedat et al., 2022a) that the learning captured by HCMs (especially in training dynamics) happens in the early epochs. Note, we fix the backbone models in line with principles of Data-Centric AI of systematically assessing the data for a fixed ML model (Liang et al., 2022; Seedat et al., 2023b; Whang et al., 2023; Ng et al., 2021; Jarrahi et al., 2022).\nB.4 HCMS\nWe outline the implementation of the 13 HCMs supported by H-CAT below. We have described the HCMs in Appendix A.\n1. Learning-based (Margin)\nAUM (Pleiss et al., 2020) : Our implementation is based on 3.\n2. Learning-based (Uncertainty)\nData-IQ (Seedat et al., 2022a) : Our implementation is based on 4.\nData Maps (Swayamdipta et al., 2020) : Our implementation is based on 5.\n3. Learning-based (Loss)\nLarge Loss (Xia et al., 2021) : Our implementation computes the loss per sample individually.\n4. Learning-based (Gradient)\nGraNd (Paul et al., 2021) : Our implementation is based on the Pytorch implementation from 6 which is based on the original JAX 7.\nVoG (Agarwal et al., 2022) : Our implementation is based on 8.\n5.Learning-based (Forgetting)\nForgetting scores (Toneva et al., 2019) : Our implementation based on the original paper tracks the learning and forgetting times per sample in order to compute the forgetting score.\n6. Learning-based (Statistics)\nEL2N (Paul et al., 2021) : Our implementation is based on the Pytorch implementation from 9 which is based on the original JAX 10.\nNoise detector (Jia et al., 2022) : Our implementation is based on 11. We use the noise detector training scripts in the repo.\n7. Distance-based\nPrototypicality (Sorscher et al., 2022) : Our implementation follows the original paper and computes prototypicality as follows. We embed the image as a low-dimensional embedding using the backbone network. We then compute the Cosine distances to the mean embedding of the specific label for that sample. The cosine distances being smaller are more prototypical.\n8. Statistical-based 3https://github.com/asappresearch/aum 4https://github.com/seedatnabeel/Data-IQ 5https://github.com/seedatnabeel/Data-IQ 6https://github.com/BlackHC/pytorch_datadiet 7https://github.com/mansheej/data_diet 8https://github.com/chirag126/VOG 9https://github.com/BlackHC/pytorch_datadiet\n10https://github.com/mansheej/data_diet 11https://github.com/Christophe-Jia/mislabel-detection\nCleanlab (Northcutt et al., 2021a) : Our implementation is based on 12\nALLSH (Zhang et al., 2022) : Our implementation follows the original paper. We compute the KL divergence in the model predictive probabilities for the pairs containing: (1) original input and (2) augmentation of the input. We perform the augmentation using AugLY 13. The augmentations are selected from Horizontal flip, Random brightness, Random Noise and Random Pixelization.\nAgreement (Carlini et al., 2019) : Our implementation of confidence agreement produces multiple predictions using MC-Dropout to create 10 pseudo-ensembles. We then compute the mean of the confidences as a measure of agreement, as per the original paper.\nB.5 IMPLEMENTATION DETAILS\nWe outline the experimental implementation details next. Details on the H-CAT and its usage can be found in Appendix C.\nHardness detection & Ranking. In this experiment, we set a different random seed for each of the three runs. For each run, we resample the dataset and change the samples that we perturb (i.e. which samples are hard). We also repeat the experiments for multiple proportions p.\nWe execute all experiments with \u2018p\u2019 values of [0.1, 0.2, 0.3, 0.4, 0.5], except for Far-OoD and Atypical experiments, where \u2018p\u2019 values are restricted to [0.05, 0.1, 0.15, 0.2, 0.25]. The rationale behind this distinction is that, in reality, Far-OoD and Atypical conditions do not typically occur in high proportions. For instance, a sample can\u2019t be considered Atypical if it constitutes 50% of occurrences. Therefore, for these conditions, we limit \u2018p\u2019 to a maximum of 0.25.\nStability/Consistency In this experiment, we maintain a consistent random data split and the set of perturbed samples, which represent the difficult examples, is also kept constant across different runs. The element of randomness is brought in solely through the training of the backbone model and by extension, the characterization of the HCM. i.e. we have a different seed for the model. This approach is taken because our aim is to evaluate the Stability/Consistency of the HCM across multiple runs when presented with the same data.\n12https://github.com/cleanlab/cleanlab 13https://github.com/facebookresearch/AugLy\nC USING H-CAT\nThis appendix discusses how to use the H-CAT framework. We link to the codebase, provide a tutorial on using H-CAT (see Sec. C.2), and also show how H-CAT can be extended to new HCMs and datasets (see Sec. C.3).\nC.1 H-CAT BENCHMARKING FRAMEWORK REPOSITORY\nTo foster ease of use and engagement, we release the H-CAT Benchmarking Framework\u2019s codebase with Apache 2.0 License. It can be found at: https://github.com/seedatnabeel/H-CAT\nC.2 TUTORIAL ON USING H-CAT\nBelow, we illustrate a tutorial demonstrating how to use H-CAT, highlighting the composable nature and the object-oriented interface making it easy to use. We also provide this in the form of a Jupyter notebook called tutorial.ipynb.\n1 from src.trainer import PyTorchTrainer 2 from src.dataloader import MultiFormatDataLoader 3 from src.models import * 4 from src.evaluator import Evaluator\nListing 1: Step 1: Necessary imports\n1 hardness = \"uniform\" 2 p=0.1 3 dataset = \"mnist\" 4 model_name = \"lenet\" 5 epochs = 20 6 seed = 0 7 8 # Defined by prior or domain knowledge 9 if hardness ==\"instance\":\n10 if dataset == \"mnist\": 11 rule_matrix = { 12 1: [7], 13 2: [7], 14 3: [8], 15 4: [4], 16 5: [6], 17 6: [5], 18 7: [1, 2], 19 8: [3], 20 9: [7], 21 0: [0] 22 } 23 if dataset == \"cifar\": 24 25 rule_matrix = { 26 0: [2], # airplane (unchanged) 27 1: [9], # automobile -> truck 28 2: [9], # bird (unchanged) 29 3: [5], # cat -> automobile 30 4: [5,7], # deer (unchanged) 31 5: [3, 4], # dog -> cat 32 6: [6], # frog (unchanged) 33 7: [5], # horse -> dog 34 8: [7], # ship (unchanged) 35 9: [9], # truck -> horse 36 } 37 38 else:\n39 rule_matrix = None\nListing 2: Step 2: Define experimental parameters\n1 characterization_methods = [ 2 \"aum\", 3 \"data_uncert\", # for both Data-IQ and Data-Maps 4 \"el2n\", 5 \"grand\", 6 \"cleanlab\", 7 \"forgetting\", 8 \"vog\", 9 \"prototypicality\",\n10 \"allsh\", 11 \"loss\", 12 \"conf_agree\", 13 \"detector\" 14 ],\nListing 3: Step 3: Define HCMs to evaluate \u2014 if excluded we evaluate all\n1 import torch 2 import torch.nn as nn 3 import torch.optim as optim 4 from torchvision import datasets, transforms 5 6 if dataset == \u2019cifar\u2019: 7 # Define transforms for the dataset 8 transform = transforms.Compose([ 9 transforms.ToTensor(),\n10 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) 11 ]) 12 13 # Load the CIFAR-10 dataset 14 train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download= True, transform=transform) 15 test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download= True, transform=transform) 16 17 elif dataset ==\u2019mnist\u2019: 18 # Define transforms for the dataset 19 transform = transforms.Compose( 20 [transforms.ToTensor(), 21 transforms.Normalize((0.5,), (0.5,))]) 22\n23 24 # Load the MNIST dataset 25 train_dataset = datasets.MNIST(root=\"./data\", train=True, download= True, transform=transform) 26 test_dataset = datasets.MNIST(root=\"./data\", train=False, download= True, transform=transform) 27\n28 29 total_samples = len(train_dataset) 30 num_classes = 10 31 32 # Set device to use 33 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nListing 4: Step 4: Load datasets\n1 # Allows importing data in multiple formats 2 3 dataloader_class = MultiFormatDataLoader(data=train_dataset, 4 target_column=None, 5 data_type=\u2019torch_dataset\u2019, 6 batch_size=64, 7 shuffle=True, 8 num_workers=0, 9 transform=None,\n10 image_transform=None, 11 perturbation_method=hardness, 12 p=p, 13 rule_matrix=rule_matrix 14 ) 15\n16 17 dataloader, dataloader_unshuffled = dataloader_class.get_dataloader() 18 flag_ids = dataloader_class.get_flag_ids()\nListing 5: Step 5: Call H-CAT Dataloader Module (with Hardness module parameterized)\n1 # Instantiate the neural network and optimizer 2 if dataset == \u2019cifar\u2019: 3 if model_name == \u2019LeNet\u2019: 4 model = LeNet(num_classes=10).to(device) 5 if model_name == \u2019ResNet\u2019: 6 model = ResNet18().to(device) 7 elif dataset == \u2019mnist\u2019: 8 if model_name == \u2019LeNet\u2019: 9 model = LeNetMNIST(num_classes=10).to(device)\n10 if model_name == \u2019ResNet\u2019: 11 model = ResNet18MNIST().to(device) 12 13 criterion = nn.CrossEntropyLoss() 14 optimizer = optim.Adam(model.parameters(), lr=0.001) 15\n16 17 # Instantiate the PyTorchTrainer class 18 trainer = PyTorchTrainer(model=model, 19 criterion=criterion, 20 optimizer=optimizer, 21 lr=0.001, 22 epochs=epochs, 23 total_samples=total_samples, 24 num_classes=num_classes, 25 characterization_methods= characterization_methods, 26 device=device) 27 28 # Train the model 29 trainer.fit(dataloader, dataloader_unshuffled) 30 31 hardness_dict = trainer.get_hardness_methods()\nListing 6: Step 6: Call H-CAT Trainer Module (with HCM module parameterized)\n1 eval = Evaluator(hardness_dict=hardness_dict, flag_ids=flag_ids, p=p) 2 3 eval_dict, raw_scores_dict = eval.compute_results() 4 5 print(eval_dict)\nListing 7: Step 7: Call H-CAT Evaluator Module\nC.3 EXTENDING H-CAT\nC.3.1 ADDING A NEW HCM\nNew HCMs can easily be added to H-CAT. We describe the process below.\nWe take an object-oriented approach and hence new HCMs can be included by inheriting from our base class Hardness_Base defined below.\n1 # Base class for HCMs 2 class Hardness_Base: 3 def __init__(self, name): 4 self.name = name 5 self._scores = None 6 7 def updates(self): 8 pass 9\n10 def compute_scores(self): 11 pass 12 13 @property 14 def scores(self): 15 if self._scores is None: 16 self.compute_scores() 17 return self._scores\nListing 8: HCM Base class\nTo create a new HCM, you would create a new class that inherits from Hardness_Base. In this new subclass, override the updates and compute_scores methods with the specific logic for this new hardness calculation method. We show an example below.\n1 class MyNewHCM(Hardness_Base): 2 def __init__(self, name): 3 super().__init__(name) 4 5 def updates(self): 6 # logic to perform updates goes here 7 pass 8\n9 def compute_scores(self): 10 # logic to compute scores goes here 11 self._scores = \"some computed value\" # Replace this with your\ncomputation\nListing 9: Example New HCM\nThe HCM then needs to be added to trainer.py in the correct part of the training loop and can be used as below\n1 hcm = MyNewHCM(\u2019some name\u2019) 2 hcm.updates() 3 print(hcm.scores)\nListing 10: Usage of the new defined HCM\nNote, one needs to think for the HCM if it gets updated every epoch or at the end of training to define placement in the training loop.\nC.3.2 USING A DIFFERENT DATASET\nWe can use different datasets with H-CAT quite simply.\nBefore delving further, we show below where the new dataset needs to be passed to the Dataloader module. i.e. We need to pass train_dataset.\n1 # Allows importing data in multiple formats 2 3 dataloader_class = MultiFormatDataLoader(data=train_dataset, 4 target_column=None, 5 data_type=\u2019torch_dataset\u2019, 6 batch_size=64, 7 shuffle=True, 8 num_workers=0, 9 transform=None,\n10 image_transform=None, 11 perturbation_method=hardness, 12 p=p, 13 rule_matrix=rule_matrix 14 )\nListing 11: Example of passing data to the Dataloader module\nThe easiest way is to define your dataset as a standard torch dataset outside of H-CAT and simply to pass that into the MultiFormatDataLoader and specify a target_column=None.\nIf one has the data as NumPy arrays or torch tensors, one would need to pass a tuple as data = (X,y) and specify a target_column.\nOther formats are also possible with H-CAT. For instance, if you have images in folders. One could pass data as a CSV file, JSON, or Python dictionary \u2014 wherein there is a column with paths to each sample and a column with the target. One can then pass the path to the CSV file or JSON file or pass the Python dict itself. In addition, one should specify the target_column. The rest is handled internally within H-CAT to convert the data to an appropriate Torch dataset for training.\nThat said, ideally if the dataset can be converted to a Torch dataset before passing it to H-CAT that is preferred.\nD ADDITIONAL EXPERIMENTS\nThis appendix presents additional experiments and full results across all setups using H-CAT to evaluate different HCMs.\nD.1 MULTIPLE HARDNESS TYPES SIMULTANEOUSLY\nOverview. As discussed in the main paper, we primarily focussed on single-source hardness. We might also expect cases where multiple hardness types manifest simultaneously. For instance, if an image is Near-OoD (e.g. blurry or different texture), we might be more likely also to mislabel it. As a demonstration of the usage of H-CAT for this purpose, we hence assess the case where both Near-OoD (Covariate and Domain) are paired with Instance-specific mislabeling.\nWe want to understand the effect of this on HCM capabilities.\nResults. We show results for both Near-OoD (Covariate) + Instance and Near-OoD (Domain) + Instance below in Figure 8.\nTakeaway. Our results indicate that H-CAT can be used to assess multiple hardness types, which manifest simultaneously. We find that HCMs which perform well individually on both types of hardness \u2014 still perform well in the simultaneous setting.\nD.2 STABILITY/CONSISTENCY: BACKBONE MODEL TYPE\nOverview. We assessed HCMs using two different backbone models - LeNet and ResNet-18. We wish to assess the stability/consistency of the backbone model. As discussed in the main paper, we can quantify this based on the Spearman rank correlation \u2014 i.e., are the HCMs ordering the samples consistently in a stable manner?\nTakeaway. Of course, the LeNet and ResNet-18 are significantly different in model type and size. However, we find that compared to the results in the main paper, we see minimal degradation in the Spearman rank correlation. This indicates that the backbone model to do the hardness characterization is minimally impacted by the backbone model. i.e. those HCMs which were the most stable and consistent remain the most stable and consistent.\nD.3 STABILITY/CONSISTENCY: PARAMETERIZATIONS\nOverview. We can train the backbone models differently, with different parameterizations. For instance, with different numbers of epochs, 10 or 20. We wish to assess stability/consistency with respect to different training parameterizations when performing the HCM evaluation. As discussed in the main paper, we can quantify this based on the Spearman rank correlation \u2014 i.e. are the HCMs ordering the samples consistently in a stable manner?\nWe make a head-to-head comparison of scores obtained from the HCMs applied to backbone models trained for 10 and 20 epochs. Ideally, we still would want a high Spearman correlation indicating sample order is maintained.\nTakeaway. We investigated the stability/consistency of the model parameterization in terms of the number of epochs used to train the backbone models. We find the Spearman correlations are not as affected by the epoch number as potentially expected. We see the magnitudes of the Spearman correlation for the different HCMs are similar to that of the main paper. Moreover, the HCMs, which were the most stable and consistent, remain the most stable and consistent.\nThis result is understandable, as it was shown in (Seedat et al., 2022a) that the majority of variation comes from the early epochs (before 10 epochs), hence after the training dynamics on which most of the metrics are computed stabilize. This means that it won\u2019t affect our HCM characterization whether we continue training the backbone till 20 epochs. This result simply mirrors this result from the literature and highlights for benchmarking purposes that we don\u2019t need to train the backbone model for too many epochs in order to have a good assessment of the HCMs\u2019 capabilities.\nD.4 ADDITIONAL AGGREGATED RESULTS\nOverview. In the main paper, we presented aggregate results for 6 out of 9 hardness manifestations or types. We present the results for the remaining three Adjacent mislabeling, Near-OOD (Domain) and Atypical (Zoom), below in Figures 11-14.\nD.5 INDIVIDUAL RESULTS - IMAGES\nD.5.1 MISLABELING: ADJACENT\nHeatmaps.\nRankings.\ndataiq (1.6)aum (1.8)datamaps (2.6)prototypicality (4)cleanlab (5)vog (6.2)conf_agree (6.8)\n(13) loss(12) grand(11) el2n(9.8) forgetting(9.2) detector(8) allsh\n(a) MNIST - LeNet\ndataiq (1)datamaps (2)aum (3)cleanlab (4)vog (5)conf_agree (6)\n(12) el2n(11) loss(10) grand(9) detector(7.8) forgetting(7.2) allsh\n(b) MNIST - ResNet\n5 10\naum (1)dataiq (2.6)vog (3.2)datamaps (3.6)cleanlab (4.6)prototypicality (6)forgetting (7)\n(13) el2n(12) loss(11) grand(10) detector(9) allsh(8) conf_agree\n(c) CIFAR - LeNet\n5 10\ndataiq (1.2)datamaps (2.2)aum (2.6)cleanlab (4)vog (5)conf_agree (6)\n(12) el2n(11) loss(10) grand(9) detector(7.6) allsh(7.4) forgetting\n(d) CIFAR - ResNet\nFigure 19: Adjacent mislabeling - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.2 MISLABELING: ASYMMETRIC\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 12.0 1.0 8.0 11.0 2.0 3.0 10.0 6.0 13.0 9.0 4.0 5.0 8.0\nasymmetric_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n11.0\n3.0\n6.0\n10.0\n1.0 2.0\n9.0\n5.0\n12.0\n7.0\n4.0\n8.0\nasymmetric_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 13.0 1.0 6.0 12.0 3.0 4.0 9.0 2.0 10.0 11.0 5.0 7.0 8.0\nasymmetric_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n12.0\n1.0\n6.0\n10.0\n2.0 3.0\n9.0\n5.0\n11.0\n8.0\n4.0\n7.0\nasymmetric_roc\n(d) CIFAR - ResNet\nFigure 25: Asymmetric mislabeling - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\naum (1)dataiq (2)datamaps (3)prototypicality (4)cleanlab (5.2)vog (5.8)conf_agree (7)\n(13) loss(12) el2n(11) grand(9.6) forgetting(9.4) detector(8) allsh\n(a) MNIST - LeNet\n5 10\ndataiq (1.2)datamaps (2.2)aum (2.6)cleanlab (4)vog (5)conf_agree (6)\n(12) loss(11) el2n(10) grand(9) detector(8) forgetting(7) allsh\n(b) MNIST - ResNet\n5 10\naum (1)dataiq (2)datamaps (3)cleanlab (4)prototypicality (5.4)vog (5.6)forgetting (7)\n(13) el2n(12) loss(11) detector(10) grand(9) allsh(8) conf_agree\n(c) CIFAR - LeNet\n5 10\naum (1)dataiq (2)datamaps (3)cleanlab (4)vog (5)conf_agree (6)\n(12) loss(11) el2n(10) grand(9) detector(7.8) forgetting(7.2) allsh\n(d) CIFAR - ResNet\nFigure 26: Asymmetric mislabeling - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.3 MISLABELING: INSTANCE\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 11.0 5.0 7.0 9.0 3.0 4.0 13.0 6.0 12.0 7.0 2.0 1.0 10.0\ninstance_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n11.0\n6.0 5.0\n10.0\n2.0 3.0\n12.0\n3.0\n8.0 7.0\n4.0\n9.0\ninstance_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 13.0 2.0 5.0 12.0 6.0 7.0 11.0 2.0 10.0 9.0 8.0 1.0 3.0\ninstance_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n11.0\n4.0 5.0\n9.0\n1.0 2.0\n11.0\n4.0\n12.0\n7.0\n4.0\n8.0\ninstance_roc\n(d) CIFAR - ResNet\nFigure 32: Instance mislabeling - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\nprototypicality (1)dataiq (2.2)datamaps (3.2)aum (3.6)cleanlab (5.4)vog (6.6)conf_agree (6.8)\n(13) loss(12) el2n(11) grand(9.2) detector(9) forgetting(8) allsh\n(a) MNIST - LeNet\n5 10\ncleanlab (2)dataiq (2.8)aum (3.4)datamaps (3.8)vog (4.6)conf_agree (5.8)\n(11) el2n(10) loss(10) grand(9) forgetting(7.8) detector(7) allsh\n(b) MNIST - ResNet\n5 10\nprototypicality (1.8)cleanlab (4.8)dataiq (4.8)vog (4.8)aum (5.8)datamaps (5.8)conf_agree (7.8)\n(11) el2n(10) loss(9.4) detector(8.6) allsh(8.2) grand(8.2) forgetting\n(c) CIFAR - LeNet\n5 10\ndataiq (2.6)cleanlab (3.6)datamaps (3.6)aum (3.8)vog (4.6)allsh (5.6)\n(12) el2n(10) loss(10) grand(8.4) forgetting(8) detector(5.8) conf_agree\n(d) CIFAR - ResNet\nFigure 33: Instance mislabeling - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.4 MISLABELING: UNIFORM\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 12.0 1.0 7.0 11.0 3.0 4.0 10.0 6.0 13.0 9.0 2.0 5.0 8.0\nuniform_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s\nde tec\ntorel2 n\nfor ge\nttin g gra ndlos s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9\n11Ra nk\nS co\nre s\n12.0\n1.0\n5.67\n11.0\n2.0 3.0\n9.33\n6.0\n11.67\n9.33\n3.33\n6.0 7.67\nuniform_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 13.0 1.0 5.0 12.0 2.0 3.0 9.0 6.0 10.0 11.0 4.0 7.0 8.0\nuniform_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n11.0\n1.0\n5.0\n10.0\n2.0 3.0\n9.0\n6.0\n12.0\n8.0\n4.0\n7.0\nuniform_roc\n(d) CIFAR - ResNet\nFigure 39: Uniform mislabeling - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\naum (1.2)dataiq (2.4)prototypicality (3)datamaps (3.4)cleanlab (5)vog (6)conf_agree (7)\n(13) loss(12) el2n(11) grand(9.8) detector(9.2) forgetting(8) allsh\n(a) MNIST - LeNet\n5 10\naum (1)dataiq (2)datamaps (3.2)prototypicality (4.3)cleanlab (4.5)vog (6)conf_agree (7)\n(13) grand(11) detector(11) loss(10) el2n(9.5) forgetting(8.8) allsh\n(b) MNIST - ResNet\n5 10\naum (1)dataiq (2)datamaps (3)cleanlab (4)prototypicality (5.4)vog (5.6)forgetting (7)\n(13) loss(12) el2n(11) detector(10) grand(9.2) allsh(8) conf_agree\n(c) CIFAR - LeNet\n5 10\naum (1.6)dataiq (2.6)datamaps (3.6)loss (4.4)cleanlab (4.6)vog (6)\n(12) forgetting(11) grand(9.4) detector(9) allsh(7.4) conf_agree(6.6) el2n\n(d) CIFAR - ResNet\nFigure 40: Uniform mislabeling - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.5 NEAR OOD: COVARIATE\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 11.0 2.0 7.0 9.0 3.0 4.0 13.0 6.0 12.0 8.0 5.0 10.0 2.0\nood_covariate_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n10.0\n3.0\n9.0 8.0\n4.0 5.0\n12.0\n7.0\n11.0\n3.0\n6.0\n1.0 ood_covariate_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 1.0 9.0 4.0 2.0 5.0 6.0 12.0 8.0 11.0 10.0 7.0 13.0 3.0\nood_covariate_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n10.0\n2.0\n5.0\n2.0\n4.0 5.0\n12.0\n6.0\n11.0\n6.0\n9.0\n1.0 ood_covariate_roc\n(d) CIFAR - ResNet\nFigure 46: Near OOD: Covariate - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\naum (1.4)dataiq (1.8)datamaps (2.8)cleanlab (4)conf_agree (5)allsh (6.4)prototypicality (7)\n(13) vog(12) el2n(11) loss(10) grand(8.8) forgetting(7.8) detector\n(a) MNIST - LeNet\n5 10\naum (1.4)dataiq (1.8)datamaps (2.8)conf_agree (4)\ncleanlab (5)detector (6.2)\n(12) vog(10) grand(10) el2n(9.6) loss(8) forgetting(6.8) allsh\n(b) MNIST - ResNet\n5 10\nallsh (1)conf_agree (2)dataiq (3.6)cleanlab (3.8)datamaps (4.6)aum (6)prototypicality (7.4)\n(13) vog(12) loss(11) el2n(10) grand(8.8) forgetting(7.8) detector\n(c) CIFAR - LeNet\n5 10\naum (1)conf_agree (2.6) cleanlab (3.2)dataiq (3.6)datamaps (4.6)allsh (6)\n(11) vog(11) el2n(10) loss(9.8) grand(8) forgetting(7) detector\n(d) CIFAR - ResNet\nFigure 47: Near OOD: Covariate - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.6 NEAR OOD: DOMAIN\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 11.0 2.0 4.0 9.0 6.0 7.0 13.0 3.0 12.0 5.0 8.0 10.0\n1.0 semantic_shift_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n11.0\n5.0\n7.0 7.0\n2.0 3.0\n12.0\n7.0\n10.0 9.0\n4.0\n1.0 semantic_shift_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 1.0 9.0 4.0 3.0 5.0 6.0 12.0 8.0 12.0 10.0 7.0 13.0 2.0\nsemantic_shift_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n9.0\n6.0\n1.0 2.0\n6.0 7.0\n12.0\n3.0\n11.0\n7.0\n10.0\n4.0\nsemantic_shift_roc\n(d) CIFAR - ResNet\nFigure 53: Near OOD: Domain - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\naum (1)dataiq (2.6)cleanlab (2.8)datamaps (3.6)conf_agree (5)prototypicality (6)allsh (7)\n(13) vog(12) el2n(11) loss(10) grand(9) forgetting(8) detector\n(a) MNIST - LeNet\n5 10\ndataiq (1)datamaps (2)aum (3)conf_agree (4.2)cleanlab (4.8)allsh (6)\n(12) vog(11) loss(10) el2n(9) grand(8) forgetting(7) detector\n(b) MNIST - ResNet\n5 10\nallsh (1)conf_agree (2)dataiq (3.4)cleanlab (4.2)datamaps (4.4)aum (6)prototypicality (7.6)\n(13) vog(12) loss(11) el2n(9.4) forgetting(8.8) grand(8.2) detector\n(c) CIFAR - LeNet\n5 10\ncleanlab (1)conf_agree (2.2) aum (2.8)dataiq (4.4)allsh (5.2)datamaps (5.4)\n(11) el2n(11) vog(10) loss(9.6) grand(8) forgetting(7) detector\n(d) CIFAR - ResNet\nFigure 54: Near OOD: Domain - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.7 FAR OOD\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 1.0 9.0 7.0 2.0 3.0 4.0 13.0 8.0 11.0 10.0 6.0 4.0 12.0\nfar_ood_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n1.0\n8.0\n6.0\n2.0 3.0\n4.0\n12.0\n7.0\n11.0\n9.0\n5.0\n10.0\nfar_ood_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 8.0 11.0 7.0 1.0 4.0 5.0 13.0 10.0 12.0 9.0 7.0 2.0 3.0\nfar_ood_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n6.0\n9.0\n5.0\n1.0 2.0\n3.0\n12.0\n8.0\n11.0 10.0\n6.0\n4.0\nfar_ood_roc\n(d) CIFAR - ResNet\nFigure 60: Far OoD - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\nallsh (2)conf_agree (2.2)dataiq (2.8)datamaps (3.8)prototypicality (4.4)cleanlab (5.8)aum (7)\n(13) loss(12) el2n(11) grand(10) forgetting(9) detector(8) vog\n(a) MNIST - LeNet\n5 10\nallsh (1)conf_agree (2)\ndataiq (3)datamaps (4)cleanlab (5)aum (6)\n(12) loss(11) el2n(10) grand(8.6) forgetting(7.8) detector(7.6) vog\n(b) MNIST - ResNet\n5 10\nconf_agree (1.2)prototypicality (1.8)vog (3)dataiq (4)datamaps (5)cleanlab (6.2)allsh (7)\n(13) loss(12) el2n(11) detector(10) forgetting(9) aum(7.8) grand\n(c) CIFAR - LeNet\n5 10\nconf_agree (1)dataiq (2)datamaps (3)cleanlab (4.6)vog (4.8)allsh (5.6)\n(12) loss(11) el2n(9.6) grand(9.2) forgetting(8.2) detector(7) aum\n(d) CIFAR - ResNet\nFigure 61: Far OoD - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.8 ATYPICAL: CROP SHIFT\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 11.0 2.0 3.0 9.0 5.0 6.0 13.0 4.0 12.0 8.0 7.0 1.0 7.0\ncrop_shift_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n11.0\n1.0\n4.0 5.0\n6.0 7.0\n12.0\n2.0\n10.0\n4.0\n8.0 9.0\ncrop_shift_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 9.0 7.0 4.0 1.0 2.0 3.0 13.0 6.0 12.0 11.0 5.0 8.0 10.0\ncrop_shift_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n10.0\n4.0\n2.0 1.0\n6.0 7.0\n12.0\n3.0\n9.0\n5.0\n8.0\n11.0\ncrop_shift_roc\n(d) CIFAR - ResNet\nFigure 67: Atypical: Crop Shift - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\nprototypicality (1)aum (2.8)dataiq (3.2)cleanlab (4.2)datamaps (4.2)conf_agree (5.6)\nallsh (7.6)\n(12) el2n(12) loss(11) grand(11) vog(8.8) forgetting(7.6) detector\n(a) MNIST - LeNet\n5 10\naum (1)dataiq (2)datamaps (3)conf_agree (4.2)cleanlab (4.8)allsh (6)\n(12) el2n(11) loss(11) grand(9) vog(8) forgetting(7) detector\n(b) MNIST - ResNet\n5 10\nconf_agree (1.4)dataiq (2.4)cleanlab (2.8)datamaps (3.4)aum (5.2)prototypicality (5.8)allsh (7.4)\n(13) loss(12) el2n(11) forgetting(9.6) detector(9) vog(8.8) grand\n(c) CIFAR - LeNet\n5 10\naum (1.2)cleanlab (2.4)conf_agree (2.4)dataiq (4)datamaps (5)vog (6.2)\n(12) el2n(11) grand(10) loss(8.8) forgetting(7.8) detector(7.2) allsh\n(d) CIFAR - ResNet\nFigure 68: Atypical: Crop Shift - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.5.9 ATYPICAL: ZOOM SHIFT\nHeatmaps.\nRankings.\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 12.0 5.0 7.0 10.0 3.0 4.0 13.0 6.0 11.0 8.0 5.0 1.0 9.0\nzoom_shift_roc\n(a) MNIST - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n1.0\n4.0\n7.0 8.0\n4.0 5.0\n12.0\n6.0\n11.0\n9.0\n6.0\n2.0\nzoom_shift_roc\n(b) MNIST - ResNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s\npro tot\nyp ica\nlityvo g\n1 3 5 7 9 11 13 Ra nk S co re s 7.0 8.0 2.0 1.0 3.0 4.0 12.0 6.0 13.0 10.0 5.0 11.0 9.0\nzoom_shift_roc\n(c) CIFAR - LeNet\nalls h au m\ncle an\nlab\ncon f_a\ngre e da tai q\nda tam\nap s de tec torel2 n\nfor ge\nttin g gra nd los s vo g\n1 3 5 7 9\n11R an\nk Sc\nor es\n11.0\n3.0 3.0\n6.0 7.0\n8.0\n12.0\n4.0\n10.0\n2.0\n9.0\n1.0 zoom_shift_roc\n(d) CIFAR - ResNet\nFigure 74: Atypical: Zoom Shift - AUROC. Ranking of HCMs.\nCD diagrams. 5 10\nprototypicality (1)aum (3)dataiq (3)cleanlab (4)datamaps (4)conf_agree (6)detector (7)\n(13) el2n(12) loss(11) grand(10) vog(9) allsh(8) forgetting\n(a) MNIST - LeNet\n2.5 5.0 7.5 10.0\ndataiq (2.8)\nallsh (3.4)\naum (3.6) cleanlab (3.8) datamaps (3.8) conf_agree (4)\n(11) vog\n(11) loss\n(10) el2n (10) grand (7.8) forgetting (6.8) detector\n(b) MNIST - ResNet\n5 10\ncleanlab (1)dataiq (2.4)conf_agree (3.2)datamaps (3.4)allsh (5)aum (6)vog (7)\n(13) loss(12) el2n(11) grand(9.8) forgetting(9.2) detector(8) prototypicality\n(c) CIFAR - LeNet\n5 10\naum (1.2)cleanlab (1.8)dataiq (3.6)conf_agree (3.8)datamaps (4.6)allsh (6)\n(12) vog(11) grand(10) el2n(9) loss(8) forgetting(7) detector\n(d) CIFAR - ResNet\nFigure 75: Atypical: Zoom Shift - AUPRC. Critical difference diagrams. Black lines connect methods that are not statistically significantly different.\nMatrix compare\nD.6 INDIVIDUAL RESULTS - TABULAR\nD.6.1 MISLABELING: UNIFORM\nHeatmaps.\nRankings.\nD.6.2 MISLABELING: ASYMMETRIC\nHeatmaps.\nRankings.\nD.6.3 MISLABELING: INSTANCE\nHeatmaps.\nRankings.\nD.6.4 NEAR OOD: COVARIATE\nHeatmaps.\nRankings.\nD.6.5 FAR OOD\nHeatmaps.\nRankings.\nD.6.6 ATYPICAL\nHeatmaps.\nRankings.\nD.7 ADDITIONAL RESULTS - MEDICAL X-RAY DATA\nOverview. Our main experiments are conducted on the MNIST and CIFAR-10 datasets. The reason is that it is important to have \"clean\" datasets for the purposes of ensuring a good simulation environment. e.g. we do not want the original data to already have mislabeling. We now extend our analysis of HCMs to an additional medical imaging dataset from the NIH (Majkowska et al., 2020). These contain chest x-rays which have been audited by multiple expert radiologists to ensure they are \"clean\". We repeat our experiments from the main paper and show the results below.\nWe highlight the following main takeaways which are maintained from the main paper experiments, showing the findings indeed are similar across datasets:\n\u2022 Hardness types vary in difficulty. Similarly to the main text, we find that different types of hardness are easier or harder to characterize. For instance, uniform mislabeling or FarOoD are much easier than data-specific hardness like Atypical. Hence, we still see the performance differences.\n\u2022 Learning dynamics-based methods with respect to output confidence are effective general-purpose HCMs. In selecting a general-purpose HCM, we find similar to the main text that HCMs that characterize samples using learning dynamics on the confidence \u2014 uncertainty-based methods, which use probabilities (DataMaps, Data-IQ) or logits (AUM), are the best performing in terms of AUPRC across the board.\n\u2022 HCMs typically used for computational efficiency are surprisingly uncompetitive. Similarly to the main text, we find that HCMs that leverage gradient changes (e.g. GraNd), typically used for selection to improve computational efficiency, fare well at low p. However, at higher p, they become notably less competitive compared to simpler and computationally cheaper methods.\n\u2022 HCMs should only be used when hardness proportions are low. In general, different HCMs have significantly reduced performance at higher proportions of hardness. This is expected as we get closer to 0.5 since it\u2019s harder to identify a clear difference between samples.\n\u2022 Individual HCMs within a broad \u201cclass\u201d of methods are NOT statistically different. We find from the critical difference diagrams that methods falling into the same class of characterization are not statistically significant from one another (based on the black connected lines), despite the minor performance differences between them. Hence, practitioners should select an HCM within the broad HCM class most suitable for the application.\n\u2022 Selecting an HCM based on the hardness is useful. We find that confidence is a good general-purpose tool if one doesn\u2019t know the hardness. However, if one knows the hardness, one can better select the HCM.\nOf course, there are subtle differences given the difference in datasets. As an example, the performance rankings exhibit higher variability. That said, this is a function of running fewer seeds compared to the main paper. The most important compared to raw ranking is the CD Diagram which provides an analysis of the statistical significance of the performance differences which the raw scores don\u2019t provide. In that case, despite the rankings being different, we see very many of the same HCMs from the main paper are similarly connected by black lines, indicating the performance differences are not statistically different.\nD.8 INSIGHTS: EFFECT OF SEVERITY OF PERTURBATION ON HARDNESS SCORES\nOverview. We ask what is the effect of the severity of the perturbation on the hardness score. Specifically, are samples with greater perturbations considered \u201charder\u201d than others by HCMs based on their scores.\nWe conducted an experiment for zoom shift, crop shift, and Near-OOD (covariate), where we have two severities of perturbation namely small and large. Specifically, for zoom (2x vs. 10x), crop shift (5 pixels vs. 20 pixels), and Near-OOD (std deviation of 0.5 vs. 2).\nWe report the results in Fig 99. The results show the percentage changes in hardness scores (in the direction of hardness, e.g. increase or decrease) for different HCMs for the different hardness types considered. In all cases, we see an increase in hardness score for increased severity, with some HCM scores showcasing greater sensitivity and changes than others. In particular, we see the gradient and loss-based approaches have the greatest changes as compared to the methods computing scores based on probabilities.\nThese results shed interesting insights into the sensitivities of different HCMS."
        }
    ],
    "title": "DISSECTING SAMPLE HARDNESS: A FINE-GRAINED ANALYSIS OF HARDNESS CHARACTERIZATION METHODS FOR DATA-CENTRIC AI",
    "year": 2024
}