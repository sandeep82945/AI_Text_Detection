{
    "abstractText": "Diffusion models are the de-facto approach for generating high-quality images and videos but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space, or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion (MDM), a novel framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a NestedUNet architecture where features and parameters for small scale inputs are nested within those of the large scales. In addition, MDM enables a progressive training schedule from lower to higher resolutions which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of our approach on various benchmarks, including class-conditioned image generation, high-resolution text-to-image, and text-to-video applications. Remarkably, we can train a single pixel-space model at resolutions of up to 1024 \u00d7 1024 pixels, demonstrating strong zero shot generalization using the CC12M dataset, which contains only 12 million images.",
    "authors": [],
    "id": "SP:ab306c731c70cb638a0ac6b8992c541c913aef45",
    "references": [
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman"
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro"
            ],
            "title": "ediffi: Text-to-image diffusion models with an ensemble of expert denoisers",
            "venue": "arXiv preprint arXiv:2211.01324,",
            "year": 2022
        },
        {
            "authors": [
                "Samy Bengio",
                "Oriol Vinyals",
                "Navdeep Jaitly",
                "Noam Shazeer"
            ],
            "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "arXiv preprint arXiv:2112.07945,",
            "year": 2021
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut"
            ],
            "title": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
            "year": 2021
        },
        {
            "authors": [
                "Hansheng Chen",
                "Jiatao Gu",
                "Anpei Chen",
                "Wei Tian",
                "Zhuowen Tu",
                "Lingjie Liu",
                "Hao Su"
            ],
            "title": "Singlestage diffusion nerf: A unified approach to 3d generation and reconstruction, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ting Chen"
            ],
            "title": "On the importance of noise scheduling for diffusion models",
            "venue": "arXiv preprint arXiv:2301.10972,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "ImageNet: A Large-scale Hierarchical Image Database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Emily Denton",
                "Arthur Szlam",
                "Rob Fergus"
            ],
            "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
            "year": 2015
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bjorn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Oran Gafni",
                "Adam Polyak",
                "Oron Ashual",
                "Shelly Sheynin",
                "Devi Parikh",
                "Yaniv Taigman"
            ],
            "title": "Makea-scene: Scene-based text-to-image generation with human priors. 2022",
            "venue": "doi: 10.48550/ARXIV",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Jiatao Gu",
                "Lingjie Liu",
                "Peng Wang",
                "Christian Theobalt"
            ],
            "title": "Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis",
            "venue": "arXiv preprint arXiv:2110.08985,",
            "year": 2021
        },
        {
            "authors": [
                "Jiatao Gu",
                "Shuangfei Zhai",
                "Yizhe Zhang",
                "Miguel Angel Bautista",
                "Josh Susskind"
            ],
            "title": "f-dm: A multistage diffusion model via progressive signal transformation",
            "venue": "arXiv preprint arXiv:2210.04955,",
            "year": 2022
        },
        {
            "authors": [
                "Jiatao Gu",
                "Alex Trevithick",
                "Kai-En Lin",
                "Josh Susskind",
                "Christian Theobalt",
                "Lingjie Liu",
                "Ravi Ramamoorthi"
            ],
            "title": "Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion",
            "venue": "arXiv preprint arXiv:2302.10109,",
            "year": 2023
        },
        {
            "authors": [
                "Yuwei Guo",
                "Ceyuan Yang",
                "Anyi Rao",
                "Yaohui Wang",
                "Yu Qiao",
                "Dahua Lin",
                "Bo Dai"
            ],
            "title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning",
            "venue": "arXiv preprint arXiv:2307.04725,",
            "year": 2023
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.02303,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Chitwan Saharia",
                "William Chan",
                "David J Fleet",
                "Mohammad Norouzi",
                "Tim Salimans"
            ],
            "title": "Cascaded diffusion models for high fidelity image generation",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey A Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "In ICLR Workshop on Deep Generative Models for Highly Structured Data,",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Jonathan Heek",
                "Tim"
            ],
            "title": "Salimans. simple diffusion: End-to-end diffusion for high resolution images",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Allan Jabri",
                "David Fleet",
                "Ting Chen"
            ],
            "title": "Scalable adaptive computation for iterative generation",
            "venue": "arXiv preprint arXiv:2212.11972,",
            "year": 2022
        },
        {
            "authors": [
                "Zahra Kadkhodaie",
                "Florentin Guth",
                "St\u00e9phane Mallat",
                "Eero P Simoncelli"
            ],
            "title": "Learning multi-scale local conditional probability models of images",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "A\u00e4ron van den Oord",
                "Karen Simonyan",
                "Ivo Danihelka",
                "Oriol Vinyals",
                "Alex Graves",
                "Koray Kavukcuoglu"
            ],
            "title": "Video pixel networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Minguk Kang",
                "Jun-Yan Zhu",
                "Richard Zhang",
                "Jaesik Park",
                "Eli Shechtman",
                "Sylvain Paris",
                "Taesung Park"
            ],
            "title": "Scaling up gans for text-to-image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "arXiv preprint arXiv:1710.10196,",
            "year": 2017
        },
        {
            "authors": [
                "Xiang Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy S Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "Diffusionlm improves controllable text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick"
            ],
            "title": "Microsoft COCO: Common Objects in Context",
            "venue": "European Conference on Computer Vision, pp",
            "year": 2014
        },
        {
            "authors": [
                "Haohe Liu",
                "Zehua Chen",
                "Yi Yuan",
                "Xinhao Mei",
                "Xubo Liu",
                "Danilo Mandic",
                "Wenwu Wang",
                "Mark D Plumbley"
            ],
            "title": "Audioldm: Text-to-audio generation with latent diffusion models",
            "venue": "arXiv preprint arXiv:2301.12503,",
            "year": 2023
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Rundi Wu",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Sergey Zakharov",
                "Carl Vondrick"
            ],
            "title": "Zero-1-to-3: Zero-shot one image to 3d object, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Nal Kalchbrenner",
                "Oriol Vinyals",
                "Lasse Espeholt",
                "Alex Graves",
                "Koray Kavukcuoglu"
            ],
            "title": "Conditional Image Generation with PixelCNN Decoders",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Oriol Vinyals",
                "Koray Kavukcuoglu"
            ],
            "title": "Neural Discrete Representation Learning",
            "year": 2017
        },
        {
            "authors": [
                "William Peebles",
                "Saining Xie"
            ],
            "title": "Scalable diffusion models with transformers",
            "venue": "arXiv preprint arXiv:2212.09748,",
            "year": 2022
        },
        {
            "authors": [
                "Dustin Podell",
                "Zion English",
                "Kyle Lacey",
                "Andreas Blattmann",
                "Tim Dockhorn",
                "Jonas M\u00fcller",
                "Joe Penna",
                "Robin Rombach"
            ],
            "title": "Sdxl: improving latent diffusion models for high-resolution image synthesis",
            "year": 1952
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "arXiv preprint arXiv:2103.00020,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Severi Rissanen",
                "Markus Heinonen",
                "Arno Solin"
            ],
            "title": "Generative modelling with inverse heat dissipation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-Net : Convolutional Networks for Biomedical Image Segmentation",
            "venue": "International Conference on Medical Image Computing and ComputerAssisted Intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Chitwan Saharia",
                "Jonathan Ho",
                "William Chan",
                "Tim Salimans",
                "David J Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Image super-resolution via iterative refinement",
            "year": 2021
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Jonathan Ho"
            ],
            "title": "Progressive distillation for fast sampling of diffusion models",
            "venue": "arXiv preprint arXiv:2202.00512,",
            "year": 2022
        },
        {
            "authors": [
                "Uriel Singer",
                "Adam Polyak",
                "Thomas Hayes",
                "Xi Yin",
                "Jie An",
                "Songyang Zhang",
                "Qiyuan Hu",
                "Harry Yang",
                "Oron Ashual",
                "Oran Gafni",
                "Devi Parikh",
                "Sonal Gupta",
                "Yaniv Taigman"
            ],
            "title": "Make-a-video: Text-to-video generation without text-video data, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "A\u00e4ron Van Den Oord",
                "Nal Kalchbrenner",
                "Koray Kavukcuoglu"
            ],
            "title": "Pixel recurrent neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Dirk Weissenborn",
                "Oscar T\u00e4ckstr\u00f6m",
                "Jakob Uszkoreit"
            ],
            "title": "Scaling autoregressive video models",
            "year": 2020
        },
        {
            "authors": [
                "Chenfei Wu",
                "Jian Liang",
                "Lei Ji",
                "Fan Yang",
                "Yuejian Fang",
                "Daxin Jiang",
                "Nan Duan"
            ],
            "title": "N\u00fcwa: Visual synthesis pre-training for neural visual world creation, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Zeyue Xue",
                "Guanglu Song",
                "Qiushan Guo",
                "Boxiao Liu",
                "Zhuofan Zong",
                "Yu Liu",
                "Ping Luo"
            ],
            "title": "Raphael: Text-to-image generation via large mixture of diffusion paths",
            "venue": "arXiv preprint arXiv:2305.18295,",
            "year": 2023
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan",
                "Ben Hutchinson",
                "Wei Han",
                "Zarana Parekh",
                "Xin Li",
                "Han Zhang",
                "Jason Baldridge",
                "Yonghui Wu"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation",
            "year": 2022
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Jiatao Gu",
                "Zhuofeng Wu",
                "Shuangfei Zhai",
                "Josh Susskind",
                "Navdeep Jaitly"
            ],
            "title": "Planner: Generating diversified paragraph via latent language diffusion model",
            "venue": "arXiv preprint arXiv:2306.02531,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Nichol & Dhariwal, 2021; Song et al., 2020) have become increasingly popular tools for generative applications, such as image (Dhariwal\n& Nichol, 2021; Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022), video (Ho et al., 2022c;a), 3D (Poole et al., 2022; Gu et al., 2023; Liu et al., 2023b; Chen et al., 2023), audio (Liu et al., 2023a), and text (Li et al., 2022; Zhang et al., 2023) generation. However scaling them to highresolution still presents significant challenges as the model must re-encode the entire high-resolution input for each step (Kadkhodaie et al., 2022). Tackling these challenges necessitates the use of deep architectures with attention blocks which makes optimization harder and uses more resources.\nRecent works (Jabri et al., 2022; Hoogeboom et al., 2023) have focused on efficient network architectures for high-resolution images. However, none of the existing methods have shown competitive results beyond 512\u00d7 512, and their quality still falls behind the main-stream cascaded/latent based methods. For example, DALL-E 2 (Ramesh et al., 2022), IMAGEN (Saharia et al., 2022) and eDiffI (Balaji et al., 2022) save computation by learning a low-resolution model together with multiple super-resolution diffusion models, where each component is trained separately. On the other hand, latent diffusion methods (LDMs) (Rombach et al., 2022; Peebles & Xie, 2022; Xue et al., 2023) only learn low-resolution diffusion models, while they rely on a separately trained high-resolution autoencoder (Oord et al., 2017; Esser et al., 2021). In both cases, the multi-stage pipeline complicates training & inference, often requiring careful tuning of hyperparameters.\nIn this paper, we present Matryoshka Diffusion Models (MDM), a novel family of diffusion models for high-resolution synthesis. Our main insight is to include the low-resolution diffusion process as part of the high-resolution generation, taking similar inspiration from multi-scale learning in GANs (Karras et al., 2017; Chan et al., 2021; Kang et al., 2023). We accomplish this by performing a joint diffusion process over multiple resolution using a Nested UNet architecture ( (see Fig. 2 and Fig. 3). Our key finding is that MDM, together with the Nested UNets architecture, enables 1) a multiresolution loss that greatly improves the speed of convergence of high-resolution input denoising and 2) an efficient progressive training schedule, that starts by training a low-resolution diffusion model and gradually adds high-resolution inputs and outputs following a schedule. Empirically, we found that the multi-resolution loss together with progressive training allows one to find an excellent balance between the training cost and the model\u2019s quality.\nWe evaluate MDM on class conditional image generation, and text conditioned image and video generation. MDM allows us to train high-resolution models without resorting to cascaded or latent diffusion. Ablation studies show that both multi-resolution loss and progressive training greatly boost training efficiency and quality. In addition, MDM yield high performance text-to-image generative models with up to 10242 resolution, trained on the reasonably small CC12M dataset. Lastly, MDM generalize gracefully to video generation, suggesting generality of our approach."
        },
        {
            "heading": "2 Diffusion Models",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are latent variable models given a pre-defined posterior distribution (named the forward diffusion process), and trained with a denoising objective. More specifically, given a data point x \u2208 RN and a fixed signal-noise schedule {\u03b1t, \u03c3t}t=1,...,T , we define a sequence of latent variables {zt}t=0,...,T that satisfies:\nq(zt|x) = N (zt;\u03b1tx, \u03c32t I), and q(zt|zs) = N (zt;\u03b1t|szs, \u03c32t|sI), (1)\nwhere z0 = x, \u03b1t|s = \u03b1t/\u03b1s, \u03c32t|s = \u03c3 2 t \u2212 \u03b12t|s\u03c3 2 s , s < t. By default, the signal-to-noise ratio (SNR, \u03b12t /\u03c32t ) decreases monotonically with t. The model then learns to reverse the process with a backward model p\u03b8(zt\u22121|zt), which can be re-written as a denoising objective:\nL\u03b8 = Et\u223c[1,T ],zt\u223cq(zt|x) [ \u03c9t \u00b7 \u2225x\u03b8(zt, t)\u2212 x\u222522 ] ,\nwhere x\u03b8(zt, t) is a neural network (often a variant of a UNet model (Ronneberger et al., 2015)) that maps a noisy input zt to its clean version x, conditioned on the time step t; \u03c9t \u2208 R+ is a loss weighting factor determined by heuristics. In practice, one can reparameterize x\u03b8 with noiseor v-prediction (Salimans & Ho, 2022) for improved performance. Unlike other generative models like GANs (Goodfellow et al., 2014), diffusion models require repeatedly applying a deep neural network x\u03b8 in the ambient space as enough computation with global interaction is critical for denoising (Kadkhodaie et al., 2022). This makes it challenging to design efficient diffusion models directly for high-resolution generation, especially for complex tasks like text-to-image synthesis. As common solutions, existing methods have focused on learning hierarchical generation:\nCascaded diffusion (Ho et al., 2022b; Ramesh et al., 2022; Saharia et al., 2022; Ho et al., 2022a) utilize a cascaded approach where a first diffusion model is used to generate data at lower resolution, and then a second diffusion model is used to generate a super-resolution version of the initial generation, taking the first stage generation as conditioning. Cascaded models can be chained multiple times until they reach the final resolution. Ho et al. (2022a); Singer et al. (2022) uses a similar approach for video synthesis as well \u2013 models are cascaded from low spatio-temporal resolution to high spatio-temporal resolution. However, since each model is trained separately, the generation quality can be bottlenecked by the exposure bias (Bengio et al., 2015) from imperfect predictions and several models need to be trained corresponding to different resolutions.\nLatent diffusion (LDM, Rombach et al., 2022) and its follow-ups (Peebles & Xie, 2022; Xue et al., 2023; Podell et al., 2023), on the other hand, handle high-resolution image generation by performing diffusion in the lower resolution latent space of a pre-trained auto-encoder, which is typically trained with adversarial objectives (Esser et al., 2021). This not only increases the complexity of learning, but bounds the generation quality due to the lossy compression process.\nEnd-to-end models Recently, several approaches have been proposed (Hoogeboom et al., 2023; Jabri et al., 2022; Chen, 2023) to train end-to-end models directly on high-resolution space. Without relying on separate models, these methods focus on efficient network design as well as shifted noise schedule to adapt high-resolution spaces. Nevertheless, without fully considering the innate structure of hierarchical generation, their results lag behind cascaded and latent models."
        },
        {
            "heading": "3 Matryoshka Diffusion Models",
            "text": "In this section, we present Matryoshka Diffusion Models (MDM), a new class of diffusion models that is trained in high-resolution space, while exploiting the hierarchical structure of data formation. MDM first generalizes standard diffusion models in the extended space (\u00a7 3.1), for which specialized nested architectures (\u00a7 3.2) and training procedures (Appendix B) are proposed."
        },
        {
            "heading": "3.1 Diffusion Models in Extended Space",
            "text": "Unlike cascaded or latent methods, MDM learns a single diffusion process with hierarchical structure by introducing a multi-resolution diffusion process in an extended space. An illustration is shown in Fig. 2. Given a data point x \u2208 RN , we define time-dependent latent zt = [ z1t , . . . ,z R t ] \u2208 RN1+...NR . Similar to Eq. (1), for each zr, r = 1, . . . , R:\nq(zrt |x) = N (zrt ;\u03b1rtDr(x), \u03c3rt 2I), (2)\nwhere Dr : RN \u2192 RNr is a deterministic \u201cdown-sample\u201d operator depending on the data. Here, Dr(x) is a coarse / lossy-compressed version of x. For instance, Dr(.) can be avgpool(.) for generating low-resolution images.\nBy default, we assume compression in a progressive manner such that N1 < N2 . . . < NR = N and DR(x) = x. Also, {\u03b1rt , \u03c3rt } are the resolution-specific noise schedule. In this paper, we follow Gu et al. (2022) and shift the noise schedule based on the input resolutions. MDM then learns\nthe backward process p\u03b8(zt\u22121|zt) with R neural denoisers xr\u03b8(zt). Each variable zrt\u22121 depends on all resolutions {z1t . . . zRt } at time step t. During inference, MDM generates all R resolutions in parallel. There is no dependency between zrt .\nModeling diffusion in the extended space has clear merits: (1) since what we care during inference is the full-resolution output zRt , all other intermediate resolutions are treated as additional hidden variables zrt , enriching the complexity of the modeled distribution;(2) the multi-resolution dependency opens up opportunities to share weights and computations across zrt , enabling us to re-allocate computation in a more efficient manner for both training and inference efficiency."
        },
        {
            "heading": "3.2 NestedUNet Architecture",
            "text": "Similar to typical diffusion models, we implement MDM in the flavor of UNet (Ronneberger et al., 2015; Nichol & Dhariwal, 2021): skip-connections are used in parallel with a computation block to preserve fine-grained input information, where the block consists of multi-level convolution and self-attention layers. In MDM, under the progressive compression assumption, it is natural that the computation for zrt is also beneficial for z r+1 t . This leads us to propose NestedUNet, an architecture that groups the latents of all resolutions {zrt } in one denoising function as a nested structure, where low resolution latents will be fed progressively along with standard down-sampling. Such multi-scale computation sharing greatly eases the learning for high-resolution generation. A pseudo code for NestedUNet compared with standard UNet is present as follows.\nAside from the simplicity aspect relative to other hierarchcal approaches, NestedUNet also allows to allocate the computation in the most efficient manner. As shown in Fig. 3, our early exploration found that MDM achieved much better scalibility when allocating most of the parameters & computation in the lowest resolution. Similar findings have also been shown in Hoogeboom et al. (2023)."
        },
        {
            "heading": "3.3 Learning",
            "text": "We train MDM using the normal denoising objective jointly at multiple resolutions, as follows:\nL\u03b8 = Et\u223c[1,T ]Ezt\u223cq(zt|x) R\u2211\nr=1\n[ \u03c9rt \u00b7 \u2225xr\u03b8(zt, t)\u2212Dr(x)\u222522 ] , (3)\nwhere \u03c9rt is the resolution-specific weighting, and by default we set \u03c9rt /\u03c9Rt = NR/Nr.\nProgressive Training While MDM can be trained end-to-end directly following Eq. (3) which has already shown better convergence than naive baselines, we found a simple progressive training technique, similarly proposed in GAN literature (Karras et al., 2017; Gu et al., 2021), greatly speeds up the training of high-resolution models w.r.t. wall clock time. More precisely, we divide up the training into R phases, where we progressively add higher resolution into the training objective in Eq. (3). This is equivalent to learning a sequence of MDMs on [z1t , . . . zrt ] until r reaching the final resolution. Thanks to the proposed architecture, we can achieve the above trivially as if progressive growing the networks (Karras et al., 2017). This training scheme avoids the costly high-resolution training from the beginning, and speeds up the overall convergence."
        },
        {
            "heading": "4 Experiments",
            "text": "MDM is a versatile technique applicableto any problem where input dimensionality can be progressively compressed. We consider two applications beyond class-conditional image generation that demonstrate the effectiveness of our approach \u2013 text-to-image and text-to-video generation."
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets In this paper, we only focus on datasets that are publicly available and easily reproducible. For image generation, we performed class-conditioned generation on ImageNet (Deng et al., 2009) at 256\u00d7256, and performed general purpose text-to-image generation using Conceptual 12M (CC12M, Changpinyo et al., 2021) at both 256 \u00d7 256 and 1024 \u00d7 1024 resolutions. As additional evidence of generality, we show results on text-to-video generation using WebVid-10M (Bain et al., 2021) at 16\u00d7 256\u00d7 256. We list the dataset and preprocessing details in Appendix F. The choice of relying extensively on CC12M for text-to-image generative models in the paper is a significant departure from prior works (Saharia et al., 2022; Ramesh et al., 2022) that rely on exceedingly large and sometimes inaccessible datasets, and so we address this choice here. We find that CC12M is sufficient for building high-quality text-to-image models with strong zero-shot capabilities in a relatively short training time (see details in Appendix D.2). This allows for a much more consistent comparison of methods for the community because the dataset is freely available and training time is feasible. We submit here, that CC12M is much more amenable as a common training and evaluation baseline for the community working on this problem.\nEvaluation In line with prior works, we evaluate our image generation models using Fre\u0301chet Inception Distance (FID, Heusel et al., 2017) (ImageNet, CC12M) and CLIP scores (Radford et al., 2021) (CC12M). To examine their zero-shot capabilities, we also report the FID/CLIP scores using COCO (Lin et al., 2014) validation set togenerate images with the CC12M trained models. We also provide additional qualitative samples for image and video synthesis in supplementary materials.\nImplementation details We implement MDMs based on the proposed NestedUNet architecture, with the innermost UNet resolution set to 64\u00d7 64. Similar to Podell et al. (2023), we shift the bulk of self-attention layers to the lower-level (16 \u00d7 16) features, resulting in total 450M parameters for the inner UNet. As described in \u00a7 3.2, the high-resolution part of the model can be easily attached on top of previous level of the NestedUNet, with a minimal increase in the parameter count. For text-to-image and text-to-video models, we use the frozen FLAN-T5 XL (Chung et al., 2022) as our text encoder due to its moderate size and performance for language encoding. Additionally, we apply two learnable self-attention layers over the text representation to enhance text-image alignment.\nFor image generation tasks, we experiment with MDMs of {642, 2562}, {642, 1282, 2562} for 256 \u00d7 256, and {642, 2562, 10242}, {642, 1282, 2562, 5122, 10242} for 1024 \u00d7 1024, respectively. For video generation, MDM is nested by the same image 64 \u00d7 64 UNet with additional attention layers for learning temporal dynamics. The overall resolution is {642, 16 \u00d7 642, 16 \u00d7 2562}. We use bi-linear interpolation for spatial Dr(.), and first-frame indexing for temporal Dr(.). Unless specified, we apply progressive and mixed-resolution training for all MDMs. We use 8 A100 GPUs for ImageNet, and 32 A100 GPUs for CC12M and WebVid-10M, respectively. See Appendices A and B for more implementation hyper-parameters and training details.\nBaseline models Aside from the comparisons with existing state-of-the-art approaches, we also report detailed analysis on MDMs against three baseline models under controlled setup:\n1. Simple DM: A standard UNet architecture directly applied to high resolution inputs; We also consider the Nested UNet architecture, but ignoring the low resolution losses; Both cases are essentially identical to recent end-to-end diffusion models like Hoogeboom et al. (2023).\n2. Cascaded DM: we follow the implementation details of Saharia et al. (2022) and train a CDM that is directly comparable with MDM where the upsampler has an identical configuration to our NestedUNet. We also apply noise augmentation to the low resolution conditioning image, and sweep over the optimal noise level during inference.\n3. Latent DM: we utilize the latent codes derived from the auto-encoders from Rombach et al. (2022),and subsequently train diffusion models that match the dimensions of the MDM UNet.\n4.2 Main Results\nComparison with baseline approaches Our comparisons to baselines are shown in Fig. 4. On ImageNet 256 \u00d7 256, we select a standard UNet our simple DM baseline. For the Cascaded DM baseline, we pretrain a 64x64 diffusion model for 200K iterations, and apply an upsampler UNet also in the same size. We apply standard noise augmentation and sweep for the optimal noise level during inference time (which we have found to be critical). For LDM experiments, we use pretrained autoencoders from Rombach et al. (2022) which downsamples the input resolution and we use the same architecture for these experiments as our 64x64 low resolution models. For MDM variants, we use a NestedUNet of the same size as the baseline UNet. We experiment with two variants, one trained directly with the multi resolution loss Eq. (3) (denoted as no PT), and another one resuming from the 64x64 diffusion model (ie, progressive training). CC12M 256x256 follows a similar setting, except that we use a single loss NestedUNet as our simple DM architecture. We monitor the\nFID curve on ImageNet, and the FID and CLIP curves on CC12M.\nComparing simple DM to MDM, we see that MDM clearly has faster convergence, and reaches better performance in the end. This suggests that the multi resolution diffusion process together with the multi resolution loss effectively improves the models convergence, with negligible added complexities. When following the progressive training schedule, we see that MDM\u2019s performance and convergence speed further improves. As a direct comparison, we see that the Cascaded DM baseline significantly underperforms MDM, while both starting from the same 64x64 model. Note\nthat this is remarkable because Cascaded DM has more combined parameters than MDM (because MDM has extensive parameter sharing across resolutions), and uses twice as many inference steps. We hypothesize that the inferior performance of Cascaded DM is largely due to the fact that our 64x64 is not aggressively trained, which causes a large gap between training and inference wrt the conditioning inputs. Lastly, compared to LDM, MDM also shows better performance. Although this is a less direct control as LDM is indeed more efficient due to its small input size, but MDM features a simpler training and inference pipeline.\nComparison with literature In Table 1, MDM is compared to existing approaches in literature, where we report FID-50K for ImageNet 256x256 and zero shot FID-30K on MSCOCO. On ImageNet, for which our architecture and hyperparameters are not optimized, MDM is able to achieve competitive FID of 3.51 with CFG. Our FID results comparable to the literature, although MDM is trained on significantly less data than the baselines like Imagen and Dalle-2.\nQualitative Results We show random samples from the trained MDMs on for image generation (ImageNet 256\u00d7256, Fig. 5), text-to-image (CC12M, 1024\u00d71024Fig. 6) and text-to-video (WebVid10M, Fig. 7). Despite training on relatively small datasets, MDMs show strong zero-shot capabilities of generating high-resolution images and videos. Note that we use the same training pipelines for all three tasks, indicating its versatile abilities of handling various data types."
        },
        {
            "heading": "4.3 Ablation Studies",
            "text": "Effects of progressive training We experiment with the progressive training schedule, where we vary the number of iterations that the low-resolution model is trained on before continuing on the target resolution (Fig. 8a). We see that more low resolution training clearly benefits that of the high-resolution FID curves. Note that training on low resolution inputs is much more efficient w.r.t. both memory and time complexity, progressive training provides a straightforward option for finding the best computational trade-offs during training.\nEffects of nested levels Next, we compare the performance of using different number of nested resolutions with experiments on CC12M. The result is shown in Fig. 8b. We see that increasing from two resolution levels to three consistently improves the model\u2019s convergence. It\u2019s also worth noting that increasing the number of nesting levels brings only negligible costs.\nCLIP-FID trade-off Lastly, we show in Fig. 8c the pereto curve of CLIP-FID on the zero-shot evaluation of COCO, achieved by varying the classifier free guidance (CFG) weight. MDM is similarly amendable to CFG as other diffusion model variants. As a comparison, we overlap the same plot reported by Imagen (Figure A.11). We see that Imagen in general demonstrates smaller FID, which we attribute it to higher diversity as a result of training on a large dataset. However, MDM demonstrates strong CLIP score, whereas we have found in practice that such high CLIP scores correlate very well with the visual quality of the generated images."
        },
        {
            "heading": "5 Related Work",
            "text": "In addition to diffusion methods covered in \u00a7 2, multiscale models have been widely used in image generation. A well-known Generative Adversarial Network (GAN) is the LAPGAN model (Denton et al., 2015) which generates lower-resolution images using lower-resolution models, that are subsequently fed into higher-resolution models to produce higher resolution images. Autoregressive models have also been applied for generation \u2013 from early works such as PixelCNN (Oord et al., 2016) and PixelRNN (Van Den Oord et al., 2016) and videos (Kalchbrenner et al., 2017; Weissenborn et al., 2020), to more recent text-to-image models(Gafni et al., 2022; Yu et al., 2022) and text to video models(Wu et al., 2021; Singer et al., 2022). While earlier works often operate in pixel space, recent works, such as Parti(Yu et al., 2022) and MakeAScene(Gafni et al., 2022) use autoencoders to preprocess images into discrete latent features which can be modeled autoregressively using large sequence-to-sequence models based on transformers. f-DM (Gu et al., 2022) proposed a generalized framework enabling progressive signal transformation across multiple scales, and derived a corresponding de-noising scheduler to transit from multiple resolution stages. This scheduler is employed in our work. Similarly, IHDM (Rissanen et al., 2023) does coarse-to-fine generation end-to-end, by reversing the heat equation, where resolution increase is implicit."
        },
        {
            "heading": "6 Discussions and Future Directions",
            "text": "In this paper we showed that sharing representations across different resolutions can lead to faster training with high quality results, when lower resolutions are trained first. We believe this is because the model is able to exploit the correlations across different resolutions more effectively, both spatially and temporally. While we explored only a small set of architectures here, we expect more improvements can be achieved from a more detailed exploration of weight sharing architectures, and new ways of distributing parameters across different resolutions in the current architecture. Another unique aspect of our work is the use of an augmented space, where denoising is performed over multiple resolutions jointly. In this formulation resolution over time and space are treated in the same way, with the differences in correlation structure in time and space being learned by different parameters of the weight sharing model. A more general way of conceptualizing the joint optimization over multiple resolutions is to decouple the losses at different resolutions, by weighting them differently. It is conceivable that a smooth transition can be achieved from training on lower to higher resolution. We also note that while we have compared our approach to LDM in the paper, these methods are complementary. It is possible to build MDM on top of autoencoder codes. While we are not making the claim that the MDM based models are reaching the SOTA, we leave the evaluation of MDM on large scale dataset and model sizes as future work."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Architectures",
            "text": "First, we show the following as the core architecture for MDM for the lowest resolution of 64\u00d7 64. Following (Podell et al., 2023), we increase the number of self-attention layers for each resnet blocks for 16 \u00d7 16 computations. To improve the text-image correspondence, we found it useful to apply additional self-attention layers on top of the language model features."
        },
        {
            "heading": "Base architecture (MDM-S64)",
            "text": "config: resolutions=[64,32,16] resolution_channels=[256,512,768] num_res_blocks=[2,2,2] num_attn_layers_per_block=[0,1,5] num_heads=8, schedule=\u2019cosine\u2019 emb_channels=1024, num_lm_attn_layers=2, lm_feature_projected_channels=1024\nThen, we configure the models for 2562 and 10242 resolutions in a nested way as follows:"
        },
        {
            "heading": "Nested architecture (MDM-S64S256)",
            "text": "config: resolutions=[256,128,64] resolution_channels=[64,128,256] inner_config:\nresolutions=[64,32,16] resolution_channels=[256,512,768] num_res_blocks=[2,2,2] num_attn_layers_per_block=[0,1,5] num_heads=8, schedule=\u2019cosine\u2019\nnum_res_blocks=[2,2,1] num_attn_layers_per_block=[0,0,0] schedule=\u2019cosine-shift4\u2019 emb_channels=1024, num_lm_attn_layers=2, lm_feature_projected_channels=1024"
        },
        {
            "heading": "Nested architecture (MDM-S64S128S256)",
            "text": "Architecture config (MDM-64,128,256): resolutions=[256,128] resolution_channels=[64,128] inner_config:\nresolutions=[128,64] resolution_channels=[128,256] inner_config:\nresolutions=[64,32,16] resolution_channels=[256,512,768] num_res_blocks=[2,2,2] num_attn_layers_per_block=[0,1,5] num_heads=8, schedule=\u2019cosine\u2019\nnum_res_blocks=[2,1] num_attn_layers_per_block=[0,0] schedule=\u2019cosine-shift2\u2019\nnum_res_blocks=[2,1] num_attn_layers_per_block=[0,0] schedule=\u2019cosine-shift4\u2019 emb_channels=1024, num_lm_attn_layers=2, lm_feature_projected_channels=1024"
        },
        {
            "heading": "Nested architecture (MDM-S64S256S1024)",
            "text": "config: resolutions=[1024,512,256] resolution_channels=[32,32,64] inner_config:\nresolutions=[256,128,64] resolution_channels=[64,128,256] inner_config:\nresolutions=[64,32,16] resolution_channels=[256,512,768] num_res_blocks=[2,2,2] num_attn_layers_per_block=[0,1,5] num_heads=8, schedule=\u2019cosine\u2019\nnum_res_blocks=[2,2,1] num_attn_layers_per_block=[0,0,0] schedule=\u2019cosine-shift4\u2019\nnum_res_blocks=[2,2,1] num_attn_layers_per_block=[0,0,0] schedule=\u2019cosine-shift16\u2019 emb_channels=1024, num_lm_attn_layers=2, lm_feature_projected_channels=1024\nNested architecture (MDM-S64S128S256S512S1024)\nconfig: resolutions=[1024,512] resolution_channels=[32,32] inner_config:\nresolutions=[512,256] resolution_channels=[32,64] inner_config:\nresolutions=[256,128] resolution_channels=[64,128] inner_config:\nresolutions=[128,64] resolution_channels=[128,256] inner_config:\nresolutions=[64,32,16] resolution_channels=[256,512,768] num_res_blocks=[2,2,2] num_attn_layers_per_block=[0,1,5] num_heads=8, schedule=\u2019cosine\u2019\nnum_res_blocks=[2,1] num_attn_layers_per_block=[0,0] schedule=\u2019cosine-shift2\u2019\nnum_res_blocks=[2,1] num_attn_layers_per_block=[0,0] schedule=\u2019cosine-shift4\u2019\nnum_res_blocks=[2,1] num_attn_layers_per_block=[0,0] schedule=\u2019cosine-shift8\u2019}\nnum_res_blocks=[2,1] num_attn_layers_per_block=[0,0] schedule=\u2019cosine-shift16\u2019 emb_channels=1024, num_lm_attn_layers=2, lm_feature_projected_channels=1024\nIn addition, we also show the models for video generation experiments, where additional temporal attention layer is performed across the temporal dimension connected with convolution-based re-\nsampling. An illustration of the architecture of video modeling is shown in Fig. 10. For ease of visualization, we use 4 frames instead of 16 which was used in our main experiments.\nNested architecture (MDM-S64T16) for video generation config:\ntemporal_axis=True temporal_resolutions=[16,8,4,2,1] resolution_channels=[256,256,256,256,256] inner_config:\nresolutions=[64,32,16] resolution_channels=[256,512,768] num_res_blocks=[2,2,2] num_attn_layers_per_block=[0,1,5] num_heads=8, schedule=\u2019cosine\u2019\nnum_res_blocks=[2,2,2,2,1] num_attn_layers_per_block=[0,0,0,0,0] num_temporal_attn_layers_per_block=[1,1,1,1,0] schedule=\u2019cosine-shift4\u2019 emb_channels=1024, num_lm_attn_layers=2, lm_feature_projected_channels=1024\nNested architecture (MDM-S64T16S256) for video generation config:\nresolutions=[256,128,64] resolution_channels=[64,128,256] inner_config:\ntemporal_axis=True temporal_resolutions=[16,8,4,2,1] resolution_channels=[256,256,256,256,256] inner_config:\nresolutions=[64,32,16] resolution_channels=[256,512,768] num_res_blocks=[2,2,2] num_attn_layers_per_block=[0,1,5] num_heads=8, schedule=\u2019cosine\u2019\nnum_res_blocks=[2,2,2,2,1] num_attn_layers_per_block=[0,0,0,0,0] num_temporal_attn_layers_per_block=[1,1,1,1,0] schedule=\u2019cosine-shift4\u2019\nnum_res_blocks=[2,2,1] num_attn_layers_per_block=[0,0,0] schedule=\u2019cosine-shift16\u2019 emb_channels=1024, num_lm_attn_layers=2, lm_feature_projected_channels=1024"
        },
        {
            "heading": "B Training details",
            "text": "For all experiments, we share all the following training parameters except the batch size and training steps differ across different experiments. default training config:\noptimizer=\u2019adam\u2019 adam_beta1=0.9 adam_beta2=0.99 adam_eps=1.e-8 learning_rate=1e-4 learning_rate_warmup_steps=30_000 weight_decay=0.0\ngradient_clip_norm=2.0 ema_decay=0.9999 mixed_precision_training=bp16\nFor ImageNet experiments, the progressive training setting is set default without specifying:\nprogressive training config: target_resolutions=[64,256] batch_size=[512,256] training_steps=[300K,500K]\nFor text-to-image generation on CC12M, we test on both 256 \u00d7 256 and 1024 \u00d7 1024 resolutions, while each resolution two types of models with various nesting levels are tested. Note that, the number of progressive training stages is not necessarily the same the actual nested resolutions in the model. For convenience, we always directly initialize the training of 1024\u00d7 1024 training with the trained model for 256\u00d7 256. Therefore, we can summarize all experiments into one config:\nprogressive training config: target_resolutions=[64,256,1024(optional)] batch_size=[2048,1024,768] training_steps=[500K,500K,100K]\nSimilarly, we list the training config for the video generation experiments as follows.\nprogressive training config: target_resolutions=[64,16x64,16x256] batch_size=[2048,512,128] training_steps=[500K,500K,300K]\nC Inference details\nIn Fig. 11, we demonstrate the typical sampling process of a trained MDM. We start by sampling independent Gaussian noises for each resolution which gives us {zrT }r=1,...,R. We then pass all the noisy images to the Nested UNet in parallel, which yields the denoised outputs {f(z)rT }r=1,...,R. We then perform one step of denoising for each {zrT } with {f(z)rT }, following the procedure of a standard diffusion model, and this gives us {zrT\u22121}r=1,...,R. In practice, we set the number of inference steps as 250 and use v-prediction (Salimans & Ho, 2022) as our model parameterization. Similar to Saharia et al. (2022), we apply \u201cdynamic thresholding\u201d to avoid over-satruation problem in the pixel predictions."
        },
        {
            "heading": "D Additional Ablations",
            "text": ""
        },
        {
            "heading": "D.1 UNet vs Nested UNet",
            "text": "As mentioned in Sec. 4, UNet and Nested UNet are very similar in capacity and they yield near identical performances when treated as standalone architectures. We verify this in Figure 12, where we train a standard diffusion with a UNet and Nested UNet on ImageNet 256x256. We see that they indeed yield tightly coupled FID curves. In addition, we also measure their efficiency during both\ntraining and inference, as shown in Table 2. It\u2019s evident that they have near identical computation costs."
        },
        {
            "heading": "D.2 Training costs",
            "text": "For all three datasets/tasks, namely ImageNet, CC12M and WebVid-10M, our best results are obtained on 4x8 A100 GPUs, for which we here report the total training costs. The results can be seen in Table 3."
        },
        {
            "heading": "D.3 Wall clock time",
            "text": "To complete the comparison in Figure 4 (a), we also provide the time efficiency measurements for each of the baseline runs (only ImageNet experiments are shown, and the comparison is similar on CC12M). This is shown in Table 4."
        },
        {
            "heading": "E Baseline details",
            "text": ""
        },
        {
            "heading": "E.1 Cascaded Diffusion Model",
            "text": "For our cascaded diffusion baseline models, we closely follow the guidelines from (Ho et al., 2022b) while making it directly comparable to our models. In particular, our cascaded diffusion models consist of two resolutions, 64x64 and 256x246. Here the 64x64 resolution models share the same architecture and training hyper parameters as MDM. For the upsampler network from 64x64 to 256x256, we upsample the 64x64 conditioning image to 256x256 and concatenate it with the 256x256 noisy inputs. Noise augmentation is applied by applying the same noise scheduels on the upsampled conditioning images, as suggested in (Saharia et al., 2022). All the cascaded diffusion models are trained with 1000 diffusion steps, same as the MDM models.\nDuring inference, we sweep over the noise level used for the conditioning low resolution inputs in the range of {1, 100, 500, 700, 1000}, similar to Saharia et al. (2022). We found that a relatively high conditioning noise level (500, or 700) is needed for our cascaded models to perform well."
        },
        {
            "heading": "E.2 Latent Diffusion Model",
            "text": "For the LDM experiments we used pretrained encoders from https://github.com/CompVis/ latent-diffusion (Rombach et al., 2022). The datasets were preprocessed using the autoencoders, and the codes from the autoencoders were modeled by our baseline U-Net diffusion models. For generation, the codes were first generated from the diffusion models, and the decoder of the autoencoders were then used to convert the codes into the images, at the end of diffusion.\nIn order to follow a similar spatial reduction to our MDM-S64S256 model we reduced the 256x256 images to codes at 64x64 resolution for the Imagenet experiments, using the KL-F4 model from https://ommer-lab.com/files/latent-diffusion/kl-f4.zip and we then trained our MDM-S64 baseline model on these spatial codes. However, for the text-to-image diffusion experiments on CC12M we found that the model performed better if we used the 8x downsampling model (KL-F8) \u2013 from https://ommer-lab.com/files/latent-diffusion/kl-f8.zip. However, since this reduced the resolution of the input to our UNet model, we modified the MDM-S64 model to not perform downsampling after the first ResNet block to preserve a similar computational footprint (and this modification also performed better). The training of the models was performed using the same set of hyperparameters as our baseline models."
        },
        {
            "heading": "F Datasets",
            "text": "ImageNet (Deng et al., 2009, https://image-net.org/download.php) contains 1.28M images across 1000 classes. We directly merge all the training images with class-labels. All images are resized to 2562 with center-crop. For all ImageNet experiments, we did not perform cross-attention, and fuse the label information together with the time embedding. We did not drop the labels for training both MDM and our baseline models. FID is computed on 50K sampled images against the entire training set images with randomly sampled class labels.\nCC12M (Changpinyo et al., 2021, https://github.com/google-research-datasets/ conceptual-12m) is a dataset with about 12 million image-text pairs meant to be used for visionand-language pre-training. As mentioned earlier, we choose CC12M as our main training set considering its moderate size for building high-quality text-to-image models with good zero-shot capabilities, and the whole dataset is freely available with less concerning issues like privacy. In this paper, we take all text-image pairs as our dataset set for text-to-image generation. More specifically,\nwe randomly sample 1/1000 of pairs as the validation set where we monitor the CLIP and FID scores during training, and use the remaining data for training. Each image by default is center-cropped and resized to desired resolutions depending on the tasks. No additional filtering or cleaning is applied.\nWebVid-10M (Bain et al., 2021, https://maxbain.com/webvid-dataset) is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content. Following the preprocessing steps of Guo et al. (2023)1, we extract each file into a sequence of frames, and randomly sample images every 4 frames to create a 16 frame long clip from the original video. Horizontal flip is applied as additional data augmentation. As the initial exploration of applying MDM on videos, we only sample one clip for each video, and training MDM on the extracted video clips."
        },
        {
            "heading": "G Additional Examples",
            "text": "We provide additional qualitative samples from the trained MDMs for ImageNet 256\u00d7256 (Figs. 13 to 15), text-to-image 256\u00d7256 and 1024\u00d71024 (Figs. 9 and 16 to 18), and text-to-video 16\u00d7256\u00d7256 (Fig. 19) tasks.\nIn particular, the prompts for Fig. 9 are given as follows:\na fluffy owl with a knitted hat holding a wooden board with \u201cThank You\u201d written on it (1024\u00d71024), batman and Joker making sushi together,\na squirrel wearing a crown on stage,\nan oil painting of Border Collie,\nan oil painting of rain at a traditional Chinese town,\na broken boat in a peacel lake, a lipstick put in front of pumpkins,\na frog drinking coffee , fancy digital Art,\na lonely dog watching sunset,\na painting of a royal girl in a classic castle,\na realistic photo of a castle,\norigami style, paper art, a fat cat drives UFO,\na teddy bear wearing blue ribbon taking selfie in a small boat in the center of a lake,\npaper art, paper cut style, cute bear,\ncrowded subway, neon ambiance, abstract black oil, gear mecha, detailed acrylic, photorealistic,\na groundhog wearing a straw hat stands on top of the table,\nan experienced chief making Frech soup in the style of golden light,\na blue jay stops on the top of a helmet of Japanese samurai, background with sakura tree (1024 \u00d7 1024).\n1https://github.com/guoyww/AnimateDiff/blob/main/animatediff/data/dataset.py"
        }
    ],
    "title": "Matryoshka Diffusion Models",
    "year": 2023
}