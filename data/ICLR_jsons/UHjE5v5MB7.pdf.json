{
    "abstractText": "Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider multi-layer perceptron (MLP) and Transformer architectures trained on modular arithmetic tasks, where (\u03be \u00b7 100%) of labels are corrupted (i.e. some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels and achieve 100% generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve 100% accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (\u201cmechanistically\u201d) interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes grokking dynamics reaching high train and test accuracy; second, it unlearns the memorizing representations, where the train accuracy suddenly jumps from 100% to 100(1\u2212 \u03be)%.1",
    "authors": [
        {
            "affiliations": [],
            "name": "GORITHMIC DATASETS"
        },
        {
            "affiliations": [],
            "name": "Darshil Doshi"
        },
        {
            "affiliations": [],
            "name": "Aritra Das"
        },
        {
            "affiliations": [],
            "name": "Tianyu He"
        },
        {
            "affiliations": [],
            "name": "Andrey Gromov"
        }
    ],
    "id": "SP:ed4e469f06ae9b55f2f5e4faf5e2c0994f2fafb0",
    "references": [
        {
            "authors": [
                "Boaz Barak",
                "Benjamin L. Edelman",
                "Surbhi Goel",
                "Sham M. Kakade",
                "eran malach",
                "Cyril Zhang"
            ],
            "title": "Hidden progress in deep learning: SGD learns parities near the computational limit",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "P.L. Bartlett"
            ],
            "title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1998
        },
        {
            "authors": [
                "Brian R. Bartoldson",
                "Ari S. Morcos",
                "Adrian Barbu",
                "Gordon Erlebacher"
            ],
            "title": "The generalizationstability tradeoff in neural network",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Beyer",
                "Olivier J. H\u00e9naff",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "A\u00e4ron van den Oord"
            ],
            "title": "Are we done with imagenet",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom Brown",
                "Dawn Song",
                "Ulfar Erlingsson"
            ],
            "title": "Extracting training data from large language models",
            "venue": "In 30th USENIX Security Symposium (USENIX Security",
            "year": 2021
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Daphne Ippolito",
                "Matthew Jagielski",
                "Katherine Lee",
                "Florian Tramer",
                "Chiyuan Zhang"
            ],
            "title": "Quantifying memorization across neural language models",
            "venue": "arXiv preprint arXiv:2202.07646,",
            "year": 2022
        },
        {
            "authors": [
                "Viv Cothey"
            ],
            "title": "Web-crawling reliability. Journal of the American Society for Information",
            "venue": "Science and Technology,",
            "year": 2004
        },
        {
            "authors": [
                "Aritra Ghosh",
                "Himanshu Kumar",
                "P Shanti Sastry"
            ],
            "title": "Robust loss functions under label noise for deep neural networks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Steven M. Girvin",
                "Kun Yang"
            ],
            "title": "Modern Condensed Matter Physics",
            "year": 2019
        },
        {
            "authors": [
                "Andrey Gromov"
            ],
            "title": "Grokking modular arithmetic, 2023",
            "venue": "URL https://arxiv.org/abs/",
            "year": 2023
        },
        {
            "authors": [
                "Bo Han",
                "Jiangchao Yao",
                "Gang Niu",
                "Mingyuan Zhou",
                "Ivor Tsang",
                "Ya Zhang",
                "Masashi Sugiyama"
            ],
            "title": "Masking: A new perspective of noisy supervision",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Danny Hernandez",
                "Tom Brown",
                "Tom Conerly",
                "Nova DasSarma",
                "Dawn Drain",
                "Sheer El-Showk",
                "Nelson Elhage",
                "Zac Hatfield-Dodds",
                "Tom Henighan",
                "Tristan Hume"
            ],
            "title": "Scaling laws and interpretability of learning from repeated data",
            "venue": "arXiv preprint arXiv:2205.10487,",
            "year": 2022
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Nitish Srivastava",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan R. Salakhutdinov"
            ],
            "title": "Improving neural networks by preventing co-adaptation of feature detectors",
            "year": 2012
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "year": 2015
        },
        {
            "authors": [
                "Arthur Jacot",
                "Fran\u00e7ois Ged",
                "Berfin \u015eim\u015fek",
                "Cl\u00e9ment Hongler",
                "Franck Gabriel"
            ],
            "title": "Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Francois Fleuret"
            ],
            "title": "Not all samples are created equal: Deep learning with importance sampling",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Xuefeng Liang",
                "Xingyu Liu",
                "Longshan Yao"
            ],
            "title": "Review\u2013a survey of learning from noisy labels",
            "venue": "ECS Sensors Plus,",
            "year": 2022
        },
        {
            "authors": [
                "Ziming Liu",
                "Ouail Kitouni",
                "Niklas S Nolte",
                "Eric Michaud",
                "Max Tegmark",
                "Mike Williams"
            ],
            "title": "Towards understanding grokking: An effective theory of representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ziming Liu",
                "Eric J Michaud",
                "Max Tegmark"
            ],
            "title": "Omnigrok: Grokking beyond algorithmic data",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "William Merrill",
                "Nikolaos Tsilivis",
                "Aman Shukla"
            ],
            "title": "A tale of two circuits: Grokking as competition of sparse and dense subnetworks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ari S. Morcos",
                "David G.T. Barrett",
                "Neil C. Rabinowitz",
                "Matthew Botvinick"
            ],
            "title": "On the importance of single directions for generalization, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Neel Nanda",
                "Lawrence Chan",
                "Tom Lieberum",
                "Jess Smith",
                "Jacob Steinhardt"
            ],
            "title": "Progress measures for grokking via mechanistic interpretability",
            "year": 2023
        },
        {
            "authors": [
                "Pascal Jr. Tikeng Notsawo",
                "Hattie Zhou",
                "Mohammad Pezeshki",
                "Irina Rish",
                "Guillaume Dumas"
            ],
            "title": "Predicting grokking long before it happens: A look into the loss landscape of models which grok, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Gabriele Paolacci",
                "Jesse Chandler",
                "Panagiotis G. Ipeirotis"
            ],
            "title": "Running experiments on amazon mechanical turk",
            "venue": "Judgment and Decision Making,",
            "year": 2010
        },
        {
            "authors": [
                "Romualdo Pastor-Satorras",
                "Claudio Castellano"
            ],
            "title": "Distinct types of eigenvector localization in networks",
            "venue": "Scientific Reports,",
            "year": 2016
        },
        {
            "authors": [
                "Mansheej Paul",
                "Surya Ganguli",
                "Gintare Karolina Dziugaite"
            ],
            "title": "Deep learning on a data diet: Finding important examples early in training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alethea Power",
                "Yuri Burda",
                "Harri Edwards",
                "Igor Babuschkin",
                "Vedant Misra"
            ],
            "title": "Grokking: Generalization beyond overfitting on small algorithmic datasets, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Hwanjun Song",
                "Minseok Kim",
                "Dongmin Park",
                "Yooju Shin",
                "Jae-Gil Lee"
            ],
            "title": "Learning from noisy labels with deep neural networks: A survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Kushal Tirumala",
                "Aram Markosyan",
                "Luke Zettlemoyer",
                "Armen Aghajanyan"
            ],
            "title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Vikrant Varma",
                "Rohin Shah",
                "Zachary Kenton",
                "J\u00e1nos Kram\u00e1r",
                "Ramana Kumar"
            ],
            "title": "Explaining grokking through circuit efficiency, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tong Xiao",
                "Tian Xia",
                "Yi Yang",
                "Chang Huang",
                "Xiaogang Wang"
            ],
            "title": "Learning from massive noisy labeled data for image classification",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Jiangchao Yao",
                "Jiajie Wang",
                "Ivor W. Tsang",
                "Ya Zhang",
                "Jun Sun",
                "Chengqi Zhang",
                "Rui Zhang"
            ],
            "title": "Deep learning from noisy image labels with quality embedding",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 1909
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Ziqian Zhong",
                "Ziming Liu",
                "Max Tegmark",
                "Jacob Andreas"
            ],
            "title": "The clock and the pizza: Two stories in mechanistic explanation of neural networks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Bojan \u017dunkovi\u010d",
                "Enej Ilievski"
            ],
            "title": "Grokking phase transitions in learning local rules with gradient descent, 2022",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider multi-layer perceptron (MLP) and Transformer architectures trained on modular arithmetic tasks, where (\u03be \u00b7 100%) of labels are corrupted (i.e. some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels and achieve 100% generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve 100% accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (\u201cmechanistically\u201d) interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes grokking dynamics reaching high train and test accuracy; second, it unlearns the memorizing representations, where the train accuracy suddenly jumps from 100% to 100(1\u2212 \u03be)%.1"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The astounding progress of deep learning in the last decade has been facilitated by massive, highquality datasets. Annotated real-world datasets inevitably contain noisy labels, due to biases of annotation schemes (Paolacci et al., 2010; Cothey, 2004) or inherent ambiguity (Beyer et al., 2020). A key challenge in training large models is to prevent overfitting the noisy data and attain robust generalization performance. On the other hand, in large models, it is possible for memorization and generalization to coexist (Zhang et al., 2017; 2021). By and large, the tussle between memorization and generalization, especially in the presence of label corruption, remains poorly understood.\nIn generative language models the problem of memorization is even more nuanced. On the one hand, some factual knowledge is critical for the language models to produce accurate information. On the other hand, verbatim memorization of the training data is generally unwanted due to privacy concerns (Carlini et al., 2021). The full scope of the conditions that affect memorization in large\naCondensed Matter Theory Center, University of Maryland, College Park bDepartment of Physics, University of Maryland, College Park cMeta AI \u2020Corresponding author *These authors contributed equally 1Code to reproduce our results is available at https://github.com/d-doshi/Grokking.git\nlanguage models (LLMs) is still under investigation (Carlini et al., 2022; Tirumala et al., 2022). In particular, larger models as well as repeated data (Hernandez et al., 2022) favor memorization.\nIn this work, we take an approach to disentangling generalization and memorization motivated by theoretical physics. Namely, we focus on simple, analytically tractable, (\u201cmechanistically\u201d) interpretable models trained on algorithmic datasets. More concretely, we study two-layer Multilayer Perceptrons (MLP) trained on modular arithmetic tasks. In this case, the network is trained to learn a rule, such as z = (m + n)%p \u2261 m + n mod p from a number of examples. These tasks are phrased as classification problems. We then force the network to memorize \u03be \u00b7 100% of the examples by corrupting the labels, i.e. injecting data points where (m + n)%p is mapped to z\u2032 \u0338= z. In this setting, the generalizing representations are understood qualitatively Nanda et al. (2023); Liu et al. (2022) as well as quantitatively, thanks to the availability of an analytic solution for the representations described in Gromov (2023). We empirically show that sufficiently large networks can memorize the corrupted examples while achieving nearly perfect generalization on the test set. The neurons responsible for memorization can be identified using inverse participation ratio (IPR) and pruned away, leading to perfect accuracy on the un-corrupted dataset. We further show that regularization dramatically affects the network\u2019s ability to memorize the corrupted examples. Weight decay, Dropout and BatchNorm all prevent memorization, but in different ways: Weight decay and Dropout eliminate the memorizing neurons by converting them into generalizing ones, while BatchNorm de-amplifies the signal coming from the memorizing neurons without eliminating them."
        },
        {
            "heading": "1.1 PRELIMINARIES",
            "text": "Grokking modular arithmetic. We consider the modular addition task (m + n)% p. It can be learned by a two-layer MLP. Explicitly, the network function takes form\nf(m,n) = W\u03d5 (Win(em \u2295 en)) = W\u03d5 (U em + V en) . (1) Here, em, en \u2208 Rp are one hot encoded numbers m,n. \u201c\u2295\u201d denotes concatenation of vectors (em \u2295 en \u2208 R2p). Win \u2208 RN\u00d72p and W \u2208 Rp\u00d7N are the first and second layer weight matrices, respectively. \u03d5 is the activation function. Win is decomposed into two N \u00d7 P blocks: U ,V \u2208 RN\u00d7p. U ,V serve as embedding vectors for m,n respectively. f(m,n) \u2208 Rp is the networkoutput on one example pair (m,n). The targets are one hot encoded answers e(m+n)% p.\nThe dataset consists of p2 examples pairs; from which we randomly pick \u03b1p2 examples for training2, and use the other (1\u2212 \u03b1)p2 examples as a test set. We will refer to \u03b1 as the data fraction. This setup exhibits grokking: delayed and sudden occurrence of generalization, long after memorization (Power et al., 2022; Nanda et al., 2023; Liu et al., 2022; Gromov, 2023).\n2Throughout the text, networks are trained with Full-batch AdamW and MSE loss, unless otherwise stated.\nPeriodic weights. Choosing quadratic activation function \u03d5(x) = x2 makes the problem analytically solvable (Gromov, 2023). Upon grokking, the trained network weights are qualitatively similar to the analytical solution presented in Gromov (2023).3 The following analytical expression for the network weights leads to 100% generalization (for sufficiently large width)\nUki =\n( 2\nN\n)\u2212 13 cos ( 2\u03c0\np i\u03c3(k) + \u03d5\n(u) k ) , Vkj = ( 2\nN\n)\u2212 13 cos ( 2\u03c0\np j\u03c3(k) + \u03d5\n(v) k\n) ,\nWqk =\n( 2\nN\n)\u2212 13 cos ( \u22122\u03c0\np q\u03c3(k)\u2212 \u03d5(u)k \u2212 \u03d5 (v) k\n) ,\n(2)\nwhere \u03c3(k) denotes a random permutation of k in SN \u2013 reflecting the permutation symmetry of the hidden neurons. The phase \u03d5(u)k and \u03d5 (v) k are uniformly i.i.d. sampled between (\u2212\u03c0, \u03c0]. We refer the reader to Appendix D for further details about the analytical solution.\nInverse participation ratio. To characterize the generalizing representations quantitatively, we utilize a quantity familiar from the physics of localization: the inverse participation ratio (IPR). Its role is to detect periodicity in the weight matrix.\nLet Uk\u00b7,Vk\u00b7(Wk\u00b7) denote the kth row(column) vectors of the weights U ,V (W ). Consider the discrete Fourier transforms of these vectors, denoted by U\u0303k\u00b7, V\u0303k\u00b7(W\u0303k\u00b7). The Fourier decompositions of the periodic rows(columns) of these weights are highly localized. We leverage this to quantify the similarity of trained weights to the analytic solution (equation 2). To that end, we define the Inverse Participation Ratio (IPR) (Girvin & Yang, 2019; Pastor-Satorras & Castellano, 2016; Gromov, 2023) for these vectors: 4\nIPR(u)k := ( \u2225U\u0303k\u00b7\u22254 \u2225U\u0303k\u00b7\u22252 )4 ; IPR(v)k := ( \u2225V\u0303k\u00b7\u22254 \u2225V\u0303k\u00b7\u22252 )4 ; IPR(w)k := ( \u2225W\u0303\u00b7k\u22254 \u2225W\u0303\u00b7k\u22252 )4 ; (3)\nwhere \u2225\u00b7\u2225P denotes the LP -norm of the vector. One can readily see from equation 3 that IPR \u2208 [1/p, 1], with higher values for periodic vectors and lower values for non-periodic ones. It is useful to quantify the periodicity of each neuron in the hidden layer of the network (indexed by k). We define per-neuron IPR by averaging over the weight-vectors connected to the neuron:\nIPRk := 1\n3\n( IPR(u)k + IPR (v) k + IPR (w) k ) . (4)\nSince trained networks generalize via the periodic features, a larger population of high-IPR neurons leads to better generalization. To quantify the overall similarity of the trained network to the analytical solution (equation 2) we average IPRk over all hidden neurons: IPR := Ek [IPRk].\n3Periodicity of weights is approximate in trained networks. 4In general IPR is defined as (\u2225Uk\u00b7\u22252r/\u2225Uk\u00b7\u22252)2r . We set r = 2, a common choice.\nLabel corruption. To introduce label corruption, we choose a fraction \u03be of the training examples, and replace the true labels with random labels. The corrupted labels are generated from a uniform random distribution over all the labels. This is often called symmetric label noise in literature (Song et al., 2022). This type of label corruption does not introduce label asymmetry."
        },
        {
            "heading": "1.2 RELATED WORKS",
            "text": "Grokking Grokking was first reported by Power et al. (2022) for Transformers trained on modular arithmetic datasets. Liu et al. (2022; 2023) presented explanations for grokking in terms of quality of representation learning as well as weight-norms. They also found grokking-like behaviours on other datasets. Nanda et al. (2023); Zhong et al. (2023) reverse-engineered grokked Transformer models and revealed the underlying learned algorithm. Barak et al. (2022) observed grokking on the sparseparity problem, which they attributed to the amplification of the so-called Fourier gap. Merrill et al. (2023) examined grokking in the sparse-parity setup and attributed it to the emergence of sparse subnetworks. Z\u030cunkovic\u030c & Ilievski (2022) discuss solvable models for grokking in a teacher-student setup. Recently, Notsawo et al. (2023) investigated the loss landscape at early training time and used it to predict the occurrence of grokking. Varma et al. (2023) demonstrated new grokking-behaviours by investigating the critical amount of data needed for grokking.\nLabel Noise Zhang et al. (2017) showed that neural networks can easily fit random labels. A large body of work has been dedicated to developing techniques for robust training with label noise: explicit regularization(Hinton et al., 2012; Ioffe & Szegedy, 2015), noise-robust loss functions (Ghosh et al., 2017), sample selection (Katharopoulos & Fleuret, 2018; Paul et al., 2021), noise modeling and dedicated architectures (Xiao et al., 2015; Han et al., 2018; Yao et al., 2019) etc. We refer the reader to recent reviews by Song et al. (2022); Liang et al. (2022) for a comprehensive overview.\nRegularization and Pruning Morcos et al. (2018) investigated the interplay between label corruption and regularization in vision tasks. They showed that BatchNorm (Ioffe & Szegedy, 2015) and Dropout (Hinton et al., 2012) have qualitatively different effects. Dropout effectively reduces the network-size but does not discourage over-reliance on a few special directions in weights space; while BatchNorm spreads the generalizing features over many dimensions. Bartoldson et al. (2020) found that there is a trade-off between stability and generalization when pruning trained networks."
        },
        {
            "heading": "2 GROKKING WITH LABEL NOISE",
            "text": "Grokking MLP on modular addition dataset is remarkably robust to label corruption. Even without explicit regularization, the model can generalize to near 100% accuracy with sizable label corruption. In many cases, the network surprisingly manages to \u201ccorrect\u201d some of the corrupted examples, resulting in Inversion (i.e. test accuracy > training accuracy). We emphasise that this is in stark contrast to the common belief that grokking requires explicit regularization. Adding regularization makes grokking more robust to label corruption, with stronger Inversion. To quantify these behaviours, we observe the training and test performance of the network with varying sizes of training dataset, amount of label corruption and regularization. We summarize our findings in the form of empirical phase diagrams (Figure 3), featuring the following phases. More detailed phase diagrams along with training and test performances are presented in Appendix L.\nCoexistence. In the absence of explicit regularization the network generalizes on test data and memorizes the corrupted training data. In other words, both train and test accuracy are close to 100% despite label corruption.\nPartial Inversion. In this phase, the network generalizes on the test data but only memorizes a fraction of the corrupted training data. Remarkably, the network predicts the \u201ctrue\u201d labels on the remaining corrupted examples. In other words, the network corrects a fraction of the corrupted data. Consequently, we see < 100% train accuracy but near-100% test accuracy, resulting in a negative generalization gap (Figure 1(b))! We term this phenomenon Partial Inversion; where \u201cinversion\u201d refers to the test accuracy being higher than train accuracy. Remarkably, partial inversion occurs even in the absence of any explicit regularization, but only when there is ample training data (leftmost panels in Figure 3(a,b)).\nFull Inversion. Upon adding regularization, the network often generalizes on the test set but does not memorize any of the corrupted examples. This results in training accuracy plateauing close to 100(1 \u2212 \u03be)% (\u03be is the corruption fraction); while the test accuracy is at 100% (Figure 1(c)). Increasing regularization enhances this phase (middle columns in Figure 3(a,b)).\nMemorization. With very high label corruption and/or very low training data-fraction the network memorizes all of the training data (corrupted as well as non-corrupted), but does not generalize. We get close to 100% accuracy on training data, but random guessing accuracy on test data in this phase.\nForgetting. With very high weight decay, the network performs poorly on both training and test data (right-most panel in Figure 3(a)). However, upon examination of the training curves (Figure 8(a)) we find that the accuracies plummet after the network has generalized! This performance drop is a result of the norms of weights collapsing to arbitrarily small values (Figure 8(b)). Forgetting exclusively occurs in grokked networks with activation functions of degree higher than 1 (e.g. we use \u03d5(x) = x2) and with adaptive optimizers. (See Appendix E for a detailed discussion.)\nConfusion. With very high dropout the network becomes effectively narrow, resulting in reduced capacity to memorize. Consequently, even in the absence of grokking, the network does not manage to memorize all the corrupted labels (right-most panel in Figure 3(b))."
        },
        {
            "heading": "2.1 GENERALIZING AND MEMORIZING SUB-NETWORKS",
            "text": "The generalizing features in our setup are \u201cmechanistically\u201d interpretable (equation 2). As a result, we can leverage measures of periodicity such as IPR to quantitatively characterize various phases presented above. It also allows us to analyze the effect of various regularization schemes. To that end, we state the following hypothesis and extensively examine it with experiments.\nHypothesis 2.1. For a two-layer MLP with quadratic activation trained on (one-hot encoded) modular addition datasets with label corruption, the high IPR neurons facilitate generalization via feature-learning whereas the low IPR neurons cause memorization of corrupted training data.\nTo test Hypothesis 2.1 we study the distribution of per-neuron IPR in various phases, with different levels of regularization. In the Coexistence phase, the network simultaneously generalizes and memorizes the corrupted train data. From Hypothesis 2.1, we expect the presence of both generalizing and memorizing neurons. Indeed, we find a bi-modal distribution of IPRs in trained networks (left column of Figure 4). Remarkably, the network not only distinguishes between the tasks of feature-learning and memorization, it also assigns distinct and independent sub-networks to each.\nAs mentioned in the previous section, adding regularization in the form of weight decay or Dropout makes the generalization more robust and facilitates Inversion. Since these phases see decreased memorization of corrupted labels, we expect to see a decrease in the number of memorizing neurons and a corresponding increase in the generalizing ones. Indeed, we observe a distribution-shift towards higher IPR in the Partial Inversion phase (middle column of Figure 4). In the Full Inversion phase (induced by weight decay or dropout), we see that memorizing neurons are almost entirely\neliminated in favour of generalizing ones (right column of Figure 4). This provides a mechanism by which the network recovers the \u201ctrue\u201d labels even for the corrupted data \u2013 in the absence of memorizing sub-network, its predictions are guided by the underlying rule and not corrupted samples.\nBatchNorm also has a regularizing effect on training (Figure 9), albeit, through a qualitatively distinct mechanism. In the network (Equation (1)), we apply BatchNorm after the activation function (post-BN):\nf(m,n) = W BN (\u03d5 (U em + V en)) . (5)\nBatchNorm layer does does not affect the IPR distribution of the neurons significantly \u2013 the low IPR neurons persist even in the Full Inversion phase (Figure 5). Instead, BatchNorm adjusts its own weights to de-amplify the low-IPR (memorizing) neurons, biasing the network towards generalizing representations. This can be seen from the strong correlation between IPRk and the corresponding BatchNorm weights in Figure 5(right). BatchNorm effectively prunes the network and it learns to do this directly from the data! Note that this effect of BatchNorm acts in conjunction with the implicit regularization from the mini-batch noise in this case. We find that decreasing the batch-size enhances Inversion even without explicit regularization (Figure 10).\nTo further isolate the effect of individual neurons on performance, we perform the two complementary pruning experiments (Figure 6) : Starting from a trained network in the Coexistence phase, we gradually prune out neurons, one-at-a-time, (i) starting from the lowest IPR neuron (ii) starting from the highest IPR neuron, while keeping track of training and test accuracies. To distinguish network\u2019s ability to memorize, we also track the accuracies on corrupted and uncorrupted parts of the training dataset. Before pruning, the network has near-100% accuracy on both train and test data. In case\n(i) (Figure 6(left)), we see a monotonic decrease in accuracy on corrupted examples as more and more low IPR neurons are pruned. The accuracy on test data as well as ucorrupted training data remains high. The resulting (overall) training accuracy plateaus around 100(1\u2212\u03be)%, corresponding to the fraction of uncorrupted examples. In case (ii) (Figure 6(right)), as more high IPR neurons are pruned, we see a decline in test accuracy (as well as the accuracy on the uncorrupted training data). The network retains its ability to memorize, so the accuracy on corrupted training data remains high.\nWe emphasize that, although each generalizing neuron learns the useful periodic features, the overall computation in the network is emergent: it is performed collectively by all generalizing neurons."
        },
        {
            "heading": "2.2 GENERAL ARCHITECTURES",
            "text": "In this subsection, we aim to check if (i) we can quantify memorization/generalization capabilities of general architectures using IPR distribution of the neurons; (ii) our understanding of regularization can be generalized to such cases.\nHypothesis 2.2. For any network that achieves non-trivial generalization accuracy on the modular addition dataset with label corruption, its operation can be decomposed into two steps: (I) The input (embedding) layer with row-wise periodic weight maps the data onto periodic features; (II) The remainder of the network nonlinearly implements trigonometric operations over the periodic neurons and maps them to the output predictions. We hypothesize that step (I) depends only on the dataset characteristics; and hence, is a universal property across various network architectures.\nTo verify Hypothesis 2.2 and better understand the impact of regularization, we conduct further experiments on 1-layer Transformers and 3-layer ReLU MLPs. Our findings, illustrated in the first two columns of Figure 7 and paralleling the observations in Figure 3(a), indicate that increased weight decay promotes generalization over memorization. IPR histograms in the last two columns of Figure 7 further confirm that weight decay encourages the correct features and corroborates the assertions made in Hypothesis 2.2.\nFor these general models, Dropout mostly leads to an extended confusion phase, while BatchNorm only facilitates coexistence. We observe weight decay to be essential for these models to have the full inversion phase. We refer the reader to Appendix G for an in-depth discussion."
        },
        {
            "heading": "3 EFFECT OF REGULARIZATION",
            "text": "In this section, we summarize the effect of various regularization techniques and compare them with existing literature."
        },
        {
            "heading": "3.1 WEIGHT DECAY",
            "text": "As shown in Figure 3(a), increasing weight decay has two notable effects: (i) Generalization with less data: The phase boundary between the Memorization and Coexistence phases shifts toward lower data-fraction. Weight decay encourages generalization even when training data is scarce; in line with classical ideas (Bartlett, 1998). (ii) Emergence of the Full Inversion phase: In this phase, weight decay prevents memorization and helps the network \u201ccorrect\u201d the labels for corrupted data points. This effect is further elucidated in Figure 4(a) \u2013 For a fixed amount of data and label corruption, increased weight decay results in diminished proportion of low IPR (memorizing) neurons in favor of high IPR (generalizing) ones."
        },
        {
            "heading": "3.2 DROPOUT",
            "text": "It is believed that dropout prevents complex co-adaptations between neurons in different layers, encouraging each neuron to learn useful features (Hinton et al., 2012; Srivastava et al., 2014). This intuition becomes more precise in our setup. The generalizing solution constitutes of periodic neurons, each corresponding to a weight-vector of a given \u201cfrequency\u201d \u03c3(k). Such a solution requires p neurons with different frequencies5, (see Equation 6). In a wide network (N = 500 in our setup), there can be multiple neurons with the same frequency.6 Due to this redundancy, generalizing solutions are more robust to dropped neurons; whereas overfitting solutions are not. As a result, training with Dropout gets attracted to such periodic solutions."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "We have investigated the interplay between memorization and generalization in two-layer neural networks trained on algorithmic datasets. The memorization was induced by creating a dataset with corrupted labels, forcing the network to memorize incorrect examples and learn the rule at the same time. We have found that the memorizing neurons can be explicitly identified and pruned away leading to perfect generalization.\nNext we showed that various regularization methods such as weight decay, Dropout and BatchNorm encourage the network to ignore the corrupted labels and lead to perfect generalization. We have leveraged the interpretability of the representations learnt on the modular addition task to quantitatively characterize the effects of these regularization methods. Namely, we have found that the weight decay and dropout act in a similar way by reducing the number of memorizing neurons; while BatchNorm de-amplifies the output of regularizing neurons without eliminating them, effectively pruning the network."
        },
        {
            "heading": "5 LIMITATIONS AND FUTURE WORK",
            "text": "Although the trained models in our setup are completely interpretable by means of analytical solutions, the training dynamics that lead to these solutions remains an open question. Solving the dynamics should also shed more light on the effects of regularization and label corruption.\nWhile our work is limited to the exactly solvable models and algorithmic datasets, it paves the way to quantitative investigation of generalization and memorization in more realistic settings.\n5More precisely, it only requires \u2308p/2\u2309 frequencies for real weights. 6The analytical solution becomes exact only at large N ; with sub-leading corrections of order 1/ \u221a N ."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "A.G.\u2019s work at the University of Maryland was supported in part by NSF CAREER Award DMR2045181, Sloan Foundation and the Laboratory for Physical Sciences through the Condensed Matter Theory Center. The authors acknowledge the University of Maryland supercomputing resources (http://hpcc.umd.edu) made available for conducting the research reported in this paper."
        },
        {
            "heading": "A FURTHER EXPERIMENTAL DETAILS",
            "text": "In every plot presented, unless explicitly stated otherwise, we use the following default setting:\nDefault setting A 2-layer fully connected network with quadratic activation function and width N = 500, initial weights sampled from N (0, (16N)\u22122/3), trained on p = 97 Modular Addition dataset. The networks are trained for 2000 optimization steps, using full-batch, MSE loss, AdamW optimizer, learning rate \u03b7 = 0.01 and \u03b2 = (0.9, 0.98).\nA.1 FIGURES IN MAIN TEXT\nFigure 1: Default setting with \u03b1 = 0.5, \u03be = 0.35 and (a) wd = 0 (b) wd = 5 and (c) wd = 15.\nFigure 2: Default setting with \u03b1 = 0.5, \u03be = 0.0, wd = 5.0.\nFigure 3: Each plot is scanned over 17 different data fractions \u03b1 ranging from 0.1 to 0.9 and 19 noise levels \u03be from 0.0 to 0.9 using the default setting.\nFigures 4 and 6: Default setting with \u03b1 = 0.5, \u03be = 0.35.\nFigure 5: 2-layer fully connected network with quadratic activation function and width N = 500, with and without BatchNorm. We train for \u223c 8000 steps, with MSE loss, Adam optimizer with minibatch of size 64, and learning rate \u03b7 = 0.005. Data fraction \u03b1 = 0.65, corruption fraction \u03be = 0.2.\nGeneral setting for minibatch training: We train the network with batch size 256 for 2000 steps. For batch sizes 8 and 64, we scale the number of total steps so as to keep the number of epochs roughly equal. (In other words, all the networks see the same amount of data.) The learning rate is scaled as 0.01 \u00b7 \u221a batch size/256.\nFigure 7: All networks are trained with MSE loss using AdamW optimizer with learning rate \u03b7 = 0.001 and (\u03b21, \u03b22) = (0.9, 0.98). (a) Encoder-only Transformer, embedding dimension dembed = 128 with ReLU activation, trained for 3000 steps with 3 random seeds; (b) Width N = 500 trained for 5000 steps with 3 random seeds."
        },
        {
            "heading": "B DETAILS OF RESOURCES USED",
            "text": "Figure 3 requires 72 hours on a single NVIDIA RTX 3090 GPU.\nFigure 7 used 60 hours on a 1/7 NVIDIA A100 GPU.\nAll the remaining figures except Figure 7 in the main text are either further experiments performed on the data from the Figure 3 experiment; or are very quick to perform on any machine.\nWe used 300 GPU hours for early-stage exploration and making phase diagrams in the appendix."
        },
        {
            "heading": "C QUANTITATIVE CHARACTERIZATION OF PHASES",
            "text": "All phase diagrams are plotted using the empirical accuracies of trained networks.\nTrained networks with \u2265 90% test accuracies get classified into Coexistence, Partial Inversion and Full Inversion based on their training accuracy (specifically, their ability to memorize corrupted examples). Coexistence phase has \u2265 90% training accuracy, since it memorizes most of the corrupted training examples. Partial Inversion phase has < 90% but \u2265 100(1.05 \u2212 \u03be)% training accuracy, where \u03be is the corruption fraction. Note that we subtract \u03be from 1.05 instead of 1.00 to account for the 1 \u2212 2% random guessing accuracy as well as statistical fluctuations around this value. Full Inversion phase has < 100(1.05 \u2212 \u03be)% training accuracy, since it does not memorize any of the corrupted examples. Memorization phase has < 90% test accuracy but \u2265 90% training accuracy, indicating that the network memorizes all of the training data, including the corrupted and uncorrupted examples. Forgetting or confusion phase has < 90% test as well as training accuracies. the network neither generalizes nor manages to memorize all the training examples. We distinguish Forgetting from confusion using the training curve. In the Forgetting phase the grokked network loses its performance due to steadily decreasing weight-norms. Whereas in the confusion phase, the network never reaches high performance."
        },
        {
            "heading": "D ANALYTICAL SOLUTION",
            "text": "We show that the network with periodic weights stated in Equation (2) is indeed the analytical solution for the modular arithmetic task. The network output is given by\nfq(m,n) = N\u2211 k=1 Wqk(Ukm + Vkn) 2\n= 2\nN N\u2211 k=1 cos ( \u22122\u03c0 p \u03c3(k)q \u2212 (\u03d5(u)k + \u03d5 (v) k ) ) \u00b7\n\u00b7 ( cos ( 2\u03c0\np \u03c3(k)m+ \u03d5\n(u) k\n) + cos ( 2\u03c0\np \u03c3(k)n+ \u03d5\n(v) k ))2 = 2\nN N\u2211 k=1 { 1 4 cos ( 2\u03c0 p \u03c3(k)(2m\u2212 q) + \u03d5(u)k \u2212 \u03d5 (v) k ) + 1\n4 cos\n( 2\u03c0\np \u03c3(k)(2m+ q) + 3\u03d5\n(u) k + \u03d5 (v) k ) + 1\n4 cos\n( 2\u03c0\np \u03c3(k)(2n\u2212 q)\u2212 \u03d5(u)k + \u03d5 (v) k ) + 1\n4 cos\n( 2\u03c0\np \u03c3(k)(2n+ q) + \u03d5\n(u) k + 3\u03d5 (v) k ) + 1\n2 cos\n( 2\u03c0\np \u03c3(k)(m+ n\u2212 q) ) + 1\n2 cos\n( 2\u03c0\np \u03c3(k)(m+ n+ q) + 2\u03d5\n(u) k + 2\u03d5 (v) k ) + 1\n2 cos\n( 2\u03c0\np \u03c3(k)(m\u2212 n\u2212 q)\u2212 2\u03d5(v)k ) + 1\n2 cos\n( 2\u03c0\np \u03c3(k)(m\u2212 n+ q) + 2\u03d5(u)k ) +cos ( \u22122\u03c0\np \u03c3(k)q \u2212 \u03d5(u)k \u2212 \u03d5 (v) k\n)} . (6)\nWe have highlighted the term that will give us the desired output with a box. Note that the desired term is the only one that does not have additive phases in the argument. Recall that the phases \u03d5 (u) k and \u03d5 (v) k randomly chosen \u2013 uniformly i.i.d. sampled between (\u2212\u03c0, \u03c0]. Consequently, as N becomes large, all other terms will vanish due to random phase approximation. The only surviving term will be the boxed term. We can write this boxed term in a more suggestive form to make the analytical solution apparent.\nfq(m,n) = 1\nN N\u2211 k=1 cos ( 2\u03c0 p \u03c3(k)(m+ n\u2212 q) ) \u2248 \u03b4p(m+ n\u2212 q) , (7)\nwhere we have defined the modular Kronecker Delta function \u03b4p(\u00b7) as Kronecker Delta function up to integer multiples of the modular base p.\n\u03b4p(x) = { 1 x = rp (r \u2208 Z) 0 otherwise , (8)\nwhere Z denotes the set of all integers. Note that \u03b4p(m+ n\u2212 q) are the desired one hot encoded labels for the Modular Arithmetic task, by definition. Thus our network output with periodic weights is indeed a solution.\nE FORGETTING PHASE\nWe saw in Figure 3(a) (right-most panel, wd=20.0) that extremely high weight decay leads to Forgetting phase. This phase has random guessing accuracies on both training and test datasets at the end of training. Naively, this would seem similar to the Confusion phase. However, Upon close inspection of their training dynamics, we find rich behaviour. Namely, the network initially undergoes dynamics similar to Full Inversion, i.e., the test accuracy reaches 100% while the train accuracy plateaus around 100(1\u2212\u03be)%. However, both accuracies subsequently decay down to random guessing. This curious phenomenon is explained by the effect of the high weight decay on the network weights. We notice that the Frobenius norms of the weights of the network steadily decay down to arbitrarily small values.\nThis curious behaviour only occurs when we use activation functions of degree higher than 1 (e.g. \u03d5(x) = x2 in this case). (More precisely, we mean that the first term in the series expansion should be of degree at least 2.) This can be intuitively explained by examining the gradients carefully. With quadratic activation functions and MSE loss, gradients take the following form\n\u2202L \u2202Uab = 4 |D|p \u2211\n(b,n)\u2208D p\u2211 q\n( N\u2211\nk=1\nWqk (Ukb + Vkn) 2 \u2212 \u03b4p(b+ n\u2212 q) ) Wqa (Uab + Van)\n\u2202L \u2202Vab = 4 |D|p \u2211\n(m,b)\u2208D p\u2211 q\n( N\u2211\nk=1\nWqk (Ukm + Vkb) 2 \u2212 \u03b4p(b+ n\u2212 q) ) Wqa (Uam + Vab)\n\u2202L \u2202Wcd = 4 |D|p \u2211\n(m,n)\u2208D\n( N\u2211\nk=1\nWck (Ukm + Vkn) 2 \u2212 \u03b4p(m+ n\u2212 q) ) (Udm + Vdn) 2 ,\n(9)\nwhere D is the training dataset and |D| is its size. For non-generalizing solutions, the term inside the bracket is of degree 0 in weights, due to the delta function term. But close to generalizing solutions, the term inside the bracket becomes effectively degree 3 in weights, since the delta function term cancels with the network output. Consequently, around the generalizing solution, the weight gradients have a higher degree dependence on the weights themselves. This makes such solutions more prone to the collapse due to high weight decay. This serves as a heuristic justification as to why only grokked networks seemed to be prone to Forgetting, while memorizing networks seem to be immune.\nIf we use vanilla Gradient Descent optimizer, the updates would plateau once the balance between gradient updates and weight decay contribution is reached. But this is not the case in Adaptive optimizers. Indeed, we find that this phenomenon exclusively occurs with Adaptive optimizers (e.g. Adam in this case).\nWe believe that it is possible to get further insight into this phase by closely examining the optimization landscape and network dynamics. Since this behaviour is exclusive to grokked networks, understanding this phase would lead to further understanding of grokking dynamics. Alternatively, a better understanding of the dynamics of grokking should give us insights into this curious phenomenon. We defer this analysis for future work and welcome an examination by our readers."
        },
        {
            "heading": "F COMPARISON OF BATCHNORM, LAYERNORM AND MINIBATCH W/O NORMALIZATION",
            "text": "Here we examine the effects of BatchNorm, LayerNorm and mini-batch noise. We find that generalization and Inversion are indeed enhanced by decreasing the batch-size. This demonstrates that implicit regularization from mini-batch noise favours simpler solutions in our setup.\nAdditionally, upon comparing Figure 9, Figure 10 and Figure 11 we find enhanced Inversion in cases with normalization layers (especially BatchNorm). For BatchNorm, we provide an explanation in the main text, with the the BatchNorm weights de-amplifying memorizing neurons in favour of generalizing ones. For LayerNorm, however, we do not find such an effect."
        },
        {
            "heading": "G TRANSFORMERS AND DEEP MLPS",
            "text": "In this section, we present (i) more weight decay experiments on different configurations of Transformers; (ii) the Effect of Dropout on Transformers; (iii) the Effect of BatchNorm on 3-layer ReLU MLPs.\nFor reasons that we do not fully understand, 3-layer ReLU MLPs with even 0.05 Dropout probabilities failed to memorize the dataset for any data fraction \u03b1 > 0.3 and noise level \u03be \u2265 0.1, even after an extensive hyperparameter search. We leave the discussion about this for the future.\nG.1 EFFECT OF DEPTH / NUMBER OF HEADS / WEIGHT TYING\nTo further support our Hypothesis 2.2, we conducted further experiments by varying the configuration of Transformer models. Figure 12 suggests that our Hypothesis holds irrespective of the Transformer configurations.\nG.2 DROPOUT PHASE DIAGRAM FOR TRANSFORMERS\nWe conduct further experiments to check the effect of Dropout on Transformers. From Figure 13, we see that with a small dropout probability, the model managed to have a coexistence phase with an average higher IPR for neurons connected to the embedding layer. Notably, any dropout probability larger than 0.1 does not help.\nG.3 EFFECT OF PARAMETERIZATION FOR 3-LAYER MLPS\nAs we mentioned in Hypothesis 2.2, the input layer should have high IPR columns irrespective of the details of the network. In this section, we show that different parameterization of the network does not break our Hypothesis.\nIn particular, we consider two different parameterizations: (i) The standard parameterization with weights sampled from N (0, 2/fan in); (ii) The parameterization with weights sampled from N (0, 2/N2/3), which corresponds to a network parameterized in a more lazy training regime (Jacot et al., 2022).\nOur results in Figure 14 show that as the network gets into a lazier regime, the row-wise IPR distribution of the input layer weight barely shifts, whereas the column-wise IPR distribution of the output layer weight has larger values. We interpret this shift as happening because the network with lazier parameterization fails to utilize the hidden layers to decode the periodic features embedded by input layers.\nG.4 BATCHNORM FOR 3-LAYER RELU MLPS\nThis subsection tests the correlation between BatchNorm weights and high IPR neurons for 3-layer ReLU MLPs. We found that BatchNorm for 3-layer MLPs only has coexistence phases and can perfectly memorize and generalize for small corruption fractions \u03be, we showed a representative training curve in Figure 15. We find that with BatchNorm, the 3-layer MLP tends to have high IPR\nneurons close to the output layer instead of having high IPR for neurons close to the input layer. We believe that failure to remove the very low IPR neurons connected to the input layer is why this network is always in the coexistence phase."
        },
        {
            "heading": "H LABEL CORRUPTION ON GENERAL DATASETS",
            "text": "We train a 4-layer ReLU MLP on the MNIST dataset with label corruption with weight decay and dropout (CrossEntropy loss). Weight decay indeed enhanses generalization and leads to Full Inversion. Very high weight decay leads to confusion. Dropout, on the other hand, is not as effective. Increasing dropout hurts both test and train accuracies in this case.\nThe generalizing features are harder to quantify for MNIST \u2013 we leave this analysis for future work."
        },
        {
            "heading": "I ADDITIONAL PRUNING CURVES",
            "text": "I.1 ACCURACIES\nIn Figure 6, we showed pruning low and high IPR neurons in the Coexistence phase. Here we show the extended version of the pruning experiments. In contrast to the plots in Figure 6, In Figure 17 we plot longer x-axes \u2013 we show accuracies with fraction of pruned neurons ranging from 0.0 to 1.0. We also perform the pruning experiment for Partial Inversion and Full Inversion phases. Pruning affects generalization and memorization differently in different phases because the proportion of high and low IPR neurons in trained networks depends on the phase.\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from lowest IPR)\n0\n20\n40\n60\n80\n100\nac cu\nra cy\n(% )\nPruning Memorizing (low IPR) neurons\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from highest IPR)\n0\n20\n40\n60\n80\n100\nac cu\nra cy\n(% )\nPruning Generalizing (high IPR) neurons\ntraining set test set corrupted training data uncorrupted training data 100(1 )%\n(a) Coexistence phase (weight decay = 0.0). (Left) Pruning from low IPR neurons retains performance on test data and uncorrupted training data; while decreases performance on corrupted training data down to to 100 \u2217 (1\u2212 \u03be)%. (Right) Pruning high IPR neurons decreases performance on test data; while retaining performance on training data. Note that the accuracy on corrupted data is retained longer than that on uncorrupted data.\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from lowest IPR)\n0\n20\n40\n60\n80\n100\nac cu\nra cy\n(% )\nPruning Memorizing (low IPR) neurons\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from highest IPR)\n0\n20\n40\n60\n80\n100\nac cu\nra cy\n(% )\nPruning Generalizing (high IPR) neurons\ntraining set test set corrupted training data uncorrupted training data 100(1 )%\n(b) Partial Inversion phase (weight decay = 5.0). (Left) Pruning from low IPR neurons retains performance on test data and uncorrupted training data; while decreases performance on corrupted training data down to to 100 \u2217 (1 \u2212 \u03be)%. (Right) Pruning high IPR neurons decreases performance on test data. It improves the performance on corrupted training data and retains it on uncorrupted training data. In the absence of periodic neurons, the (memorizing) low-IPR neurons have an increased influence on the network predictions, which leads to this increase in memorization.\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from lowest IPR)\n0\n20\n40\n60\n80\n100\nac cu\nra cy\n(% )\nPruning Memorizing (low IPR) neurons\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from highest IPR)\n0\n20\n40\n60\n80\n100\nac cu\nra cy\n(% )\nPruning Generalizing (high IPR) neurons\ntraining set test set corrupted training data uncorrupted training data 100(1 )%\n(c) Full Inversion phase (weight decay = 15.0). (Left) Pruning from low IPR neurons retains performance on test data and uncorrupted training data; while decreases performance on corrupted training data to 100 \u2217 (1\u2212 \u03be)%. (Right) Pruning high IPR neurons does not improve memorization significantly, except for the small increase in accuracy on corrupted training data towards the end. This is because the fraction of the neurons that have low IPR is very small in this case.\nFigure 17: Pruning experiments in all various phases (N = 500, \u03b1 = 0.5, \u03be = 0.35). Pruning affects generalization and memorization differently in different phases because the proportion of high and low IPR neurons in trained networks depends on the phase.\nI.2 LOSSES\nHere we show how the train and test losses are affected by pruning. Train loss is lowered by, both, memorizing (low IPR) and generalizing (high IPR) neurons. This leads to monotonic behaviors in all the cases. Test loss is lowered by periodic neurons, while increased by memorizing neurons. An effect that sits on top of these is the effective cancellation of sub-leading terms in the analytical solution Equation (6). Since the accuracies only depend on the largest logit, these sub-leading terms have little effect on them (at sufficient width). However, losses are significantly affected by them. With zero or small weight decay values, this cancellation is achieved by low IPR neurons. Thus, in these cases, low IPR neurons serve the dual purpose of memorization as well as canceling the subleading terms. With large weight decay values, this cancellation is achieved by a different algorithm, where \u201csecondary peaks\u201d appear in the Fourier spectrum of the periodic neurons. We leave an indepth analysis of these algorithmic biases for future work.\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from lowest IPR)\n0.005\n0.010\n0.015\n0.020\nM SE\nlo ss\nPruning Memorizing (low IPR) neurons\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from highest IPR)\n0.005\n0.010\n0.015\n0.020\nM SE\nlo ss\nPruning Generalizing (high IPR) neurons\ntraining set test set\n(a) Coexistence phase (weight decay = 0.0). (Left) Pruning from low IPR neurons (almost) monotonically increases the train loss. The test loss has two competing factors: lower memorization and fewer neurons that cancel the sub-leading terms. As a result, we see non-monotonic behaviour in the first half. (Right) Pruning high IPR neurons increases both train and test losses. We see a non-monotonic behaviour in the second half since the cancellation of sub-leading terms is irrelevant in the absence of periodic neurons.\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from lowest IPR)\n0.006\n0.008\n0.010\nM SE\nlo ss\nPruning Memorizing (low IPR) neurons\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from highest IPR)\n0.006\n0.008\n0.010\nM SE\nlo ss\nPruning Generalizing (high IPR) neurons\ntraining set test set\n(b) Partial Inversion phase (weight decay = 5.0). (Left) Pruning from low IPR neurons: Initially, the removal of memorizing neurons causes the train loss to increase and the test loss to decrease. In the latter half, once we start removing the periodic neurons, both train and test losses increase. (Right) Pruning from high IPR neurons: Initially, both train and test losses increase due to the removal of periodic neurons. In the latter half, the removal of the memorizing neurons causes the train and test losses to increase and decrease, respectively.\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from lowest IPR)\n0.008\n0.009\n0.010\nM SE\nlo ss\nPruning Memorizing (low IPR) neurons\n0.0 0.2 0.4 0.6 0.8 1.0 fraction of neurons pruned\n(from highest IPR)\n0.008\n0.009\n0.010\nM SE\nlo ss\nPruning Generalizing (high IPR) neurons\ntraining set test set\n(c) Full Inversion phase (weight decay = 15.0). (Left) Pruning from low IPR neurons monotonically increases the train loss, since it is affected by removal of memorizing and periodic neurons. On the other hand, test loss briefly remains constant (during the removal of the low IPR neurons), followed by a monotonic increase. (Right) Removing high IPR neurons monotonically increases both train and test losses.\nFigure 18: Pruning experiments in all various phases (N = 500, \u03b1 = 0.5, \u03be = 0.35). Pruning affects generalization and memorization differently in different phases because the proportion of high and low IPR neurons in trained networks depends on the phase.\nJ ADDITIONAL TRAINING CURVES"
        },
        {
            "heading": "K OUTPUT LOGITS",
            "text": "Here we show how the output logits of trained networks look like in various phases (i.e. different amount of weight decay) on corrupted training examples. In all cases, we see two peaks in logits, corresponding the the corrupted and the \u201ccorrect\u201d labels. Their relative value determines the degree of memorization, and hence, the phase."
        },
        {
            "heading": "L DETAILED PHASE DIAGRAMS",
            "text": "Here we replot the phase diagrams in Figure 3 together with train and test accuracies."
        },
        {
            "heading": "M CHOICE OF LOSS FUNCTION",
            "text": "CrossEntropy loss is known to be sensitive to label noise (Ghosh et al., 2017). By adding regularization, we recover similar phase diagrams. The requirement for regularization is much greater for CrossEntropy loss compared to MSE. We find that CrossEntropy training is very sensitive to weight decay, with Inversion occuring for a narrow band of weight decay values, beyond which we find Confusion. We do not observe a Forgetting phase in this case. See Figure 26. With Dropout, we find that CrossEntropy training requires very high dropout probabilities to grok and/or attain Inversion."
        },
        {
            "heading": "N CHOICE OF ACTIVATION FUNCTION",
            "text": "To check if the story depends on the activation function, we did the modular addition experiments for 2-layer ReLU network, see Figures 27 and 28. The requirement for regularization for grokking is higher compared to quadratic activation function. We noticed that the phase diagram for weight decay behaves similarly to quadratic activation functions, except for the disappearance of the Forgetting phase. Dropout does not seem as effective for ReLU networks, with the network entering Confusion phase for even moderate dropout probabilites.\nO EFFECT OF OF WIDTH"
        },
        {
            "heading": "P OTHER MODULAR ARITHMETIC DATASETS",
            "text": "In this section, we check if the phase diagram story is unique to modular addition. We repeat our experiments with modular multiplication (z = (mn)%p \u2261 mn mod p) and obtain almost identical phase diagrams to modular addition (see Figure 31)."
        },
        {
            "heading": "Q HOW MANY NEURONS CAN BE PRUNED WHILE RETAINING GENERALIZATION?",
            "text": "We showed in the pruning experiments (Figures 6, 17) that a significant proportion of neurons (starting from low IPR) can be pruned out without decreasing test error. In Figure 32 we show how many neurons can pruned from a trained network at various widths. We train networks with widths between 500 and 10000; with learning rate 0.005 and weight decay 1.0, with AdamW optimizer and MSE loss. We use corrupted modular addition data, with data fraction \u03b1 = 0.5 and corruption fraction \u03be = 0.35. We repeat the experiment 40 times, averaging over different random seeds."
        }
    ],
    "year": 2024
}