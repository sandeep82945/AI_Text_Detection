{
    "abstractText": "Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as data-constrained backdoor attacks. In such cases, previous attack methods suffer from severe efficiency degradation due to the entanglement between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: Clean Feature Suppression and Poisoning Feature Augmentation The results demonstrate remarkable improvements, with some settings achieving over 100% improvement compared to existing attacks in data-constrained scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "BACKDOOR ATTACKS"
        },
        {
            "affiliations": [],
            "name": "REAL-WORLD SCENARIOS"
        },
        {
            "affiliations": [],
            "name": "Ziqiang Li"
        },
        {
            "affiliations": [],
            "name": "Hong"
        },
        {
            "affiliations": [],
            "name": "Sun"
        },
        {
            "affiliations": [],
            "name": "Pengfei Xia"
        },
        {
            "affiliations": [],
            "name": "Heng"
        },
        {
            "affiliations": [],
            "name": "Li"
        },
        {
            "affiliations": [],
            "name": "Beihao Xia"
        },
        {
            "affiliations": [],
            "name": "Yi"
        },
        {
            "affiliations": [],
            "name": "Wu"
        },
        {
            "affiliations": [],
            "name": "Bin Li"
        }
    ],
    "id": "SP:98784697a204f23ee114e50446fa4441ce9a82ad",
    "references": [
        {
            "authors": [
                "Arto",
                "Dev Vidhani",
                "Goutham",
                "Mayank Bhaskar",
                "Ritobrata Ghosh",
                "Sujit Pal"
            ],
            "title": "Fine tuning clip with remote sensing (satellite) images and captions",
            "year": 2021
        },
        {
            "authors": [
                "Eugene Bagdasaryan",
                "Vitaly Shmatikov"
            ],
            "title": "Blind backdoors in deep learning models",
            "venue": "In 30th USENIX Security Symposium (USENIX Security",
            "year": 2021
        },
        {
            "authors": [
                "Mauro Barni",
                "Kassem Kallas",
                "Benedetta Tondi"
            ],
            "title": "A new backdoor attack in cnns by training set corruption without label poisoning",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2019
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Bo Li",
                "Kimberly Lu",
                "Dawn Song"
            ],
            "title": "Targeted backdoor attacks on deep learning systems using data poisoning",
            "venue": "arXiv preprint arXiv:1712.05526,",
            "year": 2017
        },
        {
            "authors": [
                "Ruizhe Cheng",
                "Bichen Wu",
                "Peizhao Zhang",
                "Peter Vajda",
                "Joseph E Gonzalez"
            ],
            "title": "Data-efficient language-supervised zero-shot learning with self-distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Siyuan Cheng",
                "Yingqi Liu",
                "Shiqing Ma",
                "Xiangyu Zhang"
            ],
            "title": "Deep feature space trojan attack of neural networks by controlled detoxification",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Khoa Doan",
                "Yingjie Lao",
                "Ping Li"
            ],
            "title": "Backdoor attack with imperceptible input and latent modification",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nikolaus Dr\u00e4ger",
                "Yonghao Xu",
                "Pedram Ghamisi"
            ],
            "title": "Backdoor attacks for remote sensing data with wavelet transform",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing,",
            "year": 2023
        },
        {
            "authors": [
                "Muhammad Farooq",
                "Abdul Hafeez"
            ],
            "title": "Covid-resnet: A deep learning framework for screening of covid19 from radiographs",
            "venue": "arXiv preprint arXiv:2003.14395,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Feng",
                "Benteng Ma",
                "Jing Zhang",
                "Shanshan Zhao",
                "Yong Xia",
                "Dacheng Tao"
            ],
            "title": "Fiba: Frequencyinjection based backdoor attack in medical image analysis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yansong Gao",
                "Bao Gia Doan",
                "Zhi Zhang",
                "Siqi Ma",
                "Jiliang Zhang",
                "Anmin Fu",
                "Surya Nepal",
                "Hyoungshick Kim"
            ],
            "title": "Backdoor attacks and countermeasures on deep learning: A comprehensive review",
            "year": 2007
        },
        {
            "authors": [
                "Yinghua Gao",
                "Yiming Li",
                "Linghui Zhu",
                "Dongxian Wu",
                "Yong Jiang",
                "Shu-Tao Xia"
            ],
            "title": "Not all samples are born equal: Towards effective clean-label backdoor attacks",
            "venue": "Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yunjie Ge",
                "Qian Wang",
                "Baolin Zheng",
                "Xinlu Zhuang",
                "Qi Li",
                "Chao Shen",
                "Cong Wang"
            ],
            "title": "Antidistillation backdoor attacks: Backdoors can really survive in knowledge distillation",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Micah Goldblum",
                "Dimitris Tsipras",
                "Chulin Xie",
                "Xinyun Chen",
                "Avi Schwarzschild",
                "Dawn Song",
                "Aleksander Madry",
                "Bo Li",
                "Tom Goldstein"
            ],
            "title": "Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gu",
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Badnets: Evaluating backdooring attacks on deep neural networks",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Wei Guo",
                "Benedetta Tondi",
                "Mauro Barni"
            ],
            "title": "A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Quan Huynh-Thu",
                "Mohammed Ghanbari"
            ],
            "title": "Scope of validity of psnr in image/video quality assessment",
            "venue": "Electronics letters,",
            "year": 2008
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Chaoran Li",
                "Xiao Chen",
                "Derui Wang",
                "Sheng Wen",
                "Muhammad Ejaz Ahmed",
                "Seyit Camtepe",
                "Yang Xiang"
            ],
            "title": "Backdoor attack on machine learning based android malware detectors",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Shaofeng Li",
                "Minhui Xue",
                "Benjamin Zi Hao Zhao",
                "Haojin Zhu",
                "Xinpeng Zhang"
            ],
            "title": "Invisible backdoor attacks on deep neural networks via steganography and regularization",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Shaofeng Li",
                "Hui Liu",
                "Tian Dong",
                "Benjamin Zi Hao Zhao",
                "Minhui Xue",
                "Haojin Zhu",
                "Jialiang Lu"
            ],
            "title": "Hidden backdoors in human-centric language models",
            "venue": "In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security,",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Li",
                "Yong Jiang",
                "Zhifeng Li",
                "Shu-Tao Xia"
            ],
            "title": "Backdoor learning: A survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yuezun Li",
                "Yiming Li",
                "Baoyuan Wu",
                "Longkang Li",
                "Ran He",
                "Siwei Lyu"
            ],
            "title": "Invisible backdoor attack with sample-specific triggers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ziqiang Li",
                "Chaoyue Wang",
                "Heliang Zheng",
                "Jing Zhang",
                "Bin Li"
            ],
            "title": "Fakeclr: Exploring contrastive learning for solving latent discontinuity in data-efficient gans",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Ziqiang Li",
                "Pengfei Xia",
                "Rentuo Tao",
                "Hongjing Niu",
                "Bin Li"
            ],
            "title": "A new perspective on stabilizing gans training: Direct adversarial training",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Ziqiang Li",
                "Hong Sun",
                "Pengfei Xia",
                "Beihao Xia",
                "Xue Rui",
                "Wei Zhang",
                "Bin Li"
            ],
            "title": "A proxy-free strategy for practically improving the poisoning efficiency in backdoor attacks",
            "venue": "arXiv preprint arXiv:2306.08313,",
            "year": 2023
        },
        {
            "authors": [
                "Ziqiang Li",
                "Muhammad Usman",
                "Rentuo Tao",
                "Pengfei Xia",
                "Chaoyue Wang",
                "Huanhuan Chen",
                "Bin Li"
            ],
            "title": "A systematic survey of regularization and normalization in gans",
            "venue": "ACM Comput. Surv.,",
            "year": 2023
        },
        {
            "authors": [
                "Ziqiang Li",
                "Pengfei Xia",
                "Xue Rui",
                "Bin Li"
            ],
            "title": "Exploring the effect of high-frequency components in gans training",
            "venue": "ACM Transactions on Multimedia Computing, Communications and Applications,",
            "year": 2023
        },
        {
            "authors": [
                "Ziqiang Li",
                "Pengfei Xia",
                "Hong Sun",
                "Yueqi Zeng",
                "Wei Zhang",
                "Bin Li"
            ],
            "title": "Explore the effect of data selection on poison efficiency in backdoor attacks",
            "venue": "arXiv preprint arXiv:2310.09744,",
            "year": 2023
        },
        {
            "authors": [
                "Cong Liao",
                "Haoti Zhong",
                "Anna Squicciarini",
                "Sencun Zhu",
                "David Miller"
            ],
            "title": "Backdoor embedding in convolutional neural network models via invisible perturbation",
            "venue": "arXiv preprint arXiv:1808.10307,",
            "year": 2018
        },
        {
            "authors": [
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Fine-pruning: Defending against backdooring attacks on deep neural networks",
            "venue": "In International symposium on research in attacks, intrusions, and defenses,",
            "year": 2018
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig"
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Yunfei Liu",
                "Xingjun Ma",
                "James Bailey",
                "Feng Lu"
            ],
            "title": "Reflection backdoor: A natural backdoor attack on deep neural networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kaushalya Madhawa",
                "Raul Carlomagno"
            ],
            "title": "Medclip: A pre-trained clip model for medical image search, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Rui Min",
                "Zeyu Qin",
                "Li Shen",
                "Minhao Cheng"
            ],
            "title": "Towards stable backdoor purification through feature shift tuning",
            "venue": "arXiv preprint arXiv:2310.01875,",
            "year": 2023
        },
        {
            "authors": [
                "Seyed-Mohsen Moosavi-Dezfooli",
                "Alhussein Fawzi",
                "Omar Fawzi",
                "Pascal Frossard"
            ],
            "title": "Universal adversarial perturbations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Anh Nguyen",
                "Anh Tran"
            ],
            "title": "Wanet\u2013imperceptible warping-based backdoor attack",
            "venue": "arXiv preprint arXiv:2102.10369,",
            "year": 2021
        },
        {
            "authors": [
                "Thien Duc Nguyen",
                "Phillip Rieger",
                "Markus Miettinen",
                "Ahmad-Reza Sadeghi"
            ],
            "title": "Poisoning attacks on federated learning-based iot intrusion detection system",
            "venue": "In Proc. Workshop Decentralized IoT Syst. Secur.(DISS),",
            "year": 2020
        },
        {
            "authors": [
                "Tuan Anh Nguyen",
                "Anh Tran"
            ],
            "title": "Input-aware dynamic backdoor attack",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hongjing Niu",
                "Hanting Li",
                "Feng Zhao",
                "Bin Li"
            ],
            "title": "Domain-unified prompt representations for source-free domain generalization",
            "venue": "arXiv preprint arXiv:2209.14926,",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Pan",
                "Mi Zhang",
                "Beina Sheng",
                "Jiaming Zhu",
                "Min Yang"
            ],
            "title": "Hidden trigger backdoor attack on {NLP} models via linguistic style manipulation",
            "venue": "In 31st USENIX Security Symposium (USENIX Security",
            "year": 2022
        },
        {
            "authors": [
                "Or Patashnik",
                "Zongze Wu",
                "Eli Shechtman",
                "Daniel Cohen-Or",
                "Dani Lischinski"
            ],
            "title": "Styleclip: Textdriven manipulation of stylegan imagery",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Adnan Siraj Rakin",
                "Zhezhi He",
                "Deliang Fan"
            ],
            "title": "Tbt: Targeted neural network attack with bit trojan",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aniruddha Saha",
                "Akshayvarun Subramanya",
                "Hamed Pirsiavash"
            ],
            "title": "Hidden trigger backdoor attacks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Hossein Souri",
                "Micah Goldblum",
                "Liam Fowl",
                "Rama Chellappa",
                "Tom Goldstein"
            ],
            "title": "Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch",
            "venue": "arXiv preprint arXiv:2106.08970,",
            "year": 2021
        },
        {
            "authors": [
                "Hossein Souri",
                "Liam Fowl",
                "Rama Chellappa",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Turner",
                "Dimitris Tsipras",
                "Aleksander Madry"
            ],
            "title": "Label-consistent backdoor attacks",
            "venue": "arXiv preprint arXiv:1912.02771,",
            "year": 2019
        },
        {
            "authors": [
                "Bolun Wang",
                "Yuanshun Yao",
                "Shawn Shan",
                "Huiying Li",
                "Bimal Viswanath",
                "Haitao Zheng",
                "Ben Y Zhao"
            ],
            "title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2019
        },
        {
            "authors": [
                "Cheng Wang",
                "Delei Chen",
                "Lin Hao",
                "Xuebo Liu",
                "Yu Zeng",
                "Jianwei Chen",
                "Guokai Zhang"
            ],
            "title": "Pulmonary image classification based on inception-v3 transfer learning model",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Wang",
                "Surya Nepal",
                "Carsten Rudolph",
                "Marthie Grobler",
                "Shangyu Chen",
                "Tianle Chen"
            ],
            "title": "Backdoor attacks against transfer learning with pre-trained deep learning models",
            "venue": "IEEE Transactions on Services Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing,",
            "year": 2004
        },
        {
            "authors": [
                "Long Wen",
                "Xinyu Li",
                "Liang Gao"
            ],
            "title": "A transfer convolutional neural network for fault diagnosis based on resnet-50",
            "venue": "Neural Computing and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "Conghao Wong",
                "Beihao Xia",
                "Ziming Hong",
                "Qinmu Peng",
                "Wei Yuan",
                "Qiong Cao",
                "Yibo Yang",
                "Xinge You"
            ],
            "title": "View vertically: A hierarchical network for trajectory prediction via fourier spectrums",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Tong Wu",
                "Tianhao Wang",
                "Vikash Sehwag",
                "Saeed Mahloujifar",
                "Prateek Mittal"
            ],
            "title": "Just rotate it: Deploying backdoor attacks via rotation transformation",
            "venue": "arXiv preprint arXiv:2207.10825,",
            "year": 2022
        },
        {
            "authors": [
                "Yi Wu",
                "Ziqiang Li",
                "Chaoyue Wang",
                "Heliang Zheng",
                "Shanshan Zhao",
                "Bin Li",
                "Dacheng Ta"
            ],
            "title": "Domain re-modulation for few-shot generative domain adaptation",
            "venue": "arXiv preprint arXiv:2302.02550,",
            "year": 2023
        },
        {
            "authors": [
                "Beihao Xia",
                "Conghao Wong",
                "Qinmu Peng",
                "Wei Yuan",
                "Xinge You"
            ],
            "title": "Cscnet: Contextual semantic consistency network for trajectory prediction in crowded spaces",
            "venue": "Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Xia",
                "Ziqiang Li",
                "Wei Zhang",
                "Bin Li"
            ],
            "title": "Data-efficient backdoor attacks",
            "venue": "arXiv preprint arXiv:2204.12281,",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Xia",
                "Hongjing Niu",
                "Ziqiang Li",
                "Bin Li"
            ],
            "title": "Enhancing backdoor attacks with multi-level mmd regularization",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Xia",
                "Yueqi Zeng",
                "Ziqiang Li",
                "Wei Zhang",
                "Bin Li"
            ],
            "title": "Efficient trojan injection: 90% attack success rate using 0.04% poisoned",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoling Xia",
                "Cui Xu",
                "Bing Nan"
            ],
            "title": "Inception-v3 for flower classification",
            "year": 2017
        },
        {
            "authors": [
                "Limin Yang",
                "Wenbo Guo",
                "Qingying Hao",
                "Arridhana Ciptadi",
                "Ali Ahmadzadeh",
                "Xinyu Xing",
                "Gang Wang"
            ],
            "title": "Cade: Detecting and explaining concept drift samples for security applications",
            "venue": "In USENIX security symposium,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Yang",
                "Shawn Newsam"
            ],
            "title": "Bag-of-visual-words and spatial extensions for land-use classification",
            "venue": "In Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems,",
            "year": 2010
        },
        {
            "authors": [
                "Yi Zeng",
                "Won Park",
                "Z Morley Mao",
                "Ruoxi Jia"
            ],
            "title": "Rethinking the backdoor attacks\u2019 triggers: A frequency perspective",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Zeng",
                "Minzhou Pan",
                "Hoang Anh Just",
                "Lingjuan Lyu",
                "Meikang Qiu",
                "Ruoxi Jia"
            ],
            "title": "Narcissus: A practical clean-label backdoor attack with limited information",
            "venue": "arXiv preprint arXiv:2204.05255,",
            "year": 2022
        },
        {
            "authors": [
                "Yueqi Zeng",
                "Ziqiang Li",
                "Pengfei Xia",
                "Lei Liu",
                "Bin Li"
            ],
            "title": "Efficient trigger word insertion",
            "venue": "arXiv preprint arXiv:2311.13957,",
            "year": 2023
        },
        {
            "authors": [
                "Shihao Zhao",
                "Xingjun Ma",
                "Xiang Zheng",
                "James Bailey",
                "Jingjing Chen",
                "Yu-Gang Jiang"
            ],
            "title": "Cleanlabel backdoor attacks on video recognition models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Haoti Zhong",
                "Cong Liao",
                "Anna Cinzia Squicciarini",
                "Sencun Zhu",
                "David Miller"
            ],
            "title": "Backdoor embedding in convolutional neural network models via invisible perturbation",
            "venue": "In Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy,",
            "year": 2020
        },
        {
            "authors": [
                "Zhong"
            ],
            "title": "2021). However, this approach requires a pre-trained clean model on the training set, which is not practical for data-constrained backdoor attacks. Selecting Efficient Poisoning Samples Efficient sample selection for poisoning attacks is a critical yet under-explored aspect that is distinct from trigger design",
            "year": 2022
        },
        {
            "authors": [
                "Guo"
            ],
            "title": "to investigate the contribution of different data to backdoor injection. Their research revealed that not all poisoning samples contribute equally, and appropriate sample selection can greatly enhance the efficiency of data in backdoor attacks",
            "venue": "Additionally, various studies Li et al. (2023a;d); Gao et al",
            "year": 2023
        },
        {
            "authors": [
                "Li"
            ],
            "title": "first propose a blended strategy to evade human detection by blending clean samples with the trigger to create poisoning samples",
            "venue": "Subsequent studies Zhong et al",
            "year": 2022
        },
        {
            "authors": [
                "Turner"
            ],
            "title": "poisoning samples align with their perception",
            "year": 2020
        },
        {
            "authors": [
                "Zeng"
            ],
            "title": "perturbed inputs from the target class and trigger-inserted inputs from the non-target class through pretraining on the entire training",
            "year": 2022
        },
        {
            "authors": [
                "BadNets. BadNets Gu"
            ],
            "title": "2019) is the pioneering backdoor attack in deep learning and is often used as a benchmark for subsequent research. It utilizes a 2\u00d7 2 attacker-specified pixel patch as the universal trigger pattern attached to benign samples",
            "venue": "Blended. Chen et al",
            "year": 2017
        },
        {
            "authors": [
                "Li"
            ],
            "title": "Adversarial Perturbations (UAP). Inspired by Universal Adversarial Perturbations (UAPs) in adversarial examples, some studies",
            "year": 2021
        },
        {
            "authors": [
                "Zhong"
            ],
            "title": "The setting of number-constrained backdoor attacks is similar to that of data-efficient backdoor attacks discussed in previous studies",
            "venue": "Xia et al. (2022b); Zhong et al",
            "year": 2020
        },
        {
            "authors": [
                "al. Xia"
            ],
            "title": "Adversarial Perturbations (UAPs) in adversarial examples and propose to optimize a UAP on a clean model pre-trained on the training set as the natural trigger",
            "year": 2022
        },
        {
            "authors": [
                "backdoor attacks Turner"
            ],
            "title": "In clean-label backdoor attacks, the accessible clean training set D\u2032 is defined as D",
            "year": 2021
        },
        {
            "authors": [
                "Grad-CAM Selvaraju"
            ],
            "title": "2017) to more effectively corrob3In the clean-label single-class backdoor attack, the benign feature of the accessible class",
            "year": 2017
        },
        {
            "authors": [
                "Li"
            ],
            "title": "The objective of our paper is aligned with popular backdoor attacks, as seen in previous studies Gu et al",
            "year": 2021
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "Attackers\u2019 prior knowledge. In order to simulate a realistic scenario, we assume that the attackers have no access to the models or training details. They possess only general knowledge about the class labels involved in the task. This assumption reflects a more challenging and practical setting, where attackers have limited information about the target system. Attackers",
            "year": 2019
        },
        {
            "authors": [
                "Xia"
            ],
            "title": "2023) argue that deep models inherently possess flaws, and it is easier to exploit and enhance an existing flaw to serve as a backdoor rather than implanting a new one from scratch (BadNets Gu et al",
            "venue": "Blended Chen et al",
            "year": 2017
        },
        {
            "authors": [
                "Krizhevsky"
            ],
            "title": "CIFAR-10 is a tiny object classification dataset containing 50,000 training images and 10,000 testing images. Each image has a size",
            "year": 2009
        },
        {
            "authors": [
                "CIFAR-100 Krizhevsky"
            ],
            "title": "Similar to CIFAR-10, CIFAR-100 is also a tiny object",
            "year": 2009
        },
        {
            "authors": [
                "Deng"
            ],
            "title": "ImageNet is the most popular object classification dataset containing 1.3M training images and 50K testing images. Each image has a size",
            "year": 2009
        },
        {
            "authors": [
                "Simonyan"
            ],
            "title": "ResNet-18 He et al. (2016), and MobileNet-V2",
            "year": 2014
        },
        {
            "authors": [
                "fication Xia"
            ],
            "title": "2017), pulmonary image classification Wang et al. (2019b), fault diagnosis",
            "year": 2019
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "All of them are independent of the training data therefore can be implemented",
            "year": 2017
        },
        {
            "authors": [],
            "title": "Dt), containing the images whose label is not the attack-target class k. Peak Signal-to-noise Ratio (PSNR) Huynh-Thu",
            "year": 2008
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2018) on the number-constrained backdoor attack of the CIFAR-100 dataset and VGG-16 model, where the number of clean samples owned by the defender is 50 and the poisoned rate is 2",
            "year": 2018
        },
        {
            "authors": [
                "Neural cleanse Wang"
            ],
            "title": "2019a) on the number-constrained backdoor attack of the CIFAR-100 dataset and ResNet-18 model, where the number of clean samples owned by the defender is 1500, and the poisoned rate is 2",
            "year": 2019
        },
        {
            "authors": [
                "FST Min"
            ],
            "title": "2023) on the number-constrained backdoor attack of the CIFAR-100 dataset and ResNet-18 model, where the number of clean samples owned by the defender is from 1000 to 2500 and the poisoned rate is 2",
            "year": 2023
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "The proposed CLIP-CFE method further improves the poisoning effectiveness with different triggers",
            "year": 2017
        },
        {
            "authors": [
                "Blended Chen"
            ],
            "title": "Additionally, our proposed CLIP-CFE method has no negative effect on BA, confirming that our technologies are harmless for benign accuracy under various settings and different backdoor attacks",
            "venue": "EXPERIMENTS ON MORE COMPLEX CONSTRAINTS IN DATA-CONSTRAINT BACKDOOR",
            "year": 2017
        },
        {
            "authors": [
                "Satellite Arto"
            ],
            "title": "2021) model, a large model fine-tuned on remote sensing images, further augmented the attack success rate. These outcomes demonstrate the adaptability of our methods to various domains by replacing CLIP with domain-specific pre-trained models",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural networks (DNNs) are powerful ML algorithms inspired by the human brain, used in various applications such as image recognition He et al. (2016), natural language processing Liu et al. (2023), image generation Li et al. (2023b; 2022c;b); Wu et al. (2023), and trajectory prediction Wong et al. (2022); Xia et al. (2022a). DNN effectiveness depends on training data quantity/quality. For example, Stable Diffusion (983M parameters) Rombach et al. (2022) excels in image generation due to pre-training on 5B image-text pairs. As the demand for data continues to rise, many users and businesses resort to third-party sources or online collections as a convenient means of acquiring the necessary data. However, recent studies Pan et al. (2022); Li et al. (2021b); Yang et al. (2021) have demonstrated that such practices can be maliciously exploited by attackers to contaminate the training data, significantly impairing the functionality and reliability of trained models.\nThe growing adoption of neural networks across different domains has made them an attractive target for malicious attacks. One particular attack technique gaining attention is the backdoor attack Goldblum et al. (2022); Nguyen & Tran (2020); Xia et al. (2022c). In backdoor attacks, a neural network is deliberately injected with a hidden trigger by introducing a small number of poisoning samples into the benign training set during the training. Once the model is deployed, the attacker can activate the backdoor by providing specific inputs containing the hidden trigger, causing the model to produce incorrect results. Backdoor attacks continue to present a significant and pervasive threat across multiple sectors, including image classification Gu et al. (2019), natural language processing Pan et al. (2022); Zeng et al. (2023), and malware detection Li et al. (2021a). In this paper, we focus on the widely studied field of image classification.\nIt\u2019s worth noting that previous backdoor attacks rely on a potentially overly broad assumption. They assume that all training data comes from a single source, and the collected source has been poisoned\n*The first two authors contributed equally to this paper. \u2020Corresponding Author: Bin Li.\nby attacker (as shown in O2O data collection mode in Fig. 1). This assumption grants attackers full access to the entire training dataset, making it easy to poison. However, it doesn\u2019t accurately represent real-world attack scenarios. Consider a scenario where victims have a limited sample private dataset. To compensate, they may augment it by collecting additional data from various online sources (referred to as the public dataset) and combine it with their private data for training, as depicted in the M2O data collection mode in Fig. 1. In this case, some of the sources may be secretly poisoned by attackers. Attackers cannot access the private dataset and can only manipulate a portion of the public dataset for poisoning. Consequently, a discrepancy arises between the distribution of the poisoned data and the training data, deviating from previous poisoning attack pipeline.\nIn this paper, we address a more realistic backdoor attack scenario called data-constrained backdoor attacks, where the attackers do not have access to the entire training set. To be more precise, we classify data-constrained backdoor attacks into three types based on different types of data sources: number-constrained backdoor attacks, class-constrained backdoor attacks, and domainconstrained backdoor attacks. Upon investigation, we have discovered that existing attack methods exhibit significant performance degradation when dealing with these data-constrained backdoor attacks. We propose that the entanglement between benign and poisoning features is a crucial factor contributing to this phenomenon. Entanglement refers to the neural networks utilizing both benign and poisoning features to make decisions for poisoning samples. However, this approach is not efficient for backdoor attacks. Ideally, an efficient backdoor attack should solely rely on the poison feature generated by the trigger to make decisions, irrespective of how the benign feature is expressed. To enhance the efficiency of poisoning attacks in data-constrained backdoor scenarios, we introduce two streams: Clean Feature Suppression and Poisoning Feature Augmentation to reduce the influence of clean features and amplify the expression of poisoning features, respectively. To achieve these goals, we propose three techniques utilizing the CLIP Radford et al. (2021).\nOur main contributions are summarized as follows. i) We present a novel and contemporary backdoor attack scenario called data-constrained backdoor attacks, which assumes that attackers lack access to the entire training data, making it a versatile and practical attack with broad applicability. ii) Through a systematic analysis of previous attack methods, we identify the entanglement between poisoning and benign features as the primary contributing factor to their performance degradation. iii) To address this issue, we introduce the pre-trained CLIP model into the field of backdoor attacks for the first time. We propose three innovative technologies: CLIP-CFE, CLIP-UAP, and CLIP-CFA. Extensive evaluations conducted on 3 datasets and 3 target models, and over 15 different settings demonstrate the significant superiority of our proposed CLIP-UAP and CLIP-CFA over existing backdoor attacks. Furthermore, CLIP-CFE complements existing attack methods and can be seamlessly integrated with them, resulting in further efficiency improvements."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": "Here, we provide a concise overview of the typical pipeline for backdoor attacks on neural networks. Sec. A.1 provides a comprehensive exploration of related work on backdoor attacks and CLIP."
        },
        {
            "heading": "2.1 BACKDOOR ATTACKS ON NEURAL NETWORKS",
            "text": "Backdoor attacks aim to introduce hidden triggers into DNNs, allowing the attacked models to behave correctly on clean samples while exhibiting malicious behavior when triggered by specific inputs. These attacks can occur at various stages of Artificial Intelligence (AI) system development Gao et al. (2020). The surface of backdoor attacks has been systematically categorized into six groups: code-based Bagdasaryan & Shmatikov (2021), outsourcing, pretrained model-based Wang et al. (2020); Ge et al. (2021), poisoning-based Liao et al. (2018), collaborative learning-based Nguyen et al. (2020), and post-deployment attacks Rakin et al. (2020). Among these categories, poisoning-based backdoor attacks, which involve introducing a backdoor trigger during the training process by mixing a few poisoning samples, are the most straightforward and commonly used method. This study focuses on addressing concerns related to poisoning-based backdoor attacks."
        },
        {
            "heading": "2.2 GENERAL PIPELINE OF BACKDOOR ATTACKS",
            "text": "Consider a learning model f(\u00b7; \u0398) : X \u2192 Y , where \u0398 represents the model\u2019s parameters and X(Y ) denotes the input (output) space, with given dataset D \u2282 X\u00d7Y . Backdoor attacks typically involve three essential steps: poisoning set generation, backdoor injection, and backdoor activation.\nPoisoning set generation. In this step, attackers employ a pre-defined poison generator T (x, t) to introduce a trigger t into a clean sample x. Specifically, they select a subset P \u2032 = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , P} from the clean training set D = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , N} (P \u2032 \u2282 D, and P \u226a N ) and result in the corresponding poisoning set P = {(x\u2032i, k)|x\u2032i = T (xi, t), (xi, yi) \u2208 P \u2032 , i = 1, \u00b7 \u00b7 \u00b7 , P}. Here, yi and k represent the true label and the attack-target label of the clean sample xi and the poisoning sample x\u2032i, respectively.\nBackdoor injection. In this step, The attackers mix the poisoning set P into the clean training set D and release the new dataset. The victims download the poisoning dataset and use it to train their own DNN models Gu et al. (2019):\nmin \u0398\n1\nN \u2211 (x,y)\u2208D L (f(x; \u0398), y) + 1 P \u2211 (x\u2032,k)\u2208P L (f(x\u2032; \u0398), k), (1)\nwhere L is the classification loss such as the commonly used cross entropy loss. In this case, backdoor injection into DNNs has been completed silently.\nBackdoor activation. In this step, the victims deploy their compromised DNN models on modelsharing platforms and model-selling platforms. The compromised model behaves normally when presented with benign inputs, but attackers can manipulate its predictions to align with their malicious objectives by providing specific samples containing pre-defined triggers."
        },
        {
            "heading": "3 DATA-CONSTRAINED BACKDOOR ATTACKS",
            "text": "We first show the considered pipeline of data-constrained backdoor attacks, and than illustrate the performance degradation of previous methods on proposed pipeline. Finally, we attribute the degradation to the entanglement between the benign and poisoning features during the poisoning injection."
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "Previous methods Chen et al. (2017); Liu et al. (2017); Li et al. (2022a); Nguyen & Tran (2020) have commonly followed the attack pipeline outlined in Sec. 2.2. However, this widely adopted pipeline relies on an overly loose assumption that all training data is collected from a single source and that the attacker has access to the entire training data, which is often not the case in real-world attack scenarios. In this paper, we focus on a more realistic scenario: Data-constrained Backdoor Attacks.\nPipeline of data-constrained backdoor attacks. The proposed pipeline also consists of three steps: poisoning set generation, backdoor injection, and backdoor activation. The backdoor injection and activation steps remain unchanged from the previous attack pipeline. However, in the poisoning set generation step, data-constrained attacks only assume access to a clean training set D\u2032 = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , N \u2032}, which follows a different data distribution from D. To address this,\nthe attacker randomly selects a subset P \u2032 = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , P} from the accessible dataset D\u2032, and creates the corresponding poisoning set P = {(x\u2032i, k)|x\u2032i = T (xi, t), (xi, yi) \u2208 P \u2032 , i = 1, \u00b7 \u00b7 \u00b7 , P}. Additionally, based on the different constraints imposed by the accessible training set D\u2032, data-constrained backdoor attacks are further categorized into three types: Number-constrained Backdoor Attacks, Class-constrained Backdoor Attacks, and Domain-constrained Backdoor Attacks. The details pipeline can be found in Sec. A.2.3, Sec A.2.4, and Sec. A.2.5, respectively.\nEmpirical results. Noticing that the experimental setting of this experiment can be found in Sec. A.2.2. Fig. 2 (a), (b), and (c) illustrate the attack success rate on number-constrained, classconstrained, and domain-constrained backdoor attacks, respectively. The results demonstrate that ASR experiences a significant decrease as the number of poisoning samples (P ), the number of class (C \u2032), or domain rate (the proportion of poisoning sets sampled from D\\D\u2032 and D\u2032) in the poisoning set decreases, particularly for Blended backdoor attacks. It is worth noting that Universal Adversarial Perturbations (UAP) achieves relatively favorable results even with a low poisoning rate. This can be attributed to the utilization of a proxy model that is pre-trained on the entire training set (D). However, UAP is not accessible in our settings, and we present the results for UAP to effectively demonstrate the performance degradation even when a pre-trained proxy model is available."
        },
        {
            "heading": "3.2 ENTANGLEMENT BETWEEN BENIGN AND POISONING FEATURES",
            "text": "In Sec A.3, we present three observations pertaining to data-constrained backdoor attacks, serving as compelling evidence for the presence of a significant interdependence between benign and poisoning features during the backdoor injection. This intricate entanglement is identified as the primary factor responsible for the inadequacies exhibited by current attack methodologies within data-constrained scenarios. Our study is a pioneering exploration of feature entanglement in the context of backdoor attacks, yielding fresh and valuable insights into the realm of backdoor learning. Ideally, one would anticipate backdoor models to exclusively rely on poisoning features when confronted with poisoning samples, as this would be the most efficient strategy for executing successful backdoor attacks. However, neural networks tend to be greedy and utilize all features for decision-making Li et al. (2023c), leading to activation of both poisoning and benign features during backdoor injection. This results in reduced poisoning efficiency when there is a difference in benign features between the backdoor injection and activation phases, as evidenced in data-constrained backdoor attacks."
        },
        {
            "heading": "4 CLIP-GUIDED BACKDOOR ATTACKS METHOD",
            "text": "We present our approach, which consists of two components: Clean Feature Suppression and Poisoning Feature Augmentation. These components are independent of each other and can be seamlessly combined. The threat model considered in our study has been introduced in Sec. A.4"
        },
        {
            "heading": "4.1 CLEAN FEATURE SUPPRESSION",
            "text": "As described in Sec. 3, the effectiveness of data-constrained backdoor attacks is hindered due to the entanglement of benign and poisoning features during the backdoor injection phase. To address this challenge, we propose a solution called \u201dclean feature suppression\u201d in this section. The primary objective of this approach is to minimize the impact of benign features on the decision-making process, thus amplifying the significance of poisoning features."
        },
        {
            "heading": "4.1.1 CLIP-BASED CLEAN FEATURE ERASING",
            "text": "To achieve clean feature suppression, we can employ a feature extractor pre-trained on the entire training set (As shown in Sec. Clean Feature Erasing Noise). However, since our data-constrained backdoor attacks lack access to the complete training set, an alternative solution is required. Recent studies have shown that pre-trained CLIP Radford et al. (2021) generates consistent and robust semantic representations across a wide range of (image, text) pairs, enabling impressive zero-shot classification performance comparable to supervised learning accuracy on challenging datasets like ImageNet (As shown in A.5). Hence, we can utilize the pre-trained general model CLIP, which replaces the feature extractor trained on the entire training set, allowing us to achieve clean feature suppression (As shown in CLIP for Clean Feature Erasing).\nClean Feature Erasing Noise. The technique of clean feature suppression aims to eliminate the clean information present in images by introducing optimized noise, denoted as \u03b4, which helps modify the input image to resemble the unbiased class. In accordance with the data-constrained backdoor attack pipeline outlined in Sec. 3, we assume that the chosen clean training dataset for generating the poisoning set consists of P clean examples, denoted as P \u2032 \u2282 X \u00d7 Y (where P \u2032 = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , P}). Here, xi \u2208 X represents the inputs, yi \u2208 Y = {1, 2, \u00b7 \u00b7 \u00b7 , C} represents the labels, and C denotes the total number of classes. We refer to the modified version as Pe = {(xe,i, yi)|i = 1, \u00b7 \u00b7 \u00b7 , P}, where xe,i = xi+\u03b4i represents the erased version of the training example xi \u2208 P \u2032. The term \u03b4i \u2208 \u2206 denotes the \u201dinvisible\u201d noise applied to achieve the erasing effect. The noise \u03b4i is subject to the constraint ||\u03b4i||p \u2264 \u03f5, where || \u00b7 ||p represents the Lp norm, and \u03f5 is set to a small value to ensure the stealthiness of the backdoor attacks. Our objective in erasing the clean features is to ensure that the pre-trained feature extractor does not extract any meaningful information from the given images x. This is achieved by introducing customized and imperceptible noise, denoted as \u03b4i. To be more specific, for a clean example xi, we propose to generate the noise \u03b4i that erases the features by solving the following optimization problem:\n\u03b4i = argmin \u03b4i\nL(f \u2032(xi + \u03b4i), ym) s.t. ||\u03b4i||p \u2264 \u03f5, (2)\nwhere L represents the mean squared error (MSE) loss, defined as L(a, b) = ||a\u2212b||2. The function f \u2032(\u00b7) corresponds to the pre-trained feature extractor employed for noise generation. Additionally, ym denotes the unbiased label for the classification task, which is defined as ym = [ 1C , 1 C , \u00b7 \u00b7 \u00b7 , 1 C ], where C signifies the total number of classes. While this vanilla method proves effective in erasing clean features, it requires a proxy feature extractor that has been pre-trained on the entire training set. This approach is not suitable for our data-restricted backdoor attacks.\nCLIP for Clean Feature Erasing (CLIP-CFE). Taking inspiration from CLIP\u2019s approach to zeroshot classification (Sec. A.5), we leverage a general CLIP model to optimize the feature erasing noise. This allows us to relax the need for a proxy feature extractor pre-trained on the entire training set. We consider C prompts, \u201da photo of a ci,\u201d corresponding to different classes ci in the dataset, where i = 1, \u00b7 \u00b7 \u00b7 , C. The CLIP-based feature erasing noise, denoted as \u03b4i, is proposed for the input xi by solving the following optimization problem:\n\u03b4i = argmin \u03b4i\nL(fCLIP (xi + \u03b4i,P), ym) s.t. ||\u03b4i||p \u2264 \u03f5, (3)\nwhere L represents the mean squared error (MSE) loss, ym denotes the unbiased label for the classification task defined as ym = [ 1C , 1 C , \u00b7 \u00b7 \u00b7 , 1 C ], P represents the set of prompts corresponding to different classes in the dataset, and fCLIP denotes the CLIP-based model used to obtain the label of the input image. Specifically,\nP = {p1, p2, \u00b7 \u00b7 \u00b7 , pC} = {\u201da photo of a ci\u201d|i = 1, 2, \u00b7 \u00b7 \u00b7 , C}, (4)\nfCLIP (xi + \u03b4i,P) = [\n\u27e8E\u0302i(xi + \u03b4i), E\u0302t(p1)\u27e9\u2211C i=1\u27e8E\u0302i(xi + \u03b4i), E\u0302t(pi)\u27e9 , \u00b7 \u00b7 \u00b7 , \u27e8E\u0302i(xi + \u03b4i), E\u0302t(pC)\u27e9\u2211C i=1\u27e8E\u0302i(xi + \u03b4i), E\u0302t(pi)\u27e9\n] . (5)\nTo solve the constrained minimization problem illustrated in Eq. 3, we utilize the first-order optimization method known as Projected Gradient Descent (PGD) Madry et al. (2017). The PGD method enables us to find a solution by iteratively updating the noise as follows:\n\u03b4t+1i = \u220f \u03f5 ( \u03b4ti \u2212 \u03b1 \u00b7 sign(\u2207\u03b4L(fCLIP (xi + \u03b4ti ,P), ym)) ) , (6)\nwhere t represents the current perturbation step, with a total of T = 50 steps. \u2207\u03b4L(fCLIP (xi + \u03b4ti ,P), ym) denotes the gradient of the loss with respect to the input. The projection function \u220f is applied to restrict the noise \u03b4 within the \u03f5-ball (with \u03f5 = 8/255 in our paper) around the original example x, ensuring it does not exceed this boundary. The step size \u03b1 determines the magnitude of the noise update at each iteration. The resulting erasing examples are then obtained as follows:\nPe = {(xe,i, yi)|i = 1, \u00b7 \u00b7 \u00b7 , P}, where xe,i = xi + \u03b4Ti . (7)"
        },
        {
            "heading": "4.2 POISONING FEATURE AUGMENTATION",
            "text": "In addition to eradicating clean features in images to tackle the entanglement between benign and poisoning features, enhancing the expression of poisoning features is another effective approach. In this section, we present two parallel triggers aimed at augmenting the poisoning features: CLIPbased Contrastive Feature Augmentation (4.2.1) and CLIP-based Universal Adversarial Perturbations (A.6)."
        },
        {
            "heading": "4.2.1 CLIP-BASED CONTRASTIVE FEATURE AUGMENTATION",
            "text": "While the CLIP-UAP method has shown impressive results in terms of poisoning efficiency, it requires customization for different attack-target labels. In this section, we propose a more versatile trigger design that is independent of the attack-target label, enhancing the poisoning feature. Drawing inspiration from the entanglement between benign and poisoning features discussed in Sec. 3.2, we utilize contrastive optimization to augment the poisoning feature. Our expectation is that the poisoning feature extracted from the designed trigger will be more expressive compared to the clean feature extracted from the clean samples. Specifically, given a trigger \u03b4t+1con to be optimized, two\nrandom views (query: x+\u03b4con and key: x1+\u03b4con) are created by different clean samples (x and x1). Positive pair is defined as such query-key pair, between different poisoning samples. Negative pairs are defined as pairs between poisoning example and its corresponding clean example, i.e. between x + \u03b4con and x. All views are passed through the pre-trained image encoder E\u0302i(\u00b7) of the CLIP to acquire the representation v:\nvq = E\u0302i(x+ \u03b4con), v+ = E\u0302i(x1 + \u03b4con), v\u2212 = E\u0302i(x). (8)\nCLIP-Contrastive feature augmentation (CLIP-CFA) focuses on optimizing the general trigger by maximizing the similarity between positive pairs while ensuring dissimilarity between negative pairs. To achieve this, we design a loss function as follows:\nLcon(x, x1, \u03b4con) = \u2212 \u27e8vq, v+\u27e9 \u27e8vq, v\u2212\u27e9 , (9)\nwhere \u27e8\u00b7, \u00b7\u27e9 represents the cosine similarity between two vectors and the \u03b4con is optimized with:\n\u03b4con = argmin ||\u03b4con||p\u2264\u03f5 \u2211 (x,y)\u2208D\u2032 Lcon(x, x1, \u03b4con). (10)\nSimilar to Eq. 14, we also adopt the first-order optimization method PGD Madry et al. (2017) to solve the constrained minimization problem as follows:\n\u03b4t+1con = \u220f \u03f5 ( \u03b4tcon \u2212 \u03b1 \u00b7 sign(\u2207\u03b4conLcon(x, x1, \u03b4con)) ) . (11)\nTherefore, the optimization should also be accumulated on all samples in the accessible clean training set D\u2032. Finally, the CLIP-CFA of set D\u2032 can be formulated as \u03b4con = \u03b4Tcon, and the poison generator is formulated as T (x, \u03b4con) = x+ \u03b4con."
        },
        {
            "heading": "4.3 ATTACK SUMMARY",
            "text": "we present two independent trigger design methods: CLIP-UAP (Sec. A.6) and CLIP-CFA (Sec. 4.2.1). These triggers are aimed at enhancing the expression of poisoning features and can replace previous trigger design approaches, leading to improved performance in data-constrained backdoor attacks. Additionally, in Sec. 4.1, we introduce a CLIP-CFE method. This approach minimizes the influence of clean features during the poisoning process and can be integrated into any aforementioned trigger design methods. By combining trigger design and clean feature erasing, our final approach achieves state-of-the-art performance in all types of data-constrained backdoor attacks."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We provides an overview of the experimental settings, covering datasets, model architecture, evaluation metrics, baselines, and implementations (Appendix A.7). Subsequently, we perform comprehensive experiments to assess the effectiveness of our proposed methods through answering the following research questions: RQ1: Are proposed technologies effective on three backdoor attacks? (Sec. 5.1). RQ2: Are proposed technologies harmless for Benign Accuracy? (Sec. 5.2). RQ3: Are proposed technologies stealthy for victims? (Sec. A.8). RQ4: Are proposed technologies effective for different poisoning settings? (Sec. 5.3). In this section, we present the results specifically for CIFAR-100 datasets. Experimental outcomes for CIFAR-10 and ImageNet-50 are provided in Appendix A.9 and Appendix A.10 respectively. Additionally, for more experiments and further discussions, please refer to Appendix A.11, A.12, A.13, A.14, and A.15."
        },
        {
            "heading": "5.1 RQ1: ARE PROPOSED TECHNOLOGIES EFFECTIVE ON THREE BACKDOOR ATTACKS?",
            "text": "To assess the effectiveness of our proposed technologies, we conduct attacks on various target models and datasets, evaluating the Attack Success Rate (ASR) for each target model. In order to establish a basis for comparison, we introduce two baseline attack methods: BadNets Gu et al. (2019) and Blended Chen et al. (2017), as discussed in Sec. A.2.1. Fig. 3 illustrates the performance of the following types of backdoor attacks on the CIFAR-100 dataset: (a) number-constrained, (b)\nclean-label single-class (class-constrained), (c) dirty-label single-class (class-constrained), and (d) out-of-the-domain (domain-constrained)1.\nCLIP-based poisoning feature augmentation is more effective than previous attack methods. Our proposed methods, CLIP-UAP and CLIP-CFA, outperform the baseline techniques (BadNets Gu et al. (2019) and Blended Chen et al. (2017) 2) in terms of consistency across different attacks and target models. Specifically, we achieved an ASR of 0.878, 0.825, 0.984, and 0.988 for BadNets, Blended, CLIP-UAP, and CLIP-CFA, respectively, in the number-constrained backdoor attack on the VGG-16 model. These results provide evidence that our proposed poisoning feature augmentation generates more effective triggers compared to other methods.\nCLIP-based Clean Feature Suppression is useful for different attack methods. Our proposed method, CLIP-CFE, has shown significant improvements in effectiveness compared to the baseline method without CLIP-CFE. In various cases, CLIP-CFE has enhanced the poisoning efficiency significantly. For instance, in the clean-label single-class backdoor attack on the VGG-16 dataset, we observed remarkable improvements of 187%, 150%, 110%, and 229% for BadNets, Blended, CLIP-UAP, and CLIP-CFA, respectively. However, it is worth noting that in the results of the domain-constrained backdoor attacks on MobileNet-V2 (as depicted in the right part of Fig. 3 (d)), CLIP-CFA and CLIP-UAP only slightly outperform the corresponding methods with CFE.\nMore discussion. While our technologies have shown significant improvements in poisoning efficiency compared to baselines, there are still important discussions that need to be addressed. We aim to provide answers to the following questions in a systematic manner in Appendix A.15: i) Why do we observe performance degradation in the clean-label single-class attack? ii) Why are domainconstrained backdoor attacks generally easier compared to class-constrained backdoor attacks?"
        },
        {
            "heading": "5.2 RQ2: ARE PROPOSED TECHNOLOGIES HARMLESS FOR BENIGN ACCURACY?",
            "text": "As shown in Table 1, our CLIP-UAP and CLIP-CFA exhibit similar or even better average Benign Accuracy (BA) compared to the baseline methods, BadNets Gu et al. (2019) and Blended Chen et al. (2017). Additionally, it is worth noting that our proposed method, CLIP-CFE, does not negatively impact BA. This finding confirms that our technologies are harmless to the benign accuracy compared to baseline methods, even under various settings and different backdoor attacks."
        },
        {
            "heading": "5.3 RQ4: ARE PROPOSED TECHNOLOGIES EFFECTIVE FOR DIFFERENT POISONING SETTINGS?",
            "text": "Experiments on different poison rates for number-constrained backdoor attacks. We conduct ablation studies to assess the effectiveness of our proposed methods in reducing the number of\n1Both the clean-label single-class and dirty-label single-class backdoor attacks represent extreme scenarios of the class-constrained backdoor attack. In the clean-label single-class attack, the targeted class category is set to Y \u2032 = k, while in the dirty-label single-class attack, it is set to Y \u2032 = c where c \u0338= k. Similarly, the out-of-the-domain backdoor attack is an extreme scenario of the domain-constrained backdoor attack, with a domain rate of 0. For further details, please refer to Appendix A.7.\n2While there are several more effective techniques for poisoning attacks, they typically necessitate access to the entire training data, rendering them unsuitable for our data-limited backdoor attacks.\npoisoning samples (poisoning rates) for number-constrained backdoor attacks. The results depicted in Fig. 4 (a) demonstrate the following: i) The attack success rate increases with higher poisoning rates for different attacks. ii) Our proposed CLIP-UAP and CLIP-CFA outperform the baseline techniques, BadNets Gu et al. (2019) and Blended Chen et al. (2017). iii) The incorporation of our proposed CLIP-CFE further enhances the poisoning effectiveness across different triggers.\nExperiments on different poison classes for class-constrained backdoor attacks. We conduct ablation studies to assess the effectiveness of our proposed methods in increasing the number of poisoning classes for class-constrained backdoor attacks. The results presented in Fig. 4 (b) demonstrate the following: i) The attack success rate increases with higher poisoning classes for different attacks. ii) The attack success rate of clean-label single-class attack is lower than that of dirty-label single-class attacks. iii) Our proposed methods, CLIP-UAP and CLIP-CFA, outperform the baseline techniques, BadNets Gu et al. (2019) and Blended Chen et al. (2017). iv) The incorporation of our proposed CLIP-CFE further enhances the poisoning effectiveness across different triggers.\nExperiments on different domain rates for domain-constrained backdoor attacks. We conduct ablation studies to assess the effectiveness of our methods in increasing the domain rate for domainconstrained backdoor attacks. The results depicted in Fig. 4 (c) demonstrate the following: i) The ASR increases with higher domain rates for different attacks. ii) Our proposed CLIP-UAP and CLIP-CFA outperform the baseline techniques, BadNets Gu et al. (2019) and Blended Chen et al. (2017). iii) The incorporation of our proposed CLIP-CFE further enhances the poisoning effectiveness across different triggers.\nExperiments on different large pre-trained models. We utilize the pre-trained CLIP model as the basis for our technologies. It\u2019s worth noting that the community has proposed various CLIP variants. Therefore, an important practical consideration is whether our proposed technologies remain robust when applied different pre-trained CLIP models. To investigate this, we conduct ablation studies on different CLIP models for number-constrained backdoor attacks, as depicted in Fig. 4 (d). The results demonstrate that our proposed technologies exhibit robustness across different CLIP models, with ViT-B/32 emerging as a competitive choice for all methods."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we address the challenges of data-constrained backdoor attacks, which occur in more realistic scenarios where victims collect data from multiple sources and attackers cannot access the full training data. To overcome the performance degradation observed in previous methods under data-constrained backdoor attacks, we propose three technologies from two streams that leverage the pre-trained CLIP model to enhance the efficiency of poisoning. Our goal is to inspire the research community to explore these realistic backdoor attack scenarios and raise awareness about the threats posed by such attacks. In the Sec A.16, we discuss the limitations of our approach and outline potential future directions for backdoor learning research."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was funded by the National Natural Science Foundation of China (U19B2044). Sponsored by Zhejiang Lab Open Research Project (NO. K2022QA0AB04). Supported by the Fundamental Research Funds for the Central Universities."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "CONTENTS",
            "text": "A.1 Detailed Background and Related Work . . . . . . . . . . . . . . . . . . . . . . . 16\nA.1.1 Poisoning Efficiency in Backdoor Attacks . . . . . . . . . . . . . . . . . . 16\nA.1.2 Poisoning Stealthiness in Backdoor Attacks . . . . . . . . . . . . . . . . . 16\nA.1.3 Contrastive Language-Image Pre-Training (CLIP) Model . . . . . . . . . . 17\nA.2 Detailed Pipeline of Data-constrained Backdoor Attacks. . . . . . . . . . . . . . . 17\nA.2.1 Examples of Backdoor Attacks in Our Study . . . . . . . . . . . . . . . . 17\nA.2.2 Experimental Settings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.2.3 Number-constrained Backdoor Attacks . . . . . . . . . . . . . . . . . . . 17\nA.2.4 Class-constrained Backdoor Attacks . . . . . . . . . . . . . . . . . . . . . 18\nA.2.5 Domain-constrained Backdoor Attacks . . . . . . . . . . . . . . . . . . . 19\nA.3 Analyzing the Entanglement Between Benign and Poisoning Features . . . . . . . 19\nA.4 Threat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA.5 CLIP for Zero-shot Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA.6 CLIP-based Universal Adversarial Perturbations . . . . . . . . . . . . . . . . . . . 22\nA.7 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nA.7.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nA.7.2 Model Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nA.7.3 Baseline and Comparison. . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nA.7.4 Implementations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nA.7.5 Evaluation Metrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nA.8 RQ3: Are Proposed Technologies Stealthy for Victims? . . . . . . . . . . . . . . . 25\nA.9 Experiments on the CIFAR-10 Dataset . . . . . . . . . . . . . . . . . . . . . . . . 26\nA.9.1 RQ1: Are Proposed Technologies Effective on Different Backdoor Attacks. 26\nA.9.2 RQ2: Are Proposed Three Technologies Harmless for Benign Accuracy. . . 28\nA.9.3 RQ3: Are Proposed Technologies Effective for Different Poisoning Settings. 29\nA.10 Experiments on the ImageNet-50 Dataset . . . . . . . . . . . . . . . . . . . . . . 30\nA.10.1 RQ1: Are proposed Technologies Effective on Different Backdoor Attacks. 30\nA.10.2 RQ2: Are Proposed Three Technologies Harmless for Benign Accuracy. . . 32\nA.11 Experiments on More Complex Constraints in Data-constraint Backdoor Attacks . 33\nA.12 Experiments on Domains Different From CLIP\u2019s Training Domain . . . . . . . . . 33\nA.13 Experiments on Fine-grained Datasets . . . . . . . . . . . . . . . . . . . . . . . . 34\nA.14 Experiments on ViT Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nA.15 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nA.15.1 Performance degradation in the clean-label single-class backdoor attack. . . 36\nA.15.2 Domain-constrained backdoor attacks are easier than class-constrained backdoor attacks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nA.15.3 Time Consumption of Different Attack Methods. . . . . . . . . . . . . . . 37\nA.15.4 Explaining the Difference Between Data-constraint Backdoor Attacks and backdoor attacks of federated learning. . . . . . . . . . . . . . . . . . . . 37\nA.16 Limitations and Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38"
        },
        {
            "heading": "A.1 DETAILED BACKGROUND AND RELATED WORK",
            "text": ""
        },
        {
            "heading": "A.1.1 POISONING EFFICIENCY IN BACKDOOR ATTACKS",
            "text": "Existing studies aim at improving the poisoning efficiency of backdoor attacks can be categorized into two main areas.\nDesigning Efficient Triggers The design of efficient triggers that are easier for DNNs to learn has garnered significant interest. Researchers have recently drawn inspiration from Universal Adversarial Perturbations (UAPs) Moosavi-Dezfooli et al. (2017) and optimized UAPs on pre-trained clean models to create effective triggers, which have been widely utilized in various studies Zhong et al. (2020); Li et al. (2020); Doan et al. (2021). However, this approach requires a pre-trained clean model on the training set, which is not practical for data-constrained backdoor attacks.\nSelecting Efficient Poisoning Samples Efficient sample selection for poisoning attacks is a critical yet under-explored aspect that is distinct from trigger design. Xia et al. (2022b) were among the first to investigate the contribution of different data to backdoor injection. Their research revealed that not all poisoning samples contribute equally, and appropriate sample selection can greatly enhance the efficiency of data in backdoor attacks. Additionally, various studies Li et al. (2023a;d); Gao et al. (2023); Guo et al. (2023) follow this setting, and the sample efficiency has been further improved."
        },
        {
            "heading": "A.1.2 POISONING STEALTHINESS IN BACKDOOR ATTACKS",
            "text": "Existing studies focused on increasing the stealthiness of backdoor attacks can be categorized into two main areas.\nDesigning Invisible Triggers The concept of invisible triggers aims to ensure that poisoning images are visually indistinguishable from clean samples, thus evading detection in both pixel and feature spaces. This perspective is the most straightforward approach to bypass defenses. Chen et al. (2017) first propose a blended strategy to evade human detection by blending clean samples with the trigger to create poisoning samples. Subsequent studies Zhong et al. (2020); Li et al. (2020); Doan et al. (2021) focuse on constraining the norm of the trigger through optimization methods. Moreover, some studies have explored the use of natural patterns such as warping Nguyen & Tran (2021), rotation Wu et al. (2022), style transfer Cheng et al. (2021b), frequency Feng et al. (2022); Zeng et al. (2021), and reflection Liu et al. (2020) to create triggers that are more imperceptible to human inspection. In contrast to previous works that employ universal triggers, Li et al. (2021c) employ GAN models to generate sample-specific triggers, which are similar to adversarial examples and extremely imperceptible to humans.\nClean-label Attacks. Clean-label attacks refer to backdoor attacks where the target labels of the poisoning samples align with their perception labels. Turner et al. (2019) is the first to explore clean-label attacks by employing GAN-based and adversarial-based perturbations. Compared to standard backdoor attacks, clean-label attacks are typically less effective due to the model\u2019s tendency to associate natural features, rather than backdoor triggers, with the target class. Recent studies have focused on aligning features Saha et al. (2020) or gradients Souri et al. (2021) between perturbed inputs from the target class and trigger-inserted inputs from the non-target class through pretraining on the entire training set. Additionally, some study Zeng et al. (2022) has proposed optimizing the backdoor trigger using only the knowledge about the target-class training data. In this approach, the trigger is optimized to point towards the interior of the target class, resulting in improved effectiveness."
        },
        {
            "heading": "A.1.3 CONTRASTIVE LANGUAGE-IMAGE PRE-TRAINING (CLIP) MODEL",
            "text": "Our method introduces the Contrastive Language-Image Pre-Training (CLIP) Radford et al. (2021) model into backdoor injection and we introduce it here. CLIP is a revolutionary deep learning model developed by OpenAI that is designed to connects texts and images by bringing them closer in a shared latent space, under a contrastive learning manner. The CLIP model is pre-trained on 400 million image-text pairs harvested from the Web, containing two encoder: CLIP text encoder E\u0302t(\u00b7) and CLIP image encoder E\u0302i(\u00b7). These encoders project the text and image to the CLIP common embedded feature space. Since natural language is able to express a much wider set of visual concepts, it contains ability to generalize across a wide range of tasks and domains, such as text-driven image manipulation Patashnik et al. (2021), zero-shot classification Cheng et al. (2021a), domain generalization Niu et al. (2022). To our best knowledge, our paper is the first study to explore the usage of CLIP model in the security community."
        },
        {
            "heading": "A.2 DETAILED PIPELINE OF DATA-CONSTRAINED BACKDOOR ATTACKS.",
            "text": ""
        },
        {
            "heading": "A.2.1 EXAMPLES OF BACKDOOR ATTACKS IN OUR STUDY",
            "text": "Here, we present three popular backdoor attack methods that serve as the baseline for our preliminary experiments, providing insight into the motivation discussed in Sec. 3. All attacks follow the pipeline described in Sec. 2.2.\nBadNets. BadNets Gu et al. (2019) is the pioneering backdoor attack in deep learning and is often used as a benchmark for subsequent research. It utilizes a 2\u00d7 2 attacker-specified pixel patch as the universal trigger pattern attached to benign samples.\nBlended. Chen et al. (2017) first discuss the requirement for invisibility in backdoor attacks. They propose that the poisoning image should be visually indistinguishable from its benign counterpart to evade human inspection. To meet this requirement, they introduce a blending strategy where poisoning images are created by blending the backdoor trigger with benign images. Formally, the poison generator can be formulated as T (x, t) = \u03bb \u00b7 t + (1 \u2212 \u03bb) \u00b7 x, where \u03bb represents the blend ratio (we set \u03bb = 0.15 for all experiments in this paper), and t is an attacker-specified benign image serving as the universal trigger pattern.\nUniversal Adversarial Perturbations (UAP). Inspired by Universal Adversarial Perturbations (UAPs) in adversarial examples, some studies Zhong et al. (2020); Li et al. (2020); Doan et al. (2021) propose optimizing a UAP on a pre-trained clean model as the natural trigger, formulated as T (x, t) = x + t, where t is a pre-defined UAP serving as the universal trigger pattern. It\u2019s worth noting that UAP-based backdoor attacks require a clean model pre-trained on the entire training set, which is not suitable for the discussed settings. However, to better explain our motivation that previous technologies exhibit significant performance degradation in data-constrained backdoor attacks, we assume the availability of a clean model pre-trained on the original training dataset in this section. It is important to acknowledge that this assumption does not hold in an actual attack scenario."
        },
        {
            "heading": "A.2.2 EXPERIMENTAL SETTINGS.",
            "text": "To evaluate the performance of three backdoor attack methods (BadNets, Blended, and UAP) under data-constrained scenarios, we conduct experiments on the CIFAR-10 dataset. Specifically, we consider three types of data constraints: number, class, and domain. The settings for the poisoning attacks follow those described in Sec. A.2.1. In all attacks, we set the attack-target label k to category 0. For our experiments, we select the VGG-16 model as the victim model and employ SGD as the optimizer with a weight decay of 5e-4 and a momentum of 0.9. The batch size is set to 256, and the initial learning rate is set to 0.01. The learning rate is multiplied by 0.1 at the 35-th and 55-th epochs, and the training is conducted for a total of 70 epochs."
        },
        {
            "heading": "A.2.3 NUMBER-CONSTRAINED BACKDOOR ATTACKS",
            "text": "Definition. Let D\u2032 denote the data manipulable by the malicious source, and D represent all the data available to the data collector. In the number-constrained scenario, as illustrated in Fig. 5 (a),\nthe data collector gathers data from multiple sources, including both malicious and benign sources, to form D. The data provided by each data source is independently and identically distributed. In other words, D and D\u2032 belong to the same distribution, but in terms of quantity, N \u2032 < N . The setting of number-constrained backdoor attacks is similar to that of data-efficient backdoor attacks discussed in previous studies Xia et al. (2022b); Zhong et al. (2020). Both aim to improve the Attack Success Rate (ASR) under a low poisoning rate. However, previous studies assumed that the attacker has access to the entire training set D, which enables efficient trigger design and sample selection. For example, some studies Zhong et al. (2020) draw inspiration from Universal Adversarial Perturbations (UAPs) in adversarial examples and propose to optimize a UAP on a clean model pre-trained on the training set as the natural trigger. Xia et al. Xia et al. (2022b) enhance the poisoning efficiency in backdoor attacks by selecting poisoning data from the entire training set. Although these methods have achieved remarkable results, they cannot be directly applied to number-constrained backdoor attacks due to the lack of access to the entire training set.\nExperimental results. In this section, we investigate the performance degradation of previous studies in number-constrained backdoor attacks. As shown in Fig. 2 (a), the attack success rate experiences a significant decrease as the number (P ) of poisoning samples decreases, particularly for Blended backdoor attacks. It is worth noting that Universal Adversarial Perturbations (UAP) achieves relatively favorable results even with a low poisoning rate. This can be attributed to the utilization of a proxy model that is pre-trained on the entire training set (D). However, in our settings, UAP is not accessible, and we present the results for UAP to effectively demonstrate the performance degradation even when a pre-trained proxy model is available."
        },
        {
            "heading": "A.2.4 CLASS-CONSTRAINED BACKDOOR ATTACKS",
            "text": "Definition. In the class-constrained scenario, let D\u2032 represent the data manipulable by the malicious source, and D denote all the data available to the data collector. As depicted in part (b) of Fig. 5, the data collector gathers data from multiple sources, including both malicious and benign sources, to form D. Each data source provides data belonging to different categories, resulting in D\u2032 containing only a subset of categories present in D. Therefore, D and D\u2032 follow distinct distributions. More specifically, the accessible clean training set D\u2032 \u2282 X \u00d7 Y \u2032(D\u2032 = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , N \u2032}) is a subset of the entire training set D \u2282 X \u00d7 Y (D = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , N}), where Y \u2032 \u2282 Y = {1, 2, \u00b7 \u00b7 \u00b7 , C}. Class-constrained backdoor attacks can be seen as a general setting of clean-label backdoor attacks Turner et al. (2019); Saha et al. (2020); Souri et al. (2021). In clean-label backdoor attacks, the accessible clean training set D\u2032 is defined as D\u2032 \u2282 X \u00d7 Y \u2032, where Y \u2032 = {k} and k represents the attack-target label.\nExperimental results. In this section, we explore the performance degeneration of previous studies in class-constrained backdoor attacks. As illustrated in Fig. 2 (b), attack success rate decreases as the number of class (C \u2032) in the poisoning set decreases, which is the similar as experimental results on the number-constrained backdoor attacks."
        },
        {
            "heading": "A.2.5 DOMAIN-CONSTRAINED BACKDOOR ATTACKS",
            "text": "Definition. In the domain-constrained scenario, as depicted in part (c) of Fig. 5, the data collector gathers data from multiple sources (both malicious and benign) to form D. Each data source provides data from a different domain, resulting in D\u2032 containing only a subset of the domains present in D. Consequently, D and D\u2032 belong to different distributions. We examine an extreme scenario in domain-constrained backdoor attacks, where the test dataset follows the same distribution as the benign source (D \\ D\u2032) and is outside the domain of the malicious source D\u2032 \u2282 X \u00d7 Y \u2032 (D\u2032 = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , N \u2032}). Evidently, images belonging to the same class can stem from diverse distributions, which we term as domain distinctions. To illustrate, consider the \u201dCar\u201d category found in both the ImageNet and CIFAR-10 datasets. Despite sharing this category, they exhibit disparate distributions, classifying them into distinct domains. In our context, characterized as \u201ddomain-constrained\u201d backdoor attacks, the domain pertains to the differing distributions within the same image class. This delineation underpins a tangible attack scenario. For instance, consider a victim endeavoring to train a comprehensive classifier capable of generalizing across varied data distributions. However, the assailant lacks comprehensive knowledge of the domains from which the victim sources data; thus, their capacity is confined to contaminating images within one or several domains, among the array of domains within the training set.\nExperimental results. To simulate the domain-constrained scenario, we conducted experiments with the following settings in this section: we designate the CIFAR-10 dataset as the benign source, the ImageNet dataset as the malicious source, and evaluated the attack performance on the CIFAR10 dataset. Fig. 2 (c) illustrates the results, showing a decrease in the attack success rate as the domain rate (the proportion of poisoning sets sampled from D \\ D\u2032 and D\u2032) in the poisoning set decreases. This observation aligns with the experimental findings in the number-constrained and class-constrained backdoor attacks."
        },
        {
            "heading": "A.3 ANALYZING THE ENTANGLEMENT BETWEEN BENIGN AND POISONING FEATURES",
            "text": "In this section, we provide three observations on data-constrained backdoor attacks to demonstrate the entanglement between benign and poisoning features does exist in the backdoor injection process, and it is the main reason why current attack methods fail in data-constrained scenarios.\nObservation 1: BadNet outperforms Blended notably in data-constrained attack scenarios. In a practical experimentation setting, BadNet and Blended exhibit comparable performance under unrestricted attack conditions (the leftmost point on the horizontal axis of Fig. 2). Conversely, in data-constrained attack scenarios, BadNet outperforms Blended notably. This intriguing disparity requires elucidation. BadNet employs a 2\u00d72 attacker-specified pixel patch as a universal trigger pattern attached to benign samples, whereas Blended employs an attacker-specified benign image for the same purpose. Comparatively, Blended\u2019s trigger exhibits greater feature similarity to benign images, engendering a more pronounced feature entanglement between poisoned and benign attributes. Accordingly, the performance of the blended dirty-label single-class scenario significantly lags be-\nhind other cases, lending credence to our hypothesis that entanglement underpins the degradation of data-constrained backdoor attacks.\nObservation 2: Attack efficiency of number-constrained, dirty-label single-class, and cleanlabel single-class backdoor attacks decreases in turn under the same poison rate. We further investigated our hypothesis and present our findings of entanglement in Fig. 6. As shown in Fig. 6 (a), the attack efficiency of number-constrained, dirty-label single-class, and clean-label singleclass backdoor attacks1 decreases in turn under the same poison rate. To understand the reason behind this phenomenon, we provide visualizations of the backdoor injection and activation phases for these three attacks in Fig. 6 (b). For the number-constrained backdoor attack, the distribution of poisoning samples (consisting of both benign and poisoning features) in the backdoor injection phase is the same as that in the backdoor activation phase. In other words, both benign and poisoning features are activated simultaneously during both phases. However, for the dirty-label single-class backdoor attack, the distribution of poisoning samples (consisting of single-class benign and poisoning features) in the backdoor injection phase is different from that in the backdoor activation phase. During the injection phase, both benign and poisoning features are activated, but during activation phase, only the poisoning feature is activated. This is the reason why previous attack methods on dirty-label single-class backdoor attacks exhibit performance degeneration. The clean-label single-class backdoor attack is similar to the dirty-label single-class backdoor attack in terms of the distribution of poisoning samples. However, during backdoor injection, there is competing activation3 between benign and poisoning features. Consequently, the poisoning efficiency of clean-label single-class backdoor attacks is lower than that of dirty-label single-class backdoor attacks.\nObservation 3: Substantial dissimilarities in activation between poisoned samples that share the same trigger. We have embraced Grad-CAM Selvaraju et al. (2017) to more effectively corrob-\n3In the clean-label single-class backdoor attack, the benign feature of the accessible class (the same as the attack-target class) in both poisoning and clean sets is labeled with the same label (e.g., \u201dFish\u201d in Fig. 6), and the clean set contains more samples of the attack-target class. As a result, the presence of the benign feature in the poisoning set hampers the activation of the poisoning features. In contrary, the benign feature of the accessible class in poisoning and clean sets is labeled with the different label in the dirty-label single-class backdoor attack (e.g., the benign feature in the clean set is labeled as \u201dFrog\u201d, while the benign+poisoning feature in the poisoning set is labeled as \u201dFish\u201d). Consequently, the benign feature in the poisoning set does not impact the activation of the poisoning features.\norate the presence of a correlation between the benign and the backdoor features. To substantiate this hypothesis, we have incorporated Grad-CAM outcomes to our study. Sgrad-campecifically, we have applied this technique to the BadNet-based poison model on the CIFAR-100 dataset, as depicted in Figure 7. The discernible results from these Grad-CAM visualizations underscore the substantial dissimilarities in activation between poisoned samples that share the same trigger. This visual evidence compellingly demonstrates the intricate entanglement existing between the benign and the backdoor features during instances of backdoor attacks."
        },
        {
            "heading": "A.4 THREAT MODEL",
            "text": "Attack scenario. The proliferation of large-scale artificial intelligence models, such as ChatGPT and Stable Diffusion, necessitates the collection of massive amounts of data from the web. However, the security and trustworthiness of this data cannot always be guaranteed. This data collection pipeline inadvertently introduces vulnerabilities that can be exploited by data-based backdoor attacks. Attackers can strategically inject poisoning data into the training dataset and publish it on the internet, potentially compromising the integrity and performance of these models. Unlike previous attack scenarios where all training data is sourced from a single provider, we consider a more realistic scenario in which victims collect data from multiple sources. In this scenario, attackers only have access to a portion of the training dataset. This situation mirrors the real-world training process of models that utilize diverse public data. By acknowledging the challenges posed by multi-source data collection and limited attacker access, our study provides valuable insights into the security implications of such scenarios.\nAttack goal. The objective of our paper is aligned with popular backdoor attacks, as seen in previous studies Gu et al. (2019); Li et al. (2021a). The attackers aim to activate a hidden trigger within the model by providing specific inputs, leading the model to produce incorrect results. Our attack strategy emphasizes three key properties: (i) Minimal side effects: The backdoor attacks should not adversely impact the accuracy of the model on benign inputs. (ii) Effective backdoor: The attack should have a high success rate across various datasets and models, ensuring its efficiency. (iii) Stealthy attack: The backdoor attacks should be inconspicuous and difficult to detect, maintaining their stealthiness. Our research aims to develop invigorative backdoor attacks that strike a balance between effectiveness and preserving the integrity of the model\u2019s performance on legitimate inputs.\nAttackers\u2019 prior knowledge. In order to simulate a realistic scenario, we assume that the attackers have no access to the models or training details. They possess only general knowledge about the class labels involved in the task. This assumption reflects a more challenging and practical setting, where attackers have limited information about the target system.\nAttackers\u2019 capabilities. Building upon previous studies Gu et al. (2019), we make the assumption that the attackers possess the capability to control the training data. However, we further impose a stricter assumption in this work, stating that the attackers have control over only a portion of the training data. Consequently, we divide the attack scenario into three distinct tasks, each representing different capabilities of the attacker. These tasks include: (i) Number-constrained backdoor attacks, where the attacker has access to only a subset of the training data; (ii) Class-constrained backdoor attacks, where the attacker has access to only a subset of the classes in the training data; and (iii) Domain-constrained backdoor attacks, where the attacker has access to only a subset of the domains within the training data. By considering these various constraints, we provide a comprehensive analysis of backdoor attacks in different data-constrained scenarios."
        },
        {
            "heading": "A.5 CLIP FOR ZERO-SHOT CLASSIFICATION",
            "text": "The pre-trained CLIP model Radford et al. (2021) possesses the ability to express a broader range of visual concepts and has been utilized as a general feature extractor in various tasks. These tasks include text-driven image manipulation Patashnik et al. (2021), zero-shot classification Cheng et al. (2021a), and domain generalization Niu et al. (2022). In this section, we introduce the pipeline of CLIP in zero-shot classification, which can serve as inspiration for incorporating it into our clean feature erasing approach. CLIP achieves zero-shot classification by aligning text and image features. Firstly, CLIP employs its text encoder, denoted as E\u0302t(\u00b7), to embed the input prompts (\u201da photo of a ci\u201d) into text features Ti \u2208 Rd, where i = {1, 2, \u00b7 \u00b7 \u00b7 , C} represents the classes. Subsequently, the image feature Ij \u2208 Rd of image xj is embedded using the image encoder, denoted as E\u0302i(\u00b7). During\nthe inference phase, the classification prediction yj is computed using the cosine similarity between Ti and Ij . This can be expressed as:\nyj = argmax i\n(\u27e8Ij , Ti\u27e9), i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , C}, (12)\nwhere C represents the number of classes, and \u27e8\u00b7, \u00b7\u27e9 represents the cosine similarity between two vectors."
        },
        {
            "heading": "A.6 CLIP-BASED UNIVERSAL ADVERSARIAL PERTURBATIONS",
            "text": "In this section, we also employ the widely-used pre-trained CLIP model Radford et al. (2021) to generate universal adversarial perturbations as the backdoor trigger. Xia et al. (2023) argue that deep models inherently possess flaws, and it is easier to exploit and enhance an existing flaw to serve as a backdoor rather than implanting a new one from scratch (BadNets Gu et al. (2019) and Blended Chen et al. (2017)). Universal Adversarial Perturbations (UAP) Zhong et al. (2020); Xia et al. (2023) utilize these inherent flaws in models as triggers, providing a straightforward method for augmenting the poisoning feature. However, this approach typically requires a feature extractor that has been pre-trained on the entire training set, which is not practical in data-constrained backdoor attacks. To address this limitation, we propose a CLIP-based Universal Adversarial Perturbations (CLIP-UAP) method. Specifically, given an accessible clean training set D\u2032 = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 , N \u2032} and an attack-target label k, the defined trigger can be formulated as follows:\n\u03b4uap = argmin ||\u03b4uap||p\u2264\u03f5 \u2211 (x,y)\u2208D\u2032 L(fCLIP (x+ \u03b4uap,P), k), (13)\nwhere P and fCLIP are defined as shown in Eq. 4 and Eq. 5, respectively. Similar to Eq. 6, we utilize the first-order optimization method known as Projected Gradient Descent (PGD) Madry et al. (2017) to solve the constrained minimization problem. The optimization process can be expressed as follows:\n\u03b4t+1uap = \u220f \u03f5 ( \u03b4tuap \u2212 \u03b1 \u00b7 sign(\u2207\u03b4uapL(fCLIP (x+ \u03b4tuap,P), k)) ) , (14)\nwhere t, \u2207\u03b4uapL(fCLIP (x + \u03b4tuap,P), k), and \u220f\nhold the same meaning as in Eq. 6. Unlike the sample-wise clean feature erasing noise, the CLIP-UAP serves as a universal trigger for the entire training set. Therefore, it follows the optimization formulation presented in Eq. 14 to generate \u03b4t+1uap at each step t. The optimization process is performed on all samples in the accessible clean training set D\u2032. Consequently, the CLIP-UAP for the set D\u2032 can be represented as \u03b4uap = \u03b4Tuap, and the poison generator is formulated as T (x, \u03b4uap) = x+ \u03b4uap."
        },
        {
            "heading": "A.7 EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "A.7.1 DATASETS",
            "text": "We use the following three popular datasets in image classification:\nCIFAR-10 Krizhevsky et al. (2009). CIFAR-10 is a tiny object classification dataset containing 50,000 training images and 10,000 testing images. Each image has a size of 32\u00d732\u00d73 and belongs to one of 10 classes.\nCIFAR-100 Krizhevsky et al. (2009). Similar to CIFAR-10, CIFAR-100 is also a tiny object classification dataset containing 50,000 training images and 10,000 testing images. Each image has a size of 32\u00d7 32\u00d7 3 and belongs to one of 100 classes. ImageNet-50 Deng et al. (2009). ImageNet is the most popular object classification dataset containing 1.3M training images and 50K testing images. Each image has a size of 224\u00d7 224\u00d7 3 and belongs to one of 1000 classes. For simplicity, we randomly sampled 50 categories to compose a tiny dataset: ImageNet-50. Our ImageNet-50 dataset contains 60K training images and 2.5K testing images."
        },
        {
            "heading": "A.7.2 MODEL ARCHITECTURE.",
            "text": "We verify the performance on three popular model architectures of image classification: VGG-16 Simonyan & Zisserman (2014), ResNet-18 He et al. (2016), and MobileNet-V2 Sandler et al. (2018). All of them are widely used in various areas of artificial intelligence, such as flower classification Xia et al. (2017), pulmonary image classification Wang et al. (2019b), fault diagnosis Wen et al. (2020), and Covid-19 screening Farooq & Hafeez (2020)."
        },
        {
            "heading": "A.7.3 BASELINE AND COMPARISON.",
            "text": "Our method contains two aspects: clean feature suppression and poisoning feature augmentation. Among them, poisoning feature augmentation can be accomplished through designing efficient and data-independent triggers, while clean feature suppression is orthogonal to previous trigger designing and can be integrated into any backdoor triggers. Therefore, we compare our two designed triggers CLIP-based universal adversarial perturbations (CLIP-UAP) and CLIP-based contrastive feature augmentation (CLIP-CFA) with two popular triggers: BadNets Gu et al. (2019)and Blended Chen et al. (2017). All of them are independent of the training data therefore can be implemented easily in the introduced data-constrained backdoor attacks. Although there are also contain other advanced clean-label backdoor attacks Liu et al. (2020); Barni et al. (2019); Saha et al. (2020); Souri et al. (2022) and state-of-the-art backdoor attacks Zhong et al. (2020), a substantial proportion of them Liu et al. (2020); Saha et al. (2020); Souri et al. (2022); Zhong et al. (2020) operate within the threat model that necessitates a proxy model pre-trained on the entire training set. This precondition becomes challenging to fulfill in the context of a many-to-one (M2O) data collection attack scenario. To verify the validity of the clean feature suppression, we integrate the proposed CLIP-based clean feature erasing (CLIP-CFE) onto currently designed triggers: two our designed triggers and two baseline triggers.\nA.7.4 IMPLEMENTATIONS.\nIn order to demonstrate the effectiveness of our proposed method, we conduct experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-50). For the CIFAR-10 and CIFAR-100 datasets, we choose VGG-16, ResNet-18, and MobileNet-V2 as the victim models. All models use the SGD optimizer with a momentum of 0.9, weight decay of 5e-4, and a learning rate of 0.01 (0.1 for MobileNet-V2), which is multiplied by 0.1 at epoch 35 and 55. For the ImageNet-50 dataset, we use VGG-16 and MobileNet-V2 as the victim models. We use the SGD optimizer with a momentum of 0.9, weight decay of 5e-4, and a learning rate of 0.05 (0.01 for VGG-16), which is multiplied by 0.1 at epoch 35 and 55. The complete training epochs is 70.\nIn the number-constrained scenario, we conducted experiments with poisoning rates of 0.01 (P=500), 0.015 (P=750), and 0.007 (P=453) for the CIFAR10, CIFAR100, and ImageNet-50 threes datasets, respectively. In the class-constrained scenario, experiments with poisoning rates of 0.02 (P=1000), 0.01 (P=500), and 0.02 (P=1296) for three datasets. We choose two extreme scenario in the class-constrained backdoor attacks, denoted as clean-label single-class backdoor attack and dirty-label single-class backdoor attack. Specifically, the accessed class category is set to Y \u2032 = {k}\nand Y \u2032 = {c}, c \u0338= k for clean-label single-class backdoor attack and dirty-label single-class backdoor attack, respectively. In the domain-constrained scenario, experiments with poisoning rates of 0.02 (P=1000, 1000, and 1296) for three datasets. The out-of-domain samples of all experiments are selected from other ImageNet-1K datasets that are not ImageNet-50. The attack-target class k is set to category 0 for all experiments of above three data-constrained backdoor attacks."
        },
        {
            "heading": "A.7.5 EVALUATION METRICS.",
            "text": "We evaluate the performance of our method in terms of Harmlessness: Benign Accuracy (BA), Effectiveness: Attack Success Rate (ASR), and Stealthiness: Peak Signal-to-noise Ratio (PSNR) Huynh-Thu & Ghanbari (2008) and Structural Similarity Index (SSIM) Wang et al. (2004).\nBenign Accuracy (BA). BA is the clean accuracy of the testing set Dt = {(xi, yi)|i = 1, \u00b7 \u00b7 \u00b7 ,M} and is applied to evaluate the Harmlessness of the backdoor. When BA of the infected model is similar to the accuracy of the clean model, we believe that the current attack technique is harmless.\nAttack Success Rate (ASR). ASR is applied to evaluate the effectiveness of the backdoor attack, which is the fraction of testing images with specific trigger that are predicted as the target class. Specifically, For M \u2032 images in the testing set that do not belong to the attack-target class (k), the ASR is formulated as:\nASR = \u2211M \u2032\ni=1 I(f(T (xi, t); \u0398) = k) M \u2032 , (xi, yi) \u2208 D\u2032t, (15)\nwhere D\u2032t is a subset of testing set Dt (D\u2032t \u2282 Dt), containing the images whose label is not the attack-target class k.\nPeak Signal-to-noise Ratio (PSNR) Huynh-Thu & Ghanbari (2008). PSNR is applied to measure the similarity between clean images and the corresponding poisoning images. Give a image xi \u2208 Dt\nand the corresponding poisoning image x\u2032i = T (xi, t), the PSNR is formulated as:\nPSNR = 1\nM M\u2211 i=1 PSNRi(xi, x \u2032 i),\nwhere PSNRi(xi, x \u2032 i) = 10 log10 ( 2552/MSE(xi, x \u2032 i) ) ,\nMSE(f, g) = 1\nHW H\u2211 i=1 W\u2211 j=1 (fij \u2212 gij)2 ,\n(16)\nH and W are height and width of the image, respectively. Larger PSNR means larger similarity between clean images and the corresponding poisoning images, therefore larges stealthiness of backdoor attacks.\nStructural Similarity Index (SSIM) Wang et al. (2004). Similar to PSNR, SSIM is another metrics to represent the stealthiness of backdoor attacks, which is formulated as:\nSSIM = 1\nM M\u2211 i=1 SSIMi(xi, x \u2032 i),\nwhere SSIMi(xi, x \u2032 i) = l(xi, x \u2032 i) \u00b7 c(xi, x\u2032i) \u00b7 s(xi, x\u2032i), l(f, g) = 2\u00b5f\u00b5g+C1 \u00b52f+\u00b5 2 g+C1 c(f, g) = 2\u03c3f\u03c3g+C2 \u03c32f+\u03c3 2 g+C2\ns(f, g) = \u03c3fg+C3 \u03c3f\u03c3g+C3\n,\n(17)\nwhere \u00b5 and \u03c3 are mean and variance of image, respectively. Similarly, Larger SSIM means larger similarity between clean images and the corresponding poisoning images, therefore larges stealthiness of backdoor attacks."
        },
        {
            "heading": "A.8 RQ3: ARE PROPOSED TECHNOLOGIES STEALTHY FOR VICTIMS?",
            "text": "Qualitative and Quantitative Results. Fig. 8 showcases examples of poisoning images generated by different attacks on the ImageNet-504 dataset. While our CLIP-UAP and CLIP-CFA may not achieve the highest stealthiness in terms of SSIM (as indicated in Table 2), the poisoning images generated by our methods appear more natural to human inspection compared to the baseline attacks. Additionally, incorporating CLIP-CFE has minimal impact on both PSNR and the natural appearance of the images, while achieving higher stealthiness in terms of SSIM.\nStealth in attack defense methods. Attack stealthiness needs to be evaluated through algorithm. (i) As depicted in Figure 9, our preliminary evaluation conducted on the VGG-16 and CIFAR-100\n4As shown in Appendix A.7, CIFAR-10 and CIFAR-100 have low resolution, which makes unclear visualizations. Therefore, we show the results on the ImageNet-50 dataset in this section.\ndatasets vividly demonstrates that our proposed method can attack against pruning-based defense method Liu et al. (2018) more effectively than other attack methods. (ii) As depicted in Figure 10, our preliminary evaluation conducted on the ResNet-18 model and CIFAR-100 datasets vividly demonstrates that our proposed method can attack against another pruning-based defense method Neural cleanse Wang et al. (2019a) more effectively than other attack methods. (iii) Similarly, as depicted in Figure 11, our preliminary evaluation conducted on the ResNet-18 and CIFAR-100 datasets vividly demonstrates that our proposed method can attack against tune-based defense method FST Min et al. (2023) more effectively than other attack methods."
        },
        {
            "heading": "A.9 EXPERIMENTS ON THE CIFAR-10 DATASET",
            "text": ""
        },
        {
            "heading": "A.9.1 RQ1: ARE PROPOSED TECHNOLOGIES EFFECTIVE ON DIFFERENT BACKDOOR ATTACKS.",
            "text": "In this section, we utilize our proposed technologies to attack different target models on the CIFAR10 dataset. Our objective is to verify the effectiveness of the attack and calculate the ASR for each target model. The baseline attack methods, BadNets Gu et al. (2019) and Blended Chen et al. (2017) were introduced in Sec. A.2.1. The attack performance of the number-constrained, clean-label single-class (class-constrained), dirty-label single-class (class-constrained), and out-of-the-domain (domain-constrained) backdoor attacks on CIFAR-10 dataset are reflected in Fig. 12, 13, 14, and 15, respectively.\nCLIP-based Poisoning Feature Augmentation Is More Effective Than Previous Attack Methods. Our proposed CLIP-UAP and CLIP-CFA methods outperform the BadNets Gu et al. (2019) and Blended Chen et al. (2017) baseline methods in terms of ASR under the most attacks and datasets.\nThis confirms that the proposed poisoning feature augmentation generates more efficient triggers than other methods.\nCLIP-based Clean Feature Suppression Is Useful For Different Attack Methods. Our proposed CLIP-CFE method improves the poisoning effectiveness on most cases compared to the baseline without CLIP-based Clean Feature Erasing. Only on small cases the BadNets and CLIP-UAP slightly outperform the corresponding methods with CFE."
        },
        {
            "heading": "A.9.2 RQ2: ARE PROPOSED THREE TECHNOLOGIES HARMLESS FOR BENIGN ACCURACY.",
            "text": "Table 3 illustrates that our proposed CLIP-UAP and CLIP-CFA methods have similar or even better average Benign Accuracy (BA) compared to the baseline methods BadNets Gu et al. (2019) and Blended Chen et al. (2017). Additionally, our proposed CLIP-CFE method has no negative effect on\nBA, confirming that our technologies are harmless for benign accuracy under various settings and different backdoor attacks."
        },
        {
            "heading": "A.9.3 RQ3: ARE PROPOSED TECHNOLOGIES EFFECTIVE FOR DIFFERENT POISONING SETTINGS.",
            "text": "Ablation of Different Poison Rates on The Number-constrained Backdoor Attacks. We conducted ablation studies to verify the effectiveness of the proposed methods in reducing the number of poisoning samples (poisoning rates) on the number-constrained backdoor attacks. The results in Fig. 16 illustrate that: i) The attack success rate increases with the increase of poisoning rate\nfor different attacks; ii) Our proposed CLIP-UAP and CLIP-CFA methods outperform the BadNets Gu et al. (2019) and Blended Chen et al. (2017); iii) The proposed CLIP-CFE further improves the poisoning effectiveness upon the different triggers.\nAblation of Different Poison Classes on The Class-constrained Backdoor Attacks. In this section, we conducted ablation studies to verify the effectiveness of the proposed methods in increasing the number of poisoning classes on the class-constrained backdoor attacks. The results in Fig. 17 illustrate that: i) The attack success rate increases with the increase of poisoning classes for different attacks; ii) The attack success rate of clean-label single-class attack is lower than that of dirty-label single-class attacks; iii) Our proposed CLIP-UAP and CLIP-CFA methods outperform the BadNets Gu et al. (2019) and Blended Chen et al. (2017) methods; iv) The proposed CLIP-CFE method further improves the poisoning effectiveness with different triggers.\nAblation of different domain rates on the domain-constrained backdoor attacks. In this section, we conducted ablation studies to verify the effectiveness of the proposed methods in increasing the domain rate on the domain-constrained backdoor attacks. The results in Fig. 18 illustrate that: i) The attack success rate increases with the increase of the domain rates for different attacks; ii) Our proposed CLIP-UAP and CLIP-CFA methods outperform the BadNets Gu et al. (2019) and Blended Chen et al. (2017) methods; iii) The proposed CLIP-CFE method further improves the poisoning effectiveness with different triggers."
        },
        {
            "heading": "A.10 EXPERIMENTS ON THE IMAGENET-50 DATASET",
            "text": ""
        },
        {
            "heading": "A.10.1 RQ1: ARE PROPOSED TECHNOLOGIES EFFECTIVE ON DIFFERENT BACKDOOR ATTACKS.",
            "text": "In this section, we utilized our proposed technologies to attack different target models on the ImageNet-50 dataset and calculate the ASR for each target model to verify the attack effective-\nness. The baseline attack methods, BadNets Gu et al. (2019)and Blended Chen et al. (2017) were introduced in Sec. A.2.1. Fig. 19, 20, 21, and 22 reflect the attack performance of the numberconstrained, clean-label single-class (class-constrained), dirty-label single-class (class-constrained), and out-of-the domain (domain-constrained) backdoor attacks on the ImageNet-50 dataset, respectively.\nCLIP-based Poisoning Feature Augmentation Is More Effective Than Previous Attack Methods. Our proposed CLIP-UAP and CLIP-CFA methods outperform the BadNets Gu et al. (2019) and Blended Chen et al. (2017) baseline methods in terms of consistency under different attacks and datasets. This confirms that the proposed poisoning feature augmentation generates more efficient triggers than other methods.\nCLIP-based Clean Feature Suppression Is Useful For Different Attack Methods. Our proposed CLIP-CFE method improves the poisoning effectiveness on most cases compared to the baseline without CLIP-based Clean Feature Erasing. Only on the MobileNet-V2 results of the numberconstrained backdoor attacks (right part of the Fig. 19), CLIP-UAP slightly outperform the corresponding methods with CFE."
        },
        {
            "heading": "A.10.2 RQ2: ARE PROPOSED THREE TECHNOLOGIES HARMLESS FOR BENIGN ACCURACY.",
            "text": "Table 4 illustrates that our proposed CLIP-UAP and CLIP-CFA methods have similar or even better average Benign Accuracy (BA) compared to the baseline methods BadNets Gu et al. (2019) and\nBlended Chen et al. (2017). Additionally, our proposed CLIP-CFE method has no negative effect on BA, confirming that our technologies are harmless for benign accuracy under various settings and different backdoor attacks."
        },
        {
            "heading": "A.11 EXPERIMENTS ON MORE COMPLEX CONSTRAINTS IN DATA-CONSTRAINT BACKDOOR ATTACKS",
            "text": "Previous sections have primarily focused on specific sub-variants of number, class, and domain constraints, which might not comprehensively represent all real-world limitations. This section delves into more intricate constraints. Specifically, we investigate two additional configurations:\nConfig A: Poisoning rate = 0.01 (P=500), poisoning classes=1 (1), and domain rate=0.5.\nConfig B: Poisoning rate = 0.01 (P=500), poisoning classes=3, and domain rate=0.25.\nThe experiments are performed using the VGG-16 model and the CIFAR-100 dataset. As depicted in Fig. 23, our methods and conclusions are demonstrated to be equally viable in scenarios involving more complex data constraints."
        },
        {
            "heading": "A.12 EXPERIMENTS ON DOMAINS DIFFERENT FROM CLIP\u2019S TRAINING DOMAIN",
            "text": "In this section, we verify the performance of the proposed methods on domain drastically differs from CLIP\u2019s training set. Typically, we select the commonly used dataset UCM Yang & Newsam (2010) in the field of remote sensing.\nUCM Dataset. The UCM contains 100 images in each of the 21 categories for a total of 2100 images. Each image has a size of 256\u00d7 256 and a spatial resolution of 0.3 m/pixel. The images are captured in the optical spectrum and represented in the RGB domain. The data are extracted from aerial ortho imagery from the U.S. Geological Survey (USGS) National Map. The categories in this dataset are: agricultural, airplane, baseball diamond, beach, buildings, chaparral, dense residential, forest, freeway, golf course, harbor, intersection, medium-density residential, mobile home park, overpass, parking lot, river, runway, sparse residential, storage tanks, and tennis courts.\nExperiments. Similar to Dra\u0308ger et al. (2023), we randomly select 1050 samples to form the training set, while the remaining samples make up the test set. The attack-target class k is set to \u201dagricultural\u201d. The poisoning rate is set to 0.05 in all experiments. As illustrated in Table 5, we illustrate the Attack Success Rate of number-Constrained and class-Constrained backdoor attacks on UCM dataset. The results indicate a performance decline on the UCM dataset. However, as seen in Sec. A.11.1, by adjusting the constraints (\u03f5) of the optimized noise, we observed an improved attack success rate in our methods compared to baseline methods. Additionally, replacing CLIP with the Satellite Arto et al. (2021) model, a large model fine-tuned on remote sensing images, further augmented the attack success rate. These outcomes demonstrate the adaptability of our methods to various domains by replacing CLIP with domain-specific pre-trained models."
        },
        {
            "heading": "A.13 EXPERIMENTS ON FINE-GRAINED DATASETS",
            "text": "In this section, we rigorously assess the effectiveness of our proposed methodologies using finegrained datasets, with a specific focus on the widely recognized Oxford-Flowers dataset within the domain of fine-grained image classification.\nOxford-Flowers Dataset. The Oxford-Flowers dataset comprises 102 categories of common UK flower images, totaling 8189 images, with each category containing 40 to 258 images. Our exper-\nimental setup involves employing 6140 images for training purposes, while the remaining samples constitute the test set.\nExperiments. For our experiments, we set the attack-target class (k) to 0 and maintain a consistent poisoning rate of 0.05 (p=307) across all conducted trials. Illustrated in Figure 24 (a), we showcase the Attack Success Rate of number-Constrained backdoor attacks on the Oxford-Flowers dataset. Our findings conclusively demonstrate the superior efficacy of CLIP-based poisoning feature augmentation compared to prior attack methodologies. Additionally, CLIP-based Clean Feature Suppression emerges as a valuable strategy across diverse attack methods. Moreover, as depicted in Figure 24 (b), our results unequivocally indicate that our technologies maintain benign accuracy at par with baseline methods, even under varying settings and diverse backdoor attacks. This underlines the robustness and non-disruptive nature of our methodologies."
        },
        {
            "heading": "A.14 EXPERIMENTS ON VIT ARCHITECTURE",
            "text": "For our experiments, we set the attack-target class (k) to 0 and maintain a consistent poisoning rate of 0.05 (p=500) across all conducted trials. Illustrated in Figure 25 (a), we showcase the Attack Success Rate of number-Constrained backdoor attacks on the ViT-Small architecture and CIFAR10 dataset. Our findings conclusively demonstrate the superior efficacy of CLIP-based poisoning feature augmentation compared to prior attack methodologies. Additionally, CLIP-based Clean Feature Suppression emerges as a valuable strategy across diverse attack methods. Moreover, as depicted in Figure 25 (b), our results unequivocally indicate that our technologies maintain benign accuracy at par with baseline methods, even under varying settings and diverse backdoor attacks. This underlines the robustness and non-disruptive nature of our methodologies."
        },
        {
            "heading": "A.15 DISCUSSION",
            "text": ""
        },
        {
            "heading": "A.15.1 PERFORMANCE DEGRADATION IN THE CLEAN-LABEL SINGLE-CLASS BACKDOOR ATTACK.",
            "text": "As depicted in Fig. 3 (b), Fig. 13, and Fig. 20, both the baseline and our attack methods exhibit poor Attack Success Rate (ASR) in the clean-label single-class backdoor attack. In this section, we aim to enhance the attack strength of our methods and devise a more efficient attack strategy for the clean-label single-class backdoor attack. In our optimization equations, namely Eq. 3, Eq. 13, and Eq. 10, we impose constraints on the optimized noise, denoted as \u03b4i, \u03b4uap, and \u03b4con, respectively. These constraints are specified as ||\u03b4i||p \u2264 \u03f5, ||\u03b4uap||p \u2264 \u03f5, and ||\u03b4con||p \u2264 \u03f5, where || \u00b7 ||p represents the Lp norm, and we set \u03f5 to 8/255 to ensure the stealthiness of the backdoor attacks, as observed in our previous experiments. To bolster the attack strength and subsequently increase the ASR in the clean-label single-class backdoor attack, we investigate the impact of adjusting the constraint on \u03b4i. As demonstrated in Fig. 26 and Fig. 27, significant (more than 500%) improvements are observed in the ASR of the clean-label single-class backdoor attack when we set the constraint on \u03b4i to ||\u03b4i||p \u2264 16/255. This finding validates the efficacy of our method in the clean-label singleclass backdoor attack, albeit at the expense of compromising stealthiness. This sacrifice, which is common in previous backdoor attack methods Zeng et al. (2022), is a low-cost trade-off."
        },
        {
            "heading": "A.15.2 DOMAIN-CONSTRAINED BACKDOOR ATTACKS ARE EASIER THAN CLASS-CONSTRAINED BACKDOOR ATTACKS.",
            "text": "Fig. 28 provides a visualization of the Attack Success Rate (ASR) achieved by different attack methods on the CIFAR-10 dataset in domain-constrained (domain rate set to 0) and dirty-label single-class backdoor attacks. While domain-constrained backdoor attacks impose stricter restrictions (assumptions that attackers have no access to any data in the training set), the ASR in domainconstrained backdoor attacks consistently surpasses that of dirty-label single-class backdoor attacks.\nThis observation leads us to propose that the diversity of samples in the poisoning set is another crucial factor affecting attack efficiency. Consequently, we recommend that attackers fully consider the diversity of poisoning samples during the poisoning set generation phase."
        },
        {
            "heading": "A.15.3 TIME CONSUMPTION OF DIFFERENT ATTACK METHODS.",
            "text": "We would like to highlight that in the case of BadNet and Blended, adding a pre-defined trigger into benign images to build poisoned images only needs to perform one time addition calculation, which incurs negligible time consumption. It\u2019s important to note, however, that these straightforward implementations fall short in terms of both poison efficiency and stealthiness in the context of backdoor attacks. Recent advancements in backdoor attack techniques have sought to enhance efficiency and covert effectiveness by introducing optimization-driven processes to define triggers. While these refinements do entail some additional time overhead, they significantly elevate the attack\u2019s efficacy. We undertook an evaluation of the time overhead associated with diverse attack methods using an A100 GPU. As outlined in Table 6, the time consumption exhibited by optimization-based methods grows linearly with the expansion of the poison sample count. Despite this, the overall time overhead remains remarkably modest, with the entirety of the process being accomplished within mere minutes."
        },
        {
            "heading": "A.15.4 EXPLAINING THE DIFFERENCE BETWEEN DATA-CONSTRAINT BACKDOOR ATTACKS AND BACKDOOR ATTACKS OF FEDERATED LEARNING.",
            "text": "The landscape of backdoor attacks in the context of federated learning shares certain similarities with our proposed data-constraint backdoor attacks. Notably, both scenarios assume the utilization of training data originating from diverse sources. However, it\u2019s important to note that the threat model for backdoor attacks in federated learning adopts a distinct perspective. In this model, the attacker exercises complete control over one or several participants: (1) The attacker possesses authority over the local training data of any compromised participant. (2) It wields control over\nthe local training process and associated hyperparameters, including parameters like the number of training epochs and learning rate. (3) The attacker can manipulate the model\u2019s weights prior to submitting them for aggregation. (4) It retains the capability to dynamically adjust its local training strategy from one round to the next.\nIn contrast, our data-constraint backdoor attacks center exclusively on the attacker\u2019s capability to manipulate local training data. This introduces a heightened level of challenge for potential attackers. Furthermore, to the best of our knowledge, the existing backdoor attack strategies within the realm of federated learning primarily revolve around the number-constrained scenario. Notably, the class-constrained and domain-constrained scenarios have yet to be comprehensively explored within the community\u2019s discourse on this subject."
        },
        {
            "heading": "500 0 1 0.7 1.6 1 2.1 0.9 1.9",
            "text": ""
        },
        {
            "heading": "1000 0 1.9 1.5 3.3 2 4 1.9 3.9",
            "text": ""
        },
        {
            "heading": "2000 0 3.7 2.9 6.5 4 7.9 3.9 7.8",
            "text": ""
        },
        {
            "heading": "A.16 LIMITATIONS AND FUTURE WORKS",
            "text": "In this section, we discuss the limitations of our approach and outline potential future directions for backdoor learning research. Performance Degradation in Clean-label Backdoor Attacks. Clean-label backdoor attacks present a significant challenge Zhao et al. (2020). As shown in Fig.\n3, previous methods exhibit a poor ASR, and our technologies show limited improvement in cleanlabel backdoor attacks when the poisoning rate is low. In future research, we will investigate the underlying reasons for this situation and explore more efficient attack methods specifically designed for clean-label backdoor attacks.\nApplication Limitations. Our technologies depend on the CLIP model that is pre-trained on natural images, which may limit their applicability to certain domains such as medical images or remote sensing. In such cases, a possible solution is to replace CLIP with a domain-specific pre-trained model, such as MedCLIP Madhawa & Carlomagno (2022) for medical images or Satellite Arto et al. (2021) for remote sensing, to adapt our methods to the target domain.\nTransfer to Other Domains. The attack scenario we have defined is not limited to a specific domain and can be applied to other important applications, including backdoor attacks for malware detection, deepfake detection, and federated learning. In our future work, we plan to explore the design of realistic attack scenarios and efficient backdoor attacks specifically tailored for these applications."
        }
    ],
    "year": 2024
}