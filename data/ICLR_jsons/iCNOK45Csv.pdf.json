{
    "abstractText": "Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods. Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "BACKDOOR ATTACKS"
        },
        {
            "affiliations": [],
            "name": "Ming-Yu Chung"
        },
        {
            "affiliations": [],
            "name": "Sheng-Yen Chou"
        },
        {
            "affiliations": [],
            "name": "Pin-Yu Chen"
        }
    ],
    "id": "SP:94cfc0e7928243ac3987f166714b77e189b53355",
    "references": [
        {
            "authors": [
                "Mauro Barni",
                "Kassem Kallas",
                "Benedetta Tondi"
            ],
            "title": "A new backdoor attack in cnns by training set corruption without label poisoning",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2019
        },
        {
            "authors": [
                "Alain Berlinet",
                "Christine Thomas-Agnan"
            ],
            "title": "Reproducing kernel Hilbert spaces in probability and statistics",
            "venue": "Springer Science & Business Media,",
            "year": 2011
        },
        {
            "authors": [
                "Bryant Chen",
                "Wilka Carvalho",
                "Nathalie Baracaldo",
                "Heiko Ludwig",
                "Benjamin Edwards",
                "Taesung Lee",
                "Ian Molloy",
                "Biplav Srivastava"
            ],
            "title": "Detecting backdoor attacks on deep neural networks by activation clustering",
            "venue": "In AAAI Artificial Intelligence Safety Workshop (SafeAI),",
            "year": 2018
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Bo Li",
                "Kimberly Lu",
                "Dawn Song"
            ],
            "title": "Targeted backdoor attacks on deep learning systems using data poisoning",
            "venue": "arXiv preprint arXiv:1712.05526,",
            "year": 2017
        },
        {
            "authors": [
                "Tian Dong",
                "Bo Zhao",
                "Lingjuan Lyu"
            ],
            "title": "Privacy for free: How does dataset condensation help privacy",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Yansong Gao",
                "Change Xu",
                "Derui Wang",
                "Shiping Chen",
                "Damith C Ranasinghe",
                "Surya Nepal"
            ],
            "title": "Strip: A defence against trojan attacks on deep neural networks",
            "venue": "In Proceedings of the 35th Annual Computer Security Applications Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Benyamin Ghojogh",
                "Ali Ghodsi",
                "Fakhri Karray",
                "Mark Crowley"
            ],
            "title": "Reproducing kernel hilbert space, mercer\u2019s theorem, eigenfunctions, nystr\u00f6m method, and use of kernels in machine learning: Tutorial and survey",
            "venue": "arXiv preprint arXiv:2106.08443,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gu",
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Badnets: Evaluating backdooring attacks on deep neural networks",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Bobby He",
                "Balaji Lakshminarayanan",
                "Yee Whye Teh"
            ],
            "title": "Bayesian deep ensembles via the neural tangent kernel",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "George Kimeldorf",
                "Grace Wahba"
            ],
            "title": "Some results on tchebycheffian spline functions",
            "venue": "Journal of mathematical analysis and applications,",
            "year": 1971
        },
        {
            "authors": [
                "Hae Beom Lee",
                "Dong Bok Lee",
                "Sung Ju Hwang"
            ],
            "title": "Dataset condensation with latent space knowledge factorization and sharing",
            "venue": "arXiv preprint arXiv:2208.10494,",
            "year": 2022
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Lechao Xiao",
                "Samuel Schoenholz",
                "Yasaman Bahri",
                "Roman Novak",
                "Jascha SohlDickstein",
                "Jeffrey Pennington"
            ],
            "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Saehyung Lee",
                "Sanghyuk Chun",
                "Sangwon Jung",
                "Sangdoo Yun",
                "Sungroh Yoon"
            ],
            "title": "Dataset condensation with contrastive signals",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Yige Li",
                "Xixiang Lyu",
                "Nodens Koren",
                "Lingjuan Lyu",
                "Bo Li",
                "Xingjun Ma"
            ],
            "title": "Anti-backdoor learning: Training clean models on poisoned data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yige Li",
                "Xixiang Lyu",
                "Nodens Koren",
                "Lingjuan Lyu",
                "Bo Li",
                "Xingjun Ma"
            ],
            "title": "Neural attention distillation: Erasing backdoor triggers from deep neural networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Yuezun Li",
                "Yiming Li",
                "Baoyuan Wu",
                "Longkang Li",
                "Ran He",
                "Siwei Lyu"
            ],
            "title": "Invisible backdoor attack with sample-specific triggers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Fine-pruning: Defending against backdooring attacks on deep neural networks",
            "venue": "In International Symposium on Research in Attacks, Intrusions and Defenses (RAID),",
            "year": 2018
        },
        {
            "authors": [
                "Songhua Liu",
                "Kai Wang",
                "Xingyi Yang",
                "Jingwen Ye",
                "Xinchao Wang"
            ],
            "title": "Dataset distillation via factorization",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Tengjun Liu",
                "Ying Chen",
                "Wanxuan Gu"
            ],
            "title": "Copyright-certified distillation dataset: Distilling one million coins into one bitcoin with your private key",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Yanqing Liu",
                "Jianyang Gu",
                "Kai Wang",
                "Zheng Zhu",
                "Wei Jiang",
                "Yang You"
            ],
            "title": "Dream: Efficient dataset distillation by representative matching",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Yingqi Liu",
                "Shiqing Ma",
                "Yousra Aafer",
                "Wen-Chuan Lee",
                "Juan Zhai",
                "Weihang Wang",
                "Xiangyu Zhang"
            ],
            "title": "Trojaning attack on neural networks",
            "venue": "In 25th Annual Network And Distributed System Security Symposium (NDSS 2018). Internet Soc,",
            "year": 2018
        },
        {
            "authors": [
                "Yugeng Liu",
                "Zheng Li",
                "Michael Backes",
                "Yun Shen",
                "Yang Zhang"
            ],
            "title": "Backdoor attacks against dataset distillation",
            "venue": "Network and Distributed System Security (NDSS) Symposium,",
            "year": 2023
        },
        {
            "authors": [
                "Yunfei Liu",
                "Xingjun Ma",
                "James Bailey",
                "Feng Lu"
            ],
            "title": "Reflection backdoor: A natural backdoor attack on deep neural networks",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Noel Loo",
                "Ramin Hasani",
                "Alexander Amini",
                "Daniela Rus"
            ],
            "title": "Efficient dataset distillation using random feature approximation",
            "venue": "Annual Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Noel Loo",
                "Ramin Hasani",
                "Mathias Lechner",
                "Daniela Rus"
            ],
            "title": "Dataset distillation with convexified implicit gradients",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Afshin Rostamizadeh",
                "Ameet Talwalkar"
            ],
            "title": "Foundations of Machine Learning",
            "year": 2012
        },
        {
            "authors": [
                "Timothy Nguyen",
                "Zhourong Chen",
                "Jaehoon Lee"
            ],
            "title": "Dataset meta-learning from kernel ridgeregression",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Tuan Anh Nguyen",
                "Anh Tran"
            ],
            "title": "Input-aware dynamic backdoor attack",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tuan Anh Nguyen",
                "Anh Tuan Tran"
            ],
            "title": "Wanet - imperceptible warping-based backdoor attack",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Xiangyu Qi",
                "Tinghao Xie",
                "Yiming Li",
                "Saeed Mahloujifar",
                "Prateek Mittal"
            ],
            "title": "Revisiting the assumption of latent separability for backdoor defenses",
            "venue": "In The eleventh international conference on learning representations,",
            "year": 2022
        },
        {
            "authors": [
                "Hossein Souri",
                "Liam Fowl",
                "Rama Chellappa",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Di Tang",
                "XiaoFeng Wang",
                "Haixu Tang",
                "Kehuan Zhang"
            ],
            "title": "Demon in the variant: Statistical analysis of dnns for robust backdoor contamination detection",
            "venue": "In USENIX Security Symposium,",
            "year": 2021
        },
        {
            "authors": [
                "Brandon Tran",
                "Jerry Li",
                "Aleksander Madry"
            ],
            "title": "Spectral signatures in backdoor attacks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Alexander Turner",
                "Dimitris Tsipras",
                "Aleksander Madry"
            ],
            "title": "Label-consistent backdoor attacks",
            "venue": "arXiv preprint arXiv:1912.02771,",
            "year": 2019
        },
        {
            "authors": [
                "Kai Wang",
                "Bo Zhao",
                "Xiangyu Peng",
                "Zheng Zhu",
                "Shuo Yang",
                "Shuo Wang",
                "Guan Huang",
                "Hakan Bilen",
                "Xinchao Wang",
                "Yang You"
            ],
            "title": "Cafe: Learning to condense dataset by aligning features",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Kai Wang",
                "Jianyang Gu",
                "Daquan Zhou",
                "Zheng Zhu",
                "Wei Jiang",
                "Yang You"
            ],
            "title": "Dim: Distilling dataset into generative model",
            "venue": "arXiv preprint arXiv:2303.04707,",
            "year": 2023
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Jun-Yan Zhu",
                "Antonio Torralba",
                "Alexei"
            ],
            "title": "A Efros. Dataset distillation",
            "venue": "arXiv preprint arXiv:1811.10959,",
            "year": 2018
        },
        {
            "authors": [
                "Ruonan Yu",
                "Songhua Liu",
                "Xinchao Wang"
            ],
            "title": "Dataset distillation: A comprehensive review",
            "venue": "arXiv preprint arXiv:2301.07014,",
            "year": 2023
        },
        {
            "authors": [
                "Bo Zhao",
                "Hakan Bilen"
            ],
            "title": "Dataset condensation with distribution matching",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2023
        },
        {
            "authors": [
                "Bo Zhao",
                "Konda Reddy Mopuri",
                "Hakan Bilen"
            ],
            "title": "Dataset condensation with gradient matching",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Ehsan Nezhadarya",
                "Jimmy Ba"
            ],
            "title": "Dataset distillation using neural feature regression",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods. Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks. 1"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, deep neural networks have achieved significant success in many fields, such as natural language modeling, computer vision, medical diagnosis, etc. These successes are usually built on large-scale datasets consisting of millions or even billions of samples. Under this scale of datasets, training a model becomes troublesome because of the need for sufficiently large memory to store the datasets or the need for special infrastructure to train a model. To deal with this problem, dataset distillation (Wang et al., 2018) or dataset condensation (Zhao et al., 2021) is designed to compress the information of large datasets into a small synthetic dataset. These small datasets generated by dataset distillation, called distilled datasets, still retain a certain degree of utility. Under the same model (neural network) structure, the performance of the model trained on the distilled dataset is only slightly lower than that of the model trained on the original large-scale dataset.\nHowever, with the development of dataset distillation techniques, the related security and privacy issues started to emerge (Liu et al., 2023a;c; Dong et al., 2022). In this paper, we focus on backdoor attacks on dataset distillation. In particular, as each distilled sample does not have a clear connection to the original samples, a straightforward stealthy backdoor attack is to poison a benign dataset first and then derive the corresponding distilled poisoned dataset. One can expect that the triggers can hardly be detected visually in the distilled poisoned dataset. However, these triggers, if not designed properly, can be diluted during dataset distillation, making backdoor attacks ineffective.\nLiu et al. (2023c) empirically demonstrate the feasibility of generating a poisoned dataset surviving dataset distillation. In particular, Liu et al. (2023c) propose DOORPING as a distillation-resilient backdoor. However, DOORPING suffers from two major weaknesses. First, the resiliency and optimality of a backdoor against dataset distillation remain unclear, mainly due to the lack of a theoret-\n1Code is available at https://github.com/Mick048/KIP-based-backdoor-attack.git.\nical foundation for the distillation resiliency. Second, DOORPING relies on a bi-level optimization and, as a consequence, consumes a significant amount of time to generate backdoor triggers.\nTo bridge this gap, this paper makes a step toward dataset distillation-resilient backdoors with a theoretical foundation. Our contributions can be summarized as follows:\n\u2022 To the best of our knowledge, we establish the first theoretical framework to characterize backdoor effects on dataset distillation, which explains why certain backdoors survive dataset distillation.\n\u2022 We propose two theory-induced backdoors, simple-trigger and relax-trigger. In particular, relaxtrigger and DOORPING share the same clean test accuracy (CTA) and attack success rate (ASR). However, relax-trigger relies only on ordinary (single-level) optimization procedures and can be computationally efficient.\n\u2022 We experimentally show both simple-trigger and relax-trigger signify the advanced threat vector to either completely break or weaken eight existing defenses. In particular, relax-trigger can evade all eight existing backdoor detection and cleansing methods considered in this paper."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORKS",
            "text": "Dataset Distillation. Dataset distillation is a technique for compressing the information of a target dataset into a small synthetic dataset. The explicit definition can be described as follows. Consider the input space X \u2282 Rd, the label space Y \u2282 RC , and the distribution (x, y) \u223c D, where x \u2208 X and y \u2208 Y . Suppose we are given a dataset denoted by T = {(xt, yt)}Nt=1 \u223c DN where xt \u2208 X , yt \u2208 Y , and N is the number of samples, and a synthetic dataset denoted as S = {(xs, ys)}NSs=1 where xs \u2208 X , ys \u2208 Y , NS is the number of samples in S, and NS \u226a N . The synthetic dataset S\u2217 generated by a dataset distillation method can be formulated as\nS\u2217 = argmin S L(S, T ), (1)\nwhere L is some function to measure the information loss between S and T . There are several types of L. One of the most straightforward ways to define L is to measure the model\u2019s performance. In this sense, the dataset distillation can be reformulated as\nS\u2217 = argmin S\n1\nN \u2113(fS , T ) subject to fS = argmin\nf\u2208H\n1\nNS \u2113(f,S) + \u03bb\u2225f\u22252H (2)\nwhere the model (a classifier) is denoted as f : X \u2192 Y ,H is some collection of models (hypothesis class), \u2113 is the loss function measuring the loss of model evaluated on the dataset, \u03bb \u2265 0 is the weight for the regularization term, and \u2225\u2225H is some norm defined onH. Eq. (2) forms a bi-level optimization problem. This type of dataset distillation is categorized as performance-matching dataset distillation in (Yu et al., 2023). For example, all of the methods from (Wang et al., 2018; Nguyen et al., 2021; Loo et al., 2022; Zhou et al., 2022; Loo et al., 2023) are performance-matching dataset distillation, while the methods from (Zhao & Bilen, 2023; Lee et al., 2022a; Wang et al., 2022; Zhao et al., 2021; Lee et al., 2022b; Liu et al., 2022; 2023b; Wang et al., 2023) belong to either parameterpreserving or distribution-preserving. In this paper, we focus only on performance-matching dataset distillation, with a particular example on kernel inducing points (KIP) from Nguyen et al. (2021).\nReproducing Kernel Hilbert Space and KIP. In general, the inner optimization problem in Eq. (2) does not have a closed-form solution, which not only increases the computational cost, but also increases the difficulty of analyzing this problem. To alleviate this problem, we assume our model lies in the reproducing kernel Hilbert space (RKHS) (Aronszajn, 1950; Berlinet & ThomasAgnan, 2011; Ghojogh et al., 2021). Definition 1 (Kernel). k : X \u00d7 X \u2192 R is a kerenl if the following two points hold. (a) \u2200x, x\u2032 \u2208 X , the kernel k is symmetric; i.e., k(x, x\u2032) = k(x\u2032, x). (b) \u2200n \u2208 N, \u2200{x1, x2, . . . , xn} where each xi are sampled from X , the kernel matrix K defined as Kij := k(xi, xj) is postive semi-definite. Definition 2 (Reproducing Kernel Hilbert Space). Given an kernel k : X \u00d7 X \u2192 R, the collection of real-valued model Hk = {f : X \u2192 R} is a reproducing kernel Hilbert space corresponding to the kernel k, if (a) Hk is a Hilbert space corresponding to the inner product \u27e8\u00b7, \u00b7\u27e9Hk , (b) \u2200x \u2208 X , k(\u00b7, x) \u2208 Hk, (c) \u2200x \u2208 X and f \u2208 Hk, f(x) = \u27e8f, k(\u00b7, x)\u27e9Hk (Reproducing property).\nThere are several advantages to considering RKHS for solving optimization problems. One of the most beneficial properties is that there is Representer Theorem (Kimeldorf & Wahba, 1971; Ghojogh et al., 2021) induced by the reproducing property. In particular, consider the optimization problem:\nf\u2217 = argmin f\u2208Hk\n1\nN N\u2211 i=1 \u2113(f(xi), yi) + \u03bb\u2225f\u22252Hk , (3)\nwhere f : X \u2192 R, yi \u2208 R, \u03bb \u2265 0 is the weight for the regularization term. The solution of the optimization problem f\u2217 can be expressed as the linear combination of {k(\u00b7, xi)}Ni . Furthermore, if we set \u2113(f, (x, y)) = \u2225f(x)\u2212 y\u222522, there is a closed-form expression for f\u2217:\nf\u2217(x) = k(x,X)[k(X,X) +N\u03bbI]\u22121Y , (4)\nwhere k(x,X) = [k(x, x1), k(x, x2), . . . , k(x, xN )], k(X,X) is an N \u00d7 N matrix with [k(X,X)]ij = k(xi, xj), and Y = [y1, y2, . . . , yN ]T . Now, we return to Eq. (2). By rewriting the model f : X \u2192 Y \u2282 Rc as [f1, f2, \u00b7 \u00b7 \u00b7 , f c]T , where each f i : X \u2192 R is a real-valued function and f i is bounded in the RKHSHk, the inner optimization problem for fS in Eq. (2) can be considered as c independent optimization problems and each problem has a closed-form solution as shown in Eq. (4). Thus, the solution of the inner optimization problem can be expressed as\nfS(x) T = k(x,XS)[k(XS ,XS) +NS\u03bbI] \u22121YS , (5)\nwhere k(x,XS) = [k(x, xs1), k(x, xs2), . . . , k(x, xsNS )], k(XS ,XS) is a NS \u00d7 NS matrix with [k(XS ,XS)]ij = k(xsi , xsj ), and YS is a NS \u00d7 c matrix with YS = [ys1 , ys2 , . . . , ysNS ] T .\nThen, the dataset distillation problem can be expressed as\nS\u2217 = argmin S\n1\nN N\u2211 t=1 \u2225fS(xt)\u2212 yt\u222522, (6)\nwhere fS(x)T = k(x,XS)[k(XS ,XS) +NS\u03bbI]\u22121YS as shown in Eq. (5). We reduce a two-level optimization problem to a one-level optimization problem using RKHS. Essentially, KIP (Nguyen et al., 2021) can be formulated as Eq. (6).\nAn important problem for Eq. (6) is how to construct or select a kernel k(\u00b7, \u00b7). Nevertheless, we do not discuss this problem in this paper. We directly consider the neural tangent kernel (NTK) (Jacot et al., 2018; He et al., 2020; Lee et al., 2019) induced by a three-layer neural network as the kernel k(\u00b7, \u00b7) to do the experiment in Section 4.\nBackdoor Attack. Backdoor attack introduces some malicious behavior into the model without degrading the model\u2019s performance on the original task by poisoning the dataset (Gu et al., 2019; Chen et al., 2017; Liu et al., 2018b; Turner et al., 2019; Nguyen & Tran, 2020; Barni et al., 2019; Li et al., 2021c; Nguyen & Tran, 2021; Liu et al., 2020; Tang et al., 2021; Qi et al., 2022; Souri et al., 2022). To be more specific, consider the following scenario. Suppose there are two types of distributions, (xa, ya) \u223c DA and (xb, yb) \u223c DB . DA corresponds to the original normal behavior, while DB corresponds to the malicious behavior. The goal of the backdoor attack is to construct a poisoned dataset such that the model trained on it learns well for both the original normal distribution DA and the malicious distribution DB . In other words, an attacker wants to construct a dataset D\u0303 such that the model trained on D\u0303, denoted fD\u0303, has sufficiently low risk E(xa,ya)\u223cDA\u2113(fD\u0303, (xa, ya)) and E(xb,yb)\u223cDB\u2113(fD\u0303, (xb, yb)) at the same time.\nOne approach to constructing such a dataset D\u0303 is to directly mix the benign dataset DA \u223c DNAA and the trigger dataset DB \u223c DNBB . An attacker usually wants to make the attack stealthy, and so it sets NB \u226a NA. We define DB according to the original normal behavior DA, the trigger T \u2208 Rd, and the trigger label yT \u2208 Y:\n(xb, yb) := ((1\u2212m)\u2299 xa +m\u2299 T, yT ), (7)\nwhere xa \u223c DA, m \u2208 Rd is the real-valued mask, and \u2299 is the Hadamard product."
        },
        {
            "heading": "3 PROPOSED METHODS AND THEORETICAL ANALYSIS",
            "text": "In this paper, we aim to use dataset distillation (KIP as a representative) to perform the backdoor attack. In the simplest form of KIP-based backdoor attacks (as shown in Algorithm 1 of the Appendix), we first construct the poisoned dataset D\u0303 = DA \u222aDB from DNAA and D NB B . Then, we perform KIP on D\u0303 and compress the information in D\u0303 into the distilled poisoned dataset S\u2217 = {(xs, ys)}NSs=1, where NS \u226a NA +NB . Namely, we solve the following optimization problem\nS\u2217 = argmin S\n1\nNA +NB \u2211 (x,y)\u2208D\u0303 \u2225fS(x)\u2212 y\u222522, (8)\nwhere fS(x)T = k(x,XS)[k(XS ,XS)+NS\u03bbI]\u22121YS . Essentially, the above KIP-based backdoor attack is the same as Naive attack in (Liu et al., 2023c) except that the other distillation, instead of KIP, is used in Naive attack. The experimental results in (Liu et al., 2023c) show that ASR grows but CTA drops significantly when the trigger size increases. Liu et al. (2023c) claims a trade-off between CTA and the trigger size. Nonetheless, we find that our KIP-based backdoor attack does not have such a trade-off. This motivates us to develop a theoretical framework for backdoor attacks on dataset distillation.\nBelow, we introduce the theoretical framework in Section 3.1, followed by two theory-induced backdoor attacks, simple-trigger and relax-trigger in Section 3.2 and Section 3.3, respectively."
        },
        {
            "heading": "3.1 THEORETICAL FRAMEWORK",
            "text": "We first introduce the structure of our analysis, which divides the risk of KIP-based backdoor attacks into three parts: projection loss, conflict loss, and generalization gap. Then, we provide an upper bound for each part of the risk.\nStructure of Analysis. Recall that the goal of a KIP-based backdoor attack is to construct the synthetic dataset S\u2217 such that the risk E(x,y)\u223cD\u2113(fS\u2217 , (x, y)) is sufficiently low, where D is the normal distribution DA or the malicious distribution DB . The classical framework for analyzing this problem is to divide the risk into two parts, the empirical risk and generalization gap. Namely,\nE(x,y)\u223cD \u2113(fS\u2217 , (x, y)) = E(x,y)\u223cD \u2113(fS\u2217 , (x, y))\ufe38 \ufe37\ufe37 \ufe38 Empirical risk\n+ [E(x,y)\u223cD \u2113(fS\u2217 , (x, y))\u2212 E(x,y)\u223cD \u2113(fS\u2217 , (x, y))]\ufe38 \ufe37\ufe37 \ufe38 Generalization gap\n(9)\nwhere D = {(xi, yi)}Ni=1 is the dataset sampled from the distribution DN and N is the number of samples of D. Here, we consider that D is DA \u223c DNA or DB \u223c DNB . In our framework, we continue to divide the empirical risk into two parts as\nE(x,y)\u223cD \u2113(fS\u2217 , (x, y)) \u2264 NA +NB\nN [min S E(x,y)\u223cD\u0303\u2113(fS , (x, fD\u0303(x)))\ufe38 \ufe37\ufe37 \ufe38\nProjection Loss\n+E(x,y)\u223cD\u0303\u2113(fD\u0303, (x, y))\ufe38 \ufe37\ufe37 \ufe38 Conflict Loss ]\n(10)\nwhere D\u0303 = DA\u222aDB , fD\u0303 is the model trained on D\u0303 with the weight of the regularization term \u03bb \u2265 0 and fS is the model trained on S with the weight of the regularization term \u03bbS \u2265 0. Intuitively, given a dataset D\u0303 constructed fromDNAA andD NB B , fD\u0303 is regarded as the best model derived from the information of D\u0303. The conflict loss reflects the internal information conflict between the information about DA in D\u0303 and the information about DB in D\u0303. For example, we consider a dog/cat picture classification problem. In the dataset DA, we label the dog pictures with 0 and label the cat pictures with 1. However, in the dataset DB , we label the dog pictures with 1 and label the cat pictures with 0. It is clear that the model trained on D\u0303 must perform terribly on the dataset either DA or DB . In this case, the information between DA and DB have strong conflict and the conflict loss would be large. On the other hand, projection loss reflects the loss of information caused by projecting fD\u0303 into {fS |S = {(xi, yi) \u2208 X \u00d7 Y}NSi=1}. We can also consider the projection loss as the increase in\ninformation induced by compressing the information of D\u0303 into the synthetic dataset S. Take writing an abstract for example. If we want to write a 100 words abstract to describe a 10000 words article, the abstract may suffer some lack of semantics to some degree. Such a phenomena also happens for dataset distillation. When the information of a large dataset is complex enough, the information loss for dataset distillation will be significant; When the information of a large dataset is very simple, it is possible that there is only very limited information loss. We introduce the projection loss defined above to measure this phenomenon. More details can be found below.\nConflict Loss. In a KIP-based backdoor attack, the dataset D\u0303 is defined as D\u0303 = DA \u222aDB , where DA \u223c DNAA and DB \u223c D NB B . By Eq. (5) we know that the model trained on D\u0303 with the weight of the regularization term \u03bb \u2265 0 has a closed-form solution if we constrain the model in the RKHSHck and suppose that \u2113(f, (x, y)) := \u2225f(x)\u2212 y\u222522:\nfD\u0303(x) T = k(x,XAB)[k(XAB ,XAB) + (NA +NB)\u03bbI] \u22121YAB , (11)\nwhere (NA+NB)\u00d7d matrix XAB is the matrix corresponding to the features of D\u0303, (NA+NB)\u00d7c matrix YAB is the matrix corresponding to the labels of D\u0303, k(x,XAB) is a 1\u00d7 (NA +NB) matrix, k(XAB ,XAB) is a (NA + NB) \u00d7 (NA + NB) matrix with [k(XAB ,XAB)]ij = k(xi, xj), and YAB is a (NA +NB)\u00d7 c matrix with YAB = [y1, y2, . . . , y(NA+NB)]T . Hence, we can express the conflict loss Lconflict as\nLconflict = 1\nNA +NB \u2225YAB \u2212 k(XAB ,XAB)[k(XAB ,XAB) + (NA +NB)\u03bbI]\u22121YAB\u222522. (12)\nWe can obtain the upper bound of Lconflict as Theorem 1. Theorem 1 (Upper bound of conflict loss). The conflict loss Lconflict can be bounded as\nLconflict \u2264 1 NA +NB Tr(I \u2212 k(XAB ,XAB)[k(XAB ,XAB) + (NA +NB)\u03bbI]\u22121)2\u2225YAB\u222522 (13)\nwhere Tr is the trace operator, k(XAB ,XAB) is a (NA +NB)\u00d7 (NA +NB) matrix, and YAB is a (NA +NB)\u00d7 c matrix.\nThe proof of Theorem 1 can be found in Appendix A.3. From Theorem 1, we know that the conflict loss can be characterized by Tr(I\u2212k(XAB ,XAB)[k(XAB ,XAB)+(NA+NB)\u03bbI]\u22121). However, in the latter sections, we do not utilize Tr(I\u2212k(XAB ,XAB)[k(XAB ,XAB)+(NA+NB)\u03bbI]\u22121) to construct trigger pattern generalization algorithm; instead, we use Eq. (12) directly. It is because Eq. (12) can be computed more precisely although Tr(I\u2212k(XAB ,XAB)[k(XAB ,XAB)+(NA+ NB)\u03bbI] \u22121) and Eq. (12) have similar computational cost.\nProjection Loss. To derive the upper bound of the projection loss, we first derive Lemma 1.\nLemma 1 (Projection lemma). Given a synthetic dataset S = {(xs, ys)}NSs=1, and a dataset D\u0303 = {(xi, yi)}NA+NBi=1 where (NA +NB) is the number of the samples of D\u0303. Suppose the kernel matrix k(XS ,XS) is invertible, then we have\nk(\u00b7, xi) = k(\u00b7,XS)k(XS ,XS)\u22121k(XS , xi)\ufe38 \ufe37\ufe37 \ufe38 \u2208HS + [k(\u00b7, xi) \u2212 k(\u00b7,XS)k(XS ,XS)\u22121k(XS , xi)]\ufe38 \ufe37\ufe37 \ufe38 \u2208H\u22a5S , \u2200(xi, yi) \u2208 D\u0303 (14)\nwhereHS := span({k(\u00b7, xs) \u2208 Hk|(xs, ys) \u2208 S}) andH\u22a5S is the collection of functions orthogonal to HS corresponding to the inner product \u27e8\u00b7, \u00b7\u27e9Hk . Thus, k(\u00b7,XS)k(XS ,XS)\u22121k(XS , xi) is the solution of the optimization problem:\nargmin f\u2208HS \u2211 (xs,ys)\u2208S \u2225f(xs)\u2212 k(xs, xi)\u222522. (15)\nThe proof of Lemma 1 can be found in Appendix A.2. Now, we turn to the scenario of the KIP-based backdoor attack. Given a mixed dataset D\u0303 = DA \u222aDB where DA \u223c DNAA and DB \u223c D NB B . We also constrained models in the RKHS Hck and suppose \u2113(f, (x, y)) := \u2225f(x) \u2212 y\u222522. With the help of Lemma 1, we can obtain the following theorem:\nTheorem 2 (Upper bound of projection loss). Suppose the kernel matrix of the synthetic dataset k(XS ,XS) is invertible, fS is the model trained on the synthetic dataset S with the regularization term \u03bbS , where the projection loss Lproject = minS E(x,y)\u223cD\u0303\u2113(fS , (x, fD\u0303(x))) can be bounded as\nLproject \u2264 \u2211\n(xi,yi)\u2208D\u0303\nmin XS c\u2211 j=1 |\u03b1i,j |2 NA +NB \u2225k(XAB , xi)\u2212 k(XAB ,XS)k(XS ,XS)\u22121k(XS , xi)\u222522. (16)\nwhere \u03b1i,j := [[k(XAB ,XAB) + (NA +NB)\u03bbI]\u22121YAB ]i,j , which is the weight of k(\u00b7, xi) corresponding to f j\nD\u0303 , XAB is the (NA +NB)\u00d7 d matrix corresponding to the features of D\u0303, XS is the\nNS \u00d7d matrix corresponding to the features of S, YAB is the (NA+NB)\u00d7 c matrix corresponding to the labels of D\u0303, YS is the NS \u00d7 c matrix corresponding to the labels of S.\nThe proof of Theorem 2 can be found in Appendix A.4. In Theorem 2, we first characterize the natural information loss when compressing the information of D\u0303 into an arbitrary dataset S , and then bound the information loss for the synthetic dataset S\u2217 generated by dataset compression by taking the minimum. This formulation gives some insight into the construction of our trigger generation algorithm, which is discussed in the later section.\nGeneralization Gap. Finally, for the generalization gap, we follow the existing theoretical results (Theorem 3.3 in (Mohri et al., 2012)), but modify them a bit. Let G = {g : (x, y) 7\u2192 \u2225f(x) \u2212 y\u222522|f \u2208 Hck}. Assume that the distribution D is distributed in a bounded region, and that G \u2282 C1 and the norm of the gradient of g \u2208 G have a common non-trivial upper bound. Namely, \u2225(x, y)\u2212 (x\u2032, y\u2032)\u22252 \u2264 \u0393D for any sample which is picked from D and \u2225\u2207g\u22252 \u2264 LD. Then we can obtain Theorem 3. Theorem 3 (Upper bound of generalization gap). Given a N -sample dataset D, sampled from the distribution D, the following generalization gap holds for all g \u2208 G with probability at least 1\u2212 \u03b4:\nE(x,y)\u223cD[g((x, y))]\u2212 \u2211\n(xi,yi)\u2208D\ng((xi, yi))\nN \u2264 2R\u0302D(G) + 3LD\u0393D\n\u221a log 2\n\u03b4\n2N , (17)\nwhere X is the matrix of the features of D and R\u0302D(G) is the empirical Rademacher\u2019s complexity.\nThe proof of Theorem 3 can be found in Appendix A.5. We know from Theorem 3 that the upper bound of the generalization gap is characterized by two factors, R\u0302D(G) and \u0393D. The lower R\u0302D(G) and \u0393D imply the lower generalization gap. We usually assume k(x, x) \u2264 r2 and \u221a \u27e8f, f\u27e9Hk \u2264 \u039b (as in Theorem 6.12 of (Mohri et al., 2012)). Under this setting, we can ignore R\u0302D(G) for the upper bound of the generalization gap and only focus on \u0393D. ASR relates to the risk for DB and hence corresponds to the generalization gap evaluated on DB . This theoretical consequence can be used to explain the phenomenon that ASR of the backdoor attack increases as we enlarge the trigger size.\n3.2 THEORY-INDUCED BACKDOOR: SIMPLE-TRIGGER\nConsider D in Theorem 3 as DB and the corresponding dataset D as DB . Conventionally, a cell of the mask m in Eq. (7) is 1 it corresponds to a trigger, and is 0 otherwise. Recall that the definition of DB in Eq. (7), it is clear that the \u0393DB will monotonely decrease from \u0393DA to 0 as we enlarge the trigger size. If we enlarge the trigger size, the \u0393DB drops to zero, which implies that the corresponding generalization gap will be considerably small. Thus, the success of the large trigger pattern can be attributed to its relatively small generalization gap.\nSo, given an image of size m \u00d7 n (m \u2264 n), simple-trigger generates a trigger of size m \u00d7 n. The default pattern for the trigger generated by simple-trigger is whole-white. In fact, since the generalization gap is irrelevant to the trigger pattern, we do not impost any pattern restrictions.\n3.3 THEORY-INDUCED BACKDOOR: RELAX-TRIGGER\nIn simple-trigger, we optimize the trigger through only the generalization gap. However, we know that ASR can be determined by conflict loss, projection loss, and generalization gap because of\nTheorems 1\u223c3 (i.e., all are related to DB). On the other hand, CTA is related to conflict loss and projection loss, because the generalization gap is irrelevant to CTA. That is, Eq. (17) evaluated on DA is a constant as we modify the trigger. As a result, the lower conflict loss, projection loss, and generalization gap imply a backdoor attack with greater ASR and CTA. Therefore, relax-trigger aims to construct a trigger whose corresponding DB make Eq. (12), Eq. (16), and \u0393DB sufficiently low. The computation procedures of relax-trigger can be found in Algorithm 2 of Appendix A.7.\nSuppose DA, NA and NB are fixed. To reduce the bound in Eq. (12), one considers DB as a function depending on the trigger T and then uses the optimizer to find the optimal trigger T \u2217. In this sense, we solve the following optimization problem\nargmin T\n\u2225YAB \u2212 k(XAB ,XAB)[k(XAB ,XAB) + (NA +NB)\u03bbI]\u22121YAB\u222522. (18)\nOn the other hand, a low \u0393DB can be realized by enlarging the trigger as mentioned in Section 3.2.\nFinally, to make Eq. (16) sufficiently low, we consider DB as a function of the trigger T , and then directly optimize\nargmin T  \u2211 (xi,yi)\u2208D\u0303 min XS c\u2211 j=1 |\u03b1i,j |2\u2225k(XAB , xi)\u2212 k(XAB ,XS)k(XS ,XS)\u22121k(XS , xi)\u222522  . (19) However, Eq. (19) is a bi-level optimization problem that is difficult to solve. Instead, we set the synthetic dataset S in Eq. (19) to SA, which is the distilled dataset from DA. Then, the two-level optimization problem can be converted into a one-level optimization problem below.\nargmin T  \u2211 (xi,yi)\u2208D\u0303 c\u2211 j=1 |\u03b1i,j |2\u2225k(XAB , xi)\u2212 k(XAB ,XSA)k(XSA ,XSA)\u22121k(XSA , xi)\u222522  . (20) Eq. (20) can be easily solved by directly applying optimizers like Adam (P. Kingma & Ba, 2015). Eq. (20) aims to find a trigger T such that D\u0303 generated from DA and DB will be compressed into the neighborhood of SA \u2282 (X \u00d7 Y)NS , which guarantees that CTA of the model trained on the distilled D\u0303 is similar to CTA of the model trained on the distilled DA. Overall, relax-trigger solves the following optimization,\nargmin T { \u2211 (xi,yi)\u2208D\u0303 c\u2211 j=1 |\u03b1i,j |2\u2225k(XAB , xi)\u2212 k(XAB ,XSA)k(XSA ,XSA)\u22121k(XSA , xi)\u222522\n+ \u03c1\u2225YAB \u2212 k(XAB ,XAB)[k(XAB ,XAB) + (NA +NB)\u03bbI]\u22121YAB\u222522}, (21) where \u03c1 > 0 is the penalty parameter, m is the previously chosen mask, the malicious dataset is defined as DB = {(xb, yb) = ((1\u2212m)\u2299 xa +m\u2299 T, yT )|(xa, ya) \u2208 DA}. We particularly note that Eq. (19) is converted into Eq. (20) because we use SA to replace the minimization over S. relax-trigger is different from DOORPING in (Liu et al., 2023c). DOORPING generates the trigger during the process of sample compression. In other words, DOORPING is induced by solving a bilevel optimization problem. However, relax-trigger is induced by a one-level optimization problem (Eq. (21)). The design rationale of relax-trigger is different from DOORPING. DOORPING aims to find the globally best trigger but consumes a significant amount of computation time. On the other hand, through our theoretical framework, relax-trigger aims to find the trigger that reliably compresses the corresponding D\u0303 into the neighborhood of our SA with the benefit of time efficiency."
        },
        {
            "heading": "4 EVALUATION",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTING",
            "text": "Dataset. Two datasets are chosen for measuring the backdoor performance.\n\u2022 CIFAR-10 is a 10-class dataset with 6000 32\u00d7 32 color images per class. CIFAR-10 is split into 50000 training images and 10000 testing images.\n\u2022 GTSRB contains 43 classes of traffic signs with 39270 images, which are split into 26640 training images and 12630 testing images. We resize all images to 32\u00d7 32 color images.\nDataset Distillation and Backdoor Attack. We use KIP (Nguyen et al., 2021) to implement backdoor attacks with the neural tangent kernel (NTK) induced by a 3-layer neural network, which has the same structure in the Colab notebook of (Nguyen et al., 2021). We also set the optimizer to Adam (P. Kingma & Ba, 2015), the learning rate to 0.01, and the batch size to 10\u00d7 number of class for each dataset. We run KIP with 1000 training steps to generate a distilled dataset. We perform 3 independent runs for each KIP-based backdoor attack to examine the performance.\nEvaluation Metrics. We consider two metrics, clean test accuracy (CTA) and attack success rate (ASR). Consider S as a distilled dataset from the KIP-based backdoor attack. CTA is defined as the test accuracy of the model trained on S and evaluated on the normal (clean) test dataset, while ASR is defined as the test accuracy of the model trained on S and evaluated on the trigger test dataset. Defense for Backdoor Attack. In this paper we consider eight existing defenses, SCAn (Tang et al., 2021), AC (Chen et al., 2018), SS (Tran et al., 2018), Strip (modified as a poison cleaner) (Gao et al., 2019), ABL (Li et al., 2021a), NAD (Li et al., 2021b), STRIP (backdoor input filter) (Gao et al., 2019), FP (Liu et al., 2018a), to investigate the ability to defend against KIP-based backdoor attack. The implementation of the above defenses is from the backdoor-toolbox2."
        },
        {
            "heading": "4.2 EXPERIMENTAL RESULTS",
            "text": "Performance of simple-trigger. We performed a series of experiments to demonstrate the effectiveness of simple-trigger. In our setting, NS is set to 10 \u00d7 number of classes and 50 \u00d7 number of classes for each dataset. We also configurated the trigger as 2\u00d7 2, 4\u00d7 4, 8\u00d7 8, 16\u00d7 16, 32 \u00d7 32 white square patterns. The corresponding results are shown in Table 1. The experiment results suggest that CTA and ASR of simple-trigger increase as we enlarge the trigger size, which is consistent with our theoretical analysis (Theorem 3). One can see that for the 32\u00d732 white square trigger, ASR can achieve 100% without sacrificing CTA.\nPerformance of relax-trigger. Here, we relax the setting of the mask m; i.e., each component of m is defined to be 0.3, instead of 1. This can be regarded as an increase in the trigger\u2019s transparency (the level of invisibility) for mixing an image and the trigger. Recall the definition of DB in (Eq. 7). From theory point of view, under such a mask m, \u0393DB will drop to 0.3 \u2217 \u0393DA > 0, as we enlarge the trigger. Hence, we cannot reduce the generalization gap considerably as in the experiments of simple-trigger. It turns out that to derive better CTA and ASR, we resort to consider relax-trigger.\nThe result is presented in Table 2. We compare the performance (CTA and ASR) between simpletrigger (32 \u00d7 32 white square), DOORPING and relax-trigger. For CIFAR-10, relax-trigger increases the ASR about 24% from simple-trigger without losing CTA. For GTSRB, relax-trigger not only increases the ASR about 30%, but also slightly increases the CTA. On the other hand, relax-trigger possesses higher CTA and ASR compared to DOORPING. These results confirm the effectiveness of relax-trigger. The trigger patterns of relax-trigger are visualized in Figure 1.\nOff-the-shelf Backdoor Defenses. We examine whether simple-trigger and relax-trigger can survive backdoor detection and cleansing. Here, we utilize backdoor-toolbox and retrain the distilled dataset on ResNet (default setting in backdoor-toolbox) to compute CTA and ASR. In our experimental results, the term \u201cNone\u201d denotes no defense.\n2Available at https://github.com/vtu81/backdoor-toolbox.\nFor simple-trigger, we find that both CTA and ASR of None increase as we enlarge the trigger size. Moreover, both CTA and ASR of None increase as we enlarge the size of the distilled dataset. The above implies that simple-trigger is more suitable for large-size distilled datasets. Since the CTA and ASR increase as we enlarge the trigger, we focus on 32 \u00d7 32 trigger images in the following discussion. In the case of CIFAR-10, for size 100 (see Table 3), we can find that ASR of NAD is still 1. That is, NAD fails to remove the backdoor. For the other defenses, the CTA drops over 7%, though they can reduce the ASR. Hence, we conclude that these defenses are not effective. For size 500 (see Table 5 in Appendix A.8), the ASR of SCAn is still 1, implying that SCAn fails to remove the backdoor. The other defenses, SS, Strip, ABL, STRIP, and FP considerably compromise the CTA. Overall, the above results also suggest that the defenses may be more successful when we increase the size of the distilled dataset. On the other hand, for GTSRB (see Tabel 6 and Table 7 in Appendix A.8), we also reach a similar conclusion.\nFor relax-trigger (see Table 8 in Appendix A.8), all defenses considered in this paper cannot effectively remove the backdoor. In particular, in the case of CIFAR-10, for size 100, SCAn, AC, Strip, and ABL do not reduce the ASR. They even increase ASR to some degree. On the other hand, SS, STRIP, and FP also compromise the CTA too much. Lastly, though NAD reaches a better defense result; however, the corresponding ASR still remains about 50% of None\u2019s ASR. Essentially, this suggests that NAD cannot completely defend against relax-trigger. For the other defenses, the ASR still remains over 30% of None\u2019s ASR. These defenses are ineffective against relax-trigger.\nIn the case of GTSRB, for size 430, we can also find that SCAn, NAD, and STRIP cannot successfully remove the backdoor. The ASR still remains over 70% of None\u2019s ASR. Besides, we can find that AC, SS, Stip, ABL, and FP still compromise the CTA too much. Finally, for size 2150, AC, Strip, NAD, and STRIP still remain ASR over 50% of None\u2019s ASR. Furthermore, SCAn, ABL, and FP even increase the ASR. In addition, SS decreases the CTA by about 45% of None\u2019s CTA. To sum up, relax-trigger shows strong backdoor resiliency against all the tested defenses."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we present a novel theoretical framework based on the kernel inducing points (KIP) method to study the interplay between backdoor attacks and dataset distillation. The backdoor effect is characterized by three key components: conflict loss, projection loss, and generalization gap, along with two theory-induced attacks, simple-trigger and relax-trigger. Our simple-trigger proves that enlarged trigger size leads to improved ASR without sacrificing CTA. Our relax-trigger presents a new and resilient backdoor attack scheme that either completely breaks or significantly weakens eight existing backdoor defense methods. Our study provides novel theoretical insights, unveils new risks of dataset distillation-based backdoor attacks, and calls for better defenses."
        }
    ],
    "title": "TILLATION: A KERNEL METHOD PERSPECTIVE"
}