{
    "abstractText": "Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks. In this work, we present Matcher, a novel perception paradigm that utilizes off-theshelf vision foundation models to address various perception tasks. Matcher can segment anything by using an in-context example without training. Additionally, we design three effective components within the Matcher framework to collaborate with these foundation models and unleash their full potential in diverse perception tasks. Matcher demonstrates impressive generalization performance across various segmentation tasks, all without training. For example, it achieves 52.7% mIoU on COCO-20 with one example, surpassing the state-of-the-art specialist model by 1.6%. In addition, Matcher achieves 33.0% mIoU on the proposed LVIS-92 for one-shot semantic segmentation, outperforming the state-of-the-art generalist model by 14.4%. Our visualization results further showcase the open-world generality and flexibility of Matcher when applied to images in the wild. Our code is at: https://github.com/aim-uofa/Matcher",
    "authors": [
        {
            "affiliations": [],
            "name": "FEATURE MATCHING"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        },
        {
            "affiliations": [],
            "name": "Muzhi Zhu"
        },
        {
            "affiliations": [],
            "name": "Hengtao Li"
        },
        {
            "affiliations": [],
            "name": "Hao Chen"
        },
        {
            "affiliations": [],
            "name": "Xinlong Wang"
        },
        {
            "affiliations": [],
            "name": "Chunhua Shen"
        }
    ],
    "id": "SP:178698e34c4d9801d03cdf94ed8d2a595db81817",
    "references": [
        {
            "authors": [
                "David Arthur",
                "Sergei Vassilvitskii"
            ],
            "title": "K-means++ the advantages of careful seeding",
            "venue": "In Proc. Ann. ACM SIAM Symp. on Disc. Algo.,",
            "year": 2007
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Nicolas Bonneel",
                "Michiel Van De Panne",
                "Sylvain Paris",
                "Wolfgang Heidrich"
            ],
            "title": "Displacement interpolation using lagrangian mass transport",
            "venue": "In Proc. of the SIGGRAPH Asia conf.,",
            "year": 2011
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Xianjie Chen",
                "Roozbeh Mottaghi",
                "Xiaobai Liu",
                "Sanja Fidler",
                "Raquel Urtasun",
                "Alan Yuille"
            ],
            "title": "Detect what you can: Detecting and representing objects using holistic models and body parts",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2014
        },
        {
            "authors": [
                "Ho Kei Cheng",
                "Alexander G Schwing"
            ],
            "title": "Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model",
            "venue": "In Eur. Conf",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In Nor. Amer. Chap. of the ACL,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In Int. Conf. Learn. Represent.,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "Int. J. Comput. Vis.,",
            "year": 2010
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "Lvis: A dataset for large vocabulary instance segmentation",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2022
        },
        {
            "authors": [
                "Sunghwan Hong",
                "Seokju Cho",
                "Jisu Nam",
                "Stephen Lin",
                "Seungryong Kim"
            ],
            "title": "Cost aggregation with 4d convolutional swin transformer for few-shot segmentation",
            "venue": "In Eur. Conf",
            "year": 2022
        },
        {
            "authors": [
                "Ehtesham Iqbal",
                "Sirojbek Safarov",
                "Seongdeok Bang"
            ],
            "title": "Msanet: Multi-similarity and attention guidance for boosting few-shot segmentation",
            "venue": "arXiv preprint arXiv:2206.09667,",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In Proc. Int. Conf. Mach. Learn.,",
            "year": 2021
        },
        {
            "authors": [
                "Joakim Johnander",
                "Martin Danelljan",
                "Emil Brissman",
                "Fahad Shahbaz Khan",
                "Michael Felsberg"
            ],
            "title": "A generative appearance model for end-to-end video object segmentation",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2019
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "In Proc. Adv. Neural Inf. Process",
            "year": 2022
        },
        {
            "authors": [
                "Feng Li",
                "Hao Zhang",
                "Peize Sun",
                "Xueyan Zou",
                "Shilong Liu",
                "Jianwei Yang",
                "Chunyuan Li",
                "Lei Zhang",
                "Jianfeng Gao"
            ],
            "title": "Semantic-sam: Segment and recognize anything at any granularity",
            "venue": "arXiv preprint arXiv:2307.04767,",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Li",
                "Tianhan Wei",
                "Yau Pun Chen",
                "Yu-Wing Tai",
                "Chi-Keung Tang"
            ],
            "title": "Fss-1000: A 1000-class dataset for few-shot segmentation",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Yongqing Liang",
                "Xin Li",
                "Navid Jafari",
                "Jim Chen"
            ],
            "title": "Video object segmentation with adaptive feature bank and uncertain-region refinement",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Huaijia Lin",
                "Xiaojuan Qi",
                "Jiaya Jia"
            ],
            "title": "Agss-vos: Attention guided single-shot video object segmentation",
            "venue": "In Int. Conf. Comput. Vis.,",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Eur. Conf. Comput. Vis.,",
            "year": 2014
        },
        {
            "authors": [
                "Zhihui Lin",
                "Tianyu Yang",
                "Maomao Li",
                "Ziyu Wang",
                "Chun Yuan",
                "Wenhao Jiang",
                "Wei Liu"
            ],
            "title": "Swem: Towards real-time video object segmentation with sequential weighted expectation-maximization",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Juhong Min",
                "Dahyun Kang",
                "Minsu Cho"
            ],
            "title": "Hypercorrelation squeeze for few-shot segmentation",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2021
        },
        {
            "authors": [
                "Keval Morabia",
                "Jatin Arora",
                "Tara Vijaykumar"
            ],
            "title": "Attention-based joint detection of object and semantic part",
            "venue": "arXiv preprint arXiv:2007.02419,",
            "year": 2020
        },
        {
            "authors": [
                "Khoi Nguyen",
                "Sinisa Todorovic"
            ],
            "title": "Feature weighting and boosting for few-shot segmentation",
            "venue": "In Int. Conf. Comput. Vis.,",
            "year": 2019
        },
        {
            "authors": [
                "Maxime Oquab",
                "Timoth\u00e9e Darcet",
                "Th\u00e9o Moutakanni",
                "Huy Vo",
                "Marc Szafraniec",
                "Vasil Khalidov",
                "Pierre Fernandez",
                "Daniel Haziza",
                "Francisco Massa",
                "Alaaeldin El-Nouby"
            ],
            "title": "Dinov2: Learning robust visual features without supervision",
            "venue": "arXiv preprint arXiv:2304.07193,",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "Federico Perazzi",
                "Jordi Pont-Tuset",
                "Brian McWilliams",
                "Luc Van Gool",
                "Markus Gross",
                "Alexander Sorkine-Hornung"
            ],
            "title": "A benchmark dataset and evaluation methodology for video object segmentation",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2016
        },
        {
            "authors": [
                "Jordi Pont-Tuset",
                "Federico Perazzi",
                "Sergi Caelles",
                "Pablo Arbel\u00e1ez",
                "Alex Sorkine-Hornung",
                "Luc Van Gool"
            ],
            "title": "The 2017 davis challenge on video object segmentation",
            "venue": "arXiv preprint arXiv:1704.00675,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In Proc. Int. Conf. Mach. Learn.,",
            "year": 2021
        },
        {
            "authors": [
                "Vignesh Ramanathan",
                "Anmol Kalia",
                "Vladan Petrovic",
                "Yi Wen",
                "Baixue Zheng",
                "Baishan Guo",
                "Rui Wang",
                "Aaron Marquez",
                "Rama Kovvuri",
                "Abhishek Kadian"
            ],
            "title": "Paco: Parts and attributes of common objects",
            "venue": "arXiv preprint arXiv:2301.01795,",
            "year": 2023
        },
        {
            "authors": [
                "Amirreza Shaban",
                "Shray Bansal",
                "Zhen Liu",
                "Irfan Essa",
                "Byron Boots"
            ],
            "title": "One-shot learning for semantic segmentation",
            "venue": "In Brit. Mach. Vis. Conf.,",
            "year": 2017
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Proc. Adv. Neural Inf. Process",
            "year": 2017
        },
        {
            "authors": [
                "Xinlong Wang",
                "Wen Wang",
                "Yue Cao",
                "Chunhua Shen",
                "Tiejun Huang"
            ],
            "title": "Images speak in images: A generalist painter for in-context visual learning",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2023
        },
        {
            "authors": [
                "Xinlong Wang",
                "Xiaosong Zhang",
                "Yue Cao",
                "Wen Wang",
                "Chunhua Shen",
                "Tiejun Huang"
            ],
            "title": "Seggpt: Segmenting everything in context",
            "venue": "In Int. Conf. Comput. Vis.,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "Zongxin Yang",
                "Yunchao Wei",
                "Yi Yang"
            ],
            "title": "Associating objects with transformers for video object segmentation",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2021
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia"
            ],
            "title": "Glm-130b: An open bilingual pre-trained model",
            "venue": "arXiv preprint arXiv:2210.02414,",
            "year": 2022
        },
        {
            "authors": [
                "Jian-Wei Zhang",
                "Yifan Sun",
                "Yi Yang",
                "Wei Chen"
            ],
            "title": "Feature-proxy transformer for few-shot segmentation",
            "venue": "In Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "Renrui Zhang",
                "Zhengkai Jiang",
                "Ziyu Guo",
                "Shilin Yan",
                "Junting Pan",
                "Hao Dong",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "title": "Personalize segment anything model with one shot",
            "venue": "arXiv preprint arXiv:2305.03048,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Muzhi Zhu",
                "Hengtao Li",
                "Hao Chen",
                "Chengxiang Fan",
                "Weian Mao",
                "Chenchen Jing",
                "Yifan Liu",
                "Chunhua Shen"
            ],
            "title": "Segprompt: Boosting open-world segmentation via category-level prompt learning",
            "year": 2023
        },
        {
            "authors": [
                "Xueyan Zou",
                "Jianwei Yang",
                "Hao Zhang",
                "Feng Li",
                "Linjie Li",
                "Jianfeng Gao",
                "Yong Jae Lee"
            ],
            "title": "Segment everything everywhere all at once",
            "venue": "arXiv preprint arXiv:2304.06718,",
            "year": 2023
        },
        {
            "authors": [
                "Bonneel"
            ],
            "title": "The cost cij can be obtained from the cost matrix C by utilizing the mask proposal mp and the reference mask mr",
            "year": 2011
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Pre-trained on web-scale datasets, large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Chowdhery et al., 2022; Zhang et al., 2022b; Zeng et al., 2022; Touvron et al., 2023), like ChatGPT (OpenAI, 2023), have revolutionized natural language processing (NLP). These foundation models (Bommasani et al., 2021) show remarkable transfer capability on tasks and data distributions beyond their training scope. LLMs demonstrate powerful zero-shot and fewshot generalization (Brown et al., 2020) and solve various language tasks well, e.g., language understanding, generation, interaction, and reasoning.\nResearch of vision foundation models (VFMs) is catching up with NLP. Driven by large-scale imagetext contrastive pre-training, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) perform strong zero-shot transfer ability to various classification tasks. DINOv2 (Oquab et al., 2023) demonstrates impressive visual feature matching ability by learning to capture complex information at the image and pixel level from raw image data alone. Recently, the Segment Anything Model (SAM) (Kirillov et al., 2023) has achieved impressive class-agnostic segmentation performance by training on the SA-1B dataset, including 1B masks and 11M images. Unlike LLMs (Brown et al., 2020; Touvron et al., 2023), which seamlessly incorporate various language tasks through a unified model structure and pre-training method, VFMs face limitations when directly addressing diverse perception tasks. For example, these methods often require a task-specific model structure followed by fine-tuning on a specific task (He et al., 2022; Oquab et al., 2023).\nIn this work, we aim to find a new visual research paradigm: investigating the utilization of VFMs for effectively addressing a wide range of perception tasks, e.g., semantic segmentation, part segmentation,\n\u2217Equal contribution. \u2020Part of the work was done when YL was an intern at Beijing Academy of Artificial Intelligence. CS is the corresponding author.\nand video object segmentation, without training. Using foundation models is non-trivial due to the following challenges: 1) Although VFMs contain rich knowledge, it remains challenging to directly leverage individual models for downstream perception tasks. Take SAM as an example. While SAM can perform impressive zero-shot class-agnostic segmentation performance across various tasks, it cannot provide the semantic categories for the predicted masks. Besides, SAM prefers to predict multiple ambiguous mask outputs. It is difficult to select the appropriate mask as the final result for different tasks. 2) Various tasks involve complex and diverse perception requirements. For example, semantic segmentation predicts pixels with the same semantics. However, video object segmentation needs to distinguish individual instances within those semantic categories. Additionally, the structural distinctions of different tasks need to be considered, encompassing diverse semantic granularities ranging from individual parts to complete entities and multiple instances. Thus, naively combining the foundation models can lead to subpar performance.\nTo address these challenges, we present Matcher, a novel perception framework that effectively incorporates different foundation models for tackling diverse perception tasks by using a single in-context example. We draw inspiration from the remarkable generalization capabilities exhibited by LLMs in various NLP tasks through in-context learning (Brown et al., 2020). Prompted by the in-context example, Matcher can understand the specific task and utilizes DINOv2 to locate the target by matching the corresponding semantic feature. Subsequently, leveraging this coarse location information, Matcher employs SAM to predict accurate perceptual results. In addition, we design three effective components within the Matcher framework to collaborate with foundation models and fully unleash their potential in diverse perception tasks. First, we devise a bidirectional matching strategy for accurate cross-image semantic dense matching and a robust prompt sampler for mask proposal generation. This strategy increases the diversity of mask proposals and suppresses fragmented false-positive masks induced by matching outliers. Furthermore, we perform instancelevel matching between the reference mask and mask proposals to select high-quality masks. We utilize three effective metrics, i.e., emd, purity, and coverage, to estimate the mask proposals based on semantic similarity and the quality of the mask proposals, respectively. Finally, by controlling the number of merged masks, Matcher can produce controllable mask output to instances of the same semantics in the target image.\nOur comprehensive experiments demonstrate that Matcher has superior generalization performance across various segmentation tasks, all without the need for training. For one-shot semantic segmentation, Matcher achieves 52.7% mIoU on COCO-20i (Nguyen & Todorovic, 2019), surpassing the state-of-the-art specialist model by 1.6%, and achieves 33.0% mIoU on the proposed LVIS92i, outperforming the state-of-the-art generalist model SegGPT (Wang et al., 2023b) by 14.4%. And Matcher outperforms concurrent PerSAM (Zhang et al., 2023) by a large margin (+29.2% mean mIoU on COCO-20i, +11.4% mIoU on FSS-1000 (Li et al., 2020), and +10.7% mean mIoU on LVIS-92i), suggesting that depending solely on SAM limits the generalization capabilities for semantically-driven tasks, e.g., semantic segmentation. Moreover, evaluated on two proposed benchmarks, Matcher shows outstanding generalization on one-shot object part segmentation tasks. Specifically, Matcher outperforms other methods by about 10.0% mean mIoU on both benchmarks. Matcher also achieves competitive performance for video object segmentation on both DAVIS 2017 val (Pont-Tuset et al., 2017) and DAVIS 2016 val (Perazzi et al., 2016). In addition, exhaustive ablation studies verify the effectiveness of the proposed components of Matcher. Finally, our visualization results show robust generality and flexibility never seen before.\nOur main contributions are summarized as follows:\n\u2022 We present Matcher, one of the first perception frameworks for exploring the potential of vision foundation models in tackling diverse perception tasks, e.g., one-shot semantic segmentation, one-shot object part segmentation, and video object segmentation.\n\u2022 We design three components, i.e., bidirectional matching, robust prompt sampler, and instance-level matching, which can effectively unleash the ability of vision foundation models to improve both the segmentation quality and open-set generality.\n\u2022 Our comprehensive results demonstrate the impressive performance and powerful generalization of Matcher. Sufficient ablation studies show the effectiveness of the proposed components."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Vision Foundation Models Powered by large-scale pre-training, vision foundation models have achieved great success in computer vision. Motivated by masked language modeling (Devlin et al., 2019; Liu et al., 2019) in natural language processing, MAE (He et al., 2022) uses an asymmetric encoder-decoder and conducts masked image modeling to effectively and efficiently train scalable vision Transformer (Dosovitskiy et al., 2020) models. CLIP (Radford et al., 2021) learns image representations from scratch on 400 million image-text pairs and demonstrates impressive zero-shot image classification ability. By performing image and patch level discriminative self-supervised learning, DINOv2 (Oquab et al., 2023) learns all-purpose visual features for various downstream tasks. Recently, pre-trained with 1B masks and 11M images, Segment Anything Model (SAM) (Kirillov et al., 2023) emerges with impressive zero-shot class-agnostic segmentation performance. Although vision foundation models have shown exceptional fine-tuning performance, they have limited capabilities in various visual perception tasks. However, large language models (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023), like ChatGPT (OpenAI, 2023), can solve a wide range of language tasks without training. Motivated by this, this work shows that various perception tasks can be solved training-free by utilizing off-the-shelf vision foundation models to perform in-context inference.\nVision Generalist for Segmentation Recently, a growing effort has been made to unify various segmentation tasks under a single model using Transformer architecture (Vaswani et al., 2017). The generalist Painter (Wang et al., 2023a) redefines the output of different vision tasks as images and utilizes masked image modeling on continuous pixels to perform in-context training with supervised datasets. As a variant of Painter, SegGPT (Wang et al., 2023b) introduces a novel random coloring approach for in-context training to improve the model\u2019s generalization ability. By prompting spatial queries, e.g., points, and text queries, e.g., textual prompts, SEEM (Zou et al., 2023) performs various segmentation tasks effectively. More recently, PerSAM and PerSAM-F (Zhang et al., 2023) adapt SAM for personalized segmentation and video object segmentation without training or with two trainable parameters. This work presents Matcher, a training-free framework for segmenting anything with one shot. Unlike these methods, Matcher demonstrates impressive generalization performance across various segmentation tasks by integrating different foundation models."
        },
        {
            "heading": "3 METHOD",
            "text": "Matcher is a training-free framework that segments anything with one shot by integrating an allpurpose feature extraction model (e.g., DINOv2 (Oquab et al., 2023))and a class-agnostic segmentation model (e.g., SAM (Kirillov et al., 2023)). For the given in-context example, including reference image xr and mask mr, Matcher can segment the objects or parts of a target image xt with the same semantics. The overview of Matcher is depicted in Fig. 1. Our framework consists of three components: Correspondence Matrix Extraction (CME), Prompts Generation (PG), and Controllable\nMasks Generation (CMG). First, Matcher extracts a correspondence matrix by calculating the similarity between the image features of xr and xt. Then, we conduct patch-level matching, followed by sampling multiple groups of prompts from the matched points. These prompts serve as inputs to SAM, enabling the generation of mask proposals. Finally, we perform an instance-level matching between the reference mask and mask proposals to select high-quality masks. We elaborate on the three components in the following subsections."
        },
        {
            "heading": "3.1 CORRESPONDENCE MATRIX EXTRACTION",
            "text": "We rely on off-the-self image encoders to extract features for both the reference and target images. Given inputs xr and xt, the encoder outputs patch-level features zr, zt \u2208 RH\u00d7W\u00d7C . Patch-wise similarity between the two features is computed to discovery the best matching regions of the reference mask on the target image. We define a correspondence matrix S \u2208 RHW\u00d7HW as follows,\n(S)ij = zir \u00b7 z j t\n\u2225zir\u2225 \u00b7 \u2225z j t\u2225\n, (1)\nwhere (S)ij denotes the cosine similarity between i-th patch feature zir of zr and j-th patch feature zjt of zt. We can denote the above formulation in a compact form as S = sim(zr, zt).\nIdeally, the matched patches should have the highest similarity. This could be challenging in practice, since the reference and target objects could have different appearances or even belong to different categories. This requires the encoder to embed rich and detailed information in these features."
        },
        {
            "heading": "3.2 PROMPTS GENERATION",
            "text": "Given the dense correspondence matrix, we can get a coarse segmentation mask by selecting the most similar patches in the target image. However, this naive approach leads to inaccurate, fragmented result with many outliers. Hence, we use the correspondence feature to generate high quality point and box guidance for promptable segmentation. The process involves a bidirectional patch matching and a diverse prompt sampler.\nPatch-Level Matching The encoder tends to produce wrong matches in hard cases such as ambiguous context and multiple instances. We propose a bidirectional matching strategy to eliminate the matching outliers.\n\u2022 As shown in Fig. 2, we first perform bipartite matching between the points on the reference mask Pr = {pir}Li=1 and zt to obtain the forward matched points on the target image P\u2192t = {pit}Li=1 using the forward correspondence matrix S \u2192 = sim(Pr, zt). \u2022 Then, we perform another bipartite matching, named the reverse matching between P\u2192t and zr to obtain the reverse matched points on the reference image P\u2190r = {pir}Li=1 using the reverse correspondence matrix S\u2190 = sim(zr, P\u2192t ).\n\u2022 Finally, we filter out the points in the forward set if the corresponding reverse points are not on the reference mask mr. The final matched points are P\u0302 = {pit \u2208 P\u2192t |pir in mr}.\nRobust Prompt Sampler Inspired by the effective prompt-engineering (Kojima et al., 2022; Wei et al., 2022; Li & Liang, 2021; Zhu et al., 2023), we introduce a robust prompt sampler for the promptable segmenter to support robust segmentation with various semantic granularity, from parts and whole to multiple instances. We first cluster the matched points P\u0302 based on their locations into K clusters P\u0302k with k-means++ (Arthur & Vassilvitskii, 2007). Then the following three types of subsets are sampled as prompts:\n\u2022 Part-level prompts are sampled within each cluster P p \u2282 P\u0302k; \u2022 Instance-level prompts are sampled within all matched points P i \u2282 P\u0302 ; \u2022 Global prompts are sampled within the set of cluster centers P g \u2282 C to encourage coverage,\nwhere C = {c1, c2, . . . , ck} are the cluster centers.\nIn practice, we find this strategy not only increases the diversity of mask proposals but also suppresses fragmented false-positive masks induced by matching outliers."
        },
        {
            "heading": "3.3 CONTROLLABLE MASKS GENERATION",
            "text": "The edge features of an object extracted by the image encoder can confuse background information, inducing some indistinguishable outliers. These outliers can generate some false-positive masks. To overcome this difficulty, we further select high-quality masks from the mask proposals via an instance-level matching module and then merge the selected masks to obtain the final target mask.\nInstance-Level Matching We perform the instance-level matching between the reference mask and mask proposals to select great masks. We formulate the matching to the Optimal Transport (OT) problem and employ the Earth Mover\u2019s Distance (EMD) to compute a structural distance between dense semantic features inside the masks to determine mask relevance. The cost matrix of the OT problem can be calculated by C = 12 (1\u2212 S). We use the method proposed in (Bonneel et al., 2011) to calculate the EMD, noted as emd.\nIn addition, we propose two other mask proposal metrics, i.e., purity = Num(P\u0302mp)Area(mp) and coverage = Num(P\u0302mp)\nNum(P\u0302 ) , to assess the quality of the mask proposals simultaneously, where P\u0302mp = {pit \u2208 P\u2192t |pit in mp}, Num(\u00b7) represents the number of points, Area(\u00b7) represents the area of the mask, and mp is the mask proposal. A higher degree of purity promotes the selection of part-level masks, while a higher degree of coverage promotes the selection of instance-level masks. The false-positive mask fragments can be filtered using the proposed metrics through appropriate thresholds, followed by a score-based selection process to identify the top-k highest-quality masks\nscore = \u03b1 \u00b7 (1\u2212 emd) + \u03b2 \u00b7 purity \u00b7 coverage\u03bb, (2) where \u03b1, \u03b2, and \u03bb are regulation coefficients between different metrics. By manipulating the number of merged masks, Matcher can produce controllable mask output to instances of the same semantics in the target image. More details of emd, purity and coverage are provided in Appendix A."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTS SETTING",
            "text": "Vision Foundation Models We use DINOv2 (Oquab et al., 2023) with a ViT-L/14 (Dosovitskiy et al., 2020) as the default image encoder of Matcher. Benefiting from large-scale discriminative self-supervised learning at both the image and patch level, DINOv2 has impressive patch-level representation ability, which promotes exact patch matching between different images. We use the Segment Anything Model (SAM) (Kirillov et al., 2023) with ViT-H as the segmenter of Matcher. Pre-trained with 1B masks and 11M images, SAM emerges with impressive zero-shot segmentation performance. Combining these vision foundation models has the enormous potential to touch openworld image understanding. In all experiments, we do not perform any training for the Matcher. More implementation details are provided in Appendix B."
        },
        {
            "heading": "4.2 FEW-SHOT SEMANTIC SEGMENTATION",
            "text": "Datasets For few-shot semantic segmentation, we evaluate the performance of Matcher on COCO20i (Nguyen & Todorovic, 2019), FSS-1000 (Li et al., 2020), and LVIS-92i. COCO-20i partitions the 80 categories of the MSCOCO dataset (Lin et al., 2014) into four cross-validation folds, each containing 60 training classes and 20 test classes. FSS-1000 consists of mask-annotated images from 1,000 classes, with 520, 240, and 240 classes in the training, validation, and test sets, respectively. We verify Matcher on the test sets of COCO-20i and FSS-1000 following the evaluation scheme of (Min et al., 2021). Note that, different from specialist models, we do not train Matcher on these datasets. In addition, based on the LVIS dataset (Gupta et al., 2019), we create LVIS-92i, a more challenging benchmark for evaluating the generalization of a model across datasets. After removing the classes with less than two images, we retained a total of 920 classes for further analysis. These classes were then divided into 10 equal folds for testing purposes. For each fold, we randomly sample a reference image and a target image for evaluation and conduct 2,300 episodes.\nResults We compare the Matcher against a variety of specialist models, such as HSNet (Min et al., 2021), VAT (Hong et al., 2022), FPTrans (Zhang et al., 2022a), and MSANet (Iqbal et al., 2022), as well as generalist models like Painter (Wang et al., 2023a), SegGPT (Wang et al., 2023b), and PerSAM (Zhang et al., 2023). As shown in Table 1, for COCO-20i, Matcher achieves 52.7% and 60.7% mean mIoU with one-shot and few-shot, surpassing the state-of-the-art specialist models MSANet and achieving comparable with SegGPT. Note that the training data of SegGPT includes COCO. For FSS-1000, Matcher exhibits highly competitive performance compared with specialist models and surpasses all generalist models. Furthermore, Matcher outperforms training-free PerSAM and fine-tuning PerSAM-F by a significant margin (+29.2% mean mIoU on COCO-20i, +11.4% mIoU on FSS-1000, and +10.7% mean mIoU on LVIS-92i), suggesting that depending solely on SAM results in limited generalization capabilities for semantic tasks. For LVIS-92i, we compare the cross-dataset generalization abilities of Matcher and other models. For specialist models, we report the average performance of four pre-trained models on COCO-20i. Matcher achieves 33.0% and 40.0% mean mIoU with one-shot and few-shot, outperforming the state-of-the-art generalist model SegGPT by 14.4% and 14.6%. Our results indicate that Matcher exhibits robust generalization capabilities that are not present in the other models."
        },
        {
            "heading": "4.3 ONE-SHOT OBJECT PART SEGMENTATION",
            "text": "Datasets Requiring a fine-grained understanding of objects, object part segmentation is a more challenging task than segmenting an object. We build two benchmarks to evaluate the performance of Matcher on one-shot part segmentation, i.e., PASCAL-Part and PACO-Part. Based on PASCAL VOC 2010 (Everingham et al., 2010) and its body part annotations (Chen et al., 2014), we build the PASCAL-Part dataset following (Morabia et al., 2020). The dataset consists of four superclasses, i.e., animals, indoor, person, and vehicles. There are five subclasses for animals, three for indoor, one for person, and six for vehicles. There are 56 different object parts in total. PACO (Ramanathan et al., 2023) is a newly released dataset that provides 75 object categories and 456 object part categories. Based on the PACO dataset, we build the more difficult PACO-Part benchmark for one-shot object part segmentation. We filter the object parts whose area is minimal and those with less than two images, resulting in 303 remaining object parts. We split these parts into four folds, each with about\n76 different object parts. We crop all objects out with their bounding box to evaluate the one-shot part segmentation on both two datasets. More details are provided in Appendix C.\nResults We compare our Matcher with HSNet, VAT, Painter, and PerSAM. For HSNet and VAT, we use the models pre-trained on PASCAL-5i (Shaban et al., 2017) and COCO-20i for PASCAL-Part and PACO-Part, respectively. As shown in Table 2, the results demonstrate that Matcher outperforms all previous methods by a large margin. Specifically, Matcher outperforms the SAM-based PerSAM +12.8% mean mIoU on PASCAL-Part and +13.5% on PACO-Part, respectively. SAM has shown the potential to segment any object into three levels: whole, part, and subpart (Kirillov et al., 2023). However, it cannot distinguish these ambiguity masks due to the lack of semantics. This suggests that SAM alone cannot work well on one-shot object part segmentation. Our method empowers SAM for semantic tasks by combining it with an all-purpose feature extractor and achieves effective generalization performance on fine-grained object part segmentation tasks with an in-context example."
        },
        {
            "heading": "4.4 VIDEO OBJECT SEGMENTATION",
            "text": "Datasets Video object segmentation (VOS) aims to segment a specific object in video frames. Following Wang et al. (2023b), we evaluate Matcher on the validation split of two datasets, i.e., DAVIS 2017 val (Pont-Tuset et al., 2017), and DAVIS 2016 val (Perazzi et al., 2016), under the semi-supervised VOS setting. Two commonly used metrics in VOS, the J score and the F score, are used for evaluation.\nDetails In order to track particular moving objects in a video, we maintain a reference memory containing features and the intermediate predictions of the previous frames in Matcher. We determine which frame to retain in the memory according to the score (see subsection 3.3) of the frames. Considering that objects are more likely to be similar to those in adjacent frames, we apply a decay ratio decreasing by time to the score. We fix the given reference image and mask in the memory to avoid failing when some objects disappear in intermediate frames and reappear later.\nResults We compare Matcher with the models trained with or without video data on different datasets in Table 3. The results show that Matcher can achieve competitive performance compared with the models trained with video data. Moreover, Matcher outperforms the models trained without video data, e.g., SegGPT and PerSAM-F, on both two datasets. These results suggest that Matcher can effectively generalize to VOS tasks without training.\nand J&F on DAVIS 2017 val. Default setting settings are marked in Gray .\nMatcher SegGPT PerSAM-F"
        },
        {
            "heading": "4.5 ABLATION STUDY",
            "text": "As shown in Table 4, we conduct ablation studies on both the difficult COCO-20i dataset and the simple FSS-1000 dataset for one-shot semantic segmentation and DAVIS 2017 val for video object segmentation to sufficiently verify the effectiveness of our proposed components. In this subsection, we explore the effects of matching modules (ILM), patch-level matching strategies, and different mask proposal metrics.\nAblation Study of ILM Patch-level matching (PLM) and instance-level matching (ILM) are the vital components of Matcher that bridge the gap between the image encoder and SAM to solve various few-shot perception tasks training-free. As shown in Table 4a, PLM builds the connection between matching and segmenting and empowers Matcher with the capability of performing various few-shot perception tasks training-free. And ILM enhances this capability by a large margin.\nAblation Study of Bidirectional Matching As shown in Table 4b, we explore the effects of the forward matching and the reverse matching of the proposed bidirectional matching. For the reverse matching, because the matched points P\u2192t (see subsection 3.2) are unavailable when performing reverse matching directly, we perform the reverse matching between zt and zr. Without the guidance of the reference mask, reverse matching (line 2) produces many wrong matching results, resulting in poor performance. Compared with the forward matching (line 1), our bidirectional matching strategy improves the performance by +2.1% mean mIoU on COCO-20i, by +5.9% mIoU on FSS-1000, and by +6.0% J&F on DAVIS 2017. These significant improvements show the effectiveness of the proposed bidirectional matching strategy.\nAblation Study of Different Mask Proposal Metrics As shown in Table 4c, emd is more effective on the complex COCO-20i dataset. emd evaluates the patch-level feature similarity between the mask proposals and the reference mask that encourages matching all mask proposals with the same category. In contrast, by using purity and coverage, Matcher can achieve great performance on DAVIS 2017. Compared with emd, purity and coverage are introduced to encourage selecting high-quality mask proposals. Combining these metrics to estimate mask proposals, Matcher can achieve better performance in various segmentation tasks without training.\nEffect of the Number of Frames for VOS As shown in Table 4d, we also explore the effect of the number of frames on DAVIS 2017 val. The performance of Matcher can be improved as the number of frames increases, and the optimal performance is achieved when using four frames. More ablation studies are provided in Appendix D."
        },
        {
            "heading": "4.6 QUALITATIVE RESULTS",
            "text": "To demonstrate the generalization of our Matcher, we visualize the qualitative results of one-shot segmentation in Fig. 3 from three views, i.e., object and object part segmentation, cross-style object and object part segmentation, and controllable mask output. Our Matcher can achieve higher-quality objects and parts masks than SegGPT and PerSAM-F. Better results on cross-style segmentation show the impressive generalization of Matcher due to effective all-feature matching. In addition, by manipulating the number of merged masks, Macther supports multiple instances with the same semantics. Fig. 4 shows qualitative results of VOS on DAVIS 2017. The remarkable results demonstrate that Matcher can effectively unleash the ability of foundation models to improve both the segmentation quality and open-set generality."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we present Matcher, a training-free framework integrating off-the-shelf vision foundation models for solving various few-shot segmentation tasks. Combining these foundation models properly leads to positive synergies, and Matcher emerges complex capabilities beyond individual models. The introduced universal components, i.e., bidirectional matching, robust prompt sampler, and instancelevel matching, can effectively unleash the ability of these foundation models. Our experiments demonstrate the powerful performance of Matcher for various few-shot segmentation tasks, and our visualization results show open-world generality and flexibility on images in the wild.\nLimitation and Ethics Statement While Matcher demonstrates impressive performance for semanticlevel segmentation, e.g., one-shot semantic segmentation and one-shot object part segmentation, it has relatively limited instance-level matching inherited from the image encoder, which restrains its performance for instance segmentation. However, the comparable VOS performance and the visualization of controllable mask output demonstrate that Matcher has the potential for instance-level segmentation. We will explore it in future work. Our work can unleash the potential of different foundation models for various visual tasks. In addition, our Matcher is built upon open-source foundation models without training, significantly reducing carbon emissions. We do not foresee any obvious undesirable ethical or social impacts now."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by National Science and Technology Major Project (No. 2022ZD0118700). The research was in part supported by the Supercomputing Center of Hangzhou City University, which provided advanced computing resources."
        },
        {
            "heading": "A MORE DETAILS OF INSTANCE-LEVEL MATCHING",
            "text": "The emd metric. The OT problem can be described as follows: suppose that m suppliers U = {ui|i = 1, 2, ...,m} require transport goods for n demanders D = {dj |j = 1, 2, ..., n}, where ui represents the supply units of i-th supplier and dj denotes the demand of j-th demanded. The cost of transporting each unit of goods from the i-th supplier to the j-th demander is represented by cij , and the number of units transported is denoted by \u03c0ij . The goal of the OT problem is to identify a transportation plan \u03c0 = {\u03c0ij |i = 1, ...m, j = 1, ...n} that minimizes the overall transportation cost\nmin \u03c0 \u2211m i=1 \u2211n j=1 cij\u03c0ij .\ns.t. \u2211n\nj=1 \u03c0ij = ui, \u2211m i=1\n\u03c0ij = dj ,\u2211m i=1 ui = \u2211n j=1 dj ,\n\u03c0ij \u2265 0, i = 1, 2, ...m, j = 1, 2, ...n.\n(3)\nIn the context of Matcher, the suppliers are m reference image patches covered by the reference mask, and the demanders are n target image patches covered by the mask proposal (produced by SAM). The goods that the suppliers need to transmit have the same value, i.e., ui = 1m , \u2211 ui = 1. Similarly,\nthe goods that the demanders need also have the same value, i.e., dj = 1n , \u2211\ndj = 1. The cost cij can be obtained from the cost matrix C by utilizing the mask proposal mp and the reference mask mr. Then, we use the method proposed in Bonneel et al. (2011) to calculate the EMD.\nThe purity and coverage metrics Fig. 5 shows examples to demonstrate the effects of the purity and coverage criteria in two scenarios, i.e., single instance and multiple instances. A higher degree of purity promotes the selection of part or single instance masks, while a higher degree of coverage promotes the selection of whole or multiple instance masks.\nB IMPLEMENTATION DETAILS\nWe use DINOv2 (Oquab et al., 2023) with a ViT-L/14 (Dosovitskiy et al., 2020) as the default image encoder of Matcher. And we use the Segment Anything Model (SAM) (Kirillov et al., 2023) with ViT-H as the segmenter of Matcher. In all experiments, we do not perform any training for the Matcher. We set input image sizes are 518 \u00d7 518 for one-shot semantic segmentation and object part segmentation and 896\u00d7 504 for video object segmentation. We conduct experiments from three semantic granularity for semantic segmentation, i.e., parts (PASCAL-Part and PACO-Part), whole (FSS-1000), and multiple instances (COCO-20i and LVIS-92i). We set the number of clusters to 8. For COCO-20i and LVIS-92i, we sample the instance-level points from the matched points and dense image points to encourage SAM to output more instance masks. We set the filtering thresholds emd and purity to 0.67, 0.02 and set \u03b1, \u03b2 and \u03bb to 1.0, 0.0, and 0.0, respectively. For FSS-1000, we sample the global prompts from centers. We set \u03b1, \u03b2, and \u03bb to 0.8, 0.2, and 1.0, respectively. We sample the points from the matched points and use the smallest axis-aligned box containing these\nmatched points for PASCAL-Part and PACO-Part. We set the filtering threshold coverage to 0.3 and set \u03b1, \u03b2 and \u03bb to 0.5, 0.5, and 0.0, respectively. For video object segmentation, we sample the global prompts from centers. We set the filtering threshold emd to 0.75 and set \u03b1, \u03b2, and \u03bb to 0.4, 1.0, and 1.0."
        },
        {
            "heading": "C DATASET DETAILS",
            "text": "PASCAL-Part Based on PASCAL VOC 2010 (Everingham et al., 2010) and its body part annotations (Chen et al., 2014), we build the PASCAL-Part dataset following (Morabia et al., 2020). Table 5 shows the part taxonomy of PASCAL-Part dataset. The dataset consists of four superclasses, i.e., animals, indoor, person, and vehicles. There are five subclasses for animals (bird, cat, cow, dog, horse, sheep), three for indoor (bottle, potted plant, tv monitor), one for person (person), and six for vehicles (aeroplane, bicycle, bus, car, motorbike, train). There are 56 different object parts in total.\nPACO-Part Based on the PACO (Ramanathan et al., 2023) dataset, we build the more difficult PACO-Part benchmark for one-shot object part segmentation. Firstly, we filter the categories having only 1 sample. Then, we filter low-quality examples with an extremely small pixel area within PACO, which leads to significant noise during evaluation, resulting in 303 remaining object parts. Table 6 shows the part taxonomy of the PACO-Part dataset. We split these parts into four folds, each with about 76 different object parts."
        },
        {
            "heading": "D ADDITIONAL RESULTS AND ANALYSIS",
            "text": "Effect of Different Image Encoders Table 7a shows the comparison experiments of CLIP, MAE, and DINOv2. DINOv2 achieves the best performance on all datasets. Because the text-image contrastive pre-training limits learning complex pixel-level information, CLIP cannot precisely match image patches. Although MAE can extract pixel-level features by masked image modeling, it performs poorly. We suspect that the patch-level features extracted by MAE confuse the information about the surrounding patches, resulting in mistaken feature matching. In contrast, pre-trained by image-level and patch-level discriminative self-supervised learning, DIVOv2 extracts all-purpose visual features and exhibit impressive patch-level feature matching ability. As a training-free general perception framework, Matcher can deploy different image encoders. With the continuous development of vision foundation models, the capabilities of vision foundation models will continue to improve, and Matcher\u2019s performance and generalization ability will also be enhanced. This is confirmed by the continuous improvement in performance from MAE to CLIP to DINOv2, demonstrating that Matcher has strong flexibility and scalability. Besides, we aim to make Matcher a valuable tool for assessing the performance of pre-trained foundation models on various downstream tasks.\nEffect of different types of prompts We validated the impact of different prompts on datasets with scenes involving parts (PACO-Part), the whole (FSS-1000), and multiple instances (COCO-20i) in Table 7b: 1) Part-level prompts are needed for PACO-Part, which requires segmenting parts of an instance. However, our experiment results demonstrate that using instance-level prompts yields better results because instance-level prompts cover more situations than part-level prompts. 2) FSS-1000\noften involves one instance that occupies the entire image. Thus, global prompts are used for full image coverage. 3) For COCO-20i, which requires detecting all instances in an image, instance-level points are the most effective. All the experiments are conducted on one fold in both three datasets.\nAblation of model size Table 8a shows the results of Matcher when using VFMs with different model sizes. When using SAM base and DINOv2 base, Matcher still performs well on various datasets and achieves better generalization performance on LVIS-92i than SegGPT. Besides, as the model size increases, Matcher can continuously improve performance.\nEffect of different segmenters Table 7c shows the results when using Semantic-SAM (Li et al., 2023) as the segmenter. Semantic-SAM achieves comparable performance with SAM on four benchmarks. Because Semantic-SAM can output more fine-grained masks, it performs better than SAM on PACO-Part. The results indicate that Matcher is a general segmentation framework.\nUpper bound analysis We conduct experiments on four different datasets and find that the upper bound of Matcher consistently outperforms the current performance on all datasets by a large margin in Table 7d. This indicates that the Matcher framework has more potential. Therefore, Matcher can serve as an effective evaluation criterion for VFMs, assessing the performance of different vision models from a general segmentation perspective. Based on the advantage, Matcher can contribute to developing VFMs.\nHow does few-shot segmentation work? In the few-shot setting, we concatenate multiple references\u2019 features and match them with the target image in the PLM. The remaining process is the same as the one-shot setting. Multiple samples provide richer visual details, enabling more accurate matching results and reducing outliers, resulting in performance improvement.\nVisualizations Fig. 6 shows the quality of background concept segmentation of Matcher. Fig. 7 visualizes the results of Patch-Level Matching, Robust Prompt Sampler and Instance-Level Matching. In addition, We provide more visualizations for one-shot semantic segmentation in Fig. 8, one-shot object part segmentation in Fig. 9 and Fig. 10, controllable mask output in Fig. 11, and video object segmentation in Fig. 12. The remarkable results demonstrate that Matcher can effectively unleash the ability of foundation models to improve both the segmentation quality and open-set generality.\nReference GT Prediction Reference GT Prediction Reference GT Prediction\nFigure 10: Visualization of one-shot object part segmentation on PACO-Part."
        }
    ],
    "year": 2024
}