{
    "abstractText": "Integrals with discontinuous integrands are ubiquitous, arising from discrete structure in applications like topology optimization, graphics, and computational geometry. These integrals are often part of a forward model in an inverse problem where it is necessary to reason backwards about the parameters, ideally using gradient-based optimization. Monte Carlo methods are widely used to estimate the value of integrals, but this results in a non-differentiable approximation that is amenable to neither conventional automatic differentiation nor reparameterization-based gradient methods. This significantly disrupts efforts to integrate machine learning methods in areas that exhibit these discontinuities: physical simulation and robotics, design, graphics, and computational geometry. Although bespoke domain-specific techniques can handle special cases, a general methodology to wield automatic differentiation in these discrete contexts is wanting. We introduce a differentiable variant of the simple Monte Carlo estimator which samples line segments rather than points from the domain. We justify our estimator analytically as conditional Monte Carlo and demonstrate the diverse functionality of the method as applied to image stylization, topology optimization, and computational geometry.",
    "authors": [
        {
            "affiliations": [],
            "name": "FIBER MONTE CARLO"
        }
    ],
    "id": "SP:e52deb750ee90f467b80bdca37d32821b957cd93",
    "references": [
        {
            "authors": [
                "Erik Andreassen",
                "Anders Clausen",
                "Mattias Schevenels",
                "Boyan Lazarov",
                "Ole Sigmund"
            ],
            "title": "Efficient Topology Optimization in MATLAB using 88 Lines of Code",
            "venue": "Structural and Multidisciplinary Optimization, 43:1\u201316,",
            "year": 2011
        },
        {
            "authors": [
                "Sai Praveen Bangaru",
                "Jesse Michel",
                "Kevin Mu",
                "Gilbert Bernstein",
                "Tzu-Mao Li",
                "Jonathan RaganKelley"
            ],
            "title": "Systematically Differentiating Parametric Discontinuities",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "T. Belytschko",
                "Shaoping Xiao",
                "Chandu Parimi"
            ],
            "title": "Topology Optimization with Implicit Functions and Regularization",
            "venue": "International Journal for Numerical Methods in Engineering,",
            "year": 2003
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "URL http: //github.com/google/jax",
            "venue": "JAX: Composable Transformations of Python+NumPy programs,",
            "year": 2018
        },
        {
            "authors": [
                "Emilien Dupont",
                "Hyunjik Kim",
                "SM Eslami",
                "Danilo Rezende",
                "Dan Rosenbaum"
            ],
            "title": "From Data to Functa: Your Data Point is a Function and You Should Treat it like One",
            "venue": "arXiv preprint arXiv:2201.12204,",
            "year": 2022
        },
        {
            "authors": [
                "Roy Frostig",
                "Matthew James Johnson",
                "Chris Leary"
            ],
            "title": "Compiling Machine Learning Programs via High-level Tracing",
            "venue": "Systems for Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Ioannis Gkioulekas",
                "Shuang Zhao",
                "Kavita Bala",
                "Todd Zickler",
                "Anat Levin"
            ],
            "title": "Inverse Volume Rendering with Material Dictionaries",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2013
        },
        {
            "authors": [
                "Paul Glasserman"
            ],
            "title": "Monte Carlo Methods in Financial Engineering, volume 53",
            "year": 2004
        },
        {
            "authors": [
                "Paul Glasserman",
                "Yu-Chi Ho"
            ],
            "title": "Gradient Estimation via Perturbation Analysis, volume 116",
            "venue": "Springer Science & Business Media,",
            "year": 1991
        },
        {
            "authors": [
                "Peter W Glynn"
            ],
            "title": "Likelihood Ratio Gradient Estimation for Stochastic Systems",
            "venue": "Communications of the ACM,",
            "year": 1990
        },
        {
            "authors": [
                "Andreas Griewank",
                "Andrea Walther"
            ],
            "title": "Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation",
            "year": 2008
        },
        {
            "authors": [
                "Norman P Jouppi",
                "Cliff Young",
                "Nishant Patil",
                "David Patterson",
                "Gaurav Agrawal",
                "Raminder Bajwa",
                "Sarah Bates",
                "Suresh Bhatia",
                "Nan Boden",
                "Al Borchers"
            ],
            "title": "In-Datacenter Performance Analysis of a Tensor Processing Unit",
            "venue": "In Proceedings of the 44th annual international symposium on computer architecture,",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding Variational Bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Minchen Li",
                "Zachary Ferguson",
                "Teseo Schneider",
                "Timothy Langlois",
                "Denis Zorin",
                "Daniele Panozzo",
                "Chenfanfu Jiang",
                "Danny M Kaufman"
            ],
            "title": "Incremental Potential Contact: Intersection-and Inversion-free, Large-deformation Dynamics",
            "venue": "ACM transactions on graphics,",
            "year": 2020
        },
        {
            "authors": [
                "Shichen Liu",
                "Weikai Chen",
                "Tianye Li",
                "Hao Li"
            ],
            "title": "Soft Rasterizer: Differentiable Rendering for Unsupervised Single-view Mesh Reconstruction",
            "year": 1901
        },
        {
            "authors": [
                "David JC MacKay",
                "David JC Mac Kay"
            ],
            "title": "Information Theory, Inference and Learning Algorithms",
            "venue": "Cambridge university press,",
            "year": 2003
        },
        {
            "authors": [
                "Shakir Mohamed",
                "Mihaela Rosca",
                "Michael Figurnov",
                "Andriy Mnih"
            ],
            "title": "Monte Carlo Gradient Estimation in Machine Learning",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Nikita Morozov",
                "Denis Rakitin",
                "Oleg Desheulin",
                "Dmitry Vetrov",
                "Kirill Struminsky"
            ],
            "title": "Differentiable Rendering with Reparameterized Volume Sampling",
            "venue": "arXiv preprint arXiv:2302.10970,",
            "year": 2023
        },
        {
            "authors": [
                "Georg Ch Pflug"
            ],
            "title": "Optimization of Stochastic Models: the Interface between Simulation and Optimization, volume 373",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep Learning on Point Sets for 3d Classification and Segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "J Radon"
            ],
            "title": "Uber die bestimmug von funktionen durch ihre integralwerte laengs geweisser mannigfaltigkeiten",
            "venue": "Berichte Saechsishe Acad. Wissenschaft. Math. Phys., Klass,",
            "year": 1917
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Stephen O Rice"
            ],
            "title": "Mathematical analysis of random noise",
            "venue": "The Bell System Technical Journal,",
            "year": 1944
        },
        {
            "authors": [
                "Luis A Santal\u00f3"
            ],
            "title": "Integral geometry and geometric probability",
            "venue": "Cambridge university press,",
            "year": 2004
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien Martel",
                "Alexander Bergman",
                "David Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Implicit Neural Representations with Periodic Activation Functions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nico van Dijk",
                "Kurt Maute",
                "Matthijs Langelaar",
                "Fred Keulen"
            ],
            "title": "Level-set Methods for Structural Topology Optimization: A Review",
            "venue": "Structural and Multidisciplinary Optimization,",
            "year": 2013
        },
        {
            "authors": [
                "Ronald J Williams"
            ],
            "title": "Simple Statistical Gradient-following Algorithms for Connectionist Reinforcement Learning",
            "venue": "Machine learning,",
            "year": 1992
        },
        {
            "authors": [
                "Zhirong Wu",
                "Shuran Song",
                "Aditya Khosla",
                "Fisher Yu",
                "Linguang Zhang",
                "Xiaoou Tang",
                "Jianxiong Xiao"
            ],
            "title": "3d Shapenets: A Deep Representation for Volumetric Shapes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Yuting Yang",
                "Connnely Barnes",
                "Andrew Adams",
                "Adam Finkelstein"
            ],
            "title": "A : Autodiff for Discontinuous Programs\u2013Applied to Shaders",
            "year": 2022
        },
        {
            "authors": [
                "N Donald Ylvisaker"
            ],
            "title": "The expected number of zeros of a stationary gaussian process",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1965
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Determining the value of an integral is a problem at the core of myriad applications across machine learning, statistics, and computational science and engineering. For example, in graphics, pixel shaders in rasterizing rendering engines compute a spatial integral over a collection of geometric primitives to compute an image. In machine learning and statistical inference, integrals abound in operationalizing probabilistic inference systems: marginalization and normalization are both fundamental to Bayesian computation.\nModeling the generative processes associated with these applications involves computing (or at minimum, estimating) the value of these integrals in the forward model. In applications with discrete structure, the forward model often contains integrals with parametric discontinuities. That is to say, integrals whose integrand contains some discontinuous expression (with respect to a parameter of the model). As a simple concrete case, consider two polygons P1, P2 2 \u2326 \u2713 R2; squares, for instance. Suppose the location of P1 is fixed, but we can choose an amount \u2713 2 R to translate P2 along the vector ( 1, 1)T to maximize the area of intersection L : R 7! R of the shapes (see fig. 1). This corresponds with an optimization problem whose objective is an integral with parametric discontinuity (with respect to \u2713):\nL(\u2713) = Z\n\u2326 I[x 2 (P1 \\ P2(\u2713))]dx. (1)\nIntuitively, integrals with parametric discontinuities like this arise from, e.g., hard spatial boundaries and collisions between objects in simulating physics, shadowing and overlap phenomena in a rendering pipeline, or the discrete geometry in topology optimization. With the increasing popularity of machine learning systems that are tightly coupled with physical simulation, rendering, and other geometric computation, these integrals are ubiquitous. We could use simple Monte Carlo to estimate the value of an integral with a parametric discontinuity, but we want to reason backwards about the parameters (using the derivatives r\u2713L) and simple Monte Carlo results in a nondifferentiable estimate (see fig. 1), even though L is differentiable in exact form.\nThe prospect of automatically differentiating these programs with respect to the parameters is enticing, but conventional automatic differentiation systems do not compute derivatives of these parametric discontinuities correctly, so general gradient-based optimization techniques cannot be applied directly (Bangaru et al., 2021). Existing optimization methods in these contexts involve bespoke solutions: smoothing of the discontinuous integrands (Liu et al., 2019), ignoring discontinuities (Gkioulekas et al., 2013), including discontinuities as constraints and (locally) solving nonlinear programs (Li et al., 2020), and more recently domain-specific differentiation languages (Bangaru et al., 2021). To our knowledge, no existing methodology can utilize generic automatic differentiation frameworks, which offer the advantage of domain-independent computation of derivative functions, higher-order differentiation, and mapping computations to accelerator backends (Jouppi et al., 2017) via optimizing compilers (Frostig et al., 2018).\nIn this work, we introduce Fiber Monte Carlo, a variant of simple Monte Carlo which enables differentiable integral estimates, even when the integrand contains a parametric discontinuity. Fiber Monte Carlo samples line segments from the domain rather than points, and adds up the continuous amount of those line segments under a function rather than the discrete number of points under a function. For low-dimensional, physics/geometry-oriented problems like graphics, topology optimization, computational geometry, and physical simulation, Fiber Monte Carlo can be employed to estimate these problematic integrals in an unbiased and differentiable way: enabling gradient-based optimization. It is important to note that Fiber Monte Carlo is appropriate provided the domain of integration has small dimension. This is unrelated to the dimension of the parameter: we show that there is no difficulty in training large parametric models (e.g., neural networks) with this framework. Further, we implement a variety of generic Fiber Monte Carlo estimators in a general-purpose automatic differentiation library, JAX (Bradbury et al., 2018).\nWe further illustrate the ubiquity of integrals with parametric discontinuities and the generality of the estimator by utilizing Fiber Monte Carlo in applications spanning graphics, topology optimization, and computational geometry. In section 4.1 we describe an image stylization application in which we build a simple differentiable rendering engine, using Fiber Monte Carlo as a method for implementing probabilistic pixel shaders with neural field rendering primitives. In section 4.2 we demonstrate a differentiable method for topology optimization of mechanical structures. In section 4.3 we describe a method for the amortized computation of approximate convex hulls using neural networks, with applications to membership oracles and intersection primitives that inform a variety of problems in computational geometry."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 MONTE CARLO GRADIENT ESTIMATION",
            "text": "Methods to compute derivatives of the expectation of a function (so-called stochastic derivatives) are widely studied (Mohamed et al., 2020). These methods are concerned with a class of optimization problems in which the objective L can be written as the expectation of a random variable x under\na \u2018cost function\u2019 f , with respect to a distribution p (with parameter \u2713). Often the cost function is also parametric (with parameter ), but in the standard formulation the optimization variable is the parameter \u2713 of the distribution p. The objective is often written as\nL(\u2713) = Ep\u2713(x)[f (x)] = Z\ndom x p\u2713(x)f (x)dx.\nTo use a gradient method, one aims to estimate derivatives r\u2713L(\u2713) of the objective. At a coarse level, the taxonomy of gradient estimators contains two groups. The first compute derivatives by differentiation of the measure p\u2713(x). This class includes the score function estimator (Glynn, 1990; Williams, 1992) and the measure-valued gradient (Pflug, 2012). In relation to Fiber Monte Carlo, these estimators can similarly be employed against discontinuous (even black-box) cost functions f , but differ in that they do not use derivative information from the cost function f .\nThe second class of gradient estimators compute derivatives of \u201cpaths\u201d, i.e., cleverly designed additions to a computation graph which elicit a dependency of the cost f (x) through the distributional parameters \u2713. Pathwise gradient estimators are popular in machine learning under the name \u201creparameterization\u201d (Kingma & Welling, 2013) and \u201cstochastic backpropagation\u201d (Rezende et al., 2014), and appear in perturbation analysis (Glasserman & Ho, 1991), financial engineering (Glasserman, 2004), and optimization (Pflug, 2012) dating back decades. Pathwise gradient estimators are similar to Fiber Monte Carlo in that they exploit structural characteristics of the cost f and are thus less general, but differ in that pathwise derivative methods require that f is differentiable with respect to the optimization variable. Fiber Monte Carlo is applicable in low-dimensional settings and is likewise less general as compared to methods using derivatives of measure, but in contrast to path methods, it obviates the necessity of a differentiable cost f ,"
        },
        {
            "heading": "2.2 DIFFERENTIATING PARAMETRIC DISCONTINUITIES",
            "text": "Intriguing recent work considers the project of differentiating integrals with parametric discontinuities (Bangaru et al., 2021; Yang et al., 2022; Morozov et al., 2023). These works introduce domain specific languages for computing these derivatives, aimed primarily at applications in graphics and differentiable rendering. As we see it, the core contributions are domain-specific languages which, unlike conventional automatic differentiation libraries, correctly compute the derivatives in question, manipulating the Dirac delta contributions from the discontinuities directly and sometimes even including integration as a language primitive (Bangaru et al., 2021). That said, the languages are constrained in their ability to flexibly and automatically manipulate the computation graph, resulting in more burden on the programmer to formulate the application in ways amenable to the compiler (e.g., representing discontinuities via affine functions, ensuring that an integrand is diffeomorphic, etc.). For example, Bangaru et al. (2021) utilizes a source-to-source automatic differentiation framework (Griewank & Walther, 2008). The language cannot differentiate through control flow structures or implement runtime-determined looping (i.e., it is not Turing complete). This is no shortcoming in the context of graphics applications, but represents a limitation in its applicability and wider adoption."
        },
        {
            "heading": "3 METHOD",
            "text": "The probabilistic dual between expectations and integrals is actually instructive in understanding exactly what is problematic about computing derivatives in these contexts. Simple Monte Carlo estimates integrals using random objects with precisely zero spatial extent: points. Put another way, varying the parameters of a discontinuous function over the domain means the value of the estimate can change significantly (on the order of 1/n) by suddenly including/excluding a point even with an arbitrarily small modulation of the parameter (see fig. 1)."
        },
        {
            "heading": "3.1 SAMPLING METHOD",
            "text": "The sampling method utilizes a uniform distribution over line segments (fibers) in the domain of integration; sampling fibers uniformly is the condition which implies correct (unbiased) estimates. In section 3.3 we explain that the key to differentiating these estimates is the use of objects which have extent, and not only location.\nGiven a compact set, the \u2018sampling domain\u2019 \u2326 \u21e2 Rd, the model is comprised of uniform distributions over (1) one \u2018start\u2019 point xs of each fiber (sampled uniformly over \u2326) and (2) the surface of a norm ball with fixed radius ` > 0 centered at xs. We associate with \u2326 an \u2018extended\u2019 domain \u2326 = {x | dist(x, \u2326)  `}, which is the original domain and an extra \u2018shell\u2019 of width ` which forms the domain for the endpoint of each fiber1. The sampling procedure can be written:\nxs \u21e0 Uniform(\u2326), z \u21e0 N (0, Id) . (2)\nSince the length ` is non-random, the endpoint xe 2 \u2326 of the fiber is determined as:\nxe = z `\n||z|| . (3)\nThe estimator induced using fibers as random objects is analogous to the simple Monte Carlo estimator. To estimate the population expectation of a function h : Rd 7! R of a random variable, simple Monte Carlo forms the sample expectation of the value of h over a collection of points. Instead, we form the sample expectation of the line integral of h over a collection of fibers. The estimate, given a collection of n fibers of length ` is:\n\u00b5\u0302n = 1\nn`\nnX\ni=1\nZ t=1\nt=0 h(ri(t))|r0i(t)|dt, (4)\nwhere ri(t) : [0, 1] 7! Rd is a bijective parameterization of the fiber. Section 3.2 explains why this estimator is correct and produces correct derivatives.\nAt first blush, it appears that Fiber Monte Carlo applies only in contexts where one can somehow analytically describe the line integral of the function. Operationally, we use an implicit function formulation in conjunction with implicit differentiation to obviate this apparent requirement (see section 3.3). In our formulation, we need only pointwise function evaluations to evaluate the estimator (akin to simple Monte Carlo)."
        },
        {
            "heading": "3.2 CORRECTNESS",
            "text": "In this section we explain why the Fiber Monte Carlo estimator is correct, and provide a a simple characterization of the variance. To show correctness, we must demonstrate that the induced probability distribution over points x 2 \u2326 is uniform. We first the describe the condition on the distribution of fibers that is required to ensure the distribution is uniform, and then show that the sampling method described in Section 3.1 obeys this condition.\nIn fiber sampling, we first pick a set F of equal length 1-D subsets (intervals) from the extended domain, F \u21e2 2\u2326, and a probability measure pF : F ! R over them. We sample a subset f \u21e0 pF , and then sample a point x 2 f uniformly. The marginal probability p(x) is given as:\np(x) =\nZ\nf2F p(x|f)dpF (f). (5)\nSince x is sampled uniformly on f , p(x|f) = ` I[x 2 f ], where ` = len f is independent of f . We then have\np(x) = `\nZ\nf2F I[x 2 f ]dpF (f) = ` pF (x 2 f), (6)\nwhich reduces the condition of uniform p(x), x 2 \u2326 to pF (x 2 f) = c1, 8x 2 \u2326, for some constant c1. Note that technically p(x) is defined over the extended domain \u2326, but we can use rejection sampling to get a distribution over \u2326. This condition can be summarized as: the probability of each x being contained in a sampled fiber should be independent of x. The condition holds since, as we explain in section 3.1, the extended domain allows us to ensure uniformity on points close to the boundary.\nIn Fiber Monte Carlo, we wish to compute derivatives of expectations (or integrals) containing parametric discontinuities. In the following, we let a \u21e0 A for set A denote a \u21e0 Unif(A). We assume h\u2713 : \u2326 ! R is discontinuous in \u2326, but that Ex\u21e0\u2326[h\u2713(x)] is continuous and (sub-)differentiable\n1\u2326 is strictly larger since start points near the boundary of \u2326 may induce endpoints outside of \u2326.\nin \u2713. Furthermore, we assume Ex\u21e0f [h\u2713(x)|f ], the expectation over each fiber, is continuous and (sub-)differentiable in \u2713 for almost all f 2 F . As a result of the discontinuity,\nr\u2713Ex\u21e0\u2326[h\u2713(x)] 6= Ex\u21e0\u2326[r\u2713h\u2713(x)] (7) so we cannot use the Monte Carlo estimate formed by point-wise gradient evaluations (which is what automatic differentiation frameworks will do). In Fiber Monte Carlo, we decompose the expectation into two parts, one of which is analytically computable. Per our results above,\nEx\u21e0\u2326[h\u2713(x)] = Ef\u21e0F [Ex\u21e0f [h\u2713(x)|f ]] . (8) Since by assumption Ex\u21e0f [h\u2713(x)|f ] is almost everywhere (up to a set of zero measure) a continuous function of \u2713, we can exchange expectation and differentiation as in\nr\u2713Ex\u21e0\u2326[h\u2713(x)] = r\u2713Ef\u21e0F [Ex\u21e0f [h\u2713(x)|f ]]] (9) = Ef\u21e0F [r\u2713Ex\u21e0f [h\u2713(x)|f ]]], (10)\nusing Leibniz\u2019 rule. We compute r\u2713Ex\u21e0f [h\u2713(x)|f ] exactly through the implicit function theorem, described in Section 3.3. Averaging over fibers gives an unbiased estimate of r\u2713Ex\u21e0\u2326[h\u2713(x)]. We can show that this is a variant of conditional Monte Carlo (Owen, 2013). Let h\u2713(x) denote a random variable with x \u21e0 \u2326, and Ex\u21e0f [h\u2713(x)|f ] be a random variable with f \u21e0 F . Var [h\u2713(x)] and Var [Ex\u21e0f [h\u2713(x)|f ]] is the variance for simple Monte Carlo and Fiber Monte Carlo, respectively. By the Law of Total Variance,\nVar [h\u2713(x)] = Ef\u21e0F [Varx\u21e0f [h\u2713(x)]] + Var [Ex\u21e0f [h\u2713(x)|f ]] (11) Var [Ex\u21e0f [h\u2713(x)|f ]] (12)\nwhich means the variance of Fiber Monte Carlo lower bounds that of standard Monte Carlo.\nThere exist many closely related results arising from the study of integral geometry and geometric probability (Santalo\u0301, 2004). Crofton\u2019s formula (Crofton, 1868) and its measure-theoretic generalization, the Radon transform (Radon, 1917), are similarly concerned with expectations of functions integrated over \u2018random\u2019 lines. These results form a cornerstone of integral geometry, and the theoretical basis for applications in tomographic reconstruction, electron microscopy, and seismology. In the appendix, we provide a proof of correctness using only calculus and basic analysis for the two-dimensional case."
        },
        {
            "heading": "3.3 IMPLICIT FUNCTION FORMULATION",
            "text": "We parameterize complex geometries using an implicit function formulation. The interior of a shape is represented using the zero sublevel set2 of a scalar field g : Rd\u21e5Rm 7! R with parameter \u2713 2 Rm. This is the set gz \u2318 {x 2 Rd | g\u2713(x)  0}, with boundary bd gz \u2318 {x 2 Rd | g\u2713(x) = 0}. Notice that with this formulation g can be an almost arbitrarily complex scalar field (e.g., a neural network or some general parametric function), which is crucial to parameterizing flexible geometries.\nSuppose a fiber has nonempty intersection with bd gz , and assume that exactly one endpoint of the fiber lies within the interior and the other lies outside. Then we can differentiably compute the point of intersection with the following implicit differentiation framework.\nWe call u : Rm 7! (0, 1) the \u2018interpolant\u2019 of the fiber; this is an (implicit) function of the parameters with the property that the convex combination \u21b5 = xs + u(\u2713)(xe xs) lies on the fiber and g\u2713(\u21b5) = 0 =) g\u2713(\u21b5) 2 bd gz . The optimality condition F : Rm 7! R that is satisfied when we have determined the correct intersection point between the fiber and the zero-set of the implicit scalar field is:\nF (u(\u2713), \u2713) = g\u2713(\u21b5) = 0 . (13)\nGiven an objective L : Rm 7! R: the desideratum is the total derivative of L with respect to the parameters \u2713. That is,\ndL d\u2713 = @L @u du d\u2713 + @L @\u2713 . (14)\n2Technically, we could use any \u21b5-sublevel set but we use the zero sublevel set as a canonical form.\nThe term du/d\u2713 2 Rm can be computed using implicit differentiation as follows. Note that at the solution found by bisection u\u21e4(\u2713), we know F (u\u21e4(\u2713), \u2713) = 0 by construction. Then,\nd\nd\u2713 F (u(\u2713)), \u2713) =\nd d\u2713 0 =) @F @u du d\u2713 = @F @\u2713 (15)\nComputing du/d\u2713 requires only the division of @F/@\u2713 (an m-vector) by @F/@u (a scalar). With this implicit differentiation framework, and computing the other terms using normal automatic differentiation, we are equipped with a differentiable method for computing the points of intersection. From there, we can estimate the line integral using differentiable quadrature.\nWe assume that g has no more than one zero crossing along each fiber; otherwise, the bisection method does not necessarily converge. Intuitively, if some distribution over scalar fields has no more than one expected zero crossing on some lengthscale bounded by L > 0, we would want to inform our choice of fiber length using L. In the general case (e.g., neural networks), this bound cannot be derived analytically. We find that it is straightforward to choose short enough fibers in practice, but there still exist pathological functions which have arbitrarily many zero crossings on any positive lengthscale. In the appendix, we discuss a special case using Gaussian processes as a distribution over scalar fields which provides a conceptual framework to reason about the choice of fiber length."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 IMAGE STYLIZATION",
            "text": "We examine a simplified rasterization-based 2D rendering model (Bangaru et al., 2021) as a case study for using Fiber Monte Carlo as a method for approaching an inverse problem. At the highest level, a rendering engine takes as input a collection of rendering primitives (in this case, 2D shapes with associated color information) and computes an image which depends discontinuously on the spatial configuration of the rendering primitives. We use Fiber Monte Carlo to construct a differentiable renderer which can be used to \u2018stylize\u2019 images by approximately reproducing them from a flexible collection of parametric shapes."
        },
        {
            "heading": "4.1.1 PROBLEM DEFINITION",
            "text": "Let Cs = [0, 1]3 be the set of colors that can be associated with a pixel or rendering primitive. We are given (1) a target image T 2 Cm\u21e5ns (2) a collection of nf fibers F 2 \u2326nf\u21e52\u21e52\n(where here the sampling domain \u2326 = [0, 0] \u21e5 [m, n]), and (3) a collection of k \u2018rendering primitives\u2019 R = {(ri, ci)}ki=1, where each rendering primitive is parameterized by a p-vector ri and is associated with a color ci 2 Cs. The forward model (computationally, the rendering engine) maps a collection of rendering primitives, a collection of fibers, and an image size into an image. The rendering primitives are represented by implicit functions gi : Rp \u21e5 R2 7! R with parameter ri 2 Rp. That is to say, as in section 3.3, we represent the concrete \u2018shape\u2019 as the zero sublevel set of the implicit function,\ndenoted here with rzi = {x | gi(ri,x)  0}. In exact form, the rendering model corresponds with:\nf(R,F, m, n)ij = kX\ni=1\nZ y=j+1\ny=j\nZ x=i+1\nx=i ci \u00b7 I((x, y) 2 rzi ) . (16)\nWe use Fiber Monte Carlo to estimate this integral, resulting in a function which is differentiable with respect to both the colors and rendering primitive parameters (i.e., the geometry).\nWe determine the concrete color of a pixel as a convex combination of the color values of the primitives: the weights in the combination are given by the estimated amount of spatial overlap Iij 2 Rk between each of the k primitives and pixel (i, j), normalized to sum to one.\nf\u0302(R,F, m, n)ij = c T Iij 1T Iij\n(17)\nThe induced unconstrained optimization problem is: minimizeR||f\u0302(R,F, m, n) T ||22."
        },
        {
            "heading": "4.1.2 RESULTS",
            "text": "In fig. 2 we first stylize an image using triangles as rendering primitives, as a direct comparison to previously implemented methods which use smoothing or domain specific languages (Liu et al., 2019; Bangaru et al., 2021). Then we use negated, upward translated, unnormalized Gaussian density functions as implicit functions, resulting in ellipsoidal primitives. Finally, we use neural networks as rendering primitives, to demonstrate the full generality of the implicit function formulation. Each neural network is a sinusoidal representation network (SIREN), introduced in Sitzmann et al. (2020)."
        },
        {
            "heading": "4.2 LEVEL-SET TOPOLOGY OPTIMIZATION",
            "text": "Topology optimization, also known as structural optimization, is an important tool in the design of mechanical systems that has been studied extensively in the past 30 years (van Dijk et al., 2013). The goal is to choose where to place the material of a mechanical structure to minimize a certain objective function (e.g., stiffness of the structure under load), subject to volume constraints. Due to the highly nonconvex and nondifferentiable objectives, various relaxation approaches are employed. The Solid Isotropic Material with Penalisation (SIMP) approach (Andreassen et al., 2011) is the most common, but suffers from various problems such as mesh dependence and nonphysical materials. We show that fiber sampling can be employed as a level-set approach, an alternate topology optimization algorithm. Further details about the two methods are described in the appendix.\nGiven a regular (e.g., rectangular) domain D \u21e2 R2 and a function g(x, y; \u2713) : R2 ! R parameterized by \u2713 2 Rm, we define an implicit topology as the set:\nI = {(x, y) | (x, y) 2 D, g(x, y; \u2713)  0}. (18)\nFigure 4: Examples from the (out of sample) validation set. The first row shows the original mesh (not the convex hull) and the point cloud input sampled uniformly from its surface. The second row shows the original mesh and the target convex hull. The last row illustrates the zero set of the predicted implicit function g\u2713(X) in purple.\nSince during topology optimization the domain of integration changes, it is common to instead compute integrals over the background domain and introduce a Heaviside function.\nSolving the PDE underlying the governing physics amounts to finding a function u : I ! Rd defined on the implicit domain that minimizes an energy integrated over the domain:\nu \u21e4 = arg min u2H E(u) = arg min u2H\nZ\nD\nH [g(x, y; \u2713)] j (ru(x, y)) dxdy . (19)\nwhere j(\u00b7) is a local strain energy derived from material properties. We discretize the grid and run a linear elasticity simulator with pixel values denoting densities of material at each point. To determine pixel values we rely on the standard approach for level-set topology optimization: for each pixel we measure the area of intersection with the implicit surface. The density of the material is then proportional to the area of intersection. Note that while this will lead to some intermediate densities, they are limited to the boundary of the topology. To make this intersection differentiable with respect to the parameters of the implicit function the usual approach is to smooth the boundary of the implicit surface using a relaxed Heaviside. Instead, we compute this intersection with fiber sampling, which is differentiable even with an exact Heaviside.\nWe present two industry-standard compliance minimization (stiffness maximization) setups subject to a 50% volume constraint, MMB beam and cantilever, and compare the resulting designs and associated compliance values to a typical SIMP implementation (Andreassen et al., 2011). In compliance minimization, the objective is to minimize the attained minimum strain energy from Eq 19. The two setups differ in terms of boundary conditions, visualized in Figure 3c; the MMB beam represents the right half of a bridge, while the cantilever beam is attached to a wall on the left and pulled down via a force on the bottom right. Qualitative visualizations of the learned topologies are in Figure 3. Quantitative comparisons are below:\nSIMP Fiber Sampling MMB Objective 0.747 0.745\nMMB Area 0.533 0.502\nSIMP Fiber Sampling Cantilever Objective 0.756 0.742\nCantilever Area 0.533 0.503\nThe reported objectives are the unitless ratio of the attained strain energy density to the strain energy of a non-physical \u2018gray\u2019 volume, that is, with 50% material everywhere (lower is better). Although these quantitative results indicate that fiber sampling method potentially offers better performance than SIMP, rigorous comparison of the two methods would require extensive experimentation and this is not the focus of this paper."
        },
        {
            "heading": "4.3 AMORTIZED CONVEX HULLS",
            "text": "We present a case study using Fiber Monte Carlo in an end-to-end optimization problem which amortizes the computation of approximate convex hulls in O(nd2) for point clouds comprised of n ddimensional points. Given a point cloud, the output is a set-membership oracle, which essentially answers queries as to whether a point x 2 Rd lies in the convex hull of the points; these procedures can be tied into fast approximate routines concerning intersections between objects (e.g., as in\nray tracing or collisions in computational physics). One interpretation of the oracle is information theoretic: it is an approximate but compressed representation of the convex hull."
        },
        {
            "heading": "4.3.1 PROBLEM DEFINITION",
            "text": "We formulate the problem as unconstrained minimization, where the objective is the sample expectation of the negative intersection over union between the shape produced by the forward model (given a collection of n d-dimensional points X as input) and the convex hull convX 2 X \u2713 Rd. The forward model h : Rm \u21e5 Rn\u21e5d 7! Rp is a \u2018hypernetwork\u2019 in the sense that it produces a p-vector of parameters associated with an implicit function g : Rp \u21e5 Rd 7! R for points in ddimensional space, this is similar to the modulations approach taken by Dupont et al. (2022). We use an architecture in the spirit of PointNet (Qi et al., 2017), see the appendix for details.\nGiven the parameters of \u2713 of the implicit function, convX , and a collection of nf fibers F 2 R nf\u21e52\u21e53, we estimate the intersection over union using Fiber Monte Carlo:\nL : Rp \u21e5 X \u21e5 Rnf\u21e52\u21e53 7! [0, 1] = \u00b5\u0302FMC(gz(\u2713) \\ convX) (20)\nThis results in the unconstrained optimization problem: min\u2713 E[L(g\u2713(X), convX,F)]."
        },
        {
            "heading": "4.3.2 RESULTS",
            "text": "We evaluate our method on a subset of the ModelNet40 dataset (Wu et al., 2015), which consists of several thousand CAD models partitioned into 40 object categories. We train on approximately 200 point cloud/convex hull pairs drawn from the \u2018plant\u2019 category, sampling 1000 points uniformly from the surface of each plant as input X 2 R1000\u21e53. Evaluation is done using 40 out-of-sample point clouds, also from the plant category.\nWe sample m = 40 halfspace directions at initialization, and then use Fiber Monte Carlo to locally optimize the objective in (19) with respect to the parameters of the hypernetwork. After 200 stochastic gradient updates, we achieve 97.8% accuracy on the out of sample validation set: several examples are displayed in fig. 4. The target hulls have in general more points than we use halfspaces; this is the sense in which our set membership oracles offer a compressed representation of convX ."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Fiber Monte Carlo enables gradient-based optimization of objectives which contain integrals with parametric discontinuities. Implemented within a standard automatic differentiation framework with utilities for implicit differentiation, the technique can be applied widely to problems in physical simulation and design, topology optimization, computational geometry, and graphics. Cast as a conditional Monte Carlo estimator, FMC inherits the analytic foundation of Monte Carlo methods from physics and statistics, which accelerates the work necessary to interrogate the theoretical character of the approach and understand its implications as a generic statistical estimator.\nInteracting meaningfully with discontinuities in any gradient method requires compromises and tradeoffs. Importantly, our technique is likely to scale poorly with the dimension of the integration domain. The geometry of high-dimensional spaces implies that via concentration phenomena (e.g., sphere hardening (MacKay et al., 2003)) volumes tend to be confined to regions of smaller relative extent. In the absence of more sophisticated bounding regions to sample from, naively one would expect that achieving a constant variance estimate requires a number of fibers that scales exponentially with dimension.\nThe work we present here arose as a solution to real problems we encountered in projects interacting with generative design of physical systems and differentiable simulation. With that in mind, one future thread of work is integrating the contributions here to problems at the intersection of computational modeling and simulation and generative design. Along a more theoretical axis, we believe Fiber Monte Carlo can also be employed as a general-purpose methodology for uniformly sampling from a large class of manifold structures, an open problem spanning computational geometry, numerical partial differential equation (PDE) solving, and graphics. Crisper theoretical analysis and experimentation would be required to adjudicate the veracity of that hypothesis."
        }
    ],
    "year": 2023
}