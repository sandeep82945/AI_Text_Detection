{
    "abstractText": "Regularization-based methods have so far been among the de facto choices for continual learning. Recent theoretical studies have revealed that these methods all boil down to relying on the Hessian matrix approximation of model weights. However, these methods suffer from suboptimal trade-offs between knowledge transfer and forgetting due to fixed and unchanging Hessian estimations during training. Another seemingly parallel strand of Meta-Continual Learning (MetaCL) algorithms enforces alignment between gradients of previous tasks and that of the current task. In this work we revisit Meta-CL and for the first time bridge it with regularization-based methods. Concretely, Meta-CL implicitly approximates Hessian in an online manner, which enjoys the benefits of timely adaptation but meantime suffers from high variance induced by random memory buffer sampling. We are thus highly motivated to combine the best of both worlds, through the proposal of Variance Reduced Meta-CL (VR-MCL) to achieve both timely and accurate Hessian approximation. Through comprehensive experiments across three datasets and various settings, we consistently observe that VR-MCL outperforms other SOTA methods, which further validates the effectiveness of VR-MCL.",
    "authors": [
        {
            "affiliations": [],
            "name": "VARIANCE REDUCTION"
        },
        {
            "affiliations": [],
            "name": "Yichen Wu"
        },
        {
            "affiliations": [],
            "name": "Long-Kai Huang"
        },
        {
            "affiliations": [],
            "name": "Renzhen Wang"
        },
        {
            "affiliations": [],
            "name": "Deyu Meng"
        },
        {
            "affiliations": [],
            "name": "Ying Wei"
        }
    ],
    "id": "SP:848cbf2d032029c765f39bbf934d78bc7f04d3b2",
    "references": [
        {
            "authors": [
                "Rahaf Aljundi",
                "Min Lin",
                "Baptiste Goujaud",
                "Yoshua Bengio"
            ],
            "title": "Gradient based sample selection for online continual learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zeyuan Allen-Zhu",
                "Elad Hazan"
            ],
            "title": "Variance reduction for faster non-convex optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Elahe Arani",
                "Fahad Sarfraz",
                "Bahram Zonooz"
            ],
            "title": "Learning fast, learning slow: A general continual learning method based on complementary learning system",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Zal\u00e1n Borsos",
                "Mojmir Mutny",
                "Andreas Krause"
            ],
            "title": "Coresets via bilevel optimization for continual learning and streaming",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Pietro Buzzega",
                "Matteo Boschini",
                "Angelo Porrello",
                "Davide Abati",
                "Simone Calderara"
            ],
            "title": "Dark experience for general continual learning: a strong, simple baseline",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Caccia",
                "Rahaf Aljundi",
                "Nader Asadi",
                "Tinne Tuytelaars",
                "Joelle Pineau",
                "Eugene Belilovsky"
            ],
            "title": "New insights on reducing abrupt representation change in online continual learning",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Arslan Chaudhry",
                "Marc\u2019Aurelio Ranzato",
                "Marcus Rohrbach",
                "Mohamed Elhoseiny"
            ],
            "title": "Efficient lifelong learning with a-gem",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Aristotelis Chrysakis",
                "Marie-Francine Moens"
            ],
            "title": "Online continual learning from imbalanced data",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aristotelis Chrysakis",
                "Marie-Francine Moens"
            ],
            "title": "Online bias correction for task-free continual learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Ashok Cutkosky",
                "Francesco Orabona"
            ],
            "title": "Momentum-based variance reduction in non-convex sgd",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron Defazio",
                "Francis Bach",
                "Simon Lacoste-Julien"
            ],
            "title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Cong Fang",
                "Chris Junchi Li",
                "Zhouchen Lin",
                "Tong Zhang"
            ],
            "title": "Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Aravind Rajeswaran",
                "Sham Kakade",
                "Sergey Levine"
            ],
            "title": "Online meta-learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Robert M French"
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in cognitive sciences,",
            "year": 1999
        },
        {
            "authors": [
                "Robert M Gower",
                "Mark Schmidt",
                "Francis Bach",
                "Peter Richt\u00e1rik"
            ],
            "title": "Variance-reduced methods for machine learning",
            "venue": "Proceedings of the IEEE,",
            "year": 1968
        },
        {
            "authors": [
                "Yanan Gu",
                "Xu Yang",
                "Kun Wei",
                "Cheng Deng"
            ],
            "title": "Not just selection, but exploration: Online classincremental continual learning via dual view consistency",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yiduo Guo",
                "Bing Liu",
                "Dongyan Zhao"
            ],
            "title": "Online continual learning through mutual information maximization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Gunshi Gupta",
                "Karmesh Yadav",
                "Liam Paull"
            ],
            "title": "Look-ahead meta learning for continual learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ferenc Husz\u00e1r"
            ],
            "title": "On quadratic penalties in elastic weight consolidation",
            "venue": "arXiv preprint arXiv:1712.03847,",
            "year": 2017
        },
        {
            "authors": [
                "Khurram Javed",
                "Martha White"
            ],
            "title": "Meta-learning representations for continual learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Xu Ji",
                "Joao Henriques",
                "Tinne Tuytelaars",
                "Andrea Vedaldi"
            ],
            "title": "Automatic recall machines: Internal replay, continual learning and the brain",
            "venue": "arXiv preprint arXiv:2006.12323,",
            "year": 2020
        },
        {
            "authors": [
                "Rie Johnson",
                "Tong Zhang"
            ],
            "title": "Accelerating stochastic gradient descent using predictive variance reduction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Robert W Keener"
            ],
            "title": "Theoretical statistics: Topics for a core",
            "year": 2010
        },
        {
            "authors": [
                "Prashant Khanduri",
                "Siliang Zeng",
                "Mingyi Hong",
                "Hoi-To Wai",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "A near-optimal algorithm for stochastic bilevel optimization via double-momentum",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chris Dongjoo Kim",
                "Jinseo Jeong",
                "Gunhee Kim"
            ],
            "title": "Imbalanced continual learning with partitioning reservoir sampling",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Zhizhong Li",
                "Derek Hoiem"
            ],
            "title": "Learning without forgetting",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "James L McClelland",
                "Bruce L McNaughton",
                "Randall C O\u2019Reilly"
            ],
            "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
            "venue": "Psychological review,",
            "year": 1995
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J Cohen"
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "In Psychology of learning and motivation,",
            "year": 1989
        },
        {
            "authors": [
                "Aryan Mokhtari",
                "Hamed Hassani",
                "Amin Karbasi"
            ],
            "title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Lam M Nguyen",
                "Jie Liu",
                "Katya Scheinberg",
                "Martin Tak\u00e1\u010d. Sarah"
            ],
            "title": "A novel method for machine learning problems using stochastic recursive gradient",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "German I Parisi",
                "Ronald Kemker",
                "Jose L Part",
                "Christopher Kanan",
                "Stefan Wermter"
            ],
            "title": "Continual lifelong learning with neural networks: A review",
            "venue": "Neural Networks,",
            "year": 2019
        },
        {
            "authors": [
                "Iosif Pinelis"
            ],
            "title": "Optimum bounds for the distributions of martingales in banach spaces",
            "venue": "The Annals of Probability,",
            "year": 1994
        },
        {
            "authors": [
                "Jathushan Rajasegaran",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Mubarak Shah"
            ],
            "title": "itaml: An incremental task-agnostic meta-learning approach",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jathushan Rajasegaran",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Fully online meta-learning without task boundaries",
            "venue": "arXiv preprint arXiv:2202.00263,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Riemer",
                "Ignacio Cases",
                "Robert Ajemian",
                "Miao Liu",
                "Irina Rish",
                "Yuhai Tu",
                "Gerald Tesauro"
            ],
            "title": "Learning to learn without forgetting by maximizing transfer and minimizing interference",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Hippolyt Ritter",
                "Aleksandar Botev",
                "David Barber"
            ],
            "title": "Online structured laplace approximations for overcoming catastrophic forgetting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "David Rolnick",
                "Arun Ahuja",
                "Jonathan Schwarz",
                "Timothy Lillicrap",
                "Gregory Wayne"
            ],
            "title": "Experience replay for continual learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Dongsub Shim",
                "Zheda Mai",
                "Jihwan Jeong",
                "Scott Sanner",
                "Hyunwoo Kim",
                "Jongseong Jang"
            ],
            "title": "Online class-incremental continual learning with adversarial shapley value",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Johannes Von Oswald",
                "Dominic Zhao",
                "Seijin Kobayashi",
                "Simon Schug",
                "Massimo Caccia",
                "Nicolas Zucchet",
                "Jo\u00e3o Sacramento"
            ],
            "title": "Learning where to learn: Gradient sparsity in meta and continual learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ling Xiao Wang",
                "Kevin Huang",
                "Tengyu Ma",
                "Quanquan Gu",
                "Jing Huang"
            ],
            "title": "Variance-reduced first-order meta-learning for natural language processing tasks",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Quanziang Wang",
                "Renzhen Wang",
                "Yichen Wu",
                "Xixi Jia",
                "Deyu Meng"
            ],
            "title": "Cba: Improving online continual learning via continual bias adaptor",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Renzhen Wang",
                "Xixi Jia",
                "Quanziang Wang",
                "Yichen Wu",
                "Deyu Meng"
            ],
            "title": "Imbalanced semisupervised learning with bias adaptive classifier",
            "venue": "In 11th International Conference on Learning Representations (ICLR 2023),",
            "year": 2023
        },
        {
            "authors": [
                "Yichen Wu",
                "Jun Shu",
                "Qi Xie",
                "Qian Zhao",
                "Deyu Meng"
            ],
            "title": "Learning to purify noisy labels via meta soft label corrector",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yichen Wu",
                "Long-Kai Huang",
                "Ying Wei"
            ],
            "title": "Adversarial task up-sampling for meta-learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hansi Yang",
                "James Kwok"
            ],
            "title": "Efficient variance reduction for meta-learning",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Yang",
                "Da-Wei Zhou",
                "De-Chuan Zhan",
                "Hui Xiong",
                "Yuan Jiang"
            ],
            "title": "Adaptive deep models for incremental learning: Considering capacity scalability and sustainability",
            "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Yang",
                "Da-Wei Zhou",
                "De-Chuan Zhan",
                "Hui Xiong",
                "Yuan Jiang",
                "Jian Yang"
            ],
            "title": "Cost-effective incremental deep model: Matching model capacity with the least sampling",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Dong Yin",
                "Mehrdad Farajtabar",
                "Ang Li",
                "Nir Levine",
                "Alex Mott"
            ],
            "title": "Optimization and generaliza",
            "year": 2024
        },
        {
            "authors": [
                "2020. Friedemann Zenke",
                "Ben Poole",
                "Surya Ganguli"
            ],
            "title": "Continual learning through synaptic intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023b) in continual learning. From the bi-level optimization perspective to analyze meta-learning, its inner-loop (or lower-level) optimization usually fits the training data, while the outer-loop (higher-level) centralized test data aims to increase the model\u2019s generalization ability. Therefore, meta-learning is well suited for continual learning, as we want the model to fit the current task",
            "year": 2023
        },
        {
            "authors": [
                "Chaudhry et al",
                "Aljundi"
            ],
            "title": "2019). Recently, there has been increasing interest in developing online CL methods that can fully utilize the single-pass data stream to decrease the performance difference between online and offline CL",
            "venue": "OCDVC (Gu et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Allen-Zhu",
                "Hazan",
                "Nguyen et al",
                "Fang"
            ],
            "title": "2018). Nonetheless, these methods typically require the expensive computation of the full-batch gradient by processing all samples in a task, which is impractical in the context of online continual learning where samples are processed in mini-batches and the complete sample set is unavailable. To alleviate this expensive computation problem, momentum-based",
            "year": 2018
        },
        {
            "authors": [
                "Cutkosky",
                "Orabona",
                "Khanduri"
            ],
            "title": "2021) are proposed. Specifically, these methods incorporate a momentum term to compute the stochastic gradients at two consecutive iterations on the same set of stochastic samples, thereby replacing the time-consuming computation of the full-batch gradient. Additionally, these methods have the same asymptotic convergence rate as other variance reduction methods",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Continual learning (CL) is a promising approach to constructing learning systems that can continuously process information streams, adapt to changing environments, and acquire new knowledge while consolidating and retaining previously learned knowledge (Parisi et al., 2019). In contrast to traditional supervised learning, which trains on independent and identically distributed (i.i.d) samples, continual learning involves models trained on non-stationary data distributions, where different classification tasks are presented sequentially. The violation of the i.i.d. assumption in CL can lead to catastrophic forgetting, causing a significant drop in test performance on previously learned tasks (French, 1999; McClelland et al., 1995; McCloskey & Cohen, 1989).\nIn recent years, numerous methods in the field of CL have been proposed with the aim of tackling the catastrophic forgetting problem. Regularization-based methods (Kirkpatrick et al., 2017; Ritter et al., 2018; Husza\u0301r, 2017; Zenke et al., 2017), being an important branch of these methods, aim to discern and retain the important parameter weights associated with prior tasks so as to sustain their performance. Specifically, they assess the importance of different parameter weights through the second-order Hessian of previous tasks computed at the end of each task training. The differences among these regularization-based methods primarily reside in their distinct approaches to approximating the Hessian. For example, EWC (Kirkpatrick et al., 2017) and Online-EWC (Husza\u0301r, 2017) employ the diagonal Fisher information matrix, whereas Kronecker factored Laplace approximation (KFLA) (Ritter et al., 2018) utilizes a more accurate Kronecker factorization approximation, considering off-diagonal elements. Similarly, Intelligent Synapses (IS) (Zenke et al., 2017) adopts an approximate diagonal matrix, but its elements take into account the importance of a task throughout the entire training trajectory. Despite these methods striving to improve the Hessian approximation to preserve important weights for previous tasks, their estimated Hessian matrices of previous tasks\n\u2217Part of the work was done when the author interned in Tencent AI Lab (wuyichen.am97@gmail.com). \u2020Corresponding author: Long-Kai Huang (hlongkai@gmail.com) and Ying Wei (ying.wei@ntu.edu.sg)\nremain unchanged and fail to update during subsequent training. As the model parameters gradually deviate from the point at which the Hessian was initially computed, the unaltered estimated Hessian progressively loses accuracy due to the increasing truncation error of the Taylor expansion.\nDifferent from regularization-based methods that explicitly approximate the second-order Hessian at the end of training for each task, as will be detailed in Sec. 3, Meta-Continual Learning (Meta-CL) methods utilize the second-order Hessian information implicitly through the hypergradient obtained via bi-level optimization (Riemer et al., 2019; Gupta et al., 2020; Von Oswald et al., 2021). The computation of hypergradient involves a batch of samples drawn from the memory buffer, which stores examples from previous tasks, enabling Meta-CL to leverage up-to-date second-order information from previous tasks. However, it is noteworthy that the utilization of samples drawn from the memory buffer may not offer a complete representation of previous tasks, and may even lack data pertaining to certain tasks. This can result in the presence of erroneous information in the Hessian estimation. As a consequence, there may be considerable variation in the implicitly estimated Hessian, ultimately leading to a decline in performance for previous tasks.\nTo leverage the benefits of Meta-CL, specifically the timely updating of second-order Hessian information, while mitigating the negative impact of variation, we propose a momentum-based VarianceReduced Meta-CL (VR-MCL). Through our theoretical analysis, we demonstrate that the integration of the momentum-based variance-reduced technique into Meta-CL is equivalent to imposing a penalty on its online estimated Hessian. This penalty effectively suppresses excessive model updates, thereby better preserving crucial weights for previous tasks and their performance. Our main contributions are summarized as follows:\n\u25e6 We introduce a new perspective on Meta-CL by characterizing it as a technique that approximates the Hessian matrix, thereby establishing a connection between Meta-CL and regularized methods.\n\u25e6 To address the issue of high variance in Meta-CL, we propose Variance-Reduced Meta-CL (VRMCL), which incorporates hypergradient variance reduction technique into Meta-CL.\n\u25e6 Theoretically, we demonstrate that our proposed momentum-based hypergradient variance reduction technique is equivalent to the inclusion of an effective penalty term within the implicitly estimated Hessian. Additionally, we provide the regret bound of VR-MCL to further explain its superior performance in the context of continual learning.\n\u25e6 We conduct comprehensive experiments covering three datasets and multiple continual learning settings, providing empirical evidence for the effectiveness of the proposed algorithm."
        },
        {
            "heading": "2 THE UNIFIED FRAMEWORK OF REGULARIZATION-BASED CL METHODS",
            "text": "In this section, we will first derive the unified iterative updating framework used by regularizationbased algorithms following the analysis of Yin et al. (2020) and then show the impact of Hessian approximation accuracy on the performance of these algorithms.\nAnalysis of Regularization-based methods. Continual learning considers a sequence of N tasks [T 1, T 2, \u00b7 \u00b7 \u00b7 , T N ], where the i-th task consists of N i samples, i.e., T i = (Xi,Yi) ={(xin, yin)}N i\nn=0. Here, T j represents the current training task, T [1:j] refers to all j tasks the model have seen so far, and Li(\u03b8) denotes the empirical risk of the parameter \u03b8 on T i. The objective of CL is to learn a model with parameters \u03b8 that minimizes the empirical risk of all seen tasks T [1:j], i.e., 1 j ( \u2211j i=1 L i(\u03b8)). However, in the context of regularization-based CL, we have no access to the samples of previously learned tasks T [1:j\u22121]. The model parameters \u03b8 cannot be directly optimized to minimize the corresponding empirical risks, i.e., \u2211j\u22121 i=1 L\ni(\u03b8). Therefore, regularized CL methods choose to approximate the empirical risk of previous tasks during training on T j (Yin et al., 2020). Concretely, in the simplest case where j=2, when training on T 2, the objective of regularization-based methods is to minimize 1\n2 (Lprox1 +L2), where L prox 1 (\u03b8) is an approximation of L1(\u03b8) derived through a Taylor\nexpansion at \u03b8\u03021 as Lprox1 = L1(\u03b8\u03021) + (\u03b8 \u2212 \u03b8\u03021)\u22a4\u2207L1(\u03b8\u03021) + 12 (\u03b8 \u2212 \u03b8\u0302 1)\u22a4\u22072L1(\u03b8\u03021)(\u03b8 \u2212 \u03b8\u03021). Here, \u03b8\u03021 represents the model parameters at the end training of T 1. For a more general setting where j > 2, we can approximate the empirical loss of the previous (j \u2212 1) tasks by,\nLproxj\u22121 (\u03b8) = j\u22121\u2211 i=1 Li(\u03b8\u0302i)\ufe38 \ufe37\ufe37 \ufe38 (a) +(\u03b8 \u2212 \u03b8\u0302i)\u22a4\u2207\u03b8Li(\u03b8\u0302i)\ufe38 \ufe37\ufe37 \ufe38 (b) + 1 2 (\u03b8 \u2212 \u03b8\u0302i)\u22a4Hi(\u03b8 \u2212 \u03b8\u0302i)\ufe38 \ufe37\ufe37 \ufe38\n(c)\n, (1)\nwhere \u03b8\u0302i and Hi = \u22072\u03b8Li(\u03b8\u0302i) denote the model parameter at the end of training T i and the Hessian matrix, respectively. For the concrete analysis of Lproxj\u22121 (\u03b8) in Eqn. (1), the first term (a) is not related to \u03b8 and thus can be discarded. Furthermore, since the training process at the end of each task is usually converged, the gradient \u2207Li(\u03b8\u0302i) is near zero and the term (b) can be ignored in practice. Therefore, the empirical loss approximation Lproxj\u22121 (\u03b8) is almost equivalent to approximating the Hessian matrix in item (c). Based on this approximation and the analysis, we can derive the iterative update rule of regularized methods in Proposition 1. (See Appendix A.1 for proof.)\nProposition 1. In regularization-based continual learning, if the model parameter \u03b8 is searched within the neighborhood set \u222aj\u22121i=1N i with N i={\u03b8 :d(\u03b8, \u03b8\u0302i)<\u03b4i}, then the iterative update rule of \u03b8 approximately is,\n\u03b8 :\u2248 \u03b8 \u2212 \u03b1(H1 +H2 + \u00b7 \u00b7 \u00b7+Hj\u22121)\u22121\u2207\u03b8Lj(\u03b8). (2)\nThe Effect of Hessian in Regularization-based CL. Let H = \u2211j\u22121\ni=1 H i denote the Hessian part in\nEqn. (2). This update rule implies that the Hessian H plays a crucial role in the optimization process. Specifically, by multiplying the inverse of H with the gradient \u2207\u03b8Lj(\u03b8), the gradient is suppressed along the large curvature directions, i.e., the directions along the eigenvectors corresponding to large eigenvalues of H, and amplified along the small curvature directions. In regularization-based CL algorithms, the large curvature directions of the Hessian align with the important weights storing the knowledge of the previous tasks, which suppresses the update scale to reduce forgetting, while the small curvature directions facilitate updating the model to fit the current task.\nRegularization-based CL in a Unified Framework. Given the critical role of the Hessian in model updates, various regularization-based methods primarily focus on enhancing their approximation of the Hessian. By integrating Proposition 1, we encapsulate existing regularization-based CL methods within a unified iterative update framework as summarized them in Table 1. Concretely, EWC (Kirkpatrick et al., 2017) and On-EWC (Husza\u0301r, 2017) utilize the diagonal Fisher information matrix FiD to approximate the Hi in Eqn. (2). In contrast, Intelligent Synapses (IS) (Zenke et al., 2017) employs a diagonal matrix HiD, with elements signifying the importance of a task throughout the entire training trajectory. To enhance the approximation of the Hessian, Kronecker factored Laplace approximation (KFLA) (Ritter et al., 2018) opts to use the Kronecker factorization approximation Fi, taking into account off-diagonal elements, to approximate Hi."
        },
        {
            "heading": "3 REVISITING META-CL FROM A HESSIAN APPROXIMATION PERSPECTIVE",
            "text": "In this section, we revisit the Meta-CL from a Hessian approximation perspective. Specifically, while regularization-based methods explicitly approximate the second-order Hessian, Meta-CL utilizes second-order information through the computation of hypergradient. Consequently, it can also be scrutinized within the iterative update framework presented in Eqn. (2).\nMeta-Continual Learning (Meta-CL). As one of the earliest works of Meta-CL, Riemer et al. (2019) propose a meta-learning formulation for minimizing the empirical risk of all seen tasks T [1:j]. In this approach, the data in T [1:j\u22121] are partially accessible via a memory buffer M, which stores\ndata from previous tasks. Concretely, the optimization problem is formulated as,\nmin\u03b8 L[1:j](\u03b8(K)) s.t. \u03b8(K) = UK(\u03b8; T j),\nwhere UK(\u03b8; T j)= K inner steps\ufe37 \ufe38\ufe38 \ufe37 U \u25e6\u00b7 \u00b7 \u00b7\u25e6U \u25e6U(\u03b8(0); T j(0)), U(\u03b8(k); T j (k)) = [\u03b8 \u2212 \u03b2\u2207\u03b8L j (k)(\u03b8)] \u2223\u2223 \u03b8=\u03b8(k\u22121) , (3)\nthe L[1:j](\u03b8(K)) characterizes the empirical risk of \u03b8(K) on all j tasks T [1:j] which in practice is estimated on T j and the data in memory buffer M, \u03b8(k) denotes the parameters of the k-th inner step of learning for task T j , and U(\u00b7) means one-step SGD update in the inner loop optimization. We illustrate the iterative update process for the model parameters \u03b8 for the b-th iteration, denoted by \u03b8b, in Fig. 1. In the inner loop, the model is initialized with \u03b8b(0) = \u03b8b and iteratively updated via K steps of SGD updates, where T j(k) is the data randomly sampled from T\nj in the k-th step. Following the acquisition of \u03b8b(K), in the outer loop, we randomly sample data \u03f5b from T j and the memory buffer M. Then we compute the loss L[1:j](\u03b8b(K)) and obtain the hypergradient w.r.t. \u03b8b as g \u03f5b \u03b8b\n:= \u2202L(\u03b8b(K); \u03f5b)/\u2202\u03b8b. We finally update \u03b8b using the hypergradient. It should be noted that different Meta-CL methods typically employ different approximations of the update gradient g\u03f5b\u03b8b . For instance, MER (Riemer et al., 2019) utilizes a first-order approximation, while La-MAML (Gupta et al., 2020), following MAML (Finn et al., 2017), directly computes the hypergradient.\nRevisiting from the Hessian approximation perspective. To solve the bi-level optimization problem as shown in Eqn. (3), we need to compute the hypergradient of L[1:j](\u03b8(K)) w.r.t. \u03b8, i.e.,\n\u2202L[1:j](\u03b8(K)) \u2202\u03b8 = \u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)\n\u2202\u03b8(K)\n\u03b8 ,\nwhere the Taylor approximation of the first term around \u03b8 is,\n\u2202L[1:j](\u03b8(K)) \u2202\u03b8(K) = \u2207\u03b8(K)L [1:j](\u03b8(K)) \u2248 \u2207\u03b8(K)L [1:j](\u03b8)+HjM (\u03b8(K)\u2212\u03b8) + (\u03b8(K)\u2212\u03b8) T \u2297T\u2297(\u03b8(K)\u2212\u03b8).\nNote that HjM = \u2207 2 \u03b8(K) L[1:j](\u03b8) and T denote the Hessian matrix and the third-order symmetric tensor, respectively, and \u2297 represents the Kronecker product. Through the above approximation and assuming a single inner step in optimizing T j (i.e., K=1) for simplicity, we have Proposition 2. Proposition 2. For Meta-CL with single inner step adaption, suppose that \u03b8(K) is located in the \u03f5-neighborhood N (\u03b8\u2217, \u03f5) of the optimal model parameter \u03b8\u2217=argmin\u03b8 L[1:j](\u03b8(K)), L is \u00b5-smooth, and \u03b2 < \u221a \u03b4/|\u2207\u03b8Lj(\u03b8)\u2212 (\u2207\u03b8Lj(\u03b8))2| where \u03b4 is a small number. Let \u03b1 = \u03b22, then the iterative update rule approximately is, \u03b8 :\u2248 \u03b8 \u2212 \u03b1(HjM ) \u22121\u2207\u03b8Lj(\u03b8).\nWe defer the proof and analysis with K-step (K > 1) adaption to Appendix A.2. Proposition 2 demonstrates that the use of hypergradient by Meta-CL is equivalent to implicitly employing the Hessian (HjM ) and adheres to the unified iterative update rule of regularization-based techniques. By comparing Proposition 2 and Proposition 1, we can deduce that Meta-CL utilizes the samples drawn from M to implicitly compute Hessian HjM so as to approximate (H\n1+H2 + ... +Hj\u22121) in Eqn (2). In contrast to regularized methods that estimate and fix each individual Hi, Meta-CL implicitly computes one Hessian HjM enabling it to have the up-to-date second-order information.\nDespite the advantages of Meta-CL in utilizing up-to-date Hessian information via M, it can also introduce erroneous information. For example, the samples drawn from M may not adequately\nrepresent previous tasks or may lack instances from specific tasks. This inadequacy may lead to the estimated Hessian mistakenly having a small curvature direction along important weights of these previous tasks, which in turn fails to suppress updates along this direction. Consequently, this could result in the loss of previous knowledge from those tasks and lead to forgetting."
        },
        {
            "heading": "4 VARIANCE REDUCTION ON META-CL",
            "text": "To tackle the issue of high variance in Meta-CL caused by the random sampling strategy in the memory buffer, we propose Variance-Reduced Meta-CL (VR-MCL) that effectively reduces the variance in the hypergradient of Meta-CL. Our theoretical analysis demonstrates that this variance reduction in the hypergradient is equivalent to the addition of regularization to the implicitly estimated Hessian matrix, resulting in a decrease in the variance of the Hessian matrix and an improvement in its estimation accuracy. Moreover, we also provide a theoretical guarantee of the regret bound."
        },
        {
            "heading": "4.1 VARIANCE-REDUCED META-CL (VR-MCL)",
            "text": "VR-MCL aims to reduce the variance of its implicitly estimated Hessian by diminishing the variance of its hypergradient. While there are various variance reduction approaches, such as SAG (Gower et al., 2020), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) for convex optimization as well as the non-convex variants (Allen-Zhu & Hazan, 2016; Nguyen et al., 2017; Fang et al., 2018), directly applying these methods to online continual learning is challenging due to the requirement of computing the full-batch gradient for all samples in a task. However, in the online continual learning setting, samples from a task are received in mini-batches, and the full sample set remains inaccessible, which poses the challenge to the implementation of these techniques.\nOn this account, we propose a momentum-based variance-reduced Meta-Continual Learning (MetaCL) method, inspired by previous works Cutkosky & Orabona (2019); Khanduri et al. (2021). Instead of computing the full-batch gradient which is impractical for online CL, our method, VR-MCL, retains the parameters from the previous iterative step. The b-th iteration of VR-MCL is illustrated in Fig. 2, where g\u0302\u03f5b\u22121\u03b8b\u22121 represents the momentum component and g\u03f5b\u03b8b\u22121 and g \u03f5b \u03b8b\ndenote the hypergradients on previous \u03b8b\u22121 and \u03b8b, respectively, computed using data \u03f5b. In VR-MCL, the final update gradient with reduced variance is shown in Eqn. (4), where r represents the momentum ratio. For clarity, we present the pseudo-codes of the algorithm in Appendix E.\ng\u0302\u03f5b\u03b8b = g \u03f5b \u03b8b + r(g\u0302 \u03f5b\u22121 \u03b8b\u22121 \u2212 g\u03f5b\u03b8b\u22121). (4) To comprehend why Eqn. (4) delivers a smaller gradient variance, we initially examine \u2206b := g\u0302\u03f5b\u03b8b\u2212 G\u03b8b . This term measures the error incurred when we use g\u0302 \u03f5b \u03b8b\nas the gradient instead of the true, but unknown full-batch gradient direction G\u03b8b . Through demonstrating that the term E[\u2225\u2206b\u22252] decreases over time, we can show the effectiveness of the VR-MCL in variance reduction Cutkosky & Orabona (2019). Substituting Eqn. (4) into \u2206b, we can obtain the following expression,\n\u2206b = r\u2206b\u22121 + (1\u2212 r)(g\u03f5b\u03b8b \u2212G\u03b8b) + r(g \u03f5b \u03b8b \u2212 g\u03f5b\u03b8b\u22121 \u2212 (G\u03b8b \u2212G\u03b8b\u22121)). (5) The second term (1\u2212r)(g\u03f5b\u03b8b \u2212G\u03b8b) can be modulated by adjusting the ratio of r, and the third term g\u03f5b\u03b8b\u2212g \u03f5b \u03b8b\u22121 \u2212 (G\u03b8b\u2212G\u03b8b\u22121) is expected in the order of O(\u2225\u03b8b \u2212 \u03b8b\u22121\u2225) = O(\u03b1\u2225g\u0302 \u03f5b\u22121 \u03b8b\u22121\n\u2225) given the \u00b5smooth loss function L. Thus, we have \u2225\u2206b\u2225 = r\u2225\u2206b\u22121\u2225+S where S is a number that influenced by r and the learning rate \u03b1. As \u2225\u2206b\u2225 keeps decreasing until it reaches S/(1\u2212r), choosing appropriate values of r and \u03b1 that render S/(1\u2212r) as small as possible ensures variance reduction as we expect. Remark. As previously analyzed, most variance reduction methods, such as SAG (Gower et al., 2020), SVRG (Johnson & Zhang, 2013) and their nonconvex version, are not applicable in online CL settings. Therefore, we design the momentum-based variance reduction technique specifically for Meta-CL. We also discuss other potentially viable variance reduction methods in Sec. 5."
        },
        {
            "heading": "4.2 THEORETICAL ANALYSIS OF VR-MCL.",
            "text": "In this subsection, we will demonstrate that the reduction of hypergradient variance by VR-MCL is equivalent to imposing a penalty on its implicitly estimated Hessian. As a result, it effectively mitigates the adverse effects of significant Hessian variation, as detailed in Proposition 3. Additionally, we also provide the regret bound of VR-MCL in Theorem 1 to further explain its effectiveness.\nProposition 3. Assume that the batch size for inner step adaptation is sufficiently large and the b-th step trains on T j . Let HjMb =\u2207 2 \u03b8b(K)\nL(\u03b8b; \u03f5b)= [h1b ,h2b ,\u00b7 \u00b7 \u00b7,hDb ] denote the Hessian at \u03b8b calculated on \u03f5b, where hdb is the d-th column vector of H j Mb . Similarly, H\u0302jMb\u22121 = [h\u0302 1 b\u22121,\u00b7 \u00b7 \u00b7, h\u0302Db\u22121] denotes the\nHessian for the momentum term at the (b\u22121)-th iteration. If (\u2211Dd=1 \u03bbdhdb)(\u2211Dd=1 \u03bbdh\u0302db\u22121)\u2265 0 holds for any \u03bbd \u0338=0, d={1, 2,\u00b7 \u00b7 \u00b7, D}, then we have the following iterative update rule for VR-MCL,\n\u03b8 :\u2248 \u03b8 \u2212 \u03b1g\u0302\u03f5b\u03b8b = \u03b8 \u2212 \u03b1(H j VR) \u22121\u2207\u03b8Lj(\u03b8),\nwhere (HjVR) \u22121=(HjMb) \u22121+r((H\u0302jMb\u22121) \u22121\u2212(HjMb\u22121) \u22121) with HjMb\u22121 denoting the Hessian at \u03b8b\u22121 calculated on \u03f5b. The eigenvalues of (HjVR) \u22121 are approximately given by v\u22121b + r((v\u0302b\u22121) \u22121\u2212 (vb\u22121)\u22121), where vb, v\u0302b\u22121, and vb\u22121 denote the eigenvalues of HjMb , H\u0302 j Mb\u22121 , and HjMb\u22121 , respectively.\nPlease kindly refer to Appendix A.3 for the detailed proof. Proposition 3 develops the relationship between the Hessian HjVR used in the update rule of VR-MCL and that used in the update rule of Meta-CL (i.e., HjMb ). Similar to the analysis of the effect of Hessian in Secs. 2 and 3, we reach the following conclusions.\n\u25e6 For those wrongly estimated low-curvature directions by Meta-CL, i.e., v\u0302b\u22121\u226b vb, vb\u22121 (some potential crucial weights are erroneously viewed as unimportant), we have v\u22121b + r((v\u0302b\u22121)\n\u22121\u2212 (vb\u22121) \u22121)\u2248r(v\u0302b\u22121)\u22121+(1\u2212r)v\u22121b \u2264v \u22121 b . This signifies that VR-MCL can prevent excessive updates triggered by the wrongly estimated low-curvature direction of the Hessian, thereby retaining the critical weights for previous tasks and improving the accuracy of its iterative updating formula. \u25e6 For the high-curvature directions with large eigenvalues, i.e., v\u0302b\u22121 \u2248 vb \u2248 vb\u22121, we have v\u22121b + r((v\u0302b\u22121)\n\u22121\u2212 (vb\u22121)\u22121) \u2248 v\u22121b . This suggests that VR-MCL can effectively ensure cautious updates along these high-curvature directions of Meta-CL.\nTo sum up, Meta-CL is susceptible to inaccurate estimations of HjMb due to the random samples drawn from M. In contrast, the proposed VR-MCL incorporates a regularization term on HjMb , which effectively limits the model from making large updates and improves the precision of the iterative update. As a result, VR-MCL can effectively preserve the important parameters for previous tasks, thereby maintaining their performance.\nTheorem 1 (Regret Bound of VR-MCL). The regret in online continual learning follows CRj = F\u0303 (\u03b8) \u2212 F (\u03b8\u2217), where F\u0303 (\u03b8) = \u2211j i=1 Li(\u03b8\u0302j), F (\u03b8\u2217) = min\u03b8 \u2211j i=1 Li(\u03b8). Assuming F with \u03c6Lipschitz hessian is \u00b5-strongly convex, G-Lipschitz, \u03b7-smooth and grounded on the four assumptions in Appendix A.4, \u03c3 and M are two large constants in Assumption 3 and 4 respectively, and T denotes the training iteration. Then with probability at least 1\u2212 \u03b4 for any \u03b4 \u2208 (0, 1), we can get,\nCRj \u2264 (log T + 1)(F (\u03b81)\u2212 F (\u03b8\u2217)) + LD2(log T + 1)2\n2\n+ LD2(log T + 1)2 2 + ( 16LD2 + 16\u03c3D + 4M )\u221a 2T log(8T/\u03b4) = O\u0303( \u221a T ).\nTheorem 1 illustrates that, under mild assumptions in a stochastic setting, VR-MCL attains a nearly optimal O\u0303( \u221a T ) regret bound w.h.p. for the online optimization problem. This theoretically demonstrates the effectiveness of the proposed VR-MCL algorithm. Detailed proof see Appendix A.4."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Datasets and Settings. To verify the effectiveness of the proposed VR-MCL, we conduct comprehensive experiments on commonly used datasets Seq-CIFAR10, as well as the longer task sequences\nSeq-CIFAR100 and Seq-TinyImageNet (Buzzega et al., 2020). Specifically, the Seq-CIFAR10 dataset comprises 5 tasks, with each task containing 2 classes. In contrast, Seq-CIFAR100 consists of 10 tasks, each with 10 classes, while Seq-TinyImageNet includes 20 tasks, each encompassing 10 classes. In this paper, we focus on the following settings of continual learning:\n\u2022 Online CL: the data is fed to the model in small batches and trained in a single pass. \u2022 Class Incremental: the model cannot get the oracle task index during testing.\nOur evaluation includes the metrics and experimental settings following the previous works on online CL with a single-head (Caccia et al., 2021; Ji et al., 2020; Shim et al., 2021). We choose the final Averaged accuracy (Acc) across all tasks after sequential training on each task as the main metric for comparing approaches. Moreover, under online CL, we use the Averaged Anytime Accuracy (AAA) Caccia et al. (2021) to evaluate the model through the stream tasks. Let AAj denote the test average accuracy after training on Tj , then the evaluation metrics of AAA and Acc are: AAA = 1N \u2211N j=1(AAj), Acc = AAN .\nBaselines and Training Details. To more effectively validate the efficacy of VR-MCL, which improves the Hessian component and thus better utilizes second-order information, we select key regularization-based CL methods (Husza\u0301r, 2017; Zenke et al., 2017) and Meta-CL methods (Riemer et al., 2019; Gupta et al., 2020). We also opt for other representative SOTA CL methods (Chaudhry et al., 2018; Lopez-Paz & Ranzato, 2017; Rolnick et al., 2019; Buzzega et al., 2020; Arani et al., 2021; Guo et al., 2022; Chrysakis & Moens, 2023) as baselines to further highlight the superior performance of VR-MCL. For detailed information on each baseline method, please refer to Appendix C. For fair comparison among different CL methods, we train all the models using the Stochastic Gradient Descent (SGD) optimizer under the online continual learning. Following (Chrysakis & Moens, 2023; Lopez-Paz & Ranzato, 2017) and (Gupta et al., 2020) we utilize two backbones: reduced ResNet-18 and a smaller network named PcCNN which has only 3 convlayers. For the reduced ResNet18, we set both the batch size and the replay batch size (i.e., the batch size sampled from the memory buffer M) as 32. For the smaller network PcCNN, we set both the batch size and the replay bath size as 10. The momentum ratio r and the learning rate \u03b1 of VR-MCL are both set as 0.25 for all experiments. For other training details please check Appendix C.\nTo assess the effectiveness of VR-MCL, we perform a comprehensive set of experiments and conduct ablation studies to address the following six key questions:\nQuestion 1: How does VR-MCL perform on online CL benchmarks? We report results with a 95% confidence interval on Seq-CIFAR10 and the longer task sequences Seq-CIFAR100, SeqTinyImageNet in Table 2. The methods in the bottom row are the methods utilizing second-order information while the top row are the methods of other representative CL methods. It is evident that the proposed VR-MCL substantially enhances the performance of CL methods employing second-\norder information, and also outperforms other representative SOTA CL methods by nearly 2%. It is noteworthy that the performance of VR-MCL is superior to both MER (Riemer et al., 2019) and La-MAML (Gupta et al., 2020), and significantly better than IS (Zenke et al., 2017) and OnEWC (Husza\u0301r, 2017). This observation aligns with our analysis in Table 1.\nQuestion 2: How does the performance gain from VR-MCL vary with buffer size |M|? In large-scale CL, it is impractical to store a large number of examples from previous tasks due to storage constraints. Therefore, an ideal CL method should still perform well even with a modest memory buffer. To investigate if VR-MCL is still effective with a smaller buffer size, we conduct experiments on Seq-CIFAR100 with different |M| as shown in Table 3. The results show that the performance of most methods improves with the increase of memory buffer size |M|. And our VR-MCL consistently outperforms other baselines, suggesting that VR-MCL is effective even with modest buffer sizes. Note that GEM (Lopez-Paz & Ranzato, 2017) and A-GEM (Chaudhry et al., 2018) are insensitive to the buffer size complying with the results in Shim et al. (2021).\nQuestion 3: How effective is VR-MCL in dealing with increasingly non-stationary Settings (i.e., imbalanced CL)? In the imbalanced CL setting, where the number of samples varies across tasks, an imbalance issue arises in the samples stored in M. This introduces a high variance during the calculation process of the hyper-gradient, thereby negatively affecting the impact on the implicit Hessian. To assess the effectiveness of VR-MCL in addressing these challenges, we choose different imbalanced settings and explore the performance of various methods under this setup in Table 4. Specifically, we choose three imbalanced settings. The Normal means the total number of samples per streaming task from high to low, and the Reversed is the opposite. The Random setting, on the other hand, represents a situation where there is no specific pattern in the number of samples per streaming task. From Table 4, we can see that the performance of most models degrades in this challenging setup. However, VR-MCL significantly outperforms other baselines and even surpasses methods such as CBRS (Chrysakis & Moens, 2020) and Coresets (Borsos et al., 2020), which are specifically designed for imbalanced CL. This suggests that the VR-MCL has excellent performance under the challenged imbalanced CL setting. The 95% CI for Table 4, as well as additional experimental results for different datasets and various imbalance ratios, can be found in Appendix D.4.\nQuestion 4: Whether the proposed method VR-MCL is still effective using different backbones? To verify whether the improvement of VR-MCL is related to the backbone network, we change the reduced ResNet-18 as a shallow network PcCNN following Gupta et al. (2020). The outcomes are provided in Table 5. It can be observed that VR-MCL shows a nearly 3% improvement over the other best method in terms of both the AAA and Acc metrics. This indicates that the proposed VR-MCL remains effective across different backbone architectures.\nQuestion 5: How does VR-MCL compare to other variance reduction methods? As discussed in Sec. 4.1, most existing popular variance reduction methods like SAG (Gower et al., 2020), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) cannot be directly applied since they need to compute the full-batch gradient which is not accessible in online CL. Therefore, we\ncompare VR-MCL with other techniques that promise to reduce the variance: 1) VR-MCL1, which simply increases the replay batch size; 2) VR-MCL2, which incorporates SGD with the naive momentum term. The outcomes shown in Table 6 indicate that: 1) Other variance reduction techniques are also effective, supporting our analysis in Sec. 4; 2) Our proposed momentum-based VR-MCL significantly outperforms other variance techniques, further demonstrating its effectiveness.\nQuestion 6: Whether the proposed method VRMCL can effectively reduce the variance of gradients? To explore this problem, following (Yang & Kwok, 2022), we choose the relative variance metric (i.e., E\u2225g\u03b8t\u2212E(g\u03b8t )\u2225 2\n\u2225E(g\u03b8t )\u22252 ), which can eliminate the influ-\nence of the gradient value on the variance. The outcomes are shown in Fig. 3, where the Meta-CL refers to La-MAML (Gupta et al., 2020). It can be seen that the proposed VR-MCL indeed has less variance during training, complying with the analysis in Eqn. (5).\nOther Results. Due to space constraints, additional comprehensive experiments have been provided in Appendix D, including other evaluation metrics (i.e., forgetting measure), hyperparameter selection (i.e., the selection of the momentum ratio r and learning rate \u03b1), and training time analysis."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we revisited Meta-CL and first bridged it with regularization-based methods from the Hessian matrix approximation perspective. Specifically, Meta-CL, through the use of hypergradient, implicitly approximates the Hessian in an online manner. While this approach benefits from timely adaptation, it also grapples with high variance resulting from random memory buffer sampling. Building on this understanding, we proposed the VR-MCL method, which effectively reduces the variance of the hypergradient and exhibits superior performance under the online continual learning setting. We provide theoretical proof that VR-MCL imposes a regularization term on the implicitly estimated Hessian matrix. This prevents updates from moving excessively in the wrongly estimated low-curvature directions and thus has a more accurate iterative update rule shown in Table 1. Moreover, to enhance comprehension, we also provide the regret bound of our VR-MCL."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "We thank all the anonymous reviewers for their constructive suggestions on improving this paper. This research was sponsored by the National Key R&D Program of China (2020YFA0713900), the China NSFC projects under contract 62272375, 12226004 and 62306233, the Young Elite Scientists Sponsorship Program by CAST 2023QNRC001."
        },
        {
            "heading": "A DETAILED PROOF",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF PROPOSITION 1",
            "text": "Proposition 1. In regularization-based continual learning, if the model parameter \u03b8 is searched within the neighborhood set \u222aj\u22121i=1N i with N i={\u03b8 :d(\u03b8, \u03b8\u0302i)<\u03b4i}, then the iterative update rule of \u03b8 approximately is\n\u03b8 :\u2248 \u03b8 \u2212 \u03b1(H1 +H2 + \u00b7 \u00b7 \u00b7+Hj\u22121)\u22121\u2207\u03b8Lj(\u03b8)\nProof. The empirical loss L on T [1:j] can be approximate as,\nL = j\u22121\u2211 i=1 Li(\u03b8) + Lj(\u03b8) \u2248 Lproxj\u22121 (\u03b8) + Lj(\u03b8)\n(1) = j\u22121\u2211 i=1 Li(\u03b8\u0302i)\ufe38 \ufe37\ufe37 \ufe38 (a) +(\u03b8 \u2212 \u03b8\u0302i)\u22a4\u2207\u03b8Li(\u03b8\u0302i)\ufe38 \ufe37\ufe37 \ufe38 (b) + 1 2 (\u03b8 \u2212 \u03b8\u0302i)\u22a4Hi(\u03b8 \u2212 \u03b8\u0302i)\ufe38 \ufe37\ufe37 \ufe38\n(c)\n+Lj(\u03b8)\n(2) \u2248 j\u22121\u2211 i=1 1 2 (\u03b8 \u2212 \u03b8\u0302i)\u22a4Hi(\u03b8 \u2212 \u03b8\u0302i) + Lj(\u03b8)\nHere, (1) is expand each loss Li using Taylor series at point \u03b8\u0302i to compute Lproxj\u22121 (\u03b8), where i = 1, ..., j\u22121. Step (2) is due to the term (a) is not related to \u03b8 and thus can be discarded. Furthermore, since the training process at the end of each task (i.e., \u03b8\u0302i) is usually converged, the gradient \u2207\u03b8Li(\u03b8\u0302i) is near zero and the term (b) can be ignored in practice. Then we can derive \u2202L\u2202\u03b8 as,\n\u2202L \u2202\u03b8 = j\u22121\u2211 i=1 Hi(\u03b8 \u2212 \u03b8\u0302i) +\u2207\u03b8Lj(\u03b8).\nIf the parameters \u03b8 are searched in the neighborhood set \u222aj\u22121i=1N i, where N i = {\u03b8 : d(\u03b8, \u03b8\u0302i)< \u03b4i}, then we can make the approximation \u2211j\u22121 i=1 H i(\u03b8 \u2212 \u03b8\u0302i) \u223c ( \u2211j\u22121 i=1 H i)(\u03b8 \u2212 \u03b8\u0302j\u22121) Husza\u0301r (2017). Let \u2202L\u2202\u03b8 = 0, we can obtain the iterative update formula of T j as:\n\u03b8 :\u2248 \u03b8 \u2212 \u03b1(H1 +H2 + \u00b7 \u00b7 \u00b7+Hj\u22121)\u22121\u2207\u03b8Lj(\u03b8),\nwhere \u03b1 = 1 to maintain formal consistency with other iterative rules in Table 1, (H1 +H2 + \u00b7 \u00b7 \u00b7+ Hj\u22121) are the Hessian matrices of previous tasks computed at the end of training of each T i, and \u2207\u03b8Lj(\u03b8) is the gradient of the j-th task."
        },
        {
            "heading": "A.2 PROOF OF PROPOSITION 2",
            "text": "Proposition 2. For MCL with single inner step adaption, suppose that \u03b8(K) is located in the \u03f5neighborhood N (\u03b8\u2217, \u03f5) of the optimal model parameter \u03b8\u2217 = argmin\u03b8 L[1:j](\u03b8(K)), L is \u00b5-smooth, and \u03b2 < \u221a \u03b4/|\u2207\u03b8Lj(\u03b8)\u2212 (\u2207\u03b8Lj(\u03b8))2| where \u03b4 is a small number. Thus, the iterative update rule approximately is \u03b8 :\u2248 \u03b8 \u2212 \u03b1(HjM ) \u22121\u2207\u03b8Lj(\u03b8),\nProof. (Single inner step adaption) For simplicity, we analyze the MCL with single inner step adaption (i.e., K = 1) at first. The objective function of one step inner-loop MCL could be formulated as,\n\u03b8\u2217 = argmin\u03b8L[1:j](\u03b8(K)), s.t. \u03b8(K) = \u03b8\u2212\u03b2\u2207\u03b8Lj(\u03b8).\nThen the derivative of loss L[1:j](\u03b8(K)) over \u03b8 is,\n\u2202L[1:j](\u03b8(K)) \u2202\u03b8 = \u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)\n\u2202\u03b8(K)\n\u03b8 . (1)\nFor the first term \u2202L [1:j](\u03b8(K))\n\u2202\u03b8(K) , we take the Taylor expansion at \u03b8,\n\u2202L[1:j](\u03b8(K)) \u2202\u03b8(K) = \u2207\u03b8(K)L [1:j](\u03b8(K)) \u2248 \u2207\u03b8(K)L [1:j](\u03b8)+HjM (\u03b8(K)\u2212\u03b8) + (\u03b8(K)\u2212\u03b8) T \u2297T\u2297(\u03b8(K)\u2212\u03b8),\n(2)\nwhere HjM =\u2207 2 \u03b8(K) L[1:j](\u03b8) and T denote the Hessian matrix and the third-order symmetric tensor, respectively, and \u2297 represents the Kronecker product.\nSince we hope to find the optimal parameters \u03b8\u2217 so as to achieve the lowest L[1:j], which means L[1:j](\u03b8(K))\n\u2202\u03b8 = 0. According to Eqn. (1), that is approximately to let \u2202L[1:j](\u03b8(K)) \u2202\u03b8(K) = 0.\nSuppose \u03b4 is a small number, \u03b2 < \u221a\n\u03b4/|\u2207\u03b8Lj(\u03b8)\u2212 (\u2207\u03b8Lj(\u03b8))2|, and there exists an \u03f5-neighborhood N (\u00b7) such that parameters \u03b8\u2217 \u2208 N (\u03b8\u2212\u03b2\u2207\u03b8Lj(\u03b8), \u03f5). Since L is \u00b5-smooth, which means \u2225\u2207L(\u03b8)\u2212 \u2207L(\u03b8\u2032)\u22252 \u2264 \u00b5\u2225\u03b8 \u2212 \u03b8 \u2032\u22252. Then we can get the following equations according to Eqn. (2),\n\u2202L[1:j](\u03b8\u2217) \u2202\u03b8\u2217 = \u2207\u03b8\u2217L[1:j](\u03b8\u2217)\n\u2248 \u2207\u03b8\u2217L[1:j](\u03b8) +\u22072\u03b8\u2217L[1:j](\u03b8)(\u03b8\u2217 \u2212 \u03b8) + (\u03b8\u2217\u2212\u03b8)T \u2297 T\u2297(\u03b8\u2217\u2212\u03b8) \u2248 \u2207\u03b8(K)L [1:j](\u03b8) +\u22072\u03b8(K)L [1:j](\u03b8)(\u03b8\u2217 \u2212 \u03b8) + (\u03b8(K)\u2212\u03b8)T \u2297 T\u2297(\u03b8(K)\u2212\u03b8) + o(\u03f5)\n= \u2207\u03b8(K)L [1:j](\u03b8) +\u22072\u03b8(K)L [1:j](\u03b8)(\u03b8\u2217 \u2212 \u03b8) + C \u2299 (\u03b8(K)\u2212\u03b8)T (\u03b8(K)\u2212\u03b8) + o(\u03f5)\n\u2248 o(\u00b5\u03f5) +\u22072\u03b8(K)L [1:j](\u03b8)(\u03b8\u2217 \u2212 \u03b8) + \u03b22(\u2207\u03b8Lj(\u03b8))2 + o(\u03f5)\n\u2248 o(\u00b5\u03f5) +\u22072\u03b8(K)L [1:j](\u03b8)(\u03b8\u2217 \u2212 \u03b8) + \u03b22\u2207\u03b8Lj(\u03b8) + o(\u03b4) + o(\u03f5)\n\u2248 \u22072\u03b8(K)L [1:j](\u03b8)(\u03b8\u2217 \u2212 \u03b8) + \u03b22\u2207\u03b8Lj(\u03b8) + o(\u03b4) + o(\u03f5) + o(\u00b5\u03f5) = 0,\n(3)\nwhere C means a constant vector, \u2299 denotes the element-wise multiplication operator. Then it is easy to get \u03b8 := \u03b8 \u2212 \u03b22(\u22072\u03b8(K)L [1:j](\u03b8))\u22121\u2207\u03b8Lj(\u03b8) = \u03b8 \u2212 \u03b1(HjM ) \u22121\u2207\u03b8Lj(\u03b8), where \u03b1 = \u03b22 and HjM = \u2207 2 \u03b8(K) L[1:j](\u03b8) .\n(K inner steps adaption) For MCL with K steps of inner-loop updating, \u03b8(K) = \u03b8 \u2212 \u03b2\u2207Lj(1)(\u03b8)\u2212 \u03b2\u2207Lj(2)(\u03b8(1))\u2212 ...\u2212 \u03b2\u2207L j (K)(\u03b8(K\u22121)). Similar to Eqn. (3), we can get\n\u03b8 := \u03b8 \u2212 \u03b1(HjM ) \u22121( K\u22121\u2211 i=0 \u2207Lj(i+1)(\u03b8(K\u22121))),\nwhere \u03b8(i) denotes the model parameters after the i-th inner loop adaptation and \u03b8(0) = \u03b8. It is worth noting that in MCL with multiple updating steps, the hypergradient incorporates more gradients from the inner loop, which is the main difference from the one-step inner loop MCL. However, this difference does not affect the variance reduction analysis presented in Proposition 3."
        },
        {
            "heading": "A.3 PROOF OF PROPOSITION 3",
            "text": ""
        },
        {
            "heading": "A.3.1 PROOF OF LEMMA 1",
            "text": "Lemma 1. (The linear combination of two invertible matrices is invertible under finite conditions.) Let A = [a1, a2, ..., an] \u2208 Rn\u00d7n and B = [b1, b2, ..., bn] \u2208 Rn\u00d7n denote two square matrices, where ai \u2208 Rn, bi \u2208 Rn are the i-th column vector of the matrix A and B. Suppose: 1) A and B are invertible ; 2) for any \u03bbi \u0338= 0, i = {1, 2, ..n}, ( \u2211n i=1 \u03bbiai)( \u2211n i=1 \u03bbibi) \u2265 0, then the linear combination of A and B (i.e., rA+ (1\u2212 r)B) is invertible, r \u2208 (0, 1).\nProof. (By contradiction.)\n(Step-1) We assume rA+(1\u2212r)B, where r \u2208 (0, 1), is not invertible, which means the square matrix rA+ (1\u2212 r)B = [ra1 + (1\u2212 r)b1, ..., ran + (1\u2212 r)bn] is singular.\n(Step-2) Then there must exists \u03bbi \u0338= 0, i = {1, 2, ..n}, such that \u2211n\ni=1 \u03bbi(rai+(1\u2212r)bi) = 0. That is equivalent to r \u2211n i=1 \u03bbiai+(1\u2212 r) \u2211n i=1 \u03bbibi = 0. According to condition 2, it can deduce\nthat \u2211n i=1 \u03bbiai = \u2211n i=1 \u03bbibi = 0. This is contradict with the condition that A and B are invertible.\n(Step-3) Since assuming rA + (1 \u2212 r)B is not invertible leads to a contradiction, therefore, rA+ (1\u2212 r)B is convertible."
        },
        {
            "heading": "A.3.2 PROOF OF PROPOSITION 3",
            "text": "Proposition 3. Assume that the batch size for inner step adaptation is sufficiently large. Let HjMb = \u22072\u03b8b(K)L [1:j]b(\u03b8b)=[h 1 b ,h 2 b ,\u00b7 \u00b7 \u00b7,hDb ] denote the Hessian at \u03b8b calculated on \u03f5b, where hdb is the d-th column vector of HjMb . Similarly, H\u0302 j Mb\u22121\n=[h\u03021b\u22121,\u00b7 \u00b7 \u00b7, h\u0302Db\u22121] denotes the Hessian for the momentum term at the (b\u22121)-th iteration. If ( \u2211D d=1 \u03bbdh d b)( \u2211D d=1 \u03bbdh\u0302 d b\u22121) \u2265 0 holds for any \u03bbd \u0338= 0, d = {1, 2,\u00b7 \u00b7 \u00b7, D}, then we have the following iterative update rule for VR-MCL,\n\u03b8 :\u2248 \u03b8 \u2212 \u03b1(HjVR) \u22121\u2207\u03b8Lj(\u03b8),\nwhere (HjVR) \u22121=(HjMb) \u22121+r((H\u0302jMb\u22121) \u22121\u2212(HjMb\u22121) \u22121) with H j Mb\u22121 denoting the Hessian at \u03b8b\u22121 calculated on \u03f5b. The eigenvalues of (H j VR) \u22121 are approximately given by v\u22121b +r((v\u0302b\u22121) \u22121\u2212(vb\u22121)\u22121), where vb, v\u0302b\u22121, and vb\u22121 denote the eigenvalues of H j Mb , H\u0302jMb\u22121 , and H j Mb\u22121 , respectively.\nProof. Let g\u03f5b\u03b8b denote the gradient of the b-th outer step. In proposition 2, the updating formula of MCL is \u03b8 := \u03b8 \u2212 \u03b1(HjM )\u22121\u2207\u03b8Lj(\u03b8) which means the gradient g \u03f5b \u03b8b\ncan be rewrite in the form of (HjMb)\n\u22121\u2207\u03b8Lj(\u03b8). When we adopt the variance reduction method, let m denote the momentum part, then we can get,\nm1 = g \u03f51 \u03b81 = (HjM1) \u22121\u2207\u03b8Lj(\u03b81),\nm2 = g \u03f52 \u03b82 + r(m1 \u2212 g\u03f52\u03b81) = (HMj2 ) \u22121\u2207\u03b8Lj(\u03b82) + r((HjM1) \u22121\u2207\u03b8Lj(\u03b8)\u2212 (H j M1) \u22121\u2207\u03b8L j (\u03b81)), \u00b7 \u00b7 \u00b7 mb = g\n\u03f5b \u03b8b + r(mb\u22121 \u2212 g\u03f5b\u03b8b\u22121),\nwhere (H j M1) \u22121 and \u2207\u03b8L j (\u03b8) are the Hessian and corresponding gradient in g\u03f52\u03b81 . When there has a sufficiently large batch size on the current task, then \u2207\u03b8Lj(\u03b82) \u2248 \u2207\u03b8L j (\u03b81) \u2248 \u2207\u03b8Lj(\u03b81). Therefore, we can rewrite m2 as,\nm2 = g \u03f52 \u03b82 + r(m1 \u2212 g\u03f52\u03b81) = {(H j M2 )\u22121 + r((HjM1) \u22121 \u2212 (HM1)\u22121)}\u2207\u03b8Lj(\u03b82).\nSince (HjM1) \u22121 is highly related with (HjM2) \u22121, we can get,\n(HjM2) \u22121 + r((HjM1) \u22121 \u2212 (HjM1) \u22121) \u2248 (1\u2212 r)(HjM2) \u22121 + r(HjM1) \u22121\n\u2022 For m2, H j M1 = [h11, h 2 1, ..., h D 1 ] and H j M2 = [h12, h 2 2, ..., h D 2 ] are also highly related. Based on the assumption that for any \u03bbd \u0338= 0, d = {1, 2, ..D}, ( \u2211D d=1 \u03bbdh d 1)( \u2211D d=1 \u03bbdh d 2) \u2265 0. According to\nLemma 1, we can get r(HjM1) \u22121 + (1 \u2212 r)(HjM2) \u22121 is invertible, and thus there exist a matrix (H\u0302M2) \u22121 = r(HM1) \u22121 + (1\u2212 r)(HM2)\u22121.\n\u2022 For mb, the previous assumption also holds, which is for H j Mb = [h1b , ..., h D b ], H\u0302 j M(b\u22121) =\n[h\u03021b\u22121, ..., h\u0302 D b\u22121] , for any \u03bbd \u0338= 0, d = {1, 2, ..D}, ( \u2211D d=1 \u03bbdh d b)( \u2211D d=1 \u03bbdh\u0302 d b\u22121) \u2265 0. According to this recurrence relation, we can get (H\u0302Mb) \u22121 = r(H\u0302Mb\u22121)\n\u22121+(1\u2212 r)(HMb)\u22121. Let HjVR = H\u0302Mb , we can get the following iterative update rule of VR-MCL,\n\u03b8 :\u2248 \u03b8 \u2212 \u03b1(HjVR) \u22121\u2207\u03b8Lj(\u03b8),"
        },
        {
            "heading": "A.4 REGRET BOUND OF VR-MCL",
            "text": "This section aims to establish the regret bound of VR-MCL. Our method involves three key parts: Firstly, we define the objective form of Regret Bound in continual learning. Secondly, we prove that updating the objective of VR-MCL is equivalent to updating the latent optimal loss function, by bounding the gradient of VR-MCL with the latent optimal gradients. Finally, we derive the regret bound of VR-MCL based on the aforementioned steps.\n(Part I) We first define the Regret Bound in continual learning.\nDefinition 1 (The Regret Bound in CL). Consider an online learning scenario where an agent makes a decision by choosing an action \u03b8t \u2208 Rd at each time step t = 1, ..., T , and experiences a corresponding loss Lt(\u03b8t). The primary objective of the agent is to minimize the difference between its cumulative loss and that of the best action in hindsight, which is commonly known as the regret.\nRT = \u2211T t=1 Lt (\u03b8t)\u2212min\u03b8\u2208\u0398 \u2211T t=1 Lt(\u03b8)\nSimilarly, in MCL, for the current task T j , the goal is to minimize CRj = F\u0303 (\u03b8)\u2212 F (\u03b8\u2217), where\nF\u0303 (\u03b8) := \u2211j i=1 Li(\u03b8\u0302j) and F (\u03b8\u2217) = min\u03b8 \u2211j i=1 Li(\u03b8).\nThis proof is grounded on certain assumptions, which are listed below:\nAssumption 1. The compact convex set C \u2286 Rd has diameter D, i.e., \u2200\u03b8, \u03b8\u2032 \u2208 C, \u2225\u03b8 \u2212 \u03b8\u2032\u2225 \u2264 D.\nAssumption 2. The function F (\u00b7) is G-Lipschitz, \u00b5-strongly convex, \u03c6-smooth (i.e., has \u03c6-Lipschitz gradients) and has \u03ba-Lipschitz hessian. Then, according to the Theorem 1 in Finn et al. (2019), when the step size \u03b1 < { 12\u03c6 , \u00b5 8\u03baG},the gradient \u2207F\u0303 (x, \u03f5t) is L-Lipschitz (L = 9\u03c6 8 ) over the constraint set"
        },
        {
            "heading": "C, that is",
            "text": "\u2225\u2207F\u0303 (\u03b8, \u03f5t)\u2212\u2207F\u0303 (\u03b8 \u2032 , \u03f5t)\u2225 \u2264 L\u2225\u03b8 \u2212 \u03b8 \u2032\u2225 = 9\u03c68 \u2225\u03b8 \u2212 \u03b8 \u2032\u2225,\u2200\u03b8, \u03b8\u2032 \u2208 C.\nAssumption 3. Let F\u0303t denote the t-th training step, then the distance between our computed \u2207F\u0303t(\u03b8, \u03f5t) and the gradient derived from the optimal function is bounded over the constraint set C, i.e., for any \u03b8 \u2208 C, t \u2208 {1, ..., T}, there exists \u03c32 < \u221e such that with probability 1,\n\u2225\u2207F\u0303t(\u03b8, \u03f5t)\u2212\u2207F (\u03b8)\u22252 \u2264 \u03c32\nAssumption 4. Let Ft(\u03b8) mean the optimal t-th training step, and the difference of Ft(\u03b8) and F (\u03b8) is bounded over the constraint set C, i.e., \u2200\u03b8 \u2208 C, t \u2208 {1, ..., T}, there exists M2 < \u221e such that with probability 1,\n\u2225Ft(\u03b8)\u2212 F (\u03b8)\u22252 \u2264 M2\n(Part II) In the following, we will demonstrate that updating the objective function of VR-MCL can be regarded as updating the latent optimal loss function. This is achieved by bounding the gradient of VR-MCL with the latent optimal gradients. Specifically, the updating gradient can be expressed as mt = \u2207F\u0303 (\u03b8t, \u03f5t) + r(mt\u22121 \u2212 \u2207F\u0303 (\u03b8t\u22121, \u03f5t)). Let \u03b1 denote the small learning rate and define \u2206t = mt \u2212\u2207F (\u03b8t), our aim is to prove that \u2206t is bounded. Here, we suppose the momentum ratio r = (1\u2212 \u03c1t), the learning rate \u03b1 = \u03b7t, and \u03c1t = \u03b7t = 1/(t+ 1)\u03b3 , \u03b3 \u2208 (0, 1].\nLemma 2. If Assumptions 1,2,3 are satisfied, \u2200t \u2265 1, \u03b40 \u2208 (0, 1), we have with probability at least 1\u2212 \u03b40,\n\u2225\u2206t\u2225 \u2264 2 ( 2LD + 3 \u03b3\u03c3 3\u03b3\u22121 ) (t+ 1)\u2212\u03b3/2 \u221a 2 log (4/\u03b40)\nLemma 2 demonstrates that the gradient approximation error \u2225\u2206t\u2225 is bounded with high probability as t increases.\nProof. We derive the formulation of \u2206t at first.\n\u2206t = mt \u2212\u2207F (\u03b8t) = (1\u2212 \u03c1t)\u2206t\u22121+(1\u2212 \u03c1t)(\u2207F\u0303t(\u03b8t, \u03f5t)\u2212\u2207F\u0303t(\u03b8t\u22121, \u03f5t)\u2212(\u2207F (\u03b8t)\u2212\u2207F (\u03b8t\u22121)))\n+ \u03c1t(\u2207F\u0303t(\u03b8t, \u03f5t)\u2212\u2207F (\u03b8t))\n= \u03c4\u220f k=2 (1\u2212\u03c1k)\u22061 + t\u2211 \u03c4=2 t\u220f k=\u03c4 (1\u2212\u03c1k)(\u2207F\u0303\u03c4 (\u03b8\u03c4 , \u03f5\u03c4 )\u2212\u2207F\u0303\u03c4 (\u03b8\u03c4\u22121, \u03f5\u03c4 )\u2212 (\u2207F (\u03b8\u03c4 )\u2212\u2207F (\u03b8\u03c4\u22121)))\n+ t\u2211 \u03c4=2 \u03c1\u03c4 t\u220f k=\u03c4+1 (1\u2212 \u03c1k)(\u2207F\u0303t(\u03b8\u03c4 , \u03f5\u03c4 )\u2212\u2207F (\u03b8\u03c4 ))\nLet \u2206t = \u2211t \u03c4=1 \u03b6t,\u03c4 , where \u03b6t,1 = r t\u22121\u22061 and for \u03c4 > 1, \u03b6t,\u03c4 = \u220ft k=\u03c4 (1 \u2212 \u03c1k)(\u2207F\u0303\u03c4 (\u03b8\u03c4 , \u03f5\u03c4 ) \u2212\n\u2207F\u0303\u03c4 (\u03b8\u03c4\u22121, \u03f5\u03c4 )\u2212 (\u2207F (\u03b8\u03c4 )\u2212\u2207F (\u03b8\u03c4\u22121)))+\u03c1\u03c4 \u220ft\nk=\u03c4+1(1\u2212\u03c1k)(\u2207F\u0303\u03c4 (\u03b8\u03c4 , \u03f5\u03c4 )\u2212\u2207F (\u03b8\u03c4 )). Recall \u22061 = \u2207F\u0303 (\u03b81, \u03f51)\u2212\u2207F (\u03b81). We observe that E [\u03b6t,\u03c4\u2225F\u03c4\u22121] = 0 when F\u03c4\u22121 is the \u03c3-field generate by {L1, \u03f51, ...,L\u03c4\u22121, \u03f5\u03c4\u22121}. Therefore, {\u03b6t,\u03c4}t\u03c4=1 is a martingale difference sequence. Then, we will derive upper bounds of \u2225\u03b6t,\u03c4\u2225. We initially proved the following results,\nt\u220f k=\u03c4 (1\u2212 \u03c1k) = t\u220f k=\u03c4 ( 1\u2212 1 (k + 1)\u03b3 ) = t\u220f k=\u03c4 (k + 1)\u03b3 \u2212 1 (k + 1)\u03b3 \u2264 t\u220f k=\u03c4\nk\u03b3\n(k + 1)\u03b3 =\n\u03c4\u03b3\n(t+ 1)\u03b3 ,\nwhere the inequality holds from the concavity of h(x) = x\u03b3 for any x \u2265 0. Based on this inequality we can bound \u2225\u03b6t,1\u2225 as follows,\n\u2225\u03b6t,1\u2225 \u2264 2\u03b3\n(t+ 1)\u03b3 \u2225\u2225\u2225\u2207F\u03031(\u03b81, \u03f51),\u2207F (\u03b81)\u2225\u2225\u2225 \u2264 2\u03b3\u03c3 (t+ 1)\u03b1 def = ct,1,\nwhere the second inequality holds according to Assumption 3. When \u03c4 \u2265 1, we can get,\n\u2225\u03b6t,\u03c4\u2225 \u2264 t\u220f\nk=\u03c4\n(1\u2212 \u03c1k)( \u2225\u2225\u2225\u2207F\u0303\u03c4 (\u03b8\u03c4 , \u03f5\u03c4 )\u2212\u2207F\u03c4 (\u03b8\u03c4\u22121, \u03f5\u03c4 )\u2225\u2225\u2225+ \u2225\u2207F (\u03b8\u03c4 )\u2212\u2207F (\u03b8\u03c4\u22121)\u2225)\n+ \u03c1\u03c4 t\u220f k=\u03c4+1 (1\u2212 \u03c1k)\u2225\u2207F\u0303\u03c4 (\u03b8\u03c4 , \u03f5\u03c4 )\u2212\u2207F (\u03b8\u03c4 )\u2225\n\u2264 2L\u2225\u03b8\u03c4 \u2212 \u03b8\u03c4\u22121\u2225 t\u220f\nk=\u03c4\n(1\u2212 \u03c1k) + \u03c3\u03c1\u03c4 t\u220f\nk=\u03c4+1\n(1\u2212 \u03c1k)\n\u2264 2LD\u03c1\u03c4\u22121 t\u220f\nk=\u03c4\n(1\u2212 \u03c1k) + \u03c3\u03c1\u03c4 t\u220f\nk=\u03c4+1\n(1\u2212 \u03c1k)\n(4)\nWe can further derive the final term as follows,\n\u03c1\u03c4 t\u220f k=\u03c4+1 (1\u2212 \u03c1k) = \u03c1\u03c4 \u03c1\u03c4\u22121 (1\u2212 \u03c1\u03c4 )\n( \u03c1\u03c4\u22121\nt\u220f k=\u03c4 (1\u2212 \u03c1k)\n) \u2264 1\n1\u2212 \u03c1\u03c4\n( \u03c1\u03c4\u22121\nt\u220f k=\u03c4 (1\u2212 \u03c1k)\n)\n\u2264 1 1\u2212 1/3\u03b3\n( \u03c1\u03c4\u22121\nt\u220f k=\u03c4 (1\u2212 \u03c1k)\n) \u2264 3 \u03b3\n3\u03b3 \u2212 1\n( \u03c1\u03c4\u22121\nt\u220f k=\u03c4 (1\u2212 \u03c1k)\n) .\n(5)\nBy plugging Eqn. (5) into Eqn. (4, we can get, \u2200\u03c4 \u2265 1\n\u2225\u03b6t,\u03c4\u2225 \u2264 (2LD + 3\u03b3\u03c3\n3\u03b3 \u2212 1 )\u03c1\u03c4\u22121 t\u220f k=\u03c4 (1\u2212 \u03c1k) def = ct,\u03c4\nAccording to Theorem 3.5 in Pinelis (1994), we can get \u2200\u03bb \u2265 0,\nP (\u2225\u2206t\u2225 \u2265 \u03bb) \u2264 4 exp ( \u2212 \u03bb 2\n4 \u2211t\n\u03c4=1 c 2 t,\u03c4\n) , (6)\nwhere ct,1 is defined in Eqn. (A.4) and ct,\u03c4 , (\u03c4 \u2265 1) is defined in Eqn. (4). Then we can get,\nt\u2211 \u03c4=1 c2t,\u03c4 = c 2 t,1 + t\u2211 \u03c4=2 c2t,\u03c4 = 22\u03b3\u03c32 (t+ 1)2\u03b3 + ( 2LD + 3\u03b3\u03c3 3\u03b3 \u2212 1 )2 t\u2211 \u03c4=2 ( \u03c1\u03c4\u22121 t\u220f k=\u03c4 (1\u2212 \u03c1k) )2\n\u2264 2 2\u03b3\u03c32\n(t+ 1)2\u03b3 +\n( 2LD + 3\n\u03b3\u03c3 3\u03b1\u22121 )2 (t+ 1)\u03b3 \u2264 ( ( \u221a 2)\u03b3\u03c3 )2 (t+ 1)\u03b3 + ( 2LD + 3 \u03b3\u03c3 3\u03b3\u22121 )2 (t+ 1)\u03b3 \u2264 2 ( 2LD + 3 \u03b3\u03c3 3\u03b3\u22121 )2 (t+ 1)\u03b3 ,\n(7) where the last inequality is because ( \u221a 2)\u03b3 \u2264 3\u03b3/(3\u03b3 \u2212 1) \u2200\u03b3 \u2208 (0, 1]. Substituting Eqn. (7) into Eqn. (6) and setting \u03bb = 2 ( 2LD + 3\n\u03b3\u03c3 3\u03b3\u22121\n) (t+1)\u2212\u03b3/2 \u221a 2 log (4/\u03b40) for some \u03b40 \u2208 (0, 1), we have\nwith probability at least 1\u2212 \u03b40,\n\u2225\u2206t\u2225 \u2264 2 ( 2LD + 3 \u03b3\u03c3 3\u03b3\u22121 ) (t+ 1)\u2212\u03b3/2 \u221a 2 log (4/\u03b40),\nwhich is the expected result.\n(Part III) Next, we will establish the regret bound CRj for continual learning.\nTheorem 1 (Regret Bound of VR-MCL). If F is convex and all aforementioned four Assumptions are satisfied, then with probability at least 1\u2212 \u03b4 for any \u03b4 \u2208 (0, 1),\nCRj \u2264 (log T + 1)(F (\u03b81)\u2212 F (\u03b8\u2217)) + LD2(log T + 1)2\n2\n+ LD2(log T + 1)2 2 + ( 16LD2 + 16\u03c3D + 4M )\u221a 2T log(8T/\u03b4) = O\u0303( \u221a T )\nProof. We initially define a sequence st = F\u0303t(\u03b8t)\u2212Ft(\u03b8\u2217)\u2212(F (\u03b8t)\u2212F (\u03b8\u2217)), t = 1, ..., T . It can be observed that E [st | Ft\u22121] = 0, where Ft\u22121 is the \u03c3-algebra generated by {F1, \u03f51, ..., Ft\u22121, \u03f5t\u22121}. It means that {st}Tt=1 is a martingale difference sequence. According to Assumption 4, we have\n\u2225st\u2225 = \u2225F\u0303t(\u03b8t)\u2212 Ft(\u03b8\u2217)\u2212 (F (\u03b8t)\u2212 F (\u03b8\u2217)\u2225 \u2264 2M\nAccording to Theorem 3.5 in Pinelis (1994), we can get\nP ( \u2225\nT\u2211 t=1 st\u2225 \u2265 \u03bb\n) \u2264 4 exp ( \u2212 \u03bb 2\n16TM2\n) ,\nwhere \u03bb > 0. By setting \u03bb = 4M \u221a T log(8/\u03b4) , we have with probability at least 1\u2212 \u03b4/2,\nT\u2211 t=1 st = T\u2211 t=1 (F\u0303t(\u03b8t)\u2212 Ft(\u03b8\u2217))\u2212 T\u2211 t=1 (F (\u03b8t)\u2212 F (\u03b8\u2217)) \u2264 4M \u221a T log(8/\u03b4).\nBy rearranging the above terms, we get\nCRj = F\u0303 (\u03b8)\u2212 F (\u03b8\u2217) = T\u2211\nt=1\n(F\u0303t(\u03b8t)\u2212 Ft(\u03b8\u2217)) \u2264 T\u2211\nt=1\n(F (\u03b8t)\u2212 F (\u03b8\u2217)) + 4M \u221a T log(8/\u03b4). (8)\nThen according to Lemma 2 in (Mokhtari et al., 2020), we can derive the first term in Eqn. (8) as follows,\nF (\u03b8t)\u2212 F (\u03b8\u2217) = (1\u2212 \u03b7t)(F (\u03b8t\u22121)\u2212 F (\u03b8\u2217)) + \u03b7tD\u2225\u2206t\u22121\u22252 + LD2\u03b72t\n2 \u2264 t\u22121\u220f \u03c4=1 (1\u2212 \u03b7\u03c4 )(F (\u03b81)\u2212 F (\u03b8\u2217)) + t\u22121\u2211 \u03c4=1 \u03b7\u03c4 (D\u2225\u2206\u03c4\u22252 + LD2\u03b7\u03c4 2 ) t\u22121\u220f k=\u03c4+1 (1\u2212 \u03b7k)\n= 1\nt (F (\u03b81)\u2212 F (\u03b8\u2217)) + t\u22121\u2211 \u03c4=1 1 \u03c4 + 1 (D\u2225\u2206\u03c4\u22252 + LD2 2(\u03c4 + 1) ) \u03c4 + 1 t\n= 1\nt (F (\u03b81)\u2212 F (\u03b8\u2217)) +\n1\nt t\u22121\u2211 \u03c4=1 (D\u2225\u2206\u03c4\u22252 + LD2 2(\u03c4 + 1) )\n\u2264 1 t (F (\u03b81)\u2212 F (\u03b8\u2217)) + D t t\u22121\u2211 \u03c4=1 \u2225\u2206\u03c4\u22252 + LD2 log t 2t\n(9) Summing Eqn. (9) from t = 1 to T , we obtain,\nT\u2211 t=1 F (\u03b8t)\u2212 F (\u03b8\u2217) \u2264 T\u2211 t=1 1 t (F (\u03b81)\u2212 F (\u03b8\u2217)) + T\u2211 t=1 t\u22121\u2211 \u03c4=1 D t \u2225\u2206\u03c4\u22252 + T\u2211 t=1 LD2 log t 2t\n\u2264 T\u2211\nt=1\n1 t (F (\u03b81)\u2212 F (\u03b8\u2217)) + T\u2211 t=1 t\u22121\u2211 \u03c4=1 D t \u2225\u2206\u03c4\u22252 + LD2 log T 2 T\u2211 t=1 1 t\n\u2264 (log T + 1)(F (\u03b81)\u2212 F (\u03b8\u2217)) + T\u2211\nt=1 t\u22121\u2211 \u03c4=1 D t \u2225\u2206\u03c4\u22252 + LD2(log T + 1)2 2\n(10) According to Lemma 2, we can obtain the second term is,\nT\u2211 t=1 t\u22121\u2211 \u03c4=1 D t \u2225\u2206\u03c4\u2225 \u2264 4(LD2 + \u03c3D) \u221a 2 log(8T/\u03c3) T\u2211 t=1 t\u22121\u2211 \u03c4=1\n1\nt \u221a \u03c4 + 1\n\u2264 4(LD2 + \u03c3D) \u221a 2 log(8T/\u03c3) T\u2211 t=1 2 \u221a t t\n\u2264 16(LD2 + \u03c3D) \u221a 2T log(8T/\u03c3)\n(11)\nSubstituting Eqn. (11) into Eqn. (10), we have with probability at least 1\u2212 \u03b4/2, T\u2211\nt=1\nF (\u03b8t)\u2212F (\u03b8\u2217) \u2264 log(T+1)(F (\u03b81)\u2212F \u2217(\u03b8))+ LD2(log T + 1)2\n2 +16(LD2+\u03c3D)\n\u221a 2T log(8T/\u03b4)\n(12) Combining Eqn. (12) with Eqn. (8) we have with probability at least 1\u2212 \u03b4,\nCRj \u2264 (log T + 1)(F (\u03b81)\u2212 F (\u03b8\u2217)) + LD2(log T + 1)2\n2\n+ LD2(log T + 1)2 2 + ( 16LD2 + 16\u03c3D + 4M )\u221a 2T log(8T/\u03b4) = O\u0303( \u221a T )"
        },
        {
            "heading": "B RELATED WORKS",
            "text": "Regularization-based methods (or parameter regularization methods) usually construct a quadratic regularization term to slow down the learning of weights that are important to prior tasks. As we know, the Fisher information matrix is equivalent to the Hessian matrix if the loss function\nis negative log-likelihood and we get a ground truth probabilistic model (Chapter 4.5 of (Keener, 2010)). In this way, EWC (Kirkpatrick et al., 2017) and on-EWC (Husza\u0301r, 2017), as the first work applying parameter regularization, utilize the diagonal Fisher information matrix to approximate the Hessian matrix, while Kronecker factored Laplace approximation (KFLA) (Ritter et al., 2018) exploits Kronecker factored Laplace to compute the Fisher information matrix with off-diagonal elements. To account for the trajectory of model training, IS (Zenke et al., 2017) proposes an online estimation of a diagonal matrix and assigns weights based on its contribution to loss decay. On the other hand, in class-incremental learning, IADM (Yang et al., 2019) and CE-IDM (Yang et al., 2021) report that different layers have varying characteristics. Shallow layers with limited representation tend to converge faster, while deep layers with powerful discrimination abilities tend to converge more slowly. Therefore, based on EWC (Kirkpatrick et al., 2017), IADM (Yang et al., 2019) learns an online layer-wise importance matrix.\nRehearsal-based methods address catastrophic forgetting by replaying previous task samples from a memory buffer. Experience Replay (ER) (Rolnick et al., 2019) directly sample data from previous tasks and put them jointly train with current data. On the basis of ER, recent studies have further extended this idea. Meta Experience Replay (MER) (Riemer et al., 2019) views replay as a metalearning problem for maximizing the transfer from previous tasks and minimizing the interference. Gradient-based Sample Selection (GSS) (Aljundi et al., 2019) pays attention to the samples stored in the memory module and hopes to increase the diversity of samples in the gradient space. Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017) and its lightweight variant Averaged-GEM (A-GEM) (Chaudhry et al., 2018) formulate optimization constraints such that gradients between current and old training data are aligned to further determine the final direction of optimization. Dark Experience Replay (DER++) (Buzzega et al., 2020) adds the knowledge distillation part on top of ER to regularize the logits of samples stored in the memory buffer. CLSER (Arani et al., 2021) introduces an innovative approach to episodic replay (ER) by presenting a dual memory framework. This framework incorporates both short-term and long-term semantic memories, allowing for their interaction with the episodic memory. CBA (Wang et al., 2023a) proposes to address the issue of recency bias by introducing a specially designed bias attractor, guided by the memory buffer M. Meta-Continual Learning (Meta-CL) is a class of methods that apply meta-learning (Finn et al., 2017; Wu et al., 2022; 2021; Wang et al., 2023b) in continual learning. From the bi-level optimization perspective to analyze meta-learning, its inner-loop (or lower-level) optimization usually fits the training data, while the outer-loop (higher-level) centralized test data aims to increase the model\u2019s generalization ability. Therefore, meta-learning is well suited for continual learning, as we want the model to fit the current task and still perform well on all observed tasks (i.e., previous and current tasks). Meta Experience Replay (MER) (Riemer et al., 2019), inspired by GEM (Lopez-Paz & Ranzato, 2017), utilizes replay to incentivize the alignment of gradients between old and new tasks to further maximize the transfer from previous tasks and minimize the interference. Online-aware Meta Learning (OML) (Javed & White, 2019) adopts a pre-training algorithm to learn an optimal representation offline, which is frozen when training in the downstream tasks. However, this offline training manner and assumption that the training data is a correlated data stream limit its applicability to practical scenarios. Besides, MER (Riemer et al., 2019) and OML (Javed & White, 2019) use a single sample at one time during the inner-loop optimization, which is time-consuming and not suitable for large-scale CL. To alleviate these problems, Look-ahead Meta learning (La-MAML) (Gupta et al., 2020) proposed an online gradient-based meta-learning algorithm and proved it is equivalent to a simplified version of gradient alignment. On the other hand, some Meta-CL algorithms (Rajasegaran et al., 2020; 2022) focus on combining meta-learning with task-agnostic continual learning that does not require task information during training and testing. Besides, the variance reduction has also been integrated with first-order meta-learning (Yang & Kwok, 2022; Wang et al., 2021). In this work, we focus on the online continual learning setting and provide a variance reduction Meta-CL (second-order) based on our novel understanding of Meta-CL, it also links Meta-CL with regularization-based methods.\nOnline and Imbalanced Continual learning, different from the traditional offline CL setting, are more challenging CL settings that have not been explored as much. For online CL scenarios, the initial methods still have been developed based on similar ways in offline CL (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2018; Aljundi et al., 2019). Recently, there has been increasing interest in developing online CL methods that can fully utilize the single-pass data stream to decrease the performance difference between online and offline CL. OCDVC (Gu et al., 2022) advocates for\nthe dual view consistency strategy, while SDP (Koh et al.) utilizes knowledge distillation to fully leverage the data stream. OBC (Chrysakis & Moens, 2023) proposes a simple approach to mitigate the bias of online CL by changing the optimization manner of the output layer model. OCM (Guo et al., 2022) combines these motivations and aims to reduce feature bias and fully utilize streaming data through mutual information maximization. However, none of these methods directly focus on the performance drop between online and offline CL and analyze the underlying reasons, which is a motivation of this work.\nFor imbalanced CL, where the sample number of each task is different, most works aim to directly design a sampling strategy that can balance the distribution of samples retrieved from the memory buffer that stores data from previous tasks. CBRS (Chrysakis & Moens, 2020) introduced a novel mechanism for class-balancing reservoir sampling based on the sample distribution within the buffer. PRS (Kim et al., 2020) utilizes current data stream statistics to determine the sample-in and sample-out process, maintaining a balanced replay memory. Additionally, InfoRS (Sun et al.) introduces a stochastic information theoretic reservoir sampler to select and store the informative samples. Coresets (Borsos et al., 2020) employs the bi-level optimization technique for the same purpose. Different from these methods, our proposed approach takes a different perspective. It aims to conduct a comprehensive analysis of the factors that contribute to the substantial performance drop observed in both imbalanced and online settings compared to offline learning. By understanding these underlying factors, we propose the VR-MCL to the model performance in imbalanced CL scenarios.\nVariance Reduction is a widely used technique for reducing the variance in stochastic gradients and accelerating the optimization process. Early research on variance reduction methods, such as SAG Gower et al. (2020), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013), was primarily focused on strongly convex optimization problems. However, in recent years, these methods have been extended to general non-convex optimization problems (Allen-Zhu & Hazan, 2016; Nguyen et al., 2017; Fang et al., 2018). Nonetheless, these methods typically require the expensive computation of the full-batch gradient by processing all samples in a task, which is impractical in the context of online continual learning where samples are processed in mini-batches and the complete sample set is unavailable. To alleviate this expensive computation problem, momentum-based variance reduction algorithms (Cutkosky & Orabona, 2019; Khanduri et al., 2021) are proposed. Specifically, these methods incorporate a momentum term to compute the stochastic gradients at two consecutive iterations on the same set of stochastic samples, thereby replacing the time-consuming computation of the full-batch gradient. Additionally, these methods have the same asymptotic convergence rate as other variance reduction methods.\nC IMPLEMENTATION DETAILS"
        },
        {
            "heading": "C.1 EXPERIMENTAL SETTINGS",
            "text": "Imbalanced Settings. We conducted imbalanced online continual learning experiments on Seqcifar10, consisting of five tasks, each with two classes. As shown in Fig. 4, when the imbalance ratio \u03b3 = 2, the number of samples for each class across all tasks is [5000, 4629, 4286, 3968, 3674, 3401, 3149, 2916, 2700, 2500]. We also considered the reversed version, where the number of samples for each class is [2500, 2700, 2916, 3149, 3401, 3674, 3968, 4286, 4629, 5000]. Without loss of generality, we additionally sampled a random version with [2700, 2500, 5000, 4286, 3674, 3968, 3149, 3401, 2916, 4629] samples per class across all tasks.\nBaselines. We compared the proposed VR-MCL to various baseline approaches, including regularization-based methods, rehearsal-based methods, meta-continual learning methods, and specific CL methods designed for imbalanced and online settings.\nRegularization-based methods: \u2022 On-EWC: (Husza\u0301r, 2017). It approximates each Hessian matrix by a diagonal Fisher matrix\ncalculated at the end of each task training. The larger Fisher information value means the corresponding weights are more important to the old tasks and thus should undergo less change in future tasks.\n1 2 3 4 5 6 7 8 9 10 Class index\n0\n1000\n2000\n3000\n4000\n5000\nS am\npl e\nsi ze\nImbalance ratio =2\n(a) Traditional imbalance.\n1 2 3 4 5 6 7 8 9 10 Class index\n0\n1000\n2000\n3000\n4000\n5000\nS am\npl e\nsi ze\nImbalance ratio =2 (Reverse)\n(b) Reverse imbalance.\n1 2 3 4 5 6 7 8 9 10 Class index\n0\n1000\n2000\n3000\n4000\n5000\nS am\npl e\nsi ze\nImbalance ratio =2 (Random)\n(c) Random imbalance.\nHyper-parameters. The hyperparameters used for each experimental setting are listed in Table 7. Let lr denote the learning rate, bs denote the batch size and mbs means the minibatch size (i.e., the batch size drawn from the buffer). It is worth noting that we used the same hyperparameters across different datasets and buffer sizes, which shows our method has good generalizability without requiring extensive hyperparameter tuning for each specific setting."
        },
        {
            "heading": "C.2 THE MASK TRAINING OF THE PROPOSED VR-MCL",
            "text": "For the implementation of the VR-MCL, we adopt mask training different from La-MAML (i.e., one mask training strategy on the final output layer). In this subsection, we will first introduce two mask training approaches: Mask-LM, which is the original mask training in La-MAML, and Mask-VR, which is the proposed mask training used in VR-MCL. We will then provide both mathematical and experimental analysis to compare the two mask training methods.\nIntroduction of the two Mask training methods As the output layer is shared among all tasks, meta-continual learning methods have the problem of a mismatch between the limited rehearsal data from previous tasks T [1:j\u22121] and the larger incoming data from the current task T j . To solve this problem, a masked softmax loss is applied by La-MAML Gupta et al. (2020). As shown in Figure 5, La-MAML applies a mask (i.e., Mask-LM) that only keeps the logits of the corresponding classes within a task to both the loss in the inner-loop adaptation and outer-loop optimization. The Mask-LM improves the inner-loop adaptation for the current tasks and mitigates the interference between the limited data sampled from the memory buffer M and the larger incoming data from the current task. However, the employment of the Mask-LM in the outer-loop still has a limitation. The left part of Mask-LM concentrates solely on minimizing the intra-task loss, without considering the interdependencies between different classes from various tasks. This oversight significantly reduces the model\u2019s efficacy in the class incremental setting. To address this issue, we propose a modification (i.e., Mask-VR) inspired by (Caccia et al., 2021), where we mask all seen classes that appeared in the memory buffer M. This modification enables the model to distinguish samples belonging to different tasks, which is shown in the left part of Mask-VR. Mathematical and Experimental Analysis. The main idea of Mask-VR to enlarge the preserved logits is to introduce the negative gradients so as to get more concise class prototypes and thus achieve better performance. To investigate the impact of the proposed Mask-VR, we conducted experiments and demonstrated empirically that it can effectively reduce the gradient variance. Furthermore, our findings suggest that the Mask-VR and the proposed variance reduction method can be synergistically combined to effectively reduce the gradient variance.\nWe will begin by conducting the mathematical analysis. Note that the following mathematical symbols are solely for the purpose of illustrating the Mask-VR, they are not aligned with the mathematical symbols used in the main text.\nLet Cseen refer to all seen classes, and |Cseen| is the number of classes the model has seen. In Figure 5, the current training task is T 3 and |Cseen| = 6. Let the model architecture consists of two parts: feature extractor h\u03b8 and classifier f\u03d5 and the output of the model before the softmax operation is o(x; \u03b8, \u03d5) = f\u03d5(h\u03b8(x)). Then the cross-entropy loss in continual learning Lce is,\nLce(o(x; \u03b8, \u03d5)) = \u2212 |Cseen|\u2211 i=1 lci log(p ci), pci = exp(oci)\u2211|Cseen| s=1 exp(ocs)\nwhere x is the training sample, lci \u2208 {0, 1} is the one-hot label of class ci, and o(x; \u03b8, \u03d5) = [oc1 , oc2 , ..., oc|Cseen| ] is the corresponding logit values of x. Let x\ni mean the sample of class ci, then the gradients on the classifier f\u03d5 is,\n\u2202Lce ( o ( xi; \u03b8, \u03d5 )) \u2202oci = pci \u2212 1, \u2202Lce ( o ( xi; \u03b8, \u03d5 )) \u2202ocj = pcj ,\n\u2202Lce ( o ( xi; \u03b8, \u03d5 )) \u2202\u03d5ci = \u2202Lce ( o ( xi; \u03b8, \u03d5 )) \u2202oci \u2202oci \u2202\u03d5ci = (pci \u2212 1)h\u03b8(xi), [Positive gradient]\n\u2202Lce ( o ( xi; \u03b8, \u03d5 )) \u2202\u03d5cj = \u2202Lce ( o ( xi; \u03b8, \u03d5 )) \u2202ocj \u2202ocj \u2202\u03d5cj = pcjh\u03b8(x i), [Negative gradient]\nLet W denote the matrix with all class prototypes {wc}c\u2208Cseen , then W \u25e6 h\u03b8 = f\u03d5(h\u03b8(x)). According to the above equation, we can get in Mask-LM, only wci receives a positive gradient from xi, and there are no negative gradients due to preserving only a portion of the logits. However, in Mask-VR, both positive and negative gradients are present, similar to the concept of positives and negatives in contrastive loss. Consequently, the class prototypes W learned with Mask-VR exhibit superior performance compared to those learned with Mask-LM.\nAblation studies of the Mask-Training. To better understand the effectiveness of the Mask-VR, and the proposed VR-MCL, we compare the performance of La-MAML, MCL(La-MAML with Mask-VR), as well as VR-MCL1 (i.e., VR-MCL with Mask-LM) and VR-MCL2 (i.e., VR-MCL with Mask-VR), as shown in Table 8. Moreover, we also plot the relative gradient variance of LaMAML, MCL, VR-MCL1, and VR-MCL2, as shown in Fig. 6. Based on the fact that the curves with variance reduction (VR) are all lower than those without VR, it can be concluded that: 1) the proposed VR technique indeed can reduce the variance, 2) the effects of VR and Mask-VR can be superimposed and work together."
        },
        {
            "heading": "D OTHER EXPERIMENTAL RESULTS.",
            "text": ""
        },
        {
            "heading": "D.1 THE TRAINING TIME ANALYSIS OF VR-MCL",
            "text": "To analyze the time complexity of the proposed algorithm, we compared the training time of the proposed VR-MCL with relevant Meta-CL methods on the Seq-CIFAR-10 dataset using reduced ResNet-18, as shown in Table 9. It is evident that our proposed VR-MCL method demonstrates a substantial reduction in training time compared to MER. To assess the value of the additional training time required by VR-MCL, we conducted experiments by extending the training epoch of La-MAML from 1 to 2. Surprisingly, despite the comparable training time with VR-MCL, LaMAML achieved a performance of only 38.89%, which is extremely lower than the performance of VR-MCL (i.e., 56.48%). This outcome serves as strong evidence for the superiority of VR-MCL in terms of both training efficiency and performance on Seq-CIFAR10."
        },
        {
            "heading": "D.2 OTHER EVALUATION METRICS",
            "text": "To further evaluate the effectiveness of the proposed VR-MCL, we provide the backward transfer (BWT) metric in Table 10, which measures the performance degradation in subsequent tasks. It can be observed our VR-MCL achieves the best results. It is worth noting that in Table 10, La-MAML is excluded from the analysis due to the extremely negative impact of Mask-LM utilization on its newest training task (i.e., T j) performance (Caccia et al., 2021). This impact results in large and even a positive backward transfer (BWT) value for La-MAML."
        },
        {
            "heading": "D.3 HYPERPARAMETER SELECTION",
            "text": "We utilized a grid search technique to select the optimal hyperparameters, including the learning rate \u03b1 and the momentum ratio r, as illustrated in Table 11 and Table 12. The results demonstrate that our proposed method achieves the best performance when both the momentum ratio r and learning rate \u03b1 are set to 0.25. Moreover, it is worth noting that there is small performance difference observed when different hyperparameters are chosen. This suggests that the proposed VR-MCL method exhibits robustness and is not highly sensitive to the selection of hyperparameters."
        },
        {
            "heading": "D.4 OTHER IMBALANCED CL EXPERIMENTS",
            "text": "Due to the page limit, the full results of Table 4 are shown in Table 13. To further explore the performance of VR-MCL in imbalanced settings, we select the more realistic random imbalanced scenario and present the results for a higher imbalance ratio, specifically \u03b3 = 5, in Table 14. For simplicity, we select the SOTA methods from Table 13 as the comparable methods. The results clearly demonstrate that the proposed VR-MCL surpasses other methods in the case of \u03b3 = 5, providing further evidence of the effectiveness of the variance reduction technique in addressing imbalanced CL.\nTo further demonstrate the effectiveness of the proposed VR-MCL under imbalanced settings, we performed additional verification on various imbalance settings using the Seq-CIFAR100 dataset. The results of these experiments are presented in Table 15."
        },
        {
            "heading": "D.5 OTHER RESULTS USING THE PCCNN BACKBONE",
            "text": "In Table 16, we further investigate the performance gain over the memory buffer size on the small network PcCNN. Combined with the results on reduced ResNet18, it is evident that the proposed VR-MCL has a consistent and significant performance improvement in different buffer sizes and various network structures."
        },
        {
            "heading": "D.6 OFFLINE CL PERFORMANCE",
            "text": "As shown in Table 17, we also provide the performance of different methods under the offline CL setting where we set the training epochs as 5.\nD.7 INFLUENCE OF THE INNER BATCH SIZE\nTo investigate the practical implications of the inner batch size, we conducted experiments using various inner batch sizes. The corresponding results are presented in Table 18. The findings indicate that a larger inner batch size consistently leads to improved performance. However, it is worth noting that the performance gains tend to plateau as the inner batch size increases. This can be attributed to the online nature of the setting, where each sample is observed only once. A larger inner batch size reduces the number of outer loop steps, which may potentially compromise the overall performance improvement."
        },
        {
            "heading": "E ALGORITHM.",
            "text": "As presented in Algo. 1, Steps 1-9 present the update of \u03b8 in the first iteration, where no momentum term is available. Steps 2-6 present the inner loop and steps 7-8 present the estimate of hypergradient in the outer loop. Steps 10-28 represent the update of \u03b8 in b-th iteration (b > 1). Steps 12-17 are\nthe inner loops. Steps 18-19 show the computation of the hypergradient for \u03b8b. In steps 20-25, we perform the inner loop to obtain \u03b8\u2032b(K) and compute the hypergradient for \u03b8b as g \u03f5b \u03b8b\u22121\n. During the update of memory buffer M (step 28), we employ the reservoir sampling strategy to ensure that M is updated in a way that the stored examples are uniformly sampled from the tasks during online training.\nE.1 VARYING TASK LENGTH ANALYSIS\nTo further demonstrate the effectiveness of the proposed VR-MCL across varying task lengths, we adjust the split mechanism of Seq-CIFAR100 and Seq-TinyImageNet to increase their original task numbers. Specifically, we restructure Seq-CIFAR100, originally composed of 10 tasks with 10 classes each, into 20 tasks, each with 5 classes. Similarly, we modify Seq-TinyImageNet, initially consisting of 20 tasks with 10 classes each, into 40 tasks, each comprising 5 classes. For clarity, we refer to these new division datasets as Seq-CIFAR100l and Seq-TinyImageNetl respectively. The performance of various baselines on these two datasets is presented in Table 19."
        },
        {
            "heading": "E.2 EMPIRICAL RESULTS OF THE OUTER-LOOP LOSS",
            "text": "Algorithm 1 The Algorithm of the proposed VR-MCL. Require: Initial model weights \u03b8, initial inner-loop learning rate \u03b2, outer-loop learning rate \u03b1,\nnumber of inner-loop steps K, memory buffer M, and the task number in total N . 1: Sample training data T 1(s) (s = 1, ...,K) from current task T\n1. 2: \u03b8(0) = \u03b8, \u03b80 = \u03b8 3: for i = 1 to K do 4: obtain samples T 1(i) 5: \u03b8(i) = \u03b8(i\u22121) \u2212 \u03b2\u2207L1(i)(\u03b8) 6: end for 7: sample \u03f50 from M\u222a T 1 8: g\u0302\u03f50\u03b80 = \u2207\u03b8L(\u03b8(K), \u03f50) 9: \u03b81 = \u03b80 \u2212 \u03b1g\u0302\u03f50\u03b80\n10: for j = 1 to N do 11: for b = (j\u22121)\u00d7B + 1 to j\u00d7B do 12: sample training data T j(s) (s = 1, ...,K) from current task T\nj ."
        },
        {
            "heading": "13: \u03b8b(0) = \u03b8b",
            "text": "14: for i = 1 to K do 15: obtain samples T j(i) 16: \u03b8b(i) = \u03b8b(i\u22121) \u2212 \u03b2\u2207Lj(i)(\u03b8b(i\u22121)) 17: end for 18: sample \u03f5b from M\u222a T j 19: g\u03f5b\u03b8b = \u2207L(\u03b8b(K), \u03f5b) 20: \u03b8 \u2032\nb(0) = \u03b8b\u22121 21: for i = 1 to K do 22: use the same sampled training data T j(i) 23: \u03b8 \u2032\nb(i) = \u03b8 \u2032 b(i\u22121) \u2212 \u03b2\u2207L j (i)(\u03b8 \u2032 b(i\u22121))\n24: end for 25: g\u03f5b\u03b8b\u22121 = \u2207L(\u03b8 \u2032 b(K), \u03f5b) 26: g\u0302\u03f5bb = g \u03f5b b + r(g\u0302 \u03f5b\u22121 b\u22121 \u2212 g \u03f5b \u03b8b\u22121 )"
        },
        {
            "heading": "27: \u03b8b+1 = \u03b8b \u2212 \u03b1g\u0302\u03f5bb",
            "text": "28: Update the memory buffer M with T j(i) using reservoir sampling strategy. 29: end for 30: end for\nFig. 7 illustrates the value of L[1:j](\u03b8(K)) during the training of each task T j in Seq-CIFAR10 (j=2,3,4,5). The loss L[1:j](\u03b8(K)) exhibits a significant decrease within an average of 5 outer loop update steps, suggesting a close proximity between \u03b8(K) and \u03b8\u2217 as stated in Proposition 2."
        }
    ],
    "year": 2024
}