{
    "abstractText": "Previous literature on policy diversity in reinforcement learning (RL) either focuses on the online setting or ignores the policy performance. In contrast, offline RL, which aims to learn high-quality policies from batched data, has yet to fully leverage the intrinsic diversity of the offline dataset. Addressing this dichotomy and aiming to balance quality and diversity poses a significant challenge to extant methodologies. This paper introduces a novel approach, termed Stylized Offline RL (SORL), which is designed to extract high-performing, stylistically diverse policies from a dataset characterized by distinct behavioral patterns. Drawing inspiration from the venerable Expectation-Maximization (EM) algorithm, SORL innovatively alternates between policy learning and trajectory clustering, a mechanism that promotes policy diversification. To further augment policy performance, we introduce advantage-weighted style learning into the SORL framework. Experimental evaluations across multiple environments demonstrate the significant superiority of SORL over previous methods in extracting high-quality policies with diverse behaviors. A case in point is that SORL successfully learns strong policies with markedly distinct playing patterns from a real-world human dataset of a popular basketball video game \u201dDunk City Dynasty.\u201d",
    "authors": [
        {
            "affiliations": [],
            "name": "Yihuan Mao"
        },
        {
            "affiliations": [],
            "name": "Chengjie Wu"
        },
        {
            "affiliations": [],
            "name": "Hao Hu"
        },
        {
            "affiliations": [],
            "name": "Ji Jiang"
        },
        {
            "affiliations": [],
            "name": "Tianze Zhou"
        },
        {
            "affiliations": [],
            "name": "Changjie Fan"
        },
        {
            "affiliations": [],
            "name": "Zhipeng Hu"
        },
        {
            "affiliations": [],
            "name": "Yi Wu"
        },
        {
            "affiliations": [],
            "name": "Yujing Hu"
        },
        {
            "affiliations": [],
            "name": "Chongjie Zhang"
        }
    ],
    "id": "SP:7b9dc8558d5fecf01d141f3f410f37caa3311a20",
    "references": [
        {
            "authors": [
                "Joshua Achiam",
                "Harrison Edwards",
                "Dario Amodei",
                "Pieter Abbeel"
            ],
            "title": "Variational option discovery",
            "year": 2018
        },
        {
            "authors": [
                "Hugo Araujo",
                "Mohammad Reza Mousavi",
                "Mahsa Varshosaz"
            ],
            "title": "Testing, validation, and verification of robotic and autonomous systems: A systematic review",
            "venue": "ACM Trans. Softw. Eng. Methodol.,",
            "year": 2023
        },
        {
            "authors": [
                "Victor Campos",
                "Alexander Trott",
                "Caiming Xiong",
                "Richard Socher",
                "Xavier Giro-I-Nieto",
                "Jordi Torres"
            ],
            "title": "Explore, discover and learn: Unsupervised discovery of state-covering skills",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xi Chen",
                "Ali Ghadirzadeh",
                "Tianhe Yu",
                "Jianhao Wang",
                "Alex Yuan Gao",
                "Wenzhe Li",
                "Liang Bin",
                "Chelsea Finn",
                "Chongjie Zhang"
            ],
            "title": "Lapo: Latent-variable advantage-weighted policy optimization for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "C\u00e9dric Colas",
                "Vashisht Madhavan",
                "Joost Huizinga",
                "Jeff Clune"
            ],
            "title": "Scaling map-elites to deep neuroevolution",
            "venue": "In Proceedings of the 2020 Genetic and Evolutionary Computation Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Cully",
                "Yiannis Demiris"
            ],
            "title": "Quality and diversity optimization: A unifying modular framework",
            "venue": "IEEE Transactions on Evolutionary Computation,",
            "year": 2018
        },
        {
            "authors": [],
            "title": "Robots that can adapt like animals",
            "venue": "Nature, 521:503\u2013507,",
            "year": 2015
        },
        {
            "authors": [
                "Rick Durrett"
            ],
            "title": "Probability: theory and examples, volume 49",
            "venue": "Cambridge university press,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Abhishek Gupta",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Diversity is all you need: Learning skills without a reward function",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Haobo Fu",
                "Ye Tian",
                "Hongxiang Yu",
                "Weiming Liu",
                "Shuang Wu",
                "Jiechao Xiong",
                "Ying When",
                "Kai Li",
                "Junliang Xing",
                "Qiang Fu",
                "Wei Yang"
            ],
            "title": "Greedy when sure and conservative when uncertain about the opponents",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "Off-policy deep reinforcement learning without exploration, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Zhang-Wei Hong",
                "Tzu-Yun Shann",
                "Shih-Yang Su",
                "Yi-Hsiang Chang",
                "Chun-Yi Lee"
            ],
            "title": "Diversitydriven exploration strategy for deep reinforcement learning, 2018",
            "venue": "URL https:// openreview.net/forum?id=BJsD7L1vz",
            "year": 2018
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Maximilian Igl",
                "Punit Shah",
                "Paul Mougin",
                "Sirish Srinivasan",
                "Tarun Gupta",
                "Brandyn White",
                "Kyriacos Shiarlis",
                "Shimon Whiteson"
            ],
            "title": "Latent hierarchical imitation learning for stochastic environments, 2023",
            "venue": "URL https://openreview.net/forum?id=stgewiZP0OH",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Ashvin Nair",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning with implicit qlearning",
            "year": 2021
        },
        {
            "authors": [
                "Alex Kuefler",
                "Mykel J. Kochenderfer"
            ],
            "title": "Burn-in demonstrations for multi-modal imitation learning",
            "venue": "CoRR, abs/1710.05090,",
            "year": 2017
        },
        {
            "authors": [
                "Aviral Kumar",
                "Aurick Zhou",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Conservative q-learning for offline reinforcement learning, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Michael Laskin",
                "Hao Liu",
                "Xue Bin Peng",
                "Denis Yarats",
                "Aravind Rajeswaran",
                "Pieter Abbeel"
            ],
            "title": "Cic: Contrastive intrinsic control for unsupervised skill discovery, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yunzhu Li",
                "Jiaming Song",
                "Stefano Ermon"
            ],
            "title": "Infogail: Interpretable imitation learning from visual demonstrations",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yicheng Luo",
                "Zhengyao Jiang",
                "Samuel Cohen",
                "Edward Grefenstette",
                "Marc Peter Deisenroth"
            ],
            "title": "Optimal transport for offline imitation learning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Muhammad Masood",
                "Finale Doshi-Velez"
            ],
            "title": "Diversity-inducing policy gradient: Using maximum mean discrepancy to find a set of diverse policies",
            "venue": "In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Jean-Baptiste Mouret",
                "Jeff Clune"
            ],
            "title": "Illuminating search spaces by mapping elites",
            "venue": "CoRR, abs/1504.04909,",
            "year": 2015
        },
        {
            "authors": [
                "Gerhard Neumann",
                "Jan Peters"
            ],
            "title": "Fitted q-iteration by advantage weighted regression",
            "venue": "In Proceedings of the 21st International Conference on Neural Information Processing Systems,",
            "year": 2008
        },
        {
            "authors": [
                "Shu Kay Ng",
                "Thriyambakam Krishnan",
                "Geoffrey J. McLachlan"
            ],
            "title": "The EM Algorithm, pp. 139\u2013172",
            "year": 2012
        },
        {
            "authors": [
                "Olle Nilsson",
                "Antoine Cully"
            ],
            "title": "Policy gradient assisted map-elites",
            "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Aviral Kumar",
                "Grace Zhang",
                "Sergey Levine"
            ],
            "title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Pierrot",
                "Valentin Mac\u00e9",
                "F\u00e9lix Chalumeau",
                "Arthur Flajolet",
                "Geoffrey Cideron",
                "Karim Beguir",
                "Antoine Cully",
                "Olivier Sigaud",
                "Nicolas Perrin-Gilbert"
            ],
            "title": "Diversity policy gradient for sample efficient quality-diversity optimization, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Justin K. Pugh",
                "Lisa B. Soros",
                "Kenneth O. Stanley"
            ],
            "title": "Quality diversity: A new frontier for evolutionary computation",
            "venue": "Frontiers Robotics AI,",
            "year": 2016
        },
        {
            "authors": [
                "Nur Muhammad Mahi Shafiullah",
                "Zichen Jeff Cui",
                "Ariuntuya Altanzaya",
                "Lerrel Pinto"
            ],
            "title": "Behavior transformers: Cloning k modes with one stone, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Archit Sharma",
                "Shixiang Gu",
                "Sergey Levine",
                "Vikash Kumar",
                "Karol Hausman"
            ],
            "title": "Dynamics-aware unsupervised discovery of skills",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ruimin Shen",
                "Yan Zheng",
                "Jianye Hao",
                "Zhaopeng Meng",
                "Yingfeng Chen",
                "Changjie Fan",
                "Yang Liu"
            ],
            "title": "Generating behavior-diverse game ais with evolutionary multi-objective deep reinforcement learning",
            "venue": "In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Noah Siegel",
                "Jost Tobias Springenberg",
                "Felix Berkenkamp",
                "Abbas Abdolmaleki",
                "Michael Neunert",
                "Thomas Lampe",
                "Roland Hafner",
                "Nicolas Heess",
                "Martin Riedmiller"
            ],
            "title": "Keep doing what worked: Behavior modelling priors for offline reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Valentin Villecroze",
                "Harry Braviner",
                "Panteha Naderian",
                "Chris Maddison",
                "Gabriel LoaizaGanem"
            ],
            "title": "Bayesian nonparametrics for offline skill discovery",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Qing Wang",
                "Jiechao Xiong",
                "Lei Han",
                "peng sun",
                "Han Liu",
                "Tong Zhang"
            ],
            "title": "Exponentially weighted imitation learning for batched historical data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Yutong Wang",
                "Ke Xue",
                "Chao Qian"
            ],
            "title": "Evolutionary diversity optimization with clustering-based selection for reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ziyu Wang",
                "Josh Merel",
                "Scott Reed",
                "Greg Wayne",
                "Nando de Freitas",
                "Nicolas Heess"
            ],
            "title": "Robust imitation of diverse behaviors",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Weichang Wu",
                "Junchi Yan",
                "Xiaokang Yang",
                "Hongyuan Zha"
            ],
            "title": "Reinforcement learning with policy mixture model for temporal point processes clustering",
            "venue": "ArXiv, abs/1905.12345,",
            "year": 2019
        },
        {
            "authors": [
                "Yifan Wu",
                "George Tucker",
                "Ofir Nachum"
            ],
            "title": "Behavior regularized offline reinforcement learning",
            "venue": "CoRR, abs/1911.11361,",
            "year": 2019
        },
        {
            "authors": [
                "XiaoPeng Yu",
                "Jiechuan Jiang",
                "Wanpeng Zhang",
                "Haobin Jiang",
                "Zongqing Lu"
            ],
            "title": "Model-based opponent modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ruohan Zhang",
                "Calen Walshe",
                "Zhuode Liu",
                "Lin Guan",
                "Karl S. Muller",
                "Jake A. Whritner",
                "Luxin Zhang",
                "Mary M. Hayhoe",
                "Dana H. Ballard"
            ],
            "title": "Atari-head: Atari human eye-tracking and demonstration dataset, 2019a",
            "year": 2019
        },
        {
            "authors": [
                "Yunbo Zhang",
                "Wenhao Yu",
                "Greg Turk"
            ],
            "title": "Learning novel policies for tasks",
            "venue": "In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Learning to accomplish a task with diverse behaviors, also known as quality-diversity optimization, is an emerging area within stochastic optimization research (Pugh et al., 2016; Cully & Demiris, 2018; Mouret & Clune, 2015). It aims to generate a diverse set of solutions that maximize a given objective function and is especially valuable in applications involving human interactions, such as games (Shen et al., 2021) and autonomous driving (Araujo et al., 2023). For instance, in online games, deploying AI bots with varied motion styles can enrich the gaming environment and enhance player engagement. Similarly, in autonomous driving, offering multiple solutions can cater to users with different preferences. Additionally, in opponent modeling (Yu et al., 2022), high-quality diverse opponents that resemble real opponents can significantly improve the performance of the learned policy. However, a significant limitation is that most studies in this domain heavily rely on extensive online interactions (Nilsson & Cully, 2021; Pierrot et al., 2022), leading to high costs and imposing constraints on systems with restricted online access. Recent advances in offline RL present a promising direction, allowing for policy learning from pre-collected datasets without further interactions (Fujimoto et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021). However, they often prioritize policy quality, sidelining the inherent diversity within the dataset. In this paper, we bridge this gap by targeting both diversity and high-quality policy learning from offline datasets.\n\u2217Equal advising. Code is at https://github.com/cedesu/SORL.\nThe central challenge lies in how to optimize the performance of the policies while ensuring that their behaviors are as distinguishable as possible. Balancing this in an offline setting becomes problematic due to the inherent characteristics of offline datasets. Typically, datasets capturing diverse behaviors are heterogeneous, gathered from multiple sources, leading to an action distribution with multiple modes and inconsistent data quality (Chen et al., 2022; Li et al., 2017). Studies focus on learning diverse behaviors that can capture the multi-modality of the action distribution. However, they often employ a diversity objective that is task-agnostic (Eysenbach et al., 2019; Masood & Doshi-Velez, 2019). In offline settings, this task-agnostic objective can lead to policies that perform exceedingly poorly due to inconsistent data quality. Conversely, current offline RL methods prioritize task performance maximization. To mitigate overestimation issues, a conservative constraint is often imposed on the policy, ensuring that its distribution aligns closely with the dataset (Fujimoto et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021). Amplifying this conservativeness might retain some multi-modality of the dataset. However, it does not offer control over how distinguishable the policy should be during training.\nTo address this challenge, we introduce Stylized Offline Reinforcement Learning (SORL), a twostep framework designed to derive diverse and high-quality policies from a heterogeneous offline dataset. In the first step, we perform a style clustering. Drawing inspiration from the venerable Expectation-Maximization (EM) algorithm, we classify the trajectories from the heterogeneous dataset into clusters where each represents a distinct and dominant motion style. In the second step, we employ advantage-weighted style learning, an offline RL method with a novel objective considering both performance and diversity. Here we train a set of policies to maximize the task performance, with each policy specifically constrained to the action distribution of a particular style identified in the first step. Unlike other offline RL methods that constrain the policy to the entire dataset without differentiating between types of motions, our approach effectively extracts stylistically diverse behavior in the dataset that can be characterized by distinct behavioral patterns. In contrast to diverse RL methods, we achieve high-performing policies that are in-distribution with respect to the dataset, yet less influenced by the low-quality samples.\nWe evaluate SORL across various environments and offline datasets. These include a didactic game with a hand-crafted dataset, a set of Atari games using open-sourced human data, and the popular basketball video game \u201dDunk City Dynasty\u201d with data recorded from online players. We compare SORL against the offline versions of two baseline methods that focus on learning diverse behavior from multi-modal datasets. The experimental results demonstrate the significant superiority of SORL over the baseline methods in achieving policies with higher performance while maintaining distinguishable behavior patterns. In summary, the contributions of this paper are as follows:\n1. We introduce SORL, a novel framework that addresses the limitations of both diverse RL and offline RL methods by incorporating both quality and diversity into the optimization objective.\n2. We provide comprehensive evaluations of SORL across diverse environments using datasets recorded from humans, showcasing its capability to extract high-performing, stylistically diverse policies from heterogeneous offline datasets."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Offline RL and Imitation Learning Offline reinforcement learning (RL) leverages a fixed offline dataset to learn a policy that achieves high performance in online evaluation. Previous work has focused on policy conservativeness to prevent over-estimation and mitigate out-of-distribution actions during evaluation (Fujimoto et al., 2019; Wu et al., 2019b; Peng et al., 2019; Kostrikov et al., 2021; Kumar et al., 2020). Imitation Learning involves learning the behavior policy from the dataset, instead of using the reinforcement learning. Some studies in Imitation Learning also consider the potential multi-modality of the dataset (Li et al., 2017; Kuefler & Kochenderfer, 2017; Wang et al., 2017; Igl et al., 2023; Shafiullah et al., 2022; Wu et al., 2019a). Offline skill discovery shares similarities with learning diverse policies, but its goal is to improve performance in downstream tasks through the acquisition of skills. These approaches employ similar methods to model the latent variable and capture different skills (Laskin et al., 2022; Villecroze et al., 2022).\nDiversity in RL Diversity plays a crucial role in Reinforcement Learning algorithms. In some works, diversity is supposed to enhance the overall quality by means of encouraging exploration\n(Hong et al., 2018) or better opponent modeling (Fu et al., 2022). Besides studies on diverse policies, skill discovery also shares the same requirement for diverse skills (Eysenbach et al., 2019; Campos et al., 2020; Sharma et al., 2020; Achiam et al., 2018). Other works consider optimizing both diversity and high quality in the optimization (Masood & Doshi-Velez, 2019; Zhang et al., 2019b; Zhou et al., 2022). It is important to mention that studies on diversity in the online environment promote exploration or skill discovery. However, in this paper, our focus is on diversity in the offline setting, which is beneficial for opponent modeling and various applications such as game AI and autonomous driving.\nQuality-Diversity Optimization Some research in the field of evolutionary algorithms also focuses on discovering diverse policies. They formulate the problem as the Quality-Diversity Optimization problem (Pugh et al., 2016; Cully & Demiris, 2018), where \u201dquality\u201d refers to the policy\u2019s performance, and \u201ddiversity\u201d emerges from the evolutionary iterations. An algorithm called MAPElites has been developed to generate diverse and high-quality policies (Cully, 2015; Mouret & Clune, 2015). Subsequent studies in this area aim to improve the efficiency of the evolutionary algorithm by combining it with policy gradient methods (Pierrot et al., 2022; Nilsson & Cully, 2021; Pierrot et al., 2022) or evolution strategies (Colas et al., 2020; Wang et al., 2022)."
        },
        {
            "heading": "3 PRELIMINARY",
            "text": "In this paper, we consider a Markov Decision Process (MDP) defined by the tuple (S,A, P, r, \u03c10, \u03b3), where S is the state space, A is the action space, and P : S \u00d7A\u00d7 S \u2192 R is the transition function. The reward function is denoted by r : S \u00d7 A \u2192 R. We use \u03c10 : S \u2192 R to denote the initial state distribution, and \u03b3 to denote the discount factor. In standard reinforcement learning (RL), a learning agent optimizes its polity \u03c0 : S \u00d7 A \u2192 R to maximize the expected cumulative discounted return J(\u03c0) = E\u03c0 \u2211T t=0 \u03b3\ntr(st, at), where s0 \u223c \u03c10, at \u223c \u03c0(at|st), and st+1 \u223c P (st+1|st, at). The value function V \u03c0(st) corresponds to the expected return of policy \u03c0 at state st, and the action value function Q\u03c0(st, at) refers to the expected return obtained by playing action at at state st and then following \u03c0. The advantage function A\u03c0(st, at) is defined as A\u03c0(st, at) = Q\u03c0(st, at)\u2212V \u03c0(st). We use d\u03c0(s) = \u2211T t=0 \u03b3\ntp(st = s|\u03c0) to denote the unnormalized discounted state distribution induced by policy \u03c0.\nIn offline RL, an agent learns from a pre-collected dataset D consisting of multiple trajectories without online interaction with the environment. This paper further assumes that the dataset D contains behaviors of heterogeneous policies {\u03b2(1), \u03b2(2), . . . , \u03b2(K)}. The assumption is not restrictive for many real-world scenarios. For instance, in online gaming and autonomous driving, the dataset usually contains behaviors of a number of different agents and humans, each of which may possess distinct behavioral patterns. The objective of our approach, termed Stylized Offline RL (SORL), is to learn a set of high-quality and diverse policies {\u03c0(1), \u03c0(2), . . . , \u03c0(m)} from the most representative behaviors exhibited by the dataset D."
        },
        {
            "heading": "4 METHOD",
            "text": "In order to leverage heterogeneous datasets and strike a balance between policy diversity and performance, we propose Stylized Offline RL (SORL), a novel two-step framework consisting of (1) EM-based style clustering and (2) advantage-weighted style learning. In style clustering, for each cluster that represents a distinct and dominant behavioral style, SORL assigns different weights to trajectories in the dataset. A trajectory\u2019s weight reflects its posterior probability of belonging to that style. Subsequently, SORL incorporates stylized advantage weighted regression to learn diverse and high-quality policies by constraining each policy to be conservative with respect to the corresponding weighted set of data. The SORL algorithm is illustrated in Algorithm 1."
        },
        {
            "heading": "4.1 EM-BASED STYLE CLUSTERING",
            "text": "The latent variable model is a natural way to disentangle a dataset generated by diverse policies (Li et al., 2017; Wang et al., 2017; Laskin et al., 2022). In our problem, each trajectory in the dataset is generated by an unknown latent policy. Therefore, drawing inspiration from the expectationmaximization (EM) algorithm, SORL alternates between trajectory clustering and policy learning to\nextract the most representative diverse behaviors from the heterogeneous dataset. SORL learns a set of m most representative policies {\u00b5(1), \u00b5(2), . . . , \u00b5(m)} by maximizing the log-likelihood:\nLL({\u00b5(i)}) = \u2211 \u03c4\u2208D log [ m\u2211 i=1 p(\u03c4 |z = i)p(\u00b5(i)) ] (1)\nwhere \u03c4 denotes a trajectory in the dataset, z is the latent variable indicating which policy \u03c4 belongs to, p(\u03c4 |z = i) is the probability of \u03c4 sampled under policy \u00b5(i), and p(\u00b5(i)) is the prior distribution. The policies serve as the generative model for generating the dataset. The main challenge is to identify latent z. The EM-based algorithm iteratively updates the policies and the estimation p\u0302 for the posteriors p(z|\u03c4). Upon its convergence, the estimated posterior distribution p\u0302(z|\u03c4) offers a clustering of trajectories in the dataset, and the policy \u00b5(i) imitates the behavior in cluster i."
        },
        {
            "heading": "4.1.1 E-STEP",
            "text": "In the E-step, SORL calculates the estimated posterior distribution of the latent variable p\u0302(z|\u03c4) with respect to the current policies {\u00b5(i)}. Formally, p\u0302(z = i|\u03c4) \u221d p(\u00b5(i)) \u220f (s,a)\u2208\u03c4 \u00b5\n(i)(a|s). In this paper, we assume a uniform prior. Empirically, we find that the long trajectory horizon leads to numerical instability when multiplying probabilities. Hence, we employ an alternative approach to estimate the latent variable. Since all steps in the trajectory \u03c4 share the same latent variable, the posterior distribution p\u0302(z|\u03c4) is estimated by averaging across all the samples in a trajectory.\np\u0302(z = i|\u03c4) \u2248 1 Z \u2211 (s,a)\u2208\u03c4 \u00b5(i)(a|s) (2)\nwhere Z is a normalizing factor. SORL uses equation 2 to calculate the posteriors for all trajectories of the dataset in the E-step."
        },
        {
            "heading": "4.1.2 M-STEP",
            "text": "According to Jensen\u2019s inequality (Durrett, 2019), LL({\u00b5(i)}) \u2265 \u2211 \u03c4\u2208D m\u2211 i=1 [ p\u0302(z = i|\u03c4) log p(\u03c4 |z = i)p(\u00b5 (i)) p\u0302(z = i|\u03c4) ] . (3)\nIn the M-step, with the posteriors frozen, the policies {\u00b5(i)}mi=1 are updated to maximize the right side of inequality 3, which is a lower bound of the log likelihood in equation 1. In order to maximize the objective \u2211 \u03c4\u2208D \u2211m i=1 [p\u0302(z = i|\u03c4) log p(\u03c4 |z = i)], SORL utilizes weighted imitation learning to minimize the following loss:\nLoss({\u00b5(i)}) = 1 |D| \u2211 \u03c4\u2208D m\u2211 i=1 p\u0302(z = i|\u03c4) \u2211 (s,a)\u2208\u03c4 log\u00b5(i)(a|s) (4)\nThe style clustering algorithm shares the same convergence result with the original EM algorithm, and it is guaranteed to converge to a saddle point (Ng et al., 2012)."
        },
        {
            "heading": "4.2 ADVANTAGE-WEIGHTED STYLE LEARNING",
            "text": "In the second step, SORL further improves the policies\u2019 performances with stylized advantageweighted regression. Formally, SORL learns a set of policies {\u03c0(1), \u03c0(2), . . . , \u03c0(m)} through the following constrained optimization problem:\n\u2200i \u2208 [m], \u03c0(i) = argmax J(\u03c0(i))\ns.t. Es\u223cd \u00b5(i) (s)DKL(\u03c0 (i)(\u00b7|s)||\u00b5(i)(\u00b7|s)) \u2264 \u03f5, \u222b a \u03c0(i)(a|s)da = 1, \u2200s. (5)\nThe policies learn to maximize cumulative return to avoid degenerate behavior, while still remaining diverse through constraining the KL divergence between \u00b5(i) and \u03c0(i) to be small. Maximizing the cumulative return J(\u03c0(i)) is equivalent to maximizing the expected improvement \u03b7(\u03c0(i)) = J(\u03c0(i)) \u2212 J(\u00b5(i)) (i = 1 . . .m). According to previous works, the expected improvement can be expressed in terms of advantage: (Schulman et al., 2015)\n\u03b7(\u03c0(i)) = Es\u223cd \u03c0(i) (s)Ea\u223c\u03c0(i)(a|s)[A\u00b5 (i) (s, a)] (6)\nEquation 6 poses challenges in optimization due to the unknown discounted state distribution d\u03c0(i) . To address this, a common approach is to substitute d\u03c0(i) with d\u00b5(i) to provide a good estimate of \u03b7(\u03c0(i)) (Schulman et al., 2015; Peng et al., 2019). It has been proved that the error can be bounded by the KL divergence between \u03c0(i) and \u00b5(i) (Schulman et al., 2015), which has already been constrained in our objective 5. Therefore, we optimize the following objective 11.\n\u2200i \u2208 [m], \u03c0(i) = argmaxEs\u223cd \u00b5(i) (s)Ea\u223c\u03c0(i)(\u00b7|s)A\u00b5 (i) (s, a)\ns.t. Es\u223cd \u00b5(i) (s)DKL(\u03c0 (i)(\u00b7|s)||\u00b5(i)(\u00b7|s)) \u2264 \u03f5, \u222b a \u03c0(i)(a|s)da = 1, \u2200s. (7)\nIn advantage-weighted style learning, we approximate A\u00b5 (i)\n(s, a) by A\u00b5(s, a), where \u00b5 represents the policy distribution of the entire dataset. This approximation is made because A\u00b5(s, a) often has higher quality than A\u00b5 (i) . Subsequently, we calculate the Lagrangian of the optimization problem:\nEs\u223cd \u00b5(i) (s)\n[ Ea\u223c\u03c0(i)(\u00b7|s)A\u00b5(s, a) + \u03bb(\u03f5\u2212DKL(\u03c0(i)(\u00b7|s)||\u00b5(i)(\u00b7|s))) ] + \u222b s \u03b1s(1\u2212 \u222b a\n\u03c0(i)(a|s)da) (8)\nNote that the policy \u03c0(i) of each style can be optimized independently. Taking derivative with respect to \u03c0i(a|s), we can obtain the closed-form solution \u03c0(i)\u2217(a|s) \u221d \u00b5(i)(a|s)exp( 1\u03bbA\n\u00b5(s, a)). Finally, we project \u03c0(i)\u2217 to \u03c0(i)\u03b8 paramterized by \u03b8 by minimizing the KL divergence between them. It can be proved that the parameterized policy of i-th style \u03c0(i)\u03b8 can be learned by minimizing the following loss. Please refer to Appendix D for detailed proofs.\nLoss(\u03c0 (i) \u03b8 ) = \u2212E\u03c4\u223cDp\u0302(z = i|\u03c4) \u2211 (s,a)\u2208\u03c4 log \u03c0 (i) \u03b8 (a|s) exp ( 1 \u03bb A\u00b5(s, a) ) . (9)\nCompared with the previous offline RL algorithm AWR (Peng et al., 2019) that learns a single policy, SORL learns a set of diverse policies by assigning different weights to trajectories in the dataset. SORL leverages the estimated posterior distribution calculated in step one to force different policies to focus on different clusters of trajectories that possess distinct behavior patterns.\nThe two-step SORL algorithm is illustrated in Algorithm 1. SORL leverages the EM-based style clustering to extract distinct styles from the dataset. Furthermore, it takes into account both diversity and quality improvements to perform advantage-weighted learning. As a result, the SORL algorithm extracts from the heterogeneous dataset a collection of diverse policies with high quality."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this experimental section, we aim to address the following questions: (1) Can SORL derive highperforming policies that exhibit diverse behaviors from an offline heterogeneous dataset originating from various sources? (2) How does SORL compare to prior methods that focus on learning diverse behaviors from an offline heterogeneous dataset? (3) Is SORL suitable for complex, real-world tasks, especially those involving large-scale datasets collected from human users?\nOur experiments encompass a range of tasks and offline datasets. For each experiment, we train a set of policies and evaluate them based on three criteria: quality, diversity, and consistency. These criteria respectively reflect the performance, behavioral diversity, and whether the learned diversity is inherent in the offline dataset, addressing the question (1). To answer question (2), we compare SORL against two prior methods: Off-RLPMM (Wu et al., 2019a) and InfoGAIL (Li et al., 2017). This comparison spans three sets of experiments, each with increasing levels of difficulty.\nThe first experiment involves a grid-shooting game where the dataset is manually collected from two distinct winning strategies. This experiment allows for a detailed analysis of the learned policies as we have full control over the game environment and access to the ground truth styles in the dataset. The second experiment features a set of Atari games. The datasets for these games are recorded from human players in a controlled, semi-frame-by-frame manner (Zhang et al., 2019a). As a result, the quality of the dataset is relatively high. This experiment assesses the performance of the methods when learning from heterogeneous datasets with minimal interference from inconsistent data quality. The final experiment centers on the popular basketball video game \u201dDunk City Dynasty\u201d (FuxiRL, 2023). Here, datasets are recorded directly from online players, leading to greater diversity and\nAlgorithm 1 Stylized Offline RL (SORL) 1: Input: offline heterogeneous dataset D, number of policies m 2: Output: learned diverse policies \u03c0(1)\u03b8 \u00b7 \u00b7 \u00b7\u03c0 (m) \u03b8\n3: Initialize policies \u03c0(1)\u03b8 \u00b7 \u00b7 \u00b7\u03c0 (m) \u03b8 and \u00b5 (1) \u00b7 \u00b7 \u00b7\u00b5(m) 4: # EM-based style clustering 5: while not converged do 6: # E-step 7: Calculate the posterior probability of styles p\u0302(z|\u03c4) with Equation 2 8: # M-step 9: Calculate the loss function Loss({\u00b5(i)}) defined in Equation 4\n10: Update {\u00b5(i)} by taking one step of gradient descent with respect to Loss({\u00b5(i)}) 11: end while 12: # Advantage-weighted style learning 13: Calculate the posterior probability of styles p\u0302(z|\u03c4) with Equation 2 14: while not converged do 15: Calculate the loss function Loss({\u03c0(i)\u03b8 }) defined in Equation 9 16: Update {\u03c0(i)\u03b8 } by taking one step of gradient descent with respect to Loss({\u03c0 (i) \u03b8 }) 17: end while\nconsiderable variations in data quality. This experiment evaluates the effectiveness of the methods in handling complex, real-world tasks using datasets collected from a diverse group of humans, addressing the question (3).\nIn the following, we introduce the baseline methods and the evaluation criteria, and then present the results of the three experiments."
        },
        {
            "heading": "5.1 BASELINES",
            "text": "Off-RLPMM: RLPMM (Wu et al., 2019a) adopts a similar EM framework, but it is not directly applicable in our offline setting because it requires additional online interaction to train a classifier for labeling all the trajectories in the M-step. We implement an offline version of RLPMM (OffRLPMM) that adopts a similar M-step as SORL, except that Off-RLPMM partitions the dataset into disjoint subsets instead of using SORL\u2019s weighting scheme.\nInfoGAIL: InfoGAIL (Li et al., 2017) studies imitation learning from multimodal datasets. Different from the EM stage of SORL, InfoGAIL infers the latent style of trajectories by maximizing the mutual information between the latent codes and trajectories.\nIn both baselines, we employ Behavior Cloning (BC) as the imitation learning algorithm. Additionally, in contrast to the two-stage algorithm SORL, both baselines do not have a second policy improvement stage. Detailed explanations are provided in Appendix E."
        },
        {
            "heading": "5.2 EVALUATION CRITERIA",
            "text": "We evaluate the learned policy with the following three criteria.\nQuality: The quality of a policy is assessed by the episode return of trajectories during evaluation. It reflects the performance of the policy in completing tasks.\nDiversity: Diversity is determined by the entropy of styles identified from the learned policies. With learned clustering p\u0302, each trajectory is classified into a style with the highest probability. Diversity is then calculated as the entropy of the distribution of styles over the whole dataset. In conjunction with the later-introduced consistency metric, a higher entropy suggests that policies exhibit diverse, distinguishable behaviors.\nppolularity(z = i) = 1 |D| \u2211 \u03c4\u2208D I(argmax j {p\u0302(z = j|\u03c4)} = i)\nDiversity := Entropy(ppopularity(\u00b7)) (10)\nConsistency: The consistency measures the likelihood that the learned policy generates actions corresponding to the specific cluster it is associated with within the dataset. Evaluating consistency is crucial to ensure that the behaviors we have learned are inherent in the heterogeneous datasets, rather than being unknown out-of-distribution actions, which could also artificially inflate diversity scores."
        },
        {
            "heading": "5.3 DIDACTIC EXAMPLE: GRID SHOOTING",
            "text": "In the grid shooting game, a player navigates in a 9x9 grid world, controlling a character that engages in combat against an AI opponent. Both the player and the AI have the ability to move in four directions within the grid, shoot at each other, and gather stars that randomly appear in one of the grid cells. Rewards and termination conditions are distributed based on the subsequent events:\n\u2022 Shooting: If the player and the AI opponent are positioned in the same row or column, they have the opportunity to shoot at each other. A successful shot ends the game, with the shooter being declared the winner and receiving a reward of 10.\n\u2022 Star Collection: A star is randomly placed in a grid cell, and its location is known to both the player and the AI opponent. Whichever party reaches the location of the star earns a reward of 1. After the star is collected, it reappears randomly in another grid cell. If no successful shooting occurs, the game terminates at the 100th step, and the participant with the higher accumulated rewards is announced as the winner.\nIn the Grid Shooting environment, there are two distinct winning strategies or playing styles. The first strategy emphasizes shooting the enemy, while the second focuses on collecting stars and avoiding direct confrontations with the enemy, as depicted in Figure 1. We constructed our dataset by combining trajectories from both playing styles. Using this dataset, we trained two policies with SORL and compared them with two baseline methods. The rewards earned for each event and the final winning rates are detailed in Table 1.\nFrom the table, it is evident that SORL successfully captures the core difference between the two playing styles. This is reflected in the distinct reward distributions for shooting and star collecting across the two learned policies. Furthermore, the winning rates of the policies are promising, underscoring their robust performance. In contrast, while Off-RLPMM also captures the two playing styles, the winning rates of its policies are lower than those of SORL. InfoGAIL, on the other hand,\nyields a competitive winning rate but fails to differentiate between the styles of the policies. Figure 1 shows example trajectories for the two learned policies of SORL. More detailed quantitative results including the quality, diversity and consistency is presented in Appendix A.2."
        },
        {
            "heading": "5.4 ATARI GAMES",
            "text": "In this experiment, we focus on six Atari games, including SpaceInvaders, MsPacman, MontezumaRevenge, Enduro, Riverraid, and Frostbite. For each game, We train three policies using the dataset provided by Atari Head (Zhang et al., 2019a), a large-scale dataset of human players. The dataset is recorded in a controllable semi-frame-by-frame manner, ensuring high data quality. The experimental results, which include metrics on quality, diversity, and consistency, are presented in Table 2. A visual representation of these results, using a radar plot, is available in Figure 2.\nFrom Table 2, we see results consistent with those in the grid shooting game. SORL outperforms the baseline methods, attaining the highest scores in both diversity and consistency for all games, and\nsecures the top quality score in four out of the six games. While Off-RLPMM does exhibit some diversity, its policy quality is weaker. Conversely, InfoGAIL achieves competitive quality scores but struggles to learn diverse policies. Visualizations of the stylized policies are in Appendix H."
        },
        {
            "heading": "5.5 VIDEO GAME APPLICATION",
            "text": "Dunk City Dynasty\u201d (FuxiRL, 2023) is an online mobile game where players control a character to play in a 3v3 basketball match. The game presents a formidable challenge to existing algorithms due to its high-dimensional state and action spaces. For this experiment, we collect a dataset of over 100,000 steps, directly from online players. Compared to the other two experiments, the dataset used here has notable variations in both behavioral styles and data quality. Same as Atari games, we train three policies with SORL and compare them with two baseline methods. The experimental results on quality, diversity, and consistency are presented in Table 3. The results highlight that SORL consistently outperforms in all three evaluation metrics, underscoring its robustness and adaptability in handling complex, real-world tasks, especially when working with large-scale datasets from diverse human players.\nWe observed distinct shooting range preferences among the three policies. For example, policy 1 tends to favor short-range shots, while policy 3 is inclined towards long-range shots. The preferred shooting areas for each policy are visualized in Figure 3. Additional screenshots showcasing typical behaviors of these policies can be found in Figure 5 in the Appendix B. To better illustrate the behaviors exhibited by the learned policies, we have included gameplay videos in the supplementary materials.."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we explored the extraction of diverse and high-quality behaviors from offline heterogeneous datasets. These datasets, sourced from multiple origins, inherently possess a multimodal data distribution with inconsistent data quality. Such a setting poses significant challenges to existing methods, which can be affected by low-quality data or lack the control to differentiate the behavioral while learning. To address these challenges, we introduced the Stylized Offline Reinforcement Learning (SORL) framework. SORL employs an EM-based style clustering combined with advantage-weighted policy learning. This design not only optimizes the performance of the policies but also preserves the inherent behavioral diversity found in heterogeneous datasets. Through extensive experiments, we compared SORL with two prior methods across various tasks and heterogeneous offline datasets. Our results underscore SORL\u2019s superior capability in extracting behaviors that are both high-quality and diverse. Our future work aims to offer a more flexible formulation of policy behavior, potentially allowing shared behaviors between policies, making it more relevant for real-world applications. Furthermore, we plan to integrate adaptive task learning using the diverse policies derived from SORL, enabling dynamic switching in different task scenarios."
        },
        {
            "heading": "7 REPRODUCIBILITY STATEMENT",
            "text": "The source code for the SORL algorithm and the baselines used in this study are included in the supplementary materials. The proof sketch can be found in Section 4.2, and a comprehensive proof is provided in the Appendix 4.2. For the Grid Shooting environment, the data processing steps involve using the state and action directly from the environment output. As for the Atari Head dataset, the processing steps follow the methodology outlined in the original package."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported in part by the National Natural Science Foundation of China (62176135)."
        },
        {
            "heading": "A GRID SHOOTING",
            "text": "A.1 ENVIRONMENT DETAILS\nThe state space dimension is 56. The action space is a discrete action space with 9 actions. The opponent is an agent with a fixed strategy that move randomly, and shoot with a probability once the shooting action cools down.\nA.2 QUANTITATIVE EXPERIMENT RESULTS OF THE GRID SHOOTING ENVIRONMENT\nFigure 4: The radar plot of quality, diversity and consistency in the Grid Shooting environment.\nQuality Diversity Consistency SORL 55.1\u00b1 4.6% 0.60\u00b1 0.07 86.4\u00b1 0.4% Off-RLPMM 40.4\u00b1 0.7% 0.68\u00b1 0.01 90.5\u00b1 0.0% InfoGAIL 55.3\u00b1 2.1% 0.00\u00b1 0.00 84.0\u00b1 0.3%\nTable 4: The experiment results of quality, diversity and consistency in the Grid Shooting environment."
        },
        {
            "heading": "B \u201dDUNK CITY DYNASTY\u201d",
            "text": "B.1 ENVIRONMENT DETAILS\nThe state space dimension is 468, including global state, allies\u2019 states and enemies\u2019 states. The action space is a discrete space with 52 actions. The opponent in evaluation is an agent with moderate strength, that is learned by vanilla Behavioral Cloning for similar training steps (100,000 steps).\nB.2 SCREENSHOTS IN \u201dDUNK CITY DYNASTY\u201d\nTable 3 presents three metrics in the experiment, illustrating SORL\u2019s ability to learn diverse policies while achieving satisfactory performance. Figure 5 shows screenshots from the videos showcasing each style\u2019s self-playing behavior. Supplementary materials include videos that provide additional visual demonstrations. For instance, style 1 demonstrates a preference for shooting in the short range, while style 3 favors long-range shots. The videos reveal that the proportion of goals scored inside the restricted area is 83.3%, 66.7%, and 33.3% for styles 1, 2 and 3, respectively. Furthermore, the proportion of two-point shots is 91.6%, 88.9% and 66.7%. Figure 4 depicts the probabilities of shooting in different regions. In Figure 6, (a) and (c) showcase typical shots of Style 1 and 3. Panel (a) demonstrates shots within the restricted zone, while panel (c) displays long- range shots. Style 2 is characterized by ball possession and running across the arena. The videos indicate that Style 2 has the fewest number of passes (41, 29, 39, respectively), and the lowest proportion of goals scored directly after a pass (27.7%, 22.2%, 44.4%).\nB.3 THE PLOT OF SHOOTING POSITIONS\nWe plot the shooting positions in Figure 6 based on the gameplay data. The shooting positions in the plot corresponds to the distribution in Figure 3."
        },
        {
            "heading": "C ABLATION STUDY",
            "text": "In this section, we performed ablation experiments on the advantage-weighted policy learning. Based on the algorithm description provided in Section 4, the SORL algorithm can be divided into two parts. Therefore, when the advantage-weighted policy learning is excluded, SORL reduces to the EM-based style clustering. Table 5 presents the results, indicating that the consistency and diversity metrics show similar performance, while the quality metric improves as a result of the advantage-weighted style learning."
        },
        {
            "heading": "D PROOF DETAILS",
            "text": "We provide detailed proof of solving the constrained optimization problem in Section 4.2. The original problem is as follows.\n\u2200i \u2208 [m], \u03c0(i) = argmaxEs\u223cd \u00b5(i) (s)Ea\u223c\u03c0(i)(\u00b7|s)A\u00b5 (i) (s, a)\ns.t. Es\u223cd \u00b5(i) (s)DKL(\u03c0 (i)(\u00b7|s)||\u00b5(i)(\u00b7|s)) \u2264 \u03f5,\u222b\na\n\u03c0(i)(a|s)da = 1, \u2200s.\n(11)\nIn advantage-weighted style learning, we approximate A\u00b5 (i)\n(s, a) by A\u00b5(s, a), where \u00b5 represents the policy distribution of the entire dataset. This approximation is made because A\u00b5(s, a) often has higher quality than A\u00b5 (i) . Subsequently, we calculate the Lagrangian of the optimization problem:\nL(\u03c0(i), \u03bb, \u03b1) =Es\u223cd \u00b5(i) (s)\n[ Ea\u223c\u03c0(i)(\u00b7|s)A\u00b5(s, a)\n+ \u03bb(\u03f5\u2212DKL(\u03c0(i)(\u00b7|s)||\u00b5(i)(\u00b7|s))) ]\n+ \u222b s \u03b1s(1\u2212 \u222b a \u03c0(i)(a|s)da)\n(12)\nDifferentiating on \u03c0(i),\n\u2202L\n\u2202\u03c0(i)(a|s) = d\u00b5(i)(s)[A\n\u00b5(s, a)\u2212 \u03bb log \u03c0(i)(a|s) + \u03bb log\u00b5(i)(a|s)\u2212 \u03bb]\u2212 \u03b1s (13)\nWe set \u2202L \u03c0(i)(a|s) to 0, and can get the closed-form solution \u03c0 (i)\u2217(a|s) = 1 Z(i)(s) \u00b5(i)(a|s) exp( 1\u03bbA \u00b5(s, a)), where the normalization term Z(i)(s) = exp( 1d \u00b5(i) (s) \u03b1s \u03bb + 1).\nFinally, we project \u03c0(i)\u2217 to \u03c0(i)\u03b8 paramterized by \u03b8 by minimizing the KL divergence between them. D(i) denotes the trajectories corresponding to style i in the dataset, which is unknown. Hence, we\nrewrite the expression by incorporating the probability of each trajectory being the i-th style.\nargmin \u03b8\nEs\u223cD(i) [DKL(\u03c0(i)\u2217(\u00b7|s)||\u03c0 (i) \u03b8 (\u00b7|s)]\n= argmin \u03b8\nEs\u223cD(i) [ \u222b\na\n(\u03c0(i)\u2217(a|s) log \u03c0(i)\u2217(a|s)\u2212 \u03c0(i)\u2217(a|s) log \u03c0(i)\u03b8 (a|s)) ]\n=argmin \u03b8\nEs\u223cD(i) [ \u222b\na\n(\u2212\u03c0(i)\u2217(a|s) log \u03c0(i)\u03b8 (a|s)) ]\n=argmin \u03b8\nEs\u223cD(i) [ \u222b\na\n(\u2212 1 Z(i)(s) \u00b5(i)(a|s) exp( 1 \u03bb A\u00b5(s, a)) log \u03c0 (i) \u03b8 (a|s)) ] =argmin\n\u03b8 \u2212Es\u223cD(i)Ea\u223cD(i) log \u03c0 (i) \u03b8 (a|s)\n1\nZ(i)(s) exp(\n1 \u03bb A\u00b5(s, a))\n= argmin \u03b8\n\u2212E\u03c4\u223cD(i) log \u03c0 (i) \u03b8 (a|s)\n1\nZ(i)(s) exp(\n1 \u03bb A\u00b5(s, a))\n= argmin \u03b8\n\u2212E\u03c4\u223cDp\u0302(z = i|\u03c4) log \u03c0(i)\u03b8 (a|s) 1\nZ(i)(s) exp(\n1 \u03bb A\u00b5(s, a))\n(14)\nSimilar to AWR (Peng et al., 2019) and other prior work (Neumann & Peters, 2008; Siegel et al., 2020; Wang et al., 2018), we neglect the per-state normalizing constant Z(i)(s). The policy update can be expressed as follows:\nargmin \u03b8\n\u2212E\u03c4\u223cDp\u0302(z = i|\u03c4) log \u03c0(i)\u03b8 (a|s) exp( 1\n\u03bb A\u00b5(s, a)) (15)\nThe original problem 11 has solution because it satisfies the Linear Indepence Constraint Qualification (LICQ)."
        },
        {
            "heading": "E EXPERIMENT DETAILS",
            "text": "Network structure We construct the network based on the default network of the relative task according to the codebase we use. The network of Grid Shooting and Dunk City Dynasty is a 3-layer MLP, and the network of Atari environments has three convolution layers and two linear layers.\nBesides, in order to ensure balanced learning among all the styles, we share the main network and use a LoRA module to discriminate different styles. LoRA (Hu et al., 2021) is a widely used network structure, that substitutes the original matrix of the linear layer by a matrix W and a low-\nrank multiplication of two other matrices Ai and Bi. In our setting as shown in Figure 7, the matrix W is shared among all styles of policies, while Ai, Bi varies. The input dimension d and output dimension k depends on the linear layer, and the rank r is set to 9 in this work.\nNumber of styles In Grid Shooting, we use 2 styles because we want to recover the two real styles of behaviors in the dataset. In other environments, we learn 3 styles from the dataset.\nHyperparameters and other details In Grid Shooting, we use the batch size 128, offline dataset size 20000 and the number of epochs 10. In Atari environments, we use the batch size of 32 \u00d7 5 where we sample 32 (s, a) pairs for 5 times, with the number of epochs 30. In Dunk City Dynasty, we use the batch size of around 300 and 100, 000 steps for the quality, diversity, and consistency metrics. We use 900, 000 steps for generating the playing videos of different styles.\nWe re-implement the InfoGAIL with continuous latent variables in all the environments. The 3 styles are extracted by applying (1, 0, 0), (0, 1, 0), (0, 0, 1) as the latent variable to the policy model and get the 3 different policies.\nThe Grid Shooting and Atari results in the tables show the mean and standard deviation among 3 random seeds."
        },
        {
            "heading": "F ADDITIONAL METRICS EVALUATING DIVERSITY",
            "text": "In the main text, we use the diversity metric of popularity, which is a dataset-related metric that evaluates diversity. However, the definition of diversity, especially in the offline setting, is not unique. We provide additional diversity metrics in this part. The definitions of metrics are as follows.\n1. Skill metric measures the dissimilarity of the skill set. The skill set is defined as the vector of rewards obtained by each skill, e.g., skill = (rshoot, rstar) in the grid shooting environment. And the dissimilarity between two skill sets skill1, skill2 is defined as the value of their cross product dskill(skill1, skill2) = ||skill1 \u00d7 skill2||. The dissimilarity metric is obtained by first sampling trajectories with different policies, and calculating the average dissimilarity between pair-wise skill sets, i.e., Diversityskill = Eskilli,skilljdskill(skilli, skillj) where skilli, skillj are sampled from different styles.\n2. OT (optimal transport) metric is based on the optimal transport distance (also known as Wasserstein distance). The similar idea is used in a recent work in imitation learning (Luo et al., 2023). When calculating the trajectory-level OT distance, we align steps with neighbor states together and sum up to the overall distance, i.e., dOT (\u03c41, \u03c42) = argmin\u00b5\u2208M \u2211T t=1 \u2211T t\u2032=1 ||s1t \u2212 s2t\u2032 ||\u00b5t,t\u2032 where M = {\u00b5 \u2208 RT\u00d7T , \u00b51 = 1 T 1, \u00b5\nT1 = 1 T 1}. And the diversity metric is calculated by averaging all trajectory pairs \u03c4\ni, \u03c4 j from different policies DiversityOT = E\u03c4 i,\u03c4jdOT (\u03c4 i, \u03c4 j). We employ a normalization technique to enhance the interpretability of the OT metric. The normalization is achieved by applying the formula normalize(DiversityOT ) = DiversityOT\u2212\u03b1\n\u03b1 , where \u03b1 is the OT metric on trajectories sampled from the same policy.\n3. Discrimination metric represents how different policies can be discriminated by a neural network. After collecting trajectories of different policies, we first train a neural network to predict the policy index, and the discrimination metric is just the evaluation accuracy.\nBased on the results of the additional diversity metrics presented in Table 6, we can conclude that the SORL algorithm is capable of obtaining diverse policies."
        },
        {
            "heading": "G COMPARISON WITH OFFLINE RL BASELINES",
            "text": "We present the results of standard offline RL methods in Table 7, which do not focus on diverse policy learning."
        },
        {
            "heading": "H ATARI ENVIRONMENT VISUALIZATION",
            "text": "H.1 SPACEINVADERS\nIn SpaceInvaders as Figure 8, the shooter can move horizontally, and there are three stationary bunkers positioned above the shooter. The shooter can shoot from bottom to top, in order to destroy the aliens above to get scores. During playing, there are random attacks from top bottom that can destroy the shooter. However, staying under the bunker can prevent those attacks. The bunker can also be destroyed by continuously attacking it. In addition, the aliens slowly swing left and right together all the time. In the game, players moves horizontally to shoot from a correct position with an alien above, and avoid being attacked.\nTo evaluate the diversity of learned policies, we calculate the frequency of the shooter\u2019s position over 30,000+ steps for each style. Table 8 and Figure 9 demonstrates that different styles of policies tend to prefer different positions.\nTo better understand the style of learned clusters, we analyze the game mechanics which partially explains the preferences of styles. In SpaceInvaders, a shortcut of getting high scores is to staying close to the left of the Bunker 1 (and also staying close to the right of the Bunker 2) to destroy all the aliens in the leftmost column (and the rightmost column), because there are fewer attacks from top to bottom and is close to the bunker, where destroying the aliens in the leftmost/rightmost column provides a large amount to scores. As a result, style 1 and style 3 prefers staying in the leftmost, and style 1 and style 2 prefers staying in the leftmost.\nH.2 MSPACMAN\nIn MsPacman as shown in Figure 10, the map and starting position are fixed. To evaluate the diversity of learned policies, we calculate the frequency of the beginning trajectory until the first corner for each style, based on 30 trajectories per style. There are a total of six possible corners. Table 9 displays the different preferred corners for each style.\nBesides, we analyze the visualizations of the dataset trajectories. Atari datasets in our experiment are collected by three players, which exhibit some diversity in terms of policy variations. As shown the following table 10, in the game MsPacman, different human players (named J, K and R) have varying preferences for the first visited corners. Additionally, even within a single player\u2019s gameplay, there can be variations in visitation patterns."
        },
        {
            "heading": "I SENSITIVITY ANALYSIS ON THE NUMBER OF POLICIES",
            "text": "Sensitivity analysis We have performed experiments to evaluate the diversity, quality, and consistency of SORL under different values of m, where m is the number of policies. The performance of SORL in Table 11 remains similar across different m values, indicating that the algorithm is not highly sensitive to the hyperparameter m.\nThe reward distribution of learned policies The reward distribution of the learned policies is presented in Table 12. When taking larger m value, the learned policies still differs a lot from each other. Some of them focus on shooting enemies, while others prefer collecting stars."
        },
        {
            "heading": "J DISCUSSION ON POSTERIOR APPROXIMATION",
            "text": "In the main text, we approximate the true posterior p\u0302(z = i|\u03c4) \u221d \u222b (s,a)\u2208\u03c4 \u00b5\n(i)(a|s) with p\u0302(z = i|\u03c4) \u221d \u2211 (s,a)\u2208\u03c4 \u00b5 (i)(a|s), because the consecutive multiplication leads to numerical instability.\nWe plot the distribution of p\u0302(z = 1|\u03c4) for all trajectories in the dataset in Figure 11. Figure 11a uses consecutive multiplication, while Figure 11b is the average of all steps. The results show that using\nconsecutive multiplication causes a trend of polarization in clustering, which causes unexpected numerical instability. The experiment was conducted in the grid shooting environment with the number of clusters set to m = 2."
        }
    ],
    "title": "STYLIZED OFFLINE REINFORCEMENT LEARNING: EXTRACTING DIVERSE HIGH-QUALITY BEHAVIORS FROM HETEROGENEOUS DATASETS",
    "year": 2024
}