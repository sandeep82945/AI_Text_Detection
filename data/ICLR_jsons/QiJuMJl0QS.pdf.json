{
    "abstractText": "We tackle the problem of meta-learning across heterogenous tasks. This problem seeks to extract and generalize transferable meta-knowledge through learning from tasks sampled from a multi-modal task distribution. The extracted metaknowledge can be used to create predictors for new tasks using a small number of labeled samples. Most meta-learning methods assume a homogeneous task distribution, thus limiting their generalization capacity when handling multi-modal task distributions. Recent work has shown that the generalization of meta-learning depends on the similarity of tasks in the training distribution, and this has led to many clustering approaches that aim to detect homogeneous clusters of tasks. However, these methods suffer from a significant increase in parameter complexity. To overcome this weakness, we propose a new heterogeneous meta-learning strategy that efficiently captures the multi-modality of the task distribution via modulating the routing between convolution channels in the network, instead of directly modulating the network weights. This new mechanism can be cast as a permutation learning problem. We further introduce a novel neural permutation layer based on the classical Benes routing network, which has sub-quadratic parameter complexity in the total number of channels, as compared to the quadratic complexity of the state-of-the-art Gumbel-Sinkhorn layer. We demonstrate our approach on various multi-modal meta-learning benchmarks, showing that our framework outperforms previous methods in both generalization accuracy and convergence speed.",
    "authors": [],
    "id": "SP:56019ac0c63ae90d3d9b3af1179c4b33af5a709a",
    "references": [
        {
            "authors": [
                "Antreas Antoniou",
                "Harrison Edwards",
                "Amos Storkey"
            ],
            "title": "How to train your MAML",
            "venue": "arXiv preprint arXiv:1810.09502,",
            "year": 2018
        },
        {
            "authors": [
                "S\u00e9bastien M R Arnold",
                "Praateek Mahajan",
                "Debajyoti Datta",
                "Ian Bunner",
                "Konstantinos"
            ],
            "title": "Saitas Zarkias. learn2learn: A library for Meta-Learning research, August 2020",
            "venue": "URL http: //arxiv.org/abs/2008.12284",
            "year": 2008
        },
        {
            "authors": [
                "V\u00e1clav E Bene\u0161"
            ],
            "title": "Permutation groups, complexes, and rearrangeable connecting networks",
            "venue": "Bell System Technical Journal,",
            "year": 1964
        },
        {
            "authors": [
                "Jicang Cai",
                "Saeed Vahidian",
                "Weijia Wang",
                "Mohsen Joneidi",
                "Bill Lin"
            ],
            "title": "Neural routing in meta learning",
            "venue": "arXiv preprint arXiv:2210.07932,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Collier",
                "Efi Kokiopoulou",
                "Andrea Gesmundo",
                "Jesse Berent"
            ],
            "title": "Routing networks with cotraining for continual learning",
            "venue": "arXiv preprint,",
            "year": 2020
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In Proc. ICML,",
            "year": 2017
        },
        {
            "authors": [
                "Karlis Freivalds",
                "Em\u0131\u0304ls Ozoli\u0146\u0161",
                "Agris \u0160ostaks"
            ],
            "title": "Neural shuffle-exchange networks-sequence processing in o (n log n) time",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with Gumbel-Softmax",
            "venue": "arXiv preprint arXiv:1611.01144,",
            "year": 2016
        },
        {
            "authors": [
                "Brenden M Lake",
                "Ruslan Salakhutdinov",
                "Joshua B Tenenbaum"
            ],
            "title": "Human-level concept learning through probabilistic program induction",
            "year": 2015
        },
        {
            "authors": [
                "Wei-Hong Li",
                "Xialei Liu",
                "Hakan Bilen"
            ],
            "title": "Cross-domain few-shot learning with task-specific adapters",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yanbin Liu",
                "Juho Lee",
                "Linchao Zhu",
                "Ling Chen",
                "Humphrey Shi",
                "Yi Yang"
            ],
            "title": "A multi-mode modulator for multi-domain few-shot classification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jiancheng Lyu",
                "Shuai Zhang",
                "Yingyong Qi",
                "Jack Xin"
            ],
            "title": "AutoShuffleNet: Learning permutation matrices via an exact Lipschitz continuous penalty in deep convolutional neural networks",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "G. Mena",
                "D. Belanger",
                "S. Linderman",
                "J. Snoek"
            ],
            "title": "Learning latent permutations with GumbelSinkhorn networks",
            "venue": "In Proc. ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Massimiliano Patacchiola",
                "Jack Turner",
                "Elliot J Crowley",
                "Michael O\u2019Boyle",
                "Amos J Storkey"
            ],
            "title": "Bayesian meta-learning for the few-shot setting via deep kernels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Chelsea Finn",
                "Sham M Kakade",
                "Sergey Levine"
            ],
            "title": "Meta-learning with implicit gradients",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sachin Ravi",
                "Hugo Larochelle"
            ],
            "title": "Optimization as a model for few-shot learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel"
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Jason Weston",
                "Rob Fergus"
            ],
            "title": "End-to-end memory networks",
            "venue": "In NeurIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Zhuo Sun",
                "Jijie Wu",
                "Xiaoxu Li",
                "Wenming Yang",
                "Jing-Hao Xue"
            ],
            "title": "Amortized bayesian prototype meta-learning: A new probabilistic meta-learning approach to few-shot image classification",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Eleni Triantafillou",
                "Tyler Zhu",
                "Vincent Dumoulin",
                "Pascal Lamblin",
                "Utku Evci",
                "Kelvin Xu",
                "Ross Goroshin",
                "Carles Gelada",
                "Kevin Swersky",
                "Pierre-Antoine Manzagol",
                "Hugo Larochelle"
            ],
            "title": "Metadataset: A dataset of datasets for learning to learn from few examples",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Eleni Triantafillou",
                "Hugo Larochelle",
                "Richard Zemel",
                "Vincent Dumoulin"
            ],
            "title": "Learning a universal template for few-shot dataset generalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Risto Vuorio",
                "Shao-Hua Sun",
                "Hexiang Hu",
                "Joseph J Lim"
            ],
            "title": "Multimodal model-agnostic metalearning via task-aware modulation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Huaxiu Yao",
                "Ying Wei",
                "Junzhou Huang",
                "Zhenhui Li"
            ],
            "title": "Hierarchically structured meta-learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "J. Yoon",
                "T. Kim",
                "O. Dia",
                "S. Kim",
                "Y. Bengio",
                "S. Ahn"
            ],
            "title": "Bayesian model-agnostic meta-learning",
            "venue": "In Proc. NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Xiangyu Zhang",
                "Xinyu Zhou",
                "Mengxiao Lin",
                "Jian Sun"
            ],
            "title": "ShuffleNet: An extremely efficient convolutional neural network for mobile devices",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "P. Zhou",
                "Y. Zou",
                "X. Yuan J. Feng",
                "C. Xiong",
                "S. Hoi"
            ],
            "title": "Task similarity aware meta learning: Theory-inspired improvement on MAML",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Snell"
            ],
            "title": "2017) aim to learn an embedding function fProto to encode each input into a feature vector. A class prototype of some class c is the mean vector of the embedded support data samples",
            "year": 2017
        },
        {
            "authors": [
                "Jang"
            ],
            "title": "2016) and GS(X) converges almost surely to samples of the Gumbel-Matching distribution as the temperature parameter \u03c4 anneals to 0. E BEN\u011aS NETWORK ROUTING ALGORITHM The Ben\u011bs network (Bene\u0161",
            "year": 1964
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Few-shot learning (FSL) is a challenging problem where the goal is to learn new concepts with only a small number of labeled samples, similar to how humans learn new things by incorporating prior knowledge and context. One promising approach to tackle FSL problems is meta-learning, which learns to extract and generalize transferable meta-knowledge from a distribution of tasks and quickly adapt it to unseen tasks. Many meta-learning methods (Yoon et al., 2018; Antoniou et al., 2018; Rajeswaran et al., 2019) are built upon the model-agnostic meta learning (MAML) framework (Finn et al., 2017). The MAML method learns a single set of model parameters for any arbitrary network architecture, and then fine-tunes it to deliver high performance on unseen tasks. However, most MAML variants assume a homogeneous task distribution in which all tasks originate from the same concept domain, and assume that transferable knowledge are globally shared among all tasks (Vuorio et al., 2019). These assumptions constrain the generalization capacity of these meta-learners when handling multi-modal task distributions, for which the task-specific optimal parameters could diverge significantly from one another. For example, if the task distribution consists of modes that are far apart (e.g., animal and vehicle recognition tasks), it would be impossible to find an initialization that is simultaneously close to all modes.\nRecent work has demonstrated that the generalization of MAML and, by extension, its many variants, is indeed related to the similarity of tasks in the training distribution (Zhou et al., 2021). This perspective aligns with many previous clustering approaches that aim to detect homogeneous clusters of tasks which MAML-based learners can be effectively applied (Zhou et al., 2021; Yao et al., 2019; Vuorio et al., 2019). Zhou et al. (2021) seeks to learn an ensemble of initializations, each of which is set to represent a cluster of task (i.e., a mode in the task distribution). This is achieved via augmenting the MAML loss function with an assignment step. The cluster assignment heuris-\ntic, however, is conditioned on the single-mode, vanilla MAML initialization and thus is likely not optimal in a multi-modal setting.\nAlternatively, Yao et al. (2019) and Vuorio et al. (2019) propose to implicitly cluster tasks using the embedding vectors of their few-shot data. In particular, Vuorio et al. (2019) applies a modulation network on the learned task embedding to modulate the meta-initialization of the predictor model, yielding the task-specific parameters. Yao et al. (2019) adopts a similar idea, but further imposes explicit hierarchical structure on the task space through jointly optimizing several task cluster centroids. The estimated parameter modulation is then applied to the nearest centroid based on their embedding distance. While both methods are capable of addressing the task heterogeneity challenge, they suffer from significant increase in parameter complexity since their respective modulation networks must scale with the size of the predictor model (e.g., for an average convolutional architecture with millions of parameters, the modulation network is essentially a million-output map). Even when the modulation is applied layer-wise, learning to generate that many variables is still a challenging task. This thus prevents applying these tactics on larger architectures.\nTo overcome this weakness, we aim to develop a new heterogeneous meta-learning strategy that efficiently captures the multi-modality of the task distribution via modulating the routing between neurons in the network, instead of directly modulating the network weights. Our approach is partially inspired by the ShuffleNet architecture (Zhang et al., 2018), which employs convolutional channel shuffling to encode a highly expressive solution space. The phenomenal success of ShuffleNet, which achieves comparable performance to state-of-the-art models that have many-fold more parameters, suggests that adapting the routing configuration (i.e., implied by the channel shuffling order) can potentially emulate the modulation of many neurons without incurring the extra computational costs.\nThis insight motivates us to reformulate the weight modulation network in previous heterogeneous meta-learning approaches (Yao et al., 2019; Vuorio et al., 2019) as a routing modulation network that controls task-specific shuffling of convolution channels. In particular, given a task embedding vector, our modulation network learns to generate a permutation matrix which simulates the channel shuffling operator when multiplied with the output of a convolution layer. To model this permutation network, one can adopt the Gumbel-Sinkhorn layer (Mena et al., 2018), which differentiably transforms general square matrices to discrete permutation matrices in the limit of a temperature parameter. The permutation network can be optimized via learning a mapping f : Rz \u2192 RC2 , where z and C are respectively the task embedding dimension and the number of convolutional channels.\nHowever, accurately learning a denseC\u00d7C matrix given from limited training data can be challenging, especially for large convolutional networks. To overcome this, we propose an even more compact formulation of the permutation module based on the classical Benes\u030c routing network (Benes\u030c, 1964), which can emulate any C-permutation using at most C log2 C binary switches that pairwise permute adjacent indices. Finally, to enable end-to-end learning of this compact permutation network, we approximate the discrete switches by applying the same Gumbel-softmax transformation in Mena et al. (2018) to 2\u00d7 2 general matrices. The main contributions of this paper are:\n1. We develop a more efficient heterogeneous meta-learning framework that estimates the different modalities in the task distribution via modulating the network routing configurations. This modulation operator takes the form of a permutation network that performs channel shuffling based on the few shot training data of a task. Our meta routing modulation (MRM) framework is presented in Section 3.2.\n2. We propose a compact formulation of the above permutation network based on a continuous relaxation of the classical Bene\u030cs network (Benes\u030c, 1964), which we call the Gumbel-Bene\u030cs layer (Section 3.3). The Gumbel-Bene\u030cs layer trains efficiently with limited data and scales better in the number of convolution channels than previous state-of-the-art baselines. Our approach is the first to draw a connection between the classical Bene\u030cs network and architecture modulation for meta learning.\n3. Our framework outperforms existing methods in terms of generalization accuracy and runtime on various multi-modal meta-learning benchmarks (Section 4)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Meta-learning. Existing meta-learning approaches can be broadly classified into three families: metric-based, model-based, and optimization-based methods. Model-based approaches (Sukhbaatar et al., 2015; Graves et al., 2014) aim to recognize the task identity from its few-shot data and use the task identity to adjust the model state accordingly. While these methods perform well on certain task domains, they require fixing the model architecture and thus are difficult to apply on arbitrary use cases. Metric-based methods (Snell et al., 2017; Vinyals et al., 2016) learn a task similarity metric (based on observed data) which can be used to perform inference on new tasks. Sun et al. (2021) and Patacchiola et al. (2020) respectively offer a Bayesian view on metric-based and model-based metalearning. Optimization-based methods (Finn et al., 2017; Yoon et al., 2018; Antoniou et al., 2018; Rajeswaran et al., 2019) learn a single model initialization that is amenable to fast adaption and can be applied to any arbitrary architecture. However, most existing metric-based and optimizationbased methods assume that a single metric model or parameter initialization is sufficient to capture the entire task distribution.\nHeterogeneous meta-learning. Heterogeneous meta-learning (HML) is an emerging area that develops meta-learning techniques that can generalize well to tasks drawn from a multi-modal distribution. The majority of existing HML approaches account for task heterogeneity via one of two approaches. Yao et al. (2019) and Zhou et al. (2021) explicitly maintain several local meta initializations (i.e., task clusters), to which observed tasks are assigned during training. The effectiveness of these methods depends on the quality of the many heuristics employed, such as the number of clusters and the distance metric used for cluster assignment. On the other hand, Yao et al. (2019); Vuorio et al. (2019); Liu et al. (2021); Triantafillou et al. (2021) adopt a modulation strategy that modify some components of a global meta initialization depending on some learned task embedding vector. While these approaches do not require expert understanding of the task distribution, the methods proposed by Yao et al. (2019); Vuorio et al. (2019); Liu et al. (2021) are expensive to learn, especially with large architectures, as the modulation vector scales with the number of parameters. Li et al. (2022) and Triantafillou et al. (2021) work around this scalability issue by localizing the modulation to several adapter components within the model architecture. This heuristic, however, does not modulate the remaining weights of the network, and thus assumes that the global model can adapt solely with these components. We instead a routing modulation model that can modulate the entire network at significantly more inexpensive cost.\nRouting neural networks. Routing neural networks or neural routing refers to a technique in neural network architecture where information is selectively passed between groups of neurons based on some learned decision rules. This can be accomplished through the use of routing algorithms or specialized routing layers in a neural network. The most common form of routing is by pruning computational paths (e.g., setting certain weights to zero), which is typically used to induce sparsity (Shazeer et al., 2017) in the network for computational efficiency, or to prevent catastrophic forgetting in continual learning scenarios (Collier et al., 2020). Random channel shuffling was introduced by (Zhang et al., 2018) in the context of designing compact architectures to improve model expressiveness. The ShuffleNet architecture was subsequently extended to explicitly learn the shuffling order (Lyu et al., 2020) (i.e., via optimizing for the permutation matrices that control the shuffling). Freivalds et al. (2019) proposed another differentiable neural routing formulation via a continuous approximation of the classical Ben\u0306es routing network Benes\u030c (1964). However this approach only mimics the discrete shuffling of Ben\u0306es network in spirit, offering no guarantee that the post-shuffling information will be preserve, and thus is not suitable for channel shuffling modulation. To the best of our knowledge, neural routing for meta learning has only been considered by the work of (Cai et al., 2022) in the form of heuristic pruning. In this paper, we introduce a differentiable reparameterization of the Ben\u0306es network that almost precisely models permutations, and explicitly learns to modulate channel shuffling given observed task data."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "In the meta-learning setting, we are given a task distribution T , where each task Ti \u223c T consists of a dataset Di and a learning objective Li. Similar to many other meta-learning studies, we adopt a\nsimplified setting where all tasks in T share the same learning objective L and each dataset Di = {xij ,yij}nj=1 contains n-shot supervised learning samples. The goal of meta-learning is to train a meta-modelM\u2217 that maps any task Ti to a parameter vector \u03b8i in the weight space of some predictor model G, such thatM\u2217 minimizes the expected learning loss (over random tasks):\nM\u2217 = argmin M ETi\u223cT  1 n n\u2211 j=1 L ( G(xij ; \u03b8i \u225cM(Ti)),yij ) = argmin\nM ETi\u223cT\n[ L\u2020 (M(Ti),Di)) ] , (1)\nwhere L\u2020G(\u03b8,D) denotes the averaged objective value evaluated on model architecture G with parameters \u03b8 over all data points in D. Towards this goal, the MAML framework (Finn et al., 2017) modelsM\u2217(Ti) as a fine-tuning gradient descent step with respect to Di given some base initialization \u03b8\u2217. That is,M\u2217(Ti) \u225c \u03b8\u2217 \u2212 \u03b7\u2207\u03b8L\u2020G(\u03b8\u2217,Di), where \u03b7 denotes the step size. To obtain the base initialization \u03b8\u2217, Finn et al. (2017) proposes to optimize the following loss function:\n\u03b8\u2217 = argmin \u03b8\nETi\u223cT [ L\u2020G ( \u03b8 \u2212 \u03b7\u2207\u03b8L\u2020G ( \u03b8,Dti ) ,Dvi )] , (2)\nwhere {Dti ,Dvi } denotes the train-validation split of Di. Intuitively, the goal of this loss function is to find a single initialization \u03b8\u2217 such that, given the fine-tuning step at the time of evaluating M\u2217(Ti), the adapted parameters will yield the best performance in expectation."
        },
        {
            "heading": "3.2 HETEROGENEOUS META LEARNING VIA ROUTING MODULATION",
            "text": "Motivated by our discussion above and previous works that established that learning a single initialization \u03b8\u2217 is sub-optimal when the task distribution T is multi-modal, we now introduce our heterogeneous meta-learning approach (Fig. 1). To account for task-heterogeneity, Yao et al. (2019) and Vuorio et al. (2019) apply task-specific modulation of the base parameter \u03b8\u2217 as follows:\nM\u2217(Ti) = mo(\u03b8\u2217,Di)\u2212 \u03b7\u2207\u03b8L\u2020G(mo(\u03b8\u2217,Di),Di) , (3)\nwhere mo(\u03b8\u2217,Di) abstracts the modulation operator that takes the form mo(\u03b8\u2217,Di) = \u03b8\u2217 \u2299 \u03c8(Di) in both (Yao et al., 2019) and (Vuorio et al., 2019),\u2299 denotes the point-wise multiplication operator, and \u03c8 denotes some arbitrary embedding protocol that maps a task dataset to the weight space of the predictor G. For example, Vuorio et al. (2019) models \u03c8 as an attention mechanism, whereas Yao et al. (2019) pre-trains a ProtoNet task embedding (Snell et al., 2017) for task clustering and applies a fully connected network to generate \u03c8(Di) from the cluster centroids. Both methods, however, suffer from high additional complexity since the output dimension of \u03c8 is the prohibitively large number of parameters in G. To work around this shortcoming, we instead apply the task-specific modulation tactic on the architecture routing level:\nM\u2217(Ti) = \u03b8\u2217 \u2212 \u03b7\u2207\u03b8L\u2020mo(G,Di)(\u03b8\u2217,Di) . (4)\nTo concretely describe our modulation operator, we will first re-imagine the architecture G as a sequence of neural layers {G1, G2, . . . , GM}, such that for any arbitrary input x, we can rewrite G(x) = GM \u25e6GM\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6G1(x). We assume that the output of layer Gi has dimension Ci \u00d7 di, where Ci is the number of feature channels and di is the (flattened) feature dimension of each channel. Then, our modulation operator can be succinctly applied through interleaving a sequence of routing layers {R1, R2, . . . , RM\u22121} in between the predictor layers of G; that is:\nmo(G,Di) = GM \u25e6RM\u22121 \u25e6GM\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6R1 \u25e6G1 , (5)\nwhere each routing layer takes the form of Rj(Q;Di) \u225c Pj(Di)Q for some intermediate feature Q \u2208 RCj\u00d7dj , such that Pj maps Di to a Cj \u00d7 Cj permutation matrix. Intuitively, the goal of Pj is to re-route the information flow between layer Gj and Gj+1 of the predictor net in response to the current task. It is thus appropriate for Pj to generate a permutation matrix, such that the information channels are shuffled without degrading their signals.\nTo construct such a map, we first compute the ProtoNet embedding (Snell et al., 2017) of Di and apply a convolutional layer which subsequently transforms this embedding into a general square matrix in RCj\u00d7Cj . To approximate the discrete permutation constraint, we could directly apply a Gumbel-Sinkhorn layer (Mena et al., 2018), whose output is guaranteed to converge to a permutation matrix in the limit of its temperature parameter. However, the Gumbel-Sinkhorn layer does not scale well with the total number of channels inG, and we will show in the next section that the permutation constraint can be approximated more compactly using a novel layer that we call Gumbel-Bene\u030cs. For convenience, we further let \u03c0 denote the combined weights of {P1, P2, . . . , PM\u22121}, which fully specifies our modulation network. The base predictor parameters \u03b8\u2217 and the modulation parameters \u03b3\u2217 can be jointly optimized by extending the MAML loss function (Finn et al., 2017):\n(\u03b8\u2217, \u03c0\u2217) = argmin \u03b8,\u03c0\nETi [ L\u2020mo(G,Di;\u03c0) ( \u03b8 \u2212 \u03b7\u2207\u03b8L\u2020mo(G,Di;\u03c0) ( \u03b8,Dti ) ,Dvi ) + \u03bbLProto(Di;\u03c0) ] , (6)\nwhere LProto denotes the PROTONET loss (Snell et al., 2017) and \u03bb is a trade-off hyper-parameter."
        },
        {
            "heading": "3.3 GUMBEL-BENE\u030cS ROUTING LAYER",
            "text": "The Gumbel-Sinkhorn layer is a differentiable transformation that approximately produces permutation matrices. However, in order to generate a sparse permutation matrix of size C \u00d7 C, it is necessary that the Gumbel-Sinkhorn layer also receives as input a dense C \u00d7 C matrix. Due to this requirement, each routing layer would require at least a quadratic number of parameters in terms of Cj (e.g., for the convolutional map) to generate an input to the Gumbel-Sinkhorn layer. Overall, the addition of the entire modulation component would yield an extra O( \u2211M j=1 C 2 j ) learnable parameters. Although this additional complexity would be smaller than the total number of parameters in G in most cases, it would become very expensive for larger architectures. To overcome this challenge, we adopt a classical result from Benes\u030c (1964), which shows that any permutation of C\nchannels can be emulated by exactly 2 log2 C layers of C/2 binary switches (i.e., 2 \u00d7 2 permutation matrix). Within each layer, we divide the input channels into groups of two and permute their features pairwise via multiplying with the content of the binary switches. The output of one layer is then forwarded to the next via a deterministic exchange step (Benes\u030c, 1964) (see Appendix E for a detailed description). An example routing configuration for 8 channels is shown in Fig. 2. It was shown that the Bene\u030cs network has a congestion of 1 (Benes\u030c, 1964), meaning there are no two different permutations that share the same switch configurations. As a result, it is sufficient to compactly model channel routing with just Cj log2 Cj parameters at any layer Rj .\nFinally, we can redefine our routing layer Rj using the Bene\u030cs network. This is achieved by first reformulating the convolutional map to produce a stacked tensor of continuous switch configurations. We then apply the Gumbel-Sinkhorn transformation on these continuous configurations to approximate the discrete binary switches. Finally, we perform the shuffle-exchange steps recursively to permute the input channels. Formally, we describe below the computational pathway of the routing layer Rj given the initial state and input S0 \u225c Q and task data Di:\nUj = fj(\u03d5(Di))[ S\u0302l[2k \u2212 1] S\u0302l[2k] ] = GS(Uj [l, k]) [ Sl[2k \u2212 1] Sl[2k] ] \u2200k \u2208 [1, Cj/2]\nSl+1 = exchange ( S\u0302l ) (7)\nwhere \u03d5 denotes the PROTONET embedding, fj maps task embedding to its corresponding continuous switch configurations, GS(Uj [l, k]) denotes the Gumbel-Sinkhorn transform of the component of Uj corresponding to the kth switch of lth layer. The exchange step refers to the deterministic routing of the Bene\u030cs network. The entire routing system of any Bene\u030cs network for 2n inputs can be efficiently constructed in a recursive manner (see Appendix E). Each Sl denotes the output of the lth Bene\u030cs layer, and S2 log2 C is the output of the routing layer Rj ."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We compare the performance of our method using the Gumbel-Sinkhorn permutation layer (Mena et al., 2018) (MRM-GS) and our proposed Gumbel-Benes\u030c routing layer (MRM-GB) against several meta-learning baselines, including MAML (Finn et al., 2017), its first-order approximation FO-MAML (Antoniou et al., 2018), prototypical network (PROTONET) (Snell et al., 2017), multi-modal model-agnostic meta learning (MMAML) (Vuorio et al., 2019), universal representation learning (URL) (Li et al., 2022), and hierarchically structured meta learning (HSML) (Yao et al., 2019). We adapt the MAML and PROTONET implementations from the LEARN2LEARN package (Arnold et al., 2020). Experiments are conducted on a GTX-3080 GPU with 13GB memory. Our implementation is available at https://anonymous.4open.science/r/ meta-routing-modulation-3E00/src/.\nFor MRM-GS, MRM-GB, MMAML, HSML, we parameterize the task embedding network and the predictor network using a CNN architecture with 4 hidden convolutional blocks and a feedforward classification layer. Each block consists of a 3\u00d7 3 convolution layer, followed by BATCHNORM, MAXPOOL and RELU activations. All convolution layers have C = 32 or 64 hidden channels, depending on the specific task distribution. The mapping from task embedding to modulation parameters is parameterized by a 1-layer, TANH-activated feed-forward neural network, whose output dimension depends on the method (e.g., approximately C2 for MRM-GS, C log2 C for MRMGB and 9C2 for MMAML). We apply the modulation to the first hidden convolutional layer.\nMAML, FO-MAML and URL have no embedding network. For fair comparison against the above methods, we parameterize the predictor network by a two-headed CNN architecture with 4 hidden convolutional blocks per head. Their outputs are then concatenated and forwarded to the classification layer for prediction. For URL, we used the channelwise adapters suggested by Li et al. (2022) after each convolutional block as it is the most similar strategy to our channel routing layer. Last, PROTONET has no predictor network and performs prediction via clustering the input embeddings. For the same reason as above, we parameterize its embedding network by a similar two-headed CNN architectures (no classification layer).\nMeta-learning vision baselines. The OMNIGLOT dataset (Lake et al., 2015) consists of 1623 handwritten characters from 50 different alphabets and writing systems. We randomly split the dataset by class into train (1100 classes), validation (100 classes), and test sets (423 classes), as suggested by Ravi & Larochelle (2017). The MINI-IMAGENET dataset (Vinyals et al., 2016) is a subset of the larger ImageNet dataset (Russakovsky et al., 2015) that contains 60000 images from 100 object categories. We randomly split the dataset by category into train (64 categories), validation (16 categories), and test sets (20 categories). The JIGSAW-OMNIGLOT and JIGSAW-MINI-IMAGENET datasets are obtained by segmenting the training images in the respective original datasets into 2 \u00d7 2 tiles and randomly permuting these tiles to simulate 24 different task modalities. Finally, The FLOWER-AIRCRAFT-FUNGI dataset combines: (a) The VGGFLOWER102 dataset (Triantafillou et al., 2020) consisting of 102 classes of flowers (between 40 to 258 images per class); (b) the FGVCAIRCRAFT dataset (Maji et al., 2013; Triantafillou et al., 2020) consisting of 102 classes of aircraft (100 images per class); and (c) the FGVCFUNGI dataset (Triantafillou et al., 2020) consisting of 1394 classes of fungi, with a total of 89760 images."
        },
        {
            "heading": "4.1 META-LEARNING FOR UNIMODAL TASK DISTRIBUTION",
            "text": "We show that our method performs robustly on the traditional homogeneous meta-learning setting despite the multi-modal treatment. We train all baseline methods on random batches of tasks drawn from (a) the OMNIGLOT dataset; and (b) the MINI-IMAGENET dataset. All tasks consist of randomly drawn images from 5 distinct labels. For each label, the task dataset contains ns support and nq query images. For training, both the support and query images are used to train the meta-learners. For testing, we perform fast adaptation using the support image and measure the test accuracy on the query images. We sample a batch of 32 training tasks per epoch to train each baseline method, and then evaluate their averaged performances over 5 random test tasks. We respectively let ns = 1, nq = 15 and ns = 5, nq = 5 for the OMNIGLOT and MINI-IMAGENET experiments,\nTable 1 records the average test accuracy (over 5 test tasks) for each baseline method over 2000 training epochs. We defer the plots of training loss and test accuracy versus training epochs to Appendix. Vanilla MAML achieves the best accuracy of 0.626 \u00b1 0.059 on the MINI-IMAGENET dataset, and second best accuracy of 0.977 \u00b1 0.028 on the OMNIGLOT dataset. These results are expected since their respective task distributions are unimodal. While other multimodal approaches (e.g., MMAML (Vuorio et al., 2019), HSML (Yao et al., 2019), and URL (Li et al., 2022)) tend to underperform in this standard setting, our methods are significantly more competitive. Specifically, on the OMNIGLOT dataset, MRM-GB achieves the best performance of 0.981 \u00b1 0.011. On the MINI-IMAGENET dataset, MRM-GS and MRM-GB achieve the third and second best classification accuracies of 0.613\u00b1 0.151 and 0.615\u00b1 0.005 respectively."
        },
        {
            "heading": "4.2 META-LEARNING FOR MULTI-MODAL TASK DISTRIBUTION",
            "text": "We further conduct experiments to demonstrate the performance of our method in two different settings of task heterogeneity. In the first experiment, we simulate the multi-modality of the task distribution by applying a jigsaw transformation to the training images in the OMNIGLOT and MINIIMAGENET datasets. Specifically, each training/test image is first segmented into 2 \u00d7 2 smaller\ntiles. For each sampled task, we then randomly draw a permutation of these 4 tiles and shuffle them accordingly to systematically derive new tasks that belong to 4! = 24 different modalities.\nTable 1 records the average test accuracy (over 5 test tasks) for each baseline method on the JIGSAWOMNIGLOT and JIGSAW-MINI-IMAGENET datasets, respectively over 3000 and 5000 training epochs. In this setting, the unimodal approaches (e.g., MAML, FO-MAML, and PROTONET) generally perform worse than the multimodal approaches. This observation confirms the need for multi-modal meta learning. We further observe that PROTONET consistently demonstrates the weakest performance on these task distributions. This is most likely because PROTONET tries to assign similar embeddings to images of the same label, which include different jigsaw permutations of the same image. While our approaches also make use of the PROTONET loss to embed tasks, the specificity of the shuffling will be captured by meta-training both the predictor network and the modulator network using the MAML loss. As a result, our methods consistently achieve the best (MRM-GB) and second best (MRM-GS) classification accuracies in both datasets.\nAs the JIGSAW setting is naturally suited for a channel shuffling approach, we further simulate a more realistic multi-modal task distribution via grafting three different image datasets: (a) VGGFLOWERS102, (b) FGVCAIRCRAFT and (c) FGVCFUNGI (Triantafillou et al., 2020). The combined FLOWERS-AIRCRAFT-FUNGI dataset thus has three distinct task modalities. We initialize the task distribution with only data from (a) (epoch 0), and subsequently inject data from (b) after 16000 sampled train/test tasks (epoch 1230); and from (c) after 32000 sampled train/test tasks (epoch 2460s). We use a batch size of 8 tasks per epoch instead of 32 as in the above experiments.\nTable 2 records the average test accuracy (over 5 test tasks) for MAML and all multi-modal metalearning baselines. The INJECT FLOWERS, INJECT AIRCRAFT and INJECT FUNGI columns respectively report the classification performances after 1230 training epochs starting from each injection point. The INJECT ALL column reports the average test accuracy (at epoch 3690) of the meta-learner when all three modalities are introduced from the start. Additionally, Fig. 3(a) and Fig. 3(b) plot the training loss vs. epoch for each baseline on these two scenarios.\nAs expected, each task injection causes a degradation in terms of average test accuracy due to the introduction of a new modality in the distribution. This is also shown most clearly in Fig. 3(a), in which there is a spike in training loss following the injection of a new dataset into the task stream (i.e., marked by the vertical dotted lines). This confirms that the meta initialization learned on one modality cannot be easily adapted to address tasks from another modality, and thereby further confirms the need to address task-heterogeneity in meta-learning. Out of all baselines, MAML experiences the worst degradation, in which the classification performance at epoch 3690 is 24% worse than that at epoch 1230. Our methods MRM-GS and MRM-GB perform competitively in the unimodal setting (up to epoch 1230), and outperform other baselines after new task modalities have been injected into the task stream. We also expect that the simultaneous injection scenario is slightly easier than sequential injection. This is because the meta-learner will not be skewed towards solving the earlier introduced modalities. Indeed, we observe better performances and training losses in all baselines compared to that of the sequential injection scenario. In this setting, our method, MRM-GB still achieves the best accuracy of 0.537\u00b1 0.063.\nLast, in both scenarios (i.e., jigsaw and dataset grafting), we show that our meta-routing method MRM-GB interestingly achieves an implicit clustering of tasks into the right modality. This is demonstrated via the respective t-SNE plots of the task embeddings prior to their discrete transformation into permutation matrices (see Appendix B)."
        },
        {
            "heading": "5 ABLATION STUDIES",
            "text": "To better understand our methods, we further conduct several ablation studies and defer the result to Appendix. In particular, we demonstrate that our routing method is capable of identifying related task clusters, as illustrated in Appendix C. We show that our methods are generally more parameter efficient with increasing number of channels in a 4-layer convolutional architecture (see Appendix F), and that our methods perform robustly with the ResNet-18 architecture (see Appendix G. Finally, we have conducted experiments to investigate the importance of varying the \u03bb parameter (see Appendix H, and the importance of routing layer placement (see Appendix I)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Previous meta-learning methods assume a homogeneous task distribution, which limits their generalization ability when dealing with multi-modal task distributions. Recent works have attempted to rectify this problem, but suffer from increased complexity in terms of parameters. To overcome this limitation, we propose a novel strategy for heterogeneous meta-learning. Our approach efficiently captures the multi-modality of the task distribution by modulating the routing between convolution channels in the network. This mechanism can be viewed as a permutation learning problem, which we model and solve using a compact neural permutation layer based on the classical Benes routing network. Our Gumbel-Benes layer exhibits sub-quadratic parameter complexity in the total number of channels, in contrast to the quadratic complexity of state-of-the-art Gumbel-Sinkhorn layers. We validate our approach on various multi-modal meta-learning benchmarks, demonstrating superior performance compared to previous methods in terms of both generalization accuracy and runtime. Theoretical understanding of our approach will be a future research consideration.\nSocietal Impact. While applications of our work to real data could result in ethical considerations, this is an indirect, unpredictable side-effect of our work. Our experiment uses publicly available datasets to evaluate the performance of our algorithms; no ethical considerations are raised."
        },
        {
            "heading": "A GUMBEL-BEN\u0306ES META ROUTING ALGORITHM",
            "text": "Algorithm 1 Gumbel-Ben\u0306es Meta Routing Require: Meta task data {D1, . . . ,Dnt}, convolution layers G = {G1, . . . , GM}, Gumbel-Ben\u0306es\nrouting layers R = {R1, . . . , RM} prototypical embedding network fProto, maximum number of epochs ne, trade-off parameter \u03bb. for i \u2208 [ne] do L \u2190 0 for t \u2208 [nt] do\nxt, yt \u2190 Dt \u03a6\u2190 fProto(xt) {Compute Task Embedding (see Appendix D.1} LProto \u2190 LProto(xt, yt,\u03a6) {Compute ProtoNet loss (see Appendix D.1)} mo(G,Dt)\u2190 xt for l \u2208 [M ] do mo(G,Dt)\u2190 Gl(mo(G,Dt)) mo(G,Dt)\u2190 Rl(mo(G,Dt),\u03a6) end for L \u2190 L+ 1nt\u00d7 equation 6\nend for Update G,R via backpropagation with loss L\nend for"
        },
        {
            "heading": "B ADDITIONAL PLOTS AND EXPERIMENTS",
            "text": "We conduct a qualitative ablation study to investigate the routing quality of our method, MRN-GB. In particular, we used the MRN-GB models respectively trained on the FLOWERS-AIRCRAFTFUNGI dataset (in the sequential setting) and the JIGSAW-OMNIGLOT dataset to generate the task embedding vectors for 10000 randomly sampled test tasks in each scenario. For visualization purpose, we use the continuous embedding vectors obtained prior to the Gumbel-Bene\u0306s transformation into the routing permutation matrices. We subsequently embed these vectors into a 2D-latent space using the t-SNE method (Van der Maaten & Hinton, 2008) and produce the resulting scatter plots in Fig. 4(a) and Fig. 4(b). The color labels in these scatter plots correspond to the ground truth modalities of the sampled tasks (e.g., 3 modalities for each component dataset in the FLOWERSAIRCRAFT-FUNGI scenario, and (2 \u00d7 2)! = 24 modalities for each jigsaw configuration in the JIGSAW-OMNIGLOT scenario). As expected, we observe visually distinctive clusters in each scenario, which suggests that our method has successfully learned to identify and route similar tasks through similar computational pathways."
        },
        {
            "heading": "C TASKSET CONFIGURATION",
            "text": ""
        },
        {
            "heading": "D BACKGROUND MATERIALS",
            "text": "D.1 PROTOTYPICAL NETWORK\nPrototypical networks Snell et al. (2017) aim to learn an embedding function fProto to encode each input into a feature vector. A class prototype of some class c is the mean vector of the embedded support data samples in this class. That is:\n\u03c1c \u225c 1 |Sc| \u2211\nxi\u2208Sc\nfProto(xi; \u03b8) , (8)\nwhere \u03b8 is the parameter of fProto and Sc is the support of class c. To train fProto, Snell et al. (2017) minimizes the negative log likelihood of the distribution induced by the prototypes:\nL(\u03b8) \u225c \u2211\n(xi,yi)\u2208D\n\u2212 logP (y = yi | xi)\n\u225c \u2211\n(xi,yi)\u2208D\n\u2212 log exp \u2225fProto(xi)\u2212 \u03c1yi\u2225\u2211 c\u2032\u2208C exp \u2225fProto(xi)\u2212 \u03c1c\u2032\u2225 , (9)\nwhere D and C respectively denotes the dataset and the label set. In many task representation learning frameworks, the task embedding vector is further derived from fProto by concatenating all class embeddings, i.e., \u03a6 \u225c [\u03c1c]c\u2208C .\nD.2 GUMBEL-SINKHORN LAYER\nThe Gumbel-Sinkhorn method is a technique for end-to-end learning in latent variable models that involve permutations. The Gumbel-Sinkhorn method approximates discrete permutation distribution using the continuous Sinkhorn operator, which is the analog of the softmax operator on the permutahedron. The Sinkhorn operator S(X) of a general square matrix X is a series of alternating row and column normalization steps of a general matrix and can be recursively written as follows:\nS0(X) = exp(X)\nSl(X) = Tc(Tr(S l\u22121(X)))\nS(X) = lim l\u2192\u221e\nSl(X) , (10)\nwhere Tc and Tr are the column-wise and row-wise normalization operators respectively. In the limit of l, S(X) is a doubly stochastic matrix that induces a distribution of permutations (i.e., the\nGumbel-Matching distribution), from which random permutations can be sampled from. To make this sampling differentiable (i.e., for the purpose of end-to-end optimization), the Gumbel-Sinkhorn method further applies the Gumbel reparameterization trick from Jang et al. (2016):\nGS(X) \u225c S((X + \u03f5)/\u03c4) , (11)\nwhere \u03f5 is standard Gumbel noise Jang et al. (2016) and GS(X) converges almost surely to samples of the Gumbel-Matching distribution as the temperature parameter \u03c4 anneals to 0."
        },
        {
            "heading": "E BENE\u030cS NETWORK ROUTING ALGORITHM",
            "text": "The Bene\u030cs network (Benes\u030c, 1964) is constructed in a recursive manner. For the base case, the Bene\u030cs network for permuting 2 items is simply a binary switch that either maintains the order between its inputs, or reverses it. The Bene\u030cs networks for 2n items with any n > 1 are subsequently defined using the following steps:\n\u2022 Concatenation. We first vertically concatenate two Bene\u030cs networks for 2n\u22121 items to create the upper and lower sub-networks. In addition to these sub-networks, the 2n Bene\u030cs network also consists of (a) an input layer: 2n\u22121 binary switches routing inputs into these sub-networks; and (b) an output layer: another 2n\u22121 binary switches routing outputs out of these sub-networks.\n\u2022 Input layer routing. The upper output of the kth input switch is routed to the kth input of the upper sub-network. The lower output of the kth input switch is routed to the kth input of the lower sub-network.\n\u2022 Output layer routing. The kth output of the upper sub-network is routed to the upper input of the kth output switch. The kth output of the lower sub-network is routed to the lower input of the kth output switch."
        },
        {
            "heading": "F SCALABILITY OF MODULATION NETWORKS",
            "text": "We show that our method scales well in terms of the predictor network\u2019s architecture complexity. Specifically, we vary the number of hidden channels C per convolutional block in our standard 4-block CNN architecture described above and record the training time per epoch as well as the number of modulating parameters in Table 4. For this experiment, weight modulation (MMAML, HSML) and routing modulation (MRM-GB, MRM-GS) are applied to all 4 blocks. For URL, we again adopt the channelwise adapter strategy as described in Li et al. (2022). We use 4 channelwise adapters after each convolutional block for the URL model. The reported runtimes are averaged over 200 consecutive training epochs.\nAs expected, the number of parameters in MRM-GB and URL increase almost linearly inC, resulting in the most efficient runtime at every size of the predictor architecture. We note that the increase in runtime, however, does not always correlate perfectly to the increase in the number of modulation parameters. This is due to various other factors, such as the overhead cost of computing the more complicated gradients of MRM-GB, MRM-GS compared to MMAML. Nonetheless, as C becomes very large, we observe that the number of modulation parameters in MRM-GS, MMAML and HSML will grow prohibitively expensive. For example, at C = 256 (which is reasonable for many common architectures), we were no longer able to fit the MMAML and HSML models into the GPU memory. While the complexity of URL and our MRM-GB method are on the same scale, we have shown above that our method achieves much better classification accuracy than URL. This result suggests that URL requires significantly more adapter parameters to sufficiently encode transferable knowledge, and that our method is more parameter efficient for meta learning.\nG INVESTIGATING PERFORMANCE WITH RESNET-18 ARCHITECTURE\nWe further perform meta-learning on the more difficult 1-shot, 5-way MINI-IMAGENET setting and its JIGSAW counterpart. We adopt similar training settings to the 5-shot, 5-way experiments in the main manuscript. That is, we trained every baseline for 2000 epochs, sampling 32 training tasks of\n16 images per epoch (1-shot, 15-queries). For testing, we sampled 5 test tasks with 1 training and 15 test images per task. Here, we only focus on comparing with other heterogeneous meta learning baselines (i.e., MMAML, HSMAML, URL) as the poor compatibility of uni-modal methods have been well explored. On this same setting, we further provided the comparative performance using the architecture ResNet-18, showing that our method is robust across a wide range of methods.\nH INVESTIGATING IMPORTANCE OF THE TRADE-OFF PARAMETER IN LOSS FUNCTION\nWe provide below the results of varying the parameter \u03bb in the MRN-GB and MRN-GS objective function. We used the 1-shot 5-way Omniglot dataset and 200 training epochs for each experiment. We observe that the best performance of MRN-GB is observed with \u03bb = 1.0, whereas \u03bb = 2.0 is best for MRN-GS. This result seems to suggest that the parameter \u03bb cannot be too large, causing the loss function to behave similarly to the ProtoNet loss; or too small, causing the task embedding module to capture unmeaningful patterns.\nI INVESTIGATING IMPORTANCE OF ROUTER DEPTH\nWe further investigate the impact of placing the routing layer at various depths of the network. In all of our previous experiments, we used one routing layer after every convolution block, hence the routers are distributed at various depths of the network. We now provide results on the 5- way 1-shot OMNIGLOT setting, in which we compared the performance of our methods using all 4 routers vs. the performance of using only one router at different depths (e.g., larger depth means being closer to the output layer). Our result suggests that having the routing layer placed earlier\non in the architecture is the most effective practice (e.g., putting a routing layer after the second convolution block gives the best performance). However, combining many routing layers might improve performance, such as in the case of MRN-GB. We will investigate this behavior further in future work."
        }
    ],
    "year": 2023
}