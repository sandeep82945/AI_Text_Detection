{
    "abstractText": "Modern distributed training relies heavily on communication compression to reduce the communication overhead. In this work, we study algorithms employing a popular class of contractive compressors in order to reduce communication overhead. However, the naive implementation often leads to unstable convergence or even exponential divergence due to the compression bias. Error Compensation (EC) is an extremely popular mechanism to mitigate the aforementioned issues during the training of models enhanced by contractive compression operators. Compared to the effectiveness of EC in the data homogeneous regime, the understanding of the practicality and theoretical foundations of EC in the data heterogeneous regime is limited. Existing convergence analyses typically rely on strong assumptions such as bounded gradients, bounded data heterogeneity, or large batch accesses, which are often infeasible in modern Machine Learning Applications. We resolve the majority of current issues by proposing EControl, a novel mechanism that can regulate error compensation by controlling the strength of the feedback signal. We prove fast convergence for EControl in standard strongly convex, general convex, and nonconvex settings without any additional assumptions on the problem or data heterogeneity. We conduct extensive numerical evaluations to illustrate the efficacy of our method and support our theoretical findings.",
    "authors": [
        {
            "affiliations": [],
            "name": "ERROR CONTROL"
        },
        {
            "affiliations": [],
            "name": "Yuan Gao"
        },
        {
            "affiliations": [],
            "name": "Rustem Islamov"
        },
        {
            "affiliations": [],
            "name": "Sebastian U. Stich"
        }
    ],
    "id": "SP:5ace76a09464bb2b9f01d14db3a34b88f5fa6bf8",
    "references": [
        {
            "authors": [
                "Afshin Abdi",
                "Faramarz Fekri"
            ],
            "title": "Quantized compressive sampling of stochastic gradients for efficient communication in distributed deep learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Alistarh",
                "Demjan Grubic",
                "Jerry Li",
                "Ryota Tomioka",
                "Milan Vojnovic"
            ],
            "title": "QSGD: Communicationefficient SGD via gradient quantization and encoding",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Dan Alistarh",
                "Torsten Hoefler",
                "Mikael Johansson",
                "Nikola Konstantinov",
                "Sarit Khirirat",
                "C\u00e9dric Renggli"
            ],
            "title": "The convergence of sparsified gradient methods",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jeremy Bernstein",
                "Yu-Xiang Wang",
                "Kamyar Azizzadenesheli",
                "Animashree Anandkumar"
            ],
            "title": "signsgd: Compressed optimisation for non-convex problems",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Aleksandr Beznosikov",
                "Samuel Horv\u00e1th",
                "Peter Richt\u00e1rik",
                "Mher Safaryan"
            ],
            "title": "On biased compression for distributed learning",
            "venue": "Journal on Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Baptiste Cordonnier"
            ],
            "title": "Convex optimization using sparsified stochastic gradient descent with memory",
            "venue": "Technical report,",
            "year": 2018
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Ilyas Fatkhullin",
                "Igor Sokolov",
                "Eduard Gorbunov",
                "Zhize Li",
                "Peter Richt\u00e1rik"
            ],
            "title": "Ef21 with bells & whistles: Practical algorithmic extensions of modern error feedback",
            "venue": "arXiv preprint arXiv:",
            "year": 2021
        },
        {
            "authors": [
                "Ilyas Fatkhullin",
                "Alexander Tyurin",
                "Peter Richt\u00e1rik"
            ],
            "title": "Momentum provably improves error feedback",
            "venue": "arXiv preprint arXiv:",
            "year": 2023
        },
        {
            "authors": [
                "Eduard Gorbunov",
                "Dmitry Kovalev",
                "Dmitry Makarenko",
                "Peter Richt\u00e1rik"
            ],
            "title": "Linearly converging error compensated sgd",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "Yutong He",
                "Xinmeng Huang",
                "Yiming Chen",
                "Wotao Yin",
                "Kun Yuan"
            ],
            "title": "Lower bounds and accelerated algorithms in distributed stochastic optimization with communication compression",
            "venue": "arXiv preprint arXiv:",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Horv\u00e1th",
                "Chen-Yu Ho",
                "L\u2019udov\u00edt Horv\u00e1th",
                "Atal Narayan Sahu",
                "Marco Canini",
                "Peter Richt\u00e1rik"
            ],
            "title": "Natural compression for distributed deep learning",
            "venue": "arXiv preprint arXiv:",
            "year": 2019
        },
        {
            "authors": [
                "Xinmeng Huang",
                "Yiming Chen",
                "Wotao Yin",
                "Kun Yuan"
            ],
            "title": "Lower bounds and nearly optimal algorithms in distributed learning with communication compression",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rustem Islamov",
                "Xun Qian",
                "Slavom\u00edr Hanzely",
                "Mher Safaryan",
                "Peter Richt\u00e1rik"
            ],
            "title": "Distributed newton-type methods with communication compression and bernoulli aggregation",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Quentin Rebjock",
                "Sebastian Stich",
                "Martin Jaggi"
            ],
            "title": "Error feedback fixes signsgd and other gradient compression schemes",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning",
            "year": 2019
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank J. Reddi",
                "Sebastian U. Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "SCAFFOLD: Stochastic controlled averaging for on-device federated learning",
            "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Dheevatsa Mudigere",
                "Jorge Nocedal",
                "Mikhail Smelyanskiy",
                "Ping Tak Peter Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "In Proceedings of International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Sarit Khirirat",
                "Hamid Feyzmahdavian",
                "Mikael Johansson"
            ],
            "title": "Distributed learning with compressed gradients",
            "venue": "arXiv preprint arXiv:",
            "year": 2018
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Sebastian Stich",
                "Martin Jaggi"
            ],
            "title": "Decentralized stochastic optimization and gossip algorithms with compressed communication",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Tao Lin",
                "Sebastian Stich",
                "Martin Jaggi"
            ],
            "title": "Decentralized deep learning with arbitrary communication compression",
            "venue": "In Proceedings of International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Nicolas Loizou",
                "Sadra Boreiri",
                "Martin Jaggi",
                "Sebastian Stich"
            ],
            "title": "A unified theory of decentralized sgd with changing topology and local updates",
            "venue": "In Proceedings of International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u00fd",
                "H. Brendan McMahan",
                "Felix Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated learning: strategies for improving communication efficiency",
            "venue": "In Proceedings of NIPS Private Multi-Party Machine Learning Workshop,",
            "year": 2016
        },
        {
            "authors": [
                "Yann LeCun",
                "Leon Bottou",
                "Genevieve Orr",
                "Klaus-Robert Muller"
            ],
            "title": "Efficient backprop",
            "venue": "Neural Networks: Tricks of the Trade,",
            "year": 2012
        },
        {
            "authors": [
                "Bingcong Li",
                "Shuai Zheng",
                "Parameswaran Raman",
                "Anshumali Shrivastava",
                "Georgios B. Giannakis"
            ],
            "title": "Contractive error feedback for gradient compression",
            "venue": "arXiv preprint arXiv:",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoyun Li",
                "Ping Li"
            ],
            "title": "Analysis of error feedback in federated non-convex optimization with biased compression: Fast convergence and partial participation",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Zhize Li",
                "Peter Richt\u00e1rik"
            ],
            "title": "Canita: Faster rates for distributed convex optimization with communication compression, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Zhize Li",
                "Dmitry Kovalev",
                "Xun Qian",
                "Peter Richt\u00e1rik"
            ],
            "title": "Acceleration for compressed gradient descent in distributed and federated optimization, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Zhize Li",
                "Hongyan Bao",
                "Xiangliang Zhang",
                "Peter Richt\u00e1rik. Page"
            ],
            "title": "A simple and optimal probabilistic gradient estimator for nonconvex optimization",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Xiangru Lian",
                "Ce Zhang",
                "Huan Zhang",
                "Cho-Jui Hsieh",
                "Wei Zhang",
                "Ji Liu"
            ],
            "title": "Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Maksim Makarenko",
                "Elnur Gasanov",
                "Rustem Islamov",
                "Abdurakhmon Sadiev",
                "Peter Richt\u00e1rik"
            ],
            "title": "Adaptive compression for communication-efficient distributed training",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Konstantin Mishchenko",
                "Eduard Gorbunov",
                "Martin Tak\u00e1\u010d",
                "Peter Richt\u00e1rik"
            ],
            "title": "Distributed learning with compressed gradient differences",
            "venue": "arXiv preprint arXiv:1901.09269,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Xun Qian",
                "Hanze Dong",
                "Peter Richt\u00e1rik",
                "Tong Zhang"
            ],
            "title": "Error compensated loopless svrg, quartz, and sdca for distributed optimization",
            "venue": "arXiv preprint arXiv:2109.10049,",
            "year": 2021
        },
        {
            "authors": [
                "Xun Qian",
                "Peter Richt\u00e1rik",
                "Tong Zhang"
            ],
            "title": "Error compensated distributed sgd can be accelerated",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xun Qian",
                "Hanze Dong",
                "Tong Zhang",
                "Peter Richt\u00e1rik"
            ],
            "title": "Catalyst acceleration of error compensated methods leads to better communication complexity",
            "venue": "In Proceedings of 25th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Rakhlin",
                "Ohad Shamir",
                "Karthik Sridharan"
            ],
            "title": "Making gradient descent optimal for strongly convex stochastic optimization",
            "venue": "arXiv preprint arXiv:1109.5647,",
            "year": 2011
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "year": 2022
        },
        {
            "authors": [
                "Peter Richt\u00e1rik",
                "Igor Sokolov",
                "Ilyas Fatkhullin"
            ],
            "title": "Ef21: A new, simpler, theoretically better, and practically faster error feedback",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mher Safaryan",
                "Rustem Islamov",
                "Xun Qian",
                "Peter Richtarik"
            ],
            "title": "FedNL: Making Newton-type methods applicable to federated learning",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Frank Seide",
                "Hao Fu",
                "Jasha Droppo",
                "Gang Li",
                "Dong Yu"
            ],
            "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns",
            "venue": "In Proceedings of 15th annual conference of the international speech communication association,",
            "year": 2014
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick Le Gresley",
                "Jared Casper",
                "Bryan Catanzaro"
            ],
            "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
            "year": 1909
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In Proceedings of International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Sebastian U. Stich"
            ],
            "title": "On communication compression for distributed optimization on heterogeneous data",
            "venue": "arXiv preprint arXiv: 2009.02388,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian U Stich",
                "Sai Praneeth Karimireddy"
            ],
            "title": "The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian U Stich",
                "Jean-Baptiste Cordonnier",
                "Martin Jaggi"
            ],
            "title": "Sparsified sgd with memory",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Nikko Strom"
            ],
            "title": "Scalable distributed DNN training using commodity GPU cloud computing",
            "venue": "In Proceedings of Interspeech 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Thijs Vogels",
                "Sai Praneeth Karimireddy",
                "Martin Jaggi"
            ],
            "title": "PowerSGD: Practical low-rank gradient compression for distributed optimization",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Meng Wang",
                "Weijie Fu",
                "Xiangnan He",
                "Shijie Hao",
                "Xindong Wu"
            ],
            "title": "A survey on large-scale machine learning",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Wen",
                "Cong Xu",
                "Feng Yan",
                "Chunpeng Wu",
                "Yandan Wang",
                "Yiran Chen",
                "Hai Li"
            ],
            "title": "Terngrad: Ternary gradients to reduce communication in distributed deep learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Randall Wilson",
                "Tony Martinez"
            ],
            "title": "The general inefficiency of batch training for gradient descent learning",
            "venue": "Neural Networks,",
            "year": 2003
        },
        {
            "authors": [
                "Haoyu Zhao",
                "Boyu Li",
                "Zhize Li",
                "Peter Richt\u00e1rik",
                "Yuejie Chi"
            ],
            "title": "BEER: Fast $o(1/t)$ rate for decentralized nonconvex optimization with communication compression",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shuai Zheng",
                "Ziyue Huang",
                "James Kwok"
            ],
            "title": "Communication-efficient distributed blockwise momentum sgd with error-feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "F t"
            ],
            "title": "D-EC-SGD WITH BIAS CORRECTION AND DOUBLE CONTRACITVE COMPRESSION In this section we follow algorithm D-EC-SGD with bias correction (Stich, 2020)",
            "year": 2020
        },
        {
            "authors": [
                "Li"
            ],
            "title": "The following lemmas are simple modifications of the lemmas in (Li et al., 2021) and (Richt\u00e1rik et al., 2021) in the presence of stochasticity. Therefore, we state them without proofs",
            "year": 2021
        },
        {
            "authors": [
                "Richt\u00e1rik"
            ],
            "title": "Let f be \u03bc-strongly convex and fi be L-smooth for all i \u2208 [n]. Then iterates of EF21-SGD",
            "venue": "\u03b4)\u03b3Ht",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The size of modern neural networks has increased dramatically. Consequently, the data required for efficient training is huge as well, and accumulating all available data into a single machine is often infeasible. Because of these considerations, the training of large language models (Shoeybi et al., 2019), generative models (Ramesh et al., 2021; 2022), and others (Wang et al., 2020) is performed in a distributed fashion with decentralized data. Another quickly developing instance of distributed training is Federated Learning (FL) (Konec\u030cn\u00fd et al., 2016; Kairouz et al., 2019) where the goal is to train a single model directly on the devices keeping their local data private.\nThe communication bottleneck is a main factor that limits the scalability of distributed deep learning training (Seide et al., 2014; Strom, 2015). Methods that use lossy compression have been proposed as a remedy, with great success (Seide et al., 2014; Alistarh et al., 2017). It has been observed that error compensation (EC) mechanisms are crucial to obtain high compression ratios (Seide et al., 2014; Stich et al., 2018), and variants of these techniques (Vogels et al., 2019) are already integrated in standard deep learning frameworks such as PyTorch (Paszke et al., 2019) and been successfully used in the training of transformer models (Ramesh et al., 2021).\nThese methods were primarily developed for data center training scenarios where training data is shuffled between nodes. This data uniformity was also a limiting assumption in early analyses for distributed EC (Cordonnier, 2018; Stich & Karimireddy, 2020). To make compression suitable for training beyond data centers, such as federated learning, it is essential to take data heterogeneity (also termed client drift or bias) (Karimireddy et al., 2020; Mishchenko et al., 2019) into account. Mishchenko et al. (2019) designed a method for this scenario, but it can only support the restrictive class of unbiased compressors. It turned out, that handling arbitrary compressors is a challenge. A\n\u2217Equal contribution.\nline of work developed the EF21 algorithm (Richt\u00e1rik et al., 2021) that can successfully handle the bias/drift, but cannot obtain a linear speedup in parallel training, i.e. the training time does not decrease when more devices are used for training.\nOne of the main difficulties in developing a method that simultaneously combats client drift and maintains linear acceleration has been the complicated interaction between the mechanisms of bias and error correction. In this work, we propose EControl, a novel mechanism that can regulate error compensation by controlling the strength of the feedback signal. This allows us to overcome both challenges simultaneously. Our main contributions can be summarized as:\nNew Method. We propose a novel method EControl that provably converges (i) with arbitrary contractive compressors, (ii) under arbitrary data distribution (heterogeneity), (iii) and obtains linear parallel speedup.\nA Practical Algorithm. EControl does not need to resort to impractical theoretical tricks, such as large batch sizes or repeated communication rounds, but instead is a lightweight extension that can be easily added to existing EC implementations.\nFast Convergence. We demonstrate the convergence guarantees in all standard regimes: strongly convex, general convex, and nonconvex functions. In all the cases complexities are asymptotically tight with stochastic gradients. In the nonconvex case, the rate in the noiseless setting matches the known lower bound (He et al., 2023). In the strongly convex and general convex settings, we achieve the standard linear and sublinear convergence respectively, with tight dependency on the compression parameter in the noiseless setting. To the best of our knowledge, our work is the first demonstrating that in strongly convex and general convex regimes without additional assumptions on the problem.\nEmpirical Study. We conduct extensive empirical evaluations that support our theoretical findings and show the efficacy of the new method."
        },
        {
            "heading": "2 COMMUNICATION BOTTLENECK AND ERROR COMPENSATION",
            "text": "In our work, we analyze algorithms combined with compressed communication. In particular, we consider methods utilizing practically helpful contractive compression operators.\nDefinition 1. We say that a (possibly randomized) mapping C : Rd \u2192 Rd is a contractive compression operator if for some constant 0 < \u03b4 \u2264 1 it holds\nE [ \u2225C(x)\u2212 x\u22252 ] \u2264 (1\u2212 \u03b4) \u2225x\u22252 \u2200x \u2208 Rd. (1)\nThe classic example satisfying (1) is Top-K (Stich et al., 2018), which preserves the K largest (in absolute value) entries of the input, and zeros out the remaining entries. The class of contractive compressors also includes sparsification (Alistarh et al., 2018; Stich et al., 2018) and quantization operators (Wen et al., 2017; Alistarh et al., 2017; Bernstein et al., 2018; Horv\u00e1th et al., 2019), and many others (Zheng et al., 2019; Beznosikov et al., 2020; Vogels et al., 2019; Safaryan et al., 2022; Islamov et al., 2023). In this section, we review related works on error compensation, with a focus on the works that also consider contractive compressors."
        },
        {
            "heading": "2.1 SELECTED RELATED WORKS ON ERROR COMPENSATION",
            "text": "The EC mechanism Seide et al. (2014) was first analyzed for the single worker case in Stich et al. (2018); Karimireddy et al. (2019). Extensions to the distributed setting were first conducted under the assumption of homogeneous (IID) data, a constraint imposed either implicitly by assuming bounded gradients Cordonnier (2018); Alistarh et al. (2018) or explicitly without the bounded gradient assumption Stich & Karimireddy (2020). Choco-SGD was designed for communication compression over arbitrary networks and analyzed under similar IID assumptions Koloskova et al. (2019; 2020a). The analyses of distributed EC where further developed in Lian et al. (2017); Beznosikov et al. (2020); Stich (2020).\nThe DIANA algorithm by Mishchenko et al. (2019) was proposed as a solution for the heterogeneous data case, and introduced a mechanism that was able to account for the drift/bias on nodes with different data, but only when unbiased compressors were used. However, their result inspired many\nfollow-up works that combined contractive compressors with their bias correction mechanism (which requires an additional unbiased compressor that doubles the communication cost of the algorithm per iteration), such as Gorbunov et al. (2020); Stich (2020); Qian et al. (2021b).\nRecently, Richt\u00e1rik et al. (2021) introduced EF21 that fully supports contractive compressors and presented strong analysis in the full gradient (i.e. noiseless) regime. Nevertheless, the main problem of EF21 is weak convergence guarantees in the stochastic regime, i.e. when clients have access to stochastic gradient estimators only. The analysis of EF21 and its modifications require large batches and do not have linear speedup with n. Moreover, they demonstrate poor dependency on the compression parameter \u03b4. Zhao et al. (2022) improves the dependency on the compression parameter \u03b4 in a nonconvex setting but still requires large batches. Recently, Fatkhullin et al. (2023) resolved those issues by introducing a momentum-based variant of EF21. They demonstrate asymptotically optimal complexity for nonconvex losses.\nA line of work studied accelerated methods with communication compression (Li et al., 2020; Li & Richt\u00e1rik, 2021; Qian et al., 2021a; 2023; He et al., 2023), each with different additional requirements on the problem, the compressor class, or stochasticity and batch size of the gradient oracle. These methods show accelerated rates matching lower bounds in some regimes but are typically impractical due to those requirements and many parameter tuning."
        },
        {
            "heading": "2.2 EXISTING PROBLEMS WITH ERROR COMPENSATION",
            "text": "Below we list the main problems of existing theoretical results and highlight a historical overview of the main existing theoretical results in Table 1.\nAdditional Communication. Gorbunov et al. (2020); Stich (2020) modify the original EC mechanism following the DIANA method. However, their approach requires an additional unbiased compressor. Such an idea allows for building a better sequence of compressed gradient estimators but with a doubled per-iteration communication cost.\nStrong Assumptions. Many earlier theoretical results for EC require strong assumptions, such as either the bounded gradient assumption (Koloskova et al., 2019; 2020a) or globally bounded dissimilarity assumption (Lian et al., 2017; Huang et al., 2022; He et al., 2023; Li & Li, 2023)1. Besides, Makarenko et al. (2023) analyses EF21-based algorithm under bounded iterates assumption.\nLarge Batches. Fatkhullin et al. (2023) gives an example where EF21 fails to converge with small batch size. NEOLITHIC (Huang et al., 2022; He et al., 2023) matches lower bounds with large batch requirements combined with multi-stage execution for hyperparameter tuning. These requirements make these methods less suitable for DL training, where small batches are known to improve generalization performance and convergence (LeCun et al., 2012; Wilson & Martinez, 2003; Keskar et al., 2017).\nSuboptimal Convergence Rates. Current theoretical analysis of distributed algorithms combined with contractive compressors do not match known lower bounds in the nonconvex regime (Huang et al., 2022; He et al., 2023). Zhao et al. (2022); Fatkhullin et al. (2021) do not achieve a speedup in n in the asymptotic regime while in contrast Koloskova et al. (2020a) has suboptimal rates in the low noise regime (e.g. full gradient computations). We are aware of only the works by Fatkhullin et al. (2023) and He et al. (2023), but the latter requires large batches at each iteration.\nMissing a Practical Method for the Convex Regime. He et al. (2023) analyzed the accelerated NEOLITHIC in general convex and strongly convex regimes with large batch requirements. In fact, this method is mostly of a theoretical nature as the choice of the optimal parameters relies on the gradient variance at the solution, and it was shown to be slow in practice (Fatkhullin et al., 2023). To the best of our knowledge, there is no distributed algorithm utilizing only a contractive compression operator and provably converging in general convex and strongly convex regimes under standard assumptions.\nIn our work, we propose a novel method, which we call EControl, and push the theoretical analysis of algorithms combined with contractive compression operators further in several directions.\n1This assumption heavily restricts the class of functions and typically does not hold in Federated Learning.\nTable 1: Theoretical comparison of error compensated algorithms using only contractive compressors for distributed optimization in a heterogeneous setting. We omit the comparison in the general convex regime since most of the works focus on strongly convex and general nonconvex settings. nCVX = supports nonconvex functions; sCVX = supports strongly convex functions. We present the convergence E [ \u2225\u2207f(xout)\u22252 ] \u2264 \u03b5 in the nonconvex and E [f(xout)\u2212 f\u22c6] in the strongly convex regimes for specifically chosen xout. Here F0 := f(x0)\u2212 f\u22c6.\nAlgorithm nCVX sCVX\nEC Seide et al. (2014)\nLF0\u03c3 2 n\u03b52 + LF0(\u03c3+\u03b6/\n\u221a \u03b4)\u221a\n\u03b4\u03b53/2 + LF0\u03b4\u03b5 (a) \u03c32 \u00b5n\u03b5 +\n\u221a L(\u03c3+\u03b6/ \u221a \u03b4)\n\u00b5 \u221a \u03b4\u03b5\n+ L\u00b5\u03b4 (a)\nChoco-SGD Koloskova et al. (2020a)\nLmaxF0\u03c3 2\nn\u03b52 + LmaxF0G \u03b4\u03b53/2 + LmaxF0\u03b4\u03b5 (b) \u03c32 \u00b5n\u03b5 + \u221a LmaxG \u00b5\u03b4\u03b51/2 + G 2/3 \u00b51/3\u03b4\u03b51/3 (b)\nEF21-SGD (Fatkhullin et al., 2021)\nL\u0303F0\u03c3 2\n\u03b43\u03b52 + L\u0303F0 \u03b4\u03b5 (c) L\u0303\u03c32 \u00b52\u03b43\u03b5 + L\u0303 \u00b5\u03b4 (c)\nEF21-SGD2M (Fatkhullin et al., 2023)\nLF0\u03c3 2\nn\u03b52 + LF0\u03c3\n2/3\n\u03b42/3\u03b54/3 + L\u0303F0+\u03c3\n\u221a LF0\n\u03b4\u03b5 (d) \u2717\nEControl This work\nLF0\u03c3 2\nn\u03b52 + LF0\u03c3 \u03b42\u03b53/2 + L\u0303F0\u03b4\u03b5 \u03c32 \u00b5n\u03b5 + \u221a L\u03c3 \u00b5\u03b42\u03b51/2 + L\u0303\u00b5\u03b4\nLower Bounds (He et al., 2023)\nLF0\u03c3 2\nn\u03b52 + LF0 \u03b4\u03b5 (e) \u03c32 \u00b5n\u03b5 + 1 \u03b4 \u221a L \u00b5 (f)\n(a) The analysis assumes a gradient dissimilarity bound of local gradients 1n \u2211n i=1 \u2225\u2207fi(x)\u2225 2 \u2264 \u03b62 + \u2225\u2207f(x)\u22252 (Stich, 2020).\n(b) The analysis is done under the second moment bound of the stochastic gradients E [ \u2225gi(x)\u22252 ] \u2264 G2. We emphasize that strongly convex functions do not satisfy this assumption. (c) This result requires the assumption that each batch size is at least \u03c3 2\n\u03b42\u03b5 in nonconvex regime and \u03c3\n2\n\u00b5\u03b42\u03b5 in the strongly convex regime.\n(d) The last term becomes L\u0303F0\u03b4\u03b5 if the initial batch size is at least \u03c32 LF0 . (e) The analysis is done under gradient disimilarity assumption 1n \u2211n i=1 \u2225\u2207fi(x) \u2212 \u2207f(x)\u2225 2 \u2264 \u03b62. Moreover, the result requires large mini-batches and performing 1/\u03b4 communication rounds per iteration.\n(f) The result requires large mini-batches and performing 1/\u03b4 communication rounds per iteration. Moreover, the optimal parameters are chosen with an assumption that 1n \u2211n i=1 \u2225\u2207fi(x \u22c6)\u22252 = b2 is known."
        },
        {
            "heading": "3 PROBLEM FORMULATION AND ASSUMPTIONS",
            "text": "We consider the distributed optimization problem\nf\u22c6 := min x\u2208Rd\n[ f(x) := 1\nn n\u2211 i=1 fi(x)\n] , (2)\nwhere x represents the parameters of a model we aim to train, and the objective function f : Rd \u2192 R is decomposed into n terms fi : Rd \u2192 R, i \u2208 [n] := {1, . . . , n}. Each individual function fi(x) := E\u03bei\u223cDi [fi(x, \u03bei)] is a loss associated with a local dataset Di available to client i. We let x\u22c6 := argminx\u2208Rd f(x).\nWe analyze the centralized setting where the clients are connected with each other through a server. Typically, the server receives the messages from the clients and transfers back the aggregated information. In contrast to many prior works, we study arbitrarily heterogeneous setting and do not make any assumptions on the heterogeneity level, i.e. local data distributions might be far away from each other. We now list the main assumptions on the optimization problem (2).\nFirst, we introduce assumptions on f and fi that we use to derive convergence guarantees. Assumption 1. We assume that the average function f has L-Lipschitz gradient, and each individual function fi has Li-Lipschitz gradient, i.e. for all x,y \u2208 Rd, and i \u2208 [n] it holds\n\u2225\u2207f(y)\u2212\u2207f(x)\u2225 \u2264 L \u2225y \u2212 x\u2225 , \u2225\u2207fi(y)\u2212\u2207fi(x)\u2225 \u2264 Li \u2225y \u2212 x\u2225 . (3)\nWe define a constant L\u0303 := \u221a\n1 n \u2211n i=1 L 2 i . 2 We note that most works derive convergence guarantees\nwith maximum smoothness constant Lmax := maxi\u2208[n] Li which can be much larger than L\u0303. Next, in some cases we assume \u00b5-strong convexity of f .\n2Note that the following chain of inequalities always hold: L \u2264 L\u0303 \u2264 Lmax := maxi\u2208[n] Li.\nAlgorithm 1 EC-Ideal\n1: Input: x0, eit=0d,hi\u22c6 := \u2207fi(x\u22c6), \u03b3, C\u03b4 2: h\u22c6 = 1 n \u2211n i=1 h i \u22c6 3: for t = 0, 1, 2, . . . do 4: client side: 5: compute git = g\ni(xt) 6: compute \u2206it = C\u03b4(eit + git \u2212 hi\u22c6) 7: update eit+1 = e i t + g i t \u2212 hi\u22c6 \u2212\u2206it 8: send to server \u2206it 9: server side:\n10: update xt+1 := xt \u2212 \u03b3h\u22c6 \u2212 \u03b3n \u2211n i=1 \u2206 i t\nAlgorithm 2 EControl\n1: Input: x0, ei0=0d, hi0=gi0, \u03b3, \u03b7, and C\u03b4 2: h0 = 1 n \u2211n i=1 h i 0 3: for t = 0, 1, 2, . . . do 4: client side: 5: compute git = g\ni(xt) 6: compute \u2206it = C\u03b4(\u03b7eit + git \u2212 hit) 7: update eit+1 = e i t + g i t \u2212 hit \u2212\u2206it 8: and hit+1 = h i t +\u2206 i t\n9: send to server \u2206it 10: server side: 11: update xt+1 := xt \u2212 \u03b3ht \u2212 \u03b3n \u2211n i=1 \u2206 i t\n12: and ht+1 = ht + 1n \u2211n i=1 \u2206 i t\nAssumption 2. We assume that the average function f is \u00b5-strongly convex, i.e. for all x,y \u2208 Rd it holds\nf(x) \u2265 f(y) + \u27e8\u2207f(y),x\u2212 y\u27e9+ \u00b5 2 \u2225x\u2212 y\u22252 . (4)\nWe say that f is convex if it satisfies (4) with \u00b5 = 0.\nWe highlight that we need the strong convexity (convexity resp.) only of the average function f while individual functions fi do not have to satisfy it, and they can be even nonconvex. On top of that, in our analysis we apply the strong convexity (convexity resp.) around x\u22c6 only, thus, it is sufficient to assume that (4) holds for any y and x \u2261 x\u22c6. In this case, we say that a function is \u00b5-strongly quasi-convex (quasi-convex resp.) around x\u22c6; see (Stich & Karimireddy, 2020).\nFinally, we make an assumption on the noise of local gradient estimators used by the clients. Assumption 3. We assume that we have access to a gradient oracle gi(x) : Rd \u2192 Rd for each local function fi such that for all x \u2208 Rd and i \u2208 [n] it holds\nE [ gi(x) ] = \u2207fi(x), E [ \u2225gi(x)\u2212\u2207fi(x)\u22252 ] \u2264 \u03c32. (5)\nNote that mini-batches are also allowed, effectively dividing this variance quantity by the local batch size. However, we do not need to impose any restrictions on the (minimal) batch size and, for simplicity, always assume a batch size of one.\n4 EC-Ideal AS A STARTING POINT\nTo motivate some of the key ingredients of our main method, we start with a simple idea developed in an ideal setting, which shed some light on the nuances of the interactions between bias and error correction. Assume for a moment that we have access to hi\u22c6 := \u2207fi(x\u22c6). We can utilize this additional information to modify the original EC mechanism so that we only compress the difference between the current stochastic gradient git and the h i \u22c6. See Algorithm 1 and the highlight below:\nEC-Ideal update: \u2206it = C\u03b4(eit + git \u2212 hi\u22c6) eit+1 = e i t + g i t \u2212 hi\u22c6 \u2212\u2206it.\n(6)\nIt turns out that this simple trick leads to a dramatic theoretical improvement. Now we do not have to restrict the heterogeneity of the problem in contrast to the analysis of original EC (Stich, 2020). The next theorem presents the convergence of EC-Ideal. Theorem 1 (Convergence of EC-Ideal). Let f : Rd \u2192 R be \u00b5-strongly quasi-convex around x\u22c6, and each fi be L-smooth and convex. Then there exists a stepsize \u03b3 \u2264 \u03b48\u221a6L such that after at most\nT = O\u0303\n( \u03c32\n\u00b5n\u03b5 +\n\u221a L\u03c3\n\u00b5 \u221a \u03b4\u03b51/2\n+ L\n\u00b5\u03b4\n)\niterations of Algorithm 1 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where xout is chosen randomly from {x0, . . . ,xT } with probabilities proportional to (1\u2212 \u00b5\u03b3/2)\u2212(t+1).\nWe defer the proof to Appendix E. Note that the output criteria (selecting a random iterate) is standard in the literature and for convex functions could also be replaced by a weighted average over all iterates that can be efficiently tracked over time (see e.g. Rakhlin et al., 2011). EC-Ideal achieves very desirable properties. First, it provably works for any contractive compression. Second, it achieves optimal asymptotic complexity and the standard linear convergence when \u03c32 \u2192 0. All of these results are derived without enforcing data heterogeneity bounds. However, there are several drawbacks as well. First, EC-Ideal requires knowledge of {hi\u22c6}i\u2208[n] which is unrealistic in most applications. In Appendix D we show that the algorithm, in fact, needs only approximations of {hi\u22c6}i\u2208[n] as input. Nonetheless, there is still an issue that EC-Ideal\u2019s convergence is only guaranteed under the convexity condition, which stems from the fact that we always need to control the distance between \u2207fi(xt) and \u2207fi(x\u22c6). To overcome this issue, we need to build estimators of the current gradient, instead of the gradient at the optimum, parallel to maintaining the error term.\n5 EControl IS A SOLUTION TO ALL ISSUES\nInspired by the properties of EC-Ideal, we aim to build a mechanism that will progressively learn the local gradient approximations (see also Remark 15 in Appendix B). Stich (2020); Gorbunov et al. (2020) created a learning process based on an additional compressor from a more restricted class of unbiased compression operators. To make their method work for any contractive compressor, we can replace the additional unbiased compressor with a more general contractive one, but this leads to worse complexity in the low noise regime and more restriction on the stepsize. We refer the reader to Appendix F for more detailed discussion. In this work, we propose a novel method, called EControl, that overcomes all of aforementioned issues by\n\u2022 building the error term and the gradient estimator with a single compressed message, enabling error compensation for the gradient estimator;\n\u2022 introducing a new parameter \u03b7 to precisely control the error compensation and balance error compensation carried from the gradient estimator.\nWe summarize EControl in Algorithm 2 and highlight the key ingredients as follows:\nEControl update: \u2206it = C\u03b4(\u03b7eit + git \u2212 hit) eit+1 = e i t + g i t \u2212 hit \u2212\u2206it\nhit+1 = h i t +\u2206 i t\n(7)\nFusing the error and the gradient estimator updates improves the algorithm\u2019s dependency on \u03c32 and \u03b4, but also enables the gradient estimator to carry some level of the error information. This brings forth the challenge of balancing the strength of the feedback signal, and we introduce the \u03b7 parameter to obtain finer control over the error compensation mechanism. This new parameter brings additional flexibility and allows us to stabilize the convergence of EControl. The effect of such a parameter in the context of the original EC mechanism without gradient estimator might be of independent interest. We observe in practice that Algorithm 2 with \u03b7 = 1 is unstable and sensitive to the choice of initialization, which we illustrate in Appendix C.2 with a toy example. This highlights the importance of the finer control over EC that \u03b7 enables. We refer the interested readers to Appendix C.1 for a more detailed discussion on the importance of \u03b7 from a theoretical perspective. In Appendix C.3 we also discuss the connection between EControl and EF21 when \u03b7 \u2192 0. A similar idea of weighting error terms appeared in Abdi & Fekri (2020); Li et al. (2023). However, their algorithm is based on the original EC.\n6 THEORETICAL ANALYSIS OF EControl\nWe move on to theoretical analysis of EControl. Below we summarize the convergence properties of Algorithm 2 in all standard cases. We start with the convergence results in the strongly quasi-convex regime.\nTheorem 2 (Convergence of EControl for strongly quasi-convex objective). Let f be \u00b5-strongly quasi-convex around x\u22c6 and L-smooth. Let each fi be Li-smooth. Then for \u03b7 = c\u03b4 (where c is an absolute constant we specify in the proof), there exists a \u03b3 \u2264 O(\u03b4/L\u0303)3. such that after at most\nT = O\u0303\n( \u03c32\n\u00b5n\u03b5 +\n\u221a L\u03c3\n\u00b5\u03b42\u03b51/2 +\nL\u0303\n\u00b5\u03b4 ) iterations of Algorithm 2 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where xout is chosen randomly from xt \u2208 {x0, . . . ,xT } with probabilities proportional to (1\u2212 \u03b3\u00b52 ) \u2212(t+1).\nThe asymptotic complexity of EControl in this regime with stochastic gradient matches the lower bound \u2126( \u03c3 2\n\u00b5n\u03b5 ) up to a log term, as opposed to previous works (Fatkhullin et al., 2021; Zhao et al., 2022). As we can see, the first term linearly improves with n, similar to the convergence behavior of distributed SGD. Moreover, EControl achieves standard linear convergence and a desired inverse linear dependency on the compression parameter \u03b4 in the noiseless setting. Next, we switch to a general convex setting.\nTheorem 3 (Convergence of EControl for quasi-convex objective). Let f be quasi-convex around x\u22c6 and L-smooth. Let each fi be Li-smooth. Then for \u03b7 = c\u03b4 there exists a \u03b3 \u2264 O(\u03b4/L\u0303) such that after at most\nT = O\n( R0\u03c3 2\nn\u03b52 +\n\u221a LR0\u03c3\n\u03b42\u03b53/2 + L\u0303R0 \u03b4\u03b5 ) iterations of Algorithm 2 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where R0 := \u2225x0 \u2212 x\u22c6\u22252 and xout is chosen uniformly at random from xt \u2208 {x0, . . . ,xT }.\nSimilar to the previous case, the first term enjoys a linear speedup by the number of nodes. We achieve a standard sublinear convergence and a desired inverse linear dependency on \u03b4 in the noiseless regime.\nTheorem 4 (Convergence of EControl for nonconvex objective). Let f be L-smooth, and each fi be Li-smooth. Then for \u03b7 = c\u03b4 there exists a \u03b3 \u2264 O(\u03b4/L\u0303) such that after at most\nT = O\n( LF0\u03c3 2\nn\u03b52 +\nLF0\u03c3 \u03b42\u03b53/2 + L\u0303F0 \u03b4\u03b5\n)\niterations of Algorithm 2 it holds E [ \u2225\u2207f(xout)\u22252 ] \u2264 \u03b5, where F0 := f(x0)\u2212 f\u22c6 and xout is chosen\nuniformly at random from xt \u2208 {x0, . . . ,xT }.\nThe complexity of EControl in both asymptotic and noiseless regimes matches known lower bounds (He et al., 2023) for nonconvex functions that are derived under much stronger assumptions. We achieve these results without any theoretical restrictions, such as large batch sizes or repeated communication rounds, making it easy to deploy in practice as an extension to existing EC implementations."
        },
        {
            "heading": "7 EXPERIMENTS",
            "text": "In this section, we complement our theoretical analysis of EControl with experimental results. We corroborate our theoretical findings with experimental evaluations and illustrate the practical efficiency and effectiveness of EControl in real-world scenarios."
        },
        {
            "heading": "7.1 SYNTHETIC LEAST SQUARES PROBLEM",
            "text": "First, we consider the least squares problem designed by Koloskova et al. (2020b). For each client i, fi(x) := 1 2 \u2225Aix\u2212 bi\u2225 2, where A2i := i2 n \u00b7 Id and each bi is sampled from N (0, \u03b62\ni2 Id) for some parameter \u03b6. The parameter \u03b6 controls the gradient dissimilarity of the problem. It\u2019s easy to see that\n3Note that the stepsize can depend on \u03b5. Here we use the notation \u03b3 \u2264 O(\u03b4/L\u0303) to denote that the stepsize must satisfy \u03b3 \u2264 C\u03b4\nL\u0303 , for an absolute constant C specified in the proof.\nwhen \u03b6 = 0,\u2207fi(x\u22c6) = 0,\u2200i. We add Gaussian noise to the gradients to control the stochastic level \u03c32 of the gradient. In Figure 1 one can see that simply aggregating the compressed local gradient without error compensation (termed Compressed-SGD (Khirirat et al., 2018; Alistarh et al., 2018)) does not lead to convergence, while EControl and EC do. This result shows the need to use EC to make the method convergent. Further, we use this simple synthetic problem to demonstrate some of the key features of EControl:\nIncreasing the number of clients. In Figure 2 we investigate the effect of the number of clients on the complexity of EControl. Crucially, the theory predicts that EControl achieves a linear speedup in terms of the number of clients when using a stochastic gradient. In our experiment, we fix a small stepsize and investigate the error that EControl converges to. We see that as the number of clients doubles, the error that EControl oscillates around is roughly divided by half. This confirms our theoretical prediction of the linear speedup.\nIndependence from gradient dissimilarity. In Figure 3 we see that EControl is not affected by the gradient dissimilarity parameter \u03b6 and its complexity stays stable across the three figures. On the other hand, the original EC suffers from the increasing \u03b6, taking longer to converge as \u03b6 increases. Moreover, as Theorem 2 predicts, the O( \u03c3 2\n\u00b5n\u03b5 ) term is dominant, and the performance of EControl (in terms of the number of bits sent) is superior over that of SGD with batch size n. The stepsizes \u03b3 are fine-tuned over {5\u00d7 10\u22125, 10\u22124, 5\u00d7 10\u22124, 10\u22123, 10\u22122, 10\u22121}, and for EControl we fine-tune \u03b7 over {10\u22123, 5\u00d7 10\u22123, 10\u22122, 5\u00d7 10\u22122, 10\u22121}. As \u03b6 increases, the performance of EControl is not affected, while the performance of EC deteriorates."
        },
        {
            "heading": "7.2 LOGISTIC REGRESSION PROBLEM",
            "text": "Next, we consider the Logistic Regression problem for multi-class classification trained on MNIST dataset (Deng, 2012) and implemented in Pytorch (Paszke et al., 2019). First, we split 50% of the dataset between 10 clients according to the labels (the data point with i-th labels belongs to client i + 1). The rest of the data is distributed randomly between clients. Then, for each client, we divide the local data into train (90%) and test (10%) sets. Such partition allows to make the problem\nmore heterogeneous. We compare the performance of EControl, EF21-SGDM, and EF21. For all methods, we use Top-K compression operator with K = d10 . We fine-tune the stepsizes of the methods over {1, 10\u22121, 10\u22122, 10\u22123}. Moreover, we fine-tune \u03b7 parameter for EControl over {0.2, 0.1, 0.05}, and for EF21-SGDM we set \u03b7 = 0.1 according to (Fatkhullin et al., 2023). We choose the stepsizes that achieve the best performances on the train set such that the test loss does not diverge. The results are presented in Figure 4. Note that the communication cost of the methods per iteration is the same, therefore, the number of epochs is proportional to the number of communicated bits.\nWe observe that EControl has the fastest convergence in terms of training loss. The same behavior is demonstrated with respect to test accuracy. Nevertheless, the test accuracy difference for all methods is within 1 percent."
        },
        {
            "heading": "7.3 TRAINING OF DEEP LEARNING MODELS",
            "text": "Finally, we consider the training of Deep Learning models: Resnet18 (He et al., 2016) and VGG13 (Simonyan & Zisserman, 2015). The implementation is done in Pytorch (Paszke et al., 2019). We run experiments on Cifar-10 (Krizhevsky et al., 2014) dataset. The dataset split across the clients is the same as for the Logistic Regression problem\u2014half is distributed randomly, and another half is portioned according to the labels. For the compression, we utilize Top-K compressor with Kd = 0.1. We fine-tune the stepsizes over the set {1, 0.1, 0.01} for EControl, EF21-SGDM, and EF21 and select the best stepsize on test set. For EControl and EF21-SGDM we set \u03b7 = 0.1.\nAccording to the results in Figure 5a, all methods achieve similar test accuracy, but EF21 has more unstable convergence with many ups and downs. EControl achieves a better stationary point as the training loss is smaller than for the other two methods. Similar results are demonstrated for VGG13 model; see Figure 5b. The training with EControl allows to reach a stationary point with a smaller training loss. Moreover, EControl outperforms other methods from a test accuracy point of view: it gives slightly better test accuracy than EF21 and considerably higher accuracy than EF21-SGDM."
        },
        {
            "heading": "8 DISCUSSION",
            "text": "In this work, we propose a novel algorithm EControl that provably converges efficiently in all standard settings: strongly convex, general convex, and nonconvex, with general contractive compression and without any additional assumption on the problem structure, hence resolving the open theoretical problem therein. We conduct extensive experimental evaluations of our method and show its efficacy in practice. Our method incurs little overhead compared to existing implementations of EC, and we believe it is an effective and lightweight approach to making EC suitable for distributed training of large machine learning models, especially when also using the standard momentum mechanism for reducing the variance of the stochastic gradients in deep learning training. However, the theoretical analysis of this extension is outside the scope of this paper, and we will leave it for future work."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "YG/SS acknowledge partial funding from a Google Scholar Research Award."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We provide source code as part of the supplementary material which allows to reproduce Deep Learning and synthetic least squares experiments. Additionally, for our theoretical results, we offer clear explanations of any assumptions in the main paper and complete proof of all statements in the appendix."
        },
        {
            "heading": "A USEFUL LEMMAS AND DEFINITIONS",
            "text": "In this section, we state the important notations and useful lemmas that we use in our convergence analysis. We use the following notation throughout the proofs\nFt := E [f(xt)]\u2212 f\u22c6, and Et := 1\nn n\u2211 i=1 E [\u2225\u2225eit\u2225\u22252] (8)\nFor shortness, in all our proofs we use the notation\ngit := g i(xt), gt :=\n1\nn n\u2211 i=1 git, et := 1 n n\u2211 i=1 eit, and ht := 1 n n\u2211 i=1 hit. (9)\nBesides, we additionally introduce the sequence of virtual iterates which are defined as follows\nx\u03030 := x0, x\u0303t+1 := x\u0303t \u2212 \u03b3\nn n\u2211 i=1 git. (10)\nPerforming simple derivations we get the link between real and virtual iterates of Algorithm 2\nxt \u2212 x\u0303t = \u03b3\nn n\u2211 i=1 eit. (11)\nIn addition to the notations introduced inEquation (8), we define Xt := E [ \u2225x\u0303t \u2212 x\u22c6\u22252 ] . (12)\nFinally, we introduce another quantity that bounds the size of the compressed message:\nHt := 1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit + git \u2212 hit\u2225\u22252] (13)\nThe following lemmas were taken from other works (as indicated) and we omit their proof. Lemma 1 (Lemma 8 from Stich & Karimireddy (2020)). Assume f is L-smooth and \u00b5-strongly quasi-convex. Let the sequences {xt}, {eit}i\u2208[n] be generated by Algorithm 2. If \u03b3 \u2264 14L , then\nXt+1 \u2264 ( 1\u2212 \u03b3\u00b5\n2\n) Xt \u2212 \u03b3\n2 Ft +\n\u03b32\nn \u03c32 + 3L\u03b33Et. (14)\nThe descent lemma in F\u0303t is taken from (Stich & Karimireddy, 2020) Lemma 2 (Lemma 9 from Stich & Karimireddy (2020)). Assuming that f is L-smooth, and xt and et are generated by Algorithm 2, then\nF\u0303t+1 \u2264 F\u0303t \u2212 \u03b3 4 E [ \u2225\u2207f(xt)\u22252 ] + \u03b33L2 2 Et + \u03b32L\u03c32 2n (15)\nFor the sake of completeness, we list two summation lemmas from (Stich, 2020) without proofs. The first lemma is used in order to derive convergence guarantees in the strongly convex case. Lemma 3 (Lemma 25 from Stich (2020)). Let {rt}t\u22650 and {st}t\u22650 be sequences of positive numbers satisfying rt+1 \u2264 (1\u2212min{\u03b3A, F})rt \u2212B\u03b3st + C\u03b32 +D\u03b33, (16) for some positive constants A,B > 0, C,D \u2265 0, and for constant stepsize 0 < \u03b3 \u2264 1E , for E \u2265 0, and for parameter 0 < F \u2264 1. Then there exists a constant stepsize \u03b3 \u2264 1E such that\nB\nWT T\u2211 t=0 wtst +min { A, F \u03b3 } rT+1 \u2264 r0 ( E + A F ) exp ( \u2212min { A E ,F } (T + 1) ) + 2C ln \u03c4\nA(T + 1) +\nD ln2 \u03c4\nA2(T + 1)2 for wt := (1\u2212min{\u03b3A, F})\u2212(t+1), WT := \u2211T t=0 wt, and\n\u03c4 := max { exp(1),min { A2r0(T + 1) 2\nC , A3r0(T + 1) 3 D\n}} .\nRemark 4 (Remark 26 from Stich (2020)). Lemma 3 established a bound of the order\nO\u0303 ( r0 ( E + A\nF\n) exp ( \u2212min { A\nE ,F\n} (T + 1) ) + C\nAT +\nD\nA2T 2\n)\nthat decreases with T . To ensure that this expression is smaller than \u03b5,\nT = O\u0303\n( C\nA\u03b5 +\n\u221a D A \u221a \u03b5 + 1 F log 1 \u03b5 + E A log 1 \u03b5\n) = O\u0303 ( C\nA\u03b5 +\n\u221a D A \u221a \u03b5 + 1 F + E A\n)\nsteps are sufficient.\nThe second lemma shows the convergence rate in the nonconvex or convex settings.\nLemma 5 (Lemma 27 from Stich (2020)). Let {rt}t\u22650 and {st}t\u22650 be sequences of positive numbers satisfying\nrt+1 \u2264 rt \u2212B\u03b3st + C\u03b32 +D\u03b33, (17)\nfor some positive constants A,B > 0, C,D \u2265 0, and for constant stepsize 0 < \u03b3 \u2264 1E , for E \u2265 0, and for parameter 0 < F \u2264 1. Then there exists a constant stepsize \u03b3 \u2264 1E such that\nB\nT + 1 T\u2211 t=0 st \u2264 Er0 T + 1 + 2D1/3 ( r0 T + 1 )2/3 + 2 ( Cr0 T + 1 )1/2 . (18)\nRemark 6 (Remark 28 from Stich (2020)). To ensure that the right-hand side of (18) is smaller than \u03b5 > 0,\nT = O ( Cr0 \u03b52 + \u221a Dr0 \u03b53/2 + Er0 \u03b5 ) .\nsteps are sufficient.\nB MISSING PROOFS FOR EControl\nIn this section we prove Theorem 2, Theorem 3 and Theorem 4."
        },
        {
            "heading": "B.1 STRONGLY CONVEX SETTING",
            "text": "We start with the strongly convex case setting. First, we bound the distance between two consecutive iterates.\nLemma 7. Let f be L-smooth, then:\nE [ \u2225xt+1 \u2212 xt\u22252 ] \u2264 \u03b32 ( 2(1\u2212 \u03b4)Ht + 4\u03b72Et + 4LFt + 2\u03c32\nn\n) . (19)\nProof.\n1 \u03b32 E [ \u2225xt+1 \u2212 xt\u22252 ] = E [ \u2225ht +\u2206t\u22252 ] = E [ \u2225\u2206t \u2212 \u03b7et \u2212 gt + ht + \u03b7et + gt\u22252\n] (i)\n\u2264 2E [ \u2225\u2206t \u2212 \u03b7et \u2212 gt + ht\u22252 ] + 2E [ \u2225\u03b7et + gt\u22252 ] (ii)\n\u2264 2E [ \u2225\u2206t \u2212 \u03b7et \u2212 gt + ht\u22252 ] + 2E [ \u2225\u03b7et +\u2207f(xt)\u22252 ] + 2\u03c32\nn (iii)\n\u2264 2 n n\u2211 i=1 E [\u2225\u2225\u2206it \u2212 \u03b7eit \u2212 git + hit\u2225\u22252]+ 4\u03b72n n\u2211 i=1 E [\u2225\u2225eit\u2225\u22252]\n+ 4E [ \u2225\u2207f(xt)\u22252 ] + 2\u03c32\nn (iv)\n\u2264 2(1\u2212 \u03b4) n n\u2211 i=1 E [\u2225\u2225\u03b7eit + git \u2212 hit\u2225\u22252]+ 4\u03b72n n\u2211 i=1 E [\u2225\u2225eit\u2225\u22252]\n+ 4LE [f(xt)\u2212 f\u22c6] + 2\u03c32\nn .\nwhere in (i)\u2212 (iii) we use Young\u2019s inequality; in (ii) we use Assumption 5, and in (iv) we use the definition of the compressor, L-smoothness and convexity.\nThe next lemma gives the descent of Et.\nLemma 8. For any \u03b1 > 0 we have:\nEt+1 \u2264 (1 + \u03b1)(1\u2212 \u03b7)2Et + (1 + \u03b1\u22121)(1\u2212 \u03b4)Ht (20)\nProof.\nEt+1 = 1\nn n\u2211 i=1 E [\u2225\u2225eit + git \u2212 hit \u2212\u2206it\u2225\u22252]\n(i) \u2264 (1 + \u03b1\u22121) 1 n n\u2211 i=1 E [\u2225\u2225\u03b7eit + git \u2212 hit \u2212\u2206it\u2225\u22252]+ (1 + \u03b1)(1\u2212 \u03b7)2Et\n(ii)\n\u2264 (1 + \u03b1\u22121)(1\u2212 \u03b4) 1 n n\u2211 i=1 E [\u2225\u2225\u03b7eit + git \u2212 hit\u2225\u22252]+ (1 + \u03b1)(1\u2212 \u03b7)2Et = (1 + \u03b1\u22121)(1\u2212 \u03b4)Ht + (1 + \u03b1)(1\u2212 \u03b7)2Et.\nwhere in (i) we use Young\u2019s inequality, and in (ii) we use the definition of the compressor.\nNow we give the descent of Ht.\nLemma 9. Let f be L-smooth and each fi be Li-smooth. Let \u03b7 = \u03b44k for some k \u2265 1 and \u03b3 \u2264 \u03b4 32 \u221a 2L\u0303 . Then we have:\nHt+1 \u2264 ( 1\u2212 \u03b4\n32\n) Ht + ( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n) Et + 128L\u03032L\u03b32\n\u03b4 Ft\n+ 64\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32. (21)\nProof. We unroll the definition of Ht+1\nHt+1 = 1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit+1 + git+1 \u2212 hit+1\u2225\u22252]\n= 1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit + \u03b7git \u2212 \u03b7hit \u2212 \u03b7\u2206it \u2212 hit \u2212\u2206it + git+1\u2225\u22252]\n(i) \u2264 (1 + \u03b21) 1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit + (1 + \u03b7)git \u2212 (1 + \u03b7)hit \u2212 (1 + \u03b7)\u2206it\u2225\u22252]\n+ (1 + \u03b2\u221211 ) 1\nn n\u2211 i=1 E [\u2225\u2225git+1 \u2212 git\u2225\u22252]\n(ii)\n\u2264 (1 + \u03b21) 1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit + (1 + \u03b7)git \u2212 (1 + \u03b7)hit \u2212 (1 + \u03b7)\u2206it\u2225\u22252]\n+ 2(1 + \u03b2\u221211 ) 1\nn n\u2211 i=1 E [ \u2225\u2207fi(xt+1)\u2212\u2207fi(xt)\u22252 ] + 4(1 + \u03b2\u221211 )\u03c3 2\n(iii)\n\u2264 (1 + \u03b21) 1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit + (1 + \u03b7)git \u2212 (1 + \u03b7)hit \u2212 (1 + \u03b7)\u2206it\u2225\u22252]\n+ 2(1 + \u03b2\u221211 )L\u0303 2E [ \u2225xt+1 \u2212 xt\u22252 ] + 4(1 + \u03b2\u221211 )\u03c3 2, (22)\nwhere in (i) we use Young\u2019s inequality, in (ii) we use assumption 5, and in (iii) we use smoothness of fi. Now we consider the first term in the above:\n1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit + (1 + \u03b7)git \u2212 (1 + \u03b7)hit \u2212 (1 + \u03b7)\u2206it\u2225\u22252]\n(i) \u2264 (1 + \u03b22)(1 + \u03b7)2 1\nn n\u2211 i=1 E [\u2225\u2225\u03b7eit + git \u2212 hit \u2212\u2206it\u2225\u22252]+ (1 + \u03b2\u221212 )\u03b74Et\n(ii)\n\u2264 (1 + \u03b22)(1 + \u03b7)2(1\u2212 \u03b4)Ht + (1 + \u03b2\u221212 )\u03b74Et, where in (i) we use Young\u2019s inequality, and in (ii) we use the definition of the compressor. Putting it back in (22), we have:\nHt+1 \u2264 (1 + \u03b21)(1 + \u03b22)(1 + \u03b7)2(1\u2212 \u03b4)Ht + (1 + \u03b21)(1 + \u03b2\u221212 )\u03b74Et + 2(1 + \u03b2\u221211 )L\u0303 2E [ \u2225xt+1 \u2212 xt\u22252 ] + 4(1 + \u03b2\u221211 )\u03c3 2\nBy Lemma 7, we can substitute E [ \u2225xt+1 \u2212 xt\u22252 ] and get:\nHt+1 \u2264 (1 + \u03b21)(1 + \u03b22)(1 + \u03b7)2(1\u2212 \u03b4)Ht + (1 + \u03b21)(1 + \u03b2\u221212 )\u03b74Et\n+ 2(1 + \u03b2\u221211 )L\u0303 2\u03b32 ( 2(1\u2212 \u03b4)Ht + 4\u03b72Et + 4LFt + 2\u03c32\nn ) + 4(1 + \u03b2\u221211 )\u03c3 2\n= [ (1 + \u03b21)(1 + \u03b22)(1 + \u03b7) 2(1\u2212 \u03b4) + 4(1 + \u03b2\u221211 )L\u03032\u03b32 ] Ht\n+ [ (1 + \u03b21)(1 + \u03b2 \u22121 2 )\u03b7 4 + 8(1 + \u03b2\u221211 )L\u0303 2\u03b32\u03b72 ] Et\n+ 8(1 + \u03b2\u221211 )L\u0303 2L\u03b32Ft + [ 4(1 + \u03b2\u221211 ) + 4(1 + \u03b2\u221211 )L\u0303 2\u03b32\nn\n] \u03c32.\nSince \u03b7 = \u03b44k for some k \u2265 1, we must have:\n(1 + \u03b7)2(1\u2212 \u03b4) \u2264 1\u2212 \u03b4 4 .\nLet us choose \u03b21 := \u03b416\u22122\u03b4 and \u03b22 := \u03b4 8\u22122\u03b4 , so we get:\nHt+1 \u2264 ( 1\u2212 \u03b4\n16 +\n64L\u03032\u03b32\n\u03b4\n) Ht + ( 8\u03b74\n\u03b4 +\n128L\u03032\u03b32\u03b72\n\u03b4\n) Et\n+ 128L\u03032L\u03b32\n\u03b4 Ft +\n64\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32\n= ( 1\u2212 \u03b4\n16 +\n64L\u03032\u03b32\n\u03b4\n) Ht + ( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n) Et\n+ 128L\u03032L\u03b32\n\u03b4 Ft +\n64\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32.\nIf \u03b3 \u2264 \u03b4 32 \u221a 2L\u0303 , then:\nHt+1 \u2264 ( 1\u2212 \u03b4\n32\n) Ht + ( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n) Et\n+ 128L\u03032L\u03b32\n\u03b4 Ft +\n64\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32\nFinally, we consider the Lyapunov function \u03a8 := Xt + aHt + bEt where constants a and b are set as follows: b := 48kL\u03b3 3\n\u03b4 , a := 512kb \u03b42 , where k will be set later.\nLemma 10. Let f be L-smooth and \u00b5-strongly quasi-convex around x\u22c6, and each fi be Li-smooth. Let \u03b7 = \u03b4400 , and \u03b3 \u2264 \u03b4 3200 \u221a 2L\u0303 , then we have\n\u03a8t+1 \u2264 ( 1\u2212min { \u03b3\u00b5\n2 ,\n\u03b4\n8850\n}) \u03a8t \u2212 \u03b3\n4 Ft + \u03b3\n2\u03c3 2\nn + \u03b33\n1011L\u03c32\n\u03b44 . (23)\nProof. With \u03b7 = \u03b4k(4\u22122\u03b4) we set \u03b1 = \u03b4 8k\u22122\u03b4 in Lemma 8, and get Et+1 \u2264 ( 1\u2212 \u03b4\n8k\n) Et + 8k\n\u03b4 Ht\nWe can put the link between the real and virtual iterates (11) and the descent lemma 1 for Xt together and get\n\u03a8t+1 \u2264 ( 1\u2212 \u03b3\u00b5\n2\n) Xt \u2212 \u03b3\n2 Ft +\n\u03b32\nn \u03c32 + 3L\u03b33Et\n+ a [( 1\u2212 \u03b4\n32\n) Ht + ( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n) Et + 128L\u03032L\u03b32\n\u03b4 Ft\n+ 64\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32 ] + b [( 1\u2212 \u03b4\n8k\n) Et + 8k\n\u03b4 Ht\n] .\nRearranging the terms we continue as follows \u03a8t+1 \u2264 ( 1\u2212 \u03b3\u00b5\n2\n) Xt + ( 1\u2212 \u03b4\n32 +\n8kb\n\u03b4a\n) aHt\n+ ( 1\u2212 \u03b4\n8k +\n3L\u03b33\nb +\na\nb\n( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n)) bEt\n\u2212 \u03b3 2 Ft +\n128L\u03032L\u03b32a\n\u03b4 Ft + \u03b3\n2\u03c3 2\nn +\n64a\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32.\nNow we set \u03b3 \u2264 \u03b4 32 \u221a 2kL\u0303 , and then we get:\n\u03a8t+1 \u2264 ( 1\u2212 \u03b3\u00b5\n2\n) Xt + ( 1\u2212 \u03b4\n32 +\n8kb\n\u03b4a\n) aHt + ( 1\u2212 \u03b4\n8k +\n3L\u03b33\nb +\n\u03b43a\n16k4b\n) bEt\n\u2212 \u03b3 2 Ft +\n128L\u03032L\u03b32a\n\u03b4 Ft + \u03b3\n2\u03c3 2\nn +\n64a\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32.\nLet b = 48kL\u03b3 3\n\u03b4 and a = 512kb \u03b42 , then for the coefficient next to Ht have\n1\u2212 \u03b4 32 + 8kb \u03b4a \u2264 1\u2212 \u03b4 64 .\nFor the coefficient next to Et term we have\n1\u2212 \u03b4 8k + 3L\u03b33 b + \u03b43a 16k4b \u2264 1\u2212 \u03b4 16k + 32\u03b4 k3 .\nFor the coefficient next to Ft term we have\n\u2212\u03b3 2 +\n128L\u03032L\u03b32a\n\u03b4 \u2264 \u2212\u03b3 2 + 3\u03b3 4k2 .\nFinally, for the coefficient next to \u03c32 term we have\n64a\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u2264 \u03b33 10 7k2L\n\u03b44 .\nTherefore, combining all together and setting k = 100, we get: \u03a8t+1 \u2264 ( 1\u2212 \u03b3\u00b5\n2\n) Xt + ( 1\u2212 \u03b4\n64\n) aHt + ( 1\u2212 \u03b4\n8850\n) bEt\n\u2212 \u03b3 4 Ft + \u03b3\n2\u03c3 2\nn + \u03b33\n1011L\u03c32\n\u03b44 \u2264 ( 1\u2212min { \u03b3\u00b5\n2 ,\n\u03b4\n8850\n}) \u03a8t \u2212 \u03b3\n4 Ft + \u03b3\n2\u03c3 2\nn + \u03b33\n1011L\u03c32\n\u03b44 .\nNow we give the precise statement of Theorem 2 Theorem 5. Let f be \u00b5-strongly quasi-convex around x\u22c6 and L-smooth. Let each fi be Li-smooth. Then for \u03b7 = \u03b4400 , there exists a \u03b3 \u2264 \u03b4 3200 \u221a 2L\u0303 such that after at most\nT = O\u0303\n( \u03c32\n\u00b5n\u03b5 +\n\u221a L\u03c3\n\u00b5\u03b42\u03b51/2 +\nL\u0303\n\u00b5\u03b4 ) iterations of Algorithm 2 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where xout is chosen randomly from xt \u2208 {x0, . . . ,xT } with probabilities proportional to (1\u2212 \u03b3\u00b52 ) \u2212(t+1).\nProof. The claim of theorem 5 follows from Lemma 10; Lemma 3 and remark 4 from (Stich, 2020). Note that by the initialization, we have \u03a80 = \u2225x0 \u2212 x\u22c6\u22252. Also note that by the choice of parameters \u03b3\u00b5 2 \u2264 \u03b4 8850 ."
        },
        {
            "heading": "B.2 CONVEX SETTING",
            "text": "We switch to the convex regime. The considered setting differs from the previous one by setting \u00b5 = 0. Then the claim of Lemma 10 changes as follows. Lemma 11. Let f be L-smooth and quasi convex, and each fi be Li-smooth. Let \u03b7 = \u03b4400 , and \u03b3 \u2264 \u03b4\n3200 \u221a 2L\u0303 , then we have\n\u03a8t+1 \u2264 \u03a8t \u2212 \u03b3\n4 Ft + \u03b3\n2\u03c3 2\nn + \u03b33\n1011L\u03c32\n\u03b44 , (24)\nProof. The proof immediately follows from lemma 10 by plugging in \u00b5 = 0.\nNow we give the precise statement of Theorem 3 Theorem 6. Let f be quasi-convex around x\u22c6 and L-smooth. Let each fi be Li-smooth. Then for \u03b7 = \u03b4400 , there exists a \u03b3 \u2264 \u03b4 3200 \u221a 2L\u0303 such that after at most\nT = O\n( R0\u03c3 2\nn\u03b52 +\n\u221a LR0\u03c3\n\u03b42\u03b53/2 + L\u0303R0 \u03b4\u03b5 ) iterations of Algorithm 2 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where R0 := \u2225x0 \u2212 x\u22c6\u22252 and xout is chosen uniformly at random from xt \u2208 {x0, . . . ,xT }.\nProof. The claim of theorem 6 follows from lemma 11; lemma 5 and remark 6 from (Stich & Karimireddy, 2020). Note that by the initialization \u03a80 = R0."
        },
        {
            "heading": "B.3 NONCONVEX SETTING",
            "text": "Now we give the small modification of lemma 7 that is used in the nonconvex setting. Lemma 12. Let f be L-smooth, then:\nE [\u2225xt+1 \u2212 xt\u2225] \u2264 \u03b32 ( 2(1\u2212 \u03b4)Ht + 4\u03b72Et + 4E [ \u2225\u2207f(xt)\u22252 ] + 2\u03c32\nn\n) . (25)\nProof.\n1 \u03b32 E [ \u2225xt+1 \u2212 xt\u22252 ] = E [ \u2225ht +\u2206t\u22252 ] = E [ \u2225\u2206t \u2212 \u03b7et \u2212 gt + ht + \u03b7et + gt\u22252\n] (i)\n\u2264 2E [ \u2225\u2206t \u2212 \u03b7et \u2212 gt + ht\u22252 ] + 2E [ \u2225\u03b7et + gt\u22252 ] (ii)\n\u2264 2E [ \u2225\u2206t \u2212 \u03b7et \u2212 gt + ht\u22252 ] + 2E [ \u2225\u03b7et +\u2207f(xt)\u22252 ] + 2\u03c32\nn (iii)\n\u2264 2 n n\u2211 i=1 E [\u2225\u2225\u2206it \u2212 \u03b7eit \u2212 git + hit\u2225\u22252]+ 4\u03b72n n\u2211 i=1 E [\u2225\u2225eit\u2225\u22252]\n+ 4E [ \u2225\u2207f(xt)\u22252 ] + 2\u03c32\nn (iv)\n\u2264 2(1\u2212 \u03b4) n n\u2211 i=1 E [\u2225\u2225\u03b7eit + git \u2212 hit\u2225\u22252]+ 4\u03b72n n\u2211 i=1 E [\u2225\u2225eit\u2225\u22252]\n+ 4E [ \u2225\u2207f(xt)\u22252 ] + 2\u03c32\nn .\nwhere in (i)\u2212 (iii) we use Young\u2019s inequality; in (ii) we use Assumption 5, and in (iv) we use the definition of the compressor.\nNext, we give a simple modification of lemma 9 in nonconvex setting as well. Lemma 13. Let f be L-smooth, and each fi be Li-smooth. Let \u03b7 = \u03b44k for some k \u2265 1 and \u03b3 \u2264 \u03b4\n32 \u221a 2L\u0303 . Then we have\nHt+1 \u2264 ( 1\u2212 \u03b4\n32\n) Ht + ( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n) Et + 128L\u03032\u03b32 \u03b4 E [ \u2225\u2207f(xt)\u22252 ] + 64\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32. (26)\nProof. The proof is almost exactly the same to the proof of lemma 9 where the bound of E [ \u2225xt+1 \u2212 xt\u22252 ] by lemma 7 is replaced by lemma 12.\nNow we consider the Lyapunov function \u03a8t := F\u0303t + aHt + bEt where constants a and b are set as follows: b := 8L 2\u03b33\n\u03b4 , a := 512kb \u03b42 , where k will be set later.\nLemma 14. Let f be L-smooth, and each fi be Li-smooth. Let \u03b7 = \u03b4400 and \u03b3 \u2264 \u03b4 3200 \u221a 2L\u0303 . Then we have:\n\u03a8t+1 \u2264 \u03a8t \u2212 \u03b3 8 E [ \u2225\u2207f(xt)\u22252 ] + \u03b32 L\u03c32 2n + \u03b33 1010L2\u03c32 \u03b44 . (27)\nProof. Note that lemma 8 still holds in the smooth nonconvex case. Therefore, with \u03b7 = \u03b44k , if in (20) we set \u03b1 = \u03b48k\u22122\u03b4 , then:\nEt+1 \u2264 ( 1\u2212 \u03b4\n8k\n) Et + 8k\n\u03b4 Ht.\nNow we put all inequalities together:\n\u03a8t+1 \u2264 F\u0303t \u2212 \u03b3 4 E [ \u2225\u2207f(xt)\u22252 ] + \u03b33L2 2 Et + \u03b32L 2n \u03c32\n+ a [( 1\u2212 \u03b4\n32\n) Ht + ( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n) Et + 128L\u03032\u03b32 \u03b4 E [ \u2225\u2207f(xt)\u22252 ] + 64\n\u03b4\n( 1 + 2L\u03032\u03b32\nn\n) \u03c32 ]\n+ b [( 1\u2212 \u03b4\n8k\n) Et + 8k\n\u03b4 Ht ] = F\u0303t + ( 1\u2212 \u03b4\n32 +\n8kb\n\u03b4a\n) aHt\n+ ( 1\u2212 \u03b4\n8k +\nL2\u03b33\n2b +\na\nb\n( 8\u03b43\nk444 +\n128L\u03032\u03b32\u03b4\nk242\n)) bEt\n\u2212 \u03b3 4 E [ \u2225\u2207f(xt)\u22252 ] + 128L\u03032\u03b32a \u03b4 E [ \u2225\u2207f(xt)\u22252 ] + \u03b32 L\u03c32\n2n +\n64a\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u03c32.\nSimilar as before, we now set \u03b3 \u2264 \u03b4 32 \u221a 2kL\u0303 and get:\n\u03a8t+1 \u2264 F\u0303t + ( 1\u2212 \u03b4\n32 +\n8kb\n\u03b4a\n) aHt + ( 1\u2212 \u03b4\n8k +\nL2\u03b33\n2b +\n\u03b43a\n16k4b\n) bEt\n\u2212 \u03b3 4 E [ \u2225\u2207f(xt)\u22252 ] + 128L\u03032\u03b32a \u03b4 E [ \u2225\u2207f(xt)\u22252 ] + \u03b32 L 2n \u03c32 + 64a \u03b4 ( 1 + L\u03032\u03b32 n ) \u03c32.\nLet b = 8kL 2\u03b33\n\u03b4 and a = 512kb \u03b42 , then for the coefficient next to Ht term, we have\n1\u2212 \u03b4 32 + 8kb \u03b4a \u2264 1\u2212 \u03b4 64 ;\nfor the coefficient next to Et term, we have\n1\u2212 \u03b4 8k + L2\u03b33 2b + \u03b43a 16k4b \u2264 1\u2212 \u03b4 16k + \u03b43a 16k4b \u2264 1\u2212 \u03b4 16k + 32\u03b4 k3 ;\nfor the coefficient next to E [ \u2225\u2207f(xt)\u22252 ] term, we have\n\u2212\u03b3 4 +\n128L\u03032\u03b32a\n\u03b4 \u2264 \u2212\u03b3 4 + \u03b3 8k2 ;\nfor the coefficient next to \u03c32 term, we have\n64a\n\u03b4\n( 1 + L\u03032\u03b32\nn\n) \u2264 \u03b33 10 6k2L2\n\u03b44 .\nTherefore, if we set k = 100, we get: \u03a8t+1 \u2264 F\u0303t + ( 1\u2212 \u03b4\n64\n) aHt + ( 1\u2212 \u03b4\n8850\n) bEt\n\u2212 \u03b3 8 E [ \u2225\u2207f(xt)\u22252 ] + \u03b32 L\u03c32 2n + \u03b33 1010L2\u03c32 \u03b44\n\u2264 \u03a8t \u2212 \u03b3 8 E [ \u2225\u2207f(xt)\u22252 ] + \u03b32 L\u03c32 2n + \u03b33 1010L2\u03c32 \u03b44 .\nNow we give the precise statement of Theorem 4 Theorem 7. Let f be L-smooth, and each fi be Li-smooth. Then for \u03b7 = \u03b4400 , there exists a \u03b3 \u2264 \u03b4\n3200 \u221a 2L\u0303 such that after at most\nT = O\n( LF0\u03c3 2\nn\u03b52 +\nLF0\u03c3 \u03b42\u03b53/2 + L\u0303F0 \u03b4\u03b5\n)\niterations of Algorithm 2 it holds E [ \u2225\u2207f(xout)\u22252 ] \u2264 \u03b5, where F0 := f(x0)\u2212 f\u22c6 and xout is chosen\nuniformly at random from xt \u2208 {x0, . . . ,xT }.\nProof. The claim of theorem 7 follows from lemma 14; lemma 5 and remark 6 from (Stich, 2020). Note that by our choice of initialization, \u03a80 = F0.\nRemark 15. Notice that we do not explicitly bound the distance between hit and the full local gradient \u2207fi(xt) like in the proof of EF21 (see also Appendix G). Instead, we only control the size of the error term and the compressed message. Interestingly, this way we also indirectly control the distance between hit and the local stochastic gradient:\n1\nn n\u2211 i=1 E [\u2225\u2225git \u2212 hit\u2225\u22252] \u2264 2 1n n\u2211 i=1 E [\u2225\u2225\u03b7eit + git \u2212 hit\u2225\u22252]+ 21n n\u2211 i=1 \u03b72E [\u2225\u2225eit\u2225\u22252]\n= 2Ht + 2\u03b7 2Et\nWith the choice \u03b7 \u223c \u03b4, Ht + \u03b72Et is proportional to the aHt + bEt term involved in the Lyapunov function we use in the analysis. Taking into account the convergence guarantees for the Lyapunov function, the above derivations show that the distance between hit and g i t indeed is controled and does not blow up.\nC IMPORTANCE OF \u03b7 CHOICE\nC.1 THE ROLE OF \u03b7 FROM A THEORY PERSPECTIVE\nHere we review the role of \u03b7 in the lemmas, and shine some light on why the choice \u03b7 \u223c \u03b4 is important. For simplicity, we only consider the strongly convex case.\n\u2022 The descent of Et is achieved through \u03b7. In particular, in Lemma 8, Et is scaled by (1 + \u03b1)(1\u2212 \u03b7)2. For \u03b7 \u223c \u03b4 and appropriate choice of \u03b1, we get a descent on Et at the scale (1\u2212 \u2126(\u03b4)), which is proportional to the descent of Ht in Lemma 9.\n3200\n\u221a\n2L\n, i.e. theoretical value of the stepsize.\n\u2022 Perhaps more importantly, \u03b7 controls the contribution of Et, and balances the scale between Et and Ht. Crucially, an Et term is introduced into the descent of Ht in Lemma 9 via the upper bound on E [ \u2225xt \u2212 xt+1\u22252 ] (Lemma 7). With the \u03b7 term, the contribution of Et from\nE [ \u2225xt \u2212 xt+1\u22252 ] is of the order O(\u03b32\u03b72/\u03b4). This turned out to be extremely important in\nLemma 10, where b \u223c \u03b42a (recall that a is the coefficient of Ht and b is the coefficient of Et in the Lyapunov function \u03a8). Taking a closer look at Lemma 10, we see that setting \u03b7 \u223c \u03b4 scales the Et term from the descent of Ht by a\u03b3 2\u03b72 b\u03b4 \u223c \u03b32\n\u03b4 . This allows us to pick \u03b3 \u223c \u03b4 resulting in a \u03b4 scale in front of the Et term from the descent of Ht.\nC.2 UNSTABLE BEHAVIOR WHEN \u03b7 = 1\nLet us first consider a simple problem with n = 2, d = 3 where f1 and f2 are defined as follows\nf1(x) = (1, 1, 5) \u22a4x+\n1 2 \u2225x\u22252, f2(x) = (1, 5, 1)\u22a4x+ 1 2 \u2225x\u22252.\nObviously, this problem is strongly convex. We show that \u03b7 parameter plays an essential role in stabilizing the convergence of EControl. We show that if \u03b7 = 1, then with the specific choice of initialization EControl may diverge while with \u03b7 = \u03b4 it converges.\nIn the first set of experiments we consider ei0 = 0,h i 0 = 0, while in the second one we initialize as ei0 = 0,h i 0 = \u2207fi(x0). For simplicity, we use full gradients in both cases in order not to be affected by the noise. We apply Top-1 compression operator in all the cases, i.e. \u03b4 = 13 . For EControl with \u03b7 = \u03b4 we set \u03b3 according to Theorem 2.\nWe demonstrate the convergence in both cases for EControl with \u03b7 = 1 and \u03b7 = \u03b4. The results are presented in Figure 6. We observe that in both cases EControl with \u03b7 = \u03b4 converges linearly to the solution as it is predicted by our theory. In contrast, EControl with \u03b7 = 1 converges only if hi0 = \u2207fi(x0) and diverges if hi0 = 0 regardless of the choice of \u03b3. Very small values of the stepsize postpone the gradient norm blow up. This example illustrates that \u03b7 \u223c \u03b4 makes the algorithm converge more stable, i.e. it is necessary for efficient performance.\nC.3 COMPARISON WITH EF21 AND DIMINISHING \u03b7\nNote that in Algorithm 2, when \u03b7 \u2192 0, the algorithm recovers EF21. However, the crucial difference between EControl and EF21 is precisely the fact that each update is injected the scaled error term, which is the key ingredient for achieving the linear speedup in the number of clients. In this section we use the synthetic least squares problem in Section 7.1 to demonstrate that, as \u03b7 \u2192 0, the performance of EControl degrades in the stochastic regime and its performance converges to that of EF21. The results are summarized in Figure 7.\nAlgorithm 3 EC-Approximate: EC with Approximate Bias Correction 1: Input: x0,\u03b3, eit=0d, C\u03b4 , and {hi}i\u2208[n] such that 1n \u2211n i=1 E [\u2225\u2225hi \u2212\u2207fi(x\u22c6)\u2225\u22252] \u2264 \u03b1\n2: h = 1n \u2211n i=1 h i 3: for t = 0, 1, 2, . . . do 4: git = g i t \u25bd client side 5: \u2206\u0302it = C\u03b4(eit + git \u2212 hi) 6: eit+1 = e i t + g i t \u2212 hi \u2212 \u2206\u0302it 7: send to server: \u2206\u0302it 8: xt+1 := xt \u2212 \u03b3h\u2212 \u03b3n \u2211n i=1 \u2206\u0302 i t \u25bd server side\nD EC-Approximate: APPROXIMATE BIAS CORRECTION\nIn Section 4 we described the ideal version of EC when we have access to hi\u22c6 = \u2207fi(x\u22c6). However, in practice, it is not known most of the time. Therefore, we propose to use a good enough approximation instead to make the method implementable. This idea leads to EC-Approximate summarised in Algorithm 3. There, instead of hi\u22c6 we use its estimator h\ni which should estimate the true value hi\u22c6 well enough. In more details, we run EC with hi satisfying\n1\nn n\u2211 i=1 E [\u2225\u2225hi \u2212\u2207fi(x\u22c6)\u2225\u22252] \u2264 \u03b1,\nin other words, the average distance between hi and \u2207fi(x\u22c6) is at most \u03b1. A trivial choice of h is the zero vectors, for which Algorithm 3 recovers the D-EC-SGD (Stich, 2020) algorithm with bounded gradient (at optimum) assumption. However, the smaller \u03b1 is (i.e., the better approximation we have), the better convergence is. Thus, it is beneficial to obtain hi after a preprocessing. For example, a nontrivial choice would be some hi such that 1n \u2211n i=1 E\n[\u2225\u2225hi \u2212\u2207fi(x\u22c6)\u2225\u22252] \u2264 O(\u03c32L\u03b42\u00b5 ). Such {hi}i\u2208[n] can be obtained by running EF21 for O\u0303( L\u03b4\u00b5 ) rounds, which is constant and does not depend on the accuracy \u03b5. We show that it is indeed possible in Appendix G for completeness."
        },
        {
            "heading": "D.1 CONVERGENCE ANALYSIS",
            "text": "In this section we prove the convergence of Algorithm 3. The next lemma bounds the descent of Et. Lemma 16. Let fi be L-smooth and convex. Let {hi}i\u2208[n] be such that 1 n \u2211n i=1 E [\u2225\u2225hi \u2212\u2207fi(x\u22c6)\u2225\u22252] \u2264 \u03b1, then the iterates of EC-Approximate satisfy Et+1 \u2264 ( 1\u2212 \u03b4\n2\n) Et + 8L\n\u03b4 Ft +\n4\u03b1\n\u03b4 + \u03c32. (28)\nProof.\nEt+1 = 1\nn n\u2211 i=1 E [\u2225\u2225eit + git \u2212 hi \u2212 C\u03b4(eit + git \u2212 hi)\u2225\u22252]\n\u2264 (1\u2212 \u03b4) n n\u2211 i=1 E [\u2225\u2225eit + git \u2212 hi\u2225\u22252]\n= (1\u2212 \u03b4)\nn n\u2211 i=1 E [\u2225\u2225eit +\u2207fi(xt)\u2212 hi\u2225\u22252]+ \u03c32\n\u2264 (1\u2212 \u03b4)(1 + \u03b2) n n\u2211 i=1 E [\u2225\u2225eit\u2225\u22252]\n+ (1\u2212 \u03b4)(1 + \u03b2\u22121)\nn\nn\u2211 i=1 E [\u2225\u2225\u2207fi(xt)\u2212 hi\u2225\u22252]+ \u03c32.\nwhere in the last inequality we used Young\u2019s inequality. Setting \u03b2 = \u03b42(1\u2212\u03b4) , we have: Et+1 \u2264 ( 1\u2212 \u03b4\n2\n) Et + 4\n\u03b4\n1\nn n\u2211 i=1 E [ \u2225\u2207fi(xt)\u2212\u2207fi(x\u22c6)\u22252 ] + 4\n\u03b4\n1\nn n\u2211 i=1 E [\u2225\u2225\u2207fi(x\u22c6)\u2212 hi\u2225\u22252]+ \u03c32\n\u2264 ( 1\u2212 \u03b4\n2\n) Et + 8L\n\u03b4 Ft +\n4\u03b1\n\u03b4 + \u03c32.\nwhere for the first inequality we used Young\u2019s inequality again, and for the second inequality we used the smoothness and convexity of fi, and our assumption on hi.\nNow consider the Lyapunov function \u03a8t = Xt + aEt for a := 12L\u03b3 3\n\u03b4 . We obtain the following descent lemma in \u03a8t. Lemma 17. Let f be \u00b5-strongly quasi-convex around x\u22c6, and each fi be L-smooth and convex. Let {hi}i\u2208[n] be such that 1n \u2211n i=1 E\n[\u2225\u2225hi \u2212\u2207fi(x\u22c6)\u2225\u22252] \u2264 \u03b1, and the stepsize be \u03b3 \u2264 \u03b48\u221a6L . Then the iterates of EC-Approximate satisfy\n\u03a8t+1 \u2264 (1\u2212 c)\u03a8t \u2212 \u03b3\n4 Ft + \u03b3\n2\u03c3 2\nn + \u03b33\n( 48L\u03b1\n\u03b42 +\n12L\u03c32\n\u03b4\n) (29)\nwhere a := 12L\u03b3 3\n\u03b4 and c := \u03b3\u00b5 2\nProof. Putting Lemma 1 and Lemma 16 together, we have: \u03a8t+1 \u2264 ( 1\u2212 \u03b3\u00b5\n2\n) Xt \u2212 \u03b3\n2 Ft + \u03b3\n2\u03c3 2\nn + 3L\u03b33Et\n+ a (( 1\u2212 \u03b4\n2\n) Et + 8L\n\u03b4 Ft +\n4\u03b1\n\u03b4 + \u03c32 ) = ( 1\u2212 \u03b3\u00b5\n2\n) Xt + ( 1\u2212 \u03b4\n2 +\n3L\u03b33\na\n) aEt\n+ \u03b32 \u03c32\nn + a\n( 4\u03b1\n\u03b4 + \u03c32 ) \u2212 ( \u03b3\n2 \u2212 8La \u03b4\n) Ft.\nPlugging in the choice of a, we have for the Et term:\n1\u2212 \u03b4 2 +\n3L\u03b33\na = 1\u2212 \u03b4 4 ,\nand for Ft term: \u03b3\n2 \u2212 8La \u03b4 = \u03b3 2 \u2212 \u03b3 96L\n2\u03b32\n\u03b42 \u2265 \u03b3 2 .\nCombining the above together we derive the statement of the lemma.\nTheorem 8 (EC-Approximate: EC with approximate bias correction). Let f : Rd \u2192 R be \u00b5-strongly quasi-convex, and each fi be L-smooth and convex. Let {hi}i\u2208[n] be such that 1 n \u2211n i=1 E [\u2225\u2225hi \u2212\u2207fi(x\u22c6)\u2225\u22252] \u2264 \u03b1. Let stepsize be \u03b3 \u2264 \u03b48\u221a6L . Then after at most T = O\u0303 ( \u03c32\n\u00b5n\u03b5 +\n\u221a L\u03c3\n\u00b5 \u221a \u03b4 \u221a \u03b5 +\n\u221a L\u03b1 \u00b5\u03b4 \u221a \u03b5 + L \u00b5\u03b4\n)\niterations of Algorithm 3 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where xout is chosen randomly from {x0, . . . ,xT } with probabilities proportional to (1\u2212 \u00b5\u03b3/2)\u2212(t+1).\nProof. We need to apply the results of Lemma 17, Lemma 3, and Remark 4 noticing that \u03b3\u00b52 \u2264 \u03b4 2 always due to the choice of the stepsize.\nAccording to the statement of Theorem 8, the inaccuracy in the approximation affects the higher order terms only. We clearly see that EC-Approximate achieves optimal sample complexity. This result suggests that preprocessing in the beginning of the training for a constant number of iterations may lead to better convergence guarantees in comparison with original EC.\nIn Appendix G we show that in a constant number of rounds of preprocessing using EF21, one can obtain a good {hi}i\u2208[n] with error of the order O(\u03c32) for D-EC-SGD with approximate bias correction. We summarize it in the following corollary.\nCorollary 18. Let f : Rd \u2192 R be \u00b5-strongly quasi-convex, and each fi be L-smooth and convex. Let run EF21 as a preprocessing for O ( L \u03b4\u00b5 log LF0\u03b4 \u03c32\u00b5 ) rounds. Let the stepsize be \u03b3 \u2264 \u03b4 8 \u221a 6L\n. Then after at most\nT = O\u0303 ( \u03c32\n\u00b5n\u03b5 +\nL\u03c3\n\u00b53/2\u03b42 \u221a \u03b5 +\nL\n\u00b5\u03b4 ) iterations of Algorithm 3 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where xout is chosen randomly from {x0, . . . ,xT }, chosen with probabilities proportional to (1\u2212 \u00b5\u03b3/2)\u2212(t+1).\nProof. We only need to apply the results of Lemma 25 and Lemma 26 in Theorem 8.\nE CONVERGENCE OF EC-Ideal\nIn this section we derive the convergence of EC-Ideal. The result directly follows from Theorem 8 with \u03b1 = 0.\nTheorem 1 (Convergence of EC-Ideal). Let f : Rd \u2192 R be \u00b5-strongly quasi-convex around x\u22c6, and each fi be L-smooth and convex. Then there exists a stepsize \u03b3 \u2264 \u03b48\u221a6L such that after at most\nT = O\u0303\n( \u03c32\n\u00b5n\u03b5 +\n\u221a L\u03c3\n\u00b5 \u221a \u03b4\u03b51/2\n+ L\n\u00b5\u03b4\n)\niterations of Algorithm 1 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where xout is chosen randomly from {x0, . . . ,xT } with probabilities proportional to (1\u2212 \u00b5\u03b3/2)\u2212(t+1).\nProof. We need to apply the results of Theorem 8 with \u03b1 = 0.\nAlgorithm 4 D-EC-SGD with Bias Correction and Double Contractive Compression 1: Input: x0, ei0=0d, hi0=gi0,h0 = 1n \u2211n i=1 h i 0, C\u03b41 , C\u03b42 , and \u03b3\n2: for t = 0, 1, 2, . . . do 3: compute git = g i t \u25bd client side 4: compute \u2206it = C\u03b41(eit + git \u2212 hit) and \u2206\u0302it = C\u03b42(git \u2212 hit) 5: update eit+1 = e i t + g i t \u2212 hit \u2212\u2206it and hit+1 = hit + \u2206\u0302it 6: send to server \u2206it and \u2206\u0302 i t\n7: update xt+1 := xt \u2212 \u03b3ht \u2212 \u03b3n \u2211n i=1 \u2206 i t \u25bd server side\n8: update ht+1 = ht + 1n \u2211n i=1 \u2206 i t\nF D-EC-SGD WITH BIAS CORRECTION AND DOUBLE CONTRACITVE COMPRESSION\nIn this section we follow algorithm D-EC-SGD with bias correction (Stich, 2020). In this algorithm, the learning mechanism for hit to approximate h i \u22c6 is built using additional compressor from more restricted class of unbiased compression operators. We show that unbiased compressor can be replaced by more general contractive one; see Algorithm 4 for more detailed description."
        },
        {
            "heading": "F.1 NOTATION",
            "text": "For D-EC-SGD with bias correction and double contractive compression we consider the following notation\nXt = E [ \u2225x\u0303t \u2212 x\u22c6\u22252 ] , Et = 1\nn n\u2211 i=1 E [ \u2225eit\u22252 ] , and Ht = 1 n n\u2211 i=1 E [ \u2225\u2207f(xt)\u2212 hit\u22252 ] .\n(30)"
        },
        {
            "heading": "F.2 CONVERGENCE ANALYSIS",
            "text": "Now we present the convergence rate for D-EC-SGD with bias correction and double contractive compression.\nFirst, we highlight that Lemma 1. Next, we present the descent lemma in Et. Lemma 19. The iterates of D-EC-SGD with bias correction and double contractive compression satisfy\nEt+1 \u2264 (1\u2212 \u03b4/2)Et + 2(1\u2212 \u03b41)\n\u03b41 Ht + (1\u2212 \u03b41)\u03c32. (31)\nProof. We have\nEt+1 = 1\nn n\u2211 i=1 E [ \u2225eit+1\u22252 ] = 1\nn n\u2211 i=1 E [ \u2225eit + git \u2212 hit \u2212 C\u03b41(eit + git \u2212 hit)\u22252 ] \u2264 (1\u2212 \u03b41)\nn\nn\u2211 i=1 E [ \u2225eit + git \u2212 hit\u22252 ] \u2264 (1\u2212 \u03b41)\nn\nn\u2211 i=1 E [ \u2225eit +\u2207fi(xt)\u2212 hit\u22252 ] + (1\u2212 \u03b41)\u03c32.\nUsing Young\u2019s inequality we continue\nEt+1 \u2264 (1\u2212 \u03b41)(1 + \u03b11)Et + (1\u2212 \u03b41)(1 + \u03b1\u221211 ) 1\nn n\u2211 i=1 E [ \u2225\u2207fi(xt)\u2212 hit\u22252 ] + (1\u2212 \u03b41)\u03c32.\nNow, if we choose \u03b11 = \u03b412(1\u2212\u03b41) , we obtain the statement of the lemma.\nLemma 20. Let f be L-smooth, then the iterates of D-EC-SGD with bias correction and double contractive compression satisfy\n1 \u03b32 E [ \u2225xt+1 \u2212 xt\u22252 ] \u2264 4\u03c32 + 4Ht + 8Et + 8LFt. (32)\nProof. We have\n1 \u03b32 E [ \u2225xt+1 \u2212 xt\u22252 ] = E [ \u2225ht \u00b1 et \u00b1 gt + 1 n n\u2211 i=1 \u2206it\u22252 ]\n\u2264 2E [ \u2225ht \u2212 et \u2212 gt + 1\nn n\u2211 i=1 C\u03b41(eit + git \u2212 hit)\u22252 ] + 2E [ \u2225et + gt\u22252 ] \u2264 2\nn n\u2211 i=1 E [ \u2225hit \u2212 eit \u2212 git + C\u03b41(eit + git \u2212 hit)\u22252 ] + 2 \u03c32 n + 4Et\n+ 4E [ \u2225\u2207f(xt)\u22252 ] \u2264 2(1\u2212 \u03b41)\nn\nn\u2211 i=1 E [ \u2225hit \u2212 eit \u2212 git\u22252 ] + 2 \u03c32 n + 4Et + 8LFt\n\u2264 2(1\u2212 \u03b41)\u03c32 + 4(1\u2212 \u03b41)Et + 4(1\u2212 \u03b41)Ht + 2 \u03c32\nn + 4Et + 4LFt\n\u2264 4\u03c32 + 4Ht + 8Et + 8LFt.\nIn our next lemma we state the descent in Ht.\nLemma 21. Let f be L-smooth and each fi be Li-smooth, then the iterates of D-EC-SGD with bias correction and double contractive compression satisfy\nHt+1 \u2264 (1\u2212 \u03b42 4 + 16L\u03032\u03b32 \u03b42 )Ht + 32L\u03032\u03b32 \u03b42 Et + 16L\u03032L\u03b32 \u03b42 Ft +\n( 16L\u03032\u03b32\n\u03b42 +\n8\n\u03b42\n) \u03c32\nIn particular, if \u03b3 \u2264 \u03b42 8 \u221a 2L\u0303 , then:\nHt+1 \u2264 (1\u2212 \u03b42 8 )Ht +\n32L\u03032\u03b32\n\u03b42 Et +\n16L\u03032L\u03b32\n\u03b42 Ft +\n( 16L\u03032\u03b32\n\u03b42 +\n8\n\u03b42\n) \u03c32 (33)\nProof. We have taking the expectation Et [\u00b7] w.r.t xt:\nEt [Ht+1] = 1\nn n\u2211 i=1 Et [\u2225\u2225\u2207fi(xt+1)\u2212 hit \u2212 C\u03b42(git \u2212 hit)\u2225\u22252 |]\n(i) \u2264 1 n n\u2211 i=1 (1 + \u03b2)Et [ \u2225\u2207fi(xt+1)\u2212\u2207fi(xt)\u22252 ] + 1\nn n\u2211 i=1 (1 + \u03b2\u22121)Et [\u2225\u2225\u2207fi(xt)\u2212 hit \u2212 C\u03b42(git \u2212 hit)\u2225\u22252]\n(ii) \u2264 L\u03032(1 + \u03b2)Et [ \u2225xt+1 \u2212 xt\u22252 ] + 1\nn n\u2211 i=1 (1 + \u03b2\u22121)Et [\u2225\u2225git + (\u2207fi(xt)\u2212 git)\u2212 hit \u2212 C\u03b42(git \u2212 hit)\u2225\u22252]\n(iii) \u2264 L\u03032(1 + \u03b2)Et [ \u2225xt+1 \u2212 xt\u22252 ] + 1\nn n\u2211 i=1 (1 + \u03b2\u22121)(1 + s1)Et [ \u2225git \u2212 hit \u2212 C\u03b42(git \u2212 hit)\u22252 ] + 1\nn n\u2211 i=1 (1 + \u03b2\u22121)(1 + s\u221211 )Et [\u2225\u2225\u2207fi(xt)\u2212 git\u2225\u22252]\n(iv)\n\u2264 L\u03032(1 + \u03b2)E \u2225xt+1 \u2212 xt\u22252\n+ 1\nn n\u2211 i=1 (1 + \u03b2\u22121)(1 + s1)(1\u2212 \u03b42)Et [ \u2225git \u2212 hit\u22252 ] + (1 + \u03b2\u22121)(1 + s\u221211 )\u03c3 2,\nwhere in (i) we use Young\u2019s inequality; in (ii) we L-smoothness; (iii) we use variance decomposition; in (iv) we use the definition of C\u03b42 and bounded variance assumption. Note that git\u2019s are independent from \u2207fi(xt)\u2212 hit, and Et [ \u2207fi(xt)\u2212 git ] = 0, we have:\nEt [Ht+1] \u2264 L\u03032(1 + \u03b2)Et [ \u2225xt+1 \u2212 xt\u22252 ] + 1\nn n\u2211 i=1 (1 + \u03b2\u22121)(1 + s1)(1\u2212 \u03b42) \u2225\u2225\u2207fi(xt)\u2212 hit\u2225\u22252\n+ (1 + \u03b2\u22121)(1 + s1)(1\u2212 \u03b42)\u03c32 + (1 + \u03b2\u22121)(1 + s\u221211 )\u03c32\nWe also can upper bound E \u2225xt+1 \u2212 xt\u22252 by Lemma 20 which leads to Et [Ht+1] \u2264 (1 + \u03b2\u22121)(1 + s1)(1\u2212 \u03b42)Ht\n+ L\u03032(1 + \u03b2)\u03b32 ( 8Et + 4Ht + 4LFt + 4\u03c3 2 )\n+ ( (1 + \u03b2\u22121)(1 + s\u221211 ) + (1 + \u03b2 \u22121)(1 + s1)(1\u2212 \u03b42) ) \u03c32\nNow we need to properly set all constants \u03b2, s1, to derive the lemma statement. In particular, if we choose \u03b2 = 4\u22122\u03b42\u03b42 and s1 = \u03b42 2(1\u2212\u03b42) , then\nHt+1 \u2264 (1\u2212 \u03b42 4 + 16L\u03032\u03b32 \u03b42 )Ht + 32L\u03032\u03b32 \u03b42 Et + 16L\u03032L\u03b32 \u03b42 Ft +\n( 16L\u03032\u03b32\n\u03b42 +\n8\n\u03b42\n) \u03c32\nNext we consider the Lyapunov function \u03a8t := Xt + aHt + bEt. Lemma 22. Let f be L-smooth and \u00b5-strongly quasi-convex, and each fi be Li-smooth. If \u03b3 \u2264 \u03b41\u03b42 64 \u221a 2L\u0303 , then:\n\u03a8t+1 \u2264 (1\u2212 \u03b3\u00b5\n2 )\u03a8t \u2212\n\u03b3 4 Ft + \u03b3\n2\u03c3 2\nn + \u03b33\n3100L\u03c32\n\u03b421\u03b4 2 2\n(34)\nwhere b := 12L\u03b3 3\n\u03b41 and a := 32b\u03b41\u03b42 .\nProof. By Lemma 1, Lemma 19, and Lemma 21, we have: \u03a8t+1 = Xt+1 + aHt+1 + bEt+1\n\u2264 (1\u2212 \u03b3\u00b5 2 )Xt \u2212 \u03b3 2 Ft +\n\u03b32\nn \u03c32 + 3L\u03b33Et\n+ a ( (1\u2212 \u03b42\n8 )Ht +\n32L\u03032\u03b32\n\u03b42 Et +\n16L\u03032L\u03b32\n\u03b42 Ft +\n( 16L\u03032\u03b32\n\u03b42 +\n8\n\u03b42\n) \u03c32 )\n+ b ( (1\u2212 \u03b41\n2 )Et + 2(1\u2212 \u03b41) \u03b41 Ht + (1\u2212 \u03b41) \u03c32 B ) = (1\u2212 \u03b3\u00b5\n2 )Xt +\n( 1\u2212 \u03b42\n8 + 2(1\u2212 \u03b41)b \u03b41a\n) aHt\n+ ( 1\u2212 \u03b41\n2 +\n3L\u03b33\nb +\n32L\u03032\u03b32a\n\u03b42b\n) bEt\n+\n( \u03b32\nn + a(\n16L\u03032\u03b32\n\u03b42 +\n8 \u03b42 ) + b(1\u2212 \u03b41)\n) \u03c32\n+ ( \u2212\u03b3 2 + 16L\u03032L\u03b32a\n\u03b42\n) Ft\nIf b := 12L\u03b3 3\n\u03b41 , a := 32b\u03b41\u03b42 , and if \u03b3 \u2264 \u03b41\u03b42 64 \u221a 2L\u0303 , then for the coefficients next to Ht we have:\n1\u2212 \u03b42 8 + 2(1\u2212 \u03b41)b \u03b41a \u2264 1\u2212 \u03b42 16 ;\nfor the coefficients next to Et we have:\n1\u2212 \u03b41 2 + 3L\u03b33 b + 32L\u03032\u03b32a \u03b42b \u2264 1\u2212 \u03b41 8 ;\nfor the coefficients next to \u03c32 we have:\na( 16L\u03032\u03b32\n\u03b42 +\n8 \u03b42 ) + b(1\u2212 \u03b41) \u2264 \u03b33 3100L \u03b421\u03b4 2 2 ;\nfor the coefficients next to Ft we have:\n\u2212\u03b3 2 +\n16L\u03032L\u03b32a\n\u03b42 \u2264 \u2212\u03b3 4 .\nPutting it together we have:\n\u03a8t+1 \u2264 (1\u2212 \u03b3\u00b5\n2 )\u03a8t \u2212\n\u03b3 4 Ft + \u03b3\n2\u03c3 2\nn + \u03b33\n3100L\u03c32\n\u03b421\u03b4 2 2\nwhere we note that \u03b3\u00b52 \u2264 \u03b41 8 and \u03b3\u00b5 2 \u2264 \u03b42 16 .\nTheorem 9. Let f be \u00b5-strongly quasi-convex around x\u22c6 and L-smooth. Let each fi be Li-smooth. Let \u03b3 \u2264 \u03b41\u03b42\n64 \u221a 2L\u0303 . Then after at most\nT = O\u0303\n( \u03c32\n\u00b5n\u03b5 +\n\u221a L\u03c3\n\u00b5\u03b41\u03b42\u03b51/2 +\nL\u0303\n\u00b5\u03b41\u03b42 ) iterations of Algorithm 4 it holds E [f(xout)\u2212 f\u22c6] \u2264 \u03b5, where xout is chosen randomly from xt \u2208 {x0, . . . ,xT } with probabilities proportional to (1\u2212 \u03b3\u00b52 ) \u2212(t+1).\nProof. We need to apply the results of Lemma 3, Lemma 22 and Remark 4.\nWe observe that D-EF-SGD with double contracitve compression still achieves nearly optimal asymptotic complexity with stochastic gradients, where a \u03c32 factor is hidden in the log terms. However, in the non-asymptotic regime it has poor dependency on compression parameters \u03b41 and \u03b42. In the simplest full gradient case, when \u03b41 = \u03b42 = \u03b4 and \u03c32 = 0, the linearly convergent term is proportional to \u03b4\u22122. In opposite, EF21 and EControl have only \u03b4\u22121 dependency in this setting.\nG COMPLEXITY OF EF21-SGD\nIn this section we consider EF21 mechanism. Richt\u00e1rik et al. (2021) demonstrate that EF21-GD, i.e. EF21 with full local gradient computations converges linearly. In the stochastic setting, it has been shown in (Fatkhullin et al., 2023) that EF21-SGD converges only with large batches. For completeness, we present convergence guarantees of EF21 with arbitrary batch size. In particular, we show that EF21 can converge to an error of O(\u03c3\n2L \u03b42\u00b5 ) in a log number of rounds. EF21 is summarized\nin Algorithm 5.\nAlgorithm 5 EF21 1: Input: x0,hi0 = \u2207fi(x0), \u03b3, and ht = 1n \u2211n i=1 h i t\n2: for t = 0, 1, 2, . . . do 3: git = g i t \u25bd client side 4: \u2206it = C\u03b4(git \u2212 hi) 5: hit+1 = h i t +\u2206 i t 6: send to server: \u2206it 7: xt+1 := xt \u2212 \u03b3ht \u25bd server side 8: ht+1 = ht + 1 n \u2211n i=1 \u2206 i t\nConsider Ft = f(xt)\u2212 f\u22c6 and Ht = 1n \u2211n i=1 \u2225\u2225\u2207fi(xt)\u2212 hit\u2225\u22252. The following lemmas are simple modifications of the lemmas in (Li et al., 2021) and (Richt\u00e1rik et al., 2021) in the presence of stochasticity. Therefore, we state them without proofs. Lemma 23 (Lemma 2 from Li et al. (2021)). Let f be \u00b5-strongly convex and L-smooth. Then iterates of EF21-SGD satisfy\nFt+1 \u2264 (1\u2212 \u03b3\u00b5)Ft \u2212 ( 1\n2\u03b3 \u2212 L 2\n) E [ \u2225xt+1 \u2212 xt\u22252 ] + (2\u2212 \u03b4)\u03b3\u03c32 + (1\u2212 \u03b4)\u03b3Ht. (35)\nLemma 24 (Lemma 7 from Richt\u00e1rik et al. (2021)). Let f be \u00b5-strongly convex and fi be L-smooth for all i \u2208 [n]. Then iterates of EF21-SGD satisfy\nEt [Ht+1] \u2264 ( 1\u2212 \u03b4\n4\n) Ht + 2L2 \u03b4 E [ \u2225xt+1 \u2212 xt\u22252 ] + 3\u03c32 \u03b4 . (36)\nNow consider \u03a8t := Ft + aHt, where a := 100\u03b3\u03b4 . Putting these two lemmas together, we have:\nLemma 25. Let f be \u00b5-strongly convex and fi be L-smooth for all i \u2208 [n]. Let \u03b3 \u2264 \u03b4100L . Then iterates of EF21-SGD satisfy\nEt [\u03a8t+1] \u2264 (1\u2212 c)\u03a8t \u2212 40L \u03b4 Et [ \u2225xt+1 \u2212 xt\u22252 ] + \u03b3 ( 2 + 300 \u03b42 ) \u03c32 (37)\nwhere c := \u03b3\u00b5.\nSolving the recursion, one can show that\n\u00b5\u03a8T+1 \u2264 e\u2212\u03b3\u00b5(T+1)\n\u03b3 \u03a80 +\n( 2 + 300\n\u03b42\n) \u03c32\nIn particular, this means that \u03a8 decreases to O( \u03c3 2 \u03b42\u00b5 ) in O ( L \u03b4\u00b5 log F0\u03b4L \u03c32\u00b5 ) rounds. Therefore, EF21SGD can be used as warm up algorithm to find good approximation of hi\u22c6. As we can see, the output of EF21-SGD satisfies the restriction for Algorithm 3. In particular, we show that at any iteration of EF21-SGD, we have 1n \u2211n i=1 E\n[\u2225\u2225hit \u2212\u2207fi(x\u22c6)\u2225\u2225] \u2264 4L\u03a8t if \u03b3 and a are chosen as in Lemma 25. Lemma 26. If fi is L-smooth and convex, and ht and xt are generated by EF21 with \u03b3 = \u03b4100L , then\n1\nn n\u2211 i=1 E [\u2225\u2225hit \u2212\u2207fi(x\u22c6)\u2225\u2225] \u2264 4L\u03a8t\nwhere \u03a8t = Ft + aHt and a = 1\u03b3 .\nProof.\n1\nn n\u2211 i=1 E [\u2225\u2225hit \u2212\u2207fi(x\u22c6)\u2225\u22252] \u2264 2n n\u2211 i=1 E [\u2225\u2225hit \u2212\u2207fi(xt)\u2225\u22252]+ 2n n\u2211 i=1 E [ \u2225\u2207fi(xt)\u2212\u2207fi(x\u22c6)\u22252 ] \u2264 2\nn n\u2211 i=1 E [\u2225\u2225hit \u2212\u2207fi(xt)\u2225\u22252]\n+ 4L\nn n\u2211 i=1 (fi(xt)\u2212 fi(x\u22c6)\u2212 \u27e8\u2207fi(x\u22c6),xt \u2212 x\u22c6\u27e9)\n= 2\nn n\u2211 i=1 E [\u2225\u2225hit \u2212\u2207fi(xt)\u2225\u22252]+ 4L(f(xt)\u2212 f\u22c6)\n= 2Ht + 4LFt\n\u2264 4L\u03a8t"
        }
    ],
    "year": 2024
}