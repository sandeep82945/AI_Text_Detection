{
    "abstractText": "State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model\u2019s ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.",
    "authors": [],
    "id": "SP:b4997f724f075e7c624cfef1f3c4a7bb9992d57b",
    "references": [
        {
            "authors": [
                "Shahab Bakhtiari",
                "Patrick Mineault",
                "Tim Lillicrap",
                "Christopher C. Pack",
                "Blake A. Richards"
            ],
            "title": "The functional specialization of visual cortex emerges from training parallel pathways with selfsupervised predictive learning",
            "venue": "bioRxiv,",
            "year": 2021
        },
        {
            "authors": [
                "Trenton Bricken",
                "Cengiz Pehlevan"
            ],
            "title": "Attention approximates sparse distributed memory, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "CoRR, abs/2005.14165,",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners, 2020b",
            "year": 2020
        },
        {
            "authors": [
                "Charlotte Caucheteux",
                "Jean-R\u00e9mi King"
            ],
            "title": "Brains and algorithms partially converge in natural language processing",
            "venue": "Communications Biology,",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations, 2020",
            "venue": "URL https://arxiv.org/abs/",
            "year": 2002
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "UNITER: learning universal image-text representations",
            "venue": "URL http://arxiv.org/abs/1909.11740",
            "year": 1909
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins"
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Casey White",
                "Ali Williford",
                "Daniela M Witten",
                "Jun Zhuang",
                "Hongkui Zeng",
                "Colin Farrell",
                "Lydia Ng",
                "Amy Bernard",
                "John W Phillips",
                "R Clay Reid",
                "Christof Koch"
            ],
            "title": "A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex",
            "venue": "Nature neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Saskia EJ de Vries",
                "Jerome A Lecoq",
                "Michael A Buice",
                "Peter A Groblewski",
                "Gabriel K Ocker",
                "Michael Oliver",
                "David Feng",
                "Nicholas Cain",
                "Peter Ledochowitsch",
                "Daniel Millman"
            ],
            "title": "A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex",
            "venue": "Nature neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel A Dombeck",
                "Anton N Khabbaz",
                "Forrest Collman",
                "Thomas L Adelman",
                "David W Tank"
            ],
            "title": "Imaging large-scale neural activity with cellular resolution in awake, mobile mice",
            "year": 2007
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale, 2020",
            "venue": "URL https://arxiv.org/abs/2010.11929",
            "year": 1929
        },
        {
            "authors": [
                "Zhe Gan",
                "Yen-Chun Chen",
                "Linjie Li",
                "Chen Zhu",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "Large-scale adversarial training for vision-and-language representation learning",
            "venue": "CoRR, abs/2006.06195,",
            "year": 2020
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Scott Johnston",
                "Shauna Kravec",
                "Sheer El Showk",
                "Tamera Lanham",
                "Timothy Telleen-Lawton",
                "Tom Henighan",
                "Tristan Hume",
                "Yuntao Bai",
                "Zac Hatfield-Dodds",
                "Ben Mann",
                "Dario Amodei",
                "Nicholas Joseph",
                "Sam McCandlish",
                "Tom Brown",
                "Christopher Olah",
                "Jack Clark",
                "Samuel R. Bowman",
                "Jared Kaplan"
            ],
            "title": "The capacity for moral self-correction in large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Kensho Hara",
                "Hirokatsu Kataoka",
                "Yutaka Satoh"
            ],
            "title": "Learning spatio-temporal features with 3d residual networks for action recognition, 2017",
            "venue": "URL https://arxiv.org/abs/1708",
            "year": 2017
        },
        {
            "authors": [
                "Christopher D. Harvey",
                "Forrest Collman",
                "Daniel A. Dombeck",
                "David W. Tank"
            ],
            "title": "Intracellular dynamics of hippocampal place cells during virtual navigation",
            "venue": "Nature, 461(7266):941\u2013946,",
            "year": 2009
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "year": 2015
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration, 2019",
            "venue": "URL https://arxiv.org/abs/1904.09751",
            "year": 1904
        },
        {
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder"
            ],
            "title": "Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Cheng-Zhi Anna Huang",
                "Ashish Vaswani",
                "Jakob Uszkoreit",
                "Noam Shazeer",
                "Curtis Hawthorne",
                "Andrew M. Dai",
                "Matthew D. Hoffman",
                "Douglas Eck"
            ],
            "title": "An improved relative self-attention mechanism for transformer with application to music generation",
            "venue": "CoRR, abs/1809.04281,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Sebastian Borgeaud",
                "Jean-Baptiste Alayrac",
                "Carl Doersch",
                "Catalin Ionescu",
                "David Ding",
                "Skanda Koppula",
                "Daniel Zoran",
                "Andrew Brock",
                "Evan Shelhamer",
                "Olivier J. H\u00e9naff",
                "Matthew M. Botvinick",
                "Andrew Zisserman",
                "Oriol Vinyals",
                "Jo\u00e3o Carreira"
            ],
            "title": "Perceiver IO: A general architecture for structured inputs & outputs",
            "venue": "CoRR, abs/2107.14795,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Felix Gimeno",
                "Andrew Brock",
                "Andrew Zisserman",
                "Oriol Vinyals",
                "Jo\u00e3o Carreira"
            ],
            "title": "Perceiver: General perception with iterative attention",
            "venue": "CoRR, abs/2103.03206,",
            "year": 2021
        },
        {
            "authors": [
                "Jaehun Jung",
                "Lianhui Qin",
                "Sean Welleck",
                "Faeze Brahman",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "Maieutic prompting: Logically consistent reasoning with recursive explanations, 2022",
            "year": 2022
        },
        {
            "authors": [
                "W.F. Kindel",
                "E.D. Christensen",
                "J. Zylberberg"
            ],
            "title": "Using deep learning to reveal the neural code for images in primary visual cortex",
            "venue": "J. Vis,",
            "year": 2019
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath R. Selvaraju",
                "Akhilesh Deepak Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation, 2021",
            "venue": "URL https://arxiv.org/abs/2107.07651",
            "year": 2021
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang"
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language, 2019",
            "venue": "URL https://arxiv.org/abs/",
            "year": 1908
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted",
            "venue": "windows. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows, 2021b. URL https: //arxiv.org/abs/2103.14030",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Jiasen Lu",
                "Vedanuj Goswami",
                "Marcus Rohrbach",
                "Devi Parikh",
                "Stefan Lee"
            ],
            "title": "12-in-1: Multitask vision and language representation learning",
            "venue": "CoRR, abs/1912.02315,",
            "year": 2019
        },
        {
            "authors": [
                "Juliette Millet",
                "Charlotte Caucheteux",
                "Pierre Orhan",
                "Yves Boubenec",
                "Alexandre Gramfort",
                "Ewan Dunbar",
                "Christophe Pallier",
                "Jean-Remi King"
            ],
            "title": "Toward a realistic model of speech processing in the brain with self-supervised learning, 2022",
            "venue": "URL https://arxiv.org/abs/2206",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Mineault",
                "Shahab Bakhtiari",
                "Blake Richards",
                "Christopher Pack"
            ],
            "title": "Your head is there to move you around: Goal-driven models of the primate dorsal pathway",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding, 2018",
            "venue": "URL https://arxiv.org/abs/1807.03748",
            "year": 2018
        },
        {
            "authors": [
                "Razvan Pascanu",
                "Tomas Mikolov",
                "Yoshua Bengio"
            ],
            "title": "On the difficulty of training recurrent neural networks",
            "year": 2012
        },
        {
            "authors": [
                "Jonathan W. Pillow",
                "Jonathon Shlens",
                "Liam Paninski",
                "Alexander Sher",
                "Alan M. Litke",
                "E.J. Chichilnisky",
                "Eero P. Simoncelli"
            ],
            "title": "Spatio-temporal correlations and visual signalling in a complete neuronal population",
            "venue": "July 2008a. doi: 10.1038/nature07140. URL https://doi.org/10.1038/nature07140",
            "year": 2008
        },
        {
            "authors": [
                "Jonathan W Pillow",
                "Jonathon Shlens",
                "Liam Paninski",
                "Alexander Sher",
                "Alan M Litke",
                "E J Chichilnisky",
                "Eero P Simoncelli"
            ],
            "title": "Spatio-temporal correlations and visual signalling in a complete neuronal population",
            "venue": "Aug 2008b. ISSN",
            "year": 2008
        },
        {
            "authors": [
                "Eftychios A. Pnevmatikakis",
                "Josh Merel",
                "Ari Pakman",
                "Liam Paninski"
            ],
            "title": "Bayesian spike inference from calcium imaging data",
            "venue": "In 2013 Asilomar Conference on Signals, Systems and Computers,",
            "year": 2013
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "CoRR, abs/2103.00020,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation, 2021",
            "venue": "URL https://arxiv.org/ abs/2102.12092",
            "year": 2092
        },
        {
            "authors": [
                "Greg Wayne",
                "Daniel Yamins",
                "Friedemann Zenke",
                "Joel Zylberberg",
                "Denis Therien",
                "Konrad P. Kording"
            ],
            "title": "A deep learning framework for neuroscience",
            "venue": "Nature Neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Rives",
                "Joshua Meier",
                "Tom Sercu",
                "Siddharth Goyal",
                "Zeming Lin",
                "Jason Liu",
                "Demi Guo",
                "Myle Ott",
                "C. Lawrence Zitnick",
                "Jerry Ma",
                "Rob Fergus"
            ],
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "Jos Rozen",
                "Abheesht Sharma",
                "Andrea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Tali Bers",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Steffen Schneider",
                "Jin Hwa Lee",
                "Mackenzie Weygandt Mathis"
            ],
            "title": "Learnable latent embeddings for joint behavioral and neural analysis, 2022",
            "venue": "URL https://arxiv.org/abs/2204.00673",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas A Steinmetz",
                "Peter Zatka-Haas",
                "Matteo Carandini",
                "Kenneth D Harris"
            ],
            "title": "Distributed coding of choice, action and engagement across the mouse",
            "venue": "brain. Nature,",
            "year": 2019
        },
        {
            "authors": [
                "Marcel Stimberg",
                "Romain Brette",
                "Dan FM Goodman"
            ],
            "title": "Brian 2, an intuitive and efficient neural simulator. eLife, 8:e47314, aug 2019",
            "venue": "ISSN 2050-084X. doi: 10.7554/eLife.47314. URL https://doi.org/10.7554/eLife.47314",
            "year": 2050
        },
        {
            "authors": [
                "Jeffrey N. Stirman",
                "Ikuko T. Smith",
                "Michael W. Kudenov",
                "S.L. S"
            ],
            "title": "Wide field-of-view. multiregion two-photon imaging of neuronal activity",
            "venue": "Nat. Biotechnol,",
            "year": 2016
        },
        {
            "authors": [
                "Carsen Stringer",
                "Marius Pachitariu",
                "Nicholas Steinmetz",
                "Charu Bai Reddy",
                "Matteo Carandini",
                "Kenneth D Harris"
            ],
            "title": "Spontaneous behaviors drive multidimensional, brainwide activity",
            "venue": "Science, 364(6437):eaav7893,",
            "year": 2019
        },
        {
            "authors": [
                "David Sussillo",
                "Rafal J\u00f3zefowicz",
                "L.F. Abbott",
                "Chethan Pandarinath"
            ],
            "title": "LFADS - latent factor analysis via dynamical systems",
            "venue": "CoRR, abs/1608.06315,",
            "year": 2016
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal"
            ],
            "title": "LXMERT: learning cross-modality encoder representations from transformers",
            "venue": "CoRR, abs/1908.07490,",
            "year": 2019
        },
        {
            "authors": [
                "Wilson Truccolo",
                "Uri T Eden",
                "Matthew R Fellows",
                "John P Donoghue",
                "Emery N Brown"
            ],
            "title": "A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects",
            "venue": "J Neurophysiol,",
            "year": 2005
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "James C.R. Whittington",
                "Joseph Warren",
                "Timothy E.J. Behrens"
            ],
            "title": "Relating transformers to models and neural representations of the hippocampal formation, 2021",
            "venue": "URL https: //arxiv.org/abs/2112.04035",
            "year": 2021
        },
        {
            "authors": [
                "Daniel L K Yamins",
                "James J DiCarlo"
            ],
            "title": "Using goal-driven deep learning models to understand sensory cortex",
            "venue": "Nature Neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "Daniel L.K. Yamins",
                "Ha Hong",
                "Charles F. Cadieu",
                "Ethan A. Solomon",
                "Darren Seibert",
                "James J. DiCarlo"
            ],
            "title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2014
        },
        {
            "authors": [
                "Joel Ye",
                "Chethan Pandarinath"
            ],
            "title": "Representation learning for neural population activity with neural data transformers, 2021",
            "venue": "URL https://arxiv.org/abs/2108.01210",
            "year": 2021
        },
        {
            "authors": [
                "Jason Yosinski",
                "Jeff Clune",
                "Yoshua Bengio",
                "Hod Lipson"
            ],
            "title": "How transferable are features in deep neural networks",
            "year": 2014
        },
        {
            "authors": [
                "Che-Hang Yu",
                "Jeffrey N Stirman",
                "Yiyi Yu",
                "Riichiro Hira",
                "Spencer L Smith"
            ],
            "title": "Diesel2p mesoscope with dual independent scan engines for flexible capture of dynamics in distributed neural circuitry",
            "venue": "Nature communications,",
            "year": 2021
        },
        {
            "authors": [
                "Yiyi Yu",
                "Jeffrey N Stirman",
                "Christopher R Dorsett",
                "Spencer L Smith"
            ],
            "title": "Selective representations of texture and motion in mouse higher visual areas",
            "venue": "Current Biology,",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Merlot: Multimodal neural script knowledge models, 2021",
            "venue": "URL https: //arxiv.org/abs/2106.02636",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Zhao",
                "Il Memming Park"
            ],
            "title": "Variational latent gaussian process for recovering single-trial dynamics from population spike trains",
            "venue": "Neural Computation,",
            "year": 2017
        },
        {
            "authors": [
                "Ding Zhou",
                "Xue-Xin Wei"
            ],
            "title": "Learning identifiable and interpretable latent models of highdimensional neural activity using pi-vae",
            "venue": "doi: 10.48550/ARXIV.2011.04798. URL https://arxiv.org/abs/2011.04798",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Systems neuroscience experiments are growing in complexity with a cascade of technical advances. In addition to recording neuronal activity from hundreds to thousands of neurons in parallel from multiple brain areas (Yu et al., 2021; Stirman et al., 2016; de Vries et al., 2020a;b), experimenters are also simultaneously acquiring behavioral data including eye positions and body movements (Stringer et al., 2019; Steinmetz et al., 2019). Neuroscientists seek models for these rich datasets to obtain insights into how neural circuitry is involved with behavior. However, complex datasets and experimental designs present challenges for model development, particularly in incorporating the various modalities within the modelling pipeline, and subsequently creating models for a host of different applications.\nIn recent years, deep neural networks (DNNs) have demonstrated their potential in modeling neural activity and circuitry (Yamins et al., 2014; Kubilius et al., 2018; Richards et al., 2019). Brain-inspired DNNs have exhibited responses similar to visually evoked neural responses in visual cortical areas (Yamins et al., 2014; Kindel et al., 2019; Mineault et al., 2021; Yamins & DiCarlo, 2016). These models were trained in a goal-driven approach, such as the classification of images. Then intermediate representations in the model were compared to observed neuronal activities in biological brains. In other work, autoencoder-based latent variable models have been applied to the analysis of neuronal activities, with the assumption that the single-trial spiking activity of a behavior task depends on underlying dynamics (Sussillo et al., 2016; Ye & Pandarinath, 2021; Zhao & Park, 2017; Schneider et al., 2022). These approaches can provide insight into principles of neural circuitry, but often entail inductive biases that are not always ideal. General purpose, multimodal models can provide a complementary approach.\nTo develop models for correlating neuron activities, sensory input, and behavior, we sought a powerful multimodal architecture and training objective. The transformer architecture has demonstrated its flexibility in modeling data from various domains, finding widespread adoption in applications including vision, language, and sound (Liu et al., 2021a; Brown et al., 2020a; Huang et al., 2018; Radford et al., 2021; Gan et al., 2020; Li et al., 2021). In particular, transformer-based language-vision models exhibit high performance for learning multimodal representations for text and images, enabling text-to-image generation (Ramesh et al., 2021; Radford et al., 2021). Key to this is that a) the transformer is scalable; i.e it does not suffer from the vanishing gradient problem (Pascanu et al., 2012) b) it makes few assumptions about the input data (minimal inductive bias) (Jaegle et al., 2021b;a) and c) it can contextualize information within long-duration multimodal contexts. Here we present a transformer-based model for multimodal datasets in systems neuroscience, including neuronal activity, stimuli, and behavior. We developed a generative model which firstly aligns and subsequently fuses neuronal activities, input signals (e.g., visual stimuli) and behavior.\nOur key contributions are as follows: a) Introduce an approach that reframes the population response inference problem as a causally-masked, spatiotemporal autoregressive generation problem. b) Show how to employ a generative pretraining paradigm for neural data. c) Investigate the potential of multimodal, multitask, generative transformer models to analyze and parse neuroscience data, and obtain insights."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Representational and Functional Similarities between DNNs and Mammalian Brains There are similarities between the hierarchical representations of convolutional DNNs and the visual and inferior-temporal cortices (Yamins et al., 2014; Kubilius et al., 2018; Kindel et al., 2019; Mineault et al., 2021; Yamins & DiCarlo, 2016). More recently, parallel-path networks trained using a contrastive-predictive objective were shown to separate into pathways that mirrored the functional specializations of the dorsal and ventral streams in mouse visual cortex (Bakhtiari et al., 2021). Similarly, there are parallels between human speech and language models (Caucheteux & King, 2022; Millet et al., 2022). Lastly, reports have shown the plausibility of transformers replicating specific neural functions and circuits (Whittington et al., 2021; Bricken & Pehlevan, 2022). In our work, establishing a similarity between a DNN and the brain is not our focus. Instead, we are focused on developing an analytical and functional tool. This tool, Neuroformer, can exhibit similarities in representational and functional characteristics with biological neural circuitry, and we explore that here.\nLearning Latent Dynamics of Population Activity Variational auto-encoders have been used to infer latent dynamics embedded in complex, high-dimensional neural data (Sussillo et al., 2016; Zhou & Wei, 2020). Recently, a transformer architecture has been used in place of RNNs for the same application (Ye & Pandarinath, 2021), and another project has trained a latent model using a contrastive objective (Schneider et al., 2022). These empirical findings show that this approach avoids over-fitting and outperforms previous methods. In our work, we integrate these paradigms, by employing a contrastive objective at the front end of a generative model.\nLarge Multimodal and Multitask Models The transformer has proven effective at processing a variety of modalities including vision, languange, and sound (Liu et al., 2021a; Brown et al., 2020a; Huang et al., 2018). Subsequently, the field introduced multimodal models. For example, models for interactions between images and text (Tan & Bansal, 2019; Chen et al., 2019; Lu et al., 2019). Some models attempt to align the modalities with separate feature encoders using contrastive learning approaches (Radford et al., 2021; Gan et al., 2020), and alignment and fusion (Li et al., 2021). We experimentally validate these methods on neural data, by using our own generative and contrastive learning objective. Furthermore, architecture innovations have resulted in models that are agnostic to the input modalities and can be applied to a large variety of tasks (Jaegle et al., 2021b;a). Lastly, it has been recently observed, that large models pretrained on massive datasets of text exhibit emergent properties. This has been observed in Large Language Models (LLMs) as in (Brown et al., 2020b; Wei et al., 2023; Jung et al., 2022; Creswell et al., 2022; Ganguli et al., 2023), but also in scientific domains (Rives et al., 2021; Bakhtiari et al., 2021). We take inspiration from these lines of work to\nbuild a framework for modelling neural data that can efficiently scale to large inputs and an arbitrary number of modalities."
        },
        {
            "heading": "3 MODEL",
            "text": "Workflow As input, our model takes action potential data, i.e., spike trains, from multiple neurons recorded or imaged simultaneously, and breaks them up into the Current State Ic , and past state Ip according to specified temporal context windows (wc, wp) (Fig. 1, blue and purple, respectively). We do this to discretize them and precisely align the neural representations with features from other modalities. Model inputs, including neural, visual, and other features, are optionally processed by feature encoders. The modalities are first aligned via a multimodal contrastive module (Radford et al., 2021; Chen et al., 2020; Oord et al., 2018) which aims to learn matching representations for the neural features and the features from other modalities. Subsequently, a cross-modal transformer fuses the current neural response Ic with all other features. We then decode our fused representation using a causally-masked transformer decoder. All training is self-supervised and requires no data labelling.\nFeature Backbones We assign an ID (number) to each neuron in our dataset and model it as a token. At each step, a learnable look-up table projects each neuron spike contained in our Current and Past States onto an embedding space E, resulting in vectors (Tc, E) and (Tp, E), where Tc, Tp are the corresponding state\u2019s sequence length (number of spikes + padding). For video frames, we optionally use a single layer of 3D Convolutions (Hara et al., 2017) before splitting them into patches of size ( Tf , H,W,E ), as is standard for visual transformers (Fig. 1, Feature Encoding, green) (Dosovitskiy et al., 2020; Liu et al., 2021b; Li et al., 2019). We then feed these patches into the cross-modal layers.\nMultimodal Contrastive Representation Learning Drawing from recent developments in vision and language representation learning (Radford et al., 2021; Zellers et al., 2021; Li et al., 2021), our approach leverages a contrastive learning objective before fusing our feature sets (Fig. 1, Contrastive Matching). Our architecture allows alignment of an arbitrary number of modalities; however, here we focus on the three-modal case. We use Visual features Fp,c \u223c (B, T,H,W,E), Current State neural embeddings Ic \u223c (B, Tc, E), and Behavior features Ac \u223c (B, Ta, E) as our modality representatives, where B = Batch Size and T = sequence length. These are projected to a latent dimension d using three linear layers gf , gi, ga. We calculate pairwise similarities between the modalities, yielding three matrices of dimensions (Bf , Bi), (Bi, Ba), (Ba, Bf ) where\nBf = Bi = Ba = B. The diagonal entries correspond to ground-truth pair similarities. These scores are then softmax-normalized and used to compute the contrastive loss via cross-entropy H, where the probability p should be 0 and 1 for the negative and positive pairs y, respectively. We generalize the computations with pair notation (m,n) where m and n are any of the modalities (either F, I, or A):\ns(m,n) = gm(mp,c) T gn(nc) p mn k = exp(s(m,nk)/\u03c4)\u2211K k=1 exp(s(m,nk)/\u03c4)\n(1)\nWe then apply these equations to each pair (F,I), (I,A), (A,F), and compute the contrastive loss:\nLvna = 1\n3 E(F,I,A)\u2208d[H(yfi(F ),pfi(F )) +H(yia(I),pia(I)) +H(yaf (A),paf (A))] (2)\nBy explicitly imposing alignment, we encourage the model to uncover the representational commonalities between biologically relevant features (Schneider et al., 2022). This is particularly helpful in low-data settings, mitigating the over-fitting that can occur with variable neural signals. We observe consistent performance improvement upon adding the contrastive objective (see Fig. 6, 10.)\nFeature Fusion Following alignment, the masked Current State, Ic is fused with all other features, in our case the visual F and neural Past State features, Ip by cascading cross-attention modules. This stage of the model is similar to the (Jaegle et al., 2021b;a), where cross-attention between a smaller latent array (Tc, E), in our case the Current State and larger byte arrays (Tf , sE), in our case the Past State neural features, is iteratively carried out. This architecture decision has two main advantages: a) It introduces a bottleneck imposed by the smaller Ic array, of length size Tc that upon operating with a larger feature array of length Tf , reduces the complexity of attention from O(T 2f ) to O(TcTf ). When Tc is much less than Tf (expressed mathematically as limTc\u21920,Tf\u2192\u221e Tc/Tf = 0), the complexity scales linearly with Tf . This enables the use of finer-grained features, like largeresolution visual features or in our case raw video frames, which are essential for modeling lowerlevel visual areas. b) The iterative nature of the algorithm enables us to fuse an arbitrary number of modalities with our Current State.\nCausal Spike Modelling After it has been fused with all other modalities, the resulting Current State is decoded using a causally masked transformer (Brown et al., 2020a). At each prediction step, the model predicts both which neuron will fire, and within which sub-interval (dt(n) = n \u2208 wc), using two linear projections from the last self-attention layer (Fig. 1, AR Decoding) mapping to the corresponding (pi,pdt) label distributions. Notice that two Spikes can occur at the same time, and therefore by predicting bins within which neurons fire, we can sequentially predict spikes that occur simultaneously. We optimize for this objective, by minimizing the cross entropy between the predicted and ground truth distributions:\nLce(I) = E(I)\u223cd H(yI ,pI) Lce(dt) = E(dt)\u223cd H(ydt,pdt)\nLoss Function The model is jointly optimized end-to-end using the contrastive objective Lvnc, and cross-entropy loss for both Ic and dtc, (Lce(I), Lce(dt)). The resulting loss is a weighted average of the three losses:\nL = (\u03b3)Lvnc + (\u00b5)Lce(I) + (1\u2212 \u03b3 \u2212 \u00b5)Lce(dt) (3)\nInference - Generating Simulations At inference time, the model autoregressively predicts the distributions of the nth step, (Ic(n), dtc(n)), outputting an end-of-state [EOS] token to indicate it has completed the Current Target State Wc, in a similar fashion to work in image-text generation (Ramesh et al., 2021), (Fig. 1, AR Decoding) . To sample from the respective distributions (pI ,pdt) we use nucleus sampling (Holtzman et al., 2019). The predicted Current Target State neuron IDs are then incorporated into the Past State according to the Past State Context Window wp together with the predicted sub-intervals dtc, which act as additive temporal embeddings (Fig. 1, dotted arrow from AR Decoding).\nFinetuning - Transferring to New Tasks A primary motivation for redefining the spike inference problem in a manner compatible with contemporary pretraining paradigms is the ability to utilize a broad spectrum of techniques developed within this framework. To transfer to a new task, whether it be behavioral prediction or any other downstream task, we adopt a straightforward heuristic. First, if necessary, the data distribution is discretized into n classes. Then, a mean-pooled representation from our final latent layer is projected onto the appropriate feature space pv \u2208 n. The loss is calculated using the standard multi-class cross-entropy for classification (Howard & Ruder, 2018; Yosinski et al., 2014; Hinton et al., 2015) of mean-squared error for regression.\nTraining We trained all Neuroformer models with 8 layers and 8 heads for the decoder and each of the cross-attention modules, with an embedding dimension of 256 and an 80/20 train/test split. V1+AL and Visnav models equated to a total of 40, 000, 000 and 100, 000, 000 parameters respectively. We emprically observed an increase in performance up until the specified parameter sizes. We used AdamW optimizer (Loshchilov & Hutter, 2019) in PyTorch. Learning rate was 2e-4, with linear warmup and cosyne decay schedules. Batch size was kept at 224 for pretraining and 32 for fine-tuning."
        },
        {
            "heading": "4 RESULTS",
            "text": "To evaluate performance of Neuroformer, we used both artificial data sets, with known ground truth, and real neural data sets, with authentic variability and real-world relevance. We analyzed the performance in terms of inference and features."
        },
        {
            "heading": "4.1 DATA",
            "text": "Simulated Dataset We simulated a spiking neural network characterized by directional connectivity with three hub neurons using Brian2 simulator (Stimberg et al., 2019) (Hub-net simulation). The network has N = 300 leaky integrate-and-fire neurons with stochastic current. The membrane potential (Vi) of each neuron was simulated as a stochastic differential equation:\ndVi dt = I \u2212 Vi \u03c4 + \u03c3\n\u221a 2\n\u03c4 \u00b7 xi (4)\nThe membrane time constant was set to \u03c4 = 10ms as a Gaussian random variable with zero mean and 1 standard deviation. We scaled the random variable by \u03c3 = 0.5. The total dataset equated to 1 million tokens.\nTwo-photon calcium imaging datasets We assessed the performance of Neuroformer using real neural datasets. These consisted of neuronal activity recorded from awake mice, which were either passively viewing visual stimuli (Passive-Stim data), or actively engaged in a visually-guided navigation task (Visnav data). The activity was recorded using two-photon calcium imaging, (Dombeck et al., 2007; Yu et al., 2021; 2022) and neuronal spikes were deconvolved from calcium traces via the suite2p calcium imaging processing pipeline, (Stimberg et al., 2019) complemented by a Bayesian spike inference method (Pnevmatikakis et al., 2013). The Passive-Stim data comprised recordings from 386 neurons in the primary visual cortex and a higher visual area (V1 + AL). These neurons responded reliably to drifting grating stimuli and a naturalistic movie. Two Visnav datasets were derived from recordings in the lateral visual cortex (2022 neurons) or L2/3 medial (1905 neurons), spanning V1 and multiple higher visual areas. During these recordings, the mice were engaged in a visually-guided navigation task within a virtual environment, with their movements tracked by a ball system (Harvey et al., 2009) that controlled the first-person view within the virtual environment. The Visnav neurons conveyed information about both the visual input and the speed of the animal\u2019s movements. The resulting dataset sizes equated to 80\u00d7103 , 800\u00d7103 and 1\u00d7106 tokens respectively."
        },
        {
            "heading": "4.2 ATTENTION REVEALS THE FUNCTIONAL CONNECTIVITY OF SIMULATED NEURONAL CIRCUITS",
            "text": "To test our hypothesis that attention can provide insights into neural circuitry, we first trained the Neuroformer on a simulated dataset with known ground-truth connectivity. This dataset is unimodal,\nso we simply omitted the visual cross-attention module, essentially reducing to a vanilla GPT architecture. After training, we ran masked inference on the data, aggregating the attention values that are assigned to each neuron, and counting how many times neurons attend to each other during this process. This results to two square matrices Na, Nz of size (N,N ), where N is equal to our neuronal population\u2019s size (300). By dividing (Na) by (Nz) we compute the average attention aav.(i, i) that neurons assign to one another. Our results recovered the hub connectivity structure of the ground truth neuronal circuit ( I \u2208 50, 150, 200 ) using only the spiking activity (Fig. 2). This attention analysis identified the neurons that were driving circuit activity. Compared to a simple correlation matrix, the attention analysis matrix provides directional, potentially causal, information."
        },
        {
            "heading": "4.3 REALISTIC AUTOREGRESSIVE SIMULATION OF MULTIMODAL NEURONAL DATASET",
            "text": "To validate Neuroformer as a viable analysis tool for neuronal population datasets we tested its ability to model real neuronal data. We trained Neuroformer to predict visually evoked neuronal activity in a population of 386 neurons, recorded using two-photon calcium imaging in a mouse. The neurons were recorded simultaneously, and spanned two visual cortical areas: primary visual cortex (V1) and the anterolateral (AL) higher visual area. After training, Neuroformer is able to autoregressively generate whole-trial simulations (96 seconds) of the neuronal population that closely resemble held-out trials (Fig. 3)."
        },
        {
            "heading": "4.3.1 NEUROFORMER OUTPERFORMS GLM",
            "text": "The Generalized Linear Model (GLM) (Truccolo et al., 2005; Pillow et al., 2008b) is a versatile and widely used statistical modeling framework that generalizes linear regression to accommodate non-normal response variables and non-linear relationships. It has been widely used by neuroscientists as a performant tool for stimulus-response population decoding. Although useful, it has\nsome significant disadvantages, in that it requires heavy inductive bias (linear kernel, non-linearity, Poisson spiking). This limits both its generality and flexibility in simultaneously integrating many modalities. We compared the predicted, trial-averaged responses from a GLM to Neuroformer on the V1+AL dataset and found that our model\u2019s predictions where more closely correlated with the ground-truth (Fig. 3c). Our results revealed that our model could better capture the variability across the diverse set of stimuli in the dataset. We empirically observed that a single GLM could perform well on either the diffraction gratings or naturalistic movies, but not both at the same time."
        },
        {
            "heading": "4.3.2 BIOLOGICALLY RELEVANT FEATURES EMERGE FROM STIMULUS-RESPONSE CROSS-ATTENTION",
            "text": "Again, we turned to the attention parameters in the Neuroformer, as we had for the hub neuron analysis. We plotted the spatial locations on visual stimulus frames where cross-attention was allocated, i.e., a sort of attention map. These maps exhibited localized features and structure. These maps are related to the concept of receptive fields, but are specific to the full context of ongoing visual stimuli and neuronal activity. Thus, they can reveal novel relationships and insights into potentially causal relationships among neuronal activity and stimuli (Fig. 3)."
        },
        {
            "heading": "4.3.3 HIGHLY ACCURATE AND FEW-SHOT PREDICTION OF MOUSE BEHAVIOR",
            "text": "In this section, we introduce the notion of finetuning (Howard & Ruder, 2018) for neuronal data. By reframing the neuronal modelling problem to be compatible with modern NLP approaches, we can leverage similar techniques to solve tasks that can benefit from neuronal pretraining. One such task is predicting the behavior of a mouse from its neuronal responses. This is a common thing to do in neuroscience analysis, where researchers want to investigate the relationship between neuronal activity and behavior. Furthermore, such paradigms could be useful in brain-computer interface applications. For this analysis, we used neural and behavior data from a mouse running in a virtual reality setup. Neuroformer can support both classification with a mutli-class cross-entropy objective or regression using mean-squared error, which can be chosen according to the nature of the task. Here we report results using regression (Fig. 4, Table 1) and have included classification and regression inference results for all models in the appendix (Figs 11, 12). Similarly to how multi-\ntask models in the field of vision and language work, finetuning a model for a new task involves projecting the features of the last layer onto the new feature space. (Howard & Ruder, 2018)\nDespite the inherent stochasticity in the speed of the mouse, the predictions of our model when trained on the full dataset are highly correlated with the held-out ground-truth. Additionally, Neuroformer significantly outperformed conventional approaches, achieving pearson r = 0.97, with the held-out ground truth for the respective lateral and medial datasets (Table 1). To determine the effectiveness of the masked neuronal prediction pretraining we compared the performance of pretrained vs. randomly initialized models on 10% and 1% of the finetuning data. The pretrained\nfew-shot models quickly approached the performance of the model trained on the full dataset (Fig. 5b,c). Crucially, we observed that the pretrained model that was finetuned on 1% of the data outperformed the randomly-initialized model that was finetuned on 10% of the data. This suggests that pretraining enables the model to begin learning representations that go beyond the optimization objective as has been observed in generative models within natural language generation. (Radford et al., 2019; Sanh et al., 2022)"
        },
        {
            "heading": "5 ABLATIONS",
            "text": "To explore the impact of each component of Neuroformer, we implemented a range of model versions. Starting from just decoding from the Current State, we progressively incorporated Past State,\nVideo, Behavioral modalities, and Alignment using contrastive learning. With each addition, we noted an enhancement in the model\u2019s ability to generate realistic neuronal responses (Fig. 6, 10).\n.\nOf particular interest, was the Lateral dataset where the behavioral modality we incorporated for the ablation study was eye position (see Fig. 13 for predictions). We hypothesized that by learning to extrapolate from the eye position to the visual field of the mouse, the model could improve its performance, which is what we observed. While more investigation needs to go into understanding exactly how Neuroformer incorporated eye position within this context, this result highlights the potential of our approach.\nBy observing the fluctuation in performance as components within the model are selectively incorporated, scientists can examine hypotheses about the relevance of various input modalities or phenomenological behaviors to different brain regions. Ultimately, the development of modular and performant foundation models of the brain will create new avenues for the scientific exploration of the principles that underpin neuronal function and intelligence."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We have developed a new method to enable generative pretraining of multimodal neural data, and have demonstrated its utility accross four novel neural datasets with diverse structure. Our initial observations showed that pretraining not only adopted representations that mirrored the causal patterns found in neuronal circuitry, but was also able to effectively enhance performance in downstream applications, enabling rapid few-shot adaptation to behavior-related tasks.\nOur model outperformed conventional approaches, while in comparison incorporating minimal inductive biases and retaining an identical architecture across datasets and experiments. Overall, the model was applied to 4 modalities (neural data, video, speed, eye position), and 3 decoding tasks (neuron ID, neuron dt, speed). The architecture enables easy expansion of the model to more modalities and tasks, making it compatible with almost any type of systems neuroscience dataset and decoding objective.\nConcluding, the Neuroformer enables systems neuroscience to leverage the full suite of tools in generative, multitask and multi-modal transformers. It paves the way for scaling training of neuroscience models and potentially connecting them with Large Language Models. This advancement could foster models with new emergent capabilities, benefiting not just neural data analysis but also various other applications. Our work sets the stage for promising research within this domain. 1\n1For a more thorough discussion of limitations, please see Appendix A."
        },
        {
            "heading": "A LIMITATIONS",
            "text": "In preliminary experiments, the Neuroformer was trained on simulated data, revealing potential in identifying hub network connectivity. However, its reliability varied across datasets. For instance, in a multi-channel simulation, neuron attention sometimes accurately identified the correct channel, but at other times was noisy. In another test with neurons responding to a naturalistic video via 3D gabor filters, the method\u2019s reliability in uncovering receptive fields was inconsistent. We think this technique could benefit from more advanced methods for calculating the attention like Grad-CAM.\nOur model, the first of its kind using an auto-regressive decoder-based generative pre-training approach, contrasts with encoder-based methods like LFADS or CEBRA. While these methods explicitly exploit the intrinsic dimensionality of neuronal populations, the Neuroformer, inspired by the GPT objective, takes a different approach. Out attempts to use our data with representation learning approaches yielded poor results (see Fig. 10). These methods are more typically applied to motor control tasks for BCI applications. Direct comparisons with methods like LFADS are needed, as they might currently offer better representation learning due to explicit low-dimensional constraints. In contrast, our method is more akin to an in-silico model of the brain.\nWhile transformers, like the Neuroformer, offer advantages like being modality-agnostic and contextualizing information over extended sequences, traditional methods like the GLM and CNNs have their merits. They are grounded in neuroscientific assumptions, offering interpretability. Although the Neuroformer showed promise in decoding tasks, the reasons for its performance advantages remain unclear. Factors like overparameterization, ability to model non-linearities, and effective representation learning might contribute.\nOur aim with the Neuroformer was two-fold: 1) to introduce a tool for neuroscience to flexibly incorporate data from increasingly diverse and complex experimental settings and 2) to explore generative pre-training for such data. The true potential of this method might be realized with increased scale, given the vast data available in systems neuroscience. The Neuroformer marks a step towards this, but challenges persist. We\u2019re keen on scaling these methods and finding synergies between large language and neuroscience models."
        },
        {
            "heading": "B ETHICAL STATEMENT",
            "text": "This project involves data from mouse experiments. Our AAALAC (Association for Assessment and Accreditation of Laboratory Animal Care)-accredited institution is committed to the humane care and use of animals in research and education. Our Institutional Animal Care and Use Committee (IACUC) oversees animal care and use, facilities, and procedures. The attending veterinarian and IACUC ensure that research activities involving mice meet the ethical and legal requirements as set by relevant policies and guidelines."
        },
        {
            "heading": "C HYPERPARAMETERS",
            "text": "C.1 NEUROFORMER"
        },
        {
            "heading": "D COMPARISON MODEL DETAILS",
            "text": "D.1 POPULATION RESPONSE\nD.1.1 GLM\nA generalized linear model (GLM) for neuron data was adapted from (Pillow et al., 2008a). The instant firing rate (ri(t)) of individual neurons was modeled by a linear-nonlinear cascade, including linear stimulus components (k \u2217x) and spike history components (h \u2217 y) followed by an exponential nonlinearity. \u00b5 represents the baseline firing rate of the neuron. The stimulus kernel (k) and history kernel (h) are modeled by cosine bases. The parameters are optimized by minimizing the loglikelihood for spike prediction.\nri(t) = exp(k \u2217 x+ h \u2217 y + \u00b5) (5)\nD.2 BEHAVIOR DECODING\nD.2.1 MLP\nThe MLP used for the behavior comparisons was comprised of 5 stacked MLP layers, with a hidden state of (h = 1024), GeLU non-linearity, dropout p = 0.01, layer-normalization between each of the MLPs. No improvement was observed upon adding more layers.\nD.2.2 GRU\nThe GRU model used for the behavior comparisons had the following architecture: 5 Bi-directional GRU layers with dropout p = 0.01, layer-normalization, and tapering t=0.5 for each consecutive hidden state. Tapering allowed us to stack more GRU layers without undergoing collapse. More layers resulted in a decrease in performance and eventual collapse. The first hidden state of the model was initialized as the neural past state as defined in the paper, allowing the model to integrate past information. We employed Neuroformer-style tokenization which shortened the sequence of spikes and resulted in better performance."
        },
        {
            "heading": "E BLOCK-SPARSE CAUSAL MASKING",
            "text": "To precisely align the corresponding neural features with all other modalities, we broke our state features into a Current State wc and a Past State wp according to specified temporal windows (see Table 2).\nBecause of this, we cannot simply model the neural features as a continuous string of spikes (or the equivalent language tokens) as in the case of a standard GPT model (Radford et al., 2019). The unique challenge of arranging our data in this way is that each of these intervals contains a different number of spikes. Our situation is similar to (Ramesh et al., 2021), where during inference time, the block is autoregressively populated by the predicted tokens. The difference in our case is that each resulting \u201dblock\u201d is of a different size. We therefore employ right-masking and padding to enable efficient training and causal inference (see Fig. 8).\nNote that by just masking our Current State and projecting from the Tn row of the block during inference time, we are able to fuse with any other non-masked features, while preventing any backflow of information, and retaining the causal aspect of our model."
        },
        {
            "heading": "F CALCULATING CONNECTIVITY WITH OTHER METHODS",
            "text": ""
        },
        {
            "heading": "G CAPTURING TRIAL-TO-TRIAL VARIABILITY",
            "text": "When investigating the performance of the model as we progressively incorporated each module, we examined F1 scores, and the ability of the model to capture the trial-to-trial variation across the data sets.\nTo do this, we computed the correlation across trial-averaged spikes for each neuron across groups of four trials. We compared a holdout group to the corresponding set generated by the Neuroformer, and to another corresponding group of trials from the data set. We observed a steady improvement in the distribution of correlations when compared to the ground-truth (Fig. 10), with the full model\u2019s predictions closely matching those of the ground-truth sets."
        },
        {
            "heading": "H PRECISE BEHAVIOR PREDICTIONS",
            "text": "An advantage of the Neuroformer architecture, is that modalities can be flexibly incorporated as an input or label. In the case of labels, variables can be modeled as both classes as in multi-class logistic regression using a cross-entropy objective (Fig. 11), or raw values as a linear regression using mean-squared error (Fig. 12). Our model vastly outperformed other baselines, and performed reliably across datasets and objectives, as opposed to other methods, whose performance fluctuated."
        },
        {
            "heading": "1.5 1.0 0.5 0.0 0.5 1.0 1.5",
            "text": ""
        },
        {
            "heading": "I MULTITASK DECODING",
            "text": "In addition to speed, Neuroformer is able to jointly predict many other variables. Here we show that the model could effectively predict the eye position of the mouse - even in the absence of input stimulus. We used a single model checkpoint to decode all three variables. Speed and longitudinal + latitudinal angle for eye position.\n1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0\nTrue speed\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nPr ed\nict ed\nsp ee\nd\nr = 0.93 p < 0.001\n4 2 0 2\nTrue phi\n6\n4\n2\n0\n2\nPr ed\nict ed\np hi\nr = 0.84 p < 0.001\n2 1 0 1 2 3 4 5 True th 3\n2\n1\n0\n1\n2\n3\n4\n5\nPr ed\nict ed\nth\nr = 0.92 p < 0.001\nVisnav lateral Multitask Decoding - Speed + Eye Gaze (phi, th)\nFigure 13: Decoding Speed (m/s) and Eye position phi, th (\u25e6)."
        },
        {
            "heading": "J MULTIMODAL LATENT VARIABLES",
            "text": "The contrastive part of Neuroformer, enables us to align any of the corresponding auxilliary modalities with our neural data, including visual features, behavioral features or reward. Here we show the raw features extracted from the contrastive stage that was trained to align speed with neural features.\nNo dimensionality reduction method (like T-SNE) was used for this visualization. These are the raw features, extracted from our contrastive alignment module, by setting Eclip = 3."
        },
        {
            "heading": "K CEBRA BEHAVIOR RESULTS",
            "text": "We also attempted to compare Neuroformer behavior predictions with the SOTA in representation learning methods, CEBRA (Schneider et al., 2022). Despite attempting an extensive hyperparameter search accross batch size, temperature, learning rate, offset, and decoding method (KNN and Lasso Reg.) we were unable to get the model to generalize, and typically observed heavy overfitting as seen in Fig. 15."
        }
    ],
    "year": 2023
}