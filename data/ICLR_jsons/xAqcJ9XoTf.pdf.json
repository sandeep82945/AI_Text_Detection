{
    "abstractText": "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) Non-uniqueness: there are many different eigendecompositions of the same Laplacian, and (2) Instability: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address nonuniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a \u201chard partition\u201d of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to \u201csoftly partition\u201d eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yinan Huang"
        },
        {
            "affiliations": [],
            "name": "William Lu"
        },
        {
            "affiliations": [],
            "name": "Joshua Robinson"
        },
        {
            "affiliations": [],
            "name": "Yu Yang"
        },
        {
            "affiliations": [],
            "name": "Muhan Zhang"
        },
        {
            "affiliations": [],
            "name": "Stefanie Jegelka"
        },
        {
            "affiliations": [],
            "name": "Pan Li"
        }
    ],
    "id": "SP:9e49246be9526d0fc217b4ef70594568e0f58a7d",
    "references": [
        {
            "authors": [
                "Raghu Arghal",
                "Eric Lei",
                "Shirin Saeedi Bidokhti"
            ],
            "title": "Robust graph neural networks via probabilistic lipschitz constraints",
            "venue": "In Learning for Dynamics and Control Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Vikraman Arvind",
                "Frank Fuhlbr\u00fcck",
                "Johannes K\u00f6bler",
                "Oleg Verbitsky"
            ],
            "title": "On weisfeiler-leman invariance: Subgraph counts and related graph properties",
            "venue": "Journal of Computer and System Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Pablo Barcel\u00f3",
                "Floris Geerts",
                "Juan Reutter",
                "Maksimilian Ryschkov"
            ],
            "title": "Graph neural networks with local graph parameters",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Dylan J Foster",
                "Matus J Telgarsky"
            ],
            "title": "Spectrally-normalized margin bounds for neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Beatrice Bevilacqua",
                "Fabrizio Frasca",
                "Derek Lim",
                "Balasubramaniam Srinivasan",
                "Chen Cai",
                "Gopinath Balamurugan",
                "Michael M. Bronstein",
                "Haggai Maron"
            ],
            "title": "Equivariant subgraph aggregation networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Deyu Bo",
                "Chuan Shi",
                "Lele Wang",
                "Renjie Liao"
            ],
            "title": "Specformer: Spectral graph neural networks meet transformers",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Giorgos Bouritsas",
                "Fabrizio Frasca",
                "Stefanos Zafeiriou",
                "Michael M Bronstein"
            ],
            "title": "Improving graph neural network expressivity via subgraph isomorphism counting",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Michael M Bronstein",
                "Joan Bruna",
                "Yann LeCun",
                "Arthur Szlam",
                "Pierre Vandergheynst"
            ],
            "title": "Geometric deep learning: going beyond euclidean data",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2017
        },
        {
            "authors": [
                "Guangyong Chen",
                "Pengfei Chen",
                "Chang-Yu Hsieh",
                "Chee-Kong Lee",
                "Benben Liao",
                "Renjie Liao",
                "Weiwen Liu",
                "Jiezhong Qiu",
                "Qiming Sun",
                "Jie Tang",
                "Richard S. Zemel",
                "Shengyu Zhang"
            ],
            "title": "Alchemy: A quantum chemistry dataset for benchmarking",
            "year": 1906
        },
        {
            "authors": [
                "Zhengdao Chen",
                "Lei Chen",
                "Soledad Villar",
                "Joan Bruna"
            ],
            "title": "Can graph neural networks count substructures",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "C. Chuang",
                "S. Jegelka"
            ],
            "title": "Tree mover\u2019s distance: Bridging graph metrics and stability of graph neural networks",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Fan RK Chung"
            ],
            "title": "Spectral graph theory, volume 92",
            "venue": "American Mathematical Soc.,",
            "year": 1997
        },
        {
            "authors": [
                "Moustapha Cisse",
                "Piotr Bojanowski",
                "Edouard Grave",
                "Yann Dauphin",
                "Nicolas Usunier"
            ],
            "title": "Parseval networks: Improving robustness to adversarial examples",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Nicolas Courty",
                "R\u00e9mi Flamary",
                "Amaury Habrard",
                "Alain Rakotomamonjy"
            ],
            "title": "Joint distribution optimal transportation for domain adaptation",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "David K Duvenaud",
                "Dougal Maclaurin",
                "Jorge Iparraguirre",
                "Rafael Bombarell",
                "Timothy Hirzel",
                "Al\u00e1n Aspuru-Guzik",
                "Ryan P Adams"
            ],
            "title": "Convolutional networks on graphs for learning molecular fingerprints",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Vijay Prakash Dwivedi",
                "Xavier Bresson"
            ],
            "title": "A generalization of transformer networks to graphs",
            "venue": "AAAI Workshop on Deep Learning on Graphs: Methods and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Thomas Laurent",
                "Yoshua Bengio",
                "Xavier Bresson"
            ],
            "title": "Graph neural networks with learnable structural and positional representations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Vijay Prakash Dwivedi",
                "Chaitanya K. Joshi",
                "Anh Tuan Luu",
                "Thomas Laurent",
                "Yoshua Bengio",
                "Xavier Bresson"
            ],
            "title": "Benchmarking graph neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Moshe Eliasof",
                "Fabrizio Frasca",
                "Beatrice Bevilacqua",
                "Eran Treister",
                "Gal Chechik",
                "Haggai Maron"
            ],
            "title": "Graph positional encoding via random feature propagation",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Fernando Gama",
                "Joan Bruna",
                "Alejandro Ribeiro"
            ],
            "title": "Stability properties of graph neural networks",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Vikas Garg",
                "Stefanie Jegelka",
                "Tommi Jaakkola"
            ],
            "title": "Generalization and representational limits of graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Michelle Girvan",
                "Mark EJ Newman"
            ],
            "title": "Community structure in social and biological networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2002
        },
        {
            "authors": [
                "Mark Granovetter"
            ],
            "title": "The strength of weak ties: A network theory revisited",
            "venue": "Sociological theory,",
            "year": 1983
        },
        {
            "authors": [
                "Yinan Huang",
                "Xingang Peng",
                "Jianzhu Ma",
                "Muhan Zhang"
            ],
            "title": "Boosting the cycle counting power of graph neural networks with i$\u02c62$-GNNs",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yuanfeng Ji",
                "Lu Zhang",
                "Jiaxiang Wu",
                "Bingzhe Wu",
                "Lanqing Li",
                "Long-Kai Huang",
                "Tingyang Xu",
                "Yu Rong",
                "Jie Ren",
                "Ding Xue",
                "Houtim Lai",
                "Wei Liu",
                "Junzhou Huang",
                "Shuigeng Zhou",
                "Ping Luo",
                "Peilin Zhao",
                "Yatao Bian"
            ],
            "title": "Drugood: Out-of-distribution dataset curator and benchmark for ai-aided drug discovery \u2013 a focus on affinity prediction problems with noise annotations",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Chuntao Jiang",
                "Frans Coenen",
                "Michele Zito"
            ],
            "title": "Finding frequent subgraphs in longitudinal social network data using a weighted graph mining approach",
            "venue": "In Advanced Data Mining and Applications: 6th International Conference,",
            "year": 2010
        },
        {
            "authors": [
                "Henry Kenlay",
                "Dorina Thanou",
                "Xiaowen Dong"
            ],
            "title": "On the stability of polynomial spectral graph filters",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Henry Kenlay",
                "Dorina Thano",
                "Xiaowen Dong"
            ],
            "title": "On the stability of graph convolutional neural networks under edge rewiring",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Jinwoo Kim",
                "Dat Nguyen",
                "Seonwoo Min",
                "Sungjun Cho",
                "Moontae Lee",
                "Honglak Lee",
                "Seunghoon Hong"
            ],
            "title": "Pure transformers are powerful graph learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Mehmet Koyut\u00fcrk",
                "Ananth Grama",
                "Wojciech Szpankowski"
            ],
            "title": "An efficient algorithm for detecting frequent subgraphs in biological networks. Bioinformatics, 20(suppl 1):i200\u2013i207",
            "year": 2004
        },
        {
            "authors": [
                "Devin Kreuzer",
                "Dominique Beaini",
                "Will Hamilton",
                "Vincent L\u00e9tourneau",
                "Prudencio Tossou"
            ],
            "title": "Rethinking graph transformers with spectral attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Pan Li",
                "Yanbang Wang",
                "Hongwei Wang",
                "Jure Leskovec"
            ],
            "title": "Distance encoding: Design provably more powerful neural networks for graph representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Derek Lim",
                "Joshua David Robinson",
                "Lingxiao Zhao",
                "Tess Smidt",
                "Suvrit Sra",
                "Haggai Maron",
                "Stefanie Jegelka"
            ],
            "title": "Sign and basis invariant networks for spectral graph representation learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Haggai Maron",
                "Heli Ben-Hamu",
                "Hadar Serviansky",
                "Yaron Lipman"
            ],
            "title": "Provably powerful graph networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Haggai Maron",
                "Heli Ben-Hamu",
                "Nadav Shamir",
                "Yaron Lipman"
            ],
            "title": "Invariant and equivariant graph networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Sohir Maskey",
                "Ali Parviz",
                "Maximilian Thiessen",
                "Hannes St\u00e4rk",
                "Ylli Sadikaj",
                "Haggai Maron"
            ],
            "title": "Generalized laplacian positional encoding for graph representation learning",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Morris",
                "Martin Ritzert",
                "Matthias Fey",
                "William L Hamilton",
                "Jan Eric Lenssen",
                "Gaurav Rattan",
                "Martin Grohe"
            ],
            "title": "Weisfeiler and leman go neural: Higher-order graph neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Morris",
                "Gaurav Rattan",
                "Petra Mutzel"
            ],
            "title": "Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "David McAllester",
                "Nati Srebro"
            ],
            "title": "Exploring generalization in deep learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "Nathan Srebro"
            ],
            "title": "A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Ladislav Ramp\u00e1\u0161ek",
                "Michael Galkin",
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Guy Wolf",
                "Dominique Beaini"
            ],
            "title": "Recipe for a general, powerful, scalable graph transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ladislav Ramp\u00e1\u0161ek",
                "Michael Galkin",
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Guy Wolf",
                "Dominique Beaini"
            ],
            "title": "Recipe for a general, powerful, scalable graph transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jian Shen",
                "Yanru Qu",
                "Weinan Zhang",
                "Yong Yu"
            ],
            "title": "Wasserstein distance guided representation learning for domain adaptation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jure Sokoli\u0107",
                "Raja Giryes",
                "Guillermo Sapiro",
                "Miguel RD Rodrigues"
            ],
            "title": "Robust large margin deep neural networks",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Gilbert W Stewart",
                "Ji-guang Sun"
            ],
            "title": "Matrix perturbation theory",
            "year": 1990
        },
        {
            "authors": [
                "Jonathan M Stokes",
                "Kevin Yang",
                "Kyle Swanson",
                "Wengong Jin",
                "Andres Cubillos-Ruiz",
                "Nina M Donghia",
                "Craig R MacNair",
                "Shawn French",
                "Lindsey A Carfrae",
                "Zohar Bloom-Ackermann"
            ],
            "title": "A deep learning approach to antibiotic",
            "venue": "discovery. Cell,",
            "year": 2020
        },
        {
            "authors": [
                "Behrooz Tahmasebi",
                "Derek Lim",
                "Stefanie Jegelka"
            ],
            "title": "Counting substructures with higher-order graph neural networks: Possibility and impossibility results",
            "venue": "arXiv preprint arXiv:2012.03174,",
            "year": 2020
        },
        {
            "authors": [
                "Yusuke Tsuzuku",
                "Issei Sato",
                "Masashi Sugiyama"
            ],
            "title": "Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Haorui Wang",
                "Haoteng Yin",
                "Muhan Zhang",
                "Pan Li"
            ],
            "title": "Equivariant and stable positional encoding for more powerful graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Haorui Wang",
                "Haoteng Yin",
                "Muhan Zhang",
                "Pan Li"
            ],
            "title": "Equivariant and stable positional encoding for more powerful graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "K. Xu",
                "M. Zhang",
                "J. Li",
                "S. Du",
                "K. Kawarabayashi",
                "S. Jegelka"
            ],
            "title": "How neural networks extrapolate: From feedforward to graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Gilad Yehudai",
                "Ethan Fetaya",
                "Eli Meirom",
                "Gal Chechik",
                "Haggai Maron"
            ],
            "title": "On size generalization in graph neural networks",
            "year": 2020
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaxuan You",
                "Rex Ying",
                "Jure Leskovec"
            ],
            "title": "Position-aware graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jiaxuan You",
                "Jonathan M Gomes-Selman",
                "Rex Ying",
                "Jure Leskovec"
            ],
            "title": "Identity-aware graph neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Yu",
                "Tengyao Wang",
                "Richard J"
            ],
            "title": "Samworth. A useful variant of the davis\u2013kahan theorem for statisticians",
            "year": 2015
        },
        {
            "authors": [
                "Muhan Zhang",
                "Yixin Chen"
            ],
            "title": "Link prediction based on graph neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Muhan Zhang",
                "Pan Li"
            ],
            "title": "Nested graph neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Lingxiao Zhao",
                "Wei Jin",
                "Leman Akoglu",
                "Neil Shah"
            ],
            "title": "From stars to subgraphs: Uplifting any GNN with local structure awareness",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning models for graph-structured data such as Graph Neural Networks (GNNs) and Graph Transformers have been arguably one of the most popular machine learning models on graphs, and have achieved remarkable results for numerous applications in drug discovery, computational chemistry, and social network analysis, etc. (Kipf & Welling, 2017; Bronstein et al., 2017; Duvenaud et al., 2015; Stokes et al., 2020; Zhang & Chen, 2018; Ying et al., 2021; Rampa\u0301s\u030cek et al., 2022b). However, there is a common concern about these models: the limited expressive power. For example, it is known that message-passing GNNs are at most expressive as the Weisfeiler-Leman test (Xu et al., 2019; Morris et al., 2019) in distinguishing non-isomorphic graphs, and in general cannot even approximate common functions such as the number of certain subgraph patterns (Chen et al., 2020; Arvind et al., 2020; Tahmasebi et al., 2020; Huang et al., 2023). These limitations could significantly restrict model performance, e.g., since graph substructures can be closely related to the target function in chemistry, biology and social network analysis (Girvan & Newman, 2002; Granovetter, 1983; Koyutu\u0308rk et al., 2004; Jiang et al., 2010; Bouritsas et al., 2022).\nTo alleviate expressivity limitations, there has been considerable interest in designing effective positional encodings for graphs (You et al., 2019; Dwivedi & Bresson, 2021; Wang et al., 2022a). Generalized from the positional encodings of 1-D sequences for Transformers (Vaswani et al., 2017), the idea is to endow nodes with information about their relative position within the graph and thus make them more distinguishable. Many promising graph positional encodings use the eigenvalue decomposition of the graph Laplacian (Dwivedi et al., 2023; Kreuzer et al., 2021). The eigenvalue decomposition is a strong candidate because the Laplacian fully describes the adjacency structure\n\u2217equal contribution\nof a graph, and there is a deep understanding of how these eigenvectors and eigenvalues inherit this information (Chung, 1997). However, eigenvectors have special structures that must be taken into consideration when designing architectures that process eigenvectors.\nFirstly, eigenvectors are not unique: if v is an eigenvector, then so is \u2212v. Furthermore, when there are multiplicities of eigenvalues then there are many more symmetries, since any orthogonal change of basis of the corresponding eigenvectors yields the same Laplacian. Because of this basis ambiguity, neural networks that process eigenvectors should be basis invariant: applying basis transformations to input eigenvectors should not change the output of the neural network. This avoids the pathological scenario where different eigendecompositions of the same Laplacian produce different model predictions. Several prior works have explored sign and basis symmetries of eigenvectors. For example, Dwivedi & Bresson (2021); Kreuzer et al. (2021) randomly flip the sign of eigenvectors during training so that the resulting model is robust to sign transformation. Lim et al. (2023) instead design new neural architectures that are invariant to sign flipping (SignNet) or basis transformation (BasisNet). Although these basis invariant methods have the right symmetries, they do not yet account for the fact that two Laplacians that are similar but distinct may produce completely different eigenspaces.\nThis brings us to another important consideration, that of stability. Small perturbations to the input Laplacian should only induce a limited change of final positional encodings. This \u201csmall change of Laplacians, small change of positional encodings\u201d actually generalizes the previous concept of basis invariance and proposes a stronger requirement on the networks. But this stability (or continuity) requirement is a great challenge for graphs, because small perturbations can produce completely different eigenvectors if some eigenvalues are close (Wang et al. (2022a), Lemma 3.4). Since the neural networks process eigenvectors, not the Laplacian matrix itself, they run the risk of being highly discontinuous with respect to the input matrix, leading to an inability to generalize to new graph structures and a lack of robustness to any noise in the input graph\u2019s adjacency. In contrast, stable models enjoy many benefits such as adversarial robustness (Cisse et al., 2017; Tsuzuku et al., 2018) and provable generalization (Sokolic\u0301 et al., 2017).\nUnfortunately, existing positional encoding methods are not stable. Methods that only focus on sign invariance (Dwivedi & Bresson, 2021; Kreuzer et al., 2021; Lim et al., 2023), for instance, are not guaranteed to satisfy \u201csame Laplacian, same positional encodings\u201d if multiplicity of eigenvalues exists. Basis invariant methods such as BasisNet are unstable because they apply different neural networks to different eigensubspaces. In a high-level view, they perform a hard partitioning of eigenspaces and treat each chunk separately (see Appendix C for a detailed discussion). The discontinuous nature of partitioning makes them highly sensitive to perturbations of the Laplacian. The hard partition also requires fixed eigendecomposition thus unsuitable for graph-level tasks. On the other hand, Wang et al. (2022a) proposes a provably stable positional encoding. But, to achieve stability, it completely ignores the distinctness of each eigensubspaces and processes the merged eigenspaces homogeneously. Consequently, it loses expressive power and has, e.g., a subpar performance on molecular graph regression tasks (Rampa\u0301s\u030cek et al., 2022a).\nMain contributions. In this work, we present Stable and Expressive Positional Encodings (SPE). The key insight is to perform a soft and learnable \u201cpartition\u201d of eigensubspaces in a eigenvalue dependent way, hereby achieving both stability (from the soft partition) and expressivity (from dependency on both eigenvalues and eigenvectors). Specifically:\n\u2022 SPE is provably stable. We show that the network sensitivity w.r.t. the input Laplacian is determined by the gap between the d-th and (d + 1)-th smallest eigenvalues if using the first d eigenvectors and eigenvalues. This implies our method is stable regardless of how the used d eigenvectors and eigenvalues change.\n\u2022 SPE can universally approximate basis invariant functions and is as least expressive as existing methods in distinguishing graphs. We also prove its capability in counting graph substructures.\n\u2022 We empirically illustrate that introducing more stability helps generalize better but weakens the expressive power. Besides, on the molecule graph prediction datasets ZINC and Alchemy, our method significantly outperforms other positional encoding methods. On DrugOOD (Ji et al., 2023), a ligand-based affinity prediction task with domain shifts, our method demonstrates a clear and constant improvement over other unstable positional encodings. All these validate the effectiveness of our stable and expressive method."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Notation. We always use n for the number of nodes in a graph, d \u2264 n for the number of eigenvectors and eigenvalues chosen, and p for the dimension of the final positional encoding for each node. We use \u2225\u00b7\u2225 to denote the L2 norm of vectors and matrices, and \u2225\u00b7\u2225F for the Frobenius norm of matrices. Graphs and Laplacian Encodings. Denote an undirected graph with n nodes by G = (A,X), where A \u2208 Rn\u00d7n is the adjacency matrix and X \u2208 Rn\u00d7p is the node feature matrix. Let D = diag([ \u2211n j=1 Ai,j ] n i=1) be the diagonal degree matrix. The normalized Laplacian matrix of G is a positive semi-definite matrix defined by L = I \u2212D\u22121/2AD\u22121/2. Its eigenvalue decomposition L = V diag(\u03bb)V \u22a4 returns eigenvectors V and eigenvalues \u03bb, which we denote by EVD(L) = (V ,\u03bb). In practice we may only use the smallest d \u2264 n eigenvalues and eigenvectors, so abusing notation slightly, we also denote the smallest d eigenvalues by \u03bb \u2208 Rd and the corresponding d eigenvectors by V \u2208 Rn\u00d7d. A Laplacian positional encoding is a function that produces node embeddings Z \u2208 Rn\u00d7p given (V ,\u03bb) \u2208 Rn\u00d7d \u00d7 Rd as input. Basis invariance. Given eigenvalues \u03bb \u2208 Rd, if eigenvalue \u03bbi has multiplicity di, then the corresponding eigenvectors Vi \u2208 Rn\u00d7di form a di-dimensional eigenspace. A vital symmetry of eigenvectors is the infinitely many choices of basis eigenvectors describing the same underlying eigenspace. Concretely, if Vi is a basis for the eigenspace of \u03bbi, then ViQi is, too, for any orthogonal matrix Qi \u2208 O(di). The symmetries of each eigenspace can be collected together to describe the overall symmetries of V in terms of the direct sum group O(\u03bb) := \u2295iO(di) = {\u2295iQi \u2208 R \u2211 i di\u00d7 \u2211 i di : Qi \u2208 O(di)}, i.e., block diagonal matrices with ith block belonging to O(di). Namely, for any Q \u2208 O(\u03bb), both (V ,\u03bb) and (V Q,\u03bb) are eigendecompositions of the same underlying matrix. When designing a model f that takes eigenvectors as input, we want f to be basis invariant: f(V Q,\u03bb) = f(V ,\u03bb) for any (V ,\u03bb) \u2208 Rn\u00d7d \u00d7 Rd, and any Q \u2208 O(\u03bb). Permutation equivariance. Let \u03a0(n) = {P \u2208 {0, 1}n\u00d7n : PP\u22a4 = I} be the permutation matrices of n elements. A function f : Rn \u2192 Rn is called permutation equivariant, if for any x \u2208 Rn and any permutation P \u2208 \u03a0(n), it satisfies f(Px) = P f(x). Similarly, f : Rn\u00d7n \u2192 Rn is said to be permutation equivariant if satisfying f(PXP\u22a4) = P f(X)."
        },
        {
            "heading": "3 A PROVABLY STABLE AND EXPRESSIVE PE",
            "text": "In this section we introduce our model Stable and Expressive Positional Encodings (SPE). SPE is both stable and a maximally expressive basis invariant architecture for processing eigenvector data, such as Laplacian eigenvectors. We begin with formally defining the stability of a positional encoding. Then we describe our SPE model, and analyze its stability. In the final two subsections we show that higher stability leads to improved out-of-distribution generalization, and show that SPE is a universally expressive basis invariant architecture."
        },
        {
            "heading": "3.1 STABLE POSITIONAL ENCODINGS",
            "text": "Stability intuitively means that a small input perturbation yields a small change in the output. For eigenvector-based positional encodings, the perturbation is to the Laplacian matrix, and should result in a small change of node-level positional embeddings. Definition 3.1 (PE Stability). A PE method PE : Rn\u00d7d\u00d7Rd \u2192 Rn\u00d7p is called stable, if there exist constants c, C > 0, such that for any Laplacian L,L\u2032,\n\u2225PE(EVD(L))\u2212 P\u2217PE(EVD(L\u2032))\u2225F \u2264 C \u00b7 \u2225\u2225L\u2212 P\u2217L\u2032P\u22a4\u2217 \u2225\u2225cF , (1)\nwhere P\u2217 = argminP\u2208\u03a0(n) \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F is the permutation matrix matching two Laplacians.\nIt is worth noting that here we adopt a slightly generalized definition of typical stability via Lipschitz continuity (c = 1). This definition via Ho\u0308lder continuity describes a more comprehensive stability behavior of PE methods, while retaining the essential idea of stability. Remark 3.1 (Stability implies permutation equivariance). Note that a PE method is permutation equivariant if it is stable: simply let L = PL\u2032P\u22a4 for some P \u2208 \u03a0(n) and we obtain the desired permutation equivariance PE(EVD(PLP\u22a4)) = P \u00b7 PE(EVD(L)).\nStability is hard to achieve due to the instability of eigenvalue decomposition\u2014a small perturbation of the Laplacian can produce completely different eigenvectors (Wang et al. (2022a), Lemma 3.4). Since positional encoding models process the eigenvectors (and eigenvalues), they naturally inherit this instability with respect to the input matrix. Indeed, as mentioned above, many existing positional encodings are not stable. The main issue is that they partition the eigenvectors by eigenvalue, which leads to instabilities. See Appendix C for a detailed discussion."
        },
        {
            "heading": "3.2 SPE: A POSITIONAL ENCODING WITH GUARANTEED STABILITY",
            "text": "To achieve stability, the key insight is to avoid a hard partition of eigensubspaces. Simultaneously, we should fully utilize the information in the eigenvalues for strong expressive power. Therefore, we propose to do a \u201csoft partitioning\u201d of eigenspaces by leveraging eigenvalues. Instead of treating each eigensubspace independently, we apply a weighted sum of eigenvectors in an eigenvalue dependent way. If done carefully, this can ensure that as two distinct eigenvalues converge\u2014these are exactly the degenerate points creating instability\u2014the way their respective eigenvectors are processed becomes more similar. This means that if two eigenvectors are \u201cswapped\u201d, as happens at degenerate points, the model output does not change much. The resulting method is (illustrated in Figure 1):\nSPE : SPE(V ,\u03bb) = \u03c1 ( V diag(\u03d51(\u03bb))V \u22a4,V diag(\u03d52(\u03bb))V \u22a4, ...,V diag(\u03d5m(\u03bb))V \u22a4 ), (2) where the input is the d smallest eigenvalues \u03bb \u2208 Rd and corresponding eigenvectors V \u2208 Rn\u00d7d, m is a hyper-parameter, and \u03d5\u2113 : Rd \u2192 Rd and \u03c1 : Rn\u00d7n\u00d7m \u2192 Rn\u00d7p are always permutation equivariant neural networks. Here, permutation equivariance means \u03d5\u2113(P\u03bb) = P\u03d5\u2113(\u03bb) for P \u2208 \u03a0(d) and \u03c1(PAP\u22a4) = P \u03c1(A) for any P \u2208 \u03a0(n) and input A. There are many choices of permutation equivariant networks that can be used, such as element-wise MLPs or Deep Sets (Zaheer et al., 2017) for \u03d5\u2113, and graph neural networks for \u03c1. The permutation equivariance of \u03d5\u2113 and \u03c1 ensures that SPE is basis invariant.\nNote that in Eq. (2), the term V diag(\u03d5\u2113(\u03bb))V \u22a4 looks like a spectral graph convolution operator. But they are methodologically different: SPE uses V diag(\u03d5\u2113(\u03bb))V \u22a4 to construct positional encodings, which are not used as a convolution operation to process node attributes (say as V diag(\u03d5\u2113(\u03bb))V \u22a4X). Also, \u03d5\u2113\u2019s are general permutation equivariant functions that may express the\ninteractions between different eigenvalues instead of elementwise polynomials on each eigenvalue separately which are commonly adopted in spectral graph convolution.\nIt is also worthy noticing that term V diag(\u03d5\u2113(\u03bb))V \u22a4 will reduce to hard partitions of eigenvectors in the \u2113-th eigensubspace if we let [\u03d5\u2113(\u03bb)]j = 1(\u03bbj is the l-th smallest eigenvalue). To obtain stability, what we need is to constrain \u03d5\u2113 to continuous functions to perform a continuous \u201csoft partition\u201d. Assumption 3.1. The key assumptions for SPE are as follows:\n\u2022 \u03d5\u2113 and \u03c1 are permutation equivariant (see definitions after SPE Eq. (2)). \u2022 \u03d5\u2113 is K\u2113-Lipshitz continuous: for any \u03bb,\u03bb\u2032 \u2208 Rd, \u2225\u03d5\u2113(\u03bb)\u2212 \u03d5\u2113(\u03bb\u2032)\u2225F \u2264 K\u2113 \u2225\u03bb\u2212 \u03bb\u2032\u2225 . \u2022 \u03c1 is J-Lipschitz continuous: for any [A1,A2, ...,Am] \u2208 Rn\u00d7n\u00d7m and [A\u20321,A\u20322, ...,A\u2032m] \u2208 Rn\u00d7n\u00d7m, \u2225\u03c1(A1,A2, ...,Am)\u2212 \u03c1(A\u20321,A\u20322, ...,A\u2032m)\u2225F \u2264 J \u2211m l=1 \u2225A\u2113 \u2212A\u2032\u2113\u2225F .\nThese two continuity assumptions generally hold by assuming the underlying networks have normbounded weights and continuous activation functions, such as ReLU. As a result, Assumption 3.1 is mild for most neural networks.\nNow we are ready to present our main theorem, which states that continuity of \u03d5\u2113 and \u03c1 leads to the desired stability. Theorem 3.1 (Stability of SPE). Under Assumption 3.1, SPE is stable with respect to the input Laplacian: for Laplacians L,L\u2032,\n\u2225SPE(EVD(L))\u2212 P\u2217SPE(EVD(L\u2032))\u2225F \u2264(\u03b11 + \u03b12)d 5/4 \u221a \u2225L\u2212 P\u2217LP\u22a4\u2217 \u2225F\n+ ( \u03b12 d \u03b3 + \u03b13 )\u2225\u2225L\u2212 P\u2217LP\u22a4\u2217 \u2225\u2225F , (3) where the constants are \u03b11 = 2J \u2211m l=1 K\u2113, \u03b12 = 4 \u221a 2J \u2211m l=1 M\u2113 and \u03b13 = J \u2211m\nl=1 K\u2113. Here"
        },
        {
            "heading": "M\u2113 = sup\u03bb\u2208[0,2]d \u2225\u03d5\u2113(\u03bb)\u2225 and again P\u2217 = argminP\u2208\u03a0(n)",
            "text": "\u2225\u2225L\u2212 P\u2217LP\u22a4\u2217 \u2225\u2225F. The eigengap \u03b3 = \u03bbd+1 \u2212 \u03bbd is the difference between the (d + 1)-th and d-th smallest eigenvalues, and \u03b3 = +\u221e if d = n.\nNote that the stability of SPE is determined by both the Lipschitz constants J,K\u2113 and the eigengap \u03b3 = \u03bbd \u2212 \u03bbd+1. The dependence on \u03b3 comes from the fact that we only choose to use d eigenvectors/eigenvalues. It is inevitable as long as d < n, and it disappears (\u03b3 = +\u221e) if we let d = n. This phenomenon is also observed in PEG (Wang et al. (2022a), Theorem 3.6)."
        },
        {
            "heading": "3.3 FROM STABILITY TO OUT-OF-DISTRIBUTION GENERALIZATION",
            "text": "An important implication of stability is that one can characterize the domain generalization gap by the model\u2019s Lipschitz constant (Courty et al., 2017; Shen et al., 2018). Although our method satisfies Ho\u0308lder continuity instead of strict Lipschitz continuity, we claim that interestingly, a similar bound can still be obtained for domain generalization.\nWe consider graph regression with domain shift: the training graphs are sampled from source domain L \u223c PS , while the test graphs are sampled from target domain L \u223c PT . With groundtruth function f(L) \u2208 R and a prediction model h(L) \u2208 R, we are interested in the gap between in-distribution error \u03b5s(h) = EL\u223cPS |h(L) \u2212 f(L)| and out-of-distribution error \u03b5t(h) = EL\u223cPT |h(L) \u2212 f(L)|. The following theorem states that for a base GNN equipped with SPE, we can upper bound the generalization gap in terms of the Ho\u0308lder constant of SPE, the Lipschitz constant of the base GNN and the 1-Wasserstein distance between source and target distributions. Proposition 3.1. Assume Assumption 3.1 hold, and assume a base GNN model GNN(L,X) \u2208 R that is C-Lipschitz continuous, i.e.,\n|GNN(L,X)\u2212GNN(L\u2032,X \u2032)| \u2264 C min P\u2208\u03a0(n) (\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F + \u2225X \u2212 PX \u2032\u2225F ) , (4)\nfor any Laplacians L,L\u2032 and node features X,X \u2032. Now let GNN take positional encodings as node features X = SPE(EVD(L)) and let the resulting prediction model be h(L) = GNN(L,SPE(EVD(L))). Then the domain generalization gap \u03b5t(h)\u2212 \u03b5t(s) satisfies\n\u03b5t(h)\u2212 \u03b5s(h) \u2264 2C(1 + \u03b12 d\u03b3 + \u03b13)W (PS ,PT ) + 2Cd 5/4(\u03b11 + \u03b12) \u221a W (PS ,PT ), (5)\nwhere W (PS ,PT ) is the 1-Wasserstein distance1."
        },
        {
            "heading": "3.4 SPE IS A UNIVERSAL BASIS INVARIANT ARCHITECTURE",
            "text": "SPE is a basis invariant architecture, but is it universally powerful? The next result shows that SPE is universal, meaning that any continuous basis invariant function can be expressed in the form of SPE (Eq. 2). To state the result, recall that SPE(V ,\u03bb) = \u03c1(V diag(\u03d5(\u03bb))V \u22a4), where for brevity, we express the multiple \u03d5\u2113 channels by \u03d5 = (\u03d51, . . . , \u03d5m). Proposition 3.2 (Basis Universality). SPE can universally approximate any continuous basis invariant function. That is, for any continuous f for which f(V ) = f(V Q) for any eigenvalue \u03bb and any Q \u2208 O(\u03bb), there exist continuous \u03c1 and \u03d5 such that f(V ) = \u03c1(V diag(\u03d5(\u03bb))V \u22a4).\nOnly one prior architecture, BasisNet (Lim et al., 2023), is known to have this property. However, unlike SPE, BasisNet does not have the critical stability property. Section 5 shows that this has significant empirical implications, with SPE considerably outperforming BasisNet across all evaluations. Furthermore, unlike prior analyses, we show that SPE can provably make effective use of eigenvalues: it can distinguish two input matrices with different eigenvalues using 2-layer MLP models for \u03c1 and \u03d5. In contrast, the original form of BasisNet does not use eigenvalues, though it is easy to incorporate them. Proposition 3.3. Suppose that (V ,\u03bb) and (V \u2032,\u03bb\u2032) are such that V Q = V \u2032 for some orthogonal matrix Q \u2208 O(d) and \u03bb \u0338= \u03bb\u2032. Then there exist 2-layer MLPs for each \u03d5\u2113 and a 2-layer MLP \u03c1, each with ReLU activations, such that SPE(V ,\u03bb) \u0338= SPE(V \u2032,\u03bb\u2032).\nFinally, as a concrete example of the expressivity of SPE for graph representation learning, we show that SPE is able to count graph substructures under stability guarantee. Proposition 3.4 (SPE can count cycles). Assume Assumption 3.1 hold and let \u03c1 be 2-IGNs (Maron et al., 2019b). Then SPE can determine the number of 3, 4, 5 cycles of a graph."
        },
        {
            "heading": "4 RELATED WORKS",
            "text": "Expressive GNNs. Since message-passing graph neural networks have been shown to be at most as powerful as the Weisfeiler-Leman test (Xu et al., 2019; Morris et al., 2019), there are many attempts to improve the expressivity of GNNs. We can classify them into three types: (1) high-order GNNs (Morris et al., 2020; Maron et al., 2019a;b); (2) subgraph GNNs (You et al., 2021; Zhang & Li, 2021; Zhao et al., 2022; Bevilacqua et al., 2022); (3) node feature augmentation (Li et al., 2020; Bouritsas et al., 2022; Barcelo\u0301 et al., 2021). In some senses, positional encoding can also be seen as an approach of node feature augmentation, which will be discussed below.\nPositional Encoding for GNNs. Positional encodings aim to provide additional global positional information for nodes in graphs to make them more distinguishable and add global structural information. It thus serves as a node feature augmentation to boost the expressive power of general graph neural networks (message-passing GNNs, spectral GNNs or graph transformers). Existing positional encoding methods can be categorized into: (1) Laplacian-eigenvector-based (Dwivedi & Bresson, 2021; Kreuzer et al., 2021; Maskey et al., 2022; Dwivedi et al., 2022; Wang et al., 2022b; Lim et al., 2023; Kim et al., 2022); (2) graph-distance-based (Ying et al., 2021; You et al., 2019; Li et al., 2020); and (3) random node features (Eliasof et al., 2023). A comprehensive discussion can be found in (Rampa\u0301s\u030cek et al., 2022a). Most of these methods do not consider basis invariance and stability. Notably, Wang et al. (2022a) also studies the stability of Laplacian encodings. However, their method ignores eigenvalues and thus implements a stricter symmetry that is invariant to rotations of the entire eigenspace. As a result, the \u201cover-stability\u201d restricts its expressive power. Bo et al. (2023) propose similar operations as V diag(\u03d5(\u03bb))V \u22a4. However they focus on a specific architecture design (\u03d5 is transformer) for spectral convolution instead of positional encodings, and do not provide any stability analysis.\nStability and Generalization of GNNs. The stability of neural networks is desirable as it implies better generalization (Sokolic\u0301 et al., 2017; Neyshabur et al., 2017; 2018; Bartlett et al., 2017) and\n1For graphs, W (ps, pt) := inf\u03c0\u2208\u03a0(PS ,PT ) \u222b minP\u2208\u03a0(n) \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F \u03c0(L,L\u2032)dLdL\u2032. Here\n\u03a0(PS ,PT ) is the set of product distributions whose marginal distribution is PS and PT respectively.\ntransferability under domain shifts (Courty et al., 2017; Shen et al., 2018). In the context of GNNs, many works theoretically study the stability of various GNN models (Gama et al., 2020; Kenlay et al., 2020; 2021; Yehudai et al., 2020; Arghal et al., 2022; Xu et al., 2021; Chuang & Jegelka, 2022). Finally, some works try to characterize the generalization error of GNNs using VC dimension (Morris et al., 2023) or Rademacher complexity (Garg et al., 2020)."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we use numerical experiments to verify our theory and the empirical effectiveness of our SPE. Section 5.1 tests SPE\u2019s strength as a graph positional encoder, and Section 5.2 tests the robustness of SPE to domain shifts, a key promise of stability. Section 5.3 further explores the empirical implications of stability in positional encodings. Our key finding is that there is a tradeoff between generalization and expressive power, with less stable positional encodings fitting the training data better than their stable counterparts, but leading to worse test performance. Finally, Section 5.4 tests SPE on challenging graph substructure counting tasks that message passing graph neural networks cannot solve, and SPE significantly outperforms prior positional encoding methods.\nDatasets. We primarily use three datasets: ZINC (Dwivedi et al., 2023), Alchemy (Chen et al., 2019) and DrugOOD (Ji et al., 2023). ZINC and Alchemy are graph regression tasks for molecular property prediction. DrugOOD is an OOD benchmark for AI drug discovery, for which we choose ligand-based affinity prediction as our classfication task (to determine if a drug is active). It considers three types of domains where distribution shifts arise: (1) Assay: which assay the data point belongs to; (2) Scaffold: the core structure of molecules; and (3) Size: molecule size. For each domain, the full dataset is divided into five partitions: the training set, the in-distribution (ID) validation/test sets, the out-of-distribution validation/test sets. These OOD partitions are expected to be distributed on the domains differently from ID partitions.\nImplementation. We implement SPE by: \u03d5l either being a DeepSet (Zaheer et al., 2017), elementwise MLPs or piece-wise cubic splines (see Appendix B.1 for detailed definition); and \u03c1 being GIN (Xu et al., 2019). Note that the input of \u03c1 is n \u00d7 n \u00d7m tensors, hence we first split it into n many n\u00d7m tensors, and then independently give each n\u00d7m tensors as node features to an identical GIN. Finally, we sum over the first n axes to output a permutation equivariant n\u00d7 p tensor. Baselines. We compare SPE to other positional encoding methods including (1) No positional encodings, (2) SignNet and BasisNet (Lim et al., 2023), and (3) PEG (Wang et al., 2022a). In all cases we adopt GIN as the base GNN model. For a fair comparison, all models will have comparable budgets on the number of parameters. We also conducted an ablation study to test the effectiveness of our key component \u03d5\u2113, whose results are included in Appendix B."
        },
        {
            "heading": "5.1 SMALL MOLECULE PROPERTY PREDICTION",
            "text": "We use SPE to learn graph positional encodings on ZINC and Alchemy. We let \u03d5l be Deepsets using only the top 8 eigenvectors (PE-8), and be element-wise MLPs when using all eigenvectors (PE-full). As before, we take \u03c1 to be a GIN.\nResults. The test mean absolute errorx (MAEs) are shown in Table 4. On ZINC, SPE performs much better than other baselines, both when using just 8 eigenvectors (0.0736) and all eigenvectors (0.0693) . On Alchemy, we always use all eigenvectors since the graph size only ranges from 8 to 12. For Alchemy we observe no significant improvement of any PE methods over base model w/o positional encodings. But SPE still achieves the least MAE among all these models."
        },
        {
            "heading": "5.2 OUT-OF-DISTRIBUTION GENERALIZATION: BINDING AFFINITY PREDICTION",
            "text": "We study the relation between stability and out-of-distribution generalization using the DrugOOD dataset (Ji et al., 2023). We take \u03d5l to be element-wise MLPs and \u03c1 be GIN as usual.\nResults. The results are shown in Table 2. All models have comparable Area Under ROC (AUC) on the ID-Test set. However, there is a big difference in OOD-Test performance on Scaffold and Size domains, with the unstable methods (SignNet and BasisNet) performing much worse than stable methods (No PE, PEG, SPE). This emphasizes the importance of stability in domain generalization. Note that this phenomenon is less obvious in the Assay domain, which is because the Assay domain represents concept (labels) shift instead of covariant (graph features) shift."
        },
        {
            "heading": "5.3 TRADE-OFFS BETWEEN STABILITY, GENERALIZATION AND EXPRESSIVITY",
            "text": "We hypothesize that stability has different effects on expressive power and generalization. Intuitively, very high stability means that outputs change very little as inputs change. Consequently, we expect highly stable models to have lower expressive power, but to generalize more reliably to new data. To test this behavior in practice we evaluate SPE on ZINC using 8 eigenvectors. We control the stability by tuning the complexity of underlying neural networks in the following two ways:\n1. Directly control the Lipschitz constant of each MLP in SPE (in both \u03d5\u2113 and \u03c1) by normalizing weight matrices.\n2. Let \u03d5\u2113 be a piecewise cubic spline. Increase the number of spline pieces from 1 to 6, with fewer splines corresponding to higher stability.\nSee Appendix B for full details. In both cases we use eight \u03d5\u2113 functions. We compute the summary statistics over different random seeds. As a measure of expressivity, we report the average training loss over the last 10 epochs on ZINC. As a measure of stability, we report the generalization gap (the difference between the test loss and the training loss) at the best validation epoch over ZINC.\nResults. In Figure 2, we show the trend of training error, test error and generalization gap as Lipschiz constant of individual MLPs (first row) or the number of spline pieces (second row) changes. We can see that as model complexity increases (stability decreases), the training error gets reduced (more expressive power) while the generalization gap grows. This justifies the important practical role of model stability for the trade-off between expressive power and generalization.\n5.4 COUNTING GRAPH SUBSTRUCTURES\nTo empirically study the expressive power of SPE, we follow prior works that generate random graphs (Zhao et al., 2022; Huang et al., 2023). The dataset contains Erdo\u030bs-Renyi random graphs and other random regular graphs (see Appendix M.2.1 in Chen et al. (2020)) and is randomly split into train/valid/test splitting with ratio 3:2:5. and label nodes according to the number of substructures they are part of. We aggregate the node labels to obtain the number of substructures in the overall graph and view this as a graph regression task. We let \u03d5l be element-wise MLPs and \u03c1 be GIN.\nResults. Figure 3 shows that SPE significantly outperforms SignNet in counting 3,4,5 and 6-cycles. We emphasize that linear differences in log-MAE correspond to exponentially large differences in MAE. This result shows that SPE still achieves very high expressive power, whilst enjoying improved robustness to domain-shifts thanks to its stability (see Section 5.2)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We present SPE, a learnable Laplacian positional encoding that is both provably stable and expressive. Extensive experiments show the effectiveness of SPE on molecular property prediction benchmarks, the high expressivity in learning graph substructures, and the robustness as well as generalization ability under domain shifts. In the future, this technique can be extended to link prediction or other tasks involving large graphs where stability is also crucial and desired. Finally, our analysis provides a general technique for graph eigenspace stability, not just limited to domains of positional encodings and graph learning."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank Derek Lim for a constructive discussion. Yinan Wang and Pan Li are partially supported by the NSF awards PHY-2117997, IIS-2239565."
        },
        {
            "heading": "A DEFERRED PROOFS",
            "text": "Basic conventions. Let [n] = Z \u2229 [1, n] and Ja, bK = Z \u2229 [a, b] denote integer intervals. Let \u03a0(n) be the symmetric group on n elements. Unless otherwise stated, eigenvalues are counted with multiplicity; for example, the first and second smallest eigenvalues of I \u2208 R2\u00d72 are both 1. Let \u2225\u00b7\u2225 and \u2225\u00b7\u2225F be L2-norm and Frobenius norm of matrices.\nMatrix indexing. For any matrix A \u2208 Rn\u00d7d: \u2022 Let [A]i,j \u2208 R be the entry at row i and column j\n\u2022 Let [A]\u27e8i\u27e9 \u2208 Rd be row i represented as a column vector\n\u2022 Let [A]j \u2208 Rn be column j\n\u2022 For any set J = {j1, \u00b7 \u00b7 \u00b7 , jd\u2032} \u2286 [d], let [A]J \u2208 Rn\u00d7d \u2032\nbe columns j1, \u00b7 \u00b7 \u00b7 , jd\u2032 arranged in a matrix\n\u2022 If n = d, let diag(A) \u2208 Rn be the diagonal represented as a column vector Special classes of matrices. Define the following sets:\n\u2022 All n\u00d7 n diagonal matrices: D(n) = { D \u2208 Rn\u00d7n : \u2200i \u0338= j, [D]i,j = 0 } \u2022 The orthogonal group in dimension n, i.e. all n \u00d7 n orthogonal matrices: O(n) ={\nQ \u2208 Rn\u00d7n : Q\u22121 = Q\u22a4 }\n\u2022 All n\u00d7 n permutation matrices: \u03a0(n) = { P \u2208 {0, 1}n\u00d7n : P\u22121 = P\u22a4 } \u2022 All n\u00d7 n symmetric matrices: S(n) = { A \u2208 Rn\u00d7n : A = A\u22a4\n} Spectral graph theory. Many properties of an undirected graph are encoded in its (normalized) Laplacian matrix, which is always symmetric and positive semidefinite. In this paper, we only consider connected graphs. The Laplacian matrix of a connected graph always has a zero eigenvalue with multiplicity 1 corresponding to an eigenvector of all ones, and all other eigenvalues positive. The Laplacian eigenmap technique uses the eigenvectors corresponding to the d smallest positive eigenvalues as vertex positional encodings. We assume the dth and (d+1)th smallest positive eigenvalues of the Laplacian matrices under consideration are distinct. This motivates definitions for the following sets of matrices:\n\u2022 All n\u00d7 n Laplacian matrices satisfying the properties below:\nLd(n) ={L \u2208 S(n) : L \u2ab0 0 \u2227 rank(L) = n\u2212 1 \u2227 L1 = 0 \u2227 d-th and (d+ 1)-th smallest positive eigenvalues are distinct} (6)\n\u2022 All n\u00d7 n diagonal matrices with positive diagonal entries: D+(n) = {D \u2208 D(n) : D \u227b 0} \u2022 All n\u00d7d matrices with orthonormal columns: O(n, d) = { Q \u2208 Rn\u00d7d : [Q]i \u00b7 [Q]j = 1[i = j] } Eigenvalues and eigenvectors. For the first two functions below, assume the given matrix has at least d positive eigenvalues.\n\u2022 Let \u039bd : \u22c3\nn\u2265d S(n) \u2192 D+(d) return a diagonal matrix containing the d smallest positive eigenvalues of the given matrix, sorted in increasing order.\n\u2022 Let Xd : \u22c3\nn\u2265d S(n)\u2192 O(n, d) return a matrix whose columns contain an unspecified set of orthonormal eigenvectors corresponding to the d smallest positive eigenvalues (sorted in increasing order) of the given matrix.\n\u2022 Let X\u27e8d\u27e9 : \u22c3\nn\u2265d S(n) \u2192 O(n, d) return a matrix whose columns contain an unspecified set of orthonormal eigenvectors corresponding to the d smallest eigenvalues (sorted in increasing order) of the given matrix.\n\u2022 Let \u039bd : \u22c3\nn\u2265d S(n) \u2192 D(d) return a diagonal matrix containing the d greatest eigenvalues of the given matrix, sorted in increasing order.\n\u2022 Let Xd : \u22c3\nn\u2265d S(n) \u2192 O(n, d) return a matrix whose columns contain an unspecified set of orthonormal eigenvectors corresponding to the d greatest eigenvalues (sorted in increasing order) of the given matrix.\nBatch submatrix multiplication. Let A \u2208 Rn\u00d7d be a matrix and let {Jk}pk=1 be a partition of [d]. For each k \u2208 [p], let dk = |Jk| and let Bk \u2208 Rdk\u00d7dk be a matrix. For notational convenience, let B = {(Bk,Jk)}pk=1. Define a binary star operator such that A \u22c6 B \u2208 Rn\u00d7d is the matrix where \u2200k \u2208 [p], [A \u22c6 B]Jk = [A]Jk Bk . (7) We primarily use batch submatrix multiplication in the context of orthogonal invariance, where an orthogonal matrix is applied to each eigenspace. For any (eigenvalue) matrix \u039b \u2208 D(d), define\nO(\u039b) = { {(Qk,Jk)}pk=1 : {Qk} p k=1 \u2208 p\u220f k=1 O(dk) } , (8)\nwhere Jk = { j \u2208 [d] : [diag(\u039b)]j = \u03c3k } for each k \u2208 [p] and {\u03c3k}pk=1 are the distinct values in\ndiag(\u039b). In this context, O(\u039b) is the domain of the right operand of \u22c6."
        },
        {
            "heading": "A.1 PROOF OF THEOREM 3.1",
            "text": "Theorem 3.1 (Stability of SPE). Under Assumption 3.1, SPE is stable with respect to the input Laplacian: for Laplacians L,L\u2032,\n\u2225SPE(EVD(L))\u2212 P\u2217SPE(EVD(L\u2032))\u2225F \u2264(\u03b11 + \u03b12)d 5/4 \u221a \u2225L\u2212 P\u2217LP\u22a4\u2217 \u2225F\n+ ( \u03b12 d \u03b3 + \u03b13 )\u2225\u2225L\u2212 P\u2217LP\u22a4\u2217 \u2225\u2225F , (3) where the constants are \u03b11 = 2J \u2211m l=1 K\u2113, \u03b12 = 4 \u221a 2J \u2211m l=1 M\u2113 and \u03b13 = J \u2211m\nl=1 K\u2113. Here"
        },
        {
            "heading": "M\u2113 = sup\u03bb\u2208[0,2]d \u2225\u03d5\u2113(\u03bb)\u2225 and again P\u2217 = argminP\u2208\u03a0(n)",
            "text": "\u2225\u2225L\u2212 P\u2217LP\u22a4\u2217 \u2225\u2225F. The eigengap \u03b3 = \u03bbd+1 \u2212 \u03bbd is the difference between the (d + 1)-th and d-th smallest eigenvalues, and \u03b3 = +\u221e if d = n.\nProof. Fix Laplacians L,L\u2032 \u2208 Ld(n). We will show that for any permutation matrix P \u2208 \u03a0(n),\n\u2225SPE(EVD(L))\u2212 P SPE(EVD(L\u2032))\u2225F \u2264 (\u03b11 + \u03b12) d 5 4 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 12 F\n+ ( \u03b12 d\n\u03b3 + \u03b13\n)\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F .\n(9)\nFix P \u2208 \u03a0(n). For notational convenience, we denote diag{\u03c1(\u03bb)} by \u03c1(\u039b) with \u039b = diag{\u03bb}, and let X = Xd(L), \u039b = \u039bd(L), X \u2032 = Xd(L\u2032), and \u039b\u2032 = \u039bd(L\u2032). Then\n\u2225SPE(EVD(L))\u2212 P SPE(EVD(L\u2032))\u2225F (10) (a) = \u2225\u2225\u03c1(X\u03d51(\u039b)X\u22a4, \u00b7 \u00b7 \u00b7 ,X\u03d5m(\u039b)X\u22a4)\u2212 P\u03c1(X \u2032\u03d51(\u039b\u2032)X \u2032\u22a4, \u00b7 \u00b7 \u00b7 ,X \u2032\u03d5m(\u039b\u2032)X \u2032\u22a4)\u2225\u2225F (11)\n(b) = \u2225\u2225\u03c1(X\u03d51(\u039b)X\u22a4, \u00b7 \u00b7 \u00b7 ,X\u03d5m(\u039b)X\u22a4)\u2212 \u03c1(PX \u2032\u03d51(\u039b\u2032)X \u2032\u22a4P\u22a4, \u00b7 \u00b7 \u00b7 ,PX \u2032\u03d5m(\u039b\u2032)X \u2032\u22a4P\u22a4)\u2225\u2225F (12) (c)\n\u2264 J m\u2211 \u2113=1 \u2225\u2225X\u03d5\u2113(\u039b)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b\u2032)X \u2032\u22a4P\u22a4\u2225\u2225F (13) (d)\n\u2264 J m\u2211 \u2113=1 \u2225\u2225X\u03d5\u2113(\u039b)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b)X \u2032\u22a4P\u22a4\u2225\u2225F + \u2225\u2225PX \u2032\u03d5\u2113(\u039b)X \u2032\u22a4P\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b\u2032)X \u2032\u22a4P\u22a4\u2225\u2225F\n(14)\n(e) = J m\u2211 \u2113=1 {\u2225\u2225X\u03d5\u2113(\u039b)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b)X \u2032\u22a4P\u22a4\u2225\u2225F\ufe38 \ufe37\ufe37 \ufe38 1 + \u2225\u2225X \u2032\u03d5\u2113(\u039b)X \u2032\u22a4 \u2212X \u2032\u03d5\u2113(\u039b\u2032)X \u2032\u22a4\u2225\u2225F\ufe38 \ufe37\ufe37 \ufe38 2 } ,\n(15)\nwhere (a) holds by definition of SPE, (b) holds by permutation equivariance of \u03c1, (c) holds by Lipschitz continuity of \u03c1, (d) holds by the triangle inequality, and (e) holds by permutation invariance of Frobenius norm.\nNext, we upper-bound 1 . Let \u03b4 = min { \u03b3, d\u2212 1 4 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 12 F } . The \u03b4 = 0 case is trivial, because\n\u03b4 = 0 \u21d0\u21d2 L = PL\u2032P\u22a4 =\u21d2 SPE(EVD(L)) = SPE(EVD(PL\u2032P\u22a4)) (a)\u21d0\u21d2 SPE(EVD(L)) = P SPE(EVD(L\u2032)) ,\n(16)\nwhere (a) holds due to permutation equivariance of SPE. Thus, assume \u03b4 > 0 for the remainder of this proof. Let\n{jk}p+1k=1 = {1} \u222a {j \u2208 [d+ 1] : \u03bbj \u2212 \u03bbj\u22121 \u2265 \u03b4} , 1 = j1 < \u00b7 \u00b7 \u00b7 < jp+1 (a) = d+ 1 (17)\nbe the keypoint indices at which eigengaps are greater than or equal to \u03b4, where (a) holds because \u03bbd+1 \u2212 \u03bbd = \u03b3 \u2265 min { \u03b3, d\u2212 1 4 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 12 F } = \u03b4 . (18)\nFor each k \u2208 [p], let Jk = {j \u2208 [d] : jk \u2264 j < jk+1} be a chunk of contiguous indices at which eigengaps are less than \u03b4, and let dk = |Jk| be the size of the chunk. Define a matrix \u039b\u0303 \u2208 D(d) as\n\u2200k \u2208 [p], \u2200j \u2208 Jk, [ diag(\u039b\u0303) ] j = \u03bbjk . (19)\nIt follows that\n1 (a) \u2264 \u2225\u2225\u2225X\u03d5\u2113(\u039b)X\u22a4 \u2212X\u03d5\u2113(\u039b\u0303)X\u22a4\u2225\u2225\u2225 F + \u2225\u2225\u2225X\u03d5\u2113(\u039b\u0303)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b\u0303)X \u2032\u22a4P\u22a4\u2225\u2225\u2225 F\n+ \u2225\u2225\u2225PX \u2032\u03d5\u2113(\u039b\u0303)X \u2032\u22a4P\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b)X \u2032\u22a4P\u22a4\u2225\u2225\u2225\nF\n(20)\n(b) = \u2225\u2225\u2225X\u03d5\u2113(\u039b)X\u22a4 \u2212X\u03d5\u2113(\u039b\u0303)X\u22a4\u2225\u2225\u2225 F + \u2225\u2225\u2225X\u03d5\u2113(\u039b\u0303)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b\u0303)X \u2032\u22a4P\u22a4\u2225\u2225\u2225 F\n+ \u2225\u2225\u2225X \u2032\u03d5\u2113(\u039b\u0303)X \u2032\u22a4 \u2212X \u2032\u03d5\u2113(\u039b)X \u2032\u22a4\u2225\u2225\u2225\nF\n(21)\n(c) \u2264 \u2225X\u22252 \u2225\u2225\u2225\u03d5\u2113(\u039b)\u2212 \u03d5\u2113(\u039b\u0303)\u2225\u2225\u2225 F + \u2225\u2225\u2225X\u03d5\u2113(\u039b\u0303)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b\u0303)X \u2032\u22a4P\u22a4\u2225\u2225\u2225 F\n+ \u2225X \u2032\u22252 \u2225\u2225\u2225\u03d5\u2113(\u039b\u0303)\u2212 \u03d5\u2113(\u039b)\u2225\u2225\u2225\nF\n(22)\n(d) = 2 \u2225\u2225\u2225\u03d5\u2113(\u039b)\u2212 \u03d5\u2113(\u039b\u0303)\u2225\u2225\u2225 F + \u2225\u2225\u2225X\u03d5\u2113(\u039b\u0303)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b\u0303)X \u2032\u22a4P\u22a4\u2225\u2225\u2225 F (23)\n(e) \u2264 2K\u2113 \u2225\u2225\u2225\u039b\u2212 \u039b\u0303\u2225\u2225\u2225 F + \u2225\u2225\u2225X\u03d5\u2113(\u039b\u0303)X\u22a4 \u2212 PX \u2032\u03d5\u2113(\u039b\u0303)X \u2032\u22a4P\u22a4\u2225\u2225\u2225 F (24)\n(f) = 2K\u2113 \u2225\u2225\u2225\u039b\u2212 \u039b\u0303\u2225\u2225\u2225 F\n+ \u2225\u2225\u2225\u2225\u2225 p\u2211\nk=1\n[X]Jk\n[ \u03d5\u2113(\u039b\u0303) ] Jk,Jk [X] \u22a4 Jk \u2212 P\n( p\u2211\nk=1\n[X \u2032]Jk [ \u03d5\u2113(\u039b\u0303) ] Jk,Jk [X \u2032] \u22a4 Jk ) P\u22a4 \u2225\u2225\u2225\u2225\u2225 F\n(25)\n(g) \u2264 2K\u2113 \u2225\u2225\u2225\u039b\u2212 \u039b\u0303\u2225\u2225\u2225\nF\ufe38 \ufe37\ufe37 \ufe38 3 +\np\u2211 k=1 \u2225\u2225\u2225\u2225[X]Jk [\u03d5\u2113(\u039b\u0303)]Jk,Jk [X]\u22a4Jk \u2212 P [X \u2032]Jk [ \u03d5\u2113(\u039b\u0303) ] Jk,Jk [X \u2032] \u22a4 Jk P \u22a4 \u2225\u2225\u2225\u2225 F\ufe38 \ufe37\ufe37 \ufe38\n4\n,\n(26)\nwhere (a) holds by the triangle inequality, (b) holds by permutation invariance of Frobenius norm, (c) holds by lemma A.1, (d) holds because X and X \u2032 have orthonormal columns, (e) holds by Lipschitz\ncontinuity of \u03d5\u2113, (f) holds by block matrix algebra, and (g) holds by the triangle inequality. Next, we upper-bound 3 :\n3 (a) = \u221a\u221a\u221a\u221a p\u2211 k=1 \u2211 j\u2208Jk (\u03bbj \u2212 \u03bbjk)2 (27)\n(b) = \u221a\u221a\u221a\u221a\u221a p\u2211 k=1 \u2211 j\u2208Jk  j\u2211 j\u2032=jk+1 (\u03bbj\u2032 \u2212 \u03bbj\u2032\u22121) 2 (28) (c) \u2264\n\u221a\u221a\u221a\u221a\u221a p\u2211 k=1 \u2211 j\u2208Jk  j\u2211 j\u2032=jk+1 \u03b4 2 (29) = \u03b4\n\u221a\u221a\u221a\u221a p\u2211 k=1 \u2211 j\u2208Jk (j \u2212 jk)2 (30)\n(d) = \u03b4 \u221a\u221a\u221a\u221a p\u2211 k=1 dk\u22121\u2211 j=1 j2 (31)\n\u2264 \u03b4 \u221a\u221a\u221a\u221a p\u2211 k=1 d3k , (32)\nwhere (a) holds by definition of \u039b\u0303, (b) holds because the innermost sum in (b) telescopes, (c) holds becauseJk is a chunk of contiguous indices at which eigengaps are less than \u03b4, and (d) holds because Jk is a contiguous integer interval.\nNext, we upper-bound 4 . By definition of \u039b\u0303, the entries [ diag(\u039b\u0303) ] j are equal for all j \u2208 Jk.\nBy permutation equivariance of \u03d5\u2113, the entries [ diag(\u03d5\u2113(\u039b\u0303)) ] j\nare equal for all j \u2208 Jk. Thus,[ \u03d5\u2113(\u039b\u0303) ] Jk,Jk = \u00b5\u2113,kI for some \u00b5\u2113,k \u2208 R. As \u03d5\u2113 is Lipschitz continuous and defined on a bounded domain [0, 2]d, it must be bounded by constant M\u2113 = sup\u03bb\u2208[0,2]d \u03d5\u2113(\u03bb). Then by boundedness of \u03d5\u2113,\n|\u00b5\u2113,k| = 1\u221a dk \u2225\u00b5\u2113,kI\u2225F = 1\u221a dk \u2225\u2225\u2225\u2225[\u03d5\u2113(\u039b\u0303)]Jk,Jk \u2225\u2225\u2225\u2225 F \u2264 1\u221a dk \u2225\u2225\u2225\u03d5\u2113(\u039b\u0303)\u2225\u2225\u2225 F \u2264 M\u2113\u221a dk . (33)\nTherefore,\n4 = \u2225\u2225\u2225[X]Jk (\u00b5\u2113,kI) [X]\u22a4Jk \u2212 P [X \u2032]Jk (\u00b5\u2113,kI) [X \u2032]\u22a4Jk P\u22a4\u2225\u2225\u2225F (34)\n= |\u00b5\u2113,k| \u2225\u2225\u2225[X]Jk [X]\u22a4Jk \u2212 P [X \u2032]Jk [X \u2032]\u22a4Jk P\u22a4\u2225\u2225\u2225F (35)\n\u2264 M\u2113\u221a dk \u2225\u2225\u2225[X]Jk [X]\u22a4Jk \u2212 P [X \u2032]Jk [X \u2032]\u22a4Jk P\u22a4\u2225\u2225\u2225F . (36) Now, we consider two cases. Case 1: k \u2265 2 or \u03bb1 \u2212 \u03bb0 \u2265 \u03b4. Define the matrices\nZk = [X]Jk and Z \u2032 k = [X \u2032]Jk . (37)\nThere exists an orthogonal matrix Qk \u2208 O(dk) such that \u2225Zk \u2212 PZ \u2032kQk\u2225F (a) = \u2225\u2225\u2225[Xd(L)]Jk \u2212 [Xd(PL\u2032P\u22a4)]Jk Qk\u2225\u2225\u2225F (38)\n(b) \u2264 \u221a 8 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\nmin { \u03bbjk \u2212 \u03bbjk\u22121, \u03bbjk+1 \u2212 \u03bbjk+1\u22121 } (39)\n(c) \u2264 \u221a 8 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n\u03b4 , (40)\nwhere (a) holds by lemmas A.2 and A.3, (b) holds by proposition A.1,2 and (c) holds because jk and jk+1 are keypoint indices at which eigengaps are greater than or equal to \u03b4.\nCase 2: k = 1 and \u03bb1 \u2212 \u03bb0 < \u03b4. Define the matrices Z1 = [ 1\u221a n 1 [X]J1 ] and Z \u20321 = [ 1\u221a n 1 [X \u2032]J1 ] . (41)\nThere exists an orthogonal matrix Q1 \u2208 O(d1 + 1) such that \u2225Z1 \u2212 PZ \u20321Q1\u2225F (a) = \u2225\u2225X\u27e8d1+1\u27e9(L)\u2212 X\u27e8d1+1\u27e9(PL\u2032P\u22a4)Q1\u2225\u2225F (42)\n(b) \u2264 \u221a 8 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n\u03bbj2 \u2212 \u03bbj2\u22121 (43)\n(c) \u2264 \u221a 8 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n\u03b4 , (44)\nwhere (a) holds by lemmas A.2 and A.3, (b) holds by proposition A.1,3 and (c) holds because j2 is a keypoint index at which the eigengap is greater than or equal to \u03b4.\nHence, in both cases,\u2225\u2225\u2225[X]Jk [X]\u22a4Jk \u2212 P [X \u2032]Jk [X \u2032]\u22a4Jk P\u22a4\u2225\u2225\u2225F (a)= \u2225\u2225ZkZ\u22a4k \u2212 PZ \u2032kZ \u2032\u22a4k P\u22a4\u2225\u2225F (45) (b) = \u2225\u2225ZkZ\u22a4k \u2212 PZ \u2032kQkQ\u22a4k Z \u2032\u22a4k P\u22a4\u2225\u2225F (46)\n(c) \u2264 \u2225\u2225ZkZ\u22a4k \u2212ZkQ\u22a4k Z \u2032\u22a4k P\u22a4\u2225\u2225F + \u2225\u2225ZkQ\u22a4k Z \u2032\u22a4k P\u22a4 \u2212 PZ \u2032kQkQ\u22a4k Z \u2032\u22a4k P\u22a4\u2225\u2225F (47)\n(d) \u2264 \u2225Zk\u2225 \u2225\u2225Z\u22a4k \u2212Q\u22a4k Z \u2032\u22a4k P\u22a4\u2225\u2225F + \u2225Zk \u2212 PZ \u2032kQk\u2225F \u2225Qk\u2225 \u2225Z \u2032k\u2225 \u2225P \u2225 (48)\n(e) = \u2225\u2225Z\u22a4k \u2212Q\u22a4k Z \u2032\u22a4k P\u22a4\u2225\u2225F + \u2225Zk \u2212 PZ \u2032kQk\u2225F (49)\n(f) = 2 \u2225Zk \u2212 PZ \u2032kQk\u2225F (50) (g) \u2264 \u221a 32 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n\u03b4 , (51)\nwhere (a) holds in Case 2 because\n[X]Jk [X] \u22a4 Jk \u2212 P [X \u2032]Jk [X \u2032] \u22a4 Jk P \u22a4 (52)\n= 1\nn 11\u22a4 + [X]Jk [X] \u22a4 Jk \u2212\n1 n 11\u22a4 \u2212 P [X \u2032]Jk [X \u2032] \u22a4 Jk P \u22a4 (53)\n= 1\nn 11\u22a4 + [X]Jk [X] \u22a4 Jk \u2212 P\n( 1\nn 11\u22a4 + [X \u2032]Jk [X \u2032] \u22a4 Jk\n) P\u22a4 (54)\n= [\n1\u221a n 1 [X]Jk\n] [ 1\u221a n 1\u22a4\n[X] \u22a4 Jk\n] \u2212 P [ 1\u221a n 1 [X\u2032]Jk ] [ 1\u221a n 1\u22a4\n[X\u2032] \u22a4 Jk\n] P\u22a4 (55)\n= ZkZ \u22a4 k \u2212 PZ \u2032kZ \u2032\u22a4k P\u22a4 , (56)\n(b) holds because QkQ\u22a4k = I , (c) holds by the triangle inequality, (d) holds by lemma A.1, (e) holds because Zk and Z \u2032k have orthonormal columns and Qk and P are orthogonal, (f) holds because Frobenius norm is invariant to matrix transpose, and (g) holds by substituting in eqs. (40) and (44). Combining these results,\n4 \u2264 \u221a 32M\u2113 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\u221a\ndk \u03b4 . (57)\n2We can apply proposition A.1 because Xd extracts the same contiguous interval of eigenvalue indices for any matrix in Ld(n), and PL\u2032P\u22a4 \u2208 Ld(n) by lemmas A.2 and A.3.\n3We can apply proposition A.1 because X\u27e8d1+1\u27e9 extracts the same contiguous interval of eigenvalue indices for any matrix.\nNext, we upper-bound 2 :\n2 (a)\n\u2264 \u2225X \u2032\u22252 \u2225\u03d5\u2113(\u039b)\u2212 \u03d5\u2113(\u039b\u2032)\u2225F (58) (b) = \u2225\u03d5\u2113(\u039b)\u2212 \u03d5\u2113(\u039b\u2032)\u2225F (59) (c) \u2264 K\u2113 \u2225\u039b\u2212\u039b\u2032\u2225F (60)\n(d) = K\u2113 \u221a\u221a\u221a\u221a d\u2211 j=1 ( \u03bbj \u2212 \u03bb\u2032j )2 (61)\n(e) \u2264 K\u2113 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225\nF , (62)\nwhere (a) holds by lemma A.1, (b) holds because X \u2032 has orthonormal columns, (c) holds by Lipschitz continuity of \u03d5\u2113, the notation \u03bb\u2032j in (d) is the jth smallest positive eigenvalue of L\n\u2032, and (e) holds by proposition A.3 and lemma A.3.\nCombining our results above,\n\u2225SPEn,d(L)\u2212 P SPEn,d(L\u2032)\u2225F (63)\n(a) \u2264 J m\u2211 \u2113=1 2K\u2113\u03b4 \u221a\u221a\u221a\u221a p\u2211 k=1 d3k + p\u2211 k=1 4 \u221a 2M\u2113 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\u221a dk \u03b4 +K\u2113 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F  (64) (b)\n\u2264 J m\u2211 \u2113=1\n{ 2K\u2113d 3 2 \u03b4 + 4 \u221a 2M\u2113d \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n\u03b4 +K\u2113\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F } (65)\n(c) = \u03b11d 3 2 \u03b4 + \u03b12d\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n\u03b4 + \u03b13\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n(66)\n(d) \u2264 (\u03b11 + \u03b12) d 5 4 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 12 F + ( \u03b12 d\n\u03b3 + \u03b13\n)\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F\n(67)\nas desired, where (a) holds by substituting in 1 - 4 , (b) holds because \u2211p\nk=1 d 3 k \u2264 ( \u2211p k=1 dk) 3 =\nd3 and \u2211p\nk=1 1\u221a dk \u2264 p \u2264 d, (c) holds by the definition of \u03b11 through \u03b13, and (d) holds because\n\u03b4 \u2264 d\u2212 14 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 12\nF , (68)\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225\nF \u03b4 \u2264 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F \u03b3 + d 1 4 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 12 F . (69)"
        },
        {
            "heading": "A.2 PROOF OF PROPOSITION 3.1",
            "text": "Proposition 3.1. Assume Assumption 3.1 hold, and assume a base GNN model GNN(L,X) \u2208 R that is C-Lipschitz continuous, i.e.,\n|GNN(L,X)\u2212GNN(L\u2032,X \u2032)| \u2264 C min P\u2208\u03a0(n) (\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F + \u2225X \u2212 PX \u2032\u2225F ) , (4)\nfor any Laplacians L,L\u2032 and node features X,X \u2032. Now let GNN take positional encodings as node features X = SPE(EVD(L)) and let the resulting prediction model be h(L) = GNN(L,SPE(EVD(L))). Then the domain generalization gap \u03b5t(h)\u2212 \u03b5t(s) satisfies\n\u03b5t(h)\u2212 \u03b5s(h) \u2264 2C(1 + \u03b12 d\u03b3 + \u03b13)W (PS ,PT ) + 2Cd 5/4(\u03b11 + \u03b12) \u221a W (PS ,PT ), (5)\nwhere W (PS ,PT ) is the 1-Wasserstein distance4. 4For graphs, W (ps, pt) := inf\u03c0\u2208\u03a0(PS ,PT ) \u222b minP\u2208\u03a0(n) \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F \u03c0(L,L\u2032)dLdL\u2032. Here \u03a0(PS ,PT ) is the set of product distributions whose marginal distribution is PS and PT respectively.\nProof. The proof goes in two steps. The first step shows that a Lipschitz continuous base GNN with a Ho\u0308lder continuity SPE yields an overall Ho\u0308lder continuity predictive model. The second step shows that this Ho\u0308lder continuous predictive model has a bounded generalization gap under domain shift.\nStep 1: Suppose base GNN model GNN(L,X) is C-Lipschitz and SPE method SPE(L) satifies Theorem 3.1. Let our predictive model be h(L) = GNN(L,SPE(EVD(L))) \u2208 R. Then for any Laplacians L,L\u2032 \u2208 and any permutation P \u2208 \u03a0(n) we have |h(L)\u2212 h(L\u2032)| = |GNN(L,SPE(EVD(L)))\u2212GNN(L\u2032,SPE(EVD(L\u2032))| (70)\n(a) \u2264 C (\u2225L\u2212 PL\u2032P \u2225F + \u2225SPE(EVD(L))\u2212 PSPE(EVD(L \u2032))\u2225F) (71)\n(b) \u2264 C ( 1 + \u03b12 d\n\u03b3 + \u03b13\n)\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F + C (\u03b11 + \u03b12) d 5/4 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u22251/2 F ,\n(72) := C1 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225\nF + C2 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u22251/2 F\n, (73) where (a) holds by continuity assumption of base GNN, and (b) holds by the stability result Theorem 3.1.\nStep 2: Suppose the ground-truth function h\u2217 lies in our hypothesis space (thus also satisfies eq. (73)). The absolute risk on source and target domain are defined \u03b5s(h) = EL\u223cPS |h(L) \u2212 h\u2217(L)| and \u03b5t(h) = EL\u223cPT |h(L)\u2212h\u2217(L)| respectively. Note that function f(L) = |h(L)\u2212h\u2217(L)| is also Ho\u0308lder continuous but with two times larger Ho\u0308lder constant. This is because |h(L)\u2212 h\u2217(L)| \u2264 |h(L)\u2212 h(L\u2032)|+ |h(L\u2032)\u2212 h\u2217(L)| (triangle\u2019s inequality for arbitrary L\u2032)\n(74)\n\u2264 |h(L)\u2212 h(L\u2032)|+ |h(L\u2032)\u2212 h\u2217(L\u2032)|+ |h\u2217(L)\u2212 h\u2217(L\u2032)| (triangle\u2019s inequality), (75)\nand thus for arbitrary L,L\u2032, f(L)\u2212 f(L\u2032) = |h(L)\u2212 h\u2217(L)| \u2212 |h(L\u2032)\u2212 h\u2217(L\u2032)| \u2264 |h(L)\u2212 h(L\u2032)|+ |h\u2217(L)\u2212 h\u2217(L\u2032)|\n(76) \u2264 2C1 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225\nF + 2C2 \u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u22251/2 F . (77)\nWe can show the same bound for f(L\u2032) \u2212 f(L). Thus f is Ho\u0308lder continuous with constants 2C1, 2C2, and we denote such property by \u2225f\u2225H \u2264 (2C1, 2C2) for notation convenience.\nAn upper bound of generalization gap \u03b5t(h)\u2212 \u03b5s(h) can be obtained: \u03b5t(h)\u2212 \u03b5s(h) = EL\u223cPT |h(L)\u2212 h\u2217(L)| \u2212 EL\u223cPS |h(L)\u2212 h\u2217(L)| (78)\n(a) \u2264 sup \u2225f\u2225H\u2264(C1,C2) EL\u223cPT f(L)\u2212 EL\u223cPSf(L) (79)\n= sup \u2225f\u2225H\u2264(C1,C2)\n\u222b f(L)(PT (L)\u2212 PS(L))dL (80)\n(b) = inf\n\u03c0\u2208\u03a0(n)(PT ,PS) sup\n\u2225f\u2225H\u2264(C1,C2)\n\u222b (f(L)\u2212 f(L\u2032))\u03c0(L,L\u2032)dLdL\u2032 (81)\nwhere (a) holds because \u2225|h(L)\u2212 h\u2217(L)|\u2225H \u2264 (C1, C2), and (b) holds because of the definition of product distribution\n\u03a0(PT ,PS) = { \u03c0 : \u222b \u03c0(L,L\u2032)dL\u2032 = PT (L) \u2227 \u222b \u03c0(L,L\u2032)dL = PS(L) } .\nNotice the integral can be further upper bounded using Ho\u0308lder continuity of f :\nsup \u2225f\u2225H\u2264(C1,C2)\n\u222b (f(L)\u2212 f(L\u2032))\u03c0(L,L\u2032)dLdL\u2032\n\u2264 \u222b (\n2C1 min P\u2208\u03a0(n)\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F + 2C2 min\nP\u2208\u03a0(n)\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u22251/2 F ) \u03c0(L,L\u2032)dLdL\u2032. (82)\nLet us define the Wasserstein distance of PT and PS be\nW (PT ,PS) = inf \u03c0\u2208\u03a0(PT ,PS)\n\u222b min\nP\u2208\u03a0(n)\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F \u03c0(L,L\u2032)dLdL\u2032. (83)\nThen plugging eqs. (82, 83) into (81) yields the desired result\n\u03b5t(h)\u2212 \u03b5s(h) \u2264 2C1W (PT ,PS) + 2C2 inf \u03c0\u2208\u03a0(PT ,PS)\n\u222b min\nP\u2208\u03a0(n)\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u22251/2 F \u03c0(L,L\u2032)dLdL\u2032\n(84)\n(a) \u2264 2C1W (PT ,PS) + 2C2 inf \u03c0\u2208\u03a0(PT ,PS)\n(\u222b min\nP\u2208\u03a0(n)\n\u2225\u2225L\u2212 PL\u2032P\u22a4\u2225\u2225 F \u03c0(L,L\u2032)dLdL\u2032 )1/2 (85)\n= 2C1W (PT ,PS) + 2C2W 1/2(PT ,PS), (86)\nwhere (a) holds due to the concavity of sqrt root function."
        },
        {
            "heading": "A.3 PROOF OF PROPOSITION 3.2",
            "text": "Proposition 3.2 (Basis Universality). SPE can universally approximate any continuous basis invariant function. That is, for any continuous f for which f(V ) = f(V Q) for any eigenvalue \u03bb and any Q \u2208 O(\u03bb), there exist continuous \u03c1 and \u03d5 such that f(V ) = \u03c1(V diag(\u03d5(\u03bb))V \u22a4).\nProof. In the proof we are going to show basis universality by expressing BasisNet. Fix eigenvalues \u03bb \u2208 Rd. Let \u03bb\u0303 \u2208 RL be a sorting of eigenvalues without repetition, i.e., \u03bb\u0303i = i-th smallest eigenvalues. Assume m \u2265 d. For eigenvectors V , let I\u2113 \u2282 [d] be indices of \u2113-th eigensubspaces. Recall that BasisNet is of the following form:\nBasisNet(V ,\u03bb) = \u03c1(B) ( \u03d5 (B) 1 (VIV \u22a4 I1 ), ..., \u03d5 (B) L (VILV \u22a4 IL) ) , (87)\nwhere L is number of eigensubspaces.\nFor SPE, let us construct the following \u03d5\u2113:\n[\u03d5\u2113(x)]i =  1, if xi = \u03bb\u0303\u2113, 1\u2212 xi \u2212 \u03bb\u0303\u2113 \u03bb\u0303\u2113\u22121 \u2212 \u03bb\u0303\u2113 , if xi \u2208 (\u03bb\u0303l, \u03bb\u0303l+1), xi \u2212 \u03bb\u0303\u2113\u22121 xi \u2212 \u03bb\u0303l , if xi \u2208 (\u03bb\u0303\u2113\u22121, \u03bb\u0303\u2113),\n0, otherwise.\n(88)\nNote that this is both Lipschitz continuous with Lipschitz constant 1/min\u2113(\u03bb\u0303\u2113+1 \u2212 \u03bb\u0303\u2113), and permutation equivariant (since it is elementwise). Now we have V diag{\u03d5\u2113(\u03bb)}V \u22a4 = VI\u2113V \u22a4I\u2113 , since \u03d5\u2113(\u03bb) is either 1 (when \u03bbi = \u03bb\u0303\u2113) or 0 otherwise. For \u2113 > L, we let \u03d5\u2113 = 0 by default. Then simply let \u03c1 be:\n\u03c1(A1, ...,Am) = \u03c1 (S) ( \u03d5 (S) 1 (A1), ..., \u03d5 (S) m (Am) ) = \u03c1(B) ( \u03d5 (B) 1 (A1), ..., \u03d5 (B) L (AL) ) . (89)\nHere \u03d5(S)\u2113 (A\u2113) = \u03d5 (B) \u2113 (A\u2113) for A\u2113 \u0338= 0 and WLOG \u03d5 (S) \u2113 (A\u2113) = 0 if A\u2113 = 0. And \u03c1 (S) is a function that first ignores 0 matrices and mimic \u03c1(B). Therefore,\nSPE(V ,\u03bb) = \u03c1(V diag{\u03d51(\u03bb)}V \u22a4, ...,V diag{\u03d5m(\u03bb)}V \u22a4) = \u03c1(S) ( \u03d5 (S) 1 (VI1V \u22a4 I1 ), ..., \u03d5 (S) m (VImV \u22a4 Im) )\n= BasisNet(V ,\u03bb).\n(90)\nSince BasisNet universally approximates all continuous basis invariant function, so can SPE."
        },
        {
            "heading": "A.4 PROOF OF PROPOSITION 3.4",
            "text": "Proposition 3.4 (SPE can count cycles). Assume Assumption 3.1 hold and let \u03c1 be 2-IGNs (Maron et al., 2019b). Then SPE can determine the number of 3, 4, 5 cycles of a graph.\nProof. Note that from Lim et al. (2023), Theorem 3 we know that BasisNet can count 3, 4, 5 cycles. One way to let SPE count cycles is to approximate BasisNet first and round the approximate error, thanks to the discrete nature of cycle counting. The key observation is that the implementation of BasisNet to count cycles is a special case of SPE:\n#cycles of each node = BasisNet(V ,\u03bb) = \u03c1(V diag{\u03d5\u03021(\u03bb)}V \u22a4, ...,V diag{\u03d5\u0302m(\u03bb)}V \u22a4), (91)\nwhere [\u03d5\u0302\u2113(\u03bb)]i = 1(\u03bbi is the \u2113-th smallest eigenvalue) and \u03c1 is continuous. Unfortunately, these \u03d5\u0302\u2113 are not continuous so SPE cannot express them under stability requirement. Instead, we can construct a continuous function \u03d5\u2113 to approximate discontinuous \u03d5\u0302\u2113 with arbitrary precision \u03b5, say,\n\u2200\u03bb \u2208 [0, 2]d, \u2225\u2225\u2225\u03d5\u0302(\u03bb)\u2212 \u03d5(\u03bb)\u2225\u2225\u2225 < \u03b5. (92)\nThen we can upper-bound\u2225\u2225\u2225V diag{\u03d5\u0302\u2113(\u03bb)}V \u22a4 \u2212 V diag{\u03d5\u2113(\u03bb)}V \u22a4\u2225\u2225\u2225 F (a) \u2264 \u2225V \u2225 \u2225\u2225V \u22a4\u2225\u2225 \u2225\u2225\u2225\u03d5\u0302\u2113(\u03bb)\u2212 \u03d5\u2113(\u03bb)\u2225\u2225\u2225 < \u03f5, (93)\nwhere (a) holds due to the Lemma A.1. Moreover, using the continuity of \u03c1 (defined in Assumption 3.1), we obtain\n\u2225BasisNet(V ,\u03bb)\u2212 SPE(V ,\u03bb)\u2225F \u2264 J m\u2211 \u2113=1 \u2225\u2225\u2225V \u03d5\u0302(\u03bb)V \u22a4 \u2212 V \u03d5\u2113(\u03bb)V \u22a4\u2225\u2225\u2225 F < Jd\u03b5. (94)\nNow, let \u03b5 = Jd/2, then we can upper-bound the maximal error of node-level counting:\nmax i\u2208[n] |#cycles of node i\u2212 SPE(V ,\u03bb)|2 \u2264 \u2225BasisNet(V ,\u03bb)\u2212 SPE(V ,\u03bb)\u22252F < J 2d2\u03b52 = 1/4.\n(95) =\u21d2 max\ni\u2208[n] |#cycles of node i\u2212 SPE(V ,\u03bb)| < 1/2. (96)\nThen, by applying an MLP that universally approximates rounding function, we are done with the proof."
        },
        {
            "heading": "A.5 PROOF OF PROPOSITION 3.3",
            "text": "Proposition 3.3. Suppose that (V ,\u03bb) and (V \u2032,\u03bb\u2032) are such that V Q = V \u2032 for some orthogonal matrix Q \u2208 O(d) and \u03bb \u0338= \u03bb\u2032. Then there exist 2-layer MLPs for each \u03d5\u2113 and a 2-layer MLP \u03c1, each with ReLU activations, such that SPE(V ,\u03bb) \u0338= SPE(V \u2032,\u03bb\u2032).\nProof. Our proof does not require the use of the channel dimension\u2014i.e., we take m and p to equal 1. The argument is split into two steps.\nFirst we show that for the given \u03bb,\u03bb\u2032 and any \u03d5,\u03d5\u2032 \u2208 Rd there is a choice of two layer network \u03d5(\u03bb) = W2\u03c3(W1\u03bb+ b1) + b2 such that \u03d5(\u03bb) = \u03d5 and \u03d5(\u03bb\u2032) = \u03d5\u2032. Our choices of W1,W2 will have dimensions d\u00d7 d, b1, b2 \u2208 Rd, and \u03c3 denotes the ReLU activation function.\nSecond we choose \u03d5,\u03d5\u2032 \u2208 Rd such that V \u03d5V \u22a4 has strictly positive entries, whilst V \u2032\u03d5\u2032V \u2032\u22a4 = 0 (the matrix of all zeros). The argument will conclude by choosing a1, a2, b1, b2 \u2208 R such that the 2 layer network (on the real line) \u03c1(x) = a2 \u00b7\u03c3(a1x+ b1)+ b2 (a 2 layer MLP on the real line, applied element-wise to matrices then summed over both n dimensions) produces distinct embeddings for (V ,\u03bb) and (V \u2032,\u03bb\u2032).\nWe begin with step one.\nStep 1: If \u03d5 = \u03d5\u2032 then we may simply take W1 and b1 to be the zero matrix and vector respectively. Then \u03c3(W1\u03bb + b1) = \u03c3(W1\u03bb\u2032 + b1) = 0. Then we may take W2 = I (identity) and b2 = \u03d5, which guarantees that \u03d5(\u03bb) = \u03d5 and \u03d5(\u03bb\u2032) = \u03d5\u2032.\nSo from now on assume that \u03d5 \u0338= \u03d5\u2032. Let \u03bb and \u03bb\u2032 differ in their ith entries \u03bbi, \u03bb\u2032i, and assume without loss of generality that \u03bbi < \u03bb\u2032i. Let W1 = [0, . . . ,0, ei,0, . . . ,0] the matrix of all zeros, except the ith column which is the ith standard basis vector. Then W1\u03bb is the vector of all zeros, except for i entry equaling \u03bbi (similarly for \u03bb\u2032). Next take b1 to be the vector of all zeros, except for ith entry \u2212(\u03bbi + \u03bb\u2032i)/2, the midpoint between the differing eigenvalues. These choices make z = \u03c3(W1\u03bb+ b1) = 0, and z\u2032 = \u03c3(W1\u03bb\u2032 + b1) such that z\u2032j = 0 for j \u0338= i, and z\u2032i = (\u03bb\u2032i \u2212 \u03bbi)/2. Next, taking W2 = [0, . . . ,0, ci,0, . . . ,0] where the ith column is ci = 2(\u03d5\u2032\u2212\u03d5)/(\u03bb\u2032i\u2212\u03bbi) ensures that\nW2z = 0, (97) W2z \u2032 = \u03d5\u2032 \u2212 \u03d5. (98)\nThen we may simply take b2 = \u03d5, producing the desired outputs:\n\u03d5(\u03bb) = W2\u03c3(W1\u03bb+ b1) + b2 = \u03d5, (99) \u03d5(\u03bb\u2032) = W2\u03c3(W1\u03bb \u2032 + b1) + b2 = \u03d5 \u2032 (100)\nas claimed.\nStep 2: Expanding the matrix multiplications into their sums we have: [V diag(\u03d5)V \u22a4]ij = \u2211 d \u03d5dvidvjd (101)\n[V \u2032diag(\u03d5\u2032)V \u2032\u22a4]ij = \u2211 d \u03d5\u2032dv \u2032 idv \u2032 jd. (102)\nOur first choice is to take \u03d5\u2032 = 0, ensuring that V \u2032diag(\u03d5\u2032)V \u2032\u22a4 = 0 (an n\u00d7 n matrix of all zeros). Next, we aim to pick \u03d5 such that [ V diag(\u03d5)V \u22a4]i\u2217j\u2217 > 0 for some indices i\u2217, j\u2217. In fact, this is possible for an i, j pair since each pair of eigenvectors is orthogonal, and non-zero, so for each i, j there must be a d\u2217 such that vid\u2217vjd\u2217 > 0, and we can simply take \u03d5d = 1 if d = d\u2217 and \u03d5d = 0 for d \u0338= d\u2217.\nThanks to the above choices, taking a1 = 1 and b1 = 0 ensures that \u03c3 ( a1 \u00b7 V \u2032diag(\u03d5\u2032)V \u2032\u22a4 + b1 ) = 0. (103)\nbut that, [ \u03c3 ( a1 \u00b7 V diag(\u03d5)V \u22a4 + b1 )] ij > 0 (104)\nfor some i, j. Note that in both cases, the scalar operations are applied to matrices element-wise. Finally, taking a2 = 1/ \u2211 ij [\u03c3 ( a1 \u00b7 V diag(\u03d5)V \u22a4 + b1 )] ij > 0 and b2 = 0 produces embeddings\nSPE(V ,\u03bb) = 1 \u0338= 0 = SPE(V \u2032,\u03bb\u2032). (105)"
        },
        {
            "heading": "A.6 AUXILIARY RESULTS",
            "text": "Proposition A.1 (Davis-Kahan theorem (Yu et al., 2015, Theorem 2)). Let A,A\u2032 \u2208 S(n). Let \u03bb1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn be the eigenvalues of A, sorted in increasing order. Let the columns of x,x\u2032 \u2208 O(n) contain the orthonormal eigenvectors of A and A\u2032, respectively, sorted in increasing order of their corresponding eigenvalues. Let J = Js, tK be a contiguous interval of indices in [n], and let d = |J | be the size of the interval. For notational convenience, let \u03bb0 = \u2212\u221e and \u03bbn+1 = \u221e. Then there exists an orthogonal matrix Q \u2208 O(d) such that\n\u2225\u2225[x]J \u2212 [x\u2032]J Q\u2225\u2225F \u2264 \u221a 8min\n{\u221a d \u2225L\u2212L\u2032\u2225 , \u2225L\u2212L\u2032\u2225F } min {\u03bbs \u2212 \u03bbs\u22121, \u03bbt+1 \u2212 \u03bbt} . (106)\nProposition A.2 (Weyl\u2019s inequality). Let \u03bbi : S(n) \u2192 R return the ith smallest eigenvalue of the given matrix. For all A,A\u2032 \u2208 S(n) and all i \u2208 [n], |\u03bbi(A)\u2212 \u03bbi(A\u2032)| \u2264 \u2225A\u2212A\u2032\u2225.\nProof. By Horn & Johnson (2012, Corollary 4.3.15),\n\u03bbi(A \u2032) + \u03bb1(A\u2212A\u2032) \u2264 \u03bbi(A) \u2264 \u03bbi(A\u2032) + \u03bbn(A\u2212A\u2032) . (107)\nTherefore \u03bbi(A)\u2212 \u03bbi(A\u2032) \u2208 [\u03bb1(A\u2212A\u2032), \u03bbn(A\u2212A\u2032)], and\n|\u03bbi(A)\u2212 \u03bbi(A\u2032)| = max {\u03bbi(A)\u2212 \u03bbi(A\u2032), \u03bbi(A\u2032)\u2212 \u03bbi(A)} (108) \u2264 max {\u03bbn(A\u2212A\u2032),\u2212\u03bb1(A\u2212A\u2032)} (109) = max\ni\u2208[n] |\u03bbi(A\u2212A\u2032)| (110)\n= \u03c3max(A\u2212A\u2032) (111) = \u2225A\u2212A\u2032\u2225 . (112)\nProposition A.3 (Hoffman-Wielandt corollary (Stewart & Sun, 1990, Corollary IV.4.13)). Let \u03bbi : S(n)\u2192 R return the ith smallest eigenvalue of the given matrix. For all A,A\u2032 \u2208 S(n),\u221a\u221a\u221a\u221a n\u2211\ni=1\n(\u03bbi(A)\u2212 \u03bbi(A\u2032))2 \u2264 \u2225A\u2212A\u2032\u2225F . (113)\nLemma A.1. Let {Ak}pk=1 be compatible matrices. For any \u2113 \u2208 [p],\u2225\u2225\u2225\u2225\u2225 p\u220f\nk=1\nAk \u2225\u2225\u2225\u2225\u2225 F \u2264 ( \u2113\u22121\u220f k=1 \u2225Ak\u2225 ) \u2225A\u2113\u2225F ( p\u220f k=\u2113+1 \u2225\u2225A\u22a4k \u2225\u2225 ) . (114)\nProof. First, observe that for any matrices A \u2208 Rm\u00d7r and B \u2208 Rr\u00d7n,\n\u2225AB\u2225F = \u221a\u221a\u221a\u221a n\u2211 j=1 \u2225\u2225\u2225[AB]j\u2225\u2225\u22252 = \u221a\u221a\u221a\u221a n\u2211 j=1 \u2225ABej\u22252 \u2264 \u221a\u221a\u221a\u221a n\u2211 j=1 \u2225A\u22252 \u2225Bej\u22252 = \u2225A\u2225 \u221a\u221a\u221a\u221a n\u2211 j=1 \u2225\u2225\u2225[B]j\u2225\u2225\u22252 = \u2225A\u2225 \u2225B\u2225F .\n(115)\nTherefore, \u2225\u2225\u2225\u2225\u2225 p\u220f\nk=1\nAk \u2225\u2225\u2225\u2225\u2225 F (a) \u2264 ( \u2113\u22121\u220f k=1 \u2225Ak\u2225 )\u2225\u2225\u2225\u2225\u2225 p\u220f k=\u2113 Ak \u2225\u2225\u2225\u2225\u2225 F\n(116)\n(b) = ( \u2113\u22121\u220f k=1 \u2225Ak\u2225 )\u2225\u2225\u2225\u2225\u2225 p\u2212\u2113\u220f k=0 A\u22a4p\u2212k \u2225\u2225\u2225\u2225\u2225 F\n(117)\n(c) \u2264 ( \u2113\u22121\u220f k=1 \u2225Ak\u2225 )( p\u2212\u2113\u22121\u220f k=0 \u2225\u2225A\u22a4p\u2212k\u2225\u2225 )\u2225\u2225A\u22a4\u2113 \u2225\u2225F (118)\n(d) = ( \u2113\u22121\u220f k=1 \u2225Ak\u2225 ) \u2225A\u2113\u2225F ( p\u220f k=\u2113+1 \u2225\u2225A\u22a4k \u2225\u2225 ) , (119)\nwhere (a) and (c) hold by applying eq. (115) recursively, and (b) and (d) hold because Frobenius norm is invariant to matrix transpose.\nLemma A.2 (Permutation equivariance of eigenvectors). Let A \u2208 Rn\u00d7n and P \u2208 P(n). Then for any x \u2208 Rn, Px is an eigenvector of PAP\u22a4 iff x is an eigenvector of A."
        },
        {
            "heading": "Proof.",
            "text": "Px is an eigenvector of PAP\u22a4 (a)\u21d0\u21d2 \u2203\u03bb \u2208 R, PAP\u22a4Px = \u03bbPx (120) (b)\u21d0\u21d2 \u2203\u03bb \u2208 R, PAx = \u03bbPx (121) (c)\u21d0\u21d2 \u2203\u03bb \u2208 R, PAx = P\u03bbx (122) (d)\u21d0\u21d2 \u2203\u03bb \u2208 R, Ax = \u03bbx (123) (e)\u21d0\u21d2 x is an eigenvector of A , (124)\nwhere (a) is the definition of eigenvector, (b) holds because permutation matrices are orthogonal, (c) holds by linearity of matrix-vector multiplication, (d) holds because permutation matrices are invertible, and (e) is the definition of eigenvector.\nLemma A.3 (Permutation invariance of eigenvalues). Let A \u2208 Rn\u00d7n and P \u2208 P(n). Then \u03bb \u2208 R is an eigenvalue of PAP\u22a4 iff \u03bb is an eigenvalue of A."
        },
        {
            "heading": "Proof.",
            "text": "\u03bb is an eigenvalue of PAP\u22a4 (a)\u21d0\u21d2 \u2203y \u0338= 0, PAP\u22a4y = \u03bby (125) (b)\u21d0\u21d2 \u2203y \u0338= 0, PAP\u22a4y = \u03bbPP\u22a4y (126) (c)\u21d0\u21d2 \u2203y \u0338= 0, PAP\u22a4y = P\u03bbP\u22a4y (127) (d)\u21d0\u21d2 \u2203x \u0338= 0, PAx = P\u03bbx (128) (e)\u21d0\u21d2 \u2203x \u0338= 0, Ax = \u03bbx (129) (f)\u21d0\u21d2 \u03bb is an eigenvalue of A , (130)\nwhere (a) is the definition of eigenvalue, (b) holds because permutation matrices are orthogonal, (c) holds by linearity of matrix-vector multiplication, (d)-(e) hold because permutation matrices are invertible, and (f) is the definition of eigenvalue."
        },
        {
            "heading": "B EXPERIMENTAL DETAILS AND ADDITIONAL RESULTS",
            "text": "B.1 IMPLEMENTATION OF SPE\nSPE includes parameterized permutation equivariant functions \u03c1 : Rn\u00d7n\u00d7m \u2192 Rn\u00d7p and \u03d5\u2113 : Rd \u2192 Rd.\nFor \u03d5\u2113, we treat input \u03bb as d vectors with input dimension 1 and use either elementwise-MLPs, i.e., [\u03d5\u2113(\u03bb)]i = MLP(\u03bbi), or Deepsets to process them. We also use piecewise cubic splines, which is a R to R piecewise function with cubic polynomials on each piece. Given number of pieces as a hyperparameter, the piece interval is determined by uniform chunking [0, 2], the range of eigenvalues. The learnable parameters are the coefficients of cubic functions for each piece. To construct \u03d5\u2113, we simply let one piecewise cubic spline elementwisely act on each individual eigenvalues:\n[\u03d5\u2113(\u03bb)]i = spline(\u03bbi). (131)\nFor \u03c1, in principle any permutation equivariant tensor neural networks can be applied. But in our experiments we adapt GIN as \u03c1. Here is how: for input A \u2208 Rn\u00d7n\u00d7m, we first partition A along the second axis into n many matrices Ai \u2208 Rn\u00d7m (in code we do not actually need to partition them since parallel matrix multiplication does the work). Then we treat Ai as node features of the original graph and independently and identically apply a GIN to this graph with node features Ai. This will produce node representations Zi \u2208 Rn\u00d7p from Ai. Finally we let Z = \u2211n i=1 Zi be the final output of \u03c1. Note that this whole process makes \u03c1 permutation equivariant.\nB.2 IMPLEMENTATION OF BASELINES\nFor PEG, we follow the formula of their paper and augment edge features by\nei,j \u2190 ei,j \u00b7MLP(\u2225Vi,: \u2212 Vj,:\u2225). (132)\nFor SignNet/BasisNet, we refer to their public code release at https://github.com/cptq/SignNetBasisNet. Specifically, SignNet uses GINs as \u03d5 and BasisNet uses 2-IGN as \u03d5. Note that the original BasisNet does not support inductive learning, i.e., it cannot even apply to new graph structures. This is because it has separate weights for eigensubspaces with different dimension. Here we simply initialize an unlearned weights for eigensubspaces with unseen dimensions."
        },
        {
            "heading": "B.3 OTHER TRAINING DETAILS",
            "text": "We use Adam optimizer with an initial learning rate 0.001 and 100 warm-up steps. We adopt a linear decay learning rate scheduler. Batch size is 128 for ZINC, Alchemy and substructures counting, 64 for DrugOOD."
        },
        {
            "heading": "B.4 CONTROLLING LIPSCHITZ CONSTANT OF MLPS",
            "text": "Here we state how we control the Lipschitz constant of MLPs in Section 5.3. Technically, by Lipschitz constant we actually mean an upper bound for best Lipschitz constant (the minimal Lipschitz constant for function). Note that for a compositition of Lipschitz functions (f1 \u25e6 f2 \u25e6 ... \u25e6 fk) with individual Lipschitz constants C1, C2, ..., Ck, the product C1 \u00b7 C2 \u00b7 ... \u00b7 Ck is a valid Lipschitz constant. A MLP consists of linear layers and ReLU activation. The Lipschitz constants of linear layers are simply the operator norm of weight matrices, while ReLU is 1-Lipschitz. So we can easily get the Lipschitz constant of MLPs by multiplying the operator norm of weight matrices. As a result, we can control the Lipschitz constant to be C by first normalizing weight matrices to be unit norm and then multiply a constant C1/k between layers assuming there are k layers."
        },
        {
            "heading": "B.5 GENERALIZATION GAP ON ZINC",
            "text": "We show the training loss and the generalization gap (test loss - training loss) on ZINC dataset as shown below. These loss are all evaluated at the epoch with minimal validation loss. We can see that though SignNet and BasisNet achieve a pretty low training MAE (high expressive power), their generalization gap is larger than other baselines (poor stability) and thus the final test MAE is not the best. For baseline GNN and PEG, they are pretty stable with small generalization gap, but the poor expressive power make them hard to fit the dataset well (training loss is high). In contrast, SPE has not only a lowest training MAE (high expressive power) but also a small generalization gap (good stability). That is why it can obtain the best test performance among all the models."
        },
        {
            "heading": "B.6 RUNNING TIME EVALUATION",
            "text": "We evaluate the running time of SPE and other baselines on ZINC and DrugOOD. The results represents the training/inference time on the whole training dataset (ZINC or DrugOOD) over 5 trials. We can see that the speed SPE is overall comparable to SignNet, and is much faster than BasisNet. This is possibly because BasisNet has to deal with the irregular and length-varying input [V1V \u22a4 1 , V2V \u22a4 2 , ...], which is hard for parallel computation in a batch, while SPE simply needs to deal with the more uniform V diag(\u03d5(\u03bb))V \u22a4.\nTo see how complexity grows with graph size, we construct Erdos\u2013Renyi random graphs for different graph sizes, ranging from 10 to 320. For each graph size, we construct 1,000 such random graphs with fixed node degree 2.5. Then we train and test each methods on these 1,000 graph for 10 epochs to estimate the time complexity. For fairness, each model has 60k parameters. By default we use batch size 50, and if it is out-of-memory (OOM), we use batch size 5 then. It batch size 5 still leads to OOM, we will denote OOM in the results. Below we report the average training/inference time per epoch."
        },
        {
            "heading": "B.7 ABLATION STUDY",
            "text": "One key component of SPE is to leverage eigenvalues using \u03d5\u2113(\u03bb). Here We try removing the use of eigenvalues, i.e., set \u03d5\u2113(\u03bb) = 1 to see the difference. Mathematically, this will result in SPE(V ,\u03bb) = \u03c1([V V \u22a4]m\u2113=1). This is pretty similar to PEG and we loss expressive power from this over-stable operation V V \u22a4. As shown in the table below, removing eigenvalue information leads to a dramatic drop of performance on ZINC-subset. Therefore, the processing of eigenvalues is an effective and necessary design in our method."
        },
        {
            "heading": "B.8 MORE RESULTS ON TUDATASETS",
            "text": "We further conduct experiments on TUDatasets. For each task, we randomly split dataset into training, validation and test by 8:1:1. We uniformly use batch size 128 and train 250 epoch. Architectures and hyperparameters follows the same ones as on ZINC. We report the test accuracy at the epoch with highest validation accuracy over 5 random seeds. See Table 8 for results."
        },
        {
            "heading": "C WHY PREVIOUS POSITIONAL ENCODINGS ARE UNSTABLE?",
            "text": ""
        },
        {
            "heading": "C.1 ALL SIGN-INVARIANT METHODS ARE UNSTABLE",
            "text": "One line of work (Dwivedi & Bresson, 2021; Kreuzer et al., 2021; Lim et al., 2023) is to consider the sign ambiguity of each individual eigenvectors and aim to make positional encodings invariant to sign flipping. The underlying assumption is that eigenvalues are all distinct so eigenvectors are equivalent up to a sign transformation. However, we are going to show that all these sign-invariant methods are unstable, regardless of the eigenvalues being distinct or not.\nFirstly, suppose eigenvalues are distinct. Lemma 3.4 in Wang et al. (2022a) states that Lemma C.1. For any positive semi-definite matrix B \u2208 RN\u00d7N without multiple eigenvalues, set positional encoding PE(B) as the eigenvectors given by the smallest p eigenvalues sorted as 0 = \u03bb1 < \u03bb2 < ... < \u03bbp(< \u03bbp+1) of B. For any suffciently small \u03f5 > 0, there exists a perturbation \u2206B, \u2225B\u2225F \u2264 \u03f5 such that\nmin S\u2208SN(p) \u2225PE(B)\u2212 PE(B +\u2206B)\u2225 \u2265 0.99 max 1\u2265i\u2264p |\u03bbi+1 \u2212 \u03bbi|\u22121 \u2225\u2206B\u2225F + o(\u03f5), (133)\nwhere SN(p) = {Q \u2208 Rp\u00d7p : Qi,i = \u00b11, Qi,j = 0, for i \u0338= j} is the sign flipping operations.\nThis Lemma shows that when there are two closed eigenvalues, a small perturbation to graph may still yield a huge change of eigenvectors that cannot be compensated by sign flipping. Therefore, these sign-invariant methods are highly unstable on graphs with distinct but closed eigenvalues.\nOn the other hand, if eigenvalues have repeated values, then the same graph may produce different eigenvectors that are associated by basis transformations. Simply invariant to sign flipping cannot\nhandle this basis ambiguity. As a result, these sign-invariant methods will produce different positional encodings for the same input graph. That means there is no stability gurantee for them at all."
        },
        {
            "heading": "C.2 BASISNET IS UNSTABLE",
            "text": "Another line of work (e.g, BasisNet (Lim et al., 2023)) further consider basis invariance of eigenvectors by separately dealing with each eigensubspaces instead of each individual eigenvectors. The idea is to first partition eigenvectors V \u2208 Rn\u00d7d into their corresponding eigensubspace (V1, V2, ...) according to eigenvalues, where Vk \u2208 Rn\u00d7dk is the eigenvectors in k-th eigensubspace of dimension dk. Then neural networks \u03d5dk : Rn\u00d7n \u2192 Rn\u00d7p is applied to each VkV \u22a4k and the output will be \u03c1(\u03d5d1(V1V \u22a4 1 ), \u03d5d2(V2V \u22a4 2 ), ...) where \u03c1 : Rn\u00d7(d\u00b7p) \u2192 Rn\u00d7p is a MLP. Intuitively, this method is unstable because a perturbation of graph can change the dimension of eigensubspace and thus dramatically change the input (V1, V2, ...). As an example, let us say we have three eigenvectors (d = 3), and denote the three columns of V as u1, u2, u3. We construct two graphs: the original graph A has \u03bb1 = \u03bb2 < \u03bb3 while the perturbed graph A\u2032 has \u03bb\u20321 < \u03bb \u2032 2 < \u03bb \u2032 3. Two graphs share the same eigenvectors. Note that the difference between A and A\u2032 can be arbitrarily small to make \u03bb\u20322 a little bit different from \u03bb2. BasisNet will produce the following embeddings:\nBasisNet(A) = \u03c1(\u03d52(u1u\u22a41 + u2u \u22a4 2 ), \u03d51(u3u \u22a4 3 ))\nBasisNet(A\u2032) = \u03c1(\u03d51(u1u\u22a41 ), \u03d51(u2u \u22a4 2 ), \u03d51(u3u \u22a4 3 )).\nClearly as the input to \u03c1 are completly different, there is no way to ensure stability even if \u03c1 and \u03d5 are continuous."
        },
        {
            "heading": "C.3 WHY STABILITY THEOREM 3.1 CANNOT BE APPLIED FOR PREVIOUS METHODS",
            "text": "One may wonder why we cannot prove the stability of previous hard-partition methods following the same argument in Theorem 3.1. The reason is that they do not satisfy Assumption 3.1 or the requirement that \u03d5 is equivariant, both of which are the key to prove stability result in Theorem 3.1.\nTo see this, let us first consider sign-invariant methods (e.g., SignNet). Using the notation of SPE, it is equivalent to use a \u03d5\u2113(\u03bb) to separate the \u2113-th eigenvectors, thus whose k-th entry is\n[\u03d5\u2113(\u03bb)]k = \u03b4k,\u2113. (134)\nThis \u03d5 function does not rely on \u03bb and thus is Lipschitz continuous. However, it is not permutation equivariant to \u03bb vector. So it violates \u201c\u03d5 is always permutation equivariant\u201d as stated in the definition of SPE, and thus we cannot apply Theorem 3.1 to sign-invariant methods (and they are indeed\nunstable as shown before). On the other hand, let us consider basis-invariant methods using hard partition of eigensubspaces (e.g., BasisNet). In this case, the k-th entry of the hard partition \u03d5\u2113(\u03bb) is\n[\u03d5\u2113(\u03bb)]k = { 1, if \u03bbk is \u2113-th smallest eigenvalue 0, otherwise\nThis \u03d5 function is actually discontinuous. As an example, we may consider \u03bb = (\u03bb1, \u03bb2) and plot the figure of [\u03d51(\u03bb)]2 below. Clearly there is a sharp transition from 0 to 1 when \u03bb2 approches \u03bb1. Therefore, \u03d5\u2113(\u03bb) is more like a step function instead of a constant function. They are discontinuous, and thus are not Lipschitz continuous. So Assumption 3.1 does not hold and Theorem 3.1 cannot be applied for such hard partition functions."
        }
    ],
    "title": "CODINGS FOR GRAPHS",
    "year": 2024
}