{
    "abstractText": "LoRA (Hu et al., 2021) has gained widespread acceptance in the fine-tuning of large pre-trained models for a diverse array of downstream tasks, demonstrating notable effectiveness and efficiency, and thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA\u2019s plug-and-play plugins, researchers have explored the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. However, existing approaches for LoRA fusion face inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model\u2019s generative capabilities or the distinct identity of LoRAs, leading to suboptimal outcomes. On the other hand, reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, we introduce a learnable gating function to combine multiple LoRAs, namely the Mixture of LoRA Experts (MOLE). This approach leverages hierarchical control and unrestricted branch selection. The MOLE approach not only achieves superior LoRA fusion performance compared to direct arithmetic merging but also retains the essential flexibility for effectively combining LoRAs. Extensive experimental evaluations conducted in both Natural Language Processing (NLP) and Vision & Language (V&L) domains substantiate the efficacy of MOLE.",
    "authors": [
        {
            "affiliations": [],
            "name": "LORA EXPERTS"
        }
    ],
    "id": "SP:f118b92a742e52f58856952f952460256b4c0df8",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Luke Zettlemoyer",
                "Sonal Gupta"
            ],
            "title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning",
            "venue": "arXiv preprint arXiv:2012.13255,",
            "year": 2020
        },
        {
            "authors": [
                "Shengnan An",
                "Yifei Li",
                "Zeqi Lin",
                "Qian Liu",
                "Bei Chen",
                "Qiang Fu",
                "Weizhu Chen",
                "Nanning Zheng",
                "Jian-Guang Lou"
            ],
            "title": "Input-tuning: Adapting unfamiliar inputs to frozen pretrained models",
            "venue": "arXiv preprint arXiv:2203.03131,",
            "year": 2022
        },
        {
            "authors": [
                "Srinadh Bhojanapalli",
                "Chulhee Yun",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Sanjiv Kumar"
            ],
            "title": "Low-rank bottleneck in multi-head attention models",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Shaohan Huang",
                "Damai Dai",
                "Shuming Ma",
                "Barun Patra",
                "Saksham Singhal",
                "Payal Bajaj",
                "Xia Song",
                "Xian-Ling Mao"
            ],
            "title": "On the representation collapse of sparse mixture of experts",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini"
            ],
            "title": "The pascal recognising textual entailment challenge",
            "venue": "In Machine learning challenges workshop,",
            "year": 2005
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Shuming Ma",
                "Bo Zheng",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "Stablemoe: Stable routing strategy for mixture of experts",
            "venue": "arXiv preprint arXiv:2204.08396,",
            "year": 2022
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Mandy Simons",
                "Judith Tonhauser"
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "In proceedings of Sinn und Bedeutung,",
            "year": 2019
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "arXiv preprint arXiv:2208.01618,",
            "year": 2022
        },
        {
            "authors": [
                "Rinon Gal",
                "Or Patashnik",
                "Haggai Maron",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "Stylegan-nada: Clip-guided domain adaptation of image generators",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Yuchao Gu",
                "Xintao Wang",
                "Jay Zhangjie Wu",
                "Yujun Shi",
                "Yunpeng Chen",
                "Zihan Fan",
                "Wuyou Xiao",
                "Rui Zhao",
                "Shuning Chang",
                "Weijia Wu"
            ],
            "title": "Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models",
            "venue": "arXiv preprint arXiv:2305.18292,",
            "year": 2023
        },
        {
            "authors": [
                "Ligong Han",
                "Yinxiao Li",
                "Han Zhang",
                "Peyman Milanfar",
                "Dimitris Metaxas",
                "Feng Yang"
            ],
            "title": "Svdiff: Compact parameter space for diffusion fine-tuning",
            "venue": "arXiv preprint arXiv:2303.11305,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Chengsong Huang",
                "Qian Liu",
                "Bill Yuchen Lin",
                "Tianyu Pang",
                "Chao Du",
                "Min Lin"
            ],
            "title": "Lorahub: Efficient cross-task generalization via dynamic lora composition",
            "venue": "arXiv preprint arXiv:2307.13269,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Nupur Kumari",
                "Bingliang Zhang",
                "Richard Zhang",
                "Eli Shechtman",
                "Jun-Yan Zhu"
            ],
            "title": "Multi-concept customization of text-to-image diffusion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691,",
            "year": 2021
        },
        {
            "authors": [
                "Hector Levesque",
                "Ernest Davis",
                "Leora Morgenstern"
            ],
            "title": "The winograd schema challenge",
            "venue": "In Thirteenth international conference on the principles of knowledge representation and reasoning,",
            "year": 2012
        },
        {
            "authors": [
                "Chunyuan Li",
                "Heerad Farkhoor",
                "Rosanne Liu",
                "Jason Yosinski"
            ],
            "title": "Measuring the intrinsic dimension of objective landscapes",
            "venue": "arXiv preprint arXiv:1804.08838,",
            "year": 2018
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela"
            ],
            "title": "Adversarial nli: A new benchmark for natural language understanding",
            "year": 1910
        },
        {
            "authors": [
                "Or Patashnik",
                "Zongze Wu",
                "Eli Shechtman",
                "Daniel Cohen-Or",
                "Dani Lischinski"
            ],
            "title": "Styleclip: Textdriven manipulation of stylegan imagery",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang"
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for squad",
            "venue": "arXiv preprint arXiv:1806.03822,",
            "year": 2018
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "title": "Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Andrey Voynov",
                "Qinghao Chu",
                "Daniel Cohen-Or",
                "Kfir Aberman"
            ],
            "title": "p+: Extended textual conditioning in text-to-image generation",
            "venue": "arXiv preprint arXiv:2303.09522,",
            "year": 2023
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma"
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "venue": "arXiv preprint arXiv:2006.04768,",
            "year": 2020
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman"
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426,",
            "year": 2017
        },
        {
            "authors": [
                "Yuan Xie",
                "Shaohan Huang",
                "Tianyu Chen",
                "Furu Wei"
            ],
            "title": "Moec: Mixture of expert clusters",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Jinghan Zhang",
                "Shiqi Chen",
                "Junteng Liu",
                "Junxian He"
            ],
            "title": "Composing parameter-efficient modules with arithmetic operations",
            "venue": "arXiv preprint arXiv:2306.14870,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Han"
            ],
            "title": "V&L DOMAIN C.2.1 WHY IMPROVEMENT IS MARGINAL As shown in Figure. 12, there is usually a trade-off between text-alignment and image-alignment Kumari et al",
            "year": 2023
        },
        {
            "authors": [
                "Kumari"
            ],
            "title": "2023) and Textual Inversion Gal et al. (2022a). As shown in Table 10, we found that across multiple settings with varying numbers of fused concepts, our MoLE consistently outperformed TI in terms of average text similarity and image similarity",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "A primary driver of recent success in deep learning has been largely fueled by large-scale pretrained models, e.g., OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023) in natural language processing (NLP) and CLIP (Radford et al., 2021a), DALL\u00b7E 2 (Ramesh et al., 2022) in vision-andlanguage (V&L) learning. These pretrained models are usually first pre-trained on large-scale data and then fine-tuned on downstream tasks, showing impressive performance for a very wide range of downstream tasks. However, given that the size of such models grows rapidly nowadays, full fine-tuning (i.e., updating all parameters) still contributes to an unignorable computational cost. To address that, Low-Rank Adaptation (LoRA) (Hu et al., 2021) is proposed based on the idea that overparametrized models in fact reside on a low intrinsic dimension (Li et al., 2018; Aghajanyan et al., 2020). By freezing the pretrained model weights and injecting trainable rank decomposition matrices, LoRA is verified as an efficient fine-tuning technique under limited computational resources (Lester et al., 2021; An et al., 2022). While existing LoRAs serve as plug-and-play plugins for pre-trained models, with each LoRA typically retaining one characteristic after training, an important question arises:\nHow can multiple trained LoRAs be combined flexibly and dynamically, while preserving all their individual characteristics?\nIn this work, we aim to develop a method for combining multiple trained LoRAs while preserving all their individual characteristics.\nOne of the simplest approaches is to directly combine all the LoRAs together, as proposed in previous works (Zhang et al., 2023; Huang et al., 2023). However, we find this approach to be limited. Specifically, when incorporating multiple LoRAs (e.g., usually \u2265 3 in the V&L domain), the generation capabilities of the original pre-trained model can be adversely affected, as illustrated in Figure 3.\nAnother issue with prior combination methods is the uniform weighting of one LoRA (one characteristic) across all layers. We discovered that different LoRA layers contribute to the final results\nin varying degrees. In other words, distinct layers of LoRA provide different effects, and their combination constitutes the overall LoRA characteristics, as demonstrated in Figure 2.\nThis observation inspired us to consider making decisions about the fusion weights of each LoRA layer based on the unique characteristics of different LoRA layers at various model levels. We refer to this approach as \u201chierarchical control\u201d. Drawing inspiration from a recent line of research on Mixture-of-Experts (MoE) (Dai et al., 2022; Chi et al., 2022; Xie et al., 2023), which employs a gating function to decide which experts are sparsely activated, , we regard each trained LoRA as an expert and propose a Mixture of LoRA Experts (MOLE) method to combine multiple trained LoRAs flexibly and dynamically.\nThe core aspect of our MOLE lies in hierarchical control. To be specific, as demonstrated in Figure 1, at each layer of the model, we utilize the outputs of multiple LoRAs at that layer as inputs to a gate function to determine the fusion weights for these different LoRAs at that layer. We also present the workflow of MoLE in Figure 1 (c). During training, MOLE learns the gating function for multiple LoRA experts. During inference, MOLE has two inference modes: In the first mode, MOLE can use all the LoRA experts and allocate weights for each LoRA, preserving their individual characteristics. In the second mode, we can manually mask some unwanted LoRAs without changing the gating weights. It can recalculate and distribute weights proportionally. These two modes enable MOLE to adapt to different scenarios, providing a versatile and flexible approach for effective LoRA fusion. To validate the efficiency of our proposed methods, we test our approaches in both NLP and V&L areas. Results show that MOLE outperform existing LoRA combination manners and makes generation more stable. The contributions of our paper are the following:\n\u2022 We first present a meaningful yet challenging problem, i.e., How to combine multiple trained LoRAs while keep all their characteristics not lost, to further investigate and expand the applicability of LoRA in real-life scenarios.\n\u2022 We propose Mixture of LoRA Experts (MOLE), which achieves a more stable and flexible fusion of multiple LoRAs through hierarchical weight control using gating functions.\n\u2022 We conduct extensive experiments on both V&L and NLP domain. MOLE could improve LoRA mixture performance and alleviate problems caused by existing fusion method."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 LOW-RANK ADAPTION",
            "text": "Low-Rank Adaption (LoRA) (Hu et al., 2021) is a parameter-efficient fine-tuning method to adapt Large Language Models (LLMs) to novel tasks, particularly when the available training data is limited. This approach eliminates the necessity of fine-tuning the entire model, offering significant improvements in terms of computational efficiency and resource utilization.\nInspired by the low-rank internal dimensionality (Aghajanyan et al., 2020; Wang et al., 2020; Bhojanapalli et al., 2020), LoRA hypothesizes the updates to the weights also has a low \u201cintrinsic rank\u201d during adaptation. Specifically, for a pre-trained weight matrix W \u2208 Rd\u00d7k that takes x as input, LoRA modifies the output h of W with a with a low-rank decomposition:\nh = Wx+\u2206Wx = Wx+BAx, (1)\nwhile B \u2208 Rd\u00d7r, A \u2208 Rr\u00d7d, and the rank r \u226a min(d, k). A is initialized from a random Gaussian distribution and B is initialized with all zeros. During the training stage, W is fixed, and only BA is updated. Due to its superior performance, LoRA has been employed on not only NLP domain (Hu et al., 2021; Huang et al., 2023; Zhang et al., 2023) but also V&L domain (Sung et al., 2022)."
        },
        {
            "heading": "2.2 LORA MERGING",
            "text": "In real-world scenarios, the characteristics of individual LoRA instances often fail to meet the demands of consumers. To achieve the desired outcome, it is common practice to combine multiple LoRAs, for example, by separately learning LoRAs for clothing, facial features, and limbs, and then merging them to form the desired character representation. There are two primary types of existing LoRA merging methods:\nArithmetic operation-based fusion. Due to the plug-and-play nature, the most commonly employed combination method in both V&L and NLP domains is to to directly integrate multiple LoRAs (Huang et al., 2023; Zhang et al., 2023; Ruiz et al., 2023), i.e.,\nW\u0302 = W + N\u2211 i=1 wi \u00b7\u2206Wi, (2)\nwhere W\u0302 indicates the final combination weight and wi denotes the weights of different LoRAs. Typically, it is common practice to normalize wi, s.t. \u2211N i=1 wi = 1, to prevent any adverse impact on the embedding of the original model. However, direct arithmetic merging can lead to identity confusion among multiple LoRAs, resulting in such as missing objects and attribute binding (Ruiz et al., 2023).\nReference tuning-based fusion. To address above issue, Mix-of-Show (Gu et al., 2023) introduces a gradient fusion manner with regionally controllable sampling to handle attribute binding in multiple LoRAs combination. However, this method lacks the flexibility to freely combine models, i.e., when adding or removing LoRAs, the model needs to be retrained. Furthermore, although this method excels at constructing control graphs to effectively merge multiple LoRAs to create distinct characters, it falls short in its ability to consolidate multiple LoRAs into a single conceptual character.\nIn our work, we aim to achieve better mixture performance compared to arithmetic operation-based fusion, while simultaneously preserving the flexibility and capacity to integrate multiple LoRAs into a single conceptual character."
        },
        {
            "heading": "2.3 MIXTURE-OF-EXPERTS",
            "text": "Mixture-of-Experts (MoE) is a promising approach to scale up the number of parameters within the same computational bounds. Different from standard transformer models, each MoE layer consists of N independent feed-forward networks {Ei}Ni=0 as the experts, along with a gating function \u03b1 (\u00b7) to model a probability distribution indicating the weights over these experts\u2019 outputs. For the hidden representation h \u2208 Rd of input token, the gate value of routing h to expert Ei is denoted as:\n\u03b1 (Ei) = exp (h \u00b7 ei) / N\u2211 j=0 exp (h \u00b7 ej) , (3)\nwhere ei denotes the trainable embedding of Ei. Then, the corresponding k experts, according to the top-k gated values, are activated and the output O of the MoE layer is\nO = h+ N\u2211 i=0 \u03b1 (Ei) \u00b7Ei (h) . (4)\nOur work draws inspiration from gating mechanisms to model the weight distribution of the set of frozen LoRAs at each layer, aiming to find the most suitable combination for these LoRAs."
        },
        {
            "heading": "3 METHOD",
            "text": "In this work, our goal is to automatically determine the weights of each LoRA at their respective layers within the pretrained model, thereby achieving better merging performance without losing LoRA characteristics. In this section, we first introduce some motivating observations in Section 3.1. Then, we present the details of MOLE framework in Section 3.2 and the learnable gating function in Section 3.3. After that, we describe the training objective in Section 3.4."
        },
        {
            "heading": "3.1 MOTIVATING OBSERVATION",
            "text": "Observation 1: Fusing multiple LoRA candidates directly impacts the generative ability of the original model, while applying weight normalization before the fusion process maintains this generative capacity. As shown in Figure 3, we observed that directly combining multiple LoRA layers into the original model\u2019s embedding layer resulted in excessive parameter variations, thereby affecting the final generation quality. The same observations have also been elaborated upon in (Gu et al., 2023).\nObservation 2: Different layers of LoRA learn distinct feature representations, collectively forming the overall identity of LoRA.\nInspired by (Voynov et al., 2023) finding that different layers of U-Net in text-to-image models dominate different attributes (e.g., style, color, and structure), we explore the features learned by different layers within LoRA. In Figure 2, we observed that different sub-layers of LoRA encode distinct features ( i.e., styles). For instance, in the range of 80% to 100%, these sub-layers primarily contribute to the facial characteristics of the monster toy, while in the range of 40% to 60%, they predominantly contribute to the limbs of the monster toy. This insight inspires us that when combining different LoRA layers, especially when aiming to fuse styles from multiple LoRAs, we can achieve the desired style fusion effect by adjust-\ning the weight allocation across various LoRA layers. Specifically, increasing the weights of layers corresponding to the LoRA sub-styles we want, while decreasing the weights of those we do not need."
        },
        {
            "heading": "3.2 MIXTURE OF LORA EXPERTS",
            "text": "Drawing inspiration from the above observations, we introduce the Mixture of LoRA Experts (MoLE) to effectively integrate the characteristics of multiple LoRAs. Generally, MOLE first divides the pre-trained model and its corresponding LoRA experts into multiple subparts, with each subpart having an independent gating function to determine the weights of the LoRA experts. Then the output of each subpart is obtained by weighting the outputs of the pre-trained network\u2019s subpart and the outputs of the respective corresponding subpart of LoRA experts.\nSpecifically, as shown in Figure 4, for a block parameterized by \u03b8 (contains both multi-head attention layer and feed-forward neural network) within the pre-trained model and a set of corresponding\nsubpart of unique LoRA experts \u2126 = {\u2206\u03b8i}Ni=0, the input x \u2208 RL\u00d7d is sent to both pre-trained model and LoRA experts first, where N denotes the number of LoRA experts and L, d denotes the sequence length and dimension of input data, respectively. The output of the pre-trained block \u03b8 is presented as F\u03b8 \u2208 RL\u00d7d\nx \u2032 \u03b8 = x+ f Attn(LN (x) ;\u03b8), (5)\nF\u03b8 (x) = x \u2032 \u03b8 + f FFN(LN(x\u2032\u03b8);\u03b8), (6)\nwhere fAttn (\u00b7) represents the multi-head attention layer, fFFN (\u00b7) stands for the feed-forward neural network, and LN denotes layer normalization. The output of each LoRA expert is presented as E\u2206\u03b8i (x) \u2208 RL\u00d7d,\nx \u2032 \u2206\u03b8i = x+ f Attn(LN (x) ;\u2206\u03b8i), (7)\nE\u2206\u03b8i (x) = x \u2032 \u2206\u03b8i + f FFN(LN(x\u2032\u2206\u03b8i);\u2206\u03b8i). (8)"
        },
        {
            "heading": "3.3 GATING DIVERSIFIED LORA EXPERTS",
            "text": "To achieve hierarchical control, we apply a learnable gating function G (\u00b7) to model the optimal distribution of combination weights for outputs of these LoRA experts. Specifically, by taking {E\u2206\u03b8i (x)}Ni=0 as input, G (\u00b7) first apply concatenation and normalization (for training stability) operation, i.e.\nE\u2126 (x) = Normal ( E\u2206\u03b80 (x)\u2295 ...\u2295E\u2206\u03b8N\u22121 (x) ) , (9)\nwhere E\u2126 (x) \u2208 RN\u00d7L\u00d7d and \u2295 indicates the concatenation operation. Then we flatten and reduce the E\u2126 (x) to N -dimensions by a dot-product operation with e \u2208 R\u03be\u00d7N , where e is the learnable parameter for the gating function G (\u00b7) and \u03be = N \u00d7 L\u00d7 d,\n\u03b5 = Flatten (E\u2126 (x)) \u22a4 \u00b7 e, \u03b5 \u2208 RN , (10)\nThe gate value of each LoRA expert is computed as\nG (\u03b5i) = exp (\u03b5i/\u03c4)\u2211N j=1 exp (\u03b5j/\u03c4) , (11)\nthe temperature scalar \u03c4 is learnable. The final output E\u0303\u2126(x) of the gating function G (\u00b7) is obtained by multiplying the output of each LoRA expert with the corresponding gating values and then performing a final summarization as\nE\u0303\u2126(x) = N\u2211 i=0 Gi (\u03b5i)E\u2206\u03b8i (x) , E\u0303\u2126(x) \u2208 RL\u00d7d, (12)\nin which Gi (\u00b7) represents the weight of the i-th LoRA expert. So, the final output of this block is computed by adding the output of the gating function to the output of the pre-trained network\nO (x) = F\u03b8 (x) + E\u0303\u2126 (x) . (13)\nBesides, we also explored the performance of MOLE when placing gating functions at different hierarchical levels (e.g., layer-wise), a detailed analysis can be found in Section 4.3."
        },
        {
            "heading": "3.4 TRAINING OBJECTIVE",
            "text": "Gating Balancing Loss. As shown in Figure 5 (a), we have observed that the average entropy of the distribution probabilities from the gating functions gradually decreases as the number of training steps increases, i.e., the gating function tends to converge to a state where it always produces large weights for a early-stage well-performing LoRA ( e.g., shown in Figure. 5 (b), 68% and 51% gating probability for LoRA \u03b2 among three LoRAs during training and inference phase), leading to only a handful of LoRA experts having a significant impact in the end and a loss of the characteristics of other LoRA experts. To alleviate this imbalance across multiple LoRA experts, we propose a gating balancing loss Lbalance as\nLbalance = \u2212 log ( N\u220f i=0 q(i) ) , where q(i) = 1 M M\u2211 k=1\nexp ( \u03b5ki /\u03c4 )\u2211N j=1 exp ( \u03b5kj /\u03c4\n) . (14) M represents the number of blocks where gating functions are placed and N denotes the number of LoRA experts.\nDomain-specific Loss. Besides, for adoption to different domain, we utilize different domain training objective LD. In V&L domain, we adopt text-imagegeneration based on Stable Diffusion V2.1 (Rombach et al., 2022; Ruiz et al., 2023) to validate the effectiveness of MOLE. Due to the absence of annotated data for LoRA fusion, we adopted unsupervised training with the guidance of CLIP (Radford et al., 2021b) to optimize our MOLE. Specifically, to effectively extract knowledge from CLIP, two different losses have been proposed: a global target loss (Patashnik et al., 2021) and local directional loss (Gal et al., 2022b) So, The overall training objective L is the weighted sum of the above-mentioned two losses, represented as: L = LD + \u03b1Lbalance, (15)\nwhere \u03b1 is a coefficient for weight balancing. This balanced loss encourages balanced gating because it is minimized when the dispatching is ideally balanced."
        },
        {
            "heading": "3.5 OPTIMIZATION GATING FUNCTION ONLY",
            "text": "We freeze all the parameters of LoRA experts and pre-trained model, and optimize the parameters of gating function only. Because we find that making the parameters of LoRA experts or pre-trained model trainable will lead a loss of characteristics of LoRA experts when training data is small, a detailed analysis can be found in Section 4.3."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 MOLE ON V&L DOMAIN",
            "text": "Experimental Setup. For V&L domain, we apply MOLE to text-to-image generation task and choose DreamBooth (Ruiz et al., 2023) (based on Stable Diffusion V2.1) (Rombach et al., 2022) as the base generator, which involves fine-tuning the pretrained text-to-image diffusion model on a single object or concept (using 3-5 images). During training, we process the image resolution to\n512\u00d7512 and choose Adam (Kingma & Ba, 2014) optimizer with an initial learning rate of 1e-5. We frozen both DreamBooth pipeline and all LoRA candidates, and optimize gating function only. We used DDPM sampler (Ho et al., 2020) with 50 steps in each case. we train 400 iterations for each required fusion of LoRA experts with batch size 1 and \u03b1 as 0.5.\nMetrics and Compared Baselines. We evaluate our method on (1) Image alignment, i.e., the visual similarity of generated images with the target concept, using similarity in CLIP (Radford et al., 2021a) image feature space, (2) Text-alignment of the generated images with given prompts, using text-image similarity in CLIP feature space (Radford et al., 2021a). We compared our MOLE with simple fusion and some other arithmetic operation-based fusion methods like SVDiff (Han et al., 2023).\nMain Results. As shown in Table 1, we computed alignment scores in the CLIP space for four sets of LoRA expert fusion settings (each consisting of three LoRA experts for fusion). Our MOLE, along with various comparison models, was evaluated for alignment scores for both text prompts and images within each fusion. We found that our MOLE achieved the best average scores, whether in text prompt alignment or image alignment for individual LoRA expert. This indicates that MOLE preserves visual similarity more effectively and is capable of generating images that better adhere to textual requirements and convey more meaningful content, while minimizing identity loss of LoRA experts. Besides, as shown in Figure 9, Our MOLE has higher visual similarity with the personal cat and dog images while following the text condition better."
        },
        {
            "heading": "4.2 MOLE ON NLP DOMAIN",
            "text": "Experimental Setup. For NLP domain, following (Huang et al., 2023), we employ Flan-T5 (Chung et al., 2022) as our chosen LLM and created several LoRA experts based on FLAN datasets. In details, consider the NLI (Natural Language Inference) task cluster in the FLAN dataset, which includes multiple sub-datasets like ANLI (R1-R3) (Nie et al., 2019), RTE (Dagan et al., 2005), CB (De Marneffe et al., 2019), WNLI (Levesque et al., 2012), QNLI (Rajpurkar et al., 2018), MNLI (Williams et al., 2017) and others. We individually trained dedicated LoRA experts for the ANLI (R1-R3), WNLI, and QNLI datasets. To validate the generality of MOLE, we proposed two training settings for fusion methods (both LoRAhub and MOLE), i.e., (1) In-Domain training, which involves using data that the LoRA experts have already encountered as training data (ANLI (R1-R3), WNLI, and QNLI) for fusion methods. (2) Cross-Domain training, which involves using data that the LoRA experts have not encountered as training data (RTE, CB, SNLI and MNLI) for fusion methods. We train 800 iterations for each required fusion of LoRA experts with Adam (Kingma & Ba, 2014) optimizer with an initial learning rate of 1e-5, batch size 12 and \u03b1 as 0.5.\nMetrics and Compared Baselines. We evaluate our method on NLI task cluster in the FLAN dataset and utilize accuracy as evaluation metrics. We compared our MOLE with single LoRA, simple fusion and some other arithmetic operation-based fusion methods like LoRAhub (Han et al., 2023).\nMain Results. As shown in Table 2, In the in-domain setting, MOLE consistently outperformed baseline methods in terms of both individual and average results. Note that MoLE surpassing LoRAhub 0.94 when training in QNLI. In the cross-domain setting, we observed that MOLE consistently\noutperformed baseline methods in terms of average results. For instance, MOLEr achieves the best average performance, surpassing LoRAhub by 0.61 and outperforming a single LoRA by 0.51. Furthermore, we observed that MOLE consistently outperformed LoRAhub. These results indicate that MOLE demonstrates robust domain adaptation capabilities, making it more suitable for practical applications in LoRA fusion scenarios."
        },
        {
            "heading": "4.3 DETAILED ANALYSIS OF MOLE",
            "text": "The effectiveness of gating balancing loss. As shown in Figure 6 (a), we observed that after incorporating our designed gating balancing loss function Lbalance, the rate of entropy reduction in the gating functions slowed down.\nAs a result, the model exhibits a more uniform average weight distribution across all gating functions for each LoRA expert, e.g., as shown in Figure 6 (b), the blue bar chart represents the predicted distribution of average weights for each LoRA after adding Lbalance, which is notably more uniform compared to the distribution shown by the red bar chart. Thereby the final output aligns better with the fusion of multiple LoRA experts, mitigating the loss of LoRA characteristics.\nMOLE can seamlessly transition between the roles of a single LoRA adapter and a fusion for multiple LoRA. As discussed in Section 2.2, a well-designed LoRA fusion method should not only achieve effective LoRA fusion but also retain the characteristics of individual LoRA. It should be versatile enough to function as a standalone LoRA generator, ensuring its practical applications are flexible and widespread. Figure 7 displays a comparison of the qualitative re-\nsults for the retaining ability of several fusion methods, we find that our MOLE can generate images that closely resemble the original features of the LoRA experts ( e.g., dog ears, the color of the backpack), while other fusion methods tend to produce confusion and loss of LoRA characteristics. Besides, as shown in Figure 1 (c), we can also degrade MOLE by masking out the LoRA experts we do not wish to use, transforming it into a MOLE that merges fewer LoRA experts without affecting the fusion effect of the remaining LoRA experts. As shown in Figure 12, our MoLE can achieve the same flexible LoRA fusion as simple fusion method without altering the weights of MOLE, while reference tuning-based fusion Gu et al. (2023) can not accomplish.\nHierarchical control analysis. MOLE aims to achieve improved LoRA fusion effects through finer-grained hierarchical control. As illustrated in the Figure 8, we visualize the weight distributions assigned by the gating functions learned by MOLE at different levels in both NLP and V&L domains. We observe that MOLE adaptively assigns weights to different LoRA experts at various layers. Consequently, finer-grained weight combination methods lead to superior results.\nCoarse-to-fine MOLE analysis. To explore the varying effects of setting gating functions at different granularity levels, we established four coarse-tofine levels for MOLE: matrix-wise MOLE (hierarchical control at the granularity of parameter matrices), layer-wise MOLE, block-wise MOLE and networkwise MOLE, simplified as m-MOLE, l-MOLE, b-\nMOLE and n-MOLE, respectively. As shown in Table 3, we observed that the intermediate granularity b-MOLE and l-MOLE yielded the best performance, while the coarsest granularity n-MOLE, with too few optimizable parameters (only one gating for the entire network), resulted in suboptimal performance. Furthermore, we found that the finest granularity m-MOLE did not perform well, possibly due to overly detailed control disrupting the intrinsic relationships learned in LoRA parameters ( e.g., the relationship between q and v values in attention)."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we proposes a hierarchical control approach called Mixture of LoRA Experts (MOLE) for combining multiple trained LoRAs flexibly and dynamically. The core aspect of MOLE lies in learnable gating functions, where the outputs of multiple LoRAs at each layer are utilized as inputs to a gate function to determine fusion weights. We evaluate our method in both NLP and V&L areas, demonstrating that MOLE outperforms existing LoRA combination methods and makes generation more stable."
        },
        {
            "heading": "A EXAMPLE OF MOLE",
            "text": "We thank reviewers for the insightful comments. Overall, all reviewers noted that learning a gating function to combine LoRA modules is a sensible idea, and The proposed approach does not add too many additional parameters. We apologize for the delayed response as we conducted several experiments to address your requirements, which took some time."
        },
        {
            "heading": "B MOTIVATIONS VALIDATION ON NLP DOMAIN",
            "text": "To validate our motivations in the NLP domain, we conducted the following experiments to confirm that our two observations also hold true in the NLP domain:\nObservation 1: Fusing multiple LoRA candidates directly impacts the generative ability of the original model, while applying weight normalization before the fusion process maintains this generative capacity.\nWe combined multiple LoRAs with the FLAN-T5 model. When using five or more LoRAs, the model\u2019s output became chaotic, repeating words like \u2019like like like...\u2019. This shows that directly fusing multiple LoRAs in NLP can affect generative abilities. Additionally, we applied weight normalization to combine LoRAs trained on five datasets (ANLI-R1, ANLI-R2, ANLI-R3, QNLI, and WNLI). As shown in the Table. 4, we observed a decrease in performance for the fused LoRA on four of the datasets. This indicates that while weight normalization preserves generative capacity, it also impacts the intrinsic properties of LoRA.\nObservation 2: Different layers of LoRA learn distinct feature representations, collectively forming the overall identity of LoRA.\nWe trained a LoRA on a combination of ANLI-RA, ANLI-R2, and QNLI datasets, as shown in the Table. 5 and Figure. 11. We observed significant performance variations across different layers of this LoRA when evaluated on these sub-datasets. Specifically, the 0% 20% layers performed best on QNLI, the 40% 60% layers excelled on ANLI-R2, and the 80% 100% layers outperformed others on ANLI-R1. This reinforces the idea that in the NLP domain, different layers of LoRA acquire distinct feature representations. Based on the two sets of experiments presented above, we can conclude that the motivations we proposed in Section 3.1 hold true in the NLP domain as well.\nTable 5: Motivation experiments in NLP domain. The best value is in bold and the second-best value is indicated with an un-\nderline.\nANLI-R1 ANLI-R2 QNLI\nFull LoRA 81.65 80.03 76.42 0%-20% 78.72 78.35 78.14 20%-40% 76.10 77.96 77.85 40%-60% 76.95 81.47 74.57 60%-80% 77.25 78.19 75.71 80%-100% 82.59 77.91 75.48"
        },
        {
            "heading": "C MORE EXPERIMENT RESULTS",
            "text": "C.1 NLP DOMAIN\nC.1.1 FURTHER EVALUATION ON MORE DATASETS AND TASKS\nWe conducted extensive experiments across various tasks, including Translation, Struct to Text, Closed-Book QA, and multiple subtasks within the challenging Big-Bench Hard (BBH) datasets. This was done to comprehensively evaluate the effectiveness of our MoLE. Additionally, for the sake of thorough comparison, we introduced a new LoRA merging variant called PEMs Zhang et al. (2023), recently proposed in the field of Natural Language Processing.\nThe corresponding experimental results are summarized in the following Table 7. In summary, our MoLE surpasses state-of-the-art LoRA merging variants on four distinct datasets and tasks, showcasing robust performance, with a notable highlight on the BBH dataset. Our MoLE achieves an average performance improvement of 3.8 over LoRAHub and outperforms PEMs by a substantial margin of 9.0.\nIn the domain of natural language generation tasks (Translation and Struct to Text), our MoLE consistently demonstrates superior average performance in the Translation task set, surpassing LoRAHub by 1.5 and PEMs by 2.7. Similarly, within the Struct to Text task set, our model achieves an average performance advantage of 2.1 over LoRAHub and 2.6 over PEMs. These results highlight the effectiveness of our model in generating text.\nC.1.2 EXPAND EXPERT NUMBERS\nWe conducted experiments with an extended number of LoRA (8, 24, 48, 128) in the NLP domain, and the experimental results are as follows Table. 6. Our MoLE achieved optimal performance in multiple settings with different numbers of LoRA. Particularly, when the number of LoRA is large, such as 48 and 128, our MoLE outperformed LoRAHub by 2.5 and 3.0, respectively. Through analysis, we found that LoRAHub\u2019s optimization algorithm tends to assign most LoRA weights to zero when there are many LoRA candidates. This limitation hinders LoRAHub from fully utilizing the characteristics of all LoRA. In contrast, our MoLE mitigates this issue.\nBesides, as the number of LoRA increases to a very large value (128), the performance of all methods deteriorates. This is an intriguing phenomenon, indicating that current methods, including our MoLE, do not perform well when dealing with a large number of LoRA. This is an area we plan to explore and address in the future.\nC.2 V&L DOMAIN\nC.2.1 WHY IMPROVEMENT IS MARGINAL\nAs shown in Figure. 12, there is usually a trade-off between text-alignment and image-alignment Kumari et al. (2023); Han et al. (2023). High image-alignment leads to a decrease in text-alignment where sample generations have less variance and are close to input target images. As a result, our MoLE may not perform better in both image and text sub alignment. However, as shown in Table.1 it is noteworthy that when considering the overall average across all settings, our MoLE excels in both average text alignment and image alignment, which demonstrates that our MoLE is capable of achieving superior alignment between text and images.\nC.3 MORE EXAMPLES FOR BOTH QUANTITATIVE AND QUALITATIVE EXPERIMENTS\nMore quantitative results. As shown in Table 8, We can observe that under multiple settings, our MoLE model consistently achieves the best average performance in both Text-alignment and the three Image-alignment metrics. This indicates that our MoLE is capable of maintaining the optimal alignment with text as well as the highest similarity with the original visual elements when integrating multiple V&L LoRAs.\nExpending the number of LoRA. As shown in Table 9, We increased the number of our fused LoRAs in V&L experiments because typically experiments that involve fusing multiple visual concepts set the fusion number to be 3-4. Fusing too many concepts can lead to ambiguous model outputs. Therefore, we set it to be between 3 and 6. We found that as the number of LoRAs increased, our MoLE consistently performed the best in average Text-alignment and image-alignment. This indicates the robustness of our MoLE to the number of LoRAs and its improved fusion capability.\nComparison with more LoRA merging variants. Since there are no other suitable LoRA fusion methods available for comparison in the V&L domain, we introduced two state-of-the-art algorithms for multi-concept fusion that are not LoRA-based: Custom Kumari et al. (2023) and Textual Inversion Gal et al. (2022a). As shown in Table 10, we found that across multiple settings with varying numbers of fused concepts, our MoLE consistently outperformed TI in terms of average text similarity and image similarity. Additionally, it achieved state-of-the-art performance in text similarity, demonstrating the effectiveness of our MoLE.\nTable 10: Text-alignment and image-alignment results for multiple LoRA experts fusion in CLIP feature space. We evaluate each composition pair on 200 images generated using 5 prompts with 50 steps of DDPM sampler and scale=7.5. The best performance is in bold and the second-best value is\nindicated with an underline.\nNumber of concepts Text-alignment Average Image-alignment\nSF Custom Textual Inversion SVDiff MOLE SF Custom Textual Inversion SVDiff MOLE\n3 0.678 0.751 0.709 0.728 0.759 0.694 0.761 0.720 0.719 0.757 4 0.681 0.735 0.721 0.717 0.725 0.712 0.760 0.736 0.721 0.742 5 0.652 0.731 0.704 0.723 0.762 0.682 0.798 0.710 0.708 0.737 6 0.678 0.722 0.735 0.709 0.727 0.698 0.721 0.747 0.712 0.736\nAverage 0.67225 0.734 0.717 0.719 0.752 0.692 0.760 0.728 0.715 0.743"
        }
    ],
    "year": 2023
}