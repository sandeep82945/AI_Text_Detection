{
    "abstractText": "Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aleksa Sukovic"
        },
        {
            "affiliations": [],
            "name": "Goran Radanovic"
        }
    ],
    "id": "SP:199cf59b4a01b8b9b882a71638fa122936a3b516",
    "references": [
        {
            "authors": [
                "Richard B Anderson",
                "Beth M Hartzler"
            ],
            "title": "Belief bias in the perception of sample size adequacy",
            "venue": "Thinking & Reasoning,",
            "year": 2014
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Felix Hill",
                "Jan Leike",
                "Edward Hughes",
                "Arian Hosseini",
                "Pushmeet Kohli",
                "Edward Grefenstette"
            ],
            "title": "Learning to understand goal specifications by modelling reward",
            "year": 1806
        },
        {
            "authors": [
                "Osbert Bastani",
                "Yewen Pu",
                "Armando Solar-Lezama"
            ],
            "title": "Verifiable reinforcement learning via policy extraction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ioana Bica",
                "Daniel Jarrett",
                "Alihan H\u00fcy\u00fck",
                "Mihaela van der Schaar"
            ],
            "title": "Learning\" what-if\" explanations for sequential decision-making",
            "venue": "arXiv preprint arXiv:2007.13531,",
            "year": 2020
        },
        {
            "authors": [
                "Erdem Biyik",
                "Dorsa Sadigh"
            ],
            "title": "Batch active preference-based learning of reward functions",
            "venue": "In Conference on robot learning,",
            "year": 2018
        },
        {
            "authors": [
                "Mark Bovens"
            ],
            "title": "Analysing and assessing accountability: A conceptual framework 1",
            "venue": "European law journal,",
            "year": 2007
        },
        {
            "authors": [
                "Ralph Allan Bradley",
                "Milton E Terry"
            ],
            "title": "Rank analysis of incomplete block designs: I",
            "venue": "The method of paired comparisons. Biometrika,",
            "year": 1952
        },
        {
            "authors": [
                "Miles Brundage",
                "Shahar Avin",
                "Jasmine Wang",
                "Haydn Belfield",
                "Gretchen Krueger",
                "Gillian Hadfield",
                "Heidy Khlaaf",
                "Jingying Yang",
                "Helen Toner",
                "Ruth Fong"
            ],
            "title": "Toward trustworthy ai development: mechanisms for supporting verifiable claims",
            "venue": "arXiv preprint arXiv:2004.07213,",
            "year": 2020
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Cohen",
                "Jean-Louis Vincent",
                "Neill K J Adhikari",
                "Flavia R Machado",
                "Derek C Angus",
                "Thierry Calandra",
                "Katia Jaton",
                "Stefano Giulieri",
                "Julie Delaloye",
                "Steven Opal",
                "Kevin Tracey",
                "Tom van der Poll",
                "Eric Pelfrene"
            ],
            "title": "Sepsis: a roadmap for future research",
            "venue": "Lancet Infect Dis,",
            "year": 2015
        },
        {
            "authors": [
                "Rati Devidze",
                "Goran Radanovic",
                "Parameswaran Kamalaruban",
                "Adish Singla"
            ],
            "title": "Explicable reward design for reinforcement learning agents",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yilun Du",
                "Shuang Li",
                "Antonio Torralba",
                "Joshua B Tenenbaum",
                "Igor Mordatch"
            ],
            "title": "Improving factuality and reasoning in language models through multiagent debate",
            "venue": "arXiv preprint arXiv:2305.14325,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan St BT Evans",
                "Jodie Curtis-Holmes"
            ],
            "title": "Rapid responding increases belief bias: Evidence for the dual-process theory of reasoning",
            "venue": "Thinking & Reasoning,",
            "year": 2005
        },
        {
            "authors": [
                "Ioannis Faros",
                "Aditya Dave",
                "Andreas"
            ],
            "title": "A Malikopoulos. A q-learning approach for adherenceaware recommendations",
            "venue": "arXiv preprint arXiv:2309.06519,",
            "year": 2023
        },
        {
            "authors": [
                "Julien Grand-Cl\u00e9ment",
                "Jean Pauphilet"
            ],
            "title": "The best decisions are not the best advice: Making adherence-aware recommendations",
            "venue": "arXiv preprint arXiv:2209.01874,",
            "year": 2022
        },
        {
            "authors": [
                "Gillian K Hadfield"
            ],
            "title": "Explanation and justification: Ai decision-making, law, and the rights of citizens",
            "venue": "University of Toronto/Schwartz Reisman Institute for Technology and Society,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Marcel Hildebrandt",
                "Jorge Andres Quintero Serna",
                "Yunpu Ma",
                "Martin Ringsquandl",
                "Mitchell Joblin",
                "Volker Tresp"
            ],
            "title": "Reasoning on knowledge graphs with debate dynamics",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yong Huang",
                "Rui Cao",
                "Amir Rahmani"
            ],
            "title": "Reinforcement learning for sepsis treatment: A continuous action space solution",
            "venue": "In Machine Learning for Healthcare Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey Irving",
                "Paul Christiano",
                "Dario Amodei"
            ],
            "title": "Ai safety via debate",
            "venue": "arXiv preprint arXiv:1805.00899,",
            "year": 2018
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Lu Shen",
                "Li-wei H Lehman",
                "Mengling Feng",
                "Mohammad Ghassemi",
                "Benjamin Moody",
                "Peter Szolovits",
                "Leo Anthony Celi",
                "Roger G Mark"
            ],
            "title": "Mimic-iii, a freely accessible critical care database",
            "venue": "Scientific data,",
            "year": 2016
        },
        {
            "authors": [
                "Kishor Jothimurugan",
                "Rajeev Alur",
                "Osbert Bastani"
            ],
            "title": "A composable specification language for reinforcement learning tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Z Juozapaitis",
                "A Koul",
                "A Fern",
                "M Erwig",
                "F Doshi-Velez"
            ],
            "title": "Explainable reinforcement learning via reward decomposition, ijcai",
            "venue": "In Proceedings at the International Joint Conference on Artificial Intelligence. A Workshop on Explainable Artificial Intelligence.,",
            "year": 2019
        },
        {
            "authors": [
                "Matthieu Komorowski",
                "Leo A Celi",
                "Omar Badawi",
                "Anthony C Gordon",
                "A Aldo Faisal"
            ],
            "title": "The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care",
            "venue": "Nature medicine,",
            "year": 2018
        },
        {
            "authors": [
                "Minae Kwon",
                "Sang Michael Xie",
                "Kalesha Bullard",
                "Dorsa Sadigh"
            ],
            "title": "Reward design with language models",
            "venue": "arXiv preprint arXiv:2303.00001,",
            "year": 2023
        },
        {
            "authors": [
                "Jan Leike",
                "David Krueger",
                "Tom Everitt",
                "Miljan Martic",
                "Vishal Maini",
                "Shane Legg"
            ],
            "title": "Scalable agent alignment via reward modeling: A research direction. arxiv 2018",
            "venue": "arXiv preprint arXiv:1811.07871,",
            "year": 2018
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yuzhe Ma",
                "Xuezhou Zhang",
                "Wen Sun",
                "Jerry Zhu"
            ],
            "title": "Policy poisoning in batch reinforcement learning and control",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Andrew L Maas",
                "Awni Y Hannun",
                "Andrew Y Ng"
            ],
            "title": "Rectifier nonlinearities improve neural network acoustic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "T Nathan Mundhenk",
                "Barry Y Chen",
                "Gerald Friedland"
            ],
            "title": "Efficient saliency maps for explainable ai",
            "venue": "arXiv preprint arXiv:1911.11293,",
            "year": 2019
        },
        {
            "authors": [
                "Ethan Perez",
                "Siddharth Karamcheti",
                "Rob Fergus",
                "Jason Weston",
                "Douwe Kiela",
                "Kyunghyun Cho"
            ],
            "title": "Finding generalizable evidence by learning to convince q&a models",
            "year": 1909
        },
        {
            "authors": [
                "Aniruddh Raghu",
                "Matthieu Komorowski",
                "Imran Ahmed",
                "Leo Celi",
                "Peter Szolovits",
                "Marzyeh Ghassemi"
            ],
            "title": "Deep reinforcement learning for sepsis treatment",
            "venue": "arXiv preprint arXiv:1711.09602,",
            "year": 2017
        },
        {
            "authors": [
                "Ronilo Ragodos",
                "Tong Wang",
                "Qihang Lin",
                "Xun Zhou"
            ],
            "title": "Protox: Explaining a reinforcement learning agent via prototyping",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "why should i trust you?\" explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Tom Schaul",
                "John Quan",
                "Ioannis Antonoglou",
                "David Silver"
            ],
            "title": "Prioritized experience replay",
            "venue": "arXiv preprint arXiv:1511.05952,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Pier Giuseppe Sessa",
                "Ilija Bogunovic",
                "Andreas Krause",
                "Maryam Kamgarpour"
            ],
            "title": "Contextual games: Multi-agent learning with side information",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yoav Shoham",
                "Kevin Leyton-Brown"
            ],
            "title": "Multiagent systems: Algorithmic, game-theoretic, and logical foundations",
            "year": 2008
        },
        {
            "authors": [
                "Mervyn Singer",
                "Clifford S Deutschman",
                "Christopher Warren Seymour",
                "Manu Shankar-Hari",
                "Djillali Annane",
                "Michael Bauer",
                "Rinaldo Bellomo",
                "Gordon R Bernard",
                "Jean-Daniel Chiche",
                "Craig M Coopersmith"
            ],
            "title": "The third international consensus definitions for sepsis and septic shock",
            "venue": "(sepsis-3). Jama,",
            "year": 2016
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Hado Van Hasselt",
                "Arthur Guez",
                "David Silver"
            ],
            "title": "Deep reinforcement learning with double qlearning",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "J. von Neumann",
                "O. Morgenstern"
            ],
            "title": "Theory of Games and Economic Behavior",
            "year": 1947
        },
        {
            "authors": [
                "Ziyu Wang",
                "Tom Schaul",
                "Matteo Hessel",
                "Hado Hasselt",
                "Marc Lanctot",
                "Nando Freitas"
            ],
            "title": "Dueling network architectures for deep reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "A first step towards automated management of septic patients was done in a seminal work of Komorowski"
            ],
            "title": "The problem of treating sepsis was tackled by applying reinforcement learning to devise optimal treatment strategies for prescribing doses of intravenous fluids and vasopressors",
            "venue": "The authors discretized the possible doses, which resulted in an action-space consisting of",
            "year": 2018
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "2022) proposed an approach that is based on continuous action-space, thus enabling more granular control of prescribed doses",
            "year": 2022
        },
        {
            "authors": [
                "Komorowski"
            ],
            "title": "To assemble our patient cohort, we utilize the MIMIC-III v1.4 database (Johnson et al., 2016), focusing our analysis on patients that fulfill the Sepsis-3 criteria (Singer et al., 2016)",
            "year": 2018
        },
        {
            "authors": [
                "Raghu"
            ],
            "title": "terminal environment rewards of \u00b115 for every patient discharge and death, respectively. To stabilize the training of a deep-rl policy, we also issue intermediate rewards that are clinically guided",
            "year": 2017
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2016), where the estimated action-value function for a pair (s, a) is split into separate value and advantage streams. The final network consists of 2 fullyconnected layers of size 128 using leaky-relu activation functions with slope 1e-2. The learning is done in batches of 256 (s, a, r, s\u2032) tuples sampled from a Prioritized experience replay buffer (Schaul et al., 2015) with a learning rate of 1e-4. Instead of periodically updating the target",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Reinforcement learning (RL) has been achieving impressive successes in a wide range of complex domains. However, specifying a reward function which incentivizes RL agents to exhibit a desired behavior remains difficult (Leike et al., 2018). Prior work proposes several approaches that address these difficulties (Kwon et al., 2023; Bahdanau et al., 2018; Jothimurugan et al., 2019), including those based on learning from pairwise preferences (Christiano et al., 2017). However, such reward models are not informative enough for agents to learn how to substantiate their decisions with supporting evidence \u2013 a key property needed for accountable decision-making (Bovens, 2007). Hence, we ask the following question: How can we design rewards that incentivize agents to carry out a given task, but through decisions that can be justified?\nTo answer this questions, we consider a setting in which an RL agent acts as a principal, influencing the state of the world, while a human agent acts as a verifier responsible for validating the justifiability of the decisions taken by the RL agent, based on the evidence provided. This scenario mirrors common real-world situations where accountability is critical, including healthcare scenarios where doctors are tasked with scrutinizing the validity of automated decisions. We recognize three important properties that the provided evidence should satisfy. First, it should reflect the underlying state of the world, i.e., include the key information based on which an action was taken. A naive solution is to provide the full state as evidence. However, this discards the fact that the human, as a suboptimaldecision maker, may not easily reason about the taken decision because the state might be too large\nor otherwise incomprehensible. This brings us to the second property: the provided evidence should also be concise and only reflect the most relevant information. The third property builds on the second one: given that the provided evidence contains only partial information about the state, this information should not be easy to refute. More specifically, additional information about the underlying state, e.g., additional evidence, should not change the human\u2019s judgment. Therefore, the overall challenge is to design a framework which can enable such justifications through a reward model that incentivizes both performance and justifiability.\nTo tackle this challenge, we consider a framework which modifies the environment rewards by mixing them with rewards coming from a debate-based reward model (see Figure 1). Each debate-based reward is defined through the outcome of a two-player zero-sum debate game. More specifically, we let two argumentative agents debate by taking turns providing supporting evidence contingent on the current state, each corroborating a decision made by one of two competing policies. Based on the proposed set of evidence, the human then states their preference over these two decisions, and this preference defines the debate reward. In this setup, one decision comes from a baseline policy, while the other comes from a justifiable policy that we optimize. Our approach builds upon the work of Irving et al. (2018), but we consider sequential decision-making problems and utilize debate to quantify a decision\u2019s justifiability. To this end, we recognize two main technical challenges. First, learning a proxy of a human judge that evaluates decisions solely based on the proposed evidence, with comparable performance to methods requiring full state exposure. Second, learning a representation of argumentative strategies that are solutions to different instances of the debate game. These two components are needed to enable efficient learning of the justifiable policy.\nContributions. Our contributions are as follows. (i) We formalize the problem of justifiable decision-making, modeling debate as an extensive-form game (Sec. 3). (ii) We provide a method for learning a proxy of a human judge that evaluates a decision\u2019s justifiability using proposed evidence (Sec. 4.2). (iii) We propose an approach to learning contextualized argumentative strategies that constitute approximate solutions of the debate games (Sec. 4.3). (iv) We conduct extensive empirical evaluation of our approach on a real-world problem of treating sepsis, testing the performance and justifiability of policies trained through our framework (Sec. 5.2), as well as the effectiveness and robustness of argumentative agents (Sec. 5.3, Sec. 5.4, and Sec. 5.5) 1."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Debate. Irving et al. (2018) first outlined theoretical and practical implications of debate as a method for training aligned agents. Debate has also been used to improve factuality of large language models (Du et al., 2023), reason in knowledge graphs (Hildebrandt et al., 2020), and aid in Q&A tasks (Perez et al., 2019). We build on this line of work by bringing the utility of debate in sequential decisionmaking problems, where it is used as an approach of quantifying justifiability of made decisions. In addition, by leveraging learning from pairwise preferences as in Christiano et al. (2017), our\n1Our code is publicly available at github.com/aleksa-sukovic/iclr2024-reward-design-for-justifiable-rl.\nframework enables encoding of human judgments in a form of the preferred evidence that renders a decision justified.\nReward Design. Previous work proposes several approaches that address difficulties of reward design, based on natural language (Kwon et al., 2023; Bahdanau et al., 2018), rule-based methods (Jothimurugan et al., 2019) and preferences (Biyik & Sadigh, 2018; Christiano et al., 2017). Furthermore, there is a line of work considering interpretable reward design, including reward sparsification (Devidze et al., 2021) and reward decomposition (Juozapaitis et al., 2019; Bica et al., 2020). Differently, we define a reward model as an outcome of a zero-sum debate game, which in itself is interpretable, and learn contextualized policies that solve it. Most similar to our methodology is learning from pairwise preferences as in Christiano et al. (2017), only we assume comparisons are made over justifying evidence, without exposure to the underlying state or trajectory.\nExplainable AI and Other Related Work. Our approach is most similar to the attribution-based techniques for explaining the inner workings of models. In such approaches, contributions of input features are quantified numerically (Lundberg & Lee, 2017; Ribeiro et al., 2016) or visually represented as heatmaps (Selvaraju et al., 2017; Mundhenk et al., 2019). This line of research has received a significant attention, also in the context of explaining decisions of RL agents (Ragodos et al., 2022; Bastani et al., 2018). However, one limitation of these explanations is the inability to further align them with human preferences (Hadfield, 2021; Brundage et al., 2020). In contrast, our approach additionally enables human-specified justifications, facilitating the incorporation of preferences into their generation. There is also a line of work that examines approaches for adaptation of agent\u2019s recommendations (actions) to a baseline human policy in an expert-in-loop setup (Grand-Cl\u00e9ment & Pauphilet, 2022; Faros et al., 2023). Differently, we consider a problem of learning to take actions that can be justified, where the agent itself acts as a primary decision-maker in the environment."
        },
        {
            "heading": "3 FORMAL SETUP",
            "text": "We consider a sequential decision-making problem, where an agent interacts with an environment over a series of time-steps, modeled by a discrete-time Markov Decision Processes (MDP). The episode begins by sampling a starting state from the initial state distribution \u03c1. At each time-step t, an agent observes the current state st \u2208 S , takes an action at \u2208 A and receives an environment reward re(st, at). The environment then transitions to a successor state st+1 with a probability specified by the transition function T (st+1|st, at)."
        },
        {
            "heading": "3.1 AGENTS",
            "text": "We consider two kinds of agents that operate in the defined MDP: baseline and justifiable agent.\nBaseline Agent. The baseline agent aims to maximize the expected discounted value of the environment\u2019s return given as R = \u2211T\u22121 t=0 \u03b3\ntre(st, at), where \u03b3 \u2208 [0, 1] is a user-specified discount factor. Its optimal action-value function, defined as Q\u2217(s, a) = max\u03c0 E [R|st = s, at = a, \u03c0], is the maximum expected discounted return obtained after taking action a in state s. We will refer to a deterministic policy that maximizes the expected value of R as the baseline policy \u03c0B , which satisfies \u03c0B(s) \u2208 argmaxa Q\u2217(s, a). 2\nJustifiable Agent. While the baseline agent learns to solve the environment, its decisions may not be easy to justify when evaluated by a human. To design an agent that accounts for the justifiability of its decisions, we consider a reward defined as a weighted combination of the environment reward re and the debate reward rd, which encapsulates human judgment of justifiability and is specified in the next subsection. The expected return is then defined as:\nRJ = \u2211T\u22121\nt=0 \u03b3t\n[ (1\u2212 \u03bb) \u00b7 re(st, at) + \u03bb \u00b7 rd(st, at, aBt ) ] where \u03bb \u2208 [0, 1] is a debate coefficient, balancing between environment and debate rewards, and aBt is the action of the baseline agent in state st. The value of \u03bb = 0.0 corresponds to the return of the baseline policy, whereas for the value \u03bb = 1.0 we obtain a setup similar to Christiano et al. (2017), where the agent relies only on the debate reward model. We refer to an agent that maximizes the expected value of RJ as the justifiable agent, denote its optimal action-value function as\n2In practice, we require the baseline policy to be well-performing, but not necessarily optimal.\nQ\u2217J(s, a) = max\u03c0 E [RJ |st = s, at = a, \u03c0] and its deterministic policy \u03c0J(s) \u2208 argmaxaQ\u2217J(s, a) as the justifiable policy."
        },
        {
            "heading": "3.2 REWARD MODELING VIA DEBATE",
            "text": "Our objective is to learn a reward model, denoted as rd(st, at, aBt ), which quantifies the justifiability of a decision. To this end, we introduce a reward model based on a debate game. In this model, rd(st, at, a B t ) represents the value of a debate game induced by a tuple (st, at, a B t ). In more details, for a given tuple (st, at, aBt ), the induced debate game is formulated as a two-player zero-sum extensive-form game (Shoham & Leyton-Brown, 2008), in which the first player argues for taking the decision at in state st, while the second player argues for taking the baseline decision aBt in st.\nDebate Game. With N we denote a set of nodes in a finite, rooted game tree. The action space is represented by a finite set of evidence E , and each node n \u2208 N consists of evidence proposed thus far in the game n = {e} \u2286 E . Additionally, the edges to successors of each node define actions (evidence) {e : e \u2208 E \\n} available to the acting player, where we disallow evidence repetition. The debate game is a perfect-information game: at all times, the players have no ambiguity about the evidence proposed up until the current point and have a complete knowledge about the state of the game. The game proceeds as players take turns: in turn l, player i = l mod 2+1 proposes evidence eil/2.\n3 The total number of turns L is assumed to be even and significantly smaller than the evidence set, i.e., L \u226a |E|. After the last turn, a terminal node nL = (e11, e21, ..., e1L/2, e 2 L/2) = {enL} is evaluated. The players\u2019 utilities are u1(nL) = \u2212u2(nL) = U(at, aBt , {enL}), with U defined as:\nU(at, aBt , {e}) =  +1, J (at, {e}) > J (a B t , {e})\n0, J (at, {e}) = J (aBt , {e}) \u22121, otherwise\nHere, J is a model of a human judge that, for a given decision a and evidence {e}, outputs a numerical value J (a, {e}) \u2208 R quantifying how justifiable a is under evidence {e}. Strategies and Solution Concept. A player\u2019s strategy \u03c3i : N \u2192 E outputs available evidence \u03c3i(n) \u2208 E \\n in a given node n and \u03a3i is the set of all strategies of the player i. Based on the utility function, we additionally define G({\u03c31, \u03c32}, st, at, aBt ) as the payoff (utility) of the first player, conditioned on both players following the strategy profile {\u03c31, \u03c32}. Then, a set of the best responses of the first (resp. second) player to its opponent strategy \u03c32 (resp. \u03c31) is defined as b1(\u03c32) = argmax\u03c31\u2208\u03a31 G({\u03c31, \u03c32}, st, at, aBt ) (resp. b2(\u03c31) = argmin\u03c32\u2208\u03a32 G({\u03c31, \u03c32}, st, at, aBt )). A strategy profile \u03c3\u0304 = {\u03c3\u03041, \u03c3\u03042} is a pure-strategy Nash equilibrium if \u03c3\u0304i \u2208 bi(\u03c3\u0304\u2212i). Because the debate game is a perfect-information extensive-form game, a pure-strategy Nash equilibrium exists (Shoham & Leyton-Brown, 2008), and due to its zero-sum structure, it can be obtained by solving the following max-min optimization problem: max\u03c31\u2208\u03a31 min\u03c32\u2208\u03a32 G({\u03c31, \u03c32}, st, at, aBt ). We refer to G({\u03c3\u03041, \u03c3\u03042}, st, at, aBt ) as the value of the game 4 and define rd(st, at, aBt ) to be equal to it, i.e., rd(st, at, aBt ) = \u03b1 \u00b7 G({\u03c3\u03041, \u03c3\u03042}, st, at, aBt ), where \u03b1 > 0 is a scaling coefficient."
        },
        {
            "heading": "4 LEARNING FRAMEWORK",
            "text": "To effectively use a debate game outcome during training of justifiable policies, it is necessary to devise a model of a human judge J (Sec. 4.2) that encapsulates justifiability judgment and additionally learn argumentative policies that approximate a Nash equilibrium and are able to generalize across different instances of the debate game. With r\u0302d(st, at, aBt ) we denote an approximation of rd(st, at, a B t ), obtained by running the argumentative policies from Sec. 4.3 in the debate game induced by (st, at, aBt ). In all our experiments, we set \u03b1 = 5.\n3In our implementation of the debate game, we randomly chose which player has the first turn, i.e., i = (l + \u03c4) mod 2 + 1, where \u03c4 \u223c U({0, 1}). This only affects the order of the evidence in nL.\n4For a two-player zero-sum game, the value of the game (or payoff) is unique (von Neumann & Morgenstern, 1947)."
        },
        {
            "heading": "4.1 PREFERENCE DATASET",
            "text": "We assume the human judgments are collected in a preference dataset D of tuples (st, a0, a1, p), where st is a state, a0 \u0338= a1 are two decisions and p \u2208 {0, 1} indicates which of the two decision is more justified in a particular state. The value of p = 0 (resp. p = 1) indicates that a0 (resp. a1) is more preferred."
        },
        {
            "heading": "4.2 JUDGE MODEL",
            "text": "Because asking for human feedback is expensive, we aim to learn a judge model from the dataset D of preferences that can be used to evaluate the outcome of debate games. The judge, parametrized with learnable parameter \u03b8 \u2208 Rd1 , is defined as a scalar reward function J\u03b8(a, {e}) \u2208 R quantifying how justifiable a decision a is, given evidence {e}. Similar to Christiano et al. (2017), we additionally assume that justifiability preference for decision a0 over decision a1 follows the Bradley-Terry model (Bradley & Terry, 1952):\nP(a0 \u227b a1, {e}) = expJ\u03b8(a0, {e})\nexpJ\u03b8(a0, {e}) + expJ\u03b8(a1, {e}) .\nHere, we require the judge to quantify the level of justifiability given all evidence at once. Note the lack of dependency on the state st: the judge evaluates only proposed evidence, whereas the argumentative agents are in charge of providing those evidence, contingent on the state. We optimize the parameters by minimizing the cross-entropy loss between preference-predictions and labels from the dataset. See App. C.1 for more details."
        },
        {
            "heading": "4.3 ARGUMENTATIVE AGENT",
            "text": "Our overarching goal is to learn a generalizable argumentative policy that is able to solve any debate game, conditioned on its defining tuple (st, at, aBt ). This is a difficult feat, as the evidence set available to the agent is contingent on the state st. To this end, we can treat the debate game as an instance of a contextualized extensive form game (Sessa et al., 2020), where we consider the tuple (st, at, aBt ) as a context z \u2208 Z . In a general case, the justifiable agent \u03c0J observes the state st and takes action at which, paired with an action aBt the baseline agent \u03c0\nB would have taken, sets the debate game context z = {st, at, aBt }. In the specific case of offline reinforcement learning we consider here, the contexts are sampled i.i.d. from the static dataset throughout training of both, argumentative and justifiable agents. Therefore, given a sample from the preference dataset (st, a0, a1, p) \u223c D, we set the context to z = (st, ap, a1\u2212p). Player i\u2019s contextual strategy, parametrized with learnable parameter \u03d5i \u2208 Rd2 , is defined as \u03c3c\u03d5i : Z \u2192 \u03a3\ni, mapping a context to the strategy of the player for the induced debate game 5. To learn parameters \u03d5i, we solve: max\u03d51\u2208Rd2 min\u03d52\u2208Rd2 Ez\u223cD[G({\u03c3 c \u03d51 (z), \u03c3c\u03d52(z)}, z)]."
        },
        {
            "heading": "4.4 METHOD",
            "text": "Judge. The judge J\u03b8 is parametrized by weights \u03b8 \u2208 Rd1 of a neural network with two fullyconnected layers of size 256, using parametric relu (He et al., 2015) activation and batch normalization (Ioffe & Szegedy, 2015). The network receives a vector x in R|E|, where only the values of evidence {e} are shown, while the rest are set to zero. In addition, a binary mask of the same dimension, wherein all elements corresponding to the evidence {e} are assigned a value of one, while the remaining elements are set to zero, as well as a one-hot encoded action a are passed. The learning is done for a total of 100 epochs using batches of 64 comparisons sampled from the preference dataset D, Adam optimizer and a learning rate of 5e-4. See App. C.1 for more details. Argumentative Agent. We represent parameters \u03d5 \u2208 Rd2 of the argumentative agent \u03c3c\u03d5(\u00b7|z), as weights of a neural network composed of 2 fully-connected layers of size 512 with leaky-relu activation function (Maas et al., 2013) with slope of 1e-2. The network takes as input a 44-dim state, a one-hot encoded decision for which the agent argues, as well as a binary mask of currently proposed evidence. The evidence (action) space of the policy is discrete and has 44 choices, each corresponding to exactly one state feature. To train the agent, we use PPO (Schulman et al., 2017)\n5Equivalently, we will also denote the contextual strategy of player i as \u03c3c\u03d5i(\u00b7|z) : N \u2192 E for z \u2208 Z .\nand examine two optimization strategies, namely self-play and maxmin. We train the self-play debate by letting the agent argue with a copy of itself 6, updating that copy every 100k steps, and repeating the procedure for 500 generations. For maxmin debate, we use 2 different set of weights, \u03d51 and \u03d52, to represent the agents\u2019 policies. The first agent i.e., \u03d51 is trained for 4k steps, followed by training of the second agent i.e., \u03d52 for 100k steps, repeating this procedure for 500 generations. This approach of overfitting the second agent to the current version of the first ensured learning of defense strategies against a very strong adversary. See App. C.2 for more details.\nJustifiable Agent. To learn a justifiable policy \u03c0J(s) \u2208 argmaxa Q\u2217J(s, a), we consider model-free reinforcement learning methods based on Q-learning. Our approach builds on top of Raghu et al. (2017) and is based on a variant of Deep-Q networks (Mnih et al., 2015), specifically double-deep Q-network with the dueling architecture (Wang et al., 2016; Van Hasselt et al., 2016). The final network consists of 2 fully-connected layers of size 128 using leaky-relu activation function with slope 1e-2. The learning is done in batches of 256 (s, a, r, s\u2032) tuples sampled from a Prioritized Experience Replay buffer (Schaul et al., 2015) using a learning rate of 1e-4, for a total of 25k iterations, evaluating the policy every 50 iterations on a held-out test set. When incorporating signal from the debate, i.e., r\u0302d, we augment the reward from the replay buffer as described in Sec. 3.1. Modifying just the observed reward, without changing the state dynamics, has been shown to be sufficient to induce learning of an alternative policy (Ma et al., 2019). See App. C.3 for more details, including additional loss terms and a list of all hyperparameters."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In our experiments, we aim to empirically evaluate the properties of debate as a method for reward specification. To this end, we perform quantitative and qualitative evaluation of justifiable policies (Sec. 5.2), analyze the effect of pairwise comparison only over proposed evidence on performance and justifiability (Sec. 5.3), examine the necessity of multi-agent debate in learning to provide evidence that is resilient to refutations (Sec. 5.4), and compare efficiency in providing supporting evidence for a decision of argumentative policies and SHAP feature-attribution method (Sec. 5.5)."
        },
        {
            "heading": "5.1 ENVIRONMENTAL SETUP",
            "text": "Sepsis. Data for our cohort were obtained following steps outlined in Komorowski et al. (2018), utilizing MIMIC-III v1.4 database (Johnson et al., 2016). We focus our analysis on patients that fulfill Sepsi-3 criteria (Singer et al., 2016), 18,585 in total. The patient vector consists of 40 continuous and 4 discrete features, and the action space consists of 5 \u00d7 5 discrete choices of intravenous\n6In terms of learnable parameters, this implies \u03d52 := \u03d51.\n(IV) fluids and vasopressors (VC). As in Raghu et al. (2017), the environment rewards are clinically guided. We set the environment reward re to \u00b115 for terminal states resulting in patient survival or death, respectively, and shape the intermediate rewards based on the SOFA score (measure of organ failure). See App. B for more details.\nPreference Dataset. To make a comprehensive evaluation possible, we define a synthetic groundtruth preference by making an assumption that a human judge always prefers a treatment prescribed by the clinician. Therefore, clinician\u2019s actions are the justified actions in our experiments. More formally, we bootstrap the dataset of preferences D by matching every pair (st, at) from the cohort with an alternative action ar \u223c U(A), ar \u0338= at sampled uniform-random from the action space, initializing the preference variable p to point to the true action at, as taken by the clinician (see App. D.5 for alternative dataset bootstrapping methods). The number of evidence is fixed to 6 (\u223c 13.6% of the full state) (see App. D.1 for results with L = 4 evidence). The dataset is split into chunks of 70%, 15%, 15% used for training, validation, and testing respectively. We report all our results on a held-out test set, not seen by any of the agents nor the judge. When training a judge model, we additionally augment a tuple (st, a0, a1, p) with an evidence set {e}. To generate it, we sample a predetermined number of state features uniform-random i.e., {e} \u223c U(st), from the state, which is inspired by Irving et al. (2018) and ensures evidence is contingent on the state (Sec. 3.2) 7. On the test set, the judge is able to correctly recover the underlying preference from the dataset with a relatively low accuracy of 65%.\nBaselines. When comparing effectiveness of treatment policies, we consider two baselines. First, we consider the observed reward of the clinician from the dataset (depicted as a gray horizontal line in plots). Second, the baseline policy (Sec. 3.1) serves as an indicator of the optimal treatment policy. To demonstrate the robustness of multi-agent debate, we introduce an isolated argumentative agent. This agent aims to find an evidence set {e} that maximizes J (ap, {e}), for a given ap. To achieve this, we solve a search problem akin to the debate game by applying reinforcement learning (see App. C for more details). Lastly, we use SHAP when comparing effectiveness of debate to a feature-importance approach in providing supporting evidence for a decision."
        },
        {
            "heading": "5.2 EXPERIMENT 1: EFFECTIVENESS OF TASKS POLICIES",
            "text": "To examine the potential of specifying the reward as an adversarial game and its effect on the quality of learned behaviors, we train several policies by varying the debate coefficient \u03bb.\nQuantitative Evaluation. In Plot 2a, we evaluate the performance of different justifiable policies on a held-out test set during the course of training using weighted importance sampling (WIS) (Sutton & Barto, 2018) 8. Likewise, in Plot 2b, we show the judge\u2019s preference over decisions made by justifiable policies (i.e., \u03bb > 0.0), compared to the baseline policy (i.e., \u03bb = 0.0), when the two were different 9. The observed inherent trade-off between performance and justifiability suggests that tuning the debate coefficient \u03bb is important in practice, and we further elaborate on this in Sec. 6.\nQualitative Evaluation. In addition to quantitative evaluation, we perform qualitative analysis similar to Raghu et al. (2017). Plots 2c and 2d showcase correlation between observed mortality and difference between the optimal doses suggested by policies, and the actual doses prescribed by the clinicians. For all trained policies, the lowest mortality is observed when the difference is near zero, thus further showcasing their potential validity. It is also encouraging to see that the policy trained solely with debate rewards (i.e., \u03bb = 1.0) remains quantitatively and qualitatively competitive in addition to being highly favored by the judge, even though it relies only on debate rewards."
        },
        {
            "heading": "5.3 EXPERIMENT 2: DEBATE-BASED FEEDBACK VS. STATE-BASED FEEDBACK",
            "text": "Because the judge is evaluating justifiability using only proposed evidence, a natural question to ask is how much does this affect the performance and alignment of trained policies. To provide an answer, we train a new judge that evaluates decisions based on the full state, namely J \u2032(at, st). We then use this judge instead of r\u0302d to train a new justifiable policy using state-based feedback, one\n7We discuss different approaches for defining a set of evidence in Sec. 6. 8The behavior policy used in WIS was derived via behavior cloning, further described in App. C.3. 9See Plot 6a in App. D for a detailed breakdown of actions proposed by justifiable policies.\nfor each \u03bb. Given a sample (st, a0, a1, p) \u223c D, we consider a policy \u03c0 aligned to the ground-truth preference if Q\u2217\u03c0(s, ap) \u2265 Q\u2217\u03c0(s, a1\u2212p). The results are shown in Plot 4a for various policies trained with state- and debate-based feedback. We see that, even though the debate-based approach uses only \u223c 13.6% of the full state, the achieved level of alignment remains similar. Likewise, in Plots 3a-3d we observe a matching trend when it comes to performance of the policies, although here we additionally note that the difference seems to increase in favor of policies trained with the state-based feedback as we increase the debate coefficient \u03bb. These results seem promising, as they indicate one can expect to obtain a competitive level of alignment and performance, while requiring the judge to elicit preference over relatively small number of evidence."
        },
        {
            "heading": "5.4 EXPERIMENT 3: EFFECTIVENESS OF ARGUMENTATIVE POLICIES",
            "text": "Preference Recovery Rate. We recall the judge\u2019s accuracy in correctly predicting the preferred action ap from the preference dataset was 65%. To evaluate the effectiveness of argumentative policies, for each sample (st, a0, a1, p) \u223c D we measure the judge\u2019s accuracy in predicting the more justified action ap, when evidence {e} is provided by one of the argumentative policies. In Plot 4b (green) we show the judge\u2019s accuracy when different argumentative agents propose L = 6 required evidence for action ap, averaged across 1000 different debate games. The judge\u2019s accuracy is boosted from 65% to a near 90%, demonstrating that agents can significantly amplify the capabilities of an otherwise limited judge.\nRobust Argumentation. A good supporting evidence is both convincing and not easily refutable by counterarguments. To test the robustness of the proposed evidence, we train 3 adversarial confuser agents 10, each targeting one of the three (frozen) argumentative agents. The goal of the confuser is to convince the judge of the opposing action a1\u2212p. For L = 6, to obtain evidence {e}, the agent (maxmin or self-play) and its corresponding confuser take turns and propose 3 evidence each. The isolated agent is trained to first propose L = 3 evidence, followed by the confuser proposing the remainder (see App. C.2.1 for more details and App. D.3 for additional results). Plot 4b (red) shows judge\u2019s accuracy in this setting for 1000 different debate games. We observe that the isolated argumentative agent (Sec. 5.1) is not resilient to refutations, enabling the confuser to bring the judge\u2019s accuracy down to 38%, effectively convincing it of the opposite of its preference. Differently, both maxmin and self-play agents managed to keep the judge\u2019s accuracy to about 85%."
        },
        {
            "heading": "5.5 EXPERIMENT 4: COMPARISON TO SHAP-BASED EXPLANATIONS",
            "text": "A widely used approach to analyze black-box machine learning models is through feature-attribution techniques. We aim to demonstrate that these explanations may not necessarily be as effective when used as supporting evidence. We focus on the SHAP framework (Lundberg & Lee, 2017), specifically in providing justifications for decisions of various justifiable policies (Sec. 5.2). To justify a\n10Confuser agents use the same architecture as argumentative agents, further described in App. C.\ndecision using SHAP, we select the top 6 state features, as ranked by their Shapely values. For argumentative models, we either run the debate between a0 and a1 (for maxmin and self-play agents) or propose 6 evidence in isolation (for isolated agent). In Plot 4c, we show the judge\u2019s accuracy against different approaches of proposing the evidence {e}, across 1000 comparisons (st, a0, a1, p) sampled uniform-random from the test set. The SHAP-based evidence do improve the accuracy to about 70%, but nevertheless fall short compared to the argumentative agents."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "In this work, we proposed use of a debate-based reward model as a general method of specifying desired behavior and necessary evidence to justify it. In this section, we take a step back and touch upon a couple of aspects that are relevant to future research based on this work.\nEliciting Preferences. The success of debate depends on the human\u2019s capability of judging its outcome, a process which may be affected by one\u2019s beliefs and biases. Extra care must be taken when collecting large datasets of preferences, as belief bias 11 is known to alter the results of judgment in human evaluations (Anderson & Hartzler, 2014) and is amplified in time-limited situations (Evans & Curtis-Holmes, 2005), which human annotators frequently encounter. Furthermore, in our experiments, we assumed existence of a single preference (clinician\u2019s decision). However, preferences collected from multiple raters will undoubtedly yield a higher variability in this respect. Motivated by positive results seen in this work, future research could undertake further studies that examine the effectiveness of eliciting preferences over partial state visibility through use of the debate as an amplification method.\nDefining Arguments is Difficult. We have focused on debate assuming a well-defined argument space. While state features are one possible and easy choice, finding clear and expressive arguments presents a challenge, impacting the applicability of debate. In the context of RL, one potential alternative is considering previous trajectories in support of the current decision. In domains involving text generation, evidence could be defined as sentences, paragraphs, or references supporting a claim. A potentially interesting research direction is to examine the utility of a debate in a domain of human-ai collaboration, specifically in sequential decision-making tasks.\nPractical Considerations. We would like to emphasize that when deploying our framework in practice, it is important to account for the context in which the system is being employed. The performance-justifiability trade-off from our experiments suggests that a special care ought to be given to selecting hyperparameters, in particular, those that weight the importance of the environment rewards and debate-based rewards. In practice, this means that a reward designer has to assess the potentially differing objectives encoded in these values as well as the accuracy of the proxy judge model, prior to the training process.\n11A belief bias represents a tendency to judge the strength of arguments based on the plausibility of the conclusion, instead based on how strongly they support the conclusion."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "We acknowledge a potential misuse of the debate framework for malevolent purposes, such as deception. In the advent of AI systems that surpass human performance in many tasks, novel approaches must be developed which enable defense against malicious agents. While recent results indicate that debate is useful in thwarting malicious use of AI systems, further research is paramount in ensuring one can detect and defend against nefarious purposes. Moreover, the reliance on human judgments introduces the possibility of capturing their biases. Subsequently, designed reward function can incentivize argumentative agents to amplify those biases or learn to leverage them to achieve their objective. This is particularly important for practical considerations (Sec. 6), where a reward designer is tasked with assessing and handling the performance-justifiability trade-off. It is important to note that our results do not hint at solutions for dealing with these challenges; they only demonstrate that the trade-off exists. It is therefore of great importance that future research investigates novel algorithmic approaches and methods to tackle such challenges."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research was, in part, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 project number 467367360. We thank the anonymous reviewers for their valuable comments and suggestions."
        },
        {
            "heading": "A LIST OF APPENDICES",
            "text": "In this section, we provide a brief description of the content provided in the appendices of the paper.\n\u2022 Appendix B provides details about the patient dataset and the defined environment:\no B.1 provides a general background on sepsis; o B.2 provides details about the patient cohort; o B.3 provides details about the environment\u2019s action space; o B.4 provides details about the environment\u2019s reward structure.\n\u2022 Appendix C provides details about the models:\no C.1 provides details about the judge model; o C.2 provides details about the argumentative agents and their confusers; o C.3 provides details about justifiable and baseline agents.\n\u2022 Appendix D provides additional results:\no D.1 provides results involving debates with L = 4 evidence; o D.2 provides further analysis of justifiable agent\u2019s actions; o D.3 provides additional results pertaining to the isolated agent; o D.4 provides results for agents using an alternative definition of the utility function; o D.5 provides analysis of alternative methods for the preference dataset definition."
        },
        {
            "heading": "B DATASET",
            "text": ""
        },
        {
            "heading": "B.1 SEPSIS",
            "text": "Sepsis is a life-threatening condition, defined as severe infection leading to an acute organ dysfunction, which in turn can cause a cascade of changes that damage multiple organ systems (Singer et al., 2016). It is also one of the leading causes of patient mortality (Cohen et al., 2015). Apart from administration of antibiotics and control of the infection source, a critical challenge in management of sepsis lies in the administration of intravenous fluids (IV) and vasopressors (VC). While international efforts attempt to provide a general guidance (Dellinger et al., 2004), clinicians are nevertheless tasked in devising individualized treatments based on specificities of patients.\nA first step towards automated management of septic patients was done in a seminal work of Komorowski et al. (2018). The problem of treating sepsis was tackled by applying reinforcement learning to devise optimal treatment strategies for prescribing doses of intravenous fluids and vasopressors. The authors discretized the possible doses, which resulted in an action-space consisting of 25 distinct choices. The patient data was obtained from the MIMIC-III dataset (Johnson et al., 2016), and the authors extracted a subset of 48 patient features, discretized into 4h time windows. The preliminary results indicated that a learned policy was both quantitatively and qualitatively desirable. Since then, several other works extend and improve upon on this line of research. For example, Raghu et al. (2017) proposed a continuous state-space approach based on deep reinforcement learning, on which we build on in this work. Likewise, Huang et al. (2022) proposed an approach that is based on continuous action-space, thus enabling more granular control of prescribed doses."
        },
        {
            "heading": "B.2 PATIENT COHORT",
            "text": "To assemble our patient cohort, we utilize the MIMIC-III v1.4 database (Johnson et al., 2016), focusing our analysis on patients that fulfill the Sepsis-3 criteria (Singer et al., 2016). Similar as in Komorowski et al. (2018), the sepsis is defined as a suspected existence of infection (indicated by the prescribed antibiotics) paired with a mild evidence of organ dysfunction (indicated by the\nSOFA score \u2265 2). To extract, preprocess and impute the data we utilize the pipeline described in Komorowski et al. (2018), a process which resulted in 18,585 different patients that comprised our cohort. The cohort is split into three chunks of sizes 70%, 15%, 15% representing the train, validation, and test splits. The observed mortality of the entire cohort was slightly above 6%, and the splits were selected to approximately preserve this ratio. The course of a patient treatment is represented as a trajectory consisting of state-action pairs, terminating upon patient discharge or death. The average trajectory length was 13, with 2 being the smallest and 20 being the largest. The state is a 44 dimensional vector, comprised of 40 time-varying continuous and 4 demographic discrete features, shown in Table 1. Patients\u2019 data were discretized using 4h time steps, where variables with multiple measurements within this window were averaged (e.g., heart rate) or summed (e.g., urine output) as appropriate."
        },
        {
            "heading": "B.3 ACTION SPACE",
            "text": "Argumentative Policies. The evidence (action) space of the argumentative policies is defined by the total number of state features, 44 in our case, as listed in Table 1. To prevent the agent from repeating already proposed arguments, we additionally employ action masking, setting the log probability of already presented arguments to negative infinity.\nBaseline and Justifiable Policy. To devise an action space of policies treating sepsis, we follow previous work (Komorowski et al., 2018; Raghu et al., 2017) and focus on managing the total volume of intravenous fluids (IV) and maximum dose of vasopressors (VC) administered to the patient over a 4h discretization window. The dose of each treatment is represented as one of four possible nonnull choices derived from observed doses divided into four quartiles. We additionally define another choice, designated as an option \u2019no drug given\u2019. The combination of these produced 25 possible discrete actions, 5 per each treatment, comprising the action space of the policy."
        },
        {
            "heading": "B.4 REWARDS",
            "text": "Sepsis. To define intermediate and terminal rewards for treating septic patients, following the work of Komorowski et al. (2018), we defined the primary outcome of the treatment via 90-day patient mortality. Therefore, the agent\u2019s objective is to optimize for patient survival. To this end, we issue terminal environment rewards of \u00b115 for every patient discharge and death, respectively. To stabilize the training of a deep-rl policy, we also issue intermediate rewards that are clinically guided, as in Raghu et al. (2017). These rewards are comprised of fairly reliable indicators of the patient\u2019s overall health, namely the SOFA score (measure of organ failure) and a patient\u2019s lactate levels (measure of cell-hypoxia, which is usually higher in septic patients). The rewards for intermediate time steps are then shaped as follows:\nr(st, st+1) = C01(s SOFA t+1 = s SOFA t &s SOFA t+1 > 0) + C2(s SOFA t+1 \u2212 sSOFAt ) + C2tanh(sLactatet+1 \u2212 sLactatet )\nwhere C0, C1 and C2 are tunable parameters which we set to C0 = \u22120.025, C1 = \u22120.125 and C2 = \u22122, following previous work of Raghu et al. (2017). Rewards defined in this way penalize both, the high SOFA scores and lactate values, as well as increases in these quantities."
        },
        {
            "heading": "C MODELS",
            "text": ""
        },
        {
            "heading": "C.1 JUDGE",
            "text": "We defined a judge as a scalar reward function J\u03b8(a, {e}) \u2208 R that quantifies the level of support a decision a has by the set of evidences {e}. The judge is parametrized by weights \u03b8 \u2208 Rd1 of a neural network with two hidden layers of size 256, using parametric relu (He et al., 2015) activation and batch normalization (Ioffe & Szegedy, 2015). The network takes as input values of proposed evidence {e}, a binary mask wherein all elements corresponding to the evidence {e} are assigned a value of one, while the remaining elements are set to zero, as well as a one-hot encoded action. The addition of a binary mask empirically led to a more stable learning. During training, we augment a tuple (st, a0, a1, p) \u223c D with an evidence set of state features sampled uniform-random from the state st i.e., {e} \u223c U , anew for each training batch. To learn the parameter \u03b8, we minimize the cross-entropy loss between preference-predictions and labels from the dataset:\nmin \u03b8\u2208Rd \u2212E(st,{e},a0,a1,p)\u223cD [p \u00b7 logP(a1 \u227b a0, {e}) + (1\u2212 p) \u00b7 logP(a0 \u227b a1, {e})]\nThe learning is done for a total of 100 epochs using a batch size of 64, Adam optimizer and a learning rate of 5e-4."
        },
        {
            "heading": "C.2 ARGUMENTATION",
            "text": "All argumentative models utilize the same network architecture comprised of 2 hidden layers of size 512 with a leaky-relu activation function with slope of 1e-2. The network input consists of a 44 dimensional patient state vector, the decision for which the agent is arguing, as well as a binary mask of arguments proposed thus far. The action space of the agent is represented by all 44 state features (see App. B.3 for more details). For training, we use PPO (Schulman et al., 2017), running the procedure for 1M steps using the Adam optimizer with a learning rate 5e-4 and a batch size of 128. The discount factor was empirically tuned and set to 0.9. The full list of hyperparameters and their considered tuning ranges is given in Table 2. During training, the agent\u2019s policy is stochastic: the evidence is sampled from a categorical distribution defined by its logits. To obtain a deterministic policy used in evaluations, we perform an argmax operator over obtained logits in a particular state.\nC.2.1 ISOLATED AGENT\nWhen investigating robustness of the multi-agent debate, we examine two different setups involving an isolated agent baseline: precommit (reported in the main paper, Plot 4b) and adaptive (reported in the App. D.3, Plot 6b). The former represents an easier case for the agent, but lacks the debate structure. The latter uses a full debate setup as described in Sec. 3.2, but evaluates the isolated agent in a setup slightly different from one it was trained in.\nPrecommit. In this case, an isolated agent is trained to propose evidence {e} of size L/2 that for a given ap maximizes J \u2032(ap, {e}), where J \u2032 is a new judge trained to evaluate L/2 evidence. When\nevaluating robustness (Plot 4b), the agent first proposes L/2 evidence, followed by a confuser agent which proposes the remainder.\nAdaptive. In this case, an isolated agent is trained to propose all L evidence {e} that maximize J (ap, {e}), for a given ap. When evaluating robustness (Plot 6b), the isolated agent and its associated confuser take turns and propose a total of L/2 evidence each."
        },
        {
            "heading": "C.2.2 DEBATE AGENTS",
            "text": "For multi-agent scenarios, both maxmin and self-play agents use the architecture described in the beginning of Sec. C.2, but each modify the underlying optimization pipeline. To train a self-play debate agent, we let the agent argue with a (frozen) copy of itself, updating it every 100k steps. This procedure is then repeated for a fixed number of 500 generations. To train the maxmin debate agent, we parametrize the agent\u2019s opponent with a different set of weights \u03d52 \u2208 Rd2 . The procedure starts by training the main agent for 4k steps, followed by training of its opponent for 100k steps. Like in the previous case, the procedure is repeated for 500 generations. This approach is based on the bi-level optimization and allows for overfitting the opponent to the current version of the main agent, which ensures learning of defense strategies against a very strong adversary."
        },
        {
            "heading": "C.2.3 CONFUSER AGENTS",
            "text": "The evaluation of robustness we presented in Section 5.4 required learning three separate adversaries (one for each argumentative agent), explicitly tasked in providing counterarguments that will confuse the judge. The architecture of these confuser agents is mostly the same as that of the argumentative agents, shown in Table 2. For a sample (st, a0, a1, p) \u223c D, the confuser agent is rewarded positively, whenever the judge is convinced of the alternative (non-preferred) action."
        },
        {
            "heading": "C.3 BASELINE AND JUSTIFIABLE AGENTS",
            "text": "Our approach for learning treatment policies for septic patients is based on continuous state-space models and builds on Raghu et al. (2017). The policy is based on a variant of Deep-Q networks (Mnih et al., 2015), specifically double-deep (Van Hasselt et al., 2016) Q-network, also employing the dueling architecture (Wang et al., 2016), where the estimated action-value function for a pair (s, a) is split into separate value and advantage streams. The final network consists of 2 fullyconnected layers of size 128 using leaky-relu activation functions with slope 1e-2. The learning is done in batches of 256 (s, a, r, s\u2032) tuples sampled from a Prioritized experience replay buffer (Schaul et al., 2015) with a learning rate of 1e-4. Instead of periodically updating the target network, we leverage Polyak averaging with an update coefficient \u03c4 set to 1e-3. The full list of used hyperparameters is given in Table 3. Similar to Raghu et al. (2017), we augment the standard Qnetwork loss. First, we added a regularization term that penalizes Q-values outside the allowed threshold Qthresh = \u00b120. In addition, we clip the target network outputs to \u00b120, which empirically proved to stabilize the learning. The final loss function we used is given by:\nL(\u0398) =E [ (Qdouble-target \u2212Q(s, a; \u0398))2 ] + \u03b2 \u00b7 max(|Q(s, a; \u0398)| \u2212Qthresh, 0)\nQdouble-target =r + \u03b3 \u00b7Q(s\u2032, arg maxa\u2032Q(s\u2032, a\u2032; \u0398);\u0398\u2032)\nWhere \u03b2 is a user-specified coefficient we set to \u03b2 = 5.0 in all our experiments following Raghu et al. (2017). The learning is done for a total of 25k iterations, evaluating the policy every 50 iterations using weighted importance sampling (WIS) (Sutton & Barto, 2018) on a held-out test set.\nBehavioral Policy. The calculation of the importance sampling ratio requires access to a so-called behavioral policy that generated offline samples. To obtain it, we train a behavior-cloning (BC) clinician policy to take in a state st from the patient cohort and predict the action at taken by the human clinician, minimizing the cross-entropy loss over the training dataset. The network consists of two fully-connected layers of size 64. We run the training with the Adam optimizer, using a learning rate of 1e-3 and a weight decay set to 1e-1 for a total of 100 epochs with a batch size of 64."
        },
        {
            "heading": "D ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 SHORTER DEBATES",
            "text": "In the main text, we have seen positive results involving debate using 6 arguments, which amounts to \u223c 13% of the entire state. In this section, we want to further examine the effectiveness of debate when limiting the number of evidence to L = 4, amounting to 9% of the entire state.\nThe first question that arises when further limiting the amount of state visibility during debate is the impact it has on the quality of learned task policies. In Plots 5a-5d, we evaluate the performance of different justifiable policies on a held-out test set during the course of training using weighted importance sampling, for the case of 4 and 6 evidence limit. While the reduced number of evidence exposes the judge to only 9% of the entire state, we can see that the achieved performance is comparable to the case where L = 6. Furthermore, in Plot 5f, we also confirm that trained policies are significantly more preferred to the baseline policy, a trend equivalent to the one we saw in the main text. Apart from these quantitative evaluations, in Plots 5g and 5h, we show the qualitative analysis of patient mortality from Sec. 5.2. We confirm that the lowest mortality is observed when clinician prescribed doses recommended by justifiable policies, thus further signaling potential validity of policies trained with rewards stemming from debates with L = 4 evidence.\nTo evaluate the effectiveness of argumentative agents, we repeat the evaluation from Sec. 5.4. When exposed to 4 randomly selected evidence, the judge trained via the procedure outlined in App. C.1 achieves accuracy of \u223c59%. Plot 5e shows the judge\u2019s accuracy when different argumentative agents propose L = 4 evidence 12, averaged over 1000 samples (st, a1, a2, p) \u223c D. Without the confuser, all three agents achieve performance similar to the case of L = 6 evidence, boosting the judge\u2019s accuracy to almost 90%. In the setup involving an adversary, agents propose a total of 2 evidence each before the judge evaluates the outcome. The observed trend is also similar to the one from the main text (Sec. 5.4)."
        },
        {
            "heading": "D.2 PREFERENCE BREAKDOWN",
            "text": "To gain a better understanding about the behavior of learned justifiable policies, we further examine actions they propose. In particular, Plot 6a shows the percent of times an action from the justifiable policy was preferred to that of the baseline policy (JP), percent of times when the action of the baseline policy was preferred to the action of the justifiable policy (BP), and percent of time when the two were equally preferred (EP) 13. We see that the number of actions proposed by the justifiable agent and the baseline agent that are equally preferred decreases as the parameter \u03bb is increased: for \u03bb = 0.25 two agents chose the equally justifiable action about 49% of the times, for \u03bb = 0.50 this number drops to 40%, whereas for \u03bb = 0.75 and \u03bb = 1.0 this number is 36% and 35% respectively. Out of the remaining actions, the ones from justifiable policies were increasingly more preferred, as the parameter \u03bb was increased.\nD.3 ISOLATED AGENT SETUP\nIn App. C.2.1, we described two setups which we consider when evaluating robustness of the isolated agent, namely precommit and adaptive. In Plot 6b, we show the accuracy of the judge in predicting the preferred action when evidence was proposed by an isolated agent trained with one of these approaches for 1000 different debate games. Without the confuser, both approaches amplify the capabilities of the judge, performing roughly equivalent. When faced with a confuser agent, the precommit approach performs better, since in the adaptive case, the agent is evaluated in a debate-like setup, which was not accounted for during training. In both cases, however, we observe that the isolated agent is not robust to an adversary."
        },
        {
            "heading": "D.4 ALTERNATIVE UTILITY FUNCTIONS",
            "text": "In Sec. 3.2, we have defined the utility function U to output binary values {\u22121, 0,+1} based on the justifiability rewards obtained from the judge. One might ask if there are alternative ways of\n12Like in the main text, we use the precommit setup for the isolated agent (see App. C.2.1 for more details). 13In our experiments, the judge deemed two actions equally justifiable whenever they were the same.\ndefining U that preserve the zero-sum structure and potentially positively influence the learning. In this section, we consider one particular alternative that defines the utility function based on the difference between predicted rewards, which might provide a more informative learning signal. In particular, we set u1(nL) = \u2212u2(nL) = U(at, aBt , {enL}), with U defined as U(at, aBt , {e}) = J (at, {e}) \u2212 J (aBt , {e}). We rerun the experiments from Sec. 5.4 for maxmin and self-play agents and show preference recovery success and robustness results in Plot 6c. We can see that the two approaches perform similarly in situations which do not involve a confuser agent. However, it seems that defining the utility using the difference in judge\u2019s rewards leads to slightly lower scores when debate agents are faced with an adversary."
        },
        {
            "heading": "D.5 ALTERNATIVE PREFERENCE DATASET",
            "text": "In Sec. 5.1 we construct a preference dataset by matching every pair (st, at) from the cohort with an alternative action ar \u223c U(A), ar \u0338= at sampled uniform-random from the action space. In this section, we examine additional strategies one could take when constructing such a synthetic dataset. In particular, we consider the random strategy we just described, paired with two alternative ones, namely exhaustive and offset. The exhaustive strategy pairs at with all possible alternative actions, 24 in total. The offset strategy pairs at with an alternative action that is in its neighborhood. To define a neighborhood, we recall that there are a total of 5 choices for both, vasopressors (VC) and intravenous fluids (VC). Therefore, we can write at = 5 \u2217 IV + VC, where IV,VC \u2208 {0, 1, 2, 3, 4}. To obtain an alternative action, we consider changing IV and VC by an offset sampled uniformrandom from a set {\u22121, 0, 1}, for both IV and VC. We then train a new judge for each of the datasets and show its accuracy in Plot 6d. The exhaustive variant represents the most informative, but also unrealistically large, dataset. The random variant represents somewhat of a \u201cmiddle ground\u201d in terms of dataset difficulty. Lastly, the offset variant represents the most difficult case, as differences between two actions are more nuanced. However, while the achieved accuracies reflect the difficulty of the corresponding dataset, the capabilities of a trained judge model are roughly the same."
        }
    ],
    "title": "REWARD DESIGN FOR JUSTIFIABLE SEQUENTIAL DECISION-MAKING",
    "year": 2024
}