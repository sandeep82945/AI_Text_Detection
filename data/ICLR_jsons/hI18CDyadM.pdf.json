{
    "abstractText": "Local motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images and wasting unnecessary computation on sharp pixels, especially for high-resolution images. This paper aims to adaptively and efficiently restore high-resolution locally blurred images. We propose a local motion deblurring vision Transformer (LMD-ViT) built on adaptive window pruning Transformer blocks (AdaWPT). To focus deblurring on local regions and reduce computation, AdaWPT prunes unnecessary windows, only allowing the active windows to be involved in the deblurring processes. The pruning operation relies on the blurriness confidence predicted by a confidence predictor that is trained end-to-end using a reconstruction loss with Gumbel-Softmax re-parameterization and a pruning loss guided by annotated blur masks. Our method removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements compared to state-of-the-art methods. In addition, our approach substantially reduces FLOPs by 66% and achieves more than a twofold increase in inference speed compared to Transformer-based deblurring methods. We will make our code and annotated blur masks publicly available.",
    "authors": [],
    "id": "SP:3db5ff7649e8f0412fd05fde54ca13476cd6bff0",
    "references": [
        {
            "authors": [
                "Daniel Bolya",
                "Cheng-Yang Fu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Christoph Feichtenhofer",
                "Judy Hoffman"
            ],
            "title": "Token Merging: Your vit but faster",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Liangyu Chen",
                "Xin Lu",
                "Jie Zhang",
                "Xiaojie Chu",
                "Chengpeng Chen"
            ],
            "title": "HINet: Half instance normalization network for image restoration",
            "year": 2021
        },
        {
            "authors": [
                "Sung-Jin Cho",
                "Seo-Won Ji",
                "Jun-Pyo Hong",
                "Seung-Won Jung",
                "Sung-Jea Ko"
            ],
            "title": "Rethinking coarseto-fine approach in single image deblurring",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Lanqing Guo",
                "Siyu Huang",
                "Ding Liu",
                "Hao Cheng",
                "Bihan Wen"
            ],
            "title": "Shadowformer: Global context helps image shadow removal",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Orest Kupyn",
                "Volodymyr Budzan",
                "Mykola Mykhailych",
                "Dmytro Mishkin",
                "Ji\u0159\u0131\u0301 Matas"
            ],
            "title": "Deblurgan: Blind motion deblurring using conditional adversarial networks",
            "year": 2018
        },
        {
            "authors": [
                "Orest Kupyn",
                "Tetiana Martyniuk",
                "Junru Wu",
                "Zhangyang Wang"
            ],
            "title": "DeblurGAN-v2: Deblurring (orders-of-magnitude) faster and better",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Haoying Li",
                "Ziran Zhang",
                "Tingting Jiang",
                "Peng Luo",
                "Huajun Feng"
            ],
            "title": "Real-world deep local motion deblurring",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jiezhang Cao",
                "Guolei Sun",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "SwinIR: Image restoration using swin transformer",
            "venue": "ICCVW,",
            "year": 2021
        },
        {
            "authors": [
                "Jingyun Liang",
                "Guolei Sun",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Mutual affine network for spatially variant kernel estimation in blind image super-resolution",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4096\u20134105,",
            "year": 2021
        },
        {
            "authors": [
                "Youwei Liang",
                "Chongjian Ge",
                "Zhan Tong",
                "Yibing Song",
                "Jue Wang",
                "Pengtao Xie"
            ],
            "title": "Not all patches are what you need: Expediting vision transformers via token reorganizations",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin Transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Lingchen Meng",
                "Hengduo Li",
                "Bor-Chun Chen",
                "Shiyi Lan",
                "Zuxuan Wu",
                "Yu-Gang Jiang",
                "Ser-Nam Lim"
            ],
            "title": "AdaViT: Adaptive vision transformers for efficient image recognition",
            "year": 2022
        },
        {
            "authors": [
                "Seungjun Nah",
                "Tae Hyun Kim",
                "Kyoung Mu Lee"
            ],
            "title": "Deep multi-scale convolutional neural network for dynamic scene deblurring",
            "year": 2017
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Benlin Liu",
                "Jiwen Lu",
                "Jie Zhou",
                "Cho-Jui Hsieh"
            ],
            "title": "Dynamicvit: Efficient vision transformers with dynamic token sparsification",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Wenqi Ren",
                "Jiawei Zhang",
                "Jinshan Pan",
                "Sifei Liu",
                "Jimmy Ren",
                "Junping Du",
                "Xiaochun Cao",
                "Ming-Hsuan Yang"
            ],
            "title": "Deblurring dynamic scenes via spatially varying recurrent neural networks",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Schelten",
                "Stefan Roth"
            ],
            "title": "Localized image blur removal through non-parametric kernel estimation",
            "venue": "In ICCV,",
            "year": 2014
        },
        {
            "authors": [
                "Xin Tao",
                "Hongyun Gao",
                "Xiaoyong Shen",
                "Jue Wang",
                "Jiaya Jia"
            ],
            "title": "Scale-recurrent network for deep image deblurring",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Prajit Ramachandran",
                "Aravind Srinivas",
                "Niki Parmar",
                "Blake Hechtman",
                "Jonathon Shlens"
            ],
            "title": "Scaling local self-attention for parameter efficient visual backbones",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhendong Wang",
                "Xiaodong Cun",
                "Jianmin Bao",
                "Wengang Zhou",
                "Jianzhuang Liu",
                "Houqiang Li"
            ],
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "year": 2022
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing,",
            "year": 2004
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiyang Dai",
                "Bin Xiao",
                "Lu Yuan",
                "Jianfeng Gao"
            ],
            "title": "Focal self-attention for local-global interactions in vision transformers",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Hongxu Yin",
                "Arash Vahdat",
                "Jose M Alvarez",
                "Arun Mallya",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "title": "A-ViT: Adaptive tokens for efficient vision transformer",
            "year": 2022
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "MingHsuan Yang",
                "Ling Shao"
            ],
            "title": "Multi-stage progressive image restoration",
            "year": 2021
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "MingHsuan Yang"
            ],
            "title": "Restormer: Efficient transformer for high-resolution image restoration",
            "year": 2022
        },
        {
            "authors": [
                "Jiawei Zhang",
                "Jinshan Pan",
                "Daoye Wang",
                "Shangchen Zhou",
                "Xing Wei",
                "Furong Zhao",
                "Jianbo Liu",
                "Jimmy Ren"
            ],
            "title": "Deep dynamic scene deblurring from optical flow",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2021
        },
        {
            "authors": [
                "Shanghang Zhang",
                "Xiaohui Shen",
                "Zhe Lin",
                "Radom\u0131\u0301r M\u011bch",
                "Joao P Costeira",
                "Jos\u00e9 MF Moura"
            ],
            "title": "Learning to understand image blur",
            "year": 2018
        },
        {
            "authors": [
                "Shangchen Zhou",
                "Chongyi Li",
                "Chen Change Loy"
            ],
            "title": "LEDNet: Joint low-light enhancement and deblurring in the dark",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Nah"
            ],
            "title": "Under review as a conference paper at ICLR 2024 Table 4: Quantitative comparisons on the global deblurring dataset. All the methods are trained with the ReLoBlur dataset",
            "year": 2023
        },
        {
            "authors": [
                "Nah"
            ],
            "title": "2017) together improves both the local and global deblurring performances, as shown in Table 7. Table 7: The effectiveness of joint training with the ReLoBlur dataset Li et al. (2023) and the GoPro dataset",
            "venue": "Nah et al",
            "year": 2017
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "backbones with baselines, we provide results for baselines trained with blur masks. When training the baseline methods Zamir et al",
            "venue": "Restormer Zamir et al",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "improve. However, their performances are still inferior to ours and they need more time to infer. This proves the effectiveness of our backbone and the window pruning strategy",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Local motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images and wasting unnecessary computation on sharp pixels, especially for high-resolution images. This paper aims to adaptively and efficiently restore high-resolution locally blurred images. We propose a local motion deblurring vision Transformer (LMD-ViT) built on adaptive window pruning Transformer blocks (AdaWPT). To focus deblurring on local regions and reduce computation, AdaWPT prunes unnecessary windows, only allowing the active windows to be involved in the deblurring processes. The pruning operation relies on the blurriness confidence predicted by a confidence predictor that is trained end-to-end using a reconstruction loss with Gumbel-Softmax re-parameterization and a pruning loss guided by annotated blur masks. Our method removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements compared to state-of-the-art methods. In addition, our approach substantially reduces FLOPs by 66% and achieves more than a twofold increase in inference speed compared to Transformer-based deblurring methods. We will make our code and annotated blur masks publicly available."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Contrary to global motion blur, which typically affects an entire image (Zhang et al., 2018), local motion blur is confined to specific regions within an image. Such local blur is generally the result of object movements captured by stationary cameras (Li et al., 2023; Schelten & Roth, 2014). Applying global deblurring methods to images featuring local motion blur inevitably introduces unwanted distortions in regions that were originally sharp, as illustrated in Figure 1. Moreover, the processing of sharp regions, which is not required in this context, leads to unnecessary computational expenditure. This wastage becomes particularly noticeable when dealing with high-resolution inputs.\nExisting local motion deblurring methods, such as LBAG (Li et al., 2023), address the issue by detecting local blur regions for targeted processing. While LBAG uses a gate structure to mitigate the deblurring impact on non-blurred regions, it still involves unnecessary computations as the entire image is processed by the network. In addition, the method\u2019s reliance on a Convolutional Neural Network (CNN) architecture leads to limitations, as the interactions between the image and convolution kernels are content-independent and ill-suited for modeling long-range dependencies. The Transformer architecture (Vaswani et al., 2017), which excels at long-range pixel interactions, has been successfully applied in several image restoration problems (Liang et al., 2021a; Guo et al., 2023; Wang et al., 2022; Zamir et al., 2022). However, Transformers for deblurring tasks Wang et al. (2022); Zamir et al. (2022) usually conduct attention operations on every token, which not only distorts sharp tokens in locally blurry images (as shown in Figure 1) but also require substantial memory and extend inference time. To mitigate these computational demands, strategies such as channel inter-dependence operations Liang et al. (2021b), window-division (Wang et al., 2022; Yang et al., 2021) and token-reducing (Bolya et al., 2022; Liang et al., 2022; Meng et al., 2022; Rao et al., 2021; Yin et al., 2022) have been proposed.\nDrawing inspiration from preceding work, in this paper, we propose a U-shaped local motion deblurring vision Transformer (LMD-ViT) with adaptive window pruning Transformer blocks (AdaWPT) as its core component. Our proposed LMD-ViT is the first to apply sparse vision Transformer to the local motion deblurring task. The core block, AdaWPT, focuses on locally blurred regions rather than global regions, which is made possible by removing windows unrelated to blurred areas, surpassing prior state-of-the-art methods in both deblurring performance and efficiency. Specifically, we first train a confidence predictor which is able to automatically predict the confidence of blurriness of feature maps. It is trained end-to-end by a reconstruction loss with Gumbel-Softmax reparameterization, and a pruning loss guided by our elaborately annotated local blur masks. We then design a decision layer that provides binary decision maps in which \u201c1\u201d represents the kept tokens in blur-related regions that require processing during inference while \u201c0\u201d represents the abandoned tokens in the other regions that can be removed. We also propose a window pruning strategy with Transformer layers. In detail, we apply window-based multi-head self-attention (W-MSA) Wang et al. (2022) and window-based feed-forward layers rather than enforcing these Transformer layers globally. Only the selected windows are forwarded to these window-based Transformer layers, preventing unnecessary distortion of sharp regions while also reducing computations. To further enhance content interactions, AdaWTP employs shifted window mechanism (Liang et al., 2021a; Liu et al., 2021) and position embeddings (Wang et al., 2022) among the Transformer layers. Furthermore, we insert AdaWTP in LMD-ViT under different scales and receptive fields. Therefore, AdaWTP conducts coarse pruning of windows in low-resolution layers and more refined pruning in high-resolution layers, as shown in Figure 1, which achieves a balance between computational complexity and deblurring performance. Compared to the CNN-based local deblurring methods, our proposed method achieves improved visual and quantitative performances. Compared to Transformerbased local deblurring methods, we speed up inference.\nTo summarize, our main contributions are: 1) the first sparse vision Transformer framework for local motion deblurring, LMD-ViT, which utilizes an adaptive window pruning strategy to focus computation on localized regions affected by blur and achieve efficient blur reduction without caus-\ning unnecessary distortion to sharp regions; 2) a sophisticated blurriness prediction mechanism, integrating a confidence predictor and decision layer to effectively distinguish between sharp and blurry regions; 3) carefully annotated local blur masks for ReLoBlur dataset (Li et al., 2023), which improve the performance of local deblurring methods. We explain more background information of our method in Appendix A."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "2.1 MODEL ARCHITECTURE",
            "text": "The architecture of our local motion deblurring vision Transformer (LMD-ViT) is shown at the top of Figure 2, which is a U-shaped network with an encoder stage, a bottleneck stage, and a decoder stage with skip connections. An in-projection/out-projection layer is placed at the beginning/end of the network to extract RGB images to feature maps or convert feature maps to RGB images. The encoder, bottleneck, and decoder include a series of adaptive window-token pruning Transformer blocks (AdaWPT) and down-sampling/up-sampling layers. As a key component, AdaWPT removes local blurs by a window pruning strategy with a confidence predictor, a decision layer, and several Transformer layers. It is trained with a reconstruction loss (LR) and a pruning loss (LP ) constrained by our carefully annotated blur masks. AdaWPT can be applied in any encoder/decoder/bottom-neck block and is flexible to prune at different resolutions. As shown in Figure 1, in our proposed LMDViT, windows are pruned coarsely in low-resolution blocks and finely in high-resolution blocks. This strikes a balance between computational complexity and accuracy. We detail the architectures and model hyper-parameters of LMD-ViT in Appendix B."
        },
        {
            "heading": "2.2 ADAPTIVE WINDOW PRUNING TRANSFORMER (ADAWPT)",
            "text": "As shown at the bottom of Figure 2, an adaptive window pruning Transformer block (AdaWPT) includes several AdaWPT layers. During training, each AdaWPT layer comprises a confidence predictor, a decision layer, a feature shift/reverse block, and several Transformer layers such as windowbased multi-head self-attention (W-MSA) (Wang et al., 2022), window-based locally-enhanced feedforward layer (W-LeFF), and layer normalization (LN). AdaWPT is responsible for predicting the confidence of blurriness by a confidence predictor, generating pruning decisions from a decision layer and pruning windows. It only allows the unpruned windows to be fed into the window-based Transformer layers, significantly reducing computational costs and keeping sharp regions undistorted. Besides, a feature shift/reverse block is inserted before/after pruning to promote feature interactions. In the inference phase, a window compound layer is incorporated to integrate the aban-\ndoned windows and selected windows into a unified feature map. We detail the modules in AdaWPT in the following paragraphs."
        },
        {
            "heading": "2.3 CONFIDENCE PREDICTOR",
            "text": "The confidence predictor predicts the confidence of blurriness for the input features Xi \u2208 Rn. Tokens with higher confidence are more likely to be kept and others are removed. Following Rao et al. (2021), the confidence predictor employs MLP layers to produce feature embeddings e and predict the confidence map C using Softmax:\nCi = Softmax(ei), ei = Concat ( MLP ( MLP(Xi), \u2211N j=n Dj \u00b7 MLP(Xij)\u2211N\nj=n Dj\n)) , i \u2208 N+, (1)\nwhere D, initialized using I, is the one-hot decision map to prune windows, which will be introduced in Section 2.4. i denotes the ith AdaWPT block. The confidence predictor is trained using an endto-end reconstruction loss (introduced in Section 2.5) with Gumbel-Softmax parameterization (see Section 2.4), together with a pruning loss (introduced in Section 2.5) guided by our blurry mask annotations (introduced in Section 2.4.2)."
        },
        {
            "heading": "2.4 DECISION LAYER",
            "text": "The decision layer samples from the blurriness confidence map to generate binary pruning decisions D, in which \u201c1\u201d represents tokens to be kept while \u201c0\u201d represents tokens to be abandoned. Although our goal is to perform window pruning, it is not practical to remove the zero tokens directly, for the absence of tokens halts backward propagation, and the different removal instances make parallel computing impossible in end-to-end training. To overcome this issue, we design the decision layer for training and testing, respectively.\nIn training, we apply the Gumbel-Softmax re-parameterization (Jang et al., 2017) as the decision layer, since it assures the gradients to flow through the network when sampling the training decision map Dtr from the training confidence map Ctr:\nDtri (x, y) = Gumbel-Softmax(C tr(x, y)), (2)\nwhere (x,y) represents the coordinate of each window.\nIn testing, we apply Softmax with a constant threshold \u03b2 as the decision layer:\nDtei (x, y) =\n{ 0, H ( Softmax ( Ctei(x, y)) )) < \u03b2\n1, H ( Softmax ( Ctei(x, y)) )) \u2265 \u03b2,\n(3)\nwhere H functions to sparse the output of Softmax to 1/0 when it is larger/fewer than \u03b2. The abandoned windows, that is, windows irrelevant to local blurs, are further set to zero by\nX\u2032i = Di \u00b7Xi. (4)"
        },
        {
            "heading": "2.4.1 EFFICIENT TRANSFORMER LAYERS WITH WINDOW PRUNING STRATEGY",
            "text": "Considering the quadratic computation costs with respect to a large number of tokens of highresolution images, we employ Transformer layers in non-overlapping windows rather than in a global manner to accelerate training and inference. Specifically, we choose W-MSA layer (Wang et al., 2022) for the self-attention (SA) operation. For the feed-forward structure, we develop a window-based locally-enhanced feed-forward layer (W-LeFF) as an alternative to LeFF (Wang et al., 2022) which modifies global tokens. W-LeFF uses a 3 \u00d7 3 convolution with stride 1 and reflected padding 1 in independent windows. The reflected padding ensures the continuity at the window borders and obtains almost the same performance as LeFF (Wang et al., 2022) (we compare the performance of LeFF Wang et al. (2022) and W-LeFF in Appendix E.1).\nTo enable parallel computing, we regard each window as a token group and prune based on windows. Each window owns a 0/1 decision, which is calculated by average pooling the decision map with a threshold s. Windows with an average \u2265 s are regarded as the kept windows and the others as the abandoned windows. To promote content interactions among non-overlapping windows, we also\napply relative position encoding (Wang et al., 2022) in the attention module and shift/reverse the windows by half the window size at the beginning/end of the AdaWTP.\nTo accomplish both differentiable training and fast inference, we propose different pruning strategies with W-MSA and W-LeFF in training and testing, respectively. In training, to ensure backpropagation, all the windows including the abandoned windows go through W-MSA, W-LeFF, and LN sequentially to generate training features Xti+1:\nXtri+1 \u2032 = W-MSA(LN(Xtri \u2032 )) +Xtri \u2032 , Xtri+1 = W-LeFF(LN(X tr i+1 \u2032 )) +Xtri+1 \u2032 . (5)\nIn testing, only the kept windows are processed by Transformer layers, which release a great number of unnecessary tokens to deblur. To enable future stages to perform global operations, we mend the abandoned windows to their original locations to compound a complete testing feature map Xtei+1 :\nXtei+1 \u2032 = W-MSA(LN(Xtei \u2032 \u2265 s)) +Xtei \u2032 , Xtei+1 = W-LeFF(LN(X te i+1 \u2032 \u2265 s)) +Xtei+1 \u2032 , (6)\nwhere s denotes the threshold in the average pooling operation (AVG)."
        },
        {
            "heading": "2.4.2 BLUR REGION ANNOTATION",
            "text": "To obtain the ground-truth local blur mask for supervised training of the confidence predictor, we carefully annotate local blur masks of the ReLoBlur dataset (Li et al., 2023). We mark the blurred regions with pixel value 1 and others with 0, as shown in Figure 1. To simplify the annotation process, we firstly select moving objects by the EISeg software1, which can automatically segment different components within each object. Then, we manually refine the blurry regions by 1) expanding blurry components that were not included in the initial annotation; and 2) removing redundant sharp regions. Due to the page limit, we provide more details of blur mask annotation in Appendix C."
        },
        {
            "heading": "2.5 LOSS FUNCTIONS",
            "text": "To guide adaptive window pruning and locally deblurring, we propose a pruning loss and combine it with a weighted reconstruction loss to form the total loss:\nL = LP + LR. (7)\nPruning loss. We propose a pruning loss to constrain the blurriness confidence prediction:\nLP = \u03bb0 n\u2211\ni=1\nCross-Entropy(Ci,Down-Samplei(M)), (8)\nwhere \u03bb0=0.01, and the blur mask M is down-sampled to match the resolution of the confidence maps, and i indicates a confidence predictor\u2019s location.\nReconstruction loss. To focus training on local blurred regions, following Li et al. (2023), we apply weights w on the reconstruction loss, which is a combination of L1 loss, SSIM loss, and FFT loss:\nLR = wL\u2032R(M \u00b7 S\u2032,M \u00b7 S) + (1\u2212 w)L\u2032R((1\u2212M) \u00b7 S\u2032, (1\u2212M) \u00b7 S), L\u2032R = L1(S\u2032,S) + \u03bb2SSIM(S\u2032,S) + \u03bb3FFT (S\u2032,S), (9)\nwhere S, and S\u2032 denote the sharp ground truth, and the deblurred output. w = 0.8, \u03bb1=\u03bb2=1.0, and \u03bb3=0.1 denote the weights of the loss functions."
        },
        {
            "heading": "3 EXPERIMENTS AND ANALYSES",
            "text": ""
        },
        {
            "heading": "3.1 EXPERIMENTAL SETTINGS",
            "text": "We train LMD-ViT using AdamW optimizer (Kingma & Ba, 2015) with the momentum terms of (0.9, 0.999), a batch size of 12, and an initial learning rate of 2\u00d710\u22124 that is updated every 2k steps by a cosine annealing schedule (Loshchilov & Hutter, 2017). We set the window size of AdaWPT\n1EISeg is provided by https://github.com/PaddlePaddle/PaddleSeg/tree/release/ 2.7/EISeg\nto 8 \u00d7 8, and the initial embedded dim to 32 which is doubled after passing each down-sampling layer. LMD-ViT is trained on the GoPro dataset (Nah et al., 2017) and ReLoBlur dataset (Li et al., 2023) together, in order to enable both local and global motion deblurring. The sampling ratio of the GoPro training data (Nah et al., 2017) and the ReLoBlur training data (Li et al., 2023) is close to 1:1. For a fair comparison, we train the baseline methods using the same datasets and cropping strategy. The model configurations of the compared deblurring methods follow their origin settings.\nWe evaluate our proposed LMD-ViT and baseline methods on the ReLoBlur testing dataset (Li et al., 2023) with the full image size of 2152\u00d71436 on 1 Nvidia A100 GPU. In testing, the inputs are solely of locally blurred images without accompanying blur masks. In addition to the commonlyused PSNR and SSIM (Wang et al., 2004) metrics, we calculate weighted PSNR and weighted SSIM Li et al. (2023) to better assess the local deblurring performance. We provide the evaluation results in the following sections and Appendix F. To measure model efficiency, we report the inference time, FLOPs, and model parameters."
        },
        {
            "heading": "3.2 EXPERIMENTAL RESULTS",
            "text": "Evaluations on public datasets. We first compare the proposed LMD-ViT with both CNN-based methods (Nah et al., 2017; Kupyn et al., 2019; Chen et al., 2021; Cho et al., 2021; Li et al., 2023)\nand Transformer-based methods (Zamir et al., 2022; Wang et al., 2022) on the ReLoBlur dataset (Li et al., 2023) for local motion deblurring. As depicted in Figure 1 and Figure 3, LMD-ViT exhibits superior performance compared to other state-of-the-art methods, producing clearer outputs with enhanced details. Notably, the white stripes on the student\u2019s suit and the mobile phone show significant blur reduction without artifacts and closely resemble the ground truth. We notice that the number of preserved windows slightly exceeds the number of windows in the annotated blurry areas. This is to ensure that the preserved windows effectively cover as much of the blurry area as possible. Quantitative evaluation results are presented in Table 1, which bolds the best local deblurring performance. Compared with CNN-based methods, our proposed LMD-ViT achieves an improvement of 0.50 dB in PSNR and 0.95 dB in weighted PSNR, while maintaining a comparable or even faster inference speed. Compared with Transformer-based methods, LMD-ViT demonstrates significant reductions (-66%) in FLOPs and inference time without sacrificing performance (PSNR +0.28dB), thanks to our adaptive window pruning modules. The PSNR and SSIM scores of LMD-ViT also exceed other Transformer-based deblurring methods, because our proposed pruning strategy prohibits the network from destroying sharp regions. However, global deblurring methods (i.e., Uformer (Wang et al., 2022) and Restormer (Zamir et al., 2022)) treat every region equally and may distort or blur the sharp regions inevitably.\nAdditionally, our proposed LMD-ViT could deblur globally. We evaluate LMD-ViT\u2019s global deblurring performance on the GoPro testing dataset (Nah et al., 2017). Our proposed LMD-ViT obtains comparable deblurring performance to global deblurring methods. We also observed that with a globally blurry input, LMD-ViT does not prune windows because the confidence of blurriness of all regions is nearly 100%. We explain the global deblurring performances in detail in Appendix D.\nUser study on real-world photos. To validate the effectiveness of our proposed model in real-world locally blurred images, we use a static Sony industrial camera and a static Fuji XT20 SLR camera to capture 18 locally blurred RGB images with a resolution of 6000\u00d74000. We conduct a comparison between our method and the top 3 methods listed in Table 1 on these images. Since the ground truths for these blurred images are not available, we conducted a user study involving 30 participants who are passionate about photography. Each participant is presented with randomly selected deblurred images and asked to choose the most visually promising deblurred image. As shown in Figure 4, our proposed method exhibits robust performance on real-world locally blurred images, with sharper\nreconstructed edges and consistent content. Moreover, it is the most preferred method among the participants when compared to other approaches."
        },
        {
            "heading": "3.3 ANALYSES",
            "text": "To further analyze the ability of our proposed method, we analyze the effectiveness of the window pruning strategy and blur mask annotations in this section, respectively. Due to the limited space, the analyses of W-LeFF and the number of feature channels are illustrated in Appendix E."
        },
        {
            "heading": "3.4 WINDOW PRUNING STRATEGY",
            "text": "We analyze the effects of our window pruning strategy in three aspects: the number of pruning blocks, the pruning threshold \u03b2 during inference, and pruning precision.\nThe number of pruning blocks. We fix the pruning threshold \u03b2 = 0.5 and change the number of pruning blocks. For blocks without pruning, we force the decision maps D in Equation 4 to be the all-one matrix. From line 1 to line 5 of Table 2, we find that pruning more blocks results in fewer model parameters, a higher inference speed, and more dropping scores. Notably, although we apply a window pruning mechanism in all 9 blocks, the scores outperform other baseline models listed in Table 1. Additionally, we conduct visual comparisons of all-pruned architecture (line 1) with non-pruning architecture (line 5), as shown in Figure 6. In the all-pruned network, most of the sharp regions are pruned and are not destroyed. In contrast, the sharp regions turn blurry or distorted when processed by the non-pruned network. This indicates that our adaptive window pruning mechanism can prevent the sharp regions from being destroyed. The non-pruning network treats every region\nequally like global deblurring networks (e.g., MIMO-UNet, (Cho et al., 2021), Uformer (Wang et al., 2022) and Restormer (Zamir et al., 2022)) that may harm the sharp regions inadvertently.\nPruning threshold \u03b2. We fix the pruning blocks and adjust the pruning threshold \u03b2 from 0.2 to 0.7 with 0.1 as the interval. Comparing line 1, lines 6 to line 10 in Table 2, we find that the testing performance slightly varies with different pruning thresholds \u03b2, resulting in different confidence levels and decision boundaries. Inferring with a lower nor a higher pruning threshold is neither reasonable, because the former makes the confidence predictor more inclusive, which potentially leads to fewer sharp windows to be pruned and a slower inference speed, while the latter makes the confidence predictor more conservative, which leads to faster inference speed but filters out the necessary blurry windows. To achieve a balance, we choose \u03b2 = 0.5 as it obtains relatively high evaluation scores and fast inference speed.\nPruning precision Our proposed adaptive window pruning Transformer block (AdaWPT) enables blur region selection as well as removing sharp windows. From the first pruning block (AdaWPT 1) to the last pruning block (AdaWPT 9), the preserved patches gradually correspond to the groundtruth blur annotations, as shown in Figure 1. To verify whether blurred regions are mistakenly pruned, we calculate the precision of window pruning at the last pruning block: Precision = TP/(TP + FP ), where TP refers to the number of pixels that our model correctly predicts to be blurry and preserved to process. FP refers to the number of pixels that our model incorrectly identifies to be blurry but are actually sharp, which are also pixels that our model accidentally preserved but should be actually removed. The average precision per block in the ReLoBlur testing dataset (Li et al., 2023) is 96.8%, suggesting that the preserved windows cover most blurred regions and AdaWPT significantly prunes sharp regions."
        },
        {
            "heading": "3.5 BLUR MASK ANNOTATION",
            "text": "To verify the effectiveness of our blur mask annotations, we conduct experiments on a local deblurring method LBAG (Li et al., 2023) and our proposed LMD-ViT, as illustrated in Table 3. Comparing line 1 and line 2, our manually annotated blur masks enhance both image similarity and blur detection precision for LBAG Li et al. (2023). Figure 5 also shows that the blur detection restrained by LBFMG\u2019s2 masks leads to more detection errors of local blurs. When testing LMD-ViT, our blur mask annotations improve the evaluation metrics obviously. The holes and the noise in LBFMG\u2019s2 masks (Li et al., 2023) may confuse AdaWTP to select blurry tokens."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "In this paper, we presented an adaptive and efficient approach, LMD-ViT, the first sparse vision Transformer for restoring images affected by local motion blurs. LMD-ViT is built upon our novel adaptive window pruning Transformer blocks (AdaWPT), which utilize blur-aware confidence predictors to estimate the level of blur confidence in the feature domain. This information is then used to adaptively prune unnecessary windows in low-confidence regions. To train the confidence predictor, we designed an end-to-end reconstruction loss with Gumbel-Softmax re-parameterization, along with a pruning loss guided by our meticulously annotated blur masks. Extensive experiments demonstrate that our method effectively eliminates local motion blur while ensuring minimal deformation of sharp regions, resulting in a significant improvement in image quality and inference speed. Due to limited page space, we discuss the limitations and broader impacts in Appendix G.\n1LBFMG is an automatic blur mask annotation method proposed by Li et al.Li et al. (2023)"
        },
        {
            "heading": "A RELATED WORK",
            "text": "Single image deep motion deblurring. The task of deep motion deblurring for single images originated from global deblurring (Zhang et al., 2018; Ren et al., 2021; Tao et al., 2018; Zamir et al., 2021; Zhang et al., 2021; Zhou et al., 2022). Pioneering deep global motion deblurring works utilize CNNs as basic layers and achieve promising improvements in image quality. Among them, DeepDeblur (Nah et al., 2017), a multi-scale convolutional neural network, performs residual blocks to increase convergence speed. DeblurGAN (Kupyn et al., 2018) and DeblurGAN-v2 (Kupyn et al., 2019) introduce GANs and a perceptual loss to improve subjective quality. HINet (Chen et al., 2021) applies Instance Normalization to boost performance. Recently, a CNN-based local motion deblurring method, LBAG (Li et al., 2023), bridges the gap between global and local motion deblurring by inserting gate modules at the end of MIMO-UNet architecture (Cho et al., 2021). It predicts differentiable blur masks to reduce sharp backgrounds from modifications and guide the network to deblur locally. There are also CNN-based methods that involve spatially variant predictions for blind image reconstruction Liang et al. (2021b). Although the performance is significantly improved, CNN-based methods suffer from the content-independent interactions between images and convolution kernels, as well as the limitations of long-range dependency modeling.\nGiven the Vision Transformer\u2019s (ViT) (Dosovitskiy et al., 2021) ability to capture long-range dependencies, its application to global deblurring tasks has seen a surge of interest. For example, Uformer (Wang et al., 2022) employs window-based self-attention with a learnable multi-scale restoration modulator to capture both local and global dependencies. Restormer (Zamir et al., 2022) utilizes multi-head attention and a feed-forward network to achieve long-range pixel interactions. In this paper, we build a Transformer-based local motion deblurring framework, LMD-ViT, that adaptively selects windows relevant to blurry regions for window-based self-attention and feed-forward operations, simultaneously benefiting from long-range modeling.\nVision Transformer acceleration. Transformers have proven valuable in deblurring tasks, yet their direct application in local motion deblurring for high-resolution images presents challenges concerning computational efficiency. To solve the heavy computation problem of global self-attention in Transformers, researchers have presented several techniques. For example, Wang et al. adopted pyramid structures and spatial-reduction attention (Wang et al., 2021) in image classification, object detection, and segmentation tasks. Some methods partition image features into different windows and perform self-attention on local windows (Vaswani et al., 2021; Yang et al., 2021; Wang et al., 2022) for image restoration tasks. Some image classification methods gradually reduce tokens in processing by token-halting (Meng et al., 2022; Rao et al., 2021; Yin et al., 2022) or token-merging (Liang et al., 2022; Bolya et al., 2022). Inspired by these advancements, we develop adaptive window pruning blocks (AdaWPT) to eliminate unnecessary tokens and focus deblurring only on blurred regions, which improves image quality and enables inference speed-up without compromising sharp regions."
        },
        {
            "heading": "B LMD-VIT ARCHITECTURE AND MODEL HYPER-PARAMETERS",
            "text": "As shown in Figure 2 of our main paper, our proposed LMD-ViT is a U-shaped network with one in-projection layer, four encoder stages, one bottleneck stage, four decoder stages, and one outprojection layer. Skip connections are set up between the encoder stage and the decoder stage. A locally blurred input image B \u2208 R with a shape H\u00d7W\u00d73 firstly goes through an in-projection block, which consists of a 3\u00d7 3 convolutional layer, a LeakyReLU layer, and a layer normalization block, to extract low-level features as a feature map X \u2208 R. The feature map then passes four encoder stages, each of which includes a series of AdaWPT Transformer blocks and one down-sampling layer. AdaWPT uses a blur-aware confidence predictor and Gumble-Softmax re-parameterization to select blur-related tokens. Only the selected tokens are forwarded to Transformer layers including window-based self-attention (W-MSA), window-based locally-enhanced feed-forward layer (WLeFF), and layer normalization (LN). The down-sampling layer down-samples the feature map size by 2 times and doubles the channels using 4\u00d7 4 convolution with stride 2. The feature map\u2019s shape turns to H2i \u00d7 W 2i \u00d7 3, i \u2208 {1, 2, 3, 4} after i encoder stages, and has the smallest resolution in the bottleneck, where it can sense the longest dependencies in two AdaWPTs. After the bottleneck stage, the feature map goes through four decoder stages, each of which owns an up-sampling layer and a series of AdaWTP blocks. The up-sampling layer uses 2 \u00d7 2 transposed convolution with\nstride 2 to reduce half of the feature channels and double the size of the feature map. The features put into the AdaWPT blocks are concatenations of the up-sampled features and the corresponding features from the symmetrical encoder stages through skip connections. Finally, the feature map passes the out-projection block which reshapes the flattened features to 2D feature maps and applies a 3\u00d7 3 convolution layer to obtain a residual image R. The restored sharp image S\u2032 is obtained by S\u2032 = B+R."
        },
        {
            "heading": "C MORE DETAILS OF LOCAL BLUR MASK ANNOTATION",
            "text": "As illustrated in Section 2.4.2 of our main paper, we manually mark binary masks for the locally blurred images of the ReLoBlur dataset. In our annotated blur masks, pixels with a value of 1 indicate blurriness, while pixels with a value of 0 represent sharpness. This can be seen in Figure 1 and Figure 5 of our main paper. To confirm the blurry regions, we subtract the sharp and blurred images to observe the differences in local regions. This difference map helps us to determine the approximate position of the blurriness and the border. After confirming the blurry regions, we select moving objects on the EISeg software2, which can automatically segment different components within each object. Then, we manually refine the blurry regions. To prevent leakage, we extend the blurry region by 5 pixels, ensuring that all pixels within the annotated mask are considered blurry. Regarding pixels between blurry and sharp areas, we adhere to a principle: a diffuse spot encompassing more than 5 pixels is categorized as a blurry region, while conversely, it is classified as a sharp region."
        },
        {
            "heading": "D GLOBAL DEBLURRING PERFORMANCES",
            "text": "Our proposed LMD-ViT takes into account both local and global motion deblurring. To prove that our method could also deblur globally, we test our model on the GOPRO testing dataset (Nah et al., 2017). With a globally blurry input, the predicted blurriness confidence approaches 100%. Therefore, LMD-ViT preserves all windows, represented by the white color in the 5th column of Figure 7. We compare our proposed models with other state-of-the-art Transformer-based deblurring methods in Table 4. The results show that our proposed method obtains comparable performance to the state-of-the-art global deblurring Transformers. The scores of our method are slightly lower than Uformer (Wang et al., 2022) because our proposed LMD-ViT utilizes a window pruning strategy. The pruning precision (as described in Section ) is not 100%. Some tokens miss the deblurring manipulations in some blocks and therefore they are not clear enough. The visual comparison on the GoPro testing dataset (Nah et al., 2017) also shows that LMD-ViT is not inferior to other baseline methods. It can effectively eliminate global blurriness and restore sharp textures."
        },
        {
            "heading": "E FURTHER DISCUSSION OF ABLATION STUDY",
            "text": "E.1 THE EFFECTIVENESS OF W-LEFF\nTo save computational costs as well as inference time, we apply a window-based local-enhanced feed-forward layer (W-LeFF) instead of a locally-enhanced feed-forward layer (LeFF) (Wang et al., 2022). In W-LeFF, only the non-pruned windows are processed by a feed-forward mechanism. To ensure a fair comparison between W-LeFF and LeFF, we evaluate them on the all-pruning LMDViT network architecture separately. As shown in Table 5, we observe that W-LeFF achieves nearly\n2EISeg is provided by https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.7/EISeg5\nLocally Blurred Image Uformer Ours\nLBAG\n0% 10% 20% 30% Percentage 40% 50% 60%\nOurs Uformer\nRestormer LBAG\nRestormer\nidentical performance to LeFF. Hence, substituting LeFF with W-LeFF does not compromise the local motion deblurring capabilities while simultaneously accelerating the inference speed.\nE.2 THE EFFECT OF FEATURE CHANNELS\nThe number of feature channels also affects the ability of neural networks. With a larger number of feature channels, a neural network can capture more intricate and complex relationships in the input data, resulting in better performance. To verify the capability of our proposed network architecture, we train our network with dimensions 16 and 32 respectively, and compare it with CNN-based LBAG (Li et al., 2023) with aligned model parameters, as shown in Table 6. The comparison between line 3 and line 4 shows an improvement with increased feature channels in LMD-ViT because a larger number of feature channels provides a larger number of meaningful features or attributes, which can be beneficial for window selection and feature manipulation. The comparison between line 2 and line 4 implies that, under approximate model parameters, our proposed model with the adaptive window pruning mechanism is more suitable for the local motion deblurring task with better evaluation scores, fewer model parameters, and faster inference speed.\nE.3 THE EFFECTIVENESS OF THE JOINT TRAINING TECHNIQUE\nWe train our proposed LMD-ViT and baseline methods with the ReLoBlur dataset Li et al. (2023) and the GoPro dataset together mainly for two reasons. Firstly, joint training could prevent our model from over-fitting and improve the model\u2019s robustness. Secondly, we expect our proposed LMD-ViT both deblur globally and locally. Training with the ReLoBlur dataset Li et al. (2023) and the GoPro\ndataset Nah et al. (2017) together improves both the local and global deblurring performances, as shown in Table 7.\nE.4 LOCAL DEBLURRING RESULTS UNDER DIFFERENT RESOLUTIONS\nWe augment our findings by including results for various resolutions. Employing the nearest interpolation, we down-sample the ReLoBlur testing data and conduct comparisons between our proposed model and the top 2 baseline methods in Table 8. We find that the local deblurring results of our method and the baselines drop. However, our proposed method outweighs baseline methods in terms of PSNR and SSIM scores. Moreover, it achieves fast inference in low-resolution images.\nE.5 THE EFFECTIVENESS OF OUR BACKBONE\nThe transformer baselines, such as Restormer and Uformer-B, are not trained with the local blur masks, which are deployed during the training by our proposed method. To fairly compare the backbones with baselines, we provide results for baselines trained with blur masks. When training the baseline methods Zamir et al. (2022); Wang et al. (2022), we borrow the gated block from LBAG Li et al. (2023), and the reconstruction loss from our method (introduced in Equation 9). We apply the two local blur-aware strategies to Uformer Wang et al. (2022) and Restormer Zamir et al. (2022). Table 9 shows that when trained with local blur masks, the PSNR and SSIM scores of Uformer Wang et al. (2022) and Restormer Zamir et al. (2022) improve. However, their performances are still inferior to ours and they need more time to infer. This proves the effectiveness of our backbone and the window pruning strategy.\nE.6 MASK PREDICTION ACCURACY\nTo further evaluate the local motion deblurring ability of our proposed LMD-ViT, we gauge the mask prediction accuracy of our model on the ReLoBlur testing dataset using the formula accuracy = (TP +TN)/N , where TP denotes the number of pixels correctly predicted as blurry, TN signifies the count of pixels accurately predicted as sharp, and N represents the total pixel count. The mask prediction accuracies exhibit variability across different AdaWPT blocks, with the highest achieving 94.51%. As shown in Figure 12, most AdaWPT blocks exhibit high prediction accuracies, indicating the effective recognition of local blurs by our proposed model. The first two learnable blocks show comparatively lower accuracies. Additionally, the accuracy of AdaWPT 9 drops, potentially influenced by its skip connection with AdaWPT 1.\nE.7 RESULTS OF SPECIAL CASES\nWe subjected our proposed model to testing using two distinctive input scenarios. In the case of globally blurred images sourced from the GoPro testing dataset, the corresponding masks exhibit uniformly high values, and the decision maps manifest predominantly in an all-white color, as depicted in Appendix D and visually represented in Figure 7. Conversely, when inputting sharp images from the GoPro testing dataset, the mask values are uniformly low, resulting in decision maps that predominantly display an all-black color, as illustrated in Figure 13. The results of these extreme experiments demonstrate the model\u2019s ability to effectively distinguish between blurred and sharp images, showcasing its robustness."
        },
        {
            "heading": "F MORE VISUAL RESULTS OF LOCAL MOTION DEBLURRING",
            "text": "F.1 MORE VISUAL COMPARISONS BETWEEN LMD-VIT AND THE COMPARED METHODS\nWe provide more visual results of our proposed LMD-ViT and other baseline methods for local motion deblurring, as shown in Figure 9 and Figure 10. The performance of LMD-ViT surpasses that of other state-of-the-art methods, yielding output images with improved clarity and enhanced details.\nF.2 MORE VISUAL RESULTS OF REAL-WORLD LOCAL MOTION DEBLURRING\nWe provide more visual results for real-world local motion deblurring, which are used for conducting user study, as shown in Figure 11. We conduct a comparison between our method and the top 3 methods listed in Table 1 of the main paper. The visual results show that our proposed method exhibits robust deblurring capability and outperforms the state-of-the-art methods on real-world locally blurred images."
        },
        {
            "heading": "G LIMITATIONS AND BROADER IMPACTS",
            "text": "Limitations Like other local motion deblurring methods (Nah et al., 2017; Kupyn et al., 2018; 2019; Chen et al., 2021; Cho et al., 2021; Li et al., 2023; Wang et al., 2022; Zamir et al., 2022), our proposed LMD-ViT is not capable of real-time deblurring. However, we plan to optimize our structure so that it can be applied in real time.\nBroader impacts Our proposed local motion deblurring method democratizes access and empowers users across multiple fields to enhance their visual content, fostering creativity and expression. Firstly, in the realm of consumer electronics and mobile devices, where camera stabilization technology like gimbals has mitigated global motion blur caused by camera shake, local motion blur remains a key problem for static cameras. By incorporating effective local motion deblurring algorithms into these devices, we adaptively enhance image quality with less inference time, leading to superior user experiences and heightened customer satisfaction. In the realm of forensic analysis and surveillance, where local motion blur often plagues static surveillance camera images or video frames, our method enhances the quality of these blurred visuals, enabling better object, individual, and event identification. It proves invaluable in criminal investigations, accident reconstructions, and security-related applications. Last but not least, photographers and visual artists benefit greatly, as it salvages blurred images caused by subject motion, resulting in sharper, visually appealing photographs with enhanced details and clarity. This advancement elevates the overall quality of visual information acquisition and visual arts as expressive mediums."
        }
    ],
    "year": 2023
}