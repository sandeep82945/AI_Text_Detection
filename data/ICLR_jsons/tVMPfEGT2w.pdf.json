{
    "abstractText": "In this paper, we investigate the problem of offline Preference-based Reinforcement Learning (PbRL) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coefficient. We also establish lower bounds that highlight the necessity of such concentrability and the difference from standard RL, where state-action-wise rewards are directly observed. We further extend and analyze our algorithm when the feedback is given over action pairs.",
    "authors": [],
    "id": "SP:14fc4fec0836e42863cdb39fededb65c4544d35b",
    "references": [
        {
            "authors": [
                "Y. Abdelkareem",
                "S. Shehata",
                "F. Karray"
            ],
            "title": "Advances in preference-based reinforcement learning: A review",
            "venue": "IEEE International Conference on Systems, Man, and Cybernetics (SMC),",
            "year": 2022
        },
        {
            "authors": [
                "A. Agarwal",
                "N. Jiang",
                "S.M. Kakade",
                "W. Sun"
            ],
            "title": "Reinforcement learning: Theory and algorithms",
            "venue": "Technical report",
            "year": 2019
        },
        {
            "authors": [
                "Audibert",
                "J.-Y",
                "A.B. Tsybakov"
            ],
            "title": "Fast learning rates for plug-in classifiers",
            "venue": "The Annals of statistics,",
            "year": 2007
        },
        {
            "authors": [
                "Y. Bai",
                "A. Jones",
                "K. Ndousse",
                "A. Askell",
                "A. Chen",
                "N. DasSarma",
                "D. Drain",
                "S. Fort",
                "D. Ganguli",
                "T Henighan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "D. Brown",
                "W. Goo",
                "P. Nagarajan",
                "S. Niekum"
            ],
            "title": "Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "N. Jiang"
            ],
            "title": "Information-theoretic considerations in batch reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "N. Jiang"
            ],
            "title": "Information-theoretic considerations in batch reinforcement learning",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "X. Chen",
                "H. Zhong",
                "Z. Yang",
                "Z. Wang",
                "L. Wang"
            ],
            "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng",
                "C.-A",
                "T. Xie",
                "N. Jiang",
                "A. Agarwal"
            ],
            "title": "Adversarially trained actor critic for offline reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "P.F. Christiano",
                "J. Leike",
                "T. Brown",
                "M. Martic",
                "S. Legg",
                "D. Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "A. Glaese",
                "N. McAleese",
                "M. Tr\u0119bacz",
                "J. Aslanides",
                "V. Firoiu",
                "T. Ewalds",
                "M. Rauh",
                "L. Weidinger",
                "M. Chadwick",
                "P Thacker"
            ],
            "title": "Improving alignment of dialogue agents via targeted human judgements",
            "venue": "arXiv preprint arXiv:2209.14375",
            "year": 2022
        },
        {
            "authors": [
                "Y. Hu",
                "N. Kallus",
                "X. Mao"
            ],
            "title": "Fast rates for contextual linear optimization",
            "venue": "Management Science,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Hu",
                "N. Kallus",
                "M. Uehara"
            ],
            "title": "Fast rates for the regret of offline reinforcement learning",
            "venue": "In Conference on Learning Theory. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "Y. Jin",
                "Z. Yang",
                "Z. Wang"
            ],
            "title": "Is pessimism provably efficient for offline rl? arXiv preprint arXiv:2012.15085",
            "year": 2020
        },
        {
            "authors": [
                "Y. Jin",
                "Z. Yang",
                "Z. Wang"
            ],
            "title": "Is pessimism provably efficient for offline rl",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "R. Kidambi",
                "A. Rajeswaran",
                "P. Netrapalli",
                "T. Joachims"
            ],
            "title": "Morel: Model-based offline reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "A. Kumar",
                "A. Zhou",
                "G. Tucker",
                "S. Levine"
            ],
            "title": "Conservative q-learning for offline reinforcement learning",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS)",
            "year": 2020
        },
        {
            "authors": [
                "G. Li",
                "C. Ma",
                "N. Srebro"
            ],
            "title": "Pessimism for offline linear contextual bandits using lp confidence sets. arXiv preprint arXiv:2205.10671",
            "year": 2022
        },
        {
            "authors": [
                "G. Li",
                "L. Shi",
                "Y. Chen",
                "Y. Chi",
                "Y. Wei"
            ],
            "title": "Settling the sample complexity of model-based offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2204.05275",
            "year": 2022
        },
        {
            "authors": [
                "H. Liu",
                "C. Sferrazza",
                "P. Abbeel"
            ],
            "title": "Languages are rewards: Hindsight finetuning using human feedback. arXiv preprint arXiv:2302.02676",
            "year": 2023
        },
        {
            "authors": [
                "Q. Liu",
                "A. Chung",
                "C. Szepesv\u00e1ri",
                "C. Jin"
            ],
            "title": "When is partially observable reinforcement learning not scary? arXiv preprint arXiv:2204.08967",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "A. Swaminathan",
                "A. Agarwal",
                "E. Brunskill"
            ],
            "title": "Provably good batch reinforcement learning without great exploration",
            "venue": "arXiv preprint arXiv:2007.08202",
            "year": 2020
        },
        {
            "authors": [
                "A. Luedtke",
                "A. Chambaz"
            ],
            "title": "Performance guarantees for policy learning",
            "venue": "In Annales de l\u2019IHP Probabilites et statistiques,",
            "year": 2020
        },
        {
            "authors": [
                "J. MacGlashan",
                "M.K. Ho",
                "R. Loftin",
                "B. Peng",
                "G. Wang",
                "D.L. Roberts",
                "M.E. Taylor",
                "M.L. Littman"
            ],
            "title": "Interactive learning from policy-dependent human feedback",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "R. Nakano",
                "J. Hilton",
                "S. Balaji",
                "J. Wu",
                "L. Ouyang",
                "C. Kim",
                "C. Hesse",
                "S. Jain",
                "V. Kosaraju",
                "W Saunders"
            ],
            "title": "Webgpt: Browser-assisted question-answering with human",
            "year": 2021
        },
        {
            "authors": [
                "E. Novoseller",
                "Y. Wei",
                "Y. Sui",
                "Y. Yue",
                "J. Burdick"
            ],
            "title": "Dueling posterior sampling for preference-based reinforcement learning",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "L. Ouyang",
                "J. Wu",
                "X. Jiang",
                "D. Almeida",
                "C. Wainwright",
                "P. Mishkin",
                "C. Zhang",
                "S. Agarwal",
                "K. Slama",
                "A Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "A. Pacchiano",
                "A. Saha",
                "J. Lee"
            ],
            "title": "Dueling rl: reinforcement learning with trajectory preferences. arXiv preprint arXiv:2111.04850",
            "year": 2021
        },
        {
            "authors": [
                "V. Perchet",
                "P. Rigollet"
            ],
            "title": "The multi-armed bandit problem with covariates",
            "venue": "The Annals of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "D. Ramachandran",
                "E. Amir"
            ],
            "title": "Bayesian inverse reinforcement learning",
            "year": 2007
        },
        {
            "authors": [
                "R. Ramamurthy",
                "P. Ammanabrolu",
                "K. Brantley",
                "J. Hessel",
                "R. Sifa",
                "C. Bauckhage",
                "H. Hajishirzi",
                "Y. Choi"
            ],
            "title": "Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241",
            "year": 2022
        },
        {
            "authors": [
                "P. Rashidinejad",
                "B. Zhu",
                "C. Ma",
                "J. Jiao",
                "S. Russell"
            ],
            "title": "Bridging offline reinforcement learning and imitation learning: A tale of pessimism",
            "venue": "arXiv preprint arXiv:2103.12021",
            "year": 2021
        },
        {
            "authors": [
                "P. Rashidinejad",
                "B. Zhu",
                "C. Ma",
                "J. Jiao",
                "S. Russell"
            ],
            "title": "Bridging offline reinforcement learning and imitation learning: A tale of pessimism",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "M. Rigter",
                "B. Lacerda",
                "N. Hawes"
            ],
            "title": "Rambo-rl: Robust adversarial model-based offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2204.12581",
            "year": 2022
        },
        {
            "authors": [
                "S. Ross",
                "G. Gordon",
                "D. Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "L. Shi",
                "G. Li",
                "Y. Wei",
                "Y. Chen",
                "Y. Chi"
            ],
            "title": "Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890",
            "year": 2022
        },
        {
            "authors": [
                "D. Shin",
                "A.D. Dragan",
                "D.S. Brown"
            ],
            "title": "Benchmarks and algorithms for offline preferencebased reward learning",
            "venue": "arXiv preprint arXiv:2301.01392",
            "year": 2023
        },
        {
            "authors": [
                "M. Simchowitz",
                "K.G. Jamieson"
            ],
            "title": "Non-asymptotic gap-dependent regret bounds for tabular mdps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Song",
                "Y. Zhou",
                "A. Sekhari",
                "J.A. Bagnell",
                "A. Krishnamurthy",
                "W. Sun"
            ],
            "title": "Hybrid rl: Using both offline and online data",
            "year": 2022
        },
        {
            "authors": [
                "N. Stiennon",
                "L. Ouyang",
                "J. Wu",
                "D. Ziegler",
                "R. Lowe",
                "C. Voss",
                "A. Radford",
                "D. Amodei",
                "P.F. Christiano"
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "M. Uehara",
                "N. Kallus",
                "J.D. Lee",
                "W. Sun"
            ],
            "title": "Refined value-based offline rl under realizability and partial coverage",
            "venue": "arXiv preprint arXiv:2302.02392",
            "year": 2023
        },
        {
            "authors": [
                "M. Uehara",
                "W. Sun"
            ],
            "title": "Pessimistic model-based offline rl: Pac bounds and posterior sampling under partial coverage",
            "venue": "In arXiv preprint arXiv:2107.06226",
            "year": 2021
        },
        {
            "authors": [
                "M. Wainwright"
            ],
            "title": "High-Dimensional Statistics: A Non-Asymptotic Viewpoint, volume 48 of Cambridge Series in Statistical and Probabilistic Mathematics",
            "year": 2019
        },
        {
            "authors": [
                "G. Warnell",
                "N. Waytowich",
                "V. Lawhern",
                "P. Stone"
            ],
            "title": "Deep tamer: Interactive agent shaping in high-dimensional state spaces",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "C. Wirth",
                "R. Akrour",
                "G. Neumann",
                "J F\u00fcrnkranz"
            ],
            "title": "A survey of preference-based reinforcement learning methods",
            "venue": "Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "J. Wu",
                "V. Braverman",
                "L. Yang"
            ],
            "title": "Gap-dependent unsupervised exploration for reinforcement learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "J. Wu",
                "L. Ouyang",
                "D.M. Ziegler",
                "N. Stiennon",
                "R. Lowe",
                "J. Leike",
                "P. Christiano"
            ],
            "title": "Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862",
            "year": 2021
        },
        {
            "authors": [
                "T. Xie",
                "Cheng",
                "C.-A",
                "N. Jiang",
                "P. Mineiro",
                "A. Agarwal"
            ],
            "title": "Bellman-consistent pessimism for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2106.06926",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xu",
                "R. Wang",
                "L. Yang",
                "A. Singh",
                "A. Dubrawski"
            ],
            "title": "Preference-based reinforcement learning with finite-time guarantees",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "M. Yin",
                "Wang",
                "Y.-X"
            ],
            "title": "Towards instance-optimal offline reinforcement learning with pessimism",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "T. Yu",
                "G. Thomas",
                "L. Yu",
                "S. Ermon",
                "J.Y. Zou",
                "S. Levine",
                "C. Finn",
                "T. Ma"
            ],
            "title": "Mopo: Model-based offline policy optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yue",
                "J. Broder",
                "R. Kleinberg",
                "T. Joachims"
            ],
            "title": "The k-armed dueling bandits problem",
            "venue": "Journal of Computer and System Sciences,",
            "year": 2012
        },
        {
            "authors": [
                "W. Zhan",
                "B. Huang",
                "A. Huang",
                "N. Jiang",
                "J. Lee"
            ],
            "title": "Offline reinforcement learning with realizability and single-policy concentrability",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "W. Zhan",
                "M. Uehara",
                "W. Sun",
                "J.D. Lee"
            ],
            "title": "2022b). Pac reinforcement learning for predictive state representations. arXiv preprint arXiv:2207.05738",
            "year": 2022
        },
        {
            "authors": [
                "B. Zhu",
                "J. Jiao",
                "M.I. Jordan"
            ],
            "title": "Principled reinforcement learning with human feedback from pairwise or k-wise comparisons",
            "venue": "arXiv preprint arXiv:2301.11270",
            "year": 2023
        },
        {
            "authors": [
                "D.M. Ziegler",
                "N. Stiennon",
                "J. Wu",
                "T.B. Brown",
                "A. Radford",
                "D. Amodei",
                "P. Christiano",
                "G. Irving"
            ],
            "title": "Fine-tuning language models from human",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In standard reinforcement learning (RL) setting, the agent learns to maximize an observed numerical reward signal. However, finding appropriate numerical rewards can often be challenging in practice, and getting rewards right significantly impacts the effectiveness of RL algorithms (Wirth et al., 2017). To address this challenge, preference-based RL (PbRL) with human feedback has emerged as a promising alternative (Christiano et al., 2017). In PbRL, the agent does not receive a numerical reward signal, but rather feedback from a human expert in the form of preferences for a state-action trajectory in given pairs of trajectories. PbRL has gained considerable attention across multiple application domains, including games (MacGlashan et al., 2017; Christiano et al., 2017; Warnell et al., 2018), large language models (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021; Nakano et al., 2021; Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022; Ramamurthy et al., 2022; Liu et al., 2023), and robot learning (Brown et al., 2019; Shin et al., 2023).\nIn this work, we focus on the problem of offline PbRL, where the learning process relies exclusively on pre-collected offline data without active interaction with the environment. Offline RL has gained significant attention in various applications where conducting real-time online experiments may be costly. In the context of PbRL, an offline setting is particularly relevant due to the high cost and latency associated with obtaining human feedback. One of the key challenges in offline RL is the limited coverage of available offline data. Since coverage of the entire state-action space is rarely feasible in practice (Chen and Jiang, 2019a), recent empirical and theoretical approaches to offline RL leverage pessimism so as to rely only on the coverage of one comparator policy (possibly the optimal one), i.e., the so-called partial coverage condition (Yu et al., 2020; Kidambi et al., 2020; Rashidinejad et al., 2021a; Li et al., 2022a; Shi et al., 2022; Yin and Wang, 2021; Xie et al., 2021; Uehara and Sun, 2021; Zhan et al., 2022a). In the context of PbRL, it is also crucial to develop algorithms that work under the partial coverage condition.\nDespite its significance, there are very few algorithms specifically designed for offline PbRL with strong statistical guarantees. In this work, we provide such algorithms and guarantees when prefer-\nences depend on unknown reward functions over trajectories. Notably, we consider general reward functions that can be defined over the whole trajectory rather than just state-action pairs. This is consistent with many practical settings in natural language processing. For instance, all benchmarks presented in RL4LM (Ramamurthy et al., 2022) use metrics defined over the entire trajectories. Our main contributions can be summarized as follows:\n\u2022 We propose a simple algorithm with general function approximation that consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE.\n\u2022 We prove that our algorithm can effectively compete with a target policy as long as the offline data cover the target policy. Our analysis leverages a newly defined concentrability coefficient which is tailored to PbRL. As the concentrability coefficient differs from that in the standard RL setting where state-action-wise rewards are directly observed, we establish lower bounds that highlight the necessity of our partial coverage condition. To the best of our knowledge, this is the first theoretical separation result between standard offline RL and offline PbRL.\n\u2022 We extend the algorithm to the setting where the transition kernel is unknown, where we not only construct confidence sets for the reward function but also for the system dynamics. Notably, even though the reward can be trajectory-wise, we only need to estimate the per-step transition dynamics to ensure efficient learning.\n\u2022 We further extend our results to the action-based comparison model, where preferences are defined over individual actions instead of entire trajectories based on the advantage function of the optimal policy (Ramachandran and Amir, 2007; Zhu et al., 2023). In comparison to the case of the trajectorywise comparison model, we can establish a partial coverage guarantee using a concentrability coefficient on pairs of state-action pairs rather than trajectories. In this scenario, our sample complexity only scales with a bound on the advantage function, which can be much smaller than a bound on per-trajectory rewards as shown in Ross et al. (2011); Agarwal et al. (2019)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Preference-based Reinforcement Learning. The closest work to ours is Zhu et al. (2023), which also studies offline PbRL, but their algorithm and analysis are restricted to linear models. Our algorithm and analysis extend to general function approximation. Indeed, general classes such as neural networks are commonly employed in practice (Christiano et al., 2017; Abdelkareem et al., 2022). In the special case of linear rewards and preferences over trajectories, while our algorithms differ, our guarantees recover theirs. So, our guarantees are more general; see Remark 2. Moreover, they only consider the setting where the transition kernel is known, while our work can also handle unknown transitions. Finally, in the case of action-based preferences, Zhu et al. (2023) cannot provide guarantees with partial coverage, even under their restriction to linear models. We demonstrate how to achieve meaningful guarantees under partial coverage and a soft margin (Assumption 6).\nWirth et al. (2017) provide a survey of PbRL. PbRL has received considerable attention in theoretical RL (Yue et al., 2012; Novoseller et al., 2020; Xu et al., 2020; Pacchiano et al., 2021; Chen et al., 2022) but the focus is largely on online PbRL. To the best of our knowledge, Zhu et al. (2023) is the only previous work to provide theoretical guarantees for offline PbRL.\nOffline RL. In offline RL, one of the most critical challenges is addressing the issue of insufficient coverage in the offline data. It is well-known that naive methods are unable to learn the optimal policy in such scenarios (Rashidinejad et al., 2021b). To tackle this problem, numerous algorithms have been proposed with theoretical guarantees (Liu et al., 2020; Kumar et al., 2020; Jin et al., 2021; Rashidinejad et al., 2021b; Uehara and Sun, 2021; Li et al., 2022b; Shi et al., 2022; Jin et al., 2020; Xie et al., 2021; Zhan et al., 2022a). The most relevant work is Uehara and Sun (2021), which focuses on offline model-based RL with general function approximation. However, their methods cannot be directly applied to PbRL since per-step rewards are not observable in our setting. Furthermore, even in the standard RL setting, the construction of confidence intervals differs between our approach and theirs. Another related paper is Cheng et al. (2022), which considers the general offline pessimistic RL framework in the standard setting and also subtracts a reference term in their algorithm, similar to ours. However, our motivations for such reference terms are quite different from theirs. Additional detailed comparisons are given in Section 4.1 and Remark 4."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "We first introduce our offline PbRL setting with general function approximation.\nMarkov decision processes. We consider an episodic time-inhomogeneous Markov Decision Process (MDP) denoted by M, which consists of a state space S , an action space A, an initial state distribution P \u22c60 \u2208 \u2206S , and a horizon H \u2208 N+. At each step h \u2208 [H \u2212 1], we use P \u22c6h : S \u00d7A \u2192 \u2206S to denote the ground truth transitions. The ground truth reward function for the entire trajectory is denoted by r\u22c6 : T \u2192 [0, rmax], where T = (S\u00d7A)H represents the set of all possible trajectories. Note that r\u22c6 is a trajectory-wise reward, which is more general than state-action-wise rewards commonly considered in standard RL, which is the special case where for some {r\u22c6h}Hh=1 we have r\u22c6(\u03c4) = \u2211H h=1 r \u22c6 h(sh, ah) for a trajectory \u03c4 = (s1, a1, \u00b7 \u00b7 \u00b7 , sH , aH). A history-dependent policy \u03c0 := {\u03c0h}Hh=1 is characterized by \u03c0h : (S \u00d7 A)h\u22121 \u00d7 S \u2192 \u2206A, specifying the probability of selecting actions for the agent at each step h \u2208 [H] based on the entire history. We denote the set of all such history-dependent policies as \u03a0his. Given a policy \u03c0, we define its expected reward with respect to a general reward function r and initial and transition distributions P = {Ph}H\u22121h=0 as J(\u03c0; r, P ) := E\u03c4\u223c(\u03c0,P )[r(\u03c4)]. Here, E\u03c4\u223c(\u03c0,P )[\u00b7] represents the expectation over the trajectory distribution when executing the policy \u03c0 under the transition P starting from P0. We use E\u03c4\u223c\u03c0[\u00b7] or E\u03c0[\u00b7] to denote the special case when P is the ground truth distribution P \u22c6 := {P \u22c6h} H\u22121 h=0 .\nThe optimal policy, denoted \u03c0\u22c6, is the policy that maximizes the expected reward with respect to the true reward r\u22c6 and system dynamics P \u22c6, i.e., \u03c0\u22c6 := argmax\u03c0\u2208\u03a0his J(\u03c0; r\n\u22c6, P \u22c6). As the true reward function r\u22c6 is dependent on the entire trajectory, the optimal policy \u03c0\u22c6 is generally history-dependent. Thus, designing offline PbRL algorithms that can learn history-dependent policies is crucial.\nFor any policy \u03c0, we can define its state-action visitation measure as follows: d\u03c0h(s, a) = P\u03c0,P \u22c6 (sh = s, ah = a),\u2200h \u2208 [H], where P\u03c0,P \u22c6\n(\u00b7) denotes the distribution of the trajectory when executing policy \u03c0 in P \u22c6. We will also use d\u03c0(\u03c4) to denote P\u03c0,P\u22c6(\u03c4) for the whole trajectory \u03c4 .\nA policy is Markovian if at each step it depends solely on the current state. When the reward is state-action-wise and the policy is Markovian, we can define the associated V- and Q-functions as V \u03c0h (s) = E\u03c0[ \u2211H t=h r \u22c6 t (st, at)|sh = s],\u2200h \u2208 [H], Q\u03c0h(s, a) = E\u03c0[ \u2211H t=h r \u22c6 t (st, at)|sh = s, ah = a], \u2200h \u2208 [H]. It is well-known that when the reward is state-action-wise, the optimal policy \u03c0\u22c6 is both Markovian and deterministic. Furthermore, we have V \u03c0 \u22c6\nh (s) = sup\u03c0 V \u03c0 h (s) and Q\n\u03c0\u22c6\nh (s, a) = sup\u03c0 Q \u03c0 h(s, a) for all h \u2208 [H]. For brevity, we will use V \u22c6 and Q\u22c6 to represent the optimal state-value function and Q-function, respectively. The advantage function of the optimal policy, denoted by A\u22c6, is defined to be A\u22c6h(s, a) = Q \u22c6 h(s, a)\u2212 V \u22c6h (s) for all h \u2208 [H], s \u2208 S, A \u2208 A.\nOffline Preference-based Reinforcement Learning. We focus on the problem of offline PbRL in this work. Specifically, in the trajectory-based pairwise comparison setting, we are provided with an offline dataset D = {\u03c4n,0, \u03c4n,1, on}Nn=1, where \u03c4n,0 = {s n,0 h , a n,0 h }Hh=1 and \u03c4n,1 = {s n,1 h , a n,1 h }Hh=1 are i.i.d. sampled from the distributions \u00b50 and \u00b51, respectively, and on \u2208 {0, 1} indicates preference for \u03c4n,1 over \u03c4n,2. We assume it satisfies the following preference model: Assumption 1 (Preference-based model). Given a pair of trajectories (\u03c40, \u03c41), o \u2208 {0, 1} satisfies"
        },
        {
            "heading": "P (o = 1 | \u03c40, \u03c41) = P (\u03c41 is preferred over \u03c40 | \u03c40, \u03c41) = \u03a6(r\u22c6(\u03c41)\u2212 r\u22c6(\u03c40)).",
            "text": "where \u03a6 : R \u2192 [0, 1] is a monotonically increasing link function.\nA commonly used link function is the sigmoid function \u03c3(x) = 1/{1 + exp(\u2212x)}, leading to the Bradley-Terry-Luce (BTL) model (Christiano et al., 2017).\nThe objective of offline PbRL is to learn a high-quality policy \u03c0\u0302 \u2208 \u03a0his, i.e., with J(\u03c0tar; r\u22c6, P \u22c6)\u2212 J(\u03c0\u0302; r\u22c6, P \u22c6) \u2264 \u03f5 where \u03c0tar is a target policy we want to compete with (potentially \u03c0\u22c6). General function approximation. In our paper, we estimate the reward r\u22c6 with general function approximation. We introduce a function class Gr, such as linear functions or neural networks, to approximate the true reward. For each r \u2208 Gr and trajectory pair (\u03c40, \u03c41), we denote the induced preference model with respect to r as Pr(o|\u03c40, \u03c41), defined as\nPr(o = 1 | \u03c40, \u03c41) := \u03a6(r(\u03c41)\u2212 r(\u03c40)). (1) We use bracketing numbers to measure the complexity of {Pr : r \u2208 Gr}.\nDefinition 1 (\u03f5-bracketing number of preferences). We say (g1, g2) is an \u03f5-bracket if g1(\u00b7 | \u03c40, \u03c41) \u2264 g2(\u00b7 | \u03c40, \u03c41) and \u2225g1(\u00b7 | \u03c40, \u03c41) \u2212 g2(\u00b7 | \u03c40, \u03c41)\u22251 \u2264 \u03f5 for all trajectory-pairs (\u03c40, \u03c41). The \u03f5-bracketing number of Gr, denoted by NGr (\u03f5), is the minimal number of \u03f5-brackets (gn,1, gn,2)Nn=1 needed so that for any r \u2208 Gr there is a bracket i \u2208 [N ] containing it, meaning gi,1(\u00b7|\u03c40, \u03c41) \u2264 Pr(\u00b7|\u03c40, \u03c41) \u2264 gi,2(\u00b7|\u03c40, \u03c41) for all trajectory-pairs (\u03c40, \u03c41).\nThe \u03f5-bracket number is widely used in statistics (van de Geer, 2000) to study MLE and related M-estimates. Particularly, in our setting the bracket number of reward classes will be of the same order as the covering number, another common complexity measure in statistics (Wainwright, 2019), for Pr(\u00b7|\u03c40, \u03c41) has only two dimensions. One example for which we can bound the \u03f5-bracket number is linear rewards under the BTL model (Pacchiano et al., 2021; Zhu et al., 2023).\nProposition 1. Suppose \u2225\u03d5(\u03c4)\u22252 \u2264 R \u2200\u03c4 \u2208 T , Gr \u2286 {\u03c4 7\u2192 \u27e8\u03d5(\u03c4), \u03b8\u27e9 : \u2225\u03b8\u22252 \u2264 B} for some featurization \u03d5 : T \u2192 Rd and B > 0, and the link function is \u03a6(\u00b7) = \u03c3(\u00b7). Then for any \u03f5 \u2264 1, logNGr (\u03f5) \u2264 O(d log BR\u03f5 ).\nThe proof is deferred to Appendix A. To handle unknown transitions, we similarly use function classes {GPh} H\u22121 h=0 to approximate the transition probabilities {P \u22c6h} H\u22121 0=1 . Similarly, we use NGPh (\u03f5) to denote the \u03f5-bracket number of GPh . The formal definition is deferred to Appendix E."
        },
        {
            "heading": "4 TRAJECTORY-BASED PAIRWISE-COMPARISON WITH KNOWN TRANSITION",
            "text": "In this section, we present our algorithm and analyze the sample complexity for the trajectory-based pairwise-comparison setting when the ground truth transition P \u22c6 is known. In Sections 5 and 6, we will further explore the unknown transition setting and the action-based comparison setting."
        },
        {
            "heading": "4.1 ALGORITHM",
            "text": "Our proposed algorithm, FREEHAND described in Algorithm 1, consists of the following two steps.\nConfidence set construction via MLE (Lines 2\u20133). We construct a confidence set for the ground truth reward from the implicit preference feedback. We achieve this by selecting reward models that nearly maximize the log-likelihood of observed data up to a slackness parameter \u03b6. We will show that the result, R(D), approximates the following confidence set:\nR\u2032(D) := {r \u2208 Gr : E\u03c40\u223c\u00b50,\u03c41\u223c\u00b51 [|{r(\u03c41)\u2212 r(\u03c40)} \u2212 {r\u2217(\u03c41)\u2212 r\u2217(\u03c40)}|2] \u2264 \u03be}\nfor a certain \u03be. Here the distance between r and r\u22c6 is measured using the total variation distance (i.e., \u21131 norm) of r(\u03c41)\u2212 r(\u03c40) and r\u2217(\u03c41)\u2212 r\u2217(\u03c40) over the offline data. Distributionally robust policy optimization (Line 4). After constructing the confidence set, we search for the policy that maximizes the policy value under the least favorable reward model, the r \u2208 R(D) minimizing the policy value J(\u03c0; r, P \u2217) minus E\u03c4\u223c\u00b5ref [r(\u03c4)], where \u00b5ref is an arbitrary known reference trajectory distribution. It is generally recommended to set \u00b5ref to \u00b51, as we will explain later, possibly a sample-average approximation thereof based on {\u03c41,1, . . . , \u03c4N,1}. By selecting the least favorable reward model instead of the MLE solution r\u0302, we penalize policies that are not well-covered by the offline data. The need for a reference policy arises because the approximated confidence set measures the uncertainty for reward difference between two trajectories (r(\u03c41)\u2212r(\u03c40)), but it cannot measure the uncertainty of the reward of a single trajectory.\nIn the following, we compare our algorithm to existing works. Zhu et al. (2023) consider a pessimistic offline RL algorithm for PbRL specialized to the linear reward class setting, while our FREEHAND can handle general function approximation. Specifically, they construct the confidence set using the feature-covariance-rotated \u21132-ball around the MLE \u03b8\u0302, where r\u0302(\u03c4) = \u27e8\u03d5(\u03c4), \u03b8\u0302\u27e9. In contrast, our confidence set is obtained directly from the log-likelihood objective and is generic. Uehara and Sun (2021) proposes a model-based pessimistic offline RL algorithm when we have access to rewards. The confidence set construction correspondingly differs significantly. Cheng et al. (2022) considers a general offline pessimistic RL framework. In their policy optimization step, they also subtract the value of a reference policy. This similarity is superficial, however, as the motivations are different. We subtract the value because we can only measure the difference between rewards of any two trajectories, while their motivation is to obtain a certain robustness result (their proposition 3).\nAlgorithm 1 FREEHAND: oFfline ReinforcemEnt lEarning with HumAN feeDback 1: Input: offline datset D, slackness parameter \u03b6, reference distribution \u00b5ref , true transition P \u22c6 2: MLE: compute r\u0302 = argmaxr\u2208Gr \u2211N n=1 logPr(o = o\nn | \u03c4n,1, \u03c4n,0) 3: Confidence set construction: construct\nR(D) = { r \u2208 Gr : \u2211N n=1 logPr(o = o n | \u03c4n,0, \u03c4n,1) \u2265 \u2211N n=1 logPr\u0302(o = o n | \u03c4n,0, \u03c4n,1)\u2212\u03b6 } 4: Distributionally robust planning: return\n\u03c0\u0302 = argmax\u03c0\u2208\u03a0his minr\u2208R(D) (J(\u03c0; r, P \u22c6)\u2212 E\u03c4\u223c\u00b5ref [r(\u03c4)])\nRemark 1 (Computational Efficiency). Line 4 in FREEHAND is computationally hard in general. Nevertheless, by leveraging Lagrangian formulation, we can use Lagrangian multiplier to convert the constraint r \u2208 R(D) into a regularization term of the objective function and have a feasible version of our algorithm in practice. See more details in Appendix B."
        },
        {
            "heading": "4.2 ANALYSIS",
            "text": "To analyze the sample complexity of FREEHAND, we first quantify the discrepancy between the offline data D and the distribution induced by the target policy \u03c0tar. Definition 2 (concentrability coefficient for preference-based feedback). The concentrability coefficient w.r.t. a reward class Gr, a target policy \u03c0tar, and a reference policy \u00b5ref is defined as\nCr(Gr, \u03c0tar, \u00b5ref) := max { 0, sup\nr\u2208Gr E\u03c40\u223c\u03c0tar,\u03c41\u223c\u00b5ref [r\u22c6(\u03c40)\u2212 r\u22c6(\u03c41)\u2212 r(\u03c40) + r(\u03c41)]\u221a E\u03c40\u223c\u00b50,\u03c41\u223c\u00b51 [ |r\u22c6(\u03c40)\u2212 r\u22c6(\u03c41)\u2212 r(\u03c40) + r(\u03c41)|2\n] } .\nNote, when we choose \u00b5ref = \u00b51, by Jensen\u2019s inequality, the value of Cr(Gr, \u03c0tar, \u00b51) can always be upper bounded by the per-trajectory concentration coefficient: Cr(Gr, \u03c0tar, \u00b51) \u2264 \u221a Ctr for any Gr, where Ctr := max\u03c4\u2208T d \u03c0tar (\u03c4) \u00b50(\u03c4) . Moreover, while Cr(Gr, \u03c0tar, \u00b51) becomes \u221a Ctr in the worst case (e.g., when Gr is the set of all functions mapping from T to R), it can generally be much smaller. For example, when using linear models, it is a relative condition number, as explained in Appendix D. Finally, when \u00b5ref = d\u03c0tar , our coefficient becomes 0. This implies that Cr(Gr, \u03c0tar, \u00b51) could be small when \u03c0tar and \u00b5ref are close. While the concept of concentrability coefficient has been used in offline RL with explicit reward feedback (Chen and Jiang, 2019b; Song et al., 2022), this property is unique when the feedback is in the form of preferences.\nIn our following PAC analysis, we further assume the reward class Gr is realizable and bounded. Assumption 2 (Realizability). We have r\u22c6 \u2208 Gr. Assumption 3 (Boundedness). We have 0 \u2264 r(\u03c4) \u2264 rmax for all r \u2208 Gr and \u03c4 \u2208 T . Theorem 1. For any \u03b4 \u2208 (0, 1], let \u03b6 = cMLE log(NGr (1/N)/\u03b4) where cMLE > 0 is a universal constant, then under Assumption 1,2 and 3, with probability 1\u2212 \u03b4, we have\nJ(\u03c0tar; r \u22c6, P \u22c6)\u2212 J(\u03c0\u0302; r\u22c6, P \u22c6) \u2264\n\u221a cC2r (Gr, \u03c0tar, \u00b5ref)\u03ba2 log(NGr (1/N)/\u03b4)\nN , (2)\nwhere c > 0 is a universal constant and \u03ba = (infx\u2208[\u2212rmax,rmax] \u03a6 \u2032(x))\u22121.\nTheorem 1 indicates that FREEHAND can learn an \u03f5-optimal policy compared to \u03c0tar with a sample complexity of N = O\u0303 ( C2r (Gr, \u03c0tar, \u00b5ref)\u03ba2 log(NGr (1/N)/\u03b4)\n\u03f52\n) .\nNext we provide a detailed explanation of this sample complexity. Firstly, Cr(Gr, \u03c0tar, \u00b5ref) represents the extent to which the dataset D covers the target policy \u03c0tar. In our theorem, to obtain a non-vacuous PAC guarantee, we only require the dataset D to cover the target policy \u03c0tar (i.e., Cr(Gr, \u03c0tar, \u00b5ref) < \u221e). The distributionally robust optimization step plays a crucial role in obtaining this guarantee under partial coverage. In particular, invoking the abovementioned third property of Cr(Gr, \u03c0tar, \u00b5ref), when setting \u03c0tar = \u00b5ref , (2) is reduced to\nJ(\u00b5ref ; r \u22c6, P \u22c6) \u2264 J(\u03c0\u0302; r\u22c6, P \u22c6) (3)\nThis encourages us to choose \u00b5ref = \u00b51 (or \u00b50) as it will allow us to ensure our performance is at least larger than the performance associated with the offline data.\nSecondly, log(NGr (1/N)) measures the complexity of the function class Gr. For example, when using linear models, it takes O\u0303(d). We refer the reader to van de Geer (2000) for bracketing number computations for more general classes. Thirdly, \u03ba represents the non-linearity of the link function \u03a6, which determines the difficulty of estimating the reward from human preferences. This dependence on \u03ba is present in the existing literature of PbRL, both in online settings (Pacchiano et al., 2021; Chen et al., 2022) and offline settings (Zhu et al., 2023). Remark 2 (Comparison to Zhu et al. (2023)). By specializing our result to the linear model, we recover the result in Zhu et al. (2023). Specifically, the bracketing number is calculated in Proposition 1, and Cr(Gr, \u03c0tar, \u00b5ref) is reduced to a relative condition number. The details are deferred to Appendix D. Remark 3. In practice, to compute E\u03c4\u223c\u00b51 [r(\u03c4)] in Line 3, we can use the sample average, with an additional cost of \u221a log(1/\u03b4)/N in the suboptimality bound in Eq. (2)."
        },
        {
            "heading": "4.3 DISCUSSION OF THE CONCENTRABILITY COEFFICIENT",
            "text": "In the worst-case scenario (i.e., Gr is the set of all functions mapping from T to R), the value of Cr(Gr, \u03c0tar, \u00b51) is reduced to to the per-trajectory concentrability coefficient Ctr. The pertrajectory concentrability coefficient is generally larger than the per-step concentrability coefficient Cst commonly used in the general offline RL literature. Specifically, Cst is defined as\nCst := max s,a,h\nd\u03c0tarh (s, a)/\u00b50,h(s, a),\nwhere \u00b50,h(s, a) represents the marginal distribution at step h. In this section, we show the dependence on the per-trajectory concentrability coefficient is necessary for our offline PbRL context. This is intuitively because our PbRL setting involves reward functions defined over trajectories, reflecting the fact that human feedback is also trajectory-based.\nIn the next proposition, we first show that the per-trajectory concentrability coefficient Ctr can be exponentially larger than the per-step concentrability coefficient Cst. Proposition 2. For any S \u2265 1, A \u2265 2, H \u2265 1, C \u2265 1, there exists an MDP M with horizon H , a policy \u03c0tar and a data distribution \u00b50 such that |S| = S, |A| = A and Cst = C while Ctr = CH . Proposition 2 indicates that Ctr can be significantly larger than Cst. A natural question arises as to whether we can obtain suboptimality guarantees using Cst. Unfortunately, the following lower bounds reveal that per-step concentrability is not sufficient to guarantee efficient learning in trajectory-based PbRL setting, even when the reward function is defined over state-action pairs: Theorem 2. Set \u03c0tar = \u03c0\u22c6. Then, for any C > 1 and H \u2265 2, there exists a dataset distribution \u00b51 such that we have\ninf \u03c0\u0302 sup (M,\u00b50)\u2208\u0398st(C)\nED[J(\u03c0\u22c6; r\u22c6, P \u22c6)\u2212 J(\u03c0\u0302; r\u22c6, P \u22c6)] \u2273 min { C \u2212 1, 1 } ,\nwhere \u03c0\u0302 is any mesurable function of the data D (and knows the information of \u00b51). \u0398st(C) is the set of all MDPs with per-step reward, offline distribution (M, \u00b50) such that Cst \u2264 C. Note ED is taken with respect to the randomness in D. Theorem 2 indicates that learning in our setting is intrinsically hard due to trajectory-based feedback, even if we restrict the reward function class. In addition, we can show that scaling with Ctr is necessary in our setting: Theorem 3. Set \u03c0tar = \u03c0\u22c6. Then for any C > 1 and H \u2265 1, there exists a dataset distribution \u00b51 such that we have\ninf \u03c0\u0302 sup (M,\u00b50)\u2208\u0398tr(C)\nED[J(\u03c0\u22c6; r\u22c6, P \u22c6)\u2212 J(\u03c0\u0302; r\u22c6, P \u22c6)] \u2273 min { C \u2212 1, \u221a C \u2212 1 N } ,\nwhere \u03c0\u0302 is any mesurable function of the data D (and knows the information of \u00b51). \u0398tr is the set of all MDP, offline distribution (M, \u00b50) such that Ctr \u2264 C. Note ED is taken with respect to the randomness in D.\nAlgorithm 2 FREEHAND-transition Input: offline dataset D, slackness parameter \u03b6, \u03b6Ph , reference distribution \u00b5ref MLE for reward: compute r\u0302 = argmaxr\u2208Gr \u2211N n=1 logPr(o = o\nn|\u03c4n,1, \u03c4n,0) MLE for transition: compute P\u0302h = argmaxPh\u2208GPh \u2211N n=1 \u22111 i=0 logPh(s n,i h+1|s n,i h , a n,i h ) Confidence set construction: for 0 \u2264 h \u2264 H \u2212 1, construct\nR(D) = { r \u2208 Gr : \u2211N n=1 logPr(o = o n|\u03c4n,0, \u03c4n,1) \u2265 \u2211N n=1 logPr\u0302(o = o n|\u03c4n,0, \u03c4n,1)\u2212 \u03b6 } .\nPh(D) = { Ph \u2208 GPh : N\u2211 n=1 1\u2211 i=0 logPh(s n,i h+1|s n,i h , a n,i h ) \u2265 N\u2211 n=1 1\u2211 i=0 log P\u0302h(s n,i h+1|s n,i h , a n,i h )\u2212\u03b6Ph } ,\nDistributionally robust plnanning: return \u03c0\u0302 = argmax\u03c0\u2208\u03a0his minr\u2208R(D),Ph\u2208Ph(D) J ( \u03c0; r, {Ph}H\u22121h=0 ) ) \u2212 E\u03c4\u223c\u00b5ref [r(\u03c4)].\nNote that when \u00b51 is known, we can set \u00b5ref = \u00b51 in Algorithm 1 and then Cr(Gr, \u03c0tar, \u00b51) \u2264 \u221a Ctr, which implies the sample complexity in Theorem 1 indeed nearly matches this lower bound with respect to Ctr and N when N is sufficiently large.\nIn summary, Theorem 2 and Theorem 3 imply that the scaling with the per-trajectory concentrability coefficient is essential in the trajectory-based pairwise-comparison setting, and it cannot be relaxed to the per-step concentrability without additional assumptions, such as on the reward structure. To the best of our knowledge, this is the first theoretical result indicating that trajectory-wise feedback is intrinsically harder to learn than step-wise feedback in offline PbRL."
        },
        {
            "heading": "5 TRAJECTORY-BASED COMPARISON WITH UNKNOWN TRANSITION",
            "text": "We extend the setting presented in Section 4 to the scenario where the transition function P \u22c6 is unknown. The algorithm is described in Algorithm 2. Compared to Algorithm 1, we simply added a similar step to handle unknown transitions. Hereafter, we use the convention P0(\u00b7 | s, a) := P0(\u00b7). Our sample complexity will depend on the following additional concentration coefficient: Definition 3 (Concentrability coefficient for the transition). The concentrability coefficient w.r.t. transition classes {GPh} and a target policy \u03c0tar is defined as\nCP ({GPh}, \u03c0tar) := max h:0\u2264h\u2264H\u22121 sup Ph\u2208GPh\nE(s,a)\u223cd\u03c0tarh [\u2225Ph(\u00b7 | s, a)\u2212 P \u22c6 h (\u00b7 | s, a)\u22251]\u221a\nE(s,a)\u223c(\u00b50,h/2+\u00b51,h/2)[\u2225Ph(\u00b7 | s, a)\u2212 P \u22c6h (\u00b7 | s, a)\u222521] .\nNote this is always upper-bounded by the density-ratio-based concentrability coefficient, CP ({GPh}, \u03c0tar) \u2264 sup(s,a,h)\u2208S\u00d7A\u00d7[H] d \u03c0tar h (s,a)\n\u00b50,h(s,a)/2+\u00b51,h(s,a)/2 .\nWe also assume the transition classes {GPh} H\u22121 h=0 are realizable: Assumption 4 (Realizability). Suppose that we have P \u22c6h \u2208 GPh for all h where 0 \u2264 h \u2264 H \u2212 1. In addition, any choice Ph \u2208 GPh for 0 \u2264 h \u2264 H \u2212 1 are valid transition distributions.\nThen with the above assumptions, we have the following theorem to characterize the sample complexity when the transition is unknown: Theorem 4. For any \u03b4 \u2208 (0, 1], let \u03b6 = cMLE log(NGr (1/N)/\u03b4),\u03b6Ph = cP log(HNGPh (1/N)/\u03b4) where cMLE, cP > 0 are universal constants, then under Assumption 1,2,3 and 4, we have\nJ(\u03c0tar; r \u22c6, P \u22c6)\u2212 J(\u03c0\u0302; r\u22c6, P \u22c6) \u2264 \u221a\ncC2r (Gr, \u03c0tar, \u00b5ref)\u03ba2 log(NGr (1/N)/\u03b4) N +Hrmax\n\u221a cC2P ({GPh}, \u03c0tar) log(HNP (1/N)/\u03b4)\nN ,\nwhere c > 0 and \u03ba are the same as Theorem 1 and NP := max0\u2264h\u2264H\u22121 NGPh . Compared to Theorem 1, we introduce an additional term in our guarantee to account for the unknown transitions. Once again, our result demonstrates that the learned policy can achieve performance comparable to any target policy \u03c0tar covered by the offline data, i.e., Cr(Gr, \u03c0tar, \u00b5ref) < \u221e, CP ({GPh}, \u03c0tar) < \u221e.\nAlgorithm 3 FREEHAND-action 1: Input: offline datset D. 2: MLE: compute A\u0302h = argmaxAh\u2208GAh \u2211N n=1 logPAh(o = o n h | snh, a n,0 h , a n,1 h ) \u2200h \u2208 [H]\n3: Greedy policy: return \u03c0\u0302h(s) = argmaxa\u2208A A\u0302h(s, a)\nRemark 4 (Comparison to Uehara and Sun (2021) ). Like us, Uehara and Sun (2021) proposed a model-based RL algorithm that works under partial coverage, but in the standard RL setting and with a known state-action-wise reward function. In addition to the difference in settings, which is the primary difference, our approach moreover differs from their approach because while they construct confidence intervals by defining a confidence ball around the MLE solution based on the total variation distance, we use the Kullback-Leibler (KL) distance. This may be preferable as computing the KL distance is generally easier than the total variation distance as it arises directly from the MLE objective, as practically done in Rigter et al. (2022)."
        },
        {
            "heading": "6 ACTION-BASED COMPARISON",
            "text": "Next, we turn our attention to the action-based comparison setting (Ramachandran and Amir, 2007; Zhu et al., 2023), where human evaluators provide preferences between pairs of actions instead of pairs of trajectories. In this section, we assume that the reward function r\u22c6 is state-action-wise: r\u22c6(\u03c4) = \u2211H h=1 r \u22c6 h(sh, ah) for \u03c4 = (s1, a1, \u00b7 \u00b7 \u00b7 , sH , aH). And, we consider a preference model based on Q\u22c6.\nSetting. We have datasets D = {Dh}Hh=1 with Dh = {(snh, a n,0 h , a n,1 h , o n h)}Nn=1 for each h \u2208 [H], where each sample is drawn i.i.d. from the distribution snh \u223c \u00b5h, a n,0 h \u223c \u00b50,h(\u00b7 | snh), a n,1 h \u223c \u00b51,h(\u00b7 | snh) and o n h \u2208 {0, 1} indicates preference for a 1,n h over a 0,n h in the state s n h . We assume it satisfies the following preference model:\nAssumption 5 (Action-based comparison model). Given a pair of actions a0h, a1h and state sh, o \u2208 {0, 1} satisfies\nP (oh = 1 | sh, a0h, a1h) = \u03a6(Q\u22c6h(sh, a1h)\u2212Q\u22c6h(sh, a0h)).\nHere, the aforementioned distribution can be equivalently expressed as P (onh = 1 | snh, a n,0 h , a n,1 h ) = \u03a6(A\u22c6h(s n h, a n,1 h )\u2212A\u22c6h(snh, a n,0 h )), where A\n\u22c6 denotes the optimal advantage function. Consequently, we introduce general function classes GAh to estimate the optimal advantage function A\u22c6h. In addition, for each Ah \u2208 GAh and (s, a0, a1) \u2208 S \u00d7A\u00d7A, we use PAh(\u00b7 | s, a0, a1) to represent the human preference model with respect to Ah, defined as PAh(o = 1 | s, a0, a1) := \u03a6(Ah(s, a1)\u2212Ah(s, a0)). We again use the \u03f5-bracket number of such advantage function classes to quantify their complexity, denoted as NGAh . The full formal definition is provided in Appendix E."
        },
        {
            "heading": "6.1 ALGORITHM",
            "text": "Our algorithm comprises two steps. In the first step (Line 2), our objective is to estimate the optimal advantage function using MLE. In the second step (Line 3), we determine the policy by selecting the action with the highest advantage value based on the learned advantage function."
        },
        {
            "heading": "6.2 ANALYSIS",
            "text": "Now we show that FREEHAND-action is able to learn a near-optimal policy as long as offline data covers the optimal policy. Our analysis depends on the following assumption on the margin of Q\u22c6:\nAssumption 6 (Soft margin). There exists \u03b10 \u2208 R+, \u03b2 \u2208 (0,\u221e] such that for all a \u2208 A, h \u2208 [H], \u03b1 > 0, we have P\u03c0\u22c6,P\u22c6(0 < |Q\u22c6h(sh, \u03c0\u22c6(sh))\u2212Q\u22c6h(sh, a)| < \u03b1) \u2264 (\u03b1/\u03b10)\u03b2 .\nThe soft margin is widely used in the literature on classification, decision making, and RL (Audibert and Tsybakov, 2007; Perchet and Rigollet, 2013; Luedtke and Chambaz, 2020; Hu et al., 2021; 2022; Uehara et al., 2023). Note, when the optimal Q function satisfies a gap (as in Simchowitz and Jamieson, 2019; Wu et al., 2022), the soft margin assumption holds with \u03b2 = \u221e.\nNext, we introduce the concentrability coefficient for the action-based comparison setting, which is defined as follows. Definition 4 (concentrability coefficient for action-based comparison).\nCact := sup h\u2208[H],Ah\u2208GAh\nE(s,a0)\u223cd\u03c0\u22c6h ,a1\u223cUnif(\u00b7|s)[l(Ah, s, a 0, a1)]\nEs\u223c\u00b5h,a0\u223c\u00b50,h(\u00b7|s),a1\u223c\u00b51,h(\u00b7|s)[l(Ah, s, a0, a1)] ,\nwhere l(Ah, s, a0, a1) := |A\u22c6h(s, a0)\u2212 A\u22c6h(s, a1)\u2212 Ah(s, a0) + Ah(s, a1)|2 and Unif(\u00b7 | s) is the uniform policy over A.\nWe observe that Cact \u2264 (\nsup h\u2208[H],s\u2208S\nd\u03c0 \u22c6\nh (s)\n\u00b5h(s)\n) \u00b7 (\nsup h\u2208[H],s\u2208S,a0\u2208A\n\u03c0\u22c6h(a 0 | s)\n\u00b50,h(a0 | s)\n) \u00b7 ( 1\n|A| sup\nh\u2208[H],s\u2208S,a1\u2208A\n1\n\u00b51,h(a1 | s)\n) .\nBased on this bound, we can consider simple sufficient conditions for Cact to be finite. Firstly, regarding the first term, it is sufficient for the dataset distribution \u00b5h to cover the states visited by the optimal policy \u03c0\u22c6, denoted as d\u03c0 \u22c6\nh . Regarding the second term, we require \u00b50,h to cover \u03c0 \u22c6 h.\nAdditionally, the third term can be upper bounded when \u00b51,h can cover the whole action space. This is mild because \u2200s \u2208 S;\u00b5h(s) > 0 is not controllable to the learner; but \u2200(s, a) \u2208 S\u00d7A;\u00b51,h(a | s) > 0 is controllable to the learner in the data-collection process. To summarize, Cact < \u221e primarily requires partial coverage over the state space with respect to the optimal policy, which is preferable in practical applications where S can be very large. Additionally, we introduce several assumptions on the function classes similar to those in Section 4. Assumption 7. For all h \u2208 [H], we have A\u22c6h \u2208 GAh . Assumption 8. For all h \u2208 [H] and Ah \u2208 GAh , we have |Ah(s, a)| \u2264 bmax for all (s, a) \u2208 S \u00d7A.\nWith the aforementioned assumptions, we can establish the sample complexity of FREEHAND-action. Theorem 5. Under Assumption 5,6,7 and 8, we have with probability at least 1\u2212 \u03b4 that\nJ(\u03c0\u22c6; r\u22c6, P \u22c6)\u2212 J(\u03c0\u0302; r\u22c6, P \u22c6) \u2264 cH|A| ( 2\n\u03b2\n) \u03b2\u22122 \u03b2+2 ( 1\n\u03b10\n) 2\u03b2 \u03b2+2 ( \u03ba2ACact log(HNGA(1/N)/\u03b4)\nN\n) \u03b2 \u03b2+2\n,\nwhere NGA := maxh\u2208[H] NGAh and \u03baA = 1 infx\u2208[\u2212bmax,bmax] \u03a6 \u2032(x) .\nTheorem 5 suggests that FREEHAND-action can learn a near-optimal policy as long as Cact takes a finite value under a soft margin. Specifically, when a hard margin is imposed (i.e., \u03b2 = \u221e), FREEHAND-action can learn an \u03f5-optimal policy with a sample complexity of N = O\u0303(1/\u03f5), which is faster than a typical rate O\u0303(1/\u03f52). As mentioned earlier, the quantity Cact represents the extent to which the distribution induced by the optimal policy is covered by the offline data. Therefore, there is no need for a potentially stringent condition that requires the offline data to cover the entire state space like Zhu et al. (2023).\nFurthermore, our guarantee is designed to overcome the limitations of existing approaches. In Theorem 1, our upper-bound is influenced by the parameter \u03ba. When using a common sigmoid link function, this parameter scales with \u0398(exp(rmax)). As a result, in dense reward settings where rmax scales with H , this scaling factor may lead to an explicit dependence of \u0398(exp(H)). Similar observations have been made in previous works (Zhu et al., 2023; Pacchiano et al., 2021; Chen et al., 2022). However, even if rmax scales with H , it is known that the \u2113\u221e-norm of the advantage function, denoted as bmax, can take much smaller values (Ross et al., 2011; Agarwal et al., 2019) Hence, we can avoid the explicit dependence on \u0398(exp(H))."
        },
        {
            "heading": "7 CONCLUSIONS",
            "text": "We propose the first algorithm for trajectory-wise PbRL with general function approximation and under partial coverage. We establish lower bounds that explain the differences between our PbRL model and standard RL with direct reward feedback. Moreover, we extend our algorithm to unknown transitions and to preference feedback over actions, all while maintaining strong guarantees under partial coverage."
        }
    ],
    "year": 2023
}