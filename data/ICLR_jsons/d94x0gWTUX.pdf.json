{
    "abstractText": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei Li"
        },
        {
            "affiliations": [],
            "name": "Yekun Chai"
        },
        {
            "affiliations": [],
            "name": "Shuohuan Wang"
        },
        {
            "affiliations": [],
            "name": "Yu Sun"
        },
        {
            "affiliations": [],
            "name": "Hao Tian"
        },
        {
            "affiliations": [],
            "name": "Ningyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Hua Wu"
        }
    ],
    "id": "SP:7f40b955d829cb051ec84b02debfab20234acc13",
    "references": [
        {
            "authors": [
                "Lucas Gonzalez"
            ],
            "title": "Palm 2 technical report",
            "venue": "CoRR, abs/2305.10403,",
            "year": 2023
        },
        {
            "authors": [
                "Olsson",
                "Dario Amodei",
                "Tom B. Brown",
                "Jack Clark",
                "Sam McCandlish",
                "Chris Olah",
                "Benjamin Mann",
                "Jared Kaplan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human",
            "venue": "feedback. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Mann",
                "Jared Kaplan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human",
            "venue": "feedback. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Bukharin",
                "Yixiao Li",
                "Pengcheng He",
                "Weizhu Chen",
                "Tuo Zhao"
            ],
            "title": "Deep reinforcement learning from hierarchical weak preference",
            "venue": "feedback. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Yekun Chai",
                "Shuohuan Wang",
                "Chao Pang",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu"
            ],
            "title": "Ernie-code: Beyond english-centric cross-lingual pretraining for programming languages",
            "venue": "Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "CoRR, abs/2110.14168,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Hanze Dong",
                "Wei Xiong",
                "Deepanshu Goyal",
                "Rui Pan",
                "Shizhe Diao",
                "Jipeng Zhang",
                "Kashun Shum",
                "Tong Zhang"
            ],
            "title": "RAFT: reward ranked finetuning for generative foundation model alignment",
            "venue": "CoRR, abs/2304.06767,",
            "year": 2023
        },
        {
            "authors": [
                "Chelsea Finn",
                "Sergey Levine",
                "Pieter Abbeel"
            ],
            "title": "Guided cost learning: Deep inverse optimal control via policy optimization",
            "venue": "Proceedings of the 33nd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Leo Gao",
                "John Schulman",
                "Jacob Hilton"
            ],
            "title": "Scaling laws for reward model overoptimization",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Stefano Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Mojtaba Komeili",
                "Kurt Shuster",
                "Jason Weston"
            ],
            "title": "Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur P. Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee",
                "Kristina Toutanova",
                "Llion Jones",
                "Matthew Kelcey",
                "Ming-Wei Chang",
                "Andrew M. Dai",
                "Jakob Uszkoreit",
                "Quoc Le",
                "Slav Petrov"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "venue": "Trans. Assoc. Comput. Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Hung Le",
                "Yue Wang",
                "Akhilesh Deepak Gotmare",
                "Silvio Savarese",
                "Steven Chu-Hong Hoi"
            ],
            "title": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Jan Leike",
                "David Krueger",
                "Tom Everitt",
                "Miljan Martic",
                "Vishal Maini",
                "Shane Legg"
            ],
            "title": "Scalable agent alignment via reward modeling: a research",
            "venue": "direction. CoRR,",
            "year": 2018
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Barlas Oguz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk"
            ],
            "title": "MLQA: evaluating cross-lingual extractive question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Aitor Lewkowycz",
                "Anders Andreassen",
                "David Dohan",
                "Ethan Dyer",
                "Henryk Michalewski",
                "Vinay V. Ramasesh",
                "Ambrose Slone",
                "Cem Anil",
                "Imanol Schlag",
                "Theo GutmanSolo",
                "Yuhuai Wu",
                "Behnam Neyshabur",
                "Guy Gur-Ari",
                "Vedant Misra"
            ],
            "title": "Solving quantitative reasoning problems with language models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Contractor",
                "Siva Reddy",
                "Daniel Fried",
                "Dzmitry Bahdanau",
                "Yacine Jernite",
                "Carlos Mu\u00f1oz Ferrandis",
                "Sean Hughes",
                "Thomas Wolf",
                "Arjun Guha",
                "Leandro von Werra",
                "Harm de Vries"
            ],
            "title": "Starcoder: may the source be with you",
            "venue": "CoRR, abs/2305.06161,",
            "year": 2023
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans"
            ],
            "title": "Truthfulqa: Measuring how models mimic human falsehoods",
            "year": 2022
        },
        {
            "authors": [
                "Xi Victoria Lin",
                "Todor Mihaylov",
                "Mikel Artetxe",
                "Tianlu Wang",
                "Shuohui Chen",
                "Daniel Simig",
                "Myle Ott",
                "Naman Goyal",
                "Shruti Bhosale",
                "Jingfei Du",
                "Ramakanth Pasunuru",
                "Sam Shleifer",
                "Punit Singh Koura",
                "Vishrav Chaudhary",
                "Brian O\u2019Horo",
                "Jeff Wang",
                "Luke Zettlemoyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Pan Lu",
                "Baolin Peng",
                "Hao Cheng",
                "Michel Galley",
                "Kai-Wei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Jianfeng Gao"
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "venue": "CoRR, abs/2304.09842,",
            "year": 2023
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul"
            ],
            "title": "Peft: State-of-the-art parameter-efficient fine-tuning methods",
            "venue": "https://github.com/ huggingface/peft,",
            "year": 2022
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u0131",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ramakanth Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz",
                "Edouard Grave",
                "Yann LeCun",
                "Thomas Scialom"
            ],
            "title": "Augmented language models: a survey",
            "venue": "CoRR, abs/2302.07842,",
            "year": 2023
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders",
                "Xu Jiang",
                "Karl Cobbe",
                "Tyna Eloundou",
                "Gretchen Krueger",
                "Kevin Button",
                "Matthew Knight",
                "Benjamin Chess",
                "John Schulman"
            ],
            "title": "Webgpt: Browser-assisted question-answering with human",
            "venue": "feedback. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L. Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray",
                "John Schulman",
                "Jacob Hilton",
                "Fraser Kelton",
                "Luke Miller",
                "Maddie Simens",
                "Amanda Askell",
                "Peter Welinder",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Dean Pomerleau"
            ],
            "title": "Efficient training of artificial neural networks for autonomous navigation",
            "venue": "Neural Comput.,",
            "year": 1991
        },
        {
            "authors": [
                "Cheng Qian",
                "Chi Han",
                "Yi R. Fung",
                "Yujia Qin",
                "Zhiyuan Liu",
                "Heng Ji"
            ],
            "title": "CREATOR: disentangling abstract and concrete reasonings of large language models through tool creation",
            "venue": "CoRR, abs/2305.14318,",
            "year": 2023
        },
        {
            "authors": [
                "Shuofei Qiao",
                "Honghao Gui",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "title": "Making language models better tool learners with execution",
            "venue": "feedback. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Yang",
                "Tongshuang Wu",
                "Heng Ji",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "title": "Tool learning with foundation models",
            "venue": "CoRR, abs/2304.08354,",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u0131",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use",
            "venue": "tools. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Bo Shen",
                "Jiaxin Zhang",
                "Taihong Chen",
                "Daoguang Zan",
                "Bing Geng",
                "An Fu",
                "Muhan Zeng",
                "Ailun Yu",
                "Jichuan Ji",
                "Jingyang Zhao",
                "Yuenan Guo",
                "Qianxiang Wang"
            ],
            "title": "Pangu-coder2: Boosting large language models for code with ranking",
            "venue": "feedback. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "title": "Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface",
            "venue": "CoRR, abs/2303.17580,",
            "year": 2023
        },
        {
            "authors": [
                "Parshin Shojaee",
                "Aneesh Jain",
                "Sindhu Tipirneni",
                "Chandan K. Reddy"
            ],
            "title": "Execution-based code generation using deep reinforcement learning",
            "venue": "CoRR, abs/2301.13816,",
            "year": 2023
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeff Wu",
                "Daniel M. Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F. Christiano"
            ],
            "title": "Learning to summarize from human",
            "venue": "URL https://arxiv.org/abs/2009.01325",
            "year": 2009
        },
        {
            "authors": [
                "Qiaoyu Tang",
                "Ziliang Deng",
                "Hongyu Lin",
                "Xianpei Han",
                "Qiao Liang",
                "Le Sun"
            ],
            "title": "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases",
            "venue": "CoRR, abs/2306.05301,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) have demonstrated remarkable potential in performing complex tasks that demand expertise across diverse domains, such as programming (Chen et al., 2021; Li et al., 2022; Chai et al., 2023; Li et al., 2023) and dialogue assistance (Bai et al., 2022a; Ouyang et al., 2022; OpenAI, 2023; Anil et al., 2023; Touvron et al., 2023). Leveraging reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) has emerged as a compelling approach for optimizing LLMs against reward models (RMs) to predict human preferences. RMs serve as imperfect proxies for human feedback signals, producing rewards that are pivotal for fine-tuning LLMs in RLHF. However, RMs predict human preferences relying on static internal representations stored within their weights, which inherently impose limitations of LLMs. These may encompass challenges in accessing real-time information on current events (Komeili et al., 2022) and a propensity to hallucinate erroneous facts (Zhang et al., 2023), a lack of proficiency in arithmetic computation (Lewkowycz et al., 2022), and difficulties in comprehending low-resource languages (Lin et al., 2022b), among others. These limitations underscore the imperative need to engage external sources of information to augment and improve the effectiveness of RMs.\nTo further motivate this shift, an intriguing question arises when considering the role of human labelers in generating RM data: do these labelers possess an intuitive inclination to resort to external\n\u2217Equal contribution and shared co-first authorship. Work done during LL\u2019s internship at Baidu. Correspondence to: SW, NZ. 1https://github.com/ernie-research/Tool-Augmented-Reward-Model\ntools, much like humans themselves (akin to human problem-solving behavior) ? The motivation behind this question stems from the observation that even human labelers, when faced with complex tasks, often turn to external aids such as search engines or calculators to retrieve knowledge, validate facts, and facilitate their decision-making process. On the other hand, recent studies have unveiled the impressive performance gains that can be achieved by integrating external tools into the reasoning process of LLMs. Recent works such as Chain-of-Thought (Wei et al., 2022) and ReAct (Yao et al., 2023) have demonstrated that step-by-step reasoning and tool use can significantly enhance the planning and reasoning abilities of LLMs, enabling them to successfully complete intricate tasks.\nIn response to these insights, this paper presents Themis, a tool-augmented RM framework that combines tool engagement and reasoning process in a sequential and step-by-step manner. Our approach endows RMs with the capacity to make dynamic decisions regarding which APIs to call, when to invoke them, what arguments to pass, and how to effectively incorporate the results into the broader reasoning process. This approach empowers RMs to engage in dynamic reasoning, enabling it to make high-level plans for tool use (reasoning to tools) while also interacting with external environments to incorporate additional information into its reasoning (reasoning to rewards).\nOne crucial advantage of this framework is that it offers a significant departure from vanilla pairwise RMs, which has been inherently likened to a \u201cblackbox\u201d due to its opacity in revealing the internal reasoning process. In contrast, our approach provides a transparent and sequential account of actions and verbal reasoning traces specific to a given task. This transparency not only enhances human interpretability but also engenders trustworthiness, as it unveils the inner workings of the RM\u2019s decision-making process. This facilitates fine-tuning and modification of intermediate steps to exert precise control over the reward generation process.\nTo facilitate the exploration and validation of our proposed framework, we meticulously curated a comprehensive dataset comprising interactions with seven distinct external tools. The construction of this dataset was a collaborative endeavor, synergizing the generative capabilities of GPT-4 (OpenAI, 2023) as a prompting engine, tool-executed-based filtering, and human annotations.\nWe comprehensively evaluate Themis across a diverse spectrum of domains, encompassing the utilization of these seven distinct external tools. Experimental results demonstrate that Themis yields a remarkable 17.7% improvement compared to conventional RMs that lack access to external tools, across eight distinct tasks. Moreover, Themis outperforms Gopher 280B by a substantial margin of 7.3% on TruthfulQA benchmark, consistently surpassing baseline RMs. These compelling results underscore the effectiveness and generalization capability of Themis in enhancing truthfulness and factuality in preference modeling. Furthermore, we extend our investigation to RLHF fine-tuning, revealing that our method attains an impressive 32% win rate on average across four different tasks when compared to vanilla RMs, as determined through human preference evaluation. This further demonstrates the practical utility and superiority of our approach in real-world applications.\nTo summarize, our key contribution are encapsulated as follows: (1) We advance the domain of toolaugmented preference modeling by introducing the Themis framework. This framework harnesses the power of external tools to augment preference modeling in LLMs. In doing so, it mitigates inherent limitations observed in conventional RMs. Additionally, our approach unveils the inner workings of the RM\u2019s decision-making process, providing transparency and interpretability. (2) We present a novel tool-augmented reward modeling dataset (TARA) that includes comprehensive comparison data of human preferences and detailed tool invocation processes. This dataset will be made publicly available in hopes of facilitating research advancements in the field. (3) Our contributions are substantiated through experimental evaluations conducted across eight diverse tasks within TARA, as well as benchmarking against TruthfulQA and Retarded-bar datasets. These experiments conclusively demonstrate the effectiveness of our approach in enhancing the performance of LLMs."
        },
        {
            "heading": "2 TOOL-AUGMENTED REWARD MODELING",
            "text": ""
        },
        {
            "heading": "2.1 REVISITING REWARD MODELS",
            "text": "In RLHF (Ouyang et al., 2022; Stiennon et al., 2020), RM is trained on a human preference dataset consisting of comparisons between two candidate model outputs generated in response to the same input prompt. The vanilla RM operates by mapping each input prompt and its corresponding gen-\nerated output to a scalar reward, thereby encapsulating the overall preference between them. Mathematically, for a given question denoted as x with a positively preferred answer represented as yw and a negatively preferred answer as yl, the loss function of the vanilla RM is formulated as:\nLRM = \u2212E(x,yw,yl)\u223cD[log(\u03c3(r\u03b8(x, yw)\u2212 r\u03b8(x, yl)))] (1)\nHere, r\u03b8(x, y) represents the scalar output of the RM for question x and answer y, \u03c3 denotes sigmoid function, while D denotes the preference dataset. However, as highlighted earlier, this vanilla RM approach, while useful in aligning reward scores with human preferences, is constrained by limitations pertaining to timeliness and knowledge accessibility. It faces particular challenges when confronted with complex tasks such as mathematical problem-solving and reasoning.\n2.2 THEMIS : TOOL-AUGMENTED REWARD MODELING\nFigure 1 presents an overview of the Themis framework, illustrating how it integrates tool engagement and reasoning processes in a structured, step-by-step manner. Our approach enhances RMs with the capability to make informed and dynamic decisions concerning which APIs to employ, when to invoke them, what arguments to pass, and how to effectively integrate the obtained results into the broader reasoning process. This comprehensive framework, depicted through step-by-step trajectories, encapsulates the entirety of the decision-making and reasoning journey, consisting of the following pivotal stages:\n\u2022 Thought: At this initial stage, the model evaluates whether it should engage external APIs (referred to as tool reasoning). \u2022 Action: Subsequently, the model generates the necessary API calls along with the corresponding arguments required for the interactions. \u2022 Observation: The results produced by the external APIs are collected and stored. \u2022 Rationale: This stage involves the aggregation and synthesis of previously acquired information,\nfostering both induction and reasoning processes, specifically tailored for reward modeling.\nHumanTool\nRationale\nStep 3. Tool-invoked Processes Generation via Multi-Agents Step 4. Data FilteringStep 2. ToolBank ConstructionStep 1. Question-Answer Pair Collection\nOpen-Source dataset\nRule-based\nTool invocation\nTool execution\nRationale generation\nToolBank Filter\n[Question] [Positive Answer] [Tool Inovcations]\n[Rationale]\n[Question] [Negative Answer] [Tool Inovcations]\n[Rationale]\nFigure 2: An illustration of data creation pipline for our Tool-Augmented DatAset (TARA).\n\u2022 Reward: Finally, the model leverages the accumulated insights and information to generate a\nscalar reward score with a feed-forward layer, reflecting its overall preference based on the collective evidence.\nGiven a question x and the corresponding generated output y, we consider a generalized RM agent parameterized by \u03b8 with the capability to interact with external tools. Following Yao et al. (2023), we define the agent\u2019s action state at \u2208 A at step t as a combination of a natural language thought a\u0302t and a tool acting state a\u0304t \u2208 T . These paired \u201c(thought, action)\u201d states are denoted as at = (a\u0302t, a\u0304t). The purpose of the thought component a\u0302t is to encapsulate the comprehension of pertinent information and to guide the ensuing action a\u0304t. This action is determined following a policy p\u03b8(a\u0304t|x, y, ct, a\u0302t), where ct = (a1, o1, \u00b7 \u00b7 \u00b7 , at\u22121, ot\u22121). Subsequently, the RM agent is tasked with predicting a reasoning thought based on the preceding context denoted as sT . This reasoned thought Rationale plays a pivotal role in enhancing the model\u2019s ability to summarize and reason effectively, drawing upon the historical reasoning traces before ultimately predicting the scalar reward r. This approach closely mirrors the step-by-step reasoning paradigm established by Chain-of-Thought (Wei et al., 2022), accentuating the incremental and interactive nature inherent in the RM\u2019s decision-making process. Formally, the complete reasoning trajectory is represented as c1...T = (a1, o1, \u00b7 \u00b7 \u00b7 , aT , oT , sT ). Consequently, the reward can be denoted as r\u03b8(x, y, c1\u00b7\u00b7\u00b7T ). During the training phase, we implement an auto-regressive training objective for the prediction of the next token in modeling the reasoning context ct. In the context of reward training, we produce a scalar reward based on ct by a fully connected layer and employ the same pair-wise ranking loss as utilized in conventional RMs. This loss function serves as a foundational component to discern and rank the relative preferences between different model-generated outputs. All stages, with the exception of the Reward stage, utilize language model heads to generate text tokens (same as language models). In the Reward stages, a real-valued score is produced using a feed-forward layer (same as conventional RMs).\nTraining Objectives The overall training objective is comprised of two distinct components: the pair-wise ranking loss and the auto-regressive language modeling loss. The former aligns with equation 1, while the latter is designed to empower RMs with the ability to perform tool invocation via supervised fine-tuning:\nLtotal = LRM\ufe38\ufe37\ufe37\ufe38 pair-wise ranking loss +\u03b1 ( T\u2211 t=1 (Ltool(t) + \u03b2LObservation(t)) + \u03c9LRationale )\n\ufe38 \ufe37\ufe37 \ufe38 auto-regressive language modeling loss\n(2)\nwhere \u03b2, \u03c9 \u2208 {0, 1} are hyper-parameters to modulate different training configurations, T represents the number of tool invocations, \u03b1 = 1 in our experiments. Please refer to Table 8 for details.\nConnection to Vanilla RM When \u03b1 is set to zero, the autoregressive loss term becomes null, effectively reducing Themis to standard RMs. Ideal RMs should possess the ability to discern when and whether to employ external tools. To impart this knowledge, we incorporate tool-related data alongside non-tool data, thereby instructing RMs on the appropriate timing for tool invocation. Notably, this framework inherently encompasses the functionality of vanilla RMs."
        },
        {
            "heading": "3 TOOL-AUGMENTED REWARD DATASET",
            "text": ""
        },
        {
            "heading": "3.1 DATA COLLECTION",
            "text": "The comprehensive construction process of the Tool-Augmented Reward dAtaset (TARA) is depicted in Figure 2. TARA is constructed by leveraging high-quality datasets and generating the\ntool invocation process through multi-agent interactions. This process can be subdivided into the following four key steps:\nStep 1: Question-Answer Pairs Collection Initially, we collect a reward dataset featuring each instance comprising a question, a positive answer, and a negative answer. To construct this dataset, we employ two distinct approaches: resume open-source, high-quality datasets, and generation from scratch with some heuristic methods. However, the above methods usually only produce positive answers. To address this concern, we leverage GPT-4 as a negative generation agent to generate antagonistic negative answers, which will be described in Step 3.\nStep 2: ToolBank Construction. Subsequently, we develop the toolbank, which encompasses three distinct types of tools: basic tools, query-based tools, and knowledgeable tools. Basic tools such as Calculator, Code Interpreter, and Translator, provide RMs with practical capabilities. Querybased tools, including Google, Weather, and Calendar Search, equip RMs with search capabilities to access up-to-date information. Knowledgeable tools enable RMs to tap into a knowledge base, enhancing the factual accuracy of rewards. Additionally, we introduce Multi-Tools, which contain sequential calls to multiple tools.\nStep 3: Tool-invoked Process Generation by Multi-Agents. To automate the generation of toolinvoked processes, we design a simulated environment featuring human participants and three virtual agents: the negative generation agent, the tool agent, and the rationale agent. Leveraging the substantial comprehension and generation capabilities of LLMs, we employ three GPT-4 to simulate the roles of the three agents. The negative generation agent is responsible for generating a negative answer to the question that has only a positive answer. It processes a question along with its positive answer and generates a negative answer that is indistinguishable from the positive one. The tool agent acts as an intermediary between humans and agents, deciding when, which, and how to invoke tools. Specifically, the tool agent receives a question-answer pair and produces Thought and Action stages, as outlined in Section 2.2. Then humans are tasked with invoking specific tools and observing the outcomes (Observation stage). The rationale agent is tasked with generating reasoning traces by comprehending previous contexts, synthesizing the question-answer pair, the tool invocation process, and the observations to systematically produce rewards (Rationale stage). Tool-invoked scenarios are simulated through interactions between agents and humans, yielding tool-invoked processes for RMs. The interaction process and prompts for each agent are detailed in Appendix Figure 13.\nStep 4: Data Filtering. To maintain data quality, we implement a straightforward yet effective filtering process on the generated data. Initially, we exclude tool-invoked processes acquired in Step 3 that exhibit invalid formats. Subsequently, we discard processes exceeding three interaction steps, lacking relevant function calls, or manifesting parsing errors in their execution results."
        },
        {
            "heading": "3.2 DATA STATISTICS",
            "text": "The data statistics and comparison to previous datasets are shown in Appendix Table 5. TARA comprises a total of 13,604 training datasets and 1,469 test sets, each consisting of a question, a positive answer, and a negative answer. TARA incorporates a diverse set of seven tools that span across various domains, encompassing mathematical operations, code-related inquiries, closed-ended and open-ended question answering, knowledge-based queries, and time-sensitive information requests."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTINGS",
            "text": "We utilized Vicuna-7B (Zheng et al., 2023) as the base model for Themis and compared it with conventional RMs, specifically Bert-Large (Devlin et al., 2019) and Vicuna-7B, denoted as RM (Bert-Large) and RM (Vicuna-7B) respectively, which serve as its underlying architectures. Our training and evaluation processes were conducted under two distinct settings: single-tool and mixedtool scenarios. In the single-tool configuration, the TARA data was partitioned based on tool types, with each model exclusively trained on a specific type of tool. In contrast, the mixed-tool scenario involved training models on the entirety of the TARA dataset, encompassing diverse tools. During evaluation, we compared the relative reward scores for positive and negative answers, using accu-\nracy as the evaluation metric. Further details about hyper-parameter choices and additional training specifics can be found in Appendix C.1."
        },
        {
            "heading": "4.2 MAIN RESULTS",
            "text": "Single-Tool vs. Mixed-Tool Performance. The main performance results of all models on TARA are shown in Table 1. Across both the single-tool and mixed-tool settings, it is evident that Themis consistently outperforms vanilla RMs significantly, exhibiting an improvement of +19.2% in the single-tool scenario and +17.7% in the mixed-tool context across 8 distinct tasks. Enabling access to external knowledge and information, specific tools significantly boost the performance of Calendar (+19.1%), Weather (+13.9%), Code (+14.3%), and Translator (+54.1%) respectively. Remarkably, Themis achieves a perfect accuracy of 100% on Calendar and Weather tasks. Moreover, it attains an accuracy above 98% on Code and Calendar tasks, providing substantial evidence for the efficacy and motivation behind integrating external tools into the reasoning process.\nIn mixed-tool experiments, Themis demonstrates robust performance in concurrently learning diverse tools, with 7 out of 8 tasks displaying superior performance compared to the baselines. Notably, Google Search exhibits a slight decline during mixed-tool training. This can be attributed to the diverse sources from which data is collected, especially from Reddit, resulting in overlapping domains and complexity for the models to learn effectively. Addressing this challenge necessitates meticulous data cleaning and rigorous human annotation processes for further exploration. These findings underscore the remarkable potential of Themis across a broader spectrum of tools, emphasizing its adaptability and versatility in real-world applications.\nScaling Trends in Themis. In our investigation, we explored models spanning different scales, ranging from Vicuna-7B to Vicuna-33B, all within the context of a mixed-tool setting. To enhance the efficiency of our training, we utilized the LoRA technique (Hu et al., 2022) for parameter-\nefficient fine-tuning. The experimental results, outlined in Table 1 (last 3 rows), elucidate a positive correlation between the scale of the model and its overall performance, a phenomenon that aligns with established scaling laws (Gao et al., 2023; Askell et al., 2021). We also full-parameter fine-tune Themis (Vicuna-33B) and obtain the best performance.\nEffect of Varying Training Epochs. As depicted in Figure 3 (left), we observe that RM (BertLarge) necessitates more training epochs to reach convergence, whereas RM (Vicuna-7B) achieves optimal performance after just a single epoch\u2014an observation consistent with prior research findings (Stiennon et al., 2020; Wu et al., 2021; Ouyang et al., 2022). In contrast, Themis does require additional training epochs to learn tool invocations and rewards effectively. However, it outperforms traditional RMs, even within a single training epoch.\nReward Difference Visualization. We visualize the average reward scores vary with the number of training epochs in the right of Figures 3. Themis consistently exhibits a proclivity to assign higher scores to positive answers and lower scores to negative answers, indicating a heightened level of differentiation. Moreover, this differentiation progressively intensifies as the model training."
        },
        {
            "heading": "4.3 ANALYZING THE ROLE OF TOOL USE",
            "text": "How Does Themis Learn to Use Tools? To understand the role of the tools and how the RMs learn the tool invoking, we analyze the relationship between the number of tool invocations, model performance, and training epochs as shown in Figure 4. Our findings indicate a pattern in the total number of tool calls, which initially increases and then decreases. Additionally, we observe a gradual reduction in the number of incorrect tool calls. These trends collectively suggest that Themis acquires the ability to invoke tools effectively. As depicted on the right side of Figure 4, our observations reveal that Themis consistently exhibits a higher propensity to utilize Google Search during problem-solving tasks. This aligns well with human behavior, reflecting the natural inclination of individuals to resort to search engines when seeking solutions.\nDoes Themis Really Make Decisions Based on Observations? We specifically selected data instances where Themis effectively differentiated between positive and negative answers. Subsequently, we manipulated the outcomes of its tool invocations (Observations) and adjusted its states to assess Themis\u2019s performance. For instance, let\u2019s consider the question \u201cWhat is the weather like in New York on 2023-06-24?\u201d. Both the positive answer and the initial Weather observation were recorded as \u201dSunny\u201d. We modified the observation to \u201dRaining\u201d, thus transforming the answer into a negative response. Remarkably, we observed only a marginal decrease in accuracy, highlighting the robust alignment between our method\u2019s predictions and the tool observations.\nAblation Study: Do Tool Use and Reasoning Traces Count? To comprehend the functionalities of the reasoning process of Themis, we set \u03b2 = 0 and \u03c9 = 0 to mask Observation and Rationale in Themis, respectively. The results can be seen in Table 1, highlighting the substantial contributions of both Observation and Rationale to Themis, especially in the Multi-Tools category. We find that the performance of Themis clearly drops when we exclude the Rationale component, proving the effectiveness of step-by-step reasoning before output reward scores.\nCase Study. We show some qualitative examples in Appendix D.1, showcasing the effectiveness of the tool invocations. By leveraging the external tools, Themis validates the answer accuracy and make decisions through a systematic, step-by-step reasoning process."
        },
        {
            "heading": "4.4 GENERALIZATION PROBING IN DOWNSTREAM TASKS",
            "text": "Out-of-Domain Evaluation. Ideally, Themis is expected to possess adaptive tool invocation capabilities and the ability to score unseen prompts and responses. Consequently, we select 150 instances (one instance has one positive answer and one negative answer) from HH-RLHF (Bai et al., 2022b) and denote it as HH-RLHF*. Initially, we assess the performance of both RMs and Themis (after training on our TARA dataset) in the zero-shot setting to estimate their extrapolation ability. As shown in Table 2, our findings reveal that Themis consistently outperforms all RMs, especially RM-Vicuna-7B. We further introduce 500 instances for model finetuning, and we find the effect of Themis is significantly improved. It indicates that our Themis can extrapolate to output-ofdomain scenarios effectively with minimal data fine-tuning.\nMore than RM: Truthfulness and Factuality Probing. Given that RMs are employed to rank various responses for a single prompt, it is natural to leverage RMs for addressing multiple-choice tasks. We experimented on multiple-choice problems on TruthfulQA (Lin et al., 2022a) and translated Retarded-bar2, denoted as Retarded-bar (en), to access the model\u2019s capacity for truthfulness and factuality (Refer to Appendix D.2 for dataset details). As shown in Table 3, Themis outperforms competitive LLMs including OPT 175B (+15.8%), GPT-3 (+15.8%), Galactica (+10.8%), and Gopher 280B (+7.3%) on TruthfulQA task in zero-shot evaluation. Moreover, we selected Retarded-bar, a challenging dataset containing puns, unusual punctuation, and irrational logic, to assess Themis\u2019s ability in solving fact-related confusing problems. We show that Themis overshadows the vanilla RM on Retarded-bar (en) by 5.3%. Examples in Appendix D.2 showcase that Themis can retrieve knowledge with external tools and enhance its truthfulness capability."
        },
        {
            "heading": "4.5 EXTENDED EXPERIMENTS: FROM RLHF TO RLTAF",
            "text": "Automatic Evaluation. We conducted experiments to assess the impact of Themis in RLHF, namely reinforcement learning from tool-augmented feedback (RLTAF). Following prior studies (Stiennon et al., 2020; Ouyang et al., 2022), we implemented three stages: supervised fine-tuning (SFT), traditional RMs, and fine-tuning the policy against these RMs using Proximal Policy Optimization (PPO). Utilizing TARA as our training data, detailed experimental specifics can be found\n2https://huggingface.co/datasets/hugfaceguy0001/retarded_bar\nin Appendix C.2. In RLHF, Themis employs external tools for tasks like arithmetic computation, code execution, and factual lookup. The results presented in Table 4 indicate that PPO optimized against Themis achieves lower perplexity compared to vanilla RMs.\nHuman Preference Evaluation. We further conducted human evaluation, comparing win:tie:lose ratios across four domains on 200 test prompts. As illustrated in Figure 5, our method outperforms baselines, achieving an average +32% win rate across the four different domains and consistently surpassing vanilla RMs in all four tasks. Notably, our approach demonstrated substantial improvements in fact-related question answering and arithmetic computation, providing robust evidence for the effectiveness of our methodology."
        },
        {
            "heading": "5 RELATED WORK",
            "text": ""
        },
        {
            "heading": "5.1 REWARD MODELING IN HUMAN ALIGNMENT",
            "text": "The challenge of aligning machine learning systems with human preferences has seen considerable exploration. Early efforts involved training aligned agents by imitating human behavior (Pomerleau, 1991; Abbeel & Ng, 2004; Ho & Ermon, 2016; Finn et al., 2016). However, these methods often struggled to outperform humans due to the requirement for substantial amounts of expensive data. A scalable solution was proposed by Leike et al. (2018), utilizing an RM to align machine learning systems with human performance. Stiennon et al. (2020) fine-tuned language models through reinforcement learning by training a RM to mimic human preferences in summarization tasks. Similar approaches were adopted by Nakano et al. (2021); Ouyang et al. (2022); Bai et al. (2022b), focusing on aligning LLMs like GPT-3 towards honesty, helpfulness, and harmlessness. In a parallel vein, Shen et al. (2023a); Le et al. (2022); Bukharin et al. (2023); Shojaee et al. (2023) contribute to the field by focusing on reward design in reinforcement learning for code-related tasks, which is pivotal for augmenting the code understanding and generation capabilities of models. However, these existing RMs faced significant challenges, such as real-time information processing, limited to specific tasks such as summarization and code generation, and struggle with assigning rewards for intricate tasks like mathematics., To address these limitations, our approach incorporates external tools, augmenting the reward model and mitigating these challenges."
        },
        {
            "heading": "5.2 TOOL LEARNING",
            "text": "The intersection of specialized tools and LLMs has recently gained significant attention in research (Mialon et al., 2023; Qin et al., 2023). Current studies in this area can be categorized into two main approaches: tool-oriented learning (Yao et al., 2023; Nakano et al., 2021; Qian et al., 2023; Shen et al., 2023b) and tool-augmented learning (Schick et al., 2023; Lu et al., 2023; Tang et al., 2023; Qiao et al., 2023). In tool-oriented learning, LLMs serve as decision-making hubs for the strategic use of tools. Conversely, tool-augmented learning treats tools as complementary resources that enhance LLMs\u2019 capabilities. Unlike previous works, our focus lies in tool-augmented reward modeling, aiming to align rewards with human preferences by incorporating tool assistance."
        },
        {
            "heading": "6 CONCLUSIONS AND FUTURE WORK",
            "text": "In this paper, we introduce Themis, a novel approach designed to enhance reward models by enabling interaction with external tools, thereby facilitating a step-by-step reasoning trajectory. Our contribution also includes the creation of a comprehensive tool-augmented reward dataset, TARA, which encompasses detailed data on human preferences and intricate tool invocation processes. Through comprehensive experimentation, including preference ranking analysis, ablation studies, generalization assessments, and RLHF/RLTAF probing, we have demonstrated the substantial benefits of Themis in augmenting interpretive capacity and scoring reliability. Our results underscore the effectiveness of our approach in integrating external tools seamlessly into the reward modeling process. Looking ahead, an exciting avenue for future research could involve exploring Themis in multi-turn dialogue generation. Understanding the intricate dynamics between external tools and natural language generation in interactive conversational contexts presents a promising and intriguing direction for further investigation."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to express our gratitude to the anonymous reviewers for their insightful and constructive feedback. Additionally, our appreciation goes to Yaqian Han, Pan Tang, and Yuchen Ding for their helpful assistance with the human evaluation process. This work was supported by the National Natural Science Foundation of China (No. 62206246), the Fundamental Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), Yongjiang Talent Introduction Programme (2021A-156-G), CCF-Baidu Open Fund, and Information Technology Center and State Key Lab of CAD&CG, Zhejiang University."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "To ensure the reproducibility of our experiments detailed in Section 4, we are committed to fostering transparency and accessibility in our research methodology. The source TARA datasets, fundamental to our study, have made publicly available on GitHub3. In addition to datasets, we provide comprehensive support for replicating our experiments. The detailed source code for both the conventional Reward Model (RM) and our proposed Themis approach is included in the supplementary materials. These materials encompass all necessary scripts and hyper-parameters essential for reproducing our results. The availability of datasets and thorough documentation of our experimental setup underline our dedication to promoting rigorous and replicable scientific research."
        },
        {
            "heading": "A LIMITATIONS",
            "text": "Limited Tool Scope. While our study incorporated experiments with seven distinct tools, the potential of tool-augmented Reward Models (RMs) could further be explored by expanding the range of tool APIs to encompass hundreds or even thousands of diverse tools. Additionally, integrating with interfaces like the ChatGPT plugin could offer a broader application spectrum.\nTime Overheads. The integration of external tools introduces an additional cost of complexity and might result in increased processing time. The speed of tool invocation is contingent on various factors such as network conditions and tool execution speed. Consequently, the real-time applicability of the tool-augmented RM is influenced by these contextual variables.\nSingle-Turn Data. Our preference data collection was limited to single-turn prompt-response pairs. Extending this framework to encompass multi-turn interactions could enrich the understanding of complex dialogues and enhance the applicability of tool-augmented RMs in real-world conversational contexts.\nChallenges in Data Generation. While we construct an automatic pipeline for dataset construction, we encounter certain challenges. We design some heuristic rules to generate data for particular tools, incurring associated costs that rise when extended to a broader range of tools. Additionally, we employ GPT-4 as agents to generate tool invocation processes and rationales, incurring a monetary expense associated with this computational process.\nLimited Model Scale. Our experiments primarily revolved around Vicuna-7B. Exploring the scalability of tool-augmented RMs to models surpassing 100 billion parameters could provide insights into the challenges and opportunities at larger scales, expanding the applicability of this approach.\nPreliminary RLHF Exploration. The exploration of tool-augmented RMs in Reinforcement Learning from Human Feedback (RLHF), namely Reinforcement Learning from Tool-Augmented Feedback (RLTAF), remains at its preliminary stages. Comprehensive experiments, covering a wider array of tasks and scenarios, are essential to fully understand the potential and limitations of this approach in reinforcement learning paradigms. Future research endeavors will focus on conducting in-depth and extensive evaluations to delve deeper into the capabilities of tool-augmented RMs in various RLHF settings."
        },
        {
            "heading": "B ADDITIONAL DATASET INFORMATION",
            "text": "In this section, we describe in detail the construction process of our tool-augmented reward dataset. We first introduce the generating process of each tool, then we report the details of the multi-agent prompts. Table 5 shows the data statistics of our TARA and the overview information such as source data, the number of the train-set and test-set of each tool is shown in Table 6."
        },
        {
            "heading": "Tool Name Data Source # Train # Test",
            "text": ""
        },
        {
            "heading": "B.1 DETAILS OF DATASET CONSTRUCTION",
            "text": "In this section, we introduce the details of the dataset construction. We construct the dataset based on the heuristic rule-based method and some open-source datasets with multi-agent interaction. We list instances of each tool from Figure 6 to Figure 12."
        },
        {
            "heading": "B.1.1 CONSTRUCTION WITH HEURISTIC RULE-BASED METHOD",
            "text": "We design some heuristic rules to generate the question-answer and tool invocation processes for certain tools, including Calendar, Weather, and Multi Tools.\nWeather. The Weather tool is realized by weatherAPI4 that the input consists of a city and a date, and the output provides information about the weather in the specified city on the given date. Initially, we compile a candidate city set by selecting the most common cities from Wikipedia. Subsequently, we create a candidate date set and a weather set specific to the Weather tool. Inspired by Wang et al. (2023), we initiate the process with a seed question prompt and an answer prompt, such as \u201cQuestion: What is the {weather} in {city} on {date}? Answer: The {weather} in {city} on {date} is {answer}.\u201d and expand upon it using ChatGPT. Finally, we iterate through the city set, date set, and weather set, incorporating them into the prompts to construct the question-answer pair. We seek positive answers from the Weather tool, while negative answers are perturbed based on the positive answer. The city set, date set, weather set, question and answer prompts set are listed in Table 7.\n4www.weatherapi.com\nCalendar. The construction process of the Calendar tool is similar to the Weather tool, with the primary difference lying in the question prompts and answer prompts. The Calendar tool serves three primary functions: determining the weekday of a given date, calculating the difference between two dates, and finding the date that follows another date by n days. For each of these functions, we have composed distinct seed prompts and subsequently expanded upon them using ChatGPT.\nMulti Tools. The Multi-Tools primarily involve chained invocations of the Calendar and Weather tools. An illustrative question might be, \u201cWhat is the weather like in city in the n days after date?\u201d This necessitates first invoking the Calendar tool to obtain \u201cthe n days after date\u201d and subsequently invoking the Weather tool to retrieve \u201cthe weather like in city\u201d for the specified date. Obviously, the data generation process for Multi-Tools follows the same pattern as the Weather and Calendar tools."
        },
        {
            "heading": "B.1.2 CONSTRUCTION FROM OPEN-SOURCE DATASETS",
            "text": "Some tools are challenging to create using heuristic rule-based methods. Therefore, we generate them based on some open-source and high-quality datasets.\nCalculator. GSM-8K (Cobbe et al., 2021) is a high-quality dataset comprising linguistically diverse grade school math word problems, making it well-suited for constructing math reasoning data involving tool invocation. We randomly selected 1,000 instances from GSM-8K to serve as both questions and positive answers. We query the negative generation agent to generate negative answers.\nCode. We integrate the HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) datasets as the positive data of the Code tool, encompassing questions, positive code answers, and test cases.\nAdditionally, we leverage StarCodeBase (Li et al., 2023) to generate negative code answers that fail to pass all the test lists.\nTranslator. The translation tool is powered by the Baidu Translator API5, which supports translation in over 200 languages. We created the dataset of the Translate tool based on the MLQA dataset (Lewis et al., 2020), encompassing QA instances in 7 different languages. Subsequently, we call the negative generation agent to generate the negative answers.\nWikiSearch. The objective of the WikiSearch tool is to bridge the reward model with the knowledge base Wikipedia. We randomly selected over 5,500 instances from the Natural Question dataset (Kwiatkowski et al., 2019), which comprises real anonymized, aggregated queries posed to the Google search engine and annotated with Wikipedia pages. For negative answers, we employ a negative generation agent.\nGoogle Search In contrast to other tools, we construct the data of Google Search tool based on the reward dataset WebGPT Comparison (Nakano et al., 2021), which includes questions, positive answers, and negative answers. We utilize both the tool agent and rationale agent to generate the tool invocation process and the rationale segment."
        },
        {
            "heading": "B.2 PROMPTS OF MULTI-AGENTS",
            "text": "We present the prompt of multi-agent interaction, negative generation agent, tool agent, and rationale agent in Figure 13. We formulate a simulated environment incorporating human participants and three virtual agents: the negative generation agent, the tool agent, and the rationale agent, each embodied by a GPT-4. The interaction scenario between agents and humans is depicted in the upper left corner of Figure 5, with detailed prompts for each agent provided on the left. We detail the responsibility of the three agents as follows:\n5www.fanyi-api.baidu.com\n\u2022 Negative Generation Agent: The responsibility of the negative generation agent is to generate a negative answer to a question that has only a positive answer. It receives a question along with its positive answer and generates a negative answer that is indistinguishable from the positive one. \u2022 Tool Agent: Undertaking a challenging role, the tool agent receives a question-answer pair and produces appropriate and correct tool calls to validate the reasonableness of the answer. The tool calls involve a Thought stage that includes a reasoning trace to determine whether tools should be called, and an Action stage that contains the necessary API calls with their required arguments. \u2022 Rationale Agent: The rationale agent is asked to generate the Rationale stage by comprehending previous contexts, synthesizing the question-answer pair, the tool invocation process, and the observations from the tool execution to systematically produce rewards."
        },
        {
            "heading": "B.3 DATA FILTER STRATEGIES",
            "text": "We design multiple data filter strategies in the dataset construction process. For the negative answers generated by the negative generation agent, we unify their format to match the positive answers, including the punctuation, spacing, sentence structure, and so on, preventing the emergence of superficial patterns. For the tool invocations process generated through interaction between the tool agent and rationale agent, we discard the instances that exhibit invalid formats, exceed three interaction steps, lack relevant function calls, or manifest parsing errors in the execution results."
        },
        {
            "heading": "C DETAILS OF EXPERIMENTS",
            "text": "C.1 IMPLEMENTATION DETAILS OF RM\nTraining Configuration Hyper-parameters. The hyperparameters governing various training configurations are enumerated in Table 8. Specifically, when \u03b1 = 0, our method simplifies to a vanilla RM, wherein scalar rewards are predicted through a fully connected layer without any preconditioned tool invocations. Additionally, setting \u03b2 = 0 or \u03c9 = 0 indicates that the tool-augmented reward model will not be trained on the Observation or Rationale component, enabling controlled exploration of different training settings and their influence on the model\u2019s behavior.\nExperimental Hyper-parameters. We report the experimental hyper-parameters at Table 9. We apply the same hyper-parameters for RM (Vicuna-7B) and Themis. For RM (Bert-Large), we use a larger learning rate, larger batch size, and train more epochs. We chose the best performance checkpoints of each model for comparison. We implement LoRA with the PEFT (Mangrulkar et al., 2022) framework. All models are trained in the same environment (8 \u00d7 40G A100 GPUs). Additionally, we incorporate the learning of positive answers by predicting the entire context of good samples, following the approach outlined in (Askell et al., 2021). This allows our models to emulate \u201cgood\u201d behavioral patterns in preference modeling.Furthermore, external APIs can exhibit instability or experience failures, leading to null observations. To enhance the model\u2019s robustness, we intentionally introduce a 1% random observation dropout during training. This approach simulates real-world scenarios where API unavailability may occur and equips our model to handle such situations more effectively.\nC.2 IMPLEMENTATION DETAILS OF RLHF\nDetails of RLHF. We follow Deepspeed-Chat6 to implement the RLHF process, which consists of the following three steps:\nStep 1: Collect samples and train a supervised policy (SFT). The SFT phase is similar to the standard language model finetuning. Here, we follow Deepspeed-Chat and divide our TARA data, 20% for training on Step 1 and 40% for Step 3. Note that we collect the question and the positive\n6https://github.com/microsoft/DeepSpeedExamples/tree/master/ applications/DeepSpeed-Chat\nanswer of this data as the supervised training data. And then fine-tune a pre-trained Vicuna-7B model with LoRA. The resulting model is denoted as Vicuna-7B-SFT.\nStep 2: Collect comparison data and train a reward model (RM). We utilize the method in Section 2 to train a reward model (RM) or a tool-augmented reward model (Themis) on our TARA to predict the human-preferred output.\nStep 3: Optimize the supervised policy against the reward model using PPO (PPO). We finetune the supervised policy obtained in Step 1 using the PPO algorithm (Schulman et al., 2017), with a reward signal provided by the RM or Themis obtained in Step 2. The resulting model from this step is denoted as Vicuna-7B-PPO (RM) or Vicuna-7B-PPO (Themis).\nWe list the hyper-parameters of SFT and PPO phases in Table 10."
        },
        {
            "heading": "C.3 EXPERIMENTS ON STANDARD REWARD DATASETS",
            "text": "To further demonstrate the effectiveness of our method, we conduct experiments on some standard reward datasets including the WebgGPT Comparision (Nakano et al., 2021) dataset and the HHRLHF (Bai et al., 2022a) dataset. The results can be seen in Table 11. We partition the WebGPT Comparison dataset into 13.7K training samples and 2.4K test samples. Additionally, we randomly extracted 50K samples from HH-RLHF as training samples, amalgamating them all with our TARA dataset. The results reveal that our Themis obtains a superior performance than other vanilla reward models except RAFT (LLaMA-7B) (Dong et al., 2023). However, RAFT (LLaMA-7B) performs SFT on the 112K positive training samples of the HH-RLHF dataset and then executes reward modeling on 100K pairwise samples. In contrast, Themis is trained within a multi-task learning setting, achieving comparable performance with RAFT (LLaMA-7B)."
        },
        {
            "heading": "D QUALITATIVE EXAMPLES",
            "text": ""
        },
        {
            "heading": "D.1 GENERATED EXAMPLES ON TARA",
            "text": "We show a qualitative example of the Google Search tool in Table 13. The question of the example is about \u201cSupply-Side economics\u201d, the answer not only provides a comprehensive explanation of the concept but also offers an insightful analysis of its benefits for consumers. Nonetheless, all reward models (RMs) assign low reward scores. In contrast, our Themis meticulously verifies the correctness of the answer by leveraging Google search tools and outputs a high score for the answer through rigorous reasoning. In addition, it is challenging for RMs to produce rewards for the responses related to arithmetic computation and code implementation. Nonetheless, our Themis can invoke relevant tools to verify the process of the responses, thereby providing robust and dependable rewards. As shown in Table 12, our Themis detects an error in the calculation process and outperforms other RMs."
        },
        {
            "heading": "D.2 EXAMPLES ON DOWNSTREAM TASK",
            "text": "TruthfulQA. TruthfulQA (Lin et al., 2022a) is a benchmark dataset to measure the truthfulness of language models, which comprises 817 questions that span 38 categories, including health, law, finance, and politics. Each instance of TruthfulQA contains a question and multiple choices, we pair the question with each choice and feed it into our Themis to obtain the preferred score. Then we compare the score of each choice and choose the preference corresponding to the highest score as\nthe predicted answer. The case examples can be seen in Table 14 and Table 15. With the assistance of the search tool and knowledgeable tool, our Themis can verifies the correctness of each choice reasonably.\nRetarded-bar. Retarded-bar is a challenging dataset that contains puns, unusual punctuation, and irrational logic. However, Retarded-bar is a Chinese dataset and only provides one golden answer. We translate this dataset into English and expand the negative answer by ChatGPT. We provide a case example in Table 16. The questions of the Retarded-bar dataset consistently contain puns,\nunusual punctuation, and irrational logic, making them challenging for LM to answer. However, our Themis effectively identifies errors in the calculation process and outputs a low reward score, while the reward model erroneously assigns a higher score."
        },
        {
            "heading": "D.3 GENERATED EXAMPLES IN RLHF",
            "text": "We present some qualitative examples in Table 17. We compare the responses of RM (Vicuna-7B) and Themis and find that the reward signal provided by Themis leads LM to generate responses with reduced repetition and enhanced overall coherence."
        },
        {
            "heading": "D.4 EXAMPLES IN BIASED OBSERVATION",
            "text": "The observations of the tool invocations are not always convincing since the tools may produce biased aspects or wrong aspects here, such as invalid tool invocations, incorrect tool-invoked results, biased content, and unexpected errors such as network failure. To avoid reward model interference by these biases, we have implemented several measures:\n\u2022 Strict Quality Control. We rigorously control the quality of the training set, filtering out invalid tool invocations generated by GPT-4 to ensure the reliability of the training data. \u2022 Noise Injection. Recognizing the possibility of tools not always providing accurate outputs, we intentionally introduce \u201cnoise\u201d instances in the training data. This strategy helps train the reward model to produce correct rewards even in scenarios where the tools may not work optimally. An example is shown in Table 18. Our Themis model encounters an error during the invocation of the Wiki Search tool and no results are returned. Importantly, our model demonstrates a capacity for reasoning based on its internal understanding, unaffected by the absence of tool output, which highlights the ability of our model to engage in self-reflection and form independent judgments when faced with unexpected tool behavior. \u2022 Rationale Generation. A key aspect of our Themis approach is the Rationale generation stage. Here, the tool-augmented reward model aggregates and synthesizes previously acquired information and undergoes a reasonable reasoning process, which involves RM\u2019s self-reflection instead of listening to the results of the tool. The emphasis on rationale generation encourages Themis to form reasoned decisions, relying on a process of synthesis and self-reflection rather than solely relying on the output of the tools. This approach enhances the model\u2019s ability to reason independently and avoid undue influence from tool"
        }
    ],
    "title": "TOOL-AUGMENTED REWARD MODELING",
    "year": 2024
}