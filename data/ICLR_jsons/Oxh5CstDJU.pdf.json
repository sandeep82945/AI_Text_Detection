{
    "abstractText": "TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoderfree) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://tdmpc2.com",
    "authors": [],
    "id": "SP:6e58b7d2ed17d206df8aae451c1e1b0079e7e876",
    "references": [
        {
            "authors": [
                "Bowen Baker",
                "Ilge Akkaya",
                "Peter Zhokov",
                "Joost Huizinga",
                "Jie Tang",
                "Adrien Ecoffet",
                "Brandon Houghton",
                "Raul Sampedro",
                "Jeff Clune"
            ],
            "title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Marc G Bellemare",
                "Will Dabney",
                "R\u00e9mi Munos"
            ],
            "title": "A distributional perspective on reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Richard Bellman"
            ],
            "title": "A markovian decision process",
            "venue": "Indiana Univ. Math. J.,",
            "year": 1957
        },
        {
            "authors": [
                "Christopher Berner",
                "Greg Brockman",
                "Brooke Chan",
                "Vicki Cheung",
                "Przemyslaw Debiak",
                "Christy Dennison",
                "David Farhi",
                "Quirin Fischer",
                "Shariq Hashme",
                "Chris Hesse"
            ],
            "title": "Dota 2 with large scale deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1912.06680,",
            "year": 2019
        },
        {
            "authors": [
                "Anthony Brohan",
                "Noah Brown",
                "Justice Carbajal",
                "Yevgen Chebotar",
                "Xi Chen",
                "Krzysztof Choromanski",
                "Tianli Ding",
                "Danny Driess",
                "Avinava Dubey",
                "Chelsea Finn"
            ],
            "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "venue": "arXiv preprint arXiv:2307.15818,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Vittorio Caggiano",
                "Huawei Wang",
                "Guillaume Durandau",
                "Massimo Sartori",
                "Vikash Kumar"
            ],
            "title": "Myosuite \u2013 a contact-rich simulation suite for musculoskeletal motor control",
            "venue": "https: //github.com/facebookresearch/myosuite,",
            "year": 2022
        },
        {
            "authors": [
                "Berk Calli",
                "Arjun Singh",
                "Aaron Walsman",
                "Siddhartha Srinivasa",
                "Pieter Abbeel",
                "Aaron M. Dollar"
            ],
            "title": "The ycb object and model set: Towards common benchmarks for manipulation research",
            "venue": "In 2015 International Conference on Advanced Robotics,",
            "year": 2015
        },
        {
            "authors": [
                "Xinyue Chen",
                "Che Wang",
                "Zijian Zhou",
                "Keith Ross"
            ],
            "title": "Randomized ensembled double q-learning: Learning fast without a model",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Rohan Chitnis",
                "Yingchen Xu",
                "Bobak Hashemi",
                "Lucas Lehnert",
                "Urun Dogan",
                "Zheqing Zhu",
                "Olivier Delalleau"
            ],
            "title": "Iql-td-mpc: Implicit q-learning for hierarchical model predictive control",
            "venue": "arXiv preprint arXiv:2306.00867,",
            "year": 2023
        },
        {
            "authors": [
                "Jack Clark",
                "Dario Amodei"
            ],
            "title": "Faulty reward functions in the wild",
            "venue": "OpenAI Blog,",
            "year": 2016
        },
        {
            "authors": [
                "Djork-Arn\u00e9 Clevert",
                "Thomas Unterthiner",
                "Sepp Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (elus)",
            "venue": "arXiv preprint arXiv:1511.07289,",
            "year": 2015
        },
        {
            "authors": [
                "R\u00e9mi Coulom"
            ],
            "title": "Efficient selectivity and backup operators in monte-carlo tree search",
            "venue": "Computers and Games,",
            "year": 2007
        },
        {
            "authors": [
                "Pierluca D\u2019Oro",
                "Max Schwarzer",
                "Evgenii Nikishin",
                "Pierre-Luc Bacon",
                "Marc G Bellemare",
                "Aaron Courville"
            ],
            "title": "Sample-efficient reinforcement learning by breaking the replay ratio barrier",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Tianjun Zhang",
                "Sergey Levine",
                "Russ R Salakhutdinov"
            ],
            "title": "Contrastive learning as goal-conditioned reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yunhai Feng",
                "Nicklas Hansen",
                "Ziyan Xiong",
                "Chandramouli Rajagopalan",
                "Xiaolong Wang"
            ],
            "title": "Finetuning offline world models in the real world",
            "venue": "Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u2019e",
                "Corentin Tallec",
                "Pierre H. Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo \u00c1vila Pires",
                "Zhaohan Daniel Guo",
                "Mohammad Gheshlaghi Azar",
                "Bilal Piot",
                "Koray Kavukcuoglu",
                "R\u00e9mi Munos",
                "Michal Valko"
            ],
            "title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jiayuan Gu",
                "Fanbo Xiang",
                "Xuanlin Li",
                "Zhan Ling",
                "Xiqiaing Liu",
                "Tongzhou Mu",
                "Yihe Tang",
                "Stone Tao",
                "Xinyue Wei",
                "Yunchao Yao",
                "Xiaodi Yuan",
                "Pengwei Xie",
                "Zhiao Huang",
                "Rui Chen",
                "Hao Su"
            ],
            "title": "Maniskill2: A unified benchmark for generalizable manipulation skills",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Danijar Hafner",
                "Jurgis Pasukonis",
                "Jimmy Ba",
                "Timothy Lillicrap"
            ],
            "title": "Mastering diverse domains through world models",
            "venue": "arXiv preprint arXiv:2301.04104,",
            "year": 2023
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Hao Su",
                "Xiaolong Wang"
            ],
            "title": "Stabilizing deep q-learning with convnets and vision transformers under data augmentation",
            "venue": "In Annual Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Xiaolong Wang",
                "Hao Su"
            ],
            "title": "Temporal difference learning for model predictive control",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Yixin Lin",
                "Hao Su",
                "Xiaolong Wang",
                "Vikash Kumar",
                "Aravind Rajeswaran"
            ],
            "title": "Modem: Accelerating visual model-based reinforcement learning with demonstrations",
            "year": 2023
        },
        {
            "authors": [
                "H.V. Hasselt",
                "A. Guez",
                "D. Silver"
            ],
            "title": "Deep reinforcement learning with double q-learning",
            "venue": "In Aaai,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Henderson",
                "Riashat Islam",
                "Philip Bachman",
                "Joelle Pineau",
                "Doina Precup",
                "David Meger"
            ],
            "title": "Deep reinforcement learning that matters",
            "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Hubert",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Mohammadamin Barekatain",
                "Simon Schmitt",
                "David Silver"
            ],
            "title": "Learning and planning in complex action spaces, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Zhiwei Jia",
                "Xuanlin Li",
                "Zhan Ling",
                "Shuang Liu",
                "Yiran Wu",
                "Hao Su"
            ],
            "title": "Improving Policy Optimization with Generalist-Specialist Learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Denis Yarats",
                "Rob Fergus"
            ],
            "title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
            "venue": "International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Aviral Kumar",
                "Joey Hong",
                "Anikait Singh",
                "Sergey Levine"
            ],
            "title": "Should i run offline reinforcement learning or behavioral cloning",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Aviral Kumar",
                "Rishabh Agarwal",
                "Xinyang Geng",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Offline qlearning on diverse multi-task data both scales and generalizes",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Nathan Lambert",
                "Brandon Amos",
                "Omry Yadan",
                "Roberto Calandra"
            ],
            "title": "Objective mismatch in model-based reinforcement learning",
            "venue": "Conference on Learning for Decision and Control,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lancaster",
                "Nicklas Hansen",
                "Aravind Rajeswaran",
                "Vikash Kumar"
            ],
            "title": "Modem-v2: Visuomotor world models for real-world robot manipulation",
            "venue": "arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Lavoie",
                "Christos Tsirigotis",
                "Max Schwarzer",
                "Ankit Vani",
                "Michael Noukhovitch",
                "Kenji Kawaguchi",
                "Aaron Courville"
            ],
            "title": "Simplicial embeddings in self-supervised learning and downstream classification",
            "venue": "arXiv preprint arXiv:2204.00616,",
            "year": 2022
        },
        {
            "authors": [
                "Kuang-Huei Lee",
                "Ofir Nachum",
                "Mengjiao Sherry Yang",
                "Lisa Lee",
                "Daniel Freeman",
                "Sergio Guadarrama",
                "Ian Fischer",
                "Winnie Xu",
                "Eric Jang",
                "Henryk Michalewski"
            ],
            "title": "Multi-game decision transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "T. Lillicrap",
                "J. Hunt",
                "A. Pritzel",
                "N. Heess",
                "T. Erez",
                "Y. Tassa",
                "D. Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "year": 2016
        },
        {
            "authors": [
                "Yecheng Jason Ma",
                "Shagun Sodhani",
                "Dinesh Jayaraman",
                "Osbert Bastani",
                "Vikash Kumar",
                "Amy Zhang"
            ],
            "title": "Vip: Towards universal visual reward and representation via value-implicit pre-training",
            "venue": "arXiv preprint arXiv:2210.00030,",
            "year": 2022
        },
        {
            "authors": [
                "Diganta Misra"
            ],
            "title": "Mish: A self regularized non-monotonic neural activation function",
            "venue": "arXiv preprint arXiv:1908.08681,",
            "year": 2019
        },
        {
            "authors": [
                "Mitsuhiko Nakamoto",
                "Yuexiang Zhai",
                "Anikait Singh",
                "Max Sobol Mark",
                "Yi Ma",
                "Chelsea Finn",
                "Aviral Kumar",
                "Sergey Levine"
            ],
            "title": "Cal-ql: Calibrated offline rl pre-training for efficient online finetuning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Rudy R. Negenborn",
                "Bart De Schutter",
                "Marco A. Wiering",
                "Hans Hellendoorn"
            ],
            "title": "Learning-based model predictive control for markov decision processes",
            "venue": "IFAC Proceedings Volumes,",
            "year": 2005
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Oriol Vinyals",
                "Koray Kavukcuoglu"
            ],
            "title": "Neural discrete representation learning",
            "venue": "arXiv preprint arXiv:1711.00937,",
            "year": 2017
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Scott Reed",
                "Konrad Zolna",
                "Emilio Parisotto",
                "Sergio Gomez Colmenarejo",
                "Alexander Novikov",
                "Gabriel Barth-Maron",
                "Mai Gimenez",
                "Yury Sulsky",
                "Jackie Kay",
                "Jost Tobias Springenberg"
            ],
            "title": "A generalist agent",
            "venue": "arXiv preprint arXiv:2205.06175,",
            "year": 2022
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "Ingmar Schubert",
                "Jingwei Zhang",
                "Jake Bruce",
                "Sarah Bechtle",
                "Emilio Parisotto",
                "Martin Riedmiller",
                "Jost Tobias Springenberg",
                "Arunkumar Byravan",
                "Leonard Hasenclever",
                "Nicolas Heess"
            ],
            "title": "A generalist dynamics model for control",
            "venue": "arXiv preprint arXiv:2305.10912,",
            "year": 2023
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "R. Sutton"
            ],
            "title": "Learning to predict by the methods of temporal differences",
            "venue": "Machine Learning,",
            "year": 1998
        },
        {
            "authors": [
                "Yuval Tassa",
                "Yotam Doron",
                "Alistair Muldal",
                "Tom Erez",
                "Yazhe Li",
                "Diego de Las Casas",
                "David Budden",
                "Abbas Abdolmaleki"
            ],
            "title": "Deepmind control suite",
            "venue": "Technical report,",
            "year": 2018
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Grady Williams",
                "Andrew Aldrich",
                "Evangelos A. Theodorou"
            ],
            "title": "Model predictive path integral control using covariance variable importance",
            "venue": "sampling. ArXiv,",
            "year": 2015
        },
        {
            "authors": [
                "Yifan Xu",
                "Nicklas Hansen",
                "Zirui Wang",
                "Yung-Chieh Chan",
                "Hao Su",
                "Zhuowen Tu"
            ],
            "title": "On the feasibility of cross-task transfer with model-based reinforcement",
            "year": 2023
        },
        {
            "authors": [
                "Sizhe Yang",
                "Yanjie Ze",
                "Huazhe Xu"
            ],
            "title": "Movie: Visual model-based policy adaptation for view generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Denis Yarats",
                "Rob Fergus",
                "Alessandro Lazaric",
                "Lerrel Pinto"
            ],
            "title": "Mastering visual continuous control: Improved data-augmented reinforcement learning",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Weirui Ye",
                "Shaohuai Liu",
                "Thanard Kurutach",
                "Pieter Abbeel",
                "Yang Gao"
            ],
            "title": "Mastering atari games with limited data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tianhe Yu",
                "Deirdre Quillen",
                "Zhanpeng He",
                "Ryan Julian",
                "Karol Hausman",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yifu Yuan",
                "Jianye Hao",
                "Fei Ni",
                "Yao Mu",
                "Yan Zheng",
                "Yujing Hu",
                "Jinyi Liu",
                "Yingfeng Chen",
                "Changjie Fan"
            ],
            "title": "Euclid: Towards efficient unsupervised reinforcement learning with multi-choice dynamics model",
            "venue": "arXiv preprint arXiv:2210.00498,",
            "year": 2022
        },
        {
            "authors": [
                "Baohe Zhang",
                "Raghu Rajan",
                "Luis Pineda",
                "Nathan Lambert",
                "Andr\u00e9 Biedenkapp",
                "Kurtland Chua",
                "Frank Hutter",
                "Roberto Calandra"
            ],
            "title": "On the importance of hyperparameter optimization for model-based reinforcement learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Chuning Zhu",
                "Max Simchowitz",
                "Siri Gadipudi",
                "Abhishek Gupta. Repo"
            ],
            "title": "Resilient model-based reinforcement learning by regularizing posterior predictability",
            "venue": "arXiv preprint arXiv:2309.00082,",
            "year": 2023
        },
        {
            "authors": [
                "Brian D Ziebart",
                "Andrew Maas",
                "J Andrew Bagnell",
                "Anind K Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In Proceedings of the 23rd National Conference on Artificial Intelligence,",
            "year": 2008
        },
        {
            "authors": [
                "Feng"
            ],
            "title": "2023), which penalizes trajectories with large uncertainty (as estimated by the variance in Q-value predictions) during planning. While this approach eliminates the need for training-time regularization, it still requires users to specify a coefficient that weighs estimated value relative to uncertainty for a trajectory, which is infeasible in a multi-task scenario where estimated values may differ drastically",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Training large models on internet-scale datasets has led to generalist models that perform a wide variety of language and vision tasks (Brown et al., 2020; He et al., 2022; Kirillov et al., 2023). The success of these models can largely be attributed to the availability of enormous datasets, and carefully designed architectures that reliably scale with model and data size. While researchers have recently extended this paradigm to robotics (Reed et al., 2022; Brohan et al., 2023), a generalist embodied agent that learns to perform diverse control tasks via low-level actions, across multiple embodiments, from large uncurated (i.e., mixed-quality) datasets remains an elusive goal. We argue that current approaches to generalist embodied agents suffer from (a) the assumption of near-expert trajectories for behavior cloning which severely limits the amount of available data (Reed et al., 2022; Lee et al., 2022; Kumar et al., 2022; Schubert et al., 2023; Driess et al., 2023; Brohan et al., 2023), and (b) a lack of scalable continuous control algorithms that are able to consume large uncurated datasets.\nReinforcement Learning (RL) is an ideal framework for extracting expert behavior from uncurated datasets. However, most existing RL algorithms (Lillicrap et al., 2016; Haarnoja et al., 2018) are designed for single-task learning and rely on per-task hyperparameters, with no principled method for selecting those hyperparameters (Zhang et al., 2021). An algorithm that can consume large multitask datasets will invariably need to be robust to variation between different tasks (e.g., action space dimensionality, difficulty of exploration, and reward distribution). In this work, we present TDMPC2: a significant step towards achieving this goal. TD-MPC2 is a model-based RL algorithm designed for learning generalist world models on large uncurated datasets composed of multiple task domains, embodiments, and action spaces, with data sourced from behavior policies that cover a wide range of skill levels, and without the need for hyperparameter-tuning.\nOur algorithm, which builds upon TD-MPC (Hansen et al., 2022), performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. While the TD-MPC family of algorithms has demonstrated strong empirical performance in prior work (Hansen et al., 2022; 2023; Yuan et al., 2022; Yang et al., 2023; Feng et al., 2023; Chitnis et al., 2023; Zhu et al., 2023; Lancaster et al., 2023), most successes have been limited to single-task learning with little emphasis on scaling. As shown in Figure 1, na\u0131\u0308vely increasing model and data size of TD-MPC often leads to a net decrease in agent performance, as is commonly observed in RL literature (Kumar et al., 2023). In contrast, scaling TD-MPC2 leads to consistently improved capabilities. Our algorithmic contributions, which have been key to achieving this milestone, are two-fold: (1) improved algorithmic robustness by revisiting core design choices, and (2) careful design of an architecture that can accommodate datasets with multiple embodiments and action spaces without relying on domain knowledge. The resulting algorithm, TD-MPC2, is scalable, robust, and can be applied to a variety of single-task and multi-task continuous control problems using a single set of hyperparameters.\nWe evaluate TD-MPC2 across a total of 104 diverse continuous control tasks spanning 4 task domains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al., 2022). We summarize our results in Figure 1, and visualize task domains in Figure 2. Tasks include high-dimensional state and action spaces (up to A \u2208 R39), sparse rewards, multi-object manipulation, physiologically accurate musculoskeletal motor control, complex locomotion (e.g. Dog and Humanoid embodiments), and cover a wide range of task difficulties. Our results demonstrate that TD-MPC2 consistently outperforms existing model-based and modelfree methods, using the same hyperparameters across all tasks (Figure 1, right). Here, \u201cLocomotion\u201d and \u201cPick YCB\u201d are particularly challenging subsets of DMControl and ManiSkill2, respectively. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter world model to perform 80 tasks across multiple task domains, embodiments, and action spaces (Figure 1, left). In support of open-source science, we publicly release 300+ model checkpoints, datasets, and code for training and evaluating TD-MPC2 agents, most of which has already been made available at https://tdmpc2.com. We conclude the paper with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Reinforcement Learning (RL) aims to learn a policy from interaction with an environment, formulated as a Markov Decision Process (MDP) (Bellman, 1957). We focus on infinite-horizon MDPs with continuous action spaces, which can be formalized as a tuple (S,A, T , R, \u03b3) where s \u2208 S are states, a \u2208 A are actions, T : S \u00d7A 7\u2192 S is the transition function, R : S \u00d7 A 7\u2192 R is a reward function associated with a particular task, and \u03b3 is a discount factor. The goal is to\nderive a control policy \u03c0 : S 7\u2192 A such that the expected discounted sum of rewards (return) E\u03c0 [ \u2211\u221e t=0 \u03b3\ntrt] , rt = R(st, \u03c0(st)) is maximized. In this work, we obtain \u03c0 by learning a world model (model of the environment) and then select actions by planning with the learned model.\nModel Predictive Control (MPC) is a general framework for model-based control that optimizes action sequences at:t+H of finite length such that return is maximized (or cost is minimized) over the time horizon H , which corresponds to solving the following optimization problem:\n\u03c0(st) = arg max at:t+H\nE [ H\u2211 i=0 \u03b3t+iR(st+i,at+i) ] . (1)\nThe return of a candidate trajectory is estimated by simulating it with the learned model (Negenborn et al., 2005). Thus, a policy obtained by Equation 1 will invariably be a (temporally) locally optimal policy and is not guaranteed (nor likely) to be a solution to the general reinforcement learning problem outlined above. As we discuss in the following, TD-MPC2 addresses this shortcoming of local trajectory optimization by bootstrapping return estimates beyond horizon H with a learned terminal value function.\n3 TD-MPC2\nOur work builds upon TD-MPC (Hansen et al., 2022), a model-based RL algorithm that performs local trajectory optimization (planning) in the latent space of a learned implicit world model. TDMPC2 is a practical algorithm for training massively multitask world models. Specifically, we propose a series of improvements to the TD-MPC algorithm, which have been key to achieving strong algorithmic robustness (can use the same hyperparameters across all tasks) and scaling its world model to 300\u00d7 more parameters than previously. We introduce the TD-MPC2 algorithm in the following, and provide a full list of algorithmic improvements in Appendix A.\n3.1 LEARNING AN IMPLICIT WORLD MODEL\nLearning a generative model of the environment using a reconstruction (decoder) objective is tempting due to its rich learning signal. However, accurately predicting raw future observations (e.g., images or proprioceptive features) over long time horizons is a difficult problem, and does not necessarily lead to effective control (Lambert et al., 2020). Rather than explicitly modeling dynamics using reconstruction, TD-MPC2 aims to learn a maximally useful model: a model that accurately predicts outcomes (returns) conditioned on a sequence of actions. Specifically, TD-MPC2 learns an implicit, control-centric world model from environment interaction using a combination of joint-embedding prediction (Grill et al., 2020), reward prediction, and TD-learning (Sutton, 1998), without decoding observations. We argue that this alternative formulation of model-based RL is key to modeling large datasets with modest model sizes. The world model can subsequently be used for decision-making by performing local trajectory optimization (planning) following the MPC framework.\nComponents. The TD-MPC2 architecture is shown in Figure 3 and consists of five components: Encoder z = h(s, e) \u25b7 Maps observations to their latent representations Latent dynamics z\u2032 = d(z,a, e) \u25b7 Models (latent) forward dynamics Reward r\u0302 = R(z,a, e) \u25b7 Predicts reward r of a transition Terminal value q\u0302 = Q(z,a, e) \u25b7 Predicts discounted sum of rewards (return) Policy prior a\u0302 = p(z, e) \u25b7 Predicts action a\u2217 that maximizes Q (2)\nwhere s and a are states and actions, z is the latent representation, and e is a learnable task embedding for use in multitask world models. For visual clarity, we will omit e in the following unless it is\nparticularly relevant. The policy prior p serves to guide the sample-based trajectory optimizer (planner), and to reduce the computational cost of TD-learning. During online interaction, TD-MPC2 maintains a replay buffer B with trajectories, and iteratively (i) updates the world model using data sampled from B, and (ii) collects new environment data by planning with the learned model. Model objective. The h, d,R,Q components are jointly optimized to minimize the objective\nL (\u03b8) .= E (s,a,r,s\u2032)0:H\u223cB  H\u2211 t=0 \u03bbt \u2225 z\u2032t \u2212 sg(h(s\u2032t))\u222522\ufe38 \ufe37\ufe37 \ufe38 Joint-embedding prediction + CE(r\u0302t, rt)\ufe38 \ufe37\ufe37 \ufe38 Reward prediction + CE(q\u0302t, qt)\ufe38 \ufe37\ufe37 \ufe38 Value prediction   , (3)\nwhere sg is the stop-grad operator, (z\u2032t, r\u0302t, q\u0302t) are defined in Equation 2, qt . = rt+\u03b3Q\u0304(z \u2032 t, p(z \u2032 t)) is the TD-target at step t, \u03bb \u2208 (0, 1] is a constant coefficient that weighs temporally farther time steps less, and CE is the cross-entropy. Q\u0304 used to compute the TD-target is an exponential moving average (EMA) of Q (Lillicrap et al., 2016). As the magnitude of rewards may differ drastically between tasks, TD-MPC2 formulates reward and value prediction as a discrete regression (multiclass classification) problem in a log-transformed space, which is optimized by minimizing crossentropy with rt, qt as soft targets (Bellemare et al., 2017; Kumar et al., 2023; Hafner et al., 2023).\nPolicy objective. The policy prior p is a stochastic maximum entropy (Ziebart et al., 2008; Haarnoja et al., 2018) policy that learns to maximize the objective\nLp(\u03b8) . = E\n(s,a)0:H\u223cB [ H\u2211 t=0 \u03bbt [\u03b1Q(zt, p(zt))\u2212 \u03b2H(p(\u00b7|zt))] ] , zt+1 = d(zt,at), z0 = h(s0) , (4)\nwhere H is the entropy of p which can be computed in closed form. Gradients of Lp(\u03b8) are taken wrt. p only. As magnitude of the value estimate Q(zt, p(zt)) and entropy H can vary greatly between datasets and different stages of training, it is necessary to balance the two losses to prevent premature entropy collapse (Yarats et al., 2021). A common choice for automatically tuning \u03b1, \u03b2 is to keep one of them constant, and adjusting the other based on an entropy target (Haarnoja et al., 2018) or moving statistics (Hafner et al., 2023). In practice, we opt for tuning \u03b1 via moving statistics, but empirically did not observe any significant difference in results between these two options.\nArchitecture. All components of TD-MPC2 are implemented as MLPs with intermediate linear layers followed by LayerNorm (Ba et al., 2016) and Mish (Misra, 2019) activations. To mitigate exploding gradients, we normalize the latent representation by projecting z into L fixed-dimensional simplices using a softmax operation (Lavoie et al., 2022). A key benefit of embedding z as simplices (as opposed to e.g. a discrete representation or squashing) is that it naturally biases the representation towards sparsity without enforcing hard constraints (see Appendix H for motivation and implementation). We dub this normalization scheme SimNorm. Let V be the dimensionality of each simplex g constructed from L partitions (groups) of z. SimNorm then applies the following transformation:\nz\u25e6 . = [gi, . . . ,gL] , gi = ezi:i+V /\u03c4\u2211V j=1 e zi:i+V /\u03c4 , (5)\nwhere z\u25e6 is the simplicial embedding of z, [\u00b7] denotes concatenation, and \u03c4 > 0 is a temperature parameter that modulates the \u201csparsity\u201d of the representation. As we will demonstrate in our experiments, SimNorm is essential to the training stability of TD-MPC2. Finally, to reduce bias in TD-targets generated by Q\u0304, we learn an ensemble of Q-functions using the objective from Equation 3 and maintain Q\u0304 as an EMA of each Q-function. We use 5Q-functions in practice. Targets are then computed as the minimum of two randomly sub-sampled Q\u0304-functions (Chen et al., 2021)."
        },
        {
            "heading": "3.2 MODEL PREDICTIVE CONTROL WITH A POLICY PRIOR",
            "text": "TD-MPC2 derives its closed-loop control policy by planning with the learned world model. Specifically, our approach leverages the MPC framework for local trajectory optimization using Model Predictive Path Integral (MPPI) (Williams et al., 2015) as a derivative-free optimizer with sampled action sequences (at,at+1, . . . ,at+H) of length H evaluated by rolling out latent trajectories with the model. At each decision step, we estimate parameters \u00b5\u2217, \u03c3\u2217 of a time-dependent multivariate Gaussian with diagonal covariance such that expected return is maximized, i.e.,\n\u00b5\u2217, \u03c3\u2217 = argmax (\u00b5,\u03c3) E (at,at+1,...,at+H)\u223cN (\u00b5,\u03c32)\n[ \u03b3HQ(zt+H ,at+H) +\nH\u22121\u2211 h=t \u03b3hR(zh,ah)\n] , (6)\nwhere \u00b5, \u03c3 \u2208 RH\u00d7m, A \u2208 Rm. Equation 6 is solved by iteratively sampling action sequences from N (\u00b5, \u03c32), evaluating their expected return, and updating \u00b5, \u03c3 based on a weighted average. Notably, Equation 6 estimates the full RL objective introduced in Section 2 by bootstrapping with the learned terminal value function beyond horizon H . TD-MPC2 repeats this iterative planning process for a fixed number of iterations and executes the first action at \u223c N (\u00b5\u2217t , \u03c3\u2217t ) in the environment. To accelerate convergence of planning, a fraction of action sequences originate from the policy prior p, and we warm-start planning by initializing (\u00b5, \u03c3) as the solution to the previous decision step shifted by 1. Refer to Hansen et al. (2022) for more details about the planning procedure.\n3.3 TRAINING GENERALIST TD-MPC2 AGENTS\nThe success of TD-MPC2 in diverse single-task problems can be attributed to the algorithm outlined above. However, learning a large generalist TD-MPC2 agent that performs a variety of tasks across multiple task domains, embodiments, and action spaces poses several unique challenges: (i) how to learn and represent task semantics? (ii) how to accommodate multiple observation and action spaces without specific domain knowledge? (iii) how to leverage the learned model for few-shot learning of new tasks? We describe our approach to multitask model learning in the following.\nLearnable task embeddings. To succeed in a multitask setting, an agent needs to learn a common representation that takes advantage of task similarities, while still retaining the ability to differentiate between tasks at test-time. When task or domain knowledge is available, e.g. in the form of natural language instructions, the task embedding e from Equation 2 may encode such information. However, in the general case where domain knowledge cannot be assumed, we may instead choose to learn the task embeddings (and, implicitly, task relations) from data. TD-MPC2 conditions all of its five components with a learnable, fixed-dimensional task embedding e, which is jointly trained together with other components of the model. To improve training stability, we constrain the \u21132-norm of e to be \u2264 1; this also leads to more semantically coherent task embeddings in our experiments. When finetuning a multitask TD-MPC2 agent to a new task, we can choose to either initialize e as the embedding of a semantically similar task, or simply as a random vector.\nAction masking. TD-MPC2 learns to perform tasks with a variety of observation and action spaces, without any domain knowledge. To do so, we zero-pad all model inputs and outputs to their largest respective dimensions, and mask out invalid action dimensions in predictions made by the policy prior p during both training and inference. This ensures that prediction errors in invalid dimensions do not influence TD-target estimation, and prevents p from falsely inflating its entropy for tasks with small action spaces. We similarly only sample actions along valid dimensions during planning."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate TD-MPC2 across a total of 104 diverse continuous control tasks spanning 4 task domains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al., 2022). Tasks include high-dimensional state and action spaces (up to A \u2208 R39), sparse rewards, multi-object manipulation, physiologically accurate musculoskeletal motor control, complex locomotion (e.g. Dog and Humanoid embodiments), and cover a wide range of task difficulties. In support of open-source science, we publicly release 300+ model checkpoints, datasets, and code for training and evaluating TD-MPC2 agents, most of which has already been made available at https://tdmpc2.com.\nWe seek to answer three core research questions through experimentation: \u2022 Comparison to existing methods. How does TD-MPC2 compare to state-of-the-art model-free\n(SAC) and model-based (DreamerV3, TD-MPC) methods for data-efficient continuous control? \u2022 Scaling. Do the algorithmic innovations of TD-MPC2 lead to improved agent capabilities as\nmodel and data size increases? Can a single agent learn to perform diverse skills across multiple task domains, embodiments, and action spaces?\n\u2022 Analysis. How do the specific design choices introduced in TD-MPC2 influence downstream task performance? How much does planning contribute to its success? Are the learned task embeddings semantically meaningful? Can large multi-task agents be adapted to unseen tasks?\nBaselines. Our baselines represent the state-of-the-art in data-efficient RL, and include (1) Soft Actor-Critic (SAC) (Haarnoja et al., 2018), a model-free actor-critic algorithm based on maximum entropy RL, (2) DreamerV3 (Hafner et al., 2023), a model-based method that optimizes a modelfree policy with rollouts from a learned generative model of the environment, and (3) the original version of TD-MPC (Hansen et al., 2022), a model-based RL algorithm that performs local trajectory optimization (planning) in the latent space of a learned implicit (non-generative) world model. SAC and TD-MPC use task-specific hyperparameters, whereas TD-MPC2 uses the same hyperparameters across all tasks. Additionally, it is worth noting that both SAC and TD-MPC use a larger batch size of 512, while 256 is sufficient for stable learning with TD-MPC2. Similarly, DreamerV3 uses a high update-to-data (UTD) ratio of 512, whereas TD-MPC2 uses a UTD of 1 by default. We use a 5M parameter TD-MPC2 agent in all experiments (unless stated otherwise). For reference, the DreamerV3 baseline has approx. 20M learnable parameters. See Appendix H for more details."
        },
        {
            "heading": "4.1 RESULTS",
            "text": "Comparison to existing methods. We first compare the data-efficiency of TD-MPC2 to a set of strong baselines on 104 diverse tasks in an online RL setting. Aggregate results are shown in Figure 4. We find that TD-MPC2 outperforms prior methods across all task domains. The MyoSuite results are particularly noteworthy, as we did not run any TD-MPC2 experiments on this benchmark prior to the reported results. Individual task performances on some of the most difficult tasks (high-dimensional locomotion and multi-object manipulation) are shown in Figure 5 and Figure 6. TD-MPC2 outperforms baselines by a large margin on these tasks, despite using the same hyperparameters across all tasks. Notably, TD-MPC sometimes diverges due to exploding gradients, whereas TD-MPC2 remains stable. We provide per-task visualization of gradients in Appendix G. Similarly, we observe that DreamerV3 experiences occasional numerical instabilities (Dog) and\ngenerally struggles with tasks that require fine-grained object manipulation (lift, pick, stack). See Appendix D for the full single-task RL results.\nMassively multitask world models. To demonstrate that our proposed improvements facilitate scaling of world models, we evaluate the performance of 5 multitask models ranging from 1M to 317M parameters on a collection of 80 diverse tasks that span multiple task domains and vary greatly in objective, embodiment, and action space. Models are trained on a dataset of 545M transitions obtained from the replay buffers of 240 single-task TD-MPC2 agents, and thus contain a wide variety of behaviors ranging from random to expert policies. The task set consists of all 50 Meta-World tasks, as well as 30 DMControl tasks. The DMControl task set includes 19 original DMControl tasks, as well as 11 new tasks. For completeness, we include a separate set of scaling results on the 30-task DMControl subset (345M transitions) as well. Due to our careful design of the TD-MPC2 algorithm, scaling up is straightforward:\nto improve rate of convergence we use a 4\u00d7 larger batch size (1024) compared to the single-task experiments, but make no other changes to hyperparameters.\nScaling TD-MPC2 to 317M parameters. Our scaling results are shown in Figure 7. To summarize agent performance with a single metric, we produce a normalized score that is an average of all individual task success rates (Meta-World) and episode returns normalized to the [0, 100] range (DMControl). We observe that agent capabilities consistently increase with model size on both task sets. Notably, performance does not appear to have saturated for our largest models (317M parameters) on either dataset, and we can thus expect results to continue improving beyond our considered model sizes. We refrain from formulating a scaling law, but note that normalized score appears to scale linearly with the log of model parameters (gray line in Figure 7). We also report approximate training costs in Table 1. The 317M parameter model can be trained with limited computational resources. To better understand why multitask model learning is successful, we explore the task embeddings learned by TD-MPC2 (Figure 7, right). Intriguingly, tasks that are semantically similar (e.g., Door Open and Door Close) are close in the learned task embedding space. However,\nembedding similarity appears to align more closely with task dynamics (embodiment, objects) than objective (walk, run). This makes intuitive sense, as dynamics are tightly coupled with control.\nFew-shot learning. While our work mainly focuses on the scaling and robustness of world models, we also explore the efficacy of finetuning pretrained world models for few-shot learning of unseen tasks. Specifically, we pretrain a 19M parameter TD-MPC2 agent on 70 tasks from DMControl and Meta-World, and na\u0131\u0308vely finetune the full model to each of 10 held-out tasks (5 from each domain) via online RL with an initially empty replay buffer and no changes to hyperparameters. Aggregate\nresults are shown in Figure 8. We find that TD-MPC2 improves 2\u00d7 over learning from scratch on new tasks in the low-data regime (20k environment steps1). Although finetuning world models to new tasks is very much an open research problem, our exploratory results are promising. See Appendix E for experiment details and individual task curves.\nAblations. We ablate most of our design choices for TD-MPC2, including choice of actor, various normalization techniques, regression objective, and number of Q-functions. Our main ablations, shown in Figure 9, are conducted on three of the most difficult online RL tasks, as well as largescale multitask training (80 tasks). We observe that all of our proposed improvements contribute meaningfully to the robustness and strong performance of TD-MPC2 in both single-task RL and multi-task RL. Interestingly, we find that the relative importance of each design choice is consistent across both settings. Lastly, we also ablate normalization of the learned task embeddings, shown in Appendix F. The results indicate that maintaining a normalized task embedding space (\u21132-norm of 1) is moderately important for stable multitask training, and results in more meaningful task relations."
        },
        {
            "heading": "5 LESSONS, OPPORTUNITIES, AND RISKS",
            "text": "Lessons. Historically, RL algorithms have been notoriously sensitive to architecture, hyperparameters, characteristics of the task, and even random seed (Henderson et al., 2018), with no principled method for tuning the algorithms. As a result, successful application of deep RL often requires large teams of experts with significant computational resources (Berner et al., 2019; Schrittwieser et al., 2020; Ouyang et al., 2022). TD-MPC2 \u2013 along with several other contemporary RL methods (Yarats et al., 2021; Ye et al., 2021; Hafner et al., 2023) \u2013 seek to democratize use of RL (i.e., lowering the barrier of entry for smaller teams of academics, practitioners, and individuals with fewer resources) by improving robustness of existing open-source algorithms. We firmly believe that improving algorithmic robustness will continue to have profound impact on the field. A key lesson from the development of TD-MPC2 is that the community has yet to discover an algorithm that truly masters everything out-of-the-box. While e.g. DreamerV3 (Hafner et al., 2023) has delivered strong results on challenging tasks with discrete action spaces (such as Atari games and Minecraft), we find that TD-MPC2 produces significantly better results on difficult continuous control tasks. At the same time, extending TD-MPC2 to discrete action spaces remains an open problem.\nOpportunities. Our scaling results demonstrate a path for model-based RL in which massively multitask world models are leveraged as generalist world models. While multi-task world models\n120k environment steps corresponds to 20 episodes in DMControl and 100 episodes in Meta-World.\nremain relatively underexplored in literature, prior work suggests that the implicit world model of TD-MPC2 may be better suited than reconstruction-based approaches for tasks with large visual variation (Zhu et al., 2023). We envision a future in which implicit world models are used zero-shot to perform diverse tasks on seen embodiments (Xu et al., 2023; Yang et al., 2023), finetuned to quickly perform tasks on new embodiments, and combined with existing vision-language models to perform higher-level cognitive tasks in conjunction with low-level physical interaction. Our results are promising, but such level of generalization will likely require several orders of magnitude more tasks than currently available. Lastly, we want to remark that, while TD-MPC2 relies on rewards for task learning, it is useful to adopt a generalized notion of reward as simply a metric for task completion. Such metrics already exist in the wild, e.g., success labels, human preferences or interventions (Ouyang et al., 2022), or the embedding distance between a current observation and a goal (Eysenbach et al., 2022; Ma et al., 2022) within a pre-existing learned representation. However, leveraging such rewards for large-scale pretraining is an open problem. To accelerate research in this area, we are releasing 300+ TD-MPC2 models, including 12 multitask models, as well as datasets and code, and we are beyond excited to see what the community will do with these resources.\nRisks. While we are excited by the potential of generalist world models, several challenges remain: (i) misspecification of task rewards can lead to unintended outcomes (Clark & Amodei, 2016) that may be difficult to anticipate, (ii) handing over unconstrained autonomy of physical robots to a learned model can result in catastrophic failures if no additional safety checks are in place (Lancaster et al., 2023), and (iii) data for certain applications may be prohibitively expensive for small teams to obtain at the scale required for generalist behavior to emerge, leading to a concentration of power. Mitigating each of these challenges will require new research innovations, and we invite the community to join us in these efforts."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Multiple prior works have sought to build RL algorithms that are robust to hyperparameters, architecture, as well as variation in tasks and data. For example, (1) Double Q-learning (Hasselt et al., 2016), RED-Q (Chen et al., 2021), SVEA (Hansen et al., 2021), and SR-SPR (D\u2019Oro et al., 2023) each improve the stability of Q-learning algorithms by adjusting the bias-variance trade-off in TDtarget estimation, (2) C51 (Bellemare et al., 2017) and DreamerV3 (Hafner et al., 2023) improve robustness to the magnitude of rewards by performing discrete regression in a transformed space, and (3) model-free algorithms DrQ (Kostrikov et al., 2020) and DrQ-v2 (Yarats et al., 2021) improve training stability and exploration, respectively, through use of data augmentation and several other minor but important implementation details. However, all of the aforementioned works strictly focus on improving data-efficiency and robustness in single-task online RL.\nExisting literature that studies scaling of neural architectures for decision-making typically assume access to large datasets of near-expert demonstrations for behavior cloning (Reed et al., 2022; Lee et al., 2022; Kumar et al., 2022; Schubert et al., 2023; Driess et al., 2023; Brohan et al., 2023). Gato (Reed et al., 2022) learns to perform tasks across multiple domains by training a large Transformerbased sequence model (Vaswani et al., 2017) on an enormous dataset of expert demonstrations, and RT-1 (Brohan et al., 2023) similarly learns a sequence model for object manipulation on a single (real) robot embodiment by training on a large dataset collected by human teleoperation. While the empirical results of this line of work are impressive, the assumption of large demonstration datasets is impractical. Additionally, current sequence models rely on discretization of the action space (tokenization), which makes scaling to high-dimensional continuous control tasks difficult.\nMost recently, researchers have explored scaling of RL algorithms as a solution to the aforementioned challenges (Baker et al., 2022; Jia et al., 2022; Xu et al., 2023; Kumar et al., 2023; Hafner et al., 2023). For example, VPT (Baker et al., 2022) learns to play Minecraft by first pretraining a behavior cloning policy on a large human play dataset, and then finetuning the policy with RL. GSL (Jia et al., 2022) requires no pre-existing data. Instead, GSL iteratively trains a population of \u201cspecialist\u201d agents on individual task variations, distills them into a \u201cgeneralist\u201d policy via behavior cloning, and then uses the generalist as initialization for the next population of specialists. However, this work considers strictly single-task RL and assumes full control over the initial state in each episode. Lastly, DreamerV3 (Hafner et al., 2023) successfully scales its world model in terms of parameters and shows that larger models generally are more data-efficient in an online RL setting, but does not consider multitask RL."
        },
        {
            "heading": "A Summary of Improvements 15",
            "text": ""
        },
        {
            "heading": "B Task Visualizations 16",
            "text": ""
        },
        {
            "heading": "C Task Domains 17",
            "text": ""
        },
        {
            "heading": "D Single-task Experimental Results 20",
            "text": ""
        },
        {
            "heading": "E Few-shot Experimental Results 23",
            "text": ""
        },
        {
            "heading": "F Additional Ablations 24",
            "text": ""
        },
        {
            "heading": "G Gradient Norm and Training Stability 25",
            "text": "H Implementation Details 25"
        },
        {
            "heading": "I Extending TD-MPC2 to Discrete Action Spaces 28",
            "text": ""
        },
        {
            "heading": "J Test-Time Regularization for Offline RL 29",
            "text": "K Additional Multi-task Results 30"
        },
        {
            "heading": "A SUMMARY OF IMPROVEMENTS",
            "text": "We summarize the main differences between TD-MPC and TD-MPC2 as follows:\n\u2022 Architectural design. All components of TD-MPC2 are MLPs with LayerNorm (Ba et al., 2016) and Mish (Misra, 2019) activations after each layer. We apply SimNorm normalization to the latent state z which biases the representation towards sparsity and maintaining a small \u21132-norm. We train an ensemble of Q-functions (5 by default) and additionally apply 1% Dropout (Srivastava et al., 2014) after the first linear layer in each Q-function. TD-targets are computed as the mininum of two randomly subsampled Q-functions (Chen et al., 2021). In contrast, TD-MPC is implemented as MLPs without LayerNorm, and instead uses ELU (Clevert et al., 2015) activations. TD-MPC does not constrain the latent state at all, which in some instances leads to exploding gradients (see Appendix G for experimental results). Lastly, TD-MPC learns only 2 Q-functions and does not use Dropout. The architectural differences in TD-MPC2 result in a 4M net increase in learnable parameters (5M total) for our default single-task model size compared to the 1M parameters of TD-MPC. However, as shown in Figure 7, na\u0131\u0308vely increasing the model size of TD-MPC does not lead to consistently better performance, whereas it does for TD-MPC2.\n\u2022 Policy prior. The policy prior of TD-MPC2 is trained with maximum entropy RL (Ziebart et al., 2008; Haarnoja et al., 2018), whereas the policy prior of TD-MPC is trained as a deterministic policy with Gaussian noise applied to actions. We find that a carefully tuned Gaussian noise schedule is comparable to a policy prior trained with maximum entropy. However, maximum entropy RL can more easily be applied with task-agnostic hyperparameters.\n\u2022 Planning. The planning procedure of TD-MPC2 closely follows that of TD-MPC. However, we simplify planning marginally by not leveraging momentum between iteration, as we find it to produce comparable results. We also improve the throughput of planning by approx. 2\u00d7 through a series of code-level optimizations.\n\u2022 Model objective. We revisit the training objective of TD-MPC and improve its robustness to variation in tasks, such as the magnitude of rewards. TD-MPC2 uses discrete regression (soft cross-entropy) of rewards and values in a log-transformed space, which makes the magnitude of the two loss terms independent of the magnitude of the task rewards. TDMPC uses continuous regression which leads to training instabilities in tasks where rewards are large. While this issue can be alleviated by, e.g., normalizing task rewards based on moving statistics, in the single-task case, it is difficult to design robust reward normalization schemes for multi-task learning. TD-MPC2 retains the continuous regression term for jointembedding prediction as the latent representation is already normalized by SimNorm, and discrete regression is computationally expensive for high-dimensional spaces (requires N bins for each dimension of z).\n\u2022 Multi-task model. TD-MPC2 introduces a framework for learning multi-task world models across multiple domains, embodiments, and action spaces. We introduce a normalized learnable task embedding space which all components of TD-MPC are conditioned on, and we accommodate multiple observation and action spaces by applying zero-padding and action masking during both training and inference. We train multi-task models on a large number of tasks, and finetune the model to held-out tasks (across embodiments) using online RL. TD-MPC only considers multi-task learning on a small number of tasks with shared observation and action space, and does not consider finetuning of the learned multi-task model.\n\u2022 Simplified algorithm and implementation. TD-MPC2 removes momentum in MPPI (Williams et al., 2015), and replaces prioritized experience replay sampling from the replay buffer with uniform sampling, both of which simplify the implementation with no significant change in experimental results. Finally, we also use a faster replay buffer implementation that uses multiple workers for sampling, and we increase training and planning throughput through code-level optimizations such as Q-function ensemble vectorization, which makes the wall-time of TD-MPC2 comparable to that of TD-MPC despite a larger architecture (5M vs. 1M)."
        },
        {
            "heading": "B TASK VISUALIZATIONS",
            "text": ""
        },
        {
            "heading": "C TASK DOMAINS",
            "text": "We consider a total of 104 continuous control tasks from 4 task domains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al., 2022). This section provides an exhaustive list of all tasks considered, as well as their observation and action dimensions. Environment details are listed at the end of the section. We provide (static) task visualizations in Appendix B and videos of TD-MPC2 agents performing each task at https: //www.tdmpc2.com.\nEnvironment details. We benchmark algorithms on DMControl, Meta-World, ManiSkill2, and MyoSuite without modification. All four domains are infinite-horizon continuous control environments for which we use a fixed episode length and no termination conditions. We list episode lengths, action repeats, total number of environment steps, and the performance metric used for each domain in Table 6. In all experiments, we only consider an episode successful if the final step of an episode is successful. This is a stricter definition of success than used in some of the related literature, which e.g. may consider an episode successful if success is achieved at any step within a given episode. In tasks that require manipulation of objects, such as picking up an object, our definition of success ensures that an episode in which an object is picked up but then dropped again is not considered successful.\nD SINGLE-TASK EXPERIMENTAL RESULTS"
        },
        {
            "heading": "E FEW-SHOT EXPERIMENTAL RESULTS",
            "text": "We finetune a 19M parameter TD-MPC2 agent trained on 70 tasks to each of 10 held-out tasks. Individual task curves are shown in Figure 16. We compare data-efficiency of the finetuned model to a baseline agent of similar model capacity trained from scratch. However, we find that performance of our 19M parameter baselines trained from scratch are comparable to our 5M parameter agents also trained from scratch. Our few-shot finetuning results suggest that the efficacy of finetuning is somewhat task-dependent. However, more research is needed to conclude whether this is due to task similarity (or rather lack thereof) or due to subpar task performance of the pretrained agent on the source task. We conjecture that both likely influence results.\nWhen finetuning to an unseen task, we initialize the learnable task embedding for the new task as the embedding of a semantically similar task from the pretraining dataset. We list the source task embedding used as initialization for each experiment in Table 7. We did not experiment with other initialization schemes, nor other task pairings.\nF ADDITIONAL ABLATIONS"
        },
        {
            "heading": "G GRADIENT NORM AND TRAINING STABILITY",
            "text": "H IMPLEMENTATION DETAILS\nArchitectural details. All components of TD-MPC2 are implemented as MLPs. The encoder h contains a variable number of layers (2\u22125) depending on the architecture size; all other components are 3-layer MLPs. Intermediate layers consist of a linear layer followed by LayerNorm and a Mish activation function. The latent representation is normalized as a simplicial embedding. Q-functions additionally use Dropout. We summarize the TD-MPC2 architecture for the 5M parameter base (default for online RL) model size using PyTorch-like notation:\nEncoder parameters: 167,936 Dynamics parameters: 843,264 Reward parameters: 631,397 Policy parameters: 582,668 Q parameters: 3,156,985 Task parameters: 7,680 Total parameters: 5,389,930\nArchitecture: TD-MPC2 base 5M( (task_embedding): Embedding(T, 96, max_norm=1) (encoder): ModuleDict( (state): Sequential(\n(0): NormedLinear(in_features=S+T, out_features=256, act=Mish) (1): NormedLinear(in_features=256, out_features=512, act=SimNorm)\n) ) (dynamics): Sequential( (0): NormedLinear(in_features=512+T+A, out_features=512, act=Mish) (1): NormedLinear(in_features=512, out_features=512, act=Mish) (2): NormedLinear(in_features=512, out_features=512, act=SimNorm) ) (reward): Sequential( (0): NormedLinear(in_features=512+T+A, out_features=512, act=Mish) (1): NormedLinear(in_features=512, out_features=512, act=Mish) (2): Linear(in_features=512, out_features=101,) ) (pi): Sequential( (0): NormedLinear(in_features=512+T, out_features=512, act=Mish) (1): NormedLinear(in_features=512, out_features=512, act=Mish) (2): Linear(in_features=512, out_features=2A, bias=True) ) (Qs): Vectorized ModuleList( (0-4): 5 x Sequential(\n(0): NormedLinear(in_features=512+T+A, out_features=512, dropout=0.01, act=Mish) (1): NormedLinear(in_features=512, out_features=512, act=Mish) (2): Linear(in_features=512, out_features=101, bias=True)\n) )\nwhere S is the input dimensionality, T is the number of tasks, and A is the action space. We exclude the task embedding T from single-task experiments. The exact parameter counts listed above are for S= 39, T= 80, and A= 6.\nHyperparameters. We use the same hyperparameters across all tasks. Our hyperparameters are listed in Table 8. We use the same hyperparameters for TD-MPC and SAC as in Hansen et al. (2022). DreamerV3 (Hafner et al., 2023) uses a fixed set of hyperparameters.\nWe set the discount factor \u03b3 for a task using the heuristic\n\u03b3 = clip( T 5 \u2212 1\nT 5\n, [0.95, 0.995]) (7)\nwhere T is the expected length of an episode after applying action repeat, and clip constrains the discount factor to the interval [0.95, 0.995]. Using this heuristic, we obtain \u03b3 = 0.99 for DMControl (T = 500), which is the most widely used discount factor for this task domain. Tasks with shorter episodes are assigned a lower discount factor, whereas tasks with longer episodes are assigned a higher discount factor. All of the tasks that we consider are infinite-horizon MDPs with fixed episode lengths. We use individual discount factors (set using the above heuristic) for each task in our multitask experiments. For tasks with variable or unknown episode lengths, we suggest using an empirical mean length, a qualified guess, or simply \u03b3 = 0.99. While this heuristic is introduced in TD-MPC2, we apply the same discount factor for the TD-MPC and SAC baselines to ensure that comparison is fair across all task domains.\nWe set the seed steps S (number of environment steps before any gradient updates) for a task using the heuristic S = max(5T, 1000) (8) where T again is the expected episode length of the task after applying action repeat. We did not experiment with other heuristics nor constant values, but conjecture that Equation 8 will ensure that the replay buffer B has sufficient data for model learning regardless of episode lengths.\nModel configurations. Our multitask experiments consider TD-MPC2 agents with model sizes ranging from 1M parameters to 317M parameters. Table 9 lists the exact specifications for each of our model sizes. We scale the model size by varying dimensions of fully-connected layers, the latent state dimension z, the number of encoder layers, and the number of Q-functions. We make no other modifications to the architecture nor hyperparameters across model sizes.\nSimplicial Normalization (SimNorm). SimNorm is a simple method for normalizing the latent representation z by projecting it into L fixed-dimensional simplices using a softmax operation (Lavoie et al., 2022). A key benefit of embedding z as simplices (as opposed to e.g. a discrete representation or squashing) is that it naturally biases the representation towards sparsity without enforcing hard constraints. Intuitively, SimNorm can be thought of as a \u201dsoft\u201d variant of the vector-of-categoricals approach to representation learning proposed by Oord et al. (2017) (VQ-VAE). Whereas VQ-VAE represents latent codes using a set of discrete codes (L vector partitions each consisting of a one-hot encoding), SimNorm partitions the latent state into L vector partitions of continuous values that each sum to 1 due to the softmax operator. This relaxation of the latent representation is akin to softmax being a relaxation of the argmax operator. While we do not adjust the temperature \u03c4 \u2208 [0,\u221e) of the softmax used in SimNorm in our experiments, it is useful to note that it provides a mechanism\nfor interpolating between two extremes. For example, \u03c4 \u2192 \u221e would force all probability mass onto single categories, resulting in the discrete codes (one-hot encodings) of VQ-VAE. The alternative of \u03c4 = 0 would result in trivial codes (constant vectors; uniform probability mass) and prohibit propagation of information. SimNorm thus biases representations towards sparsity without enforcing discrete codes or other hard constraints. We implement the SimNorm normalization layer (Lavoie et al., 2022) using PyTorch-like notation as follows:\ndef simnorm(self, z, V=8): shape = z.shape z = z.view(*shape[:-1], -1, V) z = softmax(z, dim=-1) return z.view(*shape)\nHere, z is the latent representation z, and V is the dimensionality of each simplex. The number of simplices L can be inferred from V and the dimensionality of z. We apply a softmax (optionally modulated by a temperature \u03c4 ) to each of L partitions of z to form simplices, and then reshape to the original shape of z.\nTD-MPC baseline implementation. We benchmark against the official implementation of TDMPC available at https://github.com/nicklashansen/tdmpc. The default TD-MPC world model has approx. 1M trainable parameters, and uses per-task hyperparameters. We use the suggested hyperparameters where available (DMControl and Meta-World). For example, TD-MPC requires tuning of the number of planning iterations, latent state dimensionality, batch size, and learning rate in order to solve the challenging Dog and Humanoid tasks. Refer to their paper for a complete list of hyperparameters.\nDreamerV3 baseline implementation. We benchmark against the official reimplementation of DreamerV3 available at https://github.com/danijar/dreamerv3. We follow the authors\u2019 suggested hyperparameters for proprioceptive control (DMControl) and use the S model size (20M parameters), as well as an update-to-data (UTD) ratio of 512. We use this model size and UTD for all tasks. Refer to their paper for a complete list of hyperparameters.\nSAC baseline implementation. We follow the TD-MPC (Hansen et al., 2022) paper in their decision to benchmark against the SAC implementation from https://github.com/ denisyarats/pytorch_sac, and we use the hyperparameters suggested by the authors (when available). Specifically, this includes tuning the latent dimension, learning rate, and batch size for the Dog task. Refer to their paper for a complete list of hyperparameters.\nI EXTENDING TD-MPC2 TO DISCRETE ACTION SPACES\nIt is desirable to develop a single algorithm that excels at tasks with continuous and discrete action spaces alike. However, the community has yet to discover such an algorithm. While e.g. DreamerV3 (Hafner et al., 2023) has delivered strong results on challenging tasks with discrete action spaces (such as Atari and Minecraft), we find that TD-MPC2 produces significantly better results on difficult continuous control tasks. At the same time, extending TD-MPC2 to discrete action spaces remains an open problem. While we do not consider discrete action spaces in this work, we acknowledge the value of such an extension. At present, the main challenge in applying TD-MPC2 to discrete actions lies in the choice of planning algorithm. TD-MPC2 relies on the MPC framework for planning, which is designed for continuous action spaces. We believe that MPC could be replaced with a planning algorithm designed for discrete action spaces, such as MCTS (Coulom, 2007) as used in MuZero (Schrittwieser et al., 2020). It is also possible that there exists a way to apply MPC to discrete action spaces that is yet to be discovered (to the best of our knowledge), similar to how recent work (Hubert et al., 2021) has discovered ways to apply MCTS to continuous action spaces through sampling."
        },
        {
            "heading": "J TEST-TIME REGULARIZATION FOR OFFLINE RL",
            "text": "Our multi-task experiments revolve around training massively multi-task world models on fixed datasets that consist of a variety of behaviors, which is an offline RL problem. We do not consider any special treatment of the offline RL problem in the main paper, and simply train TD-MPC2 agents without any additional regularization nor hyperparameter-tuning. However, we recognize that models may benefit from such regularization (conservative estimations) due to extrapolation errors when the dataset has limited state-action coverage and/or is highly skewed. Current offline RL algorithms are ill-suited for our problem setting, given that we aim to develop an algorithm that can seamlessly transition from massively multi-task offline pretraining to single-task online finetuning, without any changes in hyperparameters. Current offline RL techniques rely on (1) explicit or implicit conservatism in Q-value estimation which requires modifications to the training objective and empirically hampers online RL performance (Nakamoto et al., 2023), and (2) relies on a task-specific coefficient that balances value estimation and the regularizer. Instead, we propose to regularize the planning procedure of TD-MPC2, which can be done at test-time without any additional model updates. Concretely, we apply the test-time regularizer proposed by Feng et al. (2023), which penalizes trajectories with large uncertainty (as estimated by the variance in Q-value predictions) during planning. While this approach eliminates the need for training-time regularization, it still requires users to specify a coefficient that weighs estimated value relative to uncertainty for a trajectory, which is infeasible in a multi-task scenario where estimated values may differ drastically between tasks. To circumvent this issue, we propose a simple heuristic for automatically scaling the regularization strength at each timestep based on the (magnitude of) mean value predictions for a given latent state. Specifically, we estimate the uncertainty penalty at latent state zt of a sampled trajectory as\nut = c \u00b7 avg([q\u03021, q\u03022, . . . , q\u0302N ]) \u00b7 std([q\u03021, q\u03022, . . . , q\u0302N ]) (9)\nwhere q\u0302n is a value prediction from Q-function n in an ensemble of N Q-functions, and c is now a task-agnostic coefficient that balances return maximization and uncertainty minimization. The planning objective in Equation 6 is then redefined as\n\u00b5\u2217, \u03c3\u2217 = argmax (\u00b5,\u03c3) E (at,at+1,...,at+H)\u223cN (\u00b5,\u03c32) (10)[ \u03b3HQ(zt+H ,at+H)\u2212 ut+H (11)\n+ H\u22121\u2211 h=t ( \u03b3hR(zh,ah)\u2212 uh\n) ] , (12)\nusing the definition of task-agnostic uncertainty in Equation 9. We conduct an experiment in which we apply our proposed test-time regularization to a 19M parameter TD-MPC2 agent trained on the 80-task dataset, varying the regularization strength c. Results are shown in Table 10. Our results indicate that additional regularization (using our heuristic for automatic tuning) can indeed improve the average model performance for some values of c. Similar to what one would expect in a single-task setting, we find that large values of c (strong regularization) decrease performance, whereas small (but > 0) values of c tend to improve performance compared to TD-MPC2 without regularization. Given that our heuristic with c = 0.01 leads to meaningful improvements across 80 tasks, we expect it to work reasonably well for other datasets as well, but leave this for future work."
        },
        {
            "heading": "K ADDITIONAL MULTI-TASK RESULTS",
            "text": "To provide further insights into the effect of data size and task diversity on TD-MPC2 performance in a multi-task setting, we provide additional experiments on a 15-task subset of DMControl, selecting at random. Results for TD-MPC2 agents trained on 15 tasks, 30 tasks, and 80 tasks are shown in Figure 21. We observe that performance scales with model size across all three task suites, but numbers are higher across the board on the smallest dataset compared to similar capacity models trained on larger datasets. This makes intuitive sense, since model capacity remains the same while there is comparably less knowledge to learn."
        }
    ],
    "year": 2023
}