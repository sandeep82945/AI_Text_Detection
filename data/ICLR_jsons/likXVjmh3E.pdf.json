{
    "abstractText": "Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model f to accurately represent any smaller target model f if LoRA-rank \u2265 (width of f)\u00d7 depth of f depth of f , under a mild assumption. We also quantify the approximation error when the LoRArank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-( embedding size 2 ) LoRA adapters. All our theoretical insights are validated by numerical experiments.",
    "authors": [],
    "id": "SP:14f52cc43a20bc4f07d1668746f1caa56a028e97",
    "references": [
        {
            "authors": [
                "Kwangjun Ahn",
                "Xiang Cheng",
                "Hadi Daneshmand",
                "Suvrit Sra"
            ],
            "title": "Transformers learn to implement preconditioned gradient descent for in-context learning",
            "venue": "arXiv preprint arXiv:2306.00297,",
            "year": 2023
        },
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Dale Schuurmans",
                "Jacob Andreas",
                "Tengyu Ma",
                "Denny Zhou"
            ],
            "title": "What learning algorithm is in-context learning? investigations with linear models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Yu Bai",
                "Fan Chen",
                "Huan Wang",
                "Caiming Xiong",
                "Song Mei"
            ],
            "title": "Transformers as statisticians: Provable in-context learning with in-context algorithm selection",
            "venue": "arXiv preprint arXiv:2306.04637,",
            "year": 2023
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Shahar Mendelson"
            ],
            "title": "Rademacher and Gaussian complexities: Risk bounds and structural results",
            "venue": "In Computational Learning Theory (COLT),",
            "year": 2001
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Yoav Goldberg",
                "Shauli Ravfogel"
            ],
            "title": "BitFit: Simple parameter-efficient finetuning for Transformer-based masked language-models",
            "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2022
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Olivier Delalleau"
            ],
            "title": "On the expressive power of deep architectures",
            "venue": "In Algorithmic Learning Theory, pp",
            "year": 2011
        },
        {
            "authors": [
                "Alberto Bietti",
                "Francis Bach"
            ],
            "title": "Deep equals shallow for ReLU networks in kernel regimes",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Richard Caron",
                "Tim Traynor"
            ],
            "title": "The zero set of a polynomial",
            "venue": "WSMR Report,",
            "year": 2005
        },
        {
            "authors": [
                "Ken Chatfield",
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Return of the devil in the details: Delving deep into convolutional nets",
            "venue": "arXiv preprint arXiv:1405.3531,",
            "year": 2014
        },
        {
            "authors": [
                "Pin-Yu Chen"
            ],
            "title": "Model reprogramming: Resource-efficient cross-domain machine learning",
            "venue": "arXiv preprint arXiv:2202.10629,",
            "year": 2022
        },
        {
            "authors": [
                "George Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function",
            "venue": "Mathematics of control, signals and systems,",
            "year": 1989
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer"
            ],
            "title": "QLoRA: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional Transformers for language understanding",
            "venue": "In North American Chapter of the Association for Computational Linguistics (NAACL),",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Yujia Qin",
                "Guang Yang",
                "Fuchao Wei",
                "Zonghan Yang",
                "Yusheng Su",
                "Shengding Hu",
                "Yulin Chen",
                "Chi-Min Chan",
                "Weize Chen",
                "Jing Yi",
                "Weilin Zhao",
                "Xiaozhi Wang",
                "Zhiyuan Liu",
                "Hai-Tao Zheng",
                "Jianfei Chen",
                "Yang Liu",
                "Jie Tang",
                "Juanzi Li",
                "Maosong Sun"
            ],
            "title": "Parameter-efficient finetuning of large-scale pre-trained language models",
            "venue": "Nature Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Tuan Dinh",
                "Daewon Seo",
                "Zhixu Du",
                "Liang Shang",
                "Kangwook Lee"
            ],
            "title": "Improved input reprogramming for GAN conditioning",
            "venue": "arXiv preprint arXiv:2201.02692,",
            "year": 2022
        },
        {
            "authors": [
                "Tuan Dinh",
                "Yuchen Zeng",
                "Ruisu Zhang",
                "Ziqian Lin",
                "Michael Gira",
                "Shashank Rajput",
                "Jy-yong Sohn",
                "Dimitris Papailiopoulos",
                "Kangwook Lee"
            ],
            "title": "LIFT: Language-interfaced fine-tuning for nonlanguage machine learning tasks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Donahue",
                "Yangqing Jia",
                "Oriol Vinyals",
                "Judy Hoffman",
                "Ning Zhang",
                "Eric Tzeng",
                "Trevor Darrell"
            ],
            "title": "DeCAF: A deep convolutional activation feature for generic visual recognition",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2014
        },
        {
            "authors": [
                "Simon S Du",
                "Wei Hu",
                "Sham M Kakade",
                "Jason D Lee",
                "Qi Lei"
            ],
            "title": "Few-shot learning via learning the representation, provably",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Carl Eckart",
                "Gale Young"
            ],
            "title": "The approximation of one matrix by another of lower rank",
            "year": 1936
        },
        {
            "authors": [
                "Ronen Eldan",
                "Ohad Shamir"
            ],
            "title": "The power of depth for feedforward neural networks",
            "venue": "In Annual Conference on Learning Theory,",
            "year": 2016
        },
        {
            "authors": [
                "Gamaleldin F Elsayed",
                "Ian Goodfellow",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Adversarial Reprogramming of neural networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Engel",
                "Matthew Hoffman",
                "Adam Roberts"
            ],
            "title": "Latent constraints: Learning to generate conditionally from unconditional generative models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Matthias Englert",
                "Ranko Lazic"
            ],
            "title": "Adversarial Reprogramming revisited",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Ying Fan",
                "Olivia Watkins",
                "Yuqing Du",
                "Hao Liu",
                "Moonkyung Ryu",
                "Craig Boutilier",
                "Pieter Abbeel",
                "Mohammad Ghavamzadeh",
                "Kangwook Lee",
                "Kimin Lee"
            ],
            "title": "DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2305.16381,",
            "year": 2023
        },
        {
            "authors": [
                "Angeliki Giannou",
                "Shashank Rajput",
                "Dimitris Papailiopoulos"
            ],
            "title": "The expressive power of tuning only the Norm layers",
            "venue": "arXiv preprint arXiv:2302.07937,",
            "year": 2023
        },
        {
            "authors": [
                "Angeliki Giannou",
                "Shashank Rajput",
                "Jy-Yong Sohn",
                "Kangwook Lee",
                "Jason D. Lee",
                "Dimitris Papailiopoulos"
            ],
            "title": "Looped Transformers as programmable computers",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Michael Gira",
                "Ruisu Zhang",
                "Kangwook Lee"
            ],
            "title": "Debiasing pre-trained language models via efficient fine-tuning",
            "venue": "In Workshop on Language Technology for Equality, Diversity and Inclusion,",
            "year": 2022
        },
        {
            "authors": [
                "Moritz Hardt",
                "Tengyu Ma"
            ],
            "title": "Identity matters in deep learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "DeBERTa: Decoding-enhanced BERT with disentangled attention",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kurt Hornik",
                "Maxwell Stinchcombe",
                "Halbert White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks,",
            "year": 1989
        },
        {
            "authors": [
                "Daniel Hsu",
                "Clayton H Sanford",
                "Rocco Servedio"
            ],
            "title": "On the approximation power of two-layer networks of random ReLUs",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Shengding Hu",
                "Zhen Zhang",
                "Ning Ding",
                "Yadao Wang",
                "Yasheng Wang",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "title": "Sparse structure search for delta tuning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Like Hui",
                "Mikhail Belkin"
            ],
            "title": "Evaluation of neural architectures trained with square loss vs crossentropy in classification tasks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Gal Kaplun",
                "Andrey Gurevich",
                "Tal Swisa",
                "Mazor David",
                "Shai Shalev-Shwartz",
                "Eran Malach"
            ],
            "title": "Subtuning: Efficient finetuning for multi-task learning",
            "venue": "arXiv preprint arXiv:2302.06354,",
            "year": 2023
        },
        {
            "authors": [
                "Kenji Kawaguchi"
            ],
            "title": "Deep learning without poor local minima",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Simon Kornblith",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Do better imagenet models transfer better",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Laurent",
                "James von Brecht"
            ],
            "title": "Deep linear networks with arbitrary loss: All local minima are global",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Holden Lee",
                "Rong Ge",
                "Tengyu Ma",
                "Andrej Risteski",
                "Sanjeev Arora"
            ],
            "title": "On the ability of neural nets to express distributions",
            "venue": "In Conference on Learning Theory,",
            "year": 2017
        },
        {
            "authors": [
                "Kangwook Lee",
                "Changho Suh",
                "Kannan Ramchandran"
            ],
            "title": "Reprogramming GANs via input noise design. In Machine Learning and Knowledge Discovery in Databases ",
            "venue": "European Conference, (ECML PKDD),",
            "year": 2020
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "In Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL/IJCNLP),",
            "year": 2021
        },
        {
            "authors": [
                "Shiyu Liang",
                "R. Srikant"
            ],
            "title": "Why deep neural networks for function approximation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Valerii Likhosherstov",
                "Krzysztof Choromanski",
                "Adrian Weller"
            ],
            "title": "On the expressive power of self-attention matrices",
            "year": 2021
        },
        {
            "authors": [
                "Chaoyue Liu",
                "Libin Zhu",
                "Mikhail Belkin"
            ],
            "title": "Loss landscapes and optimization in overparameterized non-linear systems and neural networks",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Mohammed Muqeeth",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin A Raffel"
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Haihao Lu",
                "Kenji Kawaguchi"
            ],
            "title": "Depth creates no bad local minima",
            "venue": "arXiv preprint arXiv:1702.08580,",
            "year": 2017
        },
        {
            "authors": [
                "Zhou Lu",
                "Hongming Pu",
                "Feicheng Wang",
                "Zhiqiang Hu",
                "Liwei Wang"
            ],
            "title": "The expressive power of neural networks: A view from the width",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sadhika Malladi",
                "Alexander Wettig",
                "Dingli Yu",
                "Danqi Chen",
                "Sanjeev Arora"
            ],
            "title": "A kernel-based view of language model fine-tuning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Andreas Maurer",
                "Massimiliano Pontil",
                "Bernardino Romera-Paredes"
            ],
            "title": "The benefit of multitask representation learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Leon Mirsky"
            ],
            "title": "Symmetric gauge functions and unitarily invariant norms",
            "venue": "The quarterly journal of mathematics,",
            "year": 1960
        },
        {
            "authors": [
                "Samet Oymak",
                "Ankit Singh Rawat",
                "Mahdi Soltanolkotabi",
                "Christos Thrampoulidis"
            ],
            "title": "On the role of attention in prompt-tuning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Sejun Park",
                "Chulhee Yun",
                "Jaeho Lee",
                "Jinwoo Shin"
            ],
            "title": "Minimum width for universal approximation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Jorge P\u00e9rez",
                "Javier Marinkovi\u0107",
                "Pablo Barcel\u00f3"
            ],
            "title": "On the turing completeness of modern neural network architectures",
            "venue": "arXiv preprint arXiv:1901.03429,",
            "year": 2019
        },
        {
            "authors": [
                "Maithra Raghu",
                "Ben Poole",
                "Jon Kleinberg",
                "Surya Ganguli",
                "Jascha Sohl-Dickstein"
            ],
            "title": "On the expressive power of deep neural networks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2007
        },
        {
            "authors": [
                "Simo Ryu"
            ],
            "title": "Low-rank adaptation for fast text-to-image diffusion fine-tuning",
            "venue": "https://github. com/cloneofsimo/lora,",
            "year": 2023
        },
        {
            "authors": [
                "Andrew M Saxe",
                "James L McClelland",
                "Surya Ganguli"
            ],
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
            "year": 2014
        },
        {
            "authors": [
                "Daewon Seo",
                "Hongyi Wang",
                "Dimitris Papailiopoulos",
                "Kangwook Lee"
            ],
            "title": "Empirical study on the effective VC dimension of low-rank neural networks",
            "venue": "In ICML Workshop on Overparameterization: Pitfalls & Opportunities,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Sharif Razavian",
                "Hossein Azizpour",
                "Josephine Sullivan",
                "Stefan Carlsson"
            ],
            "title": "Cnn features offthe-shelf: an astounding baseline for recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2014
        },
        {
            "authors": [
                "Haizhao Shen",
                "ZuoweiYang",
                "Shijun Zhang"
            ],
            "title": "Deep network approximation characterized by number of neurons",
            "venue": "Communications in Computational Physics,",
            "year": 2020
        },
        {
            "authors": [
                "Matus Telgarsky"
            ],
            "title": "Representation benefits of deep feedforward networks",
            "venue": "arXiv preprint arXiv:1509.08101,",
            "year": 2015
        },
        {
            "authors": [
                "Matus Telgarsky"
            ],
            "title": "Benefits of depth in neural networks",
            "venue": "In Conference on Learning Theory,",
            "year": 2016
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aur\u00e9lien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "LLaMA: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Nilesh Tripuraneni",
                "Michael Jordan",
                "Chi Jin"
            ],
            "title": "On the theory of transfer learning: The importance of task diversity",
            "venue": "In Advances in neural information processing systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir N Vapnik",
                "A Ya Chervonenkis"
            ],
            "title": "On the uniform convergence of relative frequencies of events to their probabilities",
            "venue": "In Measures of Complexity: Festschrift for Alexey Chervonenkis,",
            "year": 2015
        },
        {
            "authors": [
                "Johannes von Oswald",
                "Eyvind Niklasson",
                "Ettore Randazzo",
                "Jo\u00e3o Sacramento",
                "Alexander Mordvintsev",
                "Andrey Zhmoginov",
                "Max Vladymyrov"
            ],
            "title": "Transformers learn in-context by gradient descent",
            "venue": "arXiv preprint arXiv:2212.07677,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "In EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2018
        },
        {
            "authors": [
                "Noam Wies",
                "Yoav Levine",
                "Amnon Shashua"
            ],
            "title": "The learnability of in-context learning",
            "venue": "arXiv preprint arXiv:2303.07895,",
            "year": 2023
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Aditi Raghunathan",
                "Percy Liang",
                "Tengyu Ma"
            ],
            "title": "An explanation of in-context learning as implicit bayesian inference",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Chulhee Yun",
                "Srinadh Bhojanapalli",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Sanjiv Kumar"
            ],
            "title": "Are Transformers universal approximators of sequence-to-sequence functions",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Chulhee Yun",
                "Yin-Wen Chang",
                "Srinadh Bhojanapalli",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Sanjiv Kumar"
            ],
            "title": "O(n) connections are expressive enough: Universal approximability of sparse transformers",
            "year": 2020
        },
        {
            "authors": [
                "Qingru Zhang",
                "Minshuo Chen",
                "Alexander Bukharin",
                "Pengcheng He",
                "Yu Cheng",
                "Weizhu Chen",
                "Tuo Zhao"
            ],
            "title": "Adaptive budget allocation for parameter-efficient fine-tuning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Hsu"
            ],
            "title": "2021) examined the expressive power of randomly generated twolayer FNNs. Our work shares similarities with this direction, as we also delve into scenarios with randomly generated models. Beyond characterizing expressive power by approximation error, alternative metrics have been proposed",
            "venue": "Metrics such as Vapnik-Chervonenkis (Vapnik & Chervonenkis,",
            "year": 2015
        },
        {
            "authors": [
                "Raghu"
            ],
            "title": "2017) introduced a novel metric that captures the structural properties of an FNN, and Lee et al. (2017) investigated the ability of FNNs to express distributions. Expressive Power of Transformers As TFNs have grown increasingly popular, a few studies",
            "year": 2020
        },
        {
            "authors": [
                "Likhosherstov"
            ],
            "title": "approximation theorem for TFNs in approximating sequence-to-sequence",
            "venue": "Giannou et al",
            "year": 2023
        },
        {
            "authors": [
                "Tripuraneni"
            ],
            "title": "2020) further refines this bound by studying the effect of the number of samples in the pre-trained tasks. Interestingly, the estimation error of the final layers tuning provided in Tripuraneni et al. (2020) heavily depends on the quality of the shared representation. This insight aligns with our finding on final layers tuning (Lemma 4), which implies that tuning the final layers fails to adapt an L-layer randomly generated FNN to approximate any",
            "year": 2020
        },
        {
            "authors": [
                "2023 Bai et al",
                "2023 Wies et al",
                "2022 Xie et al",
                "2022 von Oswald et al",
                "2023). Aky\u00fcrek Ahn et al"
            ],
            "title": "2023) demonstrates that transformer-based in-context learners implicitly implement standard learning algorithms, while Bai et al. (2023) presents a similar finding and posits that in-context learning performs algorithm selection like a statistician",
            "venue": "retical underpinnings of in-context learning (Akyu\u0308rek et al.,",
            "year": 2023
        },
        {
            "authors": [
                "\u2208 Il"
            ],
            "title": "Under review as a conference",
            "venue": "Existence of x1,x2",
            "year": 2024
        },
        {
            "authors": [
                "Hu"
            ],
            "title": "2022a) LoRA outperforms final layers tuning if the quality of shared representation is not good",
            "year": 2022
        },
        {
            "authors": [
                "Hu"
            ],
            "title": "2022a) Tuning attention weights is sufficient for achieving good performance on TFNs",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent foundation models, such as large language models (OpenAI, 2023; Liu et al., 2019; Touvron et al., 2023), have achieved remarkable success in a wide range of applications. Due to their substantial size, the standard full fine-tuning approach\u2014where all the model\u2019s parameters are updated for specialized tasks\u2014is becoming increasingly difficult and inefficient. This leads to the growing popularity of parameter-efficient fine-tuning approaches (Hu et al., 2022a; Liu et al., 2022b; Ben Zaken et al., 2022; Hu et al., 2022b). Instead of updating all parameters, these approaches selectively update smaller subsets of weights or introduce lightweight adapters, thereby greatly decreasing the computational and storage costs.\nThe most dominant approach along this line is Low-Rank Adaptation (LoRA) (Hu et al., 2022a), which employs lightweight low-rank adapters to pre-trained weight matrices. Far from merely enhancing computational efficiency, empirical evidence has shown that LoRA can match or even exceed the performance of full fine-tuning (Hu et al., 2022a). To date, LoRA has been widely used and achieved considerable success in adapting large language models (Hu et al., 2022a; Dinh et al., 2022b) and image generation models (Ryu, 2023; Fan et al., 2023) for various downstream tasks.\nDespite the empirical success of LoRA, little is known in theory about how it works. A notable exception (Malladi et al., 2023) showed that LoRA finetuning is approximately equivalent to full fine-tuning in the lazy regime. However, many theoretical questions remain open, such as: What is the minimum rank of the LoRA adapters required to adapt a (pre-trained) model f to match the functionality of the target model f? How does the model architecture (i.e., depth, width) affect the minimal rank? If the adapter rank is lower than this threshold, what is the resulting approximation error? Answering such questions will provide important theoretical insights into when and why LoRA achieves effective adaptation.\nOur Contributions. In this paper, we present the first set of theoretical results that characterize the expressive power of Low-Rank Adaptation (LoRA) for Fully Connected Neural Networks (FNN) and Transformer Networks (TFN). In particular, we identify the necessary LoRA-rank for adapting a frozen model to exactly match a target model. For FNN cases, we also establish the required LoRA-rank for closely approximating the target model when a small approximation error is allowed. Our work focuses solely on the expressive power of the model with low-rank adapters, i.e., we show that under which conditions, effective low-rank adapters exist for the given adaptation task. This excludes other aspects such as optimization and generalization.\nWe now present the essence of our main theoretical findings in the following informal statement. Theorem 1 (Informal). Let f be a target FNN and f0 be an arbitrary frozen FNN. Under mild conditions on ranks and network architectures, there exist low-rank adapters such that a low-rank adapted version of f0 is exactly equal to f .\nWe present the detailed formulations of Theorem 1 under two scenarios: (i) applying a uniform rank across all LoRA adapters, as detailed in Theorem 3 and the specialized instance Corollary 4 for randomly drawn frozen and target models; and (ii) allowing different ranks applied to each LoRA adapter, as described in Theorem 6. To the best of our knowledge, this is the first known theoretical results on the expressive power of LoRA. While this informal theorem is for exact approximation, we also derive the approximation bounds as well, i.e., we characterize the approximation error between the finetuned model and the target model as a function of the LoRArank, as provided in Theorem 5 for the uniform LoRA-rank scenario and Theorem 6 for general cases. Furthermore, within the same framework, we investigate the expressive power of tuning the final layers for randomly generated frozen models, as described in Lemma 4. This result allows us to contrast LoRA and final layer tuning, thereby providing insights for future algorithm development.\nWe summarize our main findings on TFN in the following informal theorem. Theorem 2 (Informal). Let f be the target TFN and f0 be the frozen TFN. Under mild conditions on ranks and network architectures, there exist low-rank adapters for attention weight matrices such that a low-rank adapted version of f0 is exactly equal to f .\nThe formal statement of Theorem 2 is provided in Theorem 7, with a specialized version in Corollary 10 tailored for randomly generated frozen and target models.\nMoreover, in Sec. 5 and G, we perform numerical experiments to corroborate our theoretical assertions. These experiments reveal that the approximation error of LoRA diminishes as the adapter rank increases. Once the rank exceeds the critical rank threshold, the approximation error becomes near zero, corroborating our theoretical findings."
        },
        {
            "heading": "1.1 RELATED WORKS",
            "text": "Expressive Power of Neural Networks Theoretical study of the expressive power of unfrozen neural networks has progressed since the first universal approximation theorem (Hornik et al., 1989), showing that sufficient network width and depth can guarantee function approximation (Bengio & Delalleau, 2011; Eldan & Shamir, 2016; Liang & Srikant, 2017). Many recent studies obtained similar results for deep neural networks with modern twists such as ReLU activations and Transformer networks (Yun et al., 2020a; Raghu et al., 2017; Telgarsky, 2016; 2015; Bietti & Bach, 2021; Oymak et al., 2023; Lee et al., 2017; Shen & Zhang, 2020; Likhosherstov et al., 2021; Hsu et al., 2021; Park et al., 2021; Yun et al., 2020b; Giannou et al., 2023b). Metrics like Vapnik-Chervonenkis and Rademacher complexities (Vapnik & Chervonenkis, 2015; Bartlett & Mendelson, 2001) assess classification capacity. However, these theories cannot fully explain the performance of frozen neural networks as they generally cannot factor in pre-trained model parameters and adaptation methods.\nExpressive Power of Adaptation Methods In stark contrast to the flourishing research on the expressive power of neural networks, there exists a limited number of works investigating the expressive power of adaptation methods. A notable exception is Giannou et al. (2023a), investigating the expressive power of normalization parameter fine-tuning. They demonstrate that fine-tuning the normalization layers alone can adapt a randomly initialized ReLU network to match any target network that is O(width) times smaller. We borrow some proof techniques from this work, including techniques for extending results from linear neural networks to ReLU neural networks. In another recent work (Englert & Lazic, 2022), the authors show that neural reprogramming (Elsayed et al., 2019; Engel et al., 2018; Lee et al., 2020; Dinh et al., 2022a; Chen, 2022), a technique that modifies only the inputs while keeping the pretrained network frozen, can adapt any random two-layer ReLU network to achieve arbitrarily high accuracy on a Bernoulli data model over hypercube vertices. Despite these early attempts, no existing study has yet explored the expressive power of LoRA, the current leading adaptation method.\nA more detailed discussion of related works is provided in Sec. B."
        },
        {
            "heading": "1.2 NOTATIONS",
            "text": "Define [N ] := {1, 2, . . . , N}. Let the operators \u2227 and \u2228 denote the minimum function and the maximum function, respectively. We use I to represent the identity matrix.\nFor a sequence of L matrices (Wl) L l=1, we simplify the product of these matrices WLWL\u22121 \u00b7 \u00b7 \u00b7W1 as \u220fL\nl=1 Wl, with matrices multiplied in descending order from WL to W1. When m > n, we\ndefine \u2211n i=m ai = 0 and \u220fn i=m ai = 1 for scalars (ai) n i=m, and \u2211n i=m Wi = O and \u220fn i=m Wi = I for square matrices (Wi) n i=m.\nSingular Value Decomposition (SVD) of the matrix W can be expressed as W = UDV \u22a4, where U ,V \u2208 RD\u00d7D are orthonormal matrices and D \u2208 RD\u00d7D is a diagonal matrix. The singular values, sorted in descending sequence, are represented on the diagonal of D, denoted as \u03c31(W ) \u2265 \u03c32(W ) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3D(W ) \u2265 0, where \u03c3d(W ) denotes the d-th largest singular value for all d \u2208 [D]. When d > D, \u03c3d(W ) is defined as zero. The best rank-r approximation (in the Frobenius norm or the 2-norm) of W is \u2211r i=1 \u03c3iuiv T i , where ui and vi are the i-th column of U and V , respectively (Eckart & Young, 1936; Mirsky, 1960). We denote this best rank-r approximation by LRr(W ), where LR is a shorthand for \u201cLow-Rank\u201d. When r \u2265 rank(W ), it is clear that LRr(W ) = W . Occasionally, the subscript r may be omitted to indicate a general low-rank approximation without specifying the rank."
        },
        {
            "heading": "2 WARM UP: EXPRESSIVE POWER OF LINEAR MODELS WITH LORA",
            "text": "Before delving into the expressive power of LoRA for FNN and TFN, we begin by investigating the simplest scenario: both the target model f and the frozen model f0 are linear, i.e.,\nTarget Model f(x) = Wx, Frozen Model f0(x) = WL \u00b7 \u00b7 \u00b7W1x = (\u220fL l=1 Wl ) x.\nThis problem serves as a simplified version of approximating a target FNN, where the target model f has a single layer, the frozen model f0 has L layers, all bias vectors in both two models are zero, and the activation functions are linear. Throughout this paper, for the sake of simplicity, we will assume that both models have the same number of neurons in each layer, i.e., W ,W1, . . . ,WL \u2208 RD\u00d7D. Nevertheless, our results are readily extendable to situations where the frozen model is wider than the target model, which is a more natural setting as the frozen models are often overparameterized to ensure high capacity and good performance across diverse tasks in practice. See the discussion in Sec. I for more details.\nThe objective here is to incorporate low-rank adapters into the frozen model so that the adapted model can effectively approximate the target model. Unless otherwise specified, we always consider a uniform LoRA-rank for all low-rank adapters throughout this paper. For a given LoRA-rank R \u2208 [D], we apply LoRA adapters \u2206W 1, . . . ,\u2206WL to the frozen model, and the adapted model can be represented as\nAdapted Model f(x) = (WL +\u2206WL) \u00b7 \u00b7 \u00b7 (W1 +\u2206W 1)x, where rank(\u2206W l) \u2264 R for all l \u2208 [L]. Since the frozen model and adpated model are all linear, we can focus on quantifying the discrepancy between the linear coefficients, i.e., \u220fL l=1(Wl+\u2206W l)\u2212W . In the subsequent lemma, we establish the minimal achievable norm, and identify the smallest LoRA-rank required for the adapted model to exactly represent the target model, i.e., f = f , under a non-singularity assumption. We will demonstrate in Sec. 3.3 that this non-singularity assumption is mild, as it can be satisfied even by randomly generated weight matrices.\nLemma 1. Define error matrix E := W \u2212\u220fLl=1 Wl, and denote its rank by RE = rank(E). For a given LoRA-rank R \u2208 [D], assume that all the weight matrices of the frozen model (Wl)Ll=1, and\u220fL\nl=1 Wl + LRr(E) are non-singular for all r \u2264 R(L\u2212 1). Then, we have the following:\nmin \u2206W l:rank(\u2206W l)\u2264R \u2225\u2225\u2225\u220fLl=1(Wl +\u2206W l)\u2212W\u2225\u2225\u2225 2 = \u03c3RL+1(E).\nThus, when R \u2265 \u2308REL \u2309, the optimal solution satisfies \u220fL l=1(Wl +\u2206W l) = W , implying f = f .\nProof Sketch. We start the proof by noting that the distance between the adapted and target models\u2225\u2225\u2225\u2225\u2225 L\u220f\nl=1 (Wl +\u2206W l)\u2212W \u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225\u2225 ( L\u220f l=1 (Wl +\u2206W l)\u2212 L\u220f l=1 Wl ) \u2212 ( W \u2212 L\u220f l=1 Wl )\u2225\u2225\u2225\u2225\u2225 2 .\nThe remaining proof aims to minimize the right-hand side under the constraint rank(\u2206W l) \u2264 R for all l \u2208 [L]. The basic idea here is to match\u220fLl=1(Wl +\u2206W l)\u2212\u220fLl=1 Wl with the best rank-r approximation of W \u2212\u220fLl=1 Wl. The key steps to solve this problem are as follows. 1. Demonstrate that \u220fL l=1(Wl +\u2206W l)\u2212\n\u220fL l=1 Wl can be decomposed into L terms:\u220fL\nl=1(Wl +\u2206W l)\u2212 \u220fL l=1 Wl = \u2211L l=1 (\u220fL i=l+1 Wi ) \u2206W l (\u220fl\u22121 i=1(Wi +\u2206W i) ) .\nSince rank(\u2206W l) \u2264 R, it follows that rank (\u220fL l=1(Wl +\u2206W l)\u2212 \u220fL l=1 Wl ) \u2264 RL.\n2. Consider the rank-RL approximation of W \u2212\u220fLl=1 Wl. Decompose this low-rank approximation into L terms \u2211L l=1 El such that rank(El) \u2264 R, where El\u2019s will be determined later.\n3. To match \u220fL l=1(Wl + \u2206W l) \u2212 \u220fL\nl=1 Wl with the rank-RL approximation of W \u2212\u220fL l=1 Wl, we let (\u220fL i=l+1 Wi ) \u2206W l (\u220fl\u22121 i=1(Wi +\u2206W i) ) = El by choosing \u2206W l =(\u220fL\ni=l+1 Wi )\u22121 El (\u220fl\u22121 i=1(Wi +\u2206W i) )\u22121 .\n4. Select appropriate (El)Ll=1 such that Wi +\u2206W i are invertible for i \u2208 [L].\nThe complete proof and the explicit construction of optimal LoRA adapters, are detailed in Sec. D.\nIn fact, this lemma delivers a crucial insight. When we consider L = 1 and R = D, the lemma becomes strikingly similar to the Eckart\u2013Young\u2013Mirsky theorem (Eckart & Young, 1936; Mirsky, 1960). However, there is a significant difference from the classical theorem on the optimal low-rank approximation, which involves a single target matrix and a single matrix as an optimization variable. Our lemma demonstrates that a comparable result can be achieved for a \u201cproduct of matrices,\u201d where each matrix is optimized subject to a low-rank constraint. That being said, even though each matrix is constrained by a low rank, the \u201ceffective rank\u201d is the sum of these low ranks, i.e., in this scenario, is LR. Consequently, once the low-rank adapters are optimally configured, one can make the product equal to the best rank LR-approximation of the target matrix. This can be viewed as an extension of the matrix approximation theorem to a product of matrices, each subject to low-rank constraints. Our main theoretical results on the expressive power of LoRA, which we will present in the subsequent sections, will build upon this core matrix approximation result."
        },
        {
            "heading": "3 EXPRESSIVE POWER OF FNNS WITH LORA",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM SETTING",
            "text": "We use FNNL,D(\u00b7; (Wl)Ll=1, (bl)Ll=1) to denote a L-layer width-D fully connected ReLU neural network with weight matrices Wl \u2208 RD\u00d7D and biases bl \u2208 RD, where l \u2208 [L]. The target FNN f and frozen FNN f0 can be represented as follows: Target FNN f := FNNL,D(\u00b7; (W l)Ll=1, (bl)Ll=1), Frozen FNN f0 := FNNL,D(\u00b7; (Wl)Ll=1, (bl)Ll=1), where W l \u2208 RD\u00d7D and bl \u2208 RD represent the weight matrix and bias vector for the l-th layer of the target model f , respectively. Likewise, Wl \u2208 RD\u00d7D, bl \u2208 RD are those for f0, for layer l \u2208 [L]. Given a specified LoRA-rank R \u2208 [D], we adapt the frozen FNN f0 into a new model f via LoRA. The adapted model f is defined as\nAdapted FNN f := FNNL,D(\u00b7; (Wl +\u2206W l)Ll=1, (b\u0302l)Ll=1), where the weight matrix for the low-rank adapter \u2206W l \u2208 RD\u00d7D satisfies specified rank constraints, updated bias vector b\u0302l \u2208 RD for l \u2208 [L]1. As noted in Sec. 2, it is common for the pretrained model to be larger than necessary. Therefore, we focus on a setting where the frozen model is deeper than the target model, i.e., L \u2265 L. Furthermore, in this section, we let the input space X \u2208 RD\u00d7D be bounded.\n1We consider the case where the bias parameters can also be updated, as suggested by Hu et al. (2022a). Experiments investigating the impact of updating bias parameters are presented in Sec. G.4."
        },
        {
            "heading": "3.2 ONE-LAYER RELU FNN APPROXIMATION",
            "text": "We start with investigating the expressive power of LoRA on one-layer FNN. In this setting, our aim is to identify LoRA adapters (\u2206W l)Ll=1 and bias vectors (b\u0302l) L l=1 such that the adapted model\nReLU((WL +\u2206WL) \u00b7 ReLU((WL\u22121 +\u2206WL\u22121) \u00b7 ReLU(\u00b7 \u00b7 \u00b7 ) + b\u0302L\u22121) + b\u0302L) closely approximates the target one-layer ReLU FNN model ReLU(W 1 \u00b7+b1). This differs from the setting described in Sec. 2, where a multi-layer FNN with linear activation functions and zero biases was used to approximate a one-layer FNN with the same properties. In the current setting, we introduce non-linearity through the use of ReLU activation functions in the frozen model and also take biases into account. Consequently, to generalize the findings to this new setting, addressing the introduced non-linearity due to the ReLU activation functions in the frozen model is the main challenge.\nWe employ the following two steps to extend the results in Sec. 2 to the current setting.\n1. (Linearization) We eliminate the nonlinearity in the first L\u2212 1 layers of the adapted model, making it equivalent to a one-layer ReLU FNN. This can be readily achieved by choosing sufficiently large bias vectors for the first L\u2212 1 layers to ensure that all ReLUs in these layers are activated. This technique of eliminating non-linearity is inspired by Giannou et al. (2023a).\n2. (Weight Matrix Alignment) We update the bias vectors of the last layer b\u0302L to align with that of the target model f , and apply the linear model approximation results (i.e., Lemma 1) to identify the low-rank adapters that match the weight matrix f .\nFollowing the steps above, we arrive at the subsequent lemma, which demonstrates that any onelayer FNN can be closely approximated by a multi-layer FNN finetuned via LoRA. The complete proof is provided in Sec. E.1.\nLemma 2. Define error matrix E := W 1\u2212 \u220fL\nl=1 Wl, with its rank represented by RE = rank(E). Consider a LoRA-rank R \u2208 [D]. Assume that the weight matrices W1, . . . ,WL \u2208 RD\u00d7D and\u220fL\nl=1 Wl + LRr(E) for all r \u2264 R(L \u2212 1) are non-singular. Let x be a random input sampled from a distribution with bounded support X and let \u03a3 = Exx\u22a4. Then, there exists rank-R or lower matrices \u2206W 1, . . . ,\u2206WL \u2208 RD\u00d7D and bias vectors b\u03021, . . . , b\u0302L \u2208 RD such that the expected squared error can be bounded as\nE \u2225\u2225f(x)\u2212 f(x)\u2225\u22252\n2 \u2264 \u2225\u03a3\u2225F \u03c32RL+1(E).\nMoreover, when R \u2265 \u2308REL \u2309, we have f(x) = f(x) for all x \u2208 X ."
        },
        {
            "heading": "3.3 MULTI-LAYER RELU FNN APPROXIMATION",
            "text": "We now generalize our discussion to the approximation of multi-layer ReLU FNNs. The key strategy for extending the results to approximating multi-layer ReLU FNNs under LoRA is model partition, inspired from Giannou et al. (2023a). To elucidate this, we start with a specific example. Example 1. Consider the case where L = 2 and L = 4. We view a two-layer target model f as a composition of two one-layer ReLU FNNs. Accordingly, we partition the four-layer adapted model f into two submodels, each consisting of two layers. For each layer in the target model, we utilize two corresponding layers in the frozen/adapted model for approximation. This problem then simplifies into a one-layer FNN approximation problem, which has already been addressed in Lemma 2.\nBased on this example, we introduce a ordered partition P = {P1, . . . , PL} to partition the layers in the adapted model f , where \u22c3L i=1 Pi = [L]. Each element Pi \u2208 P consists of consecutive integers. Given a partition P , each element Pi specifies that the layers with index l \u2208 Pi in the adapted model will be used to approximate the i-th layer in the target model. Example 1, which uses every two layers in the adapted model to approximate each layer in the target model, can be considered as a partition represented as {{1, 2} , {3, 4}}. Similarly, we extend this simple uniform partition into general cases for L-layer target FNN and L-layer frozen FNN: Pu = { P u1 , . . . , P u L } := { {1, . . . ,M} , {M + 1, . . . , 2M} , . . . , { (L\u2212 1)M + 1, . . . , L }} ,\nwhere M := \u230aL/L\u230b. The uniform partition indicates that every M layers in the adapted model are employed to approximate each layer in the target model. We use \u220f l\u2208Pi Wl to denote the product of the weight matrices from the layers l \u2208 Pi, with the later layer positioned to the left and the earlier layer to the right in the matrix product. For example, \u220f l\u2208P u1 Wl = \u220fM\nl=1 Wl = WM \u00b7 \u00b7 \u00b7W1. We first extend Lemma 2 to multi-layer FNN approximation setting using this uniform partition.\nUniform Model Partition. Given a specified LoRA-rank R \u2208 [D], to derive our results, we introduce a mild non-singularity assumption on the weight matrices of the target model and frozen model for the feasibility of our analysis. This assumption is mild, supported by Lemma 3 that even weight matrices initialized at random can meet this requirement. Assumption 1 (Non-Singularity). For a fixed LoRA-rank R \u2208 [D], the weight matrices of the frozen model (Wl)Ll=1 and matrices (\u220f l\u2208P ui Wl ) + LRr(W i \u2212 \u220f l\u2208P ui Wl) are non-singular for all r \u2264 R(M \u2212 1) and i \u2208 [L]. Lemma 3. Let (W l)Ll=1, (Wl)Ll=1 \u2208 RD\u00d7D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 1 holds \u2200R \u2208 [D].\nGiven this assumption, here we present our first main result, which shows that any frozen FNN can be adapted to exactly approximate the target FNN via LoRA. Theorem 3. Under Assumption 1, if LoRA-rank R \u2265 \u2308maxi\u2208[L] rank(W i\u2212 \u220f l\u2208P ui\nWl)/M\u2309, then there exists rank-R or lower matrices \u2206W 1, . . . ,\u2206WL \u2208 RD\u00d7D and bias vectors b\u03021, . . . , b\u0302L \u2208 RD such that the low-rank adapted model f can exactly approximate the target model f , i.e., f(x) = f(x), \u2200x \u2208 X .\nMoreover, combining Lemma 3 and Theorem 3 gives the following corollary.\nCorollary 4. Assume that the elements of (W l)Ll=1, (Wl)Ll=1 are independently drawn from arbitrary continuous distributions. When R \u2265 D/M , with probability 1, there exists rank-R or lower matrices \u2206W 1, . . . ,\u2206WL \u2208 RD\u00d7D and bias vectors b\u03021, . . . , b\u0302L \u2208 RD such that low-rank adapted model f can exactly approximate the target model f on X , i.e., f(x) = f(x), \u2200x \u2208 X .\nTo understand the implications of this corollary, let us consider L \u226b L. In this scenario, the required LoRA-rank is sufficiently small such that the dimension of the rank-R matrix is approximately 2RD. This corollary suggests that with 2RDL \u2265 2D2L/M \u2248 2D2L learnable parameters, even a random FNN can be adapted into the target model f . It is noteworthy that the total number of parameters of the target model is D2L. This indicates that even though the learnable parameters under LoRA finetuning appear to be highly constrained (low-rank constrained learnable parameters distributed across many layers), the effective expressive power of LoRA is nearly optimal up to a constant factor of 2. Our discovery provides the first theoretical insights into the practical success of LoRA. Furthermore, Theorem 3 indicates that if the model f is \u2018close\u2019 to f such that maxi\u2208[L] rank(W i \u2212\u220f\nl\u2208P ui Wl) is small, the number of learnable parameters used by LoRA can be lower than D2L.\nMeanwhile, when the employed LoRA-rank is lower than the critical threshold, the following theorem provides an upper bound for the approximation error. Theorem 5. Define the approximation error of i-th layer as Ei = \u03c3RM+1(W i \u2212\u220f\nl\u2208P ui Wl), and the magnitude of the parameters and the input as \u03b2 :=\nmaxi\u2208[L] (\u221a \u2225\u03a3\u2225F \u220fi j=1 \u2225\u2225W j\u2225\u2225F +\u2211ij=1\u220fi\u22121k=j+1 \u2225\u2225W k\u2225\u2225F \u2225\u2225bj\u2225\u22252)\u2228\u221a\u2225\u03a3\u2225F. Under Assumption 1, there exists rank-R or lower matrices (\u2206W l)Ll=1 with \u2206W l \u2208 RD\u00d7D and bias vectors (b\u0302l)Ll=1 with b\u0302l \u2208 RD such that for input x \u2208 X with Exx\u22a4 = \u03a3,\nE \u2225\u2225f(x)\u2212 f(x)\u2225\u2225\n2 \u2264 \u03b2 L\u2211 i=1 max k\u2208[L] (\u2225\u2225W k\u2225\u2225F + Ek)L\u2212i Ei. Theorem 5 provides an upper bound on the approximation error for the adapted model. This bound is influenced by several factors: (i) magnitude of the target model\u2019s parameters and the input, which\nis captured by \u03b2 and \u2225\u2225W k\u2225\u2225F, (ii) the rank of the adapter R and the discrepancy between the frozen\nmodel and the target model (W i \u2212 \u220f\nl\u2208P ui Wl)\nL i=1, both of which contribute to the term Ei, (iii) the\ndepth of the frozen model L, reflected in M and consequenly Ei.\nAll the proofs of the results derived for uniform partition are provided in Sec. E.2.\nGeneral Model Partition. We note that employing this uniform partition strategy for approximating the target model may not always yield optimal results. To illustrate this, we revisit the case considered by Example 1, where L = 2 and L = 4. Consider a scenario where the first layer of the frozen model has been pretrained to match the first layer of the target model. In this case, we can use just the first layer in f to approximate the first layer in f , and a zero LoRA-rank is sufficient for the exact representation of the first layer. The remaining three layers in f can then be used to approximate the second layer in f . Compared to uniform partition, this partition leverages more layers to approximate the second layer in f , allowing us to achieve the desired performance with a lower LoRA-rank, as per Lemma 2. This suggests that our approximation error bounds could be further optimized by considering partitioning schemes tailored to specific scenarios.\nWe now extend our results to a more general setting, where we do not assume a uniform partition. Concurrently, recent research by Zhang et al. (2023) has shown that the application of varying LoRA-ranks leads to improved results. Consequently, we permit each layer in the frozen model to utilize adapters with different LoRA-ranks. The rank of the LoRA adapter associated with the l-th layer in the frozen model is denoted by Rl, where l \u2208 [L]. This result relies on Assumption 2, an analog of Assumption 1, but revised to include a general model partition. More details, including the proofs, are provided in Sec. E.3.\nTheorem 6. Consider a partition P for the frozen model. Let Assumption 2 hold. If \u2211l\u2208Pi Rl \u2265 rank(W i \u2212 \u220f l\u2208Pi Wl) for all i \u2208 [L], there exists LoRA adapters (\u2206W l)Ll=1 with rank(\u2206W l) \u2264 Rl and biases (b\u0302l)Ll=1 such that the adapted model f can exactly approximate the target model. Moreover, define the approximation error of the i-th layer as Ei = \u03c3\u2211 l\u2208Pi\nRl+1(W i \u2212\u220f l\u2208Pi Wl), and the magnitude of the parameters and the input as \u03b2 :=\nmaxi\u2208[L] (\u221a \u2225\u03a3\u2225F \u220fi j=1 \u2225\u2225W j\u2225\u2225F +\u2211ij=1\u220fi\u22121k=j+1 \u2225\u2225W k\u2225\u2225F \u2225\u2225bj\u2225\u22252)\u2228\u221a\u2225\u03a3\u2225F. Then, there exists LoRA adapters (\u2206W l)Ll=1 with rank(\u2206W l) \u2264 Rl and biases (b\u0302l)Ll=1 such that for any input x \u2208 X with Exx\u22a4 = \u03a3, the approximation error can be bounded as\nE \u2225\u2225f(x)\u2212 f(x)\u2225\u2225\n2 \u2264 \u03b2 L\u2211 i=1 max k\u2208[L] (\u2225\u2225W k\u2225\u2225F + Ek)L\u2212i Ei. Comparison to Tuning Final Layers. Updating the final layers and keeping the initial layers frozen (Chatfield et al., 2014; Donahue et al., 2014; Sharif Razavian et al., 2014; Rahimi & Recht, 2007) is another popular model adaptation method. However, unlike LoRA, which can adapt even randomly generated networks to match a target model, empirical studies (Kornblith et al., 2019) suggest that the effectiveness of final layers tuning heavily depends on the quality of the initial layers. This indicates that merely tuning the final layers of randomly generated networks may not yield desirable performance. The following lemma rigorously supports this assertion, demonstrating that regardless of how the final layers are tuned, it is impossible to adapt a randomly generated model into even a one-layer FNN, a model of very low complexity.\nLemma 4. Let D \u2265 2 and f be a one-layer target FNN. Assume that the elements of weight matrices (Wl) L l=1 are independently drawn from arbitrary continuous distributions. With probability 1, for any tuning of the last L\u2212 1 layers, f \u0338= f .\nIn Corollary 4, we demonstrate that LoRA can adapt any randomly generated models to match the target model, using at most twice the number of learnable parameters as the target model. However, this lemma reveals that final layers tuning, even with L \u2212 1 times the learnable parameters of the target model, cannot achieve performance comparable to LoRA. In other words, LoRA requires at most 2RDL \u2264 2D2 learnable parameters to achieve an exact approximation, while final layers\ntuning fails to approximate the target model even with (L\u2212 1)D2 learnable parameters. Therefore, when L \u2265 3, LoRA can deliver strictly superior performance than final layers tuning with the same or fewer parameters. This provides insights into the empirical observation that LoRA outperforms final layers tuning (Kaplun et al., 2023; Ding et al., 2023)."
        },
        {
            "heading": "4 EXPRESSIVE POWER OF TRANSFORMER NETWORKS WITH LORA",
            "text": ""
        },
        {
            "heading": "4.1 PROBLEM SETTING",
            "text": "Transformer network, denoted as TFNL,D, is a composition of L Transformer blocks and an output layer, parameterized by weight Wo \u2208 RD\u00d7D. Each transformer block comprises a H-head selfattention layer, parameterized by weight ((W hOl,W h V l,W h Kl,W h Ql) H h=1) L l=1, followed by a tokenwise feedforward layer, parameterized by weight (W1l,W2l)Ll=1 and bias (b1l, b2l) L l=1. We assume that all weight matrices have a dimension of D \u00d7D, while the bias vectors are of dimension D. We employ the same formulations of transformer blocks as Yun et al. (2020a), with one exception: we exclude skip connections for analytical feasibility. As before, we use \u00b7 (e.g., W 1l) to represent the corresponding parameters for the target model, and \u2206\u00b7 (e.g., \u2206W hOl) to represent the corresponding low-rank update. For TFN cases,we consider scenarios where both the frozen model and the target model have L Transformer blocks. For an explicit formulation, please refer to Sec. F.2."
        },
        {
            "heading": "4.2 MAIN RESULTS ON TRANSFORMER NETWORKS",
            "text": "We now present our main findings on TFNs. The first result relies on a non-singularity assumption (Assumption 4) tailored for TFN. This assumption is mild, and models with randomly generated weights can satisfy its criteria (Lemma 14). Further details are deferred to Sec. F.2.\nThe following theorem shows that adding LoRA adapters primarily to the self-attention layers enables the adapted model f to exactly approximate the target model f . This finding is consistent with a recent observation made by Hu et al. (2022a), which indicates that a good performance can be achieved by adapting only the attention layers when applying LoRA to TFNs.\nTheorem 7. Consider a given LoRA-rank R \u2208 [D]. Let Assumption 4 hold. Let Gi be the rankbased functionality gap to i-th transformer block (i \u2208 [L]) or output layer (i = L + 1) defined in (23). If R \u2265 maxi\u2208[L+1]\u2308Gi2 \u2309, then there exists low-rank adapters with rank lower than R \u2208 [D] ((\u2206W hKl,\u2206W h Ql,\u2206W h V l,\u2206W h Ol) H h=1) L l=1,\u2206W 2L,\u2206W o with other low-rank adapters set to O, and updated bias vectors (b\u03021l, b\u03022l)Ll=1, such that for any X \u2208 RD\u00d7N , the adapted model f exactly approximates target model f , i.e., f(X) = f(X).\nProof Sketch. The primary challenge for extending our analysis to TFNs, similar to FNN cases, is the nonlinearity introduced by softmax and ReLU. To manage this, we segment a sequence of transformer blocks based on the softmax and ReLU functions. Specifically, we align the output of attention scores before the softmax is applied, and then match the output of the first feedforward layer before ReLU is applied.\nThe complete proof of Theorem 7 and results for randomly generated models can be found in Sec. F.2. Meanwhile, our results here are specifically for TFNs with multi-head attention layers. For TFNs with single-head attention layers, the construction of LoRA adapters differs due to the absence of W hOi. Since the results are similar, we defer the problem setting and results for TFNs with single-head attention layers to Sec. F.1."
        },
        {
            "heading": "5 NUMERICAL EXPERIMENTS",
            "text": "Recall that all our theoretical statements are based on our construction of the LoRA adapters presented in their corresponding proofs. To validate these results, here we empirically examine the relationship between approximation error and rank by integrating the LoRA adapters, which are constructed with the uniform partition in our proof, into the frozen model. Furthermore, we evaluate\nthe effectiveness of our constructed LoRA adapters by comparing their performance against adapters updated through gradient descent and optimized by Adam. All simulations are conducted five times using different seeds, and the reported values represent the median computed across different runs.\nSetup. We implement LoRA adapter \u2206W by reparameterizing it as \u2206W = AB\u22a4, where A,B \u2208 RD\u00d7R, and we use the same initialization scheme as proposed by Hu et al. (2022a). We employ the Mean Squared Error (MSE) to measure the approximation error, denoted by E\u0302x \u2225\u2225f(x)\u2212 f(x)\u2225\u22252\n2 , where the input x is independently\ngenerated from a Gaussian distribution N (0, I), and we generate 5000 samples.\nModel. We consider linear models and FNNs with model dimension D = 16. For linear model cases, we set L = 1, L = 2, while for FNN cases, we set L = 2, L = 4. We employ two methods to create synthetic models. (Random) The first method involves randomly generating all the weight matrices using the Xavier uniform distribution, which is the default weight initial-\nization method used in PyTorch. (Pretrained) The second method aims to simulate scenarios where the pretrained model is relatively closer to the target model. We achieve this by initially creating the target model and the frozen model in the same way as the first method and then performing full-rank updates on the frozen model via gradient descent to approximate the target model until the approximation error is reduced by 1/3.\nResults. Our results for linear model approximation and FNN approximation via LoRA are depicted in Fig.1a and 1b, respectively. Firstly, we observe that the MSE of both two cases is close to zero when R \u2265 D\nL/L = 8, which corroborates our claims. Meanwhile, a comparison between the left\nand right columns of Fig.1a suggests that pretraining can further reduce the required rank to achieve near-zero approximation error. Furthermore, the curves of our construction align well with those of the gradient update method in linear model approximation cases, confirming the optimality claimed in Lemma 1. However, for FNN approximation cases, the gradient update method outperforms our construction in the small rank region. We conjecture that the suboptimality of our construction in this multi-layer FNN approximation case may be due to inherent limitations in our construction of LoRA adapters, which is to match the intermediate output of the frozen model with that of the target model. Additionally, the uniform partition could also be one contributing factor.\nFurther experiment details and a series of additional experiments, including simulations on FNNs and TFNs at different depths, evaluation on classification tasks, empirical comparison between LoRA and the final layers tuning, investigation of the importance of updatable bias, LoRA\u2019s generalization and optimization properties, and experiments on GLUE benchmark (Wang et al., 2018), are provided in Sec. G."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "To the best of our knowledge, this paper is the first to offer a theoretical understanding of the expressive power of LoRA fine-tuning on both FNN and TFN. Our work delivers insightful results, elucidating the impact of rank, depth of the pre-trained model, and the distance between the pretrained model and the target model on the expressive power of LoRA. Despite these advancements, several intriguing questions still remain open. First, as observed in the numerical experiments, our construction of LoRA adapters for FNN and TFN may not be always optimal. Given that more complex models offer increased flexibility, an open question is whether we can devise a more parameterefficient scheme to construct the LoRA adapters, thereby deriving a tighter bound on approximation error. Second, for TFN, we have only identified the conditions under which the LoRA-adapted model exactly matches the target model, due to the analytical complexity of TFN. It would be interesting to quantify the approximation error when the rank is lower than required."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "The code for all experiments reported in this paper is publicly accessible. For the purpose of reproducibility, the code can be found at the following anonymized GitHub repository: https: //anonymous.4open.science/r/LoRA_theory-60F9."
        },
        {
            "heading": "Appendix",
            "text": "This appendix encompasses more discussions, experiments, and proofs of the results presented in the main body. Given the extensive use of notations in our paper, we begin by presenting a list of common notations in Sec. A for the reader\u2019s convenience. We then delve into a more detailed discussion of related works in Sec. B. Following this, we present the proofs of results from the main body and auxiliary results in Sec. C, D, E, and F. Specifically, we provide additional results for TFN with single-head attention layers, and TFN with multi-head attention layers under random model cases in Sec. F. Further experimental details and interesting experiment findings are provided in Sec. G. In Sec. H, we summarize our theoretical findings and elaborate on how they align with the empirical observations. Finally, we discuss how to extend our results to cases with varying model dimensions in Sec. I, while this work primarily focuses on instances where both the target model and the frozen model possess the same model width D. More potential future works are outlined in Sec. J."
        },
        {
            "heading": "A List of Common Notations 17",
            "text": ""
        },
        {
            "heading": "B Expanded Related Works 18",
            "text": ""
        },
        {
            "heading": "C Proofs Related to Linear Algebra 19",
            "text": "C.1 Common Matrix Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.2 Non-Singularity of Randomly Generated Matrices . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "D Proofs for Linear Model Approximation 20",
            "text": ""
        },
        {
            "heading": "E Proofs for FNN Approximation 25",
            "text": "E.1 Approximating One-Layer ReLU FNN via LoRA . . . . . . . . . . . . . . . . . . 25\nE.2 Approximating Multi-Layer ReLU FNN via LoRA with Uniform Model Parition . 27\nE.3 Approximating Multi-Layer ReLU FNN via LoRA with General Model Parition . . 31\nE.4 Approximating Multi-Layer ReLU FNN via Final Layers Tuning . . . . . . . . . . 32"
        },
        {
            "heading": "F Proofs for TFN Approximation 33",
            "text": "F.1 Approximating Transformer Network with Single-Head Attention Layers . . . . . 33\nF.2 Approximating Transformer Network with Multi-Head Attention Layers . . . . . . 36"
        },
        {
            "heading": "G Extended Numerical Experiments 39",
            "text": "G.1 Additional Details on Gradient Update Method . . . . . . . . . . . . . . . . . . . 40\nG.2 Validation of Our LoRA Adapter Construction . . . . . . . . . . . . . . . . . . . . 40\nG.2.1 FNN Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nG.2.2 TFN Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nG.3 Comparison to Tuning Final Layers . . . . . . . . . . . . . . . . . . . . . . . . . 41\nG.4 Benefits of Tuning Biases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\nG.5 Training Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\nG.6 Generalization Performances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\nG.7 Evaluation on Classification Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nG.8 Evaluation on Real Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44"
        },
        {
            "heading": "H Bridging Theoretical Insights and Empirical Observations 45",
            "text": ""
        },
        {
            "heading": "I Extension to Cases with Different Model Dimensions 46",
            "text": "J Other Future Works 46"
        },
        {
            "heading": "A LIST OF COMMON NOTATIONS",
            "text": "We first give a list of common notations that are used in the main body and appendix for reference.\n\u2022 f : LoRA-adapted model.\n\u2022 f : target model.\n\u2022 f0: frozen/pretrained model.\n\u2022 R: rank of LoRA adapters.\n\u2022 D: dimensionality of the model, representing the number of neurons in each layer for FNNs and the embedding size for TFNs.\n\u2022 L: depth of the (frozen) model, representing the number of layers for FNNs and the number of transformer blocks for TFNs.\n\u2022 N : sequence length of the input for TFNs.\n\u2022 x: input.\n\u2022 x: random input.\n\u2022 X: matrix input.\n\u2022 X : input space. \u2022 \u03a3: Exx\u22a4. \u2022 W : a weight matrix associated with (frozen) model. Subscripts and superscripts may be added\nfor specificity.\n\u2022 b: a bias vector associated with the (frozen) model. Subscripts may be added for specificity.\n\u2022 zl: the output of the first l layers in the (frozen) FNN.\n\u2022 Zl: the output of the first l transformer blocks in a (frozen) TFN.\n\u2022 W : a weight matrix associated with the target model. Subscripts and superscripts may be added for specificity.\n\u2022 b: a bias vector associated with the target model. Subscripts may be added for specificity.\n\u2022 zl: the intermediate output of the first l layers in target FNN given the random input x.\n\u2022 Zl: the output of the first l transformer blocks in a target TFN.\n\u2022 L: depth of the target model, representing the number of layers for FNNs and the number of transformer blocks for TFNs.\n\u2022 \u2206W : the weight matrix of a LoRA adapter.\n\u2022 b\u0302: a bias vector associated with the LoRA-adapted model.\n\u2022 z\u0302l: the output of the first l layers in the LoRA-adapted model given the random input x.\n\u2022 Z\u0302l: the output of the first l transformer blocks in the LoRA-adapted model.\n\u2022 M : the ratio of the depth of the frozen model to that of the target model, i.e., L/L.\n\u2022 P: partition P = {P1, . . . , PL}, each element Pi specifies that the layers with index l \u2208 Pi in the adapted model will be used to approximate the i-th layer in the target model.\n\u2022 Pi: the i-th element in partition P . \u2022 Pu: uniform partition Pu := {{1, . . . ,M} , {M + 1, . . . , 2M} , . . . , { (L\u2212 1)M + 1, . . . , L } }.\nThe uniform partition indicates that every M layers in the adapted model are employed to approximate each layer in the target model.\n\u2022 P ui : the i-th element in uniform partition Pu. \u2022 ID: the D\u00d7D identity matrix. When the context permits, the subscript D of ID may be omitted,\nsimplifying the notation to I .\n\u2022 Ia:b,D: a diagonal matrix where the diagonal entries from the ath to bth position are set to 1, while all remaining entries are 0s.\n\u2022 \u03c3d(\u00b7): the d-th largest singular value for the given square matrix. When d is greater than the width of the matrix, \u03c3d(\u00b7) = 0.\n\u2022 LRr(\u00b7): best rank-r approximation of a square matrix in Frobenuis norm and spectral norm. The subscript r may be omitted to indicate a general low-rank approximation without specifying the rank.\n\u2022 \u220f\nl\u2208Pi Wl: product of the weight matrices from the layers l \u2208 Pi, with the later layer positioned to the left and the earlier layer to the right in the matrix product. For example, \u220f l\u2208P u1\nWl =\u220fM l=1 Wl = WM \u00b7 \u00b7 \u00b7W1."
        },
        {
            "heading": "B EXPANDED RELATED WORKS",
            "text": "Expressive Power of Fully Connected Neural Networks The theoretical exploration of the expressive power of unfrozen fully connected neural networks has advanced since the introduction of the first universal approximation theorem (Hornik et al., 1989; Cybenko, 1989). Subsequent studies have demonstrated the benefits of depth, asserting that sufficient depth can ensure function approximation (Bengio & Delalleau, 2011; Eldan & Shamir, 2016; Liang & Srikant, 2017; Telgarsky, 2016; 2015). There are also works that have examined the expressive power of FNN from a view of width (Lu et al., 2017; Park et al., 2021; Bietti & Bach, 2021) and the number of neurons (Shen & Zhang, 2020). While these results assume that weight matrices can be arbitrarily adjusted for optimal performance, Hsu et al. (2021) examined the expressive power of randomly generated twolayer FNNs. Our work shares similarities with this direction, as we also delve into scenarios with randomly generated models. Beyond characterizing expressive power by approximation error, alternative metrics have been proposed. Metrics such as Vapnik-Chervonenkis (Vapnik & Chervonenkis, 2015; Seo et al., 2021) and Rademacher complexities (Bartlett & Mendelson, 2001) are utilized to assess classification capacity. Furthermore, Raghu et al. (2017) introduced a novel metric that captures the structural properties of an FNN, and Lee et al. (2017) investigated the ability of FNNs to express distributions.\nExpressive Power of Transformers As TFNs have grown increasingly popular, a few studies have been conducted to investigate their expressive power. Yun et al. (2020a) established the universal approximation theorem for TFNs in approximating sequence-to-sequence functions. Likhosherstov et al. (2021) characterized the self-attention layer as a matrix and demonstrated that this matrix can approximate any sparse matrices. Beyond approximation, further research has delved into other facets of TFNs\u2019 expressive power. For instance, Giannou et al. (2023b) found that looped transformers can emulate an instruction-set computer, while Pe\u0301rez et al. (2019) demonstrated that TFNs attain Turing completeness when operating with infinite precision.\nHowever, all these theories above cannot fully explain the performance of frozen neural networks as they generally cannot factor in pre-trained model parameters and adaptation methods.\nExpressive Power of Adaptation Methods Our work focuses on investigating the expressive power of adaptation methods. In stark contrast to the flourishing research on the expressive power of neural networks, there exists a limited number of works investigating the expressive power of adaptation methods. A notable exception is Giannou et al. (2023a), investigating the expressive power of normalization parameter fine-tuning. They demonstrate that fine-tuning the normalization layers alone can adapt a randomly initialized ReLU network to match any target network that is O(width) times smaller. We borrow some proof techniques from this work, including techniques for extending results from linear neural networks to ReLU neural networks. In another recent work (Englert & Lazic, 2022), the authors show that neural reprogramming (Elsayed et al., 2019; Engel et al., 2018; Lee et al., 2020; Dinh et al., 2022a; Chen, 2022), a technique that modifies only the inputs while keeping the pretrained network frozen, can adapt any random two-layer ReLU network to achieve arbitrarily high accuracy on a Bernoulli data model over hypercube vertices. Oymak et al. (2023) explores prompt-tuning within a one-layer attention architecture, revealing that the model resulting from prompt tuning (Lester et al., 2021) is more expressive than the naive self-attention model. Despite these early attempts, no existing study has yet explored the expressive power of LoRA, the current leading adaptation method.\nOther Theoretical Analysis of Adaptation Methods Lots of efforts have been taken to theoretically analyze other properties of adaptation methods such as generalization. Maurer et al. (2016) provides the generalization bounds for transfer learning, particularly for final layers tuning, demonstrating that the estimation error reduces as the pretrained task diversity and the number of samples for the target task increase. Tripuraneni et al. (2020) further refines this bound by studying the effect of the number of samples in the pre-trained tasks. Interestingly, the estimation error of the final layers tuning provided in Tripuraneni et al. (2020) heavily depends on the quality of the shared representation. This insight aligns with our finding on final layers tuning (Lemma 4), which implies that tuning the final layers fails to adapt an L-layer randomly generated FNN to approximate any one-layer target FNN if the first layer remains frozen. This failure is attributed to the poor quality of the shared random representation. Du et al. (2021) further investigates final layers tuning in few-shot cases, i.e., when there are only a few samples for the target task. A recent study by Malladi et al. (2023), which examined LoRA and full fine-tuning through the lens of the Neural Tangent Kernel (Jacot et al., 2018), suggested that if the kernel view describes full fine-tuning, then LoRA approximates full fine-tuning in the lazy regime. In contrast, our study is conducted in a more generalized setting.\nWith the rapid advancement of large language models, new adaptation methods such as in-context learning (Brown et al., 2020), prefix tuning (Li & Liang, 2021), and prompt-tuning (Lester et al., 2021) are gaining increasing attention. A particular focus of research is the exploration of the theoretical underpinnings of in-context learning (Akyu\u0308rek et al., 2023; Bai et al., 2023; Wies et al., 2023; Xie et al., 2022; von Oswald et al., 2022; Ahn et al., 2023). Akyu\u0308rek et al. (2023) demonstrates that transformer-based in-context learners implicitly implement standard learning algorithms, while Bai et al. (2023) presents a similar finding and posits that in-context learning performs algorithm selection like a statistician. Wies et al. (2023) delves into the analysis of the sample complexity of in-context learning. Other works find that in-context learning is equivalent to gradient descent (von Oswald et al., 2022; Ahn et al., 2023), and Bayesian inference (Xie et al., 2022)."
        },
        {
            "heading": "C PROOFS RELATED TO LINEAR ALGEBRA",
            "text": "In this section, we present a collection of commonly used matrix inequalities and the basic properties of randomly generated matrices."
        },
        {
            "heading": "C.1 COMMON MATRIX INEQUALITIES",
            "text": "Here, we present some commonly used basic properties for matrix multiplication including rank computation, norm inequalities, as well as key results involving the trace and Frobenius norm of matrices for reference:\nrank(AB) \u2264 rank(A) \u2227 rank(B); \u2225Ax\u22252 \u2264 \u2225A\u22252 \u2225x\u22252 ; (1) Ex\u22a4Ax = tr(ACov(x)) + (Ex)\u22a4A(Ex) = tr(AExx\u22a4); tr(AB) = tr(BA);\ntr(AB) \u2264 tr(A)tr(B); \u2225A\u2225F = \u221a tr(AA\u22a4);\n\u2225A\u2225F = tr(A) for symmetric A; \u2225A\u2225F = \u221a\u2211\ni\n\u03c32i (A)."
        },
        {
            "heading": "C.2 NON-SINGULARITY OF RANDOMLY GENERATED MATRICES",
            "text": "Although the non-singularity of randomly generated matrices is already established, we include a proof for completeness.\nTo facilitate the proof, we introduce a lemma which states that if a polynomial is non-zero, then the set of roots corresponding to a zero value of the polynomial has a Lebesgue measure of zero.\nLemma 5 (Caron & Traynor (2005)). Let p(x) be a polynomial of degree d, x \u2208 Rn. If p is not the zero polynomial, then the set S := {x \u2208 Rn | p(x) = 0} is of Lebesgue measure zero.\nWe note that the determinant of a matrix can be viewed as a polynomial function of its vectorized version. Based on this insight, we proceed with our proof. Lemma 6. Let X \u2208 RD\u00d7D be a random matrix that follows arbitrary continuous distribution with support having non-zero Lebesgue measure on RD\u00d7D. Then, X is non-singular with probability 1.\nProof of Lemma 6. The result is a direct consequence of Lemma 5. Let x = vec(X). Then, x is a random vector following arbitrary continuous distribution with a support having non-zero Lebesgue measure on RD\u00d7D.\nFirst, we establish the relationship:\nP (det(X) = 0) = P (p(x) = 0)\nfor some polynomial function p. We denote the support of random vector x by X \u2282 RD2 , and the probability density function (PDF) of x by q. Then,\nP (p(x) = 0) = \u222b X 1 {p(x) = 0} q(x)dx = \u222b X\u2229{x:p(x)=0} q(x)dx.\nBy Lemma 5, the Lebesgue measure of {x : p(x) = 0} is zero. Hence,\u222b X\u2229{x:p(x)=0} q(x)dx = 0.\nBy combining all the equations above, we conclude that P(det(X) = 0) = 0, which implies X is non-singular with probability 1."
        },
        {
            "heading": "D PROOFS FOR LINEAR MODEL APPROXIMATION",
            "text": "In this section, we present the results and corresponding proofs for the linear model approximation problem introduced in Sec. 2. The deep linear model is a common technique in theoretical deep learning research, which offers valuable insights into deep nonlinear models, and has been employed in many notable studies, including those by Saxe et al. (2014); Kawaguchi (2016); Lu & Kawaguchi (2017); Hardt & Ma (2017) and Laurent & von Brecht (2018). We employ this toy model as a preliminary model, which serves as a foundation for extending our results to nonlinear models (i.e., FNN and TFN).\nWe first provide a slightly more detailed version of Lemma 1 along with its proof. Then, we present a variant of it that allows for different LoRA-ranks for each low-rank adapter. The proof for this variant involves only a minor modification of the proof for Lemma 7.\nLemma 7. [Detailed version of Lemma 1] Define error matrix E := W \u2212\u220fLl=1 Wl, and denote its rank by RE = rank(E). For a given LoRA-rank R \u2208 [D], assume that all the weight matrices of the frozen model (Wl)Ll=1, and \u220fL l=1 Wl + LRr(E) are non-singular for all r \u2264 R(L\u2212 1). Then, the approximation error\nmin \u2206W l:rank(\u2206W l)\u2264R \u2225\u2225\u2225\u2225\u2225 L\u220f\nl=1 (Wl +\u2206W l)\u2212W \u2225\u2225\u2225\u2225\u2225 2 = \u03c3RL+1 ( W \u2212 L\u220f l=1 Wl ) \ufe38 \ufe37\ufe37 \ufe38\nError matrix E\n,\nand the optimal solution to the matrix approximation problem satisfies \u220fL\nl=1(Wl + \u2206W l) =\u220fL l=1 Wl + LRRL\u2227RE (E). Therefore, when R \u2265 \u2308REL \u2309, we have \u220fL l=1(Wl + \u2206W l) = W , implying f \u2261 f .\nProof of Lemma 7. Our goal is to find matrices \u2206W 1, . . . ,\u2206WL of rank R or lower such that the product of the adapted matrices approximates the target matrix well, i.e., we aim to solve the\nfollowing constrained optimization problem:\nmin \u2206W l:rank(\u2206W l)\u2264R \u2225\u2225\u2225\u2225\u2225 L\u220f\nl=1 (Wl +\u2206W l)\u2212W \u2225\u2225\u2225\u2225\u2225 2 .\nBy subtracting \u220fL\nl=1 Wl from both terms, the constrain optimization problem becomes\nmin \u2206W l:rank(\u2206W l)\u2264R \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 ( L\u220f l=1 (Wl +\u2206W l)\u2212 L\u220f l=1 Wl ) \ufe38 \ufe37\ufe37 \ufe38\n:=A\n\u2212 ( W \u2212\nL\u220f l=1 Wl ) \ufe38 \ufe37\ufe37 \ufe38\n:=E\n\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 2 . (2)\nTo perform analysis on (2), we start with the analysis of A as follows:\nA = L\u220f l=1 (\u2206W l +Wl)\u2212 L\u220f l=1 Wl\n= \u2206WL L\u22121\u220f l=1 (\u2206W l +Wl) +WL L\u22121\u220f l=1 (\u2206W l +Wl)\u2212 L\u220f l=1 Wl.\nHere, we have separated the first term in the product \u220fL\nl=1(\u2206W l +Wl), breaking it into two parts: one involving \u2206WL and the other WL. We can further expand the part involving WL:\nA =\u2206WL L\u22121\u220f l=1 (\u2206W l +Wl)\n+WL ( \u2206WL\u22121\nL\u22122\u220f l=1 (\u2206W l +Wl) +WL\u22121 L\u22122\u220f l=1 (\u2206W l +Wl)\n) \u2212\nL\u220f l=1 Wl.\nAt this point, it becomes clear that this expression can be iteratively decomposed. Following this pattern, we can express A as:\nA =\u2206WL L\u22121\u220f l=1 (\u2206W l +Wl) +WL\u2206WL\u22121 L\u22122\u220f l=1 (\u2206W l +Wl) (3)\n+ . . .+ ( L\u220f l=2 Wl)(\u2206W 1 +W1)\u2212 L\u220f l=1 Wl\n= L\u2211 l=1\n[ (\nL\u220f i=l+1 Wi)\u2206W l( l\u22121\u220f i=1 (Wi +\u2206W i)) ] \ufe38 \ufe37\ufe37 \ufe38\n:=Al\n.\nIn this final form, A is decomposed as A = \u2211L\nl=1 Al. It is important to note that rank(Al) \u2264 rank(\u2206W l) \u2264 R. Consequently, rank(A) \u2264 \u2211L l=1 rank(Al) \u2264 RL.\nThen, the optimization problem (2) can be relaxed into a low-rank approximation problem\n(2) \u2265 min A:rank(A)\u2264RL \u2225A\u2212E\u22252 , (4)\nwhere the optimal solution is A = LRRL\u2227RE (E) := E \u2032. Therefore, if we can identify rank-R or lower matrices (\u2206W l)Ll=1 such that\nL\u220f l=1 (Wl +\u2206W l)\u2212 L\u220f l=1\nWl\ufe38 \ufe37\ufe37 \ufe38 :=A\n= LRRL\u2227RE (W \u2212 L\u220f\nl=1 Wl)\ufe38 \ufe37\ufe37 \ufe38 :=E\u2032 , (5)\nthen we effectively solve the matrix approximation problem as defined in (2). Moreover, it is straightforward to verify that (5) directly implies all statements in this lemma. Therefore, our remaining proof focuses on proving (5).\nDenote RE\u2032 = RL \u2227RE . To derive the explicit form of E\u2032, we first refer to the SVD of E as"
        },
        {
            "heading": "E = UDV \u22a4,",
            "text": "where U and V are orthonormal matrices and the first RE diagonal entries of D are non-zero, with all remaining entries being zero. Based on this, E\u2032 is expressed as\nE\u2032 = UDI1:RL,DV \u22a4.\nHaving already derived the decomposition A = \u2211L\nl=1 Al, we next aim to decompose E \u2032 as E\u2032 =\u2211L\nl=1 E \u2032Ql, where Q1, . . . ,QL \u2208 RD\u00d7D. The goal now shifts to identifying \u2206W l,Ql such that\nAl = E \u2032Ql for each l \u2208 [L]. Achieving this would complete the proof of (5).\nTherefore, our goal becomes finding \u2206W 1, . . . ,\u2206WL with rank(\u2206W l) \u2264 R for all l \u2208 [L] such that\nAl = ( L\u220f i=l+1 Wi)\u2206W l( l\u22121\u220f i=1 (Wi +\u2206W i)) = E \u2032Ql, for all l \u2208 [L]. (6)\nOne sufficient condition for achieving (6) is that the decomposed matrices Q1,QL and low-rank adapters \u2206W 1, . . . ,\u2206WL meet the following conditions:\nL\u2211 l=1 E\u2032Ql = E \u2032, (7)\n\u2206W l = ( L\u220f i=l+1 Wi) \u22121E\u2032Ql( l\u22121\u220f i=1 (Wi +\u2206W i)) \u22121, for all l \u2208 [L] (8)\nrank(\u2206W l) \u2264 R, for all l \u2208 [L], (9) rank(Wl +\u2206W l) = D, for all l \u2208 [L\u2212 1]. (10)\nHere (7) describes the decomposition of E\u2032, (8) provides one simple solution to (6) when (10) holds, and (9) is the rank constraint on the low-rank adapter. In particular, the (10) is used to ensure the invertibility of \u220fl i=1(Wi +\u2206W i) for l \u2208 [L\u2212 1]. This condition is not necessary for l = L as the inverse of WL +\u2206WL is not required for computing any low-rank adapters.\nWe will show that the matrices (Ql)Ll=1 defined by\nQl = V I(R(l\u22121)+1)\u2227RE\u2032 :Rl\u2227RE\u2032 ,DV \u22a4, for all l \u2208 [L], (11)\nand \u2206W l defined by (8) for all l \u2208 [L] satisfies the all four conditions (7), (8), (9), and (10). We note that the definition of (Ql)Ll=1 clearly satisfies condition (7). For the remaining conditions, namely (8), (9), (10), we proceed the proof by induction.\nWhen l = 1. We begin by examining the three conditions (8), (9) and (10) under the base case l = 1. We first determine Q1 and \u2206W 1 based on (11) and (8):\n\u2206W 1 = ( L\u220f i=2 Wi) \u22121E\u2032Q1, Q1 = I1:R,D. (12)\nBy the choice of \u2206W 1, we satisfy the condition (8). Moreover, it directly follows that rank(\u2206W 1) \u2264 rank(Q1) = R, thereby fulfilling the rank constraint in (9).\nTherefore, we just need to prove that W1 + \u2206W 1 is full-rank, as required by condition (10). To compute rank(W1 +\u2206W 1), we proceed as follows:\nrank(W1 +\u2206W 1)\n(12) = rank(W1 + ( L\u220f i=2 Wi) \u22121E\u2032Q1) (Substituting for \u2206W 1)\n= rank(( L\u220f i=1 Wi) +E \u2032Q1) (Left multiplying with invertible ( L\u220f i=2 Wi) \u22121)\n= rank(( L\u220f i=1 Wi) + LRR\u2227RE\u2032 (E)). (Simplifying)\nGiven the assumption that \u220fL\nl=1 Wl + LRr(E) is full rank for all r \u2264 R(L \u2212 1), rank(W1 + \u2206W 1) = rank(( \u220fL i=1 Wi) + LRR\u2227RE\u2032 (E)) = D, satisfying the last condition (10).\nWhen l > 1. Consider l = 2, . . . , L. We assume that for i \u2208 [l\u2212 1], we have determined matrices Qi and \u2206W i based on (11) and (8), respectively, and we assume that they satisfy the conditions (8), (9), and (10).\nFirst, under the induction assumption that Wi + \u2206W i is invertible for all i \u2208 [l \u2212 1], to achieve Al = E\n\u2032Ql, we set \u2206W l based on (8). This definition ensures rank(\u2206W l) \u2264 rank(Ql) = R, thereby satisfying the condition (9). To prove that Wl+\u2206W l is full-rank (condition (10)), we focus on computing rank(Wl +\u2206W l). We proceed as follows:\nrank(Wl +\u2206W l)\n(8) = rank(Wl + ( L\u220f i=l+1 Wi) \u22121E\u2032Ql( l\u22121\u220f i=1 (Wi +\u2206W i) \u22121)) (Substituting for \u2206W l)\n= rank(ID + ( L\u220f i=l Wi) \u22121E\u2032Ql( l\u22121\u220f i=1 (Wi +\u2206W i)) \u22121) (Left multiplying invertible W\u22121l )\n= rank ( l\u22121\u220f\ni=1\n(Wi +\u2206W i) + ( L\u220f i=l Wi) \u22121E\u2032Ql ) (Right multiplying invertible l\u22121\u220f i=1 (Wi +\u2206W i))\n= rank ( (Wl\u22121 +\u2206W l\u22121) l\u22122\u220f i=1 (Wi +\u2206W i) + ( L\u220f i=l Wi) \u22121E\u2032Ql ) (Rearranging terms)\n(8) = rank ( (Wl\u22121 + ( L\u220f i=l Wi) \u22121E\u2032Ql\u22121( l\u22122\u220f i=1 (Wi +\u2206W i)) \u22121) l\u22122\u220f i=1 (Wi +\u2206W i)\n+ ( L\u220f i=l Wi) \u22121E\u2032Ql ) (Substituting for \u2206W l\u22121)\n= rank ( ( L\u220f i=l\u22121 Wi +E \u2032Ql\u22121( l\u22122\u220f i=1 (Wi +\u2206W i)) \u22121) l\u22122\u220f i=1 (Wi +\u2206W i)\n+E\u2032Ql ) (Left multiplying L\u220f i=l Wi)\n= rank ( (\nL\u220f i=l\u22121 Wi l\u22122\u220f i=1 (Wi +\u2206W i) +E \u2032Ql\u22121 +E \u2032Ql\n) (Rearranging terms)\n= \u00b7 \u00b7 \u00b7\n= rank( L\u220f i=1 Wi +E \u2032( l\u2211 i=1 Qi)) (Taking similar steps)\n= rank( L\u220f i=1 Wi + LRRl\u2227RE\u2032 (E)). (Simplifying)\nBy the assumption that \u220fL\nl=1 Wl + LRr(E) is full-rank for r \u2264 R(L \u2212 1) and consequently, rank(Wl +\u2206W l) = rank( \u220fL i=1 Wi + LRRl\u2227RE\u2032 (E)) = D, satisfying the last condition (10).\nConclusion of Inductive Proof. Thus, by induction, we show that the definitions of (\u2206W l) L l=1 in (8) and (Ql) L l=1 in (11) ensure that Al = E\n\u2032Ql for all l \u2208 [L]. Summing over l from 1 to L satisfies condition (5), thereby completing the proof.\nThe following lemma extends the results to a more general setting where different LoRA-ranks can be employed across layers.\nLemma 8. Define error matrix E := W \u2212 \u220fLl=1 Wl, and denote its rank by RE = rank(E). For a sequence of LoRA-ranks for all layers (Rl)Ll=1, assume that all the weight matrices of the frozen model (Wl)Ll=1, and \u220fL l=1 Wl + LRr(E) are non-singular for all r \u2264 \u2211L\u22121 l=1 Rl. Then, the approximation error\nmin \u2206W l:rank(\u2206W l)\u2264Rl \u2225\u2225\u2225\u2225\u2225 L\u220f\nl=1 (Wl +\u2206W l)\u2212W \u2225\u2225\u2225\u2225\u2225 2 = \u03c3\u2211L l=1 Rl+1 ( W \u2212 L\u220f l=1 Wl ) \ufe38 \ufe37\ufe37 \ufe38\nError matrix E\n,\nand the optimal solution to the matrix approximation problem satisfies \u220fL\nl=1(Wl + \u2206W l) =\u220fL l=1 Wl+LR( \u2211L l=1 Rl)\u2227RE (E). Therefore, when \u2211L l=1 Rl \u2265 RE , we have \u220fL l=1(Wl+\u2206W l) = W , implying f \u2261 f .\nProof of Lemma 8. The proof follows the same steps of Lemma 7 with only minor modifications.\nIn the current setting, we target the following constrained optimization problem:\nmin \u2206W l:rank(\u2206W l)\u2264Rl \u2225\u2225\u2225\u2225\u2225 L\u220f\nl=1 (Wl +\u2206W l)\u2212W \u2225\u2225\u2225\u2225\u2225 2 ,\nwhere we allow each LoRA adapter \u2206W l can possess different LoRA-ranks Rl, i.e., rank(\u2206W l) \u2264 Rl, l \u2208 [L]. Subtracting \u220fL l=1 Wl from both terms leads us to a similar constrained optimization problem as (2). The only distinction lies in the rank constraint:\nmin \u2206W l:rank(\u2206W l)\u2264Rl \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 ( L\u220f l=1 (Wl +\u2206W l)\u2212 L\u220f l=1 Wl ) \ufe38 \ufe37\ufe37 \ufe38\n:=A\n\u2212 ( W \u2212\nL\u220f l=1 Wl ) \ufe38 \ufe37\ufe37 \ufe38\n:=E\n\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 2 . (13)\nFollowing the same steps, we decompose A into (3). Given that rank(Al) \u2264 rank(\u2206W l) \u2264 Rl, we deduce that rank(A) \u2264 \u2211Ll=1 rank(Al) \u2264 \u2211Ll=1 Rl. Consequently, the optimization problem above can be eased into a low-rank approximation problem analogous to (4):\n(13) \u2265 min A:rank(A)\u2264 \u2211L l=1 Rl \u2225A\u2212E\u22252 ,\nwhere the optimal solution is A = LR(\u2211Ll=1 Rl)\u2227RE (E) := E\u2032. Therefore, if we can identify the LoRA adapters (\u2206W l)Ll=1 with rank(\u2206W l) \u2264 Rl such that\nL\u220f l=1 (Wl +\u2206W l)\u2212 L\u220f l=1\nWl\ufe38 \ufe37\ufe37 \ufe38 :=A\n= LR( \u2211L l=1 Rl)\u2227RE (W \u2212 L\u220f l=1\nWl)\ufe38 \ufe37\ufe37 \ufe38 :=E\u2032 ,\nthe proof is completed.\nThe remaining part of the proof adheres to the steps outlined in the proof of Lemma 7 deriving (5). The only difference is that we consider a different selection of (Ql)l = 1L that satisfies (9) here:\nQl = V I( \u2211l\u22121 i=1 Ri)\u2227RE\u2032 :( \u2211l i=1 Ri)\u2227RE\u2032 ,D V \u22a4.\nApplying the same steps with this change yields the desired outcomes.\nThis lemma illustrates that in linear cases, the total number of parameters needed to achieve an exact approximation is constant, regardless of LoRA-rank assignment. It suggests that applying a LoRA-rank of R per layer is equivalent to applying a LoRA-rank of RL at the final layer. As a result, fine-tuning only the last layer, which involves assigning a LoRA-rank of D to the last layer, is equivalent to implementing LoRA where each adapter is constrained to have a rank of D/L. Both methods can achieve an exact approximation and maintain the same parameter efficiency."
        },
        {
            "heading": "E PROOFS FOR FNN APPROXIMATION",
            "text": "In this section, we provide the full proof for deriving the main results outlined in Sec. 3. For the sake of completeness, we restate our results from the main body before presenting the proof."
        },
        {
            "heading": "E.1 APPROXIMATING ONE-LAYER RELU FNN VIA LORA",
            "text": "We first provide a slightly more detailed result on the one-layer ReLU FNN approximation (Lemma 9) along with its corresponding proof. Then, we present a variant of this lemma by allowing for different LoRA-ranks for each low-rank adapter. The proof for this variant involves only a minor modification of the proof for Lemma 9.\nLemma 9 (Detailed version of Lemma 2). Define error matrix E := W 1\u2212 \u220fL\nl=1 Wl, with its rank represented by RE = rank(E). Consider a LoRA-rank R \u2208 [D]. Assume that the weight matrices W1, . . . ,WL \u2208 RD\u00d7D and \u220fL l=1 Wl +LRr(E) for all r \u2264 R(L\u2212 1) are non-singular. Let x be a random input sampled from a distribution with bounded support X and let \u03a3 = Exx\u22a4. Then, there exists rank-R or lower matrices \u2206W 1, . . . ,\u2206WL \u2208 RD\u00d7D and bias vectors b\u03021, . . . , b\u0302L \u2208 RD such that for any input x \u2208 X ,\nf(x)\u2212 f(x) = ReLU (( LRRL\u2227RE (W 1 \u2212 L\u220f\nl=1\nWl)\u2212 (W 1 \u2212 L\u220f\nl=1\nWl)\n) x ) .\nTherefore, when R \u2265 \u2308RE/L\u2309, the adapted model exactly approximates the target model, i.e., f(x) = f(x) for all x \u2208 X . Furthermore, let x be a random input sampled from a distribution with bounded support X and let \u03a3 = Exx\u22a4. Then, the expected squared error is bounded as\nE \u2225\u2225f(x)\u2212 f(x)\u2225\u22252\n2 \u2264 \u2225\u03a3\u2225F \u03c32RL\u2227RE+1(W 1 \u2212 L\u220f l=1 Wl).\nProof of Lemma 9. This proof consists of three main steps: (i) linearize the first L \u2212 1 layers of the adapted model f to reduce it to a single-layer FNN, (ii) align the weight matrices and bias vectors of this simplified f with those of the target model f , (iii) derive an upper bound of the error E \u2225\u2225f(x)\u2212 f(x)\u2225\u22252\n2 .\nLinearization. The main challenge here stems from the non-linearities introduced by the ReLU activation function. To remove the non-linearities in the first L\u2212 1 layers of updated model f , since the input space X is bounded, we can set all the entries of b\u03021, . . . , b\u0302L\u22121 sufficiently large, thereby\nactivating all ReLUs in the first L\u2212 1 layers of f . Consequently, we have f(x) = ReLU((WL +\u2206WL)zL\u22121 + b\u0302L)\n= ReLU ( (WL +\u2206WL)ReLU((WL\u22121 +\u2206WL\u22121)zL\u22122 + b\u0302L\u22121) + b\u0302L ) = ReLU ( (WL +\u2206WL)((WL\u22121 +\u2206WL\u22121)zL\u22122 + b\u0302L\u22121) + b\u0302L\n) = ReLU ( (WL +\u2206WL)(WL\u22121 +\u2206WL\u22121)zL\u22122 + (WL +\u2206WL)b\u0302L\u22121 + b\u0302L\n) = \u00b7 \u00b7 \u00b7\n= ReLU\n( L\u220f\nl=1\n(Wl +\u2206W l)x+ ( L\u22121\u2211 l=1 L\u220f i=l+1 (Wi +\u2206W i)b\u0302l) + b\u0302L\n) ,\nwhich is equivalent to a single-layer ReLU neural network with weight matrix \u220fL\nl=1(Wl +\u2206W l) and bias vector ( \u2211L\u22121\nl=1 \u220fL i=l+1(Wi +\u2206W i)b\u0302l) + b\u0302L.\nParameter Alignment. To match the updated model f(x) and target model f(x), we proceed as follows. For weight matrix, Lemma 7 guarantees the existence of rank-R or lower matrices \u2206W 1, . . . ,\u2206WL \u2208 RD\u00d7D such that\nL\u220f l=1 (Wl +\u2206W l) = L\u220f l=1 Wl + LRRL\u2227RE (W \u2212 L\u220f l=1 Wl). (14)\nFor the bias vector, we set b\u0302L = b1\u2212 \u2211L\u22121\nl=1 \u220fL i=l+1(Wi+\u2206W i)b\u0302l such that \u2211L\u22121 l=1 \u220fL i=l+1(Wi+\n\u2206W i)b\u0302l + b\u0302L = b1. Therefore, we obtain f(x)\u2212 f(x) = ReLU (( LRRL\u2227RE (W 1 \u2212 L\u220f\nl=1\nWl)\u2212 (W 1 \u2212 L\u220f\nl=1\nWl)\n) x ) .\nError Derivation. We compute the expected squared error as follows: E \u2225\u2225f(x)\u2212 f(x)\u2225\u22252\n2 \u2264 E \u2225\u2225\u2225\u2225\u2225 ( LRRL\u2227RE (W 1 \u2212 L\u220f l=1 Wl)\u2212 (W 1 \u2212 L\u220f l=1 Wl) ) x \u2225\u2225\u2225\u2225\u2225 2\n2\n(ReLU is 1-Lipschitz)\n(1) \u2264 \u2225\u2225\u2225\u2225\u2225LRRL\u2227RE (W 1 \u2212 L\u220f l=1 Wl)\u2212 (W 1 \u2212 L\u220f l=1 Wl) \u2225\u2225\u2225\u2225\u2225 2\n2\nE \u2225x\u222522\n= \u2225\u03a3\u2225F \u03c32RL\u2227RE+1(W 1 \u2212 L\u220f\nl=1\nWl). (By the definition of LRRL\u2227RE (\u00b7))\nThis completes the proof.\nLemma 9 is extended to cases where different LoRA-ranks can be used for different low-rank adapters, as detailed in the following lemma.\nLemma 10. Define error matrix E := W 1 \u2212 \u220fL\nl=1 Wl, and denote its rank by RE = rank(E). Consider a sequence of LoRA-ranks (Rl)Ll=1. Assume that the weight matrices W1, . . . ,WL \u2208 RD\u00d7D and \u220fL l=1 Wl+LRr(E) for all r \u2264 \u2211L\u22121 l=1 Rl are non-singular. Then, there LoRA adapters (\u2206W l) L l=1 satisfying the rank constraints rank(\u2206W l) \u2264 Rl for all l \u2208 [L] and bias vectors b\u03021, . . . , b\u0302L \u2208 RD such that for any input x \u2208 X ,\nf(x)\u2212 f(x) = ReLU (( LR( \u2211L l=1 Rl)\u2227RE (W 1 \u2212 L\u220f l=1 Wl)\u2212 (W 1 \u2212 L\u220f l=1 Wl) ) x ) .\nTherefore, when \u2211L\nl=1 Rl \u2265 RE , the adapted model exactly approximates the target model, i.e., f(x) = f(x) for all x \u2208 X . Furthermore, for a random input x drawn from a distribution supported on X , and with \u03a3 = Exx\u22a4, the expected squared error is bounded by:\nE \u2225\u2225f(x)\u2212 f(x)\u2225\u22252\n2 \u2264 \u2225\u03a3\u2225F \u03c32(\u2211Ll=1 Rl)\u2227RE+1(W 1 \u2212 L\u220f l=1 Wl).\nProof of Lemma 10. This proof closely adheres to the steps detailed in the proof of Lemma 9.\nThe primary change implemented here is that, when we draw the analogy to (14), we apply Lemma 8 instead of Lemma 7. This results in\nL\u220f l=1 (Wl +\u2206W l) = L\u220f l=1 Wl + LR( \u2211L l=1 Rl)\u2227RE (W \u2212 L\u220f l=1 Wl).\nUtilizing the steps from the proof of Lemma 9 and integrating the modification specified above, we can establish the desired result."
        },
        {
            "heading": "E.2 APPROXIMATING MULTI-LAYER RELU FNN VIA LORA WITH UNIFORM MODEL PARITION",
            "text": "In this part, we restate all the results considering uniform model partition from Sec. 3.3, along with their corresponding proofs, presented in the same order. Assumption 1 (Non-Singularity). For a fixed LoRA-rank R \u2208 [D], the weight matrices of the frozen model (Wl)Ll=1 and matrices (\u220f l\u2208P ui Wl ) + LRr(W i \u2212 \u220f l\u2208P ui Wl) are non-singular for all r \u2264 R(M \u2212 1) and i \u2208 [L]. Lemma 3. Let (W l)Ll=1, (Wl)Ll=1 \u2208 RD\u00d7D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 1 holds \u2200R \u2208 [D].\nProof of Lemma 3. We first use Lemma 6 to establish that W 1, . . . ,WL,W1, . . . ,WL are nonsingular with probability 1. The goal of the remaining proof is to demonstrate that (\u220f\nl\u2208P ui Wl\n) +\nLRr(W i \u2212 \u220f\nl\u2208P ui Wl) is full-rank with probability 1. In this proof, we use p\u00b7 to denote the proba-\nbility density function, where the subscript indicates the associated random variable. Fix an arbitrary i \u2208 [L] and r \u2208 [R]. Then probability of the (\u220f\nl\u2208P ui Wl\n) +\nLRr ( W i \u2212 \u220f l\u2208P ui Wl ) being full-rank can be computed as\nP det \u220f\nl\u2208P ui\nWl + LRr W i \u2212 \u220f\nl\u2208P ui\nWl  \u0338= 0 \n= \u222b E P det \u220f\nl\u2208P ui\nWl + LRr(E)  \u0338= 0 \u2223\u2223\u2223\u2223\u2223\u2223W i \u2212 \u220f l\u2208P ui Wl = E  pW i\u2212\u220fl\u2208P ui Wl(E)dE. If the conditional random matrix (\u220f l\u2208P ui Wl ) +LRr(E) | W i\u2212 \u220f l\u2208P ui Wl = E has a continuous distribution with support of non-zero Lebesgue measure on RD\u00d7D, then\nP det \u220f\nl\u2208P ui\nWl + LRr(E)  \u0338= 0 \u2223\u2223\u2223\u2223\u2223\u2223W i \u2212 \u220f l\u2208P ui Wl = E  = 1 ensuring (\u220f l\u2208P ui Wl ) + LRr ( W i \u2212 \u220f l\u2208P ui Wl ) is full-rank with probability 1.\nConsequently, the remaining part of the proof aims to show that the conditional random matrix(\u220f l\u2208P ui Wl ) + LRr(E) | W i \u2212 \u220f l\u2208P ui Wl = E follows arbitrary continuous distribution with\nsupport having non-zero Lebesgue measure on RD\u00d7D. Denote W = \u220f\nl\u2208P ui Wl. Now, consider the conditional distribution of \u220f\nl\u2208P ui Wl | W i \u2212 \u220f l\u2208P ui Wl = E, which can be written as\npW|Wi\u2212W=E(W ) = pWi(E +W ).\nSince pWi is continuous with support of non-zero Lebesgue measure on R D\u00d7D, the same holds for \u220f\nl\u2208P ui Wl | W i \u2212 \u220f l\u2208P ui\nWl = E. Furthermore, adding a constant matrix LRr(E) to this conditional distribution preserves the desired properties, thus completing the proof.\nTheorem 3. Under Assumption 1, there exists rank-R or lower matrices (\u2206W l)Ll=1 with \u2206W l \u2208 RD\u00d7D and bias vectors (b\u0302l)Ll=1 with b\u0302l \u2208 RD when the rank of the low-rank adapter R \u2265 \u2308maxi\u2208[L] rank(W i \u2212 \u220f l\u2208P ui\nWl)/M\u2309, the low-rank adapted model f can exactly approximate the target model f , i.e., f(x) = f(x) for all input x \u2208 X .\nProof of Theorem 3. The key to this proof lies in a simple idea: for each layer i \u2208 [L] in the target model, we can update M layers (i.e., (i\u2212 1)M + 1-th layer to iM -th layer) in the frozen model to approximate it as guaranteed by Lemma 9. Hence, all layers of the target model can be approximated by the adapted model.\nModel Decomposition. We partition the adapted model f into L sub-models, each defined as\nfi(\u00b7) = FNNL,D(\u00b7; (Wl +\u2206W l)l\u2208P ui , (b\u0302l)l\u2208P ui ), i \u2208 [L].\nIn a similar manner, we break down f into L sub-models, each is a one-layer FNN:\nf i(\u00b7) = FNN1,D(\u00b7;W i, bi), i \u2208 [L]. We can then express f(x) and f(x) as compositions of their respective sub-models:\nf(\u00b7) = fL \u25e6 \u00b7 \u00b7 \u00b7 f1(\u00b7), f(\u00b7) = fL \u25e6 \u00b7 \u00b7 \u00b7 f1(\u00b7). To analyze the error E \u2225\u2225f(x)\u2212 f(x)\u2225\u2225\n2 = E \u2225\u2225f(x)\u2212 f(x)\u2225\u2225 2 , we consider the error caused by\neach submodel. Let R\u0303i = rank(W i \u2212 \u220f\nl\u2208P ui Wl) denote the rank of the discrepancy between the\ntarget weight matrix and the frozen weight matrices, where i \u2208 [L]. By Lemma 9, we can select \u2206W 1, . . . ,\u2206WL, b\u03021, . . . , b\u0302L such that\nfi(z)\u2212 f i(z) = ReLU LRRL\u2227R\u0303i(W i \u2212 \u220f l\u2208P ui Wl)\u2212 (W i \u2212 \u220f l\u2208P ui Wl)  z  , (15)\nE \u2225\u2225fi(z)\u2212 f i(z)\u2225\u222522 \u2264 \u2225\u2225Ezz\u22a4\u2225\u2225F \u03c32RL\u2227R\u0303i+1(W i \u2212 L\u220f\nl=1\nWl). (16)\nGiven these selected parameters, fi is functionally equivalent to a one-layer FNN:\nfi(z) = ReLU LRRL\u2227R\u0303i(W i \u2212 \u220f l\u2208P ui Wl) + \u220f l\u2208P ui Wl  z  .\nClearly, when R \u2265 maxi\u2308 R\u0303iM \u2309, it follows that fi = gi for all i \u2208 [L], which implies f = g.\nCorollary 4. Assume that the elements of matrices (W l)Ll=1, (Wl)Ll=1 are independently drawn from arbitrary continuous distributions. When R \u2265 D/M , there exists rank-R or lower matrices \u2206W 1, . . . ,\u2206WL \u2208 RD\u00d7D and bias vectors b\u03021, . . . , b\u0302L \u2208 RD such that low-rank adapted model f can functionally cover the target model f on X , i.e., f(x) = f(x) for all input x \u2208 X , with probability 1.\nProof of Corollary 4. To prove the statement, we start by noting that combining Lemma 3 and Theorem 3 directly gives us f(x) = f(x) on X when R \u2265 maxi\u2208[L]\u2308rank(W i \u2212 \u220f l\u2208P ui\nWl)/M\u2309. Therefore, the only thing left is to show that rank(Wi \u2212 \u220f l\u2208P ui\nWl) = D for i \u2208 [L] with probability 1. In this proof, we use p\u00b7 to denote the probability density function, where the subscript indicates the associated random variable.\nTo establish this, consider the following probability expression:\nP det Wi \u2212 \u220f\nl\u2208P ui\nWl  \u0338= 0 \n= \u222b P det (Wi \u2212W ) \u0338= 0 \u2223\u2223\u2223\u2223\u2223\u2223 \u220f l\u2208P ui Wl = W  p\u220fl\u2208P ui Wl(W )dW . Since W is independent of \u220f l\u2208P ui Wl, we have\nP det (Wi \u2212W ) \u0338= 0 \u2223\u2223\u2223\u2223\u2223\u2223 \u220f l\u2208P ui Wl = W  = P{det (Wi \u2212W ) \u0338= 0} Lemma 6===== 1. Therefore, we conclude that P { det ( Wi \u2212 \u220f l\u2208P ui Wl ) \u0338= 0 } = 1, which completes the proof.\nTheorem 5. Define the approximation error of i-th layer as Ei = \u03c3RM+1(W i \u2212\u220f l\u2208P ui Wl), and the magnitude of the parameters and the input as \u03b2 :=\nmaxi\u2208[L] (\u221a \u2225\u03a3\u2225F \u220fi j=1 \u2225\u2225W j\u2225\u2225F +\u2211ij=1\u220fi\u22121k=j+1 \u2225\u2225W k\u2225\u2225F \u2225\u2225bj\u2225\u22252)\u2228\u221a\u2225\u03a3\u2225F. Under Assumption 1, there exists rank-R or lower matrices (\u2206W l)Ll=1 with \u2206W l \u2208 RD\u00d7D and bias vectors (b\u0302l)Ll=1 with b\u0302l \u2208 RD such that for input x \u2208 X with Exx\u22a4 = \u03a3,\nE \u2225\u2225f(x)\u2212 f(x)\u2225\u2225\n2 \u2264 \u03b2 L\u2211 i=1 max k\u2208[L] (\u2225\u2225W k\u2225\u2225F + Ek)L\u2212i Ei.\nProof of Theorem 5. This proof is a continuation of the proof of Theorem 3. In this proof, we will consider a more general case, without enforcing any constraints on the rank of the adapters R. We use W\u0302i to denote the corresponding weight matrix, i.e., W\u0302i = LRRL\u2227R\u0303i(W 1 \u2212 \u220f l\u2208P ui\nWl) +\u220f l\u2208P ui Wl.\nError Decomposition. For submodel i = 2, . . . , L, we calculate the expected error of the composition of the first i sub-models,\nE \u2225z\u0302i \u2212 zi\u22252 = E \u2225\u2225fi(z\u0302i\u22121)\u2212 f i(zi\u22121)\u2225\u22252 (17)\n= E \u2225\u2225(fi(z\u0302i\u22121)\u2212 fi(zi\u22121)) + (fi(zi\u22121)\u2212 f i(zi\u22121))\u2225\u22252 (Rearranging terms) \u2264 E \u2225fi(z\u0302i\u22121)\u2212 fi(zi\u22121)\u22252\ufe38 \ufe37\ufe37 \ufe38 Ai +E \u2225\u2225fi(zi\u22121)\u2212 f i(zi\u22121)\u2225\u22252\ufe38 \ufe37\ufe37 \ufe38\nBi\n. (Applying triangle inequality)\nHere Ai represents the error resulting from the discrepancy between the first i\u2212 1 submodels, while Bi represents the error arising from the mismatch between the i-th submodel.\nComputing Ai. We start by computing the error introduced by the first i\u2212 1 submodels, denoted by Ai:\nAi = E \u2225fi(z\u0302i\u22121)\u2212 fi(zi\u22121)\u22252 = E \u2225\u2225\u2225ReLU(W\u0302i(z\u0302i\u22121 \u2212 zi\u22121))\u2225\u2225\u2225\n2 \u2264 E \u2225\u2225\u2225W\u0302i(z\u0302i\u22121 \u2212 zi\u22121)\u2225\u2225\u2225\n2 (ReLU is 1-Lipschitz)\n(1) \u2264 \u2225\u2225\u2225W\u0302i\u2225\u2225\u2225\nF E \u2225z\u0302i\u22121 \u2212 zi\u22121\u22252 . (18)\nHere,\u2225\u2225\u2225W\u0302i\u2225\u2225\u2225 F = \u2225\u2225\u2225\u2225\u2225\u2225 \u220f l\u2208P ui Wl + LRRM\u2227R\u0303i(W i \u2212 \u220f l\u2208P ui Wl) \u2225\u2225\u2225\u2225\u2225\u2225 F\n= \u2225\u2225\u2225\u2225\u2225\u2225W i + \u220f\nl\u2208P ui\nWl \u2212W i + LRRM\u2227R\u0303i(W i \u2212 \u220f l\u2208P ui Wl) \u2225\u2225\u2225\u2225\u2225\u2225 F (Rearranging terms)\n\u2264 \u2225\u2225W i\u2225\u2225F + \u2225\u2225\u2225\u2225\u2225\u2225 \u220f\nl\u2208P ui\nWl \u2212W i + LRRM\u2227R\u0303i(W i \u2212 \u220f l\u2208P ui Wl) \u2225\u2225\u2225\u2225\u2225\u2225 F\n(Applying triangle inequality)\n= \u2225\u2225W i\u2225\u2225F + \u221a\u221a\u221a\u221a\u221a D\u2211 j=RM\u2227R\u0303i+1 \u03c32j (W i \u2212 \u220f l\u2208P ui Wl) (19)\n(By the definition of W i and LRRM\u2227R\u0303i+1(\u00b7)) \u2264 max k\u2208[L] ( \u2225\u2225W k\u2225\u2225F + Ei) := \u03b1.\nBy combining (18) and (19), we get\nAi \u2264 max k\u2208[L] (\u2225\u2225W k\u2225\u2225F + Ei)E \u2225z\u0302i\u22121 \u2212 zi\u22121\u22252 \u2264 \u03b1E \u2225z\u0302i\u22121 \u2212 zi\u22121\u22252 . (20) Computing Bi. We proceed to compute the error associated with the i-th submodel, which we denote as Bi. It can be evaluated as follows:\nBi = E \u2225\u2225fi(zi\u22121)\u2212 f i(zi\u22121)\u2225\u22252\n(15) = E \u2225\u2225\u2225\u2225\u2225\u2225ReLU LRRM\u2227R\u0303i(W i \u2212 \u220f\nl\u2208P ui Wl)\u2212 (W i \u2212 \u220f l\u2208P ui Wl)  zi\u22121 \u2225\u2225\u2225\u2225\u2225\u2225\n2\n\u2264 E \u2225\u2225\u2225\u2225\u2225\u2225 LRRM\u2227R\u0303i(W i \u2212 \u220f\nl\u2208P ui Wl)\u2212 (W i \u2212 \u220f l\u2208P ui Wl)  zi\u22121 \u2225\u2225\u2225\u2225\u2225\u2225 2\n(ReLU is 1-Lipschitz)\n(1) \u2264 \u2225\u2225\u2225\u2225\u2225\u2225LRRM\u2227R\u0303i(W i \u2212 \u220f l\u2208P ui Wl)\u2212 (W i \u2212 \u220f l\u2208P ui Wl) \u2225\u2225\u2225\u2225\u2225\u2225 2 E \u2225zi\u22121\u22252\n= \u03c3RM\u2227R\u0303i+1(W i \u2212 \u220f l\u2208P ui Wl)E \u2225zi\u22121\u22252 .\nWe can further simplify E \u2225zi\u22121\u22252 as : E \u2225zi\u22121\u22252 = E\n\u2225\u2225ReLU(W i\u22121zi\u22122 + bi\u22121)\u2225\u22252 = E \u2225\u2225W i\u22121zi\u22122 + bi\u22121\u2225\u22252 (ReLU is 1-Lipschitz)\n\u2264 \u2225\u2225W i\u22121\u2225\u2225F E \u2225zi\u22122\u22252 + \u2225\u2225bi\u22121\u2225\u22252 (Applying triangle inequality and (1))\n\u2264 \u2225\u2225W i\u22121\u2225\u2225F (\u2225\u2225W i\u22122\u2225\u2225F E \u2225zi\u22123\u22252 + \u2225\u2225bi\u22122\u2225\u22252)+ \u2225\u2225bi\u22121\u2225\u22252 (Following the same steps)\n\u2264 i\u22121\u220f j=1 \u2225\u2225W j\u2225\u2225F E \u2225x\u22252 + i\u22121\u2211 j=1 i\u22121\u220f k=j+1 \u2225\u2225W k\u2225\u2225F \u2225\u2225bj\u2225\u22252 (Repeating the same steps) = \u221a\n\u2225\u03a3\u2225F i\u22121\u220f j=1 \u2225\u2225W j\u2225\u2225F + i\u22121\u2211 j=1 i\u22121\u220f k=j+1 \u2225\u2225W k\u2225\u2225F \u2225\u2225bj\u2225\u22252 \u2264 \u03b2. Therefore, we obtain\nBi \u2264 \u03b2\u03c3RM\u2227R\u0303i+1(W i \u2212 \u220f l\u2208P ui Wl).\nError Composition. Having established upper bounds for Ai and Bi, we next evaluate the expected error for the composition of the first i adapted submodels.\nE \u2225z\u0302i \u2212 zi\u22252 (17) \u2264 Ai +Bi (20) \u2264 \u03b1E \u2225z\u0302i\u22121 \u2212 zi\u22121\u22252 +Bi \u2264 \u03b1(\u03b1E \u2225z\u0302i\u22122 \u2212 zi\u22122\u22252 +Bi\u22121) +Bi\n= \u03b12E \u2225z\u0302i\u22122 \u2212 zi\u22122\u22252 + \u03b1Bi\u22121 +Bi \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b1i\u22121E \u2225z\u03021 \u2212 z1\u22252 + i\u2211\nk=2\n\u03b1i\u2212kBk. (21)\nTo compute the overall approximation error of f , which is the composite of all submodels, we have E \u2225\u2225f(x)\u2212 f(x)\u2225\u2225\n2 = E \u2225\u2225f(x)\u2212 f(x)\u2225\u2225 2 = E \u2225z\u0302L \u2212 zL\u22252\n(21) \u2264 \u03b1L\u22121E \u2225z\u03021 \u2212 z1\u22252 + L\u2211\ni=2\n\u03b1L\u2212iBi\n(16) \u2264 \u03b1L\u22121\u03b2\u03c3RM\u2227R\u0303i+1(W i \u2212 \u220f l\u2208P ui Wl) + \u03b2 L\u2211 i=2 \u03b1L\u2212i\u03c3RM\u2227R\u0303i+1(W i \u2212 \u220f l\u2208P ui Wl)\n= \u03b2 L\u2211 i=1 \u03b1L\u2212i\u03c3RM\u2227R\u0303i+1(W i \u2212 \u220f l\u2208P ui Wl)\n= \u03b2 L\u2211 i=1 \u03b1L\u2212i\u03c3RM+1(W i \u2212 \u220f l\u2208P ui Wl).\nSubstituting \u03b1 with maxk\u2208[L]( \u2225\u2225W k\u2225\u2225F + Ei) concludes the proof."
        },
        {
            "heading": "E.3 APPROXIMATING MULTI-LAYER RELU FNN VIA LORA WITH GENERAL MODEL PARITION",
            "text": "Firstly, we provide the required non-singular assumption and the lemma demonstrating the mildness of this assumption for the general model partition cases after introducing necessary notations.\nAssumption 2. For the given LoRA-rank sequence (Rl) L l=1 and partition P , the weight matrices of the frozen model W1, . . . ,WL and (\u220f l\u2208Pi Wl ) + LRr(W i \u2212 \u220fmaxPi\u22121 l=minPi\nWl) are non-singular for all r \u2264\u2211maxPi\u22121l=minPi Rl and i \u2208 [L]. Note that maxPi and minPi here represent the maximum and minimum elements in the set Pi, respectively.\nLemma 11. Let (W l)Ll=1, (Wl)Ll=1 \u2208 RD\u00d7D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 2 holds for all R \u2208 [D].\nProof of Lemma 11. Following the same steps in the proof of Lemma 3 but replacing the uniform partition with the general partition completes the proof.\nWe now restate Theorem 6 and provide its proof. Theorem 6. Consider a partition P for the frozen model. Let Assumption 2 hold. If \u2211l\u2208Pi Rl \u2265 rank(W i \u2212 \u220f l\u2208Pi Wl) for all i \u2208 [L], there exists LoRA adapters (\u2206W l)Ll=1 with rank(\u2206W l) \u2264 Rl and biases (b\u0302l)Ll=1 such that the adapted model f can exactly approximate the target model. Moreover, define the approximation error of the i-th layer as Ei = \u03c3\u2211 l\u2208Pi\nRl+1(W i \u2212\u220f l\u2208Pi Wl), and the magnitude of the parameters and the input as \u03b2 :=\nmaxi\u2208[L] (\u221a \u2225\u03a3\u2225F \u220fi j=1 \u2225\u2225W j\u2225\u2225F +\u2211ij=1\u220fi\u22121k=j+1 \u2225\u2225W k\u2225\u2225F \u2225\u2225bj\u2225\u22252)\u2228\u221a\u2225\u03a3\u2225F. Then, there exists LoRA adapters (\u2206W l)Ll=1 with rank(\u2206W l) \u2264 Rl and biases (b\u0302l)Ll=1 such that for any input x \u2208 X with Exx\u22a4 = \u03a3, the approximation error can be bounded as\nE \u2225\u2225f(x)\u2212 f(x)\u2225\u2225\n2 \u2264 \u03b2 L\u2211 i=1 max k\u2208[L] (\u2225\u2225W k\u2225\u2225F + Ek)L\u2212i Ei. Proof of Theorem 6. This proof follows the same steps as the proofs of Theorem 3 and Theorem 5, substituting the uniform partition Pu with the general partition P and applying Lemma 10 in place of Lemma 2 to derive the desired outcome."
        },
        {
            "heading": "E.4 APPROXIMATING MULTI-LAYER RELU FNN VIA FINAL LAYERS TUNING",
            "text": "We now aim to examine another commonly used model adaptation method, the final layers tuning, within the same theoretical framework.\nThe main limitation of this method, as compared to LoRA, is that while LoRA can update all layers, the tuning of final layers keeps the initial layers frozen. Consequently, a clear limitation arises when the initial layers of the frozen model f are less discriminative than the target model f . That is, if there exist two input vectors x1,x2 \u2208 RD\u00d7D such that the output of the initial layers of the frozen model f0 is the same, but the output of the target model f is different, then no matter how the final layers are tuned, it is impossible for the adapted model f to exactly approximate the target model f .\nTo formalize this, we observe that for the first layer of the frozen model, the outputs of the inputs in the non-activation region are always zero. In other words, when x1,x2 \u2208 {x : W1x+ b1 \u2264 0}, we have ReLU(W1x1+b1) = ReLU(W1x2+b1) = 0. Therefore, no matter how the subsequent layers are tuned, we still have f(x1) = f(x2). When we fix the first l\u2212 1 layers, the non-activation region becomes {x : W2(W1x+ b1) + b2 \u2264 0}. Similarly, we define the non-active region of the first l layer in the frozen model as Il = { x : \u220fl i=1 Wix+ \u2211l i=1 \u220fl j=i+1 Wjbi \u2264 0 } . Correspondingly,\nwe define I l = { x : \u220fl i=1 W ix+ \u2211l i=1 \u220fl j=i+1 W jbi \u2264 0 } .\nThe following lemma is provided based on these definitions. Lemma 12. If l \u2208 [L \u2212 1] such that Il \\ \u22c3L\ni=1 Ii \u0338= \u2205 and the weight matrices of the target model (W i) L i=1 are non-singular, then for any tuning of the last L\u2212 l layers, f \u0338= f . Proof of Lemma 12. For the simplicity of the presentation, we let I = \u22c3L\ni=1 Ii to denote the nonactivation region of the target model. Then, the condition Il \\ \u22c3L\ni=1 Ii \u0338= \u2205 can be written as Il \\ I \u0338= \u2205. Clearly, both I and Il are closed convex sets.\nCondition Il \\ I \u0338= \u2205. The condition Il \\ I \u0338= \u2205 indicates that there exists a region in Il where the ReLUs are deactivated in the l-th layer of the frozen model, but activated in the entire target model. Therefore, for any x1,x2 \u2208 Il \\ I , we have f(x1) = f(x2) regardless of how the final l + 1 layers\nare tuned. If these x1,x2 \u2208 Il \\ I satisfies f(x1) \u0338= f(x2), this proof is completed. The remaining proof is showing the existence of such x1,x2.\nExistence of x1,x2. Firstly, we show that there exists two x1,x2 \u2208 Il \\ I such that x1 \u0338= x2. Let x1 \u2208 Il \\ I . Since Il is a closed set, there exists a sequence (zi)\u221ei=1 where zi \u2208 Il and zi \u0338= x1 satisfying limi\u2192\u221e zi = x1. Note that at least one element zi must not belong to I , otherwise x1 would be in I due to the closed property of I , contradicting the selection of x1. Let x2 = zi. Therefore, we have two distinct x1,x2 \u2208 Il \\ I with x1 \u0338= x2. Then, given x1,x2 \u2208 Il \\ I such that x1 \u0338= x2, both x1,x2 activate all the ReLUs in the target model. Since x1,x2 \u0338\u2208 I and the weight matrices of the target model (W l)Ll=1 all are non-singular, we have\nf(x1)\u2212 f(x2) = WL \u00b7 \u00b7 \u00b7W 1(x1 \u2212 x2) \u0338= 0,\nimplying f(x1) \u0338= f(x2). Meanwhile, since x1,x2 \u2208 Il, the output of the initial l layers of the frozen model are equal, thus we have f(x1) = f(x2) no matter how we tune the last L \u2212 l layers. This completes the proof.\nThe following lemma reduces the assumptions to the assumption of randomly generated models. This assumption aligns with that of Corollary 4, thereby facilitating a more effective comparison between the expressive power of LoRA and the adaptation of the final layers.\nLemma 4. Let D \u2265 2 and f be a one-layer target FNN. Assume that the elements of weight matrices (Wl)Ll=1 are independently drawn from arbitrary continuous distributions. With probability 1, for any tuning of the last L\u2212 1 layers, f \u0338= f .\nProof of Lemma 4. If we can show that I1 \\ I1 \u0338= \u2205, by Lemma 12, we obtain the desired results. Therefore, the remaining proof aims to show that I1 \\ I1 \u0338= \u2205 with probability 1.\nNote that I1 \\ I1 = \u2205 holds only when W 1 = W1 (not that this is necessary condition not sufficient condition), as demonstrated in Figure 2. However, since the elements of matrices W1 are independently drawn from arbitrary continuous distributions, we have P(W1 \u0338= W 1) = 1 for all l \u2208 [L]. Therefore, I1 \\ I1 = \u2205 holds with probability 1. By Lemma 12, we complete the proof."
        },
        {
            "heading": "F PROOFS FOR TFN APPROXIMATION",
            "text": "In this section, we not only provide the proof for the results outlined in Sec. 4, but also introduce the problem setting for TFNs with single-head attention layers and present the corresponding results."
        },
        {
            "heading": "F.1 APPROXIMATING TRANSFORMER NETWORK WITH SINGLE-HEAD ATTENTION LAYERS",
            "text": "In this part, we outline the problem setting to investigate the expressive power of LoRA in TFNs that utilize single-head attention layers. The primary distinction between this setting and that of TFNs with multi-head attention layers lies in the weight matrices. Specifically, the W hOl matrices for combining different attention heads are absent in this case. Despite this difference, the derived results are consistent, albeit under slightly modified assumptions regarding the weight matrices and a different LoRA adaptation strategy.\nWe start by introducing necessary notations. For an input matrix X \u2208 RD\u00d7N , where D is the dimension of the token embeddings and N is the number of tokens, the l-th Transformer block using single-head self-attention can be expressed as:\nAttnl(Zl\u22121) = WV lZl\u22121 \u00b7 softmax ( (WKlZl\u22121) \u22a4WQlZl\u22121 ) ,\nZl := W2l \u00b7 ReLU(W1l \u00b7 Attnl(Zl\u22121) + b1l1\u22a4N ) + b2l1\u22a4N ,\nwhere the weight matrices WKl,WQl,WV l,W1l,W2l \u2208 RD\u00d7D, bias vectors b1l, b2l\u2208RD , Zl is the output of l-th transformer block, with Z0 = X . The output of the first L Transformer blocks are subsequently fed into the output layer. This produces the final output of the TFN, given by softmax(WoZL), where Wo \u2208 RD\u00d7D represents the weight matrix of the output layer. For single-head self-attention layers, the target model f , frozen model f , and the adapted model f can be formally represented as:\nTarget TFN g = TFNL,D ( \u00b7; ( (W V l,WKl,WQl,W 2l,W 1l) L l=1,W o ) , (b1l, b2l) L l=1 ) ,\nFrozen TFN f0 = TFNL,D ( \u00b7; ( (WV l,WKl,WQl,W2l,W1l) L l=1,Wo ) , (b1l, b2l) L l=1 ) ,\nAdapted TFN f = TFNL,D ( \u00b7; ( (WV l +\u2206W V l,WKl +\u2206WKl,WQl +\u2206WQl,\nW2l +\u2206W 2l,W1l +\u2206W 1l) L l=1,Wo +\u2206W o ) , (b\u03021l, b\u03022l) L l=1 ) .\nHere, WKl,WQl,W V l are the weight matrices for generating key, query, and values in the lth transformer block of the target TFN; W 1l,W 2l and b1l, b2l serve as the weight matrices and bias vectors, respectively, for the feedforward layer in the same block; W o is the weight matrix for the output layer. For the frozen TFN, the same roles are played by WKl,WQl,WV l, W1l,W2l, and b1l, b2l for all l \u2208 [L] and Wo. For the adapted model, low-rank adapters \u2206WKl,\u2206WQl,\u2206W V l,\u2206W 1l,\u2206W 2l,\u2206W o with a rank constraint R \u2208 [D] are added to each weight matrix, and the bias vectors are updated to b\u03021l, b\u03022l for all l \u2208 [L]. Given the problem setting outlined above, we give the non-singularity assumption for TFNs with single-head attention layers. Assumption 3 (Non-Singularity). All the weight matrices of both the target model and the frozen model, as well as the following matrices for all r \u2208 [D],\nW\u22a4KlWQl + LRr\n( W \u22a4 KlWQl \u2212W\u22a4KlWQl ) , where l = 1,\nWKlWQl + LRr ( W\u22121\u22a42,l\u22121W \u22a4 2,l\u22121W \u22a4 KlWQlW 2,l\u22121W \u22121 2,l\u22121 \u2212WKlWQl ) , for l \u2208 [L] \\ {1} ,\nW1lWV l + LRr ( W 1lW V l \u2212W1lWV l ) , for l = 1,\nW1lWV l + LRr ( W 1lW V lW 2,l\u22121W \u22121 2,l\u22121 \u2212W1lWV l ) , for all l \u2208 [L] \\ {1} ,\nWoW2L + LRr(W oW 2L \u2212WoW2L), are non-singular.\nLemma 13. Let the elements of all weight matrices in target model f and the frozen model f be independently sampled from continuous distributions. Then, Assumption 3 holds with probability 1.\nProof of Lemma 13. The results can be obtained by replicating the same steps outlined in the proof of Lemma 3.\nTheorem 8. Consider the rank of the adapter weight matrices R \u2208 [D]. Let Assumption 3 hold. Define the rank-based functionality gap Gi to i-th transformer block (i \u2208 [L]) or output layer (i = L+ 1) as\nGi =  maxh ( rank(W h\u22a4 KiW h Qi \u2212W h\u22a4Ki W hQi) ) \u2228maxh ( rank(W 1iW h V i \u2212W1iW hV i) ) , i = 1, maxh ( rank(W \u22a4 2,i\u22121W h\u22a4 KiW h QiW 2,i\u22121 \u2212W\u22a42,i\u22121W h\u22a4Ki W hQiW2,i\u22121) ) \u2228maxh ( rank(W 1iW h V iW 2,i\u22121 \u2212W1iW hV iW2,i\u22121) ) , 2 \u2264 i \u2264 L,\nrank(W oW 2L \u2212WoW2L), i = L+ 1.\nIf R \u2265 maxi\u2208[L+1]\u2308Gi2 \u2309, there exists rank-R or lower weight matrices for low-rank adapters (\u2206WKl,\u2206WQl,\u2206W V l,\u2206W 1l) L l=1,\u2206W 2L,\u2206W o with other low-rank adapters set to O, and updated bias vectors: (b\u03021l, b\u03022l)Ll=1, such that for any X \u2208 RD\u00d7N , the adapted model f exactly approximates f , i.e., f(X) = f(X), with probability 1.\nProof of Theorem 8. Let H l \u2208 RD\u00d7N and Zl \u2208 RD\u00d7N denote the intermediate and final outputs of the l-th transformer block in the target model f , respectively. Specifically, H l represents the output from the first feedforward layer in the l-th transformer block. They are defined as\nH l = ReLU ( W 1lW V lZl\u22121 \u00b7 softmax ( Z \u22a4 l\u22121W \u22a4 KlWQlZl\u22121 ) + b1l1 \u22a4 N ) ,\nZl = W 2lH l + b2l1 \u22a4 N ,\nwhere l \u2208 [L]. For the adapted model f , we introduce H\u0302l and Z\u0302l to denote the corresponding intermediate output of the first feedforward layer and the final output of the l-th transformer block for the adapted model, respectively:\nH\u0302l = ReLU ( (W1l +\u2206W 1l)(WV l +\u2206W V l) \u00b7 Z\u0302l\u22121\n\u00b7 softmax ( Z\u0302\u22a4l\u22121(WKl +\u2206WKl) \u22a4(WQl +\u2206WQl)Z\u0302l\u22121 ) + b\u03021l1 \u22a4 N ) ,\nZ\u0302l = (W2l +\u2206W 2l)H\u0302l + b\u03022l1 \u22a4 N ,\nwhere l \u2208 [L]. We note that Z0 = Z\u03020 = X . In this proof, we set \u2206W 2l = O for all l \u2208 [L]. Our goal is to show that adding low-rank adapters to self-attention layers and the first feedforward layers in all transformer blocks enables the adapted model f to be functionally equivalent to the target model f of the same dimensions. We start by inductively constructing the adapter weight matrices (\u2206W 1l,\u2206W V l,\u2206WKl,\u2206WQl, b\u03021l, b\u03022l)Ll=1 such that H\u0302l = H l for all l \u2208 [L]. We then select the low-rank adapters for W2L and the Wo to approximate the output of the target model. For unmentioned low-rank adapters, we set them as O.\nWhen l = 1. To achieve H\u0302l with H l for all X , the following conditions must be satisfied:\nBias Vector: b\u03021l = b1l,\nQuery and Key: (WKl +\u2206WKl)\u22a4(WQl +\u2206WQl) = W \u22a4 KlWQl\nValue and First Feedforward Layer: (W1l +\u2206W 1l)(WV l +\u2206W V l) = W 1lW V l.\nTo achieve this, we set b\u03021l = b1l to achieve (24), and select rank-R or lower matrices \u2206WKl,\u2206WQl,\u2206W 1l,\u2206W V l as suggested by Lemma 7. This ensures H\u0302l = H l for l = 1.\nWhen l > 1. Now we focus on the cases where l = 2, . . . , L. Assume the induction hypothesis holds for l \u2212 1, which is H\u0302l\u22121 = H l\u22121. This implies\nH l\u22121 = W \u22121 2,l\u22121(Zl\u22121 \u2212 b2,l\u221211\u22a4N ) = W\u221212,l\u22121(Z\u0302l\u22121 \u2212 b\u03022,l\u221211\u22a4N ) = H\u0302l\u22121.\nUsing this assumption, we express Z\u0302l\u22121 in terms of Zl\u22121:\nZ\u0302l\u22121 = W2,l\u22121W \u22121 2,l\u22121(Zl\u22121 \u2212 b2,l\u221211\u22a4N ) + b\u03022,l\u221211\u22a4N .\nLet b\u03022,l\u22121 = W2,l\u22121W \u22121 2,l\u22121b2,l\u22121, then we have\nZ\u0302l\u22121 = W2,l\u22121W \u22121 2,l\u22121Zl\u22121. (22)\nTo achieve H\u0302l = H l, we express both H\u0302l and H l in terms of Zl\u22121: H l = ReLU ( W 1lW V l \u00b7Zl\u22121 \u00b7 softmax ( Z \u22a4 l\u22121W \u22a4 KlWQlZl\u22121 ) + b1l1 \u22a4 N ) H\u0302l = ReLU ( (W1l +\u2206W 1l)(WV l +\u2206W V l) \u00b7 Z\u0302l\u22121\n\u00b7 softmax ( Z\u0302\u22a4l\u22121(WKl +\u2206WKl) \u22a4(WQl +\u2206WQl)Z\u0302l\u22121 ) + b\u03021l1 \u22a4 N ) ,\n(22) = ReLU ( (W1l +\u2206W 1l)(WV l +\u2206W V l) \u00b7W2,l\u22121W \u22121 2,l\u22121Zl\u22121\n\u00b7 softmax ( Z \u22a4 l\u22121W \u22121\u22a4 2,l\u22121W \u22a4 2,l\u22121(WKl +\u2206WKl) \u22a4\n(WQl +\u2206WQl)W2,l\u22121W \u22121 2,l\u22121Zl\u22121 ) + b\u03021l1 \u22a4 N ) .\nTherefore, we need to align the following three components:\nBias Vector: b\u03021l = b1l,\nQuery and Key: (WKl +\u2206WKl)\u22a4(WQl +\u2206WQl) = W\u22121\u22a42,l\u22121W \u22a4 2,l\u22121W \u22a4 KlWQlW 2,l\u22121W \u22121 2,l\u22121, Value and First Feedforward Layer: (W1l +\u2206W 1l)(WV l +\u2206W V l) = W 1lW V lW 2,l\u22121W\u221212,l\u22121.\nBy setting b\u03021l based on (26) and adjusting \u2206WKl,\u2206WQl,\u2206W 1l,\u2206W V l based on Lemma 7, we satisfy all three conditions above, thereby obtaining H\u0302l = H l for l \u2208 [L] \\ {1}.\nOutput Layer Analysis. By the induction method, we have established H\u0302l = H l for all l \u2208 [L]. We will complete the proof by showing that f(X) = f(X) for all X \u2208 X . The final output distribution of the target TFN f can be written as\nf(X) = softmax(W oZL) = softmax ( W o ( W 2LHL + b2L1 \u22a4 N )) .\nWe can similarly formulate the final output distribution of the adapted model f :\nf(X) = softmax((Wo +\u2206W o)Z\u0302L) = softmax ( (Wo +\u2206W o) ( (W2L +\u2206W 2L)H\u0302L + b\u03022L1 \u22a4 N )) ,\nTo align these two expressions, we select \u2206W 2L and \u2206W o based on Lemma 7, and let b\u03022L = (Wo + \u2206W o)\n\u22121W ob2L, where Wo + \u2206W o is invertible as shown in the proof of Lemma 7. Thus, the proof is complete.\nThe following corollary identifies the specific LoRA-rank required to achieve exact representation for random model cases in the current setting. Corollary 9. Assume that the elements of all the weight matrices of both the target TFN and the frozen TFN are independently drawn from arbitrary continuous distributions. If R \u2265 \u2308D2 \u2309, adding low-rank adapters of rank at most R to weight matrices in (\u2206WKl,\u2206WQl,\u2206W V l,\u2206W 1l) L l=1,\u2206W 2L,\u2206W o and tuning the bias vectors, enables the adapted model f to exactly approximate the target model f , i.e., f(X) = f(X) for all X \u2208 RD\u00d7N .\nProof of Corollary 9. By combining Lemma 13 and Theorem 8, and following the same steps in the proof of Corollary 4 which yields maxi Gi = D, we can obtain the desired outcome."
        },
        {
            "heading": "F.2 APPROXIMATING TRANSFORMER NETWORK WITH MULTI-HEAD ATTENTION LAYERS",
            "text": "In this section, we first provide the explicit formulation of TFN with multi-head attention layers.\nConsider an input matrix X \u2208 RD\u00d7N , where D is the dimension of the token embeddings and N is the number of tokens. The output of the l-th transformer block is denoted as Zl, which can be computed as follows:\nAttnl(Zl\u22121) := H\u2211\nh=1\nW hOlW h V lZl\u22121 \u00b7 softmax ( (W hKlZl\u22121) \u22a4W hQlZl\u22121 ) ,\nZl := W2l \u00b7 ReLU(W1l \u00b7 Attnl(Zl\u22121) + b1l1\u22a4N ) + b2l1\u22a4N ,\nwhere we define Z0 = X . Here, H is the number of attention heads. The weight matrices for each head h \u2208 [H] in the l-th transformer block are W hOl,W hV l,W hKl,W hQl \u2208 RD\u00d7D. The softmax operator softmax(\u00b7) is applied column-wise to the matrix. Further, W2l,W1l \u2208 RD\u00d7D are the weight matrices and b1l, b2l \u2208 RD are the bias vectors in the feedforward layers. A Transformer network, denoted as TFNL,D, is a composition of L Transformer blocks, followed by an softmax output layer softmax(Wo \u00b7), where Wo \u2208 RD\u00d7D. The final output of the TFN is given by softmax(WoZL). To study the expressive power of LoRA within TFNs featuring multi-head attention layers, we next specify the parameters of the target model f , frozen model f0, and the adapted model f , each with L transformer blocks and a dimension D.\nTo study the expressive power of LoRA within TFNs featuring multi-head attention layers, we next specify the parameters of the target model f , frozen model f0, and the adapted model f , each with L transformer blocks and a dimension D. For ease of presentation, we drop the subscript in TFNL,D, referring to it simply as TFN. Given a specified rank R \u2208 [D] for LoRA, these models are defined as follows:\nTarget TFN f = TFN ( \u00b7; ( ((W h\nOl,W h V l,W h Kl,W h Ql) H h=1,W 2l,W 1l) L l=1,W o ) , (b1l, b2l) L l=1 ) ,\nFrozen TFN f0 = TFN ( \u00b7; ( ((W hOl,W h V l,W h Kl,W h Ql) H h=1,W2l,W1l) L l=1,Wo ) , (b1l, b2l) L l=1 ) ,\nAdapted TFN f = TFN ( \u00b7; ( ((W hOl +\u2206W h Ol,W h V l +\u2206W h V l,W h Kl +\u2206W h Kl,W h Ql +\u2206W h Ql) H h=1,\nW2l +\u2206W 2l,W1l +\u2206W 1l) L l=1,Wo +\u2206W o ) , (b\u03021l, b\u03022l) L l=1 ) ,\nwhere the weight matrices \u2208 RD\u00d7D, and the bias vectors \u2208 RD. Moreover, the weight matrices of the low-rank adapters \u2206W hOl,\u2206W h V l,\u2206W h Kl,\u2206W h Ql,\u2206W 2l,\u2206W 1l for all h \u2208 [H] and l \u2208 [L] are of rank R or lower.\nWe next introduce non-singularity Assumption 4 for TFN with multi-head attention layers scenarios, which is then validated by Lemma 14. We then provide proof of our main results for TFNs \u2014 Theorem 7. Additionally, we introduce a supplementary theorem that amalgamates results for TFNs with both single-head and multi-head attention layers when the weight matrices are randomly initialized. This is articulated in Corollary 10.\nAssumption 4 (Non-Singularity). For a fixed R \u2208 [D], all the weight matrices of both the target model and the frozen model and the following matrices for all r \u2208 [R],\nW h\u22a4Kl W h Ql + LRr\n( W h\u22a4 KlW h Ql \u2212W h\u22a4Kl W hQl ) , for all h \u2208 [H] and l = 1,\nW h\u22a4Kl W h Ql + LRr ( W\u22121\u22a42,l\u22121W \u22a4 2,l\u22121W h\u22a4 KlW h QlW 2,l\u22121W \u22121 2,l\u22121 \u2212W h\u22a4Kl W hQl ) , for all h \u2208 [H] and l \u2208 [L] \\ {1} ,\nW hOlW h V l + LRr ( W\u221211l W 1lW h OlW h V l \u2212W hOlW hV l ) , for all h \u2208 [H] and l = 1,\nW hOlW h V l + LRr ( W\u221211l W 1lW h OlW h V lW 2,l\u22121W \u22121 2,l\u22121 \u2212W hOlW hV l ) , for all h \u2208 [H] and l \u2208 [L] \\ {1} ,\nWoW2L + LRr(W oW 2L \u2212WoW2L), are non-singular.\nLemma 14. Let the elements of all weight matrices in the target model f and frozen model f0 be independently sampled from continuous distributions. Then, Assumption 4 holds with probability 1.\nProof of Lemma 14. The results can be obtained by replicating the same steps outlined in the proof of Lemma 3.\nFor the reader\u2019s reference, we restate Theorem 7 here integrated with the explicit formulation of the rank-based functionality gap Gi.\nTheorem 7. Consider a given LoRA-rank R \u2208 [D]. Let Assumption 4 hold. Define the rank-based functionality gap Gi to i-th transformer block (i \u2208 [L]) or output layer (i = L+ 1) as\nGi =  maxh ( rank(W h\u22a4 KiW h Qi \u2212W h\u22a4Ki W hQi) ) \u2228maxh ( rank(W 1iW h OiW h V i \u2212W1iW hOiW hV i) ) , i = 1, maxh ( rank(W \u22a4 2,i\u22121W h\u22a4 KiW h QiW 2,i\u22121 \u2212W\u22a42,i\u22121W h\u22a4Ki W hQiW2,i\u22121) ) \u2228maxh ( rank(W 1iW h OiW h V iW 2,i\u22121 \u2212W1iW hOiW hV iW2,i\u22121) ) , 2 \u2264 i \u2264 L,\nrank(W oW 2L \u2212WoW2L), i = L+ 1. (23)\nIf R \u2265 maxi\u2208[L+1]\u2308Gi2 \u2309, then there exists low-rank adapters with rank lower than R \u2208 [D] ((\u2206W hKl,\u2206W h Ql,\u2206W h V l,\u2206W h Ol) H h=1) L l=1,\u2206W 2L,\u2206W o with other low-rank adapters set to O, and updated bias vectors (b\u03021l, b\u03022l)Ll=1, such that for any X \u2208 RD\u00d7N , the adapted model f exactly approximates target model f , i.e., f(X) = f(X).\nProof of Theorem 7. The key idea of this proof is the same as the proof of Theorem 8: our first step is to ensure that, for each transformer block, the output from the first feedforward layer in the target model matches that in the adapted model. Once this is established, we select an appropriate output layer weight matrix to complete the proof.\nSimilar to the proof of Theorem 8, we define H l \u2208 RD\u00d7N and Zl \u2208 RD\u00d7N as the intermediate and final outputs of the l-th transformer block in the target model f , respectively. In particular, H l corresponds to the output of the first feedforward layer in the l-th transformer block. They are formulated as\nH l = ReLU ( W 1l ( H\u2211\nh=1\nW h OlW h V l \u00b7Zl\u22121 \u00b7 softmax ( Z \u22a4 l\u22121W h\u22a4 KlW h QlZl\u22121 )) + b1l1 \u22a4 N ) ,\nZl = W 2lH l + b2l1 \u22a4 N .\nFor the adapted model f , we introduce H\u0302l and Z\u0302l accordingly to denote the intermediate output of the first feedforward layer and the final output of the l-th transformer block for the adapted model, respectively:\nH\u0302l = ReLU ( W1l ( H\u2211 h=1 (W hOl +\u2206W h Ol)(W h V l +\u2206W h V l) \u00b7 Z\u0302l\u22121\n\u00b7 softmax ( Z\u0302\u22a4l\u22121(W h Kl +\u2206W h Kl) \u22a4(W hQl +\u2206W h Ql)Z\u0302l\u22121 )) + b\u03021l1 \u22a4 N ) ,\nZ\u0302l = W2lH\u0302l + b\u03022l1 \u22a4 N .\nNote that Z0 = Z\u03020 = X .\nWe aim to demonstrate that adding low-rank adapters to the weight matrices allows the adapted TFN f to be functionally equivalent to the target TFN of identical dimensions. We will initiate our proof by inductively constructing the adapter weight matrices ((\u2206W hOl,\u2206W h V l,\u2206W h Kl,\u2206W h Ql) H h=1, b\u03021l, b\u03022l) L l=1 such that H\u0302l = H l for all l \u2208 [L], and then select the \u2206W 2L and the low-rank adapter for the output layer \u2206W o to approximate the output of the target model. For unmentioned low-rank adapters, we set them as O.\nWhen l = 1. To achieve H\u0302l with H l for all X , we must satisfy the following conditions:\nBias Vector: b\u03021l = b1l, (24)\nQuery and Key: (W hKl +\u2206W h Kl) \u22a4(W hQl +\u2206W h Ql) = W h\u22a4 KlW h Ql,\nValue and Output Projection: (W hOl +\u2206W h Ol)(W h V l +\u2206W h V l) = W \u22121 1l W 1lW\nh OlW h V l.\nTo achieve this, we set b\u03021l = b1l to achieve (24), and select rank-R or lower matrices \u2206W hKl,\u2206W h Ql,\u2206W h Ol,\u2206W h V l for all h \u2208 [H] as suggested by Lemma 7. This ensures H\u0302l = H l for l = 1.\nWhen l > 1. Now we focus on the cases where l = 2, . . . , L. Assume the induction hypothesis holds for l \u2212 1, which is H\u0302l\u22121 = H l\u22121. Following the same steps in the proof of Theorem 8, we let b\u03022,l\u22121 = W2,l\u22121W \u22121 2,l\u22121b2,l\u22121, thereby obtaining,\nZ\u0302l\u22121 = W2,l\u22121W \u22121 2,l\u22121Zl\u22121. (25)\nTo achieve H\u0302l = H l, we express both H\u0302l and H l in terms of Zl\u22121:\nH l = ReLU ( W 1l ( H\u2211 h=1 W h OlW h V l \u00b7Zl\u22121 \u00b7 softmax ( Z \u22a4 l\u22121W h\u22a4 KlW h QlZl\u22121 )) + b1l1 \u22a4 N ) H\u0302l = ReLU ( W1l\n( H\u2211 h=1 (W hOl +\u2206W h Ol)(W h V l +\u2206W h V l) \u00b7 Z\u0302l\u22121\n\u00b7 softmax ( Z\u0302\u22a4l\u22121(W h Kl +\u2206W h Kl) \u22a4(W hQl +\u2206W h Ql)Z\u0302l\u22121 )) + b\u03021l1 \u22a4 N ) ,\n(25) = ReLU ( W1l ( H\u2211 h=1 (W hOl +\u2206W h Ol)(W h V l +\u2206W h V l) \u00b7W2,l\u22121W \u22121 2,l\u22121Zl\u22121\n\u00b7 softmax ( Z \u22a4 l\u22121W \u22121\u22a4 2,l\u22121W \u22a4 2,l\u22121(W h Kl +\u2206W h Kl) \u22a4\n(W hQl +\u2206W h Ql)W2,l\u22121W \u22121 2,l\u22121Zl\u22121 )) + b\u03021l1 \u22a4 N ) .\nTherefore, we need to align the following three components:\nBias Vector: b\u03021l = b1l, (26)\nQuery and Key: (W hKl +\u2206W h Kl) \u22a4(W hQl +\u2206W h Ql) = W \u22121\u22a4 2,l\u22121W \u22a4 2,l\u22121W h\u22a4 KlW h QlW 2,l\u22121W \u22121 2,l\u22121,\nValue and Output Projection:\n(W hOl +\u2206W h Ol)(W h V l +\u2206W h V l) = W \u22121 1l W 1lW\nh OlW h V lW 2,l\u22121W \u22121 2,l\u22121.\nBy setting b\u03021l based on (26) and adjusting \u2206W hKl,\u2206W h Ql,\u2206W h Ol,\u2206W h V l for all h \u2208 [H] based on Lemma 7, we satisfy all three conditions above, thereby obtaining H\u0302l = H l for l \u2208 [L] \\ {1}.\nOutput Layer Analysis. By applying the induction method, we have established H\u0302l = H l for all l \u2208 [L]. Lastly, we choose the \u2206W o, \u2206W 2L and the bias vector b\u03022L using the same approach as in the proof of Theorem 8. This concludes the proof.\nThe following corollary identifies the specific LoRA-rank required to achieve exact representation for random model cases in the current setting. Corollary 10. Assume that the elements of all the weight matrices of both the target TFN and the frozen TFN are independently drawn from arbitrary continuous distributions. If R \u2265 \u2308D2 \u2309, adding low-rank adapters of rank at most R to weight matrices in ((\u2206W hKl,\u2206W h Ql,\u2206W h V l,\u2206W h Ol) H h=1) L l=1,\u2206W 2L,\u2206W o and tuning the bias vectors, enables the adapted model f to exactly approximate the target model f , i.e., f(X) = f(X) for all X \u2208 RD\u00d7N .\nProof of Corollary 10. By combining Lemma 14 and Theorem 7, and following the same steps in the proof of Corollary 4 which yields maxi Gi = D, we can obtain the desired outcome."
        },
        {
            "heading": "G EXTENDED NUMERICAL EXPERIMENTS",
            "text": "In this section, we have provided additional experimental details and conducted more comprehensive experiments on FNNs and TFNs."
        },
        {
            "heading": "G.1 ADDITIONAL DETAILS ON GRADIENT UPDATE METHOD",
            "text": "In our experiments, we utilize the Adam optimizer. We tune the learning rate \u2208 { 10\u22122, 10\u22123, 10\u22124 } and the weight decay \u2208 { 0, 10\u22122, 10\u22123, 10\u22124 } . The optimal configuration is determined based on the validation loss on a set of 256 samples independently drawn from a standard normal distribution. We run 5000 iterations for each hyperparameter setting, where at each step 256 fresh standard Gaussian samples are generated for loss and gradient computation.\nG.2 VALIDATION OF OUR LORA ADAPTER CONSTRUCTION"
        },
        {
            "heading": "G.2.1 FNN APPROXIMATION",
            "text": "In this experiment, we assess the effectiveness of our lowrank adapter construction for FNN approximation, which is detailed in the proof of Theorem 5.\nSetup. The setup and model generation here mirrors that of Sec. 5, with the addition of a case where L = 1 and L = 2, alongside the existing setting of L = 2 and L = 4. It should be noted that for both these cases, we have M = \u230aL/L\u230b = 2 here. We employ the gradient update method and the construction outlined in the proof of Theorem 5 to update the LoRA adapters.\nResults. Fig. 3 presents the results for FNN approximation. Consistent with the implications drawn in Sec. 5, the y limit changes from Fig. 3a to Fig. 3b suggest that the pretrained frozen model results in less approximation error. Additionally, we observe that our construction\u2019s performance aligns closely with the gradient update method when the target model depth L = 1. However, this alignment is not observed when L = 2 on low-rank region (i.e., R \u2264 4), This further underscores the limitation of our LoRA adapter construction, which inherently assumes that the intermediate outputs of the frozen model and the target model need to align.\nTo facilitate a more effective comparison between our construction and the gradient update method in the higher-rank region (i.e., R \u2265 6), we present the curves on a logarithmic scale, as depicted in Fig. 4. While the gradient update appears to reach the optimal performance achieved by our LoRA construction in FNNs, a gap is still discernible when viewed on a logarithmic scale. The MSE of the gradient update method is approximately 10\u22124, while for our LoRA construction, it\u2019s around 10\u22128 for a sufficiently large rank."
        },
        {
            "heading": "G.2.2 TFN APPROXIMATION",
            "text": "We assess the effectiveness of our LoRA adapter construction in approximating TFN, as detailed in the proof of Theorem 7.\nSetup. We examine target model f and frozen model f , both featuring the same architecture with L transformer blocks, a single output layer, two attention heads, and embedding size D = 16. We\nfocus on two scenarios: L = 1 and L = 2. The weight matrices for the attention layers follow a standard Gaussian distribution, while those for the linear layers are initialized using the Xavier uniform distribution, which is PyTorch\u2019s default scheme for linear layer initialization.\nResults. The observations here align with those from the experiments of FNN approximation. We note that the gradient update method outperforms our approach when the rank is relatively small but lags behind as the rank increases. This advantage of the gradient update method at minimal ranks arises from the inherent complexity of TFNs, which allows for more flexible low-rank adapter construction. Meanwhile, the gradient update method\u2019s performance does not significantly improve as the rank increases. This arises from the inherent complexity involved in optimizing TFNs. Nonetheless, our results corroborate the claims made in Theorem 7, as the approximation error must be eradicated when the rank reaches \u2308D2 \u2309 = 8.\nG.3 COMPARISON TO TUNING FINAL LAYERS\nTuning or adding the final layers only is also a common adaptation method used in various domains, including computer vision (Chatfield et al., 2014; Donahue et al., 2014; Sharif Razavian et al., 2014), and natural language processing (Devlin et al., 2019; Gira et al., 2022). Recall that Corollary 4 and Lemma 12 demonstrate that tuning final layers does not perform as well as LoRA for randomly generated models, provided the LoRA-rank satisfies the rank constraints shown in Corollary 4. In this experiment, we aim to validate this assertion and compare the performance of tuning final layers and LoRA in more general scenarios, such as when the frozen model has been pretrained, and when the LoRA-rank is smaller than required.\nSetup. The model generation in this experiment follows the same procedure as in Sec. 5. We consider FNN models with D = 16, L = 1, L = 8. In this experiment, we employ two baselines: LoRA and tuning final layers. The LoRA adapters and the final layers are updated using the gradient update method.\nResults. Figure 6a compares the MSE of LoRA and final layer tuning when the same number of tunable parameters are used. In the case of randomly generated models, we observe that final layer tuning yields a significantly higher MSE when using the same number of tunable parameters, corroborating our results in Lemma 12. However, when the frozen model has been pretrained, the performance of final layer tuning improves considerably, though it still falls short of LoRA. This aligns with conclusions drawn from previous theoretical studies such as Tripuraneni et al. (2020), which asserts that the performance of final layer tuning heavily depends on the quality of the shared representations."
        },
        {
            "heading": "G.4 BENEFITS OF TUNING BIASES",
            "text": "In our proof, as detailed in Sec. 3.2 and E.1, the updatable biases in the FNN play a crucial role in eliminating the nonlinearity of ReLUs. In this experiment, we investigate the importance of updatable biases in ensuring the success of LoRA in FNN cases.\nSetup. The model generation for this experiment adheres to the same procedure outlined in Sec. 5. We consider FNN models with parameters D = 16, L = 1, L = 2, and examine the performance of LoRA both with and without biases tuning for adapting it to match the target FNN. The LoRA adapters and biases are updated using the gradient update method.\nResults. The performance of LoRA with and without updatable biases is presented in Figure 6b. We observe that in both random and pretrained model cases, LoRA with updatable biases outperforms LoRA with fixed biases when the number of tunable parameters is relatively small. However, the performance gap is not significant and diminishes as the number of tunable parameters increases. This suggests that while tuning biases in conjunction with the low-rank adapters does enhance performance, the gain is not substantial. In other words, even without bias tuning, LoRA\u2019s performance remains competitive.\nG.5 TRAINING CURVES\nAlthough our theoretical study does not incorporate any training process, we present the training curves of the LoRA gradient update method to illuminate the optimization aspects of LoRA.\nSetup We depict the training curves of LoRA finetuning on randomly generated FNNs for R = 1, 4, 8, 13, 16. Unless stated otherwise, all settings strictly adhere to the FNN experiments described in Sec. 5.\nResults The training curves visualized in Fig. 7 reveal that models with smaller ranks (e.g., R=1,4) converge swiftly due to their limited search space, but they settle at a relatively high training loss. Medium rank models (e.g., R=8) converge more slowly. Highly overparameterized models (g, R=13,18) appear to converge faster, aligning\nwith recent advancements in optimization theory, which suggest that overparameterized models are easier to optimize (Liu et al., 2022a)."
        },
        {
            "heading": "G.6 GENERALIZATION PERFORMANCES",
            "text": "While our theoretical study only establishes the upper bound of LoRA\u2019s performance with infinite data samples, it does not consider LoRA\u2019s generalization performance in practice. Although this is beyond the current scope of our paper, we empirically investigate LoRA\u2019s generalization performance in this experiment.\nSetup. This experiment adheres to the FNN experiment settings in Sec. 5, but now includes a training set of 400 samples for the cases where L = 1, L = 2, and 800 training samples for the cases where L = 2, L = 4. We evaluate how well LoRA\u2019s training performance transfers to the test set.\nResults. Fig. 8 presents the training and test MSE versus LoRA-ranks. However, no clear pattern is observed in the variation of the gap between the training and test MSE with respect to the LoRA-ranks. This could be due to Adam not precisely finding the minimum (see Fig. 4), potentially avoiding overfitting.\nTo assess LoRA\u2019s generalization performance, we finetuned the frozen model on the training set and reported the training and test MSE. We notice an increasing generalization gap (test MSE - train MSE) as the LoRA rank\nincreases \u2013 this is very evident with L=2, and less so with L=4. This is intuitive as larger LoRA ranks imply a larger hypothesis class (e.g., the Rademacher complexity), so it is expected. We defer a detailed analysis of LoRA\u2019s generalization performance to future work but believe our simulation results provide a valuable starting point for further discussion and investigation."
        },
        {
            "heading": "G.7 EVALUATION ON CLASSIFICATION TASKS",
            "text": "Our theory and previous experiments all focus on regression cases. In this experiment, we consider binary and multi-class classification tasks and optimize the LoRA adapter vias cross entropy, and report the performance of LoRA using accuracy.\nMulti-class Classification. We follow exactly the same setting as the randomly initialized FNN experiments in Sec 5, but replace the objective function with cross enreopy and report the accuracy rather than MSE.\nAs shown in Fig. 9a, consistent with our theoretical results, our construction achieves 100% accuracy when R \u2265 8. The performance of gradient update is also similar to our observation when MSE is employed as the metric, particularly when MSE is plotted on a logarithmic scale (Fig. 4). This observation echoes the findings of Hui & Belkin (2021), which indicate that optimizing MSE is fundamentally equivalent to optimizing cross-entropy.\nBinary Classification. We have conducted binary classification tasks. We use the same setup as before but add one more output layer \u2208 R2\u00d7D which is a block diagonal matrix, with the first 8 elements in the first rows and the last 8 elements in the second row are 1 and all remaining elements are 0. We fix this output layer, optimize the cross entropy on the LoRA adapters, and report the test accuracy.\nAs shown in Fig. 9b, we observe that in this binary classification scenario, even with a very low LoRA-rank R = 1, the accuracy has been significantly improved, comparable to the results achieved by higher ranks. In the region of higher ranks, our construction significantly outperforms the gradi-\nent update method. The suboptimal performance of the gradient update method in this simulation suggests that, despite LoRA\u2019s current impressive performance in practical applications, there is potential for further refinement."
        },
        {
            "heading": "G.8 EVALUATION ON REAL DATASETS",
            "text": "In our theoretical analysis, we demonstrate how the sizes of frozen models and the distance between the frozen and target models influence the necessary LoRA-ranks to achieve the desired performance (see Lemma 1, 2, and Theorem 5, 6, 7). Specifically, our results suggest that larger models require fewer LoRA-ranks to reach the desired performance. Similarly, when the frozen model is closer to the target model, a lower LoRA-rank is sufficient to achieve the same performance. We validate these theoretical insights through experiments on the GLUE benchmark (Wang et al., 2018).\nSetup Our experiments are conducted using Tesla V100-PCIE-16GB, NVIDIA A100-SXM480GB, NVIDIA A100-SXM4-40GB, and NVIDIA L40 GPUs. For each run, a single GPU is utilized. Unless otherwise specified, all our settings align with those established by Hu et al. (2022a).\nImpact of Model Size on LoRA Rank In practice, most existing studies on LoRA use the same LoRA-rank for models of varying sizes. For instance, in the original LoRA paper (Hu et al., 2022a), Tables 9 and 10 demonstrate the use of the same LoRA-rank for RoBERTa-base (Liu et al., 2019), RoBERTa-large (Liu et al., 2019), and DeBERTa-XXL (He et al., 2021). Similarly, in the QLoRA paper (Dettmers et al., 2023), a LoRA-rank of 64 is set for different models ranging from 13B to 65B parameters (see their Appendix B.2). To validate our theoretical findings, we evaluated the performance of LoRA on models of different sizes, specifically RoBERTa-base with 110M parameters and RoBERTa-large with 340M parameters. The results are presented in Table 1.\nInitially, we observe that, in the absence of fine-tuning (LoRA-rank R = 0), there is no consistent trend \u2013 RoBERTa-base performs better on 3 datasets, while RoBERTa-large performs better on 4 datasets. However, after LoRA fine-tuning, we observe that RoBERTa-large outperforms in most cases. In fact, even when the base model is trained with a LoRA-rank three times larger, RoBERTalarge still performs better on 6 out 8 datasets. Given that the pretrained RoBERTa-large model was performing no differently from the base model, this observation supports our theoretical findings that deeper models are more expressive with LoRA training.\nImpact of Model Proximity on LoRA Rank While our theoretical results (Lemma 1, 2, and Theorem 5, 6, 7) imply that the frozen model that is closer to the target model achieves better results for a fixed LoRA-rank. To validate this, we compare the performance of pretrained RoBERTa-base with the randomly initialized RoBERTa-base fine-tuned using the same LoRA-ranks.\nThe results in Table 2 demonstrate that the pretrained RoBERTa-base significantly surpasses the randomly initialized RoBERTa-base. This observation is consistent with our theoretical findings, suggesting that the pretrained model requires lower LoRA-ranks to achieve the desired performance."
        },
        {
            "heading": "H BRIDGING THEORETICAL INSIGHTS AND EMPIRICAL OBSERVATIONS",
            "text": "In this study, we conduct a theoretical analysis of the expressive power of LoRA. Our findings provide valuable insights for future development of the LoRA algorithm and explain its practical efficacy. Table 3 summarizes the alignment of our theoretical insights with empirical observations."
        },
        {
            "heading": "Findings Theoretical Insights Empirical Observation",
            "text": ""
        },
        {
            "heading": "I EXTENSION TO CASES WITH DIFFERENT MODEL DIMENSIONS",
            "text": "This discussion only applies to linear model approximation and FNN approximation. As highlighted in Sec. 2, our results can be easily extended to scenarios where the target model, f , and the frozen model, f , have different model dimensions. Specifically, for linear model or FNN approximation, we use D to represent the number of hidden neurons per layer in the target model and D for the frozen model. We particularly consider the cases where the frozen model is wider than the target model, i.e., D \u2265 D. This is because the frozen model is typically overparameterized in practical applications.\nThe key idea for extending our analysis to scenarios with different model dimensions is expanding the dimension of the target model. For the sake of simplicity, we focus on the simplest case, the linear model approximation, as an example. In this setting, the difference between the output of the adapted model and the target model can be measured by\nf ([ x 0 ]) \u2212 [ f(x) 0 ] = L\u220f l=1 (Wl +\u2206W l) [ x 0 ] \u2212 [ Wx 0 ] , (27)\nwhere x \u2208 RD. Consequently, the last (D \u2212 D) columns and rows of \u220fLl=1 (Wl +\u2206W l) does not affect the results at all. Denote the submatrix consisting of the first d rows and d columns of a matrix W by [W ]d. Then, to approximate the target model, we aim to solve the following constrained optimization problem for a given LoRA-rank R \u2208 [D]:\nmin rank(\u2206W l)\u2264R \u2225\u2225\u2225\u2225\u2225 [ L\u220f l=1 (Wl +\u2206W l) ] D \u2212W \u2225\u2225\u2225\u2225\u2225 F .\nTo solve this problem, we first define an expanded target matrix, denoted by W\u0303 \u2208 RD\u00d7D. The expanded target matrix W\u0303 is constructed such that [ W\u0303 ] D = W , while the remaining entries\nmatches the corresponding entries in \u220f l = 1LWl. Then, the error matrix E = W\u0303 \u2212 \u220fL\nl=1 Wl, consists entirely of zeros except for the first D rows and D columns. Therefore, we obtain RE = rank(E) \u2264 D. Given the expanded target matrix, we consider the updated constrained optimization problem as follows:\nmin rank(\u2206W l)\u2264R \u2225\u2225\u2225\u2225\u2225 L\u220f\nl=1\n(Wl +\u2206W l)\u2212 W\u0303 \u2225\u2225\u2225\u2225\u2225\nF\n. (28)\nBy Lemma 1, we obtain that when the LoRA-rank R \u2265 \u230aDL \u230b, the optimal solution to (28) satisfies\u220fL l=1 (Wl +\u2206W l) = W\u0303 , given that D \u2265 RE . This result implies that [\u220fL l=1 (Wl +\u2206W l) ] D = W and therefore the approximation error defined in (27) is 0 for all input x.\nA similar analysis can be conducted for FNN approximation."
        },
        {
            "heading": "J OTHER FUTURE WORKS",
            "text": "In addition to the potential future directions mentioned by Sec. 6, there are any more interesting future directions to explore. In this paper, for TFN, we constrain the target model and the frozen model to have identical embedding size and depth, and we omit the skip connections and layer norms for simplicity. Another intriguing direction would be to study the expressive power of LoRA under TFN cases with more general settings on TFN architectures. While our theoretical analysis does not involve any training process, an interesting direction for future research would be to consider gradient-based optimization algorithms and theoretically examine how efficiently LoRA can be optimized. Finally, theoretical questions about LoRA\u2019s generalization to unseen data also remain unresolved."
        }
    ],
    "year": 2023
}