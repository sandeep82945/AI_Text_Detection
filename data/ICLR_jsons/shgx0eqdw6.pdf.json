{
    "abstractText": "Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model\u2019s probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedybased decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing decoding-time alignment, paves the way for more responsive language models in the future. Code is publicly available at: https://github.com/deeplearning-wisc/args.",
    "authors": [
        {
            "affiliations": [],
            "name": "REWARD-GUIDED SEARCH"
        },
        {
            "affiliations": [],
            "name": "Maxim Khanov"
        },
        {
            "affiliations": [],
            "name": "Jirayu Burapacheep"
        },
        {
            "affiliations": [],
            "name": "Yixuan Li"
        }
    ],
    "id": "SP:7407d8d61701f3971600b30387cfd3bf29373d03",
    "references": [
        {
            "authors": [
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Dawn Drain",
                "Deep Ganguli",
                "Tom Henighan",
                "Andy Jones",
                "Nicholas Joseph",
                "Ben Mann",
                "Nova DasSarma",
                "Nelson Elhage",
                "Zac Hatfield-Dodds",
                "Danny Hernandez",
                "Jackson Kernion",
                "Kamal Ndousse",
                "Catherine Olsson",
                "Dario Amodei",
                "Tom Brown",
                "Jack Clark",
                "Sam McCandlish",
                "Chris Olah",
                "Jared Kaplan"
            ],
            "title": "A general language assistant as a laboratory for alignment",
            "venue": "arXiv preprint arXiv:2112.00861,",
            "year": 2021
        },
        {
            "authors": [
                "Sourya Basu",
                "Nitish Shirish Keskar",
                "Lav R. Varshney"
            ],
            "title": "Mirostat: a neural text decoding algorithm that directly controls perplexity",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Anca Dragan",
                "David Krueger",
                "Dorsa Sadigh",
                "Dylan Hadfield-Menell"
            ],
            "title": "Open problems and fundamental limitations of reinforcement learning from human feedback",
            "venue": "arXiv preprint arXiv:2307.15217,",
            "year": 2023
        },
        {
            "authors": [
                "Antoine Chaffin",
                "Vincent Claveau",
                "Ewa Kijak"
            ],
            "title": "PPL-MCTS: constrained textual generation through discriminator-guided MCTS decoding",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Andrea Madotto",
                "Janice Lan",
                "Jane Hung",
                "Eric Frank",
                "Piero Molino",
                "Jason Yosinski",
                "Rosanne Liu"
            ],
            "title": "Plug and play language models: A simple approach to controlled text generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Haikang Deng",
                "Colin Raffel"
            ],
            "title": "Reward-augmented decoding: Efficient controlled text generation with a unidirectional reward model",
            "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Ameet Deshpande",
                "Vishvak Murahari",
                "Tanmay Rajpurohit",
                "Ashwin Kalyan",
                "Karthik Narasimhan"
            ],
            "title": "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "venue": "arXiv preprint arXiv:2304.05335,",
            "year": 2023
        },
        {
            "authors": [
                "Shizhe Diao",
                "Rui Pan",
                "Hanze Dong",
                "Ka Shun Shum",
                "Jipeng Zhang",
                "Wei Xiong",
                "Tong Zhang"
            ],
            "title": "Lmflow: An extensible toolkit for finetuning and inference of large foundation models",
            "venue": "arXiv preprint arXiv:2306.12420,",
            "year": 2023
        },
        {
            "authors": [
                "Hanze Dong",
                "Wei Xiong",
                "Deepanshu Goyal",
                "Rui Pan",
                "Shizhe Diao",
                "Jipeng Zhang",
                "Kashun Shum",
                "Tong Zhang"
            ],
            "title": "Raft: Reward ranked finetuning for generative foundation model alignment",
            "venue": "arXiv preprint arXiv:2304.06767,",
            "year": 2023
        },
        {
            "authors": [
                "Kawin Ethayarajh",
                "Yejin Choi",
                "Swabha Swayamdipta"
            ],
            "title": "Understanding dataset difficulty with V-usable information",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin"
            ],
            "title": "Hierarchical neural story generation",
            "venue": "In Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith"
            ],
            "title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "venue": "In Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Isaac",
                "John Mellor",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Lisa Anne Hendricks",
                "Geoffrey Irving"
            ],
            "title": "Improving alignment of dialogue agents via targeted human judgements",
            "venue": "arXiv preprint arXiv:2209.14375,",
            "year": 2022
        },
        {
            "authors": [
                "Dongyoung Go",
                "Tomasz Korbak",
                "Germ\u00e1n Kruszewski",
                "Jos Rozen",
                "Nahyeon Ryu",
                "Marc Dymetman"
            ],
            "title": "Aligning foundation models for language with preferences through $f$-divergence minimization",
            "venue": "In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Henderson",
                "Riashat Islam",
                "Philip Bachman",
                "Joelle Pineau",
                "Doina Precup",
                "David Meger"
            ],
            "title": "Deep reinforcement learning that matters",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Daphne Ippolito",
                "Reno Kriz",
                "Maria Kustikova",
                "Jo\u00e3o Sedoc",
                "Chris Callison-Burch"
            ],
            "title": "Comparison of diverse decoding methods from conditional language models",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jean Kaddour",
                "Joshua Harris",
                "Maximilian Mozes",
                "Herbie Bradley",
                "Roberta Raileanu",
                "Robert McHardy"
            ],
            "title": "Challenges and applications of large language models",
            "venue": "arXiv preprint arXiv:2307.10169,",
            "year": 2023
        },
        {
            "authors": [
                "Muhammad Khalifa",
                "Lajanugen Logeswaran",
                "Moontae Lee",
                "Honglak Lee",
                "Lu Wang"
            ],
            "title": "Grace: Discriminator-guided chain-of-thought reasoning. 2023",
            "venue": "URL https://openreview.net/ forum?id=2MiTZxLFA9",
            "year": 2023
        },
        {
            "authors": [
                "Ben Krause",
                "Akhilesh Deepak Gotmare",
                "Bryan McCann",
                "Nitish Shirish Keskar",
                "Shafiq Joty",
                "Richard Socher",
                "Nazneen Fatema Rajani"
            ],
            "title": "GeDi: Generative discriminator guided sequence generation",
            "venue": "In Findings of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Yapei Chang",
                "John Wieting",
                "Mohit Iyyer"
            ],
            "title": "Rankgen: Improving text generation with large ranking models",
            "venue": "In Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Kimin Lee",
                "Laura Smith",
                "Pieter Abbeel"
            ],
            "title": "Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Ari Holtzman",
                "Daniel Fried",
                "Percy Liang",
                "Jason Eisner",
                "Tatsunori Hashimoto",
                "Luke Zettlemoyer",
                "Mike Lewis"
            ],
            "title": "Contrastive decoding: Open-ended text generation as optimization",
            "venue": "In Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Yifei Li",
                "Zeqi Lin",
                "Shizhuo Zhang",
                "Qiang Fu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Weizhu Chen"
            ],
            "title": "Making language models better reasoners with step-aware verifier",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Hunter Lightman",
                "Vineet Kosaraju",
                "Yura Burda",
                "Harrison Edwards",
                "Bowen Baker",
                "Teddy Lee",
                "Jan Leike",
                "John Schulman",
                "Ilya Sutskever",
                "Karl Cobbe"
            ],
            "title": "Let\u2019s verify step by step",
            "venue": "ArXiv, abs/2305.20050,",
            "year": 2023
        },
        {
            "authors": [
                "Alisa Liu",
                "Maarten Sap",
                "Ximing Lu",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Noah A. Smith",
                "Yejin Choi"
            ],
            "title": "Dexperts: Decoding-time controlled text generation with experts and anti-experts",
            "venue": "In Proceedings of Annual Meeting of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Liu",
                "Carmelo Sferrazza",
                "Pieter Abbeel"
            ],
            "title": "Chain of hindsight aligns language models with feedback",
            "venue": "arXiv preprint arXiv:2302.02676,",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Ximing Lu",
                "Peter West",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi"
            ],
            "title": "NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders",
                "Xu Jiang",
                "Karl Cobbe",
                "Tyna Eloundou",
                "Gretchen Krueger",
                "Kevin Button",
                "Matthew Knight",
                "Benjamin Chess",
                "John Schulman"
            ],
            "title": "Webgpt: Browser-assisted question-answering with human feedback",
            "venue": "arXiv preprint arXiv:2112.09332,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Ngo",
                "Lawrence Chan",
                "S\u00f6ren"
            ],
            "title": "Mindermann. The alignment problem from a deep learning perspective",
            "venue": "arXiv preprint arXiv:2209.00626,",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray",
                "John Schulman",
                "Jacob Hilton",
                "Fraser Kelton",
                "Luke Miller",
                "Maddie Simens",
                "Amanda Askell",
                "Peter Welinder",
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rafael Rafailov",
                "Archit Sharma",
                "Eric Mitchell",
                "Stefano Ermon",
                "Christopher D. Manning",
                "Chelsea Finn"
            ],
            "title": "Direct preference optimization: Your language model is secretly a reward model",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Rasley",
                "Samyam Rajbhandari",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
            "venue": "In ACM SIGKDD,",
            "year": 2020
        },
        {
            "authors": [
                "Steven J. Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel"
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "Charlie Snell",
                "Ilya Kostrikov",
                "Yi Su",
                "Mengjiao Yang",
                "Sergey Levine"
            ],
            "title": "Offline rl for natural language generation with implicit language q learning",
            "venue": "arXiv preprint arXiv:2206.11871,",
            "year": 2023
        },
        {
            "authors": [
                "Feifan Song",
                "Bowen Yu",
                "Minghao Li",
                "Haiyang Yu",
                "Fei Huang",
                "Yongbin Li",
                "Houfeng Wang"
            ],
            "title": "Preference ranking optimization for human alignment",
            "venue": "arXiv preprint arXiv:2306.17492,",
            "year": 2023
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano"
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier"
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aurelien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Uesato",
                "Nate Kushman",
                "Ramana Kumar",
                "Francis Song",
                "Noah Siegel",
                "Lisa Wang",
                "Antonia Creswell",
                "Geoffrey Irving",
                "Irina Higgins"
            ],
            "title": "Solving math word problems with process- and outcome-based feedback, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yufei Wang",
                "Wanjun Zhong",
                "Liangyou Li",
                "Fei Mi",
                "Xingshan Zeng",
                "Wenyong Huang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu"
            ],
            "title": "Aligning large language models with human: A survey",
            "venue": "arXiv preprint arXiv:2307.12966,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler",
                "Ed H. Chi",
                "Tatsunori Hashimoto",
                "Oriol Vinyals",
                "Percy Liang",
                "Jeff Dean",
                "William Fedus"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Laura Weidinger",
                "John Mellor",
                "Maribeth Rauh",
                "Conor Griffin",
                "Jonathan Uesato",
                "Po-Sen Huang",
                "Myra Cheng",
                "Mia Glaese",
                "Borja Balle",
                "Atoosa Kasirzadeh",
                "Zac Kenton",
                "Sasha Brown",
                "Will Hawkins",
                "Tom Stepleton",
                "Courtney Biles",
                "Abeba Birhane",
                "Julia Haas",
                "Laura Rimell",
                "Lisa Anne Hendricks",
                "William Isaac",
                "Sean Legassick",
                "Geoffrey Irving",
                "Iason Gabriel"
            ],
            "title": "Ethical and social risks of harm from language models",
            "venue": "arXiv preprint arXiv:2112.04359,",
            "year": 2021
        },
        {
            "authors": [
                "Sean Welleck",
                "Jiacheng Liu",
                "Ximing Lu",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Naturalprover: Grounded mathematical proof generation with language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yushi Hu",
                "Weijia Shi",
                "Nouha Dziri",
                "Alane Suhr",
                "Prithviraj Ammanabrolu",
                "Noah A. Smith",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi"
            ],
            "title": "Fine-grained human feedback gives better rewards for language model training",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Yuxi Xie",
                "Kenji Kawaguchi",
                "Yiran Zhao",
                "Xu Zhao",
                "Min-Yen Kan",
                "Junxian He",
                "Qizhe Xie"
            ],
            "title": "Decomposition enhances reasoning via self-evaluation guided decoding, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Yang",
                "Dan Klein"
            ],
            "title": "FUDGE: Controlled text generation with future discriminators",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L. Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan"
            ],
            "title": "Tree of Thoughts: Deliberate problem solving with large language",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Yuan",
                "Hongyi Yuan",
                "Chuanqi Tan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang"
            ],
            "title": "Rrhf: Rank responses to align language models with human feedback without tears",
            "venue": "arXiv preprint arXiv:2304.05302,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "venue": "In Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2023
        },
        {
            "authors": [
                "Rui Zheng",
                "Shihan Dou",
                "Songyang Gao",
                "Yuan Hua",
                "Wei Shen",
                "Binghai Wang",
                "Yan Liu",
                "Senjie Jin",
                "Qin Liu",
                "Yuhao Zhou",
                "Limao Xiong",
                "Lu Chen",
                "Zhiheng Xi",
                "Nuo Xu",
                "Wenbin Lai",
                "Minghao Zhu",
                "Cheng Chang",
                "Zhangyue Yin",
                "Rongxiang Weng",
                "Wensen Cheng",
                "Haoran Huang",
                "Tianxiang Sun",
                "Hang Yan",
                "Tao Gui",
                "Qi Zhang",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Secrets of rlhf in large language models part i: Ppo",
            "venue": "arXiv preprint arXiv:2307.04964,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel M. Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B. Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving"
            ],
            "title": "Fine-tuning language models from human",
            "year": 1909
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) trained on massive datasets exhibit a remarkable ability to handle a wide array of tasks (Wei et al., 2022; Kaddour et al., 2023). However, due to the varied nature of their training data, these models can inadvertently generate misinformation and harmful outputs (Gehman et al., 2020; Weidinger et al., 2021; Deshpande et al., 2023). This concern underscores the urgent challenge of language model alignment: ensuring these models\u2019 behaviors agree with human objectives and safety considerations (Ngo et al., 2023; Casper et al., 2023).\nIn recent years, a spectrum of alignment strategies have emerged, with prominent methods showcasing the effectiveness of reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). RLHF has gained widespread adoption among state-of-the-art models, including OpenAI\u2019s GPT-4 (OpenAI, 2023), Anthropic\u2019s Claude (Anthropic, 2023), Google\u2019s Bard (Google, 2023), and Meta\u2019s Llama 2-Chat (Touvron et al., 2023b). A pivotal component within RLHF is proximal policy optimization (PPO), which employs an external reward model that mirrors human preferences for its optimization process. However, as noted in previous studies (Henderson et al., 2017; Wang et al., 2023; Rafailov et al., 2023; Zheng et al., 2023b), implementing PPO introduces challenges of unstable and costly training. Furthermore, the need to repeat PPO training when altering the reward model hinders rapid customization to evolving datasets and emerging needs.\nTo address the aforementioned challenge, we introduce Alignment as Reward-Guided Search, or ARGS, a novel framework designed to enhance the alignment of generated text with human-desired preferences. ARGS achieves this by employing a reward mechanism that directly guides the text generation process of a language model. Unlike traditional alignment approaches, our method integrates alignment into the decoding process, enabling quick realignments without having to go through the exhaustive process of retraining the foundational model using PPO. This is especially valuable in today\u2019s rapidly changing field of machine learning, and ensures that models remain relevant and responsive to contemporary requirements without the need for extensive overhauls. Specifically, at each decoding step, our key idea is to adjust the model\u2019s probabilistic prediction using a reward signal. This adjustment is crucial as it enables the generated text to both (1) maintain the semantic\n*Equal contributions. Work done while J.B. was an undergraduate researcher at UW-Madison.\nrelevance with respect to the previous context, and (2) align with the reward criteria and human preference. These two sub-goals can be flexibly traded off with proper weighting on the reward signal, which degenerates to the standard maximum-likelihood decoding when the weight is zero. Notably, our reward-guided score can be seamlessly integrated with various token selection strategies, including both greedy and stochastic sampling.\nWe validate ARGS on the large-scale HH-RLHF (Helpful and Harmless) dataset (Bai et al., 2022) and demonstrate that our technique effectively guides the generation towards outputs that are preferable. For example, our method improves the average reward by \u219119.56% relative to the standard decoding and secures a preference or tie score of 64.33% in GPT-4 evaluation. Moreover, our method excels at generating lexically diverse continuations without compromising their contextual consistency. Qualitatively, ARGS offers less redundant and more informative outputs than the standard maximum-likelihood decoding, as illustrated in Table 1. Additionally, we further emphasize the versatility of ARGS and demonstrate consistent improvement across different model architectures (LLaMa and OPT), sizes, and alignment tasks including Stanford Human Preferences (SHP) dataset (Ethayarajh et al., 2022). To summarize our contributions:\n1. We propose a novel framework ARGS , which postulates the alignment process as a rewardguided search problem that runs during decoding time. This framework not only omits the need for expensive RL training but also facilitates flexible customization to emerging needs.\n2. We conduct both qualitative and quantitative evaluations of ARGS\u2019s performance, showcasing its superiority over existing approaches. ARGS effectively guides the outputs of the neural language model in alignment with human preferences.\n3. Importantly, ARGS brings a new perspective of decoding-time alignment to the field of AI safety. While traditional alignment strategies focus on optimization during the training phase, decoding-time alignment emphasizes the pivotal role of post-training adjustments. Such a shift in focus allows models to adjust to new reward signals and user requirements without the need for extensive retraining. We hope this inspires further research into post hoc alignment, leading to more efficient and safer AI systems in real-world applications."
        },
        {
            "heading": "2 ARGS: ALIGNMENT AS REWARD-GUIDED SEARCH",
            "text": "In this section, we introduce ARGS, a novel decoding framework that facilitates the alignment of generated text with human preferences, by employing a reward mechanism that directly guides the text generation process of a language model. Our method has two main components: (1) rewardguided scoring, which assigns scores to possible continuations of the text, and (2) token selection, which selects a continuation. We detail the reward-guided scoring method in Section 2.1 and the token selection methods in Section 2.2."
        },
        {
            "heading": "2.1 REWARD-GUIDED SCORING",
            "text": "Our goal is to steer the decoded outputs of language models in alignment with human preference. At each decoding step, our key idea is to adjust the model\u2019s probabilistic prediction by a reward signal (Figure 1). This adjustment is crucial as it enables the model to generate text that is not only coherent and contextually relevant but also tailored to satisfy specific alignment criteria or objectives.\nSpecifically, a reward model (RM) assigns a scalar reward value to each response. Following Stiennon et al. (2020), reward models are often trained on a dataset comprised of paired comparisons between two responses generated for the same input or prompt. Formally, the reward modeling loss for each pair of preferred sample (x, yw) and less preferred sample (x, yl) is defined as follows:\nLRM(x, yw, yl; \u03b8) = log \u03c3(r([x, yw])\u2212 r([x, yl])), (1) where \u03b8 is the parameterization of the reward model, \u03c3(\u00b7) is the sigmoid function, r([x, y]) is the scalar reward for a given pair of input x and response y, and [x, y] represents concatenation of the prompt and response.\nGiven the previous context x<t and timestamp t, we formalize our reward-guided scoring function for a token v: s(v,x<t) = LM(v | x<t) + w \u00b7 r([x<t, v]), (2) where LM(v|x<t) is the model\u2019s assigned output for token v, w is the weight assigned to the reward scalar, and [x<t, v] represents concatenation of v to the previous context.\nOur scoring function is more desirable than the vanilla decoding strategy, since the generated text is encouraged to both (1) maintain the semantic coherence and relevance with respect to the previous context and (2) align with the reward criteria and human preference. These two sub-goals can be flexibly traded off with the weighting parameter w, which we analyze comprehensively in Section 3.2."
        },
        {
            "heading": "2.2 TOKEN SELECTION",
            "text": "Our reward-guided score can be flexibly used by different token selection strategies. Here, we consider two popular selection strategies: greedy selection and stochastic sampling. We describe both variants below, dubbed ARGS-greedy and ARGS-stochastic respectively.\nARGS-greedy. The greedy method selects a candidate continuation based on the maximum scores, which can formulated as follows:\nvselected = argmax v\u2208V (k) s(v,x<t),\nwhere V (k) is a set of the k most likely predictions according to the model\u2019s predicted probability distribution p(\u00b7|x<t), and enables effectively reducing the search space to probable tokens without considering all possible tokens.\nAfter selecting a continuation token, vselected, we construct the next context as follows:\nxt = [x<t, vselected],\nwhere xt is the new context for the next iteration. We iteratively generate the next best token using our method until we reach the desired number of tokens.\nARGS-stochastic. Stochastic method samples token from a renormalized probability distribution among the top-k candidate tokens. Specifically, a token v is randomly chosen with the following probability:\np(v,x<t, \u03c4) = exp(s(v,x<t)/\u03c4)\u2211\nvi\u2208V (k) exp(s(vi,x<t)/\u03c4)\n, (3)\nwhere \u03c4 is the temperature. A larger \u03c4 makes the distribution more uniformly distributed, leading to a random selection. Conversely, as the temperature \u03c4 approaches 0, the probability p(v,x<t, \u03c4) approaches 1 for the token v with the maximum score, similar to the greedy decoding method. We update the context, xt, using the same concatenation process as described above in ARGS-greedy.\nAlgorithm 1 ARGS-greedy Input: Previous context x with n tokens, number of candidates k, reward coefficient w, desired\nnumber of tokens m, base model LM, and reward model Output: A generated sequence with m tokens\n1: for t\u2190 n to m\u2212 1 do 2: V (k) \u2190 top-k tokens with highest likelihood 3: for v \u2208 V (k) do \u25b7 Iterate over top-k candidates 4: reward\u2190 r([x, v]) \u25b7 Compute a reward of this candidate 5: scores(v)\u2190 LM(v | x) + w \u00b7 reward 6: end for 7: vselected \u2190 argmaxv\u2208V (k) scores(v) \u25b7 Select token 8: x\u2190 [x, vselected] 9: end for\n10: return x"
        },
        {
            "heading": "2.3 IMPLEMENTATION AND COMPLEXITY",
            "text": "We exemplify the complete pipeline of our method with greedy decoding in Algorithm 1. In each decoding step, the following steps are performed: the language model computes the prediction scores for the next tokens (line 2), the rewards for all top-k tokens are computed (lines 3-6), and the context is updated with a token with the highest score (line 8). One can switch from ARGS-greedy to ARGS-stochastic, by modifying line 7 to be probabilistic sampling using Equation 3.\nThe time complexity of ARGS is primarily governed by two operations: computing the predictions and calculating the associated rewards for each candidate token. Consider the complexity of a single decoding step for a context with t tokens. The complexity for this is given by\nT (t) = TLM(t) + k \u00b7 Tr(t+ 1), where TLM and Tr denote the time complexity associated with the base model and the reward model, respectively.\nAs described in Vaswani et al. (2017), utilizing the transformer architecture results in a complexity of O(t2) for both the base model and reward model. This quadratic factor emerges due to the selfattention mechanism in transformers, which requires pairwise calculations of attention scores across all tokens. Thus, the initial decoding step for ARGS, with an original context length of n, exhibits a complexity of T (n) = O(k \u00b7 n2). For subsequent tokens, since we add one token at a time to the context, we can reuse previously calculated attention, reducing the complexity to TLM(t) = O(t). Similarly, by retaining the attention from the previously selected candidate, the reward model complexity becomes Tr(t + 1) = O(t). Therefore, each of the subsequent decoding steps is characterized by T (t) = O(k \u00b7 t), where t spans from n+ 1 to m\u2212 1. To conclude, the aggregate time complexity of our proposed approach is:\nTARGS(n,m, k) = O(k \u00b7 n2) + m\u22121\u2211 t=n+1 O(k \u00b7 t) = O(k \u00b7m2).\nIn contrast to the classical decoding methods with complexity O(m2), the ARGS approach introduces only a constant factor of k in its complexity which arises due to the need to consider the top-k candidate tokens at each decoding step. Even though this appears to add more complexity than the original method, we find that k can be considerably small (Section 3.2), rendering the complexity more tractable. We discuss the practical computation further in Section 4."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "This section presents empirical experiments to evaluate the effectiveness of our proposed method. In particular, we aim to show that ARGS can effectively guide the outputs of the neural language model in alignment with human preference, such as helpfulness and harmfulness. All of our experiments are based on open-sourced language models and datasets. Our code is released publicly for reproducible research."
        },
        {
            "heading": "3.1 SETUP",
            "text": "Our goal is to steer the model to generate helpful and harmless responses. This task is fundamental in understanding the practical applicability of our proposed decoding method in real-world scenarios, where the generation of helpful and harmless text is of utmost importance for AI assistants.\nExperimental details. To evaluate the performance of our approach, we employ ARGS on the HH-RLHF (Helpful and Harmless) dataset (Bai et al., 2022), which is the most commonly adopted benchmark for alignment. The dataset consists of 112,000 training samples and 12,500 test samples and is publicly available*. Each sample in the dataset includes a prompt and two responses, with one being preferred over the other. The selected responses are annotated based on the opinions of crowd workers, who assess which response is more helpful and harmless. For a base model, we use LLaMA-7B (Touvron et al., 2023a) as a pre-trained language model and fine-tune it on only preferred responses of the HH-RLHF dataset for one epoch. The fine-tuned model is referred to as LLaMA-7B-SFT. We then train the reward model from the fine-tuned model on HH-RLHF, employing a pairwise reward loss introduced in Ouyang et al. (2022). The trained reward model attains a final accuracy of 74.58% on the validation set. Full details on training hyperparameters are included in Appendix A.\nDecoding. We evaluate the models by producing text responses given the conversation prompts from the HH-RLHF test set. For main results, we test decoding methods based on the fine-tuned LLaMA-7B model by default. Following the standard practice, we limit the maximum lengths of the prompt and generated continuation to 2,048 and 128 tokens, respectively. For the deterministic baseline, we use greedy search. For stochastic methods, we employ top-k sampling (k = 40 and temperature = 0.7), nucleus sampling (p = 0.95), and contrastive search (k = 8 and \u03b1 = 0.6). For evaluations of our proposed method on LLaMA-7B, we use w = 1.5 and k = 10 based on the optimal average reward performance on the validation set. Following a similar rationale, for all evaluations involving OPT (Zhang et al., 2022) models, we opt for values of w = 2 and k = 10. Ablations on all the hyperparameters, including k and w, will be discussed in Section 3.2.\nEvaluation metrics. Drawing inspiration from the previous methodologies, our generation quality evaluation leverages the following metrics.\n\u2022 Average Reward: This metric represents the mean of the rewards computed by the reward model across all generations from the HH-RLHF test prompts. A higher average reward indicates model continuations that are more closely aligned with the attributes represented in the reward model, such as helpfulness and harmlessness. We use the same reward model that was employed during the ARGS decoding step.\n\u2022 Diversity: This metric aggregates n-gram repetition rates. A higher diversity score indicates the capacity to produce texts with a broad spectrum of vocabulary. The diversity score for a given continuation y is diversity(y) = \u220f4 n=2 unique n-grams(y) total n-grams(y) . \u2022 Coherence: This metric is estimated by calculating the cosine similarity between the sentence embeddings of the prompt and its continuation. As in Su et al. (2022), we utilize the pre-trained SimCSE sentence embedding model to obtain the embeddings."
        },
        {
            "heading": "3.2 RESULTS",
            "text": "ARGS effectively improves the generation performance. Figure 2 (left) shows that ARGS yields relative improvements of 19.56% in average reward over the greedy decoding baseline (w = 0); thus highlighting the benefit of incorporating a reward signal during decoding. There is a noticeable shift towards higher rewards for the generated texts by ARGS. This suggests that our method is indeed effective in aligning the generation towards more desirable outputs. Moreover, we observe in Figure 2 (middle) that the diversity metric for ARGS is generally better than the standard decoding method, which indicates that our proposed method is capable of generating lexically diverse continuations. Lastly, our method maintains comparable contextual consistency under mild weight, such as w = 0.5 and w = 1. However, we observe a degradation when w becomes too large. This is expected since our decoding mechanism has an inherent tradeoff between semantic coherence and reward, emphasizing the need for a mildly chosen weight in practice. We also repeat the experiment\n*https://huggingface.co/datasets/Dahoas/full-hh-rlhf\nwith the original non-fine-tuned LLaMA-7B model and achieve similarly improved results, which are shown in Table 10 (Appendix C).\nEffect of w and k. To further understand the impact of the choice of parameters k and w, we compare the performance of our method as the hyperparameters vary. Figure 2 depicts three main metrics of performance, average reward score, diversity, and coherence, using the same experimental setup as outlined earlier in Section 3.1. In Figure 2, we observe an increase in average reward as the weighting parameter w increases up to a particular point, after which it begins to decline. We hypothesize that a higher w may inadvertently favor short-term rewards, potentially undermining the broader semantic coherence and alignment. Regarding the number of candidates k, the performance variation between k = 40 and k = 10 is slight, suggesting that a large number of candidates may not be essential for producing aligned generations. Qualitative examples. In Table 1, we provide qualitative examples of how ARGS can steer decoded outputs to be more aligned with human preference. For the first example, the greedy approach provides unhelpful and repetitive responses, asking multiple times about the number of strings of lights to be connected. In contrast, the ARGS-greedy offers a comprehensive plan for setting up the light show, suggesting types of lights, power strips, and a strategy for a test run. For the second example, the greedy decoding yields a short and redundant query despite this information being previously provided. On the other hand, the ARGS-greedy method offers a nuanced response, offering practical interview preparation advice such as rehearsing answers, dressing appropriately, organizing necessary documents, and even preparing questions for potential employers. See Appendix D for additional qualitative examples."
        },
        {
            "heading": "3.3 GPT-4 EVALUATION",
            "text": "To address the nuanced aspects of language quality that the aforementioned metrics may not comprehensively capture, we also adopt a GPT-4-based evaluation approach for comparing the quality of responses. As investigated in Zheng et al. (2023a), using GPT-4 proxy aligns with human evaluations over 80% of the time for quality assessments, thus offering a scalable method to approximate human preferences. Following the methodologies in Chiang et al. (2023), we use GPT-4 as a proxy for human evaluation by having it review and score two responses to the same prompt on a scale from 1 to 10. We explicitly instruct the proxy to assign the score to the responses based on helpfulness, harmlessness, relevance, accuracy, and insightfulness (see the prompt template attached in Appendix B). We randomly sample 300 prompts from the test set of HH-RLHF and compare the response between ARGS and various decoding methods. To mitigate position bias (Zheng et al., 2023a), we randomize the order in which we present the generated responses to GPT-4.\nTable 2 presents the GPT-4 evaluation results, measured by the percentage of win-ties of our method over the alternative decoding strategies. A higher percentage indicates that our proposed method is more proficient in generating responses that exhibit not only contextual relevance and accuracy but also helpfulness and harmlessness. This observation is consistent with the outcomes of the automatic evaluation discussed in Section 3.2. For all decoding methods, we report in Table 10 (Appendix C) the complete evaluation metrics including average reward, diversity, and coherence."
        },
        {
            "heading": "3.4 FURTHER ANALYSIS",
            "text": "ARGS-greedy vs. ARGS-stochastic. In Table 3, we compare the performance using two variants of ARGS, namely ARGS-greedy and ARGS-stochastic. For both variants, we use the same default k and w as specified in Section 3.1. The temperature parameter \u03c4 is set to be 0.7 for ARGS-stochastic, which follows the same configuration in popular stochastic methods such as top-k sampling. In general, we find that ARGS-greedy more effectively improves the average reward and alignment with human preference. ARGS-stochastic can produce more diverse texts due to probabilistically sampling from a set of top-k tokens instead of deterministically selecting the most probable one.\nARGS is model- and task-agnostic. Our proposed method, ARGS, is inherently both model and task-agnostic, enhancing its utility across a broad array of natural language processing applications. The primary strengths of ARGS encompass (1) its compatability with diverse model architectures,\n(2) its broad applicability across various alignment tasks, and (3) its flexibility regarding the size of the model deployed. To elaborate, the language model and reward model do not need to have the same size or architecture, given that the reward model is trained effectively to capture the human preferences pertinent to a given task.\nTo validate this, we consider another helpful and harmless alignment task and perform experiments on OPT-1.3b and OPT-2.7b as base models and OPT-125m and OPT-350m as reward models (Zhang et al., 2022). We fine-tune base and reward models following the methodology in Section 3.1 on the Stanford Human Preferences (SHP) dataset (Ethayarajh et al., 2022). The dataset consists of 349,000 training samples and 36,800 test samples and is publicly available. Each sample contains a prompt paired with two responses. An annotation is provided to indicate which of the two\nresponses is more preferred. We evaluate the models on random 1,000 samples of the test set, and the average reward is calculated by the OPT-350m reward model. As shown in Figure 3, ARGS consistently outperforms the greedy baseline, suggesting that ARGS is model- and task-agnostic. Additionally, we conduct a GPT-4 evaluation, comparing ARGS with the baseline PPO. Overall, our method produces more favorable responses, achieving a win-tie rate of 72.33%."
        },
        {
            "heading": "4 DISCUSSION",
            "text": "Training-time vs. decoding-time alignment. This paper brings a novel perspective of decodingtime alignment to the field. While traditional alignment strategies focus on alignment and optimization during the training phase, decoding-time alignment emphasizes the pivotal role of post-training adjustments. One feature of decoding-time alignment is its ability to adapt in the event of altering the reward model. This omits the need to go through the exhaustive process of retraining the RL model and enables quick realignment. Thus, the shift facilitates rapid customization of evolving datasets and emerging needs, and ensures that models remain relevant and responsive to contemporary requirements without the need for extensive overhauls. Furthermore, our framework can be compatible with a wide range of models, which is especially valuable in today\u2019s rapidly changing field of machine learning with its various model designs or sizes.\nTable 4 empirically compares the performance between ARGS, Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) (Rafailov et al., 2023), on the SHP dataset. The PPO model is optimized with fine-tuned OPT-1.3b as the initial language model and OPT-350m as the reward model. Similarly, the DPO model uses the fine-tuned OPT-1.3b model as a base. Full details of training configurations are in Appendix A. The same reward and language model are used for ARGS, but notably without any further training. We observe that ARGSachieves a comparable average reward as PPO, while alleviating the need for expensive RL training. Moreover, we observe that ARGS significantly outperforms PPO and DPO in terms of diversity and coherence. Overall, the results indicate that our approach is a competitive contender compared to the status quo approach.\nComputation and alignment tradeoff. We analyze the theoretical complexity of ARGS in Section 2.3. Compared to the classic decoding methods with complexity O(m2), the ARGS approach introduces only a constant factor of k in its complexity which arises due to the need to consider the top-k candidate tokens at each decoding step. Empirically, we find that k can be considerably small. For example, using the OPT-2.7b base model, the generation time per response increases only by 1.9 times when comparing ARGS (k = 10) with conventional greedy decoding. Despite the slight increase in processing time, there was a notable enhancement in reward performance by \u21916.8%,\ndemonstrating the existence of a reasonable tradeoff between reward optimization and inference speed. The gap can be further reduced by employing a smaller reward model, or parallelizing the reward computation across k candidates. The feasibility is supported in Figure 3, where a smaller reward model (such as OPT-125m) does not significantly change the performance."
        },
        {
            "heading": "5 RELATED WORKS",
            "text": "Language model alignment. Fine-tuning language models to reflect human preferences has gained traction, with reinforcement learning from human feedback (RLHF) offering a direct route. Signals from external reward models that act as human proxies are used to refine agents through iterative trials under different RLHF frameworks (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020; Lee et al., 2021; Nakano et al., 2022; Snell et al., 2023). One notable approach is to utilize proximal policy optimization (Askell et al., 2021; Ouyang et al., 2022; Bai et al., 2022; Glaese et al., 2022). However, recognizing the challenges posed by the unstable and resource-demanding nature of RL, researchers have also explored supervised fine-tuning methods. For example, Liu et al. (2023) fine-tune the model using prompts that encompass both desirable and undesirable answers. Rafailov et al. (2023), on the other hand, take a distinctive route by modeling the language model as a Bradley-Terry model, bypassing the need for conventional reward modeling. Yuan et al. (2023); Song et al. (2023) introduce frameworks that are designed to rank multiple responses, adding to the spectrum of alignment methods. Dong et al. (2023) introduce an approach in which rewards are harnessed to curate suitable training sets for the fine-tuning of language models. Rennie et al. (2017) investigate reinforcement learning training to improve image captioning on LSTM and CNN architectures. Notably, ARGS diverges from these training-based approaches, by providing a new decoding-time framework to align language models without requiring expensive RL training.\nLanguage model decoding. A language model (LM) is a machine learning model trained to predict the probability distribution p(x) across a text sequence of variable length x = {x1, . . . , x|x|}. The probability p(xt | x<t) denotes the likelihood of predicting the next token xt, given the context x<t = {x1, . . . , xt\u22121}. Leveraging this likelihood, various decoding strategies have been proposed to generate a text continuation of the context, which can be categorized into either deterministic or stochastic methods (Ippolito et al., 2019). Notable deterministic methods include the greedy beam search, and contrastive search (Su et al., 2022). They select the text continuation with the highest probability or scoring criteria. Popular stochastic methods include top-k sampling (Fan et al., 2018) and nucleus sampling (Holtzman et al., 2020). Top-k sampling selects the k tokens with the highest likelihood, renormalizes the probabilities, and then samples from this set, while nucleus sampling selects the smallest set of tokens such that their cumulative probability exceeds a certain threshold. Unlike deterministic methods, stochasticity in these methods could cause the semantic meaning of the sampled text to diverge from or even contradict the human-written prefix (Basu et al., 2021).\nGuided decoding. Our works distinguishes itself in the token-level guided decoding literature by using a reward model that guides generation at the token level, rather than focusing on step-level verifiers that typically emphasize sentence-level analysis (Welleck et al., 2022; Uesato et al., 2022; Lightman et al., 2023; Krishna et al., 2022; Li et al., 2023b; Khalifa et al., 2023; Xie et al., 2023; Yao et al., 2023). While token-level guided decoding have been explored in the past (Dathathri et al., 2020; Krause et al., 2021; Yang & Klein, 2021; Lu et al., 2021; Chaffin et al., 2022; Liu et al., 2021; Li et al., 2023a), they have not connected language decoding directly to the alignment problem of our interest, especially in the context of utilizing a reward model. Concurrent to our work, Deng & Raffel (2023) use a decoding process that includes a reward model, however, they utilize a unidirectional reward model that is trained using a cumulative squared error loss. In contrast, ARGS employs a reward model based on a pairwise ranking loss to score preferred and nonpreferred responses, which is consistent with the existing RLHF framework."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "The ARGS framework offers a novel decoding-time approach to alignment, addressing the limitations of traditional methods. By formulating alignment as a decoding-stage problem and leveraging reward signals, our approach reduces the need for resource-intensive RL training. The consistent performance gains achieved emphasize the potential of ARGS, indicating a promising trajectory toward creating more flexibly aligned language models. Overall, ARGS framework not only improves alignment performance but also paves the way for broader applications of alignment in the future. Due to the space limit, we discuss limitations and future work in Appendix E.\nEthics statement. The ability to adapt and align models post-training means that smaller institutions or businesses without the capacity for large-scale training can still effectively tailor pre-trained models to meet their specific needs. This can potentially level the playing field, allowing those with limited computational resources to benefit from state-of-the-art models without incurring significant costs. Also, the compatibility of our method with different reward models extends its applicability across various domains and industries. This can accelerate the adoption of machine learning solutions in fields where resource constraints or rapidly changing data are prevalent. Our study does not involve human subjects or violation of legal compliance. Code will be released publicly to facilitate reproducibility and broader applicability.\nAcknowledgement The authors would like to thank ICLR anonymous reviewers for their helpful feedback. The work is supported by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements either expressed or implied, of the sponsors."
        },
        {
            "heading": "A EXPERIMENTATION DETAILS",
            "text": "Software and hardware. We conduct our experiments on servers equipped with NVIDIA RTX A6000 GPUs (48GB VRAM) and NVIDIA A100 GPUs (80GB VRAM). We use Ubuntu 22.04.2 LTS as the operating system, with NVIDIA CUDA Toolkit version 11.8 and cuDNN 8.9. All experiments are implemented in Python 3.11.4 using the PyTorch 1.12.1 framework.\nTraining LLaMA-7B on HH-RLHF. We employ the LMFlow (Diao et al., 2023) toolkit to facilitate the training of the LLaMA-7B model on the HH-RLHF dataset. Following the training scheme in Dong et al. (2023), we use the AdamW (Loshchilov & Hutter, 2019) optimizer in conjunction with DeepSpeed ZeRO stage 3 (Rasley et al., 2020). The training was performed on the entire training split. The training parameters are summarized in Table 5.\nTraining OPT models on the Stanford Human Preferences (SHP) dataset. For the training of all OPT-family models on the SHP dataset, we utilize the DeepSpeed-Chat (DeepSpeed, 2023) repository. We adopt the training scheme proposed by Ouyang et al. (2022), wherein the reward model is trained based on the supervised fine-tuned model. Their default configurations were followed: models undergo supervised fine-tuning on 20% of the training dataset, and reward modeling on the subsequent 40%. We format the response pairs by prefixing the prompt with Human: and prepending Assistant: to the model\u2019s responses, following the methodology outlined in DeepSpeed (2023). These training parameters are consistently applied across all model sizes (OPT-125m, OPT-350m, OPT-1.3b, and OPT-2.7b) and are detailed in Table 6.\nTraining configurations for PPO. For all model training with reinforcement learning with human feedback through proximal policy optimization, we adopt the DeepSpeed-Chat (DeepSpeed, 2023) repository. We follow their default configurations which are detailed in Table 7.\nTraining configurations for DPO. For experiments on DPO, we use the TRL (transformer reinforcement learning) repository from Huggingface in conjunction with the DPOTrainer module. The configuration values are detailed in Table 8."
        },
        {
            "heading": "B GPT-4 EVALUATION DETAILS",
            "text": "Table 9 presents the prompts and responses usage in our GPT-4 evaluation. Each GPT-4 request comprises both a system and a user prompt. The system prompt delineates the proxy\u2019s attributes and its specific task, while the user prompt poses a question and provides responses from the two methods."
        },
        {
            "heading": "C COMPARISON WITH ALL BASELINES",
            "text": "Table 10 provides a comprehensive comparison of ARGS, both in its greedy and stochastic variants, with various baseline methods, including vanilla greedy decoding, top-k sampling, nucleus sampling, and contrastive search. We evaluate these decoding strategies using both the base model and the fined-tuned version. It is noteworthy that even when applied to the non-finetuned model, ARGS exhibits a substantial improvement in average reward, surpassing the performance of the best baseline method by a margin of \u219127%. Moreover, in the fine-tuned version of ARGS, the method outperforms the best baseline by \u219118%. These results underscore the effectiveness of ARGS, both in its greedy and stochastic variants, in enhancing the performance of language generation, surpassing the performance of well-established baseline methods in both non-finetuned and fine-tuned scenarios."
        },
        {
            "heading": "D ADDITIONAL QUALITATIVE EXAMPLES",
            "text": "In Table 11, we provide additional qualitative examples of how ARGS can steer decoded outputs to be more aligned with human preference. See Section 3.1 for models and hyperparameters set up for LLaMA-7B."
        },
        {
            "heading": "E LIMITATIONS AND FUTURE WORK.",
            "text": "For our current evaluations, we follow the standard and commonly used benchmarks in alignment literature. In particular, HH-RLHF from Anthropic and Stanford Human Preferences (SHP) are among the largest and publicly available datasets for alignment research. These tasks allow us to draw comparisons with existing approaches more easily and directly. Nonetheless, we acknowledge the potential value in assessing more intricate tasks, such as those involving multi-step reasoning. We maintain a keen interest in extending our research to encompass more complex tasks in subsequent studies. We are also interested in exploring different various reward modeling approaches (Anony-\nmous, 2023; Go et al., 2023; Wu et al., 2023), as we have observed that a higher-quality reward model results in enhanced generation quality."
        }
    ],
    "year": 2024
}