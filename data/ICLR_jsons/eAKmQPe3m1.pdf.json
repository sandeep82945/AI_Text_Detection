{
    "abstractText": "The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-\u03b1, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024 \u00d7 1024 resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that respectively optimize pixel dependency, textimage alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-\u03b1\u2019s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART\u03b1 only takes 12% of Stable Diffusion v1.5\u2019s training time (\u223c753 vs. \u223c6,250 A100 GPU days), saving nearly $300,000 ($28,400 vs. $320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-\u03b1 excels in image quality, artistry, and semantic control. We hope PIXART-\u03b1 will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junsong Chen"
        },
        {
            "affiliations": [],
            "name": "Jincheng Yu"
        },
        {
            "affiliations": [],
            "name": "Chongjian Ge"
        },
        {
            "affiliations": [],
            "name": "Lewei Yao"
        },
        {
            "affiliations": [],
            "name": "Enze Xie"
        },
        {
            "affiliations": [],
            "name": "Yue Wu"
        },
        {
            "affiliations": [],
            "name": "Zhongdao Wang"
        },
        {
            "affiliations": [],
            "name": "James Kwok"
        },
        {
            "affiliations": [],
            "name": "Ping Luo"
        },
        {
            "affiliations": [],
            "name": "Huchuan Lu"
        },
        {
            "affiliations": [],
            "name": "Zhenguo Li"
        }
    ],
    "id": "SP:40483844a7a8a22d68f72013b97e4f81b8e48597",
    "references": [
        {
            "authors": [
                "Sylvain Viguier"
            ],
            "title": "Estimating the carbon footprint of bloom, a 176b parameter language model",
            "venue": "In arXiv preprint arXiv:2211.02001,",
            "year": 2022
        },
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro"
            ],
            "title": "ediffi: Text-to-image diffusion models with an ensemble of expert denoisers",
            "venue": "In arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bao",
                "Shen Nie",
                "Kaiwen Xue",
                "Yue Cao",
                "Chongxuan Li",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "All are worth words: A vit backbone for diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Eyal Betzalel",
                "Coby Penso",
                "Aviv Navon",
                "Ethan Fetaya"
            ],
            "title": "A study on the evaluation of generative models",
            "venue": "In arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Shoufa Chen",
                "Mengmeng Xu",
                "Jiawei Ren",
                "Yuren Cong",
                "Sen He",
                "Yanping Xie",
                "Animesh Sinha",
                "Ping Luo",
                "Tao Xiang",
                "Juan-Manuel Perez-Rua"
            ],
            "title": "Gentron: Delving deep into diffusion transformers for image and video generation",
            "venue": "arXiv preprint arXiv:2312.04557,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In arXiv,",
            "year": 2020
        },
        {
            "authors": [
                "Zhida Feng",
                "Zhenyu Zhang",
                "Xintong Yu",
                "Yewei Fang",
                "Lanxin Li",
                "Xuyi Chen",
                "Yuxiang Lu",
                "Jiaxiang Liu",
                "Weichong Yin",
                "Shikun Feng"
            ],
            "title": "Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts",
            "year": 2023
        },
        {
            "authors": [
                "Chongjian Ge",
                "Junsong Chen",
                "Enze Xie",
                "Zhongdao Wang",
                "Lanqing Hong",
                "Huchuan Lu",
                "Zhenguo Li",
                "Ping Luo"
            ],
            "title": "Metabev: Solving sensor failures for 3d detection and map segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Rohit Girdhar",
                "Alaaeldin El-Nouby",
                "Zhuang Liu",
                "Mannat Singh",
                "Kalyan Vasudev Alwala",
                "Armand Joulin",
                "Ishan Misra"
            ],
            "title": "Imagebind: One embedding space to bind them all",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiyi Huang",
                "Kaiyue Sun",
                "Enze Xie",
                "Zhenguo Li",
                "Xihui Liu"
            ],
            "title": "T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Minguk Kang",
                "Jun-Yan Zhu",
                "Richard Zhang",
                "Jaesik Park",
                "Eli Shechtman",
                "Sylvain Paris",
                "Taesung Park"
            ],
            "title": "Scaling up gans for text-to-image synthesis",
            "year": 2023
        },
        {
            "authors": [
                "Gwanghyun Kim",
                "Taesung Kwon",
                "Jong Chul Ye"
            ],
            "title": "Diffusionclip: Text-guided diffusion models for robust image manipulation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In arXiv,",
            "year": 2013
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Eric Mintun",
                "Nikhila Ravi",
                "Hanzi Mao",
                "Chloe Rolland",
                "Laura Gustafson",
                "Tete Xiao",
                "Spencer Whitehead",
                "Alexander C Berg",
                "Wan-Yen Lo"
            ],
            "title": "Segment anything",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Yuval Kirstain",
                "Adam Polyak",
                "Uriel Singer",
                "Shahbuland Matiana",
                "Joe Penna",
                "Omer Levy"
            ],
            "title": "Picka-pic: An open dataset of user preferences for text-to-image generation",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Zhiqi Li",
                "Wenhai Wang",
                "Hongyang Li",
                "Enze Xie",
                "Chonghao Sima",
                "Tong Lu",
                "Yu Qiao",
                "Jifeng Dai"
            ],
            "title": "Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqi Li",
                "Wenhai Wang",
                "Enze Xie",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo",
                "Tong Lu"
            ],
            "title": "Panoptic segformer: Delving deeper into panoptic segmentation with transformers",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Jia Ning",
                "Yue Cao",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Han Hu"
            ],
            "title": "Video swin transformer",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In arXiv,",
            "year": 2017
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Mou",
                "Xintao Wang",
                "Liangbin Xie",
                "Jian Zhang",
                "Zhongang Qi",
                "Ying Shan",
                "Xiaohu Qie"
            ],
            "title": "T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Junting Pan",
                "Keqiang Sun",
                "Yuying Ge",
                "Hao Li",
                "Haodong Duan",
                "Xiaoshi Wu",
                "Renrui Zhang",
                "Aojun Zhou",
                "Zipeng Qin",
                "Yi Wang",
                "Jifeng Dai",
                "Yu Qiao",
                "Hongsheng Li"
            ],
            "title": "Journeydb: A benchmark for generative image understanding",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "William Peebles",
                "Saining Xie"
            ],
            "title": "Scalable diffusion models with transformers",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Ethan Perez",
                "Florian Strub",
                "Harm De Vries",
                "Vincent Dumoulin",
                "Aaron Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Dustin Podell",
                "Zion English",
                "Kyle Lacey",
                "Andreas Blattmann",
                "Tim Dockhorn",
                "Jonas M\u00fcller",
                "Joe Penna",
                "Robin Rombach"
            ],
            "title": "Sdxl: Improving latent diffusion models for high-resolution image synthesis",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T. Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "venue": "OpenAI blog,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In MICCAI,",
            "year": 2015
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "In arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki"
            ],
            "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
            "venue": "In arXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Strudel",
                "Ricardo Garcia",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "title": "Segmenter: Transformer for semantic segmentation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Peize Sun",
                "Jinkun Cao",
                "Yi Jiang",
                "Rufeng Zhang",
                "Enze Xie",
                "Zehuan Yuan",
                "Changhu Wang",
                "Ping Luo"
            ],
            "title": "Transtrack: Multiple object tracking with transformer",
            "venue": "In arXiv,",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pvt v2: Improved baselines with pyramid vision transformer",
            "venue": "Computational Visual Media,",
            "year": 2022
        },
        {
            "authors": [
                "Jay Zhangjie Wu",
                "Yixiao Ge",
                "Xintao Wang",
                "Stan Weixian Lei",
                "Yuchao Gu",
                "Wynne Hsu",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou"
            ],
            "title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation",
            "venue": "arXiv preprint arXiv:2212.11565,",
            "year": 2022
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Enze Xie",
                "Lewei Yao",
                "Han Shi",
                "Zhili Liu",
                "Daquan Zhou",
                "Zhaoqiang Liu",
                "Jiawei Li",
                "Zhenguo Li"
            ],
            "title": "Difffit: Unlocking transferability of large diffusion models via simple parameter-efficient fine-tuning",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Saining Xie",
                "Zhuowen Tu"
            ],
            "title": "Holistically-nested edge detection",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Shuchen Xue",
                "Mingyang Yi",
                "Weijian Luo",
                "Shifeng Zhang",
                "Jiacheng Sun",
                "Zhenguo Li",
                "Zhi-Ming Ma"
            ],
            "title": "Sa-solver: Stochastic adams solver for fast sampling of diffusion models",
            "venue": "arXiv preprint arXiv:2309.05019,",
            "year": 2023
        },
        {
            "authors": [
                "Zeyue Xue",
                "Guanglu Song",
                "Qiushan Guo",
                "Boxiao Liu",
                "Zhuofan Zong",
                "Yu Liu",
                "Ping Luo"
            ],
            "title": "Raphael: Text-to-image generation via large mixture of diffusion paths",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Zi-Hang Jiang",
                "Francis EH Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Anyi Rao",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Hongkai Zheng",
                "Weili Nie",
                "Arash Vahdat",
                "Anima Anandkumar"
            ],
            "title": "Fast training of diffusion models with masked transformers",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Sixiao Zheng",
                "Jiachen Lu",
                "Hengshuang Zhao",
                "Xiatian Zhu",
                "Zekun Luo",
                "Yabiao Wang",
                "Yanwei Fu",
                "Jianfeng Feng",
                "Tao Xiang",
                "Philip HS Torr"
            ],
            "title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers",
            "year": 2021
        },
        {
            "authors": [
                "Daquan Zhou",
                "Bingyi Kang",
                "Xiaojie Jin",
                "Linjie Yang",
                "Xiaochen Lian",
                "Zihang Jiang",
                "Qibin Hou",
                "Jiashi Feng"
            ],
            "title": "Deepvit: Towards deeper vision transformer",
            "venue": "In arXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Daquan Zhou",
                "Zhiding Yu",
                "Enze Xie",
                "Chaowei Xiao",
                "Animashree Anandkumar",
                "Jiashi Feng",
                "Jose M Alvarez"
            ],
            "title": "Understanding the robustness in vision transformers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ermon",
                "Song"
            ],
            "title": "2021) have emerged as highly successful approaches for image generation, surpassing previous generative models such as GANs",
            "venue": "(Goodfellow et al.,",
            "year": 2014
        },
        {
            "authors": [
                "Balaji"
            ],
            "title": "matching on the image latent space and introducing cross-attention-based controlling. The results obtained with this approach have been impressive, particularly in tasks involving high-density image generation, such as text-to-image synthesis. This has served as a source of inspiration for numerous subsequent works aimed at improving text-to-image synthesis, including those by Saharia et al",
            "year": 2023
        },
        {
            "authors": [
                "Dosovitskiy et al",
                "2020a",
                "He"
            ],
            "title": "2022) show it is also a promising architecture on many computer vision tasks like image classification (Touvron et al., 2021",
            "venue": "(Radford et al.,",
            "year": 2021
        },
        {
            "authors": [
                "2022b Li et al",
                "2021 Zhao et al",
                "2022 Liu et al",
                "2022 He et al",
                "Li"
            ],
            "title": "The Diffusion Transformer (DiT) (Peebles & Xie, 2023) and its variant (Bao et al., 2023; Zheng et al., 2023) follow the step to further replace the Convolutional-based U-Net (Ronneberger et al., 2015) backbone with Transformers. This architectural choice brings about increased scalability (Chen et al., 2023) compared to U-Net-based diffusion models, allowing for the straightforward expansion of its param",
            "year": 2022
        },
        {
            "authors": [
                "Podell"
            ],
            "title": "PatchEmbed layer in ViT (Dosovitskiy et al., 2020b) is 2\u00d7",
            "year": 2023
        },
        {
            "authors": [
                "Betzalel"
            ],
            "title": "zero-shot FID is negatively correlated with visual aesthetics",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently, the advancement of text-to-image (T2I) generative models, such as DALL\u00b7E 2 (OpenAI, 2023), Imagen (Saharia et al., 2022), and Stable Diffusion (Rombach et al., 2022) has started a new era of photorealistic image synthesis, profoundly impacting numerous downstream applications, such as image editing (Kim et al., 2022), video generation (Wu et al., 2022), 3D assets creation (Poole et al., 2022), etc.\nHowever, the training of these advanced models demands immense computational resources. For instance, training SDv1.5 (Podell et al., 2023) necessitates 6K A100 GPU days, approximately costing $320,000, and the recent larger model, RAPHAEL (Xue et al., 2023b), even costs 60K A100 GPU days \u2013 requiring around $3,080,000, as detailed in Table 2. Additionally, the training contributes substantial CO2 emissions, posing environmental stress; e.g. RAPHAEL\u2019s (Xue et al., 2023b) train-\n\u2217Equal contribution. Work done during the internships of the four students at Huawei Noah\u2019s Ark Lab. \u2020 Project lead and corresponding author.\ning results in 35 tons of CO2 emissions, equivalent to the amount one person emits over 7 years, as shown in Figure 2. Such a huge cost imposes significant barriers for both the research community and entrepreneurs in accessing those models, causing a significant hindrance to the crucial advancement of the AIGC community. Given these challenges, a pivotal question arises: Can we develop a high-quality image generator with affordable resource consumption?\nIn this paper, we introduce PIXART-\u03b1, which significantly reduces computational demands of training while maintaining competitive image generation quality to the current state-of-the-art image generators, as illustrated in Figure 1. To achieve this, we propose three core designs:\nTraining strategy decomposition. We decompose the intricate text-to-image generation task into three streamlined subtasks: (1) learning the pixel distribution of natural images, (2) learning textimage alignment, and (3) enhancing the aesthetic quality of images. For the first subtask, we propose initializing the T2I model with a low-cost class-condition model, significantly reducing the learning cost. For the second and third subtasks, we formulate a training paradigm consisting of pretraining and fine-tuning: pretraining on text-image pair data rich in information density, followed by finetuning on data with superior aesthetic quality, boosting the training efficiency.\nEfficient T2I Transformer. Based on the Diffusion Transformer (DiT) (Peebles & Xie, 2023), we incorporate cross-attention modules to inject text conditions and streamline the computationintensive class-condition branch to improve efficiency. Furthermore, we introduce a reparameterization technique that allows the adjusted text-to-image model to load the original classcondition model\u2019s parameters directly. Consequently, we can leverage prior knowledge learned from ImageNet (Deng et al., 2009) about natural image distribution to give a reasonable initialization for the T2I Transformer and accelerate its training.\nHigh-informative data. Our investigation reveals notable shortcomings in existing text-image pair datasets, exemplified by LAION (Schuhmann et al., 2021), where textual captions often suffer from a lack of informative content (i.e., typically describing only a partial of objects in the images) and a severe long-tail effect (i.e., with a large number of nouns appearing with extremely low frequencies). These deficiencies significantly hamper the training efficiency for T2I models and lead to millions of iterations to learn stable text-image alignments. To address them, we propose an autolabeling pipeline utilizing the state-of-the-art vision-language model (LLaVA (Liu et al., 2023)) to generate captions on the SAM (Kirillov et al., 2023). Referencing in Section 2.4, the SAM dataset is advantageous due to its rich and diverse collection of objects, making it an ideal resource for creating high-information-density text-image pairs, more suitable for text-image alignment learning.\n1The method for estimating CO2 emissions follows Alexandra Sasha Luccioni (2022). 2The training cost refers to the cloud GPU pricing from Microsoft (2023) Azure in September 20, 2023.\nOur effective designs result in remarkable training efficiency for our model, costing only 753 A100 GPU days and $28,400. As demonstrated in Figure 2, our method consumes less than 1.25% training data volume compared to SDv1.5 and costs less than 2% training time compared to RAPHAEL. Compared to RAPHAEL, our training costs are only 1%, saving approximately $3,000,000 (PIXART-\u03b1\u2019s $28,400 vs. RAPHAEL\u2019s $3,080,000). Regarding generation quality, our user study experiments indicate that PIXART-\u03b1 offers superior image quality and semantic alignment compared to existing SOTA T2I models (e.g., DALL\u00b7E 2 (OpenAI, 2023), Stable Diffusion (Rombach et al., 2022), etc.), and its performance on T2I-CompBench (Huang et al., 2023) also evidences our advantage in semantic control. We hope our attempts to train T2I models efficiently can offer valuable insights for the AIGC community and help more individual researchers or startups create their own high-quality T2I models at lower costs."
        },
        {
            "heading": "2 METHOD",
            "text": ""
        },
        {
            "heading": "2.1 MOTIVATION",
            "text": "The reasons for slow T2I training lie in two aspects: the training pipeline and the data.\nThe T2I generation task can be decomposed into three aspects: Capturing Pixel Dependency: Generating realistic images involves understanding intricate pixel-level dependencies within images and capturing their distribution; Alignment between Text and Image: Precise alignment learning is required for understanding how to generate images that accurately match the text description; High Aesthetic Quality: Besides faithful textual descriptions, being aesthetically pleasing is another vital attribute of generated images. Current methods entangle these three problems together and directly train from scratch using vast amount of data, resulting in inefficient training. To solve this issue, we disentangle these aspects into three stages, as will be described in Section 2.2.\nAnother problem, depicted in Figure 3, is with the quality of captions of the current dataset. The current text-image pairs often suffer from text-image misalignment, deficient descriptions, infrequent diverse vocabulary usage, and inclusion of low-quality data. These problems introduce difficulties in training, resulting in unnecessarily millions of iterations to achieve stable alignment between text and images. To address this challenge, we introduce an innovative auto-labeling pipeline to generate precise image captions, as will be described in Section 2.4."
        },
        {
            "heading": "2.2 TRAINING STRATEGY DECOMPOSITION",
            "text": "The model\u2019s generative capabilities can be gradually optimized by partitioning the training into three stages with different data types.\nStage1: Pixel dependency learning. The current class-guided approach (Peebles & Xie, 2023) has shown exemplary performance in generating semantically coherent and reasonable pixels in individual images. Training a class-conditional image generation model (Peebles & Xie, 2023) for natural images is relatively easy and inexpensive, as explained in Appendix A.5. Additionally, we\nfind that a suitable initialization can significantly boost training efficiency. Therefore, we boost our model from an ImageNet-pretrained model, and the architecture of our model is designed to be compatible with the pretrained weights.\nStage2: Text-image alignment learning. The primary challenge in transitioning from pretrained class-guided image generation to text-to-image generation is on how to achieve accurate alignment between significantly increased text concepts and images.\nThis alignment process is not only timeconsuming but also inherently challenging. To efficiently facilitate this process, we construct a dataset consisting of precise text-image pairs with high concept density. The data creation pipeline will be described in Section 2.4. By employing accurate and information-rich data, our training process can efficiently handle a larger number of nouns in each iteration while encountering considerably less ambiguity compared to previous datasets. This strategic approach empowers our network to align textual descriptions with images effectively.\nStage3: High-resolution and aesthetic image generation. In the third stage, we fine-tune our model using high-quality aesthetic data for high-resolution image generation. Remarkably, we observe that the adaptation process in this stage converges significantly faster, primarily owing to the strong prior knowledge established in the preceding stages.\nDecoupling the training process into different stages significantly alleviates the training difficulties and achieves highly efficient training."
        },
        {
            "heading": "2.3 EFFICIENT T2I TRANSFORMER",
            "text": "PIXART-\u03b1 adopts the Diffusion Transformer (DiT) (Peebles & Xie, 2023) as the base architecture and innovatively tailors the Transformer blocks to handle the unique challenges of T2I tasks, as depicted in Figure 4. Several dedicated designs are proposed as follows:\n\u2022 Cross-Attention layer. We incorporate a multi-head cross-attention layer to the DiT block. It is positioned between the self-attention layer and feed-forward layer so that the model can flexibly interact with the text embedding extracted from the language model. To facilitate the pretrained weights, we initialize the output projection layer in the cross-attention layer to zero, effectively acting as an identity mapping and preserving the input for the subsequent layers.\n\u2022 AdaLN-single. We find that the linear projections in the adaptive normalization layers (Perez et al., 2018) (adaLN) module of the DiT account for a substantial proportion (27%) of the parameters. Such a large number of parameters is not useful since the class condition is not employed for our T2I model. Thus, we propose adaLN-single, which only uses time embedding as input in the first block for independent control (shown on the right side of Figure 4). Specifically, in the ith block, let S(i) = [\u03b2(i)1 , \u03b2 (i) 2 , \u03b3 (i) 1 , \u03b3 (i) 2 , \u03b1 (i) 1 , \u03b1 (i) 2 ] be a tuple of all the scales and shift\nparameters in adaLN. In the DiT, S(i) is obtained through a block-specific MLP S(i) = f (i)(c+t), where c and t denotes the class condition and time embedding, respectively. However, in adaLNsingle, one global set of shifts and scales are computed as S = f(t) only at the first block which is shared across all the blocks. Then, S(i) is obtained as S(i) = g(S,E(i)), where g is a summation function, and E(i) is a layer-specific trainable embedding with the same shape as S, which adaptively adjusts the scale and shift parameters in different blocks.\n\u2022 Re-parameterization. To utilize the aforementioned pretrained weights, all E(i)\u2019s are initialized to values that yield the same S(i) as the DiT without c for a selected t (empirically, we use t = 500). This design effectively replaces the layer-specific MLPs with a global MLP and layer-specific trainable embeddings while preserving compatibility with the pretrained weights.\nExperiments demonstrate that incorporating a global MLP and layer-wise embeddings for time-step information, as well as cross-attention layers for handling textual information, persists the model\u2019s generative abilities while effectively reducing its size."
        },
        {
            "heading": "2.4 DATASET CONSTRUCTION",
            "text": "Image-text pair auto-labeling. The captions of the LAION dataset exhibit various issues, such as text-image misalignment, deficient descriptions, and infrequent vocabulary as shown in Figure 3. To generate captions with high information density, we leverage the state-of-the-art vision-language model LLaVA (Liu et al., 2023). Employing the prompt, \u201cDescribe this image and its style in a very detailed manner\u201d, we have significantly improved the quality of captions, as shown in Figure 3.\nHowever, it is worth noting that the LAION dataset predominantly comprises of simplistic product previews from shopping websites, which are not ideal for training text-to-image generation that seeks diversity in object combinations. Consequently, we have opted to utilize the SAM dataset (Kirillov et al., 2023), which is originally used for segmentation tasks but features imagery rich in diverse objects. By applying LLaVA to SAM, we have successfully acquired high-quality text-image pairs characterized by a high concept density, as shown in Figure 10 and Figure 11 in the Appendix.\nIn the third stage, we construct our training dataset by incorporating JourneyDB (Pan et al., 2023) and a 10M internal dataset to enhance the aesthetic quality of generated images beyond realistic photographs. Refer to Appendix A.5 for details.\nAs a result, we show the vocabulary analysis (NLTK, 2023) in Table 1, and we define the valid distinct nouns as those appearing more than 10 times in the dataset. We apply LLaVA on LAION to generate LAIONLLaVA. The LAION dataset has 2.46 M distinct nouns, but only 8.5% are valid. This valid noun proportion significantly increases from 8.5% to 13.3% with LLaVA-labeled captions. Despite\nLAION\u2019s original captions containing a staggering 210K distinct nouns, its total noun number is a mere 72M. However, LAION-LLaVA contains 234M noun numbers with 85K distinct nouns, and the average number of nouns per image increases from 6.4 to 21, indicating the incompleteness of the original LAION captions. Additionally, SAM-LLaVA outperforms LAION-LLaVA with a total noun number of 328M and 30 nouns per image, demonstrating SAM contains richer objectives and superior informative density per image. Lastly, the internal data also ensures sufficient valid nouns and average information density for fine-tuning. LLaVA-labeled captions significantly increase the valid ratio and average noun count per image, improving concept density."
        },
        {
            "heading": "3 EXPERIMENT",
            "text": "This section begins by outlining the detailed training and evaluation protocols. Subsequently, we provide comprehensive comparisons across three main metrics. We then delve into the critical designs implemented in PIXART-\u03b1 to achieve superior efficiency and effectiveness through ablation studies. Finally, we demonstrate the versatility of our PIXART-\u03b1 through application extensions."
        },
        {
            "heading": "3.1 IMPLEMENTATION DETAILS",
            "text": "Training Details. We follow Imagen (Saharia et al., 2022) and DeepFloyd (DeepFloyd, 2023) to employ the T5 large language model (i.e., 4.3B Flan-T5-XXL) as the text encoder for conditional\nfeature extraction, and use DiT-XL/2 (Peebles & Xie, 2023) as our base network architecture. Unlike previous works that extract a standard and fixed 77 text tokens, we adjust the length of extracted text tokens to 120, as the caption curated in PIXART-\u03b1 is much denser to provide more fine-grained details. To capture the latent features of input images, we employ a pre-trained and frozen VAE from LDM (Rombach et al., 2022). Before feeding the images into the VAE, we resize and center-crop them to have the same size. We also employ multi-aspect augmentation introduced in SDXL (Podell et al., 2023) to enable arbitrary aspect image generation. The AdamW optimizer (Loshchilov & Hutter, 2017) is utilized with a weight decay of 0.03 and a constant 2e-5 learning rate. Our final model is trained on 64 V100 for approximately 26 days. See more details in Appendix A.5.\nEvaluation Metrics. We comprehensively evaluate PIXART-\u03b1 via three primary metrics, i.e., Fre\u0301chet Inception Distance (FID) (Heusel et al., 2017) on MSCOCO dataset (Lin et al., 2014), compositionality on T2I-CompBench (Huang et al., 2023), and human-preference rate on user study."
        },
        {
            "heading": "3.2 PERFORMANCE COMPARISONS AND ANALYSIS",
            "text": "Fidelity Assessment. The FID is a metric to evaluate the quality of generated images. The comparison between our method and other methods in terms of FID and their training time is summarized in Table 2. When tested for zero-shot performance on the COCO dataset, PIXART-\u03b1 achieves a FID score of 7.32. It is particularly notable as it is accomplished in merely 12% of the training time (753 vs. 6250 A100 GPU days) and merely 1.25% of the training samples (25M vs. 2B images) relative to the second most efficient method. Compared to state-of-the-art methods typically trained using substantial resources, PIXART-\u03b1 remarkably consumes approximately 2% of the training resources while achieving a comparable FID performance. Although the best-performing model (RAPHEAL) exhibits a lower FID, it relies on unaffordable resources (i.e., 200\u00d7 more training samples, 80\u00d7 longer training time, and 5\u00d7 more network parameters than PIXART-\u03b1). We argue that FID may not be an appropriate metric for image quality evaluation, and it is more appropriate to use the evaluation of human users, as stated in Appendix A.8. We leave scaling of PIXART-\u03b1 for future exploration for performance enhancement.\nAlignment Assessment. Beyond the above evaluation, we also assess the alignment between the generated images and text condition using T2I-Compbench (Huang et al., 2023), a comprehensive benchmark for evaluating the compositional text-to-image generation capability. As depicted in Table 3, we evaluate several crucial aspects, including attribute binding, object relationships, and complex compositions. PIXART-\u03b1 exhibited outstanding performance across nearly all (5/6) evaluation metrics. This remarkable performance is primarily attributed to the text-image alignment learning in Stage 2 training described in Section 2.2, where high-quality text-image pairs were leveraged to achieve superior alignment capabilities.\nUser Study. While quantitative evaluation metrics measure the overall distribution of two image sets, they may not comprehensively evaluate the visual quality of the images. Consequently, we\n3To ensure fairness, we convert the V100 GPU days (1656) of our training to A100 GPU days (753), assuming a 2.2\u00d7 speedup in U-Net training on A100 compared to V100, or equivalent to 332 A100 GPU days with a 5\u00d7 speedup in Transformer training, as per Rombach et al. (2022); NVIDIA (2023).\nconducted a user study to supplement our evaluation and provide a more intuitive assessment of PIXART-\u03b1\u2019s performance. Since user study involves human evaluators and can be time-consuming, we selected the top-performing models, namely DALLE-2, SDv2, SDXL, and DeepFloyd, which are accessible through APIs and capable of generating images.\nAlignment Alignment Alignment AlignmentQuality Quality Quality Quality\nPe rc\nen ta\nge (\n% )\nFigure 5: User study on 300 fixed prompts from Feng et al. (2023). The ratio values indicate the percentages of participants preferring the corresponding model. PIXART-\u03b1 achieves a superior performance in both quality and alignment.\nFor each model, we employ a consistent set of 300 prompts from Feng et al. (2023) to generate images. These images are then distributed among 50 individuals for evaluation. Participants are asked to rank each model based on the perceptual quality of the generated images and the precision of alignments between the text prompts and the corresponding images. The results presented in Figure 5 clearly indicate that PIXART-\u03b1 excels in both higher fidelity and superior alignment. For example, compared to SDv2, a current top-tier T2I model, PIXART-\u03b1 exhibits a 7.2% improvement in image quality and a substantial 42.4% enhancement in alignment."
        },
        {
            "heading": "3.3 ABLATION STUDY",
            "text": "We then conduct ablation studies on the crucial modifications discussed in Section 2.3, including structure modifications and re-parameterization design. In Figure 6, we provide visual results and perform a FID analysis. We randomly choose 8 prompts from the SAM test set for visualization and compute the zero-shot FID-5K score on the SAM dataset. Details are described below.\n\u201cw/o re-param\u201d results are generated from the model trained from scratch without reparameterization design. We supplemented with an additional 200K iterations to compensate for the missing iterations from the pretraining stage for a fair comparison. \u201cadaLN\u201d results are from the model following the DiT structure to use the sum of time and text feature as input to the MLP layer for the scale and shift parameters within each block. \u201cadaLN-single\u201d results are obtained from the model using Transformer blocks with the adaLN-single module in Section 2.3. In both \u201cadaLN\u201d and \u201cadaLN-single\u201d, we employ the re-parameterization design and training for 200K iterations.\nAs depicted in Figure 6, despite \u201cadaLN\u201d performing lower FID, its visual results are on par with our \u201cadaLN-single\u201d design. The GPU memory consumption of \u201cadaLN\u201d is 29GB, whereas \u201cadaLN-\nsingle\u201d achieves a reduction to 23GB, saving 21% in GPU memory consumption. Furthermore, considering the model parameters, the \u201cadaLN\u201d method consumes 833M, whereas our approach reduces to a mere 611M, resulting in an impressive 26% reduction. \u201cadaLN-single-L (Ours)\u201d results are generated from the model with same setting as \u201cadaLN-single\u201d, but training for a Longer training period of 1500K iterations. Considering memory and parameter efficiency, we incorporate the \u201cadaLN-single-L\u201d into our final design.\nThe visual results clearly indicate that, although the differences in FID scores between the \u201cadaLN\u201d and \u201cadaLN-single\u201d models are relatively small, a significant discrepancy exists in their visual outcomes. The \u201cw/o re-param\u201d model consistently displays distorted target images and lacks crucial details across the entire test set."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "We review related works in three aspects: Denoising diffusion probabilistic models (DDPM), Latent Diffusion Model, and Diffusion Transformer. More related works can be found in Appendix A.1. DDPMs (Ho et al., 2020; Sohl-Dickstein et al., 2015) have emerged as highly successful approaches for image generation, which employs an iterative denoising process to transform Gaussian noise into an image. Latent Diffusion Model (Rombach et al., 2022) enhances the traditional DDPMs by employing score-matching on the image latent space and introducing cross-attention-based controlling. Witnessed the success of Transformer architecture on many computer vision tasks, Diffusion Transformer (DiT) (Peebles & Xie, 2023) and its variant (Bao et al., 2023; Zheng et al., 2023) further replace the Convolutional-based U-Net (Ronneberger et al., 2015) backbone with Transformers for increased scalability (Chen et al., 2023)."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we introduced PIXART-\u03b1, a Transformer-based text-to-image (T2I) diffusion model, which achieves superior image generation quality while significantly reducing training costs and CO2 emissions. Our three core designs, including the training strategy decomposition, efficient T2I Transformer and high-informative data, contribute to the success of PIXART-\u03b1. Through extensive experiments, we have demonstrated that PIXART-\u03b1 achieves near-commercial application standards in image generation quality. With the above designs, PIXART-\u03b1 provides new insights to the AIGC community and startups, enabling them to build their own high-quality yet low-cost T2I models. We hope that our work inspires further innovation and advancements in this field.\nAcknowledgement. We would like to express our gratitude to Shuchen Xue for identifying and correcting the FID score in the paper. This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 16200021)."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 RELATED WORK",
            "text": ""
        },
        {
            "heading": "A.1.1 DENOISING DIFFUSION PROBABILISTIC MODELS",
            "text": "Diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015) and score-based generative models (Song & Ermon, 2019; Song et al., 2021) have emerged as highly successful approaches for image generation, surpassing previous generative models such as GANs (Goodfellow et al., 2014), VAEs (Kingma & Welling, 2013), and Flow (Rezende & Mohamed, 2015). Unlike traditional models that directly map from a Gaussian distribution to the data distribution, diffusion models employ an iterative denoising process to transform Gaussian noise into an image that follows the data distribution. This process can be reversely learned from an untrainable forward process, where a small amount of Gaussian noise is iteratively added to the original image."
        },
        {
            "heading": "A.1.2 LATENT DIFFUSION MODEL",
            "text": "Latent Diffusion Model (a.k.a. Stable diffusion) (Rombach et al., 2022) is a recent advancement in diffusion models. This approach enhances the traditional diffusion model by employing scorematching on the image latent space and introducing cross-attention-based controlling. The results obtained with this approach have been impressive, particularly in tasks involving high-density image generation, such as text-to-image synthesis. This has served as a source of inspiration for numerous subsequent works aimed at improving text-to-image synthesis, including those by Saharia et al. (2022); Balaji et al. (2022); Feng et al. (2023); Xue et al. (2023b); Podell et al. (2023), and others. Additionally, Stable diffusion and its variants have been effectively combined with various low-cost fine-tuning (Hu et al., 2021; Xie et al., 2023) and customization (Zhang et al., 2023; Mou et al., 2023) technologies."
        },
        {
            "heading": "A.1.3 DIFFUSION TRANSFORMER",
            "text": "Transformer architecture (Vaswani et al., 2017) have achieved great success in language models (Radford et al., 2018; 2019), and many recent works (Dosovitskiy et al., 2020a; He et al., 2022) show it is also a promising architecture on many computer vision tasks like image classification (Touvron et al., 2021; Zhou et al., 2021; Yuan et al., 2021; Han et al., 2021), object detection (Liu et al., 2021; Wang et al., 2021; 2022; Ge et al., 2023; Carion et al., 2020), semantic segmentation (Zheng et al., 2021; Xie et al., 2021; Strudel et al., 2021) and so on (Sun et al., 2020; Li et al., 2022b; Zhao et al., 2021; Liu et al., 2022; He et al., 2022; Li et al., 2022a). The Diffusion Transformer (DiT) (Peebles & Xie, 2023) and its variant (Bao et al., 2023; Zheng et al., 2023) follow the step to further replace the Convolutional-based U-Net (Ronneberger et al., 2015) backbone with Transformers. This architectural choice brings about increased scalability (Chen et al., 2023) compared to U-Net-based diffusion models, allowing for the straightforward expansion of its parameters. In our paper, we leverage DiT as a scalable foundational model and adapt it for text-to-image generation tasks.\nA.2 PIXART-\u03b1 vs. MIDJOURNEY\nIn Figure 7, we present the images generated using PIXART-\u03b1 and the current SOTA product-level method Midjourney (Midjourney, 2023) with randomly sampled prompts online. Here, we conceal the annotations of images belonging to which method. Readers are encouraged to make assessments based on the prompts provided. The answers will be disclosed at the end of the appendix.\nA.3 PIXART-\u03b1 vs. PRESTIGIOUS DIFFUSION MODELS\nIn Figure 8 and 9, we present the comparison results using a test prompt selected by RAPHAEL. The instances depicted here exhibit performance that is on par with, or even surpasses, that of existing powerful generative models."
        },
        {
            "heading": "A.4 AUTO-LABELING TECHNIQUES",
            "text": "To generate captions with high information density, we leverage state-of-the-art vision-language models LLaVA (Liu et al., 2023). Employing the prompt, \u201cDescribe this image and its style in a very detailed manner\u201d, we have significantly improved the quality of captions. We show the prompt design and process of auto-labeling in Figure 10. More image-text pair samples on the SAM dataset are shown in Figure 11."
        },
        {
            "heading": "A.5 ADDITIONAL IMPLEMENTATION DETAILS",
            "text": "We include detailed information about all of our PIXART-\u03b1 models in this section. As shown in Table 4, among the 256\u00d7256 phases, our model primarily focuses on the text-to-image alignment stage, with less time on fine-tuning and only 1/8 of that time spent on ImageNet pixel dependency.\nPIXART-\u03b1 model details. For the embedding of input timesteps, we employ a 256-dimensional frequency embedding (Dhariwal & Nichol, 2021). This is followed by a two-layer MLP that features a dimensionality matching the transformer\u2019s hidden size, coupled with SiLU activations. We adopt the DiT-XL model, which has 28 Transformer blocks in total for better performance, and the patch size of the PatchEmbed layer in ViT (Dosovitskiy et al., 2020b) is 2\u00d7.\nMulti-scale training. Inspired by Podell et al. (2023), we incorporate the multi-scale training strategy into our pipeline. Specifically, We divide the image size into 40 buckets with different aspect ratios, each with varying aspect ratios ranging from 0.25 to 4, mirroring the method used in SDXL. During optimization, a training batch is composed using images from a single bucket, and we alternate the bucket sizes for each training step. In practice, we only apply multi-scale training in the high-aesthetics stage after pretraining the model at a fixed aspect ratio and resolution (i.e. 256px). We adopt the positional encoding trick in DiffFit (Xie et al., 2023) since the image resolution and aspect change during different training stages.\nAdditional time consumption. Beside the training time discussed in Table 4, data labeling and VAE training may need additional time. We treat the pre-trained VAE as a ready-made component of a model zoo, the same as pre-trained CLIP/T5-XXL text encoder, and our total training process does not include the training of VAE. However, our attempt to train a VAE resulted in an approximate training duration of 25 hours, utilizing 64 V100 GPUs on the OpenImage dataset. As for autolabeling, we use LLAVA-7B to generate captions. LLaVA\u2019s annotation time on the SAM dataset is approximately 24 hours with 64 V100 GPUs. To ensure a fair comparison, we have temporarily excluded the training time and data quantity of VAE training, T5 training time, and LLaVA autolabeling time.\nSampling algorithm. In this study, we incorporated three sampling algorithms, namely iDDPM (Nichol & Dhariwal, 2021), DPM-Solver (Lu et al., 2022), and SA-Solver (Xue et al., 2023a). We observe these three algorithms perform similarly in terms of semantic control, albeit with minor differences in sampling frequency and color representation. To optimize computational efficiency, we ultimately chose to employ the DPM-Solver with 20 inference steps."
        },
        {
            "heading": "A.6 HYPER-PARAMETERS ANALYSIS",
            "text": "In Figure 20, we illustrate the variations in the model\u2019s metrics under different configurations across various datasets. we first investigate FID for the model and plot FID-vs-CLIP curves in Figure 20a for 10k text-image paed from MSCOCO. The results show a marginal enhancement over SDv1.5. In Figure 20b and 20c, we demonstrate the corresponding T2ICompBench scores across a range of classifier-free guidance (cfg) (Ho & Salimans, 2022) scales. The outcomes reveal a consistent and commendable model performance under these varying scales.\nA.7 MORE IMAGES GENERATED BY PIXART-\u03b1\nMore visual results generated by PIXART-\u03b1 are shown in Figure 12, 13, and 14. The samples generated by PIXART-\u03b1 demonstrate outstanding quality, marked by their exceptional fidelity and precision in faithfully adhering to the given textual descriptions. As depicted in Figure 15, PIXART-\u03b1 demonstrates the ability to synthesize high-resolution images up to 1024\u00d7 1024 pixels and contains rich details, and is capable of generating images with arbitrary aspect ratios, enhancing its versatility for real-world applications. Figure 16 illustrates PIXART-\u03b1\u2019s remarkable capacity to manipulate image styles through text prompts directly, demonstrating its versatility and creativity."
        },
        {
            "heading": "A.8 DISCCUSION OF FID METRIC FOR EVALUATING IMAGE QUALITY",
            "text": "During our experiments, we observed that the FID (Fre\u0301chet Inception Distance) score may not accurately reflect the visual quality of generated images. Recent studies such as SDXL (Podell et al., 2023) and Pick-a-pic (Kirstain et al., 2023) have presented evidence suggesting that the COCO zero-shot FID is negatively correlated with visual aesthetics.\nFurthermore, it has been stated by Betzalel et al. (Betzalel et al., 2022) that the feature extraction network used in FID is pretrained on the ImageNet dataset, which exhibits limited overlap with the current text-to-image generation data. Consequently, FID may not be an appropriate metric for evaluating the generative performance of such models, and (Betzalel et al., 2022) recommended employing human evaluators for more suitable assessments.\nThus, we conducted a user study to validate the effectiveness of our method."
        },
        {
            "heading": "A.9 CUSTOMIZED EXTENSION",
            "text": "In text-to-image generation, the ability to customize generated outputs to a specific style or condition is a crucial application. We extend the capabilities of PIXART-\u03b1 by incorporating two commonly used customization methods: DreamBooth (Ruiz et al., 2022) and ControlNet (Zhang et al., 2023).\nDreamBooth. DreamBooth can be seamlessly applied to PIXART-\u03b1 without further modifications. The process entails fine-tuning PIXART-\u03b1 using a learning rate of 5e-6 for 300 steps, without the incorporation of a class-preservation loss.\nAs depicted in Figure 17a, given a few images and text prompts, PIXART-\u03b1 demonstrates the capacity to generate high-fidelity images. These images present natural interactions with the environment under various lighting conditions. Additionally, PIXART-\u03b1 is also capable of precisely modifying the attribute of a specific object such as color, as shown in 17b. Our appealing visual results demonstrate PIXART-\u03b1 can generate images of exceptional quality and its strong capability for customized extension.\nControlNet. Following the general design of ControlNet (Zhang et al., 2023), we freeze each DiT Block and create a trainable copy, augmenting with two zero linear layers before and after it. The control signal c is obtained by applying the same VAE to the control image and is shared among all blocks. For each block, we process the control signal c by first passing it through the first zero linear layer, adding it to the layer input x, and then feeding it into the trainable copy and the second zero linear layer. The processed control signal is then added to the output y of the frozen block, which is obtained from input x. We trained the ControlNet on HED (Xie & Tu, 2015) signals using a learning rate of 5e-6 for 20,000 steps.\nAs depicted in Figure 18, when provided with a reference image and control signals, such as edge maps, we leverage various text prompts to generate a wide range of high-fidelity and diverse images. Our results demonstrate the capacity of PIXART-\u03b1 to yield personalized extensions of exceptional quality.\nA.10 DISCUSSION ON TRANSFORMER vs. U-NET\nThe Transformer-based network\u2019s superiority over convolutional networks has been widely established in various studies, showcasing attributes such as robustness (Zhou et al., 2022; Xie et al., 2021), effective modality fusion (Girdhar et al., 2023), and scalability (Peebles & Xie, 2023). Similarly, the findings on multi-modality fusion are consistent with our observations in this study compared to the CNN-based generator (U-Net). For instance, Table 3 illustrates that our model, PIXART\u03b1, significantly outperforms prevalent U-Net generators in terms of compositionality. This advantage is not solely due to the high-quality alignment achieved in the second training stage but also to the multi-head attention-based fusion mechanism, which excels at modeling long dependencies. This mechanism effectively integrates compositional semantic information, guiding the generation of vision latent vectors more efficiently and producing images that closely align with the input texts. These findings underscore the unique advantages of Transformer architectures in effectively fusing multi-modal information."
        },
        {
            "heading": "A.11 LIMITATIONS & FAILURE CASES",
            "text": "In Figure 19, we highlight the model\u2019s failure cases in red text and yellow circle. Our analysis reveals the model\u2019s weaknesses in accurately controlling the number of targets and handling specific details, such as features of human hands. Additionally, the model\u2019s text generation capability is somewhat weak due to our data\u2019s limited number of font and letter-related images. We aim to explore these unresolved issues in the generation field, enhancing the model\u2019s abilities in text generation, detail control, and quantity control in the future."
        },
        {
            "heading": "A.12 UNVEIL THE ANSWER",
            "text": "In Figure 7, we present a comparison between PIXART-\u03b1 and Midjourney and conceal the correspondence between images and their respective methods, inviting the readers to guess. Finally, in Figure 21, we unveil the answer to this question. It is difficult to distinguish between PIXART-\u03b1 and Midjourney, which demonstrates PIXART-\u03b1\u2019s exceptional performance."
        },
        {
            "heading": "2. A shanty version of Tokyo, new rustic style, bold colors with all colors palette, video game, genshin, tribe, fantasy, overwatch.",
            "text": ""
        },
        {
            "heading": "1. A cute little matte low poly isometric cherry blossom forest island, waterfalls, lighting, soft shadows, trending on Artstation, 3d render, monument valley, fez video game.",
            "text": "marvel movie character, iron man, dress up to match movie character, full body photo, American apartment, lying down, life in distress, messy, lost hope, food, wine, hd, 8k, real, reality, super detail, 8k post photo manipulation, real photo\nA worker that looks like a mixture of cow and horse is working hard to type code A female painter with a brush in hand, white background, painting, looking very powerful.\nIn this figure, we generate five outputs using the styles to control the objects . For instance, the second picture of the first sample, located at the left corner of the figure, uses the prompt \u201c Pixel Art of the black hole in the space \u201d. Better zoom in 200%."
        }
    ],
    "title": "FORMER FOR PHOTOREALISTIC TEXT-TO-IMAGE SYNTHESIS",
    "year": 2024
}