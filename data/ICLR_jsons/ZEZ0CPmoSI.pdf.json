{
    "abstractText": "This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective\u2019s structure and lead to faster convergence compared to a scalar stepsize. As a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. Moreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hanmin Li"
        },
        {
            "affiliations": [],
            "name": "Avetik Karagulyan"
        },
        {
            "affiliations": [],
            "name": "Peter Richt\u00e1rik"
        },
        {
            "affiliations": [],
            "name": "King Abdullah"
        }
    ],
    "id": "SP:7dffe326f5d471224dfd22dedf0359ee991e1bf1",
    "references": [
        {
            "authors": [
                "Mehiddin Al-Baali",
                "H Khalfan"
            ],
            "title": "An overview of some practical quasi-newton methods for unconstrained optimization. Sultan Qaboos University Journal for Science [SQUJS",
            "year": 2007
        },
        {
            "authors": [
                "Mehiddin Al-Baali",
                "Emilio Spedicato",
                "Francesca Maggioni"
            ],
            "title": "Broyden\u2019s quasi-Newton methods for a nonlinear system of equations and unconstrained optimization: a review and open problems",
            "venue": "Optimization Methods and Software,",
            "year": 2014
        },
        {
            "authors": [
                "Dan Alistarh",
                "Demjan Grubic",
                "Jerry Li",
                "Ryota Tomioka",
                "Milan Vojnovic"
            ],
            "title": "QSGD: Communicationefficient SGD via gradient quantization and encoding",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Charles G Broyden"
            ],
            "title": "A class of methods for solving nonlinear simultaneous equations",
            "venue": "Mathematics of computation,",
            "year": 1965
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck"
            ],
            "title": "Convex optimization: Algorithms and complexity",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Chih-Chung Chang",
                "Chih-Jen Lin"
            ],
            "title": "Libsvm: a library for support vector machines",
            "venue": "ACM transactions on intelligent systems and technology (TIST),",
            "year": 2011
        },
        {
            "authors": [
                "Marina Danilova",
                "Pavel Dvurechensky",
                "Alexander Gasnikov",
                "Eduard Gorbunov",
                "Sergey Guminov",
                "Dmitry Kamzolov",
                "Innokentiy Shibaev"
            ],
            "title": "Recent theoretical advances in non-convex optimization. In High-Dimensional Optimization and Probability: With a View",
            "venue": "Towards Data Science,",
            "year": 2022
        },
        {
            "authors": [
                "John E Dennis",
                "Jr.",
                "Jorge J Mor\u00e9"
            ],
            "title": "Quasi-Newton methods, motivation and theory",
            "venue": "SIAM review,",
            "year": 1977
        },
        {
            "authors": [
                "Steven Diamond",
                "Stephen Boyd"
            ],
            "title": "CVXPY: A Python-embedded modeling language for convex optimization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Aritra Dutta",
                "El Houcine Bergou",
                "Ahmed M Abdelmoniem",
                "Chen-Yu Ho",
                "Atal Narayan Sahu",
                "Marco Canini",
                "Panos Kalnis"
            ],
            "title": "On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Darina Dvinskikh",
                "Aleksandr Ogaltsov",
                "Alexander Gasnikov",
                "Pavel Dvurechensky",
                "Alexander Tyurin",
                "Vladimir Spokoiny"
            ],
            "title": "Adaptive gradient descent for convex and non-convex stochastic optimization",
            "year": 1911
        },
        {
            "authors": [
                "Boris Ginsburg",
                "Patrice Castonguay",
                "Oleksii Hrinchuk",
                "Oleksii Kuchaiev",
                "Vitaly Lavrukhin",
                "Ryan Leary",
                "Jason Li",
                "Huyen Nguyen",
                "Yang Zhang",
                "Jonathan M Cohen"
            ],
            "title": "Stochastic gradient methods with layer-wise adaptive moments for training of deep networks",
            "venue": "arXiv preprint arXiv:1905.11286,",
            "year": 2019
        },
        {
            "authors": [
                "Eduard Gorbunov",
                "Konstantin P Burlachenko",
                "Zhize Li",
                "Peter Richt\u00e1rik. Marina"
            ],
            "title": "Faster nonconvex distributed learning with compression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Robert M Gower",
                "Peter Richt\u00e1rik"
            ],
            "title": "Randomized iterative methods for linear systems",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2015
        },
        {
            "authors": [
                "Robert Mansel Gower",
                "Nicolas Loizou",
                "Xun Qian",
                "Alibek Sailanbayev",
                "Egor Shulgin",
                "Peter Richt\u00e1rik"
            ],
            "title": "Sgd: General analysis and improved rates",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "William B Gragg",
                "Richard A Tapia"
            ],
            "title": "Optimal error bounds for the Newton\u2013Kantorovich theorem",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 1974
        },
        {
            "authors": [
                "Michael Grant",
                "Stephen Boyd",
                "Yinyu Ye"
            ],
            "title": "Disciplined convex programming",
            "venue": "Global optimization: From theory to implementation,",
            "year": 2006
        },
        {
            "authors": [
                "SV Guminov",
                "Yu E Nesterov",
                "PE Dvurechensky",
                "AV Gasnikov"
            ],
            "title": "Accelerated primal-dual gradient descent with linesearch for convex, nonconvex, and nonsmooth optimization problems",
            "venue": "In Doklady Mathematics,",
            "year": 2019
        },
        {
            "authors": [
                "Filip Hanzely",
                "Konstantin Mishchenko",
                "Peter Richt\u00e1rik"
            ],
            "title": "SEGA: Variance reduction via gradient sketching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Samuel Horvath",
                "Chen-Yu Ho",
                "Ludovit Horvath",
                "Atal Narayan Sahu",
                "Marco Canini",
                "Peter Richtarik"
            ],
            "title": "Natural compression for distributed deep learning",
            "year": 1905
        },
        {
            "authors": [
                "Samuel Horv\u00e1th",
                "Dmitry Kovalev",
                "Konstantin Mishchenko",
                "Peter Richt\u00e1rik",
                "Sebastian Stich"
            ],
            "title": "Stochastic distributed learning with gradient quantization and double-variance reduction",
            "venue": "Optimization Methods and Software,",
            "year": 2023
        },
        {
            "authors": [
                "Rustem Islamov",
                "Xun Qian",
                "Peter Richt\u00e1rik"
            ],
            "title": "Distributed second order methods with fast rates and compressed communication",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sashank J Reddi",
                "Suvrit Sra",
                "Barnabas Poczos",
                "Alexander J Smola"
            ],
            "title": "Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Prateek Jain",
                "Purushottam Kar"
            ],
            "title": "Non-convex optimization for machine learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ahmed Khaled",
                "Peter Richt\u00e1rik"
            ],
            "title": "Better theory for sgd in the nonconvex world",
            "venue": "arXiv preprint arXiv:2002.03329,",
            "year": 2020
        },
        {
            "authors": [
                "Sarit Khirirat",
                "Hamid Reza Feyzmahdavian",
                "Mikael Johansson"
            ],
            "title": "Distributed learning with compressed gradients",
            "venue": "arXiv preprint arXiv:1806.06573,",
            "year": 2018
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u1ef3",
                "H Brendan McMahan",
                "Felix X Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated learning: Strategies for improving communication efficiency",
            "venue": "arXiv preprint arXiv:1610.05492,",
            "year": 2016
        },
        {
            "authors": [
                "Yin Tat Lee",
                "Zhao Song",
                "Qiuyi Zhang"
            ],
            "title": "Solving empirical risk minimization in the current matrix multiplication time",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Zhize Li",
                "Dmitry Kovalev",
                "Xun Qian",
                "Peter Richt\u00e1rik"
            ],
            "title": "Acceleration for compressed gradient descent in distributed and federated optimization",
            "venue": "arXiv preprint arXiv:2002.11364,",
            "year": 2020
        },
        {
            "authors": [
                "Zhize Li",
                "Hongyan Bao",
                "Xiangliang Zhang",
                "Peter Richt\u00e1rik"
            ],
            "title": "PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Artavazd Maranjyan",
                "Mher Safaryan",
                "Peter Richt\u00e1rik"
            ],
            "title": "GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity",
            "venue": "arXiv preprint arXiv:2210.16402,",
            "year": 2022
        },
        {
            "authors": [
                "H. Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2017
        },
        {
            "authors": [
                "George J Miel"
            ],
            "title": "Majorizing sequences and error bounds for iterative methods",
            "venue": "Mathematics of Computation,",
            "year": 1980
        },
        {
            "authors": [
                "Konstantin Mishchenko",
                "Eduard Gorbunov",
                "Martin Tak\u00e1\u010d",
                "Peter Richt\u00e1rik"
            ],
            "title": "Distributed learning with compressed gradient differences",
            "venue": "arXiv preprint arXiv:1901.09269,",
            "year": 2019
        },
        {
            "authors": [
                "Konstantin Mishchenko",
                "Grigory Malinovsky",
                "Sebastian Stich",
                "Peter Richtarik"
            ],
            "title": "ProxSkip: Yes! Local gradient steps provably lead to communication acceleration! Finally",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Moulines",
                "Francis Bach"
            ],
            "title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Lianke Qin",
                "Zhao Song",
                "Lichen Zhang",
                "Danyang Zhuo"
            ],
            "title": "An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Richt\u00e1rik",
                "Igor Sokolov",
                "Ilyas Fatkhullin"
            ],
            "title": "EF21: A new, simpler, theoretically better, and practically faster error feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mher Safaryan",
                "Filip Hanzely",
                "Peter Richt\u00e1rik"
            ],
            "title": "Smoothness matrices beat smoothness constants: Better communication compression techniques for distributed optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mher Safaryan",
                "Egor Shulgin",
                "Peter Richt\u00e1rik"
            ],
            "title": "Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2022
        },
        {
            "authors": [
                "Zhao Song",
                "Yitan Wang",
                "Zheng Yu",
                "Lichen Zhang"
            ],
            "title": "Sketching for first order method: Efficient algorithm for low-bandwidth channel and vulnerability",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Sebastian U Stich"
            ],
            "title": "Unified optimal analysis of the (stochastic) gradient method",
            "venue": "arXiv preprint arXiv:1907.04232,",
            "year": 2019
        },
        {
            "authors": [
                "Bokun Wang",
                "Mher Safaryan",
                "Peter Richt\u00e1rik"
            ],
            "title": "Theoretically better and numerically faster distributed optimization with smoothness-aware quantization techniques",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tetsuro Yamamoto"
            ],
            "title": "A convergence theorem for Newton-like methods in Banach spaces",
            "venue": "Numerische Mathematik,",
            "year": 1987
        },
        {
            "authors": [
                "Jason Yosinski",
                "Jeff Clune",
                "Anh Nguyen",
                "Thomas Fuchs",
                "Hod Lipson"
            ],
            "title": "Understanding neural networks through deep visualization",
            "venue": "arXiv preprint arXiv:1506.06579,",
            "year": 2015
        },
        {
            "authors": [
                "Adams Wei Yu",
                "Lei Huang",
                "Qihang Lin",
                "Ruslan Salakhutdinov",
                "Jaime Carbonell"
            ],
            "title": "Blocknormalized gradient method: An empirical study for training deep neural network",
            "venue": "arXiv preprint arXiv:1707.04822,",
            "year": 2017
        },
        {
            "authors": [
                "Jingzhao Zhang",
                "Sai Praneeth Karimireddy",
                "Andreas Veit",
                "Seungyeon Kim",
                "Sashank Reddi",
                "Sanjiv Kumar",
                "Suvrit Sra"
            ],
            "title": "Why are adaptive methods good for attention models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Qinghe Zheng",
                "Xinyu Tian",
                "Nan Jiang",
                "Mingqiang Yang"
            ],
            "title": "Layer-wise learning based stochastic gradient descent method for the optimization of deep convolutional neural network",
            "venue": "Journal of Intelligent & Fuzzy Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Luisa M Zintgraf",
                "Taco S Cohen",
                "Tameem Adel",
                "Max Welling"
            ],
            "title": "Visualizing deep neural network decisions: Prediction difference analysis",
            "venue": "arXiv preprint arXiv:1702.04595,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The minimization of smooth and non-convex functions is a fundamental problem in various domains of applied mathematics. Most machine learning algorithms rely on solving optimization problems for training and inference, often with structural constraints or non-convex objectives to accurately capture the learning and prediction problems in high-dimensional or non-linear spaces. However, non-convex problems are typically NP-hard to solve, leading to the popular approach of relaxing them to convex problems and using traditional methods. Direct approaches to non-convex optimization have shown success but their convergence and properties are not well understood, making them challenging for large scale optimization. While its convex alternative has been extensively studied and is generally an easier problem, the non-convex setting is of greater practical interest often being the computational bottleneck in many applications.\nIn this paper, we consider the general minimization problem:\nmin x\u2208Rd f(x), (1)\nwhere f : Rd \u2192 R is a differentiable function. In order for this problem to have a finite solution we will assume throughout the paper that f is bounded from below.\nAssumption 1. There exists f inf \u2208 R such that f(x) \u2265 f inf for all x \u2208 Rd.\nThe stochastic gradient descent (SGD) algorithm (Moulines & Bach, 2011; Bubeck et al., 2015; Gower et al., 2019) is one of the most common algorithms to solve this problem. In its most general form, it can be written as xk+1 = xk \u2212 \u03b3g(xk), (2) where g(xk) is a stochastic estimator of \u2207f(xk) and \u03b3 > 0 is a positive scalar stepsize. A particular case of interest is the compressed gradient descent (CGD) algorithm (Khirirat et al., 2018), where the estimator g is taken as a compressed alternative of the initial gradient:\ng(xk) = C(\u2207f(xk)), (3)\nand the compressor C is chosen to be a \"sparser\" estimator that aims to reduce the communication overhead in distributed or federated settings. This is crucial, as highlighted in the seminal paper by\nKonec\u030cny\u0300 et al. (2016), which showed that the bottleneck of distributed optimization algorithms is the communication complexity. In order to deal with the limited resources of current devices, there are various compression objectives that are practical to achieve. These include also compressing the model broadcasted from server to clients for local training, and reducing the computational burden of local training. These objectives are mostly complementary, but compressing gradients has the potential for the greatest practical impact due to slower upload speeds of client connections and the benefits of averaging Kairouz et al. (2021). In this paper we will focus on this latter problem.\nAn important subclass of compressors are the sketches. Sketches are linear operators defined on Rd, i.e., C(y) = Sy for every y \u2208 Rd, where S is a random matrix. A standard example of such a compressor is the Rand-k compressor, which randomly chooses k entries of its argument and scales them with a scalar multiplier to make the estimator unbiased. Instead of communicating all d coordinates of the gradient, one communicates only a subset of size k, thus reducing the number of communicated bits by a factor of d/k. Formally, Rand-k is defined as follows: S = dk \u2211k j=1 eije \u22a4 ij\n, where eij is the ij-th standard basis vector in Rd. We refer the reader to (Safaryan et al., 2022) for an overview on compressions.\nBesides the assumption that function f is bounded from below, we also assume that it is L matrix smooth, as we are trying to take advantage of the entire information contained in the smoothness matrix L and the stepsize matrix D.\nAssumption 2 (Matrix smoothness). There exists L \u2208 Sd+ such that\nf(x) \u2264 f(y) + \u27e8\u2207f(y), x\u2212 y\u27e9+ 1 2 \u27e8L(x\u2212 y), x\u2212 y\u27e9 (4)\nholds for all x, y \u2208 Rd.\nThe assumption of matrix smoothness, which is a generalization of scalar smoothness, has been shown to be a more powerful tool for improving supervised model training. In Safaryan et al. (2021), the authors proposed using smoothness matrices and suggested a novel communication sparsification strategy to reduce communication complexity in distributed optimization for convex objectives. The technique was adapted to three distributed optimization algorithms in the convex setting, resulting in significant communication complexity savings and consistently outperforming the baselines. The results of this study demonstrate the efficacy of the matrix smoothness assumption in improving distributed optimization algorithms.\nThe case of block-diagonal smoothness matrices is particularly relevant in various applications, such as neural networks (NN). In this setting, each block corresponds to a layer of the network, and we characterize the smoothness with respect to nodes in the i-th layer by a corresponding matrix Li. Unlike in the scalar setting, we favor the similarity of certain entries of the argument over the others. This is because the information carried by the layers becomes more complex, while the nodes in the same layers are similar. This phenomenon has been observed visually in various studies, such as those by Yosinski et al. (2015) and Zintgraf et al. (2017).\nWe study two matrix stepsized CGD-type algorithms and analyze their convergence properties for non-convex matrix-smooth functions. As mentioned earlier, we put special emphasis on the blockdiagonal case. We design our sketches and stepsizes in a way that leverages this structure, and we show that in certain cases, we can achieve compression without losing in the overall communication complexity."
        },
        {
            "heading": "1.1 RELATED WORK",
            "text": "Many successful convex optimization techniques have been adapted for use in the non-convex setting. Here is a non-exhaustive list: adaptivity (Dvinskikh et al., 2019; Zhang et al., 2020), variance reduction (J Reddi et al., 2016; Li et al., 2021), and acceleration (Guminov et al., 2019). A paper of particular importance for our work is that of Khaled & Richt\u00e1rik (2020), which proposes a unified scheme for analyzing stochastic gradient descent in the non-convex regime. A comprehensive overview of non-convex optimization can be found in (Jain et al., 2017; Danilova et al., 2022).\nA classical example of a matrix stepsized method is Newton\u2019s method. This method has been popular in the optimization community for a long time (Gragg & Tapia, 1974; Miel, 1980; Yamamoto, 1987).\nHowever, computing the stepsize as the inverse Hessian of the current iteration results in significant computational complexity. Instead, quasi-Newton methods use an easily computable estimator to replace the inverse of the Hessian (Broyden, 1965; Dennis & Mor\u00e9, 1977; Al-Baali & Khalfan, 2007; Al-Baali et al., 2014). An example is the Newton-Star algorithm (Islamov et al., 2021), which we discuss in Section 2.\nGower & Richt\u00e1rik (2015) analyzed sketched gradient descent by making the compressors unbiased with a sketch-and-project trick. They provided an analysis of the resulting algorithm for the linear feasibility problem. Later, Hanzely et al. (2018) proposed a variance-reduced version of this method. Sketches are also of independent interest. In particular, Song et al. (2023) described a way of designing the distribution of sketch matrices, while Lee et al. (2019); Qin et al. (2023) used sketches in solving empirical risk minimization problems.\nLeveraging the layer-wise structure of neural networks has been widely studied for optimizing the training loss function. For example, (Zheng et al., 2019) propose SGD with different scalar stepsizes for each layer, (Yu et al., 2017; Ginsburg et al., 2019) propose layer-wise normalization for Stochastic Normalized Gradient Descent, and (Dutta et al., 2020; Wang et al., 2022) propose layer-wise compression in the distributed setting.\nDCGD, proposed by Khirirat et al. (2018), has since been improved in various ways, such as in (Horvath et al., 2019; Li et al., 2020). There is also a large body of literature on other federated learning algorithms with unbiased compressors (Alistarh et al., 2017; Mishchenko et al., 2019; Gorbunov et al., 2021; Mishchenko et al., 2022; Maranjyan et al., 2022; Horv\u00e1th et al., 2023)."
        },
        {
            "heading": "1.2 CONTRIBUTIONS",
            "text": "Our paper contributes in the following ways:\n\u2022 We propose two novel matrix stepsize sketch CGD algorithms in Section 2, which, to the best of our knowledge, are the first attempts to analyze a fixed matrix stepsize for nonconvex optimization. We present a unified theorem in Section 3 that guarantees stationarity for minimizing matrix-smooth non-convex functions. The results show that taking our algorithms improve on their scalar alternatives. The complexities are summarized in Table 1 for some particular cases.\n\u2022 We design our algorithms\u2019 sketches and stepsize to take advantage of the layer-wise structure of neural networks, assuming that the smoothness matrix is block-diagonal. In Section 4, we prove that our algorithms achieve better convergence than classical methods.\n\u2022 Assuming the that the server-to-client communication is less expensive Konec\u030cny\u0300 et al. (2016); Kairouz et al. (2021), we propose distributed versions of our algorithms in Section 5, following the standard FL scheme, and prove weighted stationarity guarantees. Our theorem recovers the result for DCGD in the scalar case and improves it in general.\n\u2022 We validate our theoretical results with experiments. The plots and framework are provided in the Appendix."
        },
        {
            "heading": "1.3 PRELIMINARIES",
            "text": "The usual Euclidean norm on Rd is defined as \u2225\u00b7\u2225. We use bold capital letters to denote matrices. By Id we denote the d \u00d7 d identity matrix, and by Od we denote the d \u00d7 d zero matrix. Let Sd++ (resp. Sd+) be the set of d \u00d7 d symmetric positive definite (resp. semi-definite) matrices. Given Q \u2208 Sd++ and x \u2208 Rd, we write \u2225x\u2225Q := \u221a \u27e8Qx, x\u27e9, where \u27e8\u00b7, \u00b7\u27e9 is the standard Euclidean inner product on Rd. For a matrix A \u2208 Sd++, we define by \u03bbmax(A) (resp. \u03bbmin(A)) the largest (resp. smallest) eigenvalue of the matrix A. Let Ai \u2208 Rdi\u00d7di and d = d1 + . . . + d\u2113. Then the matrix A = Diag(A1, . . . ,A\u2113) is defined as a block diagonal d \u00d7 d matrix where the i-th block is equal to Ai. We will use diag(A) \u2208 Rd\u00d7d to denote the diagonal of any matrix A \u2208 Rd\u00d7d. Given a function f : Rd \u2192 R, its gradient and its Hessian at point x \u2208 Rd are respectively denoted as \u2207f(x) and \u22072f(x). A random vector x \u2208 Rd is an \u03b5-stationary point if E [ \u2225\u2207f(x)\u22252 ] \u2264 \u03b52, where the\nexpectation is over the randomness of the algorithm."
        },
        {
            "heading": "2 THE ALGORITHMS",
            "text": "Below we define our two main algorithms:\nxk+1 = xk \u2212DSk\u2207f(xk), (det-CGD1)\nand xk+1 = xk \u2212 T kD\u2207f(xk). (det-CGD2)\nHere, D \u2208 Sd++ is the fixed stepsize matrix. The sequences of random matrices Sk and T k satisfy the following assumption. Assumption 3. We will assume that the random sketches that appear in our algorithms are i.i.d., unbiased, symmetric and positive semi-definite for each algorithm. That is\nSk,T k \u2208 Sd+, Sk iid\u223c S and T k iid\u223c T E [ Sk ] = E [ T k ] = Id, for every k \u2208 N,\nwhere S and T are probability distributions over Sd+.\nA simple instance of det-CGD1 and det-CGD2 is the vanilla GD. Indeed, if Sk = T k = Id and D = \u03b3Id, then xk+1 = xk \u2212 \u03b3\u2207f(xk). In general, one may view these algorithms as Newton-type methods. In particular, our setting includes the Newton Star (NS) algorithm by Islamov et al. (2021):\nxk+1 = xk \u2212 ( \u22072f(xinf) )\u22121 \u2207f(xk). (NS) The authors prove that in the convex case it converges to the unique solution xinf locally quadratically, provided certain assumptions are met. However, it is not a practical method as it requires knowledge of the Hessian at the optimal point. This method, nevertheless, hints that constant matrix stepsize can yield fast convergence guarantees. Our results allow us to choose the D depending on the smoothness matrix L. The latter can be seen as a uniform upper bound on the Hessian.\nThe difference between det-CGD1 and det-CGD2 is the update rule. In particular, the order of the sketch and the stepsize is interchanged. When the sketch S and the stepsize D are commutative w.r.t. matrix product, the algorithms become equivalent. In general, a simple calculation shows that if we take T k = DSkD\u22121, (5) then det-CGD1 and det-CGD2 are the same. Defining T k according to (5), we recover the unbiasedness condition:\nE [ T k ] = DE [ Sk ] D\u22121 = Id. (6)\nHowever, in general DE [ Sk ] D\u22121 is not necessarily symmetric, which contradicts to Assumption 3. Thus, det-CGD1 and det-CGD2 are not equivalent for our purposes."
        },
        {
            "heading": "3 MAIN RESULTS",
            "text": "Before we state the main result, we present a stepsize condition for det-CGD1 and det-CGD2, respectively: E [ SkDLDSk ] \u2aaf D, (7) and E [ DT kLT kD ] \u2aaf D. (8)\nIn the case of vanilla GD (7) and (8) become \u03b3 < L\u22121, which is the standard condition for convergence. Below is the main convergence theorem for both algorithms in the single-node regime.\nTheorem 1. Suppose that Assumptions 1-3 are satisfied. Then, for each k \u2265 0\n1\nK K\u22121\u2211 k=0 E [\u2225\u2225\u2207f(xk)\u2225\u22252 D ] \u2264 2(f(x 0)\u2212 f inf) K , (9)\nif one of the below conditions is true:\ni) The vectors xk are the iterates of det-CGD1 and D satisfies (7);\nii) The vectors xk are the iterates of det-CGD2 and D satisfies (8).\nIt is important to note that Theorem 1 yields the same convergence rate for any D \u2208 Sd++, despite the fact that the matrix norms on the left-hand side cannot be compared for different weight matrices. To ensure comparability of the right-hand side of (9), it is necessary to normalize the weight matrix D that is used to measure the gradient norm. We propose using determinant normalization, which involves dividing both sides of (9) by det(D)1/d, yielding the following:\n1\nK K\u22121\u2211 k=0 E [\u2225\u2225\u2207f(xk)\u2225\u22252 D det(D)1/d ] \u2264 2(f(x 0)\u2212 f inf) det(D)1/dK . (10)\nThis normalization is meaningful because adjusting the weight matrix to D det(D)1/d allows its determinant to be 1, making the norm on the left-hand side comparable to the standard Euclidean norm. It is important to note that the volume of the normalized ellipsoid { x \u2208 Rd : \u2225x\u22252D/det(D)1/d \u2264 1 } does not depend on the choice of D \u2208 Sd++. Therefore, the results of (9) are comparable across different D in the sense that the right-hand side of (9) measures the volume of the ellipsoid containing the gradient."
        },
        {
            "heading": "3.1 OPTIMAL MATRIX STEPSIZE",
            "text": "In this section, we describe how to choose the optimal stepsize that minimizes the iteration complexity. The problem is easier for det-CGD2. We notice that (8) can be explicitly solved. Specifically, it is equivalent to\nD \u2aaf ( E [ T kLT k ])\u22121 . (11)\nWe want to emphasize that the RHS matrix is invertible despite the sketches not being so. Indeed. The map h : T \u2192 TLT is convex on Sd+. Therefore, Jensen\u2019s inequality implies\nE [ T kLT k ] \u2ab0 E [ T k ] LE [ T k ] = L \u227b Od.\nThis explicit condition on D can assist in determining the optimal stepsize. Since both D and (E [ T kLT k ] )\u22121 are positive definite, then the right-hand side of (10) is minimized exactly when\nD = ( E [ T kLT k ])\u22121 . (12)\nNote that the explicit solution of D needs to be calculated only once, at the beginning of the algorithm. It is then fixed for all iterations. The situation is different for det-CGD1. According to (10), the optimal D is defined as the solution of the following constrained optimization problem:\nminimize log det(D\u22121) subject to E [ SkDLDSk ] \u2aaf D (13)\nD \u2208 Sd++.\nProposition 1. The optimization problem (13) with respect to stepsize matrix D \u2208 Sd++, is a convex optimization problem with a convex constraint.\nThe proof of this proposition can be found in the Appendix. It is based on the reformulation of the constraint to its equivalent quadratic form inequality. Using the trace trick, we can prove that for every vector chosen in the quadratic form, it is convex. Since the intersection of convex sets is convex, we conclude the proof.\nOne could consider using the CVXPY (Diamond & Boyd, 2016) package to solve (13), provided that it is first transformed into a Disciplined Convex Programming (DCP) form (Grant et al., 2006). Nevertheless, (7) is not recognized as a DCP constraint in the general case. To make CVXPY applicable, additional steps tailored to the problem at hand must be taken.\nd\u22121Li."
        },
        {
            "heading": "4 LEVERAGING THE LAYER-WISE STRUCTURE",
            "text": "In this section we focus on the block-diagonal case of L for both det-CGD1 and det-CGD2. In particular, we propose hyper-parameters of det-CGD1 designed specifically for training NNs. Let us assume that L = Diag(L1, . . . ,L\u2113), where Li \u2208 Sdi++. This setting is a generalization of the classical smoothness condition, as in the latter case Li = LIdi for all i = 1, . . . , \u2113. Respectively, we choose both the sketches and the stepsize to be block diagonal: D = Diag(D1, . . . ,D\u2113) and Sk = Diag(Sk1 , . . . ,S k \u2113 ), where Di,S k i \u2208 S di ++.\nLet us notice that the left hand side of the inequality constraint in (13) has quadratic dependence on D, while the right hand side is linear. Thus, for every matrix W \u2208 Sd++, there exists \u03b3 > 0 such that\n\u03b32\u03bbmax ( E [ SkWLWSk ]) \u2264 \u03b3\u03bbmin(W ).\nTherefore, for \u03b3W we deduce E [ Sk(\u03b3W )L(\u03b3W )Sk ] \u2aaf \u03b32\u03bbmax ( E [ SkWLWSk ]) Id \u2aaf \u03b3\u03bbmin(W )Id \u2aaf \u03b3W . (14)\nThe following theorem is based on this simple fact applied to the corresponding blocks of the matrices D,L,Sk for det-CGD1. Theorem 2. Let f : Rd \u2192 R satisfy Assumptions 1 and 2, with L admitting the layer-separable structure L = Diag(L1, . . . ,L\u2113), where L1, . . . ,L\u2113 \u2208 Sdi++. Choose random matrices Sk1 , . . . ,Sk\u2113 \u2208 Sd+ to satisfy Assumption 3 for all i \u2208 [\u2113], and let Sk := Diag(Sk1 , . . . ,Sk\u2113 ). Furthermore, choose matrices W1, . . . ,W\u2113 \u2208 Sd++ and scalars \u03b31, . . . , \u03b3\u2113 > 0 such that\n\u03b3i \u2264 \u03bb\u22121max ( E [ W \u22121/2 i S k i WiLiWiS k i W \u22121/2 i ]) \u2200i \u2208 [\u2113]. (15)\nLetting W := Diag(W1, . . . ,W\u2113), \u0393 := Diag(\u03b31Id1 , . . . , \u03b3\u2113Id\u2113) and D := \u0393W , we get\n1\nK K\u22121\u2211 k=0 E [\u2225\u2225\u2207f(xk)\u2225\u22252 \u0393W det(\u0393W )1/d ] \u2264 2(f(x 0)\u2212 f inf) det (\u0393W ) 1/d K . (16)\nIn particular, if the scalars {\u03b3i} are chosen to be equal to their maximum allowed values from (15), then the convergence factor of (16) is equal to\ndet (\u0393W ) \u2212 1d =\n[ \u2113\u220f\ni=1\n\u03bbdimax\n( E [ W\n\u2212 12 i S k i WiLiWiS k i W \u2212 12 i\n])] 1d det(W\u22121) 1 d .\nTable 1 contains the (expected) communication complexities of det-CGD1, det-CGD2 and GD for several choices of W ,D and Sk. Here are a few comments about the table. We deduce that taking a matrix stepsize without compression (row 1) we improve GD (row 13). A careful analysis reveals that the result in row 5 is always worse than row 7 in terms of both communication and iteration complexity. However, the results in row 6 and row 7 are not comparable in general, meaning that neither of them is universally better. More discussion on this table can be found in the Appendix.\nCompression for free. Now, let us focus on row 12, which corresponds to a sampling scheme where the i-th layer is independently selected with probability qi. Mathematically, it goes as follows:\nT ki = \u03b7i qi Idi , where \u03b7i \u223c Bernoulli(qi). (17)\nJensen\u2019s inequality implies that ( l\u2211\ni=1\nqidi\n) \u00b7\nl\u220f i=1 ( 1 qi ) di d \u2265 d. (18)\nThe equality is attained when qi = q for all i \u2208 [\u2113]. The expected bits transferred per iteration of this algorithm is then equal to kexp = qd and the communication complexity equals ddet(L)1/d. Comparing with the results for det-CGD2 with rand-kexp on row 11 and using the fact that det(L) \u2264 det (diag(L)), we deduce that the Bernoulli scheme is better than the uniform sampling scheme. Notice also, the communication complexity matches the one for the uncompressed det-CGD2 displayed on row 9. This, in particular means that using the Bern-q sketches we can compress the gradients for free. The latter means that we reduce the number of bits broadcasted at each iteration without losing in the total communication complexity. In particular, when all the layers have the same width di, the number of broadcasted bits for each iteration is reduced by a factor of q."
        },
        {
            "heading": "5 DISTRIBUTED SETTING",
            "text": "In this section we describe the distributed versions of our algorithms and present convergence guarantees for them. Let us consider an objective function that is sum decomposable:\nf(x) := 1\nn n\u2211 i=1 fi(x),\nwhere each fi : Rd \u2192 R is a differentiable function. We assume that f satisfies Assumption 1 and the component functions satisfy the below condition.\nAssumption 4. Each component function fi is Li-smooth and is bounded from below: fi(x) \u2265 f infi for all x \u2208 Rd.\nThis assumption also implies that f is of matrix smoothness with L\u0304 \u2208 Sd++, where L\u0304 = 1n \u2211n\ni=1 Li. Following the standard FL framework (Konec\u030cny\u0300 et al., 2016; McMahan et al., 2017; Khirirat et al., 2018), we assume that the i-th component function fi is stored on the i-th client. At each iteration, the clients in parallel compute and compress the local gradient \u2207fi and communicate it to the central server. The server, then aggregates the compressed gradients, computes the next iterate, and in parallel broadcasts it to the clients. See the below pseudo-codes for the details.\nTheorem 3. Let fi : Rd \u2192 R satisfy Assumption 4 and let f satisfy Assumption 1 and Assumption 2 with smoothness matrix L. If the stepsize satisfies\nDLD \u2aaf D, (19)\nthen the following convergence bound is true for the iterates of Algorithm 1:\nmin 0\u2264k\u2264K\u22121\nE [\u2225\u2225\u2207f(xk)\u2225\u22252 D\ndet(D)1/d\n] \u2264 2(1 + \u03bbDn ) K ( f(x0)\u2212 f inf ) det(D)1/d K + 2\u03bbD\u2206 inf det(D)1/d n , (20)\nwhere \u2206inf := f inf \u2212 1n \u2211n i=1 f inf i and\n\u03bbD := max i\n{ \u03bbmax ( E [ L 1 2 i ( Ski \u2212 Id ) DLD ( Ski \u2212 Id ) L 1 2 i ])} .\nAlgorithm 1 Distributed det-CGD1 1: Input: Starting point x0, stepsize matrix D,\nnumber of iterations K 2: for k = 0, 1, 2, . . . ,K \u2212 1 do 3: The devices in parallel: 4: sample Ski \u223c S; 5: compute Ski \u2207fi(xk); 6: broadcast Ski \u2207fi(xk). 7: The server: 8: combines gk = Dn \u2211n i=1 S k i \u2207fi(xk);\n9: computes xk+1 = xk \u2212 gk; 10: broadcasts xk+1. 11: end for 12: Return: xK\nAlgorithm 2 Distributed det-CGD2 1: Input: Starting point x0, stepsize matrix D,\nnumber of iterations K 2: for k = 0, 1, 2, . . . ,K \u2212 1 do 3: The devices in parallel: 4: sample T ki \u223c T ; 5: compute T ki D\u2207fi(xk); 6: broadcast T ki D\u2207fi(xk). 7: The server: 8: combines gk = 1n \u2211n i T k i D\u2207fi(xk);\n9: computes xk+1 = xk \u2212 gk; 10: broadcasts xk+1. 11: end for 12: Return: xK\nThe same result is true for Algorithm 2 with a different constant \u03bbD. The proof of Theorem 3 and its analogue for Algorithm 2 are presented in the Appendix. The analysis is largely inspired by (Khaled & Richt\u00e1rik, 2020, Theorem 1). Now, let us examine the right-hand side of (20). We start by observing that the first term has exponential dependence in K. However, the term inside the brackets, 1 + \u03bbD/n, depends on the stepsize D. Furthermore, it has a second-order dependence on D, implying that \u03bb\u03b1D = \u03b12\u03bbD , as opposed to det(\u03b1D)1/d, which is linear in \u03b1. Therefore, we can choose a small enough coefficient \u03b1 to ensure that \u03bbD is of order n/K. This means that for a fixed number of iterations K, we choose the matrix stepsize to be \"small enough\" to guarantee that the numerator of the first term is bounded. The following corollary summarizes these arguments, and its proof can be found in the Appendix. Corollary 1. We reach an \u03b5-stationarity, that is the right-hand side of (20) is upper bounded by \u03b52, if the following conditions are satisfied:\nDLD \u2aaf D, \u03bbD \u2264 min { n\nK , n\u03b52 4\u2206inf det(D)1/d\n} , K \u2265 12(f(x\n0)\u2212 f inf) det(D)1/d \u03b52 . (21)\nProposition 3 in the Appendix proves that these conditions with respect to D are convex. In order to minimize the iteration complexity for getting \u03b52 error, one needs to solve the following optimization problem\nminimize log det(D\u22121) subject to D satisfies (21).\nChoosing the optimal stepsize for Algorithm 1 is analogous to solving (13). One can formulate the distributed counterpart of Theorem 2 and attempt to solve it for different sketches. Furthermore, this leads to a convex matrix minimization problem involving D. We provide a formal proof of this property in the Appendix. Similar to the single-node case, computational methods can be employed using the CVXPY package. However, some additional effort is required to transform (21) into the disciplined convex programming (DCP) format.\nThe second term in (20) corresponds to the convergence neighborhood of the algorithm. It does not depend on the number of iteration, thus it remains unchanged, after we choose the stepsize. Nevertheless, it depends on the number of clients n. In general, the term \u2206inf/n can be unbounded, when n \u2192 +\u221e. However, per Corollary 1, we require \u03bbD to be upper-bounded by n/K. Thus,\nk=0 \u2225\u2225\u2207f(xk)\u2225\u22252 D/det(D)1/d ) .\nthe neighborhood term will indeed converge to zero when K \u2192 +\u221e, if we choose the stepsize accordingly.\nWe compare our results with the existing results for DCGD. In particular we use the technique from Khaled & Richt\u00e1rik (2020) for the scalar smooth DCGD with scalar stepsizes with the results from (Khaled & Richt\u00e1rik, 2020, Corollary 1). See the Appendix for the details on the analysis of Khaled & Richt\u00e1rik (2020). Finally, we back up our theoretical findings with experiments. See Figure 1 for a simple experiment confirming that Algorithms 1 and 2 have better iteration and communication complexity compared to scalar stepsized DCGD. The graphs of the two proposed algorithms coincide, as the diagonal stepsize and the diagonal sketch commute, resulting in the same method. For more details on the experiments we refer the reader to the corresponding section in the Appendix."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we enhance compressed gradient descent method with matrix-valued stepsize for general non-convex objectives. Convergence guarantees are provided for the algorithms both in the single node case and the distributed setting. By considering the layer-wise structure of models such as neural networks, we are able to design compression mechanisms that achieve compression for free. This is the first time matrix stepsize is used and analyzed together with compression in the non-convex case. Our theoretical findings are supported with abundant numerical experiments."
        },
        {
            "heading": "6.1 LIMITATIONS",
            "text": "It is worth noting that every point in Rd can be enclosed within some volume 1 ellipsoid. Therefore, having the average D-norm of the gradient bounded by a small number does not guarantee that the average Euclidean norm is small. However, for a fixed D, the standard Euclidean norm is equivalent to the weighted D-norm. This is due to\n\u03bbmin (D)\ndet(D)1/d \u00b7 \u2225\u2207f(x)\u22252 \u2264 \u2225\u2207f(x)\u22252D/(det(D))1/d \u2264\n\u03bbmax (D) det(D)1/d \u00b7 \u2225\u2207f(x)\u22252 . (22)\nThis relation is further validated by our experiments described in the Appendix."
        }
    ],
    "title": "DET-CGD: COMPRESSED GRADIENT DESCENT WITH MATRIX STEPSIZES FOR NON-CONVEX OPTIMIZATION",
    "year": 2024
}