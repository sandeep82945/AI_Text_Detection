{
    "abstractText": "Generative modeling via diffusion-based models has been achieving state-of-the-art results on various generation tasks. Most existing diffusion models, however, are limited to a single-generation modeling. Can we generalize diffusion models with the ability of multi-task generative training for more generalizable modeling? In this paper, we propose a principled way to define a diffusion model for this purpose by constructing a unified multi-task diffusion model in a common diffusion space. We define the forward diffusion process to be driven by an information aggregation from multiple types of task-data, e.g., images for a generation task and labels for a classification task. In the reverse process, we enforce information sharing by parameterizing a shared backbone denoising network with additional task-specific decoder heads. Such a structure can simultaneously learn to generate different types of multi-task data with a multi-task loss, which is derived from a multi-task variational lower bound that generalizes the standard diffusion model. We propose several multi-task generation settings to verify our framework, including image transition, masked-image training, joint image-label and joint image-representation generative modeling. Extensive experimental results on ImageNet indicate the effectiveness of our framework for various multi-task generative modeling, which we believe is an important research direction worthy of more future explorations.",
    "authors": [],
    "id": "SP:ba23046c7a6190b17f28538b68cc522775e72774",
    "references": [
        {
            "authors": [
                "Jacob Austin",
                "Daniel D. Johnson",
                "Jonathan Ho",
                "Daniel Tarlow",
                "Rianne van den Berg"
            ],
            "title": "Structured denoising diffusion models in discrete state-spaces",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Omri Avrahami",
                "Ohad Fried",
                "Dani Lischinski"
            ],
            "title": "Blended latent diffusion",
            "venue": "arXiv preprint arXiv:2206.02779,",
            "year": 2022
        },
        {
            "authors": [
                "Omri Avrahami",
                "Dani Lischinski",
                "Ohad Fried"
            ],
            "title": "Blended diffusion for text-driven editing of natural images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhipeng Bao",
                "Martial Hebert",
                "Yu-Xiong Wang"
            ],
            "title": "Generative modeling for multi-task visual learning",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Omer Bar-Tal",
                "Dolev Ofri-Amar",
                "Rafail Fridman",
                "Yoni Kasten",
                "Tali Dekel"
            ],
            "title": "Text2live: Text-driven layered image and video editing",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Blattmann",
                "Robin Rombach",
                "Kaan Oktay",
                "Jonas M\u00fcller",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Semi-parametric neural image synthesis",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zitian Chen",
                "Yikang Shen",
                "Mingyu Ding",
                "Zhenfang Chen",
                "Hengshuang Zhao",
                "Erik G. Learned-Miller",
                "Chuang Gan"
            ],
            "title": "Mod-squad: Designing mixture of experts as modular multi-task learners",
            "venue": "ArXiv, abs/2212.08066,",
            "year": 2022
        },
        {
            "authors": [
                "Niv Cohen",
                "Rinon Gal",
                "Eli A Meirom",
                "Gal Chechik",
                "Yuval Atzmon"
            ],
            "title": " this is my unicorn, fluffy\": Personalizing frozen vision-language representations",
            "venue": "arXiv preprint arXiv:2204.01694,",
            "year": 2022
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Heshan Fernando",
                "Han Shen",
                "Miao Liu",
                "Subhajit Chaudhury",
                "Keerthiram Murugesan",
                "Tianyi Chen"
            ],
            "title": "Mitigating gradient bias in multi-objective learning: A provably convergent approach",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "arXiv preprint arXiv:2208.01618,",
            "year": 2022
        },
        {
            "authors": [
                "Shansan Gong",
                "Mukai Li",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "Lingpeng Kong"
            ],
            "title": "Diffuseq: Sequence to sequence text generation with diffusion models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Xizewen Han",
                "Huangjie Zheng",
                "Mingyuan Zhou"
            ],
            "title": "CARD: Classification and regression diffusion models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "William Harvey",
                "Saeid Naderiparizi",
                "Vaden Masrani",
                "Christian Dietrich Weilbach",
                "Frank Wood"
            ],
            "title": "Flexible diffusion modeling of long videos",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengfu He",
                "Tianxiang Sun",
                "Kuanning Wang",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "title": "Diffusionbert: Improving generative masked language models with diffusion models",
            "venue": "arXiv preprint arXiv:2211.15029,",
            "year": 2022
        },
        {
            "authors": [
                "Falk Heuer",
                "Sven Mantowsky",
                "Syed Saqib Bukhari",
                "Georg Schneider"
            ],
            "title": "Multitask-centernet (mcn): Efficient and diverse multitask learning using an anchor free approach",
            "venue": "IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey A. Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J. Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "In ICLR Workshop on Deep Generative Models for Highly Structured Data,",
            "year": 2022
        },
        {
            "authors": [
                "Yi Hu",
                "Jiazhi Yang",
                "Li Chen",
                "Keyu Li",
                "Chonghao Sima",
                "Xizhou Zhu",
                "Siqi Chai",
                "Senyao Du",
                "Tianwei Lin",
                "Wen Wang",
                "Lewei Lu",
                "Xiaosong Jia",
                "Qiang Liu",
                "Jifeng Dai",
                "Yu Qiao",
                "Hongyang Li"
            ],
            "title": "Planning-oriented autonomous driving",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Marco Tulio Ribeiro",
                "Mitchell Wortsman",
                "Suchin Gururangan",
                "Ludwig Schmidt",
                "Hannaneh Hajishirzi",
                "Ali Farhadi"
            ],
            "title": "Editing models with task",
            "venue": "arithmetic. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei"
            ],
            "title": "A Efros. Image-to-image translation with conditional adversarial networks",
            "year": 2017
        },
        {
            "authors": [
                "Junguang Jiang",
                "Baixu Chen",
                "Junwei Pan",
                "Ximei Wang",
                "Liu Dapeng",
                "Jie Jiang",
                "Mingsheng Long"
            ],
            "title": "Forkmerge: Overcoming negative transfer in multi-task learning",
            "venue": "ArXiv, abs/2301.12618,",
            "year": 2023
        },
        {
            "authors": [
                "Bahjat Kawar",
                "Shiran Zada",
                "Oran Lang",
                "Omer Tov",
                "Huiwen Chang",
                "Tali Dekel",
                "Inbar Mosseri",
                "Michal Irani"
            ],
            "title": "Imagic: Text-based real image editing with diffusion models",
            "venue": "arXiv preprint arXiv:2210.09276,",
            "year": 2022
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Mingi Kwon",
                "Jaeseok Jeong",
                "Youngjung Uh"
            ],
            "title": "Diffusion models already have a semantic latent space",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Pierre-Yves Laffont",
                "Zhile Ren",
                "Xiaofeng Tao",
                "Chao Qian",
                "James Hays"
            ],
            "title": "Transient attributes for high-level understanding and editing of outdoor scenes",
            "venue": "ACM Transactions on Graphics (proceedings of SIGGRAPH),",
            "year": 2014
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath R. Selvaraju",
                "Akhilesh Deepak Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "title": "DiffusionLM improves controllable text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Haohe Liu",
                "Zehua Chen",
                "Yiitan Yuan",
                "Xinhao Mei",
                "Xubo Liu",
                "Danilo P. Mandic",
                "Wenwu Wang",
                "MarkD . Plumbley"
            ],
            "title": "Audioldm: Text-to-audio generation with latent diffusion",
            "venue": "models. ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Yang Liu",
                "Zhaowen Wang",
                "Hailin Jin",
                "Ian James Wassell"
            ],
            "title": "Multi-task adversarial network for disentangled feature learning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alexander Long",
                "Wei Yin",
                "Thalaiyasingam Ajanthan",
                "Vu Nguyen",
                "Pulak Purkait",
                "Ravi Garg",
                "Alan Blair",
                "Chunhua Shen",
                "Anton van den Hengel"
            ],
            "title": "Retrieval augmented classification for long-tail visual recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "SDEdit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin P. Murphy"
            ],
            "title": "Probabilistic Machine Learning: An introduction",
            "venue": "URL probml.ai",
            "year": 2022
        },
        {
            "authors": [
                "William Peebles",
                "Saining Xie"
            ],
            "title": "Scalable diffusion models with transformers",
            "venue": "arXiv preprint arXiv:2212.09748,",
            "year": 2022
        },
        {
            "authors": [
                "Hoang Phan",
                "Ngoc Nguyen Tran",
                "Trung Le",
                "Toan Tran",
                "Nhat Ho",
                "Dinh Q. Phung"
            ],
            "title": "Stochastic multiple target sampling gradient descent",
            "venue": "ArXiv, abs/2206.01934,",
            "year": 2022
        },
        {
            "authors": [
                "Konpat Preechakul",
                "Nattanat Chatthee",
                "Suttisak Wizadwongsa",
                "Supasorn Suwajanakorn"
            ],
            "title": "Diffusion autoencoders: Toward a meaningful and decodable representation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "arXiv preprint arXiv:2102.12092,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip",
            "venue": "latents. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Joachim Bingel",
                "Isabelle Augenstein",
                "Anders S\u00f8gaard"
            ],
            "title": "Latent multi-task architecture learning",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven",
            "venue": "generation. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "arXiv preprint arXiv:2208.12242,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L. Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "Seyedeh Sara Mahdavi",
                "Raphael Gontijo Lopes",
                "Tim Salimans",
                "Jonathan Ho",
                "David Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "ArXiv, abs/2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman",
                "Patrick Schramowski",
                "Srivatsa Kundurthy",
                "Katherine Crowson",
                "Ludwig Schmidt",
                "Robert Kaczmarczyk",
                "Jenia Jitsev"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation",
            "venue": "image-text models. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Dmitry Senushkin",
                "Nikolay Patakin",
                "Arseny Kuznetsov",
                "Anton Konushin"
            ],
            "title": "Independent component alignment for multi-task learning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Mohit Sharma",
                "Claudio Fantacci",
                "Yuxiang Zhou",
                "Skanda Koppula",
                "Nicolas Manfred Otto Heess",
                "Jonathan Scholz",
                "Yusuf Aytar"
            ],
            "title": "Lossless adaptation of pretrained vision models for robotic manipulation",
            "venue": "ArXiv, abs/2304.06600,",
            "year": 2023
        },
        {
            "authors": [
                "Uriel Singer",
                "Adam Polyak",
                "Thomas Hayes",
                "Xi Yin",
                "Jie An",
                "Songyang Zhang",
                "Qiyuan Hu",
                "Harry Yang",
                "Oron Ashual",
                "Oran Gafni",
                "Devi Parikh",
                "Sonal Gupta",
                "Yaniv Taigman"
            ],
            "title": "Make-a-video: Text-tovideo generation without text-video data",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Malik Tiomoko",
                "Hafiz Tiomoko Ali",
                "Romain Couillet"
            ],
            "title": "Deciphering and optimizing multi-task learning: a random matrix approach",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Nilesh Tripuraneni",
                "Michael I. Jordan",
                "Chi Jin"
            ],
            "title": "On the theory of transfer learning: The importance of task diversity",
            "venue": "ArXiv, abs/2006.11650,",
            "year": 2020
        },
        {
            "authors": [
                "Haoxiang Wang",
                "Han Zhao",
                "Bo Li"
            ],
            "title": "Bridging multi-task learning and meta-learning: Towards efficient training and effective",
            "venue": "adaptation. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Jifeng Dai",
                "Zhe Chen",
                "Zhenhang Huang",
                "Zhiqi Li",
                "Xizhou Zhu",
                "Xiaowei Hu",
                "Tong Lu",
                "Lewei Lu",
                "Hongsheng Li"
            ],
            "title": "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
            "venue": "arXiv preprint arXiv:2211.05778,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Wei",
                "Karttikeya Mangalam",
                "Po-Yao Huang",
                "Yanghao Li",
                "Haoqi Fan",
                "Hu Xu",
                "Huiyu Wang",
                "Cihang Xie",
                "Alan Loddon Yuille",
                "Christoph Feichtenhofer"
            ],
            "title": "Diffusion models as masked",
            "venue": "autoencoders. ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Sen Wu",
                "Hongyang Zhang",
                "Christopher R\u00e9"
            ],
            "title": "Understanding and improving information transfer in multi-task learning",
            "venue": "ArXiv, abs/2005.00944,",
            "year": 2020
        },
        {
            "authors": [
                "Hanrong Ye",
                "Dan Xu"
            ],
            "title": "T ask p rompter : S patial -c hannel m ulti -t ask p rompting for d ense s cene u nderstanding",
            "year": 2023
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan",
                "Benton C. Hutchinson",
                "Wei Han",
                "Zarana Parekh",
                "Xin Li",
                "Han Zhang",
                "Jason Baldridge",
                "Yonghui Wu"
            ],
            "title": "Scaling autoregressive models for content-rich",
            "venue": "text-to-image generation. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "year": 2018
        },
        {
            "authors": [
                "Zijian Zhang",
                "Zhou Zhao",
                "Zhijie Lin"
            ],
            "title": "Unsupervised representation learning from pre-trained diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2212.12990,",
            "year": 2022
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "In Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "2021 Heuer et al",
                "2017 Ruder et al",
                "Ye",
                "2023 Xu",
                "2023 Sharma et al",
                "Chen"
            ],
            "title": "2022), optimization algorithms (Senushkin et al., 2023",
            "venue": "Phan et al., 2022) and task relationship learning (Hu et al.,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The field of artificial intelligence (AI) has witnessed significant advancements in generative modeling, leading to remarkable progresses such as DALL-E (Ramesh et al., 2022) and GPT-4 (OpenAI, 2023). The generative AI paradigm enables the learning of transitions from simple to complex distributions, such as from a standard Gaussian distribution to a high-dimensional image distribution. Compared to discriminative learning, generative mechanisms can arguably prioritize the overall structures of the data, offering better data fitting and potential robustness to data noise. However, while real-world applications often involve multiple data types, including images, video, text, and labels, most existing generative models primarily focus on generating a single data type or modality. Notably, the diffusion model (Sohl-Dickstein et al., 2015; Ho et al., 2020), a state-of-the-art generative model, has been independently developed for generating image, text, audio, and label data (Dhariwal & Nichol, 2021; Li et al., 2022b; Liu et al., 2023; Han et al., 2022). Can we design a principled way to enable joint modeling and generating multi-type data within the diffusion-model framework?\nFurthermore, leveraging multi-modal information through learning from multiple tasks and data sources has proven to be highly effective to learn generalized representations. Prominent examples include the ALBEF and BLIP models, which jointly learns from multi-data sources to match image and text data (Li et al., 2021; 2022a; 2023), and the BERT model, which benefits from multi-task training such as masked token prediction and consecutive sentence prediction (Devlin et al., 2019b). Can we adopt a similar setting to leverage multiple data sources and losses into the diffusion-model framework, so as to better integrate shared information among tasks for better generative modeling?\nIn this paper, we present our initial endeavor towards this goal by introducing the multi-task diffusion model, referred to as MT-Diffusion. MT-Diffusion enables the simultaneous modeling and generation of multi-source data within a unified diffusion model. By multi-task, we emphasize that MT-Diffusion is designed to 1) simultaneously generate multi-type data (potentially heterogeneous such as images and labels) within a unified model; and 2) seamlessly integrate multi-task learning losses into the\ndiffusion framework in a principled manner, supported by theoretical foundations. Our multi-task setting is highly versatile and applicable to numerous practical scenarios. It is worth noting that, as an initial investigation to multi-task diffusion models, we focus on the setting of two tasks to demonstrate the promise of the research direction, while leaving training with more tasks as interesting future work. Particularly, we construct several practical multi-task generative learning scenarios in our experiments (each with two tasks) to demonstrate the effectiveness of our framework:\n\u2022 Image transition: We consider jointly modeling multi-modal data, such as images and the corresponding semantic segmentation masks, by learning to generate both in the reverse process within our MT-Diffusion framework. We design this task as a synthetic experiment to qualitatively demonstrate the ability of our model on small-scaled datasets.\n\u2022 Masked-image training*: Motivated from the previous success on masked-language pretraining such as BERT (Devlin et al., 2019a) in language modeling, we propose to combine a pure generation task with a masked-image generation task for generative training. We demonstrate on the ImageNet dataset that our model can be more efficient in training a generative model, and can converge to a point comparable to (if not better than) the heavily tuned single-task diffusion model in terms of generative image quality. Furthermore, it can simultaneously obtain for free a great image-restoration ability for masked image recovery.\n\u2022 Joint image-label generation: We jointly model images and the corresponding labels by learning to generate both with our MT-Diffusion. We demonstrate on the ImageNet dataset that one can achieve better classification accuracy compared to pure supervised training.\n\u2022 Joint image-representation generation: We also investigate simultaneously learning to generate images and representations (e.g., CLIP representations (Radford et al., 2021)) with MT-Diffusion. As this is a larger-scale setting based on stable diffusion, we only provide qualitative results to demonstrate the ability of our model to generate high-quality images from text, while leaving more detailed investigations as interesting future work.\nOur solution for these multi-task generation problems is based on a novel generalization of the standard diffusion model. It is designed to handle data from multiple tasks through both innovative algorithm and architecture designs in the diffusion forward and reverse processes. Our general idea is illustrated in Figure 1. In the forward process, multi-type/multi-task data are first aggregated through some well-designed mechanisms (details in Section 2.2.2) so that the aggregated information can be conveniently applied to the forward noising operation of a diffusion model. To deal with potentially heterogeneous data, an effective encoder architecture design is proposed to encode multi-task data into a shared diffusion space. In the reverse process, we propose to extend the original U-Net architecture\u2020 in diffusion models to simultaneously reconstruct the multi-task data from different tasks. To this end, task-specific decoder heads are designed to be attached to the U-Net architecture to decode the diffusion latent code back to multi-task data spaces. The forward and reverse processes are then integrated within the diffusion mechanism, leading to a loss derived from a new multi-task evidence lower bound (ELBO), as a multi-task loss. Similar to the standard diffusion model, the new ELBO is simple and easy to optimize. Extensive experiments on the aforementioned four multi-task scenarios are conducted to verify the effectiveness of the MT-Diffusion framework, demonstrating that our model can achieve simultaneous generation without hurting individual task performance, a promising generalization of the standard diffusion model for multi-task generative learning.\n*The recent DiffMAE (Wei et al., 2023) is fundamentally different from ours. It is a standard conditional diffusion model to denoise the pre-masked region; ours models both image and mask generation.\n\u2020The default U-Net architecture is adopted, though the Transformer (Peebles & Xie, 2022) can also be used."
        },
        {
            "heading": "2 MULTI-TASK DIFFUSION MODELS",
            "text": ""
        },
        {
            "heading": "2.1 PRELIMINARIES ON DENIOSING DIFFUSION PROBABILITY MODELS (DDPM)",
            "text": "DDPM is a probability generative model that consists of a forward noising process and a reverse denoising process operated on a diffusion space. The forward process gradually adds Gaussian noise into the data, which ultimately become standard Gaussian samples; and the reverse process parameterizes a neural network model to reverse the forward process. Specifically, given a data sample x from the data distribution, the forward process from time t \u2212 1 to time t is defined as q(zt | zt\u22121) = N (zt; \u221a 1\u2212 \u03b2t zt\u22121, \u03b2t I), where zt represents a noisy version of the original data sample z0 at time t; {\u03b2t} is an increasing sequence converging to 1 (making xt converge to a standard Gaussian sample). A reverse process is modeled by a neural network (we consider a U-Net for image data) parameterized by \u03b8 as p\u03b8(zt\u22121 | zt) = N (zt\u22121;\u00b5\u03b8(zt, t),\u03a3\u03b8(zt, t)). Considering all time steps t = 1, \u00b7 \u00b7 \u00b7 , T , the forward and reverse processes define two joint distributions over the same set of random variables {z0, \u00b7 \u00b7 \u00b7 , zT }. By variational principle, a loss corresponding to the evidence lower bound (ELBO) can be derived to optimize the parameterized generative model \u03b8, as L = Eq(z0,\u00b7\u00b7\u00b7 ,zT ) [ \u2212 log p(zT )\u2212 \u2211 t\u22651 log p\u03b8(zt\u22121 | zt) q(zt | zt\u22121) ] ."
        },
        {
            "heading": "2.2 MULTI-TASK DIFFUSION MODELS",
            "text": "We propose the MT-Diffusion model, which aims to jointly model multi-task data by generalizing the DDPM framework. We assume each task is associated with one source/type of data (although the data can be the same for different tasks). For example, an unconditional image-generation task is associated with image data, and an image classification task with image-label paired data. More formally, suppose there are N tasks, where task i is associated with task data from space Xi. Let xi denote one data sample from the i-th task-data space, and let X \u225c {x1, \u00b7 \u00b7 \u00b7 ,xN} be the union data from the N tasks. We note that our setting is quite general in the sense that the data spaces {Xi} can be heterogeneous, e.g., the image space versus the image-label space as from our previous example.\nTo deal with potential heterogeneity of task-data spaces, we propose to define MT-Diffusion in a shared latent space, called diffusion space and denoted as Z. To this end, we propose to apply a mapping to project each of the original task-data space onto the shared diffusion space. We define the mapping with an encoder Ei for task i, i.e., Ei : Xi \u2192 Z, as illustrated in Figure 1. For simplicity, we consider non-parametric or fixed-parameter encoders. The specified encoder designs are detailed in Section 2.2.4. In the following, we first formally define the proposed MT-Diffusion by specifying the forward and reverse processes, as well as deriving the corresponding variaitonal lower bound, by extending the DDPM framework to handle multiple data sources and multi-task losses.\n2.2.1 FORWARD-REVERSE PROCESSES AND THE VARIATIONAL LOWER BOUND\nIn our design, the forward and reverse processes will be responsible for integrating multi-task data information and multi-task losses within the DDPM framework, respectively. This is implemented by first defining joint distributions over the task data and the diffusion latent variables in both forward and reverse processes. Specifically, in\nthe forward process, the noising transition from time t \u2212 1 to time t is defined to be conditioned on the task data. To this end, we propose to define a joint distribution at time t over the task data X = {x1, \u00b7 \u00b7 \u00b7 ,xN} and the diffusion latent variable zt, conditioned on information from time t\u2212 1, to endow the following decomposed form\u2021:\nq(zt,X | zt\u22121) = q(zt | zt\u22121,x1, \u00b7 \u00b7 \u00b7 ,xN ) N\u220f i=1 qi(xi) , (1)\nwhere q(zt | zt\u22121,x1, \u00b7 \u00b7 \u00b7 ,xN ) represents the transition distribution of zt from time t\u2212 1 to time t, and {qi(xi)} denotes prior distributions of the task data that we assume to be mutually independent for simplicity. We denote this process as forward aggregation, and the specific probability distributions will be defined in the next section. Furthermore, the reverse process is defined by simply reversing the\n\u2021We assume task data are time-independent, although it is also feasible to introduce time dependency.\nforward distributions, resulting in a joint distribution p\u03b8(zt\u22121,X | zt) at time t, where \u03b8 represents the reverse model parameter. Specifically, starting by sampling zT from p(zT ), we propose to decompose the reverse transition at time t into the following conditional distributions: p\u03b8(zt\u22121,X | zt) = p\u03b8(zt\u22121 | zt) \u220fN i=1 p\u03b8(xi | zt). The random variable dependency and the general forward-reverse processes are illustrated in Figure 2. Before specifying these distributions, we first derive an objective by matching the joint distributions of the forward and reverse processes. This results in a multi-task ELBO for the proposed MT-Diffusion, based on which a final loss can be defined in Section 2.2.5. Theorem 1. The negative ELBO of MT-Diffusion endows: L = Eq [L0 + L1 + L2 + L3], where\nL0 \u225c KL (q(zT | z0,X)\u2225p(zT )) , L1 \u225c \u2211 t>1 KL (q(zt\u22121 | z0, zt,X)\u2225p\u03b8(zt\u22121 | zt)) , (2)\nL2 \u225c \u2211 t\u22651 N\u2211 i=1 KL (qi(xi)\u2225p\u03b8(xi | zt)) , L3 \u225c log p\u03b8(z0 | z1) .\nRemark 1. We can see that the prior multi-task data distributions are within the loss term L2. If only a single generation task is considered, the sub-loss L2 will disappeared, reducing to the standard DDPM loss. Our multi-task diffusion objective defines the posterior of the transition probability q(zt\u22121 | zt,X) by conditioning on all task data (in L1), and additionally, as formulated in L2, parameterizes the reverse process to regularize the predicted task-data distribution p\u03b8(xi | zt\u22121) so that it matches the prior task data distribution qi(xi)."
        },
        {
            "heading": "2.2.2 FORWARD AGGREGATION",
            "text": "The forward aggregation mainly deals with the posterior transition probability q(zt\u22121 | z0, zt,X) in L1 of equation 2. To derive an explicit form, we start by specifying the forward transition probability q(zt | zt\u22121,X), which can consequently induce the marginal distribution q(zt | z0,X) as well as the posterior transition probability. To integrate different task information, we define the forward transition distribution as a Gaussian distribution by aggregating the task information into the mean parameter. Specifically, we define\nq(zt | zt\u22121,X) = N ( zt; \u221a \u03b1\u2032t zt\u22121 + \u2211N i=1 w (i) t Ei(xi)\nN + 1 , (1\u2212 \u03b1\n\u2032 t\nN + 1 ) I\n) , (3)\nwhere w(i)t denotes the weight for the i-th task representation at time t, and {\u03b1\u2032t} are weights to scale the mean and covariance of the Gaussian transition similar to DDPM. By a change of notation \u03b1t \u225c \u03b1\u2032t/(N + 1), the transition distribution can be re-written as q(zt | zt\u22121,X) = N ( zt; \u221a \u03b1t(zt\u22121 +wtE(x)), (1\u2212 \u03b1t) I) ) , which we will use in the following derivations and implementation. With these transition distributions, multi-task information can be seamlessly incorporated into the diffusion process, which can effectively translate to the reverse process with a parametric model to be defined in Section 2.2.3. Now we can derive the marginal and posterior transition distributions, which turn out to also endow simple forms of Gaussian distributions, stated in Theorem 2. Theorem 2. Given the transition distribution equation 3, the marginal transition distribution follows\nq(zt | z0,X) = N ( zt; \u221a \u03b1\u0304t z0 +\nN\u2211 i=1 \u03b1\u0303 (i) t Ei(xi), (1\u2212 \u03b1\u0304t) I\n) , (4)\nwhere \u03b1\u0304t \u225c \u220ft i=1 \u03b1i, and \u03b1\u0303 (i) t is recursively defined as \u03b1\u0303 (i) t = \u221a \u03b1t ( w (i) t + \u03b1\u0303 (i) t\u22121 ) with \u03b1\u0303(i)0 \u225c 0.\nFurthermore, the posterior transition follows q(zt\u22121 | z0, zt,X) = N ( zt\u22121; \u00b5\u0303t(zt,X), \u03b2\u0303t I ) , where\n\u00b5\u0303t(zt,X) =\n\u221a \u03b1t(1\u2212 \u03b1\u0304t\u22121) zt +(1\u2212 \u03b1t) \u221a \u03b1\u0304t\u22121 z0 + \u2211N i=1 ( (1\u2212\u03b1t)\u03b1\u0303(i)t\u221a \u03b1t \u2212 (1\u2212 \u03b1\u0304t)wt ) Ei(xi)\n1\u2212 \u03b1\u0304t\n= 1\u221a \u03b1t\n( zt \u2212\n1\u2212 \u03b1t\u221a 1\u2212 \u03b1\u0304t \u03f5\n) \u2212 N\u2211 i=1 w (i) t Ei(xi) , and \u03b2\u0303t = (1\u2212 \u03b1t)(1\u2212 \u03b1\u0304t\u22121) 1\u2212 \u03b1\u0304t . (5)\nRemark 2. The posterior transition distribution equation 5 shares a similar form as that in DDPM, with an extra term of \u2211N i=1 w (i) t Ei(xi) representing information aggregated from all tasks (thus aggregation). Note the aggregation is defined in the forward process, enabling a closed-form posterior but without losing too much modeling expressiveness compared to other complex aggregations."
        },
        {
            "heading": "2.2.3 REVERSE PARAMETRIZATION",
            "text": "Based on the ELBO in Theorem 1, the reverse model is responsible for defining two sets of distributions: p\u03b8(zt\u22121 | zt) and p\u03b8(xi | zt). The first distribution is similar to that in DDPM, and the second one is induced from decoding the diffusion latent code back to task-data spaces. To leverage these distributions within a unified architecture for task information sharing, we propose to parameterized the reverse model with a shared backbone network followed by N extra task heads, each corresponding to one task. The basic structure is illustrated in Figure 1. Specifically, for p\u03b8(zt\u22121 | zt), we follow DDPM to define it as Gaussian distributions with mean and covariance denoted as \u00b5\u03b8(zt,X) and \u03c32t I, respectively. Consequently, the KL-divergence in L1 of equation 2 reduces to matching the mean of the two Gaussians with a proper weighting scheme depending on t. Based on the form of the mean of q(zt\u22121 | z0, zt,X) in equation 5, instead of parameterizing the mean of p\u03b8(zt\u22121 | zt), we follow DDPM to parameterize the U-Net to predict the intrinsic noise in zt (denoted as \u03f5). Specifically, the parametrized U-Net model \u03f5\u03b8(zt, t) is formulated as: \u03f5\u03b8(zt, t) = \u03f5\u03b8( \u221a \u03b1\u0304t z0 +\u03b1\u0303t X+ \u221a 1\u2212 \u03b1\u0304t\u03f5, t) \u2248 \u03f5.\nFor the decoding distributions p\u03b8(xi | zt)\u2019s in the L2 term of equation 2, the distribution forms are task specific. We consider the following two cases in our experiments:\n\u2022 When task data are in the form of probability vectors, e.g., discrete labels in the classification task, we define p\u03b8(xi | zt) as a discrete distribution, parameterized by the output of the task head. In this case, the KL-term in L2 is equivalent to the cross-entropy loss.\n\u2022 When task data are in the form of continuous values, we define p\u03b8(xi | zt) as a Gaussian distribution with the mean parameterized by the output of the task head. In this case, the KL-divergence in L2 reduces to the MSE loss, similar to the case for p\u03b8(zt\u22121 | zt).\nIt is worth noting that different from p\u03b8(zt\u22121 | zt), the variables xi and zt in p\u03b8(xi | zt) can be in different feature spaces. Thus, a decoder Di(\u00b7) in the form of one task head specified above is applied to project the diffusion latent code zt back to the task-data space, based on which a proper p\u03b8(xi | zt) is defined, as illustrated in Figure 1. Specifically, the decoding process can be written as:\nAt time t : zt Diffusion\u2212\u2212\u2212\u2212\u2212\u2192 denoising ct \u225c U-Net(zt, t;\u03b8) Task i\u2212\u2212\u2212\u2212\u2192 decoding x\u0303i \u225c Di(ct;\u03b8) \u2248 xi ,\nwhere we use \u201cU-Net\u201d to denote the output from one particular component of the U-Net, serving as the input to the decoder head (see Section 2.2.4 for more details). In other words, the reverse parameterized model consists of two parts: \u03f5\u03b8(zt,x, t) and Di(zt, t;\u03b8). Detailed structure designs to integrate the decoders (together with the encoder E(\u00b7) in the forward process) into the shared U-Net backbone is discussed in the next section."
        },
        {
            "heading": "2.2.4 ENCODER-DECODER DESIGNS",
            "text": "The encoders aim to map different task-data onto the diffusion space, and the decoders project the diffusion latent code from the shared U-Net backbone back to the task-data spaces. As the encoders are associated with the forward process, we propose to avoid introducing extra trainable parameters in the encoders for simplicity. Furthermore, we propose to introduce trainable parameters to the decoders as they are parts of the parameterized reverse model. There are many possible design\nchoices for the encoders and decoders. Our guideline is to choose architectures to reuse existing components or some pretrained models as best as possible. Based on this principle, we recommend the following designs, with the detailed training pipeline and architectures illustrated in Figure 3.\nEncoder Design We consider three scenarios, indicated by \u2460\u2461\u2462 in Figure 3: 1) A task-data space is the same as the diffusion space, e.g., both image spaces. In this case, we can define the encoder as a simple mapping such as the identity mapping \u2460 in Figure 3; 2) A task-data space is inhomogeneous with the diffusion space, e.g., a label space vs. an image space. In this case, we propose to use either a pretrained generator (\u2461 in Figure 3) or the shared U-Net backbone (\u2462 in Figure 3) to transfer task-data information to the diffusion space. Particularly, for choice \u2462, we use the cross attention mechanism in the U-Net architecture to map task-data information to the diffusion space.\nDecoder Design The decoders are task specific. They accept outputs from one of the U-Net blocks (indicated by black-dash-line connections in Figure 3) and learn to generate the original task data. For example, in a classification task, the decoder is designed as a classifier that outputs a class label, associated with a cross-entropy loss."
        },
        {
            "heading": "2.2.5 TRAINING AND INFERENCE",
            "text": "Training We propose a simple training loss for our MT-Diffusion, based on the ELBO equation 2 and the specific forward and reverse parameterization described above. In the ELBO, L0 is independent of the model parameter \u03b8, thus it can be omitted in training. By substituting the specific distributions into the ELBO and adopting the simple loss idea in DDPM (Ho et al., 2020) that ignores the weights for different timesteps, the ELBO equation 2 reduces to the following training loss for the proposed MT-Diffusion:\nL \u225c L\u0303mse + \u03bb \u2211 t\u22651 N\u2211 i=1 KL (qi(xi)\u2225p\u03b8(xi | zt)) , where zt \u223c q(zt | z0,X) , (6)\nand L\u0303mse \u225c Eq [\u2211 t>1 \u2225\u03f5\u03b8(zt, t)\u2212 \u03f5\u2225 2 + log p\u03b8(z0 | z1) ] has the same form as the simple loss in DDPM; \u03bb is the weight scalar (we set it to 0.1 in our experiments). In particular, the KL terms above can endow closed forms depending on the task-specific qi(xi). For example, in a classification task with xi representing labels, both qi(xi) and p\u03b8(xi | zt) are defined as discrete distributions, making the KL divergence equivalent to the cross entropy. We apply stochastic optimization for model learning. At each iteration, a random timestep t is first sampled. Then the corresponding zt is sampled from the forward process with task information aggregation. We then feed zt to the reverse model to predict the forward noise and the task data. Finally, gradient descent is applied to update the model parameter based on the loss equation 6.\nInference A distinction of our MT-Diffusion is its ability to simultaneously generating multi-task data. We propose a generic inference procedure that can achieve both unconditional generation (with initially all missing task data X) or conditional generation (with initially parts of task data X known). The basic idea is to estimate the potentially missing task data from the corresponding heads of the reverse model outputs. The specific algorithm is summarized in Algorithm 1 in Appendix C."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "Diffusion-based Models Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) have been state-of-the-art generative models on a variety of applications including image syntheses (Dhariwal & Nichol, 2021), text-to-image generation (Ramesh et al., 2021; Saharia et al., 2022; Yu et al., 2022; Rombach et al., 2022), audio generation (Kong et al., 2021; Liu et al., 2023), video generation (Ho et al., 2022; Harvey et al., 2022; Singer et al., 2023) and text generation (Austin et al., 2021; Li et al., 2022b; He et al., 2022; Gong et al., 2023), etc. All these models, however, only focus on a single generation task, in contrast to our multi-task generation.\nManipulating Diffusion Latent Spaces There have been significant efforts to manipulating latent spaces of pretrained diffusion models for a variety of downstream tasks, including text-driven image editing, inpainting, completion and etc (Gal et al., 2022; Ruiz et al., 2022b; Cohen et al., 2022; Kawar et al., 2022; Meng et al., 2021b; Bau et al., 2021; Avrahami et al., 2022b;a; Bar-Tal et al., 2022; Lugmayr et al., 2022). There are also related works to learning a more discriminative latent space (Zhang et al., 2022; Preechakul et al., 2022) or manipulate a latent space for better representations (Kwon et al., 2023). These methods, however, are task-driven and do not provide\ntheoretical foundation on the working principles; while all the tasks considered in the literature can be accomplished with our MT-Diffusion framework by incorporating the corresponding task data.\nOur model is also related to guided diffusion, which is discussed in details in Appendix D."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "As a first work on multi-task diffusion models, we focus on evaluating our MT-Diffusion on the 4 multi-task learning settings mentioned in the Introduction, while leaving other more complicated settings such as incorporating more tasks into training as interesting future work. We describe detailed hyperparameter settings, the four tasks and encoder-decoder designs in Appendix E. In the four experiments, MT-Diffusion for image translation and joint image-representation generative modeling are mostly for illustration purposes. Thus, the results are mostly qualitative, e.g., Figure 4 demonstrates some visualizations of image-translate. More details are deferred to Appendix E."
        },
        {
            "heading": "4.1 MT-DIFFUSION FOR MASKED-IMAGE TRAINING",
            "text": "Task description and encoderdecoder design We propose the masked-image training, a new training strategy we design to improve image generation with MT-Diffusion. The task is motivated from the maskedlanguage pretraining paradigm in\nNLP models such as BERT (Devlin et al., 2019a), which achieves significant success with multi-task training. Specifically, in addition to the standard image generation task that generates images from noise, we define an additional task, called a random inpainting task, which learns to recover from randomly masked images. Consequently, the forward process is defined to start from a clean-masked\nimage pair, and gradually add noise to the pair in the forward process. To create the masked images, we randomly sample m \u223c Uniform(0 \u00b7 \u00b7 \u00b7 10) patches of size 16 \u00d7 16 from the original image and mask them out with zero pixel-values. These patches are placed randomly so they might overlap with each other. We adopt two choices for the noise prediction network \u03f5\u03b8(\u00b7): the first only considers (zt, t) as input, denoted as MT-Diffusion-U; the other takes (zt,X, t), denoted as MT-Diffusion-X. The former is more specifically designed for unconditional generation, whereas the latter is more suitable for constrained image restoration, a task we defined as inpainting a randomly masked image while keeping the unmasked region unchanged. Note current state-of-the-art diffusion models do not directly deal with this problem. Existing work for image editing and inpainting such as SDEdit (Meng et al., 2021a) and Dreambooth (Ruiz et al., 2022a) do not explicitly enforce unmask region consistency, thus their inpainting results can change the unmasked region. By considering simultaneously generating the clean-masked image pairs with MT-Diffusion, our model can maximally learn to maintain the consistency of the unmasked regions. Similar to the image-transition experiment, we use the identity map as the encoder, and replicate the output block of the original U-Net as the additional masked-image decoder, which consists of a normalization layer and a convolution layer.\nResults We implement our method based on the guided diffusion codebase (dif) on the ImageNet dataset under a resolution of 64 (Deng et al., 2009). We first demonstrate that our framework can help to improve the training efficiency for image generation. To this end, we compare our and the ADM models (Dhariwal & Nichol, 2021) before convergence at 1M iteration at image resolution 32. For a fair comparison, all models are trained from scratch with exactly the same hyperparameters (thus the numbers are not directly comparable to the reported values). We adopt the same evaluation metrics as the ADM codebase under 5K samples. The quantitative results are shown in Table 2. Note MT-Diffusion-U is designed to generate images from complete random noise while MT-Diffusion-X needs a conditional masked input, which we simply\ndefine as a complete mask (all zeros) for this purpose. It is observed that both the two variants significantly outperform the unconditioned ADM in all metrics, indicating the training efficiency and modeling effectiveness of multi-task generative learning via masked-image training. In addition, MT-Diffusion-X is found to perform better than MT-Diffusion-U. We hypothesize this is because the conditional masked-image information makes the training of the model easier and more effective.\nIn addition to pure image generation, one unique property of MT-Diffusion-X is its ability to simultaneously perform constrained image restoration. To this end, we randomly mask out some testing images with m = {5, 10, 15, 20} patches and learn to restore the masked patches. We expect\nMT-Diffusion-X to restore masked images without changing unmasked regions. Some example results are illustrated in Figure 5, which clearly demonstrate the strong ability of MT-Diffusion-X for constrained image restoration as well as generation from scratch. For quantitative evaluation, we adopt the LPIPS score (Zhang et al., 2018) that measures the semantic similarity of the original the restored images, and compare our method with one simple baseline from Meng et al. (2022), denoted as SDEdit-. The results are shown in Table 1, where the row of \u201cClean-Masked\u201d denotes the LPIPS scores of clean-masked image pairs that we include for reference. It is clear that MT-Diffusion-X obtains scores closed to zero, indicating the closed similarity between restored images and original images. SDEdit-, on the other hand, obtains very high LPIPS scores, which are even higher than the Clean-Masked baseline. This is expected since SDEdit- is not specifically designed for such a task.\nFinally, to demonstrate the ability of our model to generate high-quality images at convergence, we compare our method with ADM for pure image generation, under a resolution of 128. We train our MT-Diffusion-X from scratch. We evaluate the ADM with two versions: the released checkpoint and a continued trained version from the checkpoint with the same hyperparameters as our model, for a more fair comparison. The results are shown in Table 3 evaluated on 50K samples. Our method achieves comparable performance than ADM (if not better on some metrics such as the IS), while being able to perform more tasks such as the constrained image restoration demonstrated above. It is also noted that the continue-trained version of ADM (started from the released checkpoint) is slightly worse than the released checkpoint, indicating the latter might have been tuned for best performance, and thus it is more fair to compare our methods with the continue-trained version of ADM."
        },
        {
            "heading": "4.2 MT-DIFFUSION FOR JOINT IMAGE-LABEL GENERATION MODELING",
            "text": "Task description and encoder-decoder design This is a more heterogeneous case as labels and images are in different spaces. We follow our design principle to use the diffusion U-Net as the encoder for labels via cross attention in the U-Net. The label decoder is an additional head from some layer of the U-Net. We consider two designs: 1) Add one additional MLP layer out of the middle block of the UNet to map the diffusion latent space onto the label space. This introduces minimal extra parameters into the original reverse model but might enforce some discriminative information not helpful for pure image generation. We denote this variant as MT-Diffusion-M. 2) Add a pre-defined classifier at the end of the U-Net output. Since the U-Net output can be used to reconstruct the original image, this structure essentially makes the learning of image generation and classification in a sequential manner. In the experiments, we use the classifier provided in the guided diffusion codebase (dif) as the label decoder. We denote this variant as MT-Diffusion-E. Results We adopt the same experiment setting as the previous section. In addition to measuring the generated image quality, we also measure the classification performance using the classifier in MT-Diffusion-E. We find that for such a heterogeneous setting, continuing finetuning both the generator and classifier with single generation and classification tasks, respectively, can significantly improve single-task performance. Consequently, we adopt similar idea of classifier-free guidance to simultaneously learn a pure-generation model along with the multi-task training with the shared U-Net. We denote the finetuned models as MT-Diffusion-M\u2217 and MT-Diffusion-E\u2217. The results are reported in Table 2. It is observed that MT-Diffusion-M performs better than MT-Diffusion-E, which slightly under-perform the single-task ADM. This is expected and indicates that learning to generate heterogeneous data can trade off single-task performance. We also believe the performance gap is partly due to the un-tuned sub-optimal hyperparameter setting. With single-task finetuning, we can see that both variants outperform ADM in all metrics. To continue finetuning the classifier, we use the training and evaluation script from the codebase (dif), and compare with the pretrained classifier in classifier-guidance ADM (dif). We continue finetuing the classifier from the released checkpoint as a baseline. Top-1 and top-5 accuracies are plotted in Figure 6. It is observed that our classifier consistently outperforms the baseline, although the gap turns smaller with increasing finetuning steps."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose the multi-task diffusion model, a generalization of the standard diffusion model for multi-task generative learning. Our model is general and flexible, which can incorporate potentially heterogeneous task information into a unified diffusion model to improve multi-task performance, compared to training on a single-task setting. We define several multi-task generative problems and test them on our proposed MT-Diffusion. Extensive experiments are performed to verify the effectiveness of our proposed framework. Interesting future works include improving the framework by better network-architectures designs and applying the method to more diverse multi-task settings."
        },
        {
            "heading": "A ELBO OF THE PROPOSED MULTI-TASK DIFFUSION MODEL: THEOREM 1",
            "text": "We give detailed derivations of our multi-task diffusion ELBO in the following:\nL = Eq \u2212 log p(zT )\u2212 T N\u2211 i=1 log 1 q(xi) \u2212 \u2211 t\u22651 log p\u03b8(zt\u22121 | zt) \u220fN i=1 p\u03b8(xi | zt) q(zt | zt\u22121,X)  = Eq [ \u2212 log p(zT )\u2212 T\nN\u2211 i=1 log 1 q(x) \u2212 \u2211 t>1 log p\u03b8(zt\u22121 | zt) \u220fN i=1 p\u03b8(xi | zt) q(zt\u22121 | zt, z0,X) q(zt\u22121 | z0,X) q(zt | z0,X)\n\u2212 log p\u03b8(z0 | z1)\n\u220fN i=1 p\u03b8(xi | z1)\nq(z1 | z0,X)\n]\n= Eq [ \u2212 log p(zT )\nq(zT | z0,X) \u2212 \u2211 t>1 log p\u03b8(zt\u22121 | zt) q(zt\u22121 | zt, z0,X) \u2212 \u2211 t\u22651 N\u2211 i=1 log p\u03b8(xi | zt) q(xi) \u2212 log p\u03b8(z0 | z1)\n , where we use the fact that\nq(zt | zt\u22121,x) = q(zt | zt\u22121,x, z0) = q(zt\u22121 | zt, z0,x)q(zt | z0,x)\nq(zt\u22121 | z0,x) in the second equality."
        },
        {
            "heading": "B CALCULATING THE POSTERIOR DISTRIBUTIONS: THEOREM 2",
            "text": "In our derivation, we will frequently use the following well-known property of Gaussian random variables.\nIn the following, we first present a Lemma on calculating the posterior distribution of Gaussian random variables, based on which we derive the posterior distribution of our forward process. Lemma 3. Let \u03f51 \u223c N (\u00b51, \u03c321 I), \u03f52 \u223c N (\u00b52, \u03c322 I). Then, for \u2200a \u2265 0, b \u2265 0, the random variable \u03f5 \u225c a\u03f51 + b\u03f52 follows:\n\u03f5 \u223c N ( a\u00b51 + b\u00b52, (a 2\u03c321 + b 2\u03c322) I ) .\nNote in the forward aggregation, zt\u22121 + \u2211N i=1 w (i) t Ei(xi) = zt\u22121 +w (1) t (\u2211N i=1 w (i) t\nw (1) t\nEi(xi)\n) \u225c\nzt\u22121 +w (1) t E(X), where we define E(X) \u225c \u2211N i=1 w (i) t\nw (1) t\nEi(xi). This is equivalent to the form of\nconsidering only one extra task with task-data embedding E(X), e.g., it suffices to only consider two tasks in the proof. Consequently, in the following, we give the derivations of the forward posterior distribution with one additional task, in which case we will drop the task index i in x. Generalizing to N task is straightforward. To derive the marginal distribution q(zt | z0,x), let \u03f5t \u223c N (\u03f5; 0, I) for \u2200t. For the forward process, we have\nzt = \u221a \u03b1t(zt\u22121 +wtE(x)) + \u221a 1\u2212 \u03b1t\u03f5t\n= \u221a \u03b1t (\u221a \u03b1t\u22121 (zt\u22122 +wt\u22121E(x)) + \u221a 1\u2212 \u03b1t\u22121\u03f5t\u22121 + wtE(x) ) + \u221a 1\u2212 \u03b1t\u03f5t = \u221a \u03b1t\u03b1t\u22121 zt\u22122 + ( wt \u221a \u03b1t + wt\u22121 \u221a \u03b1t\u03b1t\u22121 ) E(x) + \u221a 1\u2212 \u03b1t\u03b1t\u22121\u03f5 = \u221a \u03b1t\u03b1t\u22121 (\u221a \u03b1t\u22122 (zt\u22123 +wt\u22122E(x)) + \u221a 1\u2212 \u03b1t\u22122\u03f5t\u22122\n) + ( wt \u221a \u03b1t + wt\u22121 \u221a \u03b1t\u03b1t\u22121 ) E(x) + \u221a 1\u2212 \u03b1t\u03b1t\u22121\u03f5\n= \u221a \u03b1t\u03b1t\u22121\u03b1t\u22122 zt\u22123 + ( wt \u221a \u03b1t + wt\u22121 \u221a \u03b1t\u03b1t\u22121 + wt\u22122 \u221a \u03b1t\u03b1t\u22121\u03b1t\u22122 ) E(x)\n+ \u221a 1\u2212 \u03b1t\u03b1t\u22121\u03b1t\u22122\u03f5 ,\nwhere we apply Lemma 3 in the third equation to consolidate the two random Gaussian variables \u03f5t\u22121 and \u03f5t into \u03f5.\nLet x\u0304t \u225c \u220ft i=1 \u03b1i, \u03b1\u0303t = \u2211t i=1 \u220ft j=i \u03b1j , and define \u03b1\u03030 = 0. We have\n\u03b1\u0303t = \u221a \u03b1t (wt + \u03b1\u0303t\u22121) , and\nq(zt | z0,x) = N ( zt; \u221a \u03b1\u0304t z0 +\u03b1\u0303tE(x), (1\u2212 \u03b1\u0304t) I ) . (7)\nAs a special case, if we define the forward transition distribution by letting wt = 1, we will have:\n\u03b1\u0303t = \u221a \u03b1t (1 + \u03b1\u0303t\u22121) . (8)\nLemma 4 (Murphy (2022)). Define the following distributions for the prior and likelihood: p(x) = N ( x;\u00b5,\u039b\u22121 ) , p(y |x) = N ( y;Ax+b,L\u22121 ) .\nLet \u03a3 = ( \u039b +AT LA )\u22121 . Then the posterior follows:\np(x |y) = N ( x; \u03a3 ( AT L(y\u2212b) + \u039b\u00b5 ) ,\u03a3 ) .\nIn our case in equation 7, we have \u00b5 \u225c \u221a \u03b1\u0304t\u22121 z0 +\u03b1\u0303t\u22121E(x), \u039b \u225c 11\u2212\u03b1\u0304t\u22121 I, A \u225c \u221a \u03b1t I, b \u225c \u221a \u03b1twtE(x), L \u225c 11\u2212\u03b1t I, and \u03a3 = (1\u2212\u03b1t)(1\u2212\u03b1\u0304t\u22121) 1\u2212\u03b1\u0304t I. Thus, the posterior q(zt\u22121 | zt, z0,x) =\nN ( zt\u22121; \u00b5\u0303t(zt, z0,x), \u03b2\u0303t I ) , where\n\u00b5\u0303t(zt, z0,x) =\n\u221a \u03b1t(1\u2212 \u03b1\u0304t\u22121) zt +(1\u2212 \u03b1t) \u221a \u03b1\u0304t\u22121 z0 +((1\u2212 \u03b1t)\u03b1\u0303t\u22121 \u2212 \u03b1t(1\u2212 \u03b1\u0304t\u22121)wt)E(x)\n1\u2212 \u03b1\u0304t (9)\n=\n\u221a \u03b1t(1\u2212 \u03b1\u0304t\u22121) zt +(1\u2212 \u03b1t) \u221a \u03b1\u0304t\u22121 z0 + ( (1\u2212 \u03b1t)\u03b1\u0303t/ \u221a \u03b1t \u2212 (1\u2212 \u03b1\u0304t)wt ) E(x)\n1\u2212 \u03b1\u0304t\n\u03b2\u0303t = (1\u2212 \u03b1t)(1\u2212 \u03b1\u0304t\u22121)\n1\u2212 \u03b1\u0304t .\nFrom the marginal distribution q(zt | z0,x), we have\nzt = \u221a \u03b1\u0304t z0 +\u03b1\u0303tE(x) + \u221a 1\u2212 \u03b1\u0304t\u03f5\n\u2192 z0 = 1\u221a \u03b1\u0304t\n( zt \u2212\u03b1\u0303tE(x)\u2212 \u221a 1\u2212 \u03b1\u0304t\u03f5 ) . (10)\nSubstituting equation 10 into equation 9, we have\n\u00b5\u0303t(zt,x, t)\n=\n\u221a \u03b1t(1\u2212 \u03b1\u0304t\u22121) zt +(1\u2212 \u03b1t) \u221a \u03b1\u0304t\u22121(\n1\u221a \u03b1\u0304t\n( zt \u2212\u03b1\u0303tE(x)\u2212 \u221a 1\u2212 \u03b1\u0304t\u03f5 ) )\n1\u2212 \u03b1\u0304t\n+\n( (1\u2212\u03b1t)\u03b1\u0303t\u221a\n\u03b1t \u2212 (1\u2212 \u03b1\u0304t)wt\n) E(x)\n1\u2212 \u03b1\u0304t\n= ( \u221a \u03b1t(1\u2212 \u03b1\u0304t\u22121) + (1\u2212 \u03b1t)/ \u221a \u03b1t) zt \u2212(1\u2212 \u03b1\u0304t)wtE(x)\u2212 (1\u2212 \u03b1t)\n\u221a (1\u2212\u03b1\u0304t)\u03b1\u0304t\u22121\n\u03b1\u0304t \u03f5\n1\u2212 \u03b1\u0304t\n= 1\u221a \u03b1t\n( zt \u2212\n1\u2212 \u03b1t\u221a 1\u2212 \u03b1\u0304t \u03f5\n) \u2212 wtE(x)\nThus, similar to the standard DDPM, we can parameterize the backward denoising process with a neural network to predict the added noise \u03f5, except that in our case, the neural network \u03f5\u03b8 would take (zt,x, t) as the input, i.e.,\n\u03f5\u03b8(zt,x, t) = \u03f5\u03b8( \u221a \u03b1\u0304t z0 +\u03b1\u0303t x+ \u221a 1\u2212 \u03b1\u0304t\u03f5,x, t) \u2248 \u03f5 .\nC INFERENCE\nThe inference algorithm is shown in Algorithm 1. If not explictly stated, the number be timesteps is set to T = 1000.\nAlgorithm 1 MT-Diffusion Inference 1: zT \u223c N (0, I) 2: if Task data xi (\u2200i) not initially available then 3: Randomly initialize task data xi 4: end if 5: for t = T, \u00b7 \u00b7 \u00b7 , 1 do 6: \u03f5 \u223c N (0, I) if t > 1, else \u03f5 = 0 7: ei = E(xi), \u2200i \u25b7 Get task data encoding 8: \u03f5\u03b8(zt, t), X\u0303 = U-Net(zt, t) \u25b7 Get estimated noise and predicted task data 9: if xi (\u2200i) not initially available then 10: Update xi from the new X\u0303,\u2200i 11: end if 12: zt\u22121 =\n1\u221a \u03b1t ( zt \u2212 1\u2212\u03b1t\u221a1\u2212\u03b1\u0304t \u03f5\u03b8(zt,x, t) ) \u2212 \u2211N i=1 w (i) t ei +\u03b2\u0303t\u03f5 \u25b7 Update diffusion latent\n13: end for 14: return z0,X"
        },
        {
            "heading": "D RELATED WORKS",
            "text": "Connections to Classifier and Classifier-Free Guidance Guided diffusion models aim to leverage prior knowledge from various guidance information for better controllable generation. For example, the classifier guidance method uses the gradient of a pretrained classifier to perturb the reverse process to generate from a class-conditional distribution (Dhariwal & Nichol, 2021). The classifier-free guidance simultaneously learns a guidance model using the same generation network of the diffusion model (Ho & Salimans, 2022). The works that try to utilize external data such as the retrievalaugmented based methods (Blattmann et al., 2022; Long et al., 2022) can also be considered as a special type of guidance. Although the final formulation has some connections with our method (see Appendix D), guided diffusion models essentially only handle a single generation task. Our method, on the other hand, can model multiple tasks within a unified diffusion model.\nOur MT-Diffusion formulation endows an closed connection with the classifier guidance and classifierfree guidance mechanisms. Specifically, from equation 5, if one defines the encoder E(\u00b7) as the gradient from a pretrained classifier, the posterior mean recovers the one for classifier guidance. By contrast, if one defines the encoder with the reverse U-Net, the posterior mean calculation in equation 5 recovers the classifier-free guidance mechanism. However, an important difference is the forward process, where our framework is designed to aggregate information from different encoders for multi-task learning, whereas both classifer guidance and classifer-free guidance do not. Overall, our method constitutes a broader framework that can be applied to different scenarios, including image transition, masked-image pretraining, joint image-label and image-representation generation investigated in the experiments.\nMulti-Task Learning Multi-Task Learning (MTL) is a paradigm in machine learning that involves training a model to perform multiple tasks simultaneously, with the idea that knowledge gained from one task can help improve the performance on other related tasks. Recent development has mainly focused on multi-task learning for predictive models instead of generative models. Apart from investigating theory in multi-task learning (Wang et al., 2021; Tiomoko et al., 2021; Tripuraneni et al., 2020; Wu et al., 2020), many existing works explore different techniques to boost model performance with multi-task learning, including but not limited to architecture designs (Heuer et al., 2021; Ruder et al., 2017; Ye & Xu, 2023; Sharma et al., 2023; Chen et al., 2022), optimization algorithms (Senushkin et al., 2023; Fernando et al., 2023; Jiang et al., 2023; Phan et al., 2022) and task relationship learning (Hu et al., 2022; Ilharco et al., 2022). Recent research interest has also be expanded to applying multi-task learning in generative models (Bao et al., 2022; Liu et al., 2018). However, the generative models are limited to more traditional models such as VAE and GAN. And\nthere is limited work on studying multi-task learning for diffusion models. Our work represents one of the first works on integrating multi-task learning with diffusion models, aiming to further improve generative performance and expand the scope of state-of-the-art diffusion models."
        },
        {
            "heading": "E DETAILED EXPERIMENTAL SETTINGS AND EXTRA RESULTS",
            "text": "In addition to evaluating on some other datasets that will be described in the specific tasks, we mainly rely on the ImageNet-1K dataset (Deng et al., 2009) with resolutions of 64\u00d764 and 128\u00d7128, where we adopt the pre-defined training and validation splits. All experiments are conducted on a A100 GPU server with a batchsize of 64, if not explicitly specified. When evaluating generation quality, we follow and adopt the popular Inception Score (IS), FID score, sFID score, Precision and Recall metrics (dif), calculated on 10K or 50K samples, where the former is for computational efficiency and latter for comparing with existing results. We note that due to our different hyperparameter settings (specified in the Appendix), some of our results are not directly comparable to some reported results in previous works. For fair comparisons, we rerun some of the baselines on our settings that are consistent with our method. One additional hyperparameter of our model is the task weights in equation 3, which we set to w(i)t = t/(1000\u2212 t) to mitigate some potentially negative influence from some heterogeneous tasks on the generated image quality when t is small. We follow most of the parameter settings as in the codebases.\nThe training procedure is summarized in Algorithm 2.\nAlgorithm 2 MT-Diffusion Training 1: repeat 2: z0 \u223c q(z0), X \u223c q(X) \u25b7 Sample z0 and task data X 3: t \u223c Scheduler(1, \u00b7 \u00b7 \u00b7 , T ) 4: {ei} = E(X) \u25b7 Get task data encoding 5: \u03f5 \u223c N(0, I) 6: zt \u223c q(zt | z0,X) \u25b7 Forward aggregation via equation 4 7: ei = E(xi), \u2200i \u25b7 Get task data encoding 8: \u03f5\u03b8(zt, t), X\u0303 = U-Net(zt, t) \u25b7 Noise and task data prediction via the reverse model 9: Take gradient descent step based on the loss equation 6 10: until Converged\nE.1 EXPERIMENT SETTINGS FOR IMAGE TRANSLATION WITH MT-DIFFUSION\nFor the Cityscape dataset, the task data corresponds to the semantic segmentation maps; and for the night2day dataset, the task data corresponds to images of day time. We adopt the latent diffusion codebase from ldm, and use the provided checkpoints of the VQ-VAE encoder-decoder (kl-f8.pt). We use the VQ-VAE encoder as the encoder for task data; and construct an additional output head by duplicate the original output block of the U-Net structure as the decoder to generate the task data, which consists of a normalization layer, a SiLU layer, and a convolution layer. We use the default hyper-parameters for training the models for the two datasets, summarized as:\n\u2022 Attention resolutions: (32, 16, 9) \u2022 Diffusion steps: 1000 \u2022 Learn sigma: False \u2022 Noise schedule: Linear \u2022 #channels: 320 \u2022 #heads: 8 \u2022 #res blocks: 2 \u2022 Resblock updown: False \u2022 Use scale shift norm: False \u2022 Learning rate: 1.0e-4\n\u2022 Batch size: 32\nTask description and encoder-decoder design This is a more taskhomologous setting. We adopt two standard datasets, the Cityscale dataset for semantic-labels to photo translation (Cordts et al., 2016) and the night2day dataset for night-to-day\nphoto translation (Laffont et al., 2014). We adopt the public codebase of latent diffusion model (LDM) (ldm). For the translation problem, the task data (original and translated images) are in the same data space, thus we do not need to explicitly define separate encoders Ei(\u00b7) for the task data. Instead, we use the same pretrained image encoder in LDM to map all images to the diffusion latent space. We add another head at the end of the U-Net as the decoder for target translated images.\nResults We perform image translation by generating target images conditioned on source images based on Algorithm 1. Some example generated image are illustrated in Figure 4. For quantitative evaluation, we follow Zhu et al. (2017) to measure the performance in terms of per-pixel accuracy, per-class accuracy and class IOU, and compare it with exiting methods (Zhu et al., 2017; Isola et al., 2017; Wang et al., 2022). The results are shown in Table 4. It is cleared that our model obtains the best accuracy compared to the baselines, except for the state-of-the-art InternImage-H model, which is a much larger image foundation model pretrained on web-scaled data, thus it is not comparable. We also calculate the IS and FID scores on the night2day dataset. Note prior work did not typically calculate these scores. We obtain an FID score of 37.93 and an IS of 3.94, and the IS score is even slightly better than that of the ground-truth data (3.65). As a comparison, the FID and IS scores with a single-task diffusion are 40.73 and 3.84, respectively.\nE.2 MT-DIFFUSION FOR MASKED-IMAGE TRAINING\nIn this task, the task data is a randomly masked version of the original images. To create a randomly masked image, we random sample a coordinate (x, y) that is within the image, then we masked out a patch (x : min(x + 16, 64)), y : min(y + 16, 64) by setting the corresponding pixel values to zeros. We repeat this process for m to control the ratio of masked regions. We adopt the latent diffusion codebase from dif. We simply define the encoder as the identity map, and define the decoder for the masked images by replicating the output block of the original U-Net, similar to the above Image Translation experiment. We adopt the default hyper-parameters for training the models, if not specified below.\n\u2022 Diffusion steps: 1000\n\u2022 Rescale learned sigmas: False\n\u2022 Rescale timesteps: False\n\u2022 Noise schedule: cosine\n\u2022 #channels: 192\n\u2022 #res blocks: 3\n\u2022 Learning rate: 7.0e-5\n\u2022 Batch size: 80\nMore random samples from scratch and image restoration results from both random masking and half masking are illustrated in Figure 7, 8, 9 and 10.\nE.3 MT-DIFFUSION FOR JOINT IMAGE-LABEL GENERATION MODELING\nIn this task, the task data are discrete labels. We use the original U-Net as the encoder, which tasks a noisy image, a label and a timestep as input. For simplicity, we set the noisy image and the timestep to\nbe zeros, although we believe better results can be obtained by jointly encoding with such information. For decoders, we proposes two options, one go out of the middle block of the U-Net and the other go out of the output block, as described in the main text. For the one from the middle block, we simply add one fully connected layer to define the decoder; and for the one from the output block, we adopt the pre-defined classifier from the codebase dif as the decoder. We adopt the default hyper-parameters for training the models, if not specified below. First, for MT-Diffusion-M:\n\u2022 Diffusion steps: 1000 \u2022 Rescale learned sigmas: False \u2022 Rescale timesteps: False \u2022 Noise schedule: cosine \u2022 #channels: 192\n\u2022 #res blocks: 3 \u2022 Learning rate: 7.0e-5 \u2022 Batch size: 75\nThe setting for MT-Diffusion-E is the same as MT-Diffusion-M, except with some extra hyperparameters for the pre-difined classifier:\n\u2022 Classifier_attention_resolutions: ( 32,16,8) \u2022 Classifier_depth: 4 \u2022 Classifier_pool: attention \u2022 Classifier_resblock_updown: True \u2022 Classifier_use_scale_shift_norm: True\nE.4 MT-DIFFUSION FOR JOINT IMAGE-REPRESENTATION GENERATION MODELING\nTask description and encoder-decoder design Finally, we apply MT-Diffusion for joint imagerepresentation generation. The setting is similar to the image-label generation setting in Section 4.2, by replacing the label data with image representation from the CLIP model Radford et al. (2021). Similarly, we use the original U-Net as the encoder for image representations via the cross-attention mechanism. For the decoder, we append a two-layer MLP to the output of the middle block of the U-Net, which is expected to output image representations. The MLP project the tensor from middle block to dimension of 1024, followed by a ReLU layer, and finally another layer output tensor of 1024. For MT-Diffusion for Joint Image-Representation:\n\u2022 Diffusion steps: 1000 \u2022 Learning rate: 1.0e-5 \u2022 Batch size: 2048\nResults We conduct large-scale experiments based on the pretrained stable diffusion model Rombach et al. (2022)\u00a7, by continuing finetuning the model on the LAION dataset Schuhmann et al. (2022) with our MT-Diffusion. We adopt the default hyperparameter setting as that in the codebase. Due to the large-scale nature, it is challenging to make fair quantitative comparisons with related methods. Thus, we only show some generated examples from our method, and leave more extensive comparisons as future work. Some randomly generated examples are shown in Figure 11 and 12, demonstrating impressive generated quality results.\nWe also provide a visulized comparison between our MT-diffusion with the stable diffusion baseline in Figure 13 and 14. From the generated images, it appears that our method can understand the semantic meaning of the images and generate better looking images.\n\u00a7https://huggingface.co/stabilityai/stable-diffusion-2"
        },
        {
            "heading": "A beautiful photograph of a girl with Switzerland",
            "text": "landscape in the background with trees. A developer working in an office, photo, detailed image A hedgehog using a calculator. A night sky filled with stars above a\nturbulent sea with giant waves.\nA photo of an astronaut riding a horse in the forest. There is a river in front of them with water lilies. A red colored dog.\nA rustic cabin sits on the edge of a giant lake. Wildflowers dot the meadow around the cabin and lake. A squirrel driving a toy car.\nA rustic wooden coffee table adorned with scented candles and many books A small chair sits in front of a table on the wooden floor. There is a bookshelf nearby the window. A sunset over a mountain range, vector image. A watercolor painting of a chair that looks like an octopus."
        }
    ],
    "year": 2023
}