{
    "abstractText": "Theoretical and empirical comparisons have been made to assess the expressive power and performance of invariant and equivariant GNNs. However, there is currently no theoretical result comparing the expressive power of k-hop invariant GNNs and equivariant GNNs. Additionally, little is understood about whether the performance of equivariant GNNs, employing steerable features up to type-L, increases as L grows \u2013 especially when the feature dimension is held constant. In this study, we introduce a key lemma that allows us to analyze steerable features by examining their corresponding invariant features. The lemma facilitates us in understanding the limitations of k-hop invariant GNNs, which fail to capture the global geometric structure due to the loss of geometric information between local structures. Furthermore, we analyze the ability of steerable features to carry information by studying their corresponding invariant features. In particular, we establish that when the input spatial embedding has full rank, the informationcarrying ability of steerable features is characterized by their dimension and remains independent of the feature types. This suggests that when the feature dimension is constant, increasing L does not lead to essentially improved performance in equivariant GNNs employing steerable features up to type-L. We substantiate our theoretical insights with numerical evidence.",
    "authors": [
        {
            "affiliations": [],
            "name": "STEERABLE FEATURES"
        },
        {
            "affiliations": [],
            "name": "Shih-Hsin Wang"
        },
        {
            "affiliations": [],
            "name": "Yung-Chang Hsu"
        },
        {
            "affiliations": [],
            "name": "Justin Baker"
        },
        {
            "affiliations": [],
            "name": "Andrea Bertozzi"
        },
        {
            "affiliations": [],
            "name": "Jack Xin"
        },
        {
            "affiliations": [],
            "name": "Bao Wang"
        }
    ],
    "id": "SP:10f5b4c667ad7fbac6e66ec1755bea206ec46139",
    "references": [
        {
            "authors": [
                "Uri Alon",
                "Eran Yahav"
            ],
            "title": "On the bottleneck of graph neural networks and its practical implications",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ilyes Batatia",
                "D\u00e1vid P\u00e9ter Kov\u00e1cs",
                "Gregor NC Simm",
                "Christoph Ortner",
                "G\u00e1bor Cs\u00e1nyi. Mace"
            ],
            "title": "Higher order equivariant message passing neural networks for fast and accurate force fields",
            "venue": "arXiv preprint arXiv:2206.07697,",
            "year": 2022
        },
        {
            "authors": [
                "Simon Batzner",
                "Albert Musaelian",
                "Lixin Sun",
                "Mario Geiger",
                "Jonathan P Mailoa",
                "Mordechai Kornbluth",
                "Nicola Molinari",
                "Tess E Smidt",
                "Boris"
            ],
            "title": "Kozinsky. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
            "venue": "Nature communications,",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Brandstetter",
                "Rob Hesselink",
                "Elise van der Pol",
                "Erik J Bekkers",
                "Max Welling"
            ],
            "title": "Geometric and physical quantities improve E(3) equivariant message passing",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriele Cesa",
                "Leon Lang",
                "Maurice Weiler"
            ],
            "title": "A program to build E(n)-equivariant steerable CNNs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Lowik Chanussot",
                "Abhishek Das",
                "Siddharth Goyal",
                "Thibaut Lavril",
                "Muhammed Shuaibi",
                "Morgane Riviere",
                "Kevin Tran",
                "Javier Heras-Domingo",
                "Caleb Ho",
                "Weihua Hu"
            ],
            "title": "Open catalyst 2020 (oc20) dataset and community challenges",
            "venue": "Acs Catalysis,",
            "year": 2021
        },
        {
            "authors": [
                "Taco Cohen",
                "Max Welling"
            ],
            "title": "Group equivariant convolutional networks",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Taco Cohen",
                "Max Welling"
            ],
            "title": "Steerable CNNs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Weitao Du",
                "He Zhang",
                "Yuanqi Du",
                "Qi Meng",
                "Wei Chen",
                "Nanning Zheng",
                "Bin Shao",
                "Tie-Yan Liu"
            ],
            "title": "Se (3) equivariant graph neural networks with complete local frames",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Weitao Du",
                "Yuanqi Du",
                "Limei Wang",
                "Dieqiao Feng",
                "Guifeng Wang",
                "Shuiwang Ji",
                "Carla Gomes",
                "Zhi-Ming Ma"
            ],
            "title": "A new perspective on building efficient and expressive 3d equivariant graph neural networks",
            "venue": "arXiv preprint arXiv:2304.04757,",
            "year": 2023
        },
        {
            "authors": [
                "Nadav Dym",
                "Haggai Maron"
            ],
            "title": "On the universality of rotation equivariant point cloud networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Marc Finzi",
                "Samuel Stanton",
                "Pavel Izmailov",
                "Andrew Gordon Wilson"
            ],
            "title": "Generalizing convolutional neural networks for equivariance to Lie groups on arbitrary continuous data",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Fabian Fuchs",
                "Daniel Worrall",
                "Volker Fischer",
                "Max Welling"
            ],
            "title": "SE(3)-transformers: 3D roto-translation equivariant attention networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Shankari Giri",
                "Johannes T Margraf",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Fast and uncertainty-aware directional message passing for non-equilibrium molecules",
            "venue": "arXiv preprint arXiv:2011.14115,",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Janek Gro\u00df",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Directional message passing for molecular graphs",
            "venue": "arXiv preprint arXiv:2003.03123,",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey E Hinton",
                "Alex Krizhevsky",
                "Sida D Wang"
            ],
            "title": "Transforming auto-encoders",
            "venue": "In International conference on artificial neural networks,",
            "year": 2011
        },
        {
            "authors": [
                "Thomas J Jech"
            ],
            "title": "The axiom of choice",
            "venue": "Courier Corporation,",
            "year": 2008
        },
        {
            "authors": [
                "Erik Jenner",
                "Maurice Weiler"
            ],
            "title": "Steerable partial differential operators for equivariant neural networks",
            "venue": "arXiv preprint arXiv:2106.10163,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Jing",
                "Stephan Eismann",
                "Patricia Suriana",
                "Raphael John Lamarre Townshend",
                "Ron Dror"
            ],
            "title": "Learning from protein structure with geometric vector perceptrons",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Chaitanya K. Joshi",
                "Cristian Bodnar",
                "Simon V Mathis",
                "Taco Cohen",
                "Pietro Lio"
            ],
            "title": "On the expressive power of geometric graph neural networks",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Bo Li",
                "Qili Wang",
                "Gim Hee Lee"
            ],
            "title": "Filtra: Rethinking steerable CNN by filter transform",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yi-Lun Liao",
                "Tess Smidt"
            ],
            "title": "Equiformer: Equivariant graph attention transformer for 3D atomistic graphs",
            "venue": "arXiv preprint arXiv:2206.11990,",
            "year": 2022
        },
        {
            "authors": [
                "Yi-Lun Liao",
                "Brandon Wood",
                "Abhishek Das",
                "Tess Smidt"
            ],
            "title": "Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations",
            "venue": "arXiv preprint arXiv:2306.12059,",
            "year": 2023
        },
        {
            "authors": [
                "Yi Liu",
                "Limei Wang",
                "Meng Liu",
                "Yuchao Lin",
                "Xuan Zhang",
                "Bora Oztekin",
                "Shuiwang Ji"
            ],
            "title": "Spherical message passing for 3D molecular graphs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Saro Passaro",
                "C Lawrence Zitnick"
            ],
            "title": "Reducing SO(3) convolutions to SO(2) for efficient equivariant gnns",
            "venue": "arXiv preprint arXiv:2302.03655,",
            "year": 2023
        },
        {
            "authors": [
                "Omri Puny",
                "Matan Atzmon",
                "Edward J. Smith",
                "Ishan Misra",
                "Aditya Grover",
                "Heli Ben-Hamu",
                "Yaron Lipman"
            ],
            "title": "Frame averaging for invariant and equivariant network design",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ryszard Raczka",
                "Asim Orhan Barut"
            ],
            "title": "Theory of group representations and applications",
            "venue": "World Scientific Publishing Company,",
            "year": 1986
        },
        {
            "authors": [
                "David W Romero",
                "Erik J Bekkers",
                "Jakub M Tomczak",
                "Mark Hoogendoorn"
            ],
            "title": "Wavelet networks: Scale equivariant learning from raw waveforms",
            "venue": "arXiv preprint arXiv:2006.05259,",
            "year": 2020
        },
        {
            "authors": [
                "V\u0131ctor Garcia Satorras",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "E(n) equivariant graph neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kristof Sch\u00fctt",
                "Oliver Unke",
                "Michael Gastegger"
            ],
            "title": "Equivariant message passing for the prediction of tensorial properties and molecular spectra",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kristof T Sch\u00fctt",
                "Huziel E Sauceda",
                "P-J Kindermans",
                "Alexandre Tkatchenko",
                "K-R M\u00fcller"
            ],
            "title": "SchNet\u2013a deep learning architecture for molecules and materials",
            "venue": "The Journal of Chemical Physics,",
            "year": 2018
        },
        {
            "authors": [
                "John Shawe-Taylor"
            ],
            "title": "Building symmetries into feedforward networks",
            "venue": "First IEE International Conference on Artificial Neural Networks,(Conf. Publ. No",
            "year": 1989
        },
        {
            "authors": [
                "Tess E Smidt",
                "Mario Geiger",
                "Benjamin Kurt Miller"
            ],
            "title": "Finding symmetry breaking order parameters with euclidean neural networks",
            "venue": "Physical Review Research,",
            "year": 2021
        },
        {
            "authors": [
                "Nathaniel Thomas",
                "Tess Smidt",
                "Steven Kearnes",
                "Lusann Yang",
                "Li Li",
                "Kai Kohlhoff",
                "Patrick Riley"
            ],
            "title": "Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds",
            "venue": "arXiv preprint arXiv:1802.08219,",
            "year": 2018
        },
        {
            "authors": [
                "Jake Topping",
                "Francesco Di Giovanni",
                "Benjamin Paul Chamberlain",
                "Xiaowen Dong",
                "Michael M. Bronstein"
            ],
            "title": "Understanding over-squashing and bottlenecks on graphs via curvature",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Soledad Villar",
                "David W Hogg",
                "Kate Storey-Fisher",
                "Weichi Yao",
                "Ben Blum-Smith"
            ],
            "title": "Scalars are universal: Equivariant machine learning, structured like classical physics",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Limei Wang",
                "Yi Liu",
                "Yuchao Lin",
                "Haoran Liu",
                "Shuiwang Ji"
            ],
            "title": "ComENet: Towards complete and efficient message passing for 3D molecular graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Maurice Weiler",
                "Gabriele Cesa"
            ],
            "title": "General E(2)-equivariant steerable CNNs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Maurice Weiler",
                "Mario Geiger",
                "Max Welling",
                "Wouter Boomsma",
                "Taco S Cohen"
            ],
            "title": "3D steerable CNNs: Learning rotationally equivariant features in volumetric data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Maurice Weiler",
                "Patrick Forr\u00e9",
                "Erik Verlinde",
                "Max Welling"
            ],
            "title": "Coordinate independent convolutional networks\u2013isometry and gauge equivariant convolutions on riemannian manifolds",
            "venue": "arXiv preprint arXiv:2106.06020,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Winter",
                "Marco Bertolini",
                "Tuan Le",
                "Frank No\u00e9",
                "Djork-Arn\u00e9 Clevert"
            ],
            "title": "Unsupervised learning of group invariant and equivariant representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel E Worrall",
                "Stephan J Garbin",
                "Daniyar Turmukhambetov",
                "Gabriel J Brostow"
            ],
            "title": "Harmonic networks: Deep translation and rotation equivariance",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Machine learning (ML) tasks have different kinds of inherent Euclidean symmetries. For instance, translating or rotating an image does not change its label (invariance) [21] while applying Euclidean isometries to a molecule results in corresponding changes in its dynamics (equivariance) [3; 5]. Enforcing Euclidean symmetry in neural network design [35; 7] significantly improves sample efficiency [16; 8], generalizability [31], and robustness [13], and preserves the principles of physics [12]. Utilizing steerable features provides a powerful framework for crafting symmetry-aware neural networks. In particular, it enables the design of neural networks with equivariance to 3D Euclidean transformations by leveraging the group representations of O(3) and SO(3). The reducibility of the group representations of O(3) or SO(3) [30; 42] allows us to decompose any steerable vector into a direct sum of steerable vectors of different types. These types of steerable vectors are defined by Wigner-D matrices [42]; in particular, type-0 and type-1 steerable vectors corresponding to scalars and 3D vectors, resp. Research in designing equivariant models, especially graph neural networks (GNNs) that update steerable features up to type L, has surged. For instance, SchNet [34], DimeNet [15], SphereNet [25], and ComENet [40] are invariant GNNs that propagate invariant features (L = 0). Equivariant GNNs, e.g., as proposed in [32; 33], update steerable features up to L = 1. Meanwhile, numerous architectures are designed to handle steerable features for L > 1; see e.g. [8; 45; 37; 42; 13; 18; 43; 22; 4; 5]."
        },
        {
            "heading": "1.1 REMARKS ON SOME REMARKABLE STUDIES",
            "text": "Recent research, especially [20], highlights performance disparities between 1-hop message-passing invariant GNNs and equivariant GNNs. [20] introduces the geometric Weisfeiler-Lehman (GWL) test and its invariant version (IGWL) to characterize the expressive power of 1-hop equivariant and\n\u2217Correspond to wangbaonj@gmail.com\ninvariant GNNs, resp., and then shows that 1-hop invariant GNNs may underperform equivariant GNNs \u2013 primarily due to their limited ability to capture the global geometry of the graph. Intriguingly, [20] also reveals that 1-hop invariant GNNs can be as expressive as equivariant GNNs for fully connected input graphs. However, the theory in [20] is limited to 1-hop message passing, leaving questions about the implications of introducing multi-hop aggregation into the message-passing of invariant GNNs. E.g., ComENet [40] and SphereNet [25] introduce the concept of completeness of edge attributes \u2013 a property held by invariant edge features that enable them to uniquely determine the spatial embedding of a point cloud up to the group action. Specifically, ComENet achieves this completeness by computing dihedral angles between local structures, gathering information from up to 2 hops neighbors. Both ComENet and SphereNet can outperform equivariant GNNs in molecular modeling tasks, even when dealing with non-fully connected input graphs.\nThe effects of steerable features of different types have also been explored. E.g. [3; 23; 2; 28] show that the performance of equivariant GNNs \u2013 that use steerable features up to type-L \u2013 can be enhanced by increasing L. However, these experiments typically lack the control of feature dimensions [12]. This requires rethinking the benefits of increasing L while maintaining the feature dimension. Indeed, [4] first compares the performance of SEGNN \u2013 a particular equivariant GNN using different L, reporting limited improvement beyond L = 1 when feature dimensions are held constant."
        },
        {
            "heading": "1.2 OUR CONTRIBUTIONS",
            "text": "We aim to address the theoretical questions in Section 1.1 and build new understandings of the expressive power and message-passing mechanisms of invariant and equivariant GNNs. We summarize our major theoretical contribution as follows: \u2022 We establish Lemma 1, a key lemma showing that any steerable feature corresponds to some\ninvariant features. This lemma serves as a cornerstone for our theoretical analysis, enabling us to examine steerable features by studying their corresponding invariant counterparts. \u2022 We demonstrate that message-passing using steerable features can be interpreted as messagepassing with their corresponding invariant features. This perspective reveals that both equivariant and invariant GNNs propagate invariant features across different local neighborhoods. However, invariant GNNs, when compared to equivariant GNNs, lack the intrinsic capability to capture geometric information between local neighborhoods, even when adopting the k-hop messagepassing. In particular, k-hop invariant GNNs may struggle to capture the changing geometry between k-hop local structures and fail to obtain accurate global invariant features. \u2022 We analyze the ability of steerable features to carry information by studying their corresponding invariant features. In particular, we establish that when the input spatial embedding has full rank, the information-carrying ability of steerable features is characterized by their dimension and remains independent of the feature types utilized in steerable features. This result indicates that when preserving the feature dimension, the performance of equivariant GNNs employing steerable features up to type-L may not increase as L grows. \u2022 We provide numerical evidence that consistently echos the above theoretical insights. 2 BACKGROUND We recap group theory, steerable vector spaces as introduced in [4], the concepts of geometric graphs and geometric Graph Neural Networks (GNNs). We also review the Geometric Weisfeiler-Lehman test (GWL) and its relevant results from [20] in Appendix D.\nGroup theory. Let G be a group, and consider a space X on which G acts. When an element x \u2208 X undergoes the group action of G, denoted as g \u00b7 x, we define the G-orbit of x as the set G \u00b7 x := {g \u00b7 x \u2208 X | g \u2208 G} \u2286 X . The quotient space X/G contains all G-orbits. Additionally, we define the stabilizer of x as the subgroup Gx := {g \u2208 G | g \u00b7 x = x} \u2286 G. Assuming that G acts on both spaces X and Y , we refer to a function f : X \u2192 Y as G-equivariant if it satisfies f(g \u00b7 x) = g \u00b7 f(x), and as G-invariant if it satisfies f(g \u00b7 x) = f(x). Steerable vector space. We revisit the concept of a steerable vector space following [4], where the group action is induced by a group representation. A vector space V over R is said to be G-steerable for G if a group representation \u03c1 : G \u2192 GL(V )1 of G is assigned to V . In other words, for any vector v \u2208 V , the transformation g \u00b7 v is given by the matrix multiplication g \u00b7 v = \u03c1(g)v. When G is SO(3) or O(3), we can decompose the representation into irreducible representations. Consequently, it is sufficient to investigate steerable vector spaces transformed by these irreducible\n1GL(V ) denotes the general linear group of the vector space V .\nrepresentations. It is known that the irreducible representations of SO(3) have dimensions 2l + 1 for l = {0} \u222a N, and they are defined by the Wigner-D matrices {Dl}. We refer to the vector space transformed by Dl of order l as a type-l SO(3)-steerable vector space, denoted as Vl.\nMoreover, notice that O(3) is the direct product of SO(3) and the inversion group I := {I,\u2212I}, implying that any representation of O(3) can be written as the product of a representation of SO(3) and a representation of I. The inversion group I has only two irreducible representations: the trivial representation \u03c1t(I) = \u03c1t(\u2212I) = 1 and the sign representation \u03c1s(I) = 1, \u03c1s(\u2212I) = \u22121. As a result, all the irreducible representation of O(3) can be expressed as {\u03c1(i)\u00b7Dl(g)|l \u2265 0, \u03c1 = \u03c1t or \u03c1s} by writing the element of O(3) as a product i \u00b7 g where i \u2208 I, g \u2208 SO(3). An alternative way to represent these irreducible representations involves the determinant, as follows:\nDlind(g) := det(g) l \u00b7Dl(det(g)g) and Dlaug(g) := det(g)l+1 \u00b7Dl(det(g)g),\nwhere g \u2208 O(3) and l \u2265 0. To avoid ambiguity, we use the terminology of ind and aug for clarification and ease of subsequent study. Lastly, let Vl,ind denote the O(3)-steerable vector space acted upon by Dlind and Vl,aug denote the O(3)-steerable vector space acted upon by D l aug. We may denote V0,ind simply as V0 since both correspond to trivial representations.\nGeometric graphs. Let (V, E) be an attributed graph comprising m nodes, where each node i \u2208 V has a feature embedding fi \u2208 Rn and a spatial embedding xi \u2208 R3. The input embeddings can be organized as two matrices F = [f1, . . . ,fm] \u2208 Rn\u00d7m and X = [x1, . . . ,xm] \u2208 R3\u00d7m. We can represent this as G = (V, E ,F ,X), referring to this attributed graph as a geometric graph. Consider two geometric graphs G1 = (V1, E1,F G1 ,XG1),G2 = (V2, E2,F G2 ,XG2) where the underlying graph structures and feature embeddings are isomorphic. In other words, there is an edge-preserving bijection b : V1 \u2192 V2 s.t. fG1i = f G2 b(i), where we do not assume the uniqueness of b. We say G1 and G2 are identical up to group action if there is a graph isomorphism b such that xG1i = g\u00b7x G2 b(i) for some g \u2208 G. Let N (k)i represent the k-hop neighborhood of node i, the set of nodes in V that are reachable from i through a path with k edges or fewer. Then we say G1 and G2 are k-hop identical if there is a graph isomorphism b such that for any node i \u2208 V1, there exists gi \u2208 G satisfying xG1j = gi \u00b7 x G2 b(j) for any j \u2208 N (k)i \u222a {i}. Otherwise, we say G1 and G2 are k-hop distinct if, for all isomorphisms b, there is a node i \u2208 V1 such that for any g \u2208 G, we have xG1j \u0338=g \u00b7 x G2 b(j) for some j \u2208 N (k) i \u222a {i}.\nk-hop geometric GNNs. We extend the framework of geometric GNNs in [20] to a k-hop setting. This framework can be regarded as an abstract of several existing invariant and equivariant GNNs, e.g. [34; 32; 4; 2]; see Appendix E for details. Consider a geometric graph G = (V, E ,F ,X) with G being SO(3) or O(3). Geometric GNNs propagate features from iteration t to t+ 1 as follows:\nf (t+1) i = UPD\n( f\n(t) i ,AGG({f (t) i ,f (t) j ,xij | j \u2208 N (k) i } ) ) , with f (0)i = fi (1)\nwhere xij := xi \u2212 xj , {{\u00b7} denotes a multiset, and UPD and AGG are learnable update and aggregate (G-equivariant) functions. Notably, the equivariance of UPD and AGG ensures that this message-passing mechanism remains equivariant, that is, we have\n\u03c1(t+1)(g)f (t+1) i = UPD\n( \u03c1(t)(g)f\n(t) i ,AGG({ \u03c1 (t)(g)f (t) i , \u03c1 (t)(g)f (t) j , g \u00b7 xij | j \u2208 N (k) i } )\n) . (2)\nwhere \u03c1(t+1)(g) and \u03c1(t)(g) are group representations acting on f (t+1)i and f (t) i , resp., and g \u00b7 xij is the regular representation. The features f (t)i in geometric GNNs consist of steerable features up to type-L, where L = 0(> 0) corresponds to G-invariant/G-equivariant GNNs."
        },
        {
            "heading": "3 MAIN THEORY",
            "text": "Steerable features and invariant features. From now on, let G be a group acting on R3\u00d7m. We define two key concepts: steerable features refer to the steerable vectors that are generated equivariantly from the input spatial embedding X , meaning that they can be expressed as f(X) for some G-equivariant function f mapping from R3\u00d7m to a steerable vector space. Similarly, invariant features are characterized as the steerable vectors f(X) produced by some G-invariant function f . Notice that these two concepts cover all the notions of existing terms, such as steerable feature fields [42; 41] and steerable feature vectors [37]; see Section 4 for more discussion. Steerable features and their corresponding invariant features. By the axiom of choice [17], we can choose a \"representative\" for each orbit in R3\u00d7m/G. Specifically, there is a (set-theoretical) function c : R3\u00d7m/G \u2192 R3\u00d7m that maps each orbit to an element within that orbit.\nNote that for any X,X \u2032 \u2208 R3\u00d7m, they belong to the same G-orbit if and only if X = g \u00b7X \u2032 for some g \u2208 G. Consequently, for any given X , the set {g \u2208 G | g \u00b7 c(G \u00b7X) = X} is not empty since c(G \u00b7X) and X lie in the same orbit G \u00b7X . Once again, applying the axiom of choice, we can select one element from this set, denoting it as gX . Then we can represent any X \u2208 R3\u00d7m using the pair X = ( c(G \u00b7X), gX ) , where gX \u00b7 c(G \u00b7X) = X . Next, we introduce a key lemma that serves as the cornerstone of our theoretical framework; a similar finding can be found in [44]. Lemma 1. Let V be a d-dimensional G-steerable vector space with the assigned group representation \u03c1 : G \u2192 GL(V ). If f : R3\u00d7m \u2192 V is G-equivariant, then there exists a unique G-invariant function \u03bb : R3\u00d7m \u2192 V \u2295d0 s.t. f(X) = \u03c1(gX)\u03bb(X), where V0 denotes the 1D trivial representation of G2. In particular, the following map is well-defined {f : R3\u00d7m \u2192 V | f : G-equivariant} \u2192 {\u03bb : R3\u00d7m \u2192 V \u2295d0 | \u03bb : G-invariant}. (3) Remark 1. While we consider the group action on R3\u00d7m, it is important to note that the same result applies to group actions on any space. One can interpret that gX absorbs the group action on X and constrains \u03bb(X) to be invariant. More precisely, we see that \u03c1(h)\u03c1(gX)\u03bb(X) = \u03c1(h)f(X) = f(h \u00b7X) = \u03c1(gh\u00b7X)\u03bb(X) for any h \u2208 G. However, it is not necessary that h \u00b7 gX = gh\u00b7X . We observe from Lemma 1 that any steerable feature with dimension d, denoted as f(X), can be substituted with a group element gX and d-dimensional invariant features \u03bb(X). However, since the selection of gX remains unaffected by the function f , what we truly observe is that the steerable feature f(X) corresponds to a unique d-dimensional invariant feature \u03bb(X). The correspondence of f(X) to \u03bb(X) shares similarities with the notion of scalarization in [9] and [10]. A detailed discussion of their distinctions is provided in Appendix A."
        },
        {
            "heading": "3.1 MESSAGE-PASSING MECHANISMS OF GEOMETRIC GNNS",
            "text": "It has been demonstrated that 1-hop invariant GNNs underperform equivariant GNNs [20]. However, the underlying mechanisms of message passing that distinguish these two approaches remain unclear. In this section, we leverage our framework to shed some light on this issue. In particular, we will show that k-hop invariant GNNs do not possess an inherent capability to capture geometric information between local structures, whereas equivariant GNNs do.\nSteerable features propagate like invariant features. We first treat the steerable features f (t)i , in the propagation scheme defined in equation (1), as functions of X and denote them as f (t)i = f (t) i (X). By applying Lemma 1, we can express f (t)i (X) = \u03c1 (t)(g)\u03bb (t) i (X), where \u03bb (t) i is G-invariant, and for simplicity, we represent gX as g. Additionally, since we have X = g \u00b7 c(G \u00b7X) where c(G \u00b7X) is G-invariant, and g\u22121xij are the invariant features corresponding to xij := xi \u2212 xj due to the uniqueness described in Lemma 1. Consequently, we arrive at the following relationship:\nUPD ( f\n(t) i (X),AGG({f (t) i (X),f (t) j (X),xij | j \u2208 N (k) i } ) ) =UPD ( \u03c1(t)(g)\u03bb\n(t) i (X),AGG({ \u03c1 (t)(g)\u03bb (t) i (X), \u03c1 (t)(g)\u03bb (t) j (X), g \u00b7 (g \u22121xij) | j \u2208 N (k)i } ) )\n=\u03c1(t+1)(g)UPD ( \u03bb (t) i (X),AGG({\u03bb (t) i (X), \u03bb (t) j (X), g \u22121xij | j \u2208 N (k)i } ) ) .\nSince f (t+1)i (X) = \u03c1 (t+1)(g)\u03bb (t+1) i (X), the uniqueness of corresponding invariant features implies\n\u03bb (t+1) i (X) = UPD ( \u03bb (t) i (X),AGG({\u03bb (t) i (X), \u03bb (t) j (X), g \u22121xij | j \u2208 N (k)i } ) ) . (4)\nThis reveals that the propagation of steerable features can be effectively understood as the propagation of their corresponding invariant features. Therefore, we can analyze the message-passing mechanism by examining how the corresponding invariant features are aggregated and updated.\nMessage aggregated from multi-hop neighborhoods. To investigate the aggregation of local features from multi-hop neighborhoods, we explicitly specify the input spatial embeddings for each steerable feature f (t)i : namely, for any iteration t and node i, we write f (t) i = f (t) i (X (t) i ) where X (t) i := [xj ]j\u2208N (tk)i \u222a{i} 3 represents the spatial embedding, consisting of all coordinates of node i and its tk-hop neighbors N (tk)i . In particular, the tk-hop neighbor N (tk) i includes all the nodes that can propagate information to node i through k-hop aggregation t times. Without loss of generality, we may assume the group representations are all the same for any iteration, i.e. \u03c1 = \u03c1(t) for any t. Next,\n2For G = SO(3), it corresponds to the type-0 steerable vector space we defined in Section 2. For simplicity, we employ the same notation here.\n3We do not make any assumptions about the order of the coordinates or indices here.\nwe utilize Lemma 1 to examine how corresponding invariant features propagate through different multi-hop neighborhoods. Specifically, we express these steerable features as \u03c1(g(t)i )\u03bb (t) i , where \u03bb (t) i = \u03bb (t) i (X (t) i ) corresponds to the local invariant features, and g (t) i denotes g (t)\nX (t) i\nfor simplicity.\nOur goal is to investigate how invariant features \u03bb(t)i at iteration t are aggregated and updated into invariant features \u03bb(t+1)i at iteration t+ 1. In particular, we have\nf (t+1) i =UPD\n( f\n(t) i ,AGG({f (t) i ,f (t) j ,xij | j \u2208 N (k) i } ) ) =UPD ( \u03c1(g\n(t) i )\u03bb (t) i ,AGG({ \u03c1(g (t) i )\u03bb (t) i , \u03c1(g (t) j )\u03bb (t) j , g (1) i \u00b7 ((g (1) i ) \u22121xij) | j \u2208 N (k)i } ) ) , (5)\nNext, by leveraging the fact that f (t+1)i = \u03c1(g (t+1) i )\u03bb (t+1) i and the uniqueness of \u03bb (t+1) i as established in Lemma 1, we deduce the propagation of the corresponding invariant features.\n\u03bb (t+1) i =UPD\n( \u03c1((g\n(t+1) i ) \u22121g (t) i )\u03bb (t) i ,AGG\n({{ \u03c1((g\n(t+1) i ) \u22121g (t) i )\u03bb (t) i ,\n\u03c1((g (t+1) i ) \u22121g (t) j )\u03bb (t) j , ((g (t+1) i ) \u22121g (1) i )(g (1) i ) \u22121xij | j \u2208 N (k)i }})) . (6)\nWe assert that the collection of elements { \u03c1 ( (g\n(t+1) i ) \u22121g (t) j ) | j \u2208 N (k)i \u222a {i} } plays a crucial role\nin enabling geometric GNNs to capture geometric information between the local structures N (tk)j . To clarify this, we consider k-hop invariant GNNs where \u03c1 is trivial, resulting in the set of elements{ \u03c1 ( (g\n(t+1) i ) \u22121g (t) j ) | j \u2208 N (k)i \u222a {i} } being inevitably overlooked. Then we have the following:\nTheorem 1. If G1 and G2 are two k-hop identical graphs, then any iteration of k-hop invariant GNNs will get the same output from these two graphs. That is, there is a graph isomorphism b such that \u03bb (t+1) i = \u03bb (t+1) b(i) for any i, even though G1 and G2 may not be identical up to group action.\nRemark 2. We also extend the IGWL test in [20] to a k-hop setting and show that (1-hop) GWL is still more powerful than k-hop IGWL; see Appendix D for details.\nTheorem 1 implies that k-hop invariant GNNs may struggle to capture the changing geometry between k-hop local structures. To illustrate this point, let\u2019s consider the examples of k-chains discussed in [20]. For any k, let\u2019s examine a pair of graphs, G1 and G2, each consisting of 2k + 2 nodes. In these graphs, there are 2k nodes arranged in a line, with differentiation based on the orientation of the two endpoints, as demonstrated in Fig. 1. By assigning the same attributes and customizing the\nspatial embeddings, we can make these graphs k-hop identical but (k + 1)-hop distinct. Specifically, the geometry between N (k)i1 and N (k) j1 and the geometry between N (k)i2 and N (k) j2 differ, as the unions N (k)i1 \u222a N (k) j1 = G1 and N (k)i2 \u222a N (k) j2\n= G2 are not identical. However, since G1 and G2 are k-hop identical, we expect that k-hop invariant GNNs will likely struggle to distinguish between the distinct geometries present in these scenarios. We further analyze this example empirically in Section 5.\nLet V and W be steerable vector spaces with the assigned faithful group representations \u03c1V and \u03c1W , i.e., the group homomorphisms \u03c1V : G \u2192 GL(V ) and \u03c1W : G \u2192 GL(W ) are injective. The injectivity implies \u03c1V ((g (t+1) i ) \u22121g (t) j ) and \u03c1W ((g (t+1) i ) \u22121g (t) j ) come from the group element (g (t+1) i ) \u22121g (t) j . They capture the same geometric information defined by (g (t+1) i ) \u22121g (t) j . With assumptions on the injectivity of UPD and AGG, we can show that equivariant GNNs that learn steerable features on faithful representations can distinguish any two k-hop distinct geometric graphs.\nTheorem 2. Consider 1-hop equivariant GNNs learning features on steerable vector space V where the aggregate function AGG learns features on steerable vector space W . Suppose V and W are faithful representations, and UPD and AGG satisfy certain assumptions on the injectivity outlined in Proposition 8 in the appendix. Then with k iterations, these equivariant GNNs learn different multisets of node features {{f (k)i }} on two k-hop distinct geometric graphs. Remark 3. It can be verified that D1 and D1ind, which correspond to type-1 steerable vector spaces V1 for SO(3) and V1,ind for O(3), are faithful representations. As a result, we conclude that equivariant GNNs defined in equation (1), that propagating steerable features up to type L > 0, all exhibit the\nsame capability to capture geometric information between two local neighborhoods. For additional discussion on the importance of faithfulness, refer to Appendix B.\nTo mitigate the limited ability of invariant GNNs mentioned above, several approaches have emerged to encode geometric information between local structures into edge attributes. For instance, ComENet introduces the concept of completeness of edge attributes, a property associated with invariant edge features. Completeness enables these edge features to uniquely determine the spatial embedding X up to isometries. ComENet achieves this completeness by encoding spherical coordinates (in triplets) and dihedral angles (in quadruplets) as edge features, which involves computation within 2-hop neighborhoods. The former specifies the locations of nodes within 1-hop neighborhoods, and the latter captures the angle between two 1-hop neighborhoods that share two nodes, thus addressing the issue of missing geometric information between these neighborhoods. Consequently, while ComENet employs 1-hop aggregation schemes, these edge attributes effectively contain all the necessary geometric information within and between 1-hop neighborhoods for molecular tasks [40].\nHowever, it\u2019s important to note that dihedral angles may not always be well-defined in quadruplets4, particularly when facing a chain structure within the graph, such as cis-trans stereoisomers or Fig. 1. Additionally, the complete edge attributes that ComENet utilizes are extracted within 2-hop neighborhoods, while Theorem 1 suggests that k-hop invariant GNNs may still struggle to capture the true global geometry effectively. This result for ComENet is empirically validated in Section 5.\nSome remarks. We have pointed out that k-hop invariant GNNs inevitably ignore the information that captures the geometry between k-hop neighborhoods. Namely, they are limited to the information within k-hop neighborhoods. While encoding complete edge attributes emerges as a potential remedy, it remains an open question whether constructing complete edge attributes can be achieved for other tasks beyond molecular graphs, especially when dihedral angles cannot be well-defined in quadruplets. Theorem 1 further suggests that invariant features confined to specific k-hop neighborhoods may not be sufficient to capture the accurate global geometry and global invariant features of geometric graphs, thereby emphasizing the need for encoding global features as a potential solution to address this limitation. We leave this intriguing avenue for future research."
        },
        {
            "heading": "3.2 COMPARING EQUIVARIANT GNNS USING DIFFERENT TYPES OF STEERABLE FEATURES",
            "text": "Remark 3 suggests that when equivariant GNNs learn steerable features up to type L, they exhibit the same capability to capture geometric information between two local neighborhoods. To understand if the performance of equivariant GNNs can be enhanced by increasing L, we analyze the informationcarrying ability of steerable features by investigating their corresponding invariant features.\nLemma 1 states that any type-l steerable feature f(X) \u2208 Vl corresponds to a (2l + 1)-dimensional invariant feature \u03bb(X) due to the decomposition f(X) = Dl(gX)\u03bb(X). Now, we raise the question: Does any (2l + 1)-dimensional invariant feature \u03bb(X) correspond to a type-l steerable feature f(X)? Note that this question is essentially asking whether the space of all type-l steerable features f(X) has a dimension of 2l + 1 since Dl(gX) is invertible. While Lemma 1 is not affected by rank(X), the rank of the spatial embedding X , the answer to this question is contingent upon the rank. This is because equivariant functions must obey \u201cCurie\u2019s principle\" [36]. For a brief intuition, refer to Fig. 4 in Appendix G. Based on this principle, we present the following results, answering the aforementioned question. Specifically, we first examine G-equivariant functions without focusing on a spatial embedding and then shift our attention to a spatial embedding to obtain a precise answer. Theorem 3. Let Xr denote the set {X \u2208 R3\u00d7m | rank(X) = r}. Then we have a one-to-one correspondence between O(3)-equivariant functions and O(3)-invariant functions:\n{f : X3 \u2192 Vl,ind | f : O(3)-equivariant} \u21c4 {\u03bb : X3 \u2192 V \u22952l+10 | \u03bb : O(3)-invariant}, (7) {f : X2 \u2192 Vl,ind | f : O(3)-equivariant} \u21c4 {\u03bb : X2 \u2192 V \u2295l+10 | \u03bb : O(3)-invariant}, {f : X1 \u2192 Vl,ind | f : O(3)-equivariant} \u21c4 {\u03bb : X1 \u2192 V \u229510 | \u03bb : O(3)-invariant}, {f : X0 \u2192 Vl,ind | f : O(3)-equivariant} = {f : X0 = {0} \u2192 {0}}. {f : X3 \u2192 Vl,aug | f : O(3)-equivariant} \u21c4 {\u03bb : X3 \u2192 V \u22952l+10 | \u03bb : O(3)-invariant}, (8) {f : X2 \u2192 Vl,aug | f : O(3)-equivariant} \u21c4 {\u03bb : X2 \u2192 V \u2295l0 | \u03bb : O(3)-invariant}, {f : X1 \u2192 Vl,aug | f : O(3)-equivariant} = {f : X1 \u2192 {0}}, {f : X0 \u2192 Vl,aug | f : O(3)-equivariant} = {f : X0 = {0} \u2192 {0}}.\n4When the nodes are collinear, there are infinitely many planes containing them.\nRemark 4. Note that any G-equivariant function f : R3\u00d7m \u2192 V can be expressed as a summation: f = \u22113 r=0 f \u00b71Xr (X), where 1Xr represents the indicator function, and f \u00b71Xr (X) can be considered as a G-equivariant function that maps from Xr to V . Consequently, the results presented above suffice to describe any O(3)-equivariant function that maps to a steerable vector space.\nCorollary 1. Let X \u2208 R3\u00d7m be a spatial embedding. We have the following relation between O(3)-steerable features and invariant features:\n1. If rank(X) = 3, there is a bijection between steerable features in Vl,ind and (2l + 1)- dimensional invariant features, as well as a bijection between steerable features in Vl,aug and (2l + 1)-dimensional invariant features.\n2. If rank(X) = 2, there is a bijection between steerable features in Vl,ind and (l + 1)- dimensional invariant features and a bijection between steerable features in Vl,aug and l-dimensional invariant features.\n3. If rank(X) = 1, there is a bijection between steerable features in Vl,ind and 1-dimensional invariant features, while there is no non-trivial steerable feature lying in Vl,aug. 4. There exist only trivial steerable feature 0 and trivial invariant feature 0 if rank(X) = 0.\nSimilar results for SO(3) are presented in Theorem 5 and Corollary 4 in Appendix C. Due to the reducibility of G-steerable vector spaces, we can decompose any G-steerable vector space into a direct sum of steerable vector spaces of different types. Then, we have the following two corollaries:\nCorollary 2. Let X3 denote the set {X \u2208 R3\u00d7m | rank(X) = 3}. Then for any G-steerable vector space of dimension d, denoted as V , we have a one-to-one correspondence:\n{f : X3 \u2192 V | f : G-equivariant} \u21c4 {\u03bb : X3 \u2192 V \u2295d0 | \u03bb : G-invariant}, (9) where the map between these two spaces is induced by the map defined in equation (3).\nCorollary 3. Let V and W be two G-steerable vector spaces of dimension d. Then for any Gequivariant function fV : X3 \u2192 V , there is a G-equivariant function fW : X3 \u2192 W such that for any X \u2208 X3, we have fV (X) = \u03c1V (gX)\u03bb(X) and fW (X) = \u03c1W (gX)\u03bb(X) for the same G-invariant function \u03bb where \u03c1V , \u03c1W are the group representation on V and W , resp.\nConsider learning steerable features in V and W , resp. Corollary 3 suggests that if dimV = dimW , regardless of their irreducible decomposition, any (learnable) G-equivariant function fV : X3 \u2192 V can be replaced by a G-equivariant function fW : X3 \u2192 W where they learn the same corresponding invariant features. That is, the invariant features carried by steerable features is primarily characterized by the feature dimension \u2013 independent of the highest type utilized.\nSpecifically, we have the following equivalence among geometric GNNs, detailed in Appendix C:\nTheorem 4. Consider two geometric GNNs learning features on steerable vector spaces V and W of the same dimension, resp. Denote their update and aggregation functions at iteration t as UPD\n(t) V , UPD (t) W and AGG (t) V , AGG (t) W . Then for any collection {(UPD (t) V ,AGG (t) V )}t, there exists\na collection {(UPD(t)W ,AGG (t) W )}t such that for any fully connected graph, they learn the same corresponding invariant features \u03bb(t)i for any iteration t \u2265 0 on each node i.\nTheorem 4 holds for any representation, especially non-faithful representations, but relies on the assumption of fully connected graphs. This result establishes the equivalence of geometric GNNs on fully connected graphs; this is similar to the equivalence of IGWL and GWL tests on fully connected graphs [20] but without strong assumptions on the injectivity of update functions and aggregate functions. Additionally, when dealing with non-fully connected graphs, our earlier exploration in Section 3.1 highlighted that learning features on faithful representations versus non-faithful representations results in different expressive powers. This discrepancy arises primarily because each node in a non-fully connected graph can only capture the global geometry through message passing.\nSome remarks. Our proof of Theorem 4 relies on a precise understanding of how these models capture global geometry through message passing, making it challenging to ascertain its validity for non-fully connected graphs under the assumption of faithfulness of representations. While we believe it may be more feasible to demonstrate this by considering specific architectures and gaining a better understanding of how they obtain global geometry from local information, we consider exploring this aspect further as a future work. Nevertheless, Theorem 2 and Remark 3 suggest that equivariant\nGNNs learning steerable features up to type-L exhibit the same capacity to capture global geometry. We propose that when the feature dimension remains constant, the performance of equivariant GNNs employing steerable features up to type-L may not increase as L grows. However, we cannot assert that using L = 1 is sufficient. The concept of expressiveness includes two key aspects: the capacity of features to carry information and the ability of a model to extract it \u2013 the latter is commonly referred to as universality [11]. We intended to focus on the former, as the latter is subject to the architecture of the given model, while the former is not. Moreover, due to the lack of regularity of functions appearing in Lemma 1, we decided to defer discussions on universality to future works. More detailed discussions and potential methods are available in Appendix B. 4 ADDITIONAL RELATED WORKS L c # Param. Feat. Dim.\n2 256 39M 2304\nNumerical comparisons in steerable feature types. In a recent experiment in [20], a comparison was made regarding using different types of steerable features. However, this experiment was not specifically designed as an invariant classification task. Therefore, the conclusion that higher-type steerable features are superior may not directly\napply to this context. Additionally, in experiments conducted in [3; 23; 2; 28], it was indeed observed that higher-type steerable features improved performance. Nevertheless, it should be noted that these experiments do not maintain fixed dimensions for hidden features, making direct comparisons challenging. Table 1 illustrates the difference in the number of parameters and the steerable feature dimension for the eSCN model [28] with varying feature type L and channels c. The ablation study in [28] compares rows 1&3, while rows 2&3 provide a more suitable comparison."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we empirically verify our theory on several benchmark tasks. First, we verify the limitations of invariant models using the synthetic k-chain dataset. Second, we perform an ablation study over the steerable feature dimension when training steerable models on the large-scale OC20 IS2RE and S2EF datasets [6]. The invariant models considered are SchNet [34], DimeNet++ [14], SphereNet [25], and ComENet [40]; the equivariant models include EGNN [32], ClofNet [9], and GVP [19]; the steerable models are eSCN [28], EquiformerV2 [24] and MACE [2]. We provide details on each model, training procedure and hyperparameters in Appendix E, F.3, and F.4 resp. k-Chain. In the k-chain task, motivated by [20] and illustrated in Figure 1, we aim to differentiate chains using the orientation of the terminal node. Each chain is made of k + 2 nodes, all possessing constant features. The connecting edges are undirected with uniform distance rij = 5. Chains are assigned binary labels based on the orientation of their final node. The dataset consists of pairs of chains, one of each label, which undergoes the same random rotation/reflection and translation. 50 transformed graph pairs are split into 50%/30%/20% train/validation/test splits with balanced labels.\nLayers 1 2 3 1 2 3 4 k-hop chain k = 2 k = 2 k = 2 k = 3 k = 3 k = 3 k = 3\nL = 0 SchNet 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.1 \u00b1 0.2 50.0 \u00b1 0.0 50.0 \u00b1 0.0 DimeNet++ 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 SphereNet 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 50.0 \u00b1 0.0 ComENet 55.0 \u00b1 4.5 59.0 \u00b1 11.6 53.0 \u00b1 6.4 54.0 \u00b1 6.2 50.0 \u00b1 0.0 46.5 \u00b1 5.0 51.0 \u00b1 2.0 EquiformerV2 71.0 \u00b1 3.0 76.0 \u00b1 8.0 83.0 \u00b1 6.4 43.0 \u00b1 9.0 67.0 \u00b1 4.6 67.9 \u00b1 9.0 61.0 \u00b1 5.4 L = 1\ntype-L features. Cell shading is based on two standard deviations above or below the expected value. Unit:%. Table 2 reports the 10-fold cross-validation mean test accuracy and standard deviation for varying chain length k and model depth. The values are shaded based on the expected value from Section 3. In general, we observe that models perform as expected. The outliers can be categorized as equivariant models EGNN and ClofNet and quasi-equivariant models eSCN and EquiformerV2. The consistent underperformance of the equivariant models can be attributed to over-squashing as discussed in [20].\nThe abnormal performance of the quasi-equivariant models eSCN and EquiformerV2 can be attributed to the error in their equivariance. More discussions on models with outlying performance are provided in Appendix F.1. Table 2 shows that invariant features L = 0 are insufficient to distinguish the geometry regardless of the model depth. Also, architectures with L = 1 or 2 and finite k-hops can learn with sufficient depth. We show further comparisons up to k = 4 in Appendix F.1.\nIS2RE. The OC20 IS2RE dataset [6] is a large-scale molecular property prediction task that uses a molecule\u2019s initial structure to predict its adsorption energy. In this task, we perform an ablation study on the order of the steerable features L and the size of the steerable feature dimension c. In particular, we select a value c to roughly fix the steerable feature dimension when varying L.\nWe list the ablation study in Table 3, showing that when the steerable feature dimension is fixed, lower order type-L steerable features may provide improved results. These results are further validated in Figure 2, which illustrates the validation curves over 4-fold cross-validation with plotted mean and shaded standard deviation. We include additional results for EquiformerV2 in Appendix F.2.\nS2EF. The OC20 S2EF dataset [6] uses the molecular structure to predict molecular adsorption energies and per-atom forces. Table 1 lists the size and the steerable feature dimension for each model. Table 4 reports the results of our training procedure against the reported results from [28]. We observe that with a fixed feature dimension and fewer training\nepochs, the eSCN[L = 2, c = 824] consistently outperforms its lower dimensional counterpart eSCN[L = 2, c = 256] and outperforms eSCN[L = 6, c = 256] on energy MAE and EFwT. These tasks do not rule out confounding factors like over-squashing [1], but strongly support our results."
        },
        {
            "heading": "6 CONCLUDING REMARKS",
            "text": "We analyze the advantages granted by steerable features in GNNs. Specifically, we highlight two key findings: (1) Propagating steerable features of type-L \u2265 1 equips geometric GNNs with inherent ability to automatically capture the geometry between local structures and obtain global invariant features from local ones. However, relying solely on propagating invariant features confined to specific k-hop neighborhoods is insufficient for enabling geometric GNNs to capture global information of all geometric graphs precisely. (2) When keeping the feature dimension constant, increasing using steerable features up to type-L cannot essentially improve the performance of equivariant GNNs. In sum, our findings highlight the necessity of incorporating global features, extending beyond k-hop neighborhoods for a fixed k, to achieve the same expressiveness in invariant GNNs as in equivariant GNNs. Additionally, the traditional trade-off between performance and computational cost of using steerable features in equivariant GNNs should be reevaluated. Specifically, when maintaining a constant feature dimension, achieving better performance may not be guaranteed by using higher-type steerable features in equivariant GNNs, and this choice may come with a computational overhead."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "This material is based on research sponsored by NSF grants DMS-1952339, DMS-1952644, DMS2151235, DMS-2152762, DMS-2152717, DMS-2219956, DMS-2219904, and DMS-2208361 and DOE grant DE-SC0023490.\nETHICS STATEMENT\nOur paper focuses on developing new theoretical understandings of the benefits of steerable features of different types in 3D equivariant graph neural networks. The paper is mainly theoretical and we do not see any potential ethical issues in our research.\nREPRODUCIBILITY STATEMENT\nWe are dedicated to upholding the principles of reproducible research. In pursuit of this commitment, we have taken several measures. For the theoretical proofs, we have included comprehensive derivations to enhance accessibility for a wide range of readers. Additionally, we have submitted the code, along with detailed documentation, to facilitate the reproduction of the numerical results presented in our paper."
        },
        {
            "heading": "A ADDITIONAL RELATED WORKS",
            "text": "Characterization of equivariant functions. The characterization of some equivariant functions has been explored in [39]. Specifically, Lemma 1 is analogous to Proposition 4 in [39], as it can be interpreted as expressing f(X) as a linear combination of the column vectors in \u03c1(gX) with \u03bb(X) as the coefficients. However, a significant distinction lies in the fact that the number of vectors involved in our linear combination is the same as the dimension of f(X), not the number of column vectors in X . Additionally, the vectors in our linear combination truly form a basis, ensuring the uniqueness of the coefficients \u03bb(X). Meanwhile, the vectors in [39]\u2019s linear combination do not necessarily form a basis and therefore cannot guarantee the uniqueness of the coefficients. Furthermore, our results cover cases of higher-type steerable features (l > 1), whereas [39] only studied l = 0 and 1.\nEquivariant moving frames and scalarization with local frames. The existing concepts, such as equivariant moving frames [27; 29] and scalarization with local frames [9; 10], exhibit similarities with Lemma 1. However, we emphasize the distinctions as follows: While equivariant moving frames in [27] and our gX both assign X to a single element in G, the distinction lies in the fact that gX is well-defined even in cases where the action of G is not free, making it not necessarily equivariant. The scalarization proposed in [9; 10] shares similarities with how we define \u03bb(X) := \u03c1(gX)\u22121 \u00b7 f(X). Nevertheless, it differs because the local frames used for scalarization are generated from two coordinates rather than all the coordinates in X . In contrast, our gX requires consideration of the entire spatial embedding X .\nAlignment. Lemma 1 indicates that steerable features correspond to certain invariant features. However, implementing this in practice may pose challenges. The selection of the group element gX and the representative c(G \u00b7X) is closely related to the notion of alignment in [46; 44]. In [46], the Kabsch alignment algorithm is adopted to optimally find c(G \u00b7X), while [44] utilizes an encoder to learn it. Nevertheless, it is worth mentioning that \u03bb(X) := f(c(G \u00b7X)) might not be well-behaved. The topological properties of the Lie group G = SO(3) or O(3) prevent us from constructing a continuous function X 7\u2192 gX , resulting in poor regularity of X 7\u2192 gX and \u03bb(X). Various notions of steerable features. Several related concepts surround steerable features in the literature, and it\u2019s beneficial to briefly recap them. The notion of a \"steerable vector space\" [4] refers to a vector space transformed by a group G through a group representation of G. \"Steerable feature fields\" [42; 41] pertain to assigning each point a steerable vector. On the other hand, \"steerable feature vectors\" [37] refer to the actual outputs of these steerable feature fields. In the context of this paper, we use the term \"steerable feature\" to specifically refer to the steerable vectors generated by an equivariant map of the spatial embedding. Our notion aligns closely with the steerable feature fields concept but emphasizes including all relevant spatial information as input. This distinction helps clarify the focus and scope of our work in geometric graph neural networks."
        },
        {
            "heading": "B ADDITIONAL REMARKS AND DISCUSSION",
            "text": "Significance of faithful representations in learning steerable features. It\u2019s worth noting that [20] suggests the extensibility of their setup to higher-order tensors. However, our finding indicates that similar results for this extension may not hold if the geometric objects lack faithfulness. One can see that faithfulness is necessary to guarantee the injectivity in the proof of Proposition 8 and Theorem 2.\nTo illustrate this issue, we can straightforwardly extend the argument from Theorem 1. Let\u2019s consider a 1-hop equivariant GNN learning features on non-faithful representations, such as D0aug or D 1 aug. Our aim is to demonstrate the existence of two 2-hop distinct geometric graphs for which the considered 1-hop equivariant GNN produces the same feature representation for any iteration.\nWhile our discussion here is somewhat informal, the details we provided can be used to establish a solid proof easily. Notice that the non-faithfulness of representations implies that features may remain unchanged even when the input undergoes a transformation by some group element g. This aligns with the challenge of using invariant features, which are expected to remain unchanged under any transformation.\nIn fact, it is sufficient to construct two 2-hop distinct geometric graphs, where their 1-hop subgraphs are identical up to certain group actions (either g or the identity). This pair of graphs can be obtained by adjusting the pair illustrated in Fig. 1. The analogous argument in the proof of Theorem 1 will\nthen demonstrate the desired results. The underlying rationale for this approach lies in the fact that aggregating features on non-faithful representations, due to their nature, fail to capture the locally changed geometry induced by the group element g.\nWe did not explicitly delve into this in the main paper, as existing models typically focus on learning features involving faithful representations (l = 1). However, a crucial determining factor for achieving the maximal expressive power in equivariant GNNs is whether the steerable features lie on faithful representations.\nLimitations: universality and the lack of regularity. In the final part of Section 3.2, we refrain from asserting that using L = 1 is sufficient due to the unaddressed consideration of universality. Here, we delve into the challenges of studying the intricate relationship between universality and internal representation, specifically the representation involved in steerable features. Additionally, we present a potential method to address them.\nLet us formally articulate the questions pivotal to the investigation of the relationship between universality and internal representation.\nConsider a G-steerable vector space V , and let X3 denote the set {X \u2208 R3\u00d7m | rank(X) = 3}. For a specific architecture of geometric GNNs capable of learning features on any G-steerable vector space, we denote the family of all possible continuous G-equivariant functions from X3 to V that this architecture can parameterize (with a sufficient number of layers) as AV . The family consisting of all continuous G-equivariant functions from X3 to V is denoted as FV . The first question to address is as follows:\nQuestion 1. Can any continuous G-equivariant function in FV be uniformly approximated on compact sets by functions in AV ?\nNow, let W be another G-steerable vector space with the same dimension as V . Let fV : X3 \u2192 V be an arbitrary continuous G-equivariant function in FV . According to Corollary 3, there exists a G-equivariant function fW : X3 \u2192 W such that for any X \u2208 X3, we have fV (X) = \u03c1V (gX)\u03bb(X) and fW (X) = \u03c1W (gX)\u03bb(X) for the same G-invariant function \u03bb where \u03c1V , \u03c1W are the group representation on V and W , resp. However, fW is not necessarily continuous, i.e. fW \u0338\u2208 FW , implying that it may not be approximated by functions in AW . In particular, the topological properties of the Lie group G = SO(3) or O(3) prevent us from constructing a continuous function X 7\u2192 gX , resulting in poor regularity of X 7\u2192 gX and \u03bb(X). Consequently, the continuity of fW cannot be guaranteed.\nAs a result, we present the following question:\nQuestion 2. Let \u00b5 denote the Lebesgue measure on R3\u00d7m. For any \u03f5 > 0 and any compact set K \u2282 X3, does there exist a continuous G-equivariant function f \u2032W : X3 \u2192 W and a measurable subset E \u2282 K with \u00b5(K \\ E) < \u03f5 such that f \u2032W (X) = fW (X) for all X \u2208 E?\nSuppose we obtain affirmative answers to both Question 1 and 2. This implies that for any \u03f5 > 0 and any compact set K \u2282 X3, there exists a function f \u2032W \u2208 FW that can be uniformly approximated on compact sets by functions in AW . Additionally, its corresponding invariant function \u03bb\u2032(X) := \u03c1W (gX)\n\u22121f \u2032W (X) is identical to \u03bb(X), the corresponding invariant function of fV (X), on some measurable set E with \u00b5(K \\ E) < \u03f5. By restricting fV to a function in AV , we observe that the given architecture is able to learn a function in AW such that its corresponding invariant function is identical to the corresponding invariant function of fV on any compact set except an \u03f5-small measurable subset. In essence, the architecture can successfully learn approximated corresponding invariant functions on steerable vector spaces of the same dimension.\nWhile the response to Question 1 depends on the architectures, the answer to Question 2 is independent of them. In the following context, we propose a potential approach to address Question 2 by constructing X 7\u2192 gX with a better regularity5. More precisely, we aim to remedy the regularity issue of X 7\u2192 gX by extending the domain X3 to its covering space X\u03033. We proceed similarly to [29] to construct the covering space for the set of all generic spatial embeddings in\n5This will not impact the validity of Lemma 1 and Corollary 3.\nX3. Namely, given the spatial embedding X \u2208 X3, Principle Component Analysis (PCA) suggests that there are three vectors v1,v2,v3 \u2208 R3 uniquely determined up to sign corresponding to three distinct singular values \u03c31 > \u03c32 > \u03c33. As a result, one obtains the equivariant moving frame X 7\u2192 F (X) := {[\u03b11v1 \u03b12v2 \u03b13v3] \u2208 O(3)|\u03b1i \u2208 {1,\u22121}} as a set-valued function. In our situation, we take an alternative viewpoint. Notice that any generic X \u2208 X3, there\u2019s a small enough neighborhood UX of X such that any choice of sign (\u03b11, \u03b12, \u03b13) gives a homeomorphism X 7\u2192 (X, [\u03b11v1 \u03b12v2 \u03b13v3]) from UX \u2282 X3 to UX,\u03b11,\u03b12,\u03b13 \u2282 X3 \u00d7 O(3). One may check that the neighborhood compatibility allows us to patch UX,\u03b11,\u03b12,\u03b13s into a topological space X\u03033. Intuitively speaking, we obtain X\u03033 via locally embedding multiple copies of small neighborhoods into X3 \u00d7 O(3). Each neighborhood UX corresponds to 23 copies of themselves in X3 \u00d7O(3). It becomes clear that X\u03033 \u21aa\u2192 X3 \u00d7O(3) \u2192 O(3) defines a continuous equivariant function X\u0303 := (X, g) 7\u2192 gX\u0303 := g and so does X\u0303 7\u2192 \u03c1W ( gX\u0303 ) . Now, for a given fW with the\ncorresponding invariant function \u03bb, we lift \u03bb to an invariant function X\u0303 7\u2192 \u03bb\u0303(X\u0303) := \u03bb ( \u03c01(X\u0303) ) on\nthe covering space. Indeed, X\u0303 7\u2192 \u03c1W ( gX\u0303 ) \u03bb\u0303(X\u0303) produces a G-equivariant continuous function, but the function is defined on the covering space X\u03033. To produce a function on X3, one need to modify \u03bb on an \u03f5-small set and also \u03bb\u0303 so that the modified invariant function \u03bb\u0303\u2032 remains continuous and f \u2032W (X) := \u03c1W ( gX\u0303 ) \u03bb\u0303\u2032(X\u0303) actually defines a function on X3. In particular, f \u2032W and fW coincide on a given compact set except an \u03f5-small measurable subset.\nAlthough addressing Question 1 and 2 may suggest that learning on steerable vector spaces of the same dimension demonstrates consistent expressive power, we cannot deny the possibility that certain choices of representation could benefit the model, allowing it to approximate the target function with fewer layers. Additionally, our exploration in Section 3.1 indicates that learning on non-faithful representations might restrict the model\u2019s expressive power and, consequently, its universality. Nevertheless, we have chosen to reserve the exploration of Question 1 and 2 for future work.\nWhile our theory reveals this limitation, it is noteworthy that empirical results surprisingly align with the claim that learning lower types of steerable features can perform as effectively as learning higher types of steerable features."
        },
        {
            "heading": "C MISSING PROOF",
            "text": "Lemma 1. Let V be a d-dimensional G-steerable vector space with the assigned group representation \u03c1 : G \u2192 GL(V ). If f : R3\u00d7m \u2192 V is G-equivariant, then there exists a unique G-invariant function \u03bb : R3\u00d7m \u2192 V \u2295d0 s.t. f(X) = \u03c1(gX)\u03bb(X), where V0 denotes the 1D trivial representation of G6. In particular, the following map is well-defined\n{f : R3\u00d7m \u2192 V | f : G-equivariant} \u2192 {\u03bb : R3\u00d7m \u2192 V \u2295d0 | \u03bb : G-invariant}. (3)\nProof of Lemma 1. The equivariance follows that, f(X) = f ( gX \u00b7 c(G \u00b7X) ) = \u03c1(gX)f ( c(G \u00b7X) ) .\nIt suffices to show that f ( c(G \u00b7X) ) induces a G-invariant map \u03bb : R3\u00d7m \u2192 V \u2295d0 . First, sending X\nto f ( c(G \u00b7X) ) defines a map from R3\u00d7m to a d-dimensional vector space. Since G \u00b7 (g \u00b7X) = G \u00b7X\nfor any g \u2208 G, we see that c(G \u00b7X) is invariant, and hence f ( c(G \u00b7X) ) is invariant to the group\naction. Therefore, \u03bb(X) := f ( c(G \u00b7X) ) is G-invariant. One can check that the uniqueness of this\nfunction directly stems from the linear independence of the columns of \u03c1(gX). Alternatively, the invertibility of \u03c1(gX) can be utilized to conclude that \u03bb(X) = \u03c1(gX)\u22121f(X).\nFor the ease of reading, we restate Theorem 1 below Theorem 1. If G1 and G2 are two k-hop identical graphs, then any iteration of k-hop invariant GNNs will get the same output from these two graphs. That is, there is a graph isomorphism b such that \u03bb (t+1) i = \u03bb (t+1) b(i) for any i, even though G1 and G2 may not be identical up to group action.\n6For G = SO(3), it corresponds to the type-0 steerable vector space we defined in Section 2. For simplicity, we employ the same notation here.\nProof. For k-hop invariant GNNs, the propagation of the (corresponding) invariant features in equation (6)7 can be expressed as follows:\n\u03bb (t+1) i = UPD ( \u03bb (t) i ,AGG ({{ \u03bb (t) i , \u03bb (t) j , ((g (t+1) i ) \u22121g (1) i )(g (1) i ) \u22121xij | j \u2208 N (k)i }}))\n= UPD ( \u03bb (t) i ,AGG ({{ \u03bb (t) i , \u03bb (t) j ,xij | j \u2208 N (k) i }})) ,\n(10)\nwhere we use \u03c1(g) = I for any g \u2208 G and the invariance property of AGG. Let G1 = (V1, E1,F G1 ,XG1),G2 = (V2, E2,F G2 ,XG2) be two k-hop identical geometric graphs. By definition, there exists a graph isomorphism b such that for any node i \u2208 V1, we can find a group element gi \u2208 G satisfies fG1i = f G2 b(i),x G1 i = gi \u00b7 x G2 b(i) and f G1 j = f G2 b(j),x G1 j = gi \u00b7 xG2b(j) whenever j \u2208 N (k) i .\nThen, utilizing the invariance of AGG once more, we observe that:\n\u03bb (1) i = UPD ( fG1i ,AGG ({{ fG1i ,f G1 j ,x G1 ij | j \u2208 N (k) i }})) = UPD ( fG2b(i),AGG ({{ fG2b(i),f G2 b(j), gi \u00b7 x G2 b(i)b(j) | b(j) \u2208 N (k) b(i)\n}})) = UPD ( fG2b(i),AGG ({{ fG2b(i),f G2 b(j),x G2 b(i)b(j) | b(j) \u2208 N (k) b(i)\n}})) = \u03bb\n(1) b(i).\nHence, it follows that \u03bb(1)i = \u03bb (1) b(i) for any i. Following this step inductively, we can demonstrate that \u03bb (t+1) i = \u03bb (t+1) b(i) for any node i.\nFor the ease of reading, we restate Theorem 4 below: Theorem 4. Consider two geometric GNNs learning features on steerable vector spaces V and W of the same dimension, resp. Denote their update and aggregation functions at iteration t as UPD\n(t) V , UPD (t) W and AGG (t) V , AGG (t) W . Then for any collection {(UPD (t) V ,AGG (t) V )}t, there exists\na collection {(UPD(t)W ,AGG (t) W )}t such that for any fully connected graph, they learn the same corresponding invariant features \u03bb(t)i for any iteration t \u2265 0 on each node i.\nLet us first provide more details about what we are going to show. We consider the following two geometric GNNs:\nf (t+1) i,\u2217 = UPD\u2217 ( f (t) i,\u2217 ,AGG\u2217({{f (t) i,\u2217 ,f (t) j,\u2217 ,xij | j \u2208 Ni}}) ) (11)\nwhere \u2217 represents the steerable vector space V or W and f (t)i,\u2217 denotes the i-th node feature learn on \u2217 at iteration t. Let \u03c1V and \u03c1W denote the group representations on V and W , resp. Remark that the corresponding invariant features are given by:\n\u03bb (t) i,\u2217 = \u03c1\u2217(g) \u22121f (t) i,\u2217 (12)\nwhere g = gX is the group element associated with input spatial embedding X .\nWe aim to show that for any collection {(UPD(t)V ,AGG (t) V )}t of G-equivariant functions, there exists a collection {(UPD(t)W ,AGG (t) W )}t of G-equivariant functions such that \u03bb (t) i,V = \u03bb (t) i,W for any t \u2265 0 and i.\nProof. We proceed with this proof by induction. First, \u03bb(t)i,V = \u03bb (t) i,W holds for t = 0 since\n\u03bb (0) i,\u2217 = f (0) i,\u2217 = fi,\n7One can also directly use equation (5).\nwhere fi denotes the input node feature of node i. Suppose \u03bb (t) i,V = \u03bb (t) i,W holds for any node i at iteration t. At iteration t+ 1, we have\n\u03bb (t+1) i,V =\u03c1V (g) \u22121f (t) i,V\n=\u03c1V (g) \u22121 UPDV ( f (t) i,V ,AGGV ({{f (t) i,V ,f (t) j,V ,xij | j \u2208 Ni}}) ) =UPDV ( \u03c1V (g) \u22121f (t) i,V ,AGGV ({{\u03c1V (g) \u22121f (t) i,V , \u03c1V (g) \u22121f (t) j,V , g \u22121xij | j \u2208 Ni}}) )\n=UPDV ( \u03bb (t) i,V ,AGGV ({{\u03bb (t) i,V , \u03bb (t) j,V , g \u22121xij | j \u2208 Ni}}) ) .\nConsider the following construction of (UPD(t)W ,AGG (t) W ): UPDW ( f (t) i,W ,AGGW ({{f (t) i,W ,f (t) j,W ,xij | j \u2208 Ni}}) ) :=\n\u03c1W (g)UPDV ( \u03c1W (g) \u22121f (t) i,W ,AGGV ({{\u03c1W (g) \u22121f (t) i,W , \u03c1W (g) \u22121f (t) j,W , g \u22121xij | j \u2208 Ni}}) ) (13) More precisely, to guarantee G-equivariance, we define UPD(t)W and AGG (t) W to be: (UPD (t) W ,AGG (t) W ):\nAGGW ( {{f (t)i,W ,f (t) j,W ,xij | j \u2208 Ni}} ) := ( \u03c1W (g)AGGV ({{\u03c1W (g)\u22121f (t)i,W , \u03c1W (g) \u22121f (t) j,W , g \u22121xij | j \u2208 Ni}}), g )\nUPDW ( f (t) i,W , (mi, g) ) := \u03c1W (g)UPDV ( \u03c1W (g) \u22121f (t) i,W , \u03c1W (g) \u22121 \u00b7mi ) (14)\nwhere mi denotes the first component of AGG (t) W )\u2019s output. The G-equivariance follows from the sense in Remark 1.\nIt is well-defined since we can construct g = gX from xij by using the assumption of fully connected graphs. Indeed, since we have \u2211 k xk = 0 (from eliminating the effects of translations),\nthen \u2211 j \u0338=i xij = mxi \u2212 \u2211\nj \u0338=i xj = (m + 1)xi where m is the number of nodes. Therefore, 1\nm+1 \u2211 k \u0338=i xik = xi and ( 1 m+1 \u2211 k \u0338=i xik ) \u2212 xij = xj for any j \u0338= i. These coordinates then determine X and hence determine gX .\nThen using the inductive assumption \u03bb(t)i,V = \u03bb (t) i,W and \u03bb (t) i,\u2217 = \u03c1\u2217(g) \u22121f (t) i,\u2217 , we obtain\n\u03bb (t+1) i,W =\u03c1W (g) \u22121f (t) i,W\n=\u03c1W (g) \u22121 UPDW ( f (t) i,W ,AGGW ({{f (t) i,W ,f (t) j,W ,xij | j \u2208 Ni}}) ) =UPDV ( \u03c1W (g) \u22121f (t) i,W ,AGGV ({{\u03c1W (g) \u22121f (t) i,W , \u03c1W (g) \u22121f (t) j,W , g \u22121xij | j \u2208 Ni}}) )\n=UPDV ( \u03bb (t) i,W ,AGGV ({{\u03bb (t) i,W , \u03bb (t) j,W , g \u22121xij | j \u2208 Ni}}) )\n=UPDV ( \u03bb (t) i,V ,AGGV ({{\u03bb (t) i,V , \u03bb (t) j,V , g \u22121xij | j \u2208 Ni}}) )\n=\u03bb (t+1) i,V .\nThis shows the construction satisfies the desired result.\nProofs and Additional Details for Section 3.2. All the theoretical results in Section 3.2 stem from the following theorem: Theorem 3. Let Xr denote the set {X \u2208 R3\u00d7m | rank(X) = r}. Then we have a one-to-one correspondence between O(3)-equivariant functions and O(3)-invariant functions:\n{f : X3 \u2192 Vl,ind | f : O(3)-equivariant} \u21c4 {\u03bb : X3 \u2192 V \u22952l+10 | \u03bb : O(3)-invariant}, (7) {f : X2 \u2192 Vl,ind | f : O(3)-equivariant} \u21c4 {\u03bb : X2 \u2192 V \u2295l+10 | \u03bb : O(3)-invariant}, {f : X1 \u2192 Vl,ind | f : O(3)-equivariant} \u21c4 {\u03bb : X1 \u2192 V \u229510 | \u03bb : O(3)-invariant}, {f : X0 \u2192 Vl,ind | f : O(3)-equivariant} = {f : X0 = {0} \u2192 {0}}.\n{f : X3 \u2192 Vl,aug | f : O(3)-equivariant} \u21c4 {\u03bb : X3 \u2192 V \u22952l+10 | \u03bb : O(3)-invariant}, (8) {f : X2 \u2192 Vl,aug | f : O(3)-equivariant} \u21c4 {\u03bb : X2 \u2192 V \u2295l0 | \u03bb : O(3)-invariant}, {f : X1 \u2192 Vl,aug | f : O(3)-equivariant} = {f : X1 \u2192 {0}}, {f : X0 \u2192 Vl,aug | f : O(3)-equivariant} = {f : X0 = {0} \u2192 {0}}.\nTheorem 5. Similarly, we have a one-to-one correspondence for SO(3)-equivariance and SO(3)-invariance:\n{f : X3 \u2192 Vl | f : SO(3)-equivariant} \u21c4 {\u03bb : X3 \u2192 V \u22952l+10 | \u03bb : SO(3)-invariant}, (15) {f : X2 \u2192 Vl | f : SO(3)-equivariant} \u21c4 {\u03bb : X2 \u2192 V \u22952l+10 | \u03bb : SO(3)-invariant}, {f : X1 \u2192 Vl | f : SO(3)-equivariant} \u21c4 {\u03bb : X1 \u2192 V \u229510 | \u03bb : SO(3)-invariant}, {f : X0 \u2192 Vl | f : SO(3)-equivariant} = {f : X0 = {0} \u2192 {0}}.\nConsider G as either O(3) or SO(3). Given the reducibility of G-steerable vector spaces, we can decompose any G-steerable vector space V into a direct sum of steerable vector spaces of different types. Then equation (7), equation (8) in Theorem 3, and equation (15) in Theorem 5 imply the following result: Corollary 2. Let X3 denote the set {X \u2208 R3\u00d7m | rank(X) = 3}. Then for any G-steerable vector space of dimension d, denoted as V , we have a one-to-one correspondence: {f : X3 \u2192 V | f : G-equivariant} \u21c4 {\u03bb : X3 \u2192 V \u2295d0 | \u03bb : G-invariant}, (9) where the map between these two spaces is induced by the map defined in equation (3). Corollary 3. Let V and W be two G-steerable vector spaces of dimension d. Then for any Gequivariant function fV : X3 \u2192 V , there is a G-equivariant function fW : X3 \u2192 W such that for any X \u2208 X3, we have fV (X) = \u03c1V (gX)\u03bb(X) and fW (X) = \u03c1W (gX)\u03bb(X) for the same G-invariant function \u03bb where \u03c1V , \u03c1W are the group representation on V and W , resp.\nProof. According to Lemma 1, there is a G-invariant function \u03bb : X3 \u2192 V \u2295d0 such that fV (X) = \u03c1V (gX)\u03bb(X) for any X \u2208 X3. Then applying Theorem 2 to W , there exist a G-equivariant function fW : X3 \u2192 W such that fW (X) = \u03c1W (gX)\u03bb(X) for any X \u2208 X3, which shows the desired result.\nThe following two theorems follow from Theorem 3 and 5 by focusing on a given spatial embedding. Corollary 1. Let X \u2208 R3\u00d7m be a spatial embedding. We have the following relation between O(3)-steerable features and invariant features:\n1. If rank(X) = 3, there is a bijection between steerable features in Vl,ind and (2l + 1)- dimensional invariant features, as well as a bijection between steerable features in Vl,aug and (2l + 1)-dimensional invariant features.\n2. If rank(X) = 2, there is a bijection between steerable features in Vl,ind and (l + 1)- dimensional invariant features and a bijection between steerable features in Vl,aug and l-dimensional invariant features.\n3. If rank(X) = 1, there is a bijection between steerable features in Vl,ind and 1-dimensional invariant features, while there is no non-trivial steerable feature lying in Vl,aug.\n4. There exist only trivial steerable feature 0 and trivial invariant feature 0 if rank(X) = 0. Corollary 4. Let X \u2208 R3\u00d7m be a spatial embedding. We have the following relation between SO(3)-steerable features and invariant features:\n1. If rank(X) = 2 or 3, there is a bijection between type-l steerable features and (2l + 1)- dimensional invariant features.\n2. If rank(X) = 1, there is a bijection between type-l steerable features and 1-dimensional invariant features.\n3. If rank(X) = 0, there exist only trivial steerable feature 0 and trivial invariant feature 0. Remark 5. It is worth mentioning that Corollary 1 and 4suggests that type-l steerable features (l > 0) is more sensitive to the rank of the spatial embedding X than invariant features (l = 0).\nBefore proving Theorem 3 and 5, we introduce the following lemma:\nLemma 2. For any fixed l \u2265 0, any SO(3)-equivariant function f (l) : R3\u00d7m \u2192 Vl has a unique decomposition: f (l) = f (l)ind + f (l) aug into a sum of O(3)-equivariant functions f (l) ind : R3\u00d7m \u2192 Vl,ind and f (l)aug : R3\u00d7m \u2192 Vl,aug, where\nf (l) ind(X) := f (l)(X) + (\u22121)l \u00b7 f (l)(\u2212X) 2 and f (l)aug(X) := f (l)(X)\u2212 (\u22121)l \u00b7 f (l)(\u2212X) 2 .\nProof of Lemma 2. Upon routine verification, it becomes apparent that:\nf (l) ind(\u2212X) = (\u22121) l \u00b7 f (l)ind(X) and f (l) aug(\u2212X) = (\u22121)l+1 \u00b7 f (l)aug(X).\nSince both f (l)(X) and f (l)(\u2212X) are SO(3)-equivariant, so are both f (l)ind and f (l) aug. Combined with the sign-change property, we conclude that f (l)ind and f (l) aug are O(3)-equivariant.To establish uniqueness, let\u2019s consider any decomposition of f (l) = g(l)ind+h (l) aug into the sum of a O(3)-equivariant function g(l)ind : R3\u00d7m \u2192 Vl,ind and a O(3)-equivariant function h (l) aug : R3\u00d7m \u2192 Vl,aug. Now, observe that the functions on both sides of the following equation,\ng (l) ind \u2212 f (l) ind = f (l) aug \u2212 h(l)aug\nare O(3)-equivariant, which, due to sign-change property: (\u22121)l \u00b7 ( g (l) ind(X)\u2212 f (l) ind(X) ) =g (l) ind(\u2212X)\u2212 f (l) ind(\u2212X)\n= f (l)aug(\u2212X)\u2212 h(l)aug(\u2212X) =(\u22121)l+1 \u00b7 ( f (l)aug(X)\u2212 h(l)aug(X) ) =\u2212 (\u22121)l \u00b7 ( g (l) ind(X)\u2212 f (l) ind(X)\n) can only be 0. In other words, g(l)ind = f (l) ind and h (l) aug = f (l) aug.\nProof of Theorem 3 and 5. We can observe that Theorem 3 imply Theorem 5 by applying Lemma 2. Therefore, it remains to prove all the correspondences for O(3). The proof strategy here is similar to that of Lemma 1 with additional care on the design of the choice function c : R3\u00d7m/G \u2192 R3\u00d7m. We recall the following decomposition:\nR3\u00d7m = 3\u2294\nr=0\nXr, where Xr := { X \u2208 R3\u00d7m \u2223\u2223rank (X) = r} . Since G := O(3) action does not affect the rank of a spatial embedding X , the decomposition is preserved under quotient:\nR3\u00d7m/G = 3\u2294\nr=0\n(Xr/G) .\nThis allows us to define the choice function in ways that respect the geometry arising from different rankX = r conditions:\n(r = 3) We shall see that the argument here is a special case of Remark 1 applied on the subset X3 \u2282 R3\u00d7m. Invoking axiom of choice, we obtain a choice function:\nc3 : X3/G \u2192 X3 with c3 (G \u00b7X) \u2208 G \u00b7X.\nNotice that in the full-rank setting, all stabilizers are the same; in particular, they are trivial:\nGX = Gc3(G\u00b7X) = G3 = {I} , \u2200X \u2208 X3.\n(r = 2) Here is where the novelty of our argument comes in. We observe that\n(G \u00b7X) \u2229 ( x\u2212y\u2013plane R2 \u00d7 {0} )m \u0338= \u2205, \u2200X \u2208 X2.\nIn other words, we can take the following special choice function: c2 : X2/G \u2192 X2 \u2229 ( R2 \u00d7 {0} )m with c2 (G \u00b7X) \u2208 G \u00b7X.\nNotably, all the choices here share the same stabilizer:\nGc2(G\u00b7X) = G2 := { I,Rz := [ 1 0 0 0 1 0 0 0 \u22121 ]} , \u2200X \u2208 X2.\nThis will be useful later on.\n(r = 1) We modify the argument in the previous (r = 2) case to reflect the rankX = 1 geometry and apply to the (r = 1) case. Again, we start with the following observation:\n(G \u00b7X) \u2229 ( z\u2013axis {0}2 \u00d7 R )m \u0338= \u2205, \u2200X \u2208 X1. We may, thus, take the following special choice function:\nc1 : X1/G \u2192 X1 \u2229 ( {0}2 \u00d7 R )m with c1 (G \u00b7X) \u2208 G \u00b7X.\nSimilarly, all the choices here share the same stabilizer:\nGc1(G\u00b7X) = G1 := {[ R 0 0 1 ] \u2208 G \u2223\u2223\u2223\u2223R \u2208 O(2)} , \u2200X \u2208 X1. (r = 0) This is the trivial case X0 = {0}, and thus the choice function is defined uniquely:\nc0 : X0/G = {G \u00b7 0} \u2192 {0} . And obviously, Gc0(G\u00b70) = G0 = G.\nFor convenience, we define the total choice function:\nc := 3\u2211 r=0 cr \u00b7 1Xr/G.\nVia axiom of choice again, we can find a G valued function:\ng(\u00b7) : R3\u00d7m \u2192 G such that X = gX \u00b7 c (G \u00b7X) .\nAs a direct consequence, given O(3)\u2013equivariant f : Xr \u2192 Vl,\u2217, we have the following formula:\nf (X) = Dl\u2217 (gX) \u00b7 f (cr (G \u00b7X)) , \u2200X \u2208 Xr.\nBy design, the formula suggests a way to relate a Vl,\u2217 steerable feature to a (2l + 1)\u2013dimensional invariant features:\n\u03bb (X) := f (cr (G \u00b7X)) = Dl\u2217 (gX) \u22121 \u00b7 f (X) , \u2200X \u2208 Xr.\nYet, upon further inspection, \u03bb (X) has some hidden structure relating to the stabilizer Gr. To be precise, the O(3)\u2013equivariance of f implies that\nDl\u2217 (g) \u00b7 \u03bb (X) = f (g \u00b7 cr (G \u00b7X)) = f (cr (G \u00b7X)) = \u03bb (X) , \u2200g \u2208 Gr. In other words, \u03bb (X) could lie in a proper subspace.\n\u03bb (X) \u2208 \u22c2\ng\u2208Gr\nker ( Dl\u2217 (g)\u2212 I ) =: Vr,\u2217.\nWe first go through the two trivial cases: V3,\u2217 =ker (0) = \u2295 \u2212l\u2264m\u2264l F \u00b7 em, V0,\u2217 = \u22c2 g\u2208G ker ( Dl\u2217 (g)\u2212 I ) = {0} .\nFor r = 2, V2,\u2217 is exactly an eigenspace of Dl\u2217 (Rz): V2,\u2217 = ker ( Dl\u2217 (Rz)\u2212 I ) .\nA direct calculation shows the following:\nDl\u2217 (Rz) = (\u22121) l+1aug(\u2217) Dl (\u03c0, 0, 0) = [ \u00b7 \u00b7 \u00b7 (\u22121)m+l+1aug(\u2217) em \u00b7 \u00b7 \u00b7 ] \u2212l\u2264m\u2264l .\nTherefore, we have: V2,\u2217 =\n\u2295 \u2212l\u2264m\u2264l,\nm \u2261 mod2 l+1aug(\u2217)\nF \u00b7 em.\nIn particular, we have dimV2,ind = l + 1, dimV2,aug = l, and V3,\u2217 = V2,ind \u2295 V2,aug. To deal with the (r = 1) case, we first notice that G1 can be generated by the following:\nRy := [ 1 0 0 0 \u22121 0 0 0 1 ] , and R\u03b1 := [ cos\u03b1 \u2212 sin\u03b1 0 sin\u03b1 cos\u03b1 0 0 0 1 ] , \u03b1 \u2208 [0, 2\u03c0) .\nThis simplifies our problem, V1,\u2217 = ker ( Dl\u2217 (Ry)\u2212 I ) \u2229 \u22c2 \u03b1 ker ( Dl\u2217 (R\u03b1)\u2212 I ) .\nDirect calculation gives: Dl\u2217 (R\u03b1) = Dl (\u03b1, 0, 0) = [ \u00b7 \u00b7 \u00b7 e\u2212i\u03b1mem \u00b7 \u00b7 \u00b7 ] \u2212l\u2264m\u2264l . Therefore, we must have: \u22c2 \u03b1 ker ( Dl\u2217 (R\u03b1)\u2212 I ) = F \u00b7 e0.\nIt remains to check whether e0 lies in the eigenspace ker ( Dl\u2217 (Ry)\u2212 I ) , or not. We perform the following calculation:\nDl\u2217 (Ry) \u00b7 e0 = (\u22121) l+1aug(\u2217) \u00b7Dl (0, \u03c0, 0) \u00b7 e0 = (\u22121)1aug(\u2217) \u00b7 e0.\nWe may now conclude that V1,ind = F \u00b7 e0 and V1,aug = {0}. Combining what we have, we obtain the following picture:\nf (X) f (c (G \u00b7X)) \u03bb (X) Vr,\u2217, Dl\u2217(gX) \u22121 X\u2208Xr \u2208\nwith each space being characterized as follows: V3,ind,V3,aug = \u2295 \u2212l\u2264m\u2264l F \u00b7 em \u2243V \u22952l+10 V2,ind = \u2295 \u2212l\u2264m\u2264l, m \u2261 mod2 l F \u00b7 em \u2243V \u2295l+10 V2,aug = \u2295 \u2212l\u2264m\u2264l, m \u0338\u2261 mod2 l F \u00b7 em \u2243V \u2295l0 V1,ind = F \u00b7 e0 \u2243V0 V1,aug,V0,ind,V0,aug = {0} .\nWe now argue that the following space of the invariant features\n\u039br,\u2217 := {\u03bb : Xr \u2192 Vr,\u2217|\u03bb : O(3)-invariant}\nhas 1\u2013to\u20131 correspondence to the following space of steerable features\nFr,\u2217 := {f : Xr \u2192 Vl,\u2217|f : O(3)-equivariant} ,\nand the correspondence is exactly given by the following formula:\nf (X) = Dl\u2217 (gX) \u00b7 \u03bb (X) .\nIndeed, for f \u2208 Fr,\u2217, the invertibility of Dl\u2217 (gX) defines a unique \u03bb. Moreover, we\u2019ve established that such \u03bb is O(3)\u2013invariant and has its images contained in Vr,\u2217 and thus, \u03bb \u2208 \u039br,\u2217. On the other hand, given \u03bb \u2208 \u039br,\u2217 and any X \u2208 Xr, we have the following,\nDl\u2217 (gX) \u00b7 \u03bb (X) = Dl\u2217 (gX) \u00b7 pVr,\u2217 \u00b7 \u03bb (X) ,\nwhere pVr,\u2217 is the matrix that represent the orthogonal projection onto Vr,\u2217. Since\ng \u00b7X =  gg\u00b7X \u00b7cr ( G \u00b7 g \u00b7X\ufe38 \ufe37\ufe37 \ufe38 =G\u00b7X ) g \u00b7 gX \u00b7cr (G \u00b7X) =\u21d2 g\u22121g\u00b7X \u00b7 g \u00b7 gX \u2208 Gr, \u2200X \u2208 Xr,\nwe obtain ( Dl\u2217 ( g\u22121g\u00b7X \u00b7 g \u00b7 gX ) \u2212 I ) \u00b7 pVr,\u2217 = 0,\nas a direct consequence. After some algebraic manipulation, we derive the following equation:\nDl\u2217 (gg\u00b7X) \u00b7 pVr,\u2217 = Dl\u2217 (g) \u00b7Dl\u2217 (gX) \u00b7 pVr,\u2217 , \u2200X \u2208 Xr.\nIn other words, the formula\n\u03bb (X) f (X) := Dl\u2217 (gX) \u00b7 \u03bb (X) Dl\u2217 (gX) \u00b7 pVr,\u2217 \u00b7 \u03bb (X) , X\u2208Xr\ndefines an O(3)\u2013equivariant function f \u2208 Fr,\u2217. With that, we establish the 1\u2013to\u20131 correspondence."
        },
        {
            "heading": "D GEOMETRIC WEISFEILER-LEHMAN TEST (GWL)",
            "text": "In this section, we will provide an overview of the geometric Weisfeiler-Lehman test (GWL) and its invariant version (IGWL), as originally introduced in [20]; however, for simplicity, we exclude input vector features. Additionally, we will extend IGWL to the k-hop setting and demonstrate that k-hop IGWL cannot distinguish any two k-hop identical graphs. As a corollary, we will establish that GWL remains strictly more powerful than k-hop IGWL. We will also prove the conditions under which k-hop invariant GNNs achieve the same expressive power as k-hop IGWL.\nBefore delving into the details, we introduce a graph-level readout, a G-invariant multiset function, at the final layer of geometric GNNs. This function maps multisets {f (t)i | i \u2208 V} to invariant features in V \u2295d \u2032\n0 for some d \u2032 > 0. This inclusion aids in understanding when geometric GNNs attain the\nmaximum expressive power characterized by the GWL test.\nGWL. Consider a geometric graph G = (V, E ,F ,X). Let C denote a countable space of colors. Initially, we assign a scalar color c(0)i \u2208 C to each node i \u2208 V through an injective mapping function HASH based on their input features fi:\nc (0) i := HASH(fi). (16)\nAdditionally, we assign an extra geometric object g(0)i to each node i \u2208 V by g (0) i = c (0) i .\nThen we define the inductive step. Assuming we have all the colors c(t\u22121)i and geometric objects g (t\u22121) i at iteration t\u22121, for each node i, we aggregate the geometric information from its neighborhood Ni into a new geometric object:\ng (t) i :=\n( (c\n(t\u22121) i , g (t\u22121) i ), {{(c (t\u22121) j , g (t\u22121) j ,xij) | j \u2208 Ni}}\n) . (17)\nNotice that the geometric objects are acted on by the group G: g \u00b7 g(t)i := ( (c (t\u22121) i , g \u00b7 g (t\u22121) i ), {{(c (t\u22121) j , g \u00b7 g (t\u22121) j , g \u00b7 xij) | j \u2208 Ni}} ) . (18)\nOne can check that the process of creating g(t)i is injective and G-equivariant. We then assign the color c(t)i at iteration t through a G-invariant and G-orbit injective map, denoted as I-HASH (t),\nc (t) i := I-HASH (t)(g (t) i ). (19)\nIn other words, I-HASH(t)(g) = I-HASH(t)(g\u2032) if and only if g = g \u00b7 g\u2032 for some g \u2208 G. The iteration terminates when the colors induce the same partitions of nodes. Then, given two attributed graphs G1 = (V1, E1),G2 = (V2, E2), if there is some iteration t s.t. {{c(t)i | i \u2208 V1}} \u0338= {{c(t)j | j \u2208 V2}}, then GWL determines that these two graphs are not geometrically isomorphic.\nInvariant GWL. For the invariant version of GWL, we do not consider the equivariant geometric object. Thus, the iteration becomes:\nc (t) i := I-HASH ( c (t\u22121) i , {{(c (t\u22121) j ,xij) | j \u2208 Ni}} ) , (20)\nwhere the initialization remains the same c(0)i := HASH(fi).\nk-hop IGWL. To extend the 1-hop aggregation in IGWL to k-hop aggregation, we replace the 1-hop neighborhood Ni with the k-hop neighborhood N (k)i :\nc (t) i := I-HASH ( c (t\u22121) i , {{(c (t\u22121) j ,xij) | j \u2208 N (k) i }} ) . (21)\nNow we extend the results in [20] for IGWL to k-hop IGWL: Proposition 1. k-hop IGWL can distinguish k-hop distinct geometric graphs with just one iteration, but it cannot differentiate k-hop identical geometric graphs no matter how many iterations are used.\nProof of Proposition 1. Let G1 = (V1, E1,F G1 ,XG1),G2 = (V2, E2,F G2 ,XG2) be two k-hop identical geometric graphs. By definition, there exists a graph isomorphism b such that for any node i \u2208 V1, we can find a group element gi \u2208 G satisfies fG1i = f G2 b(i),x G1 i = gi \u00b7 x G2 b(i) and fG1j = f G2 b(j),x G1 j = gi \u00b7 x G2 b(j) whenever j \u2208 N (k) i . This implies for any i, we have c (0) i = c (0) b(i), and hence the multisets {{(c(0)j ,x G1 ij ) | j \u2208 N (k) i }} and {{(c (0) j\u2032 ,x G2 b(i)j\u2032) | j\n\u2032 \u2208 N (k)b(i)}} are identical up to group action. Based on the definition of k-hop IGWL iterations defined in 20, we can then conclude that c(1)i = c (1) b(i) for any i. By induction, it follows that any number of k-hop IGWL iterations cannot distinguish G1 and G2. Now, let G1 = (V1, E1,F G1 ,XG1),G2 = (V2, E2,F G2 ,XG2) be two k-hop distinct geometric graphs. By definition, for any graph isomorphism b, there is a node i \u2208 V1 such that the corresponding k-hop subgraphs forming by N (k)i \u222a{i} and N (k) b(i) \u222a{b(i)} are distinct under the group action. This implies c (1) i \u0338= c (1) b(i). Since b is arbitrary, we conclude that {{c (1) i | i \u2208 V1}} =\u0338{c (1) j | j \u2208 V2}} and hence 1 iteration of k-hop IGWL is sufficient to distinguish G1 and G2.\nWe recap two key results regarding the expressive power of GWL from [20]: Proposition 2. GWL can distinguish any two k-hop distinct geometric graphs, and k iteration is sufficient. Proposition 3. Up to k iteration, GWL cannot distinguish any two k-hop identical geometric graphs.\nApplying Proposition 2 and 3, along with Proposition 1, we derive the following theorem. Notably, the case for k = 1 corresponds to Theorem 8 in [20]: Theorem 6. GWL is strictly more powerful than k-hop IGWL for any k, while they have the same expressive power when applied to fully connected graphs.\nProof. We have demonstrated that k-hop IGWL can distinguish k-hop distinct geometric graphs, which are distinguishable by GWL using k iterations.\nHowever, k-hop IGWL cannot distinguish k-hop identical geometric graphs. As illustrated in Figure 1, we provide an example where k-hop IGWL fails to distinguish such graphs, whereas GWL succeeds.\nProposition 4. Any pair of geometric graphs is distinguished by k-hop invariant GNNs is also distinguished by k-hop IGWL.\nProof. The proof is the same as the proof of Theorem 24 in [20] by replacing the 1-hop neighborhoods with k-hop neighborhoods.\nProposition 5. k-hop invariant GNNs have the same expressive power as k-hop IGWL if the following conditions hold: (1) The aggregate function AGG and update function UPD are G-orbit injective and G-invariant multiset functions (2) The graph-level readout function is an injective multiset function.\nProof. The proof is the same as the proof of Proposition 25 in [20] by replacing the 1-hop neighborhoods with k-hop neighborhoods.\nSignificance of faithful representations in learning steerable features. It has been demonstrated in [20] that the expressive power of equivariant GNNs, when learning steerable features up to type 1, is bounded by the GWL test, with equality under certain assumptions on the injectivity of UPD, AGG, and the graph-level readout function. In particular, they prove the following results:\nProposition 6. Any pair of geometric graphs is distinguished by 1-hop equivariant GNNs is also distinguished by GWL.\nProposition 7. 1-hop equivariant GNNs learning steerable features up to type 1 have the same expressive power as GWL if the following conditions hold: (1) The aggregate function AGG and update function UPD are G-orbit injective and G-equivariant multiset functions (2) The graph-level readout function is a G-orbit injective and G-invariant multiset function.\nRemark 6. As we merge scalar and vector features into steerable features fi, certain conditions in Proposition 7 have been adjusted to accommodate our setup.\nIn this work, we extend Proposition 7 to cover cases where equivariant GNNs learn steerable features on any representations. Notably, we highlight that achieving equality in this extended scenario necessitates an additional condition: the steerable features must lie on faithful representations.\nProposition 8. Consider 1-hop equivariant GNNs learning features on steerable vector space V where the aggregate function AGG learns features on steerable vector space W . Then these equivariant GNNs have the same expressive power as GWL if the following conditions hold: (1) The aggregate function AGG and update function UPD are G-orbit injective and G-equivariant multiset functions (2) The graph-level readout function is a G-orbit injective and G-invariant multiset function. (3) V , W are faithful representations.\nProof. We employ a similar strategy to the proof presented in Proposition 7 in [20] to establish our result. In this proof, we use [. . .] to denote the equivalence class generated by the actions of G. Then, any G-orbit injective function can be expressed as an injective function over the equivalence classes [. . .].\nThe GWL test updates the node color c(t)i and geometric object g (t) i as:\ng (t) i = hv (( c (t\u22121) i , g (t\u22121) i ) , {{( c (t\u22121) j , g (t\u22121) j ,xij ) | j \u2208 Ni }}) , c (t) i = hs ([ g (t) i ]) ,\nwhere hs is a G-invariant and G-orbit injective map and hv is a G-equivariant and injective operation, such as expanding the geometric multiset by copying, as shown in equation (17).\nConsider an equivariant GNN that satisfies the conditions outlined in the theorem statement. We will show by induction that at any iteration t, there always exist G-equivariant and injective functions \u03c6(t)\nsuch that f (t)i = \u03c6 (t) ( g (t) i ) for any t. Let h denote the graph-level readout function. Since h maps\ndifferent multisets of node features to unique invariant features, f (t)i = \u03c6 (t) ( g (t) i ) implies that there\nexists an injective function \u03c6(t)c such that h ({{ f (t) i | i \u2208 V }}) = \u03c6 (t) c ({{ c (t) i | i \u2208 V }}) where V denotes the set of nodes.\nFirst, we observe that f (t)i = \u03c6 (t) ( g (t) i ) holds for t = 0 because g(0)i = c (0) i = HASH(fi) for all\ni \u2208 V . Now, suppose this holds for iteration t. At iteration t+ 1, substituting f (t)i with \u03c6(t) ( g (t) i ) implies that.\nf (t+1) i =UPD ( f (t) i ,AGG({{f (t) i ,f (t) j ,xij | j \u2208 Ni}}) ) =UPD ( \u03c6(t) ( g (t) i ) ,AGG ({{ \u03c6(t) ( g (t) i ) , \u03c6(t) ( g (t) j ) ,xij | j \u2208 Ni }})) .\n(22)\nConsider the function \u03d5(c(t)i , g (t) i ) := \u03c6 (t)(g (t) i ). Suppose \u03d5(c (t) i , g (t) i ) = \u03d5(c (t) j , g (t) j ). The injectivity of \u03c6(t) then implies that g(t)i = g (t) j . As hs is G-invariant and G-orbit injective, we deduce that c (t) i = c (t) j , and therefore, \u03d5 is injective. The G-equivariance of \u03d5 is inherited directly from \u03c6 (t). By substituting \u03c6(t)(g(t)i ) with \u03d5(c (t) i , g (t) i ), we obtain\nf (t+1) i = UPD ( \u03d5 ( c (t) i , g (t) i ) ,AGG ({{ \u03d5 ( c (t) i , g (t) i ) , \u03d5 ( c (t) j , g (t) j ) ,xij | j \u2208 Ni }})) . (23)\nIt remains to show that there exists an injective function \u03d5\u2032 such that:\nf (t+1) i = \u03d5\n\u2032 ((\nc (t) i , g (t) i\n) , {{(\nc (t) j , g (t) j ) ,xij | j \u2208 Ni }}) .\nIndeed, we can define \u03c6(t+1) = \u03d5\u2032 \u25e6 h\u22121v and then we have\nf (t+1) i = \u03d5 \u2032 \u25e6 h\u22121v hv (( c (t) i , g (t) i ) , {{( c (t) j , g (t) j ) ,xij | j \u2208 Ni }}) = \u03c6(t+1) \u25e6 hv (( c (t) i , g (t) i ) , {{( c (t) j , g (t) j ) ,xij | j \u2208 Ni\n}}) = \u03c6(t+1)(g\n(t+1) i ).\n(24)\nNow consider the following construction of \u03d5\u2032, \u03d5\u2032 ((\nc (t) i , g (t) i\n) , {{(\nc (t) j , g (t) j ) ,xij | j \u2208 Ni }}) :=UPD ( \u03d5 ( c (t) i , g (t) i ) ,AGG ({{ \u03d5 ( c (t) i , g (t) i ) , \u03d5 ( c (t) j , g (t) j ) ,xij | j \u2208 Ni\n}})) To demonstrate \u03d5\u2032 is injective, we will first show that UPD and AGG are injective. This step requires the faithfulness of V and W . More precisely, the G-orbit injectivity and G-equivariance of UPD and AGG with the faithfulness of V and W imply that they are injective. Suppose UPD(fi,mi) = UPD(fj ,mj) where mi denotes the output of AGG. Due to the G-orbit injectivity, we have [fi,mi] = [fj ,mj ]. Therefore, (fi,mi) = g \u00b7 (fj ,mj) = (g \u00b7 fj , g \u00b7 mj) for some g \u2208 G. Applying the G-equivariance, we obtain\nUPD(fj ,mj) = UPD(fi,mi) (25) = UPD(g \u00b7 fj , g \u00b7mj) (26) = g \u00b7UPD(fj ,mj) (27)\nFaithfulness implies g is the identity, proving the injectivity of UPD. Similarly, we can prove the injectivity of AGG.\nNow applying the injectivity of UPD and AGG, we see that: \u03d5 ( c (t) i , g (t) i ) = \u03d5 ( c (t) j , g (t) j ) , (28){{\n\u03d5 ( c (t) i , g (t) i ) , \u03d5 ( c (t) k , g (t) k ) ,xik | k \u2208 Ni }} = {{ \u03d5 ( c (t) j , g (t) j ) , \u03d5 ( c (t) k , g (t) k ) ,xjk | k \u2208 Nj }} .\n(29)\nThe injectivity of \u03d5 then implies that( c (t) i , g (t) i ) = ( c (t) j , g (t) j ) , (30){{(\nc (t) k , g (t) k ) ,xik | k \u2208 Ni }} = {{( c (t) k , g (t) k ) ,xjk | k \u2208 Nj }} . (31)\nThis shows the injectivity of \u03d5\u2032 and thus completes the proof. Theorem 2. Consider 1-hop equivariant GNNs learning features on steerable vector space V where the aggregate function AGG learns features on steerable vector space W . Suppose V and W are faithful representations, and UPD and AGG satisfy certain assumptions on the injectivity outlined in Proposition 8 in the appendix. Then with k iterations, these equivariant GNNs learn different multisets of node features {{f (k)i }} on two k-hop distinct geometric graphs.\nProof. This can be derived from the proof of Proposition 8 and 2."
        },
        {
            "heading": "E AN OVERVIEW OF EXISTING INVARIANT AND EQUIVARIANT GNNS",
            "text": "In this section, we discuss the invariant and equivariant architectures considered in the work. However our framework covers models beyond those mentioned here, for instance it includes the models discussed in Joshi et al. [20]. It\u2019s worth noting that this framework is not confined to using 2-body aggregation; it can also employ multi-body aggregation methods.\nE.1 G-INVARIANT GNNS\nSchNet The SchNet model [34] is a 2-body G-invariant architecture which propagates type-0 features in 1-hop using relative distances:\nf (t+1) i = UPD ( f (t) i ,AGG({{f (t) i ,f (t) j , \u2225xij\u2225 | j \u2208 N (1) i }}) ) DimeNet++ The DimeNet++ model [14] is a 3-body G-invariant architecture which propagates type-0 features in 1-hop using relative distances and angles.\nf (t+1) i = UPD ( f (t) i ,AGG({{f (t) i ,f (t) j , \u2225xij\u2225,xij \u00b7 xik | j, k \u2208 N (1) i , k \u0338= j}}) ) ComENet The ComENet model [40] is a 4-body G-invariant architecture which propagates type-0 features in 1-hop with complete edge attributes (\u2225xij\u2225, \u03b8ij , \u03d5ij , \u03c4ij):\nf (t+1) i = UPD ( f (t) i ,AGG({{f (t) j , \u2225xij\u2225, \u03b8ij , \u03d5ij , \u03c4ij | j \u2208 N (1) i }}) ) ,\nwhere \u03b8ij , \u03d5ij , and \u03c4ij are computed in quadruplet (within a 2-hop neighborhood).\nE.2 G-EQUIVARIANT GNNS\nEGNN and GVP The EGNN [32] and GVP [19] models are a 3-body G-equivariant architecture which propagate type-0 and type-1 features in 1-hop using relative distances.\nf (t+1) i = UPD ( f (t) i ,AGG({{f (t) i ,f (t) j , \u2225xij\u2225 | j \u2208 N (1) i }}) ) MACE and eSCN The MACE [2] and eSCN [28] models are 2-body G-equivariant architecture which propagate up to type-L features in 1-hop using spherical harmonics.\nf (t+1) i = UPD ( f (t) i ,AGG({{f (t) i ,f (t) j , Y (x\u0302ij), \u2225xij\u2225 | j \u2208 N (1) i }}) ) EquiformerV2 The EquiformerV2 model [24] is a G-equivariant architecture, similar to the one mentioned above. It propagates up to type-L features in 1-hop by fully utilizing all neighbors within the neighborhood to create the attention weights."
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTAL DETAILS",
            "text": "F.1 ADDITIONAL RESULTS FOR k-CHAINS\nabove or below the expected value. Unit:%.\nTable 5 reports the 10-fold cross validation mean test accuracy and standard deviation for chain length k = 4 and varying model depth. The values are shaded based on the expected value from Section 3 . We remark that EGNN, ClofNet, LEFTNet [10], eSCN and EquiformerV2 do not perform as expected on this particular task. As discussed in [20] the under performance of EGNN and ClofNet is likely due to oversquashing [38; 1]. For LEFTNet, the underlying cause is attributed to its invariant design. Consequently, this architecture can be categorized as a 2-hop invariant GNN, leading to expected challenges in capturing changes in global geometry. The consistent underperformance of eSCN may be due to the task being O(3) equivariant but not SO(3) equivariant. Additionally, eSCN and EquiformerV2 are quasi-equivariant methods which introduce error into the equivariance due to their spherical activation function, see Appendix D of [28] for details. This additional error explains the significant difference in results for eSCN and EquiformerV2 as well as why they may over perform as well as under perform.\nF.2 ADDITIONAL RESULTS FOR IS2RE\nNote that for EquiformerV2 L=1, the model will fit onto the GPU. However, during training the data and the model will exceed the GPU memory capabilities. We denote the out of memory phenomenon OOM in the Table 6 and Table 7 and report the model parameters and size.\nF.3 EXPERIMENTAL DETAILS\nk-Chain The reported optimal hyperparameters are used except in the case of steerable models eSCN and EquiformerV2 where the steerable feature dimensions are reduced due to memory constraints. Steerable models using type L = 0, 1, and 2 are used with adjusted steerable feature dimension to preserve the dimension of the steerable convolution. The experiment is implemented in Google Colab [26] with invariant and equivariant models using the 16GB NVIDIA T4 GPUs and the steerable models using the 40GB NVIDIA A100 GPUs. To ensure the connectivity of the graph is respected, we adjust the cutoff radius of all models to 5.1 units.\nThe training procedure of [20] is modified to apply a softmax function to model outputs, stabilizing the classification training. The cross entropy is minimized over 1000 training epochs using the Adam optimizer. The learning rate is scheduled based on the validation accuracy with an initial learning rate of 1e-4, 0.9 learning rate decay, 25 epochs of patience, and a minimum learning rate of 1e-5. We use fixed data splits and report the mean and standard deviation of the test accuracy from 10-fold cross-validation of random model weight initializations.\nIS2RE The task is implemented using the OC20 framework using the baseline IS2RE training procedure with 12 training epochs for all models and the reported optimal model hyperparameters. Due to computational constraints, we consider the 10k molecule training data split and reduce the steerable feature dimension to train each model on a single 24GB NVIDIA RTX3090 GPU.\nThe training procedure of [6] is used for training. In particular we use the AdamW optimizer with a learning rate of 8e-4, a maximum epoch of 12 and gradient clipping if the gradient norm is greater than 20.\nS2EF The task is implemented in the OC20 framework using the baseline S2EF training procedure and the reported optimal hyperparameters for each model. Training is performed in parallel on two 24GB NVIDIA RTX4090 GPUs using the 2M molecule training data split. Due to the limited computational resources, we are only able to train a single model for 8 epochs over the span of 8 weeks. We note that we compare our results to [28] which trains 4-fold cross validation on 16GPUs with 32GB of RAM and does not report the standard deviation.\nThe training procedure of [6] is used. In particular we use the AdamW optimizer with a weighted decay of 1e-3, a cosine learning rate scheduler and an initial learning rate of 4e-4. The maximum number of epochs are 12 and gradient clipping is applied if the gradient norm greater than 100.\nF.4 HYPERPARAMETER DETAILS\nIn this section we provide details on the adjusted hyperparameters for each task. In general we implement the reported optimal hyperparameters for each model.\nF.4.1 k-CHAIN\nFor all models the cutoff hyperparameter (cutoff, max_radius, r_max) is set to 5.1. Table 8 lists the hyperparameters for ComENet, eSCN and EquiformerV2. For all other models, we use the hyperparameters outlined in Joshi et al. [20].\nF.4.2 IS2RE\nF.4.3 S2EF\nG AN ILLUSTRATIVE EXAMPLE\nWhile Lemma 1 is not affected by rank(X), the rank of the spatial embedding X , the answer to this question is contingent upon the rank. In Fig. 4, we present an example to gain intuition into this phenomenon. The graph comprises two distinct points x1,x2. We assert that all type-1 features must lie on the line passing through x1 and x2, which corresponds to a rank-1 space. Suppose there exists a type-1 steerable feature v \u0338= 0 that does not lie on this line. In such a case, we can select a rotation R around the line, which preserves x1 and x2 but changes the direction of v to Rv \u0338= v. Consequently, v cannot be generated equivariantly from x1 and x2, leading to a contradiction.\nThe rationale behind this is that different ranks of X correspond to distinct underlying symmetries inherent to X . Specifically, when rank(X) = 3, there exists no non-trivial group action capable of preserving the spatial embedding X , meaning that the stabilizer GX is empty. In contrast, in cases where rank(X) = 1 or 2 \u2013 the coordinates are confined to a line or\na plane \u2013 certain group actions can be applied without affecting X . That is, \u2203g \u2208 G s.t. g \u00b7X = X . Since g \u00b7 f(X) = f(g \u00b7X) = f(X), we observe that any steerable feature f(X) must be preserved under the action of g. This limitation inherently constrains the complexity of steerable features."
        }
    ],
    "year": 2024
}