{
    "abstractText": "We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zilin Si"
        },
        {
            "affiliations": [],
            "name": "Gu Zhang"
        },
        {
            "affiliations": [],
            "name": "Qingwei Ben"
        },
        {
            "affiliations": [],
            "name": "Branden Romero"
        },
        {
            "affiliations": [],
            "name": "Zhou Xian"
        },
        {
            "affiliations": [],
            "name": "Chao Liu"
        },
        {
            "affiliations": [],
            "name": "Chuang Gan"
        }
    ],
    "id": "SP:4dc812c833371ed805aaf3e474e20533a78effe8",
    "references": [
        {
            "authors": [
                "Arpit Agarwal",
                "Timothy Man",
                "Wenzhen Yuan"
            ],
            "title": "Simulation of vision-based tactile sensors using physics based rendering",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Wei Chen",
                "Heba Khamis",
                "Ingvars Birznieks",
                "Nathan F. Lepora",
                "Stephen J. Redmond"
            ],
            "title": "Tactile sensors for friction estimation and incipient slip detection\u2014toward dexterous robotic manipulation: A review",
            "venue": "IEEE Sensors Journal,",
            "year": 2018
        },
        {
            "authors": [
                "Weihang Chen",
                "Yuan Xu",
                "Zhenyang Chen",
                "Peiyu Zeng",
                "Renjun Dang",
                "Rui Chen",
                "Jing Xu"
            ],
            "title": "Bidirectional sim-to-real transfer for gelsight tactile sensors with cyclegan",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Zixi Chen",
                "Shixin Zhang",
                "Shan Luo",
                "Fuchun Sun",
                "Bin Fang"
            ],
            "title": "Tacchi: A pluggable and low computational cost elastomer deformation simulator for optical tactile sensors",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Church",
                "John Lloyd",
                "Raia Hadsell",
                "Nathan F. Lepora"
            ],
            "title": "Tactile sim-to-real policy transfer via real-to-sim image translation",
            "venue": "Proceedings of the 5th Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Erwin Coumans",
                "Yunfei Bai"
            ],
            "title": "Pybullet, a python module for physics simulation for games, robotics and machine",
            "year": 2016
        },
        {
            "authors": [
                "Daniel Fernandes Gomes",
                "Paolo Paoletti",
                "Shan Luo"
            ],
            "title": "Generation of gelsight tactile images for sim2real learning",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Nikolaus Hansen",
                "Sibylle D M\u00fcller",
                "Petros Koumoutsakos"
            ],
            "title": "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es)",
            "venue": "Evolutionary computation,",
            "year": 2003
        },
        {
            "authors": [
                "Carolina Higuera",
                "Byron Boots",
                "Mustafa Mukadam"
            ],
            "title": "Learning to read braille: Bridging the tactile reality gap with diffusion models",
            "venue": "arXiv preprint arXiv:2304.01182,",
            "year": 2023
        },
        {
            "authors": [
                "Yuanming Hu",
                "Yu Fang",
                "Ziheng Ge",
                "Ziyin Qu",
                "Yixin Zhu",
                "Andre Pradhana",
                "Chenfanfu Jiang"
            ],
            "title": "A moving least squares material point method with displacement discontinuity and two-way rigid body coupling",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2018
        },
        {
            "authors": [
                "Yuanming Hu",
                "Luke Anderson",
                "Tzu-Mao Li",
                "Qi Sun",
                "Nathan Carr",
                "Jonathan Ragan-Kelley",
                "Fredo Durand"
            ],
            "title": "Difftaichi: Differentiable programming for physical simulation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiao Huang",
                "Yuanming Hu",
                "Tao Du",
                "Siyuan Zhou",
                "Hao Su",
                "Joshua B Tenenbaum",
                "Chuang Gan"
            ],
            "title": "Plasticinelab: A soft-body manipulation benchmark with differentiable physics",
            "venue": "arXiv preprint arXiv:2104.03311,",
            "year": 2021
        },
        {
            "authors": [
                "Reina Ishikawa",
                "Masashi Hamaya",
                "Felix Von Drigalski",
                "Kazutoshi Tanaka",
                "Atsushi Hashimoto"
            ],
            "title": "Learning by breaking: food fracture anticipation for robotic food manipulation",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Chung Min Kim",
                "Michael Danielczuk",
                "Isabella Huang",
                "Ken Goldberg"
            ],
            "title": "Ipc-graspsim: Reducing the sim2real gap for parallel-jaw grasping with the incremental potential contact model",
            "venue": "In 2022 International Conference on Robotics and Automation (ICRA),",
            "year": 2022
        },
        {
            "authors": [
                "Xuan Li",
                "Yi-Ling Qiao",
                "Peter Yichen Chen",
                "Krishna Murthy Jatavallabhula",
                "Ming Lin",
                "Chenfanfu Jiang",
                "Chuang Gan"
            ],
            "title": "Pac-nerf: Physics augmented continuum neural radiance fields for geometry-agnostic system identification",
            "venue": "arXiv preprint arXiv:2303.05512,",
            "year": 2023
        },
        {
            "authors": [
                "Yijiong Lin",
                "John Lloyd",
                "Alex Church",
                "Nathan F. Lepora"
            ],
            "title": "Tactile gym 2.0: Sim-to-real deep reinforcement learning for comparing low-cost high-resolution robot",
            "venue": "touch. volume 7 of Proceedings of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Pingchuan Ma",
                "Peter Yichen Chen",
                "Bolei Deng",
                "Joshua B Tenenbaum",
                "Tao Du",
                "Chuang Gan",
                "Wojciech Matusik"
            ],
            "title": "Learning neural constitutive laws from motion observations for generalizable pde dynamics",
            "venue": "arXiv preprint arXiv:2304.14369,",
            "year": 2023
        },
        {
            "authors": [
                "Viktor Makoviychuk",
                "Lukasz Wawrzyniak",
                "Yunrong Guo",
                "Michelle Lu",
                "Kier Storey",
                "Miles Macklin",
                "David Hoeller",
                "Nikita Rudin",
                "Arthur Allshire",
                "Ankur Handa"
            ],
            "title": "Isaac gym: High performance gpu-based physics simulation for robot learning",
            "venue": "arXiv preprint arXiv:2108.10470,",
            "year": 2021
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Douglas Morrison",
                "Peter Corke",
                "J\u00fcrgen Leitner"
            ],
            "title": "Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Matthias M\u00fcller",
                "Bruno Heidelberger",
                "Marcus Hennix",
                "John Ratcliff"
            ],
            "title": "Position based dynamics",
            "venue": "Journal of Visual Communication and Image Representation,",
            "year": 2007
        },
        {
            "authors": [
                "Yashraj Narang",
                "Balakumar Sundaralingam",
                "Miles Macklin",
                "Arsalan Mousavian",
                "Dieter Fox"
            ],
            "title": "Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Antonin Raffin",
                "Ashley Hill",
                "Adam Gleave",
                "Anssi Kanervisto",
                "Maximilian Ernestus",
                "Noah Dormann"
            ],
            "title": "Stable-baselines3: Reliable reinforcement learning implementations",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Zilin Si",
                "Wenzhen Yuan"
            ],
            "title": "Taxim: An example-based simulation model for gelsight tactile sensors",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Zilin Si",
                "Zirui Zhu",
                "Arpit Agarwal",
                "Stuart Anderson",
                "Wenzhen Yuan"
            ],
            "title": "Grasp stability prediction with sim-to-real transfer from tactile sensing",
            "venue": "In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2022
        },
        {
            "authors": [
                "Sudharshan Suresh",
                "Zilin Si",
                "Joshua G Mangelson",
                "Wenzhen Yuan",
                "Michael Kaess"
            ],
            "title": "Shapemap 3-d: Efficient shape mapping through dense touch and vision",
            "venue": "In 2022 International Conference on Robotics and Automation (ICRA),",
            "year": 2022
        },
        {
            "authors": [
                "Shaoxiong Wang",
                "Mike Lambeta",
                "Po-Wei Chou",
                "Roberto Calandra"
            ],
            "title": "Tacto: A fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Tsun-Hsuan Wang",
                "Pingchuan Ma",
                "Andrew Everett Spielberg",
                "Zhou Xian",
                "Hao Zhang",
                "Joshua B Tenenbaum",
                "Daniela Rus",
                "Chuang Gan"
            ],
            "title": "Softzoo: A soft robot co-design benchmark for locomotion in diverse environments",
            "venue": "arXiv preprint arXiv:2303.09555,",
            "year": 2023
        },
        {
            "authors": [
                "Zhou Xian",
                "Bo Zhu",
                "Zhenjia Xu",
                "Hsiao-Yu Tung",
                "Antonio Torralba",
                "Katerina Fragkiadaki",
                "Chuang Gan"
            ],
            "title": "Fluidlab: A differentiable environment for benchmarking complex fluid manipulation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jie Xu",
                "Tao Chen",
                "Lara Zlokapa",
                "Michael Foshey",
                "Wojciech Matusik",
                "Shinjiro Sueda",
                "Pulkit Agrawal"
            ],
            "title": "An End-to-End Differentiable Framework for Contact-Aware Robot Design",
            "venue": "In Proceedings of Robotics: Science and Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jie Xu",
                "Sangwoon Kim",
                "Tao Chen",
                "Alberto Rodriguez Garcia",
                "Pulkit Agrawal",
                "Wojciech Matusik",
                "Shinjiro Sueda"
            ],
            "title": "Efficient tactile simulation with differentiability for robotic manipulation",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Kuan\u2013Ting Yu",
                "Alberto Rodriguez"
            ],
            "title": "Realtime state estimation with tactile and visual sensing. application to planar manipulation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Wenzhen Yuan",
                "Siyuan Dong",
                "Edward H Adelson"
            ],
            "title": "Gelsight: High-resolution robot tactile sensors for estimating geometry and force",
            "year": 2017
        },
        {
            "authors": [
                "Shaohong Zhong",
                "Alessandro Albini",
                "Oiwi Parker Jones",
                "Perla Maiolino",
                "Ingmar Posner"
            ],
            "title": "Touching a nerf: Leveraging neural radiance fields for tactile sensory data generation",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In the goal of enabling robots to perform human-level manipulation on a diverse set of tasks, touch is one of the most prominent components. Tactile sensing, as a modality, is unique in the sense that it provides accurate, fine-detailed information about environmental interactions in the form of contact geometries and forces. Although its efficacy has been highlighted by prior research, providing crucial feedback in grasping fragile objects (Ishikawa et al., 2022), enabling robots to perform in occluded environment (Yu & Rodriguez, 2018), and detecting incipient slip (Chen et al., 2018) for highly reactive grasping, there are still advances in tactile sensing to be made especially in the form of simulation.\nPhysics-based simulation has become a significant practical tool in the domain of robotics, by mitigating the challenges of real-world design and verification of learning algorithms. However, existing robotic simulators either lack simulation for tactile sensing or limit interactions to rigid\n\u2217Authors with equal contribution. \u2020This work was done during an internship at the MIT-IBM Watson AI Lab. 1https://difftactile.github.io/\nbodies. To accurately simulate tactile sensors which are inherently soft, it is essential to model soft body interaction\u2019s contact geometries, forces, and dynamics. Prior work (Si & Yuan, 2022) attempted to simulate contact geometries and forces for tactile sensors under (quasi-)static scenarios, and it was successfully applied to robotic perception tasks such as object shape estimation (Suresh et al., 2022), and grasp stability prediction (Si et al., 2022). However, highly dynamic manipulation tasks have not been thoroughly explored. Other prior works approach contact dynamics by either approximating sensor surface deformation using rigid-body dynamics (Xu et al., 2023) or using physics-based soft-body simulation methods such as Finite Element Method (FEM) (Narang et al., 2021). However, these methods are still limited to manipulating rigid objects.\nIn this work, we aim to build a differentiable tactile simulator, DIFFTACTILE , that supports contact-rich robotic manipulation of rigid, deformable, and articulated objects. Differentiability, as a key component of our work, provides fine-grained guidance for efficient skill learning (Huang et al., 2021; Xian et al., 2022). It also enables system identification to close the sim-to-real gap (Li et al., 2023). We implement DIFFTACTILE in Taichi (Hu et al., 2019) which leverages parallel GPU computing and auto-differentiation. To demonstrate the capability and versatility of\nour simulator, we evaluate it on a diverse set of manipulation tasks including handling fragile, deformable, dynamic objects that cannot be addressed with prior tactile simulators. We summarize our contributions below:\n\u2022 We introduce DIFFTACTILE , a platform supporting various tactile-assisted manipulation tasks. We model tactile sensors with FEM, objects in various materials (rigid, elastic, and elastoplastic) with Moving Least Square Material Point Method (MLS-MPM), and cable with Position-Based Dynamics (PBD). We simulate the contact between sensors and objects with a penalty-based contact model. In addition, we accurately simulate the optical response of tactile sensors with high spatial variation via a learning-based method.\n\u2022 Our system is differentiable and can reduce the sim-to-real gap with system identification. From a sequence of real-world data samples, we can optimize our simulator\u2019s sensor material and contact model parameters with differential physics and validate it with more general real-world scenarios.\n\u2022 We demonstrate the improvement of skill learning efficiency with tactile feedback. We evaluate it on stable and adaptive grasps of objects with diverse geometry and material properties, and four contact-rich manipulation tasks."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Tactile simulation The most recent work on tactile simulation is built upon existing rigid-body simulators. For example, Tacto (Wang et al., 2022), Tactile-Gym (Church et al., 2022; Lin et al., 2022) were built upon PyBullet (Coumans & Bai, 2016). An efficient tactile simulation (Xu et al., 2023) was built upon DiffRedMax (Xu et al., 2021), where a penalty-based contact model was used to simulate the force distribution for tactile sensors. Even though it is computationally efficient to use rigid body simulation, these tactile simulators approximate contact dynamics for soft bodies at the cost of fidelity.\nAlternatively, Finite Element Method (FEM)-based methods exist to accurately simulate soft body dynamics. A physics-based tactile simulator (Narang et al., 2021) was developed for SynTouch BioTac sensors (SynTouch) by using FEM in Isaac Gym (Makoviychuk et al., 2021). A grasp simulator also used the FEM in Isaac Gym (Kim et al., 2022) with incremental potential contact (IPC) model to handle contact dynamics. Taxim (Si & Yuan, 2022) used a superposition method to approximate the FEM. We also model tactile sensors with FEM to maintain the simulator\u2019s physical accuracy and extend the contact model to handle objects with various materials beyond rigid.\nDifferentiable physics-based simulation Differentiable physics-based simulation has become popular in recent years as it allows for efficient gradient-based policy learning compared to\ntraditional sampling-based algorithms. PlasticineLab (Huang et al., 2021), FluidLab (Xian et al., 2022), SoftZoo (Wang et al., 2023) were presented with differentiability for soft body manipulation, fluid manipulation, and soft robot co-design, respectively, by leveraging Moving Least Square Material Point Method (MLS-MPM) (Hu et al., 2018). Tacchi (Chen et al., 2023) also used MLSMPM to simulate the soft body deformation for GelSight (Yuan et al., 2017), a type of visionbased tactile sensor but did not present differentiability and contact dynamics modeling. It is shown that differential physics can be applied for system identification (Ma et al., 2023) to finetune the simulator\u2019s physical parameters and reduce the sim-to-real gaps. However, it remains unclear whether the gradient-based approach can benefit to improve the efficiency of tactile-assisted manipulation skill learning.\nOptical Simulation Taxim (Si & Yuan, 2022) showed that data-driven approaches to simulate the optical response of vision-based tactile sensors significantly outperform model-based methods such as (Wang et al., 2022; Chen et al., 2023; Agarwal et al., 2021; Gomes et al., 2021). However, there is a divergence in data-driven approaches. Previous work including (Higuera et al., 2023; Chen et al., 2022; Zhong et al., 2023) use image generation techniques like generative models to perform style transfer from a simulated image to the style of a real deformation. However, these methods are rather data-intensive since they need a large variation of real-world examples to generalize well. Instead, Taxim (Si & Yuan, 2022) takes a pixel-based approach that uses a polynomial lookup table to map surface normals to RGB directly. It is more data-efficient but makes assumptions about the sensors bidirectional reflectance distribution function (BRDF), which limits its applicability to sensors with low spatial variance.\nWe compare our work with state-of-the-art tactile simulators in Table 1. We show that our work, to the best of our knowledge, is the only work that is 1) system-wise differentiable to enable efficient skill learning, 2) can accurately model the soft body dynamics and contact dynamics, 3) supports broad categories of objects including rigid, elastic, elastoplastic, and cables, and 4) provide a dataefficient approach to simulate optical responses for vision-based tactile sensors."
        },
        {
            "heading": "3 TACTILE SIMULATION",
            "text": ""
        },
        {
            "heading": "3.1 SYSTEM OVERVIEW",
            "text": "DIFFTACTILE models the soft contact between tactile sensors and objects including contact force distribution, contact surface deformation, and optical response to provide dense tactile feedback. We present four key modules of our system: 1) a Finite Element Method (FEM)-based tactile sensor model in Section 3.2, 2) a learning-based method to simulate the optical response of tactile sensors with high spatial variation in Section 3.3, 3) rigid, elastic, and elastoplastic object models using Moving Least Square Material Point Method (MLS-MPM), and cable model using Position-Based Dynamics (PBD) in Section 3.4, 4) a penalty-based contact model in Section 3.5."
        },
        {
            "heading": "3.2 TACTILE SENSOR SIMULATION",
            "text": "We model the deformation of the tactile sensor\u2019s soft elastomer under contact forces with FEM. We discretize the sensor soft elastomer to tetrahedron elements and then apply boundary conditions at the base of the sensor with position or velocity control. Since most tactile sensors\u2019 elastomers including ours are made from hyper-elastic materials, we apply the Neo-Hookean constitutive model in our simulation to capture the non-linearity of the material property. The energy density function \u03a8 and the first Piola-Kirchhoff stress tensor P used for governing equations are defined as:\n\u03a8(I1, J) = \u00b5\n2 (I1 \u2212 3)\u2212 \u00b5log(J) +\n\u03bb 2 log2(J)\nP(F) = \u00b5(F\u2212 F\u2212T) + \u03bblog(J)F\u2212T (1)\nwhere F \u2208 R3\u00d73 is the deformation gradient, I1 = tr(FTF) is the first isotropic invariants, and J = det(F) is an additional invariant. Note that our tactile simulation can be easily customized with different shapes, sizes, and materials by replacing the input mesh model or constitutive model.\nTo get tactile outputs including visual images and marker motions for vision-based tactile sensors, we first extract the deformed surface mesh from each simulation step\u2019s FEM solution, then we interpolate the marker\u2019s locations by weighting surface node locations given a set of initial markers captured from a real sensor. We project 3D markers to the 2D image plane by using the tactile sensor\u2019s camera model."
        },
        {
            "heading": "3.3 OPTICAL SIMULATION",
            "text": "We reconstruct the optical response of a vision-based tactile sensor to contact using a data-driven approach. We model the surface of the sensor as a height function z = f(x, y), and represent the continuous spatially-varying reflectance function of the surface as a 4D vector-valued function. The function input is the 2D viewing direction (d = \u03b8, \u03c6) and 2D surface normals (x = \u2202f\u2202x , \u2202f \u2202y ), and the output is the change in reflected color c = (r, g, b). We approximate our reflectance function with a multilayer perceptron (MLP) f\u03b8 whose input is augmented with a positional encoding \u03b3(d) and \u03b3(x) rather than directly d and x to enable the network to better fit data with high-frequency variation (Mildenhall et al., 2021). Formally the encoding function is:\n\u03b3(p) = sin(20\u03c0p), cos(20\u03c0p), ..., sin(2L\u22121\u03c0p), cos(2L\u22121\u03c0p) (2)\nOur rendering scheme finally consists of approximating the deformation caused by the contact indentation using pyramid Gaussian kernels as proposed in (Si & Yuan, 2022)."
        },
        {
            "heading": "3.4 OBJECT SIMULATION",
            "text": "We aim to support broader categories of objects beyond rigid objects for more diverse manipulation applications. We leverage the Moving Least Square Material Point Method (MLS-MPM) (Hu et al., 2018) to simulate rigid, elastic, elastoplastic objects. MLS-MPM has been shown to be efficient in simulating soft bodies. For elastic objects, we implement both corotated linear elasticity and NeoHookean elasticity models. For elastoplastic objects, we use the von Mises yield criterion to model plasticity upon elasticity. For rigid objects, we first treat objects as elastic using MLS-MPM, and then we add rigidity constraints by calculating object transformation and enforcing the shape of the object. For articulated objects, we approximate the simulation by using the MPM-based approach and assign different materials for different parts. The joints are simulated as soft and thin bodies and other parts are simulated as rigid bodies.\nFor another group of deformable objects such as cables and clothes, it is common to simulate them with Position Based Dynamics (PBD) (M\u00fcller et al., 2007). We also incorporate cable objects in our simulation by using PBD, where we constrain the stretch, bending, and self-collision."
        },
        {
            "heading": "3.5 PENALTY-BASED CONTACT MODEL",
            "text": "We handle contact dynamics between sensors and objects with a penalty-based contact model similar to (Xu et al., 2023). At each simulation step, we first check contact collision by pairing the surface\ntriangle mesh from FEM with surface nodes from the object\u2019s particles (with either MPM or PBD). For each pair, we calculate the sign distance field d and normal directions n from the node to the triangle mesh. If d is negative, the node is penetrating the surface mesh and we need to apply normal penalty force to both mesh nodes and particle node to constrain the contact. In addition, we apply static or dynamic friction forces to the pair based on their relative velocities and normal forces. We represent our contact model as:\nfn = \u2212(kn + kdvn)dn ft = \u2212 vt\n||vt|| min(kt||vt||, \u00b5||fn||)\n(3)\nwhere fn and ft are contact forces in the normal and tangential direction with respect to the local surface triangle. vn and vt are the relative velocities between the pair of the triangle and node in normal and tangential directions. kn, kd, kt and \u00b5 are the parameters of contact stiffness, contact damping, friction stiffness, and friction coefficient. Then the contact force f = fn + ft is applied to both the triangle mesh nodes and the particle node of the pair as an external force.\nFEM-MPM coupling FEM is a mesh-based method and we can extract surface triangle meshes along with their associated node positions, velocities and face normal directions. MLS-MPM is a meshless hybrid Lagrangian-Eulerian method that uses Lagrangian particles and Eulerian grids to simulate continuous materials. We apply contact collision checking and contact force modeling between FEM surface mesh nodes and MPM Eulerian grids for efficiency.\nIn each simulation step, we first pre-compute the internal elastic forces for all tetrahedral meshes from the constitutive law for the FEM sensor model, and advance particles to grids for the MPM object model. Then we check contact collision and calculate external contact forces for all pairs of triangle meshes and grid, and add them to the surface nodes. In post-contact computing, we transfer the velocities and affine coefficients from the grid to particles and do particle advection for the MPM object model; and we advect the positions and velocities of the nodes based on the internal elastic forces, external contact forces, and gravity for FEM elements. We also consider the external boundaries such as tables and walls to constrain the positions of the objects.\nFEM-PBD coupling Similarly to FEM-MPM coupling, we simply replace the MPM particles with PBD particles for contact collision detection and modeling. For PBD objects, there\u2019s no precontact computation, but we need to solve the stretch, bending, and self-collision constrains after the contact, and velocity advection based on the updated positions."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 OVERVIEW",
            "text": "We present two sets of tasks with DIFFTACTILE: system identification, and tactile-assisted manipulation. For system identification, we use real-world tactile observations to optimize the simulator\u2019s system parameters and to reduce sim-to-real gaps. Then we present five manipulation tasks: grasping, surface following, cable straightening, case opening, and object reposing as shown in Fig. 2. Tactile sensing can enable safer and more adaptive grasping to handle fragile objects such as fruits. We grasp a diverse set of objects with various shapes, sizes, and materials without slipping and damaging. For the other four contact-rich manipulation tasks, surface following requires the sensor to stay in contact with a 3D surface and travel to an endpoint while maintaining a certain contact force; cable straightening requires a pair of sensors to first grasp a fixed end of the cable, and then straighten it by sliding towards the other end; case opening uses a single sensor to open an articulated object via pushing; lastly, object reposing involves using a single sensor to push an object from a lying pose to a standing pose against the wall. These four tasks represent rigid, deformable, and articulated object manipulation."
        },
        {
            "heading": "4.2 SIMULATION SETUP",
            "text": "Initialization We initialize the simulation environment with a single tactile sensor s for system identification, surface following, case opening, and object reposing, and two tactile sensors {s1, s2} mounted on a parallel jaw gripper for grasping and cable straightening. Both tactile sensors\u2019 and\nobjects\u2019 shapes are initialized with STL or OBJ mesh models and then voxelized to FEM tetrahedron meshes or MPM/PBD particles. Objects oi are initialized statically on the tabletop and we add a vertical wall for object reposing. Tactile sensors are initialized statically near objects depending on tasks but without contact. We initialize the poses of tactile sensor at time step t = 0 as Ts(0) = (Rs(0), ts(0)) \u2208 SE(3) where Rs(0) \u2208 SO(3) and ts(0) \u2208 R3 and similarly object pose as To(0). State Each tactile sensor s is represented as an FEM entity with N nodes and M tetrahedral elements. For each node ni, it contacts a 6D state vector si(t) = {pi(t), vi(t)} including a 3D position pi(t) and a 3D velocity vi(t). For each element mi, it contacts a 4D index mapping from the element to its associated four nodes. Both MPM-based and PBD-based objects are represented with particles, and similarly, each particle oi also has a 6D state vector oi(t) = {pi(t), vi(t)}. Observation We define two types of observations of each simulation step t, the state observation and the tactile observation. State observation includes tactile sensors\u2019 and objects\u2019 poses Ts(t), To(t) and each node\u2019s or particle\u2019s state si(t), oi(t). For tactile observation, we can output the sensor\u2019s surface triangle mesh as a deformation map, the sensor\u2019s surface force distribution, or an aggregated three-axis force vector.\nAction At each time step t, actions for end-effectors (either tactile sensors or gripper with kinematic chains down to tactile sensors) are queried from the controller as represented as a velocity vector vs(t) = {\u2206Rs(t),\u2206ts(t)} to update the velocities of the FEM nodes. Reward/Loss Each task\u2019s reward or loss function is formed differently based on the task objectives. We refer the readers to Section A.4 for more details."
        },
        {
            "heading": "4.3 SYSTEM IDENTIFICATION",
            "text": "Sim-to-real transfer for robot learning has been a long-standing challenge where the gap in between heavily relies on simulation fidelity. To reduce the gap, we leverage differentiable physics to optimize the physical parameters of material and contact models given example data from the real world. Our optimization targets include Lam\u00e9 parameters \u00b5 and \u03bb of the FEM sensor model, and kn , kd , kt , \u00b5 of the contact model. The optimization objectives include the 6-axis force readings and tactile marker readings under four different contact scenarios: pressing, sliding, in-plane twisting, and tilt twisting as shown in Fig. 3.\nExperimental Setup and Dataset We collect sequences of contact data from both the real world and simulation with synchronized control poses and velocities of the sensor.\nAs shown in Fig. 3, there are three types of data sequences, press-slide, press-twist-z (twist along the z-axis), and press-twist-x (twist along the x-axis). For this experiment, the sensor interacts with two surfaces with different frictional properties, acrylic and tape.\nExperimental results We evaluate two sets of experiments: Sim2Sim and Real2Sim where we use simulated data or real data respectively as inputs of the system. We optimize the sensor and contact model parameters with press-slide sequence and test on all three sequences. We compare gradientbased trajectory optimization (Ours) with three baselines, Random, RNN, and CMA-ES as shown in Table 2. Here we use pixel-wise mean squared error (MSE) between predicted and collected tactile\nmarkers as evaluation metrics. For Random, we randomly select parameters within a practical range; for RNN, we input tactile marker readings and force readings and output the predicted system parameters; for CMA-ES, we sample predicted parameters from algorithm\u2019s distribution function. Ours outperforms Random, RNN and CMA-ES on all sequences for both Sim2Sim and Real2Sim.\nWe use the identified tactile sensor parameters from Real2Sim for all following manipulation tasks. However, contact parameters such as the surface friction coefficient also depend on object materials. But these can still serve as good references and we use them by adding randomization based on the identified parameters. For object parameters, we randomize them within a range based on the tactile sensor\u2019s and contact model\u2019s parameters to make sure the system can stably run."
        },
        {
            "heading": "4.4 OPTICAL SIMULATION",
            "text": ""
        },
        {
            "heading": "Experimental setup and dataset",
            "text": "We manually collect 250 example deformations across the entire sensing surface using a 4mm spherical indenter from the real world. The pose of the sphere is manually annotated, and we split the dataset into a training set consisting of 200 examples, with the rest held out for testing.\nExperimental results We test our method against a polynomial table mapping from Taxim (Si & Yuan,\n2022). We use pixel-wise MSE, L1, SSIM, and PSNR as evaluation metrics. As shown in Table 3, our method outperforms Taxim across all metrics. Additionally, we verify the generalization and accuracy of our method by rendering a set of test probes with varying geometry, along with example real-world indentations for comparison in Fig. 4. We show our method can capture contact geometries in great detail."
        },
        {
            "heading": "4.5 GRASPING",
            "text": "Experimental setup and dataset We evaluate our simulator on grasping objects with various object properties including different shapes, sizes, weights, and material properties. As shown in Fig. 2, we select four objects from EGAD (Morrison et al., 2020) dataset with different shape complexity and assign each object with two different material properties, elastic, and elastoplastic.\nWe aim to grasp objects stably and adaptively to avoid slipping and damaging the object with gradient-based trajectory optimization. Here we use two tactile sensors as fingertips and mount them on a parallel jaw gripper. In each trajectory, the gripper first grips the object and then lifts it. Based on our goal, we define the objectives with three types of losses 1) Position loss Lpos: we set a 3D target position to reach after lifting; 2) Deformation loss Ldeform: we aim to keep the shape of the object during the grasp by using the sign distance field of the object and the L1 distance of the mass distribution between the current object and the target one to penalize the deformation (Huang et al., 2021) 3) Slipping loss Lslip: we use the shear force detected between the fingertip and the object to penalize the slippage during grasping.\nExperimental results We evaluate the grasping with or without tactile feedback on three metrics. We use Lpos for both types of objects, and we use Ldeform for elastoplastic objects only. In addition, we measure the slipping distance of the object relative to the sensor for both sets of objects, the slipping distance is denoted as Dslip. We show in Table 4 that the tactile feedback greatly improves the grasping quality."
        },
        {
            "heading": "4.6 CONTACT-RICH MANIPULATION",
            "text": "Experimental setup For all four manipulation tasks, we define two different rewards, state reward and tactile reward for manipulation skill learning. We evaluate our system\u2019s learning efficiency by comparing gradient-based trajectory optimization with a sampling-based trajectory optimization, CMA-ES (Hansen et al., 2003), and model-free RL algorithms, SAC (Haarnoja et al., 2018), and PPO Schulman et al. (2017).\nSurface following We set up a sensor to travel and follow a curved 3D surface. We define the state reward as traveling to a certain position on the 3D surface, and the tactile reward as keeping contact with the surface while maintaining a constant shear motion.\nCable straightening We set up a parallel jaw gripper with two tactile fingers and a cable with one end fixed to the wall while the other end is free. The state reward is defined as the distance between the target position (the cable is horizontally straight) and the current position for each node on the\ncable. The tactile reward is defined as the force applied to the cable to maintain the gripping while being able to slide along the cable.\nCase opening We initialize a closed case and we use a tactile sensor to push and open the lid of the case. We define the state reward as the angle of the opened lid and the tactile reward as the push forces to open the lid.\nObject reposing A block is placed flat on the table and we aim to use one tactile sensor to flip it 90 degrees and make it stand against a wall. We define the state reward as the angle between the object and the floor, and the tactile reward as the push forces to flip the object.\nExperimental Results To evaluate the performance of trained policies for different tasks, we design task-specific evaluation metrics: We use the traveling distance of the sensor in contact with the surface for the surface following task; the aggregation distance between the current and target cable nodes\u2019 locations for the cable straightening task; the orientation changes of the lid and the object from the beginning to the end of the trajectories for case opening and object reposing tasks.\nWe show all experimental results in Table 5 by comparing our proposed gradient-based optimization method with baselines. We show Ours outperforms baselines with a large margin to show its learning efficiency. And w/ tactile has better performances compared to w/o tactile for most tasks indicating tactile sensing helps on these contact-rich manipulation tasks."
        },
        {
            "heading": "5 CONCLUSIONS AND FUTURE WORK",
            "text": "We present DIFFTACTILE, a physics-based differentiable tactile simulator to advance skill learning for contact-rich robotic manipulation. By providing models for tactile sensors, multi-material objects, and penalty-based contacts, we greatly extend the capabilities and applicability of robotic simulators. The differentiability of our system aids in reducing the sim-to-real gaps by using system identification and improves the skill learning efficiency by providing gradient-based optimization. We evaluate DIFFTACTILE \u2019s versatility with the grasp of a set of various objects, and manipulation tasks including surface following, cable straightening, case opening, and object reposing. By comparing with the state-of-the-art reinforcement learning and sample-based trajectory optimization approaches, we demonstrate that DIFFTACTILE can enable efficient skill learning with tactile sensing and potentially serve as a learning platform for broader tactile-assisted manipulation tasks.\nIn future work, we plan to integrate our tactile simulator into commonly used robotic simulation frameworks to extend its usage on more general manipulation configurations such as adding tactile sensors on dexterous robotic hands for in-hand manipulation. We would also like to investigate robot learning with multi-modalities in simulation such as leveraging vision and touch feedback to improve the robustness of the policies."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 SIMULATION DETAILS",
            "text": "We implement our whole system with Taichi (Chen et al., 2023) along with Python to benefit from its high computing performance and auto-differentiability. With Taichi, our system can switch between running with CPU or being accelerated by GPU by simply passing an argument to initialize the Taichi environment. Taichi also supports automatic differential features for functions with explicit time integration. Therefore, considering the implementation difficulty and generalizability, our system is implemented with semi-explicit time integration, and without any extra effort, is fully differentiable and can be used for gradient-based trajectory optimization. The simulation pipeline for each simulation step can be seen in Fig. 6."
        },
        {
            "heading": "A.2 SYSTEM IDENTIFICATION DETAILS",
            "text": "Real-world data collection We collect sequences of contact data from the real world including the 6-axis force readings from a robot arm end-effector, the poses of a Gelsight tactile sensor, and the corresponding tactile images from the tactile sensor. We set up the experiment by mounting a GelSight tactile sensor on the end-effector of an Ur5e robot arm and then controlling the robot arm to get the sensor in contact with a tabletop surface. As discussed in (Yuan et al., 2017), four general contact patterns are essential to capture and simulate for tactile sensors including contact under normal force, shear force, in-plane torque, and tilt torque. Therefore we collect three types of sequences of contact data: press-slide, press-twist-z, and press-twist-x. For each sequence, we start by pressing the sensor normally to a flat surface with a constant velocity of 1 mm/s for 10 seconds to get in contact. Then we slide the sensor along the surface, twist it along the normal direction, or twist it along a horizontal direction to finish press-slide, press-twist-z, and press-twist-x respectively with a constant velocity of 1 mm/s or 2 degrees/s for 10 seconds.\nGradient-based estimation We define the losses including the pixel-wised tactile marker distances as the tactile loss and three-axis force errors as the force loss between the simulated and ground truth data. Since the two losses are on different numerical scales, we aggregate them by scaling with weights 10:1 as the final loss. We use Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.999. Learning rate parameters are lrkn = 20.0, lrkd = 20.0, lrkt = 5.0, lrfc = 5.0, lr\u00b5 = 50.0, lr\u03bb = 50.0 depending on their numerical scales. We run 100 optimization steps for each trajectory.\nRNN-based estimation We use the Long Short-Term Memory (LSTM) model as the network architecture. The inputs of the LSTM module are with the size of 2 \u00d7 136 + 3 = 275, where we use 136 tracked markers\u2019 2D motions from tactile images, and three-dimensional aggregated contact forces. We set the hidden layer size to 256 and used the default settings for other parameters. We use\na linear layer after the LSTM module with an input size of 256 and an output size of 6, to predict the six parameters of the sensor material and the contact model. We generate a simulated dataset that includes tactile marker readings, and three-axis contact force readings based on randomized system parameters. Our dataset has 2010 samples, 1800 for training, 200 for validation, and 10 for testing. Each sample\u2019s system parameters are randomized within pre-determined ranges, which ensures their real-world applicability as shown in Table 6. The model was trained in batch size of 32 for 3000 epochs, and using an Adam optimizer with a learning rate of 0.001."
        },
        {
            "heading": "Parameter Lower Upper",
            "text": "Random estimation We also provide a random estimation as our baseline. We use 10 sets of randomized system parameters from our dataset and then compare the simulated tactile markers with the ground-truth markers either from the simulation or the real world.\nExperimental details We evaluate the system identification with simulation-to-simulation (sim2sim) and real-to-simulation (real2sim). For sim2sim, we create a dataset by randomly sampling ten sets of parameters within appropriate ranges and simulating their corresponding tactile marker data. We validate different methods including baselines and ours by predicting parameters on this dataset. The Mean Squared Error (MSE) of the marker positions between the ground truth and the simulated ones with estimated parameters is used as the evaluation metric. We compute the mean and standard deviation of these ten average marker errors for the entire trajectory.\nFor real2sim, we collect the dataset from the real world and apply different methods including baselines and ours to estimate the parameters. Then we simulate tactile markers using the optimized parameters and evaluate the performance by computing the mean and standard deviation of the marker errors between the actual and simulated tactile marker data."
        },
        {
            "heading": "A.3 OPTICAL SIMULATION DETAILS",
            "text": ""
        },
        {
            "heading": "A.3.1 NETWORK ARCHITECTURE",
            "text": ""
        },
        {
            "heading": "A.3.2 TRAINING DETAILS",
            "text": "In our experiments we optimize our model using ADAM optimizer with a fixed learning rate of 1e-5 for 500 epochs. Each batch consists of all the data from a single example image. Training takes approximately 45 minutes for 200 examples."
        },
        {
            "heading": "A.4 DIFFTACTILE TASK AND EVALUATION DETAILS",
            "text": ""
        },
        {
            "heading": "A.4.1 TASK SETUP DETAILS",
            "text": "Reinforcement Learning (RL) We use object particles\u2019 state vector oi(t) and tactile sensor\u2019s pose Ts(t) as state observations. Additionally, we use tactile markers\u2019 position in 2D image mi = (ui, vi), three-axis contact force F (t) = (Fx, Fy, Fz), and contact location center l(t) by averaging all in-contact nodes\u2019 locations as tactile observations. Given that the total number of markers is 136, we downsample the number of object particles to four times of the number of markers, ensuring a balanced dimensionality across different input segments. The input vector is formulated as either only state observation or with additional tactile observation. Then it is fed into a Multi-Layer Perceptron (MLP) policy network.\nWe use stable-baseline3 (Raffin et al., 2021)\u2019s default PPO and SAC as our policy networks. Given an initial trajectory which is the same for all baseline methods, the policy network takes the input vector and outputs an action \u2206vs(t) of the sensor for each time step. Then we update the sensor\u2019s velocity as vs(t)+ = \u2206vs(t). We constraint the actions in the range of [\u22120.15, 0.15] for a reasonable action size.\nCMA-ES In each optimization step, we generate 20 new trajectories based on the current trajectory with a standard deviation of 0.15 for a fair comparison with RL. We evaluate each new trajectory\u2019s loss and then update the policy based on the evaluation. This then informs the generation of the next optimization step\u2019s 20 trajectories. We used the same initial trajectory as RL and ran 100 optimization steps in total for each task.\nGradient-based Optimization (Ours) , In each optimization step, we forward the simulation and calculate the defined loss, and then backpropagate the gradients from the loss to the target optimization variables. We then update the target variables with Adam optimizer. To enhance optimization efficiency, we use different learning rates for different optimization variables. The hyper-parameters can be found in Table 7, where lrp is the learning rate for translation, lro is the learning rate for orientation, and lrw is the learning rate for the gripper\u2019s width. Note that for tasks where we use a single tactile sensor, the value of lrw is listed as N/A."
        },
        {
            "heading": "A.4.2 LOSS AND REWARD",
            "text": "During training, we assign task-specific weights to state and tactile losses, denoted as \u03b1 and \u03b2. The final loss is then calculated as Ltotal = \u03b1 \u00d7 Lstate + \u03b2 \u00d7 Ltactile. The task-specific values for \u03b1 and \u03b2 are provided in Table 8. We use the losses discussed in Section 4.5 and Section 4.6 for optimization-based methods including our gradient-based method and CMA-ES; for model-free RL algorithms, we subtract the cumulative loss of two consecutive steps to obtain each step\u2019s loss, and then calculate the reward to fit the settings of RL algorithms."
        },
        {
            "heading": "A.4.3 METRICS DETAILS",
            "text": "We design task-specific metrics for evaluations. Metrics\u2019 mathematical formulas are shown in Table 10.\nSurface Follow We evaluate the continuous in-contact distance dcontact(t) the sensor travels on the surface within a fixed timestep span. Here, a longer distance means better results.\nCable Straighten Our metric is the average displacement of each particle i on the cable from its target horizontal position ||pi(t)\u2212 pi(target)||. A smaller value indicates a more desired result. Case Open We calculate the opened angle in degrees between the case lid and the horizontal tabletop \u03b8(t). The opened angle, due to gravity, can potentially show a negative value if the training results are suboptimal. Therefore, a larger value suggests better performance.\nObject Repose We measure the rotated angle in degrees of the object \u03b8(t) from its initial pose. A larger value in this context indicates better performance."
        },
        {
            "heading": "A.4.4 LOSS DETAILS",
            "text": "We design different losses used for different tasks to obtain state or tactile reward, shown in Table 11.\nGrasping The losses we used are defined in Section 4.5. \u03b3 and \u03b7 are set to 0.1.\nSurface Follow We use Lpos as the state loss and Lforce as the tactile loss.\nCable Straighten We use Lcable as the state reward which is the sum of the distance between the target position and the current position for each node on the cable. Tactile loss comprises Lforce + Lloc.\nCase Open & Object Repose We use Langle as the state loss and Lforce as the tactile loss."
        },
        {
            "heading": "A.4.5 PARAMETER DETAILS",
            "text": "We apply optimized parameters including FEM-based sensor\u2019s Lam\u00e9 parameters \u00b5 and \u03bb in manipulation tasks since we only use one kind of tactile sensor. Other parameters vary depending on tasks since different tasks use different objects. Sensor-related parameters are shown in Table 12. Object simulation parameters for different tasks are shown in Table 13, where Sobj is the object scale compared to the grid size in MLS-MPM, \u03c1 is the density, Np is the number of particles in one dimension of the space, \u00b5 and \u03bb are Lam\u00e9 parameters, and \u03c3 is the yield stress for the elastoplastic\nobject. For the articulated object, we list the Lam\u00e9 parameter for parts from the top to the bottom of the object."
        },
        {
            "heading": "A.4.6 TRAJECTORY OPTIMIZATION TRAINING LOSS CURVES",
            "text": "We show the trajectory optimization training loss curves for four contact-rich manipulation tasks in Fig. 7. For each task, we set 100 optimization iterations for fair comparison. From the curves, we show state + tactile settings converge faster than the state-only settings which indicates the benefits of using tactile information on these manipulation tasks."
        },
        {
            "heading": "A.4.7 DISCUSSIONS ON CONTACT-RICH MANIPULATION TASKS",
            "text": "For RL algorithms, both PPO and SAC\u2019s losses did not decrease in 100 iterations. There are three potential reasons: 1. The amount of data we used is insufficient for RL algorithms to learn within 100 iterations. 2. Our tasks include continuous contact, whereas RL algorithms operate on a discretized per-small-time-step basis, making it challenging to optimize. 3. RL algorithms in general require more detailed reward designs while the current reward functions are too simple to train proper policies.\nFor CMA-ES, we find that training losses decreased slowly. This is because CMA-ES is a samplebased optimization method and it is not as efficient as the gradient-based optimization method. It\ndoes show the ability to optimize the trajectory but requires more than 100 iterations of optimization to converge.\nThus, we can conclude that our proposed gradient-based optimization method with differential physics has these merits: 1) Better data usage efficiency, 2) faster converge speed with the guidance of gradients, and 3) simpler loss function design. We additionally visualize the failure cases of RL algorithms and CMA-ES on our project website."
        },
        {
            "heading": "A.5 COMPUTATIONAL RUNTIME OF THE SYSTEM",
            "text": "We report the averaged computational running speed of our system on four contact-rich manipulation tasks in Table. 14. From the table, we show the simulation speed (Forward) depends on different task settings and is significantly affected by the object simulation. The gradient backpropagation (Backward) speed is twice as slow as the simulation. This is because we optimize our system to be memory efficient. During the forward simulation, we save the states of each first simulation substep, and then during each backward step, we retract them and replay the corresponding forward step to fill in the rest substeps\u2019 states. FEM-based tactile sensor simulation can consistently run at high speed (greater than 30 FPS) even with two sensors in the CableStraighthen task. However, the simulation speed of objects varies such as the multi-material MPM-based object simulation is slower than others in the CaseOpen task, while the PBD-based cable simulation is the fastest. All experiments were conducted on a Ubuntu 18.04 with AMD Ryzen 7 5800x 8-core processor and Nvidia GeForce RTX 3060."
        },
        {
            "heading": "A.6 REAL-WORLD EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A.6.1 EXPERIMENTAL SETUP DETAILS",
            "text": "We use a Gelsight tactile sensor for both system identification tasks and sim-to-real tasks. The sensor is manufactured in the laboratory with design flexibility. The soft elastomer is made with SYLGARD 184 silicone elastomer, and in a dome shape with an inner radius of 7.5 mm and an outer radius of 15.0 mm. The sensor uses an Arducam 180-degree fisheye camera to output tactile images."
        },
        {
            "heading": "A.6.2 EXPERIMENTAL RESULTS",
            "text": "To demonstrate our proposed simulator\u2019s fidelity, we conduct two sets of experiments in the real world. First, we demonstrate that the trajectories optimized with differential physics can be deployed on real-world setups in Fig. 8. Here we show the sim-to-real transfers on SurfaceFollow and CaseOpen tasks. Then we further evaluate with a closed-loop grasp task by only using tactile sensing feedback. We train a grasp stability prediction network in simulation by using a sequence of tactile observations during grasping as inputs and predicting a binary output to indicate whether it is a stable grasp or a slippage. The prediction is then used to guide the grasp adjustment. We directly use the trained model on a real-world deformable object grasp.\nTo train the grasp stability prediction network, we apply domain randomization and generate multiple trajectories in simulation with different parameter settings to improve the generalization of sim2real transfer. The process of trajectory generation is we let the gripper close at the speed of vclose = 5 mm/s until the gripper begins to squeeze the object for Tcontact frames, then we let the gripper lift the object for Tlift frames at the speed of vlift = 1 mm/s. If the slipping distance between the object and the sensor is less than 0.75 mm, we label the trajectory as a stable grasp. The parameters we used are shown in Table 15, where Sobj and \u03c1 are the scale and density of the object. Additionally, the object shape is chosen randomly from the object set of grasping experiments in Section 4.5. For the Lam\u00e9 parameters of the tactile sensor, we use the results from system identification in Section 4.3 The Lam\u00e9 parameters of the object are set as \u00b5 = 1.428e3, \u03bb = 5.741e3. The frequency of the system is 40 Hz.\nWe generate 50 trajectories of stable grasp and 50 trajectories of unstable grasp in total. We split the training/validation set in a ratio of 7:3. We use two LSTM layers and one MLP layer as the network architecture. We use the sequence of tactile markers\u2019 2D motions as inputs for the model. Each frame\u2019s tactile markers\u2019 2D motions are obtained by subtracting the initial marker positions from the marker positions in the current frame. Due to the varying number of frames in different trajectories, we perform zero-padding at the beginning of the trajectories, making the network input size (L, 136 \u00d7 2), where L is the maximum number of frames among these trajectories. After training for 10 epochs, the success rate reaches 94.3% on the training set and 90.0% on the validation set.\nWe present our sim2real adaptive grasp policy for grasping a deformable object. Our goal is to grasp the object with minimal deformation. We applied our trained model directly on the real-world setup to perform an adaptive grasp. We first attempt to grasp and lift the object with minimal force, feeding the sequence of tactile marker motions into our trained model. If the model predicts slippage, we tighten the gripper, or if the model predicts a stable grasp, we continue lifting the object to the determined height. In Fig. 9, we show a comparison of our approach with two baselines: 1) forceful grasp where the gripper tightly grips the object, and 2) light grasp where the gripper lifts the object upon contact. We can see that our method successfully grasps and lifts the deformable object with minimal deformation while the two baselines failed by damaging the object or causing slippage."
        }
    ],
    "title": "DIFFTACTILE: A PHYSICS-BASED DIFFERENTIABLE TACTILE SIMULATOR FOR CONTACT-RICH ROBOTIC MANIPULATION",
    "year": 2024
}