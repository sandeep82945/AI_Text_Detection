{
    "abstractText": "Congestion is a common failure mode of markets, where consumers compete inefficiently on the same subset of goods (e.g., chasing the same small set of properties on a vacation rental platform). The typical economic story is that prices decongest by balancing supply and demand. But in modern online marketplaces, prices are typically set in a decentralized way by sellers, and the information about items is inevitably partial. The power of a platform is limited to controlling representations\u2014 the subset of information about items presented by default to users. This motivates the present study of decongestion by representation, where a platform seeks to learn representations that reduce congestion and thus improve social welfare. The technical challenge is twofold: relying only on revealed preferences from the choices of consumers, rather than true preferences; and the combinatorial problem associated with representations that determine the features to reveal in the default view. We tackle both challenges by proposing a differentiable proxy of welfare that can be trained end-to-end on consumer choice data. We develop sufficient conditions for when decongestion promotes welfare, and present the results of extensive experiments on both synthetic and real data that demonstrate the utility of our approach.",
    "authors": [],
    "id": "SP:ef8986a6d628c02116ddb9f46219dc6e67ab17d0",
    "references": [
        {
            "authors": [
                "George A Akerlof"
            ],
            "title": "The market for \u201clemons\u201d: Quality uncertainty and the market mechanism",
            "venue": "In Uncertainty in economics,",
            "year": 1978
        },
        {
            "authors": [
                "Miguel Alcobendas",
                "Robert Zeithammer"
            ],
            "title": "Adjustment of bidding strategies after a switch to first-price rules",
            "venue": "Available at SSRN 4036006,",
            "year": 2021
        },
        {
            "authors": [
                "Gal Bahar",
                "Omer Ben-Porat",
                "Kevin Leyton-Brown",
                "Moshe Tennenholtz"
            ],
            "title": "Fiduciary bandits",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Gagan Bansal",
                "Besmira Nushi",
                "Ece Kamar",
                "Eric Horvitz",
                "Daniel S Weld"
            ],
            "title": "Is the most accurate AI the best teammate? Optimizing AI for teamwork",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Omer Ben-Porat",
                "Moshe Tennenholtz"
            ],
            "title": "A game-theoretic approach to recommendation systems with strategic content providers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Omer Ben-Porat",
                "Moshe Tennenholtz"
            ],
            "title": "Regression equilibrium",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation,",
            "year": 2019
        },
        {
            "authors": [
                "Allison JB Chaney",
                "Brandon M Stewart",
                "Barbara E Engelhardt"
            ],
            "title": "How algorithmic confounding in recommendation systems increases homogeneity and decreases utility",
            "venue": "In Proceedings of the 12th ACM conference on recommender systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sarah Dean",
                "Sarah Rich",
                "Benjamin Recht"
            ],
            "title": "Recommendations and user agency: the reachability of collaboratively-filtered information",
            "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,",
            "year": 2020
        },
        {
            "authors": [
                "Ulrich Doraszelski",
                "Gregory Lewis",
                "Ariel Pakes"
            ],
            "title": "Just starting out: Learning and equilibrium in a new market",
            "venue": "American Economic Review,",
            "year": 2018
        },
        {
            "authors": [
                "Wenshuo Guo",
                "Kirthevasan Kandasamy",
                "Joseph Gonzalez",
                "Michael Jordan",
                "Ion Stoica"
            ],
            "title": "Learning competitive equilibria in exchange economies with bandit feedback",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "F Maxwell Harper",
                "Joseph A Konstan"
            ],
            "title": "The movielens datasets: History and context",
            "venue": "ACM transactions on interactive intelligent systems (tiis),",
            "year": 2015
        },
        {
            "authors": [
                "Sophie Hilgard",
                "Nir Rosenfeld",
                "Mahzarin R Banaji",
                "Jack Cao",
                "David Parkes"
            ],
            "title": "Learning representations by humans, for humans",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Meena Jagadeesan",
                "Nikhil Garg",
                "Jacob Steinhardt"
            ],
            "title": "Supply-side equilibria in recommender systems",
            "venue": "arXiv preprint arXiv:2206.13489,",
            "year": 2022
        },
        {
            "authors": [
                "Meena Jagadeesan",
                "Michael I Jordan",
                "Nika Haghtalab"
            ],
            "title": "Competition, alignment, and equilibria in digital marketplaces",
            "venue": "In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jon Kleinberg",
                "Sendhil Mullainathan"
            ],
            "title": "Simplicity creates inequity: implications for fairness, stereotypes, and interpretability",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation,",
            "year": 2019
        },
        {
            "authors": [
                "Ilan Kremer",
                "Yishay Mansour",
                "Motty Perry"
            ],
            "title": "Implementing the \u201cwisdom of the crowd",
            "venue": "Journal of Political Economy,",
            "year": 2014
        },
        {
            "authors": [
                "Anilesh K Krishnaswamy",
                "Haoming Li",
                "David Rein",
                "Hanrui Zhang",
                "Vincent Conitzer"
            ],
            "title": "Classification with strategically withheld data",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2024
        },
        {
            "authors": [
                "Yishay Mansour",
                "Aleksandrs Slivkins",
                "Vasilis Syrgkanis"
            ],
            "title": "Bayesian incentive-compatible bandit exploration",
            "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,",
            "year": 2015
        },
        {
            "authors": [
                "Martin Mladenov",
                "Elliot Creager",
                "Omer Ben-Porat",
                "Kevin Swersky",
                "Richard Zemel",
                "Craig Boutilier"
            ],
            "title": "Optimizing long-term social welfare in recommender systems: A constrained matching approach",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Vineet Nair",
                "Ganesh Ghalme",
                "Inbal Talgam-Cohen",
                "Nir Rosenfeld"
            ],
            "title": "Strategic representation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Gali Noti",
                "Yiling Chen"
            ],
            "title": "Learning when to advise human decision makers",
            "venue": "arXiv preprint arXiv:2209.13578,",
            "year": 2022
        },
        {
            "authors": [
                "Mark O Riedl"
            ],
            "title": "Human-centered artificial intelligence and machine learning",
            "venue": "Human Behavior and Emerging Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Sven Schmit",
                "Carlos Riquelme"
            ],
            "title": "Human interaction with recommendation systems",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Lloyd S Shapley",
                "Martin Shubik"
            ],
            "title": "The assignment game I: The core",
            "venue": "International Journal of game theory,",
            "year": 1971
        },
        {
            "authors": [
                "Behzad Tabibian",
                "Vicen\u00e7 G\u00f3mez",
                "Abir De",
                "Bernhard Sch\u00f6lkopf",
                "Manuel Gomez Rodriguez"
            ],
            "title": "Consequential ranking algorithms and long-term welfare",
            "year": 1905
        },
        {
            "authors": [
                "Tim Vieira"
            ],
            "title": "Gumbel-max trick and weighted reservoir sampling, 2014. URL http://timv ieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-wei ghted-reservoir-sampling",
            "year": 2014
        },
        {
            "authors": [
                "Fang Wu",
                "Joffre Swait",
                "Yuxin Chen"
            ],
            "title": "Feature-based attributes and the roles of consumers\u2019 perception bias and inference in choice",
            "venue": "International Journal of Research in Marketing,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Congestion is a common failure mode of markets, where consumers compete inefficiently on the same subset of goods (e.g., chasing the same small set of properties on a vacation rental platform). The typical economic story is that prices decongest by balancing supply and demand. But in modern online marketplaces, prices are typically set in a decentralized way by sellers, and the information about items is inevitably partial. The power of a platform is limited to controlling representations\u2014 the subset of information about items presented by default to users. This motivates the present study of decongestion by representation, where a platform seeks to learn representations that reduce congestion and thus improve social welfare. The technical challenge is twofold: relying only on revealed preferences from the choices of consumers, rather than true preferences; and the combinatorial problem associated with representations that determine the features to reveal in the default view. We tackle both challenges by proposing a differentiable proxy of welfare that can be trained end-to-end on consumer choice data. We develop sufficient conditions for when decongestion promotes welfare, and present the results of extensive experiments on both synthetic and real data that demonstrate the utility of our approach."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Online marketplaces have become ubiquitous as our primary means for consuming tangible goods as well as services across many different domains. Examples span a variety of commercial segments, including dining (e.g., Yelp), real estate (e.g., Zillow), vacation homestays (e.g., Airbnb), used or vintage items (e.g., eBay), handmade crafts (e.g., Etsy), and specialized freelance labor (e.g., Upwork). A key reason underlying the success of these platforms is their ability to manage an exceptionally large and diverse collections of items, to which users are given immediate and seamless access. Once a desired item has been found on a platform then obtaining it should\u2014in principle\u2014be only \u2018one click away.\u2019\nBut just like conventional markets, online markets are also prone to certain forms of market failure, which may hinder the ability of users to easily obtain valued items. One prominent type of failure, which our paper targets, is congestion. Congestion occurs when demand for certain items exceeds supply; i.e., when multiple users are interested in a single item of which there are not sufficiently-many copies available. E.g., in vacation rentals, the same vacation home may draw the interest of many users, but only one of them can rent it. This can prevent potential transactions from materializing, resulting in reduced social welfare\u2014to the detriment of users, suppliers, and the platform itself.\nIn conventional markets, the usual economic response to congestion is to set prices in an appropriate manner (e.g., [25]). In our example, if the attractive vacation home is priced correctly, then only one user (who values it most, relative to other properties) will choose it; similarly, if other items are also priced correctly in relation to user valuations, then prices can fully decongest the market and the market can obtain optimal welfare, defined as the sum of users\u2019 valuations to their assigned items.\nBut for modern online markets, this approach is unattainable for two reasons. The first reason is that many online platforms do not have control over prices, which are instead set in a decentralized way by different sellers. The second reason is more subtle, but central to the solution we advance in this paper: we argue that an inherent aspect of online markets is that users make choices under limited information, and that this limits the effectiveness of price. Online environments impose natural constraints on the amount of information consumed, due to technical limitations (e.g., restricted screen space), behavioral mechanisms (e.g., cognitive capacity, attention span, impatience), or design choices (e.g.,\nwhat information is highlighted, appears first as a default, or requires less effort to access). As such, the decisions of users are more affected by whatever information is made more readily available to them. From an economic perspective, this means they are making decisions under \u2018incorrect\u2019 preference models, for which (i) prices that decongest (or clear) at these erroneous preferences are incorrect, and (ii) prices that decongest at correct preferences still leave congestion at erroneous preferences.1\nPartial information is therefore a reality that platforms must cope with\u2014a new reality which requires new approaches. To ease congestion and improve welfare, our main thesis is that platforms can\u2014and should\u2014utilize their control over information, and in particular, on how items are represented to users. The decision of representation\u2014the default way in which items are shown to users\u2014is typically in the hands of the platform; and while providing equal access to all information may be the ideal, reality dictates that choosing some representation is inevitable.2 Given this, we propose to use machine learning to solve the necessary design problem of choosing beneficial item representations.\nTo this end, we present a new framework for learning item representations that reduce congestion and promote welfare. Since congestion results from users making choices independently according to their own individual preferences, to decongest, the platform must act to (indirectly) coordinate these idiosyncratic choices; and since representations affect choices by shaping how users \u2018perceive\u2019 value, we will seek to coordinate perceived preferences. The basic premise of our approach is that, with enough variation in true user preferences, it should be possible to find representations for which choices made under perceived values remain both valuable and diverse. For example, consider a rental unit represented as having \u2018sea view\u2019 and \u2018sunny balcony\u2019 and draws the attention of many users but does not convey other information such as \u2018noisy location\u2019; if users vary enough in how they value quietness, then showing \u2018quiet\u2019 instead of \u2018balcony\u2019 may help reduce congestion and improve outcomes.\nFrom a learning perspective, the fundamental challenge is that welfare itself (and its underlying choices) depends on private user preferences. For this, we develop a proxy objective that relies on observable choice data alone, and optimizes for representations that encourage favorable decongested solutions through users\u2019 choices. A technical challenge is that representations are combinatorial objects, corresponding to a subset of features to show. Building on recent advances in differentiable discrete optimization, we modify our objective to be differentiable, thus permitting end-to-end training using gradient methods. To provide formal grounding for our approach of decongestion by representation, we theoretically study the connection between decongestion and welfare. Using competitive equilibrium analysis, we give several simple and interpretable sufficient conditions under which reducing congestion provably improves welfare. Intuitively, this happens when it is possible to present item features across which user preferences are more diverse, while at the same time hiding features that are not too meaningful for the users. The conditions provide basic insight as to when our approach works well.\nWe also report the results of an extensive set of experiments that shed light on our proposed setting and learning approach. We first make use of synthetic data to explore the usefulness of decongestion as a proxy for welfare, considering the importance of preference variation, the role of prices, and the degree of information partiality. We then use real data of user ratings to elicit user preferences across a set of diverse items. Coupling this with simulated user behavior, we demonstrate the susceptibility of na\u00efve prediction-based methods to harmful congestion, and the ability of our congestion-aware representation learning framework to improve economic outcomes. All code will be made publicly available."
        },
        {
            "heading": "1.1 RELATED WORK",
            "text": "There is a growing recognition that many online platforms provide economic marketplaces, and considerable efforts have been dedicated to studying the role of recommender systems in this regard [7,24,26]. Some work, for example, has studied the effects of learning and recommendation on the equilibrium dynamics of two-sided markets of users and suppliers [5,20,13], exchange markets [10], or markets of competing platforms [6,14]. The main distinction is that our paper studies not what to show to users, but how. One study examined the effect of the complete absence of knowledge about some items on welfare [8]; in contrast, we study how welfare is affected by partial information about items. There are also studies on the role of information in the form of recommendations in enhancing system-wide performance with learning users; e.g., the use of selective information reporting to promote exploration by a group of users [17,19,3]. Again, this is quite distinct from our setting.\n1Economic theory has many examples of other ways in which partial information hurts markets (e.g., [1]). 2In the influential book \u2018Nudge\u2019, Thaler and Sunstein (2008) argue similarly for \u2018choice architecture\u2019 at large.\nConceptually related to our work is research in the field of human-centered AI [23] that studies AI-aided human decision making, and in particular prior work that has considered methods to learn representations of inputs to decision problems to aid in single-user decision making [12]. Related, there is work on selectively providing information in the form of advice to a user in order to optimize their decision performance [22]. It has also been argued that providing less accurate predictive information to users can sometimes improve performance [4]. These works, however, do not consider interactions between multiple users which are at the center of the types of markets we consider here.\nThough underexplored in online markets, several works in related fields have considered how representations affect decisions. For example, [16] aim to establish the role of \u2018simplicity\u2019 in decision-making aids, and in relation to fairness and equity. Works in strategic learning have emphasized the role of users in representations; i.e., in learning to choose in face of strategic representations [21], and as controlling representations themselves [18]. Here we extend the discussion on representations to markets."
        },
        {
            "heading": "2 PROBLEM SETUP",
            "text": "The main element of our setting is a market, where each market is composed of m indivisible items and n users. Within a market, items j are described by non-negative feature vectors xj \u2208 Rd+ and prices pj \u2265 0. Let X \u2208 Rm\u00d7d denote all item features, and p \u2208 Rm denote all prices, which we assume to be fixed.3 We mostly consider unit supply, in which there is only one unit of each item (e.g., as in vacation homes), but note our method directly extends to general finite supply, which we discuss later.\nEach user i in a market has a valuation function, vi(x), which determines their true value for an item with feature vector x. We use vij to denote user i\u2019s value for the jth item. We model each user with a non-negative, linear preference, with vi(x) = \u03b2\u22a4i x for some user type, \u03b2i \u2265 0. The effect is that vi(x) \u2265 0 for all items, and all item attributes contribute positively to value. We assume w.l.o.g. that values are scaled such that vi(x) \u2264 1. Users have unit demand, i.e., are interested in purchasing a single item. Given full information on items, a rational agent would choose y\u2217i = argmaxj vij \u2212 pj . Partial information. The unique aspect of our setup is that users make choices on the basis of partial information over which the system has control. For this, we model users as making decisions on an item with feature vector x based on its representation z, which is truthful but lossy: z must contain only information from x, but does not contain all of the information. We consider this to be a necessary aspect of a practical market, where users are unable to appreciate all of the complexity of goods in the market. Concretely, z reveals a subset of k \u2264 d features from x, where the set of features is determined by a binary feature mask, \u00b5 \u2208 {0, 1}d, with |\u00b5|1 = k, that is fixed for all items. Each mask induces perceived values, v\u0303, which are the values a user infers from observable features:\nv\u0303i(x) = \u03b2 \u22a4 i (x\u2299 \u00b5) = (\u03b2i)\u22a4\u00b5 z, (1)\nwhere \u2299 denotes element-wise product, and (\u03b2)\u00b5 is \u03b2 restricted to features in \u00b5. For market items xj we use v\u0303ij = \u03b2\u22a4i (xj \u2299 \u00b5). Given this, under partial information, user i makes choices yi via:4\nyi(\u00b5) = choice(X, p; vi\u00b5) := argmaxj v\u0303ij \u2212 pj , (2)\nwhere v\u0303ij \u2212 pj is agent i\u2019s perceived utility from item j, with yi(\u00b5) = 0 encoding the \u2018no choice\u2019 option, which occurs when no item has positive perceived utility. Note Eq. (2) is a (simple) special case of [29]. When clear from context, we will drop the notational dependence on \u00b5. We use y \u2208 {0, 1}n\u00d7m to describe all choices in the market, where yij = 1 if user i chose item j, and 0 otherwise. Under Eq. (2), each user is modeled as a conservative boundedly-rational decision-maker, whose perception of value derives from how items are represented, and in particular, by which features are shown. Note that together with our positivity assumptions, this ensures that representations cannot be used to portray items as more valuable than they truly are\u2014which could result in negative utility.\nAllocation. To model the effect of congestion we require an allocation mechanism, denoted a = alloc(y1, . . . , yn), where a \u2208 {0, 1}n\u00d7m has aij = 1 if item j is allocated to user i, and 0 otherwise. We will use a(\u00b5) to denote allocations that result from choices y(\u00b5). We require feasible\n3For example, this is reasonable when sellers adapt slowly (or not at all; e.g., as in ad auctions [9,2]), or when prices are set for a broader aggregate market. For discussion on adaptive prices, see Appendix A.\n4In Appx. F.1 we experiment with an alternative decision model, which shows qualitatively similar results.\nallocations, such that each item is allocated at most once and each user to at most one item. For the allocation mechanism, we use the random single round rule, where each item j in demand is allocated uniformly at random to one of the users for which yi = j. This models congestion: if several users choose the same item j, then only one of them receives it while all others receive nothing. Intuitively, for welfare to be high, we would like that: (i) allocated items give high value to their users, and (ii) many items are allocated. As we will see, this observation forms the basis of our approach.\nLearning representations. To facilitate learning, we will assume there is some (unknown) distribution over markets, D, from which we observe samples. In particular, we model markets with a fixed set of items, and users sampled iid from some pool of users. For motivation, consider vacation rentals, where the same set of properties are available each week, but the prospective vacationers differ from week to week. Because preferences \u03b2i are private to a user, we instead assume access to user features, ui \u2208 Rd \u2032 , which are informative of \u03b2i in some way. Letting U \u2208 Rn\u00d7d \u2032 denote the set of all user features, each market is thus defined by a tuple M = (U,X, p) of users, items, and prices.\nWe assume access to a sample set S = {(M (\u2113), y(\u2113))}L\u2113=1 of markets M\u2113 = (U (\u2113), X, p(\u2113)) \u223c D and corresponding user choices y(\u2113). Note this permits item prices to vary across samples, i.e., p(\u2113) can be specific to the set of users U (\u2113). Our overall goal will be to use S to learn representations that entail useful decongested allocations, as illustrated in Figure 1. Concretely, we aim optimizing the expected welfare induced by allocations, i.e., the expected sum of values of allocated items:\nWD(\u00b5) = ED [\u2211\nij a(\u00b5)ijvij\n] , a(\u00b5) = alloc(y1, . . . , yn), yi = choice(X, p; vi, \u00b5) (3)\nwhere expectation is taken also w.r.t. to possible randomization in alloc and choice. Thus, we wish to solve argmax\u00b5 WD(\u00b5). Importantly, note that while choices are made based on perceived values v\u0303, as shaped by \u00b5, welfare itself is computed on the basis of true values v\u2014which are unobserved."
        },
        {
            "heading": "3 A DIFFERENTIABLE PROXY FOR WELFARE",
            "text": "We now turn to describing our approach for learning useful decongesting representations.\nWelfare decomposition. The main difficulty in optimizing Eq. (3) is that we do not have access to true valuations. To remove the reliance on v, our first step is to decompose welfare into two terms. Let WM = \u2211 ij a\u0304ijvij be the expected welfare for a single market M , where a\u0304ij = 1 nj yij denote\nexpected allocations with nj = \u2211 i yij , and defining a\u0304ij = 0 when nj = 0. We can rewrite WM as:\nWM = \u2211 ij 1 nj yijvij = \u2211 j (1\u2212 1 + 1nj ) \u2211 i yijvij = \u2211 ij yijvij\ufe38 \ufe37\ufe37 \ufe38\n(I)\n+ \u2211\nj ( 1nj \u2212 1) \u2211 i yijvij\ufe38 \ufe37\ufe37 \ufe38\n(II)\n(4)\nIn Eq. (4), term (I) encodes the value users would have gotten from their choices\u2014had there been no supply constraints. Term (II) then corrects for this, and appropriately penalizes excessive allocations.5\nProxy welfare. Absent the vij , a natural next step is to replace Eq. (4) with a feasible lower bound proxy. For term (I), note that if yij = 1 then v\u0303ij > pj (Eq. (1)), and since \u03b2, x \u2265 0, it also holds\n5Note that no penalty is incurred if an item is chosen by at most one user, since either 1 nj \u22121 = 0 or yij = 0.\nthat vij \u2265 v\u0303ij (since masking can only decrease perceived value). Hence, we can replace vij with pj . For term (II), since 1nj \u2212 1 \u2264 0, and since we assume v \u2264 1, using nj = \u2211 i yij we can write:\nW\u0303M = \u2211\nij yijpj\ufe38 \ufe37\ufe37 \ufe38\n= selection(y,p)\n\u2212 \u2211\nj max{0, nj \u2212 1}\ufe38 \ufe37\ufe37 \ufe38\n= decongestion(y)\n\u2264WM (5)\nwhich removes the explicit dependence on values, and relies only on choices. The two terms in W\u0303M can now be interpreted as: (I) selection, which expresses the total market value of users\u2019 choices, as encoded by prices; and (II) decongestion, which penalizes excess demand per item. Notice that n \u2212 decongestion(y) is simply the number of allocated items, |alloc(y)|. To extend beyond unit-supply, we can replace nj \u2212 1 with a more general nj \u2212 cj when there are cj copies of item j. Eq. (5) still depends on values implicitly through choices y. Our next step is to replace these with predicted choices, y\u0302i(\u00b5) = f(X, p;ui, \u00b5), where f is a predictive model pretrained on choice data in S:\nf\u0302 = argmin f\u2208F \u2211 (M,y)\u2208S \u2211 i\u2208[n] L(yi, f(X, p;ui, \u00b5)) (6)\nfor some model class F and loss function L (e.g., cross-entropy), which decomposes over users. Plugging the learned f into Eq. (5) and averaging over markets in S obtains our empirical proxy objective:\nW\u0303S(\u00b5) = 1\nN \u2211 M\u2208S [\u2211 ij y\u0302ij(\u00b5)pj \u2212 \u2211 j max { 0, n\u0302j(\u00b5)\u2212 1 }] (7)\nwhere n\u0302j(\u00b5) = \u2211\ni y\u0302ij(\u00b5). We interpret this as follows: In principle, Eq. (7) seeks representations \u00b5 that entail low congestion by optimizing the decongestion term; however, since there can be many decongesting solutions, the additional selection term reguralizes learning towards good solutions.\nDifferentiable proxy welfare. One challenge in optimizing Eq. (7) is that both predicted choices y\u0302 and masks \u00b5 are discrete objects. To enable end-to-end learning, we replace these with differentiable surrogates. For y\u0302, we substitute \u2018hard\u2019 argmax predictions with \u2018soft\u2019 predictions y\u0304i(\u00b5) using softmax. For masks, instead of optimizing over individual (discrete) masks, we propose to learn masking distributions, \u03c0\u03b8, that are differentiable in their parameters \u03b8. A natural choice in this case is the multinomial distribution, where \u03b8 \u2208 Rd assigns weight \u03b8r to each feature r \u2208 [d], and masks are constructed by drawing k features sequentially without replacement in proportion to (re)normalized weights, r \u223c softmax\u03c4 (\u03b8), where \u03c4 is a temperature hyper-parameter. Our final differentiable proxy objective is:\n\u03b8\u0302 = argmax\u03b8\u2208Rd W\u0303S(\u03c0\u03b8), where W\u0303S(\u03c0\u03b8) = E\u00b5\u223c\u03c0\u03b8 [ W\u0303S(\u00b5) ] (8)\nTo solve Eq. (8), we make use of the Gumbel top-k trick [28,15]: by reparametrizing \u03c0\u03b8, variation in masks due to \u03b8 is replaced with random perturbations \u03b5; this separates \u03b8 from the sampling process, which then permits to pass gradients effectively. We then use the method from [30] to smooth the selection of the top-k elements. For the forward step, the expectation in Eq. (8) is approximated by computing an average over samples \u00b5 \u223c \u03c0\u03b8. Once \u03b8\u0302 has been learned, at test time we can either sample from \u03c0\u03b8\u0302 as a masking policy, or commit to \u00b5\u03b8\u0302, defined to include the k largest entries in \u03b8\u0302. See Figure 2 for an illustration of the different components of our proposed framework.\nPractical considerations. One artifact of transitioning from Eq. (4) to Eq. (5) is that the different terms may now become unbalanced in terms of scale. As a remedy, we propose to reweigh them\nas (1 \u2212 \u03bb)\u00b7 selection + \u03bb\u00b7 decongestion, where \u03bb is a hyper-parameter that can be tuned via experimentation; nonetheless, our empirical analysis shows that learning is fairly robust to the choice of \u03bb. In addition, we have also found it useful to add a penalty on non-choices, i.e., \u2212 \u2211 i 1{y\u0302i = 0}, also weighted by \u03bb. This can be interpreted as also reducing congestion on the \u2018no-choice\u2019 item, and as accentuating the reward of choosing real items (since the null choice provides zero utility)."
        },
        {
            "heading": "4 THEORETICAL ANALYSIS",
            "text": "The core of our approach relies on minimizing congestion as a proxy to maximizing welfare. It is therefore natural to ask: when does decongestion improve welfare? Focusing on an individual market, in this section we give simple conditions under which allocating more items guarantees an improvement in welfare. Here we consider p to be competitive-equilibrium (CE) prices of the market under full information, meaning that under full information, every item with a strictly positive price is sold and every user can be allocated an item in their demand set. Proofs are deferred to Appendix B.\nWe start from the strongest type of relation between congestion and welfare, in which allocating more items is always better and irrespective of which items and to which users. Definition 1. A market with valuations vij is congestion monotone if for all s \u2208 [m], any allocation of s items gives (weakly) better welfare than any allocation of s\u2032 < s items.\nOur first result shows that monotonicity holds in economies in which users\u2019 valuations for the items are close, as expressed in the following sufficient condition. Proposition 1. In a market with n users, m items, and valuations vij , denote vmin = minij vij and vmax = maxij vij . If vmax\u2212vminvmin \u2264 1 m\u22121 , then the market is congestion monotone.\nSuch monotonicity provides us with very strong guarantees: it will sustain under any user behavior, allocation rule, and randomized outcome. However, this property is demanding in that it considers all allocations\u2014whereas some allocations may not be admissible, i.e., result from users choosing on the basis of some representation. We now proceed to pursue this case. Definition 2 (Admissible allocation). An allocation a is admissible, denoted a\u0303, if agents are only assigned their best-response items defined with respect to perceived values v\u0303 at prices p. Definition 3 (Restricted optimality). An allocation a is restricted optimal if a is welfare-optimal at true valuations v in the economy E = (Ga, Na), where Ga and Na denote the items and agents, respectively, that are allocated; i.e., the economy restricted to the items and agents that are allocated.\nThis property, which in effect defines optimality on a restricted economy, can be established through a set of sufficient conditions by reasoning with suitable notions of competitive equilibrium that arise when working with admissible allocations. To model the way we handle congestion, let A denote a randomized allocation, with a product structure defined as follows. Let G(A) denote the set of items allocated.6 The product structure requires that for each item j \u2208 G(A), some set Nj of agents compete for j with Nj \u2229Nj\u2032 = \u2205, for all j \u0338= j\u2032. Each agent i \u2208 Nj is allocated item j uniformly at random, so that PrA[i] = 1/|Nj | is the probability that i is allocated. We say that a randomized allocation A is admissible if it is a distribution over admissible allocations, and restricted optimal if it is a distribution over restricted optimal allocations. Define W (A) as the expected total welfare at true values, considering the distribution over allocations. We say that a randomized allocation B extends A if G(B) \u2283 G(A) and PrB [i] \u2265 PrA[i] for all agents i \u2208 [n] (i.e., no agent faces more congestion). Theorem 1. Given two randomized allocations, A and B, where B extends A and B is restricted optimal, then W (B) \u2265W (A), with W (B) > W (A) if vij > 0 for all i, j.\nThe main idea behind this result is that, together with the extension property, and in a way that carefully handles randomization, restricted optimality provides an ordering on welfare.\nWe now seek conditions under which an admissible allocation is restricted optimal: If these conditions hold for any admissible allocation in the support of a randomized allocation B, then by Thm. 1, B improves welfare relative to all randomized allocations which it extends. We parametrize these conditions by the margin of an admissible allocation, which is defined as follows.\n6As explained in Section 2, throughout the paper we consider unique best responses for the users.\nDefinition 4 (Margin). Let a\u0303 be an admissible allocation with allocated items and agents G\u0303 and N\u0303 , resp. Then the margin of a\u0303 is the maximal \u2206\u22650 s.t. v\u0303ia\u0303i\u2212pa\u0303i \u2265 maxj \u0338=a\u0303i,j\u2208G\u0303[v\u0303ij\u2212pj ]+\u2206, \u2200i \u2208 N\u0303 .\nDenote agent i\u2019s hidden valuation given mask \u00b5 as vHij = vij\u2212 v\u0303ij .7 Each of the following conditions is sufficient for restricted optimality and thus the improving welfare claim of Theorem 1:\n\u2022 Condition 1: Item heterogeneity is captured in revealed features. A first property, sufficient for restricted optimality, is that items G\u0303 allocated in admissible allocation a\u0303 have similar hidden features, with |(1\u2212\u00b5)\u2299 (xj \u2212xj\u2032)|1 \u2264 \u2206, \u2200j, j\u2032 \u2208 G\u0303, where \u2206 is the margin of the admissible allocation, \u00b5 is the mask, and xj and xj\u2032 the features of allocated items j and j\u2032, respectively.\n\u2022 Condition 2: Agent indifference to hidden features. A second property is that the agents N\u0303 allocated in admissible allocation a\u0303 have relatively low preference intensity for hidden features, with |(1\u2212 \u00b5)\u2299 \u03b2i|1 \u2264 \u2206, \u2200i \u2208 N\u0303 .\n\u2022 Condition 3: Top-item value consistency and low price variation. A third property relies on the item that is most preferred to an agent considering revealed features also being, approximately, the most preferred considering hidden features. In particular, we require (1) top-item value consistency, so that if item j satisfies v\u0303ij \u2265 maxj\u2032\u2208G\u0303 v\u0303ij\u2032 , \u2200i \u2208 N\u0303 (i.e., it is top for i considering revealed features), then vHij + \u2206 \u2265 maxj\u2032\u2208G\u0303 vHij\u2032 (i.e., it is approximately top for i considering hidden features); and (2) small price variation, so that |pj \u2212 pj\u2032 | \u2264 \u2206, for all items j, j\u2032 \u2208 G\u0303.\n\u2022 Condition 4: Items have small hidden features. A fourth property that suffices for restricted optimality is that items have small hidden features, with |(1\u2212 \u00b5)\u2299 xj |1 \u2264 \u2206, \u2200j \u2208 G\u0303.\n\u2022 Condition 5: Agent preference heterogeneity is captured in revealed features. A fifth property is that the agents N\u0303 allocated in addmisible allocation a\u0303 have similar preferences for hidden features, with |(1\u2212 \u00b5)\u2299 (\u03b2i \u2212 \u03b2i\u2032)|1 \u2264 \u2206, \u2200i, i\u2032 \u2208 N\u0303 ."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 SYNTHETIC DATA",
            "text": "We first make use of synthetic data to empirically explore our setting and approach. Our main aim is to understand the importance of each step in our construction in Sec. 3. Towards this, here we abstract away optimizational and statistical issues by focusing on small individual markets for which we can enumerate all possible masks, and assuming access to fully accurate predictions y\u0302(\u00b5) = y(\u00b5). The following experiments use n = m = 8, d = 14, k = 6, and CE prices, with results averaged over 10 random instances. Additional results for an alternative decision model can be found in Appendix F.1.\nVariation in preferences. In general, congestion occurs when users have similar preferences, and our first experiment studies how the degree of preference similarity affects decongestion and welfare. Let Vhet, Vhom \u2208 Rn\u00d7m be value matrices encoding fully-heterogeneous and fully-homogeneous preferences, respectively. We create \u2018mixture markets\u2019 as follows: First, we sample random item features X . Then, for each of the above V(i), we extract user preferences B(i) by solving minB\u22650 \u2225BX\u22a4 \u2212 V(i)\u22252. Finally, for \u03b1 \u2208 [0, 1], we set B\u03b1 = (1 \u2212 \u03b1)Bhet + \u03b1Bhom to get V\u03b1 = B\u03b1X \u22a4. Thus, by varying \u03b1, we can control the degree of preference similarity.\nFig. 3 (left) presents welfare obtained by the optimal masks for the following objectives: (i) a welfare oracle (having access to v), (ii) a predictive oracle (maximizing y\u0302ij(\u00b5)vij per user), (iii) selection, (iv) decongestion, (v) the welfare lower bound in Eq. (5) (namely selection minus decongestion), and (vi) our proxy objective in Eq. (7). As expected, the general trend is that less heterogeneity entails lower attainable welfare. Prediction and selection, which consider only demand (and not supply) do not fair well, especially for larger \u03b1. As a general strategy, decongestion appears to be effective; the crux is that there can be many optimally-decongesting solutions\u2014of which some may entail very low welfare (see subplot showing results for all k-sized masks in a single market). Of these, our proxy objective encourages a decongesting solution that has also high value; results show its performances closely matches the oracle upper bound, despite using p instead of v as in the welfare lower-bound.\n7Here we assume w.l.o.g. (given that 0 \u2264 v \u2264 1) that \u03b2 \u2208 [0, 1]m and x \u2208 [0, 1]m.\nPerceptive distortion. Partial information can decrease welfare if it causes preferences to shift. This becomes more pronounced if preference shift increases homogeneity, which leads to increased congestion. Since what may cause preferences to shift is the perceptive distortion of values, it would seem plausible to seek representations that minimize distortion. This is demonstrated empirically in Fig. 3 (right). The figure shows evident anti-correlation between perceptive distortion (measured as 1 m\u2225p\u0303\u2212p\u22251) and welfare across al k-sized masks (here we set \u03b1 = 0.2). A similar anti-correlative pattern appears in relation to preference homogeneity from perceived values (measured using Kendall\u2019s coefficient of concordance), suggesting that masks are useful if they entail heterogeneous choices.\nValue dispersion. Although heterogeneity is important, it may not be sufficient. As noted, markets with smaller margins should make our method more susceptible to perceptive distortion. To explore this, we study the effects of \u2018contracting\u2019 the higher-value regime of v, achieved by taking powers \u03c1 < 1 of v (since v \u2208 [0, 1], we have v \u2264 v\u03c1 \u2264 0). Fig. 3 (center) shows results for decreasingly smaller powers \u03c1. As expected, since smaller \u03c1 generally increase values, overall potential welfare increases as well. However, as values become \u2018tighter\u2019, this negatively impacts the effectiveness of our approach."
        },
        {
            "heading": "5.2 REAL DATA",
            "text": "We now turn to experiments on real data and simulated user behavior. We use two datasets: MovieLens, which we present here; and Yelp, which exhibits similar trends, and hence deferred to Appx. G.1.\nData. We use the Movielens-100k dataset [11], which contains 100,000 movie ratings from 1,000 users and for 1,700 movies, and is publicly-available.8 Item features X and users preferences B (dimension d) were obtained by applying non-negative matrix factorization to the partial rating matrix. User features U (dimension d\u2032) were then extracted by additionally factorizing preferences B as UT\u22a4 \u2248 B, where the inferred T can be thought of as an approximate mapping from features to preferences. We experiment in two latent dimension settings: small (d = 12), which permits computing oracle baselines by enumeration; and large (d = 100). In both we set d\u2032 = d/2.\nSetup. To generate a dataset of markets S, we first sample m = 20 items uniformly from X , and then sample L = 240 sets of n = 20 users uniformly from U . Masks \u00b5 are sampled according to a \u2018default\u2019 masking policy \u03c00 that elicits feature importance from prices, but ensures full support (see \u2018price predictive\u2019 baseline below). For prices p we mainly use CE prices computed per market, but also consider other pricing schemes. Choices y are then simulated as in Eq. (2). Given S, we use a 6-fold split to form different partitions into train test sets. Results are then averaged over 6 random sample sets and 6 splits per sample set (total 36, 95% standard error bars included).\nMethod. For our method of decongestion by representation (DbR), we optimize Eq. (8) using Adam with 0.01 learning rate and for a fixed number of 300 epochs. When k > d/2, we have found it useful to set k \u2190 d\u2212 k and learn \u2018inverted\u2019 masks 1\u2212 \u00b5. For \u03bb, our main results use \u03bb = 1\u2212 k2d , with the idea that smaller k require more effort placed on decongestion, but note that this very closely matches performance for \u03bb = 0.5, and that results are fairly robust across \u03bb (see Appendix G.3). For f in Eq. (6) we train a bi-linear model (in u and x) for 150 epochs using cross-entropy. We consider three variants of our approach that differ in their test-time usage: (i) DbR(\u03c0\u0302), which samples masks from\n8https://grouplens.org/datasets/movielens/100k/\nthe learned policy \u00b5 \u223c \u03c0\u0302; (ii) DbR(\u00b5\u0302), which commits to a single sampled mask \u00b5\u0302 \u223c \u03c0\u0302 (having the lowest objective value); and (iii) DbR(\u03b8\u0302), which constructs and uses a mask \u00b5\u03b8\u0302 composed of the top-k entries in the learned \u03b8\u0302. For further details on implementation and optimization see Appendix E.\nBaselines. We measure and compare performance to the following baselines: (iv) price-pred, a prediction-based method that uses the top-k most informative features for predicting prices from item features, with the idea that these should also be most informative of values; (v) choice-pred, which aims to recover the top-k most important features for users by eliciting an estimate of T (and hence of preferences \u03b2) from the learned choice-prediction model f ; (vi) an oracle benchmark that optimizes welfare directly (when applicable); and (vii) a random benchmark reporting average performance over randomly-sampled k-sized masks.\nResults. Figure 4 (left, center) shows results for increasing values of k. Because overall welfare quickly increases with k for all methods, for an effective comparison across k we plot the relative gain in welfare compared to random, with absolute values depicted within. For the d = 12 setting (left), results show that our approach is able to learn effective representations attaining welfare that is close to oracle. Relative gains increase with k and peak at around k = 8. Prediction-based methods generally improve with k, but at a low rate. The inlaid plot shows a tight connection to the number of allocated items, suggesting the importance of (de)congestion in promoting welfare (or failing to do so). For d = 100 (center), performance of our approach steadily increase with k. Here choice-pred preforms reasonably well for k \u2248 50, but not so for large k, nor for small k, where price-pred also fails. The role of prices. Because our proxy welfare objective relies on prices for guiding decongestion (for which CE prices are especially useful), we examine the robustness of our approach to differing pricing schemes. Focusing on d = 12 and k = 6, Figure 4 (right) shows performance for (i) CE prices ranging from buyer-optimal (minimal) to seller-optimal (maximal), and (ii) increasing levels of noise applied to mid-range CE prices. Results show that overall performance degrades as prices become either higher or noisier, demonstrating the general importance of having value-reflective prices. Nonetheless, and despite its reliance on prices, our approach steadily maintains performance relative to others. Appendix G.2 shows similar results for additional variations on pricing schemes."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "In this paper, we have initiated the study of decongestion by representation, developing a differentiable learning framework that learns item representations in order to reduce congestion and improve social welfare. Our main claim is that partial information is a necessary aspect of modern online markets, and that systems have both the opportunity and responsibility in choosing representations that serve their users well. We view our approach, which pertains to \u2018hard\u2019 congestion found in tangible-goods markets, and on feature-subset representations, as taking one step towards this. At the same time, \u2018soft\u2019 congestion, which is prevalent in digital-goods markets, also caries many adverse effects. Moreover, there exist various other relevant forms of information representation (e.g., feature ranking, or even other modalities such as images or text). We leave these, as well as the study of more elaborate user choice models, as interesting directions for future work."
        },
        {
            "heading": "ETHICS STATEMENT AND BROADER PERSPECTIVES",
            "text": "Our paper considers the effect of partial information on user choices in the context of online market platforms, and proposes that platforms utilize their control over representations to promote decongestion as a means for improving social welfare. Our point of departure is that partial information is an inherent component of modern choice settings. As consumers, we have come to take this reality for granted. Still, this does not mean that we should take the system-governed decision of what information to convey about items, and how, as a given. Indeed, we believe it is not only in the power of platforms, but also their responsibility, to choose representations with care. Our work suggests that \u2018default\u2019 representations, such as those relying on predictions of user choices, may account for demand\u2014but are inappropriate when supply constraints have concrete implications on user utility.\nSoft congestion. Although our focus is primarily on tangible-goods, we believe similar arguments hold more broadly in markets for non-tangibles, such as media, software, or other digital goods. While technically such markets are not susceptible to \u2018hard\u2019 congestion since there is no physical limitation on the number of item copies that can be allocated, still there is ample evidence of \u2018softer\u2019 forms of congestion which similarly lend to negative outcomes. For example, digital marketplaces are known to exhibit hyper-popularization, arguably as the product of rich-get-richer dynamics, and which results in strong inequity across suppliers and sellers. Some recent works have considered the negative impact of such soft congestion, but mostly in the context of recommender systems; we believe our conclusions on the role of representations apply also to \u2018soft\u2019 congestion, perhaps in a more subtle form, but nonetheless carrying the same important implications for welfare.\nLimitations. We consider the task of decongestion by representation in a simplified market setting, including several assumptions on the environment and on user behavior. One key assumption relates to how we model user choice (Sec. 2). While this can perhaps be seen as less restrictive than the standard economic assumption of rationality, our work considers only one form of bounded-rational behavior, whereas in reality there could be many others (our extended experiments in Appendix F.1 take one small step towards considering other behavioral assumptions). In terms of pricing, our theoretical analysis in Sec. 4 relies on equilibrium prices with respect to true buyer preferences, which may not hold in practice. Nonetheless, our experiments in Sec. 5 and Appendix G.2 on varying pricing schemes show that while CE prices are useful for our approach\u2014they are not necessary. Our counterexample in Sec. A suggests that, in the worst case, partially-informed equilibrating prices do not \u2018solve the problem\u2019. For our experiments in Sec. 5.2, as we state and due to natural limitations, our empirical evaluation is restricted to rely on real data but simulated user behavior. Establishing our conclusions in realistic markets requires human-subject experiments as well as extensive field work. We are hopeful that our current work will serve to encourage these kinds of future endeavours.\nEthics considerations. Determining representations has an immediate and direct effect on human behavior, and hence must be done with care and consideration. Similarly to recommendation, decongestion by representation is in essence a policy problem, since committing to some representation at one point in time can affect, through user behavior, future outcomes. Our empirical results in Sec. 5 suggest that learning can work well even when the counterfactual nature of the problem is technically unaccounted for (e.g., training f once at the onset on \u03c00, and using it throughout). But this should not be taken to imply that learning of representations in practice can succeed while ignoring counterfactuals. For this, we take inspiration from the field of recommender systems, which despite its historical tendency to focus on predictive aspects of recommendations, has in recent years been placing increasing emphasis on recommendation as a policy problem, and on the implications of this.\nWhile our focus is on \u2018anonymous\u2019 representations, i.e., that are fixed across items and for all users\u2014 it is important to note that the effect of representations on users is not uniform. This comes naturally from the fact that representations affect the perception of value, which is of course personal. As such, representations are inherently individualized. And while this provides power for improving welfare, it also suggests that care must be taken to avoid discrimination on the basis of induced perceptions; e.g., decongesting by systematically diverting certain groups or individuals from their preferred choices.\nFinally, we note that while promoting welfare is our stated goal and underlies the formulation of our learning objective, the general approach we consider can in principal be used to promote other platform objectives. Since these may not necessarily align with user interests, deploying our framework in any real context should be done with integrity and under transparency, to the extent possible, by the platform."
        },
        {
            "heading": "A A NOTE ON ADAPTIVE PRICES",
            "text": "Our settings makes the assumption that prices are fixed. This is motivated by settings in which sellers are slow to adapt (or do not adapt at all), and in which representations can be adjusted to take effect more quickly. In this sense, we see representations as adapting to prices\u2014rather than vice versa.\nAdaptive prices. An alternative would be to consider prices that adapt to revealed demand, and in particular, prices p\u0303 that attain competitive equilibrium under perceived values v\u0303; i.e., \u201cpartiallyinformed competitive equilibrium prices,\" or \u201cpartially-informed prices.\" These prices would clear the market, but nonetheless have several significant drawbacks:\n\u2022 First, such prices would completely ignore true valuations v, and the actual values that users obtain from items would have no effect on the market. We find this to be unrealistic; a more plausible alternative would be to have (past) true values propagate to influence (future) prices in some manner (e.g., via users posting reviews). Fixed prices can be seen as one (indirect) way to achieve this.\n\u2022 Second, and relatedly, while partially-informed prices do solve congestion, they do so without any guarantees on welfare; in fact, in our setting, welfare under such prices can be arbitrarily low (see below). This is in contrast to fully-informed prices, which simultaneously minimize congestion and maximize welfare.\n\u2022 Third, in our setting, such partially-informed prices would likely be much lower than prices at full information. This may push sellers to leave the platform if they have an external option, or if prices fall below production costs, this reducing welfare.\n\u2022 Fourth, and most importantly, partially-informed prices still depend on what information is revealed, i.e., they will be different under different masking schemes. Thus, the problem of choosing what information to convey would remain and in fact become more difficult, as learning must now anticipate not only choices, but also induced prices, under possible representations.\nTherefore, while learning representations for adapting prices is an intriguing direction, we feel it is deserving of designated future work.\nA constructive example. We now show how in our setting, partially-informed prices can give arbitrarily-bad welfare (in the worst case) as a result of their dependence on representations. We prove this by constructing an example in which one representation yields approximately optimal welfare, whereas another yields (approximately) only a small constant fraction, under corresponding partially-informed prices. The construction works by setting half of the features to encode most of the true values of items, and the other half to encode noise. The former subset corresponds to a \u2018good\u2019 representation, for which prices need not adapt much, and hence preserves optimal choices. The latter subset corresponds to a \u2018bad\u2019 representation, which is highly uninformative of values; this causes prices to adapt in a way that entails a \u2018random\u2019 decongested allocation providing very low welfare.\nFigure 5 illustrates the values and choices under the different representations and pricing schemes for our example. The precise numerical values used in the example can be found in our code base."
        },
        {
            "heading": "B THEORETICAL ANALYSIS",
            "text": ""
        },
        {
            "heading": "COMPETITIVE EQUILIBRIUM",
            "text": "Let p = (p1, . . . , pm) denote item prices, a denote a feasible allocation (i.e., each item is allocated at most once and each user to at most one item), and vi agent i\u2019s true valuation. Let v\u0303i denote agent i\u2019s perceived valuation given mask \u00b5, and vHij = vij \u2212 v\u0303ij denote agent i\u2019s hidden valuation. We make the technical assumption that each user has a unique best response, but note the analysis extends to demand sets that are not a singleton by heuristically selecting an item from the demand set.9\nDefinition 5. (a, p) is a competitive equilibrium if (1) ai \u2208 argmaxj [vij \u2212 pj , 0] for all i, and (2) any item with pj > 0 is allocated.\nCompetitive equilibrium requires that allocation a is (1) a best response for each agent, and (2) maximizes revenue. The following is well known, the proof is included for completeness. Theorem 2. A CE is welfare optimal.\nProof. The primal assignment problem is\nmax a \u2211 i \u2211 j aijvij (9)\ns.t. \u2211 i\naij \u2264 1 ,\u2200j [dual pj ]\u2211 j aij \u2264 1 ,\u2200i [dual \u03c0i]\nxij \u2265 0\nThe dual is min \u03c0,p \u2211 j pj + \u2211 i \u03c0i (10)\ns.t. \u03c0i + pj \u2265 vij \u2200i, j [dual aij ] \u03c0i, pj \u2265 0.\nThe optimality of CE (a, p), along with \u03c0i = maxj [vij \u2212 pj , 0] to complete the dual, is established by checking complementary slackness (CS). The primal CS condition is aij > 0\u21d2 \u03c0i + pj = vij , and satisfied since agent i receives an item in its best response set when non-empty (CE), and by the construction of \u03c0i. The dual CS conditions are \u03c0i > 0 \u21d2 \u2211 j aij = 1 and pj > 0 \u21d2 \u2211 i aij = 1, and satisfied by the CE properties, since every agent with a non-zero demand set gets an item and every item with positive price is allocated.\nCE prices form a lattice, in general are not unique, and price the core of the assignment game [25]. Amongst the set of CE prices, the buyer-optimal and seller-optimal prices are especially salient."
        },
        {
            "heading": "CONGESTION MONOTONICITY",
            "text": "Proof. (Proposition 1.): Let As denote the set of all feasible allocations of exactly s items, such that every set A \u2208 As is a set of user-item pairs that represents an allocation of s items. Value matrix (vij) is congestion monotone if and only if for every s \u2264 m it holds that\nmax A\u2208As\u22121 \u2211 (i,j)\u2208A vij \u2264 min A\u2208As \u2211 (i,j)\u2208A vij .\nNext, we define \u03b4ij = vij \u2212 vmin and write every value in (vij) as vij = vmin + \u03b4ij . Using these notations, the congestion monotonicity condition is:\nvmin \u2265 (\nmax A\u2208As\u22121 \u2211 (i,j)\u2208A \u03b4ij ) \u2212 ( min A\u2208As \u2211 (i,j)\u2208A \u03b4ij ) .\n9Alternatively, one can infinitesimally perturb the preference vectors and obtain a unique best response.\nSince s \u2264 m and since the last summation is of positive terms, we have that a sufficient condition is: vmin \u2265 (m\u2212 1) \u00b7max(\u03b4ij) = (m\u2212 1) ( vmax \u2212 vmin ) , as required."
        },
        {
            "heading": "RESTRICTED OPTIMALITY",
            "text": "We start by discussing deterministic allocations and then proceed to the proof of Theorem 1 and the proofs for the sufficient conditions for restricted optimality. Let welfare W (a) = \u2211 i \u2211 j aijvij . Let Ga and Na denote the items and agents, respectively, that are allocated in allocation a. Say that a is restricted optimal if and only if a is welfare optimal at true valuations v in the economy E = (Ga, Na); i.e., the economy restricted to the items and agents that are allocated. Say that an allocation b extends a if Nb \u2283 Na and Gb \u2283 Ga (i.e., b allocates a strict superset of items and agents). Lemma 1. Given two allocations, a and b, where b extends a and b is restricted optimal, then W (b) \u2265W (a), with W (b) > W (a) if vij > 0 for all i, all j.\nProof. Allocation a is feasible in economy Ea = (Ga, Na) and thus feasible in economy Eb = (Gb, Nb), and so W (b) \u2265 W (a) since b is optimal on Eb. Moreover, if items have strictly positive value then W (b\u2032) > W (a) for allocation b\u2032, feasible in Eb, that extends a through an arbitrary assignment of items Gb \\Ga to Nb \\Na. With this, we have W (b) \u2265W (b\u2032) > W (a), and b strictly improves welfare over a.\nProof. (of Theorem 1.) Consider some deterministic allocation a in the support of A, and let P1 = PrA[a] denote the probability of assignment a. Define P2 = \u2211 b\u2208sup(B),b extends a PrB [b], which is the marginal probability of assignments that extend a. We have\n\u2211 b\u2208sup(B),b extends a PrB [b] = \u2211 b\u2208sup(B),b extends a \u220f i\u2208a PrB [i] \u00b7 \u220f i\u2208b,i/\u2208a PrB [i]\n= \u220f i\u2208a PrB [i] \u2211 b\u2208sup(B),b extends a \u220f i\u2208b,i/\u2208a PrB [i] = \u220f i\u2208a PrB [i] \u00b7 1 \u2265 \u220f i\u2208a PrA[i],\nwhere the product structure is used to replace the marginalization over the part of the assignment that extends a by probability 1, and the inequality follows since B extends A. For any such b that extends a, we have W (b) \u2265W (a) by Lemma 1, where we use the property that B is restricted optimal and thus each b in the support of B is restricted optimal. Then, since P2 \u2265 P1, and considering all such a in the support of A, we have W (B) \u2265W (A). By considering the case of vij > 0 for all i, all j, then W (b) > W (a) by Lemma 1, and we have W (B) > W (A).\nBy Theorem 1, to argue that randomized allocation B provides more welfare than randomized allocation A it suffices to argue that (1) each assignment in the support of B is restricted optimal, and (2) B extends A which means that B allocates a superset of the items and each agent is allocated something with at least as much probability in B than A (i.e., no agent faces more congestion).\nThe first set of conditions, namely Conditions 1, 2, and 3 in the main text, follow from reasoning about the following consistency property, that needs to hold between perceived and true valuations.\nDefinition 6 (Pointing consistency.). An admissible allocation a\u0303 satisfies pointing consistency if, for every agent i \u2208 N\u0303 , the allocated item a\u0303i is the best response of i at true valuations vi.\nIn other words, agent i continues to prefer item a\u0303i at prices p when moving from perceived valuation v\u0303i to true valuation vi. The following is immediate.\nLemma 2. Admissible allocation a\u0303 is restricted optimal if the pointing consistency condition holds.\nProof. (a\u0303, p) is a CE (defined with respect to true valuations) in economy (G\u0303, N\u0303).\nLemma 3. Admissible allocation a\u0303 with margin \u2206 satisfies pointing consistency (and therefore by Lemma 2 is restricted optimal), when vHia\u0303i \u2265 v H ij \u2212\u2206, for all j \u2208 G\u0303, all i \u2208 N\u0303 .\nProof. For agent i, and any j \u0338= a\u0303i, we have via\u0303i\u2212pa\u0303i = v\u0303ia\u0303i\u2212pa\u0303i+vHia\u0303i \u2265 v\u0303ij\u2212pj+\u2206+v H ij \u2212\u2206 = vij \u2212 pj , and pointing consistency, where we substitute v\u0303ia\u0303i \u2212 pa\u0303i \u2265 v\u0303ij \u2212 pj +\u2206 (margin condition) and vHia\u0303i \u2265 v H ij \u2212\u2206 (indifference assumption).\nConsidering a matrix with agents as rows and items as columns, the property in Lemma 3 is one of \u201crow-dominance\" for \u2206 = 0, such that the value of an agent for its allocated item is weakly larger than that of every other item. For this property, it suffices that there is little variation in the hidden value for any items, which is in turn provided by the set of five conditions.\nProof. (of Condition 1) This condition is sufficient for the hidden-value similarity of Lemma 3, since vHij \u2212 vHia\u0303i = \u03b2 \u22a4 i (1\u2212\u00b5)\u2299 (xj \u2212xa\u0303i) = \u2211 k:\u00b5k=0 \u03b2ik(xjk\u2212xa\u0303ik) \u2264 \u2211 k:\u00b5k=0 |\u03b2ik(xjk\u2212xa\u0303ik)| \u2264\u2211\nk:\u00b5k=0 |xjk \u2212 xa\u0303ik| = |(1 \u2212 \u00b5) \u2299 (xj \u2212 xa\u0303i)|1 \u2264 \u2206, where the penultimate inequality follows\nfrom 0 \u2264 \u03b2ik \u2264 1.\nProof. (of Condition 2) This condition is sufficient for the hidden-value similarity of Lemma 3 since vHij \u2212 vHia\u0303i = \u03b2 \u22a4 i (1\u2212\u00b5)\u2299 (xj \u2212xa\u0303i) = \u2211 k:\u00b5k=0 \u03b2ik(xjk\u2212xa\u0303ik) \u2264 \u2211 k:\u00b5k=0 |\u03b2ik(xjk\u2212xa\u0303ik)| \u2264\u2211\nk:\u00b5k=0 |\u03b2ik| = |(1\u2212 \u00b5)\u2299 \u03b2i|1 \u2264 \u2206, where the penultimate inequality follows from 0 \u2264 xj\u2032k \u2264 1,\nfor all item j\u2032 and features k.\nProof. (of Condition 3) By the margin property, we have v\u0303ia\u0303i \u2212 pa\u0303i \u2265 v\u0303ij \u2212 pj +\u2206, for any j \u2208 G\u0303, and adding pa\u0303i \u2265 pj \u2212\u2206 (price variation) we have v\u0303iz\u0303i \u2265 v\u0303ij , and so a\u0303i is the top item for i given revealed features. Given this, we have vHia\u0303i +\u2206 \u2265 v H ij , for all j \u2208 G\u0303 (top-item value consistency), which is the hidden-value similarity condition of Lemma 3.\nThe second set of conditions, namely Conditions 4 and 5 in the main text, come from considering an approximate column dominance property on hidden valuations. Considering a matrix with agents as rows and items as columns, column dominance means that the agent to which an item is allocated has weakly larger value for the item than that of any other agent. Definition 7 (Approximate column dominance). An admissible allocation a\u0303 with margin \u2206 satisfies approximate column dominance if, for each item j \u2208 G\u0303 and agent i allocated item j, we have vHij \u2265 vHi\u2032j \u2212\u2206, for all i\u2032 \u2208 N\u0303 . Lemma 4. Admissible allocation a\u0303 with margin \u2206 is restricted optimal if the approximate column dominance condition holds.\nProof. First, given margin \u2206 then\u2211 i\u2208N\u0303 \u2211 j\u2208G\u0303 a\u0303ij v\u0303ij \u2265 \u2211 i\u2208N\u0303 \u2211 j\u2208G\u0303 a\u2032ij v\u0303ij + |G\u0303|\u2206, all a\u2032, (11)\nsince we can reduce v\u0303ia\u0303i by \u2206 to each agent i, leaving the rest of the perceived values unchanged, and this item will still be in the demand set of the agent, and thus (a\u0303, p) would be a CE for these adjusted, perceived values (perceived, not true values). Thus, the total perceived value for a\u0303 is at least |G\u0303| \u00b7\u2206 better than the total perceived value of the next best allocation, considering economy (G\u0303, N\u0303). Second, we argue that approximate column dominance implies that a\u0303 approximately optimizes the total hidden value. First, suppose we have exact column dominance, with vHij \u2265 vHi\u2032j , for all i\u2032 \u2208 N\u0303 , item j \u2208 G\u0303, and agent i allocated item j. Then, allocation a\u0303 would maximize hidden values. To see this, consider the transpose of this assignment problem, so that agents become items and items become agents. This maintains the optimal assignment. a\u0303 is optimal in the transpose economy by considering zero price on each agent and items bidding on agents: by column dominance, each agent is allocated its most preferred agent. By approximate column dominance, we have\u2211\ni\u2208N\u0303 \u2211 j\u2208G\u0303 a\u0303ijv H ij + |G\u0303|\u2206 \u2265 \u2211 i\u2208N\u0303 \u2211 j\u2208G\u0303 a\u2032ijv H ij , all a \u2032, (12)\nand a\u0303 approximately optimizes total hidden value. This follows by considering the transpose economy, and noting that if we increase vHia\u0303i by \u2206, to each agent i, leaving the other hidden values unchanged,\nwe have exact column dominance and optimality of a\u0303. This means that a\u0303 is at most |G\u0303| \u00b7\u2206 worse than any other allocation. Combining (11) for perceived values and (12) for hidden values, we have\u2211\ni\u2208N\u0303 \u2211 j\u2208G\u0303 a\u0303ij(v\u0303ij + v H ij ) + |G\u0303|\u2206 \u2265 \u2211 i\u2208N\u0303 \u2211 j\u2208G\u0303 a\u2032ij(v\u0303ij + v H ij ) + |G\u0303|\u2206, all a\u2032, (13)\nand thus a\u0303 is restricted optimal, since v\u0303ij + vHij = vij .\nIt suffices for approximate column dominance that there is little variation across agents in their hidden value for an item, which is in turn provided by the following properties (approximate column dominance is also achieved by Condition 2).\nProof. (of Condition 4) When this condition holds, we have |vHij \u2212 vHi\u2032j | = |\u03b2\u22a4i (1 \u2212 \u00b5) \u2299 xj \u2212 \u03b2\u22a4i\u2032 (1 \u2212 \u00b5) \u2299 xj | = |(\u03b2i \u2212 \u03b2i\u2032)\u22a4(1 \u2212 \u00b5) \u2299 xj | = \u2211 k:\u00b5k=0 |(\u03b2ik \u2212 \u03b2i\u2032k)xjk| \u2264 \u2211 k:\u00b5k=0 |xjk| = |(1\u2212\u00b5)\u2299 xj |1 \u2264 \u2206, where the penultimate inequality follows from 0 \u2264 \u03b2i\u2032\u2032k \u2264 1, for any i\u2032\u2032, any k. This establishes that all pairs of agents have similar hidden value for any given item, and in particular approximate column dominance and vHi\u2032j \u2212 vHij \u2264 \u2206 for agent i allocated item j in a\u0303 and any other agent i\u2032 \u2208 N\u0303 .\nProof. (of Condition 5) With this, we have |vHij \u2212 vHi\u2032j | = |\u03b2\u22a4i (1 \u2212 \u00b5) \u2299 xj \u2212 \u03b2\u22a4i\u2032 (1 \u2212 \u00b5) \u2299 xj | = |(\u03b2i \u2212 \u03b2i\u2032)\u22a4(1 \u2212 \u00b5) \u2299 xj | = | \u2211 k:\u00b5k=0 (\u03b2ik \u2212 \u03b2i\u2032k)xjk| \u2264 \u2211 k:\u00b5k=0 |(\u03b2ik \u2212 \u03b2i\u2032k)xjk| \u2264\u2211\nk:\u00b5k=0 |\u03b2ik \u2212 \u03b2i\u2032k| = |(1\u2212\u00b5)\u2299 (\u03b2i\u2212 \u03b2i\u2032)|1 \u2264 \u2206, where the penultimate inequality follows from 0 \u2264 xjk \u2264 1 for any item j. This establishes that all pairs of agents have similar hidden value for any given item, and in particular approximate column dominance and vHi\u2032j \u2212 vHij \u2264 \u2206 for agent i allocated item j in a\u0303 and any other agent i\u2032 \u2208 N\u0303 ."
        },
        {
            "heading": "C METHOD: ADDITIONAL DETAILS",
            "text": "Although our approach makes use of prediction, in essence, the problem of finding optimal representations is counterfactual in nature. This is because choosing a good mask requires anticipating what users would have chosen had they made choices under this new mask; these may differ from the choices made in the observed data. As such, decongestion by representation is a policy problem. This has two implications: on how data is collected, and on how to predict well."
        },
        {
            "heading": "C.1 DEFAULT POLICY",
            "text": "To facilitate learning, we assume that training data is collected under representations determined according to a \u2018default\u2019 stochastic masking policy, \u03c00. The degree to which we can expect data to be useful for learning counterfactual masks depends on how informative \u03c00 of other representations. In particular, if there is sufficient variation in masks generated by \u03c00, then in principle it should be possible to generalize well from \u03c00 to a learned masking policy, \u03c0\u0302 (which can be deterministic). We imagine \u03c00 as concentrated around some reasonable default choice of mask, e.g., as elicited from a predictive model, or which includes features estimated to be most informative of user values. However, \u03c00 must include some degree of randomization; in particular, to enable learning, we require \u03c00 to have full support over all masks, i.e., have P\u03c00(\u00b5) \u2265 \u03f5 for all \u00b5 and for some \u03f5 > 0. In our experiments we set \u03c00 to have most probability mass concentrated around features coming from a predictive baseline (e.g., price-pred), but with some probability mass assigned to other features."
        },
        {
            "heading": "C.2 COUNTERFACTUAL PREDICTION",
            "text": "Representation learning is counterfactual since choices at test time depend on the learned mask. At train time, counterfactuality manifests in predictions: for any given \u00b5 examined during training, our objective must emulate choices y(\u00b5), which rely on v, via predictions y\u0302(\u00b5), which rely only on observed features u,X and prices p. As such, we must make use of choice data sampled from \u03c00 to predict choices to be made under differing \u00b5. There is extensive literature on learning under distribution shift, and in principle any method for off-policy learning should be applicable to our\ncase. One prominent approach relies on inverse propensity weights, which weight examples in the predictive learning objective according to the ratio of train- to test-probabilities,\nw\u03c0(\u00b5) = P\u03c0(\u00b5)\nP\u03c00(\u00b5)\nfor all masks \u00b5 in the training data, which are then used to modify Eq. (6) into:\nf\u0302 = argmin f\u2208F \u2211 (M,y)\u2208S \u2211 i\u2208[n] w\u03c0(\u00b5)L(yi, f(X, p;ui, \u00b5)) (14)\nFor the default policy, propensities \u03c1 = P\u03c00(\u00b5) are assumed to be collected and accessible as part of the training set. For the current policy \u03c0, P\u03c0(\u00b5) can be approximated from the Multivariate Wallenius\u2019 Noncentral Hypergeometric Distribution, which describes the distribution of sampling without replacement from a non-uniform multinomial distribution. . This makes the predictive objective unbiased with respect to the shifted target distribution, and as a result, makes Eq. (8) appropriate for the current \u03c0.\nIn our case, because the shifted distributions are not set a-priori, but rather, are determined by the learned representations themselves, our problem is in fact one of decision-dependent distribution shift. Our proposed solution to this is to alternate between: (i) optimizing ft in Eq. (6) to predict well for data corresponding to the current mask \u00b5t\u22121, holding parameters \u03b8t\u22121 fixed; and (ii) optimizing \u00b5t by updating parameters \u03b8t in Eq. (8) for a fixed ft. That is, we alternate between training the predictor on a fixed choice distribution, and optimizing representations for a fixed choice predictor.\nNonetheless, in our experiments we have found that simply training f to predict well on \u03c00\u2014without any reweighing or adjustments\u2014obtained good overall performance, despite an observed reduction in the predictive performance of f on counterfactual choices made under the learned \u00b5 (relative to predictive performance on \u03c00)."
        },
        {
            "heading": "D EXPERIMENTAL DETAILS: SYNTHETIC",
            "text": "Experiments were implemented in Python. See supplementary material for code.\nPrices. For computing CE prices we used the cvxpy convex optimization package to implement Eq. (10). This give some price vector in the core. To interpolate between buyer-optimal and selleroptimal core prices, we adjust Eq. (10) by: (i) solving the original Eq. (10) to obtain the optimal dual objective value; (ii) adding a constraint for the objective value to obtain the optimal value; and (iii) modifying the current objective to either minimize prices (for buyer-optimal) or maximize prices (for seller-optimal).\nPreferences. To generate mixture value matrices, we first sample two random item features matrices X(1), X(2) \u2208 [0, 1]n\u00d7d with entries sampled independently from the uniform distribution over [0, 1]. Next, we generate a fully-heterogeneous value matrix V(1) = Vhet, and a fully-homogeneous matrix V(2) = Vhom. The heterogeneous matrix is constructed by taking the preference vector (m,m\u22121, . . . , 1), normalized to [0, 1], and creating a circulant matrix, so that user i most prefers item i, and then preferences decreasing in items with increasing indices (modulo m). The homogeneous matrix is constructed by assigning the same preference vector to all users.10 Finally, to obtain the corresponding B(i), we solve for the convex objective minB\u22650 \u2225BX\u22a4 \u2212 V(i)\u22252, and for \u03b1 \u2208 [0, 1], set B\u03b1 = (1\u2212 \u03b1)Bhet + \u03b1Bhom and X = X(1) +X(2), which gives the desired V\u03b1 = B\u03b1X\u22a4. Optimization. Because we consider small n,m, d, and because as designers of the experiment we have access to v, in this experiment we are able to compute measures that rely on v. In particular, by enumerating over all d-choose-k possible masks, we are able to exactly optimize the considered objectives, compute the welfare oracle upper bound, and obtain all optimal solutions in case of ties (as in the case of the decongestion objective).\n10We also experimented with adding noise to each V(i) (small enough to retain preferences), but did not observe this to have any significant impact on results."
        },
        {
            "heading": "E EXPERIMENTAL DETAILS: REAL DATA",
            "text": ""
        },
        {
            "heading": "E.1 DATA GENERATION",
            "text": "Data and preprocessing. NMF on partial rating matrix was done by surprise11 python package For Movielens, as rating vlues range from 1 to 5, we normalize then into [0, 1] by dividing the user preferences matrix B by a factor of 5.\nPrices. CE prices p\u2217 were computed by solving the dual LP in Eq. (10), similarly to the synthetic experiments. For varying prices between buyer-optimal (pbuyer) and seller-optimal (pseller) CE prices, we interpolate between pbuyer and p\u2217 for \u03b3 \u2208 [0, 0.5], and between p\u2217 and pseller for \u03b3 \u2208 (0.5, 1], this since interpolating directly between pbuyer and pseller is prone to exhibiting many within-user ties as an artifact, and since p\u2217 is often very close to the average price point (pbuyer + pseller)/2.\nDefault masking policy. As discussed in Appendix C.1, our method requires training data to be based on masks generated from a default masking policy, \u03c00. We defined \u03c00 to be concentrated around the features selected by the price-pred predictive baseline, but ensure all features have strictly positive probability. In particular, let \u00b50 be the mask including the set of k features as chosen by price-pred. Then \u03b8 for \u03c00 is constructed as follows: first, we assign \u03b8i = 1 for all i /\u2208 \u00b50; then, we assign \u03b8i = 3 for all i \u2208 \u00b50; finally, we normalize \u03b8 using a softmax with temperature 0.05, this resulting in a distribution over features that strictly positive everywhere but at the same time tightly concentrated around \u00b50, and in a way which depends on k (since different k lead to different normalizations). An example \u03c00 is shown in Fig 7 (left)."
        },
        {
            "heading": "E.2 OUR FRAMEWORK",
            "text": "Choice prediction. The choice prediction model f is trained to predict choices (including null choices) from training data. For the class of predictors F = {f}, we use item-wise score-based bilinear classifiers parameterized by W \u2208 Rd\u00d7d\u2032 , namely:\nfW (X, p;u, \u00b5) = argmax x\u2208X\nu\u22a4W (x\u2299 \u00b5)\u2212 p\nThere are implemented as a single dense linear layer, and for training, the argmax is replaced with a differentiable softmax. We found learning to be well-behaved even under low softmax temperatures, and hence use \u03c4f = 5e\u22124 throughout. For training we used cross entropy loss as the objective. For optimization we Adam for 150 epochs, with learning rate of 1e\u22123 and batch size of 20. See Figure 6 for a schematic illustration. Training data used to train f includes user choices y made on the basis masks \u00b5 sampled from the default policy, \u00b5 \u223c \u03c00. Nonetheless, as described in C.2, recall that we would like f to predict well on the final learned mask \u00b5, but also on other masks encountered during training, and more broadly\u2014on any possible mask. Figure 7 (center+right) shows, for d = 12 and d = 100 and as a function of k, the accuracy of f on (i) data representative of the training distribution (i.e., masks sampled from \u03c00), and (ii) data which includes masks sampled uniformly at random from the set of all possible k-sized masks. As can be seen, across all k, performance on arbitrary masks\n11https://surpriselib.com/\nclosely matches in-distribution performance for d = 12, and remains relatively high for d = 100 (vs. random performance at 5% for m = 20).\nRepresentation learning. The full-framework model consists of a Gumbel-top-k layer, applied on top of a \u2018frozen\u2019 choice prediction model f , pre-trained as described above. The Gumbel-top-k layer has d trainable parameters \u03b8 \u2208 \u0398 = Rd; once passed through an additional softmax layer, this constitutes a distribution over features. As described in the main paper, given this distribution, we generate random masks by independently sampling k features i \u223c \u03b8i without replacement (and re-normalizing \u03b8). However, to ensure our framework is differentiable, we use a relaxed-top-k procedure for generating \u2018soft\u2019 k-sized masks, and for each batch, we sample in this way N soft masks, for which we adopt the procedure of [30].\nGiven a sampled batch of masks {\u00b5}, these are then plugged in to the prediction model f to obtain y\u0302(\u00b5), and finally our proxy-loss \u2212W\u0303 is computed. Optimization was carried out using the Adam optimizer for 300 epochs (at which learning converged for most cases) and with a learning rate of 1e\u22122. We set N = 20, and use temperatures \u03c4Gumbel = 2 for the Gumbel softmax, \u03c4top-k = 0.2 for the relaxed top-k, and \u03c4f = 0.01 for the softmax in the pre-trained predictive model f . Since the selection of the top-k features admits several relaxations, for larger k > d/2, we have found it useful to instead consider k \u2190 d\u2212 k in learning, and then correspondingly use \u2018inverted\u2019 masks \u00b5\u2190 1\u2212\u00b5. Variants. As noted, we evaluate three variants of our approach that differ in their usage at test-time:\n\u2022 DbR(\u03b8\u0302): Constructs a mask from the top-k entries in the learned \u03b8\u0302. \u2022 DbR(\u00b5\u0302): A heuristic for choosing a mask on the basis of training data. Here we sample 20\nmasks \u00b5\u0302 according to the multinomial distribution defined by the learned \u03b8\u0302, and commit to the sampled mask obtaining the lowest value on the proxy objective.\n\u2022 DbR(\u03c0\u0302): Emulates using \u03b8\u0302 as a masking policy \u03c0\u0302 = \u03c0\u03b8\u0302. Here we sample 50 masks \u00b5 \u223c \u03c0\u0302, evaluate for each sampled mask its performance on the entire test set, and average."
        },
        {
            "heading": "E.3 BASELINES",
            "text": "\u2022 Price predictive (price-pred): Selects the k most informative features for the regression task of predicting the price of items, based on item its features. Data includes features and prices for all items that appear in the dataset (recall markets include the same set of items, but items can be priced differently per market). We use the Lasso path (implementation by scikit-learn12) to order features in terms of their importance for prediction, and take as a mask the top k features in that order.\n\u2022 Choice predictive (choice-pred): Selects the k most informative features for the classification task of predicting user choices from user and item features. For this baseline we use the predictive model f , where we interpret learned weights W = T\u0302 as an estimated of the true\n12https://scikit-learn.org/stable/modules/generated/sklearn.linear_model .lasso_path.html\nunderlying mapping T between user features u and (unobserved) preferences \u03b2. Inferred parameters T\u0302 are then used to obtain estimated preferences per user via \u03b2\u0302 = uT\u0302 . We then average preferences over users, to obtain preferences representative of an \u2018average\u2019 user, and from which we take the top k-features, we we interpret as accounting for the largest proportion of value.\n\u2022 Random (random): Here we report performance averaged over 100 random masks sampled uniformly from the set of all k-sized masks.\nE.4 IMPLEMENTATION\nCode. All code is written in python. All methods and baselines are implemented and trained with Tensorflow13 2.11 and using Keras. CE prices were computed using the convex programming package cvxpy14.\nHardware. All experiments were run on a Linux machine wih AMD EPYC 7713 64-Core processors. For speedup runs were parallelized each across 4 CPUs.\nRuntime. Runtime for a single experimental instance of the entire pipeline was clocked as:\n\u2022 \u2248 6.5 minutes for the d = 12 setting \u2022 \u2248 13.5 minutes for the d = 100 setting\nData creation was employed once at the onset."
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTAL RESULTS: SYNTHETIC DATA",
            "text": ""
        },
        {
            "heading": "F.1 MEAN-IMPUTATION CHOICE MODEL",
            "text": "In this section we replicate our main synthetic experiment in Sec. 5.1 on a different user choice model. In the main part of the paper, we model users as contending with the partial information depicted in representations by assuming that unobserved features do not contribute towards the item\u2019s value.\n13https://www.tensorflow.org/ 14https://www.cvxpy.org/\nIn particular, here we consider users who replace masked features with mean-imputed values: for example, if some feature \u2113 is masked, then features xj\u2113 are replaced with the \u2018average\u2019 feature, x\u0304\u2113 = 1 m \u2211 j\u2032 xj\u2032\u2113, computed over and assigned to all market items j. This is in contrast to the choice model defined in (see Sec. 2) which relies on zero-imputed values. The main difference is that with mean imputation, (i) perceived values can also be higher than true values (e.g., if x\u0304\u2113 > xj\u2113 for some item j); and (ii) our proxy welfare objective in Eq. (5) is no longer a lower bound on true welfare. Nonetheless, we conjecture that if mean-imputed perceived values do not dramatically distort inherent true values, then proxy welfare can still be expected to perform well as an approximation of true welfare.\nFigure 8 shows performance for all methods considered in Sec. 5.1 on mean-imputed choice behavior, for increasing k and for a range of possible CE prices. For comparison we also include results for our main zero-imputed choice model (mid-range CE prices are used in Sec. 5.1). As can be seen, our approach retains performance for mean-imputed choices across all considered pricing schemes. Whereas for zero-imputed choices overall welfare decreases when prices are higher (likely since higher prices increase null choices), mean-imputed choices exhibit a similar degree of welfare regardless of the particular price range."
        },
        {
            "heading": "G ADDITIONAL EXPERIMENTAL RESULTS: REAL DATA",
            "text": ""
        },
        {
            "heading": "G.1 REAL DATA: ADDITIONAL DATASET - YELP RESTAURANTS",
            "text": "Here we present a replication of our main experiment in and an additional dataset\u2014the restaurants portion of the Yelp! reviews dataset, which is publicly-available.15 We use the same preprocessing\n15https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\nprocedure and experimental setup as for MovieLens (see Appendix E), but also filter to keep restaurants that received at least 20 reviews, and users that gave at least 20 reviews. Figure 10 shows results. As can be seen, general trends are qualitatively similar to the those observed for the Movielens dataset (Figure 4 in the main paper)."
        },
        {
            "heading": "G.2 REAL DATA: PRICING SCHEMES",
            "text": "Here we present robustness results for additional pricing schemes, which complement our results from Sec. 5.2. In particular, we examined performance for:\n\u2022 Prices set by solving Eq. (10), but for noisy valuations v + \u03f5v , for increasing levels of noise \u03f5. These simulate a setting where prices are CE, but for the \u2018wrong\u2019 valuations, p\u2217(v + \u03f5v). Results are shown in Figure 9 (left).\n\u2022 Prices that interpolate from mid-range CE prices (as in the main experiments) to heuristicallyset, non-CE prices. Specifically, here we use prices based on average values assigned by users to items. Results are shown in Figure 9 (right).\nOverall, as in the main paper, moving away from CE prices causes a reduction in potential welfare, and in the performance of all methods. Results here demonstrate that in the above additional pricing settings, our approach is still robust in that it maintains it\u2019s relative performance compared to baselines and the welfare oracle.\nG.3 REAL DATA: THE IMPORTANCE OF \u03bb\nIn principle, and due to the counterfactual nature of learning representations (see Appendix C, tuning \u03bb requires experimentation, i.e., deploying a learned masking model \u00b5\u0302 trained on data using some \u03bb, to be evaluated on other candidate \u03bb\u2032. Nonetheless, in our experiments we observe that learning is fairly robust to the choice of \u03bb, even if kept constant throughout training. Figure 12 (bottom-right) shows welfare (normalized) obtained for a different \u03bb on Movielens using d = 12. As can be seen, any \u03bb > 0.5 works well and on par with our heuristic choice of \u03bb = 1\u2212 k/2d, used in Sec. 5."
        },
        {
            "heading": "G.4 REAL DATA: RELATIVE AND ABSOLUTE PERFORMANCE",
            "text": "In Sec. 5, for our experiments which vary k, we chose to portray results normalized from below to match random performance (random). This was mainly since the overall effect on performance of increasing k is larger than that which can be obtained by any method (i.e., the gap between random and oracle). For completeness, Figure 12 (top row) shows unormalized results, which show in absolute terms how overall performance increases for k. Figure 12 (bottom-left) shows results normalized from both below (matching random) and above (matching oracle); as can be seen, our approach obtains fairly constant relative performance across k. For completeness, Figure 11 shows in more detail the number of allocated items for the d = 12 setting."
        }
    ],
    "year": 2023
}