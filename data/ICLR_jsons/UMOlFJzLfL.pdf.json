{
    "abstractText": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.",
    "authors": [],
    "id": "SP:9cc3dc6b5be3b03460349a36c0ef7096ec317256",
    "references": [
        {
            "authors": [
                "A. Agarwala",
                "Y. Dauphin"
            ],
            "title": "Sam operates far from home: eigenvalue regularization as a dynamical phenomenon",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Z. Allen-Zhu",
                "Y. Li",
                "Z. Song"
            ],
            "title": "A convergence theory for deep learning via over-parameterization",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "M. Andriushchenko",
                "F. Croce",
                "M. M\u00fcller",
                "M. Hein"
            ],
            "title": "Flammarion. A modern look at the relationship between sharpness and generalization",
            "venue": "arXiv preprint arXiv:2302.07011,",
            "year": 2023
        },
        {
            "authors": [
                "P.L. Bartlett",
                "P.M. Long",
                "O. Bousquet"
            ],
            "title": "The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima, 2022",
            "venue": "URL https://arxiv.org/abs/2210.01513",
            "year": 2022
        },
        {
            "authors": [
                "K. Behdin",
                "Q. Song",
                "A. Gupta",
                "A. Acharya",
                "D. Durfee",
                "B. Ocejo",
                "S. Keerthi",
                "R. Mazumder"
            ],
            "title": "mSAM: Micro-batch-averaged sharpness-aware minimization",
            "venue": "arXiv preprint arXiv:2302.09693,",
            "year": 2023
        },
        {
            "authors": [
                "R. Bhatia"
            ],
            "title": "Matrix analysis, volume 169",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "L. Bottou"
            ],
            "title": "Stochastic gradient learning in neural networks",
            "venue": "In Proceedings of Neuro-Ni\u0302mes 91, Nimes, France,",
            "year": 1991
        },
        {
            "authors": [
                "P. Chaudhari",
                "A. Choromanska",
                "S. Soatto",
                "Y. LeCun",
                "C. Baldassi",
                "C. Borgs",
                "J. Chayes",
                "L. Sagun",
                "R. Zecchina"
            ],
            "title": "Entropy-SGD: Biasing gradient descent into wide valleys",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "J. Cohen",
                "S. Kaur",
                "Y. Li",
                "J. Kolter",
                "A. Talwalkar"
            ],
            "title": "Gradient descent on neural networks typically occurs at the edge of stability",
            "venue": "arXiv preprint arXiv:2103.00065,",
            "year": 2021
        },
        {
            "authors": [
                "J.M. Cohen",
                "B. Ghorbani",
                "S. Krishnan",
                "N. Agarwal",
                "S. Medapati",
                "M. Badura",
                "D. Suo",
                "D. Cardoze",
                "Z. Nado",
                "G.E. Dahl",
                "J. Gilmer"
            ],
            "title": "Adaptive gradient methods at the edge of stability, 2022",
            "venue": "URL https://arxiv. org/abs/2207.14484",
            "year": 2022
        },
        {
            "authors": [
                "Y. Cooper"
            ],
            "title": "Global minima of overparameterized neural networks",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2021
        },
        {
            "authors": [
                "P. Foret",
                "A. Kleiner",
                "H. Mobahi",
                "B. Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Flat Minima",
            "venue": "Neural Computation, 9(1):1\u201342,",
            "year": 1997
        },
        {
            "authors": [
                "D. Huang",
                "J. Niles-Weed",
                "J. Tropp",
                "R. Ward"
            ],
            "title": "Matrix concentration for products",
            "venue": "Foundations of Computational Mathematics,",
            "year": 2022
        },
        {
            "authors": [
                "P. Izmailov",
                "D. Podoprikhin",
                "T. Garipov",
                "D. Vetrov",
                "A. Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "In 34th Conference on Uncertainty in Artificial Intelligence 2018,",
            "year": 2018
        },
        {
            "authors": [
                "S. Jastrzebski",
                "M. Szymczak",
                "S. Fort",
                "D. Arpit",
                "J. Tabor",
                "K. Cho",
                "K. Geras"
            ],
            "title": "The break-even point on optimization trajectories of deep neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Jiang",
                "B. Neyshabur",
                "H. Mobahi",
                "D. Krishnan",
                "S. Bengio"
            ],
            "title": "Fantastic generalization measures and where to find them, 2019",
            "year": 2019
        },
        {
            "authors": [
                "N. Keskar",
                "D. Mudigere",
                "J. Nocedal",
                "M. Smelyanskiy",
                "P. Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "year": 2017
        },
        {
            "authors": [
                "M. Kim",
                "D. Li",
                "S.X. Hu",
                "T.M. Hospedales"
            ],
            "title": "Fisher sam: Information geometry and sharpness aware minimisation, 2022",
            "venue": "URL https://arxiv.org/abs/2206.04920",
            "year": 2022
        },
        {
            "authors": [
                "J. Kwon",
                "J. Kim",
                "H. Park",
                "I.K. Choi"
            ],
            "title": "Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks",
            "venue": "In Proc. of ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Q. Li",
                "C. Tai",
                "E. Weinan"
            ],
            "title": "Stochastic modified equations and adaptive stochastic gradient algorithms",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "S. Liu",
                "D. Papailiopoulos",
                "D. Achlioptas"
            ],
            "title": "Bad global minima exist and sgd can reach them",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liu",
                "S. Mai",
                "X. Chen",
                "C. Hsieh",
                "Y. You"
            ],
            "title": "Towards efficient and scalable sharpness-aware minimization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "C. Ma",
                "L. Ying"
            ],
            "title": "On linear stability of sgd and input-smoothness of neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "B. Neyshabur",
                "R. Tomioka",
                "N. Srebro"
            ],
            "title": "In search of the real inductive bias: On the role of implicit regularization",
            "venue": "in deep learning,",
            "year": 2015
        },
        {
            "authors": [
                "C. Szegedy",
                "V. Vanhoucke",
                "S. Ioffe",
                "J. Shlens",
                "Z. Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "S. Ujv\u00e1ry",
                "Z. Telek",
                "A. Kerekes",
                "A. M\u00e9sz\u00e1ros",
                "F. Husz\u00e1r"
            ],
            "title": "Rethinking sharpness-aware minimization as variational inference, 2022",
            "venue": "URL https://arxiv.org/abs/2210.10452",
            "year": 2022
        },
        {
            "authors": [
                "K. Wen",
                "T. Ma",
                "Z. Li"
            ],
            "title": "How does sharpness-aware minimization minimize sharpness?, 2022",
            "venue": "URL https://arxiv.org/abs/2211.05729",
            "year": 2022
        },
        {
            "authors": [
                "L. Wu",
                "W. Su"
            ],
            "title": "The implicit regularization of dynamical stability in stochastic gradient descent",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "L. Wu",
                "Z. Zhu",
                "E. Weinan"
            ],
            "title": "Towards understanding generalization of deep learning: Perspective of loss",
            "venue": "landscapes. ArXiv,",
            "year": 2017
        },
        {
            "authors": [
                "L. Wu",
                "C. Ma"
            ],
            "title": "How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "L. Wu",
                "M. Wang",
                "W. Su"
            ],
            "title": "The alignment property of sgd noise and how it helps select flat minima: A stability analysis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xie",
                "I. Sato",
                "M. Sugiyama"
            ],
            "title": "A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Xie",
                "Q. Tang",
                "Y. Cai",
                "M. Sun",
                "P. Li"
            ],
            "title": "On the power-law hessian spectrums in deep learning",
            "venue": "arXiv preprint arXiv:2201.13011,",
            "year": 2022
        },
        {
            "authors": [
                "C. Zhang",
                "S. Bengio",
                "M. Hardt",
                "B. Recht",
                "O. Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization, 2017",
            "year": 2017
        },
        {
            "authors": [
                "J. Zhuang",
                "B. Gong",
                "L. Yuan",
                "Y. Cui",
                "H. Adam",
                "N. Dvornek",
                "S. Tatikonda",
                "J. Duncan",
                "T. Liu"
            ],
            "title": "Surrogate gap minimization improves sharpness-aware training",
            "venue": "arXiv preprint arXiv:2203.08065,",
            "year": 2022
        },
        {
            "authors": [
                "L. Ziyin",
                "B. Li",
                "T. Galanti",
                "M. Ueda"
            ],
            "title": "The probabilistic stability of stochastic gradient descent",
            "venue": "arXiv preprint arXiv:2303.13093,",
            "year": 2023
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "Here, we summarize the relevant information needed to state Theorem",
            "venue": "B RESULT OF WU ET AL",
            "year": 2022
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "\u2207f(xi,w)\u2207f(xi,w) T be the Fisher matrix that characterizes the local geometry of the loss landscape. Next, Wu et al. (2022) define the loss-scaled alignment factor",
            "year": 2022
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "w\u2217 be a global minima that is linearly stable (Definition 4). Denote by \u03bc(w) the alignment factors for linearized SGD",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Stochastic Gradient Descent (SGD) is a fundamental optimization algorithm widely used in practice. In addition to its computational efficiency, there is irrefutable evidence of its superior generalization performance even on non-convex functions, including neural networks (Bottou, 1991). For large over-parameterized neural networks, the number of points to fit is often much less than the number of free parameters in the model. In this case, there is often a high-dimensional manifold of model weights that can perfectly fit the data (Cooper, 2021); hence, focusing solely on the ability of an optimizer to minimize the loss function ignores a central part of training such networks. The primary goal in a model is not to achieve high performance on a training data set but rather to achieve strong generalization performance on previously unseen data. Although we currently lack a comprehensive theoretical explanation for the empirical success of SGD in these models, a promising hypothesis suggests that SGD naturally applies a form of implicit regularization (Zhang et al., 2017; Neyshabur et al., 2015) when multiple optima are present (Keskar et al., 2017; Liu et al., 2020). This phenomenon guides the iterative process towards more favorable optima purely through algorithmic choices.\nIn order to measure this distinguishing favorability between more and less desirable optima, prior work has proposed the concept of sharpness at a minimum as an indicator of the generalization performance of the trained model. Lower sharpness is often indicative of better generalization performance (Hochreiter & Schmidhuber, 1997). There is a wealth of empirical work exploring the relationship between sharpness and generalization performance, particularly in networks trained with SGD, e.g., (Jiang et al., 2019; Jastrzebski et al., 2019; Andriushchenko et al., 2023; Wu et al., 2017; Chaudhari et al., 2017; Izmailov et al., 2018). Furthermore, these ideas have led to new optimizers which deliberately reduce sharpness and are observed to attain improved empirical performance (Behdin et al., 2023; Foret et al., 2020). Although the connection between sharpness and generalization performance isn\u2019t precise or completely understood, the partial achievements of this theory has inspired several works, including ours, to investigate how SGD implicitly tends to converge to flatter optima.\nSharpness has been defined in several ways in prior literature, but most commonly, the sharpness of a trained neural network at a minimum is the maximum eigenvalue of the Hessian of the loss with respect to weights. Intuitively, one can see that if w\u2217 is a stationary point of a smooth function f(w) with Hessian H(w), then for perturbation v such that \u2225v\u22252 = \u03f5, f(w\u2217+v) < f(w\u2217)+O(\u03f52)\u00b7\u03bb1(H(w\u2217)), where \u03bb1(\u00b7) denotes the maximum eigenvalue. This relation follows from the Taylor expansion of f(\u00b7) around w\u2217, and we see that the sharpness at w\u2217, i.e., \u03bb1(H(w\u2217)), determines how rapidly small perturbations to the weights w can increase the value of f(w). In other words, model sharpness measures how robust the loss of the trained model is to small perturbations of the model parameters.\nIn this paper, our focus is on providing a precise characterization of how the SGD hyperparameters and properties of the loss function affect its implicit regularization of model sharpness. Towards this goal, we consider the linearized dynamics of SGD (defined in Section 2) close to the optimum. When w is close to w\u22c6, it allows us to make a useful simplification by focusing on the quadratic approximation of the loss function. In particular, we consider mean-squared stability, that is, w\u2217 is considered unstable if iterates of SGD diverge from w\u2217 under the \u21132-norm in expectation. Unlike differential equation-based approaches, which liken the SGD dynamics to a continuous flow for eliciting implicit regularization properties (Li et al., 2017; Xie et al., 2021), the linear stability analysis does not break down in the regime of large step sizes. Moreover, the linearized dynamics in gradient descent (GD) has been empirically validated to predict the sharpness of overparameterized neural networks due to the Edge-of-Stability phenomenon (Wu et al., 2018; Cohen et al., 2021). This behavior has also been observed in other optimizers (Cohen et al., 2022; Jastrzebski et al., 2019; Bartlett et al., 2022; Wen et al., 2022; Ujv\u00e1ry et al., 2022), lending further weight to this theoretical framework (Agarwala & Dauphin, 2023). While prior work has already considered the linear stability of SGD (Wu et al., 2022; 2018; Ma & Ying, 2021; Ziyin et al., 2023; Agarwala & Dauphin, 2023), our analysis provides substantial advancement over these prior results as we detail below.\nContributions:\n\u2022 We offer an interpretable yet rigorously established sufficient condition to determine the instability of a point w\u2217 under the linearized dynamics of SGD (Theorem 1). Importantly, unlike previous works, our bound applies to any additively decomposable loss function.\n\u2022 Our sufficient condition hinges on a coherence measure \u03c3, which we introduce to capture the relevant geometric characteristics of the loss surface around a minimum. This measure is intuitively connected to the Gram matrix of point-wise loss Hessians. We provide additional context and rationale for introducing this measure in Section 3.1.\n\u2022 We demonstrate that our bound is nearly optimal across a natural range of SGD hyperparameters (Theorem 2). This implies that our analysis, which offers a sufficient condition for the stability of linearized SGD dynamics, is precise and closely aligned with the behavior of SGD across various choices for the coherence measure \u03c3, batch size, and learning rate.\n\u2022 In the course of deriving Theorem 1, we present an independently useful technical lemma (Lemma 4.1). This lemma provides sufficient conditions for (i) the divergence of linearized SGD dynamics and (ii) the convergence of linearized SGD dynamics toward the specified minima. An intriguing aspect of this lemma is that it suggests a multiplicative decoupling effect between the impact of batch size and learning rate on instability and the instability introduced by the geometry of the Hessian.\n\u2022 Finally, we corroborate the validity of our theoretical findings through a series of experiments conducted on additively decomposable quadratic loss functions. Our experimental results align with our theory and underscore the significance of the Hessian coherence measure \u03c3 in determining the conditions under which SGD dynamics diverge.\nRelated Work: While extensive research investigates the intricate relationship between optimization methods, generalization error, and sharpness, the prior work most relevant to ours focuses on a linear stability analysis of SGD. In this section, we briefly compare our results to related research. However, we defer a detailed comparison of our results until Section 3.2.1, which follows the formal introduction of the problem setup and the presentation of our primary theorem.\nAn important line of work in this area is that of Wu et al. (2018; 2022); Wu & Su (2023), which progressively distill more theoretical insight into how the linear dynamics of SGD affect final sharpness of a neural network. Our work goes beyond this in multiple ways. For sake of comparison, the result in this line of work most related to our contribution is Theorem 3.3 in Wu et al. (2022), and\nwe restate this result in Appendix B. Our results are significantly more general than this theorem, in that our results apply to any general additively decomposable loss function, which answers the question raised in Agarwala & Dauphin (2023) on the behavior of SGD for cross-entropy loss.1 Additionally, we guarantee a stronger form of divergence. Even with relaxed conditions and stronger implications, the condition of our theorem is practically easier to satisfy than Theorem 3.3 in Wu et al. (2022).\nOther research delves into related questions, although our study may not align directly with them. For example, Jastrzebski et al. (2019) combine previous analysis by Wu et al. (2018) with additional assumptions about how the optimizer behaves. The paper demonstrates the impact of learning rate and batch size on sharpness throughout the training trajectories of SGD. Agarwala & Dauphin (2023) examine how batch size affects sharpness within SGD training trajectories, particularly in the context of second-order regression models. Ma & Ying (2021) provide a meticulous characterization of linear stability. However, this characterization might not be immediately interpretable and is primarily used to draw connections between behaviours of SGD and Sobolev regularization. Ziyin et al. (2023) focuses on the convergence and divergence of linearized dynamics in probability rather than in expected distance from the optimum, as considered by the other work we have mentioned."
        },
        {
            "heading": "2 PROBLEM FORMULATION",
            "text": "We consider the case where SGD is used to minimize an additively decomposable loss function L(w) = 1n \u2211n i=1 \u2113i(w), where each \u2113i(w) is twice-differentiable and w \u2208 Rd. Given learning rate \u03b7 > 0 and batch size B \u2208 [n], the dynamics of SGD are defined by the recurrence wt+1 = wt\u2212 \u03b7B \u2211 i\u2208S \u2207\u2113i(wt), where S is uniformly sampled from all B sized subsets of [n]. To facilitate our probabilistic analysis, we apply two standard simplifications. First, we consider Bernoulli sampling rather than sampling with replacement so that i \u2208 S with probability B/n and the event i \u2208 S is independent of the event j \u2208 S for all i \u0338= j. Second, we consider the quadratic approximation to the loss around a fixed point w\u2217, so that \u2113i(w) \u2248 \u2113i(w\u2217) + (w \u2212 w\u2217)T\u2207\u2113i(w\u2217) + 12 (w \u2212 w\u2217)T\u22072\u2113i(w\u2217)(w \u2212 w\u2217) (Wu et al., 2022; Ma & Ying, 2021). Since the dynamics of SGD are shift-invariant, we can assume w\u2217 = 0 without loss of generality. We restrict our attention to the case where w\u2217 is a local minimum of \u2113i(\u00b7) for all i \u2208 [n]. This assumption is particularly relevant in the context of overparameterized neural networks, where it is common for data to fit the model almost perfectly (Allen-Zhu et al., 2019).\nIn the described linearized setting, \u2207\u2113i(w) = \u2207w(\u2113i(w\u2217) + 12w T\u22072\u2113i(w\u2217)w) = \u22072\u2113i(w\u2217)w. Define Hi = \u22072\u2113i(w\u2217), which is the Hessian of \u2113i(\u00b7) at w\u2217. Note that Hi \u2208 Rd\u00d7d is a PositiveSemidefinitie matrix (PSD) since w\u2217 is a local minimum of \u2113i(\u00b7) (we refer the reader to Appendix A.1 for notation and necessary background). The linearized dynamics of SGD in our setting of interest follows below. Definition 1. Linearized SGD Dynamics: Let {Hi}i\u2208[n] be a set of d \u00d7 d PSD matrices, and let H = 1n \u2211n i=1 Hi. Let \u03b7 > 0 denote the learning rate and B \u2208 [n] be the batch size. The linearized SGD dynamics are defined by the recurrence relation:\nwt+1 =\n( I\u2212 \u03b7\nB \u2211 i\u2208S Hi\n) wt, (1)\nwhere i \u2208 S with probability Bn and the event i \u2208 S is independent from the event j \u2208 S for all i \u0338= j. We will refer to J = I\u2212 \u03b7H and J\u0302 \u223c I\u2212 \u03b7B \u2211 i\u2208S Hi as the Jacobians of GD and SGD respectively. Note that using B = n recovers the gradient descent dynamics."
        },
        {
            "heading": "3 THE ROLE OF HESSIAN GEOMETRY IN SGD INSTABILITY",
            "text": "This section introduces and motivates the Hessian coherence measure \u03c3({Hi}i\u2208[n]). Subsequently, we utilize this measure to present our primary result, Theorem 1. This theorem furnishes a sufficient\n1Note that linear stability is not meaningful for pure cross-entropy loss on perfectly fit data, since \u2225w\u2217\u22252 is not finite. However, our theory holds when using label smoothing (Szegedy et al., 2016), as commonly done in practice.\ncondition for E\u2225wk\u22252 to diverge as k \u2192 \u221e. Following this, we demonstrate that our established sufficient condition in Theorem 1 is nearly optimal across a broad range of hyperparameters. We formally state this optimality result in Theorem 2."
        },
        {
            "heading": "3.1 HESSIAN COHERENCE MEASURE",
            "text": "Note that, to understand the behavior of linearized SGD dynamics around a point w\u2217, it suffices to consider how they operate under various configurations of \u03b7, B, and {Hi}i\u2208[n]. To illustrate the effect of {Hi}i\u2208[n], let us explore two extreme scenarios.\nFirst Setting: Suppose we have Hi = e1eT1 \u2200i, where ei represents the i-th canonical basis vector. In this scenario, with all Hi being identical, we anticipate that the stochastic dynamics will closely resemble the deterministic dynamics. This expectation arises from the fact that 1B \u2211 i\u2208S Hi should exhibit strong concentration around H. Furthermore, when the elements of S are sampled from [n] without replacement, the SGD dynamics coincide with the GD dynamics. Therefore, we expect no difference in the characterization of stability of the respective linearized dynamics.\nSecond Setting: Now, let us consider the opposite extreme, where all Hi matrices are orthogonal, meaning that their inner products satisfy tr[HiHj ] = 0 \u2200i \u0338= j. In this scenario, we anticipate that randomness exerts a substantial influence on the steps taken by SGD. In the context of a full linear GD step, the component projected onto the subspace defined by Hi is \u03b7Hiw/n. However, in the stochastic setting, if Hi is not selected in the sampling process, no step is taken in this particular subspace. Conversely, if it is selected, the step taken is \u03b7Hiw/B, which significantly overshoots the deterministic step by a factor of n/B.\nThese extreme cases serve as illustrative examples, highlighting the importance of the relative geometric arrangement within the set {Hi}i\u2208[n] in determining the stability of the dynamics (alongside the learning rate and batch size). While we establish both sufficient and necessary conditions to address this geometric aspect in Lemma 4.1, our aim is also to offer an intuitive characterization that captures the significance of {Hi}i\u2208[n] without resorting to complex analytical expressions. To that end, we introduce the following measure, which succinctly captures the geometric structure within {Hi}i\u2208[n]. Definition 2. Coherence Measure: For a given set of PSD matrices {Hi}i\u2208[n], define S \u2208 Rn\u00d7n such that Sij = \u2225H1/2i H 1/2 j \u2225F . Equivalently, we may define S as the entry-wise square root of the Gram matrix of {Hi}i\u2208[n] under the trace inner product. The coherence measure \u03c3 is then defined as:\n\u03c3 = \u03bb1(S)\nmaxi\u2208[n] \u03bb1(Hi) .\nTo provide some insight into this measure, we can consider {Hi}i\u2208[n] as a collection of n vectors in Rd\u00d7d, endowed with the trace inner-product. The Gram matrix of a set of vectors compiles the pairwise inner-products among the vectors, thus representing the relative alignments and magnitudes of these vectors within the space, and the matrix S is an entry-wise renormalization of the Gram matrix. In the case where rank(Hi) = 1 for all i \u2208 [n], \u03bb1(Hi) is the i-th diagonal entry of S, and \u03c3 measure how close S is to being diagonally dominant. Due to the construction of S, \u03c3 then measures the cross-interactions within {Hi}i\u2208[n] relative to the magnitude of the individual Hessians in {Hi}i\u2208[n] under the Frobenius norm. The case where all Hi are rank one is particularly important since it occurs when \u2207\u2113(w\u2217) = 0 under loss functions such as cross-entropy loss.\nLet us examine the two extreme cases mentioned earlier. In the first case, where Hi = e1eT1 \u2200i, it follows that \u2225H1/2i H 1/2 j \u2225F = 1 \u2200i, j \u2208 [n]. Consequently, S becomes an n\u00d7 n matrix consisting of all ones, yielding \u03bb1(S) = \u221a n. Meanwhile, in the scenario, where Hi matrices are mutually orthogonal, \u2225H1/2i H 1/2 j \u2225F = 0 \u2200i \u0338= j. Therefore, S becomes the identity matrix I, and \u03bb1(S) = 1. In both cases, \u03bb1(Hi) = 1 for all i \u2208 [n]. Consequently, in the first case, \u03c3 = \u221a n, and in the second case, \u03c3 = 1. This demonstrates that our coherence measure, \u03c3, effectively distinguishes between these two extreme scenarios and increases as the alignment among the Hi matrices grows stronger. Below, we show how this measure allows us to establish a natural sufficient condition for the divergence of these linear dynamics."
        },
        {
            "heading": "3.2 SIMPLIFIED DIVERGENCE CONDITION",
            "text": "We present our sufficient condition for the linear dynamics to diverge, which relies solely on the values of \u03bb1(H), \u03b7, B, n, and our coherence measure \u03c3. Note that this bound aligns with our intuitive expectations based on the extreme cases we examine. When all Hi = e1eT1 and \u03c3 = \u221a n, the second condition in the theorem cannot be met. In such cases, we must resort to the GD condition for instability, namely, \u03bb1(H) > 2\u03b7 . Conversely, when Hi = eie T i \u2200i and \u03c3 = 1, the theorem asserts that the linear dynamics will diverge even for small values of \u03bb1(H), especially when B is small relative to n. This behavior aligns with our expectations based on the different scenarios we consider.\nTheorem 1. Let {J\u0302i}i\u2208N be a sequence of i.i.d. copies of J\u0302 defined in Definition 1. Let {Hi}i\u2208[n] have coherence measure \u03c3. If,\n\u03bb1(H) > 2\n\u03b7 or \u03bb1(H) >\n\u03c3 \u03b7 \u00b7 ( n B \u2212 1 )\u22121/2 , then, lim k\u2192\u221e E\u2225J\u0302k...J\u03022J\u03021\u22252 = \u221e.\nWe defer all proofs to Appendix A. Note that the quantity E\u2225J\u0302k...J\u03021\u22252 = Emaxw0:\u2225w0\u22252=1 \u2225wk\u22252, where wk is the random vector determined by the SGD dynamics in Definition 1, when the dynamics start from w0. In other words, if E\u2225J\u0302k...J\u03021\u22252, diverges as k \u2192 \u221e, the linearized SGD dynamics diverge from almost every starting point w0 \u2208 Rd. We highlight two observations of the condition in Theorem 1. First, this analysis supports \u201csquared-scaling\u201d between batch size and learning rate, that is, if B is increased proportional to \u03b72, the stability will not increase. Second, the Hessian alignment (captured by \u03c3) can cause instability even when \u03b7 is small and B is on the order of n."
        },
        {
            "heading": "3.2.1 COMPARISON TO PRIOR WORK",
            "text": "Based on the formal introduction of our result, we now provide a more detailed comparison to the result of Wu et al. (2022). The condition outlined in Theorem 3.3 of Wu et al. (2022) represents one of the most recent findings in this research field, which we restate in Appendix B. The contrapositive of this theorem provides a sufficient condition, namely \u2225H\u2225F > 1\u03b7 \u221a B \u00b50\n, to guarantee instability. Here, \u00b50 serves as an alignment metric, empirically argued to have a practical lower bound.\nTheorem 1 has multiple advantages over these prior results. First, the analysis in Wu et al. (2022) is confined solely to the MSE loss function. Second, their definition of stability entails that E[L(wk)] \u2264 C \u00b7 E[L(w1)] \u2200k \u2208 N, where C > 1 is a constant. This definition is notably weaker than our notion of stability. Finally, despite Theorem 1 holding for more general loss functions and guaranteeing a stronger notion of stability/instability, it is also easier to satisfy under the typical setting where H has low stable rank, i.e., when \u2225H\u22252F /\u2225H\u222522 is small. Let us ignore the effects of the measures \u00b50 and \u03c3 by considering both equal to one, then Theorem 3.3 (Wu et al., 2022) guarantees instability if \u2225H\u2225F > 1\u03b7 \u221a B while Theorem 1 guarantees instability if \u03bb1(H) > 1\u03b7 \u221a B n , which is a more general condition when the stable rank of H is less than n, as is typical in practical settings (Xie et al., 2022)."
        },
        {
            "heading": "3.3 OPTIMALITY OF THEOREM 1",
            "text": "Theorem 1 provides a sufficient condition for the linear dynamics to diverge, where the condition is of the form \u03bb1(H) > f(\u03b7, \u03c3, n,B), where f(\u03b7, \u03c3, n,B) = \u03c3\u03b7 ( n B \u2212 1 )\u22121/2 . The next theorem shows that our condition is optimal in the sense that, for a natural range of parameters (when \u03c3, nB = O(1)), the function f(\u03b7, \u03c3, n,B) is within a constant factor of its lowest possible value. This shows that our sufficient condition cannot be significantly relaxed without relying on other information about the set {Hi} as we do in Lemma 4.1. Overall, the following theorem demonstrates that our sufficient condition for divergent dynamics approaches optimality among all sufficient conditions that rely solely on \u03b7, \u03c3, B, \u03bb1(H), and n. However, it does not rule out further improvement in the important regime where B \u226a n. Theorem 2. For every choice of \u03bb1 > 0, n \u2208 N, B \u2208 [n], \u03b7 > 0, and \u03c3 \u2208 [n], that satisfies:\n\u03bb1 < 2\u03c3 \u03b7 \u00b7 ( \u03c3 + n B \u2212 1 )\u22121 ,\nThere exists a set of PSD matrices {Hi}i\u2208[n] such that \u03bb1(H) = \u03bb1 and limk\u2192\u221e E\u2225J\u0302k...J\u03021\u22252F < n."
        },
        {
            "heading": "4 SHARP STABILITY CONDITIONS OF LINEARIZED SGD",
            "text": "In the previous section, we provide a measure of the geometric coherence in {Hi}i\u2208[n] along with a theorem that provides sufficient conditions for the dynamics of SGD (Definition 1) to diverge. The proof of this theorem relies on part (i) of the following technical lemma. Aside from its utility in establishing the aforementioned theorem, the statement of the following lemma also imparts valuable insights into the behavior of linearized SGD dynamics.\nThe proof of the following lemma relies on the observation that \u2225M\u222522 \u2264 \u2225M\u22252F \u2264 d\u2225M\u222522 for all M \u2208 Rd\u00d7d, where \u2225 \u00b7 \u2225F denotes the Frobenius norm. Hence, we may focus on divergence in the Frobenius norm of the k-step linearized dynamics. Now, E\u2225J\u0302k...J\u03021\u22252F = E[tr[J\u0302k...J\u030221...J\u0302k]] = tr[E[J\u0302k...J\u030221...J\u0302k]] by linearity. By operator monotonicity of the trace, we further have tr[Nk] \u2265 tr[E[J\u0302k...J\u030221...J\u0302k]] \u2265 tr[Mk], where Nk \u2ab0 E[J\u0302k...J\u030221...J\u0302k] \u2ab0 Mk under the Loewner ordering (see Appendix A.1). The technical challenge in our proof lies in composing an inductive argument to define matrices Nk and Mk. We opt for an approach that directly bounds the matrix of the k-step linear dynamics under the Loewner ordering, which does introduce greater technical complexity compared to using norm inequalities, as seen in previous work. However, this added complexity is essential to accurately account for the alignment in the unstable eigenvectors of each J\u0302i, allowing us to provide a thorough characterization of the instability of SGD dynamics.\nLemma 4.1. Let J\u0302i be independent Jacobians of SGD dynamics described in Definition 1.\n(i) If\n\u03bb1(H) > 2\n\u03b7 or lim k\u2192\u221e\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 \u2225Hyk ...Hy1\u22252F = \u221e,\nthen limk\u2192\u221e E\u2225J\u0302k...J\u03021\u22252F = \u221e. (ii) If, for some \u03f5 \u2208 (0, 1),\n\u03f5 \u03b7 < \u03bbi(H) < 2\u2212 \u03f5 \u03b7 for all i \u2208 [d] and lim k\u2192\u221e 1 \u03f5k\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 \u2225Hyk ...Hy1\u22252F = 0,\nthen limk\u2192\u221e E\u2225J\u0302k...J\u03021\u22252F = 0.\nNotice that part (i) and part (ii) of this theorem are complementary in the sense that the condition of part (ii) is nearly the negation of the condition in part (i) except for the additional \u03f5 factor. In a sense, the parameter \u03f5 captures the balance of how close we are to instability in GD dynamics, i.e. \u03bb1(H) > 2 \u03b7 , and how much additional instability is added by the stochasticity in the dynamics of SGD. 2\nTo provide a more detailed explanation of this intuition, let us consider the setting where B \u226a n. The second term from part (ii) of the above Lemma can be approximated as:(\n\u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 \u2225Hyk ...Hy1\u22252F \u2248 \u03b72k nkBk n\u2211 y1...,yk=1 \u2225Hyk ...Hy1\u22252F = \u03b72k Bk \u00b7 E\u2225Ak...A1\u22252F ,\n(2)\nwhere Ai is independently sampled uniformly from the set {Hi}i\u2208[n]. Interestingly, this implies that the effect of the parameters B and \u03b7 can be decoupled from the structure within {Hi}i\u2208[n]. In other words, if we knew the minimal learning rate \u03b7 at which linearized SGD with a fixed batch size diverges, then we would immediately be able to determine which parameter pairs (\u03b7,B) are divergent at the given point w\u2217, since the term E\u2225Ak...A1\u22252F does not change with these hyperparameters.\n2We believe that the requirement in part (ii) that \u03bbi(H) > \u03f5\u03b7 could likely be removed by more carefully accounting for alignment of the negligible eigenvectors of J\u0302 and J and relaxing the theorem to imply boundedness of the limit. However, given that the role of part (ii) in the theorem is only to contrast with part (i), we do not think this is high priority for the purpose of this paper.\nNote that determining whether the quantity \u03b7 2k\nBk E\u2225Ak...A1\u22252F diverges for arbitrary inputs of \u03b7, B,\nand set of arbitrary symmetric matrices {Hi}i\u2208[n] would be an NP-Hard problem (see Section 3.3 of Huang et al. (2022)). Even in our case, where we have the additional constraint that each Hi is PSD, we are unaware of an efficient method to determine whether \u03b7 2k\nBk E\u2225Ak...A1\u22252F diverges, motivating\nour simplified sufficient condition in Theorem 1."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we support our prior theorems by empirically evaluating the behavior of SGD on synthetic optimization problems with additively decomposable loss functions. The high-level points our experiments support are:\n\u2022 Parameter tuples that Theorem 1 guarantees divergence do indeed diverge.\n\u2022 Parameter tuples that Theorem 2 guarantees no divergence indeed do not diverge.\n\u2022 The coherence measure \u03c3 has an important effect on the instability of SGD.\n\u2022 Our theoretical results hold when using SGD that samples without replacement.\nThe first two points outlined above serve as validation for the accuracy and soundness of our theoretical results and proofs. The third point enhances the rationale for adopting the Hessian coherence measure we introduce. Lastly, the fourth point offers justification for employing SGD with Bernoulli sampling in our theoretical analysis, as its behavior mirrors that of the more prevalent SGD approach that samples without replacement. To ensure reproducibility, we include all our implementations in the supplementary material."
        },
        {
            "heading": "5.1 EXPERIMENT SETUP",
            "text": "We leverage the construction used in the proof of Theorem 2 to verify our predictions empirically, which offers two advantages: 1) we may apply the analysis of Theorem 2 for a condition that guarantees no divergence, and 2) the construction is parameterized by \u03c3, and so we may easily test the effect of varying \u03c3. In this construction, we set Hi = m \u00b7 e1eT1 for all i \u2208 [\u03c3] and Hi = m \u00b7ei\u2212\u03c3+1eTi\u2212\u03c3+1 otherwise, with m = 2n\u03c3 . We set the dimension of the space to n\u2212\u03c3+1, so there is a unique minimizer of the loss, as this does not affect divergence. Notice that this construction essentially interpolates the two extreme settings in Section 3.1 as \u03c3 varies from \u03c3 = 1 to \u03c3 = n. Additionally, note that \u03bb1(H) = 2 by construction. In our experiments, \u03b7 \u2264 1, hence the first condition of Theorem 1, i.e., \u03bb1(H) > 2/\u03b7 will not hold or be relevant to characterizing stability.\nThe loss function that corresponds to the set of Hessians {Hi}i\u2208[n] is given by the additively decomposable quadratic function L(w) = 1n \u2211n i=1 \u2113i(w), where \u2113i(w) = w\nTHiw. Note that, for this construction of {Hi}i\u2208[n], \u2113i(w) is particularly easy to compute, as it is equivalent to squaring and rescaling a single entry of w.\nAcross all experiments, we set n = 100. For each set of parameters (B, \u03b7, \u03c3), we determine whether the combination leads to divergence or not by executing SGD for a maximum of 1000 steps. Specifically, we classify a tuple as divergent if, in the majority of the five repetitions, the norm of the parameter vector w increases by a factor of 1000. Conversely, we terminate SGD prematurely and classify the point as not divergent if the norm of w decreases by a factor of 1000 during the course of the SGD trajectory.\nIt is possible to construct a regression problem with mean-squared error that results in exactly the same optimization we describe. However, we note that Theorem 3.3 in Wu et al. (2022) does not directly apply to this problem, since the corresponding loss is zero at the optimum, and hence the loss-scaled alignment factor (see Equation 4 below) of Wu et al. (2022) is undefined at the optimum. Therefore, the condition of Theorem 3.3 in Wu et al. (2022) (see Theorem 3) cannot be satisfied."
        },
        {
            "heading": "5.2 EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "5.2.1 EFFECT OF COHERENCE MEASURE AND BATCH SIZE",
            "text": "First, we look at how the stability of SGD changes as we vary coherence measure \u03c3 and batch size B. We show results for two fixed values of the learning rate, \u03b7 = 0.8 and \u03b7 = 0.5.\nTwo key observations in Figure 1 are that all tuples (\u03c3,B) that are below the boundary given by Theorem 1 indeed diverge. This is visually shown as all points below the solid black line are red (besides some aberration due to visual smoothing). Additionally, the fact that all points above the dashed line are blue indicate that tuples (\u03c3,B) which the proof of Theorem 2 guarantees converge indeed converge.\nAn intriguing observation is the pattern where the gap between the upper and lower bounds diminishes as the batch size increases. Specifically, we notice that the lower bound more closely aligns with the actual boundary between divergence and convergence across all batch sizes when \u03b7 = 0.5. However, the upper bound is closer to the true boundary when \u03b7 = 0.8.\nFinally, we observe that the coherence measure \u03c3 exerts a substantial influence on the stability of SGD. For small values of \u03c3, SGD demonstrates instability even at high values of B. This observation underscores the importance of considering the geometry of the loss surface in understanding the behavior of SGD. Furthermore, it highlights that the coherence measure is an effective tool for capturing and accounting for the contribution of loss surface geometry to the stability of SGD."
        },
        {
            "heading": "5.2.2 EFFECT OF BATCH SIZE AND LEARNING RATE",
            "text": "Next, we examine how the stability of SGD evolves when we manipulate the batch size B and the learning rate \u03b7. For this analysis, we maintain a fixed value of \u03c3 = 5, which provides a clear boundary given the granularity of the point grid. We show both the log-scale plot, since learning rate generally varies on a log-scale, and the linear-scale plot since we expect the relationship between B and \u03b72 to be roughly linear in the boundary.\nThe plots displayed in Figure 2 further corroborate the validity of Theorem 1 and Theorem 2. Additionally, the pattern continues to support the facts that the lower bound condition more closely approximates the true boundary as learning rate decreases, and the upper bound provides a tighter approximation to the true boundary as learning rate increases."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We present precise yet interpretable, necessary and sufficient conditions to determine when a point w\u2217 is stable under linearized SGD dynamics. The sufficient condition in Theorem 1 relies on a novel\ncoherence measure \u03c3 that summarizes relevant information in the loss surface geometry. We next list some open questions our work raises:\n\u2022 In future research endeavors, it would be intriguing to close the gap between Theorem 1 and Theorem 2 to establish the actual dependency of SGD stability on \u03c3 and the hyperparameters of SGD.\n\u2022 Additionally, it would be interesting to empirically measure the value of the coherence measure \u03c3 in realistic neural networks. Acquiring knowledge about the practical range of values that \u03c3 can assume would enhance the utility of the theoretical contributions provided here for predicting the behavior of SGD in real-world scenarios. Developing efficient approaches to approximate \u03c3 in large neural networks would represent a valuable step toward achieving this objective.\n\u2022 We may also consider extending the same proof techniques to characterize the stability of sharpness-aware methods (Behdin et al., 2023; Foret et al., 2020; Zhuang et al., 2022; Liu et al., 2022; Kwon et al., 2021; Kim et al., 2022), which are commonly employed for training many overparameterized models.\n\u2022 Along these lines, it would also be useful to consider whether the stability of SGD with momentum or other adaptive gradient methods could be analyzed with this approach.\n\u2022 The convergence analysis in Lemma 4.1 could possibly used to derive fine-grained local convergence rates of SGD depending on Hessian alignment."
        },
        {
            "heading": "A PROOFS",
            "text": "A.1 PROOF PRELIMINARIES\nIn this section, we provide notation and necessary background for the following proofs.\nNotation: Let [n] denote 1, 2, ..., n. Let ei denote i-th canonical basis vector. Let Bern(p) be Bernoulli distribution with parameter p. Let 0 denote the all-zero matrix or vector, where the dimension will be clear from context. Let 1 be defined similarly as the all-ones matrix or vector. Let \u2225A\u22252 denote the spectral norm of matrix A. Let \u03bbi(A) denote the i-th largest eigenvalue of the matrix A. Let \u2225A\u2225F denote the Frobenius norm of matrix A. Let \u2225A\u2225Sp denote p-Schatten norm of matrix A, that is, the p-norm of the vector of singular values of A. Fact 1. \u21131-\u21132 Norm Inequality: For any vector x \u2208 Rd, \u2225x\u22252 \u2264 \u2225x\u22251 \u2264 \u221a d\u2225x\u22252.\nFact 2. Recursive Formula for Binomial Coefficients: For all n, k \u2208 N such that k \u2264 n, the binomial coefficients satisfy the following recursive formula:(\nn\nk\n) = ( n\u2212 1 k \u2212 1 ) + ( n\u2212 1 k ) .\nThe notion of PSD matrices can be used to define a partial order on the set of symmetric matrices as follows.\nDefinition 3. Loewner Order: The Loewner order is a partial order on the set of positive semidefinite symmetric matrices. For two positive semidefinite matrices A and B, we write A \u2aaf B to denote that B\u2212A is positive semidefinite and A \u227a B to denote that B\u2212A is positive definite.\nNote that if A is a PSD matrix, then for any symmetric B, BAB must also be PSD.\nWe frequently use the following properties of the trace in our derivations.\nProperties of the Trace:\n\u2022 tr[A] = \u2211n\ni=1 \u03bbi(A).\n\u2022 Invariance to cyclic permutation: tr[ABC] = tr[CAB].\n\u2022 Linearity of the trace: tr[cA+B] = c tr[A] + tr[B], where c \u2208 R. Lemma A.1. For any matrix M \u2208 Rn\u00d7n, \u2225M\u2225F \u2264 \u2225M\u2225S1 \u2264 \u221a n\u2225M\u2225F .\nProof. This follows from the fact that \u2225M\u2225F = \u2225M\u2225S2 and \u2225M\u2225Sp is the p-norm of the spectrum of M along with applying the \u21131-\u21132-norm inequality (Fact 1).\nLemma A.2. For any length k sequence of square matrices A1...Ak \u2208 Rd\u00d7d, tr[A1A2...Ak] \u2264\u221a d\u2225A1\u2225F \u2225A2\u2225F ...\u2225Ak\u2225F .\nProof. First, it follows from Weyl\u2019s Majorant Theorem that tr[A1A2...Ak] \u2264 \u2225A1A2...Ak\u2225S1 (see Section III.5 (Bhatia, 2013)). Then, applying Lemma A.1 implies,\ntr[A1A2...Ak] \u2264 \u2225A1A2...Ak\u2225S1 \u2264 \u221a d\u2225A1A2...Ak\u2225F .\nFinally, we conclude the lemma statment by submultiplicativity of the Frobenius norm.\nA.2 PROOFS\nProof of Theorem 1\nProof. Note that \u2225M\u222522 \u2264 \u2225M\u22252F \u2264 d\u2225M\u222522 for all M \u2208 Rd\u00d7d. Therefore, by Lemma 4.1, it suffices to show that the condition of the theorem implies that the following quantity diverges towards infinity as k \u2192 \u221e:(\n\u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 \u2225Hyk ...Hy1\u2225 2 F\nFrom here, we lower bound it by the following. Note that \u2225M\u22252F \u2265 1d\u2225M\u2225 2 S1 , where \u2225M\u2225S1 is the 1-Schatten norm (see Lemma A.1).( \u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 \u2225Hyk ...Hy1\u2225 2 F \u2265 ( \u03b72 nB \u2212 \u03b7 2 n2 )k n\u2211 y1...,yk=1 1 d \u2225Hyk ...Hy1\u2225 2 S1\n\u2265 ( \u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 1 d tr[Hyk ...Hy1 ] 2\n\u2265 ( \u03b72\nnB \u2212 \u03b7\n2\nn2\n)k 1\nd n\u2211 y=1 tr[Hky ] 2\n\u2265 ( \u03b72\nnB \u2212 \u03b7\n2\nn2\n)k 1\nnd\n( n\u2211\ny=1\ntr[Hky ] )2 The last line follows by again applying the L1-L2 norm inequality. Now we show that the complexity measure \u03c3 satisfies:\nnk\nd2 \u00b7 \u03c3k \u00b7 tr[H k] \u2264 n\u2211 y=1 tr[Hky ].\nFirst, we bound the left-hand term as follows. By Lemma A.2 we know that tr[A1A2...Ak] \u2264 d\u2225A1\u2225F \u2225A2\u2225F ...\u2225Ak\u22121\u2225F \u2225Ak\u2225F . Therefore,\ntr[Hk] = 1\nnk \u00b7 n\u2211 y1...,yk=1 tr[Hyk ...Hy1 ]\n= 1\nnk \u00b7 n\u2211 y1...,yk=1 tr[(H1/2y1 H 1/2 yk )(H 1/2 yk H 1/2 yk\u22121)...(H 1/2 y2 H 1/2 y1 )]\n\u2264 d nk\n\u00b7 n\u2211\ny1...,yk=1\n\u2225H1/2y1 H 1/2 yk \u2225F \u00b7 \u2225H 1/2 yk H 1/2 yk\u22121\u2225F \u00b7 ... \u00b7 \u2225H 1/2 y2 H 1/2 y1 \u2225F\n= d\nnk \u00b7 n\u2211 y1...,yk=1 Sy1,ykSyk,yk\u22121 ...Sy2,y1\n= d nk \u00b7 tr[Sk] \u2264 d\n2\nnk \u00b7 \u03bb1(S)k.\nTherefore,\nnk\nd2 \u00b7 \u03c3k \u00b7 tr[H k] =\nnk \u00b7maxi\u2208[n] \u03bb1(Hi)k\nd2 \u00b7 \u03bb1(S)k \u00b7 tr[Hk] \u2264 max i\u2208[n] \u03bb1(Hi)\nk \u2264 n\u2211\ny=1\ntr[Hky ].\nStarting from where we left off before,\nE tr[J\u0302k...J\u030221...J\u0302k] \u2265 ( \u03b72\nnB \u2212 \u03b7\n2\nn2\n)k 1\nnd\n( n\u2211\ny=1\ntr[Hky ]\n)2\n\u2265 ( \u03b72\nnB \u2212 \u03b7\n2\nn2\n)k 1\nnd\n( nk\nd2 \u00b7 \u03c3k \u00b7 tr[H k] )2 \u2265 ( \u03b72\nnB \u2212 \u03b7\n2\nn2\n)k 1\nnd5 \u00b7 n\n2k\n\u03c32k \u00b7 \u03bb1(H)2k\n= \u03b72k ( n\n\u03c32B \u2212 1 \u03c32\n)k 1\nnd5 \u00b7 \u03bb1(H)2k\nWe see that by the condition of the theorem that \u03bb1(H) > 1\u03b7 \u00b7 ( n \u03c32B \u2212 1 \u03c32 )\u22121/2 that this above equation diverges towards infinity as k \u2192 \u221e. Hence, we conclude the theorem statement.\nProof of Theorem 2\nProof. Construct {Hi}i\u2208[n] so that Hi = m \u00b7 e1eT1 if i \u2208 [\u03c3] and Hi = 0 otherwise, where m = \u03bb1\u00b7n\u03c3 . Note that \u03bb1(H) = \u03c3n \u00b7 m = \u03bb1. Furthermore, note that \u2225H 1/2 i H 1/2 j \u2225F = m if i, j \u2208 [\u03c3] and zero otherwise. Therefore, S, the entry-wise square root of the Gram matrix (i.e, Sij = \u2225H1/2i H 1/2 j \u2225F ), has all of the first \u03c3\u00d7\u03c3 entries equal to m and the rest equal to zero. Therefore, \u03bb1(S) = m \u00b7 \u03c3. Meanwhile, maxi\u2208[n] \u03bb1(Hi) = m. Hence, we see the Hessian coherence measure for this constructed problem is indeed equal to the chosen value of \u03c3.\nWe now show that limk\u2192\u221e E\u2225J\u0302k....J\u03021\u22252F < n. Note that J\u0302 = I \u2212 \u03b7 \u2211n\ni=1 xiHi, where the xi\u2019s are i.i.d. random variables sampled from a Bernoulli distribution with parameter p = B/n. A key observation here is that all J\u0302 are diagonal (since I and all Hi are diagonal) and hence J\u03021...J\u0302k commute. Therefore,\nE\u2225J\u0302k....J\u03021\u22252F = E tr[J\u0302k...J\u030221...J\u0302k] = E tr[J\u03022k...J\u030221] = tr[E[J\u030221]k],\nwhere in the last step we use the fact that all of the J\u0302 are independent and identically distributed. Recall that the trace is the sum of the diagonal entries in a matrix and hence tr[E[J\u030221]k] = \u2211n i=1 e T i E[J\u030221]kei. Note that eTi E[J\u030221]kei = 1 when i \u0338= 1, so tr[E[J\u030221]k] = (n \u2212 1) + eT1 tr[E[J\u030221]k]e1. To show that this quantity is bounded by n, we must show that eT1 E[J\u030221]ke1 = (eT1 E[J\u030221]e1)k is bounded. Following the base case of part (i) in Lemma 4.1, we have that:\nE[J\u030221] = I\u2212 2\u03b7H+ \u03b72H2 + ( \u03b72\nnB \u2212 \u03b7\n2\nn2 ) n\u2211 i=1 H2i\nWe can then write eT1 E[J\u030221]e1 as:\neT1 E[J\u030221]e1 = 1\u2212 2\u03b7 \u03c3 n \u00b7m+ \u03b72 \u03c3\n2\nn2 m2 +\n( \u03b72\nnB \u2212 \u03b7\n2\nn2\n) \u00b7 \u03c3 \u00b7m2,\nwhere we used the fact that eT1 He1 = \u03bb1(H) = \u03c3n \u00b7m in our construction and e T 1 H 2 i e1 = m 2 if i \u2208 [\u03c3] and zero otherwise. To show convergence, we must show that the previous equation is less than 1. This is equivalent to the following condition:\n\u03b72 \u03c32\nn2 m2 +\n( \u03b72\nnB \u2212 \u03b7\n2\nn2\n) \u00b7 \u03c3 \u00b7m2 < 2\u03b7 \u00b7 \u03c3\nn \u00b7m\n\u21d0\u21d2 \u03b72 \u03c3 2\nn2 m2 + \u03b72\n( n B \u2212 1 )\n\u03c3 \u00b7 \u03c3\n2\nn2 \u00b7m2 < 2\u03b7 \u00b7 \u03c3 n \u00b7m\n\u21d0\u21d2 \u03b72\u03bb1(H)2 + \u03b72 ( n B \u2212 1 )\n\u03c3 \u00b7 \u03bb1(H)2 < 2\u03b7 \u00b7 \u03bb1(H)\n\u21d0\u21d2 \u03bb1(H) < 2 \u03b7 \u00b7 ( 1 + n/B \u2212 1 \u03c3 )\u22121 \u21d0\u21d2 \u03bb1 < 2\u03c3 \u03b7 \u00b7 ( \u03c3 + n B \u2212 1 )\u22121 .\nTherefore, we conclude the theorem statement\nProof of Lemma 4.1\nProof. First, note that \u2225M\u22252F = tr(MTM). Since J and J\u0302 are always symmetric, to prove the theorem conclusion, we must show that limk\u2192\u221e E[tr[J\u0302k...J\u03021J\u03021...J\u0302k]] = \u221e. We will achieve this by inductively proving a PSD lower bound on the expectation of this product of matrices and then leveraging the operator monotonicity of the trace. To that end, we define the matrix:\nMr = J 2r +\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 )r n\u2211 y1...,yr=1 Hyr ...H 2 y1 ...Hyr .\nWe show that E[J\u0302k...J\u03021J\u03021...J\u0302k] \u2ab0 Mk \u2200k \u2208 N. First, let us start with the base case r = 1. Recall that H = 1\nn \u2211n i=1 Hi and H\u0302 = 1 B \u2211n i=1 xiHi, where xi \u223c Bern(B/n).\nE[J\u03021J\u03021] = E[(I\u2212 \u03b7H\u03021)(I\u2212 \u03b7H\u03021)]\n= E[I\u2212 2\u03b7H\u03021 + \u03b72H\u030221]\n= E [ I\u2212 2 ( \u03b7\nB n\u2211 i=1 xiHi\n) + ( \u03b7\nB n\u2211 i=1 xiHi\n)2]\n= I\u2212 2\u03b7 n n\u2211 i=1 Hi + \u03b72 B2 n\u2211 i,j=1 E[xixj ]HiHj\nAt this point, we have simply substituted in the relevant definitions and rearranged the terms. Now, note that E[xixj ] = B 2\nn2 if i \u0338= j and E[xixj ] = Bn otherwise. We substitute this in, and use the fact that H2 = ( 1 n \u2211n i=1 Hi )2. E[J\u03021J\u03021] = I\u2212 2\u03b7\nn n\u2211 i=1 Hi + \u03b72 nB n\u2211 i=1 H2i + \u03b72 n2 n\u2211 i \u0338=j HiHj\n= I\u2212 2\u03b7 n n\u2211 i=1 Hi + \u03b72 n2 n\u2211 i,j=1 HiHj + ( \u03b72 nB \u2212 \u03b7 2 n2 ) n\u2211 i=1 H2i\n= I\u2212 2\u03b7H+ \u03b72H2 + ( \u03b72\nnB \u2212 \u03b7\n2\nn2 ) n\u2211 i=1 H2i\n= M1\nThus, we have proved the base case E[J\u030221] \u2ab0 M1, which also implies E[J\u03022k] \u2ab0 M1, since they are identically distributed. We next prove the inductive step under the assumption that E[J\u0302k\u22121...J\u030221...J\u0302k\u22121] \u2ab0 Mk\u22121. By the Law of Total Expectation and linearity,\nE[J\u0302k...J\u030221...J\u0302k] = E[E[J\u0302k...J\u030221...J\u0302k|J\u0302k]] = E[J\u0302k \u00b7 E[J\u0302k\u22121...J\u030221...J\u0302k\u22121] \u00b7 J\u0302k] \u2ab0 E[J\u0302kMk\u22121J\u0302k].\nThe arithmetic in the next part of the argument is analogous to the argument in the base case.\nE[J\u0302kMk\u22121J\u0302k] = E[(I\u2212 \u03b7H\u0302k)Mk\u22121(I\u2212 \u03b7H\u0302k)]\n= Mk\u22121 \u2212 \u03b7Mk\u22121H\u2212 \u03b7HMk\u22121 + E\n[ \u03b72\nB2 n\u2211 i,j=1 xixjHiMk\u22121Hj\n]\n= Mk\u22121 \u2212 \u03b7Mk\u22121H\u2212 \u03b7HMk\u22121 + \u03b72\nn2 n\u2211 i,j=1 HiMk\u22121Hj\n+ \u03b72 ( 1\nnB \u2212 1 n2 ) n\u2211 i=1 HiMk\u22121Hi\n= JkMk\u22121Jk + \u03b7 2\n( 1\nnB \u2212 1 n2 ) n\u2211 i=1 HiMk\u22121Hi\nNext, we substitute in the definition of Mk\u22121. Recall that if A is symmetric and B is PSD, then ABA is also PSD.\nE[J\u0302kMk\u22121J\u0302k] = Jk J2(k\u22121) + ( \u03b72 nB \u2212 \u03b7 2 n2 )k\u22121 n\u2211 y1...,yk\u22121=1 Hyk\u22121 ...H 2 y1 ...Hyk\u22121 Jk +\u03b72 ( 1\nnB \u2212 1 n2 ) n\u2211 i=1 Hi J2(k\u22121) + ( \u03b72 nB \u2212 \u03b7 2 n2 )k\u22121 n\u2211 y1...,yk\u22121=1 Hyk\u22121 ...H 2 y1 ...Hyk\u22121 Hi \u2ab0 J2k + ( \u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 Hyk ...H 2 y1 ...Hyk\n= Mk\nNote that we are able to drop the cross-terms in the first step since ABA \u2ab0 0, whenever A is symmetric and B is PSD. Therefore, we have show that E[J\u0302k...J\u030221...J\u0302k] \u2ab0 Mk for all k \u2208 [n]. By the operator monotonicity and linearity of the trace,\nE[tr[J\u0302k...J\u030221...J\u0302k]] = tr[E[J\u0302k...J\u030221...J\u0302k]] \u2265 tr[Mk]\n= tr[J2k] +\n( \u03b72\nnB \u2212 \u03b7\n2\nn2\n)k tr  n\u2211 y1...,yk=1 Hyk ...H 2 y1 ...Hyk  = tr[J2k] + ( \u03b72\nnB \u2212 \u03b7\n2\nn2 )k n\u2211 y1...,yk=1 \u2225Hyk ...Hy1\u2225 2 F\nHence, we have proved part (i) of the lemma. We will now prove part (ii). Define the following matrix:\nNr =\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 )r n\u2211 y1...,yr=1 Hyr ...H 2 y1 ...Hyr ,\nand define N0 = I. We will prove that:\nE[J\u0302k...J\u030221...J\u0302k] \u2aaf k\u2211\nr=0\n(1\u2212 \u03f5)2(k\u2212r) ( k\nr\n) Nr, (3)\nfor all k \u2208 N, where \u03f5 > 0. First, note that the condition of the theorem implies \u2212(1\u2212 \u03f5)I \u227a J \u227a (1\u2212 \u03f5)I for some \u03f5 > 0. First, note that the base case:\nE[J\u030221] \u2aaf (1\u2212 \u03f5)2N0 +N1 = (1\u2212 \u03f5)2I+ ( \u03b72\nnB \u2212 \u03b7\n2\nn2 ) n\u2211 i=1 H2i ,\nholds from our computations in part (i), since J2 \u2aaf (1 \u2212 \u03f5)2N0. Now for the inductive step. Under the assumption that Equation 3 holds for k \u2212 1, we show it also hold for k. By the same argument as in part (i), we find that:\nE[J\u0302k...J\u030221...J\u0302k] \u2aaf E [ J\u0302k ( k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u22121\u2212r) ( k \u2212 1 r ) Nr ) J\u0302k ]\n= Jk ( k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u22121\u2212r) ( k \u2212 1 r ) Nr ) Jk\n+\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 ) n\u2211 i=1 Hi ( k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u22121\u2212r) ( k \u2212 1 r ) Nr ) Hi\n\u2aaf (1\u2212 \u03f5)2 (\nk\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u22121\u2212r) ( k \u2212 1 r ) Nr )\n+\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 ) n\u2211 i=1 Hi ( k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u22121\u2212r) ( k \u2212 1 r ) Nr ) Hi\n= k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u2212r) ( k \u2212 1 r ) Nr\n+ k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u22121\u2212r) ( k \u2212 1 r ) \u00b7 ( \u03b72 nB \u2212 \u03b7 2 n2 ) n\u2211 i=1 HiNrHi\n= k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u2212r) ( k \u2212 1 r ) Nr + k\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u22121\u2212r) ( k \u2212 1 r ) Nr+1,\nwhere the last line follows from ( \u03b72\nnB \u2212 \u03b7\n2\nn2 )\u2211n i=1 HiNrHi = Nr+1. We can rewrite the second summation\nto go from r = 1 to r = k by replacing r + 1 with r to combine the terms like so:\nk\u22121\u2211 r=0 (1\u2212 \u03f5)2(k\u2212r) ( k \u2212 1 r ) Nr + k\u2211 r=1 (1\u2212 \u03f5)2(k\u2212r) ( k \u2212 1 r \u2212 1 ) Nr\n= (1\u2212 \u03f5)2kN0 + k\u22121\u2211 r=1\n[ (1\u2212 \u03f5)2(k\u2212r) ( k \u2212 1 r ) + (1\u2212 \u03f5)2(k\u2212r) ( k \u2212 1 r \u2212 1 )] Nr +Nk\n= (1\u2212 \u03f5)2kN0 + k\u22121\u2211 r=1\n(1\u2212 \u03f5)2(k\u2212r) ( k\nr\n) Nr +Nk\n= k\u2211 r=0\n(1\u2212 \u03f5)2(k\u2212r) ( k\nr\n) Nr.\nHence, we have completed the proof by induction. Note that to go from the second to the third line in the previous equation block, we applied the recursive formula for binomial coefficients (see Fact 2). Now, doing the same as before, we can conclude that:\nE tr[J\u0302k...J\u030221...J\u0302k] \u2264 k\u2211\nr=0\n(1\u2212 \u03f5)2(k\u2212r) ( k\nr\n) tr[Nr].\nBy the condition of the theorem that limk\u2192\u221e ( \u03b72 nB \u2212 \u03b7 2 n2 )k\u2211n y1...,yk=1 \u2225Hyk ...Hy1\u2225 2 F = 0, we have that:\ntr[Nr] =\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 )r n\u2211 y1...,yr=1 tr[Hyr ...H 2 y1 ...Hyr ]\n=\n( \u03b72\nnB \u2212 \u03b7\n2\nn2 )r n\u2211 y1...,yr=1 \u2225Hyr ...Hy1\u2225 2 F ,\ngoes to zero as r \u2192 \u221e. Let nr = tr[Nr] and \u03b4 = (1 \u2212 \u03f5)2. To prove the theorem statement, it suffices to show that limk\u2192\u221e \u2211k r=0 \u03b4 k\u2212r(k r ) nr = 0, when \u03b4 \u2208 (0, 1) and (nr)r\u2208N is a sequence of positive real numbers converging to zero. By the theorem statement, we know that 1 \u03f5r\ntr[Nr] \u2192 0. Therefore, there exists some constant C such that 1\n\u03f5r tr[Nr] \u2264 C for all r \u2208 [n], which implies tr[Nr] \u2264 \u03f5rC. Therefore, by the Binomial\nformula:\nk\u2211 r=0\n\u03b4k\u2212r ( k\nr\n) nr \u2264\nk\u2211 r=0\n( k\nr\n) (1\u2212 \u03f52)k\u2212r \u00b7 C\u03f5r = C ( (1\u2212 \u03f5)2 + \u03f5 )k .\nThis term must go to zero as k \u2192 \u221e, since (1\u2212 \u03f5)2 + \u03f5 = 1\u2212 \u03f5+ \u03f52 < 1."
        },
        {
            "heading": "B RESULT OF WU ET AL. (2022)",
            "text": "Here, we summarize the relevant information needed to state Theorem 3.3 of Wu et al. (2022), which considers the settings where one has access to a dataset of the form (xi, yi)i\u2208[n], where xi \u2208 R\nm, yi \u2208 R. f(xi,w) is a function parameterized by w such that f : Rm \u00d7 Rd \u2192 R. The loss function applied to each sample is MSE, denoted by \u2113i(w) = |f(xi,w)\u2212yi|2, and the total loss over all the samples is given by L(w) = 1n \u2211n i=1 \u2113i(w). The linearized SGD dynamics considered is the same as Definition 1, except that S \u2282 [n] is sampled from all size B subsets of [n] uniformly.\nOne can now define two critical matrices. Let \u03a3(w) = 1 n \u2211n i=1 \u2207\u2113i(w)\u2207\u2113i(w)\nT \u2212 \u2207L(w)\u2207L(w)T be the noise covariance matrix, and let G(w) = 1\nn \u2211n i=1 \u2207f(xi,w)\u2207f(xi,w)\nT be the Fisher matrix that characterizes the local geometry of the loss landscape. Next, Wu et al. (2022) define the loss-scaled alignment factor (Equation 3 in Wu et al. (2022)):\n\u00b5(w) = tr[\u03a3(w)G(w)]\n2L(w)\u2225G(w)\u22252F . (4)\nThe last step before stating the theorem is the introduction of the definition of linear stability considered in Wu et al. (2022).\nDefinition 4. (Linear stability in Wu et al. (2022)) A global minimum w\u2217 is said to be linearly stable if there exists a C > 0 such that it holds for linearized dynamics that E[L(wt)] \u2264 C \u00b7E[L(w0)] \u2200t \u2265 0, with w0 being sufficiently close to w\u2217.\nFinally, the theorem follows.\nTheorem 3. (Theorem 3.3 in Wu et al. (2022)) Let w\u2217 be a global minima that is linearly stable (Definition 4). Denote by \u00b5(w) the alignment factors for linearized SGD (Equation 4). If \u00b5(w) > \u00b50, then \u2225H(w\u2217)\u2225F \u2264 1 \u03b7 \u221a B \u00b50 ."
        }
    ],
    "title": "A PRECISE CHARACTERIZATION OF SGD STABILITY USING LOSS SURFACE GEOMETRY",
    "year": 2023
}