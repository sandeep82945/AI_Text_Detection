{
    "abstractText": "Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures \u2014 binning and ECE \u2014 both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of B\u0142asiok, Gopalan, Hu, and Nakkiran (2023) \u2014 a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned reliability diagrams encode the BinnedECE. We also develop a Python package with simple, hyperparameter-free methods for measuring and plotting calibration: https://anonymous.4open. science/r/smoothece-anon/.",
    "authors": [
        {
            "affiliations": [],
            "name": "KERNEL SMOOTHING"
        }
    ],
    "id": "SP:9202fc17e9e6107e8ed90ce8b67682b887cd1942",
    "references": [
        {
            "authors": [
                "Imanol Arrieta-Ibarra",
                "Paman Gujral",
                "Jonathan Tannen",
                "Mark Tygert",
                "Cherie Xu"
            ],
            "title": "Metrics of calibration for probabilistic predictions",
            "venue": "arXiv preprint arXiv:2205.09680,",
            "year": 2022
        },
        {
            "authors": [
                "R.E. Barlow"
            ],
            "title": "Statistical Inference Under Order Restrictions: The Theory and Application of Isotonic Regression",
            "venue": "Wiley Series in Probability and Mathematical Statistics",
            "year": 1972
        },
        {
            "authors": [
                "Jaros\u0142aw B\u0142asiok",
                "Parikshit Gopalan",
                "Lunjia Hu",
                "Preetum Nakkiran"
            ],
            "title": "A unifying theory of distance from calibration",
            "venue": "In Proceedings of the 55th Annual ACM Symposium on Theory of Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Jochen Br\u00f6cker"
            ],
            "title": "Some remarks on the reliability of categorical probability forecasts",
            "venue": "Monthly weather review,",
            "year": 2008
        },
        {
            "authors": [
                "Jaros\u0142aw B\u0142asiok",
                "Parikshit Gopalan",
                "Lunjia Hu",
                "Preetum Nakkiran"
            ],
            "title": "When does optimizing a proper loss yield calibration",
            "year": 2023
        },
        {
            "authors": [
                "JB Copas"
            ],
            "title": "Plotting p against x",
            "venue": "Applied statistics, pp",
            "year": 1983
        },
        {
            "authors": [
                "A Philip Dawid"
            ],
            "title": "The well-calibrated bayesian",
            "venue": "Journal of the American Statistical Association,",
            "year": 1982
        },
        {
            "authors": [
                "Morris H DeGroot",
                "Stephen E Fienberg"
            ],
            "title": "The comparison and evaluation of forecasters",
            "venue": "Journal of the Royal Statistical Society: Series D (The Statistician),",
            "year": 1983
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Shrey Desai",
                "Greg Durrett"
            ],
            "title": "Calibration of pre-trained transformers",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "F.R. Deutsch",
                "P.H. Maserick"
            ],
            "title": "Applications of the hahn-banach theorem in approximation theory",
            "venue": "SIAM Review,",
            "year": 1967
        },
        {
            "authors": [
                "Luc Devroye",
                "Abbas Mehrabian",
                "Tommy Reddad"
            ],
            "title": "The total variation distance between highdimensional gaussians with the same mean, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Timo Dimitriadis",
                "Tilmann Gneiting",
                "Alexander I Jordan"
            ],
            "title": "Stable reliability diagrams for probabilistic classifiers",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "Timo Dimitriadis",
                "Tilmann Gneiting",
                "Alexander I Jordan",
                "Peter Vogel"
            ],
            "title": "Evaluating probabilistic classifiers: The triptych",
            "venue": "arXiv preprint arXiv:2301.10803,",
            "year": 2023
        },
        {
            "authors": [
                "Dean P. Foster",
                "Sergiu Hart"
            ],
            "title": "Smooth calibration, leaky forecasts, finite recall, and nash dynamics",
            "venue": "Games Econ. Behav.,",
            "year": 2018
        },
        {
            "authors": [
                "Parikshit Gopalan",
                "Michael P Kim",
                "Mihir A Singhal",
                "Shengjia Zhao"
            ],
            "title": "Low-degree multicalibration",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Chirag Gupta",
                "Aaditya Ramdas"
            ],
            "title": "Top-label calibration and multiclass-to-binary reductions",
            "venue": "arXiv preprint arXiv:2107.08353,",
            "year": 2021
        },
        {
            "authors": [
                "Cleve Hallenbeck"
            ],
            "title": "Forecasting precipitation in percentages of probability",
            "venue": "Monthly Weather Review,",
            "year": 1920
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Matthijs Hollemans"
            ],
            "title": "Reliability diagrams. https://github.com/hollance/ reliability-diagrams, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Sham Kakade",
                "Dean Foster"
            ],
            "title": "Deterministic calibration and nash equilibrium",
            "venue": "Journal of Computer and System Sciences,",
            "year": 2008
        },
        {
            "authors": [
                "Meelis Kull",
                "Miquel Perello-Nieto",
                "Markus K\u00e4ngsepp",
                "Hao Song",
                "Peter Flach"
            ],
            "title": "Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration",
            "year": 1910
        },
        {
            "authors": [
                "Ananya Kumar",
                "Percy S Liang",
                "Tengyu Ma"
            ],
            "title": "Verified uncertainty calibration",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aviral Kumar",
                "Sunita Sarawagi",
                "Ujjwal Jain"
            ],
            "title": "Trainable calibration measures for neural networks from kernel mean embeddings",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Donghwan Lee",
                "Xinmeng Huang",
                "Hamed Hassani",
                "Edgar Dobriban"
            ],
            "title": "T-cal: An optimal test for the calibration of predictive models",
            "venue": "arXiv preprint arXiv:2203.01850,",
            "year": 2022
        },
        {
            "authors": [
                "KD Leka",
                "Sung-Hong Park",
                "Kanya Kusano",
                "Jesse Andries",
                "Graham Barnes",
                "Suzy Bingham",
                "D Shaun Bloomfield",
                "Aoife E McCloskey",
                "Veronique Delouille",
                "David Falconer"
            ],
            "title": "A comparison of flare forecasting methods. ii. benchmarks, metrics, and performance results for operational solar flare forecasting systems",
            "venue": "The Astrophysical Journal Supplement Series,",
            "year": 2019
        },
        {
            "authors": [
                "Matthias Minderer",
                "Josip Djolonga",
                "Rob Romijnders",
                "Frances Hubis",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Dustin Tran",
                "Mario Lucic"
            ],
            "title": "Revisiting the calibration of modern neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Allan H Murphy",
                "Robert L Winkler"
            ],
            "title": "Reliability of subjective probability forecasts of precipitation and temperature",
            "venue": "Journal of the Royal Statistical Society Series C: Applied Statistics,",
            "year": 1977
        },
        {
            "authors": [
                "E.A. Nadaraya"
            ],
            "title": "On estimating regression",
            "venue": "Theory of Probability & Its Applications,",
            "year": 1964
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory F Cooper",
                "Milos Hauskrecht"
            ],
            "title": "Binary classifier calibration: Non-parametric approach",
            "venue": "arXiv preprint arXiv:1401.3390,",
            "year": 2014
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory F Cooper",
                "Milos Hauskrecht"
            ],
            "title": "Obtaining well calibrated probabilities using bayesian binning",
            "venue": "In Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Alexandru Niculescu-Mizil",
                "Rich Caruana"
            ],
            "title": "Predicting good probabilities with supervised learning",
            "venue": "In Proceedings of the 22nd international conference on Machine learning,",
            "year": 2005
        },
        {
            "authors": [
                "Jeremy Nixon",
                "Michael W Dusenberry",
                "Linchuan Zhang",
                "Ghassen Jerfel",
                "Dustin Tran"
            ],
            "title": "Measuring calibration in deep learning",
            "venue": "In CVPR Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Nobel"
            ],
            "title": "Histogram regression estimation using data-dependent partitions",
            "venue": "The Annals of Statistics,",
            "year": 1996
        },
        {
            "authors": [
                "Pertti Nurmi"
            ],
            "title": "Verifying probability of precipitation - an example from finland",
            "venue": "https://www. cawcr.gov.au/projects/verification/POP3/POP3.html,",
            "year": 2003
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Teodora Popordanoska",
                "Raphael Sayer",
                "Matthew Blaschko"
            ],
            "title": "A consistent and differentiable lp canonical calibration error estimator",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rebecca Roelofs",
                "Nicholas Cain",
                "Jonathon Shlens",
                "Michael C Mozer"
            ],
            "title": "Mitigating bias in calibration error estimation",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "J.S. Simonoff"
            ],
            "title": "Smoothing Methods in Statistics",
            "venue": "URL https://books.google.com/books?id=wFTgNXL4feIC",
            "year": 1996
        },
        {
            "authors": [
                "David B Stephenson",
                "Caio AS Coelho",
                "Ian T Jolliffe"
            ],
            "title": "Two extra components in the brier score decomposition",
            "venue": "Weather and Forecasting,",
            "year": 2008
        },
        {
            "authors": [
                "Mark Tygert"
            ],
            "title": "Plots of the cumulative differences between observed and expected values of ordered bernoulli variates",
            "venue": "arXiv preprint arXiv:2006.02504,",
            "year": 2020
        },
        {
            "authors": [
                "Juozas Vaicenavicius",
                "David Widmann",
                "Carl Andersson",
                "Fredrik Lindsten",
                "Jacob Roll",
                "Thomas Sch\u00f6n"
            ],
            "title": "Evaluating model calibration in classification",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey S. Watson"
            ],
            "title": "Smooth regression analysis",
            "venue": "Sankhya\u0304: The Indian Journal of Statistics, Series A (1961-2002),",
            "year": 1964
        },
        {
            "authors": [
                "Nik Weaver"
            ],
            "title": "Lipschitz Algebras. WORLD SCIENTIFIC, 2nd edition, 2018. doi: 10.1142/9911",
            "venue": "URL https://www.worldscientific.com/doi/abs/10.1142/9911",
            "year": 2018
        },
        {
            "authors": [
                "David Widmann",
                "Fredrik Lindsten",
                "Dave Zachariah"
            ],
            "title": "Calibration tests beyond classification",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ross Wightman"
            ],
            "title": "Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Bianca Zadrozny",
                "Charles Elkan"
            ],
            "title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers",
            "venue": "In Icml,",
            "year": 2001
        },
        {
            "authors": [
                "Bianca Zadrozny",
                "Charles Elkan"
            ],
            "title": "Transforming classifier scores into accurate multiclass probability estimates",
            "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2002
        },
        {
            "authors": [
                "B\u0142asiok"
            ],
            "title": "We show in this paper that the duality theorem connecting wCE and dCE holds much more generally, for a broad family of metrics",
            "venue": "Theorem 12 (B\u0142asiok et al",
            "year": 2023
        },
        {
            "authors": [
                "Devroye"
            ],
            "title": "Where the last bound on the total variation distance between two one-dimension Gaussians is a special case of Theorem",
            "year": 2018
        },
        {
            "authors": [],
            "title": "CONSISTENT CALIBRATION MEASURES AND SMOOTHECE Here we elaborate on the shortcomings of ECE, and resolution offered by SmoothECE, and the notion of a consistent calibration measure (B\u0142asiok et al., 2023). One fundamental issue with the ECE is: it is discontinuous in the underlying predictor, so a small change in the predictor can cause a large change",
            "year": 2023
        },
        {
            "authors": [
                "B\u0142asiok"
            ],
            "title": "work). Here, we will demonstrate this discontinuity visually, with a related example. In Figure 3, we construct three different distributions that are each small perturbations of one another. The top row shows samples (fi",
            "venue": "yi) \u2208 [0,",
            "year": 2023
        },
        {
            "authors": [
                "B\u0142asiok"
            ],
            "title": "STATISTICAL CONSISTENCY Our estimator of SmoothECE also satisfies the classical criteria of statistical consistency. Specifically Theorem 9 shows that as the number of samples m \u2192 \u221e, the finite-sample estimation error \u03b5",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Calibration is a fundamental aspect of probabilistic predictors, capturing how well predicted probabilities of events match their true frequencies (Dawid, 1982). For example, a weather forecasting model is perfectly calibrated (also called \u201cperfectly reliable\u201d) if among the days it predicts a 10% chance of rain, the observed frequency of rain is exactly 10%. There are two key questions in studying calibration: First, for a given predictive model, how do we measure its overall amount of miscalibration? This is useful for ranking different models by their reliability, and determining how much to trust a given model\u2019s predictions. Methods for quantifying miscalibration are known as calibration measures. Second, how do we convey where the miscalibration occurs? This is useful for better understanding an individual predictor\u2019s behavior (where it is likely to be over- vs. underconfident), as well as for re-calibration\u2014 modifying the predictor to make it better calibrated. The standard way to convey this information is known as a reliability diagram. Unfortunately, in machine learning, the most common methods of constructing both calibration measures and reliability diagrams suffer from well-known flaws, which we describe below.\nThe most common choice of calibration measure in machine learning is the Expected Calibration Error (ECE), more specifically its empirical variant the Binned ECE (Naeini et al., 2015). The ECE is known to be unsatisfactory for many reasons; for example, it is a discontinuous functional, so changing the predictor by an infinitesimally small amount may change its ECE drastically (Kakade & Foster, 2008; Foster & Hart, 2018; B\u0142asiok et al., 2023). Moreover, the ECE is impossible to estimate efficiently from samples (Lee et al., 2022; Arrieta-Ibarra et al., 2022), and its sampleefficient variant, the Binned ECE, is overly sensitive to choice of bin widths (Nixon et al., 2019; Kumar et al., 2019; Minderer et al., 2021). These shortcomings have been well-documented in the community, which motivated proposals of new, better-behaved calibration measures (e.g. Roelofs et al. (2022); Arrieta-Ibarra et al. (2022); Lee et al. (2022)).\nRecently, B\u0142asiok et al. (2023) proposed a theoretical definition of what constitutes a \u201cgood\u201d calibration measure. The key principle is that good measures should provide upper and lower bounds on the calibration distance dCE, which is the Wasserstein distance between the joint distribution of\nprediction-outcome pairs, and the set of perfectly calibrated such distributions (formally defined in Definition 5 below). Calibration measures which satisfy this property are called consistent calibration measures. In light of this line of work, one may think that the question of which calibration measure to choose is largely resolved: simply pick a consistent calibration measure, such as Laplace Kernel Calibration Error / MMCE (B\u0142asiok et al., 2023; Kumar et al., 2018), as suggested by B\u0142asiok et al. (2023). However, this theoretical suggestion belies the practical reality: Binned ECE remains the most popular calibration measure used in practice, even in recent studies. We believe this is partly because Binned ECE enjoys an additional property: it can be visually represented by a specific kind of reliability diagram, namely the binned histogram. This raises the question of whether there are calibration measures which are consistent in the sense of B\u0142asiok et al. (2023), and can also be represented by an appropriate reliability diagram. To be precise, we must discuss reliability diagrams more formally.\nReliability Diagrams. We consider measuring calibration in the setting of binary outcomes, for simplicity. Here, we have a joint distribution (f, y) \u223c D over predictions f \u2208 [0, 1], and true outcomes y \u2208 {0, 1}. We interpret f as the predicted probability that y = 1. The \u201ccalibration function\u201d1 \u00b5 : [0, 1]\u2192 [0, 1] is defined as the conditional expectation:\n\u00b5(f) := E D [y | f ].\nA perfectly calibrated distribution, by definition, is one with a diagonal calibration function: \u00b5(f) = f . Reliability diagrams are traditionally thought of as estimates of the calibration function \u00b5 (Naeini et al., 2014; Br\u00f6cker, 2008). In other words, reliability diagrams are one-dimensional regression methods, since the goal of regressing y on f is exactly to estimate the regression function E[y | f ]. The practice of \u201cbinning\u201d to construct reliability diagrams (as in Figure 1 left) can be equivalently thought of as using histogram regression to regress y on f .\nWith this perspective on reliability diagrams, one may wonder why histogram regression is still the most popular method, when more sophisticated regressors are available. One potential answer is that users of reliability diagrams have an additional desiderata: it should be easy to visually read off a reasonable calibration measure from the reliability diagram. For example, it is easy to visually read off the Binned ECE from a binned reliability diagram, because it is simply the integrated absolute deviation from the diagonal:\nBinnedECEk = \u222b 1 0 \u2223\u2223\u00b5\u0302k(f)\u2212 fk\u2223\u2223 dF 1In the terminology of Br\u00f6cker (2008).\nwhere k is the number of bins, \u00b5\u0302k is the histogram regression estimate of y given f , and fk is the \u201cbinned\u201d version of f \u2014 formally the histogram regression estimate of f given f . This relationship is even more transparent for the full (non-binned) ECE, where we have\nECE = \u222b 1 0 |\u00b5(f)\u2212 f | dF = E f [|\u00b5(f)\u2212 f |]\nwhere \u00b5 is the true regression function as above. However, more sophisticated regression methods do not neccesarily have such tight relationships to calibration measures. Thus we have a situation where better calibration measures exist, but they are not accompanied by reliability diagrams, and conversely better reliability diagrams exist (i.e. regression methods), but they are not associated with consistent calibration measures. We address this situation here: we present a new consistent calibration measure, SmoothECE, along with a regression method which naturally encodes this calibration measure. The SmoothECE is, per its name, equivalent to the ECE of a \u201csmoothed\u201d version of the original distribution, and the resulting reliability diagram can thus be interpreted as a smoothed estimate of the calibration function.\nWe emphasize that the idea of smoothing is not new \u2014 Gaussian kernel smoothing has been explicitly proposed as method for constructing reliability diagrams in the past (e.g. Br\u00f6cker (2008), as discussed in Arrieta-Ibarra et al. (2022)). Our contribution is two-fold: first, we give strong theoretical justification for kernel smoothing by proving it induces a consistent calibration measure. Second, and of more practical relevance, we show how to chose the kernel bandwidth in a principled way, which differs significantly from existing recommendations."
        },
        {
            "heading": "1.1 OVERVIEW OF METHOD",
            "text": "We start by describing the regression method, which defines our reliability diagram. We are given i.i.d. observations {(f1, y1), (f2, y2) . . . (fk, yk)} where fi \u2208 [0, 1] is the i-th prediction, and yi \u2208 {0, 1} is the corresponding outcome. For example, if we are measuring calibration of an ML model on a dataset of validation samples, we will have fi = F (xi) for model F evaluated on sample xi, with ground-truth label yi. We would like to estimate the true calibration function \u00b5(f) := E[f | y]. Our estimate \u00b5\u0302(f) is given by Nadaraya-Watson kernel regression (kernel smoothing) on this dataset (see Nadaraya (1964); Watson (1964); Simonoff (1996)):\n\u00b5\u0302(f) := \u2211 i K\u03c3(f, fi)yi\u2211 i K\u03c3(f, fi) . (1)\nThat is, for a given f \u2208 [0, 1] our estimate of y is the weighted average of all yi, where weights are given by the kernel function K\u03c3(f, fi). The choice of kernel, and in particular the choice of bandwidth \u03c3, is crucial for our method\u2019s theoretical guarantees. We use an essentially standard kernel (defined formally in Section 3): the Gaussian Kernel, reflected appropriately to handle boundaryeffects of the interval [0, 1]. Our choice of bandwidth \u03c3 is more subtle, but it is not a hyperparameter \u2013 we describe the explicit algorithm for choosing \u03c3 in Section 3. It suffices to say for now that the amount of smoothing \u03c3 will end up being proportional to the reported calibration error.\nReliability Diagram We then construct a reliability diagram in the standard way, by displaying a plot of the estimated calibration function \u00b5\u0302 along with a kernel density estimate of the predictions fi (see Figure 1). These two estimates, compactly presented on the same diagram, provide a tool to quickly understand and visually assess calibration properties of a given predictor. Moreover, they can be used to define a quantiative measure of overall degree of miscalibration, as we show below.\nSmoothECE A natural measure of calibration can be easily computed from data in the above reliability diagram. Specifically, let \u03b4\u0302(f) be the kernel density estimate of predictions: \u03b4\u0302(f) := 1 n \u2211 i K\u03c3(f, fi). Then, similar to the definition of ECE, we can integrate the deviation of \u00b5\u0302 from the diagonal to obtain:\n\u02dcsmECE\u03c3 := \u222b |\u00b5\u0302(t)\u2212 t|\u03b4\u0302(t)dt.\nThe measure of calibration we actually propose in Section 3, smECE\u03c3 , is closely related but not identical to the above. Briefly, to define smECE\u03c3 we consider the kernel smoothing of the difference between the outcome and the prediction (yi \u2212 fi) instead of just smoothing the outcomes yi. As it turns out, those choices lead to a calibration measure with better mathematical properties: smECE\u03c3 is monotone decreasing as the kernel bandwidth \u03c3 is increased, and smECE, at our specific bandwidth choice and applied to the population distribution, is 0 for perfectly calibrated predictors.\nWe reiterate that the choice of the scale \u03c3 is very important: too large or too small bandwidth will prevent the SmoothECE from being a consistent calibration measure. In Section 3, we will show how to algorithmically define the correct scale \u03c3\u2217. For the reliability diagram, we suggest presenting the estimates y\u0302 and \u03b4\u0302 with the same scale \u03c3\u2217, and for this scale we indeed have \u02dcsmECE\u03c3\u2217 \u2248 smECE\u03c3\u2217 (see Appendix A). Finally, note that we have been discussing finite-sample estimators of all quantities; the corresponding population quantities are defined analogously in Section 3."
        },
        {
            "heading": "1.2 SUMMARY OF OUR CONTRIBUTIONS",
            "text": "1. SmoothECE. We define a new hyperparmeter-free calibration measure, which we call the SmoothECE (abbreviated smECE). We prove that the SmoothECE is a consistent calibration measure, in the sense of B\u0142asiok et al. (2023). It also corresponds to a natural notion of distance: if SmoothECE is \u03b5, then the function f can be stochastically post-processed to make it perfectly calibrated, without perturbing f by more than \u03b5 in L1.\n2. Smoothed Reliability Diagrams. We show how to construct principled reliability diagrams which visually encode the SmoothECE. These diagrams can be thought of as \u201csmoothed\u201d versions of the usual binned reliability diagrams, where we perform NadarayaWatson kernel smoothing with the Gaussian kernel.\n3. Code. We develop a Python package which computes the SmoothECE and plots the associated smooth reliability diagram (link omitted for anonymity). It is hyperparameter-free, efficient, and includes uncertainty quantification via bootstrapping. We include several experiments in Section 4, for demonstration purposes.\n4. Extensions to general metrics. On the theoretical side, we investigate how far our construction of SmoothECE generalizes. We show that the notion of SmoothECE introduced in this paper can indeed be defined for a wider class of metrics on the space of predictions [0, 1], and we prove the appropriate generalization of our main theorem: that the smECE for a given metric is a consistent calibration measure with respect to the same metric. Finally, perhaps surprisingly, we show that under specific conditions on the metric (which are satisfied, for instance, by the dlogit metric), the associated smECE is in fact a consistent calibration measure with respect to \u21131 metric.\nOrganization. We begin by discussing the closest related works (Section 2). In Section 3 we formally define the SmoothECE and prove its mathematical and computational properties. To aid intuition, we discuss the justification behind our various design choices in Appendix A. We then prove the SmoothECE can be estimated efficiently with respect to both sample-complexity and runtime (Section 3.4). Finally, we include experimental demonstrations of our method in Section 4, and conclude in Section 5. Extensions of our results to more general metrics are provided in Appendix B."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Reliability Diagrams and Binning. Reliability diagrams, as far as we are aware, had their origins in the early reliability tables constructed by the meteorological community (Hallenbeck, 1920). These early accounts of calibration already applied the practice of binning\u2014 discretizing predictions into bins, and estimating frequencies conditional on each bin. Plots of these tables turned into binned reliability diagrams (Murphy & Winkler, 1977; DeGroot & Fienberg, 1983), which was recently popularized in the machine learning community by a series of works including Zadrozny & Elkan (2001); Niculescu-Mizil & Caruana (2005); Guo et al. (2017). Binned reliability diagrams continue to be used in studies of calibration in deep learning, including in the GPT-4 tech report (Guo et al., 2017; Nixon et al., 2019; Minderer et al., 2021; Desai & Durrett, 2020; OpenAI, 2023).\nReliability Diagrams as Regression. The connection between reliability diagrams and regression methods has been noted in the literature (e.g. Br\u00f6cker (2008); Copas (1983); Stephenson et al. (2008)). For example, Stephenson et al. (2008) observes \u201cone can consider binning to be a crude form of non-parametric smoothing.\u201d However, we remind the reader of a subtlety: our objective in this work is not identical to the regression objective, since we want an estimator that is simultaneously a reasonable regression and a consistent calibration measure. Our choice of bandwidth must thus carefully balance the two; it cannot be simply be chosen to minimize the regression test loss.\nAlternate Constructions. There have been various proposals to construct reliability diagrams which improve on binning; we mention several of them here. Many proposals can be seen as suggesting alternate regression techniques, to replace histogram regression. For example, some works suggest modifications to improve the binning method, such as adaptive bin widths or debiasing (Kumar et al., 2019; Nixon et al., 2019; Roelofs et al., 2022). These are closely related to data-dependent histogram estimators in the statistics literature (Nobel, 1996). Other works suggest using entirely different regression methods, including spline fitting (Gupta et al.), kernel smoothing (Br\u00f6cker, 2008; Popordanoska et al., 2022), and isotonic regression (Dimitriadis et al., 2021). The above methods for constructing regression-based reliability diagrams are closely related to methods for recalibration, since the ideal recalibration function is exactly the calibration function \u00b5. For example, isotonic regression (Barlow, 1972) has been used as both for recalibration (Zadrozny & Elkan, 2002; Naeini et al., 2015) and for reliability diagrams (Dimitriadis et al., 2021). Finally, Tygert (2020) and Arrieta-Ibarra et al. (2022) suggest visualizing reliability via cumulative-distribution plots, instead of estimating conditional expectations. While all the above proposals do improve upon binning in certain aspects, none of them ultimately induce consistent calibration measures in the sense of B\u0142asiok et al. (2023). See B\u0142asiok et al. (2023) for further discussion on the shortcomings of these measures.\nMulticlass Calibration. We focus on binary calibration in this work. The multi-class setting introduces several new complications\u2014 foremost, there is no consensus on how best to define calibration measures in the multi-class setting; this is an active area of research (e.g. Vaicenavicius et al. (2019); Kull et al. (2019); Widmann et al. (2020)). Howevever, our methods apply directly to any of the metrics induced by the \u201cmulticlass-to-binary\u201d reductions of Gupta & Ramdas (2021), because these are determined by the standard calibration of a related binary problem. This includes, for example, the multi-class confidence calibration.\nConsistent Calibration Measures. We warn the reader that the terminology of consistent calibration measure does not refer to the concept of statistical consistency. Rather, it refers to the definition introduced in B\u0142asiok et al. (2023), to capture calibration measures which polynomially approximate the true (Wasserstein) distance to perfect calibration."
        },
        {
            "heading": "3 SMOOTH ECE",
            "text": "In this section we will define the calibration measure smECE. As it turns out, it has slightly better mathematical properties than \u02dcsmECE defined in Section 1.1, and those properties will allow us to chose the proper scale \u03c3 in a more principled way \u2014 moreover, we will be able to relate smECE with \u02dcsmECE.\nSpecifically, the measure smECE\u03c3 enjoys the following convenient mathematical properties, which we will prove in this section.\n\u2022 The smECE\u03c3(D) is monotone decreasing as we increase the smoothing parameter \u03c3. \u2022 If D is perfectly calibrated distribution, then for any \u03c3 we have smECE\u03c3(D) = 0. Indeed,\nfor any \u03c3 we have smECE\u03c3(D) \u2264 ECE(D). \u2022 The smECE\u03c3 is Lipschitz with respect to the Wasserstein distance on the space of distri-\nbutions over [0, 1] \u00d7 {0, 1}: for any D1,D2 we have |smECE\u03c3(D1) \u2212 smECE\u03c3(D2)| \u2264 (1 + \u03c3\u22121)W1(D1,D2). This implies smECE\u03c3(D) \u2264 (1 + \u03c3\u22121)dCE(D).\n\u2022 For any distribution D and any \u03c3, there is a (probabilistic) post-processing \u03ba, such that if (f, y) \u223c D, then the distribution D\u2032 of (\u03ba(f), y) is perfectly calibrated, and moreover E |f \u2212 \u03ba(f)| \u2264 smECE\u03c3(D) + \u03c3. In particular dCE(D) \u2264 smECE\u03c3 + \u03c3.\nReflected Gaussian Kernel. In all of our kernel applications, we use a \u201creflected\u201d version of the Gaussian kernel defined as follows. Let \u03c0R : R\u2192 [0, 1] be the projection function which is identity on [0, 1], and collapses two points iff they differ by a composition of reflections around integers. That is \u03c0R(x) := (x mod 2) if (x mod 2) \u2264 1, and (2 \u2212 (x mod 2)) otherwise. The Reflected Gaussian kernel on [0, 1] with scale \u03c3, is then given by\nK\u0303\u03c3(x, y) := \u2211\nx\u0303\u2208\u03c0\u22121R (x)\n\u03d5\u03c3(x\u0303\u2212 y) = \u2211\ny\u0303\u2208\u03c0\u22121R (y)\n\u03d5\u03c3(x\u2212 y\u0303), (2)\nwhere \u03d5 is the probability density function of N (0, 1), that is \u03d5\u03c3(t) = exp(\u2212t2/2\u03c32)/ \u221a 2\u03c0\u03c32. We use the Reflected Gaussian kernel in order to alleviate the bias introduced by standard Gaussian kernel near the boundaries of the interval [0, 1].\n3.1 DEFINING SMECE\u03c3 AT SCALE \u03c3\nWe now present the construction of smECE\u03c3 , at a given scale \u03c3 > 0. We will show how to pick this \u03c3 in the subsequent section. Let D be a distribution over [0, 1]\u00d7 {0, 1} of the pair of prediction f \u2208 [0, 1] and outcome y \u2208 {0, 1}. For a given kernel K : R \u2192 R we define the kernel smoothing of the residual r := y \u2212 f as\nr\u0302D,K(t) := E(f,y)\u223cD K(t, f)(y \u2212 f)\nE(f,y)\u223cD K(t, f) . (3)\nThis differs from the definition in Section 1.1, where we applied the kernel smoothing to the outcomes y instead. In many cases of interest, the probability distributionD is going to be an empirical probability distribution over finite set of pairs {(fi, yi)} of observed predictions fi and associated observed outcomes yi. In this case, the r\u0302D(t) is just a weighted average of residuals (fi \u2212 yi) where the weight of a given sample is determined by the kernel K(fi, t). This is equivalent to the Nadaraya-Watson kernel regression (a.k.a. kernel smoothing, see Nadaraya (1964); Watson (1964); Simonoff (1996)), estimating (y \u2212 f) with respect to the independent variable f .\nWe consider also the kernel density estimation \u03b4\u0302D,K(t) := Ef,y\u223cD K(t, f). The smECEK(D) is now defined as smECEK(D) := \u222b |r\u0302D,K(t)|\u03b4\u0302D,K(t) dt. (4)\nThis notion is close to ECE of a smoothed distribution of (f, y), in a sense which can be formalized (see Appendix A). For now, let us discuss the intuitive connection. For any distribution of prediction, and outcome (f, y), we can consider an expected residual r(t) := E[f \u2212 y|f = t], then\nECE(f, y) := \u222b |r(t)|d\u00b5f (t),\nwhere \u00b5f is a measure of f . We can compare this with (4), where the conditional residual r has been replaced by its smoothed version r\u0302, and the measure \u00b5f has been replaced by \u03b4\u0302 dt \u2013 the measure of f + \u03b7 for some noise \u03b7. Equation (4) can be simplified by using the definitions of \u03b4\u0302D,K and r\u0302D,K ,\nsmECEK(D) = \u222b \u2223\u2223\u2223\u2223 E\nf,y K(t, f)(y \u2212 f) \u2223\u2223\u2223\u2223 dt. (5) In what follows, we will be focusing on the reflected Gaussian kernel with scale \u03c3, K\u0303N,\u03c3, and we shall use shorthand smECE\u03c3 to denote smECEK\u0303N,\u03c3 . We will now show how the scale \u03c3 is chosen.\n3.2 DEFINING SMECE: PROPER CHOICE OF SCALE\nFirst, we observe that smECE\u03c3 satisfies a natural monotonicity property: increasing the smoothing scale \u03c3 decreases the smECE\u03c3 . (Proof of this and subsequent lemmas can be found in Appendix D.) Lemma 1. For a distribution D over [0, 1]\u00d7 {0, 1} and \u03c31 \u2264 \u03c32, we have\nsmECE\u03c31(D) \u2265 smECE\u03c32(D).\nSeveral of our design choices were crucial to ensure this property: the choice of the reflected Gaussian kernel, and the choice to smooth the residual (y \u2212 f) as opposed to the outcome y. Note that since smECE\u03c3(D) \u2208 [0, 1], and for a given predictor D, the function \u03c3 7\u2192 smECE\u03c3(D) is a nonincreasing function of \u03c3, there is a unique \u03c3\u2217 s.t. smECE\u03c3\u2217(D) = \u03c3\u2217 (and we can find it efficiently using binary search). Thus we can define: Definition 2 (SmoothECE). We define smECE(D) to be the unique \u03c3\u2217 satisfying smECE\u03c3\u2217(D) = \u03c3\u2217. We also write this quantity as smECE\u2217(D) for clarity.\n3.3 SMECE IS A CONSISTENT CALIBRATION MEASURE\nWe will show that \u03c3\u2217 defined in the previous subsection is a convenient scale on which the smECE ofD should be evaluated. The formal requirement that smECE\u03c3\u2217 meets is captured by the notion of consistent calibration measure, introduced in B\u0142asiok et al. (2023). We provide the definition below, but before we do, let us recall the definition of the Wasserstein metric.\nFor a metric space (X , d), let us consider \u2206(X ) to be the space of all probability distributions over X . We define the Wasserstein metric on the space \u2206(X) (sometimes called earth-movers distance) Peyr\u00e9 et al. (2019). Definition 3 (Wasserstein distance). For two distributions \u00b5, \u03bd \u2208 \u2206(X ) we define the Wasserstein distance\nW1(\u00b5, \u03bd) := inf \u03b3\u2208\u0393(\u00b5,\u03bd) E (x,y)\u223c\u03b3 d(x, y),\nwhere \u0393(\u00b5, \u03bd) is the family of all couplings of distributions \u00b5 and \u03bd. Definition 4. A probability distribution D over [0, 1] \u00d7 {0, 1} of prediction f and outcome y is perfectly calibrated if ED[y|f ] = f . We denote the family of all perfectly calibrated distributions by P \u2282 \u2206([0, 1]\u00d7 {0, 1}). Definition 5 (Consistent calibration measure (B\u0142asiok et al., 2023)). For a probability distribution D over [0, 1]\u00d7 {0, 1} we define the distance to calibration dCE(D) to be the Wasserstein distance to the nearest perfectly calibrated distribution, associated with metric d on [0, 1]\u00d7{0, 1} which puts two disjoint intervals infinitely far from each other.\nConcretely\nd((f1, y1), (f2, y2)) := { |f1 \u2212 f2| if y1 = y2 \u221e otherwise .\nand dCE(D) = inf\nD\u2208P W1(D,D\u2032).\nFinally, any function \u00b5 assigning to distributions over [0, 1]\u00d7{0, 1} a non-negative real calibration score, is called consistent calibration measure if it is polynomially upper and lower bounded by dCE, i.e. there are constants c1, c2, \u03b11, \u03b12, s.t.\nc1dCE(D)\u03b11 \u2264 \u00b5(D) \u2264 c2dCE(D)\u03b12 .\nWith this definition in hand, we prove the following. Theorem 6. The measure smECE(D) is a consistent calibration measure.\nThis theorem is a consequence of the following two inequalities. First of all, if we add the penalty proportional to the scale of noise \u03c3, then smECE\u03c3 upper bounds the distance to calibration. Lemma 7. For any \u03c3, we have\ndCE(D) \u2272 smECE\u03c3(D) + \u03c3.\nOn the other hand, as soon as the scale of the noise is sufficiently large compared to the distance to calibration, the smECE of a predictor is itself upper bounded as follows. Lemma 8. Let (f, y) be any predictor. Then for any \u03c3 we have\nsmECE\u03c3(D) \u2264 ( 1 + 1\n\u03c3\n) dCE(D).\nIn particular if \u03c3 > 2 \u221a dCE(D), then\nsmECE\u03c3(D) \u2264 2 \u221a dCE(D).\nThis lemma, together with the fact that \u03c3 7\u2192 smECE\u03c3 is non-increasing, directly implies that the fixpoint satisfies \u03c3\u2217 \u2264 2 \u221a dCE(D). On the other hand, using Lemma 7, at this fixpoint we have dCE(D) \u2264 \u03c3\u2217 + smECE\u03c3\u2217(D) = 2\u03c3\u2217. That is\n1 2 dCE(D) \u2264 smECE\u2217(D) \u2264 2\n\u221a dCE(D),\nproving the Theorem 6."
        },
        {
            "heading": "3.4 SAMPLE AND RUNTIME EFFICIENCY",
            "text": "Here we prove that our method is efficient with respect to both sample complexity and runtime. We want to estimate smECE of the underlying distribution D over [0, 1] \u00d7 {0, 1}, using samples from this distribution. Specifically, let us sample independently at random m pairs (fi, yi) \u223c D, and let us define D\u0302 to be the empirical distribution over the multiset {(fi, yi)}; that is, to sample from D\u0302, we pick a uniformly random i \u2208 [m] and output (fi, yi). Then, the plug-in estimator satisfies the following generalization bound.\nTheorem 9. For a given \u03c30 > 0 if m \u2273 \u03c3\u221210 \u03b5\u22122, then with probability at least 2/3 over the choice of independent random sample (fi, yi)mi=1 (with (fi, yi) \u223c D), we have simultanously for all \u03c3 \u2265 \u03c30,\n|smECE\u03c3(D)\u2212 smECE\u03c3(D\u0302)| \u2264 \u03b5.\nIn particular if smECE\u2217(D) > \u03c30, then (with probability at least 2/3) we have |smECE\u2217(D) \u2212 smECE\u2217(D\u0302)| \u2264 \u03b5.\nThe proof is in Appendix D.6. The success probability 2/3 can be amplified arbitrarily in the standard way, by taking the median of independent trials. Finally, it is clear that smECE can be computed efficiently, by using the Fast Fourier Transform for convolutions. For completeness, in Appendix C we show that smECE can be approximated up to error \u03b5 in time O(n log \u03b5\u22121 + M log \u03b5\u22121 log3/2 M) in the RAM model, where M = \u2308\u03b5\u22121\u03c3\u22121\u2309."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We include several experiments demonstrating our method on public datasets in various domains, from deep learning to meteorology. The sample sizes vary between several hundred to 50K, to show how our method behaves for different data sizes. In each setting, we compare the classical binned reliability diagram to the smooth diagram generated by our Python package. Our diagrams include kernel density estimates of the predictions (at the same kernel bandwidth \u03c3\u2217 used to compute the SmoothECE). For binned diagrams, the number of bins is chosen to be optimal for the regression test MSE loss, optimized via cross-validation.\nDeep Networks. Figure 2a shows the confidence calibration of ResNet32 (He et al., 2016) on the ImageNet validation set (Deng et al., 2009). ImageNet is an image classification task with 1000 classes, and has a validation set of 50,000 samples. In this multi-class setting, the model f outputs a distribution over k = 1000 classes, f : X \u2192 \u2206k. Confidence calibration is defined as calibration of the pairs (argmaxc\u2208[k] fc(x) , 1{f(x) = y}), which is a distribution over [0, 1] \u00d7 {0, 1}. That is, confidence calibration measures the agreement between confidence and correctness of the predictions. We use the publicly available data from Hollemans (2020), evaluating the models trained by Wightman (2019).\nSolar Flares. Figure 2b shows the calibration of a method for forecasting solar flares, over a period of 731 days. We use the data from Leka et al. (2019), which was used to compare reliability diagrams in Dimitriadis et al. (2021). Specifically, we consider forecasts of the event that a class\nC1.0+ solar flare occurs on a given day, made by the DAFFS forecasting model developed by NorthWest Research Associates. Overall, such solar flares occur on 25.7% of the 731 recorded days. We use the preprocesssed data from the replication code at: https://github.com/TimoDimi/ replication_DGJ20. For further details of this dataset, we refer the reader to Dimitriadis et al. (2023, Section 6.1) and Leka et al. (2019).\nPrecipitation in Finland. Figure 2c shows the calibration of daily rain forecasts made by the Finnish Meteorological Institute (FMI) in 2003, for the city of Tampere in Finland. Forecasts are made for the probability that precipitation exceeds 0.2mm over a 24 hour period; the dataset includes records for 346 days (Nurmi, 2003).\nSynthetic Data. For demonstration purposes, we apply our method to a simple synthetic dataset in Figure 2d. One thousand samples fi \u2208 [0, 1] are drawn uniformly in the interval [0, 1], and the conditional distribution of labels E[yi | fi] is given by the green line in Figure 2d. Here, instead of kernel density estimates, we show bootstrapped confidence bands around our estimated regressor. Note that the true conditional distribution is non-monotonic in this example.\nLimitations. One limitation of our method is that since it is generic, there may be better tools to use in special cases, when we can assume more structure in the prediction distributions. For example, if the predictor is known to only output a small finite set of possible probabilities, then it is reasonable to simple estimate conditional probabilities by using these points as individual bins. The rain forecasts in Figure 2c have this structure, since the forecasters only predict probabilities in multiples of 10% \u2013 in such cases, using bins which are correctly aligned is a very reasonable option."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We have presented a method of computing calibration error which is both mathematically wellbehaved (i.e. consistent in the sense of B\u0142asiok et al. (2023)), and can be visually represented in a reliability diagram. We also released a python package which efficiently implements our suggested method. We hope this work aids practitioners in computing, analyzing, and visualizing the reliability of probabilistic predictors."
        },
        {
            "heading": "A DISCUSSION: DESIGN CHOICES",
            "text": "Here we discuss the motivation behind several design choices that may a-priori seem ad-hoc. In particular, the choice to smooth the residuals (yi \u2212 fi) when computing the smECE, but to smooth the outcomes yi directly when plotting the reliability diagram.\nFor the purpose of constructing the reliability diagram, it might be tempting to plot a function y\u2032(f) := r\u0302(f)+ f (of smoothed residual as defined in (3), shifted back by the prediction f ), as well as the smoothed density \u03b4\u0302(t), as in the definition of smECE. This is a fairly reasonable approach, unfortunately it has a particularly undesirable feature \u2014 there is no reason for y\u2032(t) := r\u0302(t) + t to be bounded in the interval [0, 1]. It is therefore visually quite counter-intuitive, as the plot of y(t) is supposed to be related with our guess on the average outcome y given (slightly noisy version of) the prediction t.\nAs discussed in Section 1.1, we instead consider the kernel regression on y, as opposed to the kernel regression on the residual y \u2212 f , and plot exactly this, together with the density \u03b4\u0302. Specifically, let us define\ny\u0302D,K(t) := Ef,y\u223cD K(t, f)y Ef,y\u223cD K(t, f) . (6)\nand chose as the reliability diagram a plot of a pair of functions t 7\u2192 y\u0302D,K(t) and t 7\u2192 \u03b4\u0302D,K(t) \u2014 the first plot is our estimation (based on the kernel regression) of the outcome y for a given prediction t, the other is the estimation of the density of prediction t. As discussed in the Section 1.1, we will focus specifically on the kernel K being the reflected Gaussian kernel, defined by (2).\nIt is now tempting to define the calibration error related with this diagram, as an ECE of this new random variable over [0, 1]\u00d7 {0, 1}, analogously to the definition of smECE, by considering\n\u02dcsmECE\u03c3(D) := \u222b |y\u0302D,K(t)\u2212 t|\u03b4\u0302D,K(t) dt. (7)\nThis definition can be readily interpreted: for a random pair (f, y) and an \u03b7 \u223c N (0, \u03c3) independent, we can consider a pair (f + \u03b7, y). It turns out that\n\u02dcsmECE\u03c3(D) = ECE(\u03c0R(f + \u03b7), y), where \u03c0R : R\u2192 [0, 1] collapses points that differ by reflection around integers (see Section 1.1). Unfortunately, despite being directly connected with more desirable reliability diagrams, and having more immediate interpretation as a ECE of a noisy prediction, this newly introduced measure \u02dcsmECE has its own problems, and is generally mathematically much poorer-behaved than smECE. In particular it is no longer the case that if we start with the perfectly calibrated distribution, and apply some smoothing with relatively large bandwidth \u03c3, the value of the integral (7) stays small. In fact it might be growing as we add more smoothing2.\nNevertheless, if we chose the correct bandwidth \u03c3\u2217, as guided by the smECE consideration, the integral (7), which is visually encoded by the reliability diagram we propose, should still be within constant factor from the actual smECE\u2217\u03c3(D), and hence provides a consistent calibration measure Lemma 10. For any \u03c3 we have\n\u02dcsmECE\u03c3(D) = smECE\u03c3(D)\u00b1 c\u03c3, where c = \u221a\n2/\u03c0 \u2264 0.8. In particular, for \u03c3\u2217 s.t. smECE\u03c3\u2217(D) = \u03c3\u2217, we have\n\u02dcsmECE\u03c3\u2217(D) \u2248 smECE\u03c3\u2217(D).\n(The proof can be found in Appendix D.3)."
        },
        {
            "heading": "B GENERAL METRICS",
            "text": "Our previous discussion implicitly assumed the the trivial metric on the interval d(u, v) = |u \u2212 v|. We will now explore which aspects of our results extend to more general metrics over the interval [0, 1]. This is relevant if, for example, our application downstream of the predictor is more sensitive to miscalibration near the boundaries.\nThe study of consistency measures with respect to general metrics is also motivated by the results of B\u0142asiok et al. (2023). There it was shown that for any proper loss function l, there was an associated metric dl on [0, 1] such that the predictor has small weak calibration error with respect to dl if and only if the loss l cannot be significantly improved by post-composition with a Lipschitz function with respect to dl. Specifically, they proved\nwCEdl(D) \u2272 E (f,y)\u223cD [l(f, y)]\u2212 inf \u03ba E (f,y)\u223cD\n[l(\u03ba(f), y)] \u2272 \u221a wCEdl(D),\n2This can be easily seen, if we consider the trivial perfectly calibrated distribution, where outcome y \u223c Bernoulli(1/2) and prediction f is deterministic 1/2. Then \u02dcsmECE\u03c3(D) = C\u03c3 for some constant C \u2248 0.79.\nwhere \u03ba : [0, 1] \u2192 [0, 1] ranges over all functions Lipschitz with respect to the dl metric, and wCEdl(D) is the weak calibration error (as introduced by Kakade & Foster (2008), and extended to general metrics in B\u0142asiok et al. (2023), see Definition 11).\nThe most intuitive special case of the above result is the square loss function, which corresponds to a trivial metric on the interval d(u, v) = |u \u2212 v|. In practice, different proper loss functions are also extensively used \u2014 the prime example being the cross entropy loss l(f, y) := \u2212(y ln p+ (1\u2212 y) ln(1\u2212 p)), which is connected with the metric dlogit(u, v) := | log(u/(1\u2212u))\u2212 log(v/(1\u2212 v))| on [0, 1]. Thus, we may want to generalize our results to also apply to non-trivial metrics."
        },
        {
            "heading": "B.1 GENERAL DUALITY",
            "text": "We will prove a more general statement of the duality theorem in B\u0142asiok et al. (2023). Specifically, they showed that the minimization problem in the definition of the dCE, can be dualy expressed as a maximal correlation between residual r := y \u2212 f and a bounded Lipschitz function of the prediction f . This notion, which we will refer to as weak calibration error first appeared in Kakade & Foster (2008), and was further explored in Gopalan et al. (2022); B\u0142asiok et al. (2023); B\u0142asiok et al. (2023)3. Definition 11 (Weak calibration error). For a distributionD over [0, 1]\u00d7{0, 1} of pairs of prediction and outcome, and a metric d on the space [0, 1] of all possible predictions, we define\nwCEd(D) := sup w\u2208Ld E f,y\u223cD (f \u2212 y)w(f), (8)\nwhere the supremum is taken over all functions w : [0, 1] \u2192 [\u22121, 1] which are 1-Lipschitz with respect to the metric d.4\nFor the trivial metric on the interval d(u, v) = |u\u2212 v|, wCE was known to be linearly related with dCE by B\u0142asiok et al. (2023). We show in this paper that the duality theorem connecting wCE and dCE holds much more generally, for a broad family of metrics. Theorem 12 (B\u0142asiok et al. (2023)). If a metric d on the interval satisfies d(u, v) \u2273 |v \u2212 u| then wCEd \u2248 dCEd.\nThe more general formulation provided by Theorem 12 can be shown by following closely the original proof step by step. We provide an alternate proof (simplified and streamlined) in Appendix D.8.\nB.2 THE dCEdlogit IS A CONSISTENT CALIBRATION MEASURE WITH RESPECT TO \u21131\nAs in turns out, for a relatively wide family of metrics on the space of predictions (including the dlogit metric), the associated calibration measures are consistent calibration measures with respect to the \u21131 metric. The main theorem we prove in this section is the following. Theorem 13. If a metric d : [0, 1]2 \u2192 R\u222a{\u00b1\u221e} satisfies d(u, v) \u2273 |u\u2212v| and moreover for some c > 0, \u2200\u03b5, \u2200u, v \u2208 [\u03b5, 1\u2212 \u03b5], d(u, v) \u2264 |u\u2212 v|\u03b5\u2212c, then dCEd is a consistent calibration measure.\nThe proof of this theorem (as is the case for many proofs of consistency for calibration measures) heavily uses the duality Theorem 12 \u2014 since proving that a function is a consistent calibration measure amounts to providing a lower and upper bound, it is often convenient to use the dCE formulation for one bound and wCE for the other.\nThe lower bound in Theorem 13 is immediate \u2014 since d(u, v) \u2265 \u21131(u, v), the induced Wasserstein distances on the space [0, 1] \u00d7 {0, 1} satisfy the same inequality, hence dCEd \u2265 dCE\u21131 , and dCE\u21131 \u2265 dCE/2 by Claim 31. As it turns out, if the metric of interest is well-behaved except near the endpoints of the unit interval, we can also prove the converse inequality, and lower bound wCE(D) by polynomial of wCEd(D).\n3Weak calibration was called smooth calibration error in Gopalan et al. (2022); B\u0142asiok et al. (2023). We revert back to the original terminology weak calibration error to avoid confusion with the notion of smECE developed in this paper.\n4In B\u0142asiok et al. (2023)\nLemma 14. Let d : [0, 1]2 \u2192 R+ \u222a {\u221e} be any metric satisfying for some c > 0,\n\u2200\u03b5, \u2200u, v \u2208 [\u03b5, 1\u2212 \u03b5], d(u, v) \u2264 |u\u2212 v|\u03b5\u2212c,\nthen wCEd(D)q \u2272 wCE(D), where q := max(c+ 1, 2).\n(Proof in Appendix D.5.)\nWe are ready now to prove the main theorem here\nProof of Theorem 13. We have dCEd(D) \u2265 dCE(D)/2 by our previous discussion, on the other hand Theorem 12 and Lemma 14 imply the converse inequality:\ndCEd(D) \u2248 wCEd(D) \u2264 wCE(D)1/q \u2248 dCE(D)1/q.\nCorollary 15. For a metric induced by cross-entropy loss function dlogit(u, v) := | ln(u/(1\u2212v))\u2212 ln(v/(1\u2212 v))|, the wCEdlogit is a consistent calibration measure.\nProof. To verify the conditions of Theorem 13 it is enough to check that logit(v) := ln(v/(1\u2212 v)) satisfies min(t, 1\u2212 t)c \u2264 ddt logit(t) \u2264 C. Since d dt logit(t) = 1 t(1\u2212t) , these conditions are satisfied with c = 1 and C = 4."
        },
        {
            "heading": "B.3 GENERALIZED SMOOTHECE",
            "text": "We now generalize the definition of SmoothECE to other metrics, and show that it remains a consistent calibration measure with respect to its metric. Motivated by the logit example discussed above, a concrete way to introduce a non-trivial metric on a space of predictions [0, 1], is to consider a continuous and increasing function h : [0, 1]\u2192 R\u222a{\u00b1\u221e}, and the metric obtained by pulling back the metric from R to [0, 1] through h, i.e. dh(u, v) := |h(u)\u2212 h(v)|. Using the isomorphism h between ([0, 1], dh) and a subinterval of (R\u222a{\u00b1\u221e}, |\u00b7|), we can introduce a generalization of the notion of smECE, where the kernel-smoothing is being applied in the image of h.\nMore concretely, for a probability distributionD over [0, 1]\u00d7{0, 1}, a kernel K : R\u00d7R\u2192 R+ and an increasing continuous map h : [0, 1]\u2192 R \u222a {\u00b1\u221e} we define\nr\u0302K,h(t) := E(f,y) K(t, h(f))(f \u2212 y)\nE(f,y) K(t, h(f))\n\u03b4\u0302K,h(t) := E (f,y) K(t, h(f)).\nAgain, we define\nsmECEK,h(D) := \u222b r\u0302K,h(t)\u03b4\u0302K,h(t) dt,\nwhich simplifies to smECEK,h(D) = \u222b \u2223\u2223\u2223\u2223 E\n(f,y)\u223cD K(t, h(f))(f \u2212 y) \u2223\u2223\u2223\u2223 dt. As it turns out, with the duality theorem in place (Theorem 12) the entire content of Section 3 can be carried over in this more general context without much trouble.\nSpecifically, if we define smECE\u03c3,h := smECEKN,\u03c3,h, where KN,\u03c3 is a Gaussian kernel with scale \u03c3, then \u03c3 7\u2192 smECE\u03c3,h(f, y) is non-increasing in \u03c3, and therefore there is a unique fixed point \u03c3\u2217 s.t. \u03c3\u2217 = smECE\u03c3\u2217,h(f, y).\nWe can now define smECE\u2217,h(f, y) := \u03c3\u2217, and we have the following generalization of Theorem 6, showing that SmoothECE remains a consistent calibration even under different metrics.\nTheorem 16. For any increasing and continuous function h : [0, 1] \u2192 R \u222a {\u00b1\u221e}, if we define dh : [0, 1] 2 \u2192 R+ to be the metric dh(u, v) = max(|h(u)\u2212 h(v)|, 2) then\ndCEdh(D) \u2272 smECE\u2217,h(D) \u2272 \u221a dCEdh(D).\n(Proof in Appendix D.7.)\nNote that if the function h is such that the associated metric dh satisfies the conditions of Theorem 13, as an additional corollary we can deduce that smECE\u2217,h is also a consistent calibration measure in a standard sense."
        },
        {
            "heading": "B.4 OBTAINING PERFECTLY CALIBRATED PREDICTOR VIA POST-PROCESSING",
            "text": "One of the appealing properties of the notion dCE as it was introduced in B\u0142asiok et al. (2023), was the theorem stating that if a predictor (f, y) is close to calibrated, then in fact a nearby perfectly calibrated predictor can be obtained simply by post-processing all the predictions by a univariate function. Specifically, they showed that for a distribution D over [0, 1]\u00d7{0, 1}, there is \u03ba : [0, 1]\u2192 [0, 1] such that for (f, y) \u223c D the pair (\u03ba(f), y) is perfectly calibrated and moreover E |\u03ba(f)\u2212f | \u2272\u221a\ndCE(D). As it turns out, through the notion of smECEh we can prove a similar in spirit statement regarding the more general distances to calibration dCEdh . The only difference is that we allow the postprocessing \u03ba to be a randomized function. Theorem 17. For any increasing function h : [0, 1]\u2192 R\u222a{\u00b1\u221e}, and any distributionD supported on [0, 1] \u00d7 {0, 1}, there is a probabilistic function \u03ba : [0, 1] \u2192 [0, 1] such that for (f, y) \u223c D, the pair (\u03ba(f), y) is perfectly calibrated and\nE dh(\u03ba(f), f) \u2272 smECE\u2217,h(D),\nwhere dh is the metric induced by h. In particular E dh(\u03ba(f), f) \u2272 \u221a dCEdh(D).\nProof. Let us consider a distribution D over [0, 1] \u00d7 {0, 1} and a monotone function h, such that smECEh,\u2217 = \u03c3\u2217.\nFirst, let us define the randomized function \u03ba1: let \u03c00 : R\u2192 R be a projection of R to h([0, 1]), and let \u03b7 \u223c N (0, \u03c3\u2217). We define\n\u03ba1(f) := h \u22121(\u03c00(h(f) + \u03b7)).\nWe claim that this \u03ba1 satisfy the following two properties:\n1. E(f,y)\u223cD |d(f, \u03ba\u2032(f))| \u2272 \u03c3\u2217,\n2. ECE(\u03ba\u2032(f), y) \u2272 \u03c3\u2217.\nIndeed, the first inequality is immediate:\nE[d(f, \u03ba\u2032(f)] = E[|h(f)\u2212 \u03c00(h(f) + \u03b7)|] \u2264 E |\u03b7| \u2264 \u03c3\u2217.\nThe proof that ECE(\u03ba\u2032(f), y) \u2272 \u03c3\u2217 is identical to the proof of Lemma 28, where such a statement was shown for the standard metric (corresponding to h(x) = x).\nFinally, those two properties together imply the statement of the theorem: indeed, if ECE(f \u2032, y) \u2264 \u03c3\u2217, we can take \u03ba2(t) := E[y|f \u2032 = t]. In this case pair (\u03ba2(f \u2032), y) is perfectly calibrated, and by definition of ECE, we have E |\u03ba2(f \u2032)\u2212 f \u2032| = ECE(f \u2032, y). Composing now \u03ba = \u03ba2 \u25e6 \u03ba1, we have\nE (f,y)\u223cD |\u03ba(f)\u2212 f | \u2264 E[|\u03ba2 \u25e6 \u03ba1(f)\u2212 \u03ba1(f)|] + E[|\u03ba1(f)\u2212 f |] \u2272 \u03c3\u2217.\nMoreover distribution D\u2032 of (\u03ba(f), y) is perfectly calibrated."
        },
        {
            "heading": "C ALGORITHMS",
            "text": "In this section we discuss how smECE can be computed efficiently: for any \u03c3, the quantity smECE\u03c3 can be approximated up to error \u03b5 in time O(n+M\u22121 log3/2 M\u22121) in the RAM model, where M = \u2308\u03b5\u22121\u03c3\u22121\u2309. In order to find an optimal scale \u03c3\u2217, we need to perform a binary search, involving log \u03b5\u22121 evaluations of smECE\u03c3 .\nThe explicit procedure to compute smECE is given in Algorithm 2.\nWe shall first observe that the convolution with the reflected Gaussian kernel can be expressed in terms of a convolution with a shift-invariant kernel. This is useful, since such a convolution can be implemented in time O(M logM) using Fast Fourier Transform, where m is the size of the discretization. Claim 18. For any function g : [0, 1] \u2192 R, the convolution with the reflected Gaussian kernel g \u2217 KN,\u03c3 can be equivalently computed as follows. Take an extension of g to the entire real line g\u0303 : R\u2192 R defined as g\u0303(x) := g(\u03c0R)(x)). Then\n[g \u2217 K\u0303N,\u03c3](t) = [g\u0303 \u2217KN,\u03c3](t),\nwhere KN,\u03c3 : R \u00d7 R \u2192 R is the standard Gaussian kernel K\u03c3(t1, t2) = exp(\u2212(t1 \u2212 t2) 2/2\u03c3)/ \u221a 2\u03c0\u03c32.\nProof. Elementary calculation. We can now restrict g\u0303 to the interval [\u2212T, T + 1] where T := \u2308 \u221a log(2\u03b5\u22121)\u2309, convolve such a restricted g\u0303 with a Gaussian, and restrict the convolution in turn to the interval \u03b5. Indeed, such a restriction introduces very small error, for every t \u2208 [0, 1] we have.\n[(1[\u2212T,T+1] \u00b7 g\u0303) \u2217KN,\u03c3](t)\u2212 [g\u0303 \u2217KN,\u03c3](t) \u2264 (1\u2212 \u03a6(T/\u03c3)) + (1\u2212 \u03a6((T + 1)/\u03c3)) \u2264 \u221a 2/\u03c0(T/\u03c3) exp(\u2212(T/\u03c3)2/2).\nIn practice, it is enough to reflect the function g only twice, around two of the boundary points (corresponding to the choice T = 1). For instance, when \u03c3 < 0.38, the above bound implies that the introduced additive error is smaller than \u03c32, and the error term rapidly improves as \u03c3 is getting smaller.\nLet us now discuss computation of smECE\u03c3 for a given scale \u03c3. To this end, we discretize the interval [0, 1], splitting it into M equal length sub-intervals. For a sequence of observations (fi, yi) we round each ri to the nearest integer multiple of 1/M , mapping it to a bucket bi = round(Mfi). In each bucket b \u2208 {0, . . .M}, we collect the residues of all observation falling in this bucket hb := \u2211 i:bi=b (fi \u2212 yi).\nIn the next step, we apply the Claim 18, and produce a wrapping h\u0303 of the sequence h \u2014 extending it to integer multiples of 1/M in the interval [\u2212T, T + 1] by pulling back h through the map \u03c0R.\nThe method smECE\u03c3 then proceeds to compute convolution h\u0303 \u2217 K with the discretization of the Gaussian kernel probability density function, i.e. K\u0303t := exp(\u2212t2/2\u03c32), and Kt := K\u0303t /\u2211 i K\u0303t .\nThis convolution r\u0303 := h \u2217K can be computed in time O(Q logQ), where Q = MT , using a Fast Fourier Transform, and is implemented in standard mathematical libraries. Finally, we report the sum of absolute values of the residuals \u2211 |r\u0303i| as an approximation to smECE\u03c3 as an approximation to smECE\u03c3 .\nAlgorithm 1: Efficient estimation of smECE\u03c3 , at fixed scale \u03c3 Function Discretization({(fi, zi}ni=1, M ) is\nh\u2190 zeros(M + 1); for i \u2208 [n] do\nb\u2190 round(Mfi); hb \u2190 hb + zi;\nend return h;\nend Function Wrap(h, T) is\nM \u2190 len(h); for i \u2208 [(2T + 1)M ] do\nj \u2190 (i mod 2M); if j > M then\nj \u2190 2M \u2212 j; end h\u0303i \u2190 hj ;\nend return h\u0303;\nend Function smECE(\u03c3, {fi, yi}n1 ) is\nh\u2190 Discretization({fi, fi \u2212 yi}, \u2308\u03c3\u22121\u03b5\u22121\u2309); h\u0303\u2190Wrap(h, \u2308 \u221a log(2\u03b5\u22121)); K \u2190 DiscreteGaussianKernel(\u03c3, \u2308\u03c3\u22121\u03b5\u22121\u2309); r\u0303 \u2190 h\u0303 \u2217K; return \u2211(T+1)M\u22121 i=TM |r\u0303i|;\nend\nAlgorithm 2: Efficient estimation of smECE\u2217: using binary search over \u03c3 to find a root of decreasing function smECE\u03c3 \u2212 \u03c3. Data: (fi, yi)n1 , \u03b5 Result: smECE\u2217({(fi, yi)}) l\u2190 0; u\u2190 1; while u\u2212 l > \u03b5 do\n\u03c3 \u2190 (u+ l)/2; if smECE\u03c3({fi, yi}) < \u03c3 then\nu\u2190 \u03c3; else\nl\u2190 \u03c3; end\nend return u;"
        },
        {
            "heading": "D OMITTED PROOFS",
            "text": ""
        },
        {
            "heading": "D.1 PROOF OF THEOREM 6",
            "text": "In this section we will prove Lemma 21 and Lemma 23, two main steps in the proof of Theorem 6, corresponding to respectively lower and upper bound. As it turns out, those two lemmas are true for a much wider class of kernels. The restriction on the kernel K to be a Gaussian kernel stems from the monotonicity property (Lemma 28), which was convenient for us to define the scale invariant measure smECE\u2217 by considering a fix-point scale \u03c3\u2217. In Appendix D.2 we will show that the Reflected Gaussian kernel satisfies the conditions of Lemma 21 and Lemma 23.\nWe will first define a dual variant of dCE. Definition 19. We define the weak calibration error to be the maximal correlation of the residual (f \u2212 y) with a 1-Lipschitz function and [\u22121, 1] bounded function of a predictor, i.e.\nwCE(D) := sup w\u2208L E (f,y)\u223cD w(f)(f \u2212 y),\nwhere L is a family of all 1-Lipschitz functions from [0, 1] to [\u22121, 1].\nTo show that smECE\u2217 is a consistent calibration measure we will heavily use the duality theorem proved in B\u0142asiok et al. (2023) \u2014 the wCE and dCE are (up to a constant factor) equivalent. A similar statement is proved in this paper, in a greater generality (see Theorem 12). Theorem 20 (B\u0142asiok et al. (2023)). For any distribution D over [0, 1]\u00d7 {0, 1} we have\ndCE(D) \u2264 wCE(D) \u2264 2dCE(D).\nIntuitively, this is useful since showing that a new measure smECE is a consistent calibration measure corresponds to upper and lower bounding it by polynomials of dCE. With the duality theorem above, we can use the minimization formulation dCE for one direction of the inequality, and the maximization formulation wCE for the other.\nIndeed, we will first show that wCE is upper bounded by smECE if we add the penalty parameter for the \u201cscale\u201d of the kernel K. Lemma 21. Let U \u2282 R be (possible infinite) interval containing [0, 1] and K : U \u00d7 U \u2192 R be a non-negative symmetric kernel satisfying for every t0 \u2208 [0, 1], \u222b K(t0, t) dt = 1, and \u222b |t \u2212 t0|K(t, t0) dt \u2264 \u03b3. Then wCE(D) \u2264 smECEK(D) + \u03b3.\nProof. Let us consider an arbitrary 1-Lipschitz function w : [0, 1] \u2192 [\u22121, 1], and take \u03b7 \u223c K as in the lemma statement. Since kernel K is nonnegative, and \u222b K(t, t0) dt = 0, we can sample triple (f\u0303 , f, y) s.t. (f, y) \u223c D, and f\u0303 is distributed according to density K(\u00b7, f). In particular E |f\u0303 \u2212 f | \u2264 \u03b3. We can bound now\nE (f,y)\u223cD [w(f)(f \u2212 y)] \u2264 E[w(f\u0303)(f \u2212 y)] + E |f \u2212 f\u0303 ||f \u2212 y|\n\u2264 \u03b3 + E [ w(f\u0303)(f \u2212 y) ] . (9)\nWe now observe that\nE[(f \u2212 y)|f\u0303 = t] = Ef,y K(t, f)(f \u2212 y) Ef,y K(t, f) = r\u0302(t),\nand the marginal density of f\u0303 is exactly\n\u00b5f\u0303 (t) = E (f,y)\u223cD K(t, f) = \u03b4\u0302(t).\nThis leads to E [ w(f\u0303)(f \u2212 y) ] = \u222b w(t)r\u0302(t)\u03b4\u0302(t) dt \u2264 \u222b |r\u0302(t)|\u03b4\u0302(t) dt = smECEK(f, y). (10)\nCombining (11) and (12) we conclude the statement of this lemma.\nTo show that smECEK(D) is upper bounded by dCE, we will first show that smECEK is zero for perfectly calibrated distributions, and then we will show that for well-behaved kernels smECEK(D) is Lipschitz with respect the Wasserstein distance on the space of distributions. Claim 22. For any perfectly calibrated distribution D and for any kernel K we have\nsmECEK(D) = 0.\nProof. Indeed, by the definition of r\u0302 we have\nr\u0302(t) = Ef,y K(f, t)(f \u2212 y)\nEf,y K(f, t) ,\nSince the distribution D is perfectly calibrated, we have E(f,y)\u223cD[(f \u2212 y)|f ] = 0, hence\nE f,y [K(f, t)(f \u2212 y)] = E f\n[ E\n(f,y)\u223cD [K(f, t)(f \u2212 y)|f ]\n] = E\nf\n[ K(f, t) E\n(f,y)\u223cD [(f \u2212 y)|f ]\n] = 0.\nThis means that the function r\u0302(t) is identically zero, and therefore smECEK(D) = \u222b t |r\u0302(t)|\u03b4\u0302(t) dt = 0.\nLemma 23. Let K be a symmetric, non-negative kernel, such that for and let \u03bb \u2264 1 be a constant such that for any t0, t1 \u2208 [0, 1] we have \u222b |K(t0, t) \u2212K(t1, t)|dt \u2264 |t0 \u2212 t1|/\u03bb. Let D1,D2 be a pair of distributions over [0, 1]\u00d7 {0, 1}. Then\n|smECEK(D1)\u2212 smECEK(D2)| \u2264 ( 1\n\u03bb + 1\n) W1(D1,D2).\nProof. We have\nsmECEK(D) = \u222b \u2223\u2223\u2223\u2223 E\n(f,y)\u223cD [K(t, f)(y \u2212 f)] \u2223\u2223\u2223\u2223 dt. If we have a coupling (f1, f2, y1, y2) s.t. E[|f1\u2212 f2|+ |y1\u2212 y2|] \u2264 \u03b4, (f1, y1) \u223c D1 and (f2, y2) \u223c D2, then by triangle inequality we can decompose\n|smECEK(D1)\u2212 smECEK(D2)| \u2264 \u222b\nE (f1,f2,y1,y2) [|K(t, f1)\u2212K(t, f2)||y1 \u2212 f1|dt\n+ \u222b E\n(f1,f2,y1,y2) [K(t, f2)(|f1 \u2212 f2|+ |y1 \u2212 y2|] dt.\nWe can bound those two terms separately\u222b E\n(f1,f2,y1) [|K(t, f1)\u2212K(t, f2)||y1\u2212f1|] dt \u2264 E (f1,f2,y1)\n\u222b |K(t, f1)\u2212K(t, f2)|dt \u2264 1\n\u03bb E[|f1\u2212f2|] \u2264 \u03b4/\u03bb,\nand similarly\u222b E [K(t, f2)(|f1 \u2212 f2|+ |y1 \u2212 y2|)] dt = E [\u222b t K(t, f2) dt \u00b7 (|f1 \u2212 f2|+ |y1 \u2212 y2| ] = E[|f1\u2212f2|+|y1\u2212y2|] \u2264 \u03b4.\nCorollary 24. Under the same assumptions on K as in Lemma 23, for any distribution D over [0, 1]\u00d7 {0, 1}, smECEK(D) \u2264 ( 1\n\u03bb + 1\n) dCE(D).\nProof. By definition there is a perfectly calibrated distributionD\u2032, such that W1(D,D\u2032) \u2264 dCE(D). By Claim 22, smECEK(D\u2032) = 0, and the corollary follows directly from Lemma 23."
        },
        {
            "heading": "D.2 FACTS ABOUT REFLECTED GAUSSIAN KERNEL",
            "text": "We wish to now argue that Lemma 21 and Lemma 23 imply the more specialized statements Lemma 7 and Lemma 8 respectively \u2014 the reflected Gaussian kernel KN,\u03c3 satisfies conditions of Lemma 21 and Lemma 23 with \u03b3 and \u03bb proportional to \u03c3. We\nLemma 25. Reflected Gaussian kernel K\u0303N,\u03c3 defined by (2) satisfies\n1. For every t0, we have \u222b K\u0303N,\u03c3(t, t0) dt = 1.\n2. For every t0, we have \u222b |t\u2212 t0|K\u0303N,\u03c3(t, t0) dt \u2264 \u221a 2/\u03c0\u03c3.\n3. For every t0, t1, we have \u222b |K\u0303N,\u03c3(t, t0)\u2212 K\u0303N,\u03c3(t, t0)|dt \u2264 |t0 \u2212 t1|/(2\u03c3).\nProof. For any given t0, the function K\u0303N,\u03c3(t0, \u00b7) is a probability density function of a random variable \u03c0R(t0 + \u03b7) where \u03b7 \u223c N (0, \u03c3) and \u03c0R : R\u2192 [0, 1] is defined in Section 1.1. In particular, we have |\u03c0R(x)\u2212 \u03c0R(y)| \u2264 |x\u2212 y|.\nThe property 1 is satisfied, since the K\u0303N,\u03c3(\u00b7, t0) is a probability density function. The property 2 follows since\u222b\n|t\u2212 t0|K\u0303N,\u03c3(t, t0) dt = E \u03b7\u223cN (0,\u03c3 |\u03c0R(t0 + \u03b7)\u2212 t0| = E \u03b7\u223cN (0,\u03c3 |\u03c0R(t0 + \u03b7)\u2212 \u03c0R(t0)|\n\u2264 E \u03b7\u223cN (0,\u03c3\n|\u03b7| = \u03c3 \u221a 2/\u03c0.\nFinally, the property 2 again follows from the same fact for a Gaussian random variable: the integral |K\u0303N,\u03c3(t, t0) \u2212 K\u0303N,\u03c3(t, t0)| is just a total variation distance between \u03c0R(t0 + \u03b7) and \u03c0R(t1 + \u03b7) where \u03b7 \u223c N (0, \u03c3), but by data processing inequality we have\nTV (\u03c0R(t0 + \u03b7), \u03c0R(t1 + \u03b7)) \u2264 TV (t0 + \u03b7, t1 + \u03b7) \u2264 |t0 \u2212 t1|/(2\u03c3).\nWhere the last bound on the total variation distance between two one-dimension Gaussians is a special case of Theorem 1.3 in Devroye et al. (2018)5.\nDefinition 26. We say that a paramterized family of kernels K\u03c3 : U\u00d7U \u2192 R where [0, 1] \u2282 U \u2282 R is a proper kernel family if for any \u03c31 \u2264 \u03c32 there is a non-negative kernel H\u03c31,\u03c32 : U \u00d7 U \u2192 R, satisfying \u2225H\u03c31,\u03c32\u22251\u21921 \u2264 1 and K\u03c32 = K\u03c31 \u2217H\u03c31,\u03c32 ."
        },
        {
            "heading": "Here the notation K \u2217H is denotes",
            "text": "[K \u2217H](t1, t2) := \u222b U K(t1, t)H(t, t2) dt,\nand\n\u2225H\u22251\u21921 := sup t0\u2208U \u222b U |H(t0, t)|dt.\nClaim 27. The family of reflected Gaussian kernels K\u0303N,\u03c3 is a proper kernel family, with\nK\u0303\u03c31,N = K\u0303\u03c32,N \u2217 K\u0303\u221a\u03c321\u2212\u03c322 ,N .\nProof. Let \u03c33 := \u221a \u03c321 \u2212 \u03c322 , we wish to show that K\u0303\u03c31,N = K\u0303\u03c32,N \u2217 K\u0303\u03c33,N . In order to show this, it is enough to prove that for any f , we have f \u2217 K\u0303\u03c31,N = f \u2217 K\u0303\u03c32,N \u2217 K\u0303\u03c33,N . This is true by Claim 18, since this property holds for standard Gaussian kernel K\u03c32,N \u2217K\u03c33,N = K\u03c31,N (it is here equivalent to saying that for two independent random variables Z2 \u223c N (0, \u03c32) and Z3 \u223c N (0, \u03c33) we have Z2 + Z2 \u223c N (0, \u03c31)).\n5This special case, where the two variances are equal, is in fact an elementary calculation.\nD.3 USEFUL PROPERTIES OF SMECE.\nLemma 28 (Monotonicity of smECE). Let K\u03c3 be any proper kernel family parameterized by \u03c3. If \u03c31 \u2264 \u03c32, then\nsmECEK\u03c31 (D) \u2265 smECEK\u03c32 (D).\nProof. Let us define\nh\u03c3(t) := E (f,y)\u223cD K\u03c3(t, f)(f \u2212 y) = r\u0302(t)\u03b4\u0302(t),\nsuch that\nsmECEK\u03c3 (D) = \u2225h\u03c3\u22251 := \u222b |h\u03c3(t)|dt.\nSince \u03c31 \u2264 \u03c32 and K\u03c3 is a proper kernel family, we can write K\u03c32 = K\u03c31 \u2217H\u03c31,\u03c32 . We have now,\nh\u03c31 \u2217H\u03c31,\u03c32 = (\nE (f,y)\n(f \u2212 y)K\u03c31(\u00b7, f) ) \u2217H\u03c31,\u03c32\n= E f,y (f \u2212 y)[K\u03c31 \u2217H\u03c31,\u03c32(\u00b7, f)] = E f\u2212y (f \u2212 y)K\u03c32(\u00b7, f)\n= h\u03c32 .\nOn the other hand for any function f we have \u2225f \u2217 H\u03c31,\u03c32\u22251 \u2264 \u2225f\u22251\u2225H\u03c31,\u03c32\u22251\u21921, and \u2225H\u03c31,\u03c32\u22251\u21921 \u2264 1 by the definition of proper kernel family. Therefore\nCorollary 29. In particular for \u03c31 \u2264 \u03c32 we have smECE\u03c32(D) \u2264 smECE\u03c31(D).\nProof. Reflected Gaussian kernels form a proper kernel family by Claim 27. Lemma 30. For any \u03c3, we have \u02dcsmECE\u03c3(D) = smECE\u03c3(D)\u00b1 \u03c3 \u221a 2/\u03c0.\nProof. Let\nf\u0302(t) := Ef,y K\u0303N,\u03c3(t, f)f Ef,y K\u0303N,\u03c3(t, f) .\nWe have\n| \u02dcsmECE\u03c3(f, y)\u2212 smECE\u03c3(f, y)| \u2264 \u222b |f\u0302(t)\u2212 t|\u03b4\u0302(t) dt\n\u2264 \u222b\nE f [K\u0303N,\u03c3(t, f)|f \u2212 t|] dt\n= E f\n\u222b K\u03c3(t, f)|f \u2212 t|dt\n= E f E Z\u223cN (f,\u03c3) |f \u2212 \u03c0R(Z)|\n\u2264 E Z\u223cN (0,\u03c3)\n|Z| = \u221a 2/\u03c0.\nD.4 EQUIVALENCE BETWEEN DEFINITIONS OF dCE FOR TRIVIAL METRICS\nThe dCE(D) was defined in B\u0142asiok et al. (2023) as a Wasserstein distance to the set of perfectly calibrated distributions over X := [0, 1]\u00d7 {0, 1}, where X is equipped with a metric\nd1((f1, y1), (f2, y2)) := { |f1 \u2212 f2| if y1 = y2 \u221e otherwise .\nWhile generalizing the notion to that of dCEd, where d is a general metric on [0, 1], we chose a different metric on X (specifically, we put a different metric on the second coordinate), that is d\u0303((f1, y1), (f2, y2)) = d(f1, f2) + |y1 \u2212 y2|. As it turns out, for the case of a trivial metric on the space of predictions, this choice is inconsequential, but the new definition has better generalization properties.\nClaim 31. For the metric \u21131(f1, f2) = |f1 \u2212 f2|, we have dCE(D) \u2272 dCE\u21131(D) \u2264 dCE(D), for some universal constant c.\nProof. The lower bound dCE\u21131 \u2264 dCE is immediate, since dCE\u21131 is a distance of D to P with respect to a Wasserstein distance induced by the metric d1 on [0, 1]\u00d7{0, 1}, dCE is the Wasserstein distance with respect to the metric d2, and we have a pointwise bound d1(u, v) \u2264 d2(u, v), implying W1,d1(D1,D2) \u2264W1,d2(D1,D2). The other bound follows from Theorem 12 and Theorem 20 \u2014 dCE and dCE\u21131 are within constant factor from wCE\u21131 ."
        },
        {
            "heading": "D.5 PROOF OF LEMMA 14",
            "text": "Proof. Let us take w(x) : [0, 1]\u2192 [\u22121, 1] as in the definition of wCEd, a 1-Lipschitz function with respect to the metric d, such that E(y \u2212 f)w(f) = wCEd(f, y) = \u03b5. We wish to show that wCE(f, y) \u2273 \u03b5c+1. Indeed, let us take w\u0303(X) := w(\u03c0I(x)) where I := [\u03b3, 1\u2212 \u03b3], \u03c0I : [0, 1]\u2192 I is a projection onto the interval I , and \u03b3 := \u03b5/C for some large constant C.\nNote that w\u0303 is O(\u03b5\u2212c)-Lipschitz with respect to the standard metric on [0, 1]. If E(f \u2212 y)w\u0303(f) \u2265 \u03b5/2, we immediately have wCE(f, y) \u2273 \u03b5c+1 (we can use w\u0303/L as a test function, where L = O(\u03b5\u2212c) is a Lipcshitz constant for function w\u0303). Otherwise E(f \u2212 y)(w(f) \u2212 w\u0303(f)) \u2265 \u03b5/2. Let us call w2 := (w\u2212 w\u0303)/2, such that E(f \u2212y)w2(f) \u2265 \u03b5/4, and moreover supp(w2) \u2282 [0, 1]\\ I , where w2 is 1-Lipschitz with respect to d.\nSince [0, 1] \\ I has two connected components [0, \u03b3) and (1\u2212 \u03b3, 1], on one of those two connected components correlation between the residual (y \u2212 f) and w2 has to be at least \u03b5/8. Since the other case is analogous, let us assume for concreteness, that\nE(y \u2212 f)w3(f) \u2265 \u03b5/8,\nwhere w3(x) = w2(x) for x \u2208 [0, \u03b3) and w3(x) = 0 otherwise. We will show that this implies Pr(f \u2264 \u03b3\u2227y = 1) \u2273 \u03b5, and refer to Claim 32 to finish the argument. Indeed\nE(y\u2212f)w3(f) \u2264 E [(1\u2212 f)1[f \u2264 \u03b3 \u2227 y = 1]]+E [f1[f \u2264 \u03b3 \u2227 y = 0]] \u2264 Pr(f \u2264 \u03b3\u2227y = 1)+\u03b3,\nhence Pr(f \u2264 \u03b3 \u2227 y = 1) \u2265 \u03b5/8\u2212 \u03b3 \u2265 \u03b5/16,\nwhere we finally specify \u03b3 := \u03b5/32.\nTo finish the proof, it is enough to show the following\nClaim 32. For a random pair (f, y) of prediction and outcome, if Pr(f \u2264 \u03b3 \u2227 y = 1) \u2265 \u03b5 or Pr(f \u2265 1\u2212 \u03b3 \u2227 y = 0) \u2265 \u03b5, where \u03b3 = \u03b5/8, then wCE(f, y) \u2273 \u03b52.\nProof. We will only consider the case Pr(f \u2264 \u03b3 \u2227 y = 1) \u2265 \u03b5. The other case is identical. Let us take w(x) := max(1\u2212 x/2\u03b3, 0). We have\nE(y \u2212 f)w(f) \u2265 1\n2 Pr(f \u2264 \u03b3 \u2227 y = 1)\u2212 2\u03b3 Pr(f \u2264 \u03b3 \u2227 y = 0) \u2265 \u03b5/2\u2212 2\u03b3 \u2265 \u03b5/4.\nSince w is O(1/\u03b5)-Lipschitz, we have wCE(f, y) \u2273 \u03b52."
        },
        {
            "heading": "D.6 SAMPLE COMPLEXITY \u2014 PROOF OF THEOREM 9",
            "text": "Lemma 33. Let X : [0, 1] \u2192 R be a random function, satisfying with probability 1, \u2225X\u22251 :=\u222b 1 0 |X(t)|dt \u2264 1 and sup\nt X(t) \u2264 \u03c3. Assume moreover that for every t, we have E[X(t)] = 0.\nConsider now m independent realizations X1, X2, . . . Xm : [0, 1]\u2192 R, each identically distributed as X(t), and finally let\nX(t) := 1\nm\n\u2211 Xi(t).\nThen\nE [ \u2225X(t)\u222521 ] \u2264 1\n\u03c3m .\nProof. By Cauchy-Schwartz inequality \u2225X\u22251 \u2264 \u2225X\u22252\u22251\u22252 = \u2225X\u22252, hence E[\u2225X\u222521] \u2264 E[\u2225X\u222522] = E [\u222b X(t)2 dt ] = \u222b E[X(t)2] dt\n= 1\nm\n\u222b E[X(t)2] dt\n= 1\nm E[\u2225X\u222522] \u2264\n1 m E[\u2225X\u22251\u2225X\u2225\u221e] \u2264 1 \u03c3m .\nProof of Theorem 9. Let us first focus on the case \u03c3 = \u03c30. For a pair (f, y) \u2208 [0, 1]\u00d7 {0, 1}, let us define X(\u03c30)f,y : [0, 1]\u2192 R as\nX (\u03c30) f,y (t) := K\u0303\u03c30(f, t)(f \u2212 y).\nNote that smECE\u03c30(D\u0302) = \u2225 \u2211 i X (\u03c30) fi,yi /m\u22251, and similarly smECE(D) = \u2225Ef,y\u223cD X(\u03c30)f,y \u22251.\nDefine X\u0303(\u03c30)i := X (\u03c30) fi,yi \u2212 Ef,y\u223cD X(\u03c30)f,y \u2014 this is a random function, since (fi, yi) is chosen at random from distribution D, and note that:\n1. Random functions X\u0303(\u03c30)i for i \u2208 {1, . . . ,m} are independent and identically distributed.\n2. With probability 1, we have \u2225X\u0303(\u03c300)i \u22251 \u2264 2maxf \u2225K\u0303\u03c30(f, \u00b7)\u22251 = 2.\n3. Similarly, with probability 1 we have \u2225X\u0303(\u03c30)i \u2225\u221e \u2264 2 sup t1,t2 K\u0303\u03c30(t1, t2) \u2264 2\u03c3\u221210 .\n4. For any t \u2208 [0, 1] and i \u2208 {1, . . . ,m}, we have E[X\u0303(\u03c30)i (t)] = 0.\nTherefore, we can apply Lemma 33 to deduce\nE [\u2225\u2225\u2225\u2225 1m \u2211 X\u0303i \u2225\u2225\u2225\u22252 1 ] \u2264 1 \u03c30m ,\nhence, if m \u2273 \u03b5\u22122\u03c3\u221210 , by Chebyshev inequality with probability at least 2/3 we can bound \u2225 \u2211\ni X\u0303 (\u03c30) i /m\u22251 \u2264 \u03b5, and if this event holds, using triangle inequality smECE\u03c30(D)\u2212 smECE\u03c30(D\u0302)| \u2264 \u2225 \u2211 i X\u0303 (\u03c30) i \u2225/m\u03b5.\nFinally, for \u03c3 > \u03c30, note that X (\u03c3) i = X (\u03c30) i \u2217 K\u0303N,\u221a\u03c32\u2212\u03c320 (Claim 27) and therefore as soon as \u2225 \u2211\nX\u0303 (\u03c30) i \u2225 \u2264 \u03b5, we also have \u2225 \u2211 i X\u0303 (\u03c3) i /m\u22251 = \u2225 \u2211 i X\u0303 (\u03c30) i \u2217 K\u0303N,\u221a\u03c32\u2212\u03c320\u22251\n\u2264 \u2225 \u2211 i X\u0303 (\u03c30) i \u22251\u2225K\u0303\u22251\u21921 \u2264 \u03b5,\nwhere \u2225K\u0303\u22251\u21921 := sup\nt1 \u222b t2 |K\u0303(t1, t2)|dt \u2264 1."
        },
        {
            "heading": "This implies |smECE\u03c3(D)\u2212 smECE\u03c3(D\u0302)| < \u03b5 for all \u03c3 \u2265 \u03c30.",
            "text": "Finally, if smECE\u2217(D) = \u03c3\u2217 \u2265 \u03c30, we have smECE\u03c3\u2217(D) = \u03c3\u2217, hence smECE\u03c3\u2217(D\u0302) \u2265 \u03c3\u2217 \u2212 \u03b5, and by monotonicity smECE\u03c3\u2217\u2212\u03b5(D\u0302) \u2265 \u03c3\u2217 \u2212 \u03b5, implying smECE\u2217(D\u0302) \u2265 \u03c3\u2217 \u2212 \u03b5. Identical argument shows smECE\u2217(D\u0302) \u2264 \u03c3\u2217 + \u03b5."
        },
        {
            "heading": "D.7 PROOF OF THEOREM 16",
            "text": "The Lemma 21 and Lemma 23 have their correspondent versions in the more general setting where a metric is induced on the space of predictions [0, 1] by a monotone function h : [0, 1] \u2192 R \u2014 the proofs are almost identical to those supplied in the special case, except we need to use the more general version of the duality theorem between wCE and dCE, with respect to a metric d (Theorem 12). Lemma 34. Let h be an increasing function h : [0, 1]\u2192 R \u222a {\u00b1\u221e} and dh(u, v) = |h(u)\u2212 h(v)| be the induced metric on [0, 1]. Assume moreover that K(t) is a probability density function, such that for \u03b7 \u223c K we have E |\u03b7| = \u03b3. Finally, let us abuse the notation to define the associated kernel on R\u00d7 R as K(x, y) := K(x\u2212 y). Then\nwCEd(f, y) \u2264 smECEK,dh(f, y) + \u03b3.\nProof. Let us consider an arbitrary 1-Lipschitz function w : [0, 1]\u2192 [\u22121, 1] with respect to dh, and take \u03b7 \u223c K as in the lemma statement. Let I = h([0, 1], and let us take w\u0303 : I \u2192 R to be given by w\u0303(t) = w(h\u22121(t)), and note that w\u0303 is Lipschitz, since w was Lipschitz with respect to dh. We can therefore extend w\u0303 to a [\u22121, 1] valued Lipschitz function on the entire line R. Now for any f, \u03b7 we have |w\u0303(h(f) + \u03b7)\u2212 w\u0303(h(f))| \u2264 |\u03b7|, and we can bound\nE (f,y)\u223cD [w(f)(f \u2212 y)] \u2264 E[w\u0303(h(f) + \u03b7)(f \u2212 y)] + E |\u03b7||f \u2212 y|\n\u2264 E |\u03b7|+ E [E[w\u0303(h(f) + \u03b7)(f \u2212 y)] \u2264 \u03b3 + E [E[w\u0303(h(f) + \u03b7)(f \u2212 y)] . (11)\nWe now observe that\nE[(f \u2212 y)|h(f) + \u03b7 = t] = Ef,y K(t, h(f))(f \u2212 y) Ef,y K(t, h(f)) = r\u0302(t),\nand \u00b5f+\u03b7(t) = E\nf,y K(t, h(f)) = \u03b4\u0302(t),\nwhere \u00b5f+\u03b7 is the measure of h(f) + \u03b7.\nThis leads to E [w\u0303(h(f) + \u03b7)(f \u2212 y)] = \u222b w\u0303(t)r\u0302(t)\u03b4\u0302(t) dt \u2264 \u222b |r\u0302(t)|\u03b4\u0302(t) dt = smECEK,dh(f, y). (12)\nCombining (11) and (12) we conclude the statement of this lemma.\nLemma 35. Let h be an increasing function h : [0, 1]\u2192 R\u222a{\u00b1\u221e}, and dh(u, v) := |h(u)\u2212h(v)| be the induced metric on [0, 1].\nLet K(t) be a probability density function of a random variable, such that for \u03b7 \u223c K we have E \u03b7 = 0, and let \u03bb \u2264 1 be a constant such that for any \u03b5, we have TV(\u03b7, \u03b7+\u03b5) \u2264 \u03b5/\u03bb, finally by the abuse of notation let us define te assocated kernel on R\u00d7 R as K(x, y) := K(x\u2212 y). Then Then\nsmECEK,h(D) \u2264 ( 1\n\u03bb + 1\n) dCEdh(D)."
        },
        {
            "heading": "D.8 GENERAL DUALITY THEOREM (PROOF OF THEOREM 12)",
            "text": "Let P \u2282 \u2206([0, 1] \u00d7 {0, 1}) be the family of perfectly calibrated distributions. This set is cut from the full probability simplex \u2206([0, 1] \u00d7 {0, 1}) by a family of linear constraints, specifically \u00b5 \u2208 P if and only if \u2200t, (1\u2212 t)\u00b5(t, 1)\u2212 t\u00b5(t, 0) = 0. Definition 36. Let F(H,R) be a family of all functions from H to R. For a convex set of probability distributions Q \u2282 \u2206(H), we define Q\u2217 \u2282 F(H,R) to be a set of all functions q, s.t. for all D \u2208 Q we have Ex\u223cD q(x) \u2264 0. Claim 37. The set P\u2217 \u2282 F([0, 1] \u00d7 {0, 1},R) is given by the following inequalities. A function H \u2208 P\u2217 if and only if\n\u2200t, E y\u223cBer(t) H(t, y) \u2264 0.\nLemma 38. Let W1(D1,D2) be the Wasserstein distance between two distributions D1,D2 \u2208 \u2206([0, 1]\u00d7, {0, 1}) with arbitrary metric d on the set [0, 1]\u00d7 {0, 1}, and let Q \u2282 \u2206([0, 1]\u00d7 {0, 1}) be a convex set of probability distributions."
        },
        {
            "heading": "The value of the minimization problem",
            "text": "min D1\u2208Q\nW1(D1,D)\nis equal to\nmax E (f,y)\u223cD H(f, y)\ns.t. H is Lipschitz with respect to d, H \u2208 Q\u2217.\nProof. Let us consider a linear space V of all finite signed Radon measures on X := [0, 1]\u00d7{0, 1}, satisfying \u00b5(X) = 0. We equip this space with the norm \u2225\u00b5\u2225V := EMD(\u00b5+, \u00b5\u2212) for measures s.t. \u00b5+(X) = 1 (and extended by \u2225\u03bb\u00b5\u2225V = \u03bb\u2225\u00b5\u2225V to entire space). The dual of this space is Lip0(X) \u2014 space of all Lipschitz functions on X which are 0 on some fixed base point x0 \u2208 X (the choice of base point is inconsequential). The norm on Lip0(X) is \u2225W\u2225L given by the Lipschitz constant of W (see Chapter 3 in Weaver (2018) for proofs and more extended discussion).\nFor a function H on X and a measure \u00b5 on X , we will write H(\u00b5) to denote \u222b W d\u00b5.\nThe weak duality is clear: for any Lipschitz function H \u2208 Q\u2217, and any distribution D1 \u2208 Q we have H(D) \u2264 H(D1) +W1(D1,D) = W1(D1,D). For the strong duality, we shall now apply the following simple corollary of Hahn-Banach theorem.\nClaim 39 (Deutsch & Maserick (1967), Theorem 2.5). Let (X, \u2225 \u00b7 \u2225X) be a normed linear space, x0 \u2208 X , and P \u2282 X a convex set, and let d(x, P ) := inf\np\u2208P \u2225x \u2212 p\u2225X . Then there is w \u2208 X\u2217, such\nthat \u2225w\u2225X\u2217 = 1 and inf p\u2208P w(p)\u2212 w(x) = d(x, P ).\nTake a convex set P \u2282 V given by P := {D\u2212q : q \u2208 Q}. Clearly minD1\u2208Q W1(D,D1) = d(0, P ) by definition of the space V , and hence using the claim above, we deduce\nd(0, P ) = max H\u0303\u2208Lip0:\u2225H\u0303\u2225L=1 inf p\u2208P H\u0303(p).\nTaking H\u0303 which realizes this maximum, we can now consider a shift H := H\u0303 \u2212 sup q\u2208Q H\u0303(Q), so that H \u2208 Q\u2217, and verify\nmin D1\u2208Q W1(D,D1) = d(0, P ) = inf p\u2208P H\u0303(p) = H\u0303(D)\u2212 sup q\u2208Q H\u0303(q) = H(D).\nCorollary 40. For any metric d on [0, 1]\u00d7{0, 1}, the dCEd(D) is equal to the value of the following maximization program\nmax E (f,y)\u223cD H(f, y)\ns.t. H is Lipschitz with respect to d \u2200t, E\ny\u223cBer(t) H(t, y) \u2264 0.\nLemma 41. For any metric d on [0, 1] if we define d\u0302 to be a metric on [0, 1] \u00d7 {0, 1} given by d\u0302((f1, y1), (f2, y2)) := d(f1, f2) + |y1 \u2212 y2|, we have\nwCEd(D) \u2265 dCEd\u0302(D)/2\nProof. We shall compare the value of wCEd(D) with the optimal value of the dual as in Corollary 40.\nLet us assume that for a distribution D we have a function H : [0, 1] \u00d7 {0, 1} \u2192 R, s.t. E(f,y)\u223cD H(f, y) = OPT, which is Lipschitz with respect to d\u0302. We wish to find a function w : [0, 1]\u2192 [\u22121, 1] which is Lipschitz with respect to d, s.t.\nE f,y (f \u2212 y)w(f) \u2265 OPT/2.\nLet us take w(f) := H(f, 0)\u2212H(f, 1).\nWe will show instead that w is 2-Lipschitz, [\u22121, 1] bounded and satisfies Ef,y(f \u2212y)w(f) \u2265 OPT, and the statement of the lemma will follow by scaling.\nLet us define w(f) := H(f, 0)\u2212H(f, 1). The condition\n\u2200f, E y\u223cBer(f) H(f, y) \u2264 0\nis equivalent to fw(f) \u2265 H(f, 0). Hence\nH(f, y) = yH(f, 1) + (1\u2212 y)H(f, 0) = H(f, 0)\u2212 yw(f) \u2264 (f \u2212 y)w(f),\nwhich implies E(f \u2212 y)w(f) \u2265 EH(f, y).\nMoreover, the function w(f) is bounded by construction of the metric d\u0302 and the assumption that H(f, y) was Lipschitz. Indeed |w(f)| = |H(f, 0)\u2212H(f, 1)| \u2264 d\u0302((f, 0), (f, 1)) \u2264 1."
        },
        {
            "heading": "E CONSISTENT CALIBRATION MEASURES AND SMOOTHECE",
            "text": "Here we elaborate on the shortcomings of ECE, and resolution offered by SmoothECE, and the notion of a consistent calibration measure (B\u0142asiok et al., 2023). One fundamental issue with the ECE is: it is discontinuous in the underlying predictor, so a small change in the predictor can cause a large change in its ECE. A simple example of this phenomenon was presented in the Introduction of B\u0142asiok et al. (2023) (also formalized as Lemma 4.8 in the same work). Here, we will demonstrate this discontinuity visually, with a related example.\nIn Figure 3, we construct three different distributions that are each small perturbations of one another. The top row shows samples (fi, yi) \u2208 [0, 1] \u00d7 {0, 1} from each distribution. The first distribution (first column) is nearly perfectly calibrated. This is is evident from both the smoothECE diagram (middle row) and the binnedECE diagram (bottom row). To construct the second and third distributions, we shift each positive sample (y = 1) slightly to the right, and each negative sample (y = 0) slightly to the left. No sample moves more than 0.05 from its initial position in Distribution 1, so this is a small perturbation to the predictions fi. We see that the smooth reliability diagram in the middle row does not change much between all three distributions, and the smECE metric itself also stays nearly constant (the smECE metric is listed in the upper-left of each plot).\nHowever, by Distribution 3, the binned reliability diagram (bottom row) has changed drastically, and appears to be significantly mis-calibrated. Moreover, the binned ECE metric (with 20 bins) has changed from 0.06 in the first distribution to 0.31 in the third distribution. The binned ECE thus incorrectly reports that Distribution 3 is far from calibrated. This essentially occurs because we have shifted the positive and negative samples into disjoint bins by Distribution 3.\nThis example demonstrates how standard (binned) ECE is not robust to small perturbations in the predictor, and can severely over-estimate the calibration error. Smooth ECE, on the other hand, does not suffer this flaw. The formal definition of a consistent calibration measure in B\u0142asiok et al. (2023) enforces this robustness/continuity property, in addition to other natural theoretical properties. We refer to the exposition in B\u0142asiok et al. (2023) for further theoretical details."
        },
        {
            "heading": "E.1 STATISTICAL CONSISTENCY",
            "text": "Our estimator of SmoothECE also satisfies the classical criteria of statistical consistency. Specifically Theorem 9 shows that as the number of samples m \u2192 \u221e, the finite-sample estimation error \u03b5 \u2192 0. In fact, Theorem 9 is stronger than just asymptotic consistency\u2014 it provides a quantitative generalization bound."
        }
    ],
    "title": "SMOOTH ECE: PRINCIPLED RELIABILITY DIAGRAMS",
    "year": 2023
}