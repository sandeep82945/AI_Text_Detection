{
    "abstractText": "Integral reinforcement learning (IntRL) demands the precise computation of the utility function\u2019s integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method \u2013 in this case, the quadrature rule \u2013 can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration\u2019s convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL\u2019s policy iteration and Newton\u2019s method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton\u2019s method, with its upper bound proportional to the computational error. Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function. We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Mat\u00e9rn kernel to be O(N\u22122) and O(N\u2212b), where N is the number of evenly-spaced samples and b is the Mat\u00e9rn kernel\u2019s smoothness parameter. These theoretical findings are finally validated by two canonical control tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenhan Cao"
        },
        {
            "affiliations": [],
            "name": "Wei Pan"
        }
    ],
    "id": "SP:616f2a0b2c782c9a20cd775b5a43d9cb13f1f270",
    "references": [
        {
            "authors": [
                "Murad Abu-Khalaf",
                "Frank L Lewis"
            ],
            "title": "Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network hjb approach",
            "year": 2005
        },
        {
            "authors": [
                "Kendall Atkinson"
            ],
            "title": "An introduction to numerical analysis",
            "venue": "John wiley & sons,",
            "year": 1991
        },
        {
            "authors": [
                "Leemon C Baird"
            ],
            "title": "Reinforcement learning in continuous time: Advantage updating",
            "venue": "IEEE International Conference on Neural Networks (ICNN\u201994),",
            "year": 1994
        },
        {
            "authors": [
                "Randal W Beard",
                "George N Saridis",
                "John T Wen"
            ],
            "title": "Galerkin approximations of the generalized hamilton-jacobi-bellman",
            "venue": "equation. Automatica,",
            "year": 1997
        },
        {
            "authors": [
                "Fran\u00e7ois-Xavier Briol",
                "Chris J Oates",
                "Mark Girolami",
                "Michael A Osborne",
                "Dino Sejdinovic"
            ],
            "title": "Probabilistic integration: a role in statistical computation? 2019",
            "year": 2019
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "Ricky TQ Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David K Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jon Cockayne",
                "Chris J Oates",
                "Timothy John Sullivan",
                "Mark Girolami"
            ],
            "title": "Bayesian probabilistic numerical methods",
            "venue": "SIAM review,",
            "year": 2019
        },
        {
            "authors": [
                "Kenji Doya"
            ],
            "title": "Reinforcement learning in continuous time and space",
            "venue": "Neural computation,",
            "year": 2000
        },
        {
            "authors": [
                "Philipp Hennig",
                "Michael A Osborne",
                "Hans P Kersting"
            ],
            "title": "Probabilistic Numerics: Computation as Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Holt",
                "Alihan H\u00fcy\u00fck",
                "Zhaozhi Qian",
                "Hao Sun",
                "Mihaela van der Schaar"
            ],
            "title": "Neural laplace control for continuous-time delayed systems",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Yu Jiang",
                "Zhong-Ping Jiang"
            ],
            "title": "Robust adaptive dynamic programming",
            "year": 2017
        },
        {
            "authors": [
                "Motonobu Kanagawa",
                "Bharath K Sriperumbudur",
                "Kenji Fukumizu"
            ],
            "title": "Convergence guarantees for kernel-based quadrature rules in misspecified settings",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Motonobu Kanagawa",
                "Bharath K Sriperumbudur",
                "Kenji Fukumizu"
            ],
            "title": "Convergence analysis of deterministic kernel-based quadrature rules in misspecified settings",
            "venue": "Foundations of Computational Mathematics,",
            "year": 2020
        },
        {
            "authors": [
                "Toni Karvonen",
                "Simo S\u00e4rkk\u00e4"
            ],
            "title": "Classical quadrature rules via gaussian processes",
            "venue": "IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP),",
            "year": 2017
        },
        {
            "authors": [
                "Peter Lancaster"
            ],
            "title": "Error analysis for the newton-raphson method",
            "venue": "Numerische Mathematik,",
            "year": 1966
        },
        {
            "authors": [
                "Jae Young Lee",
                "Jin Bae Park",
                "Yoon Ho Choi"
            ],
            "title": "Integral reinforcement learning for continuoustime input-affine nonlinear systems with simultaneous invariant explorations",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Frank L Lewis",
                "Draguna Vrabie"
            ],
            "title": "Reinforcement learning and adaptive dynamic programming for feedback control",
            "venue": "IEEE circuits and systems magazine,",
            "year": 2009
        },
        {
            "authors": [
                "FW Lewis",
                "Suresh Jagannathan",
                "Aydin Yesildirak"
            ],
            "title": "Neural network control of robot manipulators and non-linear systems",
            "venue": "CRC press,",
            "year": 1998
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Jonathan J Hunt",
                "Alexander Pritzel",
                "Nicolas Heess",
                "Tom Erez",
                "Yuval Tassa",
                "David Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971,",
            "year": 2015
        },
        {
            "authors": [
                "Michael Lutter",
                "Christian Ritter",
                "Jan Peters"
            ],
            "title": "Deep lagrangian networks: Using physics as model prior for deep learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Hamidreza Modares",
                "Frank L Lewis"
            ],
            "title": "Optimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning",
            "year": 2014
        },
        {
            "authors": [
                "Hamidreza Modares",
                "Frank L Lewis",
                "Mohammad-Bagher Naghibi-Sistani"
            ],
            "title": "Integral reinforcement learning and experience replay for adaptive optimal control of partially-unknown constrained-input continuous-time systems",
            "year": 2014
        },
        {
            "authors": [
                "Erich Novak"
            ],
            "title": "Deterministic and stochastic error bounds in numerical analysis, volume 1349",
            "year": 2006
        },
        {
            "authors": [
                "Anthony O\u2019Hagan"
            ],
            "title": "Bayes\u2013hermite quadrature",
            "venue": "Journal of statistical planning and inference,",
            "year": 1991
        },
        {
            "authors": [
                "Alexander M Ostrowski"
            ],
            "title": "Solution of equations and systems of equations: Pure and applied mathematics: A series of monographs and textbooks",
            "venue": "vol. 9,",
            "year": 2016
        },
        {
            "authors": [
                "Nicol\u00f2 Pagliana",
                "Alessandro Rudi",
                "Ernesto De Vito",
                "Lorenzo Rosasco"
            ],
            "title": "Interpolation and learning with scale dependent kernels",
            "venue": "arXiv preprint arXiv:2006.09984,",
            "year": 2020
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "Matthias Seeger"
            ],
            "title": "Gaussian processes for machine learning",
            "venue": "International journal of neural systems,",
            "year": 2004
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. nature,",
            "year": 2016
        },
        {
            "authors": [
                "David Silver",
                "Thomas Hubert",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Matthew Lai",
                "Arthur Guez",
                "Marc Lanctot",
                "Laurent Sifre",
                "Dharshan Kumaran",
                "Thore Graepel"
            ],
            "title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm",
            "venue": "arXiv preprint arXiv:1712.01815,",
            "year": 2017
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Introduction to reinforcement learning, volume 135",
            "venue": "MIT press Cambridge,",
            "year": 1998
        },
        {
            "authors": [
                "Minoru Urabe"
            ],
            "title": "Convergence of numerical iteration in solution of equations. Journal of Science of the Hiroshima University, Series A (Mathematics",
            "venue": "Physics, Chemistry),",
            "year": 1956
        },
        {
            "authors": [
                "Kyriakos G Vamvoudakis",
                "Frank L Lewis"
            ],
            "title": "Online actor\u2013critic algorithm to solve the continuoustime infinite horizon optimal control",
            "venue": "problem. Automatica,",
            "year": 2010
        },
        {
            "authors": [
                "Kyriakos G Vamvoudakis",
                "Draguna Vrabie",
                "Frank L Lewis"
            ],
            "title": "Online adaptive algorithm for optimal control with integral reinforcement learning",
            "venue": "International Journal of Robust and Nonlinear Control,",
            "year": 2014
        },
        {
            "authors": [
                "Draguna Vrabie",
                "Frank Lewis"
            ],
            "title": "Neural network approach to continuous-time direct adaptive optimal control for partially unknown nonlinear systems",
            "venue": "Neural Networks,",
            "year": 2009
        },
        {
            "authors": [
                "Draguna Vrabie",
                "O Pastravanu",
                "Murad Abu-Khalaf",
                "Frank L Lewis"
            ],
            "title": "Adaptive optimal control for continuous-time linear systems based on policy",
            "venue": "iteration. Automatica,",
            "year": 2009
        },
        {
            "authors": [
                "Brent A Wallace",
                "Jennie Si"
            ],
            "title": "Continuous-time reinforcement learning control: A review of theoretical results, insights on performance, and needs for new designs",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Haoran Wang",
                "Xun Yu Zhou"
            ],
            "title": "Continuous-time mean\u2013variance portfolio selection: A reinforcement learning framework",
            "venue": "Mathematical Finance,",
            "year": 2020
        },
        {
            "authors": [
                "Holger Wendland"
            ],
            "title": "Scattered data approximation, volume 17",
            "venue": "Cambridge university press,",
            "year": 2004
        },
        {
            "authors": [
                "Cagatay Yildiz",
                "Markus Heinonen",
                "Harri L\u00e4hdesm\u00e4ki"
            ],
            "title": "Continuous-time model-based reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yaofeng Desmond Zhong",
                "Biswadip Dey",
                "Amit Chakraborty"
            ],
            "title": "Symplectic ode-net: Learning hamiltonian dynamics with control",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Lillicrap"
            ],
            "title": "approach) and DDPG (a DTRL",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent advancements in reinforcement learning (RL) have prominently focused on discrete-time (DT) systems. Notable applications range from Atari games Schrittwieser et al. (2020) to the game of Go Silver et al. (2016; 2017) as well as large language models Bubeck et al. (2023). However, most physical and biological systems are inherently continuous in time and driven by differential equation dynamics. This inherent discrepancy underscores the need for the evolution of continuoustime RL (CTRL) algorithms Baird (1994); Lewis et al. (1998); Abu-Khalaf & Lewis (2005); Vrabie & Lewis (2009); Vrabie et al. (2009); Lewis & Vrabie (2009); Vamvoudakis & Lewis (2010); Modares et al. (2014); Lee et al. (2014); Modares & Lewis (2014); Vamvoudakis et al. (2014); Yildiz et al. (2021); Holt et al. (2023); Wallace & Si (2023).\nUnfortunately, adopting CTRL presents both conceptual and algorithmic challenges. First, Qfunctions are known to vanish in CT systems Baird (1994); Abu-Khalaf & Lewis (2005); Lewis & Vrabie (2009), which makes even simple RL algorithms such as Q-learning infeasible for CT systems. Second, the one-step transition model in DT system needs to be replaced with time derivatives, which leads to the CT Bellman equation (also called nonlinear Lyapunov equation or generalized Hamilton\u2013Jacobi Bellman equation) governed by a complex partial differential equation (PDE) rather than a simple algebraic equation (AE) as in DT systems Lewis & Vrabie (2009); Vrabie & Lewis (2009); Vamvoudakis & Lewis (2010):\nCT Bellman Equation (PDE): V\u0307 u(x(t)) = \u2212l(x(t), u(x(t)), (1a) DT Bellman Equation (AE): \u2206V u(x(k)) = \u2212l(x(k), u(x(k)). (1b)\nIn (1a) and (1b), the symbols x, u, l, V represent the state, control policy, utility function and value function, respectively. While traditional DTRL focuses on maximizing a \u201creward function\u201d, our\nwork, in line with most CTRL literature, employs a \u201cutility function\u201d to represent costs or penalties, aiming to minimize its associated value. Solving the Bellman equation refers to the policy evaluation (PEV), which is a vital step in policy iteration (PI) of RL Sutton et al. (1998); Lewis & Vrabie (2009). However, CT Bellman equation cannot be solved directly because the explicit form of l(x(t), u(x(t))) hinges on the explicit form of the state trajectory x(t), which is generally unknown.\nThe CT Bellman equation can be formulated as the interval reinforcement form Vrabie & Lewis (2009); Lewis & Vrabie (2009); Modares et al. (2014); Modares & Lewis (2014); Vamvoudakis et al. (2014), which computes the value function V through the integration of the utility function:\nV u(x(t)) = \u03be(l) + V u(x(t+\u2206T )), \u03be(l) = \u222b t+\u2206T t l(x(s), u(s)) ds. (2)\nHere, \u03be(l) represents the integral of the utility function. When the system\u2019s internal dynamics is unknown, \u03be(l) can only be computed by a quadrature rule solely utilizing state samples in discrete time, i.e.,\n\u03be(l) \u2248 \u03be\u0302(l) = N\u2211 i=1 wil(x(ti), u(x(ti))). (3)\nHere, the time instants for collecting state samples are t1 = t < t2 < \u00b7 \u00b7 \u00b7 < tN = t+\u2206T , with N being the sample size. The quadrature rule is characterized by a set of weights {wi}Ni=1, which can be chosen from classical methods like the trapezoidal rule or advanced probabilistic methods such as Bayesian quadrature (BQ) O\u2019Hagan (1991); Karvonen & Sa\u0308rkka\u0308 (2017); Cockayne et al. (2019); Briol et al. (2019); Hennig et al. (2022). For simplicity, we denote the computational error for the quadrature rule as Err(\u03be\u0302(l)) := |\u03be(l) \u2212 \u03be\u0302(l)|. In real-world applications, sensors of autonomous systems are the primary source of state samples. For example, state samples are gathered from various sensors when the CTRL algorithm trains a drone navigation controller. These sensors include the gyroscope for orientation, the accelerometer for motion detection, the optical flow sensor for positional awareness, the barometer for altitude measurement, and GPS for global positioning. If these sensors operate at a sampling frequency of 10 Hz and the time interval \u2206T in (2) is set to 1 second, we would obtain N = 11 state samples within that duration.\nThe impact of computational methods on the solution of the CT Bellman equation is emphasized by Yildiz et al. (2021). Echoing this finding, our study further reveals that the choice of computational method can also impact the performance of the learned controller. As illustrated in Figure 1, we investigate the performance of the Integral Reinforcement Learning (IntRL) algorithm Vrabie & Lewis (2009). We utilize both the trapezoidal rule and the BQ with a Mate\u0301rn kernel to compute the PEV step, applying different sample sizes N . Our results for a canonical control task (Example 1 of Vrabie & Lewis (2009)) indicate that larger sample sizes diminish accumulated costs. In addition, we observe notable differences in accumulated costs between these two quadrature methods. This trend highlights a crucial insight: the computational method itself can be a determining factor in the control performance. This phenomenon is not exclusive to the IntRL algorithm but applies to the CTRL algorithm when internal dynamics is known, as elaborated in Appendix A.\nThe impact of computational methods \u2014 specifically, the quadrature rule as described in (3) for unknown internal dynamics scenarios \u2014 on control performance is a compelling yet underexplored topic. This gap prompts crucial questions: How does the computation impact control performance? How can we quantitatively describe this impact? Despite the importance of these questions, existing literature has yet to address them. Our paper aims to bridge this gap by investigating the intricate relationship between computational methods and controller performance. We choose to focus our analysis on the IntRL algorithm Vrabie & Lewis (2009). This choice is motivated by IntRL\u2019s role as a foundational CTRL algorithm in unknown internal dynamics scenarios Modares et al. (2014); Lee et al. (2014); Modares & Lewis (2014); Vamvoudakis et al. (2014); Wallace & Si (2023). As\nwe will illustrate in Section 2, understanding the impact of computation on control in scenarios with unknown internal dynamics is more crucial and merits deeper examination.\nThe contributions of this paper can be summarized as follows: (1) We show that the PEV step in IntRL essentially computes the integral of the utility function. This computation relies solely on state samples obtained at discrete time intervals, introducing an inherent computational error. This error is bounded by the product of the integrand\u2019s norm in the reproducing kernel Hilbert space (RKHS), and the worst-case error. When employed as the quadrature rule, BQ minimizes the worstcase error, which coincides precisely with BQ\u2019s posterior covariance. Additionally, we analyze the computational error\u2019s convergence rate concerning the sample size, focusing on both Wiener and Mate\u0301rn kernels for evenly spaced samples. (2) We demonstrate that PI of IntRL can be interpreted as Newton\u2019s method applied to the Hamilton-Jacobi-Bellman (HJB) equation. Viewed from this perspective, the computational error in PEV acts as an extra error term in each Newton\u2019s method iteration, with its upper bound proportional to the computational error. (3) We present the local convergence rate of PI for IntRL, in terms of sample size, for both the trapezoidal rule and BQ using the Mate\u0301rn kernel. Specifically, the convergence rate for PI is O(N\u22122) when using the trapezoidal rule, and O(N\u2212b) when using BQ with a Mate\u0301rn kernel, where b is the smoothness parameter. Our paper is the first to explore the impact of computational errors on the controller learned by IntRL.\nThe remainder of this paper is organized as follows: Section II provides a detailed problem formulation. Our main results are presented in Section III, followed by simulations in Section IV. Section V draws conclusions and discussions on this new topic."
        },
        {
            "heading": "2 PROBLEM FORMULATION",
            "text": "We assume that the governing system is control affine and the system\u2019s internal dynamics f is not necessarily known throughout the paper:\nx\u0307 = f(x) + g(x)u, (4) where x \u2208 \u2126 \u2286 Rnx is the system state, u \u2208 Rnu is the control input, f : Rnx \u2192 Rnx and g : Rnx \u2192 Rnx\u00d7nu are locally Lispchitz mappings with f(0) = 0. The objective is to find a control policy u that minimizes the performance index:\nJ (x0, u) = \u222b \u221e 0 l (x(s), u(s)) ds, x(0) = x0, (5)\nwhere l(x, u) = q(x) + u\u22a4R(x)u is the utility function, with q(x) and R(x) being the positive definite function and positive definite matrix, respectively. Furthermore, the system (4) is assumed to be zero-state observable through q Beard et al. (1997). The optimal control policy u\u2217 and its associated optimal value function V \u2217 are defined as:\nV \u2217(x0) := min u\u2208A(\u2126) J(x0, u), u \u2217(x0) := argmin u\u2208A(\u2126) J(x0, u). (6)\nHere, u \u2208 A(\u2126) represents that u is an admissible policy on \u2126. For more details of the definition of an admissible policy, please refer to Appendix B. The optimal value function can be determined by the HJB equation, which is illustrated in the following theorem: Theorem 1 (HJB Equation Properties Vrabie & Lewis (2009)). A unique, positive definite, and continuous function V \u2217 serves as the solution to the HJB equation:\nHJB Equation: G(V \u2217) = 0, G(V ) := (\u2207xV )\u22a4 f \u2212 1 4 (\u2207xV )\u22a4 gR\u22121g\u22a4\u2207xV + q. (7) In this case, V \u2217 is the optimal value function defined in (6). Consequently, the optimal control policy can be represented as u\u2217(x) = \u2212 12R \u22121g(x)\u22a4\u2207xV \u2217.\nWhile solving the HJB equation poses a challenge as it lacks an analytical solution, IntRL tackles this issue by utilizing PI to iteratively solve the infinite horizon optimal control problem. This approach is taken without necessitating any knowledge of the system\u2019s internal dynamics f Vrabie & Lewis (2009). Consider u(0)(x(t)) \u2208 A(\u2126) be an admissible policy, PI of IntRL performs PEV and policy improvement (PIM) iteratively:\nPEV: V (i) (x(t)) = \u222b t+\u2206T t l(x(s), u(i)(x(s)) ds+ V (i) (x(t+\u2206T )) , V (i)(x) = 0, (8a)\nPIM: u(i+1)(x) = \u22121 2 R\u22121g(x)\u22a4\u2207xV (i). (8b)\nIn the PI process, the PEV step (8a) assesses the performance of the currently derived control policy, while the PIM step (8b) refines this policy based on the evaluated value function. According to Vrabie & Lewis (2009), these iterative steps converge to an optimal control policy and its corresponding optimal value function when computations, i.e., the integration of the utility function \u03be(l), are performed flawlessly. However, any computational error Err(\u03be\u0302(l)) of the quadrature rule in (3) can adversely affect the accuracy of PEV. This inaccuracy can subsequently influence the updated control policies during the PIM step. Such computational errors may accumulate over successive iterations of the PI process and finally impact the learned controller, leading to the phenomenon of \u201ccomputation impacts control\u201d in IntRL.\nThe impact of computation requires more consideration for systems with unknown internal dynamics. When the system\u2019s internal dynamics is known, adaptive numerical solvers such as the RungeKutta series, offer an accurate solution to the CT Bellman equation (refer to Appendix C). These solvers adjust their step sizes according to the local behavior of the equation they solve, promising high computational accuracy. However, this step-size adjustment mechanism becomes a limitation in unknown internal dynamics cases. For instance, the Runge-Kutta (4,5) method produces both fourth- and fifth-order approximations, and the difference between the two approximations refines the step size. In real-world autonomous systems, state samples are typically acquired from sensors at evenly spaced intervals, challenging alignment with adaptive solver steps. Hence, adaptive numerical solvers are not suitable for unknown internal dynamics. In such cases, approximation methods such as quadrature rules become essential, but might introduce significant computational errors in the PEV step. This fact underscores the importance of considering the computational impact on control performance when facing unknown internal dynamics.\nGiven the preceding discussion, it is evident that computational errors significantly influence control performance by affecting the convergence behavior of PI. With this understanding as the backdrop, this paper aims to examine the convergence rate of PI for IntRL in the presence of computational errors. Specifically, we seek to address two key questions: (1) How does the computational error Err(\u03be\u0302(l)) in the PEV step of PI impact the IntRL algorithm\u2019s convergence? (2) How to quantify the computational error Err(\u03be\u0302(l)) in the PEV step and what is the optimal quadrature rule?"
        },
        {
            "heading": "3 THEORETICAL ANALYSIS",
            "text": "3.1 CONVERGENCE ANALYSIS OF PI\nIn this subsection, we answer the first question, i.e., we examine the impact of computational error in the PEV step on the convergence behavior of IntRL. Initially, we utilize the Fre\u0301chet differential to demonstrate the correspondence between the standard PI process in IntRL (without computational error) and Newton\u2019s method within the Banach space to which the value function belongs.\nLemma 1 (PI as Newton\u2019s Method in Banach Space). Consider a Banach space V \u2282 {V (x)|V (x) : \u2126 \u2192 R, V (0) = 0}, equipped with the norm \u2225 \u00b7 \u2225\u2126. For G(V ) as defined in (7), its Ga\u0302teaux and Fre\u0301chet derivatives at V can be expressed as\nG\u2032(V )W = \u2202W \u2202x\u22a4 f \u2212 1 2 \u2202W \u2202x\u22a4 gR\u22121g\u22a4 \u2202V \u2202x . (9)\nConsider the value function V (i) at the ith iteration of PI, executing PIM (8b) and PEV (8a) iteratively corresponds to Newton\u2019s method applied to solve the HJB equation (7). This Newton\u2019s method iteration is formulated as:\nV (i+1) = V (i) \u2212 [ G\u2032(V (i)) ]\u22121 G(V (i)). (10)\nThe proof of the lemma is presented in Appendix D. In this lemma, we demonstrate that a single iteration consisting of both the PIM and PEV in IntRL is equivalent to one iteration of Newton\u2019s method for solving the HJB equation in a Banach space. This parallel allows us to view the computational error introduced in the PEV step of the ith iteration as an extra error term, denoted E(i), in Newton\u2019s method iteration. We define the value function with the incorporated error term E(i)\n(highlighted in blue) as V\u0302 (i). Its iterative update is given by: V\u0302 (i+1) = V\u0302 (i) \u2212 [ G\u2032(V\u0302 (i)) ]\u22121 G(V\u0302 (i)) + E(i), V\u0302 (0) = V (0). (11)\nAssume that the error term E(i) has a bounded norm, denoted as \u2225E(i)\u2225\u2126 \u2264 \u03f5\u0304, where the bound \u03f5\u0304 is proportional to the computational error introduced during the PEV step, which will be shown later in Section 3.3. The subsequent theorem describes the convergence behavior of Newton\u2019s method when an extra bounded error term is included. Theorem 2 (Convergence of Newton\u2019s Method with Bounded Error). Define B0 = {V \u2208 V : \u2225V \u2212 V (0)\u2225\u2126 \u2264 r0}, B = {V \u2208 V : \u2225V \u2212 V (0)\u2225\u2126 \u2264 r0 + d} with d \u2265 0, r0 = \u2225 [ G\u2032(V (0)) ]\u22121 G(V (0))\u2225\u2126, and L0 = supV \u2208B0 \u2225 [ G\u2032(V (0)) ]\u22121 G\u2032\u2032(V )\u2225\u2126. If we assume the following conditions: (i) G is twice Fre\u0301chet continuous differentiable on B; (ii) r0L0 \u2264 12 ; (iii) G\u2032(V ) is nonsingular for \u2200V \u2208 B; (iv) There exists a number \u03a6,M > 0 such that 1 \u03a6 \u2265 supV \u2208B \u2225[G \u2032(V )]\u22121\u2225\u2126, M \u2265 supV \u2208B \u2225G\u2032\u2032(V )\u2225\u2126 and \u03a6\u2212Mr0 > \u221a 2M\u03f5\u0304\u03a6; (v) d > 2\u03f5\u0304\u03a6\u03a6\u2212Mr0 . Then we have V\u0302 (i) \u2208 B and\n\u2225V\u0302 (i) \u2212 V \u2217\u2225\u2126 \u2264 2\u03f5\u0304\u03a6\n\u03a6\u2212Mr0 +\n2\u2212i(2r0L0) 2i\nL0 , (12)\nwhere V \u2217 is optimal value function.\nThe proof can be found in Appendix E. Theorem 2 clarifies that the convergence behavior of Newton\u2019s method, when augmented with an extra bounded error term, hinges on two key elements: the cumulative effect of the bounded error term over the iterations and the inherent convergence properties of the standard PI algorithm. It should be noted that the convergence property described in Theorem 2 is local and applies when the underlying operator G(V ) is twice continuously differentiable. This iteration commences with an estimate of the constant d to ascertain the value of B. If condition (iv) of the theorem is not met, we are likely encountering a pathological situation, which necessitates choosing a smaller d until the condition (iv) is satisfied or (iv) and (v) are found to be incompatible in which case our analysis breaks down. This theorem extends existing insights from earlier work on the error analysis of Newton\u2019s method Urabe (1956), which assumes a strict Lipschitz condition for the operator G. However, this Lipschitz condition can be difficult to verify for utility functions l that do not include a time discount factor. A comprehensive representation of the convergence analysis conducted in this subsection is provided in Figure 2."
        },
        {
            "heading": "3.2 COMPUTATIONAL ERROR QUANTIFICATION",
            "text": "After analyzing the convergence property, our subsequent focus is quantifying the computational error inherent to each PEV step of IntRL. As discussed in Section 1, when internal dynamics is unknown, the integral of the utility function \u03be(l) (2) in the PEV step must be approximated using state samples in discrete time. This approximation will inevitably introduce computational integration error, especially when dealing with sparse samples, such as those collected from sensors on\nreal-world autonomous systems. A tight upper bound of the computational error, given a specific quadrature rule \u03be\u0302(l), can be determined by the Cauchy-Schwarz inequality, provided that l belongs to the RKHS HK Kanagawa et al. (2016; 2020); Briol et al. (2019):\nErr ( \u03be\u0302(l) ) \u2264 \u2225l\u2225HK \u00b7 e(HK), e(HK) := sup\nl\u2208HK ,\u2225l\u2225HK\u22641 Err\n( \u03be\u0302(l) ) . (13)\nHere, HK represents the RKHS induced by the kernel function K with \u2225\u00b7\u2225HK being its corresponding norm, and e(HK) is the worst-case error. From (13), it is clear that if a quadrature rule satisfies e(HK) = O(N\u2212b) with b > 0, then the computational error satisfies Err ( \u03be\u0302(l) ) = O(N\u2212b). Next,\nwe will discuss which quadrature rule minimizes the worst-case error.\nMinimum of the worst case error \u2013 BQ: The optimal quadrature rule that minimizes the worst-case error e(HK) is the maximum a posteriori (MAP) estimate of BQ Briol et al. (2019). Specifically, the minimization can be achieved when the kernel function of BQ is set as the kernel function K that induces RKHS HK . BQ aims to approximate the integral \u03be(l) by specifying a Gaussian process prior to the integrand (in this case, the integrand is the utility function l) denoted as l \u223c GP(0,K) and conditioning this prior on the integrand evaluated at the discrete time {l(x(ti), u(x(ti)))}Ni=1 to obtain the Gaussian posterior \u03be(l) \u223c N ( \u00b5\u03be(l),\u03a3\u03be(l) ) O\u2019Hagan (1991); Karvonen & Sa\u0308rkka\u0308 (2017); Cockayne et al. (2019); Briol et al. (2019); Hennig et al. (2022):\n\u00b5\u03be(l) = \u222b t+\u2206T t K(s, T )\u22121 dsK\u22121TT l(T ),\n\u03a3\u03be(l) = \u222b t+\u2206T t \u222b t+\u2206T t [ K(s, s\u2032)\u2212K(s, T )K\u22121TTK(T, s \u2032) ] dsds\u2032.\nHere, T = {ti}Ni=1 represents the set of time instants, and l(T ) corresponds to the evaluated values of the utility function at these sample points, with (l(T ))i = l(x(ti), u(x(ti))) being the evaluation at the ith sample point. In addition, (K(\u00b7, T ))i := K(\u00b7, ti) and (KTT )ij := K(ti, tj). Note that the posterior mean \u00b5\u03be(l) takes the form of a quadrature rule, with weights wi being the elements of the vector \u222b t+\u2206T t\nK(s, T )\u22121 dsK\u22121TT . The BQ estimate of \u03be(l) can be obtained using the MAP estimate, i.e., the posterior mean \u03be\u0302BQ(l) = \u00b5\u03be(l). In this case, the worst-case error equals the square root of the posterior variance e(HK) = \u221a \u03a3\u03be(l) Kanagawa et al. (2016; 2020); Briol et al. (2019).\nBQ with Wiener Kernel (Trapezoidal Rule) When the kernel function of the BQ is chosen as the Wiener process (we call as Wiener kernel for simplicity), i.e., KWiener(s, s\u2032) := max {s, s\u2032} \u2212 s\u2032\u2032 with s\u2032\u2032 < t, the posterior mean and covariance of BQ are calculated as Hennig et al. (2022):\n\u00b5\u03be(l) = N\u22121\u2211 i=1 l(x(ti), u(x(ti))) + l(x(ti+1), u(x(ti+1))) 2 \u03b4i, \u03a3\u03be(l) = 1 12 N\u22121\u2211 i=1 \u03b43i .\nHere, \u03b4i := ti+1 \u2212 ti. The BQ estimate is equal to the integral approximated by the trapezoidal rule Sul\u2019din (1959); Hennig et al. (2022). For data points evenly spaced at the interval, i.e., \u03b4i = \u2206TN\u22121 , the\nposterior variance of BQ becomes \u03a3\u03be(l) = \u2206T 3 12(N\u22121)2 , implying a linear convergence rate O(N \u22121) of the computational error bound when l belongs to the RKHS induced by Wiener kernel. However, this might not represent a tight upper bound, since the convergence rate of the trapezoidal rule can achieve O(N\u22122) when the integrand function is twice differentiable Atkinson (1991).\nBQ with Mate\u0301rn kernel (In Sobolev Space) The Sobolev space W b2 of order b \u2208 N is defined by W b2 := {F \u2208 L2 : D\u03b1F \u2208 L2 exists \u2200|\u03b1| \u2264 b}, where \u03b1 := (\u03b11, \u03b12, ..., \u03b1d) with \u03b1k \u2265 0 is a multi-index and |\u03b1| = \u2211d k=1 \u03b1k Adams & Fournier (2003). Besides, D \u03b1F is the \u03b1th weak\nderivative of F and its norm is defined by \u2225F\u2225W b2 = (\u2211 |\u03b1|\u2264b \u2225D\u03b1F\u22252L2 ) 1 2 . The Sobolev space W b2 is the RKHS with the reproducing kernel K being the Mate\u0301rn kernel with smoothness parameter b Seeger (2004); Mate\u0301rn (2013); Kanagawa et al. (2020); Pagliana et al. (2020). More details about the Mate\u0301rn kernel can be found in Appendix F. In addition, the computational error for the optimal quadrature rule, i.e., BQ with Mate\u0301rn kernel, achieves O(N\u2212b) if l \u2208 W b2 Novak (2006); Kanagawa et al. (2020). The illustration of BQ can be found in Appendix G and the technical flowchart of computational error quantification can be summarized in Figure 3."
        },
        {
            "heading": "3.3 CONVERGENCE RATE OF INTRL FOR DIFFERENT QUADRATURE RULES",
            "text": "After illustrating how to quantify the computational error, we will delve deeper into understanding its implications on the solution of PEV (8a). And, more importantly, we aim to uncover its impact on the convergence of PI. Consider the case where the value function is approximated by the linear combination of basis functions Vrabie & Lewis (2009); Vamvoudakis et al. (2014):\nV (i)(x) = \u03c9(i) \u22a4 \u03d5(x), \u03d5(x) := [\u03d51(x), \u03d52(x), ..., \u03d5n\u03d5(x)] \u22a4 . (14)\nHere, \u03d5(x) represent the basis functions with \u03d5k(0) = 0,\u2200k = 1, 2, ..., n\u03d5, and \u03c9(i) \u2208 Rn\u03d5 denotes the parameters determined at the ith iteration. A typical assumption of value function and the basis function is expressed as Vrabie & Lewis (2009); Vamvoudakis et al. (2014): Assumption 1 (Continuity, Differentiability and Linear Independence). The value function obtained at each PEV iteration (8a), is continuous and differentiable across \u2126, denoted by V (i) \u2208 C1(\u2126). Besides, \u03d5k(x) \u2208 C1(\u2126), \u2200k = 1, 2, ..., n\u03d5, and the set {\u03d5k(x)} n\u03d5 1 are linearly independent.\nSuch an assumption is commonly encountered in optimal control Lewis et al. (1998); Abu-Khalaf & Lewis (2005); Vrabie & Lewis (2009); Vrabie et al. (2009), suggesting that the value function can be uniformly approximated by \u03d5 when its dimension n\u03d5 approaches infinity. In our analysis, we focus primarily on the impact of computational error. To isolate this effect, we make the assumption that the learning errors introduced by the approximation in (14) can be neglected. This assumption is based on the premise that such errors can be effectively minimized or rendered negligible through adequate training duration and optimal hyperparameter tuning, among other techniques. With this approximation (14), the parameter \u03c9(i) is found via the linear matrix equation:\n\u0398(i)\u03c9(i) = \u039e(i), \u039e(i) :=  \u03be(T1, T2, u (i)) \u03be(T2, T3, u (i))\n... \u03be(Tm, Tm+1, u (i))\n , \u0398(i) :=  \u03d5 \u22a4(x(T2))\u2212 \u03d5\u22a4(x(T1)) \u03d5\u22a4(x(T3))\u2212 \u03d5\u22a4(x(T2))\n... \u03d5\u22a4(x(Tm+1))\u2212 \u03d5\u22a4(x(Tm))  . (15)\nHere, \u0398(i) has full column rank given approximate selections of T1, T2, ..., Tm+1. This condition can be seen as the persistence condition Wallace & Si (2023), ensuring that equation (15) yields a unique solution. For more discussions, see Appendix H. Besides, \u03be(Tk, Tk+1, u(i)) :=\u222b Tk+1 Tk\nl(x(s), u(i)(x(s))) ds represents the integral over the time interval from Tk to Tk+1 of the utility function l. As previously discussed, achieving the exact value of this integral is not feasible. It is instead approximated using a quadrature rule, with its computational error bounded by (13). We symbolize the computational error\u2019s upper bound for \u03be(Tk, Tk+1, u(i)) as \u03b4\u03be(Tk, Tk+1, u(i)). Therefore, the integral value for \u03be(Tk, Tk+1, u(i)) lies within the interval [\u03be\u0302(Tk, Tk+1, u(i)) \u2212 \u03b4\u03be(Tk, Tk+1, u (i)), \u03be\u0302(Tk, Tk+1, u (i)) + \u03b4\u03be(Tk, Tk+1, u\n(i))]. Here, \u03be\u0302(Tk, Tk+1, u(i)) signifies the integral\u2019s estimate using the quadrature rule. For simplicity, this can be formulated in matrix form as: \u039e(i) \u2208 [\u039e\u0302(i) \u2212 \u03b4\u039e(i), \u039e\u0302(i) + \u03b4\u039e(i)]. Consequently, due to the computational error, the actual linear\nmatrix equation we solve in the PEV step becomes (16) instead of (15):\n\u0398\u0302(i)\u03c9\u0302(i) = \u039e\u0302(i). (16)\nHere, \u0398\u0302(i) is the data matrix constructed by the states generated by the control policy u\u0302(i) := \u2212 12R\n\u22121g(x)\u22a4\u2207xV\u0302 (i\u22121). Incorporating the convergence analysis of PI in Section 3.1 with the computational error quantification for each PI step in Section 3.2, we can present our main results. Theorem 3 (Convergence Rate of IntRL Concerning Computational Error). Assume that \u2126 is a bounded set, and the conditions outlined in Theorem 2 hold. Under Assumption 1, we have:\n|V\u0302 (i)(x)\u2212 V \u2217(x)| \u2264 2\u03a6\u03f5\u0304 \u03a6\u2212Mr0\n+ 2\u2212i(2r0L0) 2i\nL0 , \u2200x \u2208 \u2126. (17)\nHere, \u03f5\u0304 = \u2225\u03d5\u2225\u221e \u00b7 supi { \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4\u22252\u2225\u03b4\u039e(i)\u22252 } is the upper bound of the extra error\nterm E(i) in (11), satisfying E(i) \u2264 \u03f5\u0304, where \u2225\u03d5\u2225\u221e := max1\u2264k\u2264n\u03d5 supx\u2208\u2126 |\u03d5k(x)|. Besides, the definitions of \u03a6,M, r0, L0 are consistent with Theorem 2.\nThe proof is given in appendix I. In (17), \u03b4\u039e(i) is a vector with its kth row defined as \u03b4\u03be(Tk, Tk+1, u\u0302\n(i)), serving as an upper bound for the computational error of the integral \u03be(Tk, Tk+1, u\u0302 (i)) = \u222b Tk+1 Tk\nl(x(s), u\u0302(i)(x(s)))ds. By invoking Theorem 3, we can relate the computational error \u03b4\u03be(Tk, Tk+1, u\u0302(i)) to the upper bound of the extra error term \u03f5\u0304 in the modified Newton\u2019s method (11). This relationship establishes the convergence rate of IntRL. Combining with prior discussions of the computational error for the trapezoidal rule and the BQ with Mate\u0301rn kernel, we can end our analysis with the subsequent corollary: Corollary 1 (Convergence Rate of IntRL for Trapezoidal Rule and BQ with Matern Kernel). Assuming that the sample points are uniformly distributed within each time interval [Tk, Tk+1] for k = 1, 2, ...,m, and that the IntRL algorithm has run for sufficient iterations (i.e., i \u2192 \u221e). Under the conditions in Theorem 3, if l belongs to the RKHS induced by the Wiener kernel and the integrals in the PEV step are calculated by the trapezoidal rule, we have |V\u0302 (\u221e)(x) \u2212 V \u2217(x)| = O(N\u22122). Here, V\u0302 (\u221e)(x) is defined as V\u0302 (\u221e)(x) := limi\u2192\u221e V\u0302 (i)(x). If l belongs to the RKHS W b2 induced by the Mate\u0301rn kernel and the integrals in the PEV step are calculated by the BQ with Mate\u0301rn kernel (having smoothness parameter b), we have |V\u0302 (\u221e)(x)\u2212 V \u2217(x)| = O(N\u2212b)."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "In this section 1, we validate the convergence rate of the IntRL for the trapzoidal rule and BQ with Mate\u0301rn kernel as demonstrated in Corollary 1.\nExample 1 (Linear System): First, we will consider the canonical linear-quadratic regulator problem defined by system dynamics x\u0307 = Ax+ Bu and the utility function l(x, u) = x\u22a4Qx+ u\u22a4Ru. The associated PI can be obtained by setting \u03d5(x) = x \u2297 x. The parameter of value function \u03c9(i) equals vec(Pi), where Pi is the solution to the Lyapunov equation Vrabie et al. (2009). In this case, the optimal parameter of the value function \u03c9\u2217 = vec(P \u2217) can be determined by solving the Ricatti equation Vrabie et al. (2009). Consider the third-order system Jiang & Jiang (2017):\nA = [ 0 1 0 0 0 1\n\u22120.1 \u22120.5 \u22120.7\n] , B = [ 0 0 1 ] , Q = I3\u00d73, R = 1.\nThe solution of the Ricatti equation is P \u2217 = [ 2.36 2.24 0.90 2.24 4.24 1.89 0.90 1.89 1.60 ] . The initial policy of the PI is\nchosen as an admissible policy u = \u2212K0x with K0 = [0, 0, 0]. We use the trapezoidal rule and the BQ with Mate\u0301rn kernel (smoothness parameter b = 4) for evenly spaced samples with size 5 \u2264 N \u2264 15 to compute the PEV step of IntRL. The l2-norm of the difference of the parameters \u2225\u03c9\u0302(\u221e) \u2212 \u03c9\u2217\u22252 concerning the sample size can be shown in Figure 4, where \u03c9\u0302(\u221e) := limi\u2192\u221e \u03c9\u0302(i) is the learned parameter after sufficient iterations and \u03c9\u2217 is the optimal parameter.\n1The code is available at https://github.com/anonymity678/Computation-Impacts-Control.git.\nWe demonstrate that the convergence rate of the parameter \u03c9\u0302(\u221e) aligns with the derived upper bound for the value function as outlined in Corollary 1. This observation is sufficient to validate Corollary 1 since the convergence rate of \u03c9\u0302(\u221e) inherently serves as an upper bound for the convergence rate of the value function V\u0302 (\u221e). Specifically, we have:\n|V (\u221e)(x)\u2212 V \u2217(x)| = \u2225(\u03c9\u0302(\u221e) \u2212 \u03c9\u2217)\u03d5(x)\u22252 \u2264 \u2225\u03c9\u0302(\u221e) \u2212 \u03c9\u2217\u22252\u2225\u03d5(x)\u22252, \u2200x \u2208 \u2126.\nExample 2 (Nonlinear System): Then we consider a canonical nonlinear system in the original paper of IntRL (Example 1 in Vrabie & Lewis (2009)):\nx\u03071 = \u2212x1 + x2, x\u03072 = \u22120.5(x1 + x2) + 0.5x2 sin2(x1) + sin(x1)u, with the utility function defined as l(x, u) = x21 + x 2 2 + u\n2. The optimal value function for this system is V \u2217(x) = 0.5x21 + x 2 2 and the optimal controller is u\n\u2217(x) = \u2212 sin(x1)x2. Due to the simple formulation of the value function, we set the basis function to be \u03d5(x) := x \u2297 x as in Vrabie & Lewis (2009). The initial policy is chosen as u(0) = \u22121.5x1 sin(x1)(x1 +x2) and we use the trapezoidal rule and the BQ with the Mate\u0301rn kernel (smoothness parameter b = 4) for evenly spaced samples with size 5 \u2264 N \u2264 15 to compute the PEV step. The convergence rate of the learned parameter for Example 2 can be shown in Figure 5 and more simulation results regarding the convergence rate of the controller and the average accumulated cost can be found in Appendix J."
        },
        {
            "heading": "5 CONCLUSION AND DISCUSSION",
            "text": "This paper primarily identifies the phenomenon termed \u201ccomputation impact control\u201d. Yet, it is possible to harness computational uncertainty, specifically the posterior variance in the BQ, to devise a \u201ccomputation-aware\u201d control policy. One approach is to view computational uncertainty with pessimism. In situations where computational uncertainty poses significant challenges, robust control methodologies can be employed to derive a conservative policy. Alternatively, a more optimistic perspective would embrace computational uncertainty as an opportunity for exploration.\nETHICS STATEMENT\nOur research strictly adheres to ethical guidelines and does not present any of the issues related to human subjects, data set releases, harmful insights, conflicts of interest, discrimination, privacy, legal compliance, or research integrity. All experiments and methodologies were conducted in a simulation environment, ensuring no ethical or fairness concerns arise.\nREPRODUCIBILITY STATEMENT\nThe source code associated with this paper is available for download through an anonymous GitHub repository at https://github.com/anonymity678/Computation-Impacts-Control.git. All simulation results have been uploaded to the repository, ensuring that readers and researchers can reproduce the findings presented in this paper with ease. Besides, all the theoretical proofs for the theorems and lemmas are provided in the appendix."
        },
        {
            "heading": "A MOTIVATING EXAMPLE FOR KNOWN INTERNAL DYNAMICS",
            "text": "The phenomenon \u201ccomputation impacts control performance\u201d also exists for the CTRL algorithm with known internal dynamics Yildiz et al. (2021). In Figure 6, we evaluate the impact of different numerical ordinary differential equation (ODE) solvers on the accumulated costs when applying the CTRL algorithm with known internal dynamics. Specifically, we consider the fixed-step Euler method with varying step sizes and the adaptive Runge-Kutta (4,5) method. Our results from the Cartpole task show that the adaptive ODE solver, in this case the Runge-Kutta (4,5) method, consistently outperforms the Euler method, yielding lower accumulated costs. Additionally, a smaller step size is associated with improved performance within the Euler method."
        },
        {
            "heading": "B DEFINITION OF ADMISSIBLE POLICIES",
            "text": "Definition 1 (Admissible Policies Vrabie & Lewis (2009)). A control policy u is admissible with respect to the cost, denoted as u \u2208 A(\u2126), if u is continuous on \u2126, u(0) = 0, u stabilizes the system (4) on \u2126, and J(x0, u) in (5) is finite for all x0 \u2208 \u2126."
        },
        {
            "heading": "C CT BELLMAN EQUATION FOR KNOWN INTERNAL DYNAMICS",
            "text": "When the internal dynamics is considered to be known, which can be derived from fundamental principles such as Lagrangian or Hamiltonian mechanics, or inferred using deep learning models Chen et al. (2018), Lutter et al. (2018), and Zhong et al. (2019). In this case, the CT Bellman equation can be addressed by formulating an augmented ODE:\n[ x\u0307(t)\nV\u0307 (x(t))\n] = [ f(x(t)) + g(x)u(x(t))\nl(x(t), u(x(t)))\n] .\nHere, both the state x and value function V act as independent variables of the augmented ODE. This can be solved by numerical ODE solvers ranging from fixed-step solvers like the Euler method to adaptive solvers such as the Runge-Kutta series Yildiz et al. (2021). The computational error for solving the CT bellman equation can be sufficiently small by choosing the adaptive ODE solvers."
        },
        {
            "heading": "D PROOF OF LEMMA 1",
            "text": "Proof. For \u2200V \u2208 V and W \u2208 Vn \u2282 V, where Vn denotes the neighborhood of V . By the definition of G(V ) in (7), we have\nG(V + sW )\u2212G(V )\n= \u2202(V + sW ) \u2202x\u22a4 f \u2212 1 4 \u2202(V + sW ) \u2202x\u22a4 gR\u22121g\u22a4 \u2202(V + sW ) \u2202x \u2212 \u2202V \u2202x\u22a4 f + 1 4 \u2202V \u2202x\u22a4 gR\u22121g\u22a4 \u2202V \u2202x\n= s \u2202W \u2202x\u22a4 f \u2212 1 4 s2 \u2202W \u2202x\u22a4 gR\u22121g\u22a4 \u2202W \u2202x \u2212 1 2 s \u2202W \u2202x\u22a4 gR\u22121g\u22a4 \u2202V \u2202x .\nThus, the Ga\u0302teaux differential Kantorovich & Akilov (2016) at V is\nL(W ) = G\u2032(V )W\n= lim s\u21920 G(V + sW )\u2212G(V ) s\n= \u2202W \u2202x\u22a4 f \u2212 1 2 \u2202W \u2202x\u22a4 gR\u22121g\u22a4 \u2202V \u2202x .\nThen we will prove that L(W ) is continuous on Vn. For \u2200W0 \u2208 Vn, we have L(W )\u2212 L(W0) = \u2225G\u2032(V )W \u2212G\u2032(V )W0\u2225\u2126\n= \u2225\u2225\u2225\u2225\u2202(W \u2212W0)\u2202x\u22a4 f \u2212 12 \u2202(W \u2212W0)\u2202x\u22a4 gR\u22121g\u22a4 \u2202V\u2202x \u2225\u2225\u2225\u2225 \u2126\n\u2264 ( \u2225f\u2225\u2126 + \u2225\u2225\u2225\u222512gR\u22121g\u22a4 \u2202V\u2202x \u2225\u2225\u2225\u2225 \u2126 ) \u00b7 \u2225\u2225\u2225\u2225\u2202(W \u2212W0)\u2202x \u2225\u2225\u2225\u2225 \u2126\n\u2264 ( \u2225f\u2225\u2126 + \u2225\u2225\u2225\u222512gR\u22121g\u22a4 \u2202V\u2202x \u2225\u2225\u2225\u2225 \u2126 ) \u00b7m1 \u00b7 \u2225W \u2212W0\u2225\u2126,\nwhere m1 > 0. Then, for \u2200\u03f5 > 0, there exists \u03b4 = \u03f5m1(\u2225f\u2225\u2126+\u2225 12 gR\u22121g\u22a4 \u2202V\u2202x \u2225\u2126) satisfying \u2225L(W ) \u2212 L(W0)\u2225\u2126 < \u03f5 when \u2225W \u2212W0\u2225\u2126 < \u03b4. Due to the continuity, L is also the Fre\u0301chet derivative at V Kantorovich & Akilov (2016).\nThen, we prove PI defined in (8a)(8b) equals (10). From (10), we obtain\nG\u2032(V (i))V (i+1) = G\u2032(V (i))V (i) \u2212G(V (i)). (18) According to (9) and (8b), we have\nG\u2032(V (i))V (i+1) = \u2202V (i+1) \u2202x\u22a4 f \u2212 1 2 \u2202V (i+1) \u2202x\u22a4 gR\u22121g\u22a4 \u2202V (i) \u2202x (19a)\n= \u2202V (i+1)\n\u2202x\u22a4\n( f + gu(i+1) ) ,\nG\u2032(V (i))V (i) = \u2202V (i) \u2202x\u22a4 f \u2212 1 2 \u2202V (i) \u2202x\u22a4 gR\u22121g\u22a4 \u2202V (i) \u2202x (19b)\n= \u2202V (i)\n\u2202x\u22a4 f \u2212 2u(i+1)\n\u22a4 Ru(i+1),\nG(V (i)) = \u2202V (i) \u2202x\u22a4 f + q \u2212 1 4 \u2202V (i) \u2202x\u22a4 gR\u22121g\u22a4 \u2202V (i) \u2202x (19c)\n= \u2202V (i)\n\u2202x\u22a4 f + q \u2212 u(i+1)\n\u22a4 Ru(i+1).\nPluging (19) into (18), we obtain\n\u2202V (i+1)\n\u2202x\u22a4\n( f + gu(i+1) ) = \u2212q \u2212 u(i+1) \u22a4 Ru(i+1).\nWhich means V\u0307 (i+1) = \u2212q \u2212 u(i+1) \u22a4 Ru(i+1). (20)\nFinally, (8a) can be obtained by integrating (20)."
        },
        {
            "heading": "E PROOF OF THEOREM 2",
            "text": "First, we introduce a useful lemma for the convergence of the standard Newton\u2019s method (without computational error).\nLemma 2 (Convergence of the Standard Newton\u2019s Method Ostrowski (2016); Lancaster (1966)). Suppose these exists an V (0) \u2208 V for which [ G\u2032(V (0)) ]\u22121 exists and G is twice Fre\u0301chet differentiable on B0, where B0 := {V \u2208 V : \u2225V \u2212 V (0)\u2225\u2126 \u2264 r0}. Define r0 := \u2225V (1) \u2212 V (0)\u2225\u2126 = \u2225 [ G\u2032(V (0)) ]\u22121 G(V (0))\u2225\u2126 and L0 := supV \u2208B0 \u2225 [ G\u2032(V (0)) ]\u22121 G\u2032\u2032(V )\u2225\u2126, if we have r0L0 \u2264 12 , then the sequence V (i) \u2208 B0 generated by Newton\u2019s method converges to the unique solution of G(V ) = 0, i.e., V \u2217 in B0. Moreover, we have \u2225V (i) \u2212 V \u2217\u2225\u2126 \u2264 2 \u2212i(2r0L0) 2i\nL0 .\nThen we will analyze the convergence property of Newton\u2019s method for solving G(V ) = 0 with an extra error term E(i) \u2264 \u03f5\u0304 in (11). We first study the iteration error between the value function for standard Newton\u2019s method V (i) and the value function for Newton\u2019s method incorporated with an extra error term V\u0302 (i). Specifically, we have the subsequent proposition:\nProposition 1 (Iteration Error for Newton\u2019s Method). If V\u0302 (i) \u2208 B, G\u2032\u2032 exists and continuous in B, where B := {V \u2208 V : \u2225V \u2212 V (0)\u2225\u2126 \u2264 r0 + d} with d \u2265 0. If G\u2032(V\u0302 (i)) has an inverse then\nV (i+1) \u2212 V\u0302 (i+1) = [ G\u2032(V\u0302 (i)) ]\u22121 {\u222b V\u0302 (i) V (i) G\u2032\u2032(V (i) \u2212 V, \u00b7) dV + \u222b V\u0302 (i) V (i) G\u2032\u2032(hi, \u00b7) dV } \u2212 E(i),\n(21) where hi = \u2212 [ G\u2032(V (i)) ]\u22121 G(V (i)).\nProof. Subtract (10) with (11), we have\nV (i+1) \u2212 V\u0302 (i+1) = V (i) \u2212 V\u0302 (i) \u2212 [ G\u2032(V (i)) ]\u22121 G(V (i)) + [ G\u2032(V\u0302 (i)) ]\u22121 G(V\u0302 (i))\u2212 E(i)\n= [ G\u2032(V\u0302 (i)) ]\u22121 [ G(V\u0302 (i)) +G\u2032(V\u0302 (i))(V (i) \u2212 V\u0302 (i)) +G\u2032(V\u0302 (i))hi ] \u2212 E(i).\n(22) Besides, we notice that\nG(V\u0302 (i)) = G(V (i)) +G\u2032(V (i))(V\u0302 (i) \u2212 V (i)) + \u222b V\u0302 (i) V (i) G\u2032\u2032(V\u0302 (i) \u2212 V, \u00b7) dV,\nG\u2032(V\u0302 (i)) = G\u2032(V (i)) + \u222b V\u0302 (i) V (i) G\u2032\u2032(V ) dV,\nG\u2032(V\u0302 (i))hi = \u2212G(V (i)) + \u222b V\u0302 (i) V (i) G\u2032\u2032(hi, \u00b7) dV.\n(23)\nBy substituting (23) into (22), we can obtain (21).\nThen, we will use Proposition 1 to achieve the bound of the iteration error \u2225V (i) \u2212 V\u0302 (i)\u2225\u2126, as summarized in the subsequent lemma.\nLemma 3 (Iteration Error Bound for Newton\u2019s Method). Under the Assumption in Lemma 2 and additionally assume that G\u2032\u2032 exists and is continuous in B and G\u2032(V ) is nonsingular for \u2200V \u2208 B. If there exists a number \u03a6,M > 0, such that 1\u03a6 \u2265 supV \u2208B \u2225[G\n\u2032(V )]\u22121\u2225\u2126, M \u2265 supV \u2208B \u2225G\u2032\u2032(V )\u2225\u2126 and \u03a6\u2212Mr0 > \u221a 2M\u03f5\u0304\u03a6 and d > 2\u03f5\u0304\u03a6\u03a6\u2212Mr0 . We have V\u0302 (i) \u2208 B and\n\u2225V (i) \u2212 V\u0302 (i)\u2225\u2126 \u2264 2\u03f5\u0304\u03a6\n\u03a6\u2212Mr0 . (24)\nProof. The Lemma is proved by induction. Recall that V (0) = V\u0302 (0), we obtain\n\u2225V (1) \u2212 V\u0302 (1)\u2225\u2126 = \u2225E(0)\u2225\u2126 \u2264 \u03f5\u0304\n\u2264 \u03f5\u0304\u03a6 \u03a6\u2212Mr0\n\u2264 2\u03f5\u0304\u03a6 \u03a6\u2212Mr0 .\nThus (24) holds for i = 1. Now suppose (24) is proved for i. Because d > 2\u03f5\u0304\u03a6\u03a6\u2212Mr0 and V (i) \u2208 B0 according to Lemma 2, we have V\u0302 (i) \u2208 B. Then by setting V = V (i)+ \u03c4(V\u0302 (i)\u2212V (i)) \u2208 B, we get\u2225\u2225\u2225\u2225\u2225 \u222b V\u0302 (i) V (i) G\u2032\u2032(V (i) \u2212 V, \u00b7) dV \u2225\u2225\u2225\u2225\u2225 \u2126 \u2264 \u2225V (i) \u2212 V\u0302 (i)\u22252\u2126 \u2225\u2225\u2225\u2225\u222b 1 0 \u03c4G\u2032\u2032(V (i) + \u03c4(V\u0302 (i) \u2212 V (i))) d\u03c4 \u2225\u2225\u2225\u2225 \u2126\n\u2264 1 2 M\u2225V (i) \u2212 V\u0302 (i)\u22252\u2126.\nDenoting \u03b4 = 2\u03f5\u0304\u03a6\u03a6\u2212Mr0 , by (21), we have\n\u2225V (i+1) \u2212 V\u0302 (i+1)\u2225\u2126\n= \u2225\u2225\u2225\u2225\u2225[G\u2032(V\u0302 (i))]\u22121 {\u222b V\u0302 (i) V (i) G\u2032\u2032(V (i) \u2212 V, \u00b7) dV + \u222b V\u0302 (i) V (i) G\u2032\u2032(hi, \u00b7) dV } \u2212 E(i) \u2225\u2225\u2225\u2225\u2225 \u2126\n\u2264 \u2225\u2225\u2225\u2225[G\u2032(V\u0302 (i))]\u22121\u2225\u2225\u2225\u2225\n\u2126\n{\u2225\u2225\u2225\u2225\u2225 \u222b V\u0302 (i) V (i) G\u2032\u2032(V (i) \u2212 V, \u00b7) dV \u2225\u2225\u2225\u2225\u2225 \u2126 + \u2225\u2225\u2225\u2225\u2225 \u222b V\u0302 (i) V (i) G\u2032\u2032(hi, \u00b7) dV \u2225\u2225\u2225\u2225\u2225 \u2126 + \u03f5\u0304 }\n\u2264 1 \u03a6\n{ 1\n2 M\u03b42 +M\u2225hi\u2225\u2126\u03b4 + \u03f5\u0304\n} .\nAccording to Lemma 2, \u2225hi\u2225\u2126 < r0, i = 1, 2, ..., so we have\u2225\u2225\u2225V (i+1) \u2212 V\u0302 (i+1)\u2225\u2225\u2225 \u2126 \u2264 M\u03b4 2 + 2M\u2225hi\u2225\u2126\u03b4 + 2\u03a6\u03f5\u0304 2\u03a6\n\u2264 \u03b4 2 M\u03b4 + 2Mr0 + (\u03a6\u2212Mr0) \u03a6\n= \u03b4\n2\nM\u03b4 +Mr0 +\u03a6\n\u03a6 .\nThe condition \u03a6\u2212Mr0 > \u221a 2M\u03f5\u0304\u03a6 implies that\nM\u03b4 +Mr0 < \u03a6.\nHence \u2225V (i+1) \u2212 V\u0302 (i+1)\u2225\u2126 \u2264 \u03b4 =\n2\u03f5\u0304\u03a6\n\u03a6\u2212Mr0 .\nThis completes the proof of Lemma 3.\nUsing a direct combination of Lemma 2 and Lemma 3, and perform triangle inequality in Banach space, we can obtain Theorem 2."
        },
        {
            "heading": "F DETAILS FOR MATE\u0301RN KERNEL",
            "text": "We focus on scale-dependent Mate\u0301rn kernels, which are radial basis kernels with Fourier decay characterized by a smoothness parameter b > 1/2 for samples belonging to R Wendland (2004); Pagliana et al. (2020):\nKMate\u0301rn(x, x \u2032) =\n( \u2225x\u2212 x\u2032\u2225\n\u03c1\n)b\u2212 12 \u03b2b\u2212 12 ( \u2225x\u2212 x\u2032\u2225\n\u03c1\n) ,\nwhere \u2225x \u2212 x\u2032\u2225 is the Euclidean distance between x and x\u2032, b is a parameter that controls the smoothness of the function, \u03c1 is the length scale parameter and \u03b2\u03b1 is the modified Bessel function of the second kind with parameter \u03b1 Seeger (2004). A specific and important property of the Mate\u0301rn kernel is that for b > 12 , the RKHS corresponding to different scales are all equivalent to the Sobolev space W b2 Seeger (2004); Mate\u0301rn (2013); Kanagawa et al. (2020).\nG ILLUSTRATION OF BQ\nHere, we will take the integral \u222b 10 2 [ t 10 sin ( 3\u03c0 5 t) + 2 ] dt (25)\nas an example to illustrate BQ using Wiener kernel and Mate\u0301rn kernel, see Figure 7 and 8 respectively. It can be seen from the figures that BQ can be understood as first fitting the integrand using Gaussian process regression, and then performing integration. The accuracy of BQ is related to the choice of the Gaussian process kernel and the sample size. For evenly spaced samples, as the sample size increases, the accuracy of BQ improves. Besides, we also plot the integral values and their computational errors for further illustration; see figure 9."
        },
        {
            "heading": "H DISCUSSION ABOUT FULL COLUMN RANK CONDITION",
            "text": "Lemma 4 (Full Column Rank of \u0398(i) Vrabie & Lewis (2009)). Suppose u(i)(x) \u2208 A(\u2126), and the set {\u03d5k} n\u03d5 1 is linearly independent. Then, \u2203T > 0 such that for \u2200x(t) generated by u(i)(x), {\u03d5k(x(t+ T ))\u2212 \u03d5k(x(t))} n\u03d5 1 is also linear independent.\nBased on this lemma, there exist values of T1, T2, ..., Tm+1 such that \u0398(i) is invertible and thus (15) has unique solution."
        },
        {
            "heading": "I PROOF OF THEOREM 3",
            "text": "First, we will give a proposition which presents the upper bound of the error between \u03c9(i) and \u03c9\u0302(i) if the same control policy u\u0302(i) is utilized.\nProposition 2 (Solution Error of PEV). For the same control policy u\u0302(i), the difference of the solution of the (15) and (16) can be bounded by:\n\u2225\u03c9(i) \u2212 \u03c9\u0302(i)\u22252 \u2264 \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4\u22252\u2225\u03b4\u039e(i)\u22252.\nProof. For the same control policy u(i), we have \u0398\u0302(i) = \u0398(i). Consider the linear matrix equation (15) and (16), we have\n\u2225\u03c9(i) \u2212 \u03c9\u0302(i)\u22252 = \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4(\u039e(i) \u2212 \u039e\u0302(i))\u22252 \u2264 \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4\u22252\u2225\u039e(i) \u2212 \u039e\u0302(i)\u22252 \u2264 \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4\u22252\u2225\u03b4\u039e(i)\u22252.\nIn this equation, \u03b4\u039e(i) is a vector with its kth row defined as \u03b4\u03be(Tk, Tk+1, u\u0302(i)). Note that \u03b4\u03be(Tk, Tk+1, u\u0302(i)) serves as an upper bound for the computational error of the integral \u03be(Tk, Tk+1, u\u0302 (i)) = \u222b Tk+1 Tk l(x(s), u\u0302(i)(x(s))) ds.\nThen we present the proof of Theorem 3:\nProof. We choose the norm in the Banach space V as the infinity norm, i.e., \u2225V \u2225\u2126 = \u2225V \u2225\u221e = supx\u2208\u2126 |V (x)|. Because \u2126 is bounded and \u03d5k \u2208 C1,\u2200k = 1, 2, ..., n\u03d5, thus \u2225\u03d5k\u2225\u221e = supx\u2208\u2126 |\u03d5k(x)| and \u2225\u03d5\u2225\u221e := max1\u2264k\u2264n\u03d5 {\u2225\u03d5k\u2225\u221e} exists. From Proposition 2 and CauchySchwarz inequality, we have\u2223\u2223\u2223E(i)(x)\u2223\u2223\u2223 = \u2225E(i)(x)\u22252\n\u2264 \u2225\u03c9\u0302(i) \u2212 \u03c9(i)\u22252\u2225\u03d5(x)\u22252 \u2264 \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4\u22252\u2225\u03b4\u039e(i)\u22252\u2225\u03d5(x)\u22252\n\u2264 sup i\n{ \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4\u22252\u2225\u03b4\u039e(i)\u22252 } \u2225\u03d5(x)\u22252, \u2200x \u2208 \u2126.\nThus, we have \u2225E(i)\u2225\u221e = sup\nx |E(i)(x)|\n\u2264 sup i\n{ \u2225(\u0398\u0302(i)\u22a4\u0398\u0302(i))\u22121\u0398\u0302(i)\u22a4\u22252\u2225\u03b4\u039e(i)\u22252 } \u2225\u03d5\u2225\u221e\n= \u03f5\u0304.\n(26)\nCombining (26) with (12) in Theorem 2, we can obtain\n\u2225V\u0302 (i) \u2212 V \u2217\u2225\u221e \u2264 2\u03a6\u03f5\u0304\n\u03a6\u2212Mr0 +\n2\u2212i(2r0L0) 2i\nL0 ,\nwhich directly leads to\n|V\u0302 (i)(x)\u2212 V \u2217(x)| \u2264 2\u03a6\u03f5\u0304 \u03a6\u2212Mr0\n+ 2\u2212i(2r0L0) 2i\nL0 , \u2200x \u2208 \u2126."
        },
        {
            "heading": "J ADDITIONAL SIMULATION RESULTS",
            "text": "In this subsection, we show additional simulations results for the convergence rate of the controller and the accumulated costs for Example 1 and Example 2 in Section 4.\nExample 1:\nControl Gain Matrix: For linear systems, the learned control policy adheres to u(\u221e)(x) = \u2212K\u0302(\u221e)x where K\u0302(\u221e) is the learned control gain matrix which is computed as Jiang & Jiang (2017):\nK\u0302(\u221e) = R\u22121B\u22a4P\u0302 (\u221e).\nHere, P\u0302 (\u221e) ensures \u03c9\u0302(\u221e) = vec(P\u0302 (\u221e)). The Frobenius norm difference between the learned and optimal control gain matrix K\u2217 = [0.90 1.89 1.60], denoted as \u2225K\u0302(\u221e) \u2212K\u2217\u2225F , is depicted in Figure 10.\nAverage Accumulated Cost: The average accumulated costs of the learned and optimal policies, represented as J and J\u2217 respectively, with initial state x0 \u223c N (0, 1002 \u00b7 I3\u00d73), are defined as:\nJ = Ex0\u223cN (0,1002\u00b7I3\u00d73) { V\u0302 (\u221e)(x0) } , J\u2217 = Ex0\u223cN (0,1002\u00b7I3\u00d73) {V \u2217(x0)} .\nTheir difference concerning the sample size N is shown in Figure 11.\nExample 2:\nControl Policy: For nonlinear systems, the control gain matrix does not exist. Instead, the learned control policy is given by\nu\u0302(\u221e)(x) = \u22121 2 R\u22121g(x)\u22a4\u2207xV\u0302 (\u221e).\nTherefore, we present the average control difference across state x \u223c N (0, 1002 \u00b7 I2\u00d72): Ex\u223cN (0,1002\u00b7I2\u00d72) { |u\u0302(\u221e)(x)\u2212 u\u2217(x)| } ,\nwhere u\u2217(x) represents the optimal controller. The average difference between the learned and optimal control policies is presented in Figure 12.\nAverage Accumulated Cost: The average accumulated costs of the learned and optimal policies, represented as J and J\u2217 respectively, with initial state x0 \u223c N (0, 1002 \u00b7 I2\u00d72), are defined as:\nJ = Ex0\u223cN (0,1002\u00b7I2\u00d72) { V\u0302 (\u221e)(x0) } , J\u2217 = Ex0\u223cN (0,1002\u00b7I2\u00d72) {V \u2217(x0)} .\nTheir difference concerning the sample size N is shown in Figure 13."
        },
        {
            "heading": "K DISCUSSION ABOUT THE LIMITATIONS OF INTRL",
            "text": "In this appendix, we want to discuss the limitations of IntRL algorithm. IntRL is a canonical control task in CTRL, just like the Q learning in DTRL. However, IntRL is still underdeveloped for highdimensional systems compared to Q learning. As shown in Vrabie & Lewis (2009); Modares et al. (2014); Wallace & Si (2023), IntRL is typically applied to systems with no more than 4 dimensions. This is because IntRL is hard to converge in high-dimensional systems. The difficulties in achieving convergence in high-dimensional systems can be attributed to several factors:\n\u2022 Limited Approximation Capability: IntRL uses a linear combination of basis functions to approximate the value function, which is less powerful than the neural network-based approaches prevalent in DTRL. Besides, in Wallace & Si (2023), the author observes that the condition number of the pseudo-inverted matrices for IntRL will degrade significantly due to an additional basis function. This conditioning issue limits IntRL to leverage complex basis functions to obtain larger approximation capability. It is emphasised in Wallace & Si (2023) that severe numerical breakdowns to even small increments in problem dimension.\n\u2022 Lack of Effective Exploration Mechanisms: IntRL struggles with poor data distribution due to inadequate exploration strategies. Common DTRL methods improve exploration by adding noise during data collection, but in continuous systems, this leads to complex stochastic differential equations (SDEs) that are challenging to manage. In Wallace & Si (2023), the author emphasises that IRL\u2019s lack of exploration noise causes data quality degradation especially when the state is regulated to the origin. Besides, it is observed that a lack of exploration noise will result in the phenomenon of \u201dhyperparameter deadlock\u201d.\n\u2022 Absence of Advanced Training Techniques: Techniques like replay buffers, parallel exploration, delayed policy updates, etc., which enhance sample efficiency and training in DTRL, are lacking in IntRL.\n\u2022 No Discount Factor: The absence of a discount factor in IntRL makes it difficult to ensure that policy iteration acts as a contraction mapping, which can affect the stability of algorithm\u2019s training process.\n\u2022 Non-existence of Q function: In CT systems, the concept of a Q function is not directly applicable. The absence of Q function impedes the direct translation of many DTRL methodologies and insights to IntRL.\nConsidering these challenges, CTRL algorithms should be investigated and developed for complex and high-dimensional scenarios in future research."
        },
        {
            "heading": "L FURTHER DISCUSSION ABOUT THE MOTIVATION OF CTRL",
            "text": "Many physical and biological systems are inherently continuous in time, governed by ODEs. Compared to discretising time and then applying DTRL algorithms, directly employing CTRL offers these advantages Jiang & Jiang (2017); Wallace & Si (2023):\n\u2022 Smoother Control Law: Direct application of CTRL typically results in smoother control outputs. This contrasts with the outcomes of coarse discretization, where control is less smooth and can lead to suboptimal performance Doya (2000).\n\u2022 Time Partitioning: When addressing CTRL directly, there\u2019s no need to predefine time partitioning. Instead, it is efficiently managed by numerical integration algorithms, which find the appropriate granularity Yildiz et al. (2021). Thus, CTRL is often a better choice when the time interval is uneven.\n\u2022 Enhanced Precision with CT Transition Models: Employing CT transition models in modelling CT systems offers a higher degree of precision compared to DT transition models. As evidenced in Wallace & Si (2023), comparisons between CT and DT trajectories, such as in the CartPole and Acrobat tasks, reveal that CT models align more closely with true solutions, especially in scenarios involving irregularly sampled data.\nWhile direct comparisons of performance between DTRL and CTRL in literature are rare, CTRL\u2019s prominence in the financial sector, especially in portfolio management, is notable. Many portfolio management models inherently rely on Stochastic Differential Equations (SDEs). A notable example includes the comparison in Wang & Zhou (2020) between the EMV algorithm (a CTRL-based approach) and DDPG (a DTRL approach) Lillicrap et al. (2015). This comparison showed the EMV algorithm\u2019s superior performance, highlighting CTRL\u2019s advantages in scenarios that utilize SDEbased models.\nThe distinct benefits of CTRL across various domains, from its enhanced precision in modeling to its adaptability in handling irregular time intervals, establish it as a vital and influential methodology. Its applicability in diverse sectors, notably in complex and dynamic fields like engineering and finance, make CTRL an increasingly relevant and powerful tool."
        }
    ],
    "year": 2024
}