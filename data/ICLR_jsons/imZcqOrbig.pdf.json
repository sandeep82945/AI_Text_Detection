{
    "abstractText": "A promising direction for pre-training 3D point clouds is to leverage the massive amount of data in 2D, whereas the domain gap between 2D and 3D creates a fundamental challenge. This paper proposes a novel approach to point-cloud pre-training that learns 3D representations by leveraging pre-trained 2D networks. Different from the popular practice of predicting 2D features first and then obtaining 3D features through dimensionality lifting, our approach directly uses a 3D network for feature extraction. We train the 3D feature extraction network with the help of the novel 2D knowledge transfer loss, which enforces the 2D projections of the 3D feature to be consistent with the output of pre-trained 2D networks. To prevent the feature from discarding 3D signals, we introduce the multi-view consistency loss that additionally encourages the projected 2D feature representations to capture pixel-wise correspondences across different views. Such correspondences induce 3D geometry and effectively retain 3D features in the projected 2D features. Experimental results demonstrate that our pre-trained model can be successfully transferred to various downstream tasks, including 3D shape classification, part segmentation, 3D object detection, and semantic segmentation, achieving state-of-the-art performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "YOU NEED"
        },
        {
            "affiliations": [],
            "name": "POINT-CLOUD PRE-TRAINING"
        },
        {
            "affiliations": [],
            "name": "Siming Yan"
        },
        {
            "affiliations": [],
            "name": "Chen Song"
        },
        {
            "affiliations": [],
            "name": "Youkang Kong"
        },
        {
            "affiliations": [],
            "name": "Qixing Huang"
        }
    ],
    "id": "SP:adcbe86e733f372abea251bb488b7382cbbcf268",
    "references": [
        {
            "authors": [
                "Mohamed Afham",
                "Isuru Dissanayake",
                "Dinithi Dissanayake",
                "Amaya Dharmasiri",
                "Kanchana Thilakarathna",
                "Ranga Rodrigo"
            ],
            "title": "Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "I. Armeni",
                "A. Sax",
                "A.R. Zamir",
                "S. Savarese"
            ],
            "title": "Joint 2D-3D-Semantic Data for Indoor Scene Understanding",
            "venue": "ArXiv e-prints,",
            "year": 2017
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Yujin Chen",
                "Matthias Nie\u00dfner",
                "Angela Dai"
            ],
            "title": "4dcontrast: Contrastive learning with dynamic correspondences for 3d scene understanding",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Sumit Chopra",
                "Raia Hadsell",
                "Yann LeCun"
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),",
            "year": 2005
        },
        {
            "authors": [
                "Christopher Choy",
                "JunYoung Gwak",
                "Silvio Savarese"
            ],
            "title": "4d spatio-temporal convnets: Minkowski convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Camille Couprie",
                "Cl\u00e9ment Farabet",
                "Laurent Najman",
                "Yann LeCun"
            ],
            "title": "Indoor semantic segmentation using depth information",
            "venue": "arXiv preprint arXiv:1301.3572,",
            "year": 2013
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas Funkhouser",
                "Matthias Nie\u00dfner"
            ],
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Morteza Daneshmand",
                "Ahmed Helmi",
                "Egils Avots",
                "Fatemeh Noroozi",
                "Fatih Alisinanoglu",
                "Hasan Sait Arslan",
                "Jelena Gorbova",
                "Rain Eric Haamer",
                "Cagri Ozcinar",
                "Gholamreza Anbarjafari"
            ],
            "title": "3d scanning: A comprehensive survey",
            "venue": "arXiv preprint arXiv:1801.08863,",
            "year": 2018
        },
        {
            "authors": [
                "Matt Deitke",
                "Dustin Schwenk",
                "Jordi Salvador",
                "Luca Weihs",
                "Oscar Michel",
                "Eli VanderBilt",
                "Ludwig Schmidt",
                "Kiana Ehsani",
                "Aniruddha Kembhavi",
                "Ali Farhadi"
            ],
            "title": "Objaverse: A universe of annotated 3d objects",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Carl Doersch",
                "Abhinav Gupta",
                "Alexei A Efros"
            ],
            "title": "Unsupervised visual representation learning by context prediction",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Runpei Dong",
                "Zekun Qi",
                "Linfeng Zhang",
                "Junbo Zhang",
                "Jianjian Sun",
                "Zheng Ge",
                "Li Yi",
                "Kaisheng Ma"
            ],
            "title": "Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning",
            "venue": "arXiv preprint arXiv:2212.08320,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Abdullah Hamdi",
                "Silvio Giancola",
                "Bernard Ghanem"
            ],
            "title": "MVTN: multi-view transformation network for 3d shape recognition",
            "venue": "IEEE/CVF International Conference on Computer Vision, ICCV 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Abdullah Hamdi",
                "Silvio Giancola",
                "Bernard Ghanem"
            ],
            "title": "Voint cloud: Multi-view point cloud representation for 3d understanding",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ji Hou",
                "Benjamin Graham",
                "Matthias Nie\u00dfner",
                "Saining Xie"
            ],
            "title": "Exploring data-efficient 3d scene understanding with contrastive scene contexts",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ji Hou",
                "Xiaoliang Dai",
                "Zijian He",
                "Angela Dai",
                "Matthias Nie\u00dfner"
            ],
            "title": "Mask3d: Pre-training 2d vision transformers by learning masked 3d priors",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Siyuan Huang",
                "Yichen Xie",
                "Song-Chun Zhu",
                "Yixin Zhu"
            ],
            "title": "Spatio-temporal self-supervised representation learning for 3d point clouds",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Jiang",
                "Eduard Trulls",
                "Jan Hosang",
                "Andrea Tagliasacchi",
                "Kwang Moo Yi"
            ],
            "title": "Cotr: Correspondence transformer for matching across images",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Evangelos Kalogerakis",
                "Melinos Averkiou",
                "Subhransu Maji",
                "Siddhartha Chaudhuri"
            ],
            "title": "3d shape segmentation with projective convolutional networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Asako Kanezaki",
                "Yasuyuki Matsushita",
                "Yoshifumi Nishida"
            ],
            "title": "Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Brian R Kent"
            ],
            "title": "3D scientific visualization with blender",
            "year": 2015
        },
        {
            "authors": [
                "Abhijit Kundu",
                "Xiaoqi Yin",
                "Alireza Fathi",
                "David A. Ross",
                "Brian Brewington",
                "Thomas A. Funkhouser",
                "Caroline Pantofaru"
            ],
            "title": "Virtual multi-view fusion for 3d semantic segmentation",
            "venue": "Computer Vision - ECCV 2020 - 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Haotian Liu",
                "Mu Cai",
                "Yong Jae Lee"
            ],
            "title": "Masked discrimination for self-supervised learning on point clouds",
            "year": 2022
        },
        {
            "authors": [
                "Yueh-Cheng Liu",
                "Yu-Kai Huang",
                "Hung-Yueh Chiang",
                "Hung-Ting Su",
                "Zhe-Yu Liu",
                "Chin-Tang Chen",
                "Ching-Yu Tseng",
                "Winston H Hsu"
            ],
            "title": "Learning from 2d: Contrastive pixel-to-point knowledge transfer for 3d pretraining",
            "venue": "arXiv preprint arXiv:2104.04687,",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Zheng Zhang",
                "Yue Cao",
                "Han Hu",
                "Xin Tong"
            ],
            "title": "Group-free 3d object detection via transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ishan Misra",
                "Rohit Girdhar",
                "Armand Joulin"
            ],
            "title": "An end-to-end transformer model for 3d object detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Maxime Oquab",
                "Timoth\u00e9e Darcet",
                "Th\u00e9o Moutakanni",
                "Huy Vo",
                "Marc Szafraniec",
                "Vasil Khalidov",
                "Pierre Fernandez",
                "Daniel Haziza",
                "Francisco Massa",
                "Alaaeldin El-Nouby"
            ],
            "title": "Dinov2: Learning robust visual features without supervision",
            "venue": "arXiv preprint arXiv:2304.07193,",
            "year": 2023
        },
        {
            "authors": [
                "Yatian Pang",
                "Wenxiao Wang",
                "Francis EH Tay",
                "Wei Liu",
                "Yonghong Tian",
                "Li Yuan"
            ],
            "title": "Masked autoencoders for point cloud self-supervised learning",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Charles R Qi",
                "Or Litany",
                "Kaiming He",
                "Leonidas J Guibas"
            ],
            "title": "Deep hough voting for 3d object detection in point clouds",
            "venue": "In proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Haozhe Qi",
                "Chen Feng",
                "Zhiguo Cao",
                "Feng Zhao",
                "Yang Xiao"
            ],
            "title": "P2b: Point-to-box network for 3d object tracking in point clouds",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Guocheng Qian",
                "Yuchen Li",
                "Houwen Peng",
                "Jinjie Mai",
                "Hasan Hammoud",
                "Mohamed Elhoseiny",
                "Bernard Ghanem"
            ],
            "title": "Pointnext: Revisiting pointnet++ with improved training and scaling strategies",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "arXiv preprint arXiv:1511.06434,",
            "year": 2015
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yongming Rao",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yongming Rao",
                "Benlin Liu",
                "Yi Wei",
                "Jiwen Lu",
                "Cho-Jui Hsieh",
                "Jie Zhou"
            ],
            "title": "Randomrooms: unsupervised pre-training from synthetic shapes and randomized layouts for 3d object detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Gernot Riegler",
                "Ali Osman Ulusoy",
                "Andreas Geiger"
            ],
            "title": "Octnet: Learning deep 3d representations at high resolutions",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Sauder",
                "Bjarne Sievers"
            ],
            "title": "Self-supervised deep learning on point clouds by reconstructing space",
            "venue": "arXiv preprint arXiv:1901.08396,",
            "year": 2019
        },
        {
            "authors": [
                "Jiayao Shan",
                "Sifan Zhou",
                "Zheng Fang",
                "Yubo Cui"
            ],
            "title": "Ptt: Point-track-transformer module for 3d single object tracking in point clouds",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2021
        },
        {
            "authors": [
                "Shuran Song",
                "Samuel P Lichtenberg",
                "Jianxiong Xiao"
            ],
            "title": "Sun rgb-d: A rgb-d scene understanding benchmark suite",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Hang Su",
                "Subhransu Maji",
                "Evangelos Kalogerakis",
                "Erik G. Learned-Miller"
            ],
            "title": "Multi-view convolutional neural networks for 3d shape recognition",
            "venue": "IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Liyao Tang",
                "Yibing Zhan",
                "Zhe Chen",
                "Baosheng Yu",
                "Dacheng Tao"
            ],
            "title": "Contrastive boundary learning for point cloud segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Mikaela Angelina Uy",
                "Quang-Hieu Pham",
                "Binh-Son Hua",
                "Duc Thanh Nguyen",
                "Sai-Kit Yeung"
            ],
            "title": "Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data",
            "year": 2019
        },
        {
            "authors": [
                "Haiyang Wang",
                "Shaocong Dong",
                "Shaoshuai Shi",
                "Aoxue Li",
                "Jianan Li",
                "Zhenguo Li",
                "Liwei Wang"
            ],
            "title": "Cagroup3d: Class-aware grouping for 3d object detection on point clouds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hanchen Wang",
                "Qi Liu",
                "Xiangyu Yue",
                "Joan Lasenby",
                "Matt J Kusner"
            ],
            "title": "Unsupervised point cloud pre-training via occlusion completion",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Peng-Shuai Wang",
                "Yang Liu",
                "Yu-Xiao Guo",
                "Chun-Yu Sun",
                "Xin Tong"
            ],
            "title": "O-CNN: octree-based convolutional neural networks for 3d shape analysis",
            "venue": "ACM Trans. Graph.,",
            "year": 2017
        },
        {
            "authors": [
                "Peng-Shuai Wang",
                "Yu-Qi Yang",
                "Qian-Fang Zou",
                "Zhirong Wu",
                "Yang Liu",
                "Xin Tong"
            ],
            "title": "Unsupervised 3d learning for shape analysis via multiresolution instance discrimination",
            "venue": "ACM Trans. Graphic,",
            "year": 2020
        },
        {
            "authors": [
                "Tengfei Wang",
                "Ting Zhang",
                "Bo Zhang",
                "Hao Ouyang",
                "Dong Chen",
                "Qifeng Chen",
                "Fang Wen"
            ],
            "title": "Pretraining is all you need for image-to-image translation",
            "venue": "arXiv preprint arXiv:2205.12952,",
            "year": 2022
        },
        {
            "authors": [
                "Xin Wei",
                "Ruixuan Yu",
                "Jian Sun"
            ],
            "title": "View-gcn: View-based graph convolutional network for 3d shape analysis",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Aoran Xiao",
                "Jiaxing Huang",
                "Dayan Guan",
                "Xiaoqin Zhang",
                "Shijian Lu",
                "Ling Shao"
            ],
            "title": "Unsupervised point cloud representation learning with deep neural networks: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Saining Xie",
                "Jiatao Gu",
                "Demi Guo",
                "Charles R Qi",
                "Leonidas Guibas",
                "Or Litany"
            ],
            "title": "Pointcontrast: Unsupervised pre-training for 3d point cloud understanding",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Chenfeng Xu",
                "Shijia Yang",
                "Tomer Galanti",
                "Bichen Wu",
                "Xiangyu Yue",
                "Bohan Zhai",
                "Wei Zhan",
                "Peter Vajda",
                "Kurt Keutzer",
                "Masayoshi Tomizuka"
            ],
            "title": "Image2point: 3d point-cloud understanding with 2d image pretrained models",
            "venue": "arXiv preprint arXiv:2106.04180,",
            "year": 2021
        },
        {
            "authors": [
                "Mingye Xu",
                "Mutian Xu",
                "Tong He",
                "Wanli Ouyang",
                "Yali Wang",
                "Xiaoguang Han",
                "Yu Qiao"
            ],
            "title": "Mm3dscene: 3d scene understanding by customizing masked modeling with informative-preserved reconstruction and self-distilled consistency",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Le Xue",
                "Mingfei Gao",
                "Chen Xing",
                "Roberto Mart\u0131\u0301n-Mart\u0131\u0301n",
                "Jiajun Wu",
                "Caiming Xiong",
                "Ran Xu",
                "Juan Carlos Niebles",
                "Silvio Savarese"
            ],
            "title": "Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Siming Yan",
                "Zhenpei Yang",
                "Chongyang Ma",
                "Haibin Huang",
                "Etienne Vouga",
                "Qixing Huang"
            ],
            "title": "Hpnet: Deep primitive segmentation using hybrid representations",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Siming Yan",
                "Yuqi Yang",
                "Yuxiao Guo",
                "Hao Pan",
                "Peng-shuai Wang",
                "Xin Tong",
                "Yang Liu",
                "Qixing Huang"
            ],
            "title": "3d feature prediction for masked-autoencoder-based point cloud pretraining",
            "venue": "arXiv preprint arXiv:2304.06911,",
            "year": 2023
        },
        {
            "authors": [
                "Siming Yan",
                "Zhenpei Yang",
                "Haoxiang Li",
                "Chen Song",
                "Li Guan",
                "Hao Kang",
                "Gang Hua",
                "Qixing Huang"
            ],
            "title": "Implicit autoencoder for point-cloud self-supervised representation learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14530\u201314542,",
            "year": 2023
        },
        {
            "authors": [
                "Yaoqing Yang",
                "Chen Feng",
                "Yiru Shen",
                "Dong Tian"
            ],
            "title": "Foldingnet: Point cloud auto-encoder via deep grid deformation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Li Yi",
                "Vladimir G. Kim",
                "Duygu Ceylan",
                "I-Chao Shen",
                "Mengyan Yan",
                "Hao Su",
                "Cewu Lu",
                "Qixing Huang",
                "Alla Sheffer",
                "Leonidas Guibas"
            ],
            "title": "A scalable active framework for region annotation in 3d shape collections",
            "venue": "ACM Trans. Graph.,",
            "year": 2016
        },
        {
            "authors": [
                "Xumin Yu",
                "Lulu Tang",
                "Yongming Rao",
                "Tiejun Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Point-bert: Pretraining 3d point cloud transformers with masked point modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Renrui Zhang",
                "Ziyu Guo",
                "Peng Gao",
                "Rongyao Fang",
                "Bin Zhao",
                "Dong Wang",
                "Yu Qiao",
                "Hongsheng Li"
            ],
            "title": "Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training",
            "venue": "arXiv preprint arXiv:2205.14401,",
            "year": 2022
        },
        {
            "authors": [
                "Renrui Zhang",
                "Liuhui Wang",
                "Yu Qiao",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "title": "Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders",
            "venue": "arXiv preprint arXiv:2212.06785,",
            "year": 2022
        },
        {
            "authors": [
                "Yabin Zhang",
                "Jiehong Lin",
                "Chenhang He",
                "Yongwei Chen",
                "Kui Jia",
                "Lei Zhang"
            ],
            "title": "Masked surfel prediction for self-supervised point cloud learning",
            "year": 2022
        },
        {
            "authors": [
                "Zaiwei Zhang",
                "Rohit Girdhar",
                "Armand Joulin",
                "Ishan Misra"
            ],
            "title": "Self-supervised pretraining of 3d features on any point-cloud",
            "venue": "arXiv preprint arXiv:2101.02691,",
            "year": 2021
        },
        {
            "authors": [
                "Chaoda Zheng",
                "Xu Yan",
                "Jiantao Gao",
                "Weibing Zhao",
                "Wei Zhang",
                "Zhen Li",
                "Shuguang Cui"
            ],
            "title": "Box-aware feature enhancement for single object tracking on point clouds",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Tinghui Zhou",
                "Philipp Krahenbuhl",
                "Mathieu Aubry",
                "Qixing Huang",
                "Alexei A. Efros"
            ],
            "title": "Learning dense correspondence via 3d-guided cycle consistency",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Chengxu Zhuang",
                "Siming Yan",
                "Aran Nayebi",
                "Daniel Yamins"
            ],
            "title": "Self-supervised neural network models of higher visual cortex development",
            "venue": "In 2019 Conference on Cognitive Computational Neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Chengxu Zhuang",
                "Siming Yan",
                "Aran Nayebi",
                "Martin Schrimpf",
                "Michael C Frank",
                "James J DiCarlo",
                "Daniel LK Yamins"
            ],
            "title": "Unsupervised neural network models of the ventral visual stream",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "A promising direction for pre-training 3D point clouds is to leverage the massive amount of data in 2D, whereas the domain gap between 2D and 3D creates a fundamental challenge. This paper proposes a novel approach to point-cloud pre-training that learns 3D representations by leveraging pre-trained 2D networks. Different from the popular practice of predicting 2D features first and then obtaining 3D features through dimensionality lifting, our approach directly uses a 3D network for feature extraction. We train the 3D feature extraction network with the help of the novel 2D knowledge transfer loss, which enforces the 2D projections of the 3D feature to be consistent with the output of pre-trained 2D networks. To prevent the feature from discarding 3D signals, we introduce the multi-view consistency loss that additionally encourages the projected 2D feature representations to capture pixel-wise correspondences across different views. Such correspondences induce 3D geometry and effectively retain 3D features in the projected 2D features. Experimental results demonstrate that our pre-trained model can be successfully transferred to various downstream tasks, including 3D shape classification, part segmentation, 3D object detection, and semantic segmentation, achieving state-of-the-art performance."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The rapid development of commercial data acquisition devices (Daneshmand et al., 2018) and pointbased deep learning networks (Qi et al., 2017a;b; Choy et al., 2019) has led to a growing research interest in models that can directly process 3D point clouds without voxelization. Remarkable success has been achieved in various applications, including but not limited to object detection (Misra et al., 2021; Liu et al., 2021b; Wang et al., 2022a), segmentation (Qian et al., 2022; Tang et al., 2022; Zhao et al., 2021; Yan et al., 2021), and tracking (Qi et al., 2020; Zheng et al., 2021; Shan et al., 2021).\nDespite the significant advances in 3D point cloud processing, acquiring task-specific 3D annotations is a highly expensive and severely limited process due to the geometric complexity. The shortage of data annotations highlights the need for adapting pre-training paradigms. Instead of training the deep network from randomly initialized weights, prior work suggests that pre-training the network on a relevant but different pre-task and later fine-tuning the weights using task-specific labels often leads to superior performance. In natural language processing (Devlin et al., 2018) and 2D vision (He et al., 2022; Radford et al., 2015; Doersch et al., 2015; He et al., 2020; Zhuang et al., 2021; 2019), pre-trained models are the backbones of many exciting applications, such as real-time chatbots (Touvron et al., 2023; OpenAI, 2023) and graphic designers (Meng et al., 2021; Wang et al., 2022b). However, pretraining on point clouds has yet to demonstrate a universal performance improvement. From-scratch training remains a common practice in 3D vision.\nInitial attempts towards 3D point-cloud pre-training primarily leverage contrastive learning (Chopra et al., 2005), especially when the point clouds are collected from indoor scenes (Xie et al., 2020; Rao et al., 2021; Liu et al., 2021a; Zhang et al., 2021; Chen et al., 2022). However, the broad application of contrastive learning-based pre-training techniques is impeded by the requirement of large batch sizes and the necessity to carefully define positive and negative pairs. In contrast to natural language processing and 2D vision, pre-training on 3D point clouds presents two unique challenges.\nFirst, the data is extremely scarce, even without annotations. Public 3D datasets are orders of magnitude smaller than 2D image datasets. Second, the lack of data annotations necessitates 3D pre-training methods to adhere to the self-supervised learning paradigm. Without strong supervision, pre-task design becomes particularly crucial in effective knowledge acquisition.\nFigure 2 illustrates our novel approach to 3D pre-training, which is designed to address the aforementioned challenges. Our key idea is to leverage pre-trained 2D networks in the image domain to enhance the performance of 3D representation learning while bridging the 2D-3D\ndomain gap. We begin by applying a 3D-based network to an input point cloud with M points, which generates a 3D feature volume represented by an M \u00d7 C-dimensional embedding, where C is the length of the per-point feature vectors. To compensate for data scarcity, we align the 3D feature volume with features predicted by a 2D encoder trained on hundreds of millions of images. To this end, we project the 3D feature volume into pixel embeddings and obtain H \u00d7W \u00d7 C image-like feature maps in 2D. We then use pre-trained image encoders to extract hierarchical features from corresponding RGB images and train a similarly structured encoder on projected 2D maps to enforce feature alignment. To ensure geometrical meaningfulness, we project the same 3D feature volume into multiple camera views and perform hierarchical feature alignment in each individual view. Our pipeline is generic enough to support all major 2D pre-trained models (e.g., CLIP (Radford et al., 2021), SAM (Kirillov et al., 2023), DINOv2 (Oquab et al., 2023)). In our experiments, DINOv2 exhibits the best performance.\nOne issue with the approach described above is that the 3D feature learning may overfit to the pre-trained 2D networks. Consequently, it could potentially discard 3D features that are critical for 3D recognition but not well-captured during image-based 2D pre-training. To address this issue, we introduce an auxiliary pre-task to predict 2D multi-view pixel-wise correspondences from pairs of projected 2D feature maps. As such correspondences induce 3D depth information, the learned feature representations are forced to capture 3D signals.\nOur approach is fundamentally different from existing work based on multi-view representations (Su et al., 2015; Kalogerakis et al., 2017; Kanezaki et al., 2018; Wei et al., 2020; Kundu et al., 2020; Hamdi et al., 2021; 2023), which either aggregate image-based analysis results or lift 2D feature maps into 3D for understanding. Our approach employs a 3D network for feature extraction trained with the additional help of pre-trained 2D models. While multi-view based 3D understanding approaches (Su et al., 2015; Kalogerakis et al., 2017) tremendously benefit from the rich experience in 2D vision and have long dominated the design of shape classification networks when 3D data is limited, recent 3D-based understanding networks (Riegler et al., 2017; Qi et al., 2017a;b; Wang et al., 2017) are shown to outperform their multi-view counterparts. The superior performance is the result of enhanced architecture design and the clear advantage of directly learning 3D patterns for geometry understanding. In our setting, the 3D feature extraction network provides implicit regularization on the type of features extracted from pre-trained 2D models. We seek to combine the advantage of multi-view representations and the geometric capacity of 3D networks, while leveraging the rich knowledge embedded in pre-trained 2D networks.\nIn summary, we make the following contributions:\n\u2022 We formulate point-cloud pre-training as learning a multi-view consistent 3D feature volume.\n\u2022 To compensate for data scarcity, we leverage pre-trained 2D image-based models to supervise 3D pre-training through perspective projection and hierarchical feature-based 2D knowledge transfer.\n\u2022 To prevent overfitting to pre-trained 2D networks, we develop an auxiliary pre-task where the goal is to predict the multi-view pixel-wise correspondences from the 2D pixel embeddings.\n\u2022 We conduct extensive experiments to demonstrate the effectiveness of our approach across a wide range of downstream tasks, including shape classification, part segmentation, 3D detection, and semantic segmentation, achieving consistent improvement over baselines (Figure 1)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Point-cloud pre-training The success of point-based deep neural networks demonstrates the potential for machine learning models to directly perceive point clouds. Point-based pre-training refers to the practice of pre-training the point-based prediction network on one or more pre-tasks before fine-tuning the weights on downstream tasks. The expectation is that the knowledge about pretasks will be transferred to downstream tasks, and the network can achieve better performance than random parameter initialization. We refer readers to (Xiao et al., 2023) for a comprehensive survey of the field, in which we highlight one important observation, stating that point-based pre-training \u201cstill lags far behind as compared with its counterparts\u201d, and training from-scratch \u201cis still the prevalent approach\u201d. The challenges and opportunities call for innovations in point-based pre-training methods.\nShape-level pre-training. Self-reconstruction is a popular pre-task at the shape level, where the network encodes the given point cloud as representation vectors capable of being decoded back to the original input data (Yang et al., 2018; Sauder & Sievers, 2019). To increase the pre-task difficulty, it is common for self-reconstruction-based methods to randomly remove a certain percentage of the points from the input (Wang et al., 2021; Pang et al., 2022; Zhang et al., 2022a; Yu et al., 2022; Yan et al., 2023b;a). In addition to self-reconstruction, (Rao et al., 2020) develop a multi-task approach that unifies contrastive learning, normal estimation, and self-reconstruction into the same pipeline. There are also attempts to connect 3D pre-training to 2D (Afham et al., 2022; Xu et al., 2021; Dong et al., 2022; Zhang et al., 2022b), or to explore the inverse relationship (Hou et al., 2023). For example, (Dong et al., 2022) utilize pre-trained 2D transformers as cross-modal teachers. (Zhang et al., 2022b) leverage self-supervised pre-training and masked autoencoding to obtain high-quality 3D features from 2D pre-trained models. However, these methods focus on synthetic objects, leading to significant performance degradation in downstream tasks using real data from indoor scenes.\nScene-level pre-training. Existing scene-level pre-training methods focus on exploiting the contrastive learning paradigm. As the first pre-training method with demonstrated success at the scene level, PointContrast (Xie et al., 2020) defines the contrastive loss using pairs of 3D points across multiple views. RandomRoom (Rao et al., 2021) exploits synthetic datasets and defines the contrastive loss using pairs of CAD objects instead. Experimentally, RandomRoom additionally confirms that synthetic shape-level pre-training is beneficial to real-world scene-level downstream tasks. DepthContrast (Zhang et al., 2021) simplifies PointContrast by proposing an effective pre-training method requiring only single-view depth scans. 4DContrast (Chen et al., 2022) further includes temporal dynamics in the contrastive formulation, where the pre-training data comprises sequences of synthetic scenes with objects in different locations. In contrast to these approaches, we focus on utilizing pre-trained 2D networks to boost the performance of 3D pre-training without leveraging contrastive learning."
        },
        {
            "heading": "3 METHOD",
            "text": "We present our pre-training approach pipeline, denoted as MVNet in the following sections. As illustrated in Figure 2, given a pair of RGB-D scans, we first project the 2D pixels into 3D point clouds using camera parameters. We then extract the point-cloud feature volumes using the feature encoding network (Section 3.1). Subsequently, we project the feature volume onto two different views to generate 2D feature maps (Section 3.2). We design the 2D knowledge transfer module to learn from large-scale 2D pre-trained models (Section 3.3). Finally, we utilize the multi-view consistency module to ensure the agreement of different view features (Section 3.4), promoting that the 3D feature encoding network extracts 3D-aware features from pre-trained 2D image models. Finally, the weights of the feature encoding network are transferred to downstream tasks for fine-tuning."
        },
        {
            "heading": "3.1 FEATURE ENCODING NETWORK",
            "text": "Let (I1, D1) and (I2, D2) be two RGB-D scans from the same scene, where I1 and I2 denote the RGB images, and D1 and D2 represent the depth maps. Using camera parameters, we project the RGB-D scans into a colored point cloud P \u2208 RM\u00d76 with M points, with the first three channels representing coordinates and the remaining three channels representing RGB values. The feature encoding network of MVNet takes P as input and outputs a dense feature field FP \u2208 RM\u00d7C , where each point is associated with a feature vector of dimension C.\nThe network architecture adopts the design of Sparse Residual U-Net (SR-UNet) (Choy et al., 2019). Our network includes 21 convolution layers for the encoder and 13 convolution layers for the decoder, where the encoder and the decoder are connected with extensive skip connections. Specifically, the input point cloud P is first voxelized into M \u2032 voxels, yielding a grid-based representation V \u2208 RM \u2032\u00d76. In our implementation, we use the standard Cartesian voxelization with the grid length set to 0.05 m. The output of the network is a set of C-dimensional per-voxel features, FV \u2208 RM\n\u2032\u00d7C , jointly generated by the encoder and the decoder. Due to space constraints, we refer interested readers to the supplementary material for implementation details.\nNext, we interpolate the per-voxel features FV to obtain a C-dimensional feature vector for each of M points in the input point cloud. This is achieved via the classical KNN algorithm based on point-to-point distances. As shown in the middle of Figure 2, the resulting FP is our point-cloud feature volume, awaiting further analysis and processing.\nImportantly, unlike existing work that utilizes multi-view representations from 2D feature extraction networks, MVNet exploits a 3D feature extraction network. As a result, MVNet has the intrinsic ability to capture valuable 3D patterns for 3D understanding."
        },
        {
            "heading": "3.2 POINT-CLOUD FEATURE VOLUME PROJECTION",
            "text": "The key idea of MVNet is to learn 3D feature representations through 2D projections, enabling us to leverage large-scale pre-trained models in 2D. To this end, we proceed to project the feature volume FP back onto the two input views, generating 2D feature maps, F1 and F2, both with dimensions H \u00d7W \u00d7 C. Here, H \u00d7W is the spatial resolution of the input. The projection operation uses the one-to-one mapping between each pixel in each input view and the corresponding 3D point in the concatenated 3D point cloud P . We refer readers to the supplementary material for the implementation details and the conversion formula between 2D and 3D coordinates."
        },
        {
            "heading": "3.3 2D KNOWLEDGE TRANSFER MODULE",
            "text": "The first feature learning module of MVNet, F2d, aims to transfer knowledge from largescale 2D pre-trained models. To this end, we consider each input RGB image Ii and the corresponding projected 2D feature map Fi. Our goal is to train Fi using a pretrained 2D model fp that takes Ii as input. Our implementation uses ViT-B (Dosovitskiy et al., 2020) as the network architecture. More details are provided in Section 4.3.\nThe technical challenge here lies in the semantic differences between Fi and Ii. As illustrated in Figure 3, we introduce an additional network fk that takes Fi as input. fk shares the same network architecture as fp except the first layer, which is expanded channel-wise to accommodate the Cdimensional input. Let N be the number of layers in fk and fp. We define the training loss as\nL2d = N\u2211 j=3 \u2225Bpre-trainedj \u2212B 2D j \u222522 (1)\nwhere Bpre-trainedj denotes the output feature map of the jth block in the pre-trained 2D model fp, and B2Dj denotes the output feature map of the j\nth block in fk. The objective of the 2D knowledge transfer process is to minimize L2d, thereby enabling fk to learn the hierarchical features through knowledge transfer from the pre-trained 2D model. We add the loss starting from the third layer, which experimentally leads to the best performance.\nDuring training, the weights of the 2D pre-trained model fp are unchanged, whereas fk and the feature encoding network are jointly optimized. In our implementation, we select ViT-B (Dosovitskiy et al., 2020) as the network backbone for both 2D neural network fk and fp. We take the pre-trained weights of DINOv2 (Oquab et al., 2023) for fp. DINOv2 has been demonstrated to generate features with strong transferability. We refer readers to Section 4.3 for a detailed analysis of these design choices through extensive ablation studies.\n3.4 MULTI-VIEW CONSISTENCY MODULE\nThe goal of the previous module is to leverage pre-trained 2D networks. However, pre-trained 2D networks only contain feature representations that are primarily suitable for 2D tasks. A complete reliance on the 2D knowledge transfer module encourages the 3D feature encoding network to discard geometric cues that are important for 3D recognition. To address this issue, we introduce a novel multi-view consistency module Fm, where the goal is to use the projected features, F1 and F2, to predict dense correspondences between the two corresponding input images. Prior research has demonstrated that dense pixelwise correspondences between calibrated images allows the faithful recovery of 3D geometry (Zhou et al., 2016). Therefore, enforcing ground-truth correspondences to be recoverable from F1 and F2 prevents the learned feature volume from discarding 3D features.\nOur dense correspondence module adopts a cross attention-based transformer decoder D. As depicted in Figure 4, we concatenate the two views\u2019 feature maps side by side, forming a feature map Fc. The context feature map Fc is fed into a cross attention-based transformer decoder D, along with the query point x from the first view I1. Finally, we process the output of the transformer decoder with a fully connected layer to obtain our estimate for the corresponding point, x\u2032, in the second view I2:\nx\u2032 = Fm(x|F1, F2) = D(x, F1 \u2295 F2) (2)\nFollowing (Jiang et al., 2021), we design the loss for correspondence error and cycle consistency:\nLm = ||xgt \u2212 x\u2032||22 + ||x\u2212Fm(x\u2032|F1, F2)||22 (3)\nwhere xgt denotes the ground truth corresponding point of x in the second view I2.\nCombing (1) and (3), the overall loss function is defined as:\nL = L2d + \u03bbLm (4)\nwhere \u03bb = 0.5 is the trade-off parameter."
        },
        {
            "heading": "4 EXPERIMENTAL ANALYSIS",
            "text": ""
        },
        {
            "heading": "4.1 PRE-TRAINING SETTING",
            "text": "Data preparation We choose ScanNet (Dai et al., 2017) as the pre-training dataset, which contains approximately 2.5M RGB-D scans from 1,513 indoor scenes. Following (Qi et al., 2019), we downsample 190K RGB-D scans from 1,200 video sequences in the training set. For each scan, we build the scan pair by taking it as the first scan (I1, D1) and select five other frames with an overlap ratio between [0.4, 0.8] as the second scan (I2, D2) candidates. We also evaluate our model at the shape level. We pre-trained our model on Objaverse (Deitke et al., 2023), which contains approximately 800K real-world 3D objects. We also pre-trained our model on ShapeNet (Chang et al., 2015) for fair comparison with previous approaches. For each 3D object, we use Blender (Kent, 2015) to render 6 images with their camera angles spaced equally by 60 degrees. At each training step, we randomly choose two consecutive images for pre-training.\nNetwork training Our pre-training model is implemented using PyTorch, employing the AdamW optimizer (Loshchilov & Hutter, 2017) with a weight decay of 10\u22124. The learning rate is set to 10\u22123. The model is trained for 200 epochs on eight 32GB Nvidia V100 GPUs, with 64 as the batch size."
        },
        {
            "heading": "4.2 DOWNSTREAM TASKS",
            "text": "The goal of pre-training is to learn features that can be transferred effectively to various downstream tasks. In the following experiments, we adopt the supervised fine-tuning strategy, a popular way to evaluate the transferability of pre-trained features. Specifically, we initialize the network with pre-trained weights as described in Section 3 and fine-tune the weights for each downstream task.\nSemantic segmentation We use the original training and validation splits of ScanNet (Dai et al., 2017) and report the mean Intersection over Union (IoU) on the validation split. We also evaluate on S3DIS (Armeni et al., 2017) that has six large areas, where one area is chosen as the validation set, and the remaining areas are utilized for training. We report the Overall Accuracy (OA), mean Accuracy (mAcc), and mean IoU for Area-5 and the 6-fold cross-validation results.\nWe adopt SR-UNet (Choy et al., 2019) as the semantic segmentation architecture and add a head layer at the end of the feature encoding network for semantic class prediction. Although it is intuitive to transfer both the encoder and decoder pre-trained weights in segmentation fine-tuning, we observe that only transferring the encoder pre-trained weights results in a better performance. We fine-tune our model using the SGD+momentum optimizer with a batch size of 48, 10,000 iterations, an initial learning rate of 0.01, and a polynomial-based learning rate scheduler with a power factor of 0.9. The same data augmentation techniques in (Chen et al., 2022) are used.\nAs demonstrated in Table 1, on both datasets, we achieve the best performance compared to other pre-training methods, with +1.5 val mIoU improvement on ScanNet compared to the second-best method. On the S3DIS dataset, we report +1.6 mIoU improvement on the Area 5 validation set.\n3D object detection ScanNet (Dai et al., 2017) contains instance labels for 18 object categories, with 1,201 scans for training and 312 for validation. SUN RGB-D (Song et al., 2015) comprises of 5K RGB-D images annotated with amodal, 3D-oriented bounding boxes for objects from 37 categories.\nWe employ the encoder of the SR-UNet (Choy et al., 2019) as the detection encoder and transfer the pre-trained weights for network initialization. For the decoder design of 3D detection, we consider VoteNet (Qi et al., 2019) and CAGroup3D (Wang et al., 2022a). VoteNet is a classical 3D detection approach commonly used by existing pre-training methods (Xie et al., 2020; Zhang et al., 2021; Rao et al., 2021; Huang et al., 2021). However, VoteNet\u2019s performance lags behind the state-ofthe-art. Therefore, we also evaluate our approach on CAGroup3D, which offers a stronger decoder architecture and delivers state-of-the-art 3D detection performance. During training, for the VoteNet setting, we fine-tune our model with the Adam optimizer (the batch size is 8, and the initial learning rate is 0.001). The learning rate is decreased by a factor of 10 after 80 and 120 epochs. For the CAGroup3D setting, we decrease the learning rate at 80 and 110 epochs.\nAs shown in Table 2, on ScanNet, our approach (MVNet) improves VoteNet by +4.5/+3.1 points on mAP@0.25 and mAP@0.5, outperforming other pre-training approaches. When replacing VoteNet with CAGroup3D, MVNet further elevates the mAP@0.25/mAP@0.5 to 76.0/62.4. On the SUN RGB-D dataset, MVNet also exhibits consistent improvements. Specifically, MVNet achieves +3.8/+4.6 points on mAP@0.25 and mAP@0.5 under the VoteNet backbone and 67.6/51.7 points on mAP@0.25 and mAP@0.5 under the CAGroup3D backbone.\n3D shape part segmentation In addition to scene-level downstream tasks, we also evaluate MVNet at the shape level. Our experiments on 3D shape part segmentation utilizes the ShapeNetPart (Yi et al., 2016), which comprises 16,880 models across 16 shape categories. We adopt the standard evaluation protocol as described by (Qi et al., 2017b), sampling 2,048 points from each shape. We report the evaluation metrics in terms of per-class mean Intersection over Union (cls. mIoU) and instance-level mean Intersection over Union (ins. mIoU).\nTo make a fair comparison, in accordance with (Yu et al., 2022), we employ a standard transformer network with a depth of 12 layers, a feature dimension of 384, and 6 attention heads, denoted as MVNet-B. The experimental results are summarized in Table 3. With the same network backbone, our approach demonstrates consistent improvements. Additionally, more results involving pre-training approaches with different network backbones can be found in the supplementary material.\n3D shape classification We also conduct experiments on the real-world ScanObjectNN dataset (Uy et al., 2019), which contains approximately 15,000 scanned objects distributed across 15 classes. The dataset is partitioned into three evaluation splits: OBJ-BG, OBJ-ONLY, and PB-T50-RS, where PBT50-RS is considered the most challenging one. We employ the same transformer-based classification network (MVNet-B) as described by (Yu et al., 2022). The experimental outcomes, presented in Table 3, reveal consistent performance gains across all three test sets.\nModel scalability The scalability of a model is a crucial factor when evaluating the effectiveness of a pre-training approach. Unfortunately, many previous methods overlook this aspect. The commonly used transformer architecture has limited capacity due to the small number of parameters. To further\nsubstantiate the efficacy of our proposed approach, we improve the network\u2019s feature dimension to 768 and double the number of attention heads to 12. As evidenced by the results in Table 3, our scaled model, MVNet-L, attains significant improvements across all evaluated settings."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "We proceed to present an ablation study to justify our design choices. Due to the space constraint, we focus on the semantic segmentation results using the ScanNet dataset.\nChoice of 2D pre-trained models As one of the most popular backbone architectures, ViT (Dosovitskiy et al., 2020) has two variants: ViT-S and ViT-B. ViT-S (Small) contains 12 transformer blocks, each of which has 6 heads and 384 hidden dimensions. ViT-B (Base) contains 12 transformer blocks, each of which has 12 heads and 768 hidden dimensions. We also compare four different pretraining methods. As shown in Figure 5, \u2018ViT-X\u2019,\u2018CLIPX\u2019, \u2018SAM-X\u2019, and \u2018DINOv2-X\u2019 represent pre-training the model by ImageNet-1K classification (Deng et al., 2009), the model with the CLIP approach (Radford et al., 2021), the model with the SAM approach (Kirillov et al., 2023), and the model with DINOv2 (Oquab et al., 2023), respectively. We observe that all the models show improvements compared with training from scratch (72.2 mIoU). Larger\nmodels (X-B) show consistent improvements compared with smaller models (X-S). DINOv2-B yields the best performance in terms of semantic segmentation accuracy. This demonstrates the advantage of leveraging DINOv2\u2019s strong transferability properties in our method.\nLoss design Table 4a demonstrates the performance of our method with different loss configurations. We investigate the effectiveness of each loss component: L2d (2D knowledge transfer loss) and Lm (multi-view consistency loss). The table shows that using either loss independently leads to an improvement in performance. However, combining both losses yields the best results, validating the importance of incorporating both 2D knowledge transfer and multi-view consistency in our approach.\nMasking ratio During pre-training, our approach involved masking the input point cloud guided by the corresponding RGB image. Following the conventional practice in masked autoencoder-based pre-training (He et al., 2022), we divided the image into regular non-overlapping patches. Then we sample a subset of patches and mask the remaining ones. The remaining part is projected onto the point cloud, forming the input for our method. We employ this masking operation as a data augmentation technique, which assists in the development of more robust features during pre-training. As evidenced in Table 4b, an appropriate masking operation (i.e., 30%) during pre-training could enhance performance in downstream tasks.\nView overlap ratio In Table 4c, we examine the impact of the view overlap ratio on the performance of our method. The view overlap ratio refers to the percentage of a shared field of view between the\ntwo views used for multi-view consistency. It is important to note that in this experiment, we only use one candidate second view. Compared to the results without the multi-view consistency loss (74.7 mIoU), overlaps between 40% and 80% all result in improvements. However, when the overlap is too small, specifically, at 20%, we fail to observe any improvement. We believe this is because when the overlap is insufficient, multi-view correspondences become too sparse, obtaining very limited 3D signals. As a result, our default setting selects views with overlaps in the range of [40%, 80%] as candidate second views, which ensures a balance between shared and unshared information for robust model performance.\nNumber of views In Table 4d, we investigate the impact of using different numbers of candidate second views. We experiment with 1 view, 2 views, 4 views, and 5 views. L2d denotes only adding 2D knowledge transfer loss, while Lm represents only adding multi-view consistency loss. We find that incorporating more views leads to improved performance for both modules. However, it shows minor performance variations for 2D knowledge transfer module. Our interpretation is that since the loss is applied on each view independently, the different number of candidate views exert minimal influence. Note that we did not experiment with more views as we observed that when the number of views exceeds 5, some candidates have a very small overlap with the first scan, which, as proven in the previous paragraph, can impede the learning of the multi-view consistency module.\n2D knowledge transfer loss We proceed to investigate the optimal locations to enforce the 2D knowledge transfer loss. We explore the impact of incorporating the loss starting from different layers of fk. As shown in Figure 6, adding the feature-based loss starting from the third layer yields the best performance in terms of semantic segmentation val mIoU on ScanNet. When adding the loss on the first and second layers, the model\u2019s performance is negatively impacted. We believe this is because the inputs of the 2D pre-trained model and the 2D neural network differ. Forcefully adding the loss on the early layers may not be beneficial. Furthermore, the performance drops when the loss is added starting from the fourth layer and beyond. This demonstrates that only regularizing the last few layers weakens the knowledge transferred from the pre-trained networks."
        },
        {
            "heading": "5 CONCLUSION AND LIMITATION",
            "text": "In this paper, we present a novel approach to 3D point-cloud pre-training. We leverage pre-trained 2D networks to supervise 3D pre-training through perspective projection and hierarchical feature-based 2D knowledge transfer. We also introduce a novel pre-task to enhance the geometric structure through correspondence prediction. Extensive experiments demonstrate the effectiveness of our approach.\nPotential avenues for future work include exploring additional pre-training data, such as NYUv2 (Couprie et al., 2013), and investigating the effect of increasing the size of the feature encoding network. Furthermore, we plan to extend our approach to outdoor settings. MVNet opens up new avenues for 3D point-cloud pre-training and provides a promising direction for future research.\nAcknowledgement. We would like to acknowledge the gifts from Google, Adobe, Wormpex AI, and support from NSF IIS-2047677, HDR-1934932, CCF-2019844, and IARPA WRIVA program."
        }
    ],
    "year": 2024
}